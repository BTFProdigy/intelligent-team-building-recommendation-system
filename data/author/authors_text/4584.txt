Architecture and Design Considerations in NESPOLE!:
a Speech Translation System for E-commerce Applications
Alon Lavie,
Chad Langley,
Alex Waibel
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
Fabio Pianesi,
Gianni Lazzari,
Paolo Coletti
ITC-irst
Trento, Italy
Loredana Taddei,
Franco Balducci
AETHRA
Ancona, Italy
1. INTRODUCTION
NESPOLE! 1 is a speech-to-speech machine translation research
project funded jointly by the European Commission and the US
NSF. The main goal of the NESPOLE! project is to advance the
state-of-the-art of speech-to-speech translation in a real-world set-
ting of common users involved in e-commerce applications. The
project is a collaboration between three European research labs
(IRST in Trento Italy, ISL at University of Karlsruhe in Germany,
CLIPS at UJF in Grenoble France), a US research group (ISL at
Carnegie Mellon in Pittsburgh) and two industrial partners (APT
- the Trentino provincial tourism bureau, and Aethra - an Italian
tele-communications commercial company). The speech-to-speech
translation approach taken by the project builds upon previous work
that the research partners conducted within the context of the C-
STAR consortium (see http://www.c-star.org). The pro-
totype system developed in NESPOLE! is intended to provide ef-
fective multi-lingual speech-to-speech communication between all
pairs of four languages (Italian, German, French and English) within
broad, but yet restricted domains. The first showcase currently un-
der development is in the domain of tourism and travel information.
The NESPOLE! speech translation system is designed to be an
integral part of advanced e-commerce technology of the next gener-
ation. We envision a technological scenario in which multi-modal
(speech, video and gesture) interaction plays a significant role, in
addition to the passive browsing of pre-designed web pages as is
common in e-commerce today. The interaction between client and
provider will need to support online communication with agents
(both real and artificial) on the provider side. The language barrier
then becomes a significant obstacle for such online communica-
tion between the two parties, when they do not speak a common
language. Within the tourism and travel domain, one can imagine
a scenario in which users (the clients) are planning a recreational
trip and are searching for specific detailed information about the
1NESPOLE! - NEgotiating through SPOken Lan-
guage in E-commerce. See the project website at
http://nespole.itc.it/
.
regions they wish to visit. Initial general information is obtained
from a web site of a tourism information provider. When more
detailed or special information is required, the customer has the
option of opening an online video-conferencing connection with a
human agent of the tourism information provider. Speech transla-
tion is integrated within the video-conference connection; the two
parties each speak in their native language and hear the synthesized
translation of the speech of the other participant. Text translation
(in the form of subtitles) can also be provided. Some multi-modal
communication between the parties is also available. The provider
agent can send web pages to the display of the customer, and both
sides can annotate and refer to pictures and diagrams presented on
a shared whiteboard application.
In this paper we describe the design considerations behind the ar-
chitecture that we have developed for the NESPOLE! speech trans-
lation system in the scenario described above. In order to make the
developed prototype as realistic as possible for use by a common
user, we assume only minimal hardware and software is available
on the customer side. This does include a PC-type video camera,
commercially available internet video-conferencing software (such
as Microsoft Netmeeting), standard audio and video hardware and
a standard web browser. However, no speech recognition and/or
translation software is assumed to reside locally on the PC of the
customer. This implies a server-type architecture in which speech
recognition and translation are accomplished via interaction with
a dedicated server. The extent to which this server is centralized
or distributed is one of the major design considerations taken into
account in our system.
2. NESPOLE! INTERLINGUA-BASED
TRANSLATION APPROACH
Our translation approach builds upon previous work that we have
conducted within the context of the C-STAR consortium. We use
an interlingua-based approach with a relatively shallow task-oriented
interlingua representation [2] [1], that was initially designed for the
C-STAR consortium and has been significantly extended for the
NESPOLE! project. Interlingual machine translation is convenient
when more than two languages are involved because it does not re-
quire each language to be connected by a set of transfer rules to
each other language in each direction [3]. Adding a new language
that has all-ways translation with existing languages requires only
writing one analyzer that maps utterances into the interlingua and
one generator that maps interlingua representations into sentences.
The interlingua approach also allows each partner group to imple-
ment an analyzer and generator for its home language only. A fur-
Figure 1: General Architecture of NESPOLE! System
ther advantage is that it supports a paraphrase generation back into
the language of the speaker. This provides the user with some con-
trol in case the analysis of an utterance failed to produce a correct
interlingua. The following are three examples of utterances tagged
with their corresponding interlingua representation:
Thank you very much
c:thank
And we?ll see you on February twelfth.
a:closing (time=(february, md12))
On the twelfth we have a single and a double
available.
a:give-information+availability+room
(room-type=(single & double),time=(md12))
3. NESPOLE! SYSTEM ARCHITECTURE
DESIGN
Several main considerations were taken into account in the de-
sign of the NESPOLE! Human Language Technology (HLT) server
architecture: (1) The desire to cleanly separate the actual HLT
system from the communication channel between the two parties,
which makes use of the speech translation capabilities provided by
the HLT system; (2) The desire to allow each research site to in-
dependently develop its language specific analysis and generation
modules, and to allow each site to easily integrate new and im-
proved components into the global NESPOLE! HLT system; and
(3) The desire of the research partners to build to whatever ex-
tent possible upon software components previously developed in
the context of the C-STAR consortium. We will discuss the ex-
tent to which the designed architecture achieves these goals after
presenting an overview of the architecture itself.
Figure 1 shows the general architecture of the current NESPOLE!
system. Communication between the client and agent is facilitated
by a dedicated module - the Mediator. This module is designed to
control the video-conferencing connection between the client and
the agent, and to integrate the speech translation services into the
communication. The mediator handles audio and video data as-
sociated with the video-conferencing application and binary data
associated with a shared whiteboard application. Standard H.323
data formats are used for these three types of data transfer. Speech-
to-speech translation of the utterances captured by the mediator is
accomplished through communication with the NESPOLE! global
HLT server. This is accomplished via socket connections with
language-specific HLT servers. The communication between the
mediator and each HLT server consists mainly of linear PCM au-
dio packets (some text and control messages are also supported and
are described later in this section).
Communication with Mediator
Speech
Recognizer
Module
Parser/Analysis
IF
text
Analysis Chain
Speech
Synthsizer
Generation
Module
IF
text
.
Generation
Chain
Communication with CommSwitch
audio audio
Language X HLT Server
Figure 2: Architecture of NESPOLE! Language-specific HLT Servers
The global NESPOLE! HLT server comprises four separate lang-
uage-specific servers. Additional language-specific HLT servers
can easily be integrated in the future. The internal architecture
of each language-specific HLT server is shown in figure 2. Each
language-specific HLT server consists of an analysis chain and a
generation chain. The analysis chain receives an audio stream cor-
responding to a single utterance and performs speech recognition
followed by parsing and analysis of the input utterance into the in-
terlingua representation (IF). The interlingua is then transmitted to
a central HLT communication switch (the CS), that forwards it to
the HLT servers for the other languagesas appropriate. IF messages
received from the central communication switch are processed by
the generation chain. A generation module first generates text in
the target language from the IF. The text utterance is then sent to
a speech synthesis module that produces an audio stream for the
utterance. The audio is then communicated externally to the me-
diator, in order to be integrated back into the video-conferencing
stream between the two parties.
The mediator can, in principle, support multiple one-to-one com-
munication sessions between client and agent. However, the de-
sign supports multiple mediators, which, for example, could each
be dedicated to a different provider application. Communication
with the mediator is initiated by the client by an explicit action
via the web browser. This opens a communication channel to the
mediator, which contacts the agent station, establishes the video-
conferencing connection between client and agent, and starts the
whiteboard application. The specific pair of languages for a dia-
logue is determined in advance from the web page from which the
client initiates the communication. The mediator then establishes a
socket communication channel with the two appropriate language
specific HLT servers. Communication between the two language
specific HLT servers, in the form of IF messages, is facilitated by
the NESPOLE! global communication switch (the CS). The lan-
guage specific HLT servers may in fact be physically distributed
over the internet. Each language specific HLT server is set to ser-
vice analysis requests coming from the mediator side, and genera-
tion requests arriving from the CS.
Some further functionality beyond that described above is also
supported. As described earlier, the ability to produce a textual
paraphrase of an input utterance and to display it back to the orig-
inal speaker provides useful user control in the case of translation
failures. This is supported in our system in the following way. In
addition to the translated audio, each HLT server also forwards the
generated text in the output language to the mediator, which then
displays the text on a dedicated application window on the PC of
the target user. Additionally, at the end of the processing of an in-
put utterance by the analysis chain of an HLT server, the resulting
IF is passed internally to the generation chain, which produces a
text generation from the IF. The result is a textual paraphrase of the
input utterance in the source language. This text is then sent back
to the mediator, which forwards it to the party from which the ut-
terance originated. The paraphrase is then displayed to the original
speaker in the dedicated application window. If the paraphrase is
wrong, it is likely that the produced IF was incorrect, and thus the
translation would also be wrong. The user may then use a button
on the application interface to signal that the last displayed para-
phrase was wrong. This action triggers a message that is forwarded
by the mediator to the other party, indicating that the last displayed
translation should be ignored. Further functionality is planned to
support synchronization between multi-modal events on the white-
board and their corresponding speech actions. As these are in very
preliminary stages of planning we do not describe them here.
4. DISCUSSION AND CONCLUSIONS
We believe that the architectural design described above has sev-
eral strengths and advantages. The clean separation of the HLT
server dedicated to the speech translation services from the exter-
nal communication modules between the two parties allows the re-
search partners to develop the HLT modules with a large degree
of independence. Furthermore, this separation will allow us in the
future to explore other types of mediators for different types of ap-
plications. One such application being proposed for development
within the C-STAR consortium is a speech-to-speech translation
service over mobile phones. The HLT server architecture described
here would be able to generally support such alternative external
communication modalities as well.
The physical distribution of the individual language specific HLT
servers allows each site to independently develop, integrate and
test its own analysis and generation modules. The organization of
each language specific HLT server as an independent module al-
lows each of the research sites to develop its unique approaches to
analysis and generation, while adhering to a simple communication
protocol between the HLT servers and externally with the mediator.
This allowed the research partners to ?jump-start? the project with
analysis and generation modules previously developed for the C-
STAR consortium, and incrementally develop these modules over
time. Furthermore, the global NESPOLE! communication switch
(the CS) supports testing of analysis and generation among the four
languages in isolation from the external parts of the system. Cur-
rently, requests for analysis of a textual utterance can be transmitted
to the HLT servers via the CS, with the resulting IF sent (via the CS)
to all HLT servers for generation. This gives us great flexibility in
developing and testing our translation system. The functionality of
the CS was originally developed for our previous C-STAR project,
and was reused with little modification.
Support for additional languages is also very easy to incorpo-
rate into the system by adding new language-specific HLT servers.
Any new language specific HLT server needs only to adhere to the
communication protocols with both the global NESPOLE! commu-
nication switch (the CS) and the external mediator. The C-STAR
consortium plans to use the general architecture described here for
its next phase of collaboration, with support for at least three asian
languages (Japanese, Korean and Chinese) in addition to the lan-
guages currently covered by the NESPOLE! project.
The first prototype of the NESPOLE! speech translation system
is currently in advanced stages of full integration. A showcase
demonstration of the prototype system to the European Commis-
sion is currently scheduled for late April 2001.
5. ACKNOWLEDGMENTS
The research work reported here was supported in part by the
National Science Foundation under Grant number 9982227. Any
opinions, findings and conclusions or recomendations expressed in
this material are those of the author(s) and do not necessarily reflect
the views of the National Science Foundation (NSF).
6. REFERENCES
[1] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[2] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[3] S. Nirenburg, J. Carbonell, M. Tomita, and K. Goodman.
Machine Translation: A Knowledge-Based Approach. Morgan
Kaufmann, San Mateo, California, 1992.
Domain Portability in Speech-to-Speech Translation
Alon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin Han
Alicia Tribble, Donna Gates, Dorcas Wallace and Kay Peterson
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
1. INTRODUCTION
Speech-to-speech translation has made significant advances over
the past decade, with several high-visibility projects (C-STAR, Verb-
mobil, the Spoken Language Translator, and others) significantly
advancing the state-of-the-art. While speech recognition can cur-
rently effectively deal with very large vocabularies and is fairly
speaker independent, speech translation is currently still effective
only in limited, albeit large, domains. The issue of domain porta-
bility is thus of significant importance, with several current research
efforts designed to develop speech-translation systems that can be
ported to new domains with significantly less time and effort than
is currently possible.
This paper reports on three experiments on portability of a speech-
to-speech translation system between semantic domains.1 The ex-
periments were conducted with the JANUS system [5, 8, 12], ini-
tially developed for a narrow travel planning domain, and ported
to the doctor-patient domain and an extended tourism domain. The
experiments cover both rule-based and statistical methods, and hand-
written as well as automatically learned rules. For rule-based sys-
tems, we have investigated the re-usability of rules and other knowl-
edge sources from other domains. For statistical methods, we have
investigated how much additional training data is needed for each
new domain. We are also experimenting with combinations of
hand-written and automatically learned components. For speech
recognition, we have conducted studies of what parameters change
when a recognizer is ported from one domain to another, and how
these changes affect recognition performance.
2. DESCRIPTION OF THE INTERLINGUA
The first two experiments concern the analysis component of our
interlingua-based MT system. The analysis component takes a sen-
tence as input and produces an interlingua representation as output.
We use a task-oriented interlingua [4, 3] based on domain actions.
Examples of domain actions are giving information about the on-
set of a symptom (e.g., I have a headache) or asking a patient
1We have also worked on the issue of portability across languages
via our interlingua approach to translation [3] and on portability of
speech recognition across languages [10].
.
to perform some action (e.g., wiggle your fingers). The interlin-
gua, shown in the example below, has five main components: (1) a
speaker tag such as a: for doctor (agent) and c: for a patient (cus-
tomer), (2) a speech act, in this case, give-information (3)
some concepts (+body-state and+existence), and (4) some
arguments (body-state-spec= andbody-location=), and
(5) some sub-arguments (identifiability=no and
inside=head).
I have a pain in my head.
c:give-information+existence+body-state
(body-state-spec=(pain,identifiability=no),
body-location=(inside=head))
3. EXPERIMENT 1:
EXTENSION OF SEMANTIC GRAMMAR
RULES BY HAND AND BY AUTOMATIC
LEARNING
Experiment 1 concerns extension of the coverage of semantic
grammars in the medical domain. Semantic grammars are based
on semantic constituents such as request information phrases (e.g.,
I was wondering : : : ) and location phrases (e.g., in my right arm)
rather than syntactic constituents such as noun phrases and verb
phrases. In other papers [12, 5], we have described how our mod-
ular grammar design enhances portability across domains. The
portable grammar modules are the cross-domain module, contain-
ing rules for things like greetings, and the shared module, contain-
ing rules for things like times, dates, and locations. Figure 1 shows
a parse tree for the sentence How long have you had this pain? XDM
indicates nodes that were produced by cross-domain rules. MED in-
dicates nodes that were produced by rules from the new medical
domain grammar.
The preliminary doctor-patient grammar focuses on three med-
ical situations: give-information+existence ? giving
information about the existence of a symptom (I have been get-
ting headaches); give-information+onset ? giving infor-
mation about the onset of a symptom (The headaches started three
months ago); and give-information+occurrence ? giv-
ing information about the onset of an instance of the symptoms
(The headaches start behind my ears). Symptoms are expressed
as body-state (e.g., pain), body-object (e.g., rash), and
body-event (e.g., bleeding).
Our experiment on extendibility was based on a hand written
seed grammar that was extended by hand and by automatic learn-
ing. The seed grammar covered the domain actions mentioned
above, but did not cover very many ways to phrase each domain
action. For example, it might have covered The headaches started
[request-information+existence+body-state]::MED
( WH-PHRASES::XDM
( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )
HAVE-GET-FEEL::MED ( GET ( have ) ) you
HAVE-GET-FEEL::MED ( HAS ( had ) )
[super_body-state-spec=]::MED
( [body-state-spec=]::MED
( ID-WHOSE::MED
( [identifiability=]
( [id:non-distant] ( this ) ) )
BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )
Figure 1: Parser output with nodes produced by medical and cross-domain grammars.
Seed Extended Learned
IF 37.2 37.2 31.3
Domain Action 37.2 37.2 31.3
Speech Act
Recall 43.3 48.2 49.3
Precision 71.0 75.0 45.8
Concept List
Recall 2.2 10.1 32.5
Precision 12.5 42.2 25.1
Top-Level Arguments
Recall 0.0 7.2 29.6
Precision 0.0 42.2 34.4
Top-Level Values
Recall 0.0 8.3 29.8
Precision 0.0 50.0 39.2
Sub-Level Arguments
Recall 0.0 28.3 14.1
Precision 0.0 48.2 12.6
Sub-level Values
Recall 1.2 28.3 14.1
Precision 6.2 48.2 12.9
Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data
three months ago but not I started getting the headaches three months
ago. The seed grammar was extended by hand and by automatic
learning to cover a development set of 133 utterances. The re-
sult was two new grammars, a human-extended grammar and a
machine-learned grammar, referred to as the extended and learned
grammars in Table 1. The two new grammars were then tested on
132 unseen sentences in order to compare generality of the rules.
Results are reported only for 83 of the 132 sentences which were
covered by the current interlingua design. The remaining 49 sen-
tences were not covered by the current interlingua design and were
not scored. Results are shown in Table 1.
The parsed test sentences were scored in comparison to a hand-
coded correct interlingua representation. Table 1 separates results
for six components of the interlingua: speech act, concepts, top-
level arguments, top-level values, sub-level arguments, and sub-
level values, in addition to the total interlingua, and the domain
action (speech act and concepts combined). The components of the
interlingua were described in Section 2.
The scores for the total interlingua and domain action are re-
ported as percent correct. The scores for the six components of the
interlingua are reported as average percent precision and recall. For
example, if the correct interlingua for a sentence has two concepts,
and the parser produces three, two of which are correct and one of
which is incorrect, the precision is 66% and the recall is 100%.
Several trends are reflected in the results. Both the human-ex-
tended grammar and the machine-learned grammar show improved
performance over the seed grammar. However, the human extended
grammar tended to outperform the automatically learned grammar
in precision, whereas the automatically learned grammar tended to
outperform the human extended grammar in recall. This result is to
be expected: humans are capable of formulating correct rules, but
may not have time to analyze the amount of data that a machine can
analyze. (The time spent on the human extended grammar after the
seed grammar was complete was only five days.)
Grammar Induction: Our work on automatic grammar induc-
tion for Experiment 1 is still in preliminary stages. At this point,
we have experimented with completely automatic induction (no in-
teraction with a user)2 of new grammar rules starting from a core
grammar and using a development set of sentences that are not
parsable according to the core grammar. The development sen-
tences are tagged with the correct interlingua, and they do not stray
from the concepts covered by the core grammar ? they only cor-
respond to alternative (previously unseen) ways of expressing the
same set of covered concepts. The automatic induction is based
on performing tree matching between a skeletal tree representation
obtained from the interlingua, and a collection of parse fragments
2Previous work on our project [2] investigated learning of grammar
rules with user interaction.
[give-information+onset+symptom]
[manner=]
[sudden]
suddenly
[symptom-location=]
DETP
DET
POSS
my
BODYLOCATION
BODYFLUID
[urine]
urine
became [adj:symptom-name=]
ADJ-SYMPTOM
FUNCTION-ADJ-VALS [attribute=]
[color_attribute]
colored
[abnormal]
dis
Parse chunk #1 Parse chunk #2 Parse chunk #3
Original interlingua:
give-information+onset+symptom
(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,
manner=sudden)
Learned Grammar Rule:
s[give-information+onset+symptom]
( [manner=] [symptom-location=] *+became [adj:symptom-name=] )
Figure 2: A reconstructed parse tree from the Interlingua
that is derived from parsing the new sentence with the core gram-
mar. Extensions to the existing rules are hypothesized in a way that
would produce the correct interlingua representation for the input
utterance.
Figure 2 shows a tree corresponding to an automatically learned
rule. The input to the learning algorithm is the interlingua (shown
in bold boxes in the figure) and three parse chunks (circled in the
figure). The dashed edges are augmented by the learning algorithm.
4. EXPERIMENT 2:
PORTING TO A NEW DOMAIN
USING A HYBRID RULE-BASED AND
STATISTICAL ANALYSIS APPROACH
We are in the process of developing a new alternative analysis
approach for our interlingua-based speech-translation systems that
combines rule-based and statistical methods and we believe inher-
ently supports faster porting into new domains. The main aspects
of the approach are the following. Rather than developing com-
plete semantic grammars for analyzing utterances into our interlin-
gua (either completely manually, or using grammar induction tech-
niques), we separate the task into two main levels. We continue to
develop and maintain rule-based grammars for phrases that corre-
spond to argument-level concepts of our interlingua representation
(e.g., time expressions, locations, symptom-names, etc.). However,
instead of developing grammar rules for assembling the argument-
level phrases into appropriate domain actions, we apply machine
learning and classification techniques [1] to learn these mappings
from a corpus of interlingua tagged utterances. (Earlier work on
this task is reported in [6].)
We believe this approach should prove to be more suitable for
fast porting into new domains for the following reasons. Many of
the required argument-level phrase grammars for a new domain are
likely to be covered by already existing grammar modules, as can
be seen by examining the XDM (cross-domain) nodes in Figure 1.
The remaining new phrase grammars are fairly fast and straightfor-
ward to develop. The central questions, however, are whether the
statistical methods used for classifying strings of arguments into
domain actions are accurate enough, and what amounts of tagged
data are required to obtain reasonable levels of performance. To
assess this last question, we tested the performance of the current
speech-act and concept classifiers for the expanded travel-domain
when trained with increasing amounts of training data. The results
of these experiments are shown in Figure 3. We also report the
performance of the domain-action classification derived from the
combined speech-act and concepts. As can be seen, performance
reaches a relative plateau at around 4000-5000 utterances. We see
these results as indicative that this approach should indeed prove to
be significantly easier to port to new domains. Creating a tagged
database of this order of magnitude can be done in a few weeks,
rather than the months required for complete manual grammar de-
velopment time.
5. EXPERIMENT 3:
PORTING THE SPEECH RECOGNIZER
TO NEW DOMAINS
When the speech recognition components (acoustic models, pro-
nunciation dictionary, vocabulary, and language model) are ported
across domains and languages mainly three types of mismatches
Speech Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Concept Sequence Classification Accuracy for 16-
fold Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Dialog Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Figure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training Data
Baseline Systems WER on Different Tasks [%]
BN (Broadcast News) h4e98 1, all F-conditions 18.5
ESST (scheduling and travel planning domain) 24.3
BN+ESST 18.4
C-STAR (travel planning domain) 20.2
Adaptation!Meeting Recognition
ESST on meeting data 54.1
BN on meeting data 44.2
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (16 meetings) 38.7
BN+ESST on meeting data 42.2
+ language model interpolation (16 meetings) 39.0
Adaptation! Doctor-Patient Domain
C-STAR on doctor-patient data 34.1
+ language model interpolation ( 34 dialogs) 25.1
Table 2: Recognition Results
occur: (1) mismatches in recording condition; (2) speaking style
mismatches; as well as (3) vocabulary and language model mis-
matches. In the past these problems have mostly been solved by
collecting large amounts of acoustic data for training the acoustic
models and development of the pronunciation dictionary, as well
as large text data for vocabulary coverage and language model cal-
culation. However, especially for highly specialized domains and
conversational speaking styles, large databases cannot always be
provided. Therefore, our research has focused on the problem of
how to build LVCSR systems for new tasks and languages [7, 9]
using only a limited amount of data. In this third experiment we
investigate the results of porting the speech recognition component
of our MT system to different new domains. The experiments and
improvements were conducted with the Janus Speech Recognition
Toolkit JRTk [13].
Table 2 shows the results of porting four baseline speech recog-
nition systems to the doctor-patient domain, and to the meeting do-
main. The four baseline systems are trained on Broadcast News
(BN), English SpontaneousScheduling Task (ESST), combined BN
and ESST, and the travel planning domain of the C-STAR consor-
tium (http://www.c-star.org). The given tasks illustrate
a variety of domain size, speaking styles and recording conditions
ranging from clean spontaneous speech in a very limited domain
(ESST, C-STAR) to highly conversational multi-party speech in an
extremely broad domain (Meeting). As a consequence the error
rates on the meeting data are quite high but using MAP (Maximum
A Posteriori) acoustic model adaptation and language model adap-
tation the error rate can be reduced by about 10.2% relative over the
BN baseline system. With the doctor-patient data the drop in error
rate was less severe which can be explained by the similar speaking
style and recording conditions for C-STAR and doctor-patient data.
Details about the applied recognition engine can be found in [10]
for ESST and [11] for the BN system.
6. ACKNOWLEDGMENTS
The research work reported here was funded in part by the DARPA
TIDES Program and supported in part by the National Science
Foundation under Grant number 9982227. Any opinions, findings
and conclusions or recomendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the
National Science Foundation (NSF) or DARPA.
7. REFERENCES
[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. TiMBL: Tilburg Memory Based Learner, version 3.0
Reference Guide. Technical Report Technical Report 00-01,
ILK, 2000. Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`. Epiphenomenal Grammar Acquisition with
GSG. In Proceedings of the Workshop on Conversational
Systems of the 6th Conference on Applied Natural Language
Processing and the 1st Conference of the North American
Chapter of the Association for Computational Linguistics
(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,
D. Koll, and A. Waibel. The Janus-III Translation System.
Machine Translation. To appear.
[6] M. Munk. Shallow statistical parsing for machine translation.
Master?s thesis, University of Karlsruhe, Karlsruhe,
Germany, 1999. http://www.is.cs.cmu.edu/papers/
speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel. Polyphone Decision Tree
Specialization for Language Adaptation. In Proceedings of
the ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel. Interactive Translation of Conversational Speech.
Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,
and M. Woszczyna. Multilinguality in Speech and Spoken
Language Systems. Proceedings of the IEEE, Special Issue
on Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.
Multilingual Speech Recognition, chapter From Speech Input
to Augmented Word Lattices, pages 33?45. Springer Verlag,
Berlin, Heidelberg, New York, artificial Intelligence edition,
2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,
F. Metze, and M. Bett. Advances in Meeting Recognition.
Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,
A. Lavie, L. Levin, and A. Waibel. A Modular Approach to
Spoken Language Translation for Large Domains. In
Proceedings of Conference of the Association for Machine
Translation in the Americas (AMTA?98), Langhorn, PA,
October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.
Recognition of Conversational Telephone Speech using the
Janus Speech Engine. In Proceedings of the ICASSP?97,
Mu?nchen, Germany, 1997.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 740?747, Vancouver, October 2005. c?2005 Association for Computational Linguistics
BLANC1: Learning Evaluation Metrics for MT
Lucian Vlad Lita and Monica Rogati and Alon Lavie
Carnegie Mellon University
{llita,mrogati,alavie}@cs.cmu.edu
Abstract
We introduce BLANC, a family of dy-
namic, trainable evaluation metrics for ma-
chine translation. Flexible, parametrized
models can be learned from past data and
automatically optimized to correlate well
with human judgments for different cri-
teria (e.g. adequacy, fluency) using dif-
ferent correlation measures. Towards this
end, we discuss ACS (all common skip-
ngrams), a practical algorithm with train-
able parameters that estimates reference-
candidate translation overlap by comput-
ing a weighted sum of all common skip-
ngrams in polynomial time. We show that
the BLEU and ROUGE metric families are
special cases of BLANC, and we compare
correlations with human judgments across
these three metric families. We analyze the
algorithmic complexity of ACS and argue
that it is more powerful in modeling both
local meaning and sentence-level structure,
while offering the same practicality as the
established algorithms it generalizes.
1 Introduction
Although recent MT evaluation methods show
promising correlations to human judgments in terms
of adequacy and fluency, there is still considerable
room for improvement (Culy and Riehemann, 2003).
Most of these studies have been performed at a sys-
tem level and have not investigated metric robust-
ness at a lower granularity. Moreover, even though
the emphasis on adequacy vs. fluency is application-
dependent, automatic evaluation metrics do not dis-
tinguish between the need to optimize correlation
with regard to one or the other.
Machine translation automatic evaluation metrics
face two important challenges: the lack of powerful
features to capture both sentence level structure and
local meaning, and the difficulty of designing good
functions for combining these features into meaning-
ful quality estimation algorithms.
In this paper, we introduce BLANC1, an automatic
MT evaluation metric family that is a generaliza-
tion of popular and successful metric families cur-
rently used in the MT community (BLEU, ROUGE, F-
measure etc.). We describe an efficient, polynomial-
time algorithm for BLANC, and show how it can be
optimized to target adequacy, fluency or any other
criterion. We compare our metric?s performance
with traditional and recent automatic evaluation met-
rics. We also describe the parameter conditions under
which BLANC can emulate them.
Throughout the remainder of this paper, we dis-
tinguish between two components of automatic MT
evaluation: the statistics computed on candidate
and reference translations and the function used in
defining evaluation metrics and generating transla-
tion scores. Commonly used statistics include bag-
of-words overlap, edit distance, longest common sub-
sequence, ngram overlap, and skip-bigram overlap.
Preferred functions are various combinations of pre-
cision and recall (Soricut and Brill, 2004), including
1Since existing evaluation metrics (e.g. BLEU, ROUGE) are
special cases of our metric family, it is only natural to name it
Broad Learning and Adaptation for Numeric Criteria (BLANC) ?
white light contains light of all frequencies
740
weighted precision and F-measures (Van-Rijsbergen,
1979).
BLANC implements a practical algorithm with
learnable parameters for automatic MT evaluation
which estimates the reference-candidate translation
overlap by computing a weighted sum of common
subsequences (also known as skip-ngrams). Com-
mon skip-ngrams are sequences of words in their
sentence order that are found both in the reference
and candidate translations. By generalizing and sep-
arating the overlap statistics from the function used
to combine them, and by identifying the latter as a
learnable component, BLANC subsumes the ngram
based evaluation metrics as special cases and can
better reflect the need of end applications for ade-
quacy/fluency tradeoffs .
1.1 Related Work
Initial work in evaluating translation quality focused
on edit distance-based metrics (Su et al, 1992; Akiba
et al, 2001). In the MT context, edit distance (Lev-
enshtein, 1965) represents the amount of word inser-
tions, deletions and substitutions necessary to trans-
form a candidate translation into a reference trans-
lation. Another evaluation metric based on edit dis-
tance is the Word Error Rate (Niessen et al, 2000)
which computes the normalized edit distance. BLEU
is a weighted precision evaluation metric introduced
by IBM (Papineni et al, 2001). BLEU and its exten-
sions/variants (e.g. NIST (Doddington, 2002)) have
become de-facto standards in the MT community and
are consistently being used for system optimization
and tuning. These methods rely on local features
and do not explicitly capture sentence-level features,
although implicitly longer n-gram matches are re-
warded in BLEU. The General Text Matcher (GTM)
(Turian et al, 2003) is another MT evaluation method
that rewards longer ngrams instead of assigning them
equal weight.
(Lin and Och, 2004) recently proposed a set of
metrics (ROUGE) for MT evaluation. ROUGE-L is a
longest common subsequence (LCS) based automatic
evaluation metric for MT. The intuition behind it is
that long common subsequences reflect a large over-
lap between a candidate translation and a reference
translation. ROUGE-W is also based on LCS, but
assigns higher weights to sequences that have fewer
gaps. However, these metrics still do not distinguish
among translations with the same LCS but different
number of shorter sized subsequences, also indica-
tive of overlap. ROUGE-S attempts to correct this
problem by combining the precision/recall of skip-
bigrams of the reference and candidate translations.
However, by using skip-ngrams with n?=2, we might
be able to capture more information encoded in the
higher level sentence structure. With BLANC, we
propose a way to exploit local contiguity in a man-
ner similar to BLEU and also higher level structure
similar to ROUGE type metrics.
2 Approach
We have designed an algorithm that can perform a
full overlap search over variable-size, non-contiguous
word sequences (skip-ngrams) efficiently. At first
glance, in order to perform this search, one has to
first exhaustively generate all skip-ngrams in the can-
didate and reference segments and then assess the
overlap. This approach is highly prohibitive since the
number of possible sequences is exponential in the
number of words in the sentence. Our algorithm ?
ACS (all common skip-ngrams) ? directly constructs
the set of overlapping skip-ngrams through incremen-
tal composition of word-level matches. With ACS,
we can reduce computation complexity to a fifth de-
gree polynomial in the number of words.
Through the ACS algorithm, BLANC is not limited
only to counting skip-ngram overlap: the contribu-
tion of different skip-ngrams to the overall score is
based on a set of features. ACS computes the over-
lap between two segments of text and also allows
local and global features to be computed during the
overlap search. These local and global features are
subsequently used to train evaluation models within
the BLANC family. We introduce below several sim-
ple skip-ngram-based features and show that special-
case parameter settings for these features emulate the
computation of existing ngram-based metrics. In or-
der to define the relative significance of a particular
skip-ngram found by the ACS algorithm, we employ
an exponential model for feature integration.
2.1 Weighted Skip-Ngrams
We define skip-ngrams as sequences of n words taken
in sentence order allowing for arbitrary gaps. In algo-
rithms literature skip-ngrams are equivalent to subse-
quences. As special cases, skip-ngrams with n=2 are
741
referred to as skip-bigrams and skip-ngrams with no
gaps between the words are simply ngrams. A sen-
tence S of size |S| has C(|S|, n) = |S|!(|S|?n)!n! skip-
ngrams.
For example, the sentence ?To be or not to be? has
C(6, 2) = 15 corresponding skip-bigrams including
?be or?, ?to to?, and three occurrences of ?to be?.
It also has C(6, 4) = 15 corresponding skip-4grams
(n = 4) including ?to be to be? and ?to or not to?.
Consider the following sample reference and can-
didate translations:
R0: machine translated text is evaluated automatically
K1: machine translated stories are chosen automatically
K2: machine and human together can forge a friendship that
cannot be translated into words automatically
K3: machine code is being translated automatically
The skip-ngram ?machine translated automati-
cally? appears in both the reference R0 and all candi-
date translations. Arguably, a skip-bigram that con-
tains few gaps is likely to capture local structure
or meaning. At the same time, skip-ngrams spread
across a sentence are also very useful since they may
capture part of the high level sentence structure.
We define a weighting feature function for skip-
ngrams that estimates how likely they are to capture
local meaning and sentence structure. The weighting
function ? for a skip-ngram w1 ..wn is defined as:
?(w1..wn) = e???G(w1..wn) (1)
where ? ? 0 is a decay parameter and G(w1..wn)
measures the overall gap of the skip-ngram w1..wn in
a specific sentence. This overall skip-ngram weight
can be decomposed into the weights of its constituent
skip-bigrams:
?(w1..wn) = e???G(w1,..,wn) (2)
= e???
Pn?1
i=1 G(wi,wi+1)
=
n?1
?
i=1
?(wi wi+1) (3)
In equation 3, ?(wi wi+1) is the number of words
between wi and wi+1 in the sentence. In the example
above, the skip-ngram ?machine translated automat-
ically? has weight e?3? for sentence K1 and weight
e?12? = 1 for sentence K2.
In our initial experiments the gap G has been ex-
pressed as a linear function, but different families of
functions can be explored and their corresponding pa-
rameters learned. The parameter ? dictates the be-
havior of the weighting function. When ? = 0 ?
equals e0 = 1, rendering gap sizes irrelevant. In this
case, skip-ngrams are given the same weight as con-
tiguous ngrams. When ? is very large, ? approaches
0 if there are any gaps in the skip-ngram and is 1 if
there are no gaps. This setting has the effect of con-
sidering only contiguous ngrams and discarding all
skip-ngrams with gaps.
In the above example, although the skip-ngram
?machine translated automatically? has the same cu-
mulative gap in both in K1 and K3, the occurrence in
K1 has is a gap distribution that more closely reflects
that of the reference skip-ngram in R0. To model gap
distribution differences between two occurrences of a
skip-ngram, we define a piece-wise distance function
?XY between two sentences x and y. For two succes-
sive words in the skip-ngram, the distance function is
defined as:
?XY (w1w2) = e???|GX(w1,w2)?GY (w1,w2)| (4)
where ? ? 0 is a decay parameter. Intuitively, the
? parameter is used to reward better aligned skip-
ngrams. Similar to the ? function, the overall ?XY
distance between two occurrences of a skip-ngram
with n > 1 is:
?XY (w1..wn) =
n?1
?
i=1
?XY (wiwi+1) (5)
Note that equation 5 takes into account pairs of skip-
ngrams skip in different places by summing over
piecewise differences. Finally, using an exponen-
tial model, we assign an overall score to the matched
skip-ngram. The skip-ngram scoring function Sxy al-
lows independent features to be incorporated into the
overall score:
Sxy(wi..wk) = ?(wi..wk) ? ?xy(wi..wk)
?e?1f1(wi..wk) ? ... ? e?hfh(wi..wk) (6)
where features f1..fh can be functions based on the
syntax, semantics, lexical or morphological aspects
of the skip-ngram. Note that different models for
combining skip-ngram features can be used in con-
junction with ACS.
742
2.2 Multiple References
In BLANC we incorporate multiple references in a
manner similar to the ROUGE metric family. We
compute the precision and recall of each size skip-
ngrams for individual references. Based on these we
combine the maximum precision and maximum re-
call of the candidate translation obtained using all
reference translations and use them to compute an ag-
gregate F-measure.
The F-measure parameter ?F is modeled by
BLANC. In our experiments we optimized ?F indi-
vidually for fluency and adequacy.
2.3 The ACS Algorithm
We present a practical algorithm for extracting All
Common Skip-ngrams (ACS) of any size that appear
in the candidate and reference translations. For clar-
ity purposes, we present the ACS algorithm as it
relates to the MT problem: find all common skip-
ngrams (ACS) of any size in two sentences X and Y :
wSKIP ? Acs(?, ?,X, Y ) (7)
= {wSKIP1..wSKIPmin(|X|,|Y |)} (8)
where wSkipn is the set of all skip-ngrams of size n
and is defined as:
wSKIPn = {?w1..wn? | wi ? X,wi ? Y,?i ? [1..n]
and wi ? wj ,?i < j ? [1..n]}
Given two sentences X and Y we observe a match
(w, x, y) if word w is found in sentence X at index x
and in sentence Y at index y:
(w, x, y) ? {0 ? x ? |X|, 0 ? y ? |Y |,
w ? V, and X[x] = Y [y] = w} (9)
where V is the vocabulary with a finite set of words.
In the following subsections, we present the fol-
lowing steps in the ACS algorithm:
1. identify all matches ? find matches and generate
corresponding nodes in the dependency graph
2. generate dependencies ? construct edges ac-
cording to pairwise match dependencies
3. propagate common subsequences ? count
all common skip-ngrams using corresponding
weights and distances
In the following sections we use the following exam-
ple to illustrate the intermediate steps of ACS.
X. ?to be or not to be?
Y. ?to exist or not be?
2.3.1 Step 1: Identify All Matches
In this step we identify all word matches (w, x, y)
in sentences X and Y . Using the example above, the
intermediate inputs and outputs of this step are:
Input: X. ?to be or not to be?
Y. ?to exist or not be?
Output: (to,1,1); (to,5,1); (or,3,3); (be,2,5); . . .
For each match we create a corresponding node N
in a dependency graph. With each node we associate
the actual word matched and its corresponding index
positions in both sentences.
2.3.2 Step 2: Generate Dependencies
A dependency N1 ? N2 occurs when the two
corresponding matches (w1, x1, y1) and (w2, x2, y2)
can form a valid common skip-bigram: i.e. when
x1 < x2 and y1 < y2. Note that the matches can
cover identical words, but their indices cannot be the
same (x1 6= x2 and y1 6= y2) since a skip-bigram
requires two different word matches.
In order to facilitate the generation of all common
subsequences, the graph is populated with the
appropriate dependency edges:
for each node N in DAG
for each node M 6=N in DAG
if N(x)?M(x) and N(y)?M(y)
create edge E: N?M
compute ?XY (E)
compute ?(E)
This step incorporates the concepts of skip-ngram
weight and distance into the graph. With each edge
E : N1 ? N2 we associate step-wise weight and dis-
tance information for the corresponding skip-bigram
formed by matches (w1, x1, y1) and (w2, x2, y2).
Note that rather than counting all skip-ngrams,
which would be exponential in the worst case sce-
nario, we only construct a structure of match depen-
dencies (i.e. skip-bigrams). As in dynamic program-
ming, in order to avoid exponential complexity, we
compute individual skip-ngram scores only once.
2.3.3 Step 3: Propagate Common Subsequences
In this last step, the ACS algorithm counts all com-
mon skip-ngrams using corresponding weights and
distances. In the general case, this step is equiva-
lent measuring the overlap of the two sentences X
and Y . As a special case, if no features are used, the
743
ACS algorithm is equivalent to counting the number
of common skip-ngrams regardless of gap sizes.
// depth first search (DFS)
for each node N in DAG
compute node N?s depth
// initialize skip-ngram counts
for each node N in DAG
vN [1]? 1
for i=2 to LCS(X,Y)
vN [i] = 0
// compute ngram counts
for d=1 to MAXDEPTH
for each node N of depth d in DAG
for each edge E: N?M
for i=2 to d
vM [i] += Sxy(?(E), ?(E), vN [i-1])
After algorithm ACS is run, the number of skip-
ngrams (weighted skip-ngram score) of size k is sim-
ply the sum of the number of skip-ngrams of size k
ending in each node N ?s corresponding match:
wSKIPk =
?
Ni?DAG
vNi [k] (10)
2.3.4 ACS Complexity and Feasibility
In the worst case scenario, both sentences X and Y
are composed of exactly the same repeated word: X
= ?w w w w .. ? and Y = ?w w w w ..?. We let m = |X|
and n = |Y |. In this case, the number of matches is
M = n ? m. Therefore, Step 1 has worst case time
and space complexity of O(m ? n). However, em-
pirical data suggest that there are far fewer matches
than in the worst-case scenario and the actual space
requirements are drastically reduced. Even in the
worst-case scenario, if we assume the average sen-
tences is fewer than 100 words, the number of nodes
in the DAG would only be 10, 000. Step 2 of the al-
gorithm consists of creating edges in the dependency
graph. In the worst case scenario, the number of di-
rected edges is O(M2) and furthermore if the sen-
tences are uniformly composed of the same repeated
word as seen above, the worst-case time and space
complexity is m(m+1)/2 ?n(n+1)/2 = O(m2n2).
In Step 3 of the algorithm, the DFS complexity for
computing of node depths is O(M) and the complex-
ity of LCS(X,Y ) is O(m ? n). The dominant step
is the propagation of common subsequences (skip-
ngram counts). Let l be the size of the LCS. The up-
per bound on the size of the longest common subse-
quence is min(|X|, |Y |) = min(m,n). In the worst
case scenario, for each node we propagate l count val-
ues (the size of vector v) to all other nodes in the
DAG. Therefore, the time complexity for Step 3 is
O(M2 ? l) = O(m2n2l) (fifth degree polynomial).
3 BLANC as a Generalization of BLEU and
ROUGE
Due to its parametric nature, the All Common Sub-
sequences algorithm can emulate the ngram compu-
tation of several popular MT evaluation metrics. The
weighting function ? allows skip-ngrams with differ-
ent gap sizes to be assigned different weights. Param-
eter ? controls the shape of the weighting function.
In one extreme scenario, if we allow ? to take
very large values, the net effect is that all contiguous
ngrams of any size will have corresponding weights
of e0 = 1 while all other skip-ngrams will have
weights that are zero. In this case, the distance
function will only apply to contiguous ngrams which
have the same size and no gaps. Therefore, the dis-
tance function will also be 1. The overall result is
that the ACS algorithm collects contiguous common
ngram counts for all ngram sizes. This is equivalent
to computing the ngram overlap between two sen-
tences, which is equivalent to the ngram computa-
tion performed BLEU metric. In addition to comput-
ing ngram overlap, BLEU incorporates a thresholding
(clipping) on ngram counts based on reference trans-
lations, as well as a brevity penalty which makes sure
the machine-produced translations are not too short.
In BLANC, this is replaced by standard F-measure,
which research (Turian et al, 2003) has shown it can
be used successfully in MT evaluation.
Another scenario consists of setting the ? and ?
parameters to 0. In this case, all skip-ngrams are as-
signed the same weight value of 1 and skip-ngram
matches are also assigned the same distance value of
1 regardless of gap sizes and differences in gap sizes.
This renders all skip-ngrams equivalent and the ACS
algorithm is reduced to counting the skip-ngram over-
lap between two sentences. Using these counts, pre-
cision and recall-based metrics such as the F-measure
can be computed. If we let the ? and ? parameters to
be zero, disregard redundant matches, and compute
744
0 50 100
0
50
100
150
200
Arabic 2003
Sentence Length
#s
en
te
nc
es
0 50 100
0
50
100
150
200
250
300
350
Chinese 2003
Sentence Length
#s
en
te
nc
es
0 50 100
100
102
104
ACS #Matches
Sentence Length
Av
g 
#M
at
ch
es
0 50 100
100
105
ACS #Edges
Sentence Length
Av
g 
#E
dg
es
0 50 100
100
105
1010
ACS #Feature Calls
Sentence Length
Av
g 
#T
ot
al
Arabic
Chinese
Worst Case
Figure 1: Empirical and theoretical behavior of ACS on 2003 machine translation evaluation data (semilog scale).
the ACS only for skip-ngrams of size 2, the ACS algo-
rithm is equivalent to the ROUGE-S metric (Lin and
Och, 2004). This case represents a specific parameter
setting in the ACS skip-ngram computation.
The longest common subsequence statistic has also
been successfully used for automatic machine trans-
lation evaluation in the ROUGE-L (Lin and Och,
2004) algorithm. In BLANC, if we set both ? and
? parameters to zero, the net result is a set of skip-
bigram (common subsequence) overlap counts for all
skip-bigram sizes. Although dynamic programming
or suffix trees can be used to compute the LCS much
faster, under this parameter setting the ACS algorithm
can also produce the longest common subsequence:
LCS(X,Y )? argmax
k
ACS(wSKIPk) > 0
where Acs(wSKIPk) is the number of common
skip-ngrams (common subsequences) produced by
the ACS algorithm.
ROUGE-W (Lin and Och, 2004) relies on a
weighted version of the longest common subse-
quence, under which longer contiguous subsequences
are assigned a higher weight than subsequences that
incorporate gaps. ROUGE-W uses the polynomial
function xa in the weighted LCS computation. This
setting can also be simulated by BLANC by adjusting
the parameters ? to reward tighter skip-ngrams and ?
to assign a very high score to similar size gaps. In-
tuitively, ? is used to reward skip-ngrams that have
smaller gaps, while ? is used to reward better aligned
skip-ngram overlap.
4 Scalability & Data Exploration
In Figure 1 we show theoretical and empirical prac-
tical behavior for the ACS algorithm on the 2003
TIDES machine translation evaluation data for Ara-
bic and Chinese. Sentence length distribution is
somewhat similar for the two languages ? only a very
small amount of text segments have more than 50
tokens. We show the ACS graph size in the worst
case scenario, and the empirical average number of
matches for both languages as a function of sentence
length. We also show (on a log scale) the upper bound
on time/space complexity in terms of total number
of feature computations. Even though the worst-
case scenario is tractable (polynomial), the empirical
amount of computation is considerably smaller in the
form of polynomials of lower degree. In Figure 1,
sentence length is the average between reference and
candidate lengths.
Finally, we also show the total number of fea-
ture computations involved in performing a full over-
lap search and computing a numeric score for the
745
reference-candidate translation pair. We have exper-
imented with the ACS algorithm using a worst-case
scenario where all words are exactly the same for a
fifty words reference translation and candidate trans-
lation. In practice when considering real sentences
the number of matches is very small. In this setting,
the algorithm takes less than two seconds on a low-
end desktop system when working on the worst case
scenario, and less then a second for all candidate-
reference pairs in the TIDES 2003 dataset. This re-
sult renders the ACS algorithm very practical for au-
tomatic MT evaluation.
5 Experiments & Results
In the dynamic metric BLANC, we have implemented
the ACS algorithm using several parameters includ-
ing the aggregate gap size ?, the displacement feature
?, a parameter for regulating skip-ngram size contri-
bution, and the F-measure ?F parameter.
Until recently, most experiments that evaluate au-
tomatic metrics correlation to human judgments have
been performed at a system level. In such experi-
ments, human judgments are aggregated across sen-
tences for each MT system and compared to aggre-
gate scores for automatic metrics. While high scor-
ing metrics in this setting are useful for understand-
ing relative system performance, not all of them are
robust enough for evaluating the quality of machine
translation output at a lower granularity. Sentence-
level translation quality estimation is very useful
when MT is used as a component in a pipeline of text-
processing applications (e.g. question answering).
The fact that current automatic MT evaluation met-
rics including BLANC do not correlate well with hu-
man judgments at the sentence level, does not mean
we should ignore this need and focus only on system
level evaluation. On the contrary, further research is
required to improve these metrics. Due to its train-
able nature, and by allowing additional features to be
incorporated into its model, BLANC has the potential
to address this issue.
For comparison purposes with previous literature,
we have also performed experiments at system level
for Arabic. The datasets used consist of the MT trans-
lation outputs from all systems available through the
Tides 2003 evaluation (663 sentences) for training
and Tides 2004 evaluation (1353 sentences) for test-
ing.
We compare (Table 1) the performance of BLANC
on Arabic translation output with the performance
of more established evaluation metrics: BLEU and
NIST, and also with more recent metrics: ROUGE-
L and ROUGE-S (using an unlimited size skip win-
dow), which have been shown to correlate well with
human judgments at system level ? as confirmed by
our results. We have performed experiments in which
case information is preserved as well as experiments
that ignore case information. Since the results are
very similar, we only show here experiments under
the former condition. In order to maintain consis-
tency, when using any metric we apply the same pre-
processing provided by the MTEval script. When
computing the correlation between metrics and hu-
man judgments, we only keep strictly positive scores.
While this is not fully equivalent to BLEU smooth-
ing, it partially mitigates the same problem of zero
count ngrams for short sentences. In future work we
plan to implement smoothing for all metrics, includ-
ing BLANC.
We train BLANC separately for adequacy and flu-
ency, as well as for system level and segment level
correlation with human judgments. The BLANC pa-
rameters are currently trained using a simple hill-
climbing procedure and using several starting points
in order to decrease the chance of reaching a local
maximum.
BLANC proves to be robust across criteria and
granularity levels. As expected, different parameter
values of BLANC optimize different criteria (e.g. ad-
equacy and fluency). We have observed that train-
ing BLANC for adequacy results in more bias to-
wards recall (?F =3) compared to training it for flu-
ency (?F =2). This confirms our intuition that a dy-
namic, parametric metric is justified for automatic
evaluation.
6 Conclusions & Future Work
In previous sections we have defined simple distance
functions. More complex functions can also be incor-
porated in ACS. Skip-ngrams in the candidate sen-
tence might be rewarded if they contain fewer gaps in
the candidate sentence and penalized if they contain
more. Different distance functions could also be used
in ACS, including functions based on surface-form
features and part-of-speech features.
Most of the established MT evaluation methods are
746
Tides 2003 Arabic
System Level Segment Level
Method Adequacy Fluency Adequacy Fluency
BLEU 0.950 0.934 0.382 0.286
NIST 0.962 0.939 0.439 0.304
ROUGE-L 0.974 0.926 0.440 0.328
ROUGE-S 0.949 0.935 0.360 0.328
BLANC 0.988 0.979 0.492 0.391
Tides 2004 Arabic
System Level Segment Level
Method Adequacy Fluency Adequacy Fluency
BLEU 0.978 0.994 0.446 0.337
NIST 0.987 0.952 0.529 0.358
ROUGE-L 0.981 0.985 0.538 0.412
ROUGE-S 0.937 0.980 0.367 0.408
BLANC 0.982 0.994 0.565 0.438
Table 1: Pearson correlation of several metrics with human judgments at system level and segment level for fluency and adequacy.
static functions according to which automatic evalu-
ation scores are computed. In this paper, we have
laid the foundation for a more flexible, parametric ap-
proach that can be trained using existing MT data and
that can be optimized for highest agreement with hu-
man assessors, for different criteria.
We have introduced ACS, a practical algorithm
with learnable parameters for automatic MT evalu-
ation and showed that ngram computation of popu-
lar evaluation methods can be emulated through dif-
ferent parameters by ACS. We have computed time
and space bounds for the ACS algorithm and argued
that while it is more powerful in modeling local and
sentence structure, it offers the same practicality as
established algorithms.
In our experiments, we trained and tested BLANC
on data from consecutive years, and therefore tai-
lored the metric for two different operating points
in MT system performance. In this paper we show
that BLANC correlates well with human performance
when trained on previous year data for both sentence
and system level.
In the future, we plan to investigate the stability
and performance of BLANC and also apply it to auto-
matic summarization evaluation. We plan to optimize
the BLANC parameters for different criteria in addi-
tion to incorporating syntactic and semantic features
(e.g. ngrams, word classes, part-of-speech).
In previous sections we have defined simple dis-
tance functions. More complex functions can also
be incorporated in ACS. Skip-ngrams in the candi-
date sentence might be rewarded if they contain fewer
gaps in the candidate sentence and penalized if they
contain more. Different distance functions could also
be used in ACS, including functions based on surface-
form features and part-of-speech features.
Looking beyond the BLANC metric, this paper
makes the case for the need to shift to trained, dy-
namic evaluation metrics which can adapt to individ-
ual optimization criteria and correlation functions.
We plan to make available an implementation of
BLANC at http://www.cs.cmu.edu/ llita/blanc.
References
Y. Akiba, K. Iamamurfa, and E. Sumita. 2001. Using
multiple edit distances to automatically rank machine
translation output. MT Summit VIII.
C. Culy and S.Z. Riehemann. 2003. The limits of n-
gram translation evaluation metrics. Machine Transla-
tion Summit IX.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. Human Language Technology Conference
(HLT).
V.I. Levenshtein. 1965. Binary codes capable of cor-
recting deletions, insertions, and reversals. Doklady
Akademii Nauk SSSR.
C.Y. Lin and F.J. Och. 2004. Automatic evaluation of
machine translation quality using longest common sub-
sequence and skip bigram statistics. ACL.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evaluation
for mt research. LREC.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. IBM Research Report.
R. Soricut and E. Brill. 2004. A unified framework for
automatic evaluation using n-gram co-occurence statis-
tics. ACL.
K.Y. Su, M.W. Wu, and J.S. Chang. 1992. A new quanti-
tative quality measure for machine translation systems.
COLING.
J.P. Turian, L. Shen, and I.D. Melamed. 2003. Evaluation
of machine translation and its evaluation. MT Summit
IX.
C.J. Van-Rijsbergen. 1979. Information retrieval.
747
SPEECHALATOR: TWO-WAY SPEECH-TO-SPEECH TRANSLATION IN YOUR HAND
Alex Waibel
 
, Ahmed Badran
 
, Alan W Black
 
, Robert Frederking
 
, Donna Gates
 
Alon Lavie
 
, Lori Levin
 
, Kevin Lenzo

, Laura Mayfield Tomokiyo
Juergen Reichert

, Tanja Schultz   , Dorcas Wallace   , Monika Woszczyna , Jing Zhang
 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA

Cepstral, LLC,

Multimodal Technologies Inc,

Mobile Technologies Inc.
speechalator@speechinfo.org
ABSTRACT
This demonstration involves two-way automatic speech-
to-speech translation on a consumer off-the-shelf PDA. This
work was done as part of the DARPA-funded Babylon project,
investigating better speech-to-speech translation systems for
communication in the field. The development of the Speecha-
lator software-based translation system required addressing
a number of hard issues, including a new language for the
team (Egyptian Arabic), close integration on a small device,
computational efficiency on a limited platform, and scalable
coverage for the domain.
1. BACKGROUND
The Speechalator was developed in part as the next genera-
tion of automatic voice translation systems. The Phrasalator
is a one-way device that can recognize a set of pre-defined
phrases and play a recorded translation, [1]. This device
can be ported easily to new languages, requiring only a
hand translation of the phrases and a set of recorded sen-
tences. However, such a system severely limits communica-
tion as the translation is one way, thus reducing one party?s
responses to simple pointing and perhaps yes and no.
The Babylon project addresses the issues of two-way
communication where either party can use the device for
conversation. A number of different groups throughout the
US were asked to address specific aspects of the task, such
as different languages, translation techniques and platform
specifications. The Pittsburgh group was presented with
three challenges. First, we were to work with Arabic, a lan-
guage with which the group had little experience, to test our
capabilities in moving to new languages quickly. Second,
we were instructed to use an interlingua approach to trans-
lation, where the source language is translated into an in-
termediate form that is shared between all languages. This
step streamlines expansion to new languages, and CMU has
a long history in working with interlingua based translation
systems. Third, we were constrained to one portable PDA-
class device to host the entire two-way system: two recog-
nizers, two translation engines, and two synthesizers.
2. RECOGNITION
We used an HMM-based recognizer, developed by Multi-
modal Technologies Inc, which has been specifically tuned
for PDAs. The recognizer allows a grammar to be tightly
coupled with the recognizer, which offers important effi-
ciencies considering the limited computational power of the
device. With only minor modification we were able to gen-
erate our interlingua interchange format (IF) representation
directly as output from the recognizer, removing one mod-
ule from the process.
MTI?s recognizer requires under 1M of memory with
acoustic models of around 3M per language. Special op-
timizations deal with the slow processor and ensure low
use of memory during decoding. The Arabic models were
bootstrapped from the GlobalPhone [2] Arabic collections
as well as data collected as part of this project.
3. TRANSLATION
As part of this work we investigated two different tech-
niques for translation, both interlingua based. The first was
purely knowledge-based, following our previous work [3].
The engine developed for this was too large to run on the
device, although we were able to run the generation part off-
line seamlessly connected by a wireless link from the hand-
held device. The second technique we investigated used
a statistical training method to build a model to translate
structured interlingua IF to text in the target language. Be-
cause this approach was developed with the handheld in
mind, it is efficient enough to run directly on the device,
and is used in this demo.
4. SYNTHESIS
The synthesis engine is Cepstral?s Theta system. As the
Speechalator runs on very small hardware devices (at least
small compared to standard desktops), it was important that
the synthesis footprint remained as small as possible.
The speechalator is to be used for people with little ex-
posure to synthetic speech, and the output quality must be
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 29-30
                                                         Proceedings of HLT-NAACL 2003
very high. Cepstral?s unit selection voices, tailored to the
domain, meet the requirements for both quality and size.
Normal unit selection voices may take hundreds of megabytes,
but the 11KHz voices developed by Cepstral were around 9
megabytes each.
5. ARABIC
The Arabic language poses a number of challenges for any
speech translation system. The first problem is the wide
range of dialects of the language. Just as Jamaican and
Glaswegian speakers may find it difficult to understand each
other?s dialect of English, Arabic speakers of different di-
alects may find it impossible to communicate.
Modern Standard Arabic (MSA) is well-defined and widely
understood by educated speakers across the Arab world.
MSA is principally a written language and not a spoken lan-
guage, however. Our interest was in dealing with a normal
spoken dialect, and we chose Egyptian Arabic; speakers of
that dialect were readily accessible to us, and media influ-
ences have made it perhaps the most broadly understood of
the regional dialects.
Another feature of Arabic is that the written form, ex-
cept in specific rare cases, does not include vowels. For
speech recognition and synthesis, this makes pronunciations
hard. Solutions have been tested for recognition where the
vowels are not explicitly modeled, but implicitly modeled
by context. This would not work well for synthesis; we have
defined an internal romanization, based on the CallHome
[4] romanization, from which full phonetic forms can easily
be derived. This romanization is suitable for both recog-
nizer and synthesis systems, and can easily be transformed
into the Arabic script for display.
6. SYSTEM
The end-to-end system runs on a standard Pocket PC de-
vice. We have tested it on a number of different machines,
including various HP (Compaq) iPaq machines (38xx 39xx)
and Dell Axims. It can run on 32M machines, but runs best
on a 64M machine with about 40M made available for pro-
gram space. Time from the end of spoken input to start of
translated speech is around 2-4 seconds depending on the
length of the sentence and the actual processor. We have
found StrongARM 206MHz processors, found on the older
Pocket PCs, slightly faster than XScale 400MHz, though no
optimization for the newer processors has been attempted.
Upon startup, the user is presented with the screen as
shown in Figure 1. A push-to-talk button is used and the
speaker speaks in his language. The recognized utterance
is first displayed, with the translation following, and the ut-
terance is then spoken in the target language. Buttons are
provided for replaying the output and for switching the in-
put to the other language.
7. DISCUSSION
The current demonstration is designed for the medical inter-
view domain, with the doctor speaking English and the pa-
tient speaking Arabic. At this point in the project no formal
evaluation has taken place. However, informally, in office-
like acoustic environments, accuracy within domain is well
over 80%.
Arabic input Screen
Speechalator snapshot
8. REFERENCES
[1] Sarich, A., ?Phraselator, one-way speech translation
system,? http://www.sarich.com/translator/, 2001.
[2] T. Schultz and A. Waibel, ?The globalphone project:
Multilingual lvcsr with janus-3,? in Multilingual Infor-
mation Retrieval Dialogs: 2nd SQEL Workshop, Plzen,
Czech Republic, 1997, pp. 20?27.
[3] A. Lavie, et al ?A multi-perspective evaluation of
the NESPOLE! speech-to-speech translation system,?
in Proceedings of ACL 2002 workshop on Speech-to-
speech Translation: Algorithms and Systems, Philadel-
phia, PA., 2002.
[4] Linguistic Data Consortium, ?Callhome egyptian ara-
bic speech,? 1997.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 129?132,
New York, June 2006. c?2006 Association for Computational Linguistics
Parser Combination by Reparsing 
 
Kenji Sagae and Alon Lavie 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
{sagae,alavie@cs.cmu.edu} 
  
 
Abstract 
We present a novel parser combination 
scheme that works by reparsing input sen-
tences once they have already been parsed 
by several different parsers.  We apply this 
idea to dependency and constituent parsing, 
generating results that surpass state-of-the-
art accuracy levels for individual parsers. 
1 Introduction 
Over the past decade, remarkable progress has 
been made in data-driven parsing.  Much of this 
work has been fueled by the availability of large 
corpora annotated with syntactic structures, espe-
cially the Penn Treebank (Marcus et al, 1993).  In 
fact, years of extensive research on training and 
testing parsers on the Wall Street Journal (WSJ) 
corpus of the Penn Treebank have resulted in the 
availability of several high-accuracy parsers. 
We present a framework for combining the out-
put of several different accurate parsers to produce 
results that are superior to those of each of the in-
dividual parsers.  This is done in a two stage proc-
ess of reparsing.  In the first stage, m different 
parsers analyze an input sentence, each producing 
a syntactic structure.  In the second stage, a parsing 
algorithm is applied to the original sentence, taking 
into account the analyses produced by each parser 
in the first stage.  Our approach produces results 
with accuracy above those of the best individual 
parsers on both dependency and constituent pars-
ing of the standard WSJ test set. 
2 Dependency Reparsing 
In dependency reparsing we focus on unlabeled 
dependencies, as described by Eisner (1996).  In 
this scheme, the syntactic structure for a sentence 
with n words is a dependency tree representing 
head-dependent relations between pairs of words. 
When m parsers each output a set of 
dependencies (forming m dependency structures) 
for a given sentence containing n words,  the 
dependencies can be combined in a simple word-
by-word voting scheme, where each parser votes 
for the head of each of the n words in the sentence, 
and the head with most votes is assigned to each 
word.  This very simple scheme guarantees that the 
final set of dependencies will have as many votes 
as possible, but it does not guarantee that the final 
voted set of dependencies will be a well-formed 
dependency tree.  In fact, the resulting graph may 
not even be connected.  Zeman & ?abokrtsk? 
(2005) apply this dependency voting scheme to 
Czech with very strong results.  However, when 
the constraint that structures must be well-formed 
is enforced, the accuracy of their results drops 
sharply. 
Instead, if we reparse the sentence based on the 
output of the m parsers, we can maximize the 
number of votes for a well-formed dependency 
structure.  Once we have obtained the m initial 
dependency structures to be combined, the first 
step is to build a graph where each word in the 
sentence is a node.  We then create weighted 
directed edges between the nodes corresponding to 
words for which dependencies are obtained from 
each of the initial structures.1  In cases where more 
than one dependency structure indicates that an 
edge should be created, the corresponding weights 
are simply added.  As long as at least one of the m 
initial structures is a well-formed dependency 
structure, the directed graph created this way will 
be connected. 
                                                 
1
 Determining the weights is discussed in section 4.1. 
129
Once this graph is created, we reparse the 
sentence using a dependency parsing algorithm 
such as, for example, one of the algorithms 
described by McDonald et al (2005).  Finding the 
optimal dependency structure given the set of 
weighted dependencies is simply a matter of 
finding the maximum spanning tree (MST) for the 
directed weighted graph, which can be done using 
the Chu-Liu/Edmonds directed MST algorithm 
(Chu & Liu, 1965; Edmonds, 1967).  The 
maximum spanning tree maximizes the votes for 
dependencies given the constraint that the resulting 
structure must be a tree.  If projectivity (no 
crossing branches) is desired, Eisner?s (1996) 
dynamic programming algorithm  (similar to CYK) 
for dependency parsing can be used instead.   
3 Constituent Reparsing 
In constituent reparsing we deal with labeled con-
stituent trees, or phrase structure trees, such as 
those in the Penn Treebank (after removing traces, 
empty nodes and function tags).  The general idea 
is the same as with dependencies.  First, m parsers 
each produce one parse tree for an input sentence.  
We then use these m initial parse trees to guide the 
application of a parse algorithm to the input. 
Instead of building a graph out of words (nodes) 
and dependencies (edges), in constituent reparsing 
we use the m initial trees to build a weighted parse 
chart.  We start by decomposing each tree into its 
constituents, with each constituent being a 4-tuple 
[label, begin, end, weight], where label is the 
phrase structure type, such as NP or VP, begin is 
the index of the word where the constituent starts, 
end is the index of the word where the constituent 
ends plus one, and weight is the weight of the con-
stituent.  As with dependencies, in the simplest 
case the weight of each constituent is simply 1.0, 
but different weighting schemes can be used.  
Once the initial trees have been broken down into 
constituents, we put all the constituents from all of 
the m trees into a single list.  We then look for each 
pair of constituents A and B where the label, begin, 
and end are identical, and merge A and B into a 
single constituent with the same label, begin, and 
end, and with weight equal to the weight of A plus 
the weight of B.  Once no more constituent mergers 
are possible, the resulting constituents are placed 
on a standard parse chart, but where the constitu-
ents in the chart do not contain back-pointers indi-
cating what smaller constituents they contain.  
Building the final tree amounts to determining 
these back-pointers.  This can be done by running a 
bottom-up chart parsing algorithm (Allen, 1995) 
for a weighted grammar, but instead of using a 
grammar to determine what constituents can be 
built and what their weights are, we simply con-
strain the building of constituents to what is al-
ready in the chart (adding the weights of constitu-
ents when they are combined).  This way, we per-
form an exhaustive search for the tree that repre-
sents the heaviest combination of constituents that 
spans the entire sentence as a well-formed tree. 
A problem with simply considering all constitu-
ents and picking the heaviest tree is that this favors 
recall over precision.  Balancing precision and re-
call is accomplished by discarding every constitu-
ent with weight below a threshold t before the 
search for the final parse tree starts.  In the simple 
case where each constituent starts out with weight 
1.0 (before any merging), this means that a con-
stituent is only considered for inclusion in the final 
parse tree if it appears in at least t of the m initial 
parse trees.  Intuitively, this should increase preci-
sion, since we expect that a constituent that ap-
pears in the output of more parsers to be more 
likely to be correct.  By changing the threshold t 
we can control the precision/recall tradeoff.  
Henderson and Brill (1999) proposed two parser 
combination schemes, one that picks an entire tree 
from one of the parsers, and one that, like ours, 
builds a new tree from constituents from the initial 
trees.  The latter scheme performed better, produc-
ing remarkable results despite its simplicity.  The 
combination is done with a simple majority vote of 
whether or not constituents should appear in the 
combined tree.  In other words, if a constituent ap-
pears at least (m + 1)/2 times in the output of the m 
parsers, the constituent is added to the final tree.  
This simple vote resulted in trees with f-score sig-
nificantly higher than the one of the best parser in 
the combination.  However, the scheme heavily 
favors precision over recall.  Their results on WSJ 
section 23 were 92.1 precision and 89.2 recall 
(90.61 f-score), well above the most accurate 
parser in their experiments (88.6 f-score). 
4 Experiments 
In our dependency parsing experiments we used 
unlabeled dependencies extracted from the Penn 
130
Treebank using the same head-table as Yamada 
and Matsumoto (2003), using sections 02-21 as 
training data and section 23 as test data, following 
(McDonald et al, 2005; Nivre & Scholz, 2004; 
Yamada & Matsumoto, 2003).  Dependencies ex-
tracted from section 00 were used as held-out data, 
and section 22 was used as additional development 
data.  For constituent parsing, we used the section 
splits of the Penn Treebank as described above, as 
has become standard in statistical parsing research. 
4.1 Dependency Reparsing Experiments 
Six dependency parsers were used in our combina-
tion experiments, as described below. 
The deterministic shift-reduce parsing algorithm 
of (Nivre & Scholz, 2004) was used to create two 
parsers2, one that processes the input sentence from 
left-to-right (LR), and one that goes from right-to-
left (RL).  Because this deterministic algorithm 
makes a single pass over the input string with no 
back-tracking, making decisions based on the pars-
er?s state and history, the order in which input to-
kens are considered affects the result.  Therefore, 
we achieve additional parser diversity with the 
same algorithm, simply by varying the direction of 
parsing.  We refer to the two parsers as LR and RL. 
The deterministic parser of Yamada and Ma-
tsumoto (2003) uses an algorithm similar to Nivre 
and Scholz?s, but it makes several successive left-
to-right passes over the input instead of keeping a 
stack.  To increase parser diversity, we used a ver-
sion of Yamada and Matsumoto?s algorithm where 
the direction of each of the consecutive passes over 
the input string alternates from left-to-right and 
right-to-left.  We refer to this parser as LRRL. 
The large-margin parser described in 
(McDonald et al, 2005) was used with no altera-
tions.  Unlike the deterministic parsers above, this 
parser uses a dynamic programming algorithm 
(Eisner, 1996) to determine the best tree, so there 
is no difference between presenting the input from 
left-to-right or right-to-left. 
Three different weight configurations were con-
sidered: (1) giving all dependencies the same 
weight; (2) giving dependencies different weights, 
depending only on which parser generated the de-
pendency; and (3) giving dependencies different 
                                                 
2
 Nivre and Scholz use memory based learning in their 
experiments.  Our implementation of their parser uses 
support vector machines, with improved results. 
weights, depending on which parser generated the 
dependency, and the part-of-speech of the depend-
ent word.  Option 2 takes into consideration that 
parsers may have different levels of accuracy, and 
dependencies proposed by more accurate parsers 
should be counted more heavily.  Option 3 goes a 
step further, attempting to capitalize on the specific 
strengths of the different parsers. 
The weights in option 2 are determined by com-
puting the accuracy of each parser on the held-out 
set (WSJ section 00).  The weights are simply the 
corresponding parser?s accuracy (number of cor-
rect dependencies divided by the total number of 
dependencies).  The weights in option 3 are deter-
mined in a similar manner, but different accuracy 
figures are computed for each part-of-speech. 
Table 1 shows the dependency accuracy and 
root accuracy (number of times the root of the de-
pendency tree was identified correctly divided by 
the number of sentences) for each of the parsers, 
and for each of the different weight settings in the 
reparsing experiments (numbered according to 
their descriptions above). 
 
System Accuracy Root Acc. 
LR 91.0 92.6 
RL 90.1 86.3 
LRRL 89.6 89.1 
McDonald 90.9 94.2 
Reparse dep 1 91.8 96.0 
Reparse dep 2 92.1 95.9 
Reparse dep 3 92.7 96.6 
Table 1: Dependency accuracy and root accuracy of 
individual dependency parsers and their combination 
under three different weighted reparsing settings. 
4.2 Constituent Reparsing Experiments 
The parsers that were used in the constituent 
reparsing experiments are: (1) Charniak and John-
son?s (2005) reranking parser; (2) Henderson?s 
(2004) synchronous neural network parser; (3) 
Bikel?s (2002) implementation of the Collins 
(1999) model 2 parser; and (4) two versions of Sa-
gae and Lavie?s (2005) shift-reduce parser, one 
using a maximum entropy classifier, and one using 
support vector machines. 
Henderson and Brill?s voting scheme mentioned 
in section 3 can be emulated by our reparsing ap-
proach by setting all weights to 1.0 and t to (m + 
1)/2, but better results can be obtained by setting 
appropriate weights and adjusting the preci-
sion/recall tradeoff.  Weights for different types of 
131
constituents from each parser can be set in a simi-
lar way to configuration 3 in the dependency ex-
periments.  However, instead of measuring accu-
racy for each part-of-speech tag of dependents, we 
measure precision for each non-terminal label.   
The parameter t is set using held-out data (from 
WSJ section 22) and a simple hill-climbing proce-
dure.  First we set t to (m + 1)/2 (which heavily 
favors precision).  We then repeatedly evaluate the 
combination of parsers, each time decreasing the 
value of t (by 0.01, say).  We record the values of t 
for which precision and recall were closest, and for 
which f-score was highest. 
Table 2 shows the accuracy of each individual 
parser and for three reparsing settings.  Setting 1 is 
the emulation of Henderson and Brill?s voting.  In 
setting 2, t is set for balancing precision and recall.  
In setting 3, t is set for highest f-score.  
 
System Precision Recall F-score 
Charniak/Johnson 91.3 90.6 91.0 
Henderson 90.2 89.1 89.6 
Bikel (Collins) 88.3 88.1 88.2 
Sagae/Lavie (a) 86.9 86.6 86.7 
Sagae/Lavie (b) 88.0 87.8 87.9 
Reparse 1 95.1 88.5 91.6 
Reparse 2 91.8 91.9 91.8 
Reparse 3 93.2 91.0 92.1 
Table 2: Precision, recall and f-score of each constituent 
parser and their combination under three different 
reparsing settings. 
5 Discussion 
We have presented a reparsing scheme that pro-
duces results with accuracy higher than the best 
individual parsers available by combining their 
results.  We have shown that in the case of de-
pendencies, the reparsing approach successfully 
addresses the issue of constructing high-accuracy 
well-formed structures from the output of several 
parsers.  In constituent reparsing, held-out data can 
be used for setting a parameter that allows for bal-
ancing precision and recall, or increasing f-score.  
By combining several parsers with f-scores ranging 
from 91.0% to 86.7%, we obtain reparsed results 
with a 92.1% f-score. 
References 
Allen, J. (1995). Natural Language Understanding (2nd 
ed.). Redwood City, CA: The Benjamin/Cummings 
Publishing Company, Inc. 
Bikel, D. (2002). Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings 
of HLT2002. San Diego, CA. 
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In 
Proceedings of the 43rd meeting of the Association 
for Computational Linguistics. Ann Arbor, MI. 
Chu, Y. J., & Liu, T. H. (1965). On the shortest arbores-
cence of a directed graph. Science Sinica(14), 1396-
1400. 
Edmonds, J. (1967). Optimum branchings. Journal of 
Research of the National Bureau of Standards(71B), 
233-240. 
Eisner, J. (1996). Three new probabilistic models for 
dependency parsing: An exploration. In Proceedings 
of the International Conference on Computational 
Linguistics (COLING'96). Copenhagen, Denmark. 
Henderson, J. (2004). Discriminative training of a neu-
ral network statistical parser. In Proceedings of the 
42nd Meeting of the Association for Computational 
Linguistics. Barcelona, Spain. 
Henderson, J., & Brill, E. (1999). Exploiting diversity in 
natural language processing: combining parsers. In 
Proceedings of the Fourth Conference on Empirical 
Methods in Natural Language Processing (EMNLP). 
Marcus, M. P., Santorini, B., & Marcinkiewics, M. A. 
(1993). Building a large annotated corpus of English: 
The Penn Treebank. Computational Linguistics, 19. 
McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. 
(2005). Non-Projective Dependency Parsing using 
Spanning Tree Algorithms. In Proceedings of the 
Conference on Human Language Technolo-
gies/Empirical Methods in Natural Language Proc-
essing (HLT-EMNLP). Vancouver, Canada. 
Nivre, J., & Scholz, M. (2004). Deterministic depend-
ency parsing of English text. In Proceedings of the 
20th International Conference on Computational Lin-
guistics (pp. 64-70). Geneva, Switzerland. 
Sagae, K., & Lavie, A. (2005). A classifier-based parser 
with linear run-time complexity. In Proceedings of 
the Ninth International Workshop on Parsing Tech-
nologies. Vancouver, Canada. 
Yamada, H., & Matsumoto, Y. (2003). Statistical de-
pendency analysis using support vector machines. In 
Proceedings of the Eighth International Workshop on 
Parsing Technologies. Nancy, France. 
Zeman, D., & ?abokrtsk?, Z. (2005). Improving Parsing 
Accuracy by Combining Diverse Dependency Pars-
ers. In Proceedings of the International Workshop on 
Parsing Technologies. Vancouver, Canada. 
132
Proceedings of the 43rd Annual Meeting of the ACL, pages 197?204,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic Measurement of Syntactic Development in Child Language
Kenji Sagae and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15232
{sagae,alavie}@cs.cmu.edu
Brian MacWhinney
Department of Psychology
Carnegie Mellon University
Pittsburgh, PA 15232
macw@cmu.edu
Abstract
To facilitate the use of syntactic infor-
mation in the study of child language
acquisition, a coding scheme for Gram-
matical Relations (GRs) in transcripts of
parent-child dialogs has been proposed by
Sagae, MacWhinney and Lavie (2004).
We discuss the use of current NLP tech-
niques to produce the GRs in this an-
notation scheme. By using a statisti-
cal parser (Charniak, 2000) and memory-
based learning tools for classification
(Daelemans et al, 2004), we obtain high
precision and recall of several GRs. We
demonstrate the usefulness of this ap-
proach by performing automatic measure-
ments of syntactic development with the
Index of Productive Syntax (Scarborough,
1990) at similar levels to what child lan-
guage researchers compute manually.
1 Introduction
Automatic syntactic analysis of natural language has
benefited greatly from statistical and corpus-based
approaches in the past decade. The availability of
syntactically annotated data has fueled the develop-
ment of high quality statistical parsers, which have
had a large impact in several areas of human lan-
guage technologies. Similarly, in the study of child
language, the availability of large amounts of elec-
tronically accessible empirical data in the form of
child language transcripts has been shifting much of
the research effort towards a corpus-based mental-
ity. However, child language researchers have only
recently begun to utilize modern NLP techniques
for syntactic analysis. Although it is now common
for researchers to rely on automatic morphosyntactic
analyses of transcripts to obtain part-of-speech and
morphological analyses, their use of syntactic pars-
ing is rare.
Sagae, MacWhinney and Lavie (2004) have
proposed a syntactic annotation scheme for the
CHILDES database (MacWhinney, 2000), which
contains hundreds of megabytes of transcript data
and has been used in over 1,500 studies in child lan-
guage acquisition and developmental language dis-
orders. This annotation scheme focuses on syntactic
structures of particular importance in the study of
child language. In this paper, we describe the use
of existing NLP tools to parse child language tran-
scripts and produce automatically annotated data in
the format of the scheme of Sagae et al We also
validate the usefulness of the annotation scheme and
our analysis system by applying them towards the
practical task of measuring syntactic development in
children according to the Index of Productive Syn-
tax, or IPSyn (Scarborough, 1990), which requires
syntactic analysis of text and has traditionally been
computed manually. Results obtained with current
NLP technology are close to what is expected of hu-
man performance in IPSyn computations, but there
is still room for improvement.
2 The Index of Productive Syntax (IPSyn)
The Index of Productive Syntax (Scarborough,
1990) is a measure of development of child lan-
guage that provides a numerical score for grammat-
ical complexity. IPSyn was designed for investigat-
ing individual differences in child language acqui-
197
sition, and has been used in numerous studies. It
addresses weaknesses in the widely popular Mean
Length of Utterance measure, or MLU, with respect
to the assessment of development of syntax in chil-
dren. Because it addresses syntactic structures di-
rectly, it has gained popularity in the study of gram-
matical aspects of child language learning in both
research and clinical settings.
After about age 3 (Klee and Fitzgerald, 1985),
MLU starts to reach ceiling and fails to properly dis-
tinguish between children at different levels of syn-
tactic ability. For these purposes, and because of its
higher content validity, IPSyn scores often tells us
more than MLU scores. However, the MLU holds
the advantage of being far easier to compute. Rel-
atively accurate automated methods for computing
the MLU for child language transcripts have been
available for several years (MacWhinney, 2000).
Calculation of IPSyn scores requires a corpus of
100 transcribed child utterances, and the identifica-
tion of 56 specific language structures in each ut-
terance. These structures are counted and used to
compute numeric scores for the corpus in four cat-
egories (noun phrases, verb phrases, questions and
negations, and sentence structures), according to a
fixed score sheet. Each structure in the four cate-
gories receives a score of zero (if the structure was
not found in the corpus), one (if it was found once
in the corpus), or two (if it was found two or more
times). The scores in each category are added, and
the four category scores are added into a final IPSyn
score, ranging from zero to 112.1
Some of the language structures required in the
computation of IPSyn scores (such as the presence
of auxiliaries or modals) can be recognized with the
use of existing child language analysis tools, such
as the morphological analyzer MOR (MacWhinney,
2000) and the part-of-speech tagger POST (Parisse
and Le Normand, 2000). However, more complex
structures in IPSyn require syntactic analysis that
goes beyond what POS taggers can provide. Exam-
ples of such structures include the presence of an
inverted copula or auxiliary in a wh-question, con-
joined clauses, bitransitive predicates, and fronted
or center-embedded subordinate clauses.
1See (Scarborough, 1990) for a complete listing of targeted
structures and the IPSyn score sheet used for calculation of
scores.
Sentence (input):
We eat the cheese sandwich
Grammatical Relations (output):
[Leftwall]     We     eat     the     cheese     sandwich
SUBJ
ROOT OBJ
DET
MOD
Figure 1: Input sentence and output produced by our
system.
3 Automatic Syntactic Analysis of Child
Language Transcripts
A necessary step in the automatic computation of
IPSyn scores is to produce an automatic syntac-
tic analysis of the transcripts being scored. We
have developed a system that parses transcribed
child utterances and identifies grammatical relations
(GRs) according to the CHILDES syntactic annota-
tion scheme (Sagae et al, 2004). This annotation
scheme was designed specifically for child-parent
dialogs, and we have found it suitable for the iden-
tification of the syntactic structures necessary in the
computation of IPSyn.
Our syntactic analysis system takes a sentence
and produces a labeled dependency structure repre-
senting its grammatical relations. An example of the
input and output associated with our system can be
seen in figure 1. The specific GRs identified by the
system are listed in figure 2.
The three main steps in our GR analysis are: text
preprocessing, unlabeled dependency identification,
and dependency labeling. In the following subsec-
tions, we examine each of them in more detail.
3.1 Text Preprocessing
The CHAT transcription system2 is the format
followed by all transcript data in the CHILDES
database, and it is the input format we use for syn-
tactic analysis. CHAT specifies ways of transcrib-
ing extra-grammatical material such as disfluency,
retracing, and repetition, common in spontaneous
spoken language. Transcripts of child language may
contain a large amount of extra-grammatical mate-
2http://childes.psy.cmu.edu/manuals/CHAT.pdf
198
SUBJ, ESUBJ, CSUBJ, XSUBJ
COMP, XCOMP
JCT, CJCT, XJCT
OBJ, OBJ2, IOBJ
PRED, CPRED, XPRED
MOD, CMOD, XMOD
AUX NEG DET QUANT POBJ PTL
CPZR COM INF VOC COORD ROOT
Subject, expletive subject, clausal subject (finite and non?finite) Object, second object, indirect object
Clausal complement (finite and non?finite) Predicative, clausal predicative (finite and non?finite)
Adjunct, clausal adjunct (finite and non?finite) Nominal modifier, clausal nominal modifier (finite and non?finite)
Auxiliary Negation Determiner Quantifier Prepositional object Verb particle
CommunicatorComplementizer Infinitival "to" Vocative Coordinated item Top node
Figure 2: Grammatical relations in the CHILDES syntactic annotation scheme.
rial that falls outside of the scope of the syntactic an-
notation system and our GR identifier, since it is al-
ready clearly marked in CHAT transcripts. By using
the CLAN tools (MacWhinney, 2000), designed to
process transcripts in CHAT format, we remove dis-
fluencies, retracings and repetitions from each sen-
tence. Furthermore, we run each sentence through
the MOR morphological analyzer (MacWhinney,
2000) and the POST part-of-speech tagger (Parisse
and Le Normand, 2000). This results in fairly clean
sentences, accompanied by full morphological and
part-of-speech analyses.
3.2 Unlabeled Dependency Identification
Once we have isolated the text that should be ana-
lyzed in each sentence, we parse it to obtain unla-
beled dependencies. Although we ultimately need
labeled dependencies, our choice to produce unla-
beled structures first (and label them in a later step)
is motivated by available resources. Unlabeled de-
pendencies can be readily obtained by processing
constituent trees, such as those in the Penn Tree-
bank (Marcus et al, 1993), with a set of rules to
determine the lexical heads of constituents. This
lexicalization procedure is commonly used in sta-
tistical parsing (Collins, 1996) and produces a de-
pendency tree. This dependency extraction proce-
dure from constituent trees gives us a straightfor-
ward way to obtain unlabeled dependencies: use an
existing statistical parser (Charniak, 2000) trained
on the Penn Treebank to produce constituent trees,
and extract unlabeled dependencies using the afore-
mentioned head-finding rules.
Our target data (transcribed child language) is
from a very different domain than the one of the data
used to train the statistical parser (the Wall Street
Journal section of the Penn Treebank), but the degra-
dation in the parser?s accuracy is acceptable. An
evaluation using 2,018 words of in-domain manu-
ally annotated dependencies shows that the depen-
dency accuracy of the parser is 90.1% on child lan-
guage transcripts (compared to over 92% on section
23 of the Wall Street Journal portion of the Penn
Treebank). Despite the many differences with re-
spect to the domain of the training data, our domain
features sentences that are much shorter (and there-
fore easier to parse) than those found in Wall Street
Journal articles. The average sentence length varies
from transcript to transcript, because of factors such
as the age and verbal ability of the child, but it is
usually less than 15 words.
3.3 Dependency Labeling
After obtaining unlabeled dependencies as described
above, we proceed to label those dependencies with
the GR labels listed in Figure 2.
Determining the labels of dependencies is in gen-
eral an easier task than finding unlabeled dependen-
cies in text.3 Using a classifier, we can choose one
of the 30 possible GR labels for each dependency,
given a set of features derived from the dependen-
cies. Although we need manually labeled data to
train the classifier for labeling dependencies, the size
of this training set is far smaller than what would be
necessary to train a parser to find labeled dependen-
3Klein and Manning (2002) offer an informal argument that
constituent labels are much more easily separable in multidi-
mensional space than constituents/distituents. The same argu-
ment applies to dependencies and their labels.
199
cies in one pass.
We use a corpus of about 5,000 words with man-
ually labeled dependencies to train TiMBL (Daele-
mans et al, 2003), a memory-based learner (set to
use the k-nn algorithm with k=1, and gain ratio
weighing), to classify each dependency with a GR
label. We extract the following features for each de-
pendency:
? The head and dependent words;
? The head and dependent parts-of-speech;
? Whether the dependent comes before or after
the head in the sentence;
? How many words apart the dependent is from
the head;
? The label of the lowest node in the constituent
tree that includes both the head and dependent.
The accuracy of the classifier in labeling depen-
dencies is 91.4% on the same 2,018 words used to
evaluate unlabeled accuracy. There is no intersec-
tion between the 5,000 words used for training and
the 2,018-word test set. Features were tuned on a
separate development set of 582 words.
When we combine the unlabeled dependencies
obtained with the Charniak parser (and head-finding
rules) and the labels obtained with the classifier,
overall labeled dependency accuracy is 86.9%, sig-
nificantly above the results reported (80%) by Sagae
et al (2004) on very similar data.
Certain frequent and easily identifiable GRs, such
as DET, POBJ, INF, and NEG were identified with
precision and recall above 98%. Among the most
difficult GRs to identify were clausal complements
COMP and XCOMP, which together amount to less
than 4% of the GRs seen the training and test sets.
Table 1 shows the precision and recall of GRs of par-
ticular interest.
Although not directly comparable, our results
are in agreement with state-of-the-art results for
other labeled dependency and GR parsers. Nivre
(2004) reports a labeled (GR) dependency accuracy
of 84.4% on modified Penn Treebank data. Briscoe
and Carroll (2002) achieve a 76.5% F-score on a
very rich set of GRs in the more heterogeneous and
challenging Susanne corpus. Lin (1998) evaluates
his MINIPAR system at 83% F-score on identifica-
tion of GRs, also in data from the Susanne corpus
(but using simpler GR set than Briscoe and Carroll).
GR Precision Recall F-score
SUBJ 0.94 0.93 0.93
OBJ 0.83 0.91 0.87
COORD 0.68 0.85 0.75
JCT 0.91 0.82 0.86
MOD 0.79 0.92 0.85
PRED 0.80 0.83 0.81
ROOT 0.91 0.92 0.91
COMP 0.60 0.50 0.54
XCOMP 0.58 0.64 0.61
Table 1: Precision, recall and F-score (harmonic
mean) of selected Grammatical Relations.
4 Automating IPSyn
Calculating IPSyn scores manually is a laborious
process that involves identifying 56 syntactic struc-
tures (or their absence) in a transcript of 100 child
utterances. Currently, researchers work with a par-
tially automated process by using transcripts in elec-
tronic format and spreadsheets. However, the ac-
tual identification of syntactic structures, which ac-
counts for most of the time spent on calculating IP-
Syn scores, still has to be done manually.
By using part-of-speech and morphological anal-
ysis tools, it is possible to narrow down the num-
ber of sentences where certain structures may be
found. The search for such sentences involves pat-
terns of words and parts-of-speech (POS). Some
structures, such as the presence of determiner-noun
or determiner-adjective-noun sequences, can be eas-
ily identified through the use of simple patterns.
Other structures, such as front or center-embedded
clauses, pose a greater challenge. Not only are pat-
terns for such structures difficult to craft, they are
also usually inaccurate. Patterns that are too gen-
eral result in too many sentences to be manually ex-
amined, but more restrictive patterns may miss sen-
tences where the structures are present, making their
identification highly unlikely. Without more syntac-
tic analysis, automatic searching for structures in IP-
Syn is limited, and computation of IPSyn scores still
requires a great deal of manual inspection.
Long, Fey and Channell (2004) have developed
a software package, Computerized Profiling (CP),
for child language study, which includes a (mostly)
200
automated computation of IPSyn.4 CP is an exten-
sively developed example of what can be achieved
using only POS and morphological analysis. It does
well on identifying items in IPSyn categories that
do not require deeper syntactic analysis. However,
the accuracy of overall scores is not high enough to
be considered reliable in practical usage, in particu-
lar for older children, whose utterances are longer
and more sophisticated syntactically. In practice,
researchers usually employ CP as a first pass, and
manually correct the automatic output. Section 5
presents an evaluation of the CP version of IPSyn.
Syntactic analysis of transcripts as described in
section 3 allows us to go a step further, fully au-
tomating IPSyn computations and obtaining a level
of reliability comparable to that of human scoring.
The ability to search for both grammatical relations
and parts-of-speech makes searching both easier and
more reliable. As an example, consider the follow-
ing sentences (keeping in mind that there are no ex-
plicit commas in spoken language):
(a) Then [,] he said he ate.
(b) Before [,] he said he ate.
(c) Before he ate [,] he ran.
Sentences (a) and (b) are similar, but (c) is dif-
ferent. If we were looking for a fronted subordinate
clause, only (c) would be a match. However, each
one of the sentences has an identical part-speech-
sequence. If this were an isolated situation, we
might attempt to fix it by having tags that explic-
itly mark verbs that take clausal complements, or by
adding lexical constraints to a search over part-of-
speech patterns. However, even by modifying this
simple example slightly, we find more problems:
(d) Before [,] he told the man he was cold.
(e) Before he told the story [,] he was cold.
Once again, sentences (d) and (e) have identical
part-of-speech sequences, but only sentence (e) fea-
tures a fronted subordinate clause. These limited toy
examples only scratch the surface of the difficulties
in identifying syntactic structures without syntactic
4Although CP requires that a few decisions be made man-
ually, such as the disambiguation of the lexical item ??s? as
copula vs. genitive case marker, and the definition of sentence
breaks for long utterances, the computation of IPSyn scores is
automated to a large extent.
analysis beyond part-of-speech and morphological
tagging. In these sentences, searching with GRs
is easy: we simply find a GR of clausal type (e.g.
CJCT, COMP, CMOD, etc) where the dependent is
to the left of its head.
For illustration purposes of how searching for
structures in IPSyn is done with GRs, let us look
at how to find other IPSyn structures5:
? Wh-embedded clauses: search for wh-words
whose head, or transitive head (its head?s head,
or head?s head?s head...) is a dependent in
GR of types [XC]SUBJ, [XC]PRED, [XC]JCT,
[XC]MOD, COMP or XCOMP;
? Relative clauses: search for a CMOD where the
dependent is to the right of the head;
? Bitransitive predicate: search for a word that is
a head of both OBJ and OBJ2 relations.
Although there is still room for under- and over-
generalization with search patterns involving GRs,
finding appropriate ways to search is often made
trivial, or at least much more simple and reliable
than searching without GRs. An evaluation of our
automated version of IPSyn, which searches for IP-
Syn structures using POS, morphology and GR in-
formation, and a comparison to the CP implemen-
tation, which uses only POS and morphology infor-
mation, is presented in section 5.
5 Evaluation
We evaluate our implementation of IPSyn in two
ways. The first is Point Difference, which is cal-
culated by taking the (unsigned) difference between
scores obtained manually and automatically. The
point difference is of great practical value, since
it shows exactly how close automatically produced
scores are to manually produced scores. The second
is Point-to-Point Accuracy, which reflects the overall
reliability over each individual scoring decision in
the computation of IPSyn scores. It is calculated by
counting how many decisions (identification of pres-
ence/absence of language structures in the transcript
being scored) were made correctly, and dividing that
5More detailed descriptions and examples of each structure
are found in (Scarborough, 1990), and are omitted here for
space considerations, since the short descriptions are fairly self-
explanatory.
201
number by the total number of decisions. The point-
to-point measure is commonly used for assessing the
inter-rater reliability of metrics such as the IPSyn. In
our case, it allows us to establish the reliability of au-
tomatically computed scores against human scoring.
5.1 Test Data
We obtained two sets of transcripts with correspond-
ing IPSyn scoring (total scores, and each individual
decision) from two different child language research
groups. The first set (A) contains 20 transcripts of
children of ages ranging between two and three. The
second set (B) contains 25 transcripts of children of
ages ranging between eight and nine.
Each transcript in set A was scored fully manu-
ally. Researchers looked for each language structure
in the IPSyn scoring guide, and recorded its pres-
ence in a spreadsheet. In set B, scoring was done
in a two-stage process. In the first stage, each tran-
script was scored automatically by CP. In the second
stage, researchers checked each automatic decision
made by CP, and corrected any errors manually.
Two transcripts in each set were held out for de-
velopment and debugging. The final test sets con-
tained: (A) 18 transcripts with a total of 11,704
words and a mean length of utterance of 2.9, and
(B) 23 transcripts with a total of 40,819 words and a
mean length of utterance of 7.0.
5.2 Results
Scores computed automatically from transcripts
parsed as described in section 3 were very close
to the scores computed manually. Table 2 shows a
summary of the results, according to our two eval-
uation metrics. Our system is labeled as GR, and
manually computed scores are labeled as HUMAN.
For comparison purposes, we also show the results
of running Long et al?s automated version of IPSyn,
labeled as CP, on the same transcripts.
Point Difference
The average (absolute) point difference between au-
tomatically computed scores (GR) and manually
computed scores (HUMAN) was 3.3 (the range of
HUMAN scores on the data was 21-91). There was
no clear trend on whether the difference was posi-
tive or negative. In some cases, the automated scores
were higher, in other cases lower. The minimum dif-
System Avg. Pt. Difference Point-to-Point
to HUMAN Reliability
GR (Total) 3.3 92.8%
CP (Total) 8.3 85.4%
GR (Set A) 3.7 92.5%
CP (Set A) 6.2 86.2%
GR (Set B) 2.9 93.0%
CP (Set B) 10.2 84.8%
Table 2: Summary of evaluation results. GR is our
implementation of IPSyn based on grammatical re-
lations, CP is Long et al?s (2004) implementation of
IPSyn, and HUMAN is manual scoring.
Histogram of Point Differences (3 point bins)
0
10
20
30
40
50
60
3 6 9 12 15 18 21Point Difference
Freque
ncy (%) GRCP
Figure 3: Histogram of point differences between
HUMAN scores and GR (black), and CP (white).
ference was zero, and the maximum difference was
12. Only two scores differed by 10 or more, and 17
scores differed by two or less. The average point dif-
ference between HUMAN and the scores obtained
with Long et al?s CP was 8.3. The minimum was
zero and the maximum was 21. Sixteen scores dif-
fered by 10 or more, and six scores differed by 2 or
less. Figure 3 shows the point differences between
GR and HUMAN, and CP and HUMAN.
It is interesting to note that the average point dif-
ferences between GR and HUMAN were similar on
sets A and B (3.7 and 2.9, respectively). Despite the
difference in age ranges, the two averages were less
than one point apart. On the other hand, the average
difference between CP and HUMAN was 6.2 on set
A, and 10.2 on set B. The larger difference reflects
CP?s difficulty in scoring transcripts of older chil-
dren, whose sentences are more syntactically com-
plex, using only POS analysis.
202
Point-to-Point Accuracy
In the original IPSyn reliability study (Scarborough,
1990), point-to-point measurements using 75 tran-
scripts showed the mean inter-rater agreement for
IPSyn among human scorers at 94%, with a min-
imum agreement of 90% of all decisions within a
transcript. The lowest agreement between HUMAN
and GR scoring for decisions within a transcript was
88.5%, with a mean of 92.8% over the 41 transcripts
used in our evaluation. Although comparisons of
agreement figures obtained with different sets of
transcripts are somewhat coarse-grained, given the
variations within children, human scorers and tran-
script quality, our results are very satisfactory. For
direct comparison purposes using the same data, the
mean point-to-point accuracy of CP was 85.4% (a
relative increase of about 100% in error).
In their separate evaluation of CP, using 30 sam-
ples of typically developing children, Long and
Channell (2001) found a 90.7% point-to-point ac-
curacy between fully automatic and manually cor-
rected IPSyn scores.6 However, Long and Channell
compared only CP output with manually corrected
CP output, while our set A was manually scored
from scratch. Furthermore, our set B contained
only transcripts from significantly older children (as
in our evaluation, Long and Channell observed de-
creased accuracy of CP?s IPSyn with more com-
plex language usage). These differences, and the
expected variation from using different transcripts
from different sources, account for the difference in
our results and Long and Channell?s.
5.3 Error Analysis
Although the overall accuracy of our automatically
computed scores is in large part comparable to man-
ual IPSyn scoring (and significantly better than the
only option currently available for automatic scor-
ing), our system suffers from visible deficiencies in
the identification of certain structures within IPSyn.
Four of the 56 structures in IPSyn account for al-
most half of the number of errors made by our sys-
tem. Table 3 lists these IPSyn items, with their re-
spective percentages of the total number of errors.
6Long and Channell?s evaluation also included samples
from children with language disorders. Their 30 samples of
typically developing children (with a mean age of 5) are more
directly comparable to the data used in our evaluation.
IPSyn item Error
S11 (propositional complement) 16.9%
V15 (copula, modal or aux for 12.3%
emphasis or ellipsis)
S16 (relative clause) 10.6%
S14 (bitransitive predicate) 5.8%
Table 3: IPSyn structures where errors occur most
frequently, and their percentages of the total number
of errors over 41 transcripts.
Errors in items S11 (propositional complements),
S16 (relative clauses), and S14 (bitransitive predi-
cates) are caused by erroneous syntactic analyses.
For an example of how GR assignments affect IP-
Syn scoring, let us consider item S11. Searching for
the relation COMP is a crucial part in finding propo-
sitional complements. However, COMP is one of
the GRs that can be identified the least reliably in
our set (precision of 0.6 and recall of 0.5, see table
1). As described in section 2, IPSyn requires that
we credit zero points to item S11 for no occurrences
of propositional complements, one point for a single
occurrence, and two points for two or more occur-
rences. If there are several COMPs in the transcript,
we should find about half of them (plus others, in
error), and correctly arrive at a credit of two points.
However, if there are very few or none, our count is
likely to be incorrect.
Most errors in item V15 (emphasis or ellipsis)
were caused not by incorrect GR assignments, but
by imperfect search patterns. The searching failed to
account for a number of configurations of GRs, POS
tags and words that indicate that emphasis or ellip-
sis exists. This reveals another general source of er-
ror in our IPSyn implementation: the search patterns
that use GR analyzed text to make the actual IP-
Syn scoring decisions. Although our patterns are far
more reliable than what we could expect from POS
tags and words alone, these are still hand-crafted
rules that need to be debugged and perfected over
time. This was the first evaluation of our system,
and only a handful of transcripts were used during
development. We expect that once child language
researchers have had the opportunity to use the sys-
tem in practical settings, their feedback will allow us
to refine the search patterns at a more rapid pace.
203
6 Conclusion and Future Work
We have presented an automatic way to annotate
transcripts of child language with the CHILDES
syntactic annotation scheme. By using existing re-
sources and a small amount of annotated data, we
achieved state-of-the-art accuracy levels.
GR identification was then used to automate the
computation of IPSyn scores to measure grammati-
cal development in children. The reliability of our
automatic IPSyn was very close to the inter-rater re-
liability among human scorers, and far higher than
that of the only other computational implementation
of IPSyn. This demonstrates the value of automatic
GR assignment to child language research.
From the analysis in section 5.3, it is clear that the
identification of certain GRs needs to be made more
accurately. We intend to annotate more in-domain
training data for GR labeling, and we are currently
investigating the use of other applicable GR parsing
techniques.
Finally, IPSyn score calculation could be made
more accurate with the knowledge of the expected
levels of precision and recall of automatic assign-
ment of specific GRs. It is our intuition that in a
number of cases it would be preferable to trade re-
call for precision. We are currently working on a
framework for soft-labeling of GRs, which will al-
low us to manipulate the precision/recall trade-off
as discussed in (Carroll and Briscoe, 2002).
Acknowledgments
This work was supported in part by the National Sci-
ence Foundation under grant IIS-0414630.
References
Edward J. Briscoe and John A. Carroll. 2002. Robust ac-
curate statistical annotation of general text. Proceed-
ings of the 3rd International Conference on Language
Resources and Evaluation, (pp. 1499?1504). Las Pal-
mas, Gran Canaria.
John A. Carroll and Edward J. Briscoe. 2002. High pre-
cision extraction of grammatical relations. Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics, (pp. 134-140). Taipei, Taiwan.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics. Seattle, WA.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. Proceedings of the 34th
Meeting of the Association for Computational Linguis-
tics (pp. 184-191). Santa Cruz, CA.
Walter Daelemans, Jacub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. TiMBL: Tilburg Memory
Based Learner, version 5.1, Reference Guide. ILK Re-
search Group Technical Report Series no. 04-02, 2004.
T. Klee and M. D. Fitzgerald. 1985. The relation be-
tween grammatical development and mean length of
utterance in morphemes. Journal of Child Language,
12, 251-269.
Dan Klein and Christopher D. Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics (pp.
128-135).
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems. Granada, Spain.
Steve H. Long and Ron W. Channell. 2001. Accuracy of
four language analysis procedures performed automat-
ically. American Journal of Speech-Language Pathol-
ogy, 10(2).
Steven H. Long, Marc E. Fey, and Ron W. Channell.
2004. Computerized Profiling (Version 9.6.0). Cleve-
land, OH: Case Western Reserve University.
Brian MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Mahwah, NJ: Lawrence Erlbaum
Associates.
Mitchel P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewics. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19.
Joakim Nivre and Mario Scholz. 2004. Deterministic de-
pendency parsing of English text. Proceedings of In-
ternational Conference on Computational Linguistics
(pp. 64-70). Geneva, Switzerland.
Christophe Parisse and Marie-Thrse Le Normand. 2000.
Automatic disambiguation of the morphosyntax in
spoken language corpora. Behavior Research Meth-
ods, Instruments, and Computers, 32, 468-481.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2004.
Adding Syntactic annotations to transcripts of parent-
child dialogs. Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC 2004). Lisbon, Portugal.
Hollis S. Scarborough. 1990. Index of Productive Syn-
tax. In Applied Psycholinguistics, 11, 1-22.
204
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 101?104, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Multi-Engine Machine Translation Guided by Explicit Word Matching 
 
 
Shyamsundar Jayaraman Alon Lavie 
Language Technologies Institute  Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213 Pittsburgh, PA 15213 
shyamj@cs.cmu.edu alavie@cs.cmu.edu 
 
 
Abstract 
We describe a new approach for syntheti-
cally combining the output of several dif-
ferent Machine Translation (MT) engines 
operating on the same input.  The goal is 
to produce a synthetic combination that 
surpasses all of the original systems in 
translation quality.  Our approach uses the 
individual MT engines as ?black boxes? 
and does not require any explicit coopera-
tion from the original MT systems.  A de-
coding algorithm uses explicit word 
matches, in conjunction with confidence 
estimates for the various engines and a tri-
gram language model in order to score 
and rank a collection of sentence hypothe-
ses that are synthetic combinations of 
words from the various original engines.  
The highest scoring sentence hypothesis 
is selected as the final output of our sys-
tem.  Experiments, using several Arabic-
to-English systems of similar quality, 
show a substantial improvement in the 
quality of the translation output.  
1 Introduction 
A variety of different paradigms for machine 
translation (MT) have been developed over the 
years, ranging from statistical systems that learn 
mappings between words and phrases in the source 
language and their corresponding translations in 
the target language, to Interlingua-based systems 
that perform deep semantic analysis.  Each ap-
proach and system has different advantages and 
disadvantages.  While statistical systems provide 
broad coverage with little manpower, the quality of 
the corpus based systems rarely reaches the quality 
of knowledge based systems. 
With such a wide range of approaches to ma-
chine translation, it would be beneficial to have an 
effective framework for combining these systems 
into an MT system that carries many of the advan-
tages of the individual systems and suffers from 
few of their disadvantages.  Attempts at combining 
the output of different systems have proved useful 
in other areas of language technologies, such as the 
ROVER approach for speech recognition (Fiscus 
1997).  Several approaches to multi-engine ma-
chine translation systems have been proposed over 
the past decade. The Pangloss system and work by 
several other researchers attempted to combine 
lattices from many different MT systems (Fred-
erking et Nirenburg 1994, Frederking et al1997; 
Tidhar & K?ssner 2000; Lavie, Probst et al 2004).  
These systems suffer from requiring cooperation 
from all the systems to produce compatible lattices 
as well as the hard research problem of standardiz-
ing confidence scores that come from the individ-
ual engines. In 2001, Bangalore et alused string 
alignments between the different translations to 
train a finite state machine to produce a consensus 
translation.  The alignment algorithm described in 
that work, which only allows insertions, deletions 
and substitutions, does not accurately capture long 
range phrase movement. 
In this paper, we propose a new way of com-
bining the translations of multiple MT systems 
based on a more versatile word alignment algo-
rithm.  A ?decoding? algorithm then uses these 
alignments, in conjunction with confidence esti-
mates for the various engines and a trigram lan-
guage model, in order to score and rank a 
collection of sentence hypotheses that are synthetic 
combinations of words from the various original 
engines.  The highest scoring sentence hypothesis 
is selected as the final output of our system. We 
101
experimentally tested the new approach by com-
bining translations obtained from combining three 
Arabic-to-English translation systems. Translation 
quality is scored using the METEOR MT evalua-
tion metric (Lavie, Sagae  et al2004).  Our ex-
periments demonstrate that our new MEMT system 
achieves a substantial improvement over all of the 
original systems, and also outperforms an ?oracle? 
capable of selecting the best of the original systems 
on a sentence-by-sentence basis. 
The remainder of this paper is organized as 
follows.  In section 2 we describe the algorithm for 
generating multi-engine synthetic translations.  
Section 3 describes the experimental setup used to 
evaluate our approach, and section 4 presents the 
results of the evaluation.  Our conclusions and di-
rections for future work are presented in section 5.  
2 The MEMT Algorithm 
Our Multi-Engine Machine Translation 
(MEMT) system operates on the single ?top-best? 
translation output produced by each of several MT 
systems operating on a common input sentence.  
MEMT first aligns the words of the different trans-
lation systems using a word alignment matcher.  
Then, using the alignments provided by the 
matcher, the system generates a set of synthetic 
sentence hypothesis translations.  Each hypothesis 
translation is assigned a score based on the align-
ment information, the confidence of the individual 
systems, and a language model.  The hypothesis 
translation with the best score is selected as the 
final output of the MEMT combination. 
2.1 The Word Alignment Matcher 
The task of the matcher is to produce a word-
to-word alignment between the words of two given 
input strings.  Identical words that appear in both 
input sentences are potential matches.  Since the 
same word may appear multiple times in the sen-
tence, there are multiple ways to produce an 
alignment between the two input strings.   The goal 
is to find the alignment that represents the best cor-
respondence between the strings.  This alignment 
is defined as the alignment that has the smallest 
number of ?crossing edges.   The matcher can also 
consider morphological variants of the same word 
as potential matches.  To simultaneously align 
more than two sentences, the matcher simply pro-
duces alignments for all pair-wise combinations of 
the set of sentences. 
In the context of its use within our MEMT ap-
proach, the word-alignment matcher provides three 
main benefits.  First, it explicitly identifies trans-
lated words that appear in multiple MT transla-
tions, allowing the MEMT algorithm to reinforce 
words that are common among the systems.  Sec-
ond, the alignment information allows the algo-
rithm to ensure that aligned words are not included 
in a synthetic combination more than once. Third, 
by allowing long range matches, the synthetic 
combination generation algorithm can consider 
different plausible orderings of the matched words, 
based on their location in the original translations. 
2.2 Basic Hypothesis Generation 
After the matcher has word aligned the original 
system translations, the decoder goes to work.  The 
hypothesis generator produces synthetic combina-
tions of words and phrases from the original trans-
lations that satisfy a set of adequacy constraints.  
The generation algorithm is an iterative process 
and produces these translation hypotheses incre-
mentally.  In each iteration, the set of existing par-
tial hypotheses is extended by incorporating an 
additional word from one of the original transla-
tions.  For each partial hypothesis, a data-structure 
keeps track of the words from the original transla-
tions which are accounted for by this partial hy-
pothesis.  One underlying constraint observed by 
the generator is that the original translations are 
considered in principle to be word synchronous in 
the sense that selecting a word from one original 
translation normally implies ?marking? a corre-
sponding word in each of the other original transla-
tions as ?used?.  The way this is determined is 
explained below.  Two partial hypotheses that have 
the same partial translation, but have a different set 
of words that have been accounted for are consid-
ered different.  A hypothesis is considered ?com-
plete? if the next word chosen to extend the 
hypothesis is the explicit end-of-sentence marker 
from one of the original translation strings.  At the 
start of hypothesis generation, there is a single hy-
pothesis, which has the empty string as its partial 
translation and where none of the words in any of 
the original translations are marked as used. 
In each iteration, the decoder extends a hy-
pothesis by choosing the next unused word from 
102
one of the original translations.  When the decoder 
chooses to extend a hypothesis by selecting word w 
from original system A, the decoder marks w as 
used. The decoder then proceeds to identify and 
mark as used a word in each of the other original 
systems.  If w is aligned to words in any of the 
other original translation systems, then the words 
that are aligned with w are also marked as used.  
For each system that does not have a word that 
aligns with w, the decoder establishes an artificial 
alignment between w and a word in this system.  
The intuition here is that this artificial alignment 
corresponds to a different translation of the same 
source-language word that corresponds to w.  The 
choice of an artificial alignment cannot violate 
constraints that are imposed by alignments that 
were found by the matcher.  If no artificial align-
ment can be established, then no word from this 
system will be marked as used.  The decoder re-
peats this process for each of the original transla-
tions.  Since the order in which the systems are 
processed matters, the decoder produces a separate 
hypothesis for each order. 
Each iteration expands the previous set of partial 
hypotheses, resulting in a large space of complete 
synthetic hypotheses.  Since this space can grow 
exponentially, pruning based on scoring of the par-
tial hypotheses is applied when necessary. 
2.3 Confidence Scores 
A major component in the scoring of hypothe-
sis translations is a confidence score that is as-
signed to each of the original translations, which 
reflects the translation adequacy of the system that 
produced it.  We associate a confidence score with 
each word in a synthetic translation based on the 
confidence of the system from which it originated.  
If the word was contributed by several different 
original translations, we sum the confidences of the 
contributing systems.  This word confidence score 
is combined multiplicatively with a score assigned 
to the word by a trigram language model. The 
score assigned to a complete hypothesis is its geo-
metric average word score.  This removes the in-
herent bias for shorter hypotheses that is present in 
multiplicative cumulative scores. 
2.4 Restrictions on Artificial Alignments 
The basic algorithm works well as long the 
original translations are reasonably word synchro-
nous. This rarely occurs, so several additional con-
straints are applied during hypothesis generation.  
First, the decoder discards unused words in origi-
nal systems that ?linger? around too long. Second, 
the decoder limits how far ahead it looks for an 
artificial alignment, to prevent incorrect long-range 
artificial alignments.  Finally, the decoder does not 
allow an artificial match between words that do not 
share the same part-of-speech.  
3 Experimental Setup 
We combined outputs of three Arabic-to-English 
machine translation systems on the 2003 TIDES 
Arabic test set.  The systems were AppTek?s rule 
based system, CMU?s EBMT system, and 
Systran?s web-based translation system. 
We compare the results of MEMT to the indi-
vidual online machine translation systems.  We 
also compare the performance of MEMT to the 
score of an ?oracle system? that chooses the best 
scoring of the individual systems for each sen-
tence.  Note that this oracle is not a realistic sys-
tem, since a real system cannot determine at run-
time which of the original systems is best on a sen-
tence-by-sentence basis.  One goal of the evalua-
tion was to see how rich the space of synthetic 
translations produced by our hypothesis generator 
is.  To this end, we also compare the output se-
lected by our current MEMT system to an ?oracle 
system? that chooses the best synthetic translation 
that was generated by the decoder for each sen-
tence.  This too is not a realistic system, but it al-
lows us to see how well our hypothesis scoring 
currently performs. This also provides a way of 
estimating a performance ceiling of the MEMT 
approach, since our MEMT can only produce 
words that are provided by the original systems 
(Hogan and Frederking 1998). 
Due to the computational complexity of run-
ning the oracle system, several practical restric-
tions were imposed.  First, the oracle system only 
had access to the top 1000 translation hypotheses 
produced by MEMT for each sentence.  While this 
does not guarantee finding the best translation that 
the decoder can produce, this method provides a 
good approximation.  We also ran the oracle ex-
periment only on the first 140 sentences of the test 
sets due to time constraints. 
All the system performances are measured us-
ing the METEOR evaluation metric (Lavie, Sagae 
103
et al, 2004).  METEOR was chosen since, unlike 
the more commonly used BLEU metric (Papineni 
et al, 2002), it provides reasonably reliable scores 
for individual sentences.  This property is essential 
in order to run our oracle experiments.  METEOR 
produces scores in the range of [0,1], based on a 
combination of unigram precision, unigram recall 
and an explicit penalty related to the average 
length of matched segments between the evaluated 
translation and its reference. 
4 Results 
System METEOR Score 
System A 0.4241 
System B 0.4231 
System C 0.4405 
Choosing best original translation 0.4432 
MEMT System  0.5183 
 
Table 1: METEOR Scores on TIDES 2003 Dataset 
 
On the 2003 TIDES data, the three original sys-
tems had similar METEOR scores.  Table 1 shows 
the scores of the three systems, with their names 
obscured to protect their privacy.  Also shown are 
the score of MEMT?s output and the score of the 
oracle system that chooses the best original transla-
tion on a sentence-by-sentence basis.  The score of 
the MEMT system is significantly better than any 
of the original systems, and the sentence oracle. 
On the first 140 sentences, the oracle system that 
selects the best hypothesis translation generated by 
the MEMT generator has a METEOR score of 
0.5883.  This indicates that the scoring algorithm 
used to select the final MEMT output can be sig-
nificantly further improved. 
5 Conclusions and Future Work 
Our MEMT algorithm shows consistent im-
provement in the quality of the translation com-
pared any of the original systems.  It scores better 
than an ?oracle? that chooses the best original 
translation on a sentence-by-sentence basis. Fur-
thermore, our MEMT algorithm produces hypothe-
ses that are of yet even better quality, but our 
current scoring algorithm is not yet able to effec-
tively select the best hypothesis.  The focus of our 
future work will thus be on identifying features 
that support improved hypothesis scoring. 
Acknowledgments 
This research work was partly supported by a grant 
from the US Department of Defense.  The word 
alignment matcher was developed by Satanjeev 
Banerjee.  We wish to thank Robert Frederking, 
Ralf Brown and Jaime Carbonell for their valuable 
input and suggestions. 
References  
Bangalore, S., G.Bordel, and G. Riccardi (2001). Com-
puting Consensus Translation from Multiple Machine 
Translation Systems.  In Proceedings of IEEE Auto-
matic Speech Recognition and Understanding Work-
shop (ASRU-2001), Italy. 
Fiscus, J. G.(1997). A Post-processing System to Yield 
Reduced Error Word Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). In IEEE Workshop 
on Automatic Speech Recognition and Understanding 
(ASRU-1997). 
Frederking, R. and S. Nirenburg. Three Heads are Better 
than One. In Proceedings of the Fourth Conference 
on Applied Natural Language Processing (ANLP-
94), Stuttgart, Germany, 1994. 
Hogan, C. and R.E.Frederking (1998). An Evaluation of 
the Multi-engine MT Architecture. In Proceedings of 
the Third Conference of the Association for Machine 
Translation in the Americas, pp. 113-123. Springer-
Verlag, Berlin . 
Lavie, A., K. Probst, E. Peterson, S. Vogel, L.Levin, A. 
Font-Llitjos and J. Carbonell (2004). A Trainable 
Transfer-based Machine Translation Approach for 
Languages with Limited Resources. In Proceedings 
of Workshop of the European Association for Ma-
chine Translation (EAMT-2004), Valletta, Malta. 
Lavie, A., K. Sagae and S. Jayaraman (2004). The Sig-
nificance of Recall in Automatic Metrics for MT 
Evaluation. In Proceedings of the 6th Conference of 
the Association for Machine Translation in the 
Americas (AMTA-2004), Washington, DC. 
Papineni, K., S. Roukos, T. Ward and W-J Zhu (2002). 
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of 40th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-2002), Philadelphia, PA. 
Tidhar, Dan and U. K?ssner (2000). Learning to Select 
a Good Translation. In Proceedings of the 17th con-
ference on Computational linguistics (COLING 
2000), Saarbr?cken, Germany. 
104
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 691?698,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Best-First Probabilistic Shift-Reduce Parser
Kenji Sagae and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{sagae,alavie}@cs.cmu.edu
Abstract
Recently proposed deterministic classifier-
based parsers (Nivre and Scholz, 2004;
Sagae and Lavie, 2005; Yamada and Mat-
sumoto, 2003) offer attractive alternatives
to generative statistical parsers. Determin-
istic parsers are fast, efficient, and sim-
ple to implement, but generally less ac-
curate than optimal (or nearly optimal)
statistical parsers. We present a statis-
tical shift-reduce parser that bridges the
gap between deterministic and probabilis-
tic parsers. The parsing model is essen-
tially the same as one previously used
for deterministic parsing, but the parser
performs a best-first search instead of a
greedy search. Using the standard sec-
tions of the WSJ corpus of the Penn Tree-
bank for training and testing, our parser
has 88.1% precision and 87.8% recall (us-
ing automatically assigned part-of-speech
tags). Perhaps more interestingly, the pars-
ing model is significantly different from
the generative models used by other well-
known accurate parsers, allowing for a
simple combination that produces preci-
sion and recall of 90.9% and 90.7%, re-
spectively.
1 Introduction
Over the past decade, researchers have devel-
oped several constituent parsers trained on an-
notated data that achieve high levels of accu-
racy. Some of the more popular and more ac-
curate of these approaches to data-driven parsing
(Charniak, 2000; Collins, 1997; Klein and Man-
ning, 2002) have been based on generative mod-
els that are closely related to probabilistic context-
free grammars. Recently, classifier-based depen-
dency parsing (Nivre and Scholz, 2004; Yamada
and Matsumoto, 2003) has showed that determin-
istic parsers are capable of high levels of accu-
racy, despite great simplicity. This work has led to
the development of deterministic parsers for con-
stituent structures as well (Sagae and Lavie, 2005;
Tsuruoka and Tsujii, 2005). However, evaluations
on the widely used WSJ corpus of the Penn Tree-
bank (Marcus et al, 1993) show that the accuracy
of these parsers still lags behind the state-of-the-
art.
A reasonable and commonly held assumption is
that the accuracy of deterministic classifier-based
parsers can be improved if determinism is aban-
doned in favor of a search over a larger space of
possible parses. While this assumption was shown
to be true for the parser of Tsuruoka and Tsu-
jii (2005), only a moderate improvement resulted
from the addition of a non-greedy search strategy,
and overall parser accuracy was still well below
that of state-of-the-art statistical parsers.
We present a statistical parser that is based on
a shift-reduce algorithm, like the parsers of Sagae
and Lavie (2005) and Nivre and Scholz (2004), but
performs a best-first search instead of pursuing a
single analysis path in deterministic fashion. The
parser retains much of the simplicity of determin-
istic classifier-based parsers, but achieves results
that are closer in accuracy to state-of-the-art statis-
tical parsers. Furthermore, a simple combination
of the shift-reduce parsing model with an existing
generative parsing model produces results with ac-
curacy that surpasses any that of any single (non-
reranked) parser tested on the WSJ Penn Tree-
bank, and comes close to the best results obtained
with discriminative reranking (Charniak and John-
691
son, 2005).
2 Parser Description
Our parser uses an extended version of the basic
bottom-up shift-reduce algorithm for constituent
structures used in Sagae and Lavie?s (2005) de-
terministic parser. For clarity, we will first de-
scribe the deterministic version of the algorithm,
and then show how it can be extended into a proba-
bilistic algorithm that performs a best-first search.
2.1 A Shift-Reduce Algorithm for
Deterministic Constituent Parsing
In its deterministic form, our parsing algorithm
is the same single-pass shift-reduce algorithm as
the one used in the classifer-based parser of Sagae
and Lavie (2005). That algorithm, in turn, is sim-
ilar to the dependency parsing algorithm of Nivre
and Scholz (2004), but it builds a constituent tree
and a dependency tree simultaneously. The al-
gorithm considers only trees with unary and bi-
nary productions. Training the parser with arbi-
trary branching trees is accomplished by a sim-
ple procedure to transform those trees into trees
with at most binary productions. This is done
by converting each production with n children,
where n > 2, into n ? 1 binary productions.
This binarization process is similar to the one de-
scribed in (Charniak et al, 1998). Additional non-
terminal nodes introduced in this conversion must
be clearly marked. Transforming the parser?s out-
put into arbitrary branching trees is accomplished
using the reverse process.
The deterministic parsing algorithm involves
two main data structures: a stack S, and a queue
W . Items in S may be terminal nodes (part-of-
speech-tagged words), or (lexicalized) subtrees of
the final parse tree for the input string. Items in W
are terminals (words tagged with parts-of-speech)
corresponding to the input string. When parsing
begins, S is empty and W is initialized by insert-
ing every word from the input string in order, so
that the first word is in front of the queue.
The algorithm defines two types of parser ac-
tions, shift and reduce, explained below:
? Shift: A shift action consists only of remov-
ing (shifting) the first item (part-of-speech-
tagged word) from W (at which point the
next word becomes the new first item), and
placing it on top of S.
? Reduce: Reduce actions are subdivided into
unary and binary cases. In a unary reduction,
the item on top of S is popped, and a new
item is pushed onto S. The new item consists
of a tree formed by a non-terminal node with
the popped item as its single child. The lex-
ical head of the new item is the same as the
lexical head of the popped item. In a binary
reduction, two items are popped from S in
sequence, and a new item is pushed onto S.
The new item consists of a tree formed by a
non-terminal node with two children: the first
item popped from S is the right child, and the
second item is the left child. The lexical head
of the new item may be the lexical head of its
left child, or the lexical head of its right child.
If S is empty, only a shift action is allowed. If
W is empty, only a reduce action is allowed. If
both S and W are non-empty, either shift or re-
duce actions are possible, and the parser must de-
cide whether to shift or reduce. If it decides to re-
duce, it must also choose between a unary-reduce
or a binary-reduce, what non-terminal should be at
the root of the newly created subtree to be pushed
onto the stack S, and whether the lexical head of
the newly created subtree will be taken from the
right child or the left child of its root node. Fol-
lowing the work of Sagae and Lavie, we consider
the complete set of decisions associated with a re-
duce action to be part of that reduce action. Pars-
ing terminates when W is empty and S contains
only one item, and the single item in S is the parse
tree for the input string.
2.2 Shift-Reduce Best-First Parsing
A deterministic shift-reduce parser based on the
algorithm described in section 2.1 does not handle
ambiguity. By choosing a single parser action at
each opportunity, the input string is parsed deter-
ministically, and a single constituent structure is
built during the parsing process from beginning to
end (no other structures are even considered).
A simple extension to this idea is to eliminate
determinism by allowing the parser to choose sev-
eral actions at each opportunity, creating different
paths that lead to different parse trees. This is es-
sentially the difference between deterministic LR
parsing (Knuth, 1965) and Generalized-LR pars-
ing (Tomita, 1987; Tomita, 1990). Furthermore,
if a probability is assigned to every parser action,
the probability of a parse tree can be computed
692
simply as the product of the probabilities of each
action in the path that resulted in that parse tree
(the derivation of the tree). This produces a prob-
abilistic shift-reduce parser that resembles a gen-
eralized probabilistic LR parser (Briscoe and Car-
roll, 1993), where probabilities are associated with
an LR parsing table. In our case, although there
is no LR table, the action probabilities are associ-
ated with several aspects of the current state of the
parser, which to some extent parallel the informa-
tion contained in an LR table. Instead of having
an explicit LR table and pushing LR states onto
the stack, the state of the parser is implicitly de-
fined by the configurations of the stack and queue.
In a way, there is a parallel between how mod-
ern PCFG-like parsers use markov grammars as
a distribution that is used to determine the proba-
bility of any possible grammar rules, and the way
a statistical model is used in our parser to assign
a probability to any transition of parser states (in-
stead of a symbolic LR table).
Pursuing every possible sequence of parser ac-
tions creates a very large space of actions for
even moderately sized sentences. To find the most
likely parse tree efficiently according to the prob-
abilistic shift-reduce parsing scheme described so
far, we use a best-first strategy. This involves an
extension of the deterministic shift-reduce algo-
rithm into a best-first shift-reduce algorithm. To
describe this extension, we first introduce a new
data structure Ti that represents a parser state,
which includes a stack Si and a queue Wi. In
the deterministic algorithm, we would have a sin-
gle parser state T that contains S and W . The
best-first algorithm, on the other hand, has a heap
H containing multiple parser states T1 ... Tn.
These states are ordered in the heap according to
their probabilities, so that the state with the highest
probability is at the top. State probabilities are de-
termined by multiplying the probabilities of each
of the actions that resulted in that state. Parser ac-
tions are determined from and applied to a parser
state Ti popped from the top of H . The parser
actions are the same as in the deterministic ver-
sion of the algorithm. When the item popped from
the top of the heap H contains a stack Si with a
single item and an empty queue (in other words,
meets the acceptance criteria for the determinis-
tic version of the algorithm), the item on top of
Si is the tree with the highest probability. At that
point, parsing terminates if we are searching for
the most probable parse. To obtain a list of n-best
parses, we simply continue parsing once the first
parse tree is found, until either n trees are found,
or H is empty.
We note that this approach does not use dy-
namic programming, and relies only on the best-
first search strategy to arrive at the most prob-
able parse efficiently. Without any pruning of
the search space, the distribution of probability
mass among different possible actions for a parse
state has a large impact on the behavior of the
search. We do not use any normalization to ac-
count for the size (in number of actions) of dif-
ferent derivations when calculating their probabili-
ties, so it may seem that shorter derivations usually
have higher probabilities than longer ones, causing
the best-first search to approximate a breadth-first
search in practice. However, this is not the case if
for a given parser state only a few actions (or, ide-
ally, only one action) have high probability, and all
other actions have very small probabilities. In this
case, only likely derivations would reach the top of
the heap, resulting in the desired search behavior.
The accuracy of deterministic parsers suggest that
this may in fact be the types of probabilities a clas-
sifier would produce given features that describe
the parser state, and thus the context of the parser
action, specifically enough. The experiments de-
scribed in section 4 support this assumption.
2.3 Classifier-Based Best-First Parsing
To build a parser based on the deterministic al-
gorithm described in section 2.1, a classifier is
used to determine parser actions. Sagae and Lavie
(2005) built two deterministic parsers this way,
one using support vector machines, and one using
k-nearest neighbors. In each case, the set of fea-
tures and classes used with each classifier was the
same. Items 1 ? 13 in figure 1 shows the features
used by Sagae and Lavie. The classes produced
by the classifier encode every aspect of a parser
action. Classes have one of the following forms:
SHIFT : represents a shift action;
REDUCE-UNARY-XX : represents a unary re-
duce action, where the root of the new sub-
tree pushed onto S is of type XX (where XX
is a non-terminal symbol, typically NP , V P ,
PP , for example);
REDUCE-LEFT-XX : represents a binary re-
duce action, where the root of the new sub-
693
tree pushed onto S is of non-terminal type
XX. Additionally, the head of the new subtree
is the same as the head of the left child of the
root node;
REDUCE-RIGHT-XX : represents a binary re-
duce action, where the root of the new sub-
tree pushed onto S is of non-terminal type
XX. Additionally, the head of the new sub-
tree is the same as the head of the right child
of the root node.
To implement a parser based on the best-first al-
gorithm, instead of just using a classifier to de-
termine one parser action given a stack and a
queue, we need a classification approach that pro-
vides us with probabilities for different parser ac-
tions associated with a given parser state. One
such approach is maximum entropy classification
(Berger et al, 1996), which we use in the form
of a library implemented by Tsuruoka1 and used
in his classifier-based parser (Tsuruoka and Tsujii,
2005). We used the same classes and the same fea-
tures as Sagae and Lavie, and an additional feature
that represents the previous parser action applied
the current parser state (figure 1).
3 Related Work
As mentioned in section 2, our parsing approach
can be seen as an extension of the approach of
Sagae and Lavie (2005). Sagae and Lavie eval-
uated their deterministic classifier-based parsing
framework using two classifiers: support vector
machines (SVM) and k-nearest neighbors (kNN).
Although the kNN-based parser performed poorly,
the SVM-based parser achieved about 86% preci-
sion and recall (or 87.5% using gold-standard POS
tags) on the WSJ test section of the Penn Tree-
bank, taking only 11 minutes to parse the test set.
Sagae and Lavie?s parsing algorithm is similar to
the one used by Nivre and Scholz (2004) for de-
terministic dependency parsing (using kNN). Ya-
mada and Matsumoto (2003) have also presented
a deterministic classifier-based (SVM-based) de-
pendency parser, but using a different parsing al-
gorithm, and using only unlabeled dependencies.
Tsuruoka and Tsujii (2005) developed a
classifier-based parser that uses the chunk-parsing
algorithm and achieves extremely high parsing
speed, but somewhat low recall. The algorithm
1The SS MaxEnt library is publicly available from
http://www-tsujii.is.s.u-tokyo.ac.jp/ tsuruoka/maxent/.
is based on reframing the parsing task as several
sequential chunking tasks.
Finally, our parser is in many ways similar to
the parser of Ratnaparkhi (1997). Ratnaparkhi?s
parser uses maximum-entropy models to deter-
mine the actions of a parser based to some extent
on the shift-reduce framework, and it is also capa-
ble of pursuing several paths and returning the top-
n highest scoring parses for a sentence. However,
in addition to using different features for parsing,
Ratnaparkhi?s parser uses a different, more com-
plex algorithm. The use of a more involved algo-
rithm allows Ratnaparkhi?s parser to work with ar-
bitrary branching trees without the need of the bi-
narization transform employed here. It breaks the
usual reduce actions into smaller pieces (CHECK
and BUILD), and uses two separate passes (not
including the part-of-speech tagging pass) for de-
termining chunks and higher syntactic structures
separately. Instead of keeping a stack, the parser
makes multiple passes over the input string, like
the dependency parsing algorithm used by Ya-
mada and Matsumoto. Our parser, on the other
hand, uses a simpler stack-based shift-reduce (LR-
like) algorithm for trees with only unary and bi-
nary productions.
4 Experiments
We evaluated our classifier-based best-first parser
on the Wall Street Journal corpus of the Penn Tree-
bank (Marcus et al, 1993) using the standard split:
sections 2-21 were used for training, section 22
was used for development and tuning of parame-
ters and features, and section 23 was used for
testing. Every experiment reported here was per-
formed on a Pentium4 3.2GHz with 2GB of RAM.
Each tree in the training set had empty-node and
function tag information removed, and the trees
were lexicalized using the same head-table rules as
in the Collins (1999) parser (these rules were taken
from Bikel?s (2002) implementation of the Collins
parser). The trees were then converted into trees
containing only unary and binary productions, us-
ing the binarization transform described in section
2. Classifier training instances of features paired
with classes (parser actions) were extracted from
the trees in the training set, and the total number
of training instances was about 1.9 million. It is in-
teresting to note that the procedure of training the
best-first parser is identical to the training of a de-
terministic version of the parser: the deterministic
694
Let:
S(n) denote the nth item from the top of the stack S, and
W (n) denote the nth item from the front of the queue W .
Features:
1. The head-word (and its POS tag) of: S(0), S(1), S(2), andS(3)
2. The head-word (and its POS tag) of: W (0), W (1), W (2) and W (3)
3. The non-terminal node of the root of: S(0), and S(1)
4. The non-terminal node of the left child of the root of: S(0), and S(1)
5. The non-terminal node of the right child of the root of: S(0), and S(1)
6. The POS tag of the head-word of the left child of the root of: S(0), and
S(1)
7. The POS tag of the head-word of the right child of the root of: S(0),
and S(1)
8. The linear distance (number of words apart) between the head-words of
S(0) and S(1)
9. The number of lexical items (words) that have been found (so far) to
be dependents of the head-words of: S(0), and S(1)
10. The most recently found lexical dependent of the head-word of S(0)
that is to the left of S(0)?s head
11. The most recently found lexical dependent of the head-word of S(0)
that is to the right of S(0)?s head
12. The most recently found lexical dependent of the head-word of S(1)
that is to the left of S(1)?s head
13. The most recently found lexical dependent of the head-word of S(1)
that is to the right of S(1)?s head
14. The previous parser action applied to the current parser state
Figure 1: Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). The
features described in items 1 ? 7 are more directly related to the lexicalized constituent trees that are built
during parsing, while the features described in items 8 ? 13 are more directly related to the dependency
structures that are built simultaneously to the constituent structures.
695
algorithm is simply run over all sentences in the
training set, and since the correct trees are known
in advance, we can simply record the features and
correct parser actions that lead to the construction
of the correct tree.
Training the maximum entropy classifier with
such a large number (1.9 million) of training in-
stances and features required more memory than
was available (the maximum training set size we
were able to train with 2GB of RAM was about
200,000 instances), so we employed the training
set splitting idea used by Yamada and Matsumoto
(2003) and Sagae and Lavie (2005). In our case,
we split the training data according to the part-
of-speech (POS) tag of the head-word of the item
on top of the stack, and trained each split of the
training data separately. At run-time, every trained
classifier is loaded, and the choice of classifier
to use is made by looking at the head-word of
the item on top of the stack in the current parser
state. The total training time (a single machine
was used and each classifier was trained in se-
ries) was slightly under nine hours. For compar-
ison, Sagae and Lavie (2005) report that train-
ing support vector machines for one-against-all
multi-class classification on the same set of fea-
tures for their deterministic parser took 62 hours,
and training a k-nearest neighbors classifier took
11 minutes.
When given perfectly tagged text (gold part-of-
speech tags extracted from the Penn Treebank),
our parser has labeled constituent precision and re-
call of 89.40% and 88.79% respectively over all
sentences in the test set, and 90.01% and 89.32%
over sentences with length of at most 40 words.
These results are at the same level of accuracy as
those obtained with other state-of-the-art statisti-
cal parsers, although still well below the best pub-
lished results for this test set (Bod, 2003; Char-
niak and Johnson, 2005). Although the parser is
quite accurate, parsing the test set took 41 minutes.
By implementing a very simple pruning strategy,
the parser can be made much faster. Pruning the
search space is done by only adding a new parser
state to the heap if its probability is greater than
1/b of the probability of the most likely state in
the heap that has had the same number of parser
actions. By setting b to 50, the parser?s accuracy
is only affected minimally, and we obtain 89.3%
precision and 88.7% recall, while parsing the test
set in slightly under 17 minutes and taking less
than 60 megabytes of RAM. Under the same con-
ditions, but using automatically assigned part-of-
speech tags (at 97.1% accuracy) using the SVM-
Tool tagger (Gimenez and Marquez, 2004), we
obtain 88.1% precision and 87.8% recall. It is
likely that the deterioration in accuracy is aggra-
vated by the training set splitting scheme based on
POS tags.
A deterministic version of our parser, obtained
by simply taking the most likely parser action as
the only action at each step (in other words, by set-
ting b to 1), has precision and recall of 85.4% and
84.8%, respectively (86.5% and 86.0% using gold-
standard POS tags). More interestingly, it parses
all 2,416 sentences (more than 50,000 words) in
only 46 seconds, 10 times faster than the deter-
ministic SVM parser of Sagae and Lavie (2005).
The parser of Tsuruoka and Tsujii (Tsuruoka and
Tsujii, 2005) has comparable speed, but we obtain
more accurate results. In addition to being fast,
our deterministic parser is also lean, requiring only
about 25 megabytes of RAM.
A summary of these results is shown in table 1,
along with the results obtained with other parsers
for comparison purposes. The figures shown in
table 1 only include experiments using automat-
ically assigned POS tags. Results obtained with
gold-standard POS tags are not shown, since they
serve little purpose in a comparison with existing
parsers. Although the time figures reflect the per-
formance of each parser at the stated level of ac-
curacy, all of the search-based parsers can trade
accuracy for increased speed. For example, the
Charniak parser can be made twice as fast at the
cost of a 0.5% decrease in precision/recall, or ten
times as fast at the cost of a 4% decrease in preci-
sion/recall (Roark and Charniak, 2002).
4.1 Reranking with the Probabililstic
Shift-Reduce Model
One interesting aspect of having an accurate pars-
ing model that is significantly different from other
well-known generative models is that the com-
bination of two accurate parsers may produce
even more accurate results. A probabilistic shift-
reduce LR-like model, such as the one used in
our parser, is different in many ways from a lex-
icalized PCFG-like model (using markov a gram-
mar), such as those used in the Collins (1999)
and Charniak (2000) parsers. In the probabilis-
tic LR model, probabilities are assigned to tree
696
Precision Recall F-score Time (min)
Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17
Deterministic (MaxEnt) (this paper) 85.4 84.8 85.1 < 1
Charniak & Johnson (2005) 91.3 90.6 91.0 Unk
Bod (2003) 90.8 90.7 90.7 145*
Charniak (2000) 89.5 89.6 89.5 23
Collins (1999) 88.3 88.1 88.2 39
Ratnaparkhi (1997) 87.5 86.3 86.9 Unk
Tsuruoka & Tsujii (2005): deterministic 86.5 81.2 83.8 < 1*
Tsuruoka & Tsujii (2005): search 86.8 85.0 85.9 2*
Sagae & Lavie (2005) 86.0 86.1 86.0 11*
Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse
the test set. We first show results for the parsers described here, then for four of the most accurate or
most widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally for three recent
classifier-based parsers. For the purposes of direct comparisons, only results obtained with automatically
assigned part-of-speech tags are shown (tags are assigned by the parser itself or by a separate part-of-
speech tagger). * Times reported by authors running on different hardware.
derivations (not the constituents themselves) based
on the sequence of parser shift/reduce actions.
PCFG-like models, on the other hand, assign prob-
abilities to the trees directly. With models that dif-
fer in such fundamental ways, it is possible that
the probabilities assigned to different trees are in-
dependent enough that even a very simple combi-
nation of the two models may result in increased
accuracy.
We tested this hypothesis by using the Char-
niak (2000) parser in n-best mode, producing the
top 10 trees with corresponding probabilities. We
then rescored the trees produced by the Charniak
parser using our probabilistic LR model, and sim-
ply multiplied the probabilities assigned by the
Charniak model and our LR model to get a com-
bined score for each tree2. On development data
this resulted in a 1.3% absolute improvement in f-
score over the 1-best trees produced by the Char-
niak parser. On the test set (WSJ Penn Treebank
section 23), this reranking scheme produces preci-
sion of 90.9% and recall of 90.7%, for an f-score
of 90.8%.
2The trees produced by the Charniak parser may include
the part-of-speech tags AUX and AUXG, which are not part
of the original Penn Treebank tagset. See (Charniak, 2000)
for details. These are converted deterministically into the ap-
propriate Penn Treebank verb tags, possibly introducing a
small number of minor POS tagging errors. Gold-standard
tags or the output of a separate part-of-speech tagger are not
used at any point in rescoring the trees.
5 Conclusion
We have presented a best-first classifier-based
parser that achieves high levels of precision and
recall, with fast parsing times and low memory re-
quirements. One way to view the parser is as an
extension of recent work on classifier-based deter-
ministic parsing. It retains the modularity between
parsing algorithms and learning mechanisms asso-
ciated with deterministic parsers, making it simple
to understand, implement, and experiment with.
Another way to view the parser is as a variant of
probabilistic GLR parsers without an explicit LR
table.
We have shown that our best-first strategy re-
sults in significant improvements in accuracy over
deterministic parsing. Although the best-first
search makes parsing slower, we have imple-
mented a beam strategy that prunes much of the
search space with very little cost in accuracy. This
strategy involves a parameter that can be used to
control the trade-off between accuracy and speed.
At one extreme, the parser is very fast (more than
1,000 words per second) and still moderately ac-
curate (about 85% f-score, or 86% using gold-
standard POS tags). This makes it possible to
apply parsing to natural language tasks involv-
ing very large amounts of text (such as question-
answering or information extraction with large
corpora). A less aggressive pruning setting results
in an f-score of about 88% (or 89%, using gold-
standard POS tags), taking 17 minutes to parse the
WSJ test set.
697
Finally, we have shown that by multiplying the
probabilities assigned by our maximum entropy
shift-reduce model to the probabilities of the 10-
best trees produced for each sentence by the Char-
niak parser, we can rescore the trees to obtain
more accurate results than those produced by ei-
ther model in isolation. This simple combination
of the two models produces an f-score of 90.8%
for the standard WSJ test set.
Acknowledgements
We thank John Carroll for insightful discussions at
various stages of this work, and the reviewers for
their detailed comments. This work was supported
in part by the National Science Foundation under
grant IIS-0414630.
References
A. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of HLT2002. San Diego, CA.
R. Bod. 2003. An efficient implementation of a new
dop model. In Proceedings of the European chapter
of the 2003 meeting of the Association for Computa-
tional Linguistics. Budapest, Hungary.
E. Briscoe and J. Carroll. 1993. Generalised proba-
bilistic lr parsing of natural language (corpora) with
unification-based grammars. Computational Lin-
guistics, 19(1):25?59.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd meeting of
the Association for Computational Linguistics. Ann
Arbor, MI.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora. Montreal, Canada.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 132?139.
Seattle, WA.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics, pages 16?23.
M. Collins. 1999. Head-Driven Models for Natural
Language Parsing. Phd thesis, University of Penn-
sylvania.
J. Gimenez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector
machines. In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Lisbon, Portugal.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002). Vancouver,
BC.
D. E. Knuth. 1965. On the translation of lan-
guages from left to right. Information and Control,
8(6):607?639.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewics.
1993. Building a large annotated corpus of english:
The penn treebank. Computational Linguistics, 19.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics, pages 64?70. Geneva, Switzer-
land.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of the Second Conference on Empir-
ical Methods in Natural Language Processing. Prov-
idence, RI.
B. Roark and E. Charniak. 2002. Measuring effi-
ciency in high-accuracy, broad coverage statistical
parsing. In Proceedings of the Efficiency in Large-
scale Parsing Systems Workshop at COLING-2000.
Luxembourg.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies. Vancouver, BC.
Masaru Tomita. 1987. An efficient augmented
context-free parsing algorithm. Computational Lin-
guistics, 13:31?46.
Masaru Tomita. 1990. The generalized lr
parser/compiler - version 8.4. In Proceedings of
the International Conference on Computational Lin-
guistics (COLING?90), pages 59?63. Helsinki, Fin-
land.
Y. Tsuruoka and K. Tsujii. 2005. Chunk parsing
revisited. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technologies. Vancou-
ver, Canada.
H. Yamada and Yuji Matsumoto. 2003. Statistical de-
pendency analysis using support vector machines.
In Proceedings of the Eighth International Work-
shop on Parsing Technologies. Nancy, France.
698
Evaluation of a Practical Interlingua 
for Task-Oriented Dialogue 
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi, 
Dorcas Wallace, Taro Watanabe, Monika Woszczyna 
Language Technologies Inst i tute,  Carnegie Mellon Univers i ty  and 
IRST  ITC,  Trento, I taly 
Internet:  l s l?cs ,  cmu. edu 
Abstract 
IF (Interchange Format), the interlingua used by 
the C-STAR consortium, is a speech-act based in- 
terlingua for task-oriented ialogue. IF was de- 
signed as a practical interlingua that could strike 
a balance between expressivity and simplicity. If 
it is too simple, components of meaning will be 
lost and coverage of unseen data will be low. On 
the other hand, if it is too complex, it cannot be 
used with a high degree of consistency by collab- 
orators on different continents. In this paper, we 
suggest methods for evaluating the coverage of IF 
and the consistency with which it was used in the 
C-STAR consortium. 
Introduction 
IF (Interchange Format) is an interlingua used by 
the C-STAR consortium 1 for task-oriented ia- 
logues. Because it is used in five different coun- 
tries for six different languages, it had to achieve 
a careful balance between being expressive hough 
and being simple enough to be used consistently. 
If it was not expressive nough, components of 
meaning would be lost and coverage of unseen data 
would be low. On the other hand, if was not sim- 
ple enough, different system developers would use 
it inconsistently and the wrong meanings would be 
translated. IF is described in our previous papers 
(\[PT98, LGLW98, LLW+\]). 
For this paper, we have proposed methods for 
evaluating the coverage of IF and the degree to 
which it can be used consistently across C-STAR 
sites. Coverage was measured by having human IF 
specialists annotate unseen data. Consistency was 
measured by two means. The first was inter-coder 
agreement among IF specialists at Carnegie Mel- 
lonUniversity and ITC-irst (Centre per la ricerca 
lhttp://www.c-star.org 
18 
scientifica e tecnologica). The second, less direct 
method, was a cross-site nd-to-end evaluation of 
English-to-Italian translation where the English- 
to-IF analysis grammars were written at CMU and 
IF-to-Italian generation was developed at IRST. If 
the English and Italian grammar writers did not 
agree on the meaning of the IF, wrong transla- 
tions will be produced. In this way, the cross-site 
evaluation can be an indirect indicator of whether 
the CMU and IRST IF specialists agreed on the 
meaning of IF representations. For comparison, 
we also present within-site nd-to-end evaluations 
of English-to-German, English-to-Japanese, and 
English-to-IF-to-English, where all of the analysis 
and generation grammars were written at CMU. 
The  In terchange Format  
Because we are working with task-oriented dia- 
logues, adequate rendering of the speech act in the 
target language often overshadows the need for lit- 
eral translation of the words. IF is therefore based 
on domain actions (DAs), which consist of on 
speech acts plus domain-specific concepts. An ex- 
ample of a DA is give-information+price+room 
(giving information about the price of a room). 
DAs are composed from 45 general speech acts 
(e.g., acknowledge, give- information, accept) 
and about 96 domain-specific oncepts (e.g, 
pr ice,  temporal, room, f l ight ,  ava i lab i l i ty ) .  
In addition to the DA, IF representations can con- 
tain arguments such as room-type, dest inat ion,  
and price. There are about 119 argument types. 
In the following example, the DA consists 
of a speaker tag (a: for agent), the speech- 
act give- information,  and two main concepts, 
+price and +room. The DA is followed by a list 
of arguments: room-type= and price=. The ar- 
guments have values that represent-information 
for the type of room double and the cost repre- 
Percent 
Cumulatlve Percent Count 
Coverage 
15.7 15,7 652 
19.8 4.1 172 
28.3 3.4 143 
26.0 2.7 113 
28.0 2.0 85 
30.1 2.0 85 
31,9 1.9 78 
33.7 1.8 75 
35.5 1.8 73 
37.2 1.7 70 
38.8 1.6 66 
40.3 l .S 64 
41.7 1,4 60 
43.2 1.4 60 
44.5 1.3 56 
45.8 1.3 52 
46.9 1.2 48 
48.0 1.1 46 
49.1 1.1 44 
50.1 1.0 42 
NA* ;:; 244 
DA 
acknowledge 
aff i rm 
thank 
introduce-self 
give-lnformation+prtce 
greeting 
give-lnfor marion+tern poral 
give-lnformatlon+numeral 
give-in formation+ pr ice+room 
request-in for matio n+ payment 
give-information + payment 
g ive- inform+features+room 
give-in form -t- availabil ity + room 
accept 
give-information+personal-data 
req-act +reserv+ feat ures+room 
req- verif-give-inforra +numera l  
offer+help 
apologize 
request-inform+personal-data 
no-tag 
Figure 1: Coverage of Top20 DAs and No-tag in 
development data 
sented with the complex argument price= which 
has its own arguments quantity=, currency= and 
per-unit=. This IF representation is neutral be- 
tween sentences that have different verbs, sub- 
jects, and objects uch as A double room costs 150 
dollars a night, The price of  a double room is 150 
dollars a night, and A double room is 150 dollars 
a night. ~ 
AGENT: ''a double room costs $150 a night.'' 
a:give-information+price+room 
( room-type=doub le ,  
price=(quantity=lSO, 
currency=dollar, 
per-unit=night) 
Coverage and D is t r ibut ion  of  
Dia logue  Acts  
In this section, we address the coverage of IF for 
task-oriented dialogues about ravel planning. We 
want to know whether a very simple interlingua 
like IF can have good coverage. We are using a 
rather subjective measure of coverage: IF experts 
hand-tagged unseen data with IF representations 
and counted the percentage ofutterances towhich 
no IF could be assigned. (When they tagged the 
unseen data, they were not told that the IF was 
being tested for coverage. The tagging was done 
for system development purposes.) Our end-to- 
end evaluation described in the following sections 
can be taken as a less subjective measure of cov- 
2When we add anaphora resolution, we will need 
to know whether a verb (cost) or a noun (price) was 
used. This will be an issue our new project, NESPOLEI 
(http://nespole. itc. it/). 
Percent 
Cumulative Percent Count Speech Act 
Coverage 
30.1 80.1 1250 glve-lnformation 
45,8 15.7 655 acknowledge 
57,7 11.9 498 request- lnformation 
62,7 5,0 209 request-verif ication-give-inform 
87.6 4.9 203 request-actlon 
71.7 4.1 172 affirm 
75,1 3.4 143 thank 
77,9 2.7 113 introduce-self 
80.2 2.4 98 offer 
82,4 2.1 89 accept 
84.4 2.0 85 greeting 
85.7 1.3 55 suggest 
66.8 I . I  44 apologize 
87.8 1.0 41 closing 
88.5 0.8 32 negate.give-information 
89.2 0.6 27 delay-action 
89,8 0.6 25 introduce-topic 
90,2 0.5 19 please-wait 
90.6 0.4 15 reject 
91.0 0.4 15 request-suggestlon 
Figure 2: 
data 
Coverage of speech-acts in development 
erage. However, the score of an end-to-end evalu- 
ation encompasses grammar coverage problems as 
well as IF coverage problems. 
The development portion of the coverage x- 
periment proceeded as follows. Over a period of 
two years, a database of travel planning dialogues 
was collected by C-STAR partners in the U.S., 
Italy, and Korea. The dialogues were role-playing 
dialogues between a person pretending to be a 
traveller and a person pretending to be a travel 
agent. For the English and Italian dialogues, the 
traveller and agent were talking face-to-face in the 
same language - -  both speaking English or both 
speaking Italian. The Korean dialogues were also 
role playing dialogues, but one participant was 
speaking Korean and the other was speaking En- 
glish. From these dialogues, only the Korean ut- 
terances are included in the database. Each utter- 
ance in the database is annotated with an English 
translation and an IF representation. Table 1 sum- 
marizes the amount of data in each language. The 
English, Italian, and Korean data was used for IF 
development. 
The development database contains over 4000 
dialogue act units, which are covered by a total of 
about 542 distinct DAs (346 agent DAs and 278 
client DAs). Figures 1 and 2 show the cumulative 
coverage of the top twenty DA's and speech acts 
in the development data. Figure 1 also shows the 
percentage ofno-tag utterances (the ones we de- 
cided not to cover) in the development data. The 
first column shows the percent of the development 
data that is covered cumulatively by the DA's or 
speech acts from the top of the table to the cur- 
rent line. For example, acknowledg e and aff irm 
together account for 19.8 percent of the data. The 
19 
Language(s) Type  of Dialogue Number  of DA Units  
D'evelopment Data: 
English 
Italian 
Korean-English 
Test Data: 
Japanese-English 
monolingual 
monolingual 
biiingual (only 'Korean 
utterances are included) 
bilingual (Japanese and 
English utterances are 
included) 
Table 1: The IF Database 
2698 
1142 
6069 
Percent 
' Cumulat ive Percent Count DA 
Cover~,--= - " 4.6 263 no-tag 
15.6 15.6 ? - 885 acknowledge 
20.2 4,6 260 thank 
23.7 3.5 200 introduce-self 
27.0 3.4 191 affirm 
29.7 2.7 153 apologize 
32.3 2.6 147 greeting 
34.6 2.3 128 closing 
36.3 1.7 98 give- information+personal-data 
38.0 1.7 95 glve-inform ation +t  em poraI 
39.5 1.6 89 give-in formation +price 
41.1 1.5 88 please-wait 
42.5 1.4 82 give-inform+telephone-number 
43.8 1.3 75 g ive- informat ion+features+room 
45.0 I . I  65 request- inform+personal-data 
46.0 1.0 59 give-in for m ?temp oral-.{- arrival 
47.0 1.0 55 accept 
48.0 l.O 55 give-infor m +avai labi l i ty + room 
48.9 1.0 55 give-information+price-broom 
49.8 0.9 50 verify 
50.7 0.9 49 request-in form +tempora l+arr iva l  
Figure 3: Coverage of Top 20 DAs and No-tag in 
test data 
Percent 
Cumulat ive 
Coverage 
25.6 
Percent Count DA 
25.6 1454 give-information 
41.7 16.1 916 acknowledge 
53.6 11.9 677 request- information 
58.2 4.6 260 thank 
62,0 3.7 213 request-verification-give-inform 
65.5 3.5 200 introduce-self 
68.8 3.4 191 a f f i rm 
72.0 3.2 181 request -act ion  
74.8 2.8 159 accept 
77.5 2.7 153 apologize 
80.1 2.6 147 greet ing 
82.4 2.3 130 closing 
84.4 2.1 117 suggest 
86.3 1.8 104 verlfy-give-information 
87.9 1.7 94 offer 
89.5 1.5 88 please-wait 
90.6 I . I  65 negate-glve-lnformation 
91.5 0.9 50 verify 
92.0 0.5 30 negate 
92.5 0.5 . 26 request-aff irmatlon 
Figure 4: Coverage of Top 20 SAs in test data 
second column shows the percent of the develop- 
ment data covered by each DA or speech act. The 
third column shows the number of times each DA 
or speech act occurs in the development data. 
The evaluation portion of the coverage x- 
periment was carried out on 124 dialogues (6069 
dialogue act units) that were collected at ATR, 
Japan. One participant in each dialogue was 
speaking Japanese and the other was speaking En- 
glish. Both Japanese and English utterances are 
included in the data. The 124 Japanese-English 
dialogues were not examined closely by system de- 
velopers during IF development. After the IF de- 
sign was finalized and frozen in Summer 1999, the 
Japanese-English data was tagged with IFs. No 
further IF development took place at this point 
except hat values for arguments were added. For 
example, Miyako could be added as a hotel name, 
but no new speech acts, concepts, or argument 
types could be added. Sentences were tagged as 
no-tag if the IF did not cover them. 
Figures 3 and 4 show the cumulative cover- 
age of the top twenty DAs and speech acts in the 
Japanese-English data, including the percent of 
no-tag sentences. 
Notice that the percentage of no-tag was 
lower in our test data than in our development 
data. This is because the role playing instructions 
for the test data were more restrictive than the 
role playing instructions for the development data. 
Figures 1 and 3 show that slightly more of the test 
data is covered by slightly fewer DAs. 
Cross-Site Reliability of IF 
Representations 
In this section we attempt o measure how reliably 
IF is used by researchers at different sites. Recall 
that one of the design criteria of IF was consis- 
tency of use by researchers who are separated by 
oceans. This criterion limits the complexity of IF. 
Two measures of consistency are used - inter-coder 
agreement and a cross-site nd-to-end evaluation. 
Inter -Coder  Agreement:  Inter-coder agree- 
ment is a direct measure of consistency among 
20 
Percent Agreement 
Speech-act 82.14 
Dialog-act 65.48 
Concept lists 88.00 
Argument lists I 85.79 
Table 2: Inter-coder Agreement between CMU 
and IRST 
C-STAR partners. We used 84 DA units from 
the Japanese-English data described above. The 
84 DA units consisted of some coherent dialogue 
fragments and and some isolated sentences. The 
data was coded at CMU and at IRST. We counted 
agreement on ~he components ofthe IF separately. 
Table 2 shows agreement on speech acts, dialogue 
acts (speech act plus concepts), concepts, and ar- 
guments. The results are reported in Table 2 in 
terms of percent agreement. Further work might 
include some other calculation of agreement such 
as Kappa or precision and recall of the coders 
against each other. Figure 5 shows a fragment of 
a dialogue coded by CMU and IRST. The coders 
disagreed on the IF middle sentence, I'd like a twin 
room please. One coded it as an acceptance of a 
twin room, the other coded it as a preference for 
a twin room. 
Cross-Site Evaluation: As an approximate and 
indirect measure of consistency, we have compared 
intra-site end-to-end evaluation with cross-site 
end-to-end evaluation. An end-to-end evaluation 
includes an analyzer, which maps the source lan- 
guage input into IF and a generator, which maps 
IF into target language sentences. The intra-site 
evaluation was carried out on English-German, 
English-Japanese, and English-IF-English trans- 
lation. The English analyzer and the German, 
Japanese, and English generators were all writ- 
ten at CMU by IF experts who worked closely 
with each other. The cross-site valuation was car- 
ried out on English-Italian translation, involving 
an English analyzer written at CMU and an Ital- 
ian generator written at IRST. The IF experts at 
CMU and IRST were in occasional contact with 
each other by email, and met in person two or 
three times between 1997 and 1999. 
A number of factors contribute to the success 
of an inter-site valuation, just one of which is that 
the sites used IF consistently with each other. An- 
other factor is that the two sites used similar de- 
velopment data and have approximately the same 
coverage. If the inter-site valuation results are 
about as good as the intra-site results, we can con- 
clude that all factors are handled acceptably, in- 
cluding consistency of IF usage. If the inter-site 
results are worse than the intra-site results, con- 
sistency of IF use or some other factor may be 
to blame. Before conducting this evaluation, we 
already knew that there was some degree of cross- 
site consistency in IF usage because we conducted 
successful inter-continental demos with speech 
translation and video conferencing in Summer 
1999. (The demos and some of the press coverage 
are reported on the C-STAR web site.) The de- 
mos included ialogues in English-Italian, English- 
German, English-Japanese, English-Korean, and 
English-French. At a later date, an Italian-Korean 
demo was produced with no additional work, thus 
illustrating the well-cited advantage of an inter- 
lingual approach in a multi-lingual situation. The 
end-to-end evaluation reported here goes beyond 
the demo situation to include data that was un- 
seen by system developers. 
Evaluation Data: The Summer 1999 intra-site 
evaluation was conducted on about 130 utterances 
from a CMU user study. The traveller was played 
by a second time user - -  someone who had partici- 
pated in one previous user study, but had no other 
experience with our MT system. The travel agent 
was played by a system developer. Both people 
were speaking English, but they were in different 
rooms, and their utterances were paraphrased us- 
ing IF. The end-to-end procedure was that (1) an 
English utterance was spoken and decoded by the 
JANUS speech recognizer, (2) the output of the rec- 
ognizer was parsed into an IF representation, and 
(3) a different English utterance (supposedly with 
the same meaning) was generated from the IF rep- 
resentation. The speakers had no other means of 
communication with each other. 
In order to evaluate English-German and 
English-Japanese translation, the IFs of the 130 
test sentences were fed into German and Japanese 
generation components atCMU. The data used in 
the evaluation was unseen by system developers 
at the time of the evaluation. For English-Italian 
translation, the IF representations produced by 
the English analysis component were sent to IRST 
to be generated in Italian. 
Evaluation Scoring: In order to score the eval- 
uation, input and output sentences were compared 
by bilingual people, or monolingual people in the 
case of English-IF-English evaluation. A score of 
ok is assigned if the target language utterance is
comprehensible and no components ofmeaning are 
deleted, added, or" changed by the translation. A
21 
We have singles, and t,ins and also Japanese rooms available on the eleventh. 
CMU a:give-information+availability+room 
(room-type=(single ~ twin ~ japanese_style), time=mdll) 
IRST a:give-in2ormation+availability+room 
(room-type=(single ~ twin & japanese_style), time=mdll) 
I'd like a twin room, please. 
CMU c:accept+features+room (room-typeffitwin) 
IBST c:give-information+preference+features+room (room-type=twin) 
A twin room is fourteen thousand yen. 
CMU a:give-information+price+room 
(room-type=twin, price=(currency=yen, quantity=f4000)) 
IRST a:give-in.formation+price+room 
(room-type=twin, price=(currency=yen, quantity=f4000)) 
Figure 5: Examples of IF coding from CMU and IRST 
.o  
Method 
1 Recosnition only 
2 Transcription 
3 Recosnition 
4 Transcription 
5 Recognition 
6 Transcription 
7 Recognition 
8 Transcription 
9 Recognition 
10 Transcription 
11 Recognition 
I OutPut Language II OK+Perfect Perfect Grader I No. of Graders 
En$1ish 
English. 
En$1ish 
Japanese 
Japanese 
German 
German 
German 
German 
Italian 
Italian 
78 % 62 % CMU 3 
74 % 54 % CMU 3 
59 ~ 42 % CMU 3 
777 % 59 % CMU 2 
62 % 4,5 % CMU 2 
70 %- ..... s9 % CMU " 
58 % 34 % CMU 2 
67 ~ 43 % IRST 2 
59 % 36 % IRST 2 
73 % 51% IRST .... . .  6 
61% 42 % IRST 6 
Figure 6: Translation Grades for English to English, Japanese, German, and Italian 
score of perfect is assigned if, in addition to the 
previous criteria, the translation is fluent in the 
target language. A score of bad is assigned if the 
target language sentence is incomprehensible or 
some element of meaning has been added, deleted, 
or changed. The evaluation procedure isdescribed 
in detail in \[GLL+96\]. In Figure 6, acceptable is
the sum of per fec t  and ok scores, s 
Figure 6 shows the results of the intra-site 
and inter-site evaluations. The first row grades 
the speech recognition output against a human- 
produced transcript of what was said. This gives 
us a ceiling for how well we could do if trans- 
lation were perfect, given speech recognition er- 
rors. Rows 2 through 7 show the results of the 
intra-site evaluation. All analyzers and genera- 
tors were written at CMU, and the results were 
graded by CMU researchers. (The German re- 
sults are a lower than the English and Japanese 
results because a shorter time was spent on gram- 
mar development.) Rows 8 and 9 report on CMU's 
intra~site valuation of English-German transla~ 
Sin another paper (\[LBL+00\]), we describe a task- 
based evaluation which focuses on success of commu- 
nicative goals and how long it takes to achieve them. 
tion (the same system as in Rows 6 and 7), but 
the results were graded by researchers at IRST. 
Comparing Rows 6 and 7 with Rows 8 and 9, we 
can check that CMU and IRST graders were us- 
ing roughly the same grading criteria: a difference 
of up to ten percent among graders is normal in 
our experience. Rows 10 and 11 show the results 
of the inter-site English-Italian evaluation. The 
CMU English analyzer produced IF representa- 
tions which were sent to IRST and were fed into 
IRST's Italian generator. The results were graded 
by IRST researchers. 
Conclusions drawn from the inter-site valuation: 
Since the inter-site evaluation results are compa- 
rable to the intra-site results, we conclude that re- 
searchers at IRST and CMU are using IF at least 
as consistently as researchers within CMU. 
Future Plans 
In the next phase of C-STAR, we will cover de- 
scriptive sentences (e.g., The castle was built in 
the thirteenth century and someone was impris- 
oned in the tower) as well as task-oriented sen- 
tences. Descriptive sentences will be represented 
22 
in a more traditional frame-based interlingua fo- 
cusing on lexical meaning and grammatical fea- 
tures of the sentences. We are working on disam- 
biguating literal from task-oriented meanings in 
context. For example That's great could be an ac- 
ceptance (like I'll take it) (task oriented) or could 
just express appreciation. Sentences may also con- 
tain a combination of task oriented (e.g., Can you 
tell me) and descriptive (how long the castle has 
been standing) components. 
\[GLL+96\] 
\[LBL+O0\] 
\[LGLW98\] 
Re ferences  
Donna Gates, A. Lavie, L. Levin, 
A. Waibel, M. Gavald~, L. Mayfield, 
M:-Woszczyna, and P. Zhan. End-to- 
End Evaluation in JANUS: A Speech- 
to-Speech Translation System. In Pro- 
ceedings of ECAI-96, Budapest, Hun- 
gary, 1996. 
Lori Levin, Boris Bartlog, Ari- 
adna Font Llitjos, Donna Gates, Alon 
Lavie, Dorcas Wallace, Taro Watan- 
abe, and Monika Woszczyna. Lessons 
Learned from a Task-Based Evaluation 
of Speech-to-Speech MT. In Proceed- 
ings of LREC 2000, Athens, Greece, 
June to appear, 2000. 
Lori Levin, D. Gates, A. Lavie, and 
A. Waibel. An Interlingua Based on 
Domain Actions for Machine Transla- 
tion of Task-Oriented Dialogues. In 
Proceedings of the International Con- 
ference on Spoken Language Process- 
ing (ICSLP'98), Sydney, Australia, 
1998. 
\[LLW +\] 
\[PT98\] 
Lori Levin, A. Lavie, M. Woszczyna, 
D. Gates, M. Gavald~, D. Koll, and 
A. Waibel. The Janus-III Translation 
System. Machine Translation. To ap- 
pear. 
Fabio Pianesi and Lucia Tovena. Us- 
ing the Interchange Format for Encod- 
ing Spoken Dialogue. In Proceedings of
SIG-IL Workshop, 1998. 
23 
Spoken Language Parsing Using Phrase-Level Grammars and Trainable 
Classifiers 
Chad Langley, Alon Lavie, Lori Levin, Dorcas Wallace, Donna Gates, and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, USA 
{clangley|alavie|lsl|dorcas|dmg|kay}@cs.cmu.edu 
 
Abstract 
In this paper, we describe a novel 
approach to spoken language analysis 
for translation, which uses a combination 
of grammar-based phrase-level parsing 
and automatic classification. The job of 
the analyzer is to produce a shallow 
semantic interlingua representation for 
spoken task-oriented utterances. The 
goal of our hybrid approach is to provide 
accurate real-time analyses while 
improving robustness and portability to 
new domains and languages. 
1 Introduction 
Interlingua-based approaches to Machine 
Translation (MT) are highly attractive in systems 
that support a large number of languages. For each 
source language, an analyzer that converts the 
source language into the interlingua is required. 
For each target language, a generator that converts 
the interlingua into the target language is needed. 
Given analyzers and generators for all supported 
languages, the system simply connects the source 
language analyzer with the target language 
generator to perform translation. 
Robust and accurate analysis is critical in 
interlingua-based translation systems. In speech-to-
speech translation systems, the analyzer must be 
robust to speech recognition errors, spontaneous 
speech, and ungrammatical inputs as described by 
Lavie (1996). Furthermore, the analyzer should run 
in (near) real time. 
In addition to accuracy, speed, and robustness, 
the portability of the analyzer with respect to new 
domains and new languages is an important 
consideration. Despite continuing improvements in 
speech recognition and translation technologies, 
restricted domains of coverage are still necessary 
in order to achieve reasonably accurate machine 
translation. Porting translation systems to new 
domains or even expanding the coverage in an 
existing domain can be very difficult and time-
consuming.  This creates significant challenges in 
situations where translation is needed for a new 
domain within relatively short notice. Likewise, 
demand can be high for translation systems that 
can be rapidly expanded to include new languages 
that were not previously considered important. 
Thus, it is important that the analysis approach 
used in a translation system be portable to new 
domains and languages. 
One approach to analysis in restricted domains 
is to use semantic grammars, which focus on 
parsing semantic concepts rather than syntactic 
structure. Semantic grammars can be especially 
useful for parsing spoken language because they 
are less susceptible to syntactic deviations caused 
by spontaneous speech effects. However, the focus 
on meaning rather than syntactic structure 
generally makes porting to a new domain quite 
difficult. Since semantic grammars do not exploit 
syntactic similarities across domains, completely 
new grammars must usually be developed. 
While grammar-based parsing can provide very 
accurate analyses on development data, it is 
difficult for a grammar to completely cover a 
domain, a problem that is exacerbated by spoken 
input. Furthermore, it generally takes a great deal 
of effort by human experts to develop a high-
coverage grammar. On the other hand, machine 
learning approaches can generalize beyond training 
data and tend to degrade gracefully in the face of 
noisy input. Machine learning methods may, 
however, be less accurate on clearly in-domain 
input than grammars and may require a large 
amount of training data. 
We describe a prototype version of an analyzer 
that combines phrase-level parsing and machine 
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 15-22.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
learning techniques to take advantage of the 
benefits of each. Phrase-level semantic grammars 
and a robust parser are used to extract low-level 
interlingua arguments from an utterance. Then, 
automatic classifiers assign high-level domain 
actions to semantic segments in the utterance. 
2 MT System Overview 
The analyzer we describe is used for English and 
German in several multilingual human-to-human 
speech-to-speech translation systems, including the 
NESPOLE! system (Lavie et al, 2002). The goal 
of NESPOLE! is to provide translation for 
common users within real-world e-commerce 
applications. The system currently provides 
translation in the travel and tourism domain 
between English, French, German and Italian.  
NESPOLE! employs an interlingua-based 
translation approach that uses four basic steps to 
perform translation. First, an automatic speech 
recognizer processes spoken input. The best-
ranked hypothesis from speech recognition is then 
passed through the analyzer to produce interlingua. 
Target language text is then generated from the 
interlingua. Finally, the target language text is 
synthesized into speech. 
This interlingua-based translation approach 
allows for distributed development of the 
components for each language. The components 
for each language are assembled into a translation 
server that accepts speech, text, or interlingua as 
input and produces interlingua, text, and 
synthesized speech. In addition to the analyzer 
described here, the English translation server uses 
the JANUS Recognition Toolkit for speech 
recognition, the GenKit system (Tomita & Nyberg, 
1988) for generation, and the Festival system 
(Black et al, 1999) for synthesis. 
NESPOLE! uses a client-server architecture 
(Lavie et al, 2001) to enable users who are 
browsing the web pages of a service provider (e.g. 
a tourism bureau) to seamlessly connect to a 
human agent who speaks a different language. 
Using commercially available software such as 
Microsoft NetMeeting?, a user is connected to the 
NESPOLE! Mediator, which establishes 
connections with the agent and with translation 
servers for the appropriate languages. During a 
dialogue, the Mediator transmits spoken input from 
the users to the translation servers and synthesized 
translations from the servers to the users. 
3 The Interlingua 
The interlingua used in the NESPOLE! system is 
called Interchange Format (IF) (Levin et al, 1998; 
Levin et al, 2000). The IF defines a shallow 
semantic representation for task-oriented 
utterances that abstracts away from language-
specific syntax and idiosyncrasies while capturing 
the meaning of the input. Each utterance is divided 
into semantic segments called semantic dialog 
units (SDUs), and an IF is assigned to each SDU. 
An IF representation consists of four parts: a 
speaker tag, a speech act, an optional sequence of 
concepts, and an optional set of arguments. The 
representation takes the following form: 
 
speaker : speech act +concept* (argument*) 
 
The speaker tag indicates the role of the speaker 
in the dialogue. The speech act captures the 
speaker?s intention. The concept sequence, which 
may contain zero or more concepts, captures the 
focus of an SDU. The speech act and concept 
sequence are collectively referred to as the domain 
action (DA). The arguments use a feature-value 
representation to encode specific information from 
the utterance. Argument values can be atomic or 
complex. The IF specification defines all of the 
components and describes how they can be legally 
combined. Several examples of utterances with 
corresponding IFs are shown below. 
 
Thank you very much. 
a:thank 
Hello. 
c:greeting (greeting=hello) 
How far in advance do I need to book a room for the Al-
Cervo Hotel? 
c:request-suggestion+reservation+room ( 
   suggest-strength=strong, 
   time=(time-relation=before, 
     time-distance=question), 
   who=i, 
   room-spec=(room, identifiability=no, 
     location=(object-name=cervo_hotel))) 
4 The Hybrid Analysis Approach 
Our hybrid analysis approach uses a combination 
of grammar-based parsing and machine learning 
techniques to transform spoken utterances into the 
IF representation described above. The speaker tag 
is assumed to be given. Thus, the goal of the 
analyzer is to identify the DA and arguments.  
The hybrid analyzer operates in three stages. 
First, semantic grammars are used to parse an 
utterance into a sequence of arguments. Next, the 
utterance is segmented into SDUs. Finally, the DA 
is identified using automatic classifiers. 
4.1 Argument Parsing 
The first stage in analysis is parsing an utterance 
for arguments. During this stage, utterances are 
parsed with phrase-level semantic grammars using 
the robust SOUP parser (Gavald?, 2000). 
4.1.1 The Parser 
The SOUP parser is a stochastic, chart-based, top-
down parser that is designed to provide real-time 
analysis of spoken language using context-free 
semantic grammars. One important feature 
provided by SOUP is word skipping. The amount 
of skipping allowed is configurable and a list of 
unskippable words can be defined. Another feature 
that is critical for phrase-level argument parsing is 
the ability to produce analyses consisting of 
multiple parse trees. SOUP also supports modular 
grammar development (Woszczyna et al, 1998). 
Subgrammars designed for different domains or 
purposes can be developed independently and 
applied in parallel during parsing. Parse tree nodes 
are then marked with a subgrammar label. When 
an input can be parsed in multiple ways, SOUP can 
provide a ranked list of interpretations. 
In the prototype analyzer, word skipping is only 
allowed between parse trees. Only the best-ranked 
argument parse is used for further processing. 
4.1.2 The Grammars 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument 
grammar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains phrase-
level rules for parsing arguments defined in the IF. 
Top-level argument grammar nonterminals 
correspond to top-level arguments in the IF. 
The pseudo-argument grammar contains top-
level nonterminals that do not correspond to 
interlingua concepts. These rules are used for 
parsing common phrases that can be grouped into 
classes to capture more useful information for the 
classifiers. For example, all booked up, full, and 
sold out might be grouped into a class of phrases 
that indicate unavailability. In addition, rules in the 
pseudo-argument grammar can be used for 
contextual anchoring of ambiguous arguments. For 
example, the arguments [who=] and [to-whom=] 
have the same values. To parse these arguments 
properly in a sentence like ?Can you send me the 
brochure??, we use a pseudo-argument grammar 
rule, which refers to the arguments [who=] and [to-
whom=] within the appropriate context.  
The cross-domain grammar contains rules for 
parsing whole DAs that are domain-independent. 
For example, this grammar contains rules for 
greetings (Hello, Good bye, Nice to meet you, etc.). 
Cross-domain grammar rules do not cover all 
possible domain-independent DAs. Instead, the 
rules focus on DAs with simple or no argument 
lists. Domain-independent DAs with complex 
argument lists are left to the classifiers. Cross-
domain rules play an important role in the 
prediction of SDU boundaries. 
Finally, the shared grammar contains common 
grammar rules that can be used by all other 
subgrammars. These include definitions for most 
of the arguments, since many can also appear as 
sub-arguments. RHSs in the argument grammar 
contain mostly references to rules in the shared 
grammar. This method eliminates redundant rules 
in the argument and shared grammars and allows 
for more accurate grammar maintenance. 
4.2 Segmentation 
The second stage of processing in the hybrid 
analysis approach is segmentation of the input into 
SDUs. The IF representation assigns DAs at the 
SDU level. However, since dialogue utterances 
often consist of multiple SDUs, utterances must be 
segmented into SDUs before DAs can be assigned. 
Figure 1 shows an example utterance containing 
four arguments segmented into two SDUs. 
 
SDU1  SDU2  
greeting= disposition= visit-spec= location= 
hello i would like to take a vacation in val di fiemme 
Figure 1. Segmentation of an utterance into SDUs. 
The argument parse may contain trees for cross-
domain DAs, which by definition cover a complete 
SDU. Thus, there must be an SDU boundary on 
both sides of a cross-domain tree. Additionally, no 
SDU boundaries are allowed within parse trees. 
The prototype analyzer drops words skipped 
between parse trees, leaving only a sequence of 
trees. The parse trees on each side of a potential 
boundary are examined, and if either tree was 
constructed by the cross-domain grammar, an SDU 
boundary is inserted. Otherwise, a simple statistical 
model similar to the one described by Lavie et al 
(1997) estimates the likelihood of a boundary. 
The statistical model is based only on the root 
labels of the parse trees immediately preceding and 
following the potential boundary position. Suppose 
the position under consideration looks like 
[A1?A2], where there may be a boundary between 
arguments A1 and A2. The likelihood of an SDU 
boundary is estimated using the following formula: 
 
])C([A  ])C([A
])AC([  ])C([A])AF([A
21
21
21
+
?+?
??  
 
The counts C([A1?]), C([?A2]), C([A1]), C([A2]) 
are computed from the training data. An evaluation 
of this baseline model is presented in section 6.  
4.3 DA Classification 
The third stage of analysis is the identification of 
the DA for each SDU using automatic classifiers. 
After segmentation, a cross-domain parse tree may 
cover an SDU. In this case, analysis is complete 
since the parse tree contains the DA. Otherwise, 
automatic classifiers are used to assign the DA. In 
the prototype analyzer, the DA classification task 
is split into separate subtasks of classifying the 
speech act and concept sequence. This reduces the 
complexity of each subtask and allows for the 
application of specialized techniques to identify 
each component. 
One classifier is used to identify the speech act, 
and a second classifier identifies the concept 
sequence. Both classifiers are implemented using 
TiMBL (Daelemans et al, 2000), a memory-based 
learner. Speech act classification is performed first. 
Input to the speech act classifier is a set of binary 
features that indicate whether each of the possible 
argument and pseudo-argument labels is present in 
the argument parse for the SDU. No other features 
are currently used. Concept sequence classification 
is performed after speech act classification. The 
concept sequence classifier uses the same feature 
set as the speech act classifier with one additional 
feature: the speech act assigned by the speech act 
classifier. We present an evaluation of this baseline 
DA classification scheme in section 6. 
4.4 Using the IF Specification 
The IF specification imposes constraints on how 
elements of the IF representation can legally 
combine. DA classification can be augmented with 
knowledge of constraints from the IF specification, 
providing two advantages over otherwise na?ve 
classification. First, the analyzer must produce 
valid IF representations in order to be useful in a 
translation system. Second, using knowledge from 
the IF specification can improve the quality of the 
IF produced, and thus the translation. 
Two elements of the IF specification are 
especially relevant to DA classification. First, the 
specification defines constraints on the 
composition of DAs. There are constraints on how 
concepts are allowed to pair with speech acts as 
well as ordering constraints on how concepts are 
allowed to combine to form a valid concept 
sequence. These constraints can be used to 
eliminate illegal DAs during classification. The 
second important element of the IF specification is 
the definition of how arguments are licensed by 
speech acts and concepts. In order for an IF to be 
valid, at least one speech act or concept in the DA 
must license each argument. 
The prototype analyzer uses the IF specification 
to aid classification and guarantee that a valid IF 
representation is produced. The speech act and 
concept sequence classifiers each provide a ranked 
list of possible classifications. When the best 
speech act and concept sequence combine to form 
an illegal DA or form a legal DA that does not 
license all of the arguments, the analyzer attempts 
to find the next best legal DA that licenses the 
most arguments. Each of the alternative concept 
sequences (in ranked order) is combined with each 
of the alternative speech acts (in ranked order). For 
each possible legal DA, the analyzer checks if all 
of the arguments found during parsing are licensed. 
If a legal DA is found that licenses all of the 
arguments, then the process stops. If not, one 
additional fallback strategy is used. The analyzer 
then tries to combine the best classified speech act 
with each of the concept sequences that occurred in 
the training data, sorted by their frequency of 
occurrence. Again, the analyzer checks if each 
legal DA licenses all of the arguments and stops if 
such a DA is found. If this step fails to produce a 
legal DA that licenses all of the arguments, the 
best-ranked DA that licenses the most arguments is 
returned. In this case, any arguments that are not 
licensed by the selected DA are removed. This 
approach is used because it is generally better to 
select an alternative DA and retain more arguments 
than to keep the best DA and lose the information 
represented by the arguments. An evaluation of 
this strategy is presented in the section 6. 
5 Grammar Development and 
Classifier Training 
During grammar development, it is generally 
useful to see how changes to the grammar affect 
the IF representations produced by the analyzer. In 
a purely grammar-based analysis approach, full 
interlingua representations are produced as the 
result of parsing, so testing new grammars simply 
requires loading them into the parser. Because the 
grammars used in our hybrid approach parse at the 
argument level, testing grammar modifications at 
the complete IF level requires retraining the 
segmentation model and the DA classifiers. 
 When new grammars are ready for testing, 
utterance-IF pairs for the appropriate language are 
extracted from the training database. Each 
utterance-IF pair in the training data consists of a 
single SDU with a manually annotated IF. Using 
the new grammars, the argument parser is applied 
to each utterance to produce an argument parse. 
The counts used by the segmentation model are 
then recomputed based on the new argument 
parses. Since each utterance contains a single 
SDU, the counts C([?A2]) and C([A1?]) can be 
computed directly from the first and last arguments 
in the parse respectively. 
Next, the training examples for the DA 
classifiers are constructed. Each training example 
for the speech act classifier consists of the speech 
act from the annotated IF and a vector of binary 
features with a positive value set for each argument 
or pseudo-argument label that occurs in the 
argument parse. The training examples for the 
concept sequence classifiers are similar with the 
addition of the annotated speech act to the feature 
vector. After the training examples are constructed, 
new classifiers are trained. 
Two tools are available to support easy testing 
during grammar development. First, the entire 
training process can be run using a single script. 
Retraining for a new grammar simply requires 
running the script with pointers to the new 
grammars. Then, a special development mode of 
the translation servers allows the grammar writers 
to load development grammars and their 
corresponding segmentation model and DA 
classifiers. The translation server supports input in 
the form of individual utterances or files and 
allows the grammar developers to look at the 
results of each stage of the analysis process. 
6 Evaluation 
We present the results from recent experiments to 
measure the performance of the analyzer 
components and of end-to-end translation using the 
analyzer. We also report the results of an ablation 
experiment that used earlier versions of the 
analyzer and IF specification. 
6.1 Translation Experiment 
 
Acceptable Perfect 
SR Hypotheses 66% 56% 
Translation from 
Transcribed Text 58% 43% 
Translation from 
SR Hypotheses 45% 32% 
Table 1. English-to-English end-to-end translation 
 
Acceptable Perfect 
Translation from 
Transcribed Text 55% 38% 
Translation from 
SR Hypotheses 43% 27% 
Table 2. English-to-Italian end-to-end translation 
Tables 1 and 2 show end-to-end translation 
results of the NESPOLE! system. In this 
experiment, the input was a set of English 
utterances. The utterances were paraphrased back 
into English via the interlingua (Table 1) and 
translated into Italian (Table 2). The data used to 
train the DA classifiers consisted of 3350 SDUs 
annotated with IF representations. The test set 
contained 151 utterances consisting of 332 SDUs 
from 4 unseen dialogues. Translations were 
compared to human transcriptions and graded as 
described in (Levin et al, 2000). A grade of 
perfect, ok, or bad was assigned to each 
translation by human graders. A grade of perfect 
or ok is considered acceptable. The table shows the 
average of grades assigned by three graders. 
The row in Table 1 labeled SR Hypotheses 
shows the grades when the speech recognizer 
output is compared directly to human transcripts. 
As these grades show, recognition errors can be a 
major source of unacceptable translations. These 
grades provide a rough bound on the translation 
performance that can be expected when using input 
from the speech recognizer since meaning lost due 
to recognition errors cannot be recovered. The 
rows labeled Translation from Transcribed Text 
show the results when human transcripts are used 
as input. These grades reflect the combined 
performance of the analyzer and generator. The 
rows labeled Translation from SR Hypotheses 
show the results when the speech recognizer 
produces the input utterances. As expected, 
translation performance was worse with the 
introduction of recognition errors. 
 
Precision Recall 
70% 54% 
Table 3. SDU boundary detection performance 
Table 3 shows the performance of the 
segmentation model on the test set. The SDU 
boundary positions assigned automatically were 
compared with manually annotated positions. 
 
 
Classifier Accuracy 
Speech Act 65% 
Concept Sequence 54% 
Domain Action 43% 
Table 4. Classifier accuracy on transcription 
 
Frequency 
Speech Act 33% 
Concept Sequence 40% 
Domain Action 14% 
Table 5. Frequency of most common DA elements 
Table 4 shows the performance of the DA 
classifiers, and Table 5 shows the frequency of the 
most common DA, speech act, and concept 
sequence in the test set. Transcribed utterances 
were used as input and were segmented into SDUs 
before analysis. This experiment is based on only 
293 SDUs. For the remaining SDUs in the test set, 
it was not possible to assign a valid representation 
based on the current IF specification. 
These results demonstrate that it is not always 
necessary to find the canonical DA to produce an 
acceptable translation. This can be seen by 
comparing the Domain Action accuracy from Table 
4 with the Transcribed grades from Table 1. 
Although the DA classifiers produced the 
canonical DA only 43% of the time, 58% of the 
translations were graded as acceptable. 
 
 
Changed 
Speech Act 5% 
Concept Sequence 26% 
Domain Action 29% 
Table 6. DA elements changed by IF specification 
In order to examine the effects of using IF 
specification constraints, we looked at the 182 
SDUs which were not parsed by the cross-domain 
grammar and thus required DA classification. 
Table 6 shows how many DAs, speech acts, and 
concept sequences were changed as a result of 
using the constraints. DAs were changed either 
because the DA was illegal or because the DA did 
not license some of the arguments. Without the IF 
specification, 4% of the SDUs would have been 
assigned an illegal DA, and 29% of the SDUs 
(those with a changed DA) would have been 
assigned an illegal IF. Furthermore, without the IF 
specification, 0.38 arguments per SDU would have 
to be dropped while only 0.07 arguments per SDU 
were dropped when using the fallback strategy. 
The mean number of arguments per SDU was 1.47. 
6.2 Ablation Experiment 
Classification Accuracy (16-fold Cross 
Validation)
0
0.2
0.4
0.6
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
e
a
n
 
A
cc
ur
a
c
y
Speech Act
Concept
Sequence
Domain Action
 
Figure 2: DA classifier accuracy with varying 
amounts of data 
Figure 2 shows the results of an ablation 
experiment that examined the effect of varying the 
training set size on DA classification accuracy. 
Each point represents the average accuracy using a 
16-fold cross validation setup. 
The training data contained 6409 SDU-
interlingua pairs. The data were randomly divided 
into 16 test sets containing 400 examples each. In 
each fold, the remaining data were used to create 
training sets containing 500, 1000, 2000, 3000, 
4000, 5000, and 6009 examples. 
The performance of the classifiers appears to 
begin leveling off around 4000 training examples. 
These results seem promising with regard to the 
portability of the DA classifiers since a data set of 
this size could be constructed in a few weeks. 
7 Related Work 
Lavie et al (1997) developed a method for 
identifying SDU boundaries in a speech-to-speech 
translation system. Identifying SDU boundaries is 
also similar to sentence boundary detection. 
Stevenson and Gaizauskas (2000) use TiMBL 
(Daelemans et al, 2000) to identify sentence 
boundaries in speech recognizer output, and Gotoh 
and Renals (2000) use a statistical approach to 
identify sentence boundaries in automatic speech 
recognition transcripts of broadcast speech. 
Munk (1999) attempted to combine grammars 
and machine learning for DA classification. In 
Munk?s SALT system, a two-layer HMM was used 
to segment and label arguments and speech acts. A 
neural network identified the concept sequences. 
Finally, semantic grammars were used to parse 
each argument segment. One problem with SALT 
was that the segmentation was often inaccurate and 
resulted in bad parses. Also, SALT did not use a 
cross-domain grammar or interlingua specification. 
Cattoni et al (2001) apply statistical language 
models to DA classification. A word bigram model 
is trained for each DA in the training data. To label 
an utterance, the most likely DA is assigned. 
Arguments are identified using recursive transition 
networks. IF specification constraints are used to 
find the most likely valid DA and arguments. 
8 Discussion and Future Work 
One of the primary motivations for developing the 
hybrid analysis approach described here is to 
improve the portability of the analyzer to new 
domains and languages. We expect that moving 
from a purely grammar-based parsing approach to 
this hybrid approach will help attain this goal. 
The SOUP parser supports portability to new 
domains by allowing separate grammar modules 
for each domain and a grammar of rules shared 
across domains (Woszczyna et al, 1998). This 
modular grammar design provides an effective 
method for adding new domains to existing 
grammars. Nevertheless, developing a full 
semantic grammar for a new domain requires 
significant effort by expert grammar writers. 
The hybrid approach reduces the manual labor 
required to port to new domains by incorporating 
machine learning. The most labor-intensive part of 
developing full semantic grammars for producing 
IF is writing DA-level rules. This is exactly the 
work eliminated by using automatic DA classifiers. 
Furthermore, the phrase-level argument grammars 
used in the analyzer contain fewer rules than a full 
semantic grammar. The argument-level grammars 
are also less domain-dependent than the full 
grammars and thus more reusable. The DA 
classifiers should also be more tolerant than full 
grammars of deviations from the domain. 
We analyzed the grammars from a previous 
version of the translation system, which produced 
complete IFs using strictly grammar-based parsing, 
to estimate what portion of the grammar was 
devoted to the identification of domain actions. 
Approximately 2200 rules were used to cover 400 
DAs. Nonlexical rules made up about half of the 
grammar, and the DA rules accounted for about 
20% of the nonlexical rules. Using these figures, 
we can project the number of DA rules that would 
have to be added to the current system, which uses 
our hybrid analysis approach. The database for the 
new system contains approximately 600 DAs. 
Assuming the average number of rules per DA is 
the same as before, roughly 3300 DA-level rules 
would have to be added to the current grammar, 
which has about 17500 nonlexical rules, to cover 
the DAs in the database. 
Our hybrid approach should also improve the 
portability of the analyzer to new languages. Since 
grammars are language specific, adding a new 
language still requires writing new argument 
grammars. Then the DA classifiers simply need to 
be retrained on data for the new language. If 
training data for the new language were not 
available, DA classifiers using only language-
independent features, from the IF for example, 
could be trained on data for existing languages and 
used for the new language. Such classifiers could 
be used as a starting point until training data was 
available in the new language. 
The experimental results indicate the promise 
of the analysis approach we have described. The 
level of performance reported here was achieved 
using a simple segmentation model and simple DA 
classifiers with limited feature sets. We expect that 
performance will substantially improve with a 
more informed design of the segmentation model 
and DA classifiers. We plan to examine various 
design options, including richer feature sets and 
alternative classification techniques. We are also 
planning experiments to evaluate robustness and 
portability when the coverage of the NESPOLE! 
system is expanded to the medical domain later 
this year. In these experiments, we will measure 
the effort needed to write new argument grammars, 
the extent to which existing argument grammars 
are reusable, and the effort required to expand the 
argument grammar to include DA-level rules. 
9 Acknowledgements 
The research work reported here was supported by 
the National Science Foundation under Grant 
number 9982227. Special thanks to Alex Waibel 
and everyone in the NESPOLE! group for their 
support on this work. 
References 
Black, A., P. Taylor, and R. Caley. 1999. The 
Festival Speech Synthesis System: System 
Documentation. Human Computer Research 
Centre, University of Edinburgh, Scotland. 
http://www.cstr.ed.ac.uk/projects/festival/ma
nual 
Cattoni, R., M. Federico, and A. Lavie. 2001. 
Robust Analysis of Spoken Input Combining 
Statistical and Knowledge-Based Information 
Sources. In Proceedings of the IEEE Automatic 
Speech Recognition and Understanding 
Workshop, Trento, Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2000. TiMBL: Tilburg Memory 
Based Learner, version 3.0, Reference Guide. 
ILK Technical Report 00-01. 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz 
Gavald?, M. 2000. SOUP: A Parser for Real-
World Spontaneous Speech. In Proceedings of 
the IWPT-2000, Trento, Italy. 
Gotoh, Y. and S. Renals. Sentence Boundary 
Detection in Broadcast Speech Transcripts. 2000. 
In Proceedings on the International Speech 
Communication Association Workshop: 
Automatic Speech Recognition: Challenges for 
the New Millennium, Paris. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. 
Enhancing the Usability and Performance of 
NESPOLE! ? a Real-World Speech-to-Speech 
Translation System. In Proceedings of HLT-
2002, San Diego, CA. 
Lavie, A., C. Langley, A. Waibel, et al 2001. 
Architecture and Design Considerations in 
NESPOLE!: a Speech Translation System for E-
commerce Applications. In Proceedings of HLT-
2001, San Diego, CA. 
Lavie, A., D. Gates, N. Coccaro, and L. Levin. 
1997. Input Segmentation of Spontaneous Speech 
in JANUS: a Speech-to-speech Translation 
System. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-
96 Workshop, E. Maier, M. Mast, and S. 
Luperfoy (eds.), LNCS series, Springer Verlag. 
Lavie, A. 1996. GLR*: A Robust Grammar-
Focused Parser for Spontaneously Spoken 
Language. PhD dissertation, Technical Report 
CMU-CS-96-126, Carnegie Mellon University, 
Pittsburgh, PA. 
Levin, L., D. Gates, A. Lavie, et al 2000. 
Evaluation of a Practical Interlingua for Task-
Oriented Dialogue. In Workshop on Applied 
Interlinguas: Practical Applications of 
Interlingual Approaches to NLP, Seattle. 
Levin, L., D. Gates, A. Lavie, and A. Waibel. 
1998. An Interlingua Based on Domain Actions 
for Machine Translation of Task-Oriented 
Dialogues. In Proceedings of ICSLP-98, Vol. 4, 
pp. 1155-1158, Sydney, Australia. 
Munk, M. 1999. Shallow Statistical Parsing for 
Machine Translation. Diploma Thesis, Karlsruhe 
University. 
Stevenson, M. and R. Gaizauskas. Experiments on 
Sentence Boundary Detection. 2000. In 
Proceedings of ANLP and NAACL-2000, Seattle. 
Tomita, M. and E. H. Nyberg. 1988. Generation 
Kit and Transformation Kit, Version 3.2: User?s 
Manual. Technical Report CMU-CMT-88-
MEMO, Carnegie Mellon University, Pittsburgh, 
PA. 
Woszczyna, M., M. Broadhead, D. Gates, et al 
1998. A Modular Approach to Spoken Language 
Translation for Large Domains. In Proceedings 
of AMTA-98, Langhorne, PA. 
Domain Specific Speech Acts for Spoken Language Translation 
Lori Levin, Chad Langley, Alon Lavie,  
Donna Gates, Dorcas Wallace and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, United States 
{lsl,clangley,alavie,dmg,dorcas,kay+}@cs.cmu.edu 
Abstract 
We describe a coding scheme for ma-
chine translation of spoken task-
oriented dialogue. The coding scheme 
covers two levels of speaker intention ? 
domain independent speech acts and 
domain dependent domain actions. Our 
database contains over 14,000 tagged 
sentences in English, Italian, and Ger-
man. We argue that domain actions, and 
not speech acts, are the relevant dis-
course unit for improving translation 
quality. We also show that, although 
domain actions are domain specific, the 
approach scales up to large domains 
without an explosion of domain actions 
and can be coded with high inter-coder 
reliability across research sites. Fur-
thermore, although the number of do-
main actions is on the order of ten times 
the number of speech acts, sparseness is 
not a problem for the training of classi-
fiers for identifying the domain action. 
We describe our work on developing 
high accuracy speech act and domain 
action classifiers, which is the core of 
the source language analysis module of 
our NESPOLE machine translation sys-
tem. 
1 Introduction 
The NESPOLE and C-STAR machine translation 
projects use an interlingua representation based 
on speaker intention rather than literal meaning. 
The speaker's intention is represented as a 
domain independent speech act followed by do-
main dependent concepts. We use the term 
domain action to refer to the combination of a 
speech act with domain specific concepts. Exam-
ples of domain actions and speech acts are shown 
in Figure 1. 
 
c:give-information+party  
?I will be traveling with my husband and 
our two children ages two and eleven? 
 
c:request-information+existence+facility  
?Do they have parking available?" 
?Is there someplace to go ice skating?" 
 
c:give-information+view+information-
object  
?I see the bus icon?  
 
Figure 1: Examples of Speech Acts and Domain 
Actions. 
 
Domain actions are constructed compositionally 
from an inventory of speech acts and an inven-
tory of concepts. The allowable combinations of 
speech acts and concepts are formalized in a hu-
man- and machine-readable specification docu-
ment. The specification document is supported 
by a database of over 14,000 tagged sentences in 
English, German, and Italian. 
The discourse community has long recog-
nized the potential for improving NLP systems 
by identifying speaker intention. It has been hy-
pothesized that predicting speaker intention of 
the next utterance would improve speech recog-
nition (Reithinger et al, Stolcke et al), or reduce 
ambiguity for machine translation (Qu et al, 
1996, Qu et al, 1997). Identifying speaker inten-
tion is also critical for sentence generation. 
We argue in this paper that the explicit repre-
sentation of speaker intention using domain ac-
tions can serve as the basis for an effective 
language-independent representation of meaning 
for speech-to-speech translation and that the 
relevant units of speaker intention are the domain 
specific domain action as well as the domain in-
dependent speech act. After a brief description of 
our database, we present linguistic motivation for 
domain actions. We go on to show that although 
domain actions are domain specific, there is not 
an explosion or exponential growth of domain 
actions when we scale up to a larger domain or 
port to a new domain. Finally we will show that, 
although the number of domain actions is on the 
order of ten times the number of speech acts, 
data sparseness is not a problem in training a 
domain action classifier. We present extensive 
work on developing a high-accuracy classifier 
for domain actions using a variety of classifica-
tion approaches and conclusions on the adequacy 
of these approaches to the task of domain action 
classification.  
2 Data Collection Scenario and Data-
base 
Our study is based on data that was collected for 
the NESPOLE and C-STAR speech-to-speech 
translation projects. Three domains are included. 
The NESPOLE travel domain covers inquiries 
about vacation packages. The C-STAR travel 
domain consists largely of reservation and pay-
ment dialogues and overlaps only about 50% in 
vocabulary with the NESPOLE travel domain. 
The medical assistance domain includes dia-
logues about chest pain and flu-like symptoms. 
There were two data collection protocols for 
the NESPOLE travel domain ? monolingual and 
bilingual. In the monolingual protocol, an Eng-
lish speaker in the United States had a conversa-
tion with an Italian travel agent speaking (non-
native) English in Italy. Monolingual data was 
also collected for German, French and Italian. 
Bilingual data was collected during user studies 
with, for example, an English speaker in the 
United States talking to an Italian-speaking travel 
agent in Italy, with the NESPOLE system pro-
viding the translation between the two parties. 
The C-STAR data consists of only monolingual 
role-playing dialogues with both speakers at the 
same site. The medical dialogues are monolin-
gual with doctors playing the parts of both doctor 
and patient. 
The dialogues were transcribed and multi-
sentence utterances were broken down into mul-
tiple Semantic Dialogue Units (SDUs) that each 
correspond to one domain action. Some SDUs 
have been translated into other NESPOLE or C-
STAR languages. Over 14,000 SDUs have been 
tagged with interlingua representations including 
domain actions as well as argument-value pairs. 
Table 1 summarizes the number of tagged SDUs 
in complete dialogues in the interlingua database. 
There are some additional tagged dialogue frag-
ments that are not counted. Figure 2 shows an 
excerpt from the database. 
 
 
 
English NESPOLE Travel 4691 
English C-STAR Travel 2025 
German NESPOLE Travel 1538 
Italian NESPOLE Travel 2248 
English Medical Assistance 2001 
German Medical Assistance 1152 
Italian Medical Assistance 935 
Table 1: Tagged SDUs in the Interlingua Data-
base. 
 
e709wa.19.0  comments: DATA from 
e709_1_0018_ITAGOR_00 
 
e709wa.19.1  olang ITA  lang ITA Prv CMU   
?hai in mente una localita specifica?" 
e709wa.19.1  olang ITA  lang GER  Prv CMU   
?haben Sie einen bestimmten Ort im Sinn?" 
e709wa.19.1  olang ITA  lang FRE  Prv 
CLIPS ?" 
e709wa.19.1  olang ITA  lang ENG  Prv CMU   
?do you have a specific place in mind" 
e709wa.19.1                   IF  Prv CMU   
a:request-information+disposition+object  
(object-spec=(place, modifier=specific, 
identifiability=no), disposi-
tion=(intention, who=you)) 
e709wa.19.1  comments: Tagged by dmg 
 
Figure 2: Excerpt from the Interlingua Database. 
3 Linguistic Argument for Domain Ac-
tions 
Proponents of Construction Grammar (Fillmore 
et. al. 1988, Goldberg 1995) have argued that 
human languages consist of constructional units 
that include a syntactic structure along with its 
associated semantics and pragmatics. Some con-
structions follow the typical syntactic rules of the 
language but have a semantic or pragmatic focus 
that is not compositionally predictable from the 
parts. Other constructions do not even follow the 
typical syntax of the language (e.g., Why not go? 
with no tensed verb). 
Our work with multilingual machine transla-
tion of spoken language shows that fixed expres-
sions cannot be translated literally. For example, 
Why not go to the meeting? can be translated 
into Japanese as Kaigi ni itte mitara doo? (meet-
ing to going see/try-if how), which differs from 
the English in several ways. It does not have a 
word corresponding to not; it has a word that 
means see/try that does not appear in the English 
sentence; and so on. In order to produce an ac-
ceptable translation, we must find a common 
ground between the English fixed expression 
Why not V-inf? and the Japanese fixed expression 
-te mittara doo?. The common ground is the 
speaker's intention (in this case, to make a sug-
gestion) rather than the syntax or literal meaning. 
Speaker intention is partially captured with a 
direct or indirect speech act. However, whereas 
speech acts are generally domain independent, 
task-oriented language abounds with fixed ex-
pressions that have domain specific functions. 
For example, the phrases We have? or There 
are? in the hotel reservation domain express 
availability of rooms in addition to their more 
literal meanings of possession and existence. In 
the past six years, we have been successful in 
using domain specific domain actions as the ba-
sis for translation of limited-domain task-
oriented spoken language (Levin et al, 1998, 
Levin et al 2002; Langley and Lavie, 2003) 
4 Scalability and Portability of Domain 
Actions 
Domain actions, like speech acts, convey speaker 
intention. However, domain actions also repre-
sent components of meaning and are therefore 
more numerous than domain independent speech 
acts. 1168 unique domain actions are used in our 
NESPOLE database, in contrast to only 72 
speech acts. We show in this section that domain 
actions yield good coverage of task-oriented do-
mains, that domain actions can be coded effec-
tively by humans, and that scaling up to larger 
domains or porting to new domains is feasible 
without an explosion of domain actions.  
 
Coverage of Task-Oriented Domains: Our 
NESPOLE domain action database contains dia-
logues from two task-oriented domains: medical 
assistance and travel. Table 2 shows the number 
of speech acts and concepts that are used in the 
travel and medical domains.  The 1168 unique 
domain actions that appear in our database are 
composed of the 72 speech acts and 125 con-
cepts. 
 
 Travel Medical Combined 
DAs 880 459 1168 
SAs 67 44 72 
Concepts 91 74 125 
Table 2: DA component counts in NESPOLE 
data. 
 
Our domain action based interlingua has quite 
high coverage of the travel and medical dia-
logues we have collected. To measure how well 
the interlingua covers a domain, we define the 
no-tag rate as the percent of sentences that are 
not covered by the interlingua, according to a 
human expert. The no-tag rate for the English 
NESPOLE travel dialogues is 4.3% for dialogues 
that have been used for system development.  
We have also estimated the domain action no-
tag rate for unseen data using the NESPOLE 
travel database (English, German, and Italian 
combined). We randomly selected 100 SDUs as 
seen data and extracted their domain actions. We 
then randomly selected 100 additional SDUs 
from the remaining data and estimated the no-tag 
rate by counting the number of SDUs not cov-
ered by the domain actions in the seen data. We 
then added the unseen data to the seen data set 
and randomly selected 100 new SDUs. We re-
peated this process until the entire database had 
been seen, and we repeated the entire sampling 
process 10 times. Although the number of do-
main actions increases steadily with the database 
size (Figure 4), the no-tag rate for unseen data 
stabilizes at less than 10%.  
We also randomly selected half of the SDUs 
(4200) from the database as seen data and ex-
tracted the domain actions. Holding the seen data 
set fixed, we then estimated the no-tag rates in 
increasing amounts of unseen data from the re-
maining half of the database. We repeated this 
process 10 times. With a fixed amount of seen 
data, the no-tag rate remains stable for increasing 
amounts of unseen data. We observed similar no-
tag rate results for the medical assistance domain 
and for the combination of travel and medical 
domains. 
It is also important to note that although there 
is a large set of uncommon domain actions, the 
top 105 domain actions cover 80% of the sen-
tences in the travel domain database. Thus do-
main actions are practical for covering task-
oriented domains. 
 
Intercoder Agreement: Intercoder agreement is 
another indicator of manageability of the domain 
action based interlingua. We calculate intercoder 
agreement as percent agreement. Three interlin-
gua experts at one NESPOLE site achieved 94% 
agreement (average pairwise agreement) on 
speech acts and 88% agreement on domain ac-
tions. Across sites, expert agreement on speech 
acts is still quite high (89%), although agreement 
on domain actions is lower (62%). Since many 
domain actions are similar in meaning, some dis-
agreement can be tolerated without affecting 
translation quality. 
 
Figure 3: DAs to cover data (English). 
Figure 4: DAs to cover data (All languages). 
 
Scalability and Portability: The graphs in Figure 
3 and Figure 4 illustrate growth in the number of 
domain actions as the database size increases and 
as new domains are added. The x-axis represents 
the sample size randomly selected from the data-
base. The y-axis shows the number of unique 
domain actions (types) averaged over 10 samples 
of each size. Figure 3 shows the growth in do-
main actions for three English databases 
(NESPOLE travel, C-STAR travel, and medical 
assistance) as well as the growth in domain ac-
tions for a database consisting of equal amounts 
of data from each domain. Figure 4 shows the 
growth in domain actions for combined English, 
German, and Italian data in the NESPOLE travel 
and medical domains.  
Figure 3 and Figure 4 show that the number 
of domain actions increases steadily as the data-
base grows. However, closer examination reveals 
that scalability to larger domains and portability 
to new domains are in fact feasible.  The curves 
representing combined domains (travel plus 
medical in Figure 4 and NESPOLE travel, C-
STAR travel, and medical in Figure 3) show only 
a small increase in the number of domain actions 
when two domains are combined. In fact, there is 
a large overlap between domains.  In Table 3 the 
Overlap columns show the number of DA types 
and tokens that are shared between the travel and 
medical domains. We can see around 70% of DA 
tokens are covered by DA types that occur in 
both domains. 
 
 
DA 
Types 
Type 
Overlap 
DA 
Tokens 
Token 
Overlap 
NESPOLE 
Travel 880 171 8477 
6004 
(70.8%) 
NESPOLE 
Medical 459 171 4088 
2743 
(67.1%) 
Table 3: DA Overlap (All languages). 
5 A Hybrid Analysis Approach for Pars-
ing Domain Actions 
Langley et al (2002; Langley and Lavie, 2003) 
describe the hybrid analysis approach that is used 
in the NESPOLE! system (Lavie et al, 2002). 
The hybrid analysis approach combines gram-
mar-based phrasal parsing and machine learning 
techniques to transform utterances into our inter-
lingua representation. Our analyzer operates in 
three stages to identify the domain action and 
arguments. 
First, an input utterance is parsed into a se-
quence of arguments using phrase-level semantic 
grammars and the SOUP parser (Gavald?, 2000). 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument gram-
mar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains 
phrase-level rules for parsing arguments defined 
in the interlingua. The pseudo-argument gram-
mar contains rules for parsing common phrases 
that are not covered by interlingua arguments. 
For example, all booked up, full, and sold out 
might be grouped into a class of phrases that in-
dicate unavailability. The cross-domain grammar 
contains rules for parsing complete DAs that are 
domain independent. For example, this grammar 
contains rules for greetings (Hello, Good bye, 
Nice to meet you, etc.). Finally, the shared 
grammar contains low-level rules that can be 
used by all other subgrammars. 
After argument parsing, the utterance is seg-
mented into SDUs using memory-based learning 
(k-nearest neighbor) techniques. Spoken utter-
ances often consist of several SDUs. Since DAs 
are assigned at the SDU level, it is necessary to 
segment utterances before assigning DAs. 
0
100
200
300
400
500
600
700
800
900
0 1000 2000 3000 4000 5000 6000 7000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical C-STAR
Nespole Travel+Medical C-STAR + Nespole Travel+Medical
0
100
200
300
400
500
600
700
800
900
1000
0 1000 2000 3000 4000 5000 6000 7000 8000 9000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical Nespole Travel+Medical
The final stage in the hybrid analysis ap-
proach is domain action classification.  
6 Domain Action Classification 
Identifying the domain action is a critical step in 
the analysis process for our interlingua-based 
translation systems. One possible approach 
would be to manually develop grammars de-
signed to parse input utterances all the way to the 
domain action level. However, while grammar-
based parsing may provide very accurate analy-
ses, it is generally not feasible to develop a 
grammar that completely covers a domain. This 
problem is exacerbated with spoken input, where 
disfluencies and deviations from the grammar are 
very common. Furthermore, a great deal of effort 
by human experts is generally required to de-
velop a wide-coverage grammar. 
An alternative to writing full domain action 
grammars is to train classifiers to identify the 
DA. Machine learning approaches allow the ana-
lyzer to generalize beyond training data and tend 
to degrade gracefully in the face of noisy input. 
Machine learning methods may, however, be less 
accurate than grammars, especially on common 
in-domain input, and may require a large amount 
of training data in order to achieve adequate lev-
els of performance. In the hybrid analyzer de-
scribed above, classifiers are used to identify the 
DA for domain specific portions of utterances 
that are not covered by the cross-domain gram-
mar. 
We tested classifiers trained to classify com-
plete DAs. We also split the DA classification 
task into two subtasks: speech act classification 
and concept sequence classification. This simpli-
fies the task of each classifier, allows for the use 
of different approaches and/or feature sets for 
each task, and reduces data sparseness. Our hy-
brid analyzer uses the output of each classifier 
along with the interlingua specification to iden-
tify the DA (Langley et al, 2002; Langley and 
Lavie, 2003). 
7 Experimental Setup 
We conducted experiments to assess the per-
formance of several machine-learning ap-
proaches on the DA classification tasks. We 
evaluated all of the classifiers on English and 
German input in the NESPOLE travel domain.  
7.1 Corpus 
The corpus used in all of the experiments was the 
NESPOLE! travel and tourism database. Since 
our goal was to evaluate the SA and concept se-
quence classifiers and not segmentation, we cre-
ated training examples for each SDU in the 
database rather than for each utterance. Table 4 
contains statistics regarding the contents of the 
corpus for our classification tasks. Table 5 shows 
the frequency of the most common domain ac-
tion, speech act, and concept sequence in the 
corpus. These frequencies provide a baseline that 
would be achieved by a simple classifier that al-
ways returned the most common class. 
 
 English German 
SDUs 8289 8719 
Domain Actions 972 1001 
Speech Acts 70 70 
Concept Sequences 615 638 
Vocabulary Size 1946 2815 
Table 4: Corpus Statistics. 
 
 English German 
DA (acknowledge) 19.2% 19.7% 
SA (give-information) 41.4% 40.7% 
Concept Sequence 
(No concepts) 
38.9% 40.3% 
Table 5: Most frequent DAs, SAs, and CSs. 
 
All of the results presented in this paper were 
produced using a 20-fold cross validation setup. 
The corpus was randomly divided into 20 sets of 
equal size. Each of the sets was held out as the 
test set for one fold with the remaining 19 sets 
used as training data. Within each language, the 
same random split was used for all of the classi-
fication experiments. Because the same split of 
the data was used for different classifiers, the 
results of two classifiers on the same test set are 
directly comparable. Thus, we tested for signifi-
cance using two-tailed matched pair t-tests. 
7.2 Machine Learning Approaches 
We evaluated the performance of four different 
machine-learning approaches on the DA classifi-
cation tasks: memory-based learning (k-Nearest-
Neighbor), decision trees, neural networks, and 
na?ve Bayes n-gram classifiers. We selected 
these approaches because they vary substantially 
in the their representations of the training data 
and their methods for selecting the best class. 
Our purpose was not to implement each ap-
proach from scratch but to test the approach for 
our particular task. Thus, we chose to use exist-
ing software for each approach ?off the shelf.? 
The ease of acquiring and setting up the software 
influenced our choice. Furthermore, the ease of 
incorporating the software into our online trans-
lation system was also a factor. 
Our memory-based classifiers were imple-
mented using TiMBL (Daelemans et al, 2002). 
We used C4.5 (Quinlan, 1993) for our decision 
tree classifiers. Our neural network classifiers 
were implemented using SNNS (Zell et al, 
1998). We used Rainbow (McCallum, 1996) for 
our na?ve Bayes n-gram classifiers. 
8 Experiments 
In our first experiment, we compared the per-
formance of the four machine learning ap-
proaches. Each SDU was parsed using the 
argument and pseudo-argument grammars de-
scribed above. The feature set for the DA and SA 
classifiers consisted of binary features indicating 
the presence or absence of labels from the 
grammars in the parse forest for the SDU. The 
feature set included 212 features for English and 
259 features for German. The concept sequence 
classifiers used the same feature set with the ad-
dition of the speech act. 
In the SA classification experiment, the 
TiMBL classifier used the IB1 (k-NN) algorithm 
with 1 neighbor and gain ratio feature weighting. 
The C4.5 classifier required at least one instance 
per branch and used node post-pruning. Both the 
TiMBL and C4.5 classifiers used the binary fea-
tures described above and produced the single 
best class as output. The SNNS classifier used a 
simple feed-forward network with 1 input unit 
for each binary feature, 1 hidden layer containing 
15 units, and 1 output unit for each speech act. 
The network was trained using backpropagation. 
The order of presentation of the training exam-
ples was randomized in each epoch, and the 
weights were updated after each training exam-
ple presentation. In order to simulate the binary 
features used by the other classifiers as closely as 
possible, the Rainbow classifier used a simple 
unigram model whose vocabulary was the set of 
labels included in the binary feature set. The 
setup for the DA classification experiment was 
identical except that the neural network had 50 
hidden units. 
The setup of the classifiers for the concept se-
quence classification experiment was very simi-
lar. The TiMBL and C4.5 classifiers were set up 
exactly as in the DA and SA experiments with 
one extra feature whose value was the speech act. 
The SNNS concept sequence classifier used a 
similar network with 50 hidden units. The SA 
feature was represented as a set of binary input 
units. The Rainbow classifier was set up exactly 
as in the DA and SA experiments. The SA fea-
ture was not included. 
As mentioned above, both experiments used a 
20-fold cross-validation setup. In each fold, the 
TiMBL, C4.5, and Rainbow classifiers were sim-
ply trained on 19 subsets of the data and tested 
on the remaining set. The SNNS classifiers re-
quired a more complex setup to determine the 
number of epochs to train the neural network for 
each test set. Within each fold, a cross-validation 
setup was used to determine the number of train-
ing epochs. Each of the 19 training subsets for a 
fold was used as a validation set. The network 
was trained on the remaining 18 subsets until the 
accuracy on the validation set did not improve 
for 50 consecutive epochs. The network was then 
trained on all 19 training subsets for the average 
number of epochs from the validation sets. This 
process was used for all 20-folds in the SA clas-
sification experiment. For the DA and concept 
sequence experiments, this process ran for ap-
proximately 1.5 days for each fold. Thus, this 
process was run for the first two folds, and the 
average number of epochs from those folds was 
used for training. 
 
 English German 
TiMBL 49.69% 46.51% 
C4.5 48.90% 46.58% 
SNNS 49.39% 46.21% 
Rainbow 39.74% 38.32% 
Table 6: Domain Action classifier accuracy. 
 
 English German 
TiMBL 69.82% 67.57% 
C4.5 70.41% 67.90% 
SNNS 71.52% 67.61% 
Rainbow 51.39% 46.00% 
Table 7: Speech Act classifier accuracy. 
 
 English German 
TiMBL 69.59% 67.08% 
C4.5 68.47% 66.45% 
SNNS 71.35% 68.67% 
Rainbow 51.64% 51.50% 
Table 8: Concept Sequence classifier accuracy. 
 Table 6, Table 7, and Table 8 show the aver-
age accuracy of each learning approach on the 
20-fold cross validation experiments for domain 
action, speech act, and concept classification re-
spectively. For DA classification, there were no 
significant differences between the TiMBL, 
C4.5, and SNNS classifiers for English or Ger-
man. In the SA experiment, the difference be-
tween the TiMBL and C4.5 classifiers for 
English was not significant. The SNNS classifier 
was significantly better than both TiMBL and 
C4.5 (at least p=0.0001). For German SA classi-
fication, there were no significant differences 
between the TiMBL, C4.5, and SNNS classifiers. 
For concept sequence classification, SNNS was 
significantly better than TiMBL and C4.5 (at 
least p=0.0001) for both English and German. 
For English only, TiMBL was significantly better 
than C4.5 (p=0.005). 
For both languages, the Rainbow classifier 
performed much worse than the other classifiers. 
However, the unigram model over arguments did 
not exploit the strengths of the n-gram classifica-
tion approach. Thus, we ran another experiment 
in which the Rainbow classifier was trained on 
simple word bigrams. No stemming or stop 
words were used in building the bigram models. 
 
 English German 
Domain Action 48.59% 48.09% 
Speech Act 79.00% 77.46% 
Concept Sequence 56.87% 57.77% 
Table 9: Rainbow accuracy with word bigrams. 
 
Table 9 shows the average accuracy of the 
Rainbow word bigram classifiers using the same 
20-fold cross-validation setup as in the previous 
experiments. As we expected, using word bi-
grams rather than parse label unigrams improved 
the performance of the Rainbow classifiers. For 
German DA classification, the word bigram clas-
sifier was significantly better than all of the pre-
vious German DA classifiers (at least p=0.005). 
Furthermore, the Rainbow word bigram SA clas-
sifiers for both languages outperformed all of the 
SA classifiers that used only the parse labels. 
Although the argument parse labels provide 
an abstraction of the words present in an SDU, 
the words themselves also clearly provided use-
ful information for classification, at least for the 
SA task. Thus, we conducted additional experi-
ments to examine whether combining parse and 
word information could further improve per-
formance. 
We chose to incorporate word information 
into the TiMBL classifiers used in the first ex-
periment. Although the SNNS SA classifier per-
formed significantly better than the TiMBL SA 
classifier for English, there was no significant 
difference for SA classification in German. Fur-
thermore, because of the complexity and time 
required for training with SNNS, we preferred 
working with TiMBL. 
We tested two approaches to adding word in-
formation to the TiMBL classifier. In both ap-
proaches, the word-based information for each 
fold was computed only based on the data in the 
training set. In our first approach, we added bi-
nary features for the 250 words that had the 
highest mutual information with the class. Each 
feature indicated the presence or absence of the 
word in the SDU. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 3 neighbors, and unweighted voting. The 
second approach we tested combined the Rain-
bow word bigram classifier with the TiMBL 
classifier. We added one input feature for each 
possible speech act to the TiMBL classifier. The 
value of each SA feature was the probability of 
the speech act computed by the Rainbow word 
bigram classifier. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 11 neighbors, and inverse linear distance 
weighted voting. 
 
 English German 
TiMBL + words 78.59% 75.98% 
TiMBL + Rainbow 81.25% 78.93% 
Table 10: Word+Parse SA classifier accuracy. 
 
Table 10 shows the average accuracy of the 
SA classifiers that combined parse and word in-
formation using the same 20-fold cross-
validation setup as the previous experiments. 
Although adding binary features for individual 
words improved performance over the classifiers 
with no word information, it did not allow the 
combined classifiers to outperform the Rainbow 
word bigram classifiers. However, for both 
languages, adding the probabilities computed by 
the Rainbow bigram model resulted in a SA clas-
sifier that outperformed all previous classifiers. 
The improvement in accuracy was highly signifi-
cant for both languages. 
We conducted a similar experiment for com-
bining parse and word information in the concept 
sequence classifiers. The first condition was 
analogous to the first condition in the combined 
SA classification experiment. The second condi-
tion was slightly different. A concept sequence 
can be broken down into a set of individual con-
cepts. The set of individual concepts is much 
smaller than the set of concept sequences (110 
for English and 111 for German). Thus, we used 
a Rainbow word bigram classifier to compute the 
probability of each individual concept rather than 
the complete concept sequence. The probabilities 
for the individual concepts were added to the 
parse label features for the combined classifier. 
In both conditions, the performance of the com-
bined classifiers was roughly the same as the 
classifiers that used only parse labels as features. 
 
 English German 
TiMBL + words 56.48% 54.98% 
Table 11: Word+Parse DA classifier accuracy. 
 
Table 11 shows the average accuracy of DA 
classifiers for English and German using a setup 
similar to the first approach in the combined SA 
experiment. In this experiment, we added binary 
features for the 250 words that the highest mu-
tual information with the class. We used a 
TiMBL classifier with gain ratio feature weight-
ing and one neighbor. The improvement in accu-
racy for both languages was highly significant. 
 
 English German 
TiMBL SA 
+ TiMBL CS 49.63% 46.50% 
TiMBL+Rainbow SA 
+ TiMBL CS 57.74% 53.93% 
Table 12: DA accuracy of SA+CS classifiers. 
 
Finally, Table 12 shows the results from two 
tests to compare the performance of combining 
the best output of the SA and concept sequence 
classifiers with the performance of the complete 
DA classifiers. In the first test, we combined the 
output from the TiMBL SA and CS classifiers 
shown in Table 7 and Table 8. The performance 
of the combined SA+CS classifiers was almost 
identical to that of the TiMBL DA classifiers 
shown in Table 6. In the second test, we com-
bined our best SA classifier (TiMBL+Rainbow, 
shown in Table 10) with the TiMBL CS classi-
fier. In this case, we had mixed results. The per-
formance of the combined classifiers was better 
than our best DA classifier for English and worse 
for German. 
9 Discussion 
One of our main goals was to determine the fea-
sibility of automatically classifying domain ac-
tions. As the data in Table 4 show, DA 
classification is a challenging problem with ap-
proximately 1000 classes. Even when the task is 
divided into subproblems of identifying the SA 
and concept sequence, the subtasks remain diffi-
cult. The difficulty is compounded by relatively 
sparse training data with unevenly distributed 
classes. Although the most common classes in 
our training corpus had over 1000 training exam-
ples, many of the classes had only 1 or 2 exam-
ples. 
Despite these difficulties, our results indicate 
that domain action classification is feasible. For 
SA classification in particular we were able to 
achieve very strong performance. Although per-
formance on concept sequence and DA classifi-
cation is not as high, it is still quite strong, 
especially given that there are an order of magni-
tude more classes than in SA classification. 
Based on our experiments, it appears that all of 
the learning approaches we tested were able to 
cope with data sparseness at the level found in 
our data, with the possible exception of the na?ve 
Bayes n-gram approach (Rainbow) for the con-
cept sequence task. 
One additional point worth noting is that there 
is evidence that domain action classification 
could be performed reasonably well using only 
word-based information. Although our best-
performing classifiers combined word and argu-
ment parse information, the na?ve Bayes word 
bigram classifier (Rainbow) performed very well 
on the SA classification task. With additional 
data, the performance of the concept sequence 
and DA word bigram classifiers could be ex-
pected to improve. Cattoni et al (2001) also ap-
ply statistical language models to DA 
classification. A word bigram model is trained 
for each DA, and the DA with the highest likeli-
hood is assigned to each SDU. Arguments are 
identified using recursive transition networks, 
and interlingua specification constraints are used 
to find the most likely valid interlingua represen-
tation. Although it is clear that argument infor-
mation is useful for the task, it appears that 
words alone can be used to achieve reasonable 
performance. 
Another goal of our experiments was to help 
in the selection of a machine learning approach 
to be used in our hybrid analyzer. Certainly one 
of the most important considerations is how well 
the learning approach performs the task. For SA 
classification, the combination of parse features 
and word bigram probabilities clearly gave the 
best performance. For concept sequence classifi-
cation, no learning approach clearly outper-
formed any other (with the exception that the 
na?ve Bayes n-gram approach performed worse 
than other approaches). However, the perform-
ance of the classifiers is not the only considera-
tion to be made in selecting the classifier for our 
hybrid analyzer. 
Several additional factors are also important 
in selecting the particular machine learning ap-
proach to be used. One important attribute of the 
learning approach is the speed of both classifica-
tion and training. Since the classifiers are part of 
a translation system designed for use between 
two humans to facilitate (near) real-time com-
munication, the DA classifiers must classify in-
dividual utterances online very quickly. 
Furthermore, since humans must write and test 
the argument grammars, training and batch 
classification should be fast so that the grammar 
writers can update the grammars, retrain the clas-
sifiers, and test efficiently. 
The machine learning approach should also 
be able to easily accommodate both continuous 
and discrete features from a variety of sources. 
Possible sources for features include words 
and/or phrases in an utterance, the argument 
parse, the interlingua representation of the argu-
ments, and properties of the dialogue (e.g. 
speaker tag). The classifier should be able to eas-
ily combine features from any or all of these 
sources. 
Another desirable attribute for the machine 
learning approach is the ability to produce a 
ranked list of possible classes. Our interlingua 
specification defines how speech acts and con-
cepts are allowed to combine as well as how ar-
guments are licensed by the domain action. 
These constraints can be used to select an alter-
native DA if the best DA violates the specifica-
tion. 
Based on all of these considerations, the 
TiMBL+Rainbow classifier, which combines 
parse label features with word bigram probabili-
ties, seems like an excellent choice for speech act 
classification. It was the most accurate classifier 
that we tested. Furthermore, the main TiMBL 
classifier meets all of the requirements discussed 
above except the ability to produce a complete 
ranked list of the classes for each instance. How-
ever, such a list could be produced as a backup 
from the Rainbow probability features. Adding 
new features to the combined classifier would 
also be very easy because TiMBL was the pri-
mary classifier in the combination. Finally, since 
both TiMBL and Rainbow provide an online 
server mode for classifying single instances, in-
corporating the combined classifier into an 
online translation system would not be difficult. 
Since there were no significant differences in the 
performance of most of the concept sequence 
classifiers, this combined approach is probably 
also a good option for that task. 
10 Conclusion 
We have described a representation of 
speaker intention that includes domain independ-
ent speech acts as well as domain dependent do-
main actions. We have shown that domain 
actions are a useful level of abstraction for ma-
chine translation of task-oriented dialogue, and 
that, in spite of their domain specificity, they are 
scalable to larger domains and portable to new 
domains.  
We have also presented classifiers for domain 
actions that have been comparatively tested and 
used successfully in the NESPOLE speech-to-
speech translation system. We experimentally 
compared the effectiveness of several machine-
learning approaches for classification of domain 
actions, speech acts, and concept sequences on 
two input languages. Despite the difficulty of the 
classification tasks due to a large number of 
classes and relatively sparse data, the classifiers 
exhibited strong performance on all tasks. We 
also demonstrated how the combination of two 
learning approaches could be used to improve 
performance and overcome the weaknesses of 
the individual approaches. 
Acknowledgements: NESPOLE was funded 
by NSF (Grant number 9982227) and the EU. 
The NESPOLE partners are ITC-irst, Universite 
Joseph Fourrier, Universitat Karlsruhe, APT 
Trentino travel board, and AETHRA telecom-
munications. We would like to acknowledge the 
contribution of the following people in particu-
lar: Fabio Pianesi, Emanuele Pianta, Nadia 
Mana, and Herve Blanchon. 
References 
Cattoni, R., M. Federico, and A. Lavie. 2001. Robust 
Analysis of Spoken Input Combining Statistical 
and Knowledge-Based Information Sources. In 
Proceedings of the IEEE ASRU Workshop, Trento, 
Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2002. TiMBL: Tilburg Memory 
Based Learner, version 4.3, Reference Guide. ILK 
Technical Report 02-10. Available from 
http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.
gz. 
Fillmore, C.J., Kay, P. and O'Connor, M.C. 1988. 
Regularity and Idiomaticity in Grammatical Con-
structions. Language, 64(3), 501-538. 
Gavald?, M. 2000. SOUP: A Parser for Real-World 
Spontaneous Speech. In Proceedings of IWPT-
2000, Trento, Italy. 
Goldberg, Adele E. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. 
Chicago University Press. 
Langley, C. and A. Lavie. 2003. Parsing Domain Ac-
tions with Phrase-Level Grammars and Memory-
Based Learners. To appear in Proceedings of 
IWPT-2003. Nancy, France. 
Langley, C., A. Lavie, L. Levin, D. Wallace, D. 
Gates, and K. Peterson. 2002. Spoken Language 
Parsing Using Phrase-Level Grammars and Train-
able Classifiers. In Workshop on Algorithms for 
Speech-to-Speech Machine Translation at ACL-02. 
Philadelphia, PA. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. Enhancing 
the Usability and Performance of NESPOLE! ? a 
Real-World Speech-to-Speech Translation System. 
In Proceedings of HLT-2002. San Diego, CA. 
Levin, L., D. Gates, A. Lavie, A. Waibel. 1998. An 
Interlingua Based on Domain Actions for Machine 
Translation of Task-Oriented Dialogues. In Pro-
ceedings of ICSLP 98, Vol. 4, pages 1155-1158, 
Sydney, Australia. 
Levin, L., D. Gates, D. Wallace, K. Peterson, A. La-
vie F. Pianesi, E. Pianta, R. Cattoni, N. Mana. 
2002. Balancing Expressiveness and Simplicity in 
an Interlingua for Task Based Dialogue. In Pro-
ceedings of Workshop on Spoken Language Trans-
lation. ACL-02, Philadelphia. 
McCallum, A. K. 1996. Bow: A toolkit for statistical 
language modeling, text retrieval, classification and 
clustering. 
http://www.cs.cmu.edu/~mccallum/bow. 
Qu, Y., B. DiEugenio, A. Lavie, L. Levin and C.P. 
Rose. 1997. Minimizing Cumulative Error in Dis-
course Context. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-96 
Workshop, E. Maier, M. Mast and S. LuperFoy 
(eds.), LNCS series, Springer Verlag. 
Qu, Y., C. P. Rose, and B. DiEugenio. 1996. Using 
Discourse Predictions for Ambiguity Resolution. In 
Proceedings of COLING-1996. 
Quinlan, J. R. 1993. C4.5: Programs for Machine 
Learning. San Mateo: Morgan Kaufmann. 
Reithinger, N., R. Engel, M. Kipp, M. Klesen. 1996. 
Predicting Dialogue Acts for a Speech-To-Speech 
Translation System. DFKI GmbH Saarbruecken. 
Verbmobil-Report 151. 
http://verbmobil.dfki.de/cgi-
bin/verbmobil/htbin/doc-access.cgi 
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. 
Bates, D. Jurafsky, P. Taylor, R. Martin, M. 
Meteer, and C. Van Ess-Dykema. 2000. Dialogue 
Act Modeling for Automatic Tagging and Recogni-
tion of Conversational Speech. Computational Lin-
guistics 26:3, 339-371.  
Zell, A., G. Mamier, M. Vogt, et al 1998. SNNS: 
Stuttgart Neural Network Simulator User Manual, 
Version 4.2. 
Unsupervised Induction of Natural Language Morphology Inflection Classes 
Christian Monson, Alon Lavie, Jaime Carbonell, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University  
5000 Forbes Ave. 
Pittsburgh, USA 15213 
{cmonson, alavie+, jgc+, lsl+}@cs.cmu.edu 
 
Abstract 
We propose a novel language-independent 
framework for inducing a collection of mor-
phological inflection classes from a monolin-
gual corpus of full form words.  Our approach 
involves two main stages.  In the first stage, 
we generate a large data structure of candidate 
inflection classes and their interrelationships.  
In the second stage, search and filtering tech-
niques are applied to this data structure, to 
identify a select collection of "true" inflection 
classes of the language.  We describe the basic 
methodology involved in both stages of our 
approach and present an evaluation of our 
baseline techniques applied to induction of 
major inflection classes of Spanish.  The pre-
liminary results on an initial training corpus 
already surpass an F1 of 0.5 against ideal 
Spanish inflectional morphology classes. 
1 Introduction 
Many natural language processing tasks, such as 
morphological analysis and parsing, have mature 
solutions when applied to resource-rich European 
and Asian languages.  Addressing these same tasks 
in less studied low-density languages, however, 
poses exciting challenges.   
These languages have limited available re-
sources: with perhaps a few million speakers there 
is likely no native speaker linguist and frequently 
there is little electronic text readily available.  To 
compound the difficulties, while low-density lan-
guages abound, comparatively little financial re-
sources are available to address their challenges.  
These considerations suggest developing systems 
to automatically induce solutions for NLP tasks in 
new languages. 
The AVENUE project (Lavie et al 2003; Car-
bonell et al, 2002; Probst et al, 2002) at Carnegie 
Mellon University seeks to apply automatic induc-
tion methods to develop rule-based machine trans-
lation systems between pairs of languages where 
one of the languages is low-density and the other is 
resource-rich.  We are currently pursuing MT sys-
tems with Mapudungun, an indigenous language 
spoken by 900,000 people in southern Chile and 
Argentina, and Aymara, spoken by 3 million peo-
ple in Bolivia, Peru, and northern Chile, as low-
density languages and Spanish the resource rich 
language. 
A vital first step in a rule-based machine transla-
tion system is morphological analysis.  This paper 
outlines a framework for automatic natural lan-
guage morphology induction inspired by the tradi-
tional and linguistic concept of inflection classes.  
Additional details concerning the candidate inflec-
tion class framework can be found in Monson 
(2004).  This paper then goes on to describe one 
implemented search strategy within this frame-
work, presenting both a simple summary of results 
and an in depth error analysis. 
While the intent of this research direction is to 
define techniques applicable to low-density lan-
guages, this paper employs English to illustrate the 
main conjectures and Spanish, a language with a 
reasonably complex morphological system, for 
quantitative analysis.  All experiments detailed in 
this paper are over a Spanish newswire corpus of 
40,011 tokens and 6,975 types. 
2 Previous Work 
It is possible to organize much of the recent 
work on unsupervised morphology induction by 
considering the bias each approach has toward dis-
covering morphologically related words that are 
also orthographically similar. 
At one end of the spectrum is the work of 
Yarowsky et al (2001), who derive a morphologi-
cal analyzer for a language, L, by projecting the 
morphological analysis of a resource-rich language 
onto L through a clever application of statistical 
machine translation style word alignment prob-
abilities.  The word alignments are trained over a 
sentence aligned parallel bilingual text for the lan-
guage pair.  While the probabilistic model they use 
to generalize their initial system contains a bias 
toward orthographic similarity, the unembellished 
algorithm contains no assumptions on the ortho-
graphic shape of related word forms. 
Next along the spectrum of orthographic similar-
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
ity bias is the work of Schone and Jurafsky (2000), 
who first acquire a list of pairs of potential mor-
phological variants (PPMV?s) using an ortho-
graphic similarity technique due to Gaussier 
(1999), in which pairs of words from a corpus vo-
cabulary with the same initial string are identified.  
They then apply latent semantic analysis (LSA) to 
score each PPMV with a semantic distance.  Pairs 
measuring a small distance, those whose potential 
variants tend to occur where a neighborhood of the 
nearest hundred words contains similar counts of 
individual high-frequency forms, are then pro-
posed as true morphological variants of one anther.  
In later work, Schone and Jurafsky (2001) extend 
their technique to identify not only suffixes but 
also prefixes and circumfixes by building both 
forward and backward tries over a corpus. 
Goldsmith (2001), by searching over a space of 
morphology models limited to substitution of suf-
fixes, ties morphology yet closer to orthography.  
Segmenting word forms in a corpus, Goldsmith 
creates an inventory of stems and suffixes.  Suf-
fixes which can interchangeably concatenate onto 
a set of stems form a signature.  After defining the 
space of signatures, Goldsmith searches for that 
choice of word segmentations resulting in a mini-
mum description length local optimum. 
Finally, the work of Harris (1955; 1967), and 
later Hafer and Weiss (1974), has direct bearing on 
the approach taken in this paper.  Couched in mod-
ern terms, their work involves first building tries 
over a corpus vocabulary, and then selecting, as 
morpheme boundaries, those character boundaries 
with high branching count in the tries. 
The work in this paper also has a strong bias to-
ward discovering morphologically related words 
that share a similar orthography.  In particular, the 
morphology model we use is, akin to Goldsmith, 
limited to suffix substitution.  The novel proposal 
we bring to the table, however, is a formalization 
of the full search space of all candidate inflection 
classes.  With this bulwark in place, defining 
search strategies for morpheme discovery becomes 
a natural and straightforward activity. 
3 Inflection Classes as Motivation 
When learning the morphology of a foreign lan-
guage, it is common for a student to study tables of 
inflection classes.  In Spanish, for example, a regu-
lar verb belongs to one of three inflection 
classes?verbs that take the -ar infinitive suffix 
inflect for various syntactic features with one set of 
suffixes, verbs that take the -er infinitive suffix 
realize the same set of syntactic features with a 
second set of suffixes, while -ir verbs take yet a 
third set. 
Carstairs-McCarthy formalizes the concept of an 
inflection class in chapter 16 of The Handbook of 
Morphology (1998).  In his terminology, a lan-
guage with inflectional morphology contains lex-
emes which occur in a variety of word forms.  
Each word form carries two pieces of information: 
1) Lexical content and 
2) Morphosyntactic properties. 
For example, the English word form gave ex-
presses the lexeme GIVE plus the morphosyntactic 
property Past, while gives expresses GIVE plus the 
properties 3rd Person, Singular, and Non-Past. 
A set of morphosyntactic properties realized 
with a single word form is defined to be a cell, 
while a paradigm is a set of cells exactly expressed 
by the word forms of some lexeme.   
A particular natural language may have many 
paradigms.  In English, a language with very little 
inflectional morphology, there are at least two 
paradigms, a noun paradigm consisting of two 
cells, Singular and Plural, and a paradigm for 
verbs, consisting of the five cells given (with one 
choice of naming convention) as the first column 
of Table 1. 
Lexemes that belong to the same paradigm may 
still differ in their morphophonemic realizations of 
various cells in that paradigm?each paradigm 
may have several associated inflection classes 
which specify, for the lexemes belonging to that 
inflection class, the surface instantiation for each 
cell of the paradigm.   
Three of the inflection classes within the English 
verb paradigm are found in Table 1 under the col-
umns labeled A through C.  Each inflection class 
Inflection Classes Verb 
Paradigm A B C 
Basic 
blame 
roam 
solve 
show 
sow 
saw 
sing 
ring 
3rd Person 
Singular     
Non-past 
-/z/ 
blames 
roams 
solves 
-/z/ 
shows 
sows 
saws 
-/z/ 
sings 
rings 
 
Past 
-/d/ 
blamed 
roamed 
solved 
-/d/ 
showed 
sowed 
sawed 
V? /eI/ 
sang 
rang 
 
Perfective       
or Passive 
-/d/ 
blamed 
roamed 
solved 
-/n/ 
shown 
sown 
sawn 
V? /?/ 
sung 
rung 
 
Progressive 
-/i?/ 
blaming 
roaming 
solving 
-/i?/ 
showing 
sowing 
sawing 
-/i?/ 
singing 
ringing 
 
 
Table 1: A few inflection classes of the Eng-
lish verb paradigm 
column consists of entries corresponding to the 
cells of the verb paradigm.  Each entry contains an 
informal notation for the morphophonemic process 
which the inflection class applies to the basic form 
of a lexeme and examples of word forms filling the 
corresponding paradigm cell. 
Inflection class A is one of the largest and most 
productive verb inflection classes in English, in-
flection class B contains the Perfective/Passive 
suffix -/n/, and C is a small ?irregular? inflection 
class of strong verbs. 
The task our morphology induction system en-
gages is exactly the discovery of the inflection 
classes of a natural language.  Unlike the analysis 
in Table 1, however, the rest of this paper treats 
word forms as simply strings of characters as op-
posed to strings of phonemes. 
4 Empirical Inflection Classes 
There are two stages in our approach to unsu-
pervised morphology induction.  First, we define a 
search space over a set of candidate inflection 
classes, and second, we search this space for those 
candidates most likely to be part of a true inflec-
tion class in the language.  In both stages of our 
approach we intentionally exploit the fact that suf-
fixes belonging to the same natural language in-
flection class frequently occur interchangeably on 
the same stems. 
4.1 Candidate Inflection Class Search Space 
To define a search space wherein we hope to 
identify inflection classes of a natural language, 
our algorithm accepts as input a monolingual cor-
pus for that language and proposes candidate mor-
pheme boundaries at every character boundary in 
every word form in the corpus vocabulary.  We 
call each string before a candidate morpheme 
boundary a candidate stem or c-stem, and each 
string after a boundary a c-suffix.  We define a 
candidate inflection class (CIC) to be a set of c-
suffixes for which there exists at least one c-stem, 
t, such that each c-suffix in the CIC concatenated 
to t produces a word form in the vocabulary.  For 
convenience, let the set of c-stems which generate 
a CIC, C, be called the adherent c-stems of C; let 
the number of adherent c-stems of C be C?s adher-
ent size; and let the size of the set of c-suffixes in 
C be the level of C.  We denote a CIC in this paper 
by a period delimited sequence of c-suffixes. 
While CIC?s effectively model suffix substitu-
tion on bound stems, we would also like to model 
suffix concatenation onto free stems.  To this end, 
the set of candidate morpheme boundaries our al-
gorithm proposes include those boundaries after 
the final character in each word form.  In this paper 
we assume a suffix, which we denote as ?, follows 
all word form final boundaries.  A CIC contains 
the ? c-suffix when each c-stem in the CIC can 
occur, not only bound to other c-suffixes in the 
CIC, but also as a free stem.  For generality, the 
boundary before the first character of each word 
form is also a candidate morpheme boundary. 
 Table 2 illustrates the type of CIC?s produced 
by our algorithm.  The CIC?s in this table, arranged 
in a systematic but arbitrary order, are each derived 
Vocabulary: blame
blames roams
blamed roamed
roaming
?.s.d
blame
?.s
blame.solve
?.d
blame
s.d s.ed.ing e.es.ing
blame roam solv
s.ed e.ing
roam solv
s s.ing es.ing
blame.roam.solve roam solv
d ed.ing ng
blame.roame roam roami.solvi
ing g
roam.solv roamin.solvin
lame
b
solves
solve
ame.ames.amed
bl
me.mes.med e.es.ed
solving
oams.oamed.oaming
bla blam r
me.mes e.es olve.olves.olvingame.ames
ame.amed
bla blam.solv sbl
me.med e.ed
...bla blam
bl
mes.med es.ed
bla blam
me e
bla blam.solv
amed
bl.ro
mes es
bla blam.solvbl
ames
med ed
bla.roa blam.roam
?
blame.blames.blamed.roams.roamed.roaming.solve.solves.solving
lame.lames.lamed
b
lame.lames
b
lame.lamed
b
lames.lamed
b
lames
b
lamed
b
bl
ames.amed
bl
ame
Table 2: Some of the CIC's, arranged in a systematic but arbitrary order, derived from a toy vo-
cabulary. Each entry is specified as a period delimited sequence of c-suffixes in bold above a   
period delimited sequence of adherent c-stems in italics 
from one or more forms in a small vocabulary con-
sisting of a subset of the word forms found under 
inflection class A from Table 1.  Proposing, as our 
procedure does, morpheme boundaries at every 
character boundary in every word form necessarily 
produces many ridiculous CIC?s, such as 
ame.ames.amed, from the forms blame, blames, 
and blamed and the c-stem bl.  Dispersed among 
the incorrect CIC?s generated by our algorithm, 
however, are also CIC?s that seem very reasonable, 
such as ?.s, from the c-stems blame and tease.   
Note that where Table 1 lists all the surface 
forms of the three lexemes BLAME, ROAM, and 
SOLVE, the vocabulary of Table 2 mimics the vo-
cabulary of a text corpus from a highly inflected 
language where we expect few, if any, lexemes to 
occur in the complete set of possible surface forms.  
Specifically, the vocabulary of Table 2 lacks the 
surface form blaming of the lexeme BLAME, solved 
of the lexeme SOLVE, and the root form roam of 
the lexeme ROAM.  Hence, while the reasonable 
CIC ?.s arises from the pairs of surface forms 
(blame, blames) and (solve, solves), there is no 
way for the form roams to contribute to the ?.s 
CIC because the surface form roam is missing 
from this vocabulary.  In other words, we lack evi-
dence for a ? suffix on the c-stem roam.  Also no-
tice that, as a result of English spelling rules, the 
CIC s.ed generated from the pair of surface forms 
(roams, roamed) is separate from each of the 
CIC?s s.d and es.ed generated from the pair of sur-
face forms (blames, blamed).   
Looking at Table 2, it is clear there is structure 
among the CIC?s.  In particular, at least two types 
of relations hold between CIC?s.  First, hierarchi-
cally, the c-suffixes of one CIC may be a superset 
of the c-suffixes of another CIC.  For example the 
c-suffixes in the CIC e.es.ed are a superset of the 
c-suffixes in the CIC e.ed.  Second, cutting across 
this hierarchical structure there is structure be-
tween CIC?s which propose different morpheme 
boundaries within the same word forms.  Compare 
the CIC?s me.mes.med and e.es.ed; each is de-
rived from exactly the triple of word forms blame, 
blames, and blamed, but differ in the placement of 
the hypothesized morpheme boundary.   
Taken together the hierarchical c-suffix set in-
clusion relations and the morpheme boundary rela-
tions impose a lattice structure on the space of 
CIC?s.  Figure 1 diagrams the CIC lattice over an 
interesting subset of the columns of Table 2.  Hier-
archical links, represented by solid lines, connect 
any given CIC often to more than one parent and 
more than one child.  The empty CIC (not pictured 
in Figure 1) can be considered the child of all level 
one CIC?s (including the ? CIC), but there is no 
universal parent of all top level CIC?s.  Moving up 
the lattice always results in a monotonic decrease 
in adherent size because a parent CIC requires 
each adherent c-stem to form a word with a super-
set of the c-suffixes of each child. 
Horizontal morpheme boundary links, dashed 
lines, connect a CIC, C, with a neighbor to the 
right if each c-suffix in C begins with the same 
character.  This entails that there is at most one 
morpheme boundary link leading to the right of 
each CIC.  There may, however, be as many links 
leading to the left as there are characters in the or-
thography.  The only CIC with depicted multiple 
left links in Figure 1 is ?, which has left links to 
the CIC?s e, s, and d.  A number of left links ema-
nating from the CIC?s in Figure 1 are not shown; 
among others absent from the figure is the left link 
from the CIC e.es leading to the CIC ve.ves with 
the adherent sol.  Since left links effectively divide 
a CIC into separate CIC?s, one for each character 
in the orthography, adherent count monotonically 
decreases as left links are followed. 
To better visualize what a CIC lattice looks like 
when derived from real data, Figure 2 contains a 
portion of a hierarchical lattice automatically gen-
erated from our Spanish newswire corpus.  Each 
entry in Figure 2 contains the c-suffixes compris-
ing the CIC, the adherent size of the CIC, and a 
sample of adherent c-stems.  The lattice in Figure 2 
covers: 
e.es 
blam 
solv 
e.ed 
blam 
es 
blam 
solv 
?.s.d 
blame 
?.s 
blame 
solve 
? 
blame 
blames 
blamed 
roams 
roamed 
roaming 
solve 
solves 
solving 
e.es.ed 
blam 
ed 
blam 
roam 
d 
blame 
roame 
?.d 
blame 
s.d 
blame 
s 
blame 
roam 
solve 
es.ed 
blam 
e 
blam 
solv 
me.mes 
bla 
me.med 
bla 
mes 
bla 
me.mes.med 
bla 
med 
bla 
roa 
mes.med 
bla 
me 
bla 
Figure 1: Portion of a CIC lattice from the 
toy vocabulary in Table 2 
c-suffix set inclusion links 
morpheme boundary links 
1) The productive Spanish inflection class for 
adjectives, a.as.o.os, covering the four adjec-
tive paradigm cells: feminine singular, femi-
nine plural, masculine singular, and mascu-
line plural, respectively, 
2) All possible CIC subsets of the adjective 
CIC, e.g. a.as.o, a.os, etc. and, 
3) The imposter CIC a.as.o.os.tro, together 
with its rogue descendents, a.tro, and tro.   
Other CIC?s that are descendents of a.as.o.os.tro 
and that contain the c-suffix tro do not supply ad-
ditional adherents and hence are not present either 
in Figure 2 or in our program?s representation of 
the CIC lattice.  The CIC?s a.as.tro and os.tro, for 
example, both have only the one adherent, cas, 
already possessed by their common ancestor 
a.as.o.os.tro.  Strictly speaking we have simplified 
for exposition, as the CIC a.as.o.os.tro is not actu-
ally present in the algorithm?s representation ei-
ther, because the c-stem cas occurred with a num-
ber of additional c-suffixes yielding the CIC: 
a.as.i.o.os.sandra.tanier.ter.tro.trol.  
4.2 Search 
Given the framework of CIC lattices, the key 
task for automatic morphology induction is to 
autonomously separate the nonsense CIC?s from 
the useful ones, thus identifying linguistically 
plausible inflection classes.  This section treats the 
CIC lattices as a hypothesis space of valid inflec-
tion classes and searches this space for CIC?s most 
likely to be true inflection classes in a language. 
There are many possible search strategies and 
heuristics applicable to the CIC lattice, and while 
for future work we intend to explore a variety of 
search techniques, this paper presents a reasonable 
and intuitive baseline search procedure.  We have 
investigated a series of algorithms which build 
upon each other.  Each algorithm employs a num-
ber of parameters which are tuned by hand.  These 
parameters are only interesting in so far as they 
help us find true CIC?s from among the many in 
the lattice.  The performance of each algorithm is 
described in section 6. 
4.2.1 Vertical Only 
 To motivate the general approach we have 
taken, compare the adherent sizes of the various 
CIC?s in Figure 2.  The target CIC a.as.o.os, corre-
sponding to the Spanish adjective inflection class, 
has 43 adherents.  Its various descendents must 
occur with monotonically increasing adherent 
sizes, but frequently a child will not more than 
double or triple its immediate parent?s adherent 
size, and never is there a difference greater than a 
factor of ten. Notice also the large adherent counts 
of the level one descendents of a.as.o.os, the 
smallest is as with 404 adherents.   
Contrast this behavior with that of CIC?s involv-
ing the spurious suffix tro.  The CIC a.as.o.os.tro 
occurs in the corpus with exactly one adherent, 
cas.  Additionally, the word forms cena, supper, 
and centro, center, occur yielding the CIC a.tro 
with two adherents.  In total tro is the final string 
of only 16 individual word forms. 
In general, we expect that true suffixes in a lan-
guage will both occur frequently and occur at-
tached to a large number of stems which also ac-
cept other suffixes from the same inflection class.  
These considerations led us to propose three pa-
rameters for our basic search strategy: 
L1 SIZE:  A level one adherent size cutoff 
TOP SIZE:  An absolute adherent size cutoff 
RATIO:  A parent-to-child adherent size      
ratio cutoff 
The L1 SIZE parameter requires a c-suffix to be 
frequent, while the TOP SIZE and RATIO parame-
ters require a suffix to be substitutable for other c-
suffixes in a reasonable number of c-stems. 
a.as.o.os 
43 
african 
cas 
jur?dic 
l 
... 
a.as.o.os.tro 
1 
cas 
a.as.os 
50 
afectad 
cas 
jur?dic 
l 
... 
a.as.o 
59 
cas 
citad 
jur?dic 
l 
... 
a.o.os 
105 
impuest 
indonesi 
italian 
jur?dic 
... 
a.as 
199 
huelg 
incluid 
industri 
inundad 
... 
a.os 
134 
impedid 
impuest 
indonesi 
inundad 
... 
as.os 
68 
cas 
implicad 
inundad 
jur?dic 
... 
a.o 
214 
id 
indi 
indonesi 
inmediat 
... 
as.o 
85 
intern 
jur?dic 
just 
l 
... 
a.tro 
2 
cas 
cen 
a 
1237 
huelg 
ib 
id 
iglesi 
... 
as 
404 
huelg 
huelguist 
incluid 
industri 
... 
os 
534 
humor?stic 
human 
h?gad 
impedid 
... 
o 
1139 
hub 
hug 
human 
huyend 
... 
tro 
16 
catas 
ce 
cen 
cua 
... 
as.o.os 
54 
cas 
implicad 
jur?dic 
l 
... 
 
Figure 2: Hierarchical CIC lattice automati-
cally derived from Spanish 
o.os 
268 
human 
implicad 
indici 
indocumentad 
... 
We apply these three parameters by beginning 
our search at the bottom of the lattice.  Each level 
one CIC with an adherent count larger than L1 
SIZE is placed in a list of path CIC?s.  Then for 
each path CIC, C, we remove C from the list of 
path CIC?s, and in turn consider each of C?s hier-
archical parents, Pi.  If Pi?s adherent size is at least 
TOP SIZE, and if the ratio of Pi?s adherent size to 
C?s adherent size is larger than RATIO, then Pi is 
placed in the list of path CIC?s.  If no parent of C 
can be placed in the list of path CIC?s, and if C?s 
level is greater than one, then C is placed in a list 
of selected CIC?s.  When there are no more CIC?s 
in the list of path CIC?s, the search ends and the 
CIC?s in the selected list are the CIC?s the algo-
rithm believes are true CIC?s of the language. 
As an illustration suppose we explored the lattice 
in Figure 2 with the following parameter settings: 
L1 SIZE:  100 
TOP SIZE:  2 
RATIO:  0.1 
Our search algorithm begins by comparing the 
adherent size of each level one CIC to L1 SIZE.  
The only level one CIC with an adherent count less 
than 100 is tro with 16 adherents, preventing tro 
from being placed in the list of path CIC?s.   
Each of the surviving level one CIC?s is then 
considered in turn.  The algorithm comes to the 
CIC a, where the ratios of adherent sizes between 
each of its parents a.tro, a.as, a.o, and a.os and 
itself are 0.002, 0.161, 0.173, and 0.108 respec-
tively.  Each of these ratios, except that between a 
and a.tro, at 0.002, is larger than 0.1.  And since 
the adherent sizes of a.as, a.o, and a.os are each 
larger than TOP SIZE, these three CIC?s are placed 
in the list of path CIC?s.   
From this point, every hierarchical link in Figure 
2 leading to the CIC a.as.o.os passes the TOP SIZE 
and RATIO cutoffs.  Thus the algorithm reaches a 
state where the only CIC in the list of path CIC?s is 
a.as.o.os.  When this good CIC is removed from 
the list of path CIC?s, the algorithm finds that its 
only parent is a.as.o.os.tro with its lone adherent.  
Since TOP SIZE requires a parent to have at least 
two adherents, a.as.o.os.tro cannot be placed in 
the list of path CIC?s.  As no parent can be placed 
in the list of path CIC?s, a.as.o.os is placed in the 
list of selected CIC?s?which is the desired out-
come.  The list of path CIC?s is now empty and the 
search ends. 
4.2.2 Horizontal Blocking 
 To improve performance over the Vertical Only 
algorithm we next incorporated knowledge from 
the horizontal morpheme boundary links.  Monson 
(2004) describes how morpheme boundary links in 
a CIC lattice can be thought of as branchings in a 
vocabulary trie where identical subtries are con-
flated.  Harris (1955) discusses how the branching 
count in a suffix trie can be exploited to identify 
morpheme boundaries.  We extend the spirit of 
Harris? work in our algorithm through the use of 
two search parameters: 
HORIZ RATIO: A cutoff over: 
sizeadherent 
character in  ending adherents of #
argmax cc  
HORIZ SIZE: An adherent size cutoff 
Left Blocking 
In the first variant of horizontal blocking we ap-
ply these two horizontal parameters when consid-
ering a CIC, C, removed from the list of path 
CIC?s.  If the adherent size of C is larger than 
HORIZ SIZE and the maximum percentage of ad-
herents of C that end in any one character is larger 
than HORIZ RATIO, then C is simply thrown out. 
For example, suppose we used the following 
horizontal parameter settings: 
HORIZ RATIO:  0.5 
HORIZ SIZE:  10 
 The CIC da.do in our Spanish corpus has 62 
adherents, 46, or a fraction of 0.742, of which end 
in the character a (ada and ado fill the feminine 
and masculine past participle cells for the -ar verb 
inflection class).  If our Left Blocking search algo-
rithm reached the CIC da.do, it would be dis-
carded because while its adherent size is larger 
than HORIZ SIZE more than half of its adherents 
end with the same character.  Notice that this algo-
rithm does not explicitly follow leftward mor-
pheme boundary links.  The rationale for this be-
havior is that ada.ado will likely be explored inde-
pendently by a separate vertical path.  In future 
experiments we intend to investigate the effect of 
ensuring that the CIC to the left is explored by 
overtly placing the leftward CIC in the list of path 
CIC?s. 
Right Blocking 
 So far we have only described an algorithm to 
block paths where the correct morpheme boundary 
is to the left of the current hypothesis.  There are 
also CIC?s where a morpheme boundary should be 
moved to the right. The CIC cada.cado with seven 
adherents is one such. 
Accordingly, whenever we encounter a CIC, C, 
all of whose c-suffixes begin with the same charac-
ter (e.g. c in cada.cado) our algorithm poses the 
question, if we were considering the CIC to the 
right (e.g. ada.ado) would we have triggered Left 
Blocking?  If Left Blocking would not have been 
triggered then we throw C out.  In other words, we 
prefer the rightmost possible morpheme boundary, 
unless there is some reason to believe the mor-
pheme boundary should be to the left. 
Taking a closer look at cada.cado, the CIC to its 
right, ada.ado, has 46 adherents of which the char-
acter c ends the most, 7 or a fraction of 0.152.  If 
we were using a HORIZ RATIO of 0.5 as in the pre-
vious section, Left Blocking would not be trig-
gered from ada.ado and so Right Blocking is trig-
gered, throwing out cada.cado.  On the other hand, 
if we were considering blocking ada.ado, where 
both c-suffixes begin with a, the HORIZ RATIO pa-
rameter would need to be larger than 0.742 before 
right blocking would throw out ada.ado.   
Right Blocking Recursive 
 In addition to standard Right Blocking we ex-
plored recursively looking at the next most right 
neighbor of a CIC if the immediate right neighbor 
falls below the HORIZ SIZE threshold.  The ration-
ale behind this variant stems from CIC?s such as 
icada.icado with 4 adherents, crit, publ, ratif, and 
ub.  Since icada.icado?s immediate right neighbor 
cada.cado has only 7 adherents itself we may not 
want to base our blocking decision on so little data.  
Instead we consider the CIC ada.ado, discussed in 
the previous section, which has a large enough ad-
herent size that we might feel confident in our 
judgment.   
Full Horizontal Blocking 
The final version of the search we tried was to 
combine Left Blocking and Right Blocking Recur-
sive while constraining both to use the same values 
for the HORIZ RATIO and HORIZ SIZE parameters. 
5 Evaluation 
To evaluate the performance of the various base-
line search strategies, we first decided on a stan-
dard set of six inflection classes for Spanish: two 
for nouns, ?.s and ?.es, one for adjectives, 
a.as.o.os, and three for verbs, corresponding to the 
traditional -ar, -er, and -ir verb inflection classes.  
We call these six inflection classes our set of stan-
dard IC?s.  We make no claim as to the truth or 
completeness of the set of standard inflection 
classes we used in this evaluation.  The standard 
IC?s we compiled were simply some of the most 
common suffixes filling some of the most common 
morphosyntactic properties marked in Spanish. 
We then defined measures of recall, precision, 
and fragmentation over these standard IC?s (Figure 
3).  As defined, recall measures the fraction of 
unique suffixes in the standard IC?s that are found 
within those selected CIC?s that are subsets of 
some inflection class in the standard; precision 
measures the fraction of unique suffixes among all 
the selected CIC?s that are found within those se-
lected CIC?s that are subsets of an inflection class 
in the standard; and fragmentation measures re-
dundancy, specifically calculating the ratio of the 
number of selected CIC?s that are subsets of stan-
dard IC?s to the number of inflection classes in the 
standard.  High values for recall and precision are 
desirable, while a fragmentation of exactly 1 im-
plies that the number of usefully selected CIC?s is 
the same as the number of inflection classes in the 
standard. 
6 Results and Error Analysis 
For each of the search variants described in sec-
tion 4.2 we executed a by-hand search over the 
relevant parameters for those settings that optimize 
the F1 measure (the harmonic mean of recall and 
precision).  The best performing parameter settings 
are presented in Table 3 while quantitative results 
using these settings are plotted in Figure 4.   
Examining the performance of each algorithm 
(Figure 4) reveals that the simple Vertical only 
search achieves a high precision at the expense of a 
low recall measure.  The simple Vertical search 
also gives the smallest fragmentation, which, when 
combined with the high precision score, indicates a 
conservative algorithm that selects few CIC?s.  The 
parameter settings which achieve the highest F1 for 
Left Block alone and Right Block alone each pro-
duce much higher recall than the simple Vertical 
search.  Right Block Recursive increases precision 
significantly over simple Right Block and achieves 
U
U
sIC' standard 
sIC' standard 
sCIC' selected 
 of elements
 if  of elements
Recall
?
?
?
?
=
I
I
C
I
ICC
U
U
sCIC' selected 
sIC' standard 
sCIC' selected 
 of elements
 if  of elements
Precision
?
?
?
?
=
C
I
C
C
ICC
 
 {
sIC' standard
ionFragmentat sIC' standard 
sCIC' selected 
 if 1 
 if 0 ?
?
?
?
?
=
I
C
IC
IC
 
Figure 3: Three performance measures to   
optimize 
the highest F1 measure of any search variant.  
While Full Horizontal Block also performs well, 
sharing the values of HORIZ RATIO and HORIZ 
SIZE forced a compromise between Left Block and 
Right Block Recursive that did not significantly 
outperform either algorithm alone. 
Of the 83 unique suffixes in the hand compiled 
standard inflection classes, 21 did not share a c-
stem with any other c-suffix in the Spanish news-
wire corpus used for this evaluation?placing an 
upper limit on recall of 0.75 for the search algo-
rithms presented in this paper. 
Examining the parameter settings that yielded 
the highest F1 measure for each search variant 
(Table 3) is also enlightening.  Early experiments 
with Vertical only search clearly demonstrated that 
a TOP SIZE of two, or restricting the CIC?s permit-
ted to be selected to those with at least two adher-
ents, always resulted in better performance than 
other possible settings.  A TOP SIZE of one places 
no restriction on the adherent size of a CIC, ram-
pantly selecting CIC?s, such as the level 10 CIC 
given at the end of section 4.1, that consist of 
many c-suffixes that happen to validly concatenate 
onto a single c-stem?obliterating reasonable pre-
cision.  Higher settings for TOP SIZE induce a 
graceful degradation in recall.  Thus all experi-
ments reported here used a TOP SIZE of two. 
Beyond TOP SIZE the only parameters available 
to the basic Vertical algorithm are L1 SIZE and 
RATIO, which provide only crude means to halt the 
search of bad paths.  In particular, if a level one 
CIC, C, has more than L1 SIZE adherents, and has 
some parent which passes the RATIO cutoff, then 
some ancestor of C will be selected by the algo-
rithm as a good CIC.  Hence, the Vertical only al-
gorithm ensures search gets off on the right foot by 
using the highest values for the L1 SIZE and RATIO 
parameters of any algorithm variant.  Performance 
falls off quickly above L1 SIZE settings of 192, 
indicating that this parameter in this algorithm is 
sensitive to the size of the training corpus. 
In contrast, the horizontal blocking search algo-
rithms have additional parameters available to cull 
out bad search paths, and can hence afford to use 
lower (and more stable) values for L1 SIZE and 
RATIO.  Recall that the Left Blocking algorithm 
discards paths determined to be using a morpheme 
boundary too far to the right, while the Right 
Blocking algorithm discards paths using mor-
pheme boundaries too far to the left.  Notice that 
since, as reasoned in section 4.1, adherent count 
monotonically decreases as morpheme boundary 
links are followed to the left, if the L1 SIZE cutoff 
blocks a particular CIC, C, all CIC?s to the left of 
C will also be blocked.  From these facts it follows 
that a large L1 SIZE will reject some paths result-
ing from morpheme boundaries chosen too far to 
the left, which would otherwise have been pursued 
in the Left Blocking algorithm.  The Right Block-
ing algorithm, however, receives no such benefit, 
and achieves its best performance by maximizing 
recall with a small L1 SIZE. 
Examining the best performing parameter values 
for the Right Blocking Recursive algorithm reveals 
a curious behavior in which low values for L1 SIZE 
and RATIO allow a permissive vertical search while 
stringent values of HORIZ RATIO and, particularly, 
HORIZ SIZE constrain the search.  One explanation 
for these facts might be that following the mono-
tonically increasing chain of CIC adherent sizes 
along right horizontal links allows the algorithm to 
Figure 4: Recall, Precision, F1 and Fragmen-
tation Results for each search algorithm:     
Vertical, Left Blocking, Right Blocking,   
Right Blocking Recursive, and                     
Full Horizontal Blocking 
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
1.
0
V
LB
RB
RBR
FHB
A
lg
o
ri
th
m
Recall/Precision/F-Measure
0 3 6 9 12 15 18 21 24 27 30
Fragmentation
Recall Precision
F-Measure Fragmentation
Table 3: Hand tuned optimal parameter set-
tings for each search algorithm:                       
Vertical, Left Blocking, Right Blocking,       
Right Blocking Recursive, and                      
Full Horizontal Blocking 
Algorithm TOP SIZE 
L1 
SIZE RATIO 
HORIZ 
RATIO 
HORIZ 
SIZE 
V 2 192 0.3   
LB 2 64 0.1 0.3 3 
RB 2 27 0.2 0.5 27 
RBR 2 27 0.05 0.5 243 
FHB 2 27 0.2 0.3 3 
 
make intelligent blocking decisions backed by suf-
ficient data. 
The best performing parameter values for the 
Full Horizontal Search are a compromise between 
the well performing values for the Left Blocking 
and those for the Right Blocking algorithms.  This 
parameter value compromise does not draw benefit 
from the recursion in the Right Block Recursive 
algorithm, but instead employs Right Block as a 
replacement for the relatively higher L1 SIZE pa-
rameter in the Left Blocking algorithm. 
It is also interesting to examine CIC?s selected 
by the search algorithms.  Table 4 lists all of the 
CIC?s selected by the conservative Vertical search 
algorithm together with a random sample of CIC?s 
selected by Right Blocking Recursive, the algo-
rithm which reached the highest F1 measure of any 
algorithm variant.   
Perhaps the most striking feature of Table 4 is 
the extent to which the CIC?s overlap.  Very few 
individual c-suffixes occur in only one CIC.  Of all 
the CIC?s in Table 4, only ?.s and a.as.o.os, both 
among the CIC?s selected by the Vertical algo-
rithm, represent complete inflection classes in the 
standard IC?s.  The remaining CIC?s are proper 
subsets of various verbal inflection classes.  The 
overlapping nature of the selected CIC?s suggests 
an additional step, which we do not investigate 
here, of conflating CIC?s into a fewer number of 
meta-CIC?s. 
The only verbal inflection class for which sub-
sets are able to pass the large L1 SIZE cutoff im-
posed by the Vertical search algorithm is -ar, the 
most frequent of the three major inflection classes 
in Spanish.  The Right Blocking Recursive algo-
rithm on the other hand identifies significant por-
tions of all three verbal inflection classes.  
The c-suffixes appearing in italics in Table 4 
correspond to no suffix found in any standard IC.  
These alien c-suffixes fall into two categories. 
1) The c-suffixes aciones, aci?n, and adores 
are noun forming derivational suffixes.   
2) The remaining c-suffixes were formed by 
choosing a morpheme boundary too far to 
the right.   
It is the second type of mistake that the Left 
Blocking search algorithm was specifically de-
signed to address.  Unfortunately na?vely combin-
ing the Right Blocking Recursive with the Left 
Blocking algorithm did not improve performance.  
We expect that by using separate horizontal pa-
Vertical
ar er ir 23 of 23 Selected CIC's
? ?.s
? a.aba.ada.adas.ado.ar.as
? a.aba.ada.ado.ando.ar
? a.aba.ada.ado.ar.ar?.en.?
a.aciones.aci?n .ada.adas.ar.aron
? a.ada.adas.ado.ar.ar?
? a.ada.adas.ar.aron.?
? a.ada.ado.ar.aron.ar?.?
? a.ada.ado.ar.ar?.ar?n.en.?
? a.ada.ado.ar.o.?
? a.ada.ados.ar.aron.?
? a.ado.ar.ara.aron.e.?
? a.ado.ar.aron.?
? a.an.ar.?
? a.as.o.os
? ? ? ? a.as
? aba.ado.ando.ar.aron.ar?
? aba.ado.ar.aron.ar?.en
? ada.ado.ados.ar.aron.?
? ada.ado.ando.ar.aron.?
? ada.ado.ar.ar?.o.?
? ada.ado.ar.en.o.?
? ado.ar.aron.ar?.ar?n.en
N A
dj Verbs
 
Table 4: All of the CIC?s selected by the conservative Vertical search algorithm (left), and a random 
sample of CIC?s selected by the algorithm with best F1 measure, Right Blocking Recursive (right).  For 
each CIC row, a dot is placed in the columns representing standard IC?s for which that CIC is a subset.  
The c-suffixes in italics are in no standard IC. 
Right Blocking Recursive
ar er ir 23 of 204 Selected CIC's
?.ba.n.ndo
? a.aba.ado.ados.ar.ar?.ar?n
a.aciones.aci?n .adas.ado.ar
? a.ada.adas.ado.ar.ar?
? a.adas.ado.an.ar
? a.ado.ados.ar.?
? a.ado.an.arse.?
? a.ado.aron.arse.?
? aba.ada.ado.ar.o.os
aciones.aci?n .ado.ados
aciones .ado.ados.ar?
aci?n .ado.an.e
? ada.adas.ado.ados.aron.?
? ada.ado.ados.ar.o
ado.adores .o
? ado.ados.arse.e
? ado.ar.aron.arse.ar?
do.dos.ndo.r.ron
? ? e.ida.ido
? emos.ido.?a.?an
? ida.ido.idos.ir.i?
? ido.iendo.ir
? ido.ir.ro
N A
dj Verbs
 
rameters for left blocking and for right blocking 
we could combine these two algorithms in a less 
constrained fashion that would result in better 
overall performance. 
7 Future Work  
We believe the heuristic search strategy de-
scribed in this paper can be significantly improved 
upon.  We plan to investigate search strategies for 
both the vertical and horizontal links in our CIC 
lattices.  We currently have plans to employ statis-
tical independence and correlation tests to adjacent 
CIC?s as a guide to search (Monson, 2004).  Other 
search criteria we are considering are information 
gain and minimum description length measures. 
There are also modifications to the search strat-
egy that may significantly improve performance.  
For example, it may be advantageous to actively 
follow horizontal morpheme boundary links, in-
stead of merely blocking paths, when a morpheme 
boundary error is discovered.  The next immediate 
step we will take is to scale our implementation to 
investigate performance changes as we increase 
the size of our Spanish corpus. 
The intention of this work is to produce a lan-
guage independent morphology induction algo-
rithm.  Hence, we plan to apply this work to a vari-
ety of languages, both well studied resource-rich 
languages as well as low-density languages of in-
terest to the AVENUE project. 
8 Acknowledgements 
The research reported in this paper was funded 
in part by NSF grant number IIS-0121631. 
References 
Jaime Carbonell, Katharina Probst, Erik Peterson, 
Christian Monson, Alon Lavie, Ralf Brown, and 
Lori Levin.  2002. Automatic Rule Learning for 
Resource-Limited MT. In Proceedings of the 5th 
Conference of the Association for Machine 
Translation in the Americas (AMTA-02). 
Andrew Carstairs-McCarthy. 1998. ?Paradigmatic 
Structure: Inflectional Paradigms and Morpho-
logical Classes.? The Handbook of Morphology. 
Eds. Andrew Spencer and Arnold M. Zwicky. 
Blackwell Publishers Inc., Massachusetts, USA, 
322-334. 
?ric Gaussier. 1999. Unsupervised learning of 
derivational morphology from inflectional lexi-
cons. In Proceedings of ACL ?99 Workshop: Un-
supervised Learning in Natural Language Proc-
essing. 
John Goldsmith. 2001. Unsupervised learning of 
the morphology of a natural language. Computa-
tional Linguistics, 27(2): 153-198. 
Margaret A. Hafer and Stephen F. Weiss. 1974. 
Word segmentation by letter successor varieties. 
Information Storage and Retrieval, 10:371-385. 
Zellig Harris. 1955. From phoneme to morpheme. 
Language, 31:190-222. Reprinted in Harris 
1970. 
Zellig Harris. 1967. Morpheme boundaries within 
words: Report on a computer test. Transforma-
tion and Discourse Analysis Papers 73, Depart-
ment of Linguistics, University of Pennsylvania. 
Reprinted in Harris 1970. 
Zellig Harris. 1970. Papers in Structural and 
Transformational Linguistics. D. Reidel, 
Dordrecht, Holland. 
Alon Lavie, Stephan Vogel, Lori Levin, Erik Pe-
terson, Katharina Probst, Ariadna Font Llitj?s, 
Rachel Reynolds, Jaime Carbonell, and Richard 
Cohen. 2003. Experiments with a Hindi-to-
English Transfer-based MT System under a Mis-
erly Data Scenario. ACM Transactions on Asian 
Language Information Processing (TALIP), to 
appear in 2(2). 
Christian Monson. 2004. A Framework for Unsu-
pervised Natural Language Morphology Induc-
tion.  In Proceedings of the Student Workshop at 
ACL-04. 
Katharina Probst, Lori Levin, Erik Peterson, Alon 
Lavie, and Jaime Carbonell. 2002. MT for Re-
source-Poor Languages using Elicitation-based 
Learning of Syntactic Transfer Rules. Machine 
Translation, Special Issue on Embedded MT, 
17(4): 245-270. 
Patrick Schone and Daniel Jurafsky. 2000. Knowl-
edge-free Induction of Morphology Using Latent 
Semantic Analysis. In Proceedings of the Fourth 
Conference on Computational Natural Language 
Learning and of the Second Learning Language 
in Logic Workshop, 67-72. 
Patrick Schone and Daniel Jurafsky. 2001. Knowl-
edge-free Induction of Inflectional Morpholo-
gies. In Proceedings of the North American 
Chapter of the Association of Computational 
Linguistics. 183-191. 
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis 
tools via robust projection across aligned cor-
pora. In Proceedings of the Human Language 
Technology Conference, 161-168. 
 
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
METEOR: An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments 
 
 
Satanjeev Banerjee Alon Lavie 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213 Pittsburgh, PA 15213 
banerjee+@cs.cmu.edu alavie@cs.cmu.edu 
Abstract 
We describe METEOR, an automatic 
metric for machine translation evaluation 
that is based on a generalized concept of 
unigram matching between the machine-
produced translation and human-produced 
reference translations. Unigrams can be 
matched based on their surface forms, 
stemmed forms, and meanings; further-
more, METEOR can be easily extended to 
include more advanced matching strate-
gies.  Once all generalized unigram 
matches between the two strings have 
been found, METEOR computes a score 
for this matching using a combination of 
unigram-precision, unigram-recall, and a 
measure of fragmentation that is designed 
to directly capture how well-ordered the 
matched words in the machine translation 
are in relation to the reference.  We 
evaluate METEOR by measuring the cor-
relation between the metric scores and 
human judgments of translation quality.  
We compute the Pearson R correlation 
value between its scores and human qual-
ity assessments of the LDC TIDES 2003 
Arabic-to-English and Chinese-to-English 
datasets.  We perform segment-by-
segment correlation, and show that 
METEOR gets an R correlation value of 
0.347 on the Arabic data and 0.331 on the 
Chinese data.  This is shown to be an im-
provement on using simply unigram-
precision, unigram-recall and their har-
monic F1 combination. We also perform 
experiments to show the relative contribu-
tions of the various mapping modules. 
 
1 Introduction 
Automatic Metrics for machine translation (MT) 
evaluation have been receiving significant atten-
tion in the past two years, since IBM's BLEU met-
ric was proposed and made available (Papineni et 
al 2002).  BLEU and the closely related NIST met-
ric (Doddington, 2002) have been extensively used 
for comparative evaluation of the various MT sys-
tems developed under the DARPA TIDES research 
program, as well as by other MT researchers.  The 
utility and attractiveness of automatic metrics for 
MT evaluation has consequently been widely rec-
ognized by the MT community.  Evaluating an MT 
system using such automatic metrics is much 
faster, easier and cheaper compared to human 
evaluations, which require trained bilingual evalua-
tors.  In addition to their utility for comparing the 
performance of different systems on a common 
translation task, automatic metrics can be applied 
on a frequent and ongoing basis during system de-
velopment, in order to guide the development of 
the system based on concrete performance im-
provements. 
Evaluation of Machine Translation has tradi-
tionally been performed by humans.  While the 
main criteria that should be taken into account in 
assessing the quality of MT output are fairly intui-
tive and well established, the overall task of MT 
evaluation is both complex and task dependent.  
MT evaluation has consequently been an area of 
significant research in itself over the years.  A wide 
range of assessment measures have been proposed, 
not all of which are easily quantifiable. Recently 
developed frameworks, such as FEMTI (King et al 
2003), are attempting to devise effective platforms 
for combining multi-faceted measures for MT 
evaluation in effective and user-adjustable ways.  
While a single one-dimensional numeric metric 
cannot hope to fully capture all aspects of MT 
65
evaluation, such metrics are still of great value and 
utility.  
In order to be both effective and useful, an 
automatic metric for MT evaluation has to satisfy 
several basic criteria.  The primary and most intui-
tive requirement is that the metric have very high 
correlation with quantified human notions of MT 
quality.  Furthermore, a good metric should be as 
sensitive as possible to differences in MT quality 
between different systems, and between different 
versions of the same system.  The metric should be 
consistent (same MT system on similar texts 
should produce similar scores), reliable (MT sys-
tems that score similarly can be trusted to perform 
similarly) and general (applicable to different MT 
tasks in a wide range of domains and scenarios).  
Needless to say, satisfying all of the above criteria 
is extremely difficult, and all of the metrics that 
have been proposed so far fall short of adequately 
addressing most if not all of these requirements.  
Nevertheless, when appropriately quantified and 
converted into concrete test measures, such re-
quirements can set an overall standard by which 
different MT evaluation metrics can be compared 
and evaluated.  
In this paper, we describe METEOR1, an auto-
matic metric for MT evaluation which we have 
been developing.  METEOR was designed to ex-
plicitly address several observed weaknesses in 
IBM's BLEU metric.   It is based on an explicit 
word-to-word matching between the MT output 
being evaluated and one or more reference transla-
tions.  Our current matching supports not only 
matching between words that are identical in the 
two strings being compared, but can also match 
words that are simple morphological variants of 
each other (i.e. they have an identical stem), and 
words that are synonyms of each other.  We envi-
sion ways in which this strict matching can be fur-
ther expanded in the future, and describe these at 
the end of the paper.  Each possible matching is 
scored based on a combination of several features.  
These currently include unigram-precision, uni-
gram-recall, and a direct measure of how out-of-
order the words of the MT output are with respect 
to the reference.   The score assigned to each indi-
vidual sentence of MT output is derived from the 
best scoring match among all matches over all ref-
erence translations.  The maximal-scoring match-
                                                           
1 METEOR: Metric for Evaluation of Translation with Explicit ORdering 
ing is then also used in order to calculate an aggre-
gate score for the MT system over the entire test 
set.  Section 2 describes the metric in detail, and 
provides a full example of the matching and scor-
ing.  
In previous work (Lavie et al, 2004), we com-
pared METEOR with IBM's BLEU metric and it?s 
derived NIST metric, using several empirical 
evaluation methods that have been proposed in the 
recent literature as concrete means to assess the 
level of correlation of automatic metrics and hu-
man judgments.  We demonstrated that METEOR 
has significantly improved correlation with human 
judgments.  Furthermore, our results demonstrated 
that recall plays a more important role than preci-
sion in obtaining high-levels of correlation with 
human judgments.  The previous analysis focused 
on correlation with human judgments at the system 
level.  In this paper, we focus our attention on im-
proving correlation between METEOR score and 
human judgments at the segment level. High-levels 
of correlation at the segment level are important 
because they are likely to yield a metric that is sen-
sitive to minor differences between systems and to 
minor differences between different versions of the 
same system.  Furthermore, current levels of corre-
lation at the sentence level are still rather low, of-
fering a very significant space for improvement.  
The results reported in this paper demonstrate that 
all of the individual components included within 
METEOR contribute to improved correlation with 
human judgments.  In particular, METEOR is 
shown to have statistically significant better corre-
lation compared to unigram-precision, unigram-
recall and the harmonic F1 combination of the two. 
We are currently in the process of exploring 
several further enhancements to the current 
METEOR metric, which we believe have the po-
tential to significantly further improve the sensitiv-
ity of the metric and its level of correlation with 
human judgments.  Our work on these directions is 
described in further detail in Section 4. 
 
2 The METEOR Metric 
2.1 Weaknesses in BLEU Addressed in 
METEOR 
The main principle behind IBM?s BLEU metric 
(Papineni et al 2002) is the measurement of the 
66
overlap in unigrams (single words) and higher or-
der n-grams of words, between a translation being 
evaluated and a set of one or more reference trans-
lations.  The main component of BLEU is n-gram 
precision: the proportion of the matched n-grams 
out of the total number of n-grams in the evaluated 
translation.  Precision is calculated separately for 
each n-gram order, and the precisions are com-
bined via a geometric averaging.  BLEU does not 
take recall into account directly.  Recall ? the pro-
portion of the matched n-grams out of the total 
number of n-grams in the reference translation, is 
extremely important for assessing the quality of 
MT output, as it reflects to what degree the transla-
tion covers the entire content of the translated sen-
tence.  BLEU does not use recall because the 
notion of recall is unclear when matching simulta-
neously against a set of reference translations 
(rather than a single reference).  To compensate for 
recall, BLEU uses a Brevity Penalty, which penal-
izes translations for being ?too short?.  The NIST 
metric is conceptually similar to BLEU in most 
aspects, including the weaknesses discussed below. 
BLEU and NIST suffer from several weak-
nesses, which we attempt to address explicitly in 
our proposed METEOR metric: 
The Lack of Recall:  We believe that the fixed 
brevity penalty in BLEU does not adequately com-
pensate for the lack of recall.  Our experimental 
results strongly support this claim. 
Use of Higher Order N-grams: Higher order 
N-grams are used in BLEU as an indirect measure 
of a translation?s level of grammatical well-
formedness.  We believe an explicit measure for 
the level of grammaticality (or word order) can 
better account for the importance of grammatical-
ity as a factor in the MT metric, and result in better 
correlation with human judgments of translation 
quality. 
Lack of Explicit Word-matching Between 
Translation and Reference:  N-gram counts don?t 
require an explicit word-to-word matching, but this 
can result in counting incorrect ?matches?, particu-
larly for common function words. 
Use of Geometric Averaging of N-grams: 
Geometric averaging results in a score of ?zero? 
whenever one of the component n-gram scores is 
zero.  Consequently, BLEU scores at the sentence 
(or segment) level can be meaningless.  Although 
BLEU was intended to be used only for aggregate 
counts over an entire test-set (and not at the sen-
tence level), scores at the sentence level can be 
useful indicators of the quality of the metric.  In 
experiments we conducted, a modified version of 
BLEU that uses equal-weight arithmetic averaging 
of n-gram scores was found to have better correla-
tion with human judgments. 
2.2 The METEOR Metric 
METEOR was designed to explicitly address the 
weaknesses in BLEU identified above.  It evaluates 
a translation by computing a score based on ex-
plicit word-to-word matches between the transla-
tion and a reference translation. If more than one 
reference translation is available, the given transla-
tion is scored against each reference independ-
ently, and the best score is reported. This is 
discussed in more detail later in this section.   
Given a pair of translations to be compared (a 
system translation and a reference translation), 
METEOR creates an alignment between the two 
strings. We define an alignment as a mapping be-
tween unigrams, such that every unigram in each 
string maps to zero or one unigram in the other 
string, and to no unigrams in the same string. Thus 
in a given alignment, a single unigram in one string 
cannot map to more than one unigram in the other 
string. This alignment is incrementally produced 
through a series of stages, each stage consisting of 
two distinct phases. 
In the first phase an external module lists all the 
possible unigram mappings between the two 
strings. Thus, for example, if the word ?computer? 
occurs once in the system translation and twice in 
the reference translation, the external module lists 
two possible unigram mappings, one mapping the 
occurrence of ?computer? in the system translation 
to the first occurrence of ?computer? in the refer-
ence translation, and another mapping it to the sec-
ond occurrence. Different modules map unigrams 
based on different criteria. The ?exact? module 
maps two unigrams if they are exactly the same 
(e.g. ?computers? maps to ?computers? but not 
?computer?). The ?porter stem? module maps two 
unigrams if they are the same after they are 
stemmed using the Porter stemmer (e.g.: ?com-
puters? maps to both ?computers? and to ?com-
puter?). The ?WN synonymy? module maps two 
unigrams if they are synonyms of each other.  
In the second phase of each stage, the largest 
subset of these unigram mappings is selected such 
67
that the resulting set constitutes an alignment as 
defined above (that is, each unigram must map to 
at most one unigram in the other string). If more 
than one subset constitutes an alignment, and also 
has the same cardinality as the largest set, 
METEOR selects that set that has the least number 
of unigram mapping crosses. Intuitively, if the two 
strings are typed out on two rows one above the 
other, and lines are drawn connecting unigrams 
that are mapped to each other, each line crossing is 
counted as a ?unigram mapping cross?. Formally, 
two unigram mappings (ti, rj) and (tk, rl) (where ti 
and tk are unigrams in the system translation 
mapped to unigrams rj and rl in the reference trans-
lation respectively) are said to cross if and only if 
the following formula evaluates to a negative 
number:  
(pos(ti) ? pos(tk)) * (pos(rj) ? pos(rl)) 
where pos(tx) is the numeric position of the uni-
gram tx in the system translation string, and pos(ry) 
is the numeric position of the unigram ry in the ref-
erence string. For a given alignment, every pair of 
unigram mappings is evaluated as a cross or not, 
and the alignment with the least total crosses is 
selected in this second phase. Note that these two 
phases together constitute a variation of the algo-
rithm presented in (Turian et al 2003). 
Each stage only maps unigrams that have not 
been mapped to any unigram in any of the preced-
ing stages. Thus the order in which the stages are 
run imposes different priorities on the mapping 
modules employed by the different stages. That is, 
if the first stage employs the ?exact? mapping 
module and the second stage employs the ?porter 
stem? module, METEOR is effectively preferring 
to first map two unigrams based on their surface 
forms, and performing the stemming only if the 
surface forms do not match (or if the mapping 
based on surface forms was too ?costly? in terms 
of the total number of crosses). Note that 
METEOR is flexible in terms of the number of 
stages, the actual external mapping module used 
for each stage, and the order in which the stages 
are run. By default the first stage uses the ?exact? 
mapping module, the second the ?porter stem? 
module and the third the ?WN synonymy? module.  
In section 4 we evaluate each of these configura-
tions of METEOR.  
Once all the stages have been run and a final 
alignment has been produced between the system 
translation and the reference translation, the 
METEOR score for this pair of translations is 
computed as follows.  First unigram precision (P) 
is computed as the ratio of the number of unigrams 
in the system translation that are mapped (to uni-
grams in the reference translation) to the total num-
ber of unigrams in the system translation. 
Similarly, unigram recall (R) is computed as the 
ratio of the number of unigrams in the system 
translation that are mapped (to unigrams in the ref-
erence translation) to the total number of unigrams 
in the reference translation. Next we compute 
Fmean by combining the precision and recall via a 
harmonic-mean (van Rijsbergen, 1979) that places 
most of the weight on recall.  We use a harmonic 
mean of P and 9R.  The resulting formula used is: 
PR
PRFmean
9
10
+=  
Precision, recall and Fmean are based on uni-
gram matches. To take into account longer 
matches, METEOR computes a penalty for a given 
alignment as follows. First, all the unigrams in the 
system translation that are mapped to unigrams in 
the reference translation are grouped into the few-
est possible number of chunks such that the uni-
grams in each chunk are in adjacent positions in 
the system translation, and are also mapped to uni-
grams that are in adjacent positions in the reference 
translation. Thus, the longer the n-grams, the fewer 
the chunks, and in the extreme case where the en-
tire system translation string matches the reference 
translation there is only one chunk. In the other 
extreme, if there are no bigram or longer matches, 
there are as many chunks as there are unigram 
matches. The penalty is then computed through the 
following formula: 
3
_#
#*5.0 ???
?
???
?=
matchedunigrams
chunksPenalty  
For example, if the system translation was ?the 
president spoke to the audience? and the reference 
translation was ?the president then spoke to the 
audience?, there are two chunks: ?the president? 
and ?spoke to the audience?. Observe that the pen-
alty increases as the number of chunks increases to 
a maximum of 0.5. As the number of chunks goes 
to 1, penalty decreases, and its lower bound is de-
cided by the number of unigrams matched. The 
parameters if this penalty function were deter-
mined based on some experimentation with de-
68
veopment data, but have not yet been trained to be 
optimal. 
Finally, the METEOR Score for the given 
alignment is computed as follows:  
 
)1(* PenaltyFmeanScore ?=  
 
This has the effect of reducing the Fmean by the 
maximum of 50% if there are no bigram or longer 
matches. 
For a single system translation, METEOR com-
putes the above score for each reference transla-
tion, and then reports the best score as the score for 
the translation. The overall METEOR score for a 
system is calculated based on aggregate statistics 
accumulated over the entire test set, similarly to 
the way this is done in BLEU.  We calculate ag-
gregate precision, aggregate recall, an aggregate 
penalty, and then combine them using the same 
formula used for scoring individual segments. 
3 Evaluation of the METEOR Metric 
3.1. Data 
We evaluated the METEOR metric and compared 
its performance with BLEU and NIST on the 
DARPA/TIDES 2003 Arabic-to-English and Chi-
nese-to-English MT evaluation data released 
through the LDC as a part of the workshop on In-
trinsic and Extrinsic Evaluation Measures for MT 
and/or Summarization, at the Annual Meeting of 
the Association of Computational Linguistics 
(2005). The Chinese data set consists of 920 sen-
tences, while the Arabic data set consists of 664 
sentences. Each sentence has four reference trans-
lations.  Furthermore, for 7 systems on the Chinese 
data and 6 on the Arabic data, every sentence 
translation has been assessed by two separate hu-
man judges and assigned an Adequacy and a Flu-
ency Score.  Each such score ranges from one to 
five (with one being the poorest grade and five the 
highest).  For this paper, we computed a Combined 
Score for each translation by averaging the ade-
quacy and fluency scores of the two judges for that 
translation.  We also computed an average System 
Score for each translation system by averaging the 
Combined Score for all the translations produced 
by that system. (Note that although we refer to 
these data sets as the ?Chinese? and the ?Arabic? 
data sets, the MT evaluation systems analyzed in 
this paper only evaluate English sentences pro-
duced by translation systems by comparing them to 
English reference sentences). 
3.2 Comparison with BLEU and NIST MT 
Evaluation Algorithms  
In this paper, we are interested in evaluating 
METEOR as a metric that can evaluate translations 
on a sentence-by-sentence basis, rather than on a 
coarse grained system-by-system basis. The stan-
dard metrics ? BLEU and NIST ? were however 
designed for system level scoring, hence comput-
ing sentence level scores using BLEU or the NIST 
evaluation mechanism is unfair to those algo-
rithms. To provide a point of comparison however, 
table 1 shows the system level correlation between 
human judgments and various MT evaluation algo-
rithms and sub components of METEOR over the 
Chinese portion of the Tides 2003 dataset. Specifi-
cally, these correlation figures were obtained as 
follows: Using each algorithm we computed one 
score per Chinese system by calculating the aggre-
gate scores produced by that algorithm for that sys-
tem. We also obtained the overall human judgment 
for each system by averaging all the human scores 
for that system?s translations. We then computed 
the Pearson correlation between these system level 
human judgments and the system level scores for 
each algorithm; these numbers are presented in 
table 1.  
 
System ID Correlation 
BLEU 0.817 
NIST 0.892 
Precision 0.752 
Recall 0.941 
F1 0.948 
Fmean 0.952 
METEOR 0.964 
 
Table 1: Comparison of human/METEOR correlation 
with BLEU and NIST/human correlations 
 
Observe that simply using Recall as the MT 
evaluation metric results in a significant improve-
ment in correlation with human judgment over 
both the BLEU and the NIST algorithms. These 
correlations further improve slightly when preci-
sion is taken into account (in the F1 measure), 
69
when the recall is weighed more heavily than pre-
cision (in the Fmean measure) and when a penalty 
is levied for fragmented matches (in the main 
METEOR measure).  
3.3 Evaluation Methodology  
As mentioned in the previous section, our main 
goal in this paper is to evaluate METEOR and its 
components on their translation-by-translation 
level correlation with human judgment. Towards 
this end, in the rest of this paper, our evaluation 
methodology is as follows: For each system, we 
compute the METEOR Score for every translation 
produced by the system, and then compute the cor-
relation between these individual scores and the 
human assessments (average of the adequacy and 
fluency scores) for the same translations. Thus we 
get a single Pearson R value for each system for 
which we have human assessments. Finally we 
average the R values of all the systems for each of 
the two language data sets to arrive at the overall 
average correlation for the Chinese dataset and the 
Arabic dataset. This number ranges between -1.0 
(completely negatively correlated) to +1.0 (com-
pletely positively correlated).  
We compare the correlation between human as-
sessments and METEOR Scores produced above 
with that between human assessments and preci-
sion, recall and Fmean scores to show the advan-
tage of the various components in the METEOR 
scoring function. Finally we run METEOR using 
different mapping modules, and compute the corre-
lation as described above for each configuration to 
show the effect of each unigram mapping mecha-
nism. 
3.4 Correlation between METEOR Scores 
and Human Assessments 
 
System ID Correlation 
ame 0.331 
ara 0.278 
arb 0.399 
ari 0.363 
arm 0.341 
arp 0.371 
Average 0.347 
 
Table 2: Correlation between METEOR Scores and 
Human Assessments for the Arabic Dataset 
We computed sentence by sentence correlation 
between METEOR Scores and human assessments 
(average of adequacy and fluency scores) for each 
translation for every system. Tables 2 and 3 show 
the Pearson R correlation values for each system, 
as well as the average correlation value per lan-
guage dataset.  
 
System ID Correlation 
E09 0.385 
E11 0.299 
E12 0.278 
E14 0.307 
E15 0.306 
E17 0.385 
E22 0.355 
Average 0.331 
 
Table 3: Correlation between METEOR Scores and 
Human Assessments for the Chinese Dataset 
3.5 Comparison with Other Metrics 
We computed translation by translation correla-
tions between human assessments and other met-
rics besides the METEOR score, namely precision, 
recall and Fmean. Tables 4 and 5 show the correla-
tions for the various scores.  
 
Metric Correlation 
Precision 0.287 
Recall 0.334 
Fmean 0.340 
METEOR 0.347 
 
Table 4: Correlations between human assessments and 
precision, recall, Fmean and METEOR Scores, aver-
aged over systems in the Arabic dataset 
 
 
Metric Correlation 
Precision 0.286 
Recall 0.320 
Fmean 0.327 
METEOR 0.331 
 
Table 5: Correlations between human assessments and 
precision, recall, Fmean and METEOR Scores, aver-
aged over systems in the Chinese dataset 
 
We observe that recall by itself correlates with 
human assessment much better than precision, and 
that combining the two using the Fmean formula 
70
described above results in further improvement. By 
penalizing the Fmean score using the chunk count 
we get some further marginal improvement in cor-
relation. 
3.6 Comparison between Different Map-
ping Modules 
To observe the effect of various unigram mapping 
modules on the correlation between the METEOR 
score and human assessments, we ran METEOR 
with different sequences of stages with different 
mapping modules in them. In the first experiment 
we ran METEOR with only one stage that used the 
?exact? mapping module. This module matches 
unigrams only if their surface forms match. (This 
module does not match unigrams that belong to a 
list of ?stop words? that consist mainly of function 
words). In the second experiment we ran 
METEOR with two stages, the first using the ?ex-
act? mapping module, and the second the ?Porter? 
mapping module. The Porter mapping module 
matches two unigrams to each other if they are 
identical after being passed through the Porter 
stemmer. In the third experiment we replaced the 
Porter mapping module with the WN-Stem map-
ping module. This module maps two unigrams to 
each other if they share the same base form in 
WordNet. This can be thought of as a different 
kind of stemmer ? the difference from the Porter 
stemmer is that the word stems are actual words 
when stemmed through WordNet in this manner. 
In the last experiment we ran METEOR with three 
stages, the first two using the exact and the Porter 
modules, and the third the WN-Synonymy map-
ping module.  This module maps two unigrams 
together if at least one sense of each word belongs 
to the same synset in WordNet. Intuitively, this 
implies that at least one sense of each of the two 
words represent the same concept. This can be 
thought of as a poor-man?s synonymy detection 
algorithm that does not disambiguate the words 
being tested for synonymy. Note that the 
METEOR scores used to compute correlations in 
the other tables (1 through 4) used exactly this se-
quence of stages.  
Tables 6 and 7 show the correlations between 
METEOR scores produced in each of these ex-
periments and human assessments for both the 
Arabic and the Chinese datasets. On both data sets, 
adding either stemming modules to simply using 
the exact matching improves correlations. Some 
further improvement in correlation is produced by 
adding the synonymy module.  
 
Mapping module sequence 
used (Arabic) 
Correlation 
Exact 0.312 
Exact, Porter 0.329 
Exact, WN-Stem 0.330 
Exact, Porter, WN-Synonym 0.347 
 
Table 6: Comparing correlations produced by different 
module stages on the Arabic dataset. 
 
Mapping module sequence 
used (Chinese) 
Correlation 
Exact 0.293 
Exact, Porter 0.318 
Exact, WN-Stem 0.312 
Exact, Porter, WN-Synonym 0.331 
 
Table 7: Comparing correlations produced by different 
module stages, on the Chinese dataset 
3.7 Correlation using Normalized Human 
Assessment Scores 
One problem with conducting correlation ex-
periments with human assessment scores at the 
sentence level is that the human scores are noisy ? 
that is, the levels of agreement between human 
judges on the actual sentence level assessment 
scores is not extremely high.  To partially address 
this issue, the human assessment scores were nor-
malized by a group at the MITRE Corporation.  To 
see the effect of this noise on the correlation, we 
computed the correlation between the METEOR 
Score (computed using the stages used in the 4th 
experiment in section 7 above) and both the raw 
human assessments as well as the normalized hu-
man assessments.  
 
 Arabic Dataset 
Chinese 
Dataset 
Raw human as-
sessments 0.347 0.331 
Normalized hu-
man assessments 0.403 0.365 
 
Table 8: Comparing correlations between METEOR 
Scores and both raw and normalized human assessments 
 
71
Table 8 shows that indeed METEOR Scores cor-
relate better with normalized human assessments. 
In other words, the noise in the human assessments 
hurts the correlations between automatic scores 
and human assessments. 
4 Future Work 
The METEOR metric we described and evaluated 
in this paper, while already demonstrating great 
promise, is still relatively simple and na?ve.  We 
are in the process of enhancing the metric and our 
experimentation in several directions: 
Train the Penalty and Score Formulas on 
Data: The formulas for Penalty and METEOR 
score were manually crafted based on empirical 
tests on a separate set of development data. How-
ever, we plan to optimize the formulas by training 
them on a separate data set, and choosing that for-
mula that best correlates with human assessments 
on the training data.  
Use Semantic Relatedness to Map Unigrams:   
So far we have experimented with exact mapping, 
stemmed mapping and synonymy mapping be-
tween unigrams. Our next step is to experiment 
with different measures of semantic relatedness to 
match unigrams that have a related meaning, but 
are not quite synonyms of each other.  
More Effective Use of Multiple Reference 
Translations:  Our current metric uses multiple 
reference translations in a weak way: we compare 
the translation with each reference separately and 
select the reference with the best match.  This was 
necessary in order to incorporate recall in our met-
ric, which we have shown to be highly advanta-
geous.  As our matching approach improves, the 
need for multiple references for the metric may in 
fact diminish.  Nevertheless, we are exploring 
ways in which to improve our matching against 
multiple references.  Recent work by (Pang et al 
2003) provides the mechanism for producing se-
mantically meaningful additional ?synthetic? refer-
ences from a small set of real references.  We plan 
to explore whether using such synthetic references 
can improve the performance of our metric. 
Weigh Matches Produced by Different Mod-
ules Differently: Our current multi-stage approach 
prefers metric imposes a priority on the different 
matching modules. However, once all the stages 
have been run, unigrams mapped through different 
mapping modules are treated the same.  Another 
approach to treating different mappings differently 
is to apply different weights to the mappings pro-
duced by different mapping modules. Thus ?com-
puter? may match ?computer? with a score of 1, 
?computers? with a score of 0.8 and ?workstation? 
with a score of 0.3. As future work we plan to de-
velop a version of METEOR that uses such 
weighting schemes. 
Acknowledgements 
We acknowledge Kenji Sagae and Shyamsundar 
Jayaraman for their work on the METEOR system. 
We also wish to thank John Henderson and Wil-
liam Morgan from MITRE for providing us with 
the normalized human judgment scores used for 
this work. 
References  
George Doddington. 2002. Automatic Evaluation of 
Machine Translation Quality using N-gram Co-
occurrence Statistics.  In Proceedings of 2nd Human 
Language Technologies Conference (HLT-02). San 
Diego, CA. pp. 128-132.  
Margaret King, Andrei Popescu-Belis and Eduard 
Hovy. 2003.  FEMTI: Creating and Using a Frame-
work for MT Evaluation.  In Proceedings of MT 
Summit IX, New Orleans, LA. Sept. 2003. pp. 224-
231. 
Alon Lavie, Kenji Sagae and Shyamsundar Jayaraman, 
2004.  The Significance of Recall in Automatic Met-
rics for MT Evaluation.  In Proceedings of AMTA-
2004, Washington DC.  September 2004. 
Bo Pang, Kevin Knight and Daniel Marcu. 2003.  Syn-
tax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New 
Sentences. In Proceedings of HLT-NAACL 2003.  
Edmonton, Canada. May 2003. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002.  BLEU: a Method for Automatic 
Evaluation of Machine Translation.  In Proceedings 
of the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL-02).  Philadelphia, 
PA. July 2002. pp. 311-318. 
Joseph P. Turian, Luke Shen and I. Dan Melamed. 
2003.  Evaluation of Machine Translation and its 
Evaluation.  In Proceedings of MT Summit IX, New 
Orleans, LA. Sept. 2003.  pp. 386-393. 
C. van Rijsbergen. 1979.  Information Retrieval.  But-
terworths.  London, England. 2nd Edition. 
72
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 125?132,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Classifier-Based Parser with Linear Run-Time Complexity 
 
 
Kenji Sagae and Alon Lavie 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
{sagae,alavie}@cs.cmu.edu 
 
 
 
 
 
 
 
Abstract 
We present a classifier-based parser that 
produces constituent trees in linear time.  
The parser uses a basic bottom-up shift-
reduce algorithm, but employs a classifier 
to determine parser actions instead of a 
grammar.  This can be seen as an exten-
sion of the deterministic dependency 
parser of Nivre and Scholz (2004) to full 
constituent parsing.  We show that, with 
an appropriate feature set used in classifi-
cation, a very simple one-path greedy 
parser can perform at the same level of 
accuracy as more complex parsers.  We 
evaluate our parser on section 23 of the 
WSJ section of the Penn Treebank, and 
obtain precision and recall of 87.54% and 
87.61%, respectively. 
1 Introduction 
Two classifier-based deterministic dependency 
parsers for English have been proposed recently 
(Nivre and Scholz, 2004; Yamada and Matsumoto, 
2003).  Although they use different parsing algo-
rithms, and differ on whether or not dependencies 
are labeled, they share the idea of greedily pursu-
ing a single path, following parsing decisions made 
by a classifier.  Despite their greedy nature, these 
parsers achieve high accuracy in determining de-
pendencies.  Although state-of-the-art statistical 
parsers (Collins, 1997; Charniak, 2000) are more 
accurate, the simplicity and efficiency of determi-
nistic parsers make them attractive in a number of 
situations requiring fast, light-weight parsing, or 
parsing of large amounts of data.  However, de-
pendency analyses lack important information con-
tained in constituent structures.  For example, the 
tree-path feature has been shown to be valuable in 
semantic role labeling (Gildea and Palmer, 2002). 
We present a parser that shares much of the 
simplicity and efficiency of the deterministic de-
pendency parsers, but produces both dependency 
and constituent structures simultaneously.  Like the 
parser of Nivre and Scholz (2004), it uses the basic 
shift-reduce stack-based parsing algorithm, and 
runs in linear time.  While it may seem that the 
larger search space of constituent trees (compared 
to the space of dependency trees) would make it 
unlikely that accurate parse trees could be built 
deterministically, we show that the precision and 
recall of constituents produced by our parser are 
close to those produced by statistical parsers with 
higher run-time complexity. 
One desirable characteristic of our parser is its 
simplicity.  Compared to other successful ap-
proaches to corpus-based constituent parsing, ours 
is remarkably simple to understand and implement.  
An additional feature of our approach is its modu-
larity with regard to the algorithm and the classifier 
that determines the parser?s actions.  This makes it 
very simple for different classifiers and different 
sets of features to be used with the same parser 
with very minimal work.  Finally, its linear run-
time complexity allows our parser to be considera-
bly faster than lexicalized PCFG-based parsers.  
On the other hand, a major drawback of the classi-
fier-based parsing framework is that, depending on 
125
the classifier used, its training time can be much 
longer than that of other approaches. 
Like other deterministic parsers (and unlike 
many statistical parsers), our parser considers the 
problem of syntactic analysis separately from part-
of-speech (POS) tagging.  Because the parser 
greedily builds trees bottom-up in one pass, con-
sidering only one path at any point in the analysis, 
the task of assigning POS tags to words is done 
before other syntactic analysis.  In this work we 
focus only on the processing that occurs once POS 
tagging is completed.  In the sections that follow, 
we assume that the input to the parser is a sentence 
with corresponding POS tags for each word. 
2 Parser Description 
Our parser employs a basic bottom-up shift-reduce 
parsing algorithm, requiring only a single pass over 
the input string.  The algorithm considers only 
trees with unary and binary branching.  In order to 
use trees with arbitrary branching for training, or 
generating them with the parser, we employ an 
instance of the transformation/detransformation 
process described in (Johnson, 1998).  In our case, 
the transformation step involves simply converting 
each production with n children (where n > 2) into 
n ? 1 binary productions.  Trees must be lexical-
ized1, so that the newly created internal structure of 
constituents with previous branching of more than 
two contains only subtrees with the same lexical 
head as the original constituent.  Additional non-
terminal symbols introduced in this process are 
clearly marked.  The transformed (or ?binarized?) 
trees may then be used for training.  Detransforma-
tion is applied to trees produced by the parser.  
This involves the removal of non-terminals intro-
                                                          
1
 If needed, constituent head-finding rules such as those men-
tioned in Collins (1996) may be used. 
 
                                        Transform 
 
 
                                                                                NP 
 
                    NP                                                               NP*                                          
                                                                                                                                                                                                                              
                                    PP                                                      NP*                                   
                                                                                                                                                                                                                                     
                                           NP                                                       PP                                       
                                                                                                                                                                  
  Det     Adj     N        P         N                                                              NP                                                                                             
                                                                                                                                                                                            
   The    big    dog    with    fleas                    Det   Adj      N       P         N                       
                                                                                                                                                                                                                     
                                                                      The   big    dog    with    fleas                    
                                                                                                                                                                                                                              
                                                                                                                                                                                                                              
                                     Detransform                                                                                                                                                        
                                                                                                                              
                                                                                                                              
Figure 1: An example of the binarization transform/detransform.  The original tree (left) has one 
node (NP) with four children.  In the transformed tree, internal structure (marked by nodes with as-
terisks) was added to the subtree rooted by the node with more than two children.  The word ?dog? 
is the head of the original NP, and it is kept as the head of the transformed NP, as well as the head of 
each NP* node. 
126
duced in the transformation process, producing 
trees with arbitrary branching.  An example of 
transformation/detransformation is shown in figure 
1. 
2.1 Algorithm Outline 
The parsing algorithm involves two main data 
structures: a stack S, and a queue W.  Items in S 
may be terminal nodes (POS-tagged words), or 
(lexicalized) subtrees of the final parse tree for the 
input string.  Items in W are terminals (words 
tagged with parts-of-speech) corresponding to the 
input string.  When parsing begins, S is empty and 
W is initialized by inserting every word from the 
input string in order, so that the first word is in 
front of the queue. 
Only two general actions are allowed: shift and 
reduce.  A shift action consists only of removing 
(shifting) the first item (POS-tagged word) from W 
(at which point the next word becomes the new 
first item), and placing it on top of S.  Reduce ac-
tions are subdivided into unary and binary cases.  
In a unary reduction, the item on top of S is 
popped, and a new item is pushed onto S.  The new 
item consists of a tree formed by a non-terminal 
node with the popped item as its single child.  The 
lexical head of the new item is the same as the 
lexical head of the popped item.  In a binary reduc-
tion, two items are popped from S in sequence, and 
a new item is pushed onto S.  The new item con-
sists of a tree formed by a non-terminal node with 
two children: the first item popped from S is the 
right child, and the second item is the left child.  
The lexical head of the new item is either the lexi-
cal head of its left child, or the lexical head of its 
right child. 
If S is empty, only a shift action is allowed.  If 
W is empty, only a reduce action is allowed.  If 
both S and W are non-empty, either shift or reduce 
actions are possible.  Parsing terminates when W is 
empty and S contains only one item, and the single 
item in S is the parse tree for the input string.  Be-
cause the parse tree is lexicalized, we also have a 
dependency structure for the sentence.  In fact, the 
binary reduce actions are very similar to the reduce 
actions in the dependency parser of Nivre and 
Scholz (2004), but they are executed in a different 
order, so constituents can be built.  If W is empty, 
and more than one item remain in S, and no further 
reduce actions take place, the input string is re-
jected. 
2.2 Determining Actions with a Classifier 
A parser based on the algorithm described in the 
previous section faces two types of decisions to be 
made throughout the parsing process.  The first 
type concerns whether to shift or reduce when both 
actions are possible, or whether to reduce or reject 
the input when only reduce actions are possible.  
The second type concerns what syntactic structures 
are created.  Specifically, what new non-terminal is 
introduced in unary or binary reduce actions, or 
which of the left or right children are chosen as the 
source of the lexical head of the new subtree pro-
duced by binary reduce actions.  Traditionally, 
these decisions are made with the use of a gram-
mar, and the grammar may allow more than one 
valid action at any single point in the parsing proc-
ess.  When multiple choices are available, a gram-
mar-driven parser may make a decision based on 
heuristics or statistical models, or pursue every 
possible action following a search strategy.  In our 
case, both types of decisions are made by a classi-
fier that chooses a unique action at every point, 
based on the local context of the parsing action, 
with no explicit grammar.  This type of classifier-
based parsing where only one path is pursued with 
no backtracking can be viewed as greedy or deter-
ministic. 
In order to determine what actions the parser 
should take given a particular parser configuration, 
a classifier is given a set of features derived from 
that configuration.  This includes, crucially, the 
two topmost items in the stack S, and the item in 
front of the queue W.  Additionally, a set of context 
features is derived from a (fixed) limited number 
of items below the two topmost items of S, and 
following the item in front of W.  The specific fea-
tures are shown in figure 2. 
The classifier?s target classes are parser actions 
that specify both types of decisions mentioned 
above.  These classes are: 
? SHIFT: a shift action is taken; 
? REDUCE-UNARY-XX: a unary reduce ac-
tion is taken, and the root of the new subtree 
pushed onto S is of type XX (where XX is a 
non-terminal symbol, typically NP, VP, PP, 
for example); 
? REDUCE-LEFT-XX: a binary reduce action 
is taken, and the root of the new subtree 
pushed onto S is of non-terminal type XX.  
127
Additionally, the head of the new subtree is 
the same as the head of the left child of the 
root node; 
? REDUCE-RIGHT-XX: a binary reduce ac-
tion is taken, and the root of the new subtree 
pushed onto S is of non-terminal type XX.  
Additionally, the head of the new subtree is 
the same as the head of the right child of the 
root node. 
2.3 A Complete Classifier-Based Parser than 
Runs in Linear Time 
When the algorithm described in section 2.1 is 
combined with a trained classifier that determines 
its parsing actions as described in section 2.2, we 
have a complete classifier-based parser.  Training 
the parser is accomplished by training its classifier.  
To that end, we need training instances that consist 
of sets of features paired with their classes corre-
Let: 
 
 S(n) denote the nth item from the top of the stack S, and 
 W(n) denote the nth item from the front of the queue W. 
 
Features: 
 
? The head-word (and its POS tag) of: S(0), S(1), S(2), and S(3)  
? The head-word (and its POS tag) of: W(0), W(1), W(3) and W(3)  
? The non-terminal node of the root of: S(0), and S(1) 
? The non-terminal node of the left child of the root of: S(0), and S(1) 
? The non-terminal node of the right child of the root of: S(0), and S(1) 
? The non-terminal node of the left child of the root of: S(0), and S(1) 
? The non-terminal node of the left child of the root of: S(0), and S(1) 
? The linear distance (number of words apart) between the head-words of S(0) and S(1) 
? The number of lexical items (words) that have been found (so far) to be dependents of 
the head-words of: S(0), and S(1) 
? The most recently found lexical dependent of the head of the head-word of S(0) that is 
to the left of S(0)?s head 
? The most recently found lexical dependent of the head of the head-word of S(0) that is 
to the right of S(0)?s head 
? The most recently found lexical dependent of the head of the head-word of S(0) that is 
to the left of S(1)?s head 
? The most recently found lexical dependent of the head of the head-word of S(0) that is 
to the right of S(1)?s head 
Figure 2: Features used for classification.  The features described in items 1 ? 7 are more di-
rectly related to the lexicalized constituent trees that are built during parsing, while the fea-
tures described in items 8 ? 13 are more directly related to the dependency structures that are 
built simultaneously to the constituent structures. 
128
sponding to the correct parsing actions.  These in-
stances can be obtained by running the algorithm 
on a corpus of sentences for which the correct 
parse trees are known.  Instead of using the classi-
fier to determine the parser?s actions, we simply 
determine the correct action by consulting the cor-
rect parse trees.  We then record the features and 
corresponding actions for parsing all sentences in 
the corpus into their correct trees.  This set of fea-
tures and corresponding actions is then used to 
train a classifier, resulting in a complete parser. 
When parsing a sentence with n words, the 
parser takes n shift actions (exactly one for each 
word in the sentence).  Because the maximum 
branching factor of trees built by the parser is two, 
the total number of binary reduce actions is n ? 1, 
if a complete parse is found.  If the input string is 
rejected, the number of binary reduce actions is 
less than n ? 1.  Therefore, the number of shift and 
binary reduce actions is linear with the number of 
words in the input string.  However, the parser as 
described so far has no limit on the number of 
unary reduce actions it may take.  Although in 
practice a parser properly trained on trees reflect-
ing natural language syntax would rarely make 
more than 2n unary reductions, pathological cases 
exist where an infinite number of unary reductions 
would be taken, and the algorithm would not ter-
minate.  Such cases may include the observation in 
the training data of sequences of unary productions 
that cycle through (repeated) non-terminals, such 
as A->B->A->B.  During parsing, it is possible that 
such a cycle may be repeated infinitely. 
This problem can be easily prevented by limit-
ing the number of consecutive unary reductions 
that may be made to a finite number.  This may be 
the number of non-terminal types seen in the train-
ing data, or the length of the longest chain of unary 
productions seen in the training data.  In our ex-
periments (described in section 3), we limited the 
number of consecutive unary reductions to three, 
although the parser never took more than two 
unary reduction actions consecutively in any sen-
tence.  When we limit the number of consecutive 
unary reductions to a finite number m, the parser 
makes at most (2n ? 1)m unary reductions when 
parsing a sentence of length n.  Placing this limit 
not only guarantees that the algorithm terminates, 
but also guarantees that the number of actions 
taken by the parser is O(n), where n is the length of 
the input string.  Thus, the parser runs in linear 
time, assuming that classifying a parser action is 
done in constant time. 
3 Similarities to Previous Work 
As mentioned before, our parser shares similarities 
with the dependency parsers of Yamada and Ma-
tsumoto (2003) and Nivre and Scholz (2004) in 
that it uses a classifier to guide the parsing process 
in deterministic fashion.  While Yamada and Ma-
tsumoto use a quadratic run-time algorithm with 
multiple passes over the input string, Nivre and 
Scholz use a simplified version of the algorithm 
described here, which handles only (labeled or 
unlabeled) dependency structures. 
Additionally, our parser is in some ways similar 
to the maximum-entropy parser of Ratnaparkhi 
(1997).  Ratnaparkhi?s parser uses maximum-
entropy models to determine the actions of a shift-
reduce-like parser, but it is capable of pursuing 
several paths and returning the top-K highest scor-
ing parses for a sentence.  Its observed time is lin-
ear, but parsing is somewhat slow, with sentences 
of length 20 or more taking more than one second 
to parse, and sentences of length 40 or more taking 
more than three seconds.  Our parser only pursues 
one path per sentence, but it is very fast and of 
comparable accuracy (see section 4).  In addition, 
Ratnaparkhi?s parser uses a more involved algo-
rithm that allows it to work with arbitrary branch-
ing trees without the need of the binarization 
transform employed here.  It breaks the usual re-
duce actions into smaller pieces (CHECK and 
BUILD), and uses two separate passes (not includ-
ing the POS tagging pass) for determining chunks 
and higher syntactic structures separately. 
Finally, there have been other deterministic 
shift-reduce parsers introduced recently, but their 
levels of accuracy have been well below the state-
of-the-art.  The parser in Kalt (2004) uses a similar 
algorithm to the one described here, but the classi-
fication task is framed differently.  Using decision 
trees and fewer features, Kalt?s parser has signifi-
cantly faster training and parsing times, but its ac-
curacy is much lower than that of our parser.  
Kalt?s parser achieves precision and recall of about 
77% and 76%, respectively (with automatically 
tagged text), compared to our parser?s 86% (see 
section 4).  The parser of Wong and Wu (1999) 
uses a separate NP-chunking step and, like Ratna-
parkhi?s parser, does not require a binary trans-
129
form.  It achieves about 81% precision and 82% 
recall with gold-standard tags (78% and 79% with 
automatically tagged text).  Wong and Wu?s parser 
is further differentiated from the other parsers 
mentioned here in that it does not use lexical items, 
working only from part-of-speech tags. 
4 Experiments 
We conducted experiments with the parser de-
scribed in section 2 using two different classifiers: 
TinySVM (a support vector machine implementa-
tion by Taku Kudo)2, and the memory-based 
learner TiMBL (Daelemans et al, 2004).  We 
trained and tested the parser on the Wall Street 
Journal corpus of the Penn Treebank (Marcus et 
al., 1993) using the standard split: sections 2-21 
were used for training, section 22 was used for de-
velopment and tuning of parameters and features, 
and section 23 was used for testing.  Every ex-
periment reported here was performed on a Pen-
tium IV 1.8GHz with 1GB of RAM. 
Each tree in the training set had empty-node 
and function tag information removed, and the 
                                                          
2
 http://chasen.org/~taku/software/TinySVM 
trees were lexicalized using similar head-table 
rules as those mentioned in (Collins, 1996).  The 
trees were then converted into trees containing 
only unary and binary branching, using the binari-
zation transform described in section 2.  Classifier 
training instances of features paired with classes 
(parser actions) were extracted from the trees in the 
training set, as described in section 2.3.  The total 
number of training instances was about 1.5 million. 
The classifier in the SVM-based parser (de-
noted by SVMpar) uses the polynomial kernel with 
degree 2, following the work of Yamada and Ma-
tsumoto (2003) on SVM-based deterministic de-
pendency parsing, and a one-against-all scheme for 
multi-class classification.  Because of the large 
number of training instances, we used Yamada and 
Matsumoto?s idea of splitting the training instances 
into several parts according to POS tags, and train-
ing classifiers on each part.  This greatly reduced 
the time required to train the SVMs, but even with 
the splitting of the training set, total training time 
was about 62 hours.  Training set splitting comes 
with the cost of reduction in accuracy of the parser, 
but training a single SVM would likely take more 
than one week.  Yamada and Matsumoto experi-
enced a reduction of slightly more than 1% in de-
 
Precision Recall Dependency Time (min) 
Charniak 89.5 89.6 92.1 28 
Collins 88.3 88.1 91.5 45 
Ratnaparkhi 87.5 86.3 Unk Unk 
Y&M - - 90.3 Unk 
N&S - - 87.3 21 
MBLpar 80.0 80.2 86.3 127 
SVMpar 87.5 87.6 90.3 11 
 
Table 1: Summary of results on labeled precision and recall of constituents, dependency accu-
racy, and time required to parse the test set.  The parsers of Yamada and Matsumoto (Y&M) and 
Nivre and Scholz (N&S) do not produce constituent structures, only dependencies.  ?unk? indi-
cates unknown values.  Results for MBLpar and SVMpar using correct POS tags (if automatically 
produced POS tags are used, accuracy figures drop about 1.5% over all metrics).  
 
130
pendency accuracy due to training set splitting, and 
we expect that a similar loss is incurred here. 
When given perfectly tagged text (gold tags ex-
tracted from the Penn Treebank), SVMpar has la-
beled constituent precision and recall of 87.54% 
and 87.61%, respectively, and dependency accu-
racy of 90.3% over all sentences in the test set.  
The total time required to parse the entire test set 
was 11 minutes.  Out of more than 2,400 sen-
tences, only 26 were rejected by the parser (about 
1.1%).  For these sentences, partial analyses were 
created by combining the items in the stack in flat 
structures, and these were included in the evalua-
tion.  Predictably, the labeled constituent precision 
and recall obtained with automatically POS-tagged 
sentences were lower, at 86.01% and 86.15%.  The 
part-of-speech tagger used in our experiments was 
SVMTool (Gim?nez and M?rquez, 2004), and its 
accuracy on the test set is 97%. 
The MBL-based parser (denoted by MBLpar) 
uses the IB1 algorithm, with five nearest 
neighbors, and the modified value difference met-
ric (MVDM), following the work of Nivre and 
Scholz (2004) on MBL-based deterministic de-
pendency parsing.  MBLpar was trained with all 
training instances in under 15 minutes, but its ac-
curacy on the test set was much lower than that of 
SVMpar, with constituent precision and recall of 
80.0% and 80.2%, and dependency accuracy of 
86.3% (24 sentences were rejected).  It was also 
much slower than SVMpar in parsing the test set, 
taking 127 minutes.  In addition, the total memory 
required for running MBLpar (including the classi-
fier) was close to 1 gigabyte (including the trained 
classifier), while SVMpar required only about 200 
megabytes (including all the classifiers). 
Table 1 shows a summary of the results of our 
experiments with SVMpar and MBLpar, and also 
results obtained with the Charniak (2000) parser, 
the Bikel (2003) implementation of the Collins 
(1997) parser, and the Ratnaparkhi (1997) parser.  
We also include the dependency accuracy from 
Yamada and Matsumoto?s (2003) SVM-based de-
pendency parser, and Nivre and Scholz?s (2004) 
MBL-based dependency parser.  These results 
show that the choice of classifier is extremely im-
portant in this task.  SVMpar and MBLpar use the 
same algorithm and features, and differ only on the 
classifiers used to make parsing decisions.  While 
in many natural language processing tasks different 
classifiers perform at similar levels of accuracy, we 
have observed a dramatic difference between using 
support vector machines and a memory-based 
learner.  Although the reasons for such a large dis-
parity in results is currently the subject of further 
investigation, we speculate that a relatively small 
difference in initial classifier accuracy results in 
larger differences in parser performance, due to the 
deterministic nature of the parser (certain errors 
may lead to further errors).  We also believe classi-
fier choice to be one major source of the difference 
in accuracy between Nivre and Scholz?s parser and 
Yamada and Matsumoto?s parser.  
While the accuracy of SVMpar is below that of 
lexicalized PCFG-based statistical parsers, it is 
surprisingly good for a greedy parser that runs in 
linear time.  Additionally, it is considerably faster 
than lexicalized PCFG-based parsers, and offers a 
good alternative for when fast parsing is needed.  
MBLpar, on the other hand, performed poorly in 
terms of accuracy and speed. 
5 Conclusion and Future Work 
We have presented a simple shift-reduce parser 
that uses a classifier to determine its parsing ac-
tions and runs in linear time.  Using SVMs for 
classification, the parser has labeled constituent 
precision and recall higher than 87% when using 
the correct part-of-speech tags, and slightly higher 
than 86% when using automatically assigned part-
of-speech tags.  Although its accuracy is not as 
high as those of state-of-the-art statistical parsers, 
our classifier-based parser is considerably faster 
than several well-known parsers that employ 
search or dynamic programming approaches.  At 
the same time, it is significantly more accurate 
than previously proposed deterministic parsers for 
constituent structures. 
We have also shown that much of the success 
of a classifier-based parser depends on what classi-
fier is used.  While this may seem obvious, the dif-
ferences observed here are much greater than what 
would be expected from looking, for example, at 
results from chunking/shallow parsing (Zhang et 
al., 2001; Kudo and Matsumoto, 2001; Veenstra 
and van den Bosch, 2000). 
Future work includes the investigation of the ef-
fects of individual features, the use of additional 
classification features, and the use of different clas-
sifiers.  In particular, the use of tree features seems 
appealing.  This may be accomplished with SVMs 
131
using a tree kernel, or the tree boosting classifier 
BACT described in (Kudo and Matsumoto, 2004).  
Additionally, we plan to investigate the use of the 
beam strategy of Ratnaparkhi (1997) to pursue 
multiple parses while keeping the run-time linear. 
References 
Charniak, E.  2000.  A maximum-entropy-inspired 
parser.  Proceedings of the First Annual Meeting of 
the North American Chapter of the Association for 
Computational Linguistics.  Seattle, WA. 
Collins, M. 1997.  Three generative, lexicalized models 
for statistical parsing.  Proceedings of the 35th An-
nual Meeting of the Association for Computational 
Linguistics (pp. 16-23).  Madrid, Spain. 
Daelemans, W., Zavrel, J., van der Sloot, K., and van 
den Bosch, A.  2004.  TiMBL: Tilburg Memory 
Based Learner, version 5.1, reference guide.  ILK Re-
search Group Technical Report Series no. 04-02, 
2004. 
Gildea, D., and Palmer, M.  2002.  The necessity of syn-
tactic parsing for predicate argument recognition.  
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (pp. 239-246).  
Philadelphia, PA. 
Kalt, T. 2004.  Induction of greedy controllers for de-
terministic treebank parsers.  Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing.  Barcelona, Spain. 
Kudo, T., and Matsumoto, Y. 2004.  A boosting algo-
rithm for classification of semi-structured text.  Pro-
ceedings of the 2004 Conference on Empirical 
Methods in Natural Language Processing.  Barce-
lona, Spain. 
Kudo, T., and Matsumoto, Y.  2001.  Chunking with 
support vector machines.  Proceedings of the Second 
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics.  Pittsburgh, 
PA. 
Johnson, M. 1998.  PCFG models of linguistic tree rep-
resentations.  Computational Linguistics, 24:613-632. 
Marcus, M. P., Santorini, B., and Marcinkiewics, M. A. 
1993.  Building a large annotated corpus of English: 
the Penn Treebank.  Computational Linguistics, 19. 
Nivre, J., and Scholz, M.  2004.  Deterministic depend-
ency parsing of English text.  Proceedings of the 20th 
International Conference on Computational Linguis-
tics  (pp. 64-70).  Geneva, Switzerland. 
Ratnaparkhi, A. 1997.  A linear observed time statistical 
parser based on maximum entropy models.  Proceed-
ings of the Second Conference on Empirical Methods 
in Natural Language Processing.  Providence, Rhode 
Island. 
Veenstra, J., van den Bosch, A.  2000.  Single-classifier 
memory-based phrase chunking.  Proceedings of 
Fourth Workshop on Computational Natural Lan-
guage Learning (CoNLL 2000).  Lisbon, Portugal. 
Wong, A., and Wu. D. 1999.  Learning a lightweight 
robust deterministic parser.  Proceedings of the Sixth 
European Conference on Speech Communication and 
Technology.  Budapest. 
Yamada, H., and Matsumoto, Y.  2003.  Statistical de-
pendency analysis with support vector machines.  
Proceedings of the Eighth International Workshop on 
Parsing Technologies.  Nancy, France. 
Zhang, T., Damerau, F., and Johnson, D. 2002.  Text 
chunking using regularized winnow.  Proceedings of 
the 39th Annual Meeting of the Association for Com-
putational Linguistics.  Tolouse, France. 
  
132
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 25?32,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
High-accuracy Annotation and Parsing of CHILDES Transcripts
Kenji Sagae
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
sagae@is.s.u-tokyo.ac.jp
Eric Davis
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
dhdavis@cs.cmu.edu
Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
alavie@cs.cmu.edu
Brian MacWhinney
Department of Psychology
Carnegie Mellon University
Pittsburgh, PA 15213
macw@cmu.edu
Shuly Wintner
Department of Computer Science
University of Haifa
31905 Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
Corpora of child language are essential for
psycholinguistic research. Linguistic anno-
tation of the corpora provides researchers
with better means for exploring the develop-
ment of grammatical constructions and their
usage. We describe an ongoing project that
aims to annotate the English section of the
CHILDES database with grammatical re-
lations in the form of labeled dependency
structures. To date, we have produced a cor-
pus of over 65,000 words with manually cu-
rated gold-standard grammatical relation an-
notations. Using this corpus, we have devel-
oped a highly accurate data-driven parser for
English CHILDES data. The parser and the
manually annotated data are freely available
for research purposes.
1 Introduction
In order to investigate the development of child lan-
guage, corpora which document linguistic interac-
tions involving children are needed. The CHILDES
database (MacWhinney, 2000), containing tran-
scripts of spoken interactions between children at
various stages of language development with their
parents, provides vast amounts of useful data for lin-
guistic, psychological, and sociological studies of
child language development. The raw information in
CHILDES corpora was gradually enriched by pro-
viding a layer of morphological information. In par-
ticular, the English section of the database is aug-
mented by part of speech (POS) tags for each word.
However, this information is usually insufficient for
investigations dealing with the syntactic, semantic
or pragmatic aspects of the data.
In this paper we describe an ongoing effort aim-
ing to annotate the English portion of the CHILDES
database with syntactic information based on gram-
matical relations represented as labeled dependency
structures. Although an annotation scheme for syn-
tactic information in CHILDES data has been pro-
posed (Sagae et al, 2004), until now no significant
amount of annotated data had been made publicly
available. In the process of manually annotating sev-
eral thousands of words, we updated the annotation
scheme, mostly by extending it to cover syntactic
phenomena that occur in real data but were unac-
counted for in the original annotation scheme.
The contributions of this work fall into three main
categories: revision and extension of the annota-
tion scheme for representing syntactic information
in CHILDES data; creation of a manually annotated
65,000 word corpus with gold-standard syntactic
analyses; and implementation of a complete parser
that can automatically annotate additional data with
high accuracy. Both the gold-standard annotated
data and the parser are freely available. In addi-
tion to introducing the parser and the data, we re-
port on many of the specific annotation issues that
we encountered during the manual annotation pro-
25
cess, which should be helpful for those who may
use the annotated data or the parser. The anno-
tated corpora and the parser are freely available from
http://childes.psy.cmu.edu/.
We describe the annotation scheme in the next
section, along with issues we faced during the pro-
cess of manual annotation. Section 3 describes the
parser, and an evaluation of the parser is presented in
section 4. We analyze the remaining parsing errors
in section 5 and conclude with some applications of
the parser and directions for future research in sec-
tion 6.
2 Syntactic annotation
The English section of the CHILDES database is
augmented with automatically produced ambiguous
part-of-speech and morphological tags (MacWhin-
ney, 2000). Some of these data have been manually
disambiguated, but we found that some annotation
decisions had to be revised to facilitate syntactic an-
notation. We discuss below some of the revisions we
introduced, as well as some details of the syntactic
constructions that we account for.
2.1 The morphological annotation scheme
The English morphological analyzer incorporated
in CHILDES produces various part-of-speech tags
(there are 31 distinct POS tags in the CHILDES
tagset), including ADJective, ADVerb, COmmuni-
cator, CONJunction, DETerminer, FILler, Noun,
NUMeral, ONomatopoeia, PREPosition, PROnoun,
ParTicLe, QuaNtifier, RELativizer and Verb1. In
most cases, the correct annotation of a word is obvi-
ous from the context in which the word occurs, but
sometimes a more subtle distinction must be made.
We discuss some common problematic issues below.
Adverb vs. preposition vs. particle The words
about, across, after, away, back, down, in, off, on,
out, over, up belong to three categories: ADVerb,
PREPosition and ParTicLe. To correctly annotate
them in context, we apply the following criteria.
First, a preposition must have a prepositional ob-
ject, which is typically realized as a noun phrase
(which may be topicalized, or even elided). Sec-
ond, a preposition forms a constituent with its noun
1We use capital letters to denote the actual tag names in the
CHILDES tagset.
phrase object. Third, a prepositional object can be
fronted (for example, he sat on the chair becomes
the chair on which he sat), whereas a particle-NP
sequence cannot (*the phone number up which he
looked cannot be obtained from he looked up the
phone number). Finally, a manner adverb can be
placed between the verb and a preposition, but not
between a verb and a particle.
To distinguish between an adverb and a particle,
the meaning of the head verb is considered. If the
meaning of the verb and the target word, taken to-
gether, cannot be predicted from the meanings of the
verb and the target word separately, then the target
word is a particle. In all other cases it is an adverb.
Verbs vs. auxiliaries Distinguishing between
Verb and AUXiliary is often straightforward, but
special attention is given when tagging the verbs be,
do and have. If the target word is accompanied by an
non-finite verb in the same clause, as in I have had
enough or I do not like eggs, it is an auxiliary. Ad-
ditionally, in interrogative sentences, the auxiliary is
moved to the beginning of the clause, as in have I
had enough? and do I like eggs?, whereas the main
verb is not. However, this test does not always work
for the verb be, which may head a non-verbal pred-
icate, as in John is a teacher, vs. John is smiling. In
verb-participle constructions headed by the verb be,
if the participle is in the progressive tense, then the
head verb is labeled as auxiliary.
Communicators vs. locative adverbs COmmu-
nicators can be hard to distinguish from locative ad-
verbs, especially at the beginning of a sentence. Our
convention is that CO must modify an entire sen-
tence, so if a word appears by itself, it cannot be a
CO. For example, utterances like here or there are
labeled as ADVerb. However, if these words appear
at the beginning of a sentence, are followed by a
break or pause, and do not clearly express a location,
then they are labeled CO. Additionally, in here/there
you are/go, here and there are labeled CO.
2.2 The syntactic annotation scheme
Our annotation scheme for representing grammati-
cal relations, or GRs (such as subjects, objects and
adjuncts), in CHILDES transcripts is a slightly ex-
tended version of the scheme proposed by Sagae et
al. (2004), which was inspired by a general annota-
26
tion scheme for grammatical relations (Carroll et al,
1998), but adapted specifically for CHILDES data.
Our scheme contains 37 distinct GR types. Sagae
et al reported 96.5% interannotator agreement, and
we do not believe our minor updates to the annota-
tion scheme should affect interannotator agreement
significantly.
The scheme distinguishes among SUBJects, (fi-
nite) Clausal SUBJects2 (e.g., that he cried moved
her) and XSUBJects (eating vegetables is impor-
tant). Similarly, we distinguish among OBJects,
OBJect2, which is the second object of a ditran-
sitive verb, and IOBjects, which are required verb
complements introduced by prepositions. Verb com-
plements that are realized as clauses are labeled
COMP if they are finite (I think that was Fraser) and
XCOMP otherwise (you stop throwing the blocks).
Additionally, we mark required locative adjectival
or prepositional phrase arguments of verbs as LOCa-
tives, as in put the toys in the box/back.
PREDicates are nominal, adjectival or prepo-
sitional complements of verbs such as get, be
and become, as in I?m not sure. Again, we
specifically mark Clausal PREDicates (This is
how I drink my coffee) and XPREDicates (My goal
is to win the competition).
Adjuncts (denoted by JCT) are optional modi-
fiers of verbs, adjectives or adverbs, and we dis-
tinguish among non-clausal ones (That?s much bet-
ter; sit on the stool), finite clausal ones (CJCT, Mary
left after she saw John) and non-finite clausal ones
(XJCT, Mary left after seeing John).
MODifiers, which modify or complement nouns,
again come in three flavors: MOD (That?s a nice
box); CMOD (the movie that I saw was good ); and
XMOD (the student reading a book is tall ).
We then identify AUXiliary verbs, as in did you
do it? ; NEGation (Fraser is not drinking his coffee);
DETerminers (a fly); QUANTifiers (some juice); the
objects of prepositions (POBJ, on the stool); verb
ParTicLes (can you get the blocks out? ); ComPle-
mentiZeRs (wait until the noodles are cool ); COM-
municators (oh, I took it); the INfinitival to; VOCa-
tives (Thank you, Eve); and TAG questions (you
know how to count, don?t you? ).
2As with the POS tags, we use capital letters to represent the
actual GR tags used in the annotation scheme.
Finally, we added some specific relations for han-
dling problematic issues. For example, we use
ENUMeration for constructions such as one, two,
three, go or a, b, c. In COORDination construc-
tions, each conjunct is marked as a dependent of the
conjunction (e.g., go and get your telephone). We
use TOPicalization to indicate an argument that is
topicalized, as in tapioca, there is no tapioca. We
use SeRiaL to indicate serial verbs as in come see
if we can find it or go play with your toys. Finally,
we mark sequences of proper names which form the
same entity (e.g., New York ) as NAME.
The format of the grammatical relation (GR) an-
notation, which we use in the examples that follow,
associates with each word in a sentence a triple i|j|g,
where i is the index of the word in the sentence, j the
index of the word?s syntactic head, and g is the name
of the grammatical relation represented by the syn-
tactic dependency between the i-th and j-th words.
If the topmost head of the utterance is the i-th word,
it is labeled i|0|ROOT. For example, in:
a cookie .
1|2|DET 2|0|ROOT 3|2|PUNCT
the first word a is a DETerminer of word 2 (cookie),
which is itself the ROOT of the utterance.
2.3 Manual annotation of the corpus
We focused our manual annotation on a set of
CHILDES transcripts for a particular child, Eve
(Brown, 1973), and we refer to these transcripts,
distributed in a set of 20 files, as the Eve corpus.
We hand-annotated (including correcting POS tags)
the first 15 files of the Eve corpus following the
GR scheme outlined above. The annotation pro-
cess started with purely manual annotation of 5,000
words. This initial annotated corpus was used to
train a data-driven parser, as described later. This
parser was then used to label an additional 20,000
words automatically, followed by a thorough manual
checking stage, where each syntactic annotation was
manually verified and corrected if necessary. We re-
trained the parser with the newly annotated data, and
proceeded in this fashion until 15 files had been an-
notated and thoroughly manually checked.
Annotating child language proved to be challeng-
ing, and as we progressed through the data, we no-
ticed grammatical constructions that the GRs could
27
not adequately handle. For example, the original GR
scheme did not differentiate between locative argu-
ments and locative adjuncts, so we created a new GR
label, LOC, to handle required verbal locative argu-
ments such as on in put it on the table. Put licenses
a prepositional argument, and the existing JCT rela-
tion could not capture this requirement.
In addition to adding new GRs, we also faced
challenges with telegraphic child utterances lack-
ing verbs or other content words. For instance,
Mommy telephone could have one of several mean-
ings: Mommy this is a telephone, Mommy I want
the telephone, that is Mommy?s telephone, etc. We
tried to be as consistent as possible in annotating
such utterances and determined their GRs from con-
text. It was often possible to determine the VOC
reading vs.the MOD (Mommy?s telephone) reading
by looking at context. If it was not possible to deter-
mine the correct annotation from context, we anno-
tated such utterances as VOC relations.
After annotating the 15 Eve files, we had 18,863
fully hand-annotated utterances, 10,280 adult
and 8,563 child. The utterances consist of 84,226
GRs (including punctuation) and 65,363 words.
The average utterance length is 5.3 words (in-
cluding punctuation) for adult utterances, 3.6 for
child, 4.5 overall. The annotated Eve corpus
is available at http://childes.psy.cmu.
edu/data/Eng-USA/brown.zip. It was used
for the Domain adaptation task at the CoNLL-2007
dependency parsing shared task (Nivre, 2007).
3 Parsing
Although the CHILDES annotation scheme pro-
posed by Sagae et al (2004) has been used in prac-
tice for automatic parsing of child language tran-
scripts (Sagae et al, 2004; Sagae et al, 2005), such
work relied mainly on a statistical parser (Char-
niak, 2000) trained on the Wall Street Journal por-
tion of the Penn Treebank, since a large enough cor-
pus of annotated CHILDES data was not available
to train a domain-specific parser. Having a corpus
of 65,000 words of CHILDES data annotated with
grammatical relations represented as labeled depen-
dencies allows us to develop a parser tailored for the
CHILDES domain.
Our overall parsing approach uses a best-first
probabilistic shift-reduce algorithm, working left-to-
right to find labeled dependencies one at a time. The
algorithm is essentially a dependency version of the
data-driven constituent parsing algorithm for prob-
abilistic GLR-like parsing described by Sagae and
Lavie (2006). Because CHILDES syntactic annota-
tions are represented as labeled dependencies, using
a dependency parsing approach allows us to work
with that representation directly.
This dependency parser has been shown to have
state-of-the-art accuracy in the CoNLL shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre, 2007)3. Sagae and Tsujii (2007) present a
detailed description of the parsing approach used in
our work, including the parsing algorithm. In sum-
mary, the parser uses an algorithm similar to the LR
parsing algorithm (Knuth, 1965), keeping a stack of
partially built syntactic structures, and a queue of
remaining input tokens. At each step in the pars-
ing process, the parser can apply a shift action (re-
move a token from the front of the queue and place
it on top of the stack), or a reduce action (pop the
two topmost stack items, and push a new item com-
posed of the two popped items combined in a sin-
gle structure). This parsing approach is very similar
to the one used successfully by Nivre et al (2006),
but we use a maximum entropy classifier (Berger et
al., 1996) to determine parser actions, which makes
parsing extremely fast. In addition, our parsing ap-
proach performs a search over the space of possible
parser actions, while Nivre et al?s approach is de-
terministic. See Sagae and Tsujii (2007) for more
information on the parser.
Features used in classification to determine
whether the parser takes a shift or a reduce action
at any point during parsing are derived from the
parser?s current configuration (contents of the stack
and queue) at that point. The specific features used
are:4
? Word and its POS tag: s(1), q(2), and q(1).
? POS: s(3) and q(2).
3The parser used in this work is the same as the probabilistic
shift-reduce parser referred to as ?Sagae? in the cited shared
task descriptions. In the 2007 shared task, an ensemble of shift-
reduce parsers was used, but only a single parser is used here.
4s(n) denotes the n-th item from the top of the stack (where
s(1) is the item on the top of the stack), and q(n) denotes the
n-th item from the front of the queue.
28
? The dependency label of the most recently at-
tached dependent of: s(1) and s(2).
? The previous parser action.
4 Evaluation
4.1 Methodology
We first evaluate the parser by 15-fold cross-
validation on the 15 manually curated gold-standard
Eve files (to evaluate the parser on each file, the re-
maining 14 files are used to train the parser). Single-
word utterances (excluding punctuation) were ig-
nored, since their analysis is trivial and their inclu-
sion would artificially inflate parser accuracy mea-
surements. The size of the Eve evaluation corpus
(with single-word utterances removed) was 64,558
words (or 59,873 words excluding punctuation). Of
these, 41,369 words come from utterances spoken
by adults, and 18,504 come from utterances spo-
ken by the child. To evaluate the parser?s portabil-
ity to other CHILDES corpora, we also tested the
parser (trained only on the entire Eve set) on two ad-
ditional sets, one taken from the MacWhinney cor-
pus (MacWhinney, 2000) (5,658 total words, 3,896
words in adult utterances and 1,762 words in child
utterances), and one taken from the Seth corpus (Pe-
ters, 1987; Wilson and Peters, 1988) (1,749 words,
1,059 adult and 690 child).
The parser is highly efficient: training on the en-
tire Eve corpus takes less that 20 minutes on stan-
dard hardware, and once trained, parsing the Eve
corpus takes 18 seconds, or over 3,500 words per
second.
Following recent work on dependency parsing
(Nivre, 2007), we report two evaluation measures:
labeled accuracy score (LAS) and unlabeled accu-
racy score (UAS). LAS is the percentage of tokens
for which the parser predicts the correct head-word
and dependency label. UAS ignores the dependency
labels, and therefore corresponds to the percentage
of words for which the correct head was found. In
addition to LAS and UAS, we also report precision
and recall of certain grammatical relations.
For example, compare the parser output of go buy
an apple to the gold standard (Figure 1). This se-
quence of GRs has two labeled dependency errors
and one unlabeled dependency error. 1|2|COORD
for the parser versus 1|2|SRL is a labeled error be-
cause the dependency label produced by the parser
(COORD) does not match the gold-standard anno-
tation (SRL), although the unlabeled dependency is
correct, since the headword assignment, 1|2, is the
same for both. On the other hand, 5|1|PUNCT ver-
sus 5|2|PUNCT is both a labeled dependency error
and an unlabeled dependency error, since the head-
word assignment produced by the parser does not
match the gold-standard.
4.2 Results
Trained on domain-specific data, the parser per-
formed well on held-out data, even though the train-
ing corpus is relatively small (about 60,000 words).
The results are listed in Table 1.
LAS UAS
Eve cross-validation 92.0 93.8
Table 1: Average cross-validation results, Eve
The labeled dependency error rate is about 8%
and the unlabeled error rate is slightly over 6%. Per-
formance in individual files ranged between the best
labeled error rate of 6.2% and labeled error rate of
4.4% for the fifth file, and the worst error rates of
8.9% and 7.8% for labeled and unlabeled respec-
tively in the fifteenth file. For comparison, Sagae et
al. (2005) report 86.9% LAS on about 2,000 words
of Eve data, using the Charniak (2000) parser with
a separate dependency-labeling step. Part of the rea-
son we obtain levels of accuracy higher than usu-
ally reported for dependency parsers is that the aver-
age sentence length in CHILDES transcripts is much
lower than in, for example, newspaper text. The av-
erage sentence length for adult utterances in the Eve
corpus is 6.1 tokens, and 4.3 tokens for child utter-
ances5.
Certain GRs are easily identifiable, such as DET,
AUX, and INF. The parser has precision and recall
of nearly 1.00 for those. For all GRs that occur more
than 1,000 times in the Eve corpus (which contrains
more than 60,000 tokens), precision and recall are
above 0.90, with the exception of COORD, which
5This differs from the figures in section 2.3 because for the
purpose of parser evaluation we ignore sentences composed
only of a single word plus punctuation.
29
go buy an apple .
parser: 1|2|COORD 2|0|ROOT 3|4|DET 4|2|OBJ 5|1|PUNCT
gold: 1|2|SRL 2|0|ROOT 3|4|DET 4|2|OBJ 5|2|PUNCT
Figure 1: Example output: parser vs. gold annotation
occurs 1,163 times in the gold-standard data. The
parser?s precision for COORD is 0.73, and recall
is 0.84. Other interesting GRs include SUBJ, OBJ,
JCT (adjunct), COM, LOC, COMP, XCOMP, CJCT
(subordinate clause acting as an adjunct), and PTL
(verb particle, easily confusable with prepositions
and adverbs). Their precision and recall is shown
in table 2.
GR Precision Recall F-score
SUBJ 0.96 0.96 0.96
OBJ 0.93 0.94 0.93
JCT 0.91 0.90 0.90
COM 0.96 0.95 0.95
LOC 0.95 0.90 0.92
COMP 0.83 0.86 0.84
XCOMP 0.86 0.87 0.87
CJCT 0.61 0.59 0.60
PTL 0.97 0.96 0.96
COORD 0.73 0.84 0.78
Table 2: Precision, recall and f-score of selected
GRs in the Eve corpus
We also tested the accuracy of the parser on child
utterances and adult utterances separately. To do
this, we split the gold standard files into child and
adult utterances, producing gold standard files for
both child and adult utterances. We then trained
the parser on 14 of the 15 Eve files with both child
and adult utterances, and parsed the individual child
and adult files. Not surprisingly, the parser per-
formed slightly better on the adult utterances due to
their grammaticality and the fact that there was more
adult training data than child training data. The re-
sults are listed in Table 3.
LAS UAS
Eve - Child 90.0 91.7
Eve - Adult 93.1 94.8
Table 3: Average child vs. adult results, Eve
Our final evaluation of the parser involved test-
ing the parser on data taken from a different parts of
the CHILDES database. First, the parser was trained
on all gold-standard Eve files, and tested on man-
ually annotated data taken from the MacWhinney
transcripts. Although accuracy was lower for adult
utterances (85.8% LAS) than on Eve data, the accu-
racy for child utterances was slightly higher (92.3%
LAS), even though child utterances were longer on
average (4.7 tokens) than in the Eve corpus.
Finally, because a few aspects of the many tran-
script sets in the CHILDES database may vary in
ways not accounted for in the design of the parser
or the annotation of the training data, we also re-
port results on evaluation of the Eve-trained parser
on a particularly challenging test set, the Seth cor-
pus. Because the Seth corpus contains transcriptions
of language phenomena not seen in the Eve corpus
(see section 5), parser performance is expected to
suffer. Although accuracy on adult utterances is high
(92.2% LAS), accuracy on child utterances is very
low (72.7% LAS). This is due to heavy use of a GR
label that does not appear at all in the Eve corpus
that was used to train the parser. This GR is used to
represent relations involving filler syllables, which
appear in nearly 45% of the child utterances in the
Seth corpus. Accuracy on the sentences that do not
contain filler syllables is at the same level as in the
other corpora (91.1% LAS). Although we do not ex-
pect to encounter many sets of transcripts that are as
problematic as this one in the CHILDES database, it
is interesting to see what can be expected from the
parser under unfavorable conditions.
The results of the parser on the MacWhinney and
Seth test sets are summarized in table 4, where Seth
(clean) refers to the Seth corpus without utterances
that contain filler sylables.
5 Error Analysis
A major source for parser errors on the Eve cor-
pus (112 out of 5181 errors) was telegraphic speech,
30
LAS UAS
MacWhinney - Child 92.3 94.8
MacWhinney - Adult 85.8 89.4
MacWhinney - Total 88.0 91.2
Seth - Child 72.7 82.0
Seth - Adult 92.2 94.4
Seth - Total 84.6 89.5
Seth (clean) - Child 91.1 92.7
Seth (clean) - Total 92.0 93.9
Table 4: Training on Eve, testing on MacWhinney
and Seth
as in Mommy telephone or Fraser tape+recorder
floor. Telegraphic speech may be the most chal-
lenging, since even for a human annotator, deter-
mining a GR is difficult. The parser usually labeled
such utterances with the noun as the ROOT and the
proper noun as the MOD, while the gold annotation
is context-dependent as described above.
Another category of errors, with about 150 in-
stances, is XCOMP errors. The majority of the er-
rors in this category revolve around dropped words
in the main clause, for example want eat cookie. Of-
ten, the parser labels such utterances with COMP
GRs, because of the lack of to. Exclusive training on
utterances of this type may resolve the issue. Many
of the errors of this type occur with want : the parser
could be conditioned to assign an XCOMP GR with
want as the ROOT of an utterance.
COORD and PRED errors would both benefit
from more data as well. The parser performs ad-
mirably on simple coordination and predicate con-
structions, but has troubles with less common con-
structions such as PRED GRs with get, e.g., don?t
let your hands get dirty (69 errors), and coordina-
tion of prepositional objects, as in a birthday cake
with Cathy and Becky (154 errors).
The performance drop on the Seth corpus can be
explained by a number of factors. First and fore-
most, Seth is widely considered in the literature to
be the child who is most likely to invalidate any the-
ory (Wilson and Peters, 1988). He exhibits false
starts and filler syllables extensively, and his syn-
tax violates many ?universal? principles. This is
reflected in the annotation scheme: the Seth cor-
pus, following the annotation of Peters (1983), is
abundant with filler syllables. Because there was
no appropriate GR label for representing the syn-
tactic relationships involving the filler syllables, we
annotated those with a special GR (not used during
parser training), which the parser is understandably
not able to produce. Filler syllables usually occur
near the start of the sentence, and once the parser
failed to label them, it could not accurately label the
remaining GRs. Other difficulties in the Seth cor-
pus include the usage of dates, of which there were
no instances in the Eve corpus. The parser had not
been trained on the new DATE GR and subsequently
failed to parse it.
6 Conclusion
We described an annotation scheme for represent-
ing syntactic information as grammatical relations
in CHILDES data, a manually curated gold-standard
corpus of 65,000 words annotated according to this
GR scheme, and a parser that was trained on the an-
notated corpus and produces highly accurate gram-
matical relations for both child and adult utterances.
These resources are now freely available to the re-
search community, and we expect them to be in-
strumental in psycholinguistic investigations of lan-
guage acquisition and child language.
Syntactic analysis of child language transcripts
using a GR scheme of this kind has already been
shown to be effective in a practical setting, namely
in automatic measurement of syntactic development
in children (Sagae et al, 2005). That work relied on
a phrase-structure statistical parser (Charniak, 2000)
trained on the Penn Treebank, and the output of that
parser had to be converted into CHILDES grammat-
ical relations. Despite the obvious disadvantage of
using a parser trained on a completely different lan-
guage genre, Sagae et al (2005) demonstrated how
current natural language processing techniques can
be used effectively in child language work, achiev-
ing results that are close to those obtained by man-
ual computation of syntactic development scores for
child transcripts. Still, the use of tools not tailored
for child language and extra effort necessary to make
them work with community standards for child lan-
guage transcription present a disincentive for child
language researchers to incorporate automatic syn-
tactic analysis into their work. We hope that the GR
31
representation scheme and the parser presented here
will make it possible and convenient for the child
language community to take advantage of some of
the recent developments in natural language parsing,
as was the case with part-of-speech tagging when
CHILDES specific tools were first made available.
Our immediate plans include continued improve-
ment of the parser, which can be achieved at least in
part by the creation of additional training data from
other English CHILDES corpora. We also plan to
release automatic syntactic analyses for the entire
English portion of CHILDES.
Although we have so far focused exclusively on
English CHILDES data, dependency schemes based
on functional relationships exist for a number of lan-
guages (Buchholz and Marsi, 2006), and the general
parsing techniques used in the present work have
been shown to be effective in several of them (Nivre
et al, 2006). As future work, we plan to adapt
existing dependency-based annotation schemes and
apply our current syntactic annotation and pars-
ing framework to other languages in the CHILDES
database.
Acknowledgments
We thank Marina Fedner for her help with annota-
tion of the Eve corpus. This work was supported in
part by the National Science Foundation under grant
IIS-0414630.
References
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
Amaximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Roger Brown. 1973. A first language: the early stages.
George Allen & Unwin Ltd., London.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new proposal.
In Proceedings of the 1st International Conference on
Language Resources and Evaluation, pages 447?454,
Granada, Spain.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
D. Knuth. 1965. On the translation of languages from
left to right. Information and Control, 8(6):607?639.
Brian MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates,
Mahwah, NJ, third edition.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of the Tenth Conference on
Computational Natural Language Learning.
Joakim Nivre, editor. 2007. CoNLL-XI Shared Task on
Multilingual Dependency Parsing, Prague, June. As-
sociation for Computational Linguistics.
Ann M. Peters. 1983. The Units of Language Acquisi-
tion. Monographs in Applied Psycholinguistics. Cam-
bridge University Press, New York.
Ann M. Peters. 1987. The role of immitation in the de-
veloping syntax of a blind child. Text, 7:289?311.
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 691?698, Sydney, Australia, July. Association
for Computational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with lr models and parser
ensembles. In Proceedings of the Eleventh Conference
on Computational Natural Language Learning.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2004.
Adding syntactic annotations to transcripts of parent-
child dialogs. In Proceedings of the Fourth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2004), Lisbon, Portugal.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 197?204, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
B. Wilson and Ann M. Peters. 1988. What are you
cookin? on a hot?: A three-year-old blind child?s ?vi-
olation? of universal constraints on constituent move-
ment. Language, 64:249?273.
32
Proceedings of the Second Workshop on Statistical Machine Translation, pages 228?231,
Prague, June 2007. c?2007 Association for Computational Linguistics
Meteor: An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments
Alon Lavie and Abhaya Agarwal
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{alavie,abhayaa}@cs.cmu.edu
Abstract
Meteor is an automatic metric for Ma-
chine Translation evaluation which has been
demonstrated to have high levels of corre-
lation with human judgments of translation
quality, significantly outperforming the more
commonly used Bleu metric. It is one of
several automatic metrics used in this year?s
shared task within the ACL WMT-07 work-
shop. This paper recaps the technical de-
tails underlying the metric and describes re-
cent improvements in the metric. The latest
release includes improved metric parameters
and extends the metric to support evalua-
tion of MT output in Spanish, French and
German, in addition to English.
1 Introduction
Automatic Metrics for MT evaluation have been re-
ceiving significant attention in recent years. Evalu-
ating an MT system using such automatic metrics is
much faster, easier and cheaper compared to human
evaluations, which require trained bilingual evalua-
tors. Automatic metrics are useful for comparing
the performance of different systems on a common
translation task, and can be applied on a frequent
and ongoing basis during MT system development.
The most commonly used MT evaluation metric in
recent years has been IBM?s Bleu metric (Papineni
et al, 2002). Bleu is fast and easy to run, and it
can be used as a target function in parameter op-
timization training procedures that are commonly
used in state-of-the-art statistical MT systems (Och,
2003). Various researchers have noted, however, var-
ious weaknesses in the metric. Most notably, Bleu
does not produce very reliable sentence-level scores.
Meteor , as well as several other proposed metrics
such as GTM (Melamed et al, 2003), TER (Snover
et al, 2006) and CDER (Leusch et al, 2006) aim to
address some of these weaknesses.
Meteor , initially proposed and released in 2004
(Lavie et al, 2004) was explicitly designed to im-
prove correlation with human judgments of MT qual-
ity at the segment level. Previous publications on
Meteor (Lavie et al, 2004; Banerjee and Lavie,
2005) have described the details underlying the met-
ric and have extensively compared its performance
with Bleu and several other MT evaluation met-
rics. This paper recaps the technical details underly-
ing Meteor and describes recent improvements in
the metric. The latest release extends Meteor to
support evaluation of MT output in Spanish, French
and German, in addition to English. Furthermore,
several parameters within the metric have been opti-
mized on language-specific training data. We present
experimental results that demonstrate the improve-
ments in correlations with human judgments that re-
sult from these parameter tunings.
2 The Meteor Metric
Meteor evaluates a translation by computing a
score based on explicit word-to-word matches be-
tween the translation and a given reference trans-
lation. If more than one reference translation is
available, the translation is scored against each refer-
ence independently, and the best scoring pair is used.
Given a pair of strings to be compared, Meteor cre-
ates a word alignment between the two strings. An
alignment is mapping between words, such that ev-
ery word in each string maps to at most one word
in the other string. This alignment is incrementally
produced by a sequence of word-mapping modules.
The ?exact? module maps two words if they are ex-
actly the same. The ?porter stem? module maps two
words if they are the same after they are stemmed us-
ing the Porter stemmer. The ?WN synonymy? mod-
ule maps two words if they are considered synonyms,
based on the fact that they both belong to the same
?synset? in WordNet.
The word-mapping modules initially identify all
228
possible word matches between the pair of strings.
We then identify the largest subset of these word
mappings such that the resulting set constitutes an
alignment as defined above. If more than one maxi-
mal cardinality alignment is found, Meteor selects
the alignment for which the word order in the two
strings is most similar (the mapping that has the
least number of ?crossing? unigram mappings). The
order in which the modules are run reflects word-
matching preferences. The default ordering is to
first apply the ?exact? mapping module, followed by
?porter stemming? and then ?WN synonymy?.
Once a final alignment has been produced between
the system translation and the reference translation,
the Meteor score for this pairing is computed as
follows. Based on the number of mapped unigrams
found between the two strings (m), the total num-
ber of unigrams in the translation (t) and the total
number of unigrams in the reference (r), we calcu-
late unigram precision P = m/t and unigram recall
R = m/r. We then compute a parameterized har-
monic mean of P and R (van Rijsbergen, 1979):
Fmean =
P ? R
? ? P + (1? ?) ? R
Precision, recall and Fmean are based on single-
word matches. To take into account the extent to
which the matched unigrams in the two strings are
in the same word order, Meteor computes a penalty
for a given alignment as follows. First, the sequence
of matched unigrams between the two strings is di-
vided into the fewest possible number of ?chunks?
such that the matched unigrams in each chunk are
adjacent (in both strings) and in identical word or-
der. The number of chunks (ch) and the number of
matches (m) is then used to calculate a fragmenta-
tion fraction: frag = ch/m. The penalty is then
computed as:
Pen = ? ? frag?
The value of ? determines the maximum penalty
(0 ? ? ? 1). The value of ? determines the
functional relation between fragmentation and the
penalty. Finally, the Meteor score for the align-
ment between the two strings is calculated as:
score = (1? Pen) ? Fmean
In all previous versions of Meteor , the values of
the three parameters mentioned above were set to be:
? = 0.9, ? = 3.0 and ? = 0.5, based on experimen-
tation performed in early 2004. In the latest release,
we tuned these parameters to optimize correlation
with human judgments based on more extensive ex-
perimentation, as reported in section 4.
3 Meteor Implementations for
Spanish, French and German
We have recently expanded the implementation of
Meteor to support evaluation of translations in
Spanish, French and German, in addition to English.
Two main language-specific issues required adapta-
tion within the metric: (1) language-specific word-
matching modules; and (2) language-specific param-
eter tuning. The word-matching component within
the English version of Meteor uses stemming and
synonymy modules in constructing a word-to-word
alignment between translation and reference. The re-
sources used for stemming and synonymy detection
for English are the Porter Stemmer (Porter, 2001)
and English WordNet (Miller and Fellbaum, 2007).
In order to construct instances of Meteor for Span-
ish, French and German, we created new language-
specific ?stemming? modules. We use the freely
available Perl implementation packages for Porter
stemmers for the three languages (Humphrey, 2007).
Unfortunately, we have so far been unable to obtain
freely available WordNet resources for these three
languages. Meteor versions for Spanish, French
and German therefore currently include only ?exact?
and ?stemming? matching modules. We are investi-
gating the possibility of developing new synonymy
modules for the various languages based on alterna-
tive methods, which could then be used in place of
WordNet. The second main language-specific issue
which required adaptation is the tuning of the three
parameters within Meteor , described in section 4.
4 Optimizing Metric Parameters
The original version of Meteor (Banerjee and
Lavie, 2005) has instantiated values for three pa-
rameters in the metric: one for controlling the rela-
tive weight of precision and recall in computing the
Fmean score (?); one governing the shape of the
penalty as a function of fragmentation (?) and one
for the relative weight assigned to the fragmenta-
tion penalty (?). In all versions of Meteor to date,
these parameters were instantiated with the values
? = 0.9, ? = 3.0 and ? = 0.5, based on early data ex-
perimentation. We recently conducted a more thor-
ough investigation aimed at tuning these parameters
based on several available data sets, with the goal of
finding parameter settings that maximize correlation
with human judgments. Human judgments come in
the form of ?adequacy? and ?fluency? quantitative
scores. In our experiments, we looked at optimizing
parameters for each of these human judgment types
separately, as well as optimizing parameters for the
sum of adequacy and fluency. Parameter adapta-
229
Corpus Judgments Systems
NIST 2003 Ara-to-Eng 3978 6
NIST 2004 Ara-to-Eng 347 5
WMT-06 Eng-to-Fre 729 4
WMT-06 Eng-to-Ger 756 5
WMT-06 Eng-to-Spa 1201 7
Table 1: Corpus Statistics for Various Languages
tion is also an issue in the newly created Meteor
instances for other languages. We suspected that
parameters that were optimized to maximize corre-
lation with human judgments for English would not
necessarily be optimal for other languages.
4.1 Data
For English, we used the NIST 2003 Arabic-to-
English MT evaluation data for training and the
NIST 2004 Arabic-to-English evaluation data for
testing. For Spanish, German and French we used
the evaluation data provided by the shared task at
last year?s WMT workshop. Sizes of various corpora
are shown in Table 1. Some, but not all, of these data
sets have multiple human judgments per translation
hypothesis. To partially address human bias issues,
we normalize the human judgments, which trans-
forms the raw judgment scores so that they have sim-
ilar distributions. We use the normalization method
described in (Blatz et al, 2003). Multiple judgments
are combined into a single number by taking their
average.
4.2 Methodology
We performed a ?hill climbing? search to find the
parameters that achieve maximum correlation with
human judgments on the training set. We use Pear-
son?s correlation coefficient as our measure of corre-
lation. We followed a ?leave one out? training proce-
dure in order to avoid over-fitting. When n systems
were available for a particular language, we train the
parameters n times, leaving one system out in each
training, and pooling the segments from all other
systems. The final parameter values are calculated
as the mean of the n sets of trained parameters that
were obtained. When evaluating a set of parameters
on test data, we compute segment-level correlation
with human judgments for each of the systems in the
test set and then report the mean over all systems.
4.3 Results
4.3.1 Optimizing for Adequacy and Fluency
We trained parameters to obtain maximum cor-
relation with normalized adequacy and fluency judg-
Adequacy Fluency Sum
? 0.82 0.78 0.81
? 1.0 0.75 0.83
? 0.21 0.38 0.28
Table 2: Optimal Values of Tuned Parameters for
Different Criteria for English
Adequacy Fluency Sum
Original 0.6123 0.4355 0.5704
Adequacy 0.6171 0.4354 0.5729
Fluency 0.6191 0.4502 0.5818
Sum 0.6191 0.4425 0.5778
Table 3: Pearson Correlation with Human Judg-
ments on Test Data for English
ments separately and also trained for maximal corre-
lation with the sum of the two. The resulting optimal
parameter values on the training corpus are shown in
Table 2. Pearson correlations with human judgments
on the test set are shown in Table 3.
The optimal parameter values found are somewhat
different than our previous metric parameters (lower
values for all three parameters). The new parame-
ters result in moderate but noticeable improvements
in correlation with human judgments on both train-
ing and testing data. Tests for statistical significance
using bootstrap sampling indicate that the differ-
ences in correlation levels are all significant at the
95% level. Another interesting observation is that
precision receives slightly more ?weight? when op-
timizing correlation with fluency judgments (versus
when optimizing correlation with adequacy). Recall,
however, is still given more weight than precision.
Another interesting observation is that the value of
? is higher for fluency optimization. Since the frag-
mentation penalty reflects word-ordering, which is
closely related to fluency, these results are consistent
with our expectations. When optimizing correlation
with the sum of adequacy and fluency, optimal val-
ues fall in between the values found for adequacy and
fluency.
4.3.2 Parameters for Other Languages
Similar to English, we trained parameters for
Spanish, French and German on the available WMT-
06 training data. We optimized for maximum corre-
lation with human judgments of adequacy, fluency
and for the sum of the two. Resulting parameters
are shown in Table 4.3.2. For all three languages, the
parameters that were found to be optimal were quite
different than those that were found for English, and
using the language-specific optimal parameters re-
230
Adequacy Fluency Sum
French:? 0.86 0.74 0.76
? 0.5 0.5 0.5
? 1.0 1.0 1.0
German:? 0.95 0.95 0.95
? 0.5 0.5 0.5
? 0.6 0.8 0.75
Spanish:? 0.95 0.62 0.95
? 1.0 1.0 1.0
? 0.9 1.0 0.98
Table 4: Tuned Parameters for Different Languages
sults in significant gains in Pearson correlation levels
with human judgments on the training data (com-
pared with those obtained using the English opti-
mal parameters)1. Note that the training sets used
for these optimizations are comparatively very small,
and that we currently do not have unseen test data
to evaluate the parameters for these three languages.
Further validation will need to be performed once ad-
ditional data becomes available.
5 Conclusions
In this paper we described newly developed
language-specific instances of the Meteor metric
and the process of optimizing metric parameters for
different human measures of translation quality and
for different languages. Our evaluations demonstrate
that parameter tuning improves correlation with hu-
man judgments. The stability of the optimized pa-
rameters on different data sets remains to be inves-
tigated for languages other than English. We are
currently exploring broadening the set of features
used in Meteor to include syntax-based features
and alternative notions of synonymy. The latest re-
lease of Meteor is freely available on our website
at: http://www.cs.cmu.edu/~alavie/METEOR/
Acknowledgements
The work reported in this paper was supported by
NSF Grant IIS-0534932.
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An Automatic Metric for MT Evalua-
tion with Improved Correlation with Human Judg-
ments. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures
1Detailed tables are not included for lack of space.
for Machine Translation and/or Summarization,
pages 65?72, Ann Arbor, Michigan, June.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Technical Re-
port Natural Language Engineering Workshop Fi-
nal Report, Johns Hopkins University.
Marvin Humphrey. 2007. Perl In-
terface to Snowball Stemmers.
http://search.cpan.org/ creamyg/Lingua-Stem-
Snowball-0.941/lib/Lingua/Stem/Snowball.pm.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The Significance of Recall in Auto-
matic Metrics for MT Evaluation. In Proceedings
of the 6th Conference of the Association for Ma-
chine Translation in the Americas (AMTA-2004),
pages 134?143, Washington, DC, September.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using
Block Movements. In Proceedings of the Thir-
teenth Conference of the European Chapter of the
Association for Computational Linguistics.
I. Dan Melamed, Ryan Green, and Joseph Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the HLT-NAACL 2003
Conference: Short Papers, pages 61?63, Edmon-
ton, Alberta.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Pro-
ceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 311?
318, Philadelphia, PA, July.
Martin Porter. 2001. The Porter Stem-
ming Algorithm. http://www.tartarus.org/ mar-
tin/PorterStemmer/index.html.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proceedings of the 7th Confer-
ence of the Association for Machine Translation in
the Americas (AMTA-2006), pages 223?231, Cam-
bridge, MA, August.
C. van Rijsbergen, 1979. Information Retrieval.
Butterworths, London, UK, 2nd edition.
231
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 65?72,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation
Idan Szpektor, Ido Dagan
Dept. of Computer Science
Bar Ilan University
szpekti@cs.biu.ac.il
Alon Lavie
Language Technologies Inst.
Carnegie Mellon University
alavie+@cs.cmu.edu
Danny Shacham, Shuly Wintner
Dept. of Computer Science
University of Haifa
shuly@cs.haifa.ac.il
Abstract
We describe a system which enhances the
experience of museum visits by providing
users with language-technology-based in-
formation retrieval capabilities. The sys-
tem consists of a cross-lingual search en-
gine, augmented by state of the art semantic
expansion technology, specifically designed
for the domain of the museum (history and
archaeology of Israel). We discuss the tech-
nology incorporated in the system, its adap-
tation to the specific domain and its contri-
bution to cultural heritage appreciation.
1 Introduction
Museum visits are enriching experiences: they pro-
vide stimulation to the senses, and through them to
the mind. But the experience does not have to end
when the visit ends: further exploration of the ar-
tifacts and their influence on the visitor is possible
after the visit, either on location or elsewhere. One
common means of exploration is Information Re-
trieval (IR) via a Search Engine. For example, a mu-
seum could implement a search engine over a col-
lection of documents relating to the topics exhibited
in the museum.
However, such document collections are usually
much smaller than general collections, in particular
the World Wide Web. Consequently, phenomena in-
herent to natural languages may severely hamper the
performance of human language technology when
applied to small collections. One such phenomenon
is the semantic variability of natural languages, the
ability to express a specific meaning in many dif-
ferent ways. For example, the expression ?Archae-
ologists found a new tomb? can be expressed also
by ?Archaeologists discovered a tomb? or ?A sar-
cophagus was dug up by Egyptian Researchers?. On
top of monolingual variability, the same information
can also be expressed in different languages. Ignor-
ing natural language variability may result in lower
recall of relevant documents for a given query, espe-
cially in small document collections.
This paper describes a system that attempts to
cope with semantic variability through the use of
state of the art human language technology. The
system provides both semantic expansion and cross
lingual IR (and presentation of information) in the
domain of archaeology and history of Israel. It
was specifically developed for the Hecht Museum
in Haifa, Israel, which contains a small but unique
collection of artifacts in this domain. The system
provides different users with different capabilities,
bridging over language divides; it addresses seman-
tic variation in novel ways; and it thereby comple-
ments the visit to the museum with long-lasting in-
stillation of information.
The main component of the system is a domain-
specific search engine that enables users to specify
queries and retrieve information pertaining to the do-
main of the museum. The engine is enriched by lin-
guistic capabilities which embody an array of means
for addressing semantic variation. Queries are ex-
panded using two main techniques: semantic expan-
sion based on textual entailment; and cross-lingual
expansion based on translation of Hebrew queries
to English and vice versa. Retrieved documents are
presented as links with associated snippets; the sys-
tem also translates snippets from Hebrew to English.
The main contribution of this work is, of course,
the system itself, which was recently demonstrated
65
successfully at the museum and which we believe
could be useful to a variety of museum visitor types,
from children to experts. For example, the system
provides Hebrew speakers access to English doc-
uments pertaining to the domain of the museum,
and vice versa, thereby expanding the availability
of multilingual material to museum visitors. More
generally, it is an instance of adaptation of state of
the art human language technology to the domain
of cultural heritage appreciation, demonstrating how
general resources and tools are adapted to a specific
domain, thereby improving their accuracy and us-
ability. Finally, it provides a test-bed for evaluating
the contribution of language technology in general,
as well as specific components and resources, to a
large-scale natural language processing system.
2 Background and Motivation
Internet search is hampered by the complexity of
natural languages. The two main characteristics of
this complexity are ambiguity and variability: the
former refers to the fact that a given text can be
interpreted in more than one way; the latter indi-
cates that the same meaning can be linguistically ex-
pressed in several ways. The two phenomena make
simple search techniques too weak for unsophisti-
cated users, as existing search engines perform only
direct keyword matching, with very limited linguis-
tic processing of the texts they retrieve.
Specifically, IR systems that do not address the
variability in languages may suffer from lower re-
call, especially in restricted domains and small doc-
ument locations. We next describe two prominent
types of variability that we think should be ad-
dressed in IR systems.
2.1 Textual Entailment and Entailment Rules
In many NLP applications, such as Question An-
swering (QA), Information Extraction (IE) and In-
formation Retrieval (IR), it is crucial to recognize
that a specific target meaning can be inferred from
different text variants. For example, a QA system
needs to induce that ?Mendelssohn wrote inciden-
tal music? can be inferred from ?Mendelssohn com-
posed incidental music? in order to answer the ques-
tion ?Who wrote incidental music??. This type of
reasoning has been identified as a core semantic in-
ference task by the generic textual entailment frame-
work (Dagan et al, 2006; Bar-Haim et al, 2006).
The typical way to address variability in IR is to
use lexical query expansion (Lytinen et al, 2000;
Zukerman and Raskutti, 2002). However, there are
variability patterns that cannot be described using
just constant phrase to phrase entailment. Another
important type of knowledge representation is en-
tailment rules and paraphrases. An entailment rule
is a directional relation between two templates, text
patterns with variables, e.g., ?X compose Y ?
X write Y ?. The left hand side is assumed to en-
tail the right hand side in certain contexts, under
the same variable instantiation. Paraphrases can be
viewed as bidirectional entailment rules. Such rules
capture basic inferences in the language, and are
used as building blocks for more complex entail-
ment inference. For example, given the above en-
tailment rule, a QA system can identify the answer
?Mendelssohn? in the above example. This need
sparked intensive research on automatic acquisition
of paraphrase and entailment rules.
Although knowledge-bases of entailment-rules
and paraphrases learned by acquisition algorithms
were used in other NLP applications, such as QA
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002) and IE (Sudo et al, 2003; Romano et al,
2006), to the best of our knowledge the output of
such algorithms was never applied to IR before.
2.2 Cross Lingual Information Retrieval
The difficulties caused by variability are amplified
when the user is not a native speaker of the language
in which the retrieved texts are written. For exam-
ple, while most Israelis can read English documents,
fewer are comfortable with the specification of Eng-
lish queries. In a museum setting, some visitors may
be able to read Hebrew documents but still be rel-
atively poor at searching for them. Other visitors
may be unable to read Hebrew texts, but still benefit
from non-textual information that are contained in
Hebrew documents (e.g., pictures, maps, audio and
video files, external links, etc.)
This problem is addressed by the paradigm of
Cross-Lingual Information Retrieval (CLIR). This
paradigm has become a very active research area
in recent years, addressing the needs of multilingual
and non-English speaking communities, such as the
66
European Union, East-Asian nations and Spanish
speaking communities in the US (Hull and Grefen-
stette, 1996; Ballesteros and Croft, 1997; Carbonell
et al, 1997). The common approach for CLIR is
to translate a query in a source language to another
target language and then issue the translated query
to retrieve target language documents. As explained
above, CLIR research has to address various generic
problems caused by the variability and ambiguity of
natural languages, as well as specific problems re-
lated to the particular languages being addressed.
3 Coping with Semantic Variability in IR
We describe a search engine that is capable of per-
forming: (a) semantic English information retrieval;
and (b) cross-lingual (Hebrew-English and English-
Hebrew) information retrieval, allowing users to
pose queries in either of the two languages and re-
trieve documents in both. This is achieved by two
sub-processes of the search engine: first, the en-
gine performs shallow semantic linguistic inference
and supports the retrieval of documents which con-
tain phrases that imply the meaning of the translated
query, even when no exact match of the translated
keywords is found. This is enabled by automatic ac-
quisition of semantic variability patterns that are fre-
quent in the language, which extend traditional lexi-
cal query expansion techniques. Second, the engine
translates the original or expanded query to the tar-
get language, based on several linguistic processes
and a machine readable bilingual dictionary. The re-
sult is a semantic expansion of a given query to a va-
riety of alternative wordings in which an answer to
this query may be expressed in the target language
of the retrieved documents.
These enhancements are facilitated via a speci-
fication of the domain. As our system is specifi-
cally designed to work in the domain of the history
and archaeology, we could focus our attention on re-
sources and tools that are dedicated to this domain.
Thus, for example, lexicons and dictionaries, whose
preparation is always costly and time consuming,
were developed with the specific domain in mind;
and textual entailment and paraphrase patterns were
extracted for the specific domain. While the result-
ing system is focused on visiting the Hecht Museum,
the methodology which we used and discuss here
can be adapted to other areas of cultural heritage, as
well as to other narrow domains, in the same way.
3.1 Setting Up a Basic Retrieval Application
We created a basic retrieval system in two steps:
first, we collected relevant documents; then, we im-
plemented a search engine over the collected docu-
ments.
In order to construct a local corpus, an archae-
ology expert searched the Web for relevant sites
and pages. We then downloaded all the documents
linked from those pages using a crawler. The expert
looked for documents in both English and Hebrew.
In total, we collected a non-comparable bilingual
corpus for Archaeology containing several thousand
documents in English and Hebrew.
We implemented our enhanced retrieval modules
on top of the basic Jakarta Lucene indexing and
search engine1. All documents were indexed using
Lucene, but instead of inflected words, we indexed
the lemma of each word (see detailed description of
our Hebrew lemmatization in Section 3.3). In order
to match the indexed terms, query terms (either He-
brew or English) were also lemmatized before the
index was searched, in a manner similar to lemma-
tizing the documents.
3.2 Query Expansion Using Entailment Rules
As described in Section 2.1, entailment rules had not
been used as a knowledge resource for expanding IR
queries, prior to our work. In this paper we use this
resource instead of the typical lexical expansion in
order to test its benefit. Most entailment rules cap-
ture relations between different predicates. We thus
focus on documents retrieved for queries that con-
tain a predicate over one or two entities, which we
term here Relational IR. We would like to retrieve
only documents that describe an occurrence of that
predicate, but possibly in words different than the
ones used in the query. In this section we describe
in detail how we learn entailment rules and how we
apply them in query expansion.
Automatically Learning Entailment Rules from
the Web Many algorithms for automatically learn-
ing paraphrases and entailment rules have been
explored in recent years (Lin and Pantel, 2001;
1http://jakarta.apache.org/lucene/docs/index.html
67
Ravichandran and Hovy, 2002; Shinyama et al,
2002; Barzilay and Lee, 2003; Sudo et al, 2003;
Szpektor et al, 2004; Satoshi, 2005). In this pa-
per we use TEASE (Szpektor et al, 2004), a state-
of-the-art unsupervised acquisition algorithm for
lexical-syntactic entailment rules.
TEASE acquires entailment relations for a given
input template from the Web. It first retrieves from
the Web sentences that match the input template.
From these sentences it extracts the variable instan-
tiations, termed anchor-sets, which are identified as
being characteristic for the input template based on
statistical criteria.
Next, TEASE retrieves from the Web sentences
that contain the extracted anchor-sets. The retrieved
sentences are parsed and the anchors found in each
sentence are replaced with their corresponding vari-
ables. Finally, from this retrieved corpus of parsed
sentences, templates that are assumed to entail or
be entailed by the input template are learned. The
learned templates are ranked by the number of oc-
currences they were learned from.
Entailment Rules for Domain Specific Query Ex-
pansion Our goal is to use the knowledge-base of
entailment rules learned by TEASE in order to per-
form query expansion. The two subtasks that arise
are: (a) acquiring an appropriate knowledge-base
of rules; and (b) expanding a query given such a
knowledge-base.
TEASE learns entailment rules for a given input
template. As our document collection is domain
specific, a list of such relevant input templates can
be prepared. In our case, we used an archaeology
expert to generate a list of verbs and verb phrases
that relate to archaeology, such as: ?excavate?, ?in-
vade?, ?build?, ?reconstruct?, ?grow? and ?be located
in?. We then executed TEASE on each of the tem-
plates representing these verbs in order to learn from
the Web rules in which the input templates partici-
pate. An example for such rules is presented in Ta-
ble 1. We learned approximately 3900 rules for 80
input templates.
Since TEASE learns lexical-syntactic rules, we
need a syntactic representation of the query. We
parse each query using the Minipar dependency
parser (Lin, 1998). We next try to match the left
hand side template of every rule in the learned
knowledge-base. Since TEASE does not identify
the direction of the relation learned between two
templates, we try both directional rules that are in-
duced from a learned relation. Whenever a match
is found, a new query is generated, in which the
constant terms of the matched left hand side tem-
plate are replaced with the constant terms of the right
hand side template. For example, given the query
?excavations of Jerusalem by archaeologists? and a
learned rule ?excavation of Y by X ? X dig in Y ?,
a new query is generated, containing the terms ?ar-
chaeologists dig in Jerusalem?. Finally, we retrieve
all the documents that contain all the terms of at least
one of the expanded queries (including the original
query). The basic search engine provides a score for
each document. We re-score each document as the
sum of scores it obtained from the different queries
that it matched. Figure 1 shows an example of our
query expansion, where the first retrieved documents
do not contain the words used to describe the predi-
cate in the query, but other ways to describe it.
All the templates learned by TEASE contain two
variables, and thus the rules that are learned can only
be applied to queries that contain predicates over
two terms. In order to broaden the coverage of the
learned rules, we automatically generate also all the
partial templates of a learned template. These are
templates that contain just one of variables in the
original template. We then generate rules between
these partial templates that correspond to the origi-
nal rules. With partial templates/rules, expansion for
the query in Figure 1 becomes possible.
3.3 Cross-lingual IR
Until very recently, linguistic resources for Hebrew
were few and far between (Wintner, 2004). The last
few years, however, have seen a proliferation of re-
sources and tools for this language. In this work we
utilize a relatively large-scale lexicon of over 22,000
entries (Itai et al, 2006); a finite-state based mor-
phological analyzer of Hebrew that is directly linked
to the lexicon (Yona and Wintner, 2007); a medium-
size bilingual dictionary of some 24,000 word pairs;
and a rudimentary Hebrew to English machine trans-
lation system (Lavie et al, 2004). All these re-
sources had to be adapted to the domain of the Hecht
museum.
Cross-lingual language technology is utilized in
68
Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two
retrieved texts (listed under ?matched query?) do not contain the original query.
three different components of the system: Hebrew
documents are morphologically processed to pro-
vide better indexing; query terms in English are
translated to Hebrew and vice versa; and Hebrew
snippets are translated to English. We discuss each
of these components in this section.
Linguistically-aware indexing The correct level
of indexing for morphologically-rich language has
been a matter of some debate in the information re-
trieval literature. When Arabic is concerned, Dar-
wish and Oard (2002) conclude that ?Character n-
grams or lightly stemmed words were found to
typically yield near-optimal retrieval effectiveness?.
Since Hebrew is even more morphologically (and
orthographically) ambiguous than Arabic, and espe-
cially in light of the various prefix particles which
can be attached to Hebrew words, we opted for full
morphological analysis of Hebrew documents be-
fore they are indexed, followed by indexing on the
lexeme.
We use the HAMSAH morphological analyzer
(Yona and Wintner, 2007), which was recently re-
written in Java and is therefore more portable and
efficient (Wintner, 2007). We processed the entire
domain specific corpus described above and used
the resulting lexemes to index documents. This pre-
processing brought to the foreground several omis-
sions of the analyzer, mostly due to domain-specific
terms missing in the lexicon. We selected the one
thousand most frequent words with no morphologi-
cal analysis and added their lexemes to the lexicon.
While we do not have quantitative evaluation met-
rics, the coverage of the system improved in a very
evident way.
Query translation When users submit a query in
one language they are provided with the option to re-
quest a translation of the query to the other language,
thereby retrieving documents in the other language.
The motivation behind this capability is that users
who may be able to read documents in a language
may find the specification of queries in that language
too challenging; also, retrieving documents in a for-
eign language may be useful due to the non-textual
information in the retrieved documents, especially in
a museum environment.
In order to support cross-lingual query specifica-
tion we capitalized on a medium-size bilingual dic-
tionary that was already used for Hebrew to Eng-
lish machine translation. Since the coverage of the
dictionary was rather limited, and many domain-
specific items were missing, we chose the one thou-
sand most frequent lexemes which had no transla-
69
Input Template Learned Template
X excavate Y X discover Y , X find Y ,
X uncover Y , X examine Y ,
X unearth Y , X explore Y
X construct Y X build Y , X develop Y ,
X create Y , X establish Y
X contribute to Y X cause Y , X linked to Y ,
X involve in Y
date X to Y X built in Y , X began in Y ,
X go back to Y
X cover Y X bury Y ,
X provide coverage for Y
X invade Y X occupy Y , X attack Y ,
X raid Y , X move into Y
X restore Y X protect Y , X preserve Y ,
X save Y , X conserve Y
Table 1: Examples for correct templates that were
learned by TEASE for input templates.
tions and translated them manually, augmenting the
lexicon with missing Hebrew lexemes where neces-
sary and expanding the bilingual dictionary to cover
this domain.
In order to translate query terms we use the He-
brew English dictionary also as an English-Hebrew
dictionary. While this is known to be sub-optimal,
our current results support such an adaptation in lieu
of dedicated directional bilingual dictionaries.
Translating a query from one language to another
may introduce ambiguity where none exists. For
example, the query term spinh ?vessel? is unam-
biguous in Hebrew, but once translated into English
will result in retrieving documents on both senses
of the English word. Usually, this problem is over-
come since users tend to specify multi-term queries,
and the terms disambiguate each other. However,
a more systematic solution can be offered since we
have access to semantic expansion capabilities (in a
single language). That is, expanding the query in
the source language will result in more query terms
which, when translated, are more likely to disam-
biguate the context. We leave such an extension for
future work.
Snippet translation When Hebrew documents are
retrieved, we augment the (Hebrew) snippet which
the system produces by an English translation. We
use an extended, improved version of a rudimentary
Hebrew to English MT system developed by Lavie
et al (2004). Extensions include an improved mor-
phological analysis of the input, an extended bilin-
gual dictionary and a revised set of transfer rules,
as well as a more modern transfer engine and a
much larger language model for generating the tar-
get (English) sentences.
The MT system is transfer based: it performs lin-
guistic pre-processing of the source language (in our
case, morphological analysis) and post-processing
of the target (generation of English word forms), and
uses a small set of transfer rules to translate local
structures from the source to the target and create
translation hypotheses, which are stored in a lattice.
A statistical language model is used to decode the
lattice and select the best hypotheses.
The benefit of this architecture is that domain spe-
cific adaptation of the system is relatively easy, and
does not require a domain specific parallel corpus
(which we do not have). The system has access
to our domain-specific lexicon and bilingual dictio-
nary, and we even refined some transfer rules due to
peculiarities of the domain. One advantage of the
transfer-based approach is that it enables us to treat
out-of-lexicon items in a unique way. We consider
such items proper names, and transfer rules process
them as such. As an example, Figure 2 depicts the
translation of a Hebrew snippet meaning A jar from
the early bronze period with seashells from the Nile.
The word nilws ?Nile? is missing from the lexicon,
but this does not prevent the system from producing
a legible translation, using the transliterated form
where an English equivalent is unavailable.
4 Conclusions
We described a system for cross-lingual and
semantically-enhanced retrieval of information in
the cultural heritage domain, obtained by adapting
existing state-of-the-art tools and resources to the
domain. The system enhances the experience of mu-
seum visits, using language technology as a vehi-
cle for long-lasting instillation of information. Due
to the novelty of this application and the dearth of
available multilingual annotated resources in this
domain, we are unable to provide a robust, quan-
70
Figure 2: Translation example
Query Without Expansion With Expansion
Relevant Total Relevant Total
in Top 10 Retrieved in Top 10 Retrieved
discovering boats 2 2 5 86
growing vineyards 0 0 6 8
Persian invasions 5 5 8 22
excavations of the Byzantine period 10 37 10 100
restoring mosaics 0 0 3 69
Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved
documents (up to 100) for a sample of queries.
titative evaluation of the approach. A preliminary
analysis of a sample of queries is presented in Ta-
ble 2. It illustrates the potential of expansion for
document collections of narrow domain. In what
follows we provide some qualitative impressions.
We observed that the system was able to learn
many expansion rules that cannot be induced from
manually constructed lexical resources, such as the-
sauri or WordNet (Fellbaum, 1998). This is espe-
cially true for rules that are specific for a narrow do-
main, e.g. ?X restore Y ? X preserve Y ?. Fur-
thermore, the system learned lexical syntactic rules
that cannot be expressed by a mere lexical substitu-
tion, but include also a syntactic transformation. For
example, ?date X to Y ? X go back to Y ?.
In addition, since rules are acquired by searching
the Web, they are not necessarily restricted to learn-
ing from the target domain, but can be learned from
similar terminology in other domains. For example,
the rule ?X discover Y ? X find Y ? was learned
from contexts such as {X=?astronomers? ;Y =?new
planets?} and {X=?zoologists? ;Y =?new species?}.
The quality of the rules that were automatically
acquired is mediocre. We found that although many
rules were useful for expansion, they had to be
manually filtered in order to retain only rules that
achieved high precision.
Finally, we note that applying semantic query ex-
pansion (using entailment rules), followed by Eng-
lish to Hebrew query translation, results in query ex-
pansion for Hebrew using techniques that were so
far applicable only to resource-rich languages, such
as English.
Acknowledgements
This research was supported by the Israel Internet
Association; by THE ISRAEL SCIENCE FOUN-
DATION (grant No. 137/06 and grant No. 1095/05);
by the Caesarea Rothschild Institute for Interdisci-
plinary Application of Computer Science at the Uni-
versity of Haifa; by the ITC-irst/University of Haifa
collaboration; and by the US National Science Foun-
dation (grants IIS-0121631, IIS-0534217, and the
Office of International Science and Engineering).
71
We wish to thank the Hebrew Knowledge Center
at the Technion for providing resources for Hebrew.
We are grateful to Oliviero Stock, Martin Golumbic,
Alon Itai, Dalia Bojan, Erik Peterson, Nurit Mel-
nik, Yaniv Eytani and Noam Ordan for their help
and support.
References
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 84?91.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Jaime G. Carbonell, Yiming Yang, Robert E. Frederk-
ing, Ralf D. Brown, Yibing Geng, and Danny Lee.
1997. Translingual information retrieval: A compar-
ative evaluation. In IJCAI (1), pages 708?715.
Ido Dagan, Oren Glickman, and Bernardo. Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Lecture Notes in Computer Science, Volume
3944, volume 3944, pages 177?190.
Kareem Darwish and Douglas W. Oard. 2002. Term se-
lection for searching printed Arabic. In SIGIR ?02:
Proceedings of the 25th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 261?268, New York, NY,
USA. ACM Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
D. A. Hull and G. Grefenstette. 1996. Querying across
languages. a dictionary-based approach to multilingual
information retrieval. In Proceedings of the 19th ACM
SIGIR Conference, pages 49?57.
Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A
computational lexicon of contemporary Hebrew. In
Proceedings of The fifth international conference on
Language Resources and Evaluation (LREC-2006).
Alon Lavie, Shuly Wintner, Yaniv Eytani, Erik Peterson,
and Katharina Probst. 2004. Rapid prototyping of a
transfer-based Hebrew-to-English machine translation
system. In Proceedings of TMI-2004: The 10th Inter-
national Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, MD,
October.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
S. Lytinen, N. Tomuro, and T. Repede. 2000. The use of
wordnet sense tagging in faqfinder. In Proceedings of
the AAAI00 Workshop on AI and Web Search.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Sekine Satoshi. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of HLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of ACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shuly Wintner. 2004. Hebrew computational linguis-
tics: Past and future. Artificial Intelligence Review,
21(2):113?138.
Shuly Wintner. 2007. Finite-state technology as a pro-
gramming environment. In Alexander Gelbukh, edi-
tor, Proceedings of the Conference on Computational
Linguistics and Intelligent Text Processing (CICLing-
2007), volume 4394 of Lecture Notes in Computer Sci-
ence, pages 97?106, Berlin and Heidelberg, February.
Springer.
Shlomo Yona and Shuly Wintner. 2007. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering. To appear.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In Pro-
ceedings of ACL.
72
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 117?125,
Prague, June 2007. c?2007 Association for Computational Linguistics
ParaMor: Minimally Supervised Induction of Paradigm  
 Structure and Morphological Analysis 
 
Christian Monson, Jaime Carbonell, Alon Lavie, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA, USA 15213 
{cmonson, alavie+, jgc+, lsl+}@cs.cmu.edu 
Abstract 
Paradigms provide an inherent 
organizational structure to natural language 
morphology. ParaMor, our minimally 
supervised morphology induction 
algorithm, retrusses the word forms of raw 
text corpora back onto their paradigmatic 
skeletons; performing on par with state-of-
the-art minimally supervised morphology 
induction algorithms at morphological 
analysis of English and German. ParaMor 
consists of two phases. Our algorithm first 
constructs sets of affixes closely mimicking 
the paradigms of a language. And with 
these structures in hand, ParaMor then 
annotates word forms with morpheme 
boundaries. To set ParaMor?s few free 
parameters we analyze a training corpus of 
Spanish. Without adjusting parameters, we 
induce the morphological structure of 
English and German. Adopting the 
evaluation methodology of Morpho 
Challenge 2007 (Kurimo et al, 2007), we 
compare ParaMor?s morphological 
analyses with Morfessor (Creutz, 2006), a 
modern minimally supervised morphology 
induction system. ParaMor consistently 
achieves competitive F1 measures. 
1 Introduction 
Words in natural language (NL) have internal 
structure. Morphological processes derive new lex-
emes from old ones or inflect the surface form of 
lexemes to mark morphosyntactic features such as 
tense, number, person, etc. This paper address 
minimally supervised induction of productive natu-
ral language morphology from text. Minimally su-
pervised induction of morphology interests us both 
for practical and theoretical reasons. In linguistic 
theory, the morpheme is often defined as the 
smallest unit of language which conveys meaning. 
And yet, without annotating for meaning, recent 
work on minimally supervised morphology induc-
tion from written corpora has met with some suc-
cess (Creutz, 2006). We are curious how far this 
program can be pushed. From a practical perspec-
tive, minimally supervised morphology induction 
would help create morphological analysis systems 
for languages outside the traditional scope of NLP. 
However, to develop our method we induce the 
morphological structure of three well-understood 
languages, English, German, and Spanish. 
1.1 Inherent Structure in NL Morphology 
The approach we have taken to induce morpho-
logical structure has explicit roots in linguistic the-
ory. Cross-linguistically, natural language organ-
izes inflectional morphology into paradigms and 
inflection classes. A paradigm is a set of mutually 
exclusive operations that can be performed on a 
word form. Each mutually exclusive morphologi-
cal operation in a paradigm marks a lexeme for 
some set or cell of morphosyntactic features. An 
inflection class, meanwhile, specifies the proce-
dural details that a particular set of adherent lex-
emes follow to realize the surface form filling each 
paradigm cell. Each lexeme in a language adheres 
to a single inflection class for each paradigm the 
lexeme realizes. The lexemes belonging to an in-
flection class may have no relationship binding 
them together beyond an arbitrary morphological 
stipulation that they adhere to the same inflection 
class. But for this paper, an inflection class may 
117
also refer to a set of lexemes that inflect similarly 
for phonological or orthographic reasons. Working 
with text we intentionally blur phonology and or-
thography. 
A simple example will help illustrate paradigms, 
inflection classes, and the mutual exclusivity of 
cells. As shown in Table 1, all English verbs 
belong to a single common paradigm of five cells: 
One cell marks a verb for the morphosyntactic 
feature values present tense 3rd person, as in eats; 
another cell marks past tense, as in ate; a third cell 
holds a surface form typically used to mark 
progressive aspect, eating; a fourth produces a 
passive participle, eaten; and finally there is the 
unmarked cell, in this example eat.  
Aside from inflection classes each containing 
only a few irregular lexemes, such as that 
containing eat, there are no English verbal 
inflection classes that arbitrarily differentiate 
lexemes on purely morphological grounds. There 
are, however, several inflection classes that realize 
surface forms only for verbs with particular 
phonology or orthography. The ?silent-e? inflection 
class is one such. To adhere to the ?silent-e? 
inflection class a lexeme must fill the unmarked 
paradigm cell with a form that ends in an unspoken 
character e, as in dance. The other paradigm cells 
in the ?silent-e? inflection class are filled by 
applying orthographic rules such as:  
Progressive Aspect Cell ? replace the final e of 
the unmarked form with the string ing, 
dance  dancing  
Past Cell ? substitute ed, dance  danced  
Paradigm cells are mutually exclusive. In the Eng-
lish verbal paradigm, although English speakers 
can express progressive past actions with a 
grammatical construction, viz. was eating, there is 
no surface form of the lexeme eat that 
simultaneously fills both the progressive and the 
past cells of the verbal paradigm, *ateing. 
1.2 ParaMor 
Paradigms and inflection classes, the inherent 
structure of natural language morphology, form the 
basis of ParaMor, our minimally supervised 
morphological induction algorithm. In ParaMor?s 
first phase, we find sets of mutually exclusive 
strings which closely mirror the inflection classes 
of a language?although ParaMor does not 
differentiate between syncretic word forms of the 
same lexeme filling different paradigm cells, such 
as ed-suffixed forms which can fill either the past 
or the passive cells of English verbs. In ParaMor?s 
second phase we employ the structured knowledge 
contained within the discovered inflection classes 
to segment word forms into morpheme-like pieces.  
Languages employ a variety of morphological 
processes to arrive at grammatical word forms?
processes including suffix-, prefix-, and infixation, 
reduplication, and template filling. Furthermore, 
the application of word forming processes often 
triggers phonological (or orthographic) change, 
such a as the dropped final e of the ?silent-e? 
inflection class, see Table 1. Despite the wide 
range of morphological processes and their 
complicating concomitant phonology, a large caste 
of inflection classes, and hence paradigms, can be 
represented as mutually exclusive substring 
substitutions. In the ?silent-e? inflection class, for 
example, the word-final strings e.ed.es.ing can be 
substituted for one another to produce the surface 
forms that fill the paradigm cells of lexemes 
belonging to this inflection class. In this paper we 
focus on identifying word final suffix morphology. 
While we focus on suffixes, the methods we 
employ can be straightforwardly generalized to 
prefixes and ongoing work seeks to model 
sequences of concatenative morphemes. 
Inducing the morphology of a language from a 
naturally occurring text corpus is challenging. In 
languages with a rich morphological structure, sur-
face forms filling particular cells of an inflection 
class may be relatively rare. In the Spanish news-
wire text over which we developed ParaMor there 
are 50,000 unique types. Among these types, in-
Table 1: The English verbal paradigm, left col-
umn, and two inflection classes of the verbal 
paradigm. The verb eat fills the cells of its in-
flection class with the five surface forms 
shown in the second column. Verbs belonging 
to the ?silent-e? inflection class inflect follow-
ing the pattern of the third column. 
            Inflection Class Paradigm 
Cells ?eat? ?silent-e? 
Unmarked eat dance, erase, ? 
Present, 3rd eats dances, erases, ? 
Past Tense ate danced, erased, ? 
Progressive eating dancing, erasing, ? 
Passive eaten danced, erased, ? 
 
118
stances of first and second person verb forms are 
few. The suffix imos which fills the first person 
plural indicative present cell for the ir verbal in-
flection class of Spanish occurs on only 77 unique 
lexemes. And yet we aim to identify candidate in-
flection classes which closely model the true in-
flection classes of a language, covering as many 
inflectional paradigm cells as possible. 
Fortunately, we can leverage the paradigm struc-
ture of natural language morphology itself to retain 
many inflections which, because of data sparse-
ness, might be missed if considered in isolation. 
ParaMor begins with a recall-centric search for 
partial candidate inflection classes. Many of the 
candidates which result from this initial search are 
incorrect. But intermingled with the false positives 
are candidates which collectively model significant 
fractions of true inflection classes. Hence, Pa-
raMor?s next step is to cluster the initial partial 
candidate inflection classes into larger groups. This 
clustering effectively uses the larger correct initial 
candidates as nuclei to which smaller correct can-
didates accrete. With as many initial true candi-
dates as possible safely corralled with other candi-
dates covering the same inflection class, ParaMor 
completes the paradigm discovery phase by dis-
carding the large number of erroneous initially se-
lected candidate inflection classes. Finally, with a 
strong grasp on the paradigm structure, ParaMor 
straightforwardly segments the words of a corpus 
into morphemes. 
1.3 Related Work 
In this section we highlight previously proposed 
minimally supervised approaches to the induction 
of morphology that, like ParaMor, draw on the 
unique structure of natural language morphology. 
One facet of NL morphological structure com-
monly leveraged by morphology induction algo-
rithms is that morphemes are recurrent building 
blocks of words. Brent et al (1995), Goldsmith 
(2001), and Creutz (2006) emphasize the building 
block nature of morphemes when they each use 
recurring word segments to efficiently encode a 
corpus. These approaches then hypothesize that 
those recurring segments which most efficiently 
encode a corpus are likely morphemes. Another 
technique that exploits morphemes as repeating 
sub-word segments encodes the lexemes of a cor-
pus as a  character tree, i.e. trie, (Harris, 1955; 
Hafer and Weis, 1974), or as a finite state automa-
ton (FSA) over characters (Johnson, H. and Martin, 
2003; Altun and M. Johnson, 2001). A trie or FSA 
conflates multiple instances of a morpheme into a 
single sequence of states. Because the choice of 
possible succeeding characters is highly con-
strained within a morpheme, branch points in the 
trie or FSA are likely morpheme boundaries. Often 
trie similarities are used as a first step followed by 
further processing to identify morphemes (Schone 
and Jurafsky, 2001).  
The paradigm structure of NL morphology has 
also been previously leveraged. Goldsmith (2001) 
uses morphemes to efficiently encode a corpus, but 
he first groups morphemes into paradigm like 
structures he calls signatures. To date, the work 
that draws the most on paradigm structure is 
Snover (2002). Snover incorporates paradigm 
structure into a generative statistical model of 
morphology. Additionally, to discover paradigm 
like sets of suffixes, Snover designs and searches 
networks of partial paradigms. These networks are 
the direct inspiration for ParaMor?s morphology 
scheme networks described in section 2.1. 
2 ParaMor: Inflection Class Identification 
2.1 Search 
A Search Space: The first stage of ParaMor is a 
search procedure designed to identify partial in-
flection classes containing as many true productive 
suffixes of a language as possible. To search for 
these partial inflection classes we must first define 
a space to search over. In a naturally occurring 
corpus not all possible surface forms occur. In a 
corpus, each stem adhering to an inflection class 
will likely be observed in combination with only a 
subset of the suffixes in that inflection class. Each 
box in Figure 1 depicts a small portion of the em-
pirical co-occurrence of suffixes and stems from a 
Spanish newswire corpus of 50,000 types. Each 
box in this figure contains a list of suffixes at the 
top in bold, together with the total number, and a 
few examples (in italics), of stems that occurred in 
separate word forms with each suffix in that box. 
For example, the box containing the suffixes e, 
er?, ieron, and i? contains the stems deb and 
padec because the word forms debe, padece, de-
ber?, padecer?, etc. all occurred in the corpus. We 
call each possible pair of suffix and stem sets a 
scheme, and say that the e.er?.ieron.i? scheme 
covers the words debe, padece, etc. Note that a 
scheme contains both stems that occurred with ex-
actly the set of suffixes in that scheme, as well as 
119
stems that occurred with suffixes beyond just those 
in the scheme. For example, in addition to the four 
suffixes e, er?, ieron, and i?, the stem deb oc-
curred with the suffixes er and ido, as evident from 
the top left scheme e.er.er?.ido.ieron.i? which 
contains the stem deb. Intuitively, a scheme is a 
subset of the suffixes filling the paradigm cells of a 
true inflection class together with the stems that 
empirically occurred with that set of suffixes.  
The schemes in Figure 1 cover portions of the er 
and the ir Spanish verbal inflection classes. The 
top left scheme of the figure contains suffixes in 
the er inflection class, while the top center scheme 
contains suffixes in the ir inflection class. The six 
suffixes in the top left scheme and the six suffixes 
in the top center scheme are just a few of the 
suffixes in the full er and ir inflection classes. As 
is fairly common for inflection classes across 
languages, the sets of suffixes in the Spanish er 
and ir inflection classes overlap. That is, verbs that 
belong to the er inflection class can take as a suffix 
certain strings of characters that verbs belonging to 
the ir inflection class can also take. The suffixes 
that are unique to the er verb inflection class in the 
top left scheme are er and er?; while the unique 
suffixes for the ir class in the top center scheme are 
ir and ir?. In the third row of the figure, the 
scheme e.ido.ieron.i? contains only suffixes found 
in both the er and ir schemes. 
 While the example schemes in Figure 1 are cor-
rect and do occur in a real Spanish newswire cor-
pus, the schemes are atypically perfect. There is 
only one suffix appearing in Figure 1 that is not a 
true suffix of Spanish?azar in the upper right 
scheme. In unsupervised morphology induction we 
do not know a priori the correct suffixes of a lan-
guage. Hence, we form schemes by proposing can-
didate morpheme boundaries at every character 
boundary in every word, including the character 
boundary after the final character in each word 
form, to allow for empty suffixes. 
Schemes of suffixes and their exhaustively co-
occurring stems define a natural search space over 
partial inflection classes because schemes readily 
organize by the suffixes and stems they contain. 
We define a parent-child relationship between a 
parent scheme, P  and a child scheme C , when P  
contains all the suffixes that C  contains and when 
P  contains exactly one more suffix than C . In 
Figure 1, parent child relations are represented by 
solid lines connecting boxed schemes. The scheme 
e.er.er?.ido.ieron.i?, for example, is the parent of 
three depicted children in Figure 1, one of which is 
e.er.er?.ieron.i?.  
Our search strategy exploits a fundamental 
aspect of the relationship between parent and child 
schemes. Consider the number of stems in a parent 
scheme P  as compared to the number of stems in 
any one of its children C . Since P  contains all the 
suffixes which C  contains, and because P  only 
contains stems that occurred with every suffix in 
P , P  can at most contain exactly the stems C  
contains and typically will contain fewer. In the 
Spanish corpus from which the scheme network of 
Figure 1 was built, 32 stems occur in forms with 
each of the five suffixes e, er, er?, ieron, and i? 
attached. But only 28 of these 32 stems occur in 
yet another form involving ido?the stem deb did 
but the stems padec and romp did not, for example. 
A Search Strategy: To search for schemes 
which cover portions of the true inflection classes 
of a language, ParaMor?s search starts at the bot-
tom of the network. The lowest level in the scheme 
e.er.er?.ido.ieron.i? 
28: deb, escog, ofrec, roconoc, vend, ... 
e.ido.ieron.ir.ir?.i? 
28: asist, dirig, exig, ocurr, sufr, ... 
e.er?.ido.ieron.i? 
28: deb, escog, ... 
e.er.ido.ieron.i? 
46: deb, parec, recog... 
e.ido.ieron.ir?.i? 
28: asist, dirig, ... 
 
e.ido.ieron.ir.i? 
39: asist, bat, sal, ... 
e.er.er?.ieron.i? 
32: deb, padec, romp, ... 
e.ido.ieron.i? 
86: asist, deb, hund,... 
e.er?.ieron.i? 
32: deb, padec, ... 
er.ido.ieron.i? 
58: ascend, ejerc, recog, ... 
ido.ieron.ir.i? 
44: interrump, sal, ... 
Figure 1: A small portion of a morphology scheme network?our search space of partial empirical in-
flection classes. This network was built from a Spanish Newswire corpus of 50,000 types, 1.26 million 
tokens. Each box contains a scheme. The suffixes of each scheme appear in bold at the top of each box. 
The total number of adherent stems for each scheme, together with a few exemplar stems, is in italics. 
Stems are underlined if they do not appear in any parent shown in this figure. 
azar.e.ido.ieron.ir.i? 
1: sal 
120
network consists of schemes which contain exactly 
one suffix together with all the stems that occurred 
in the corpus with that suffix attached. ParaMor 
considers each one-suffix scheme in turn beginning 
with that scheme containing the most stems, work-
ing toward schemes containing fewer. From each 
bottom scheme, ParaMor follows a single greedy 
upward path from child to parent. As long as an 
upward path takes at least one step, making it to a 
scheme containing two or more alternating suf-
fixes, our search strategy accepts the terminal 
scheme of the path as likely modeling a portion of 
a true inflection class. 
Each greedily chosen upward step is based on 
two criteria. The first criterion considers the 
number of adherent stems in the current scheme as 
compared to its parents? adherent sizes. A variety 
of statistics could judge the stem-strength of parent 
schemes: ranging from simple ratios through 
(dis)similarity measures, such as the dice 
coefficient or mutual information, to full fledged 
statistical tests. After experimenting with a range 
of such statistics we found, somewhat surprisingly, 
that measuring the ratio of parent stem size to child 
stem size correctly identifies parent schemes which 
contain only true suffixes just as consistently as 
more sophisticated tests. While a full report of our 
experiments is beyond the scope of this paper, the 
short explanation of this behavior is data 
sparseness. Many upward search steps start from 
schemes containing few stems. And when little 
data is available no statistic is particularly reliable.  
Parent-child stem ratios have two additional 
computational advantages over other measures. 
First, they are quick to compute and second, the 
parent with the largest stem ratio is always that 
parent with the most stems. So, being greedy, each 
search step simply moves to that parent, P , with 
the most stems, as long as the parent-child stem 
ratio to P  is large. The threshold above which a 
stem ratio is considered large enough to warrant an 
upward step is a free parameter. As the goal of this 
initial search stage is to identify schemes contain-
ing as wide a variety of productive suffixes as pos-
sible, we want to set the parent-child stem ratio 
threshold as low as possible. But a ratio threshold 
that is too small will allow search paths to schemes 
containing unproductive and spurious suffixes. In 
practice, for Spanish, we have found that setting 
the parent-child stem ratio cutoff much below 0.25 
results in schemes that begin to include only mar-
ginally productive derivational suffixes. For this 
paper we leave the parent-child stem ratio cutoff 
parameter at 0.25.  
Alone, stem strength assessments of parent 
schemes, such as parent-child stem ratios, falter as 
a search path nears the top of the morphology 
scheme network. Monotonically decreasing adher-
ent stem size causes statistics that assess parents? 
stem-strength to become less and less reliable. 
Hence, the second criterion governing each search 
step helps to halt upward search paths before judg-
ing parents? worth becomes impossible. While 
there are certainly many possible stopping criteria, 
ParaMor?s policy stops each upward search path 
when there is no parent scheme with more stems 
than it has suffixes. We devised this halting condi-
tion for two reasons. First, requiring each path 
scheme to contain more stems than suffixes attains 
high suffix recall. High recall results from setting a 
low bar for upward movement at the bottom of the 
network. Search paths which begin from schemes 
whose single suffix is rare in the text corpus can 
often take one or two upward search steps and 
reach a scheme containing the necessary three or 
four stems. Second, this halting criterion requires 
the top scheme of search paths that climb high in 
the network to contain a comparatively large num-
ber of stems. Reigning in high-reaching search 
paths before the stem count falls too far, captures 
path-terminal schemes which cover a large number 
of word types. In the second stage of ParaMor?s 
inflection class identification phase these larger 
terminal schemes effectively vacuum up the useful 
smaller paths that result from the more rare suf-
fixes. Figure 2 contains examples of schemes se-
lected by ParaMor?s initial search. 
To evaluate ParaMor at paradigm identification, 
we hand compiled an answer key of the inflection 
classes of Spanish. This answer key contains nine 
productive inflection classes. Three contain the 
suffixes of the ar, er, and ir verbal inflection 
classes. There are two orthographically differenti-
ated inflection classes for nouns in the answer key: 
one for nouns that form the plural by adding s, and 
one for nouns that take es. Adjectives in Spanish 
inflect for gender and number. Arguably, gender 
and number each constitute separate paradigms, 
each with two cells. But here we conflated these 
into a single inflection class with four cells. Fi-
nally, there are three inflection classes in our an-
swer key covering Spanish clitics. Spanish verbal 
clitics behave orthographically as agglutinative 
sequences of suffixes.  
121
In a corpus of Spanish newswire text of 50,000 
types and 1.26 million tokens, the initial search 
identifies schemes containing 92% of all ideal in-
flectional suffixes of Spanish, or 98% of the ideal 
suffixes that occurred at least twice in the corpus. 
There are selected schemes which contain portions 
of each of the nine inflection classes in the answer 
key. The high recall of the initial search comes, of 
course, at the expense of precision. While there are 
nine inflection-classes and 87 unique suffixes in 
the hand-built answer key for Spanish, 8339 
schemes are selected containing 9889 unique can-
didate suffixes.  
2.2 Clustering Partial Inflection Classes 
While the third step of inflection class identifica-
tion, discussed in Section 2.3, directly improves 
the initial search?s low precision by filtering out 
bogus schemes, the second step, described here, 
conflates selected schemes which model portions 
of the same inflection class. Consider the fifth and 
twelfth schemes selected by ParaMor from our 
Spanish corpus, as shown in Figure 2. Both of 
these schemes contain a large number of suffixes 
from the Spanish ar verbal inflection class. And 
while each contains many overlapping suffixes, 
each possesses correct suffixes which the other 
does not. Meanwhile, the 1591st selected scheme 
contains four suffixes of the ir verbal inflection 
class, including the only instance of ir? that occurs 
in any selected scheme. Containing only six stems, 
the 1591st scheme could accidentally be filtered out 
during the third phase of inflection class identifica-
tion. Hence, the rationale for clustering initial se-
lected schemes is two fold. First, by consolidating 
schemes which cover portions of the same inflec-
tion class we produce sets of suffixes which more 
closely model the paradigm structure of natural 
language morphology. And, second, corralling cor-
rect schemes safeguards against losing unique suf-
fixes. 
The clustering of schemes presents two unique 
challenges. First, we must avoid over-clustering 
schemes which model distinct inflection classes. 
As noted in Section 2.1, it is common, cross-
linguistically, for the suffixes of inflection classes 
to overlap. Looking at Figure 2, we must be careful 
not to merge the 209th selected scheme, which 
models a portion of the er verbal inflection class, 
with the 1591st selected scheme, which models the 
ir class?despite these schemes sharing two suf-
fixes, ido and idos. As the second challenge, the 
many small schemes which the search strategy 
produces act as distractive noise during clustering. 
While small schemes containing correct suffixes 
do exist, e.g. the 1591st scheme, the vast majority 
of schemes containing few stems and suffixes are 
incorrect collections of word final strings that hap-
pen to occur in corpus word forms attached to a 
small number of shared initial strings. ParaMor?s 
clustering algorithm should, for example, avoid 
placing ?.s and ?.ipo, respectively the 1st and 
1590th selected schemes, in the same cluster. Al-
though ?.ipo shares the null suffix with the valid 
nominal scheme ?.s, the string ?ipo? is not a mor-
phological suffix of Spanish. 
To form clusters of related schemes while ad-
dressing both the challenge of observing a lan-
guage?s paradigm structure as well as the challenge 
of merging in the face of many small incorrectly 
selected schemes, ParaMor adapts greedy hierar-
chical agglomerative clustering. We modify vanilla 
bottom-up clustering by placing restrictions on 
which clusters are allowed to merge. The first re-
striction helps ensure that schemes modeling dis-
tinct but overlapping inflection classes remain 
separated. The restriction: do not place into the 
same cluster suffixes which share no stem in the 
corpus. This restriction retains separate clusters for 
separate inflection classes because a lexeme?s stem 
Figure 2: The suffixes of some schemes selected 
by the initial search over a Spanish corpus of 
50,000 types. While some selected schemes 
contain large numbers of correct suffixes, such 
as the 1st, 2nd, 5th, 12th, 209th, and 1591st selected 
schemes; many others are incorrect collections 
of word final strings. 
 1) ?.s 5501 stems 
 2) a.as.o.os 892 stems 
... 
 5) a.aba.aban.ada.adas.ado.ados.an.ando.   
ar.aron.arse.ar?.ar?n.? 25 stems 
... 
 12) a.aba.ada.adas.ado.ados.an.ando.ar.   
aron.ar?.ar?n.e.en.? 21 stems 
... 
 209) e.er.ida.idas.ido.idos.imiento.i? 9 stems 
... 
1590) ?.ipo 4 stems 
1591) ido.idos.ir.ir? 6 stems 
1592) ?.e.iu 4 stems 
1593) iza.izado.izan.izar.izaron.izar?n.iz? 
... 8 stems 
122
occurring with suffixes unique to that lexeme?s 
inflection class will not occur with suffixes unique 
to some other inflection class.  
Alone, requiring all pairs of suffixes in a cluster 
to occur in the corpus with some common stem 
will not prevent small bogus schemes, such as 
?.ipo from attaching to correct schemes, such as 
?.s?the ipo.s scheme contains two ?stems,? the 
word form initial strings ?ma? and ?t?. And so a 
second restriction is required. This second restric-
tion employs a heuristic specifically adapted to 
ParaMor?s initial search strategy. As discussed in 
Section 2.1, in addition to many schemes which 
contain only few suffixes, ParaMor?s initial net-
work search also identifies multiple overlapping 
schemes containing significant subsets of the suf-
fixes in an inflection class. The 5th, 12th, and 209th 
selected schemes of Figure 2 are three such larger 
schemes. ParaMor restricts cluster merges heuristi-
cally by requiring at least one large scheme for 
each small scheme the cluster contains, where we 
measure the size of a scheme as the number of 
unique word forms it covers. The threshold size 
above which schemes are considered large is the 
second of ParaMor?s two free parameters. The 
scheme size threshold is reused during ParaMor?s 
filtering stage. We discuss the unsupervised proce-
dure we use to set the size threshold when we pre-
sent the details of cluster filtering in Section 2.3. 
We have found that with these two cluster re-
strictions in place, the particular metric we use to 
measure the similarity of scheme-clusters does not 
significantly affect clustering. For the experiments 
we report here, we measure the similarity of 
scheme-clusters as the cosine between the sets of 
all possible stem-suffix pairs the clusters contain. 
A stem-suffix pair occurs in a cluster if some 
scheme belonging to that cluster contains both that 
stem and that suffix. With these adaptations, we 
allow agglomerative clustering to proceed until 
there are no more clusters that can legally be 
merged.  
2.3 Filtering of Inflection Classes 
With most valid schemes having found a safe ha-
ven in a cluster with other schemes modeling the 
same inflection class, we turn our attention to im-
proving scheme-cluster precision. ParaMor applies 
a series of filters, culling out unwanted scheme-
clusters. The first filter is closely related to the 
cluster restriction on scheme size discussed in Sec-
tion 2.2. ParaMor discards all unclustered schemes 
falling below the size threshold used during clus-
tering. Figure 3 graphs the number of Spanish clus-
ters which survive this size-based filtering step as 
the threshold size is varied. Figure 3 also contains 
a plot of the recall of unique Spanish suffixes as a 
function of this threshold. As the size threshold is 
increased the number of remaining clusters quickly 
drops. But suffix recall only slowly falls during the 
steep decline in cluster count, indicating ParaMor 
discards mostly bogus schemes containing illicit 
suffixes. Because recall is relatively stable, the ex-
act size threshold we use should have only a minor 
effect on ParaMor?s final morphological analyses. 
In fact, we have not fully explored the ramifica-
tions various threshold values have on the final 
morphological word segmentations, but have sim-
ply picked a reasonable setting, 37 covered word 
types. At this threshold, the number of scheme-
clusters is reduced by more than 98%, while the 
number of unique candidate suffixes in any cluster 
is reduced by more than 85%. Note that the initial 
number of selected schemes, 8339, falls outside the 
scale of Figure 3. 
Of the scheme-clusters which remain after size 
based filtering is complete, by far the largest cate-
gory of incorrect clusters contains schemes which, 
like the 1593rd selected scheme, shown in Figure 2, 
incorrectly hypothesize morpheme boundaries one 
or more characters to the left of the true boundary. 
To filter out these incorrectly segmented clusters 
we use a technique inspired by Harris (1955). For 
each initial string common to all suffixes in the 
cluster, for each scheme in the cluster, we examine 
the network scheme containing the suffixes formed 
by stripping the initial string from the scheme?s 
Figure 3: The # of clusters and their recall of 
unique Spanish suffixes as the scheme-cluster 
size cutoff is varied. The value of each function 
at the threshold we use in all experiments re-
ported in this paper is that of the larger symbol. 
0
200
400
600
800
1000
0 50 100 150
Scheme or Cluster Size
# 
o
f C
lu
st
er
s
0
0.2
0.4
0.6
0.8
1
R
ec
a
ll
# of Clusters
Recall
 
123
suffixes. We then measure the entropy of leftward 
trie characters of the stripped scheme. If the en-
tropy is large, then the character stripped scheme is 
likely at a morpheme boundary and the original 
scheme is likely modeling an incorrect morpheme 
boundary. This algorithm would throw out the 
1593rd selected scheme because the stems in the 
scheme a.ado.an.ar.aron.ar?n.? end in a wide 
variety of characters, yielding high trie entropy, 
and signaling a likely morpheme boundary. 
Because we apply morpheme boundary filtering 
after we have clustered, the redundancy of the 
many schemes in the cluster makes this filter quite 
robust, letting us set the cutoff parameter as low as 
we like avoiding another free parameter. 
2.4 Segmentation and Evaluation 
Word segmentation is our final step of morpholo-
gical analysis. ParaMor?s current segmentation 
algorithm is perhaps the most simple paradigm 
inspired segmentation algorithm possible. Essen-
tially, ParaMor strips off suffixes which likely par-
ticipate in a paradigm. To segment any word, w , 
ParaMor identifies all scheme-clusters that contain 
a non-empty suffix that matches a word final string 
of w . For each such matching suffix, Cf ? , 
where C is the cluster containing f , we strip f  
from w  obtaining a stem t . If there is some sec-
ond suffix Cf ??  such that ft ?.  is a word form 
found in either of the training or the test corpora, 
then ParaMor proposes a segmentation of w  be-
tween t  and f . ParaMor, here, identifies f  and 
f ?  as mutually exclusive suffixes from the same 
paradigm. If ParaMor finds no complex analysis, 
then we propose w  itself as the sole analysis of the 
word. Note that for each word form, ParaMor may 
propose multiple separate segmentation analyses 
each containing a single proposed stem and suffix. 
To evaluate ParaMor?s morphological segmenta-
tions we follow the methodology of Morpho Chal-
lenge 2007 (Kurimo et al, 2007), a minimally su-
pervised morphology induction competition. Word 
segmentations are evaluated in Morpho Challenge 
2007 by comparing against hand annotated mor-
phological analyses. The correctness of proposed 
morphological analyses is computed in Morpho 
Challenge 2007 by comparing pairs of word forms 
which share portions of their analyses. Recall is 
measured by first sampling pairs of words from the 
answer analyses which share a stem or morphosyn-
tactic feature and then noting if that pair of word 
forms shares a morpheme in any of their proposed 
analyses. Precision is measured analogously, sam-
pling morpheme-sharing pairs of words from the 
proposed analyses and noting if that pair of words 
shares a feature in any correct analysis of those 
words.  
We evaluate ParaMor on two languages not 
examined during the development of ParaMor?s 
induction algorithms: English and German. And 
we evaluate with each of these two languages at 
two tasks:  
1. Analyzing inflectional morphology alone 
2. Jointly analyzing inflectional and derivational 
morphology.  
We constructed Morpho Challenge 2007 style 
answer keys for each language and each task using 
the Celex database (Burnage, 1990). The English 
and German corpora we test over are the corpora 
available through Morpho Challenge 2007. The 
English corpus contains nearly 385,000 types, 
while the German corpus contains more than 1.26 
million types. ParaMor induced paradigmatic 
scheme-clusters over these larger corpora by 
reading just the top 50,000 most frequent types. 
But with the scheme-clusters in hand, ParaMor 
segmented all the types in each corpus. 
We compare ParaMor to Morfessor v0.9.2 
(Creutz, 2006), a state-of-the-art minimally super-
vised morphology induction algorithm. Morfessor 
has a single free parameter. To make for stiff com-
petition, we report results for Morfessor at that pa-
rameter setting which maximized F1 on each sepa-
rate test scenario. We did not vary the two free pa-
rameters of ParaMor, but hold each of ParaMor?s 
parameters at a setting which produced reasonable 
Spanish suffix sets, see sections 2.1-2.2. Table 2 
contains the evaluation results. To estimate the 
variance of our experimental results we measured 
Morpho Challenge 2007 style precision, recall, and 
F1 on multiple non-overlapping pairs of 1000 fea-
ture-sharing words.  
Neither ParaMor nor Morfessor arise in Table 2 
as clearly superior. Each algorithm outperforms the 
other at F1 in some scenario. Examining precision 
and recall is more illuminating. ParaMor attains 
particularly high recall of inflectional affixes for 
both English and German. We conjecture that Pa-
raMor?s strong performance at identifying inflec-
tional morphemes comes from closely modeling 
the natural paradigm structure of language. Con-
versely, Morfessor places its focus on precision 
and does not rely on any property exclusive to in-
flectional (or derivational) morphology. Hence, 
124
Morfessor attains high precision with reasonable 
recall when graded against an answer key contain-
ing both inflectional and derivational morphology. 
We are excited by ParaMor?s strong 
performance and are eager to extend our algorithm. 
We believe the precision of ParaMor?s simple 
segmentation algorithm can be improved by 
narrowing down the proposed analyses for each 
word to the most likely. Perhaps ParaMor and 
Morfessor?s vastly different strategies for 
morphology induction could be combined into a 
hybrid strategy more successful than either alone. 
And ambitiously, we hope to extend ParaMor to 
analyze languages with agglutinative sequences of 
affixes by generalizing the definition of a scheme.  
Acknowledgements 
The research reported in this paper was funded in 
part by NSF grant number IIS-0121631. 
References 
Altun, Yasemin, and Mark Johnson. "Inducing 
SFA with -Transitions Using Minimum 
Description Length." Finite State Methods in 
Natural Language Processing Workshop at 
ESSLLI Helsinki: 2001.  
Brent, Michael R., Sreerama K. Murthy, and 
Andrew Lundberg. "Discovering Morphemic 
Suffixes: A Case Study in MDL Induction." The 
Fifth International Workshop on Artificial Intel-
ligence and Statistics Fort Lauderdale, Florida, 
1995.  
Burnage, Gavin. Celex?A Guide for Users. 
Springer, Centre for Lexical information, 
Nijmegen, the Netherlands, 1990. 
Creutz, Mathias. ?Induction of the Morphology of 
Natural Language: Unsupervised Morpheme 
Segmentation with Application to Automatic 
Speech Recognition.? Ph.D. Thesis in Computer 
and Information Science, Report D13. Helsinki: 
University of Technology, Espoo, Finland, 2006. 
Goldsmith, John. "Unsupervised Learning of the 
Morphology of a Natural Language." Computa-
tional Linguistics 27.2 (2001): 153-198.  
Hafer, Margaret A., and Stephen F. Weiss. "Word 
Segmentation by Letter Successor Varieties." 
Information Storage and Retrieval 10.11/12 
(1974): 371-385. 
Harris, Zellig. "From Phoneme to Morpheme." 
Language 31.2 (1955): 190-222. Reprinted in 
Harris 1970. 
Harris, Zellig. Papers in Structural and 
Transformational Linguists. Ed. D. Reidel, 
Dordrecht 1970. 
Johnson, Howard, and Joel Martin. "Unsupervised 
Learning of Morphology for English and Inuk-
titut." Human Language Technology Conference 
/ North American Chapter of the Association for 
Computational Linguistics (HLT-NAACL). 
Edmonton, Canada: 2003. 
Kurimo, Mikko, Mathias Creutz, and Matti 
Varjokallio. ?Unsupervised Morpheme Analysis 
? Morpho Challenge 2007.? March 26, 2007. 
<http://www.cis.hut.fi/morphochallenge2007/> 
Schone, Patrick, and Daniel Jurafsky. "Know-
ledge-Free Induction of Inflectional Morpho-
logies." North American Chapter of the 
Association for Computational Linguistics 
(NAACL). Pittsburgh, Pennsylvania: 2001. 183-
191. 
Snover, Matthew G. "An Unsupervised Knowledge 
Free Algorithm for the Learning of Morphology 
in Natural Languages." Sever Institute of Tech-
nology, Computer Science Saint Louis, Mis-
souri: Washington University, M.S. Thesis, 
2002. 
Table 2: ParaMor segmentations compared to Morfessor?s (Creutz, 2006) evaluated for Precision, Recall, 
F1, and standard deviation of F1, , in four scenarios. Segmentations over English and German are each 
evaluated against correct morphological analyses consisting, on the left, of inflectional morphology 
only, and on the right, of both inflectional and derivational morphology. 
 Inflectional Morphology Only Inflectional & Derivational Morphology 
 English German English German 
 P R F1  P R F1  P R F1  P R F1  
Morfessor 53.3 47.0 49.9 1.3 38.7 44.2 41.2 0.8 73.6 34.0 46.5 1.1 66.9 37.1 47.7 0.7 
ParaMor 33.0 81.4 47.0 0.9 42.8 68.6 52.7 0.8 48.9 53.6 51.1 0.8 60.0 33.5 43.0 0.7 
 
125
Proceedings of the Third Workshop on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Meteor, m-bleu and m-ter: Evaluation Metrics for
High-Correlation with Human Rankings of Machine Translation
Output
Abhaya Agarwal and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{abhayaa,alavie}@cs.cmu.edu
Abstract
This paper describes our submissions to the
machine translation evaluation shared task in
ACL WMT-08. Our primary submission is the
Meteor metric tuned for optimizing correla-
tion with human rankings of translation hy-
potheses. We show significant improvement
in correlation as compared to the earlier ver-
sion of metric which was tuned to optimized
correlation with traditional adequacy and flu-
ency judgments. We also describe m-bleu and
m-ter, enhanced versions of two other widely
used metrics bleu and ter respectively, which
extend the exact word matching used in these
metrics with the flexible matching based on
stemming and Wordnet in Meteor .
1 Introduction
Automatic Metrics for MT evaluation have been re-
ceiving significant attention in recent years. Evalu-
ating an MT system using such automatic metrics is
much faster, easier and cheaper compared to human
evaluations, which require trained bilingual evalua-
tors. The most commonly used MT evaluation met-
ric in recent years has been IBM?s Bleu metric (Pa-
pineni et al, 2002). Bleu is fast and easy to run,
and it can be used as a target function in parameter
optimization training procedures that are commonly
used in state-of-the-art statistical MT systems (Och,
2003). Various researchers have noted, however, var-
ious weaknesses in the metric. Most notably, Bleu
does not produce very reliable sentence-level scores.
Meteor , as well as several other proposed metrics
such as GTM (Melamed et al, 2003), TER (Snover
et al, 2006) and CDER (Leusch et al, 2006) aim to
address some of these weaknesses.
Meteor , initially proposed and released in 2004
(Lavie et al, 2004) was explicitly designed to im-
prove correlation with human judgments of MT qual-
ity at the segment level. Previous publications on
Meteor (Lavie et al, 2004; Banerjee and Lavie,
2005; Lavie and Agarwal, 2007) have described the
details underlying the metric and have extensively
compared its performance with Bleu and several
other MT evaluation metrics. In (Lavie and Agar-
wal, 2007), we described the process of tuning free
parameters within the metric to optimize the corre-
lation with human judgments and the extension of
the metric for evaluating translations in languages
other than English.
This paper provides a brief technical description of
Meteor and describes our experiments in re-tuning
the metric for improving correlation with the human
rankings of translation hypotheses corresponding to
a single source sentence. Our experiments show sig-
nificant improvement in correlation as a result of re-
tuning which shows the importance of having a met-
ric tunable to different testing conditions. Also, in
order to establish the usefulness of the flexible match-
ing based on stemming and Wordnet, we extend two
other widely used metrics bleu and ter which use
exact word matching, with the matcher module of
Meteor .
2 The Meteor Metric
Meteor evaluates a translation by computing a
score based on explicit word-to-word matches be-
tween the translation and a given reference trans-
lation. If more than one reference translation is
available, the translation is scored against each refer-
ence independently, and the best scoring pair is used.
Given a pair of strings to be compared, Meteor cre-
ates a word alignment between the two strings. An
alignment is mapping between words, such that ev-
ery word in each string maps to at most one word
in the other string. This alignment is incrementally
produced by a sequence of word-mapping modules.
The ?exact? module maps two words if they are ex-
actly the same. The ?porter stem? module maps two
words if they are the same after they are stemmed us-
115
ing the Porter stemmer. The ?WN synonymy? mod-
ule maps two words if they are considered synonyms,
based on the fact that they both belong to the same
?synset? in WordNet.
The word-mapping modules initially identify all
possible word matches between the pair of strings.
We then identify the largest subset of these word
mappings such that the resulting set constitutes an
alignment as defined above. If more than one maxi-
mal cardinality alignment is found, Meteor selects
the alignment for which the word order in the two
strings is most similar (the mapping that has the
least number of ?crossing? unigram mappings). The
order in which the modules are run reflects word-
matching preferences. The default ordering is to
first apply the ?exact? mapping module, followed by
?porter stemming? and then ?WN synonymy?.
Once a final alignment has been produced between
the system translation and the reference translation,
the Meteor score for this pairing is computed as
follows. Based on the number of mapped unigrams
found between the two strings (m), the total num-
ber of unigrams in the translation (t) and the total
number of unigrams in the reference (r), we calcu-
late unigram precision P = m/t and unigram recall
R = m/r. We then compute a parametrized har-
monic mean of P and R (van Rijsbergen, 1979):
Fmean =
P ?R
? ? P + (1? ?) ?R
Precision, recall and Fmean are based on single-
word matches. To take into account the extent to
which the matched unigrams in the two strings are
in the same word order, Meteor computes a penalty
for a given alignment as follows. First, the sequence
of matched unigrams between the two strings is di-
vided into the fewest possible number of ?chunks?
such that the matched unigrams in each chunk are
adjacent (in both strings) and in identical word or-
der. The number of chunks (ch) and the number of
matches (m) is then used to calculate a fragmenta-
tion fraction: frag = ch/m. The penalty is then
computed as:
Pen = ? ? frag?
The value of ? determines the maximum penalty
(0 ? ? ? 1). The value of ? determines the
functional relation between fragmentation and the
penalty. Finally, the Meteor score for the align-
ment between the two strings is calculated as:
score = (1 ? Pen) ? Fmean
The free parameters in the metric, ?, ? and ? are
tuned to achieve maximum correlation with the hu-
man judgments as described in (Lavie and Agarwal,
2007).
3 Extending Bleu and Ter with
Flexible Matching
Many widely used metrics like Bleu (Papineni et al,
2002) and Ter (Snover et al, 2006) are based on
measuring string level similarity between the refer-
ence translation and translation hypothesis, just like
Meteor . Most of them, however, depend on find-
ing exact matches between the words in two strings.
Many researchers (Banerjee and Lavie, 2005; Liu and
Gildea, 2006), have observed consistent gains by us-
ing more flexible matching criteria. In the following
experiments, we extend the Bleu and Ter metrics
to use the stemming and Wordnet based word map-
ping modules from Meteor .
Given a translation hypothesis and reference pair,
we first align them using the word mapping modules
from Meteor . We then rewrite the reference trans-
lation by replacing the matched words with the cor-
responding words in the translation hypothesis. We
now compute Bleu and Ter with these new refer-
ences without changing anything inside the metrics.
To get meaningful Bleu scores at segment level,
we compute smoothed Bleu as described in (Lin and
Och, 2004).
4 Re-tuning Meteor for Rankings
(Callison-Burch et al, 2007) reported that the inter-
coder agreement on the task of assigning ranks to
a given set of candidate hypotheses is much better
than the intercoder agreement on the task of assign-
ing a score to a hypothesis in isolation. Based on
that finding, in WMT-08, only ranking judgments
are being collected from the human judges.
The current version of Meteor uses parameters
optimized towards maximizing the Pearson?s corre-
lation with human judgments of adequacy scores. It
is not clear that the same parameters would be op-
timal for correlation with human rankings. So we
would like to re-tune the parameters in the metric
for maximizing the correlation with ranking judg-
ments instead. This requires computing full rankings
according to the metric and the humans and then
computing a suitable correlation measure on those
rankings.
4.1 Computing Full Rankings
Meteor assigns a score between 0 and 1 to every
translation hypothesis. This score can be converted
116
Language Judgments
Binary Sentences
English 3978 365
German 2971 334
French 1903 208
Spanish 2588 284
Table 1: Corpus Statistics for Various Languages
to rankings trivially by assuming that a higher score
indicates a better hypothesis.
In development data, human rankings are avail-
able as binary judgments indicating the preferred hy-
pothesis between a given pair. There are also cases
where both the hypotheses in the pair are judged to
be equal. In order to convert these binary judgments
into full rankings, we do the following:
1. Throw out all the equal judgments.
2. Construct a directed graph where nodes corre-
spond to the translation hypotheses and every
binary judgment is represented by a directed
edge between the corresponding nodes.
3. Do a topological sort on the resulting graph and
assign ranks in the sort order. The cycles in the
graph are broken by assigning same rank to all
the nodes in the cycle.
4.2 Measuring Correlation
Following (Ye et al, 2007), we first compute the
Spearman correlation between the human rankings
and Meteor rankings of the translation hypotheses
corresponding to a single source sentence. Let N be
the number of translation hypotheses and D be the
difference in ranks assigned to a hypothesis by two
rankings, then Spearman correlation is given by:
r = 1?
6
?
D2
N(N2 ? 1)
The final score for the metric is the average of the
Spearman correlations for individual sentences.
5 Experiments
5.1 Data
We use the human judgment data from WMT-07
which was released as development data for the eval-
uation shared task. Amount of data available for
various languages is shown in Table 1. Development
data contains the majority judgments (not every hy-
potheses pair was judged by same number of judges)
which means that in the cases where multiple judges
judged the same pair of hypotheses, the judgment
given by majority of the judges was considered.
English German French Spanish
? 0.95 0.9 0.9 0.9
? 0.5 3 0.5 0.5
? 0.45 0.15 0.55 0.55
Table 2: Optimal Values of Tuned Parameters for Various
Languages
Original Re-tuned
English 0.3813 0.4020
German 0.2166 0.2838
French 0.2992 0.3640
Spanish 0.2021 0.2186
Table 3: Average Spearman Correlation with Human
Rankings for Meteor on Development Data
5.2 Methodology
We do an exhaustive grid search in the feasible ranges
of parameter values, looking for parameters that
maximize the average Spearman correlation over the
training data. To get a fair estimate of performance,
we use 3-fold cross validation on the development
data. Final parameter values are chosen as the best
performing set on the data pooled from all the folds.
5.3 Results
5.3.1 Re-tuning Meteor for Rankings
The re-tuned parameter values are shown in Ta-
ble 2 while the average Spearman correlations for
various languages with original and re-tuned param-
eters are shown in Table 3. We get significant im-
provements for all the languages. Gains are specially
pronounced for German and French.
Interestingly, weight for recall becomes even higher
than earlier parameters where it was already high.
So it seems that ranking judgments are almost en-
tirely driven by the recall in all the languages. Also
the re-tuned parameters for all the languages except
German are quite similar.
5.3.2 m-bleu and m-ter
Table 4 shows the average Spearman correlations
of m-bleu and m-ter with human rankings. For
English, both m-bleu and m-ter show considerable
improvements. For other languages, improvements
in m-ter are smaller but consistent. m-bleu , how-
ever, doesn?t shows any improvements in this case.
A possible reason for this behavior can be the lack of
a ?WN synonymy? module for languages other than
English which results in fewer extra matches over the
exact matching baseline. Additionally, French, Ger-
man and Spanish have a richer morphology as com-
pared to English. The morphemes in these languages
117
Exact Match Flexible Match
English: Bleu 0.2486 0.2747
Ter 0.1598 0.2033
French: Bleu 0.2906 0.2889
Ter 0.2472 0.2604
German: Bleu 0.1829 0.1806
Ter 0.1509 0.1668
Spanish: Bleu 0.1804 0.1847
Ter 0.1787 0.1839
Table 4: Average Spearman Correlation with Human
Rankings for m-bleu and m-ter
carry much more information and different forms of
the same word may not be as freely replaceable as in
English. A more fine grained strategy for matching
words in these languages remains an area of further
investigation.
6 Conclusions
In this paper, we described the re-tuning of Me-
teor parameters to better correlate with human
rankings of translation hypotheses. Results on the
development data indicate that the re-tuned ver-
sion is significantly better at predicting ranking than
the earlier version. We also presented enhanced
Bleu and Ter that use the flexible word match-
ing module from Meteor and show that this re-
sults in better correlations as compared to the de-
fault exact matching versions. The new version of
Meteor will be soon available on our website at:
http://www.cs.cmu.edu/~alavie/METEOR/ . This
release will also include the flexible word matcher
module which can be used to extend any metric with
the flexible matching.
Acknowledgments
The work reported in this paper was supported by
NSF Grant IIS-0534932.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72, Ann Arbor,
Michigan, June.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Levels
of Correlation with Human Judgments. In Proceedings
of the Second ACL Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Republic,
June.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman.
2004. The Significance of Recall in Automatic Metrics
for MT Evaluation. In Proceedings of the 6th Confer-
ence of the Association for Machine Translation in the
Americas (AMTA-2004), pages 134?143, Washington,
DC, September.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of the Thirteenth Conference of
the European Chapter of the Association for Compu-
tational Linguistics.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In COLING ?04: Proceedings
of the 20th international conference on Computational
Linguistics, page 501, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Ding Liu and Daniel Gildea. 2006. Stochastic itera-
tive alignment for machine translation evaluation. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 539?546, Morristown, NJ, USA.
Association for Computational Linguistics.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and Recall of Machine Translation. In Pro-
ceedings of the HLT-NAACL 2003 Conference: Short
Papers, pages 61?63, Edmonton, Alberta.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), pages 223?231, Cambridge, MA, Au-
gust.
C. van Rijsbergen, 1979. Information Retrieval. Butter-
worths, London, UK, 2nd edition.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence level machine translation evaluation as a rank-
ing. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
118
Proceedings of the Third Workshop on Statistical Machine Translation, pages 163?166,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Statistical Transfer Systems for French?English
and German?English Machine Translation
Greg Hanneman and Edmund Huber and Abhaya Agarwal and Vamshi Ambati
and Alok Parlikar and Erik Peterson and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, ehuber, abhayaa, vamshi, aup, eepeter, alavie}@cs.cmu.edu
Abstract
We apply the Stat-XFER statistical transfer
machine translation framework to the task of
translating from French and German into En-
glish. We introduce statistical methods within
our framework that allow for the principled
extraction of syntax-based transfer rules from
parallel corpora given word alignments and
constituency parses. Performance is evaluated
on test sets from the 2007 WMT shared task.
1 Introduction
The Carnegie Mellon University statistical trans-
fer (Stat-XFER) framework is a general search-
based and syntax-driven framework for develop-
ing MT systems under a variety of data condi-
tions (Lavie, 2008). At its core is a transfer en-
gine using two language-pair-dependent resources:
a grammar of weighted synchronous context-free
rules (possibly augmented with unification-style fea-
ture constraints), and a probabilistic bilingual lexi-
con of syntax-based word- and phrase-level transla-
tions. The Stat-XFER framework has been used to
develop research MT systems for a number of lan-
guage pairs, including Chinese?English, Hebrew?
English, Urdu?English, and Hindi?English.
In this paper, we describe our use of the frame-
work to create new French?English and German?
English MT systems for the 2008 Workshop on Sta-
tistical Machine Translation shared translation task.
We first describe the acquisition and processing of
resources for each language pair and the roles of
those resources within the Stat-XFER system (Sec-
tion 2); we then report results on common test sets
(Section 3) and share some early analysis and future
directions (Section 4).
2 System Description
Building a new machine translation system under
the Stat-XFER framework involves constructing a
bilingual translation lexicon and a transfer gram-
mar. Over the past six months, we have developed
new methods for extracting syntax-based translation
lexicons and transfer rules fully automatically from
parsed and word-aligned parallel corpora. These
new methods are described in detail by Lavie et
al. (2008). Below, we detail the statistical meth-
ods by which these resources were extracted for our
French?English and German?English systems.
2.1 Lexicon
The bilingual lexicon is automatically extracted
from automatically parsed and word-aligned paral-
lel corpora. To obtain high-quality statistical word
alignments, we run GIZA++ (Och and Ney, 2003)
in both the source-to-target and target-to-source di-
rections, then combine the resulting alignments with
the Sym2 symmetric alignment heuristic of Ortiz-
Mart??nez et al (2005)1. From this data, we extract a
lexicon of both word-to-word and syntactic phrase-
to-phrase translation equivalents.
The word-level correspondences are extracted di-
rectly from the word alignments: parts of speech for
these lexical entries are obtained from the preter-
1We use Sym2 over more well-known heuristics such as
?grow-diag-final? because Sym2 has been shown to give the
best results for the node-alignment subtask that is part of our
processing chain.
163
ws cs wt ct r
paru V appeared V 0.2054
paru V seemed V 0.1429
paru V found V 0.0893
paru V published V 0.0804
paru V felt V 0.0714
.
.
.
.
.
.
.
.
.
paru V already ADV 0.0089
paru V appear V 0.0089
paru V authoritative ADJ 0.0089
Table 1: Part of the lexical entry distribution for the
French (source) word paru.
minal nodes of parse trees of the source and target
sentences. If parsers are unavailable for either lan-
guage, we have also experimented with determin-
ing parts of speech with independent taggers such
as TreeTagger (Schmid, 1995). Alternatively, parts
of speech may be projected through the word align-
ments from one language to the other if the infor-
mation is available on at least one side. Syntactic
phrase-level correspondences are extracted from the
parallel data by applying the PFA node alignment
algorithm described by Lavie et al (2008). The
yields of the aligned parse tree nodes are extracted
as constituent-level translation equivalents.
Each entry in the lexicon is assigned a rule score,
r, based on its source-side part of speech cs, source-
side text ws, target-side part of speech ct, and target-
side text wt. The score is a maximum-likelihood es-
timate of the distribution of target-language transla-
tion and source- and target-language parts of speech,
given the source word or phrase.
r = p(wt, ct, cs |ws) (1)
? #(wt, ct, ws, cs)#(ws) + 1
(2)
We employ add-one smoothing in the denominator
of Equation 2 to counteract overestimation in the
case that #(ws) is small. Rule scores provide a way
to promote the more likely translation alternatives
while still retaining a high degree of diversity in the
lexicon. Table 1 shows part of the lexical distribu-
tion for the French (source) word paru.
The result of the statistical word alignment pro-
cess and lexical extraction is a bilingual lexicon con-
taining 1,064,755 entries for French?English and
1,111,510 entries for German?English. Sample lex-
ical entries are shown in Figure 1.
2.2 Grammar
Transfer grammars for our earlier statistical transfer
systems were manually created by in-house experts
of the languages involved and were therefore small.
The Stat-XFER framework has since been extended
with procedures for automatic grammar acquisition
from a parallel corpus, given constituency parses for
source or target data or both. Our French and Ger-
man systems used the context-free grammar rule ex-
traction process described by Lavie et al (2008).
For French, we used 300,000 parallel sentences from
the Europarl training data parsed on the English side
with the Stanford parser (Klein and Manning, 2003)
and on the French side with the Xerox XIP parser
(A??t-Mokhtar et al, 2001). For German, we used
300,000 Europarl sentence pairs parsed with the En-
glish and German versions of the Stanford parser2.
The set of rules extracted from the parsed corpora
was filtered down after scoring to improve system
performance and run time. The final French rule set
was comprised of the 1500 most frequently occur-
ring rules. For German, rules that occurred less than
twice were filtered out, leaving a total of 16,469. In
each system, rule scores were again calculated by
Equation 2, with ws and wt representing the full
right-hand sides of the source and target grammar
rules.
A secondary version of our French system used a
word-level lexicon extracted from the intersection,
rather than the symmetricization, of the GIZA++
alignments in each direction; we hypothesize that
this tends to improve precision at the expense of re-
call. The word-level lexicon was supplemented with
syntax-based phrase-level entries obtained from the
PFA node alignment algorithm. The grammar
contained the 700 highest-frequency and the 500
highest-scoring rules extracted from the parallel
parsed corpus. This version had a total lexicon size
of 2,023,531 entries and a total grammar of 1034
rules after duplicates were removed. Figure 2 shows
2Due to a combination of time constraints and paucity of
computational resources, only a portion of the Europarl parallel
corpus was utilized, and none of the supplementary news com-
mentary training data was integrated.
164
)(
{VS,248840}
V::V |: ["paru"] ?> ["appeared"]
  (*score* 0.205357142857143)
)
  (*score* 0.763636363636364)
{NP,2000012}
NP::NP |: ["ein" "Beispiel"] ?> ["an" "example"]
(
Figure 1: Sample lexical entries for French and German.
sample grammar rules automatically learned by the
process described above.
2.3 Transfer Engine
The Stat-XFER transfer engine runs in a two-stage
process, first applying the grammar and lexicon
to an input sentence, then running a decoder over
the resulting lattice of scored translation pieces.
Scores for each translation piece are based on a
log-linear combination of several features: language
model probability, rule scores, source-given-target
and target-given-source lexical probabilities, parse
fragmentation, and length. For more details, see
Lavie (2008). The use of a German transfer gram-
mar an order of magnitude larger than the corre-
sponding French grammar was made possible due to
a recent optimization made in the engine. When en-
abled, it constrains the search of translation hypothe-
ses to the space of hypotheses whose structure satis-
fies the consituent structure of a source-side parse.
3 Evaluation
We trained our model parameters on a subset of
the provided ?dev2006? development set, optimiz-
ing for case-insensitive IBM-style BLEU (Papineni
et al, 2002) with several iterations of minimum error
rate training on n-best lists. In each iteration?s list,
we also included the lists from previous iterations in
order to maintain a diversity of hypothesis types and
scores. The provided ?test2007? and ?nc-test2007?
data sets, identical with the test data from the 2007
Workshop on Statistical Machine Translation shared
task, were used as internal development tests.
Tables 2, 3, and 4 report scores on these data sets
for our primary French, secondary French, and Ger-
man systems. We report case-insensitive scores for
version 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-style
BLEU (Papineni et al, 2002), and version 5 of TER
(Snover et al, 2006).
Data Set METEOR BLEU TER
dev2006 0.5332 0.2063 64.81
test2007 0.5358 0.2078 64.75
nc-test2007 0.5369 0.1719 69.83
Table 2: Results for the primary French?English system
on provided development and development test sets.
Data Set METEOR BLEU TER
dev2006 0.5330 0.2086 65.02
test2007 0.5386 0.2129 64.29
nc-test2007 0.5311 0.1680 70.90
Table 3: Results for the secondary French?English sys-
tem on provided development and development test sets.
4 Analysis and Conclusions
From the development test results in Section 3, we
note that the Stat-XFER systems? performance cur-
rently lags behind the state-of-the-art scores on the
2007 test data3. This may be in part due to the low
volume of training data used for rule learning. A key
research question in our approach is how to distin-
guish low-frequency correct and useful transfer rules
from ?noisy? rules that are due to parser errors and
incorrect word alignments. We believe that learning
rules from more data will help alleviate this prob-
lem by proportionally increasing the counts of good
rules compared to incorrect ones. We also plan to
study methods for more effective rule set pruning,
regardless of the volume of training data used.
The difference in metric scores between in-
domain and out-of-domain data is partly due to ef-
fects of reference length on the metrics used. De-
tailed output from METEOR and BLEU shows that
the reference translations for the test2007 set are
about 94% as long as the primary French?English
3Top scores on the 2007 test data are approximately 0.60
METEOR, 0.33 BLEU, and 57.6 TER. See Callison-Burch et
al. (2007) for full results.
165
(
  (*score* 0.866050808314088
)
{PP,1627955}
PP:PP [PRE "d?" "autres" N] ?> [PRE "other" N]
  (X1::Y1)
  (X4::Y3)
)
(
{PP,3000085}
PP:ADVP ["vor" CARD "Monaten"] ?> [NUM "months" "ago"]
  (*score* 0.9375)
  (X2::Y1)
)
Figure 2: Sample grammar rules for French and German.
Data Set METEOR BLEU TER
dev2006 0.4967 0.1794 68.68
test2007 0.5052 0.1878 67.94
nc-test2007 0.4939 0.1347 74.38
Table 4: Results for the German?English system on pro-
vided development and development test sets.
system?s translations. On this set, our system has
approximately balanced precision (0.62) and recall
(0.66). However, the nc-test2007 references are only
84% as long as our output, a situation that hurts our
system?s precision (0.57) but boosts its recall (0.68).
METEOR, as a metric that favors recall, shows a
negligible increase in score between these two test
sets, while BLEU and TER report significant relative
drops of 17.3% and 7.8%. This behavior appears to
be consistent on the test2007 and nc-test2007 data
sets across systems (Callison-Burch et al, 2007).
Acknowledgments
This research was supported in part by NSF grants
IIS-0121631 (AVENUE) and IIS-0534217 (LE-
TRAS), and by the DARPA GALE program. We
thank the members of the Parsing and Semantics
group at Xerox Research Centre Europe for assisting
in parsing the French data using their XIP parser.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
Proceedings of the Seventh International Workshop on
Parsing Technologies, Beijing, China, October.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10. MIT Press, Cambridge,
MA.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 228?231, Prague, Czech Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed paral-
lel corpora. In Proceedings of the Second Work-
shop on Syntax and Structure in Statistical Transla-
tion, Columbus, OH, June. To appear.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2005. Thot: A toolkit to train
phrase-based models for statistical machine transla-
tion. In Proceedings of the 10th Machine Translation
Summit, pages 141?148, Phuket, Thailand, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
166
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 87?95,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Syntax-driven Learning of Sub-sentential Translation Equivalents and
Translation Rules from Parsed Parallel Corpora
Alon Lavie
alavie@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Alok Parlikar
aup@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Vamshi Ambati
vambati@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
We describe a multi-step process for automati-
cally learning reliable sub-sentential syntactic
phrases that are translation equivalents of each
other and syntactic translation rules between
two languages. The input to the process is a
corpus of parallel sentences, word-aligned and
annotated with phrase-structure parse trees.
We first apply a newly developed algorithm
for aligning parse-tree nodes between the two
parallel trees. Next, we extract all aligned
sub-sentential syntactic constituents from the
parallel sentences, and create a syntax-based
phrase-table. Finally, we treat the node align-
ments as tree decomposition points and extract
from the corpus all possible synchronous par-
allel tree fragments. These are then converted
into synchronous context-free rules. We de-
scribe the approach and analyze its application
to Chinese-English parallel data.
1 Introduction
Phrase-based Statistical MT (PB-SMT) (Koehn et
al., 2003) has become the predominant approach to
Machine Translation in recent years. PB-SMT re-
quires broad-coverage databases of phrase-to-phrase
translation equivalents. These are commonly ac-
quired from large volumes of automatically word-
aligned sentence-parallel text corpora. Accurate
identification of sub-sentential translation equiva-
lents, however, is a critical process in all data-driven
MT approaches, including a variety of data-driven
syntax-based approaches that have been developed
in recent years. (Chiang, 2005) (Imamura et al,
2004) (Galley et al, 2004).
In this paper, we describe a multi-step process for
automatically learning reliable sub-sentential syn-
tactic phrases that are translation equivalents of each
other and syntactic translation rules between two
languages. The input to the process is a corpus of
parallel sentences, word-aligned and annotated with
phrase-structure parse trees for both languages. Our
method consists of three steps. In the first step,
we apply a newly developed algorithm for aligning
parse-tree nodes between the two parallel trees. In
the second step, we extract all aligned sub-sentential
syntactic constituents from the parallel sentences,
and create a syntax-based phrase-table. Our syn-
tactic phrases come with constituent ?labels? which
can guide their syntactic function during decoding.
In the final step, we treat the node alignments as
tree decomposition points and extract from the cor-
pus all possible synchronous parallel tree fragments.
These are then converted into synchronous context-
free rules. Our methods do not depend on any spe-
cific properties of the underlying phrase-structure
representations or the parsers used, and were de-
signed to be applicable even when these represen-
tations are quite different for the two languages.
The approach described is used to acquire the re-
sources for a statistical syntax-based MT approach
that we have developed (Stat-XFER), briefly de-
scribed below. The resulting resources can, how-
ever, be used in any syntax-based data-driven MT
approach other than our own. The focus of this pa-
per is on our syntax-driven process for extracting
phrases and rules from data. We describe the ap-
proach and analyze its effectiveness when applied to
large-volumes of Chinese-English parallel data.
87
1.1 The Stat-XFER MT Framework
Stat-XFER is a search-based syntax-driven frame-
work for building MT systems. The underlying for-
malism is based on synchronous context-free gram-
mars. The synchronous rules can optionally be aug-
mented by unification-style feature constraints. The
synchronous grammars can be acquired automati-
cally from data, but also manually developed by ex-
perts. A simple example transfer-rule (for Chinese-
to-English) can be seen below:
{NP,1062753}
NP::NP [DNP NP] -> [NP PP]
(
(*score* 0.946640316205534)
(X2::Y1)
(X1::Y2)
)
Each rule has a unique identifier followed by a
synchronous rule for both source and target sides.
The alignment of source-to-target constituents is ex-
plicitly represented using ?X? indices for the source
side, and ?Y? indices for the target side. Rules can
also have lexical items on either side, in which case
no alignment information is required for these ele-
ments. Feature constraints can optionally be speci-
fied for both source and target elements of the rule.
We do not address the learning of feature constraints
in the work described here, and concentrate only
on the acquisition of the synchronous CFG rules.
The rules can be modeled statistically and assigned
scores, which can then be used as decoding features.
The Stat-XFER framework also includes a fully-
implemented transfer engine that applies the trans-
fer grammar to a source-language input sentence at
runtime, and produces collections of scored word
and phrase-level translations according to the gram-
mar. These are collected into a lattice data-structure.
Scores are based on a log-linear combination of sev-
eral features, and a beam-search controls the un-
derlying parsing and transfer process. A second-
stage monotonic decoder is responsible for combin-
ing translation fragments into complete translation
hypotheses (Lavie, 2008)
2 PFA Algorithm for Node Aligment
2.1 Objectives of the Algorithm
Our objective of the first stage of our approach is to
detect sub-sentential constituent correspondences in
parallel sentences, based on phrase-structure parses
for the two corresponding sentences. Given a pair
of parallel sentences and their corresponding parse
trees, our goal is to find pairings of nodes in the
source and target trees whose yields are translation
equivalents of each other. Our current approach only
considers complete constituents and their contigious
yields, and will therefore not align discontiguous
phrases or partial constituents. Similar to phrase ex-
traction methods in PB-SMT, we rely on word-level
alignments (derived manually or automatically) as
indicators for translation equivalence. The assump-
tion applied is that if two words are aligned with
each other, they carry the same meaning and can be
treated as translation equivalents. Constituents are
treated as compositional units of meaning and trans-
lation equivalence.
2.2 Related Work
Aligning nodes in parallel trees has been in-
vestigated by a number of previous researchers.
(Samuelsson and Volk, 2007) describe a process for
manual alignment of nodes in parallel trees. This
approach is well suited for generating reliable par-
allel treebanks, but is impractical for accumulating
resources from large parallel data. (Tinsley et al,
2007) use statistical lexicons derived from automatic
statistical word alignment for aligning nodes in par-
allel trees. In our approach, we use the word align-
ment information directly, which we believe may be
more reliable than the statistical lexicon. (Groves et
al., 2004) propose a method of aligning nodes be-
tween parallel trees automatically, based on word
alignments. In addition to the word alignment in-
formation, their approach uses the constituent labels
of nodes in the trees, and the general structure of the
tree. Our approach is more general in the sense that
we only consider the word alignments, thereby mak-
ing the approach applicable to any parser or phrase-
structure representation, even ones that are quite dif-
ferent for the two languages involved.
88
2.3 Unaligned Words and Contiguity
Word-level alignment of phrase-level translation
equivalents often leaves some words unaligned. For
example, some languages have articles, while oth-
ers do not. It is thus reasonable to expect that con-
stituent pairs in parallel trees that are good transla-
tion equivalents of each other may contain some un-
aligned words. Our PFA node-alignment algorithm
allows for such constituents to be matched.
Different languages have different word orders. In
English, an adjective always comes before a noun,
while in French, in most cases, the adjective fol-
lows its noun. Our node alignment algorithm allows
aligning of constituents regardless of the word order
expressed by the linear precedence relation of their
sub-constituents. As long as one piece of contiguous
text dominated by a node covers the same word-level
alignments as the yield of a node in the parallel tree,
the two nodes can be aligned.
2.4 Wellformedness constraints
Given a pair of word-aligned sentences and their
corresponding parse trees S and T , represented as
sets of constituent nodes, our PFA node alignment
algorithm produces a collection of aligned node-
pairs (Si, Tj). The underlying assumptions of com-
positionality in meaning and word-level alignments
being indicative of translation equivalence lead di-
rectly to the following node alignment wellformed-
ness criteria:
1. If a node Si is linked to a node Tj , then any
node within the subtree of node Si can only be
linked to nodes within the subtree of node Tj .
2. If a node Si is linked to a node Tj , then any
node that dominates the node Si can only be
linked to nodes that dominate the node Tj .
3. If a node Si is linked to a node Tj , then the
word alignments of the yields of the two con-
stituents must satisfy the following:
(a) Every word in the yield of the node Si
must be aligned to one or more words in
the yield of the node Tj , or it should be
unaligned.
(b) Every word in the yield of the node Tj
must be aligned to one or more words in
the yield of the node Si, or it should be
unaligned.
(c) There should be at least one alignment be-
tween the yields of nodes Si and Tj . Thus,
the words in the yields can not all be un-
aligned.
2.5 Arithmetic Representation
Our PFA algorithm uses a arithmetic mapping that
elegently carries over the constraints characterized
by the wellformedness constraints elaborated above.
This mapping is designed to ensure that each aligned
word, which carries a distinct ?piece of meaning?
can be uniquely identified, and also inherently re-
flects the compositional properties of constituent
translation equivalence. This is accomplished by
assigning numerical values to the nodes of the two
parse trees being aligned, in a bottom-up fashion,
starting from the leaf nodes of the trees. Leaf nodes
that correspond to words that are aligned are each
assigned a unique prime number. Unaligned leaf
nodes are assigned a value of ?1?. Constituent nodes
in the parse trees are then assigned a value that is
the product of all its sub-constituent nodes. Because
of the arithmetic property that any composite num-
ber can be uniquely factored into primes, it should
be evident that the value of every constituent node
uniquely identifies the aligned words that are cov-
ered by its yield. Consequently, by assigning the
same prime values to the aligned words of both trees,
retrieving aligned constituent nodes is as simple as
finding the set of nodes in the two trees that carry the
same numerical value. Note that by assigning values
of ?1? to unaligned words, these unaligned words
do not influence the numerical values assigned to
constituent nodes, thus reflecting their treatment as
?don?t cares? with respect to the translation equiva-
lence of constituent nodes.
2.6 Description of the PFA Algorithm
The PFA algorithm uses the concept of ?composite
meaning as prime factorization?, and hence the name
(Prime Factorization and Alignments). The algo-
rithm assigns values to the leaf nodes, propogates
the values up the tree, and then compares the node
values across the trees to align the nodes. As de-
scribed above, leaf nodes which have word align-
ments are assigned unique prime numbers, and the
89
Figure 1: Node-Aligned Parallel Sentences
same prime is assigned to the corresponding aligned
words in the parallel sentences. Leaf nodes corre-
sponding to unaligned words are assigned the value
?1?. The treatment of ?one-to-many? word align-
ments is a special case. Such alignments are con-
sidered to carry the same meaning, and should thus
be assigned the same value. To accomplish this, if a
single word is aligned to multiple words in the other
language, we assign the same prime number to all
words on the ?multiple? side, and assign the product
of these to the single word equivalent.
Another special case is when the parse trees con-
tain unary productions. In this case, the values of
both nodes involved in this production are the same.
Our node alignment algorithm breaks this ?tie? by
selecting the node that is ?lower? in the tree (the
daughter node of the unary production). A simi-
lar situation with two nodes being assigned identical
values can arise when one or more unaligned words
are attached directly to the parent node. Here too,
our algorithm aligns the ?lower? node and leaves
the ?higher? node unaligned. These decisions reflect
our desire to be conservative with respect to such
ambiguous cases, and their implications on the no-
tion of translational equivalence. This also provides
some robustness against noisy alignments.
It is straightfoward to verify that the PFA algo-
rithm satisfies the wellformedness constraints de-
scribed above. Also, since multiplication is com-
mutative, the algorithm is not effected by differing
word orders within parallel constituent structures.
The PFA algorithm run on a sample Chinese-
English parallel sentence is shown in Figure 1. The
value of each node as shown as a part of its label.
The aligned nodes are marked by shapes. A triangle
aligns to a triangle, and squares to squares.
3 Syntax-based Sub-sentential Phrase
Extraction
The alignment of nodes as described in the previous
section allows us to build a comprehensive syntax-
based phrase-to-phrase translation lexicon from a
parallel corpus. To build a syntax-based ?phrase
table?, we simply extract all aligned constituent
nodes along with their yields and enter them into
a database, while accumulating frequency counts.
In addition to the source-to-target phrase corre-
spondences, we record the constituent labels of the
aligned constituent nodes on both the source and tar-
get sides (which may be different). These labels
?connect? the phrases with synatactic transfer rules
during decoding. The set of phrases extracted from
the example sentence in Figure 1 is shown in Fig-
ure 2.
90
Figure 2: Phrases extracted from Aligned Nodes
The process of building syntax-based ?phrase ta-
bles? from large corpora of sentence-parallel data is
quite similar to the corresponding process in phrase-
based SMT systems. Our phrase correspondences,
however, only reflect contiguous and complete con-
stituent correspondences. We also note that the ex-
tracted phrase tables in both approaches can be mod-
eled statistically in similar ways. Similar to common
practice in PB-SMT, we currently use the frequency
counts of the phrases to calculate relative likelihood
estimates and use these as features in our Stat-XFER
decoder.
4 Evaluation of the PFA algorithm
The accuracy of our node alignment algorithm de-
pends on both the quality of the word alignments
as well as the accuracy of the parse trees. We per-
formed several experiments to assess the effects of
these underlying resources on the accuracy of our
approach. The most accurate condition is when the
parallel sentences are manually word-aligned, and
when verified correct parse trees are available for
both source and target sentences. Performance is
expected to degrade when word alignments are pro-
duced using automatic methods, and when correct
parse trees are replaced with automatic parser out-
put. In these experiments, we used a manually word-
aligned parallel Chinese-English TreeBank consist-
ing of 3342 parallel sentences.
4.1 Manual Constituent Node Alignments
We first investigated the accuracy of our approach
under the most accurate condition. We sampled 30
sentences from the Chinese-English treebank cor-
pus. A bilingual expert from our group then man-
ually aligned the nodes in these trees. These node
Precision Recall F-1 F-0.5
0.8129 0.7325 0.7705 0.7841
Table 1: Accuracy of PFA Node Alignments against
Manual Node Alignments
alignments were then used as a ?gold standard?. We
then used the accurate parse trees and the manually
created word alignments for these sentence pairs,
and ran the PFA node algorithm, and compared the
resulting node alignments with the gold standard
alignments. The Precision, Recall, F-1 and F-0.5 re-
sults are reported in Table 1.
We manually inspected cases where there was a
mismatch between the manual and automatic node
alignments, and found several trends. Many of
the alignment differences were the result of one-to-
many or many-to-many word alignemnts. For ex-
ample, in some cases a verb in Chinese was word-
aligned to an auxiliary and a head verb on the en-
glish side (e.g. have and put). The PFA algorithm
in this case node-aligns the VP that governs the Chi-
nese verb to the VP that contains both auxiliary and
head verbs on the English side. The gold standard
human alignments, however, in some cases, aligned
the VP of the Chinese verb to the English VP that
governs just the main verb. Other mismatches were
attributed to errors or inconsistencies in the manual
word alignment and to the treatment of traces and
fillers in the parse trees.
4.2 Effect of Using Automatic Word
Alignments
We next tested how sensitive the PFA algorithm is
to errors in automatic word alignment. We use the
entire 3342 sentences in the parallel treebank for
this experiment. We first ran the algorithm with
the correct parse trees and manual word-alignments
as input. We use the resulting node alignments
as the gold standard in this case. We then used
GIZA++ to get bidirectional word alignments, and
combined them using various strategies. In this sce-
nario, the trees are high-quality (from the treebank),
but the alignments are noisy. The results obtained
are shown in Table 2. Unsurprisingly, the ?Union?
combination method has the best precision but worst
recall, while the ?Intersection? combination method
has the best recall but worst precision. The four
91
Comb Method Prec Rec F-1 F-0.5
Intersection 0.6382 0.5395 0.5846 0.6014
Union 0.8114 0.2915 0.4288 0.5087
Sym1 0.7142 0.4534 0.5546 0.5992
Sym2 0.7135 0.4631 0.5616 0.6045
Grow-Diag-Final 0.7777 0.3462 0.4790 0.5493
Grw-Diag-Fin-And 0.6988 0.4700 0.5619 0.6011
Table 2: Manual Trees, Automatic Node Alignments
other methods for combining word alignments fall
in between. Three of the four (all except ?grow-
diag-final?) behave quite similarly. We generally be-
lieve that precision is somewhat more important than
recall for this task, and have thus used the ?sym2?
method (Ortiz-Mart??nez et al, 2005) (which has the
best F-0.5 score) for our translation experiments.
4.3 Effect of Using Automatic Parses
We evaluated the effect of parsing errors (as re-
flected in automatically derived parse trees) on the
quality of the node alignments. We parsed the tree-
bank corpus on both English and Chinese using the
Stanford parser, and extracted phrases using manual
word alignments. Compared to the phrases extracted
from the manual trees, we obtained a precision of
0.8749, and a recall of 0.7227, that is, an F-0.5 mea-
sure of 0.8174. We then evaluated the most ?noisy?
condition that involves both automatic word align-
ments and automatic parse trees. We evaluated the
phrase extraction with different Viterbi combination
strategies. The ?sym2? combination gave the best
results, with a precision of 0.6251, recall of 0.3566,
thus an F-0.5 measure of 0.4996.
5 Synchronous Tree Fragment and CFG
Rule Extraction
5.1 Related Work
Syntax-based reordering rules can be used as a pre-
processing step for PB-SMT (and other approaches),
to decrease the word-order and syntactic distor-
tion between the source and target languages (Xia
and McCord, 2004). A variety of hierarchical and
syntax-based models, which are applied during de-
coding, have also been developed. Many of these
approaches involve automatic learning and extrac-
tion of the underlying syntax-based rules from data.
The underlying formalisms used has been quite
broad and include simple formalisms such as ITGs
(Wu, 1997), hierarchical synchronous rules (Chiang,
2005), string to tree models by (Galley et al, 2004)
and (Galley et al, 2006), synchronous CFG models
such (Xia and McCord, 2004) (Yamada and Knight,
2001), synchronous Lexical Functional Grammar
inspired approaches (Probst et al, 2002) and others.
Most of the previous approaches for acquiring
syntactic transfer or reordering rules from paral-
lel corpora use syntactic information from only one
side of the parallel corpus, typically the target side.
(Hearne and Way, 2003) describes an approach that
uses syntactic information from the source side to
derive reordering subtrees, which can then be used
within a ?data-oriented translation? (DOT) MT sys-
tem, similar in framework to (Poutsma, 2000). Our
work is different from the above in that we use syn-
tactic trees for both source and target sides to infer
constituent node alignments, from which we then
learn synchronous trees and rules. Our process of
extraction of rules as synchronous trees and then
converting them to synchronous CFG rules is most
similar to that of (Galley et al, 2004).
5.2 Synchronous Tree Fragment Pair
Extraction
The main concept underlying our syntactic rule ex-
traction process is that we treat the node alignments
discovered by the PFA algorithm (described in pre-
vious sections) as synchronous tree decomposition
points. This reflects the fact that these nodes denote
points in the synchronous parse trees where transla-
tion correspondences can be put together composi-
tionally. Using the aligned nodes as decomposition
points, we break apart the synchronous trees into
collections of minimal synchronous tree fragments.
Finally, the synchronous fragments are also con-
verted into synchronous context-free rules. These
are then collected into a database of synchronous
rules.
The input to our rule extraction process consists of
the parallel parse trees along with their node align-
ment information. The constituent nodes in the par-
allel trees that were aligned by the PFA node align-
ment algorithm are treated as tree decomposition
points. At each such decomposition point, spliting
the two parallel trees results in two partial trees or
tree fragments. One synchronous pair consists of
92
the subtrees that are headed by the aligned nodes
where the decomposition took place. Since the sub-
trees are rooted at aligned nodes, their yields are
translation equivalents of each other. The other syn-
chronous tree fragment pair consists of the remain-
ing portions of the trees. The translation equivalence
of the complete tree (or subtree) prior to decomposi-
tion implies that these tree fragments (which exclude
the detached subtrees) also correspond to translation
equivalents. The tree fragments that are obtained by
decomposing the synchronous trees in this fashion
are similar to the Synchronous Tree Insertion Gram-
mar of (Shieber and Schabes, 1990).
We developed a tree traversal algorithm that de-
composes parallel trees into all minimal tree frag-
ments. Given two synchronous trees and their node
alignment decomposition information, our tree frag-
ment extraction algorithm operates by an ?in-order?
traversal of the trees top down, starting from the root
nodes. The traversal can be guided by either the
source or target parse tree. Each node in the tree
that is marked as an aligned node triggers a decom-
position. The subtree that is rooted at this node is
removed from the currently traversed tree. A copy
of the removed subtree is then recursively processed
for top-down decomposition. If the current tree node
being explored is not an aligned node (and thus is not
a decomposition point), the traversal continues down
the tree, possibly all the way to the leaves of the tree.
Decomposition is performed on the corresponding
parallel tree at the same time. We apply this pro-
cess on all the aligned constituent nodes (decompo-
sition points) to obtain all possible decomposed syn-
chronous tree fragment pairs from the original par-
allel parse trees. This results in a collection of all
minimal synchronous subtree fragments. These syn-
chronous subtree fragments are minimal in the sense
that they do not contain any internal aligned nodes.
Another property of the synchronous subtree frag-
ments is that their frontier nodes are either aligned
nodes from the original tree or leaf nodes (corre-
sponding to lexical items). Figure 3 shows some
sample tree fragment pairs that were obtained from
the example discussed earlier in Figure 1.
5.3 Synchronous Transfer Rule Creation
In the last step, we convert the synchronous tree
fragment pairs obtained as described above into syn-
Figure 3: Tree Fragment Pairs Extracted from Aligned
Nodes
chronous context-free rules. This creates rules in a
format that is compatible with the Stat-XFER for-
malism that was described in Section 1. Our system
currently does not use the internal tree structure in-
formation that is contained in the synchronous tree
fragments. Therefore, only the syntactic category la-
bels of the roots of the tree fragments, and the nodes
on the fragment frontier are relevant to decoding.
This in essense corresponds to a ?flattening? of the
synchronous tree fragment into a synchronous con-
text free style rule.
The flattening of the tree fragments is accom-
plished by an ?in-order? traversal on each of the tree
fragments to produce a string representation. Fron-
tier nodes in the fragment are either labeled con-
stituent nodes or leaf nodes of the original parse tree.
These form the right-hand sides of the flattened rule.
The positions of the constituent nodes in the output
string are numbered to keep track of alignment of the
nodes, which is often non-monotonic due to reorder-
ing between the source and target languages. Finally
the root constituent label of the source tree fragment
becomes the source-side parent category of the rule,
while the root label of the target tree fragment be-
comes the target side parent category.
Accurate automatic transfer rule learning re-
quires accurate word alignments and parse struc-
tures. Thus, to favor high precision (at the expense
of some loss of recall), in our work to date on Chi-
nese and other languages, while we extract syntactic
phrases from all available parallel data, we extract
93
rules only from manually word-aligned parsed par-
allel data. To compensate for the limited amount of
data, we generalize the rules as much as possible.
Elements in the rules that originate from leaf nodes
in the parse trees are generalized to their part-of-
speech categories, if the corresponding words were
one-to-one aligned in the parallel sentences. Un-
aligned words and words that are part of one-to-
many alignments are not generalized to the POS
level and remain lexicalized in the final rule.
The phrase table extracted from the corpus and the
rules are scored together to ensure that they are con-
sistent when used in our translation system. For all
Stat-XFER experiments to date, we have used just
the source side conditionig with a constant smooth-
ing factor for robustness to noise.
6 Extraction Applied to Chinese-English
Parallel Data
We used the pipeline of PFA node alignment fol-
lowed by rule extraction to build resources for a
Stat-XFER Chinese-to-English MT system. The
syntax-based phrase table was constructed from
two large parallel corpora released by LDC for the
DARPA/GALE program. The parallel sentences for
both English and Chinese were parsed using the
Stanford parser. The first corpus consists of about
1.2 million sentence pairs. Our extraction process
applied to this corpus resulted in a syntax-based
phrase table of about 9.2 million entries. The other
data source used was a parallel corpus of about 2.6
million sentences, but many of its entries were from
a Chinese-English lexicon. From this corpus, we ex-
tracted 8.75 million phrases.
Rule learning was performed on a 10K-sentence
parallel corpus that was manually word-aligned, re-
leased by LDC for the DARPA/GALE program.
This manually word-aligned corpus includes the par-
allel Chinese-English treebank of 3,343 sentence
pairs. The treebank sentences come with verified
correct parse trees for English and Chinese. The rest
of the 10K corpus was parsed by the Stanford parser.
The complete 10K parallel corpus was node aligned
and rules were extracted as described in Section 5.
Figure 3 shows two synchronous tree fragments that
were extracted from the example node-aligned sen-
tence pair in Figure 1. After generalization and flat-
Figure 4: Rules Extracted from Tree Pairs
Table 3: Statistics for Chinese-English Rules
tening, we obtain rules such as those shown in Fig-
ure 4. The above process resulted in a collection
of almost 100K rules. Some statistics on this rule
set are shown in Table 3. Analysis of this rule set
indicates that only about 4% of these rules were ob-
served more than once in the data. These include
the most general and useful rules for mapping Chi-
nese syntactic structures to their corresponding En-
glish structures. Most of the ?singleton? rules are
highly lexicalized. A large portion of the singleton
rules are noisy rules, but many of them are good and
useful rules. Experiments indicate that removing all
singleton rules hurts translation performance.
7 Conclusions
The process described in this paper provides a fully
automated solution for extracting large collection
of reliable syntax-based phrase tables and syntac-
tic synchronous transfer rules from large volumes
of parsed parallel corpora. In conjunction with the
Stat-XFER syntax-based framework, this provides a
fully automated solution for building syntax-based
MT systems. The current performance of this ap-
proach still lags behind state-of-the-art phrase-based
systems when trained on the same parallel data but is
showing encouraging improvements. Furthermore,
the resources extracted by our process can be used
by various other syntax-based MT approaches.
94
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Su-
san Dumais; Daniel Marcu and Salim Roukos, editors,
HLT-NAACL 2004: Main Proceedings, pages 273?
280, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL ?06:
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 961?968, Morristown, NJ, USA.
Association for Computational Linguistics.
Declan Groves, Mary Hearne, and Andy Way. 2004. Ro-
bust sub-sentential alignment of phrase-structure trees.
In COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics, page 1072,
Morristown, NJ, USA. Association for Computational
Linguistics.
M. Hearne and A. Way. 2003. Seeing the wood for the
trees: Data-oriented translation.
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Example-based machine transla-
tion based on syntactic transfer with statistical mod-
els. In COLING ?04: Proceedings of the 20th in-
ternational conference on Computational Linguistics,
page 99, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Alon Lavie. 2008. A general search-based syntax-driven
framework for machine translation. In Invited paper in
Proceedings of CICLing-2008, pages 362?375. Com-
putational Linguistics and Intelligent Text Processing,
LNCS 4919,Springer.
D. Ortiz-Mart??nez, I. Garc??a-Varea, and F. Casacuberta.
2005. Thot: a toolkit to train phrase-based statisti-
cal translation models. In Tenth Machine Translation
Summit. AAMT, Phuket, Thailand, September.
Arjen Poutsma. 2000. Data-oriented translation. In Pro-
ceedings of the 18th conference on Computational lin-
guistics, pages 635?641, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie,
and Jaime Carbonell. 2002. Mt for minority lan-
guages usingelicitation-based learning of syntactic-
transfer rules. Machine Translation, 17(4):245?270.
Yvonne Samuelsson and Martin Volk. 2007. Alignment
tools for Parallel Treebanks. In Proceedings of the
GLDV Fruhjahrstagung.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proceedings of the 13th
Conference on Computational Linguistics, pages 253?
258, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
John Tinsley, Mary Hearne, and Andy Way. 2007. Ex-
ploiting Parallel Treebanks to Improve Phrase-Based
Statistical Machine Translation. In Proceedings of
the Sixth International Workshop on Treebanks and
Linguistic Theories (TLT-07), pages 175?187, Bergen,
Norway.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical machine translation system with automatically
learned rewrite patterns. In COLING ?04: Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics, page 508, Morristown, NJ, USA.
Association for Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL ?01: Proceedings
of the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 523?530, Morristown, NJ,
USA. Association for Computational Linguistics.
95
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 49?58,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
Evaluating an Agglutinative Segmentation Model for ParaMor 
Christian Monson, Alon Lavie, Jaime Carbonell, Lori Levin 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15217, USA 
 {cmonson, alavie, jgc, lsl}@cs.cmu.edu
Abstract 
This paper describes and evaluates a modifica-
tion to the segmentation model used in the un-
supervised morphology induction system, Pa-
raMor. Our improved segmentation model 
permits multiple morpheme boundaries in a 
single word. To prepare ParaMor to effectively 
apply the new agglutinative segmentation 
model, two heuristics improve ParaMor?s pre-
cision. These precision-enhancing heuristics 
are adaptations of those used in other unsuper-
vised morphology induction systems, including 
work by Hafer and Weiss (1974) and Gold-
smith (2006). By reformulating the segmenta-
tion model used in ParaMor, we significantly 
improve ParaMor?s performance in all lan-
guage tracks and in both the linguistic evalua-
tion as well as in the task based information re-
trieval (IR) evaluation of the peer operated 
competition Morpho Challenge 2007. Para-
Mor?s improved morpheme recall in the lin-
guistic evaluations of German, Finnish, and 
Turkish is higher than that of any system which 
competed in the Challenge. In the three lan-
guages of the IR evaluation, our enhanced Pa-
raMor significantly outperforms, at average 
precision over newswire queries, a morpho-
logically na?ve baseline; scoring just behind the 
leading system from Morpho Challenge 2007 
in English and ahead of the first place system 
in German.  
1 Unsupervised Morphology Induction 
Analyzing the morphological structure of words 
can benefit natural language processing (NLP) ap-
plications from grapheme-to-phoneme conversion 
(Demberg et al, 2007) to machine translation 
(Goldwater and McClosky, 2005). But many of the 
world?s languages currently lack morphological 
analysis systems. Unsupervised induction could fa-
cilitate, for these lesser-resourced languages, the 
quick development of morphological systems from 
raw text corpora. Unsupervised morphology induc-
tion has been shown to help NLP tasks including 
speech recognition (Creutz, 2006) and information 
retrieval (Kurimo et al, 2007b). In this paper we 
work with languages like Spanish, German, and 
Turkish for which morphological analysis systems 
already exist. 
The baseline ParaMor algorithm which we ex-
tend here competed in the English and German 
tracks of Morpho Challenge 2007 (Monson et al, 
2007b). The peer operated competitions of the 
Morpho Challenge series standardize the evalua-
tion of unsupervised morphology induction algo-
rithms (Kurimo et al, 2007a; 2007b). The ParaMor 
algorithm showed promise in the 2007 Challenge, 
placing first in the linguistic evaluation of German. 
Developed after the close of Morpho Challenge 
2007, our improvements to the ParaMor algorithm 
could not officially compete in this Challenge. 
However, the Morpho Challenge 2007 Organizing 
Committee (Kurimo et al, 2008) graciously over-
saw the quantitative evaluation of our agglutinative 
version of ParaMor.  
1.1 Related Work 
A variety of approaches to unsupervised morphol-
ogy induction have shown promise in past work: 
Here we highlight three techniques which have 
been used in a number of unsupervised morphol-
ogy induction algorithms. Since character se-
quences are less predictable at morpheme bounda-
ries than within any particular morpheme (see dis-
cussion in section 2.1), a first unsupervised mor-
49
phology induction technique measures the predict-
ability of word-internal character sequences. Harris 
(1955) was the first to propose the branching factor 
of the character tree of a corpus vocabulary as a 
measure of character predictability. Character trees 
have been incorporated into a number of more re-
cently proposed unsupervised morphology induc-
tion systems (Schone and Jurafsky, 2001; Wicen-
towski, 2002; Goldsmith, 2006; Bordag, 2007). 
Johnson and Martin (2003) generalize from charac-
ter trees and model morphological character se-
quences with minimized finite state automata. 
Bernhard (2007) measures character predictability 
by directly computing transitional probabilities be-
tween substrings of words. 
A second successful technique has used the 
minimum description length principle to capture 
the morpheme as a recurrent structure of morphol-
ogy. The Linguistica system of Goldsmith (2006), 
the Morfessor system of Creutz (2006), and the 
system described in Brent et al (1995) take this 
approach. 
A third technique leverages inflectional para-
digms as the organizational structure of morphol-
ogy. The ParaMor algorithm, which this paper ex-
tends, joins Snover (2002), Zeman (2007), and 
Goldsmith?s Linguistica in building morphology 
models around the paradigm.  
ParaMor tackles three challenges that face mor-
phology induction systems which Goldsmith's Lin-
guistica algorithm does not yet address. First, sec-
tion 2.2 of this paper introduces an agglutinative 
segmentation model. This agglutinative model seg-
ments words into as many morphemes as the data 
justify. Although Goldsmith (2001) and Goldsmith 
and Hu (2004) discuss ideas for segmenting indi-
vidual words into more than two morphemes, the 
implemented Linguistica algorithm, as presented in 
Goldsmith (2006), permits at most a single mor-
pheme boundary in each word. Second, ParaMor 
decouples the task of paradigm identification from 
that of word segmentation (Monson et al, 2007b). 
In contrast, morphology models in Linguistica in-
herently encode both a belief about paradigm 
structure on individual words as well as a segmen-
tation of those words. Without ParaMor?s decoup-
ling of paradigm structure from specific segmenta-
tion models, our algorithm for agglutinative seg-
mentation (section 2.2) would not have been possi-
ble. Third, the evaluation of ParaMor in this paper 
is over much larger corpora than any published 
evaluation of Linguistica. Goldsmith (2006) seg-
ments the Brown corpus of English, which, after 
discarding numbers and punctuation, has a vocabu-
lary size of 47,607 types. Using Linguistica, Creutz 
(2006) successfully segments a Finnish corpus of 
250,000 tokens (approximately 130,000 types), but 
Creutz notes that Linguistica is memory intensive 
and not runable for larger corpora. In the evalua-
tions of Morpho Challenge 2007, ParaMor seg-
mented the words from corpora with over 42 mil-
lion tokens and vocabularies as large as 2.2 million 
types.  
2 ParaMor 
This section briefly outlines the high level struc-
ture of ParaMor as described in detail in Monson et 
al. (2007a; 2007b). ParaMor takes the inflectional 
paradigm as the basic building block of morphol-
ogy. A paradigm is a mutually substitutable set of 
morphological operations. For example, most ad-
jectives in Spanish inflect for two paradigms. First, 
adjectives are marked for gender: an a suffix 
marks feminine, an o masculine. Then Spanish ad-
jectives mark number: an s suffix signals plural, 
while no marking, ? in this paper, indicates singu-
lar. The four surface forms of the cross-product of 
the gender and number paradigms on the Spanish 
word for ?beautiful? are then: bello, bella, bellos, 
and bellas.  
ParaMor is a two stage algorithm. In the first 
stage, ParaMor identifies candidate paradigms 
which likely model suffixes of morphological pa-
radigms and their cross-products. Since some 70% 
of the world?s languages are significantly suffixing 
(Dryer, 2005), ParaMor only attempts to identify 
suffix paradigms. ParaMor?s first stage consists of 
three pipelined steps. In the first step, ParaMor 
searches a space of candidate partial paradigms, 
called schemes, for those which possibly model 
suffixes of true paradigms. The second step merges 
selected schemes which appear to model the same 
paradigm. And in the third step, ParaMor discards 
scheme clusters which likely do not model true 
paradigms.  
The second stage of the ParaMor algorithm 
segments word forms using the candidate para-
digms identified in the first stage. Section 2.2 of 
this paper introduces a new segmentation model 
for ParaMor?s second stage that allows more than 
one morpheme boundary in a single word?as is 
50
needed to correctly segment Spanish plural adjec-
tives. As this agglutinative segmentation model re-
lies on the paradigms learned in ParaMor?s first 
stage, section 2.1 presents solutions to two types of 
paradigm model error that the baseline ParaMor 
system makes. The solutions to these two error 
types are similar in nature to ideas proposed in the 
unsupervised morphology induction work of Hafer 
and Weiss (1974) and Goldsmith (2006). 
2.1 Precision at Paradigm Identification 
Table 1 presents 14 of the more than 8000 schemes 
identified during one baseline run of ParaMor?s 
scheme search step. Each row of Table 1 lists a 
scheme that was selected while searching over a 
Spanish newswire corpus of 50,000 types. On the 
far left of Table 1, the Rank column states the or-
dinal rank at which that row?s scheme was selected 
during the search procedure: the first scheme Pa-
raMor selects is ?.s; a.as.o.os is the second; ido.-
idos.ir.ir? is the 1566th selected scheme, etc. The 
right four columns of Table 1, present raw data on 
the selected schemes, giving the number of can-
didate suffixes in that scheme, the proposed suf-
fixes themselves, the number of candidate stems in 
the scheme, and a sample of those candidate stems. 
Each candidate stem in a ParaMor scheme forms a 
word that occured in the input corpus with each 
candidate suffix belonging to that scheme; for 
example, from the first selected scheme, the candi-
date stem apoyada joins to the candidate suffix s to 
form the word apoyadas ?supported (adjective 
feminine plural)??a word which occured in the 
Spanish newswire corpus.  
Between the rank on the left and the scheme 
details on the right of Table 1, are columns which 
categorize the scheme on its success, or failure, to 
model a true paradigm of Spanish. A dot appears in 
the columns marked Noun, Adjective, or Verb if the 
majority of the candidate suffixes in a row?s 
scheme attempt to model suffixes in a paradigm of 
that part of speech. A dot appears in the Derivation 
column if one or more candidate suffixes of the 
scheme models a Spanish derivational suffix. The 
Good column is marked if the candidate suffixes of 
a scheme take the surface form of true paradig-
matic suffixes. Initially selected schemes in Table 
1 that correctly capture suffixes of real Spanish 
paradigms are the 1st, 2nd, 5th, 13th, 30th, and 1566th 
selected schemes. While some smaller paradigms 
of Spanish are perfectly identified (including ?.s, 
which marks singular and plural on many nouns 
and adjectives, and the adjectival cross-product 
paradigm of gender and number, a.as.o.os) many 
selected schemes do not satisfactorily model Span-
ish suffixes. Incorrect schemes in Table 1 are 
marked in the Error columns.  
The vast majority of unsatisfactory paradigm 
models fail for one of two reasons. First, many 
schemes contain candidate suffixes which system-
Model of Error 
Verb 
Ra
nk
 
No
un
 
Ad
jec
tiv
e 
ar er ir 
De
riv
ati
on
 
Go
od
 
St
em
 In
ter
na
l 
Su
ffi
x I
nt
er
na
l 
Ch
an
ce
 Candidate Suffixes Candidate Stems 
1 ?  ?      ?     2 ?.s 5513 apoyada, barata, hombro, oficina, reo, ? 
2  ?      ?     4 a.as.o.os 899 apoyad, captad, dirigid, junt, pr?xim, ? 
3   ?       ?   14 ?.ba.ban.da.das.do.dos.n.ndo.r.ron.rse.r?.r?n 25 apoya, disputa, lanza, lleva, toma, ? 
5   ?     ?     15 a.aba.aban.ada.adas.ado.ados.an.ando.ar.aron.arse.ar?.ar?n.? 24 apoy, desarroll, disput, lanz, llev, ? 
11  ?     ?   ?    5 ta.tamente.tas.to.tos 22 cier, direc, ins?li, modes, sangrien, ? 
12   ?    ?    ?   14 ?.ba.ci?n.da.das.do.dos.n.ndo.r.ron.r?.r?n.r?a 16 acepta, concentra, fija, provoca, ? 
13   ?     ?     15 a.aba.ada.adas.ado.ados.an.ando.ar.aron.ar?.ar?n.e.en.? 20 apoy, declar, enfrent, llev, tom, ? 
30    ?  ?   ?     11 a.e.en.ida.idas.ido.idos.iendo.ieron.i?.?a 15 cumpl, escond, recib, transmit, vend, ? 
1000          ?  3 ?.g.gs 4 h, k, on, s 
1566     ?   ?     4 ido.idos.ir.ir? 6 conclu, cumpl, distribu, exclu, reun, segu 
2000      ?   ?    2 lia.liana 5 austra, ita, ju, sici, zu 
3000          ?  3 ?.a.anar 4 all, am, g, s 
4000          ?  3 ?.e.ince 4 l, pr, qu, v 
8000   ?      ?    2 trada.trarnos 3 concen, demos, encon 
               
 
Table 1. Candidate partial paradigms, or schemes, that the baseline ParaMor algorithm selected during its first step, 
search, of its first stage, paradigm identification. This baseline ParaMor run was over a Spanish newswire corpus of 
50,000 types. While some selected schemes contain suffixes from true paradigms, other schemes contain incorrectly 
segmented candidate suffixes. 
  
51
atically misanalyze word forms. These schemes 
consistently hypothesize either stem-internal or 
suffix-internal morpheme boundaries. Schemes 
which hypothesize incorrect morpheme boundaries 
include the 3rd, 11th, 12th, 2000th, and 8000th se-
lected schemes of Table 1. Among these, the 3rd 
and 12th selected schemes place morpheme boun-
daries internal to true suffixes. For example, the 3rd 
selected scheme contains truncated forms of suf-
fixes that occur correctly in the 5th selected 
scheme. Symmetrically, the candidate suffixes in 
the 11th, 2000th, and 8000th selected schemes hy-
pothesize morpheme boundaries internal to true 
Spanish stems, inadvertently including portions of 
stems within their suffix lists. In a random sample 
of 100 schemes from the 8240 schemes that the 
baseline ParaMor algorithm selects over our Span-
ish corpus, 59 schemes hypothesized an incorrect 
morpheme boundary. 
The second most prevalent reason for model 
failure occurs when the candidate suffixes of a 
scheme are related not by belonging to the same 
paradigm, but rather by a chance co-occurrence on 
a few candidate stems of the text. Schemes which 
arise from chance string collisions in Table 1 in-
clude the 1000th, 3000th, and 4000th selected 
schemes. The string lengths of the candidate stems 
and candidate suffixes of these chance schemes are 
often quite short. The longest candidate stem in 
any of the three chance-error schemes of Table 1 is 
three characters long; and all three selected 
schemes propose the suffix ?, which has length 
zero. Short stems and short suffixes in selected 
schemes are easily explained combinatorially: The 
inventory of possible strings grows exponentially 
with the length of the string. Because there just 
aren?t very many length one, length two, or even 
length three strings, it should come as no surprise 
when a variety of candidate suffixes happen to oc-
cur attached to the same set of short stems. In our 
random sample of 100 initially selected schemes, 
35 were erroneously selected as a result of a 
chance collision of word types. 
The next two sub-sections present solutions to 
the two types of paradigm model failure in the 
baseline algorithm that are exemplified in Table 1. 
These first two extensions aim to improve preci-
sion by reducing the number of schemes ParaMor 
erroneously selects. 
 
Correcting Morpheme Boundary Errors 
Most of the baseline selected schemes which incor-
rectly hypothesize a morpheme boundary do so at 
stem-internal positions. Indeed, in our random 
sample of 100 schemes, 51 of the 59 schemes with 
morpheme boundary errors incorrectly hypothe-
sized a boundary stem-internally. For this reason, 
the baseline ParaMor algorithm already discarded 
schemes that likely misplace a boundary stem-
internally (Monson et al, 2007b). Although there 
are fewer schemes that misplace a morpheme 
boundary suffix-internally, suffix-internal error 
schemes contain short suffixes that can generalize 
to segment a large number of word forms. (See 
section 2.2 for a description of ParaMor?s morpho-
logical segmentation model). To measure the in-
fluence of suffix-internal error schemes on mor-
pheme segmentation, we examined ParaMor?s 
baseline segmentations of a random sample of 100 
word forms from the 50,000 words of our Spanish 
corpus. In these 100 words, 82 morpheme bounda-
ries were introduced that should not have been. 
And 40 of these 82 incorrectly proposed bounda-
ries were placed by schemes which hypothesized a 
morpheme boundary internal to true suffixes.  
To address the problem of suffix-internal mis-
placed boundaries we adapt an idea originally pro-
posed by Harris (1955) and extended by Hafer and 
Weiss (1974): Take any string t. Let F be the set of 
strings such that for each Ff ? , t.f is a word form 
of a particular natural language. Harris noted that 
when the boundaries between t and each f fall at 
morpheme boundaries, the strings in F typically 
begin in a wide variety of characters; but when the 
t-f boundaries are morpheme-internal, each legiti-
mate word final string must first complete the er-
roneously split morpheme, and so the strings in F 
will begin with one of a very few characters. This 
argument similarly holds when the roles of t and f 
are reversed. Hafer and Weiss (1974) describe a 
number of variations to Harris? letter variety algo-
rithm. Their most successful variation uses entropy 
to measure character variety.  
Goldsmith?s (2006) Linguistica algorithm pio-
neered the use of entropy in a paradigm-based un-
supervised morphology induction system. Linguis-
tica measures the entropy of stem-final characters 
in a set of initially selected paradigm models. 
When entropy falls below a threshold, Linguistica 
considers relocating the morpheme boundary of 
52
each word covered by that paradigm model. If, af-
ter boundary relocation, the resulting description 
length of Linguistica?s morphology model de-
creases, Linguistica accepts the relocated bounda-
ries.  
To identify suffix-internal morpheme boundary 
errors among ParaMor?s initially selected schemes, 
we follow Hafer and Weiss (1974) and Goldsmith 
(2006) in using entropy as a measure of the variety 
in boundary-adjacent character distributions. In a 
ParaMor style scheme, the candidate stems form a 
set of word-initial strings, and the candidate suf-
fixes a set of word-final strings. If a scheme?s 
stems end in a very few unique characters, the 
scheme has likely hypothesized an incorrect suffix-
internal morpheme boundary. Consider the 3rd se-
lected scheme in Table 1. All 25 of the 3rd 
scheme?s stems end in the character ?a?. Conse-
quently, we measure the entropy of the distribution 
of final characters in each scheme?s candidate 
stems. Where Linguistica modifies paradigm mod-
els which appear to incorrectly place morpheme 
boundaries, our extension to ParaMor permanently 
removes schemes. To avoid introducing a free pa-
rameter, our extension to ParaMor flags a scheme 
as a likely boundary error only when virtually all 
of that scheme?s candidate stems end in the same 
character. We flag a scheme if its entropy is below 
a threshold set close to zero, 0.5. The baseline Pa-
raMor algorithm discards schemes which it be-
lieves hypothesize an incorrect stem-internal mor-
pheme boundary only after the scheme clustering 
step of ParaMor?s paradigm identification stage. 
Our extension follows suit: If we flag more than 
half of the schemes in a cluster as likely proposing 
a suffix-internal boundary, then we discard that 
cluster. Referencing Table 1, this first extension to 
ParaMor successfully removes both the 3rd and the 
12th selected schemes.  
Correcting Chance String Collision Errors 
Scheme errors due to chance string collisions are 
the second most prevalent error type. As described 
above, the string lengths of the candidate stems 
and suffixes of chance schemes are typically short. 
When the stems and suffixes of a scheme are short, 
then the underlying types which support a scheme 
are also short. Where the baseline ParaMor algo-
rithm explicitly builds schemes over all types in a 
corpus, we modify ParaMor to exclude short types 
from the vocabulary during morphology induction. 
Goldsmith (2006) also uses string-length thresh-
olds to restrict what paradigm models the Linguis-
tica algorithm produces. 
Excluding short types during ParaMor?s mor-
phology induction stage does not preclude short 
types from being analyzed as containing multiple 
morphemes during ParaMor?s segmentation stage. 
As section 2.2 describes, ParaMor?s segmentation 
algorithm is independent of the set of types from 
which schemes and scheme clusters are built. 
The string length that types must meet to join 
the induction vocabulary is a free parameter. Pa-
raMor is designed to identify the productive inflec-
tional paradigms of a language. Unless a paradigm 
is restricted to occur only with short stems, a pos-
sible but unusual scenario (as with the English ad-
jectival comparative, c.f. faster but *exquisiter) we 
can expect a productive paradigm to occur with a 
reasonable number of longer stems in a corpus. 
Hence, ParaMor needn?t be overly concerned 
about discarding short types. A qualitative exam-
ination of Spanish data suggested discarding types 
five characters or less in length; we use this cutoff 
in all experiments described in this paper. 
Excluding short types from the paradigm induc-
tion vocabulary virtually eliminates the entire cate-
gory of chance scheme. In a random sample of 100 
schemes that ParaMor selected when short types 
were excluded, only one scheme contained types 
related only by chance string similarity, down from 
35 when short types were not excluded. Returning 
to Table 1, excluding types five characters or less 
in length bars ten of the twelve word types which 
support the erroneous 3000th selected scheme ?.a.-
anar. Among the excluded types are valid Spanish 
words such as ganar ?to gain?. But also eliminated 
are several meaningless acronyms such as the sin-
gle letters g and s. Without these short types, Pa-
raMor rightly cannot select the 3000th scheme. 
2.2 Segmentation 
An Agglutinative Model 
With the improvement in scheme precision that re-
sults from the two extensions discussed in section 
2.1, we are ready to propose a more realistic model 
of morphology. ParaMor?s baseline segmentation 
algorithm distrusts ParaMor?s induced scheme 
models. The baseline algorithm assumes each word 
form can contain at most a single morpheme 
boundary. If it detects more than one morpheme 
53
boundary, then the baseline algorithm proposes a 
separate morphological analysis for each possible 
boundary. In contrast, our extended model of seg-
mentation vests more trust in the induced schemes, 
assuming that scheme clusters which propose dif-
ferent morpheme boundaries are simply modeling 
different valid morpheme boundaries. And our ex-
tension proposes a single morphological analysis 
containing all hypothesized morpheme boundaries.  
To detect morpheme boundaries, ParaMor 
matches each word, w, in the full vocabulary of a 
corpus against the clusters of schemes which are 
the final output of ParaMor?s paradigm identifica-
tion stage. When a suffix, f, of some scheme-
cluster, C, matches a word-final string of w, i.e. 
fuw .= , ParaMor attempts to replace f in turn with 
each suffix f ?  of C. If the string fu ?.  occurs in 
the full corpus vocabulary, then, on the basis of 
this paradigmatic evidence, ParaMor identifies a 
morpheme boundary in w between u and f . 
For example, to detect morpheme boundaries in 
the Spanish word apoyados ?supports (adjective 
masculine plural)?, ParaMor matches all word-
final strings of apoyados against the candidate suf-
fixes of ParaMor?s induced scheme clusters. The 
word-final strings of apoyados are s, os, dos, ados, 
yados, ?. The scheme clusters that our extended 
version of ParaMor induces include clusters which 
contain schemes very similar to the 1st, 2nd, and 5th 
baseline selected schemes, see Table 1. In particu-
lar, our extended ParaMor identifies separate 
scheme clusters that contain the candidate suffixes: 
s and ?; os and o; and ados and ado. Substituting 
? for s, o for os, or ado for ados yields the Spanish 
string apoyado ?supports (adjective masculine sin-
gular)?. It so happens, that apoyado does occur in 
our Spanish corpus, and so ParaMor has found 
paradigmatic evidence for three morpheme boun-
daries. Crucially, our ParaMor extension from sec-
tion 2.1 that removes schemes which hypothesize 
suffix internal morpheme boundaries correctly dis-
cards all schemes which contained the candidate 
suffix dos. Consequently, no scheme cluster exists 
to incorrectly suggest the morpheme boundary 
*apoya + dos, as the 3rd baseline selected scheme 
would have. Where ParaMor?s baseline segmenta-
tion algorithm would propose three separate analy-
ses of apoyados, one for each detected morpheme 
boundary: apoy +ados, apoyad +os, and apoyado 
+s; our extended segmentation algorithm produces 
the single correct analysis: apoy +ad +o +s.  
It is interesting to note that although each of Pa-
raMor?s individual paradigm models proposes a 
single morpheme boundary, our agglutinative seg-
mentation model can recover multiple boundaries 
in a single word. Using this idea it may be possible 
to quickly adapt Linguistica for agglutinative lan-
guages. Instead of interpreting the sets of stems 
and affixes that Goldsmith?s Linguistica algorithm 
produces as immediate segmentations of words, 
these signatures can be thought of as models of 
paradigms that may generalize to new words. 
Augmenting ParaMor?s Segmentations 
With its focus on the paradigm, ParaMor special-
izes at analyzing inflectional morphology (Monson 
et al, 2007a). Morpho Challenge 2007 requires al-
gorithms to analyze both inflectional and deriva-
tional morphology (Kurimo et al, 2007a; 2007b). 
To compete in the challenge, we combine Pa-
raMor?s morphological segmentations with seg-
mentations from Morfessor (Creutz, 2006), an un-
supervised morphology induction algorithm which 
learns both inflectional and derivational morphol-
ogy. We incorporate the segmentations from Mor-
fessor into the segmentations that the ParaMor sys-
tem produces by straightforwardly adding the Mor-
fessor segmentation for each word as an additional 
separate analysis to those ParaMor produces (Mon-
son et al, 2007b). Morfessor has one free parame-
ter, which we optimize separately for each lan-
guage of Morpho Challenge 2007.  
ParaMor also has several free parameters, in-
cluding the type length parameter and the parame-
ter over stem-final character entropy described in 
section 2.1. We do not adjust any of ParaMor?s pa-
rameters from language to language, but fix them 
at values that produce reasonable Spanish para-
digms and segmentations. As in Monson et al 
(2007b), to avoid adjusting ParaMor?s parameters 
we limit ParaMor?s paradigm induction vocabulary 
to 50,000 frequent types for each language.  
3 Evaluation 
To evaluate our extensions to the ParaMor algo-
rithm, we follow the methodology of the peer op-
erated Morpho Challenge 2007. All segmentations 
produced by our extensions were sent to the Mor-
pho Challenge Organizing Committee (Kurimo et 
al., 2008). The Organizing Committee evaluated 
our segmentations and returned the automatically 
54
calculated quantitative results. Using the evalua-
tion methodology of Morpho Challenge 2007 per-
mits us to compare our algorithms against the un-
supervised morphology induction systems which 
competed in the 2007 Challenge. Of the many al-
gorithms for unsupervised morphology induction 
discussed with the related work in section 1.1, five 
participated in Morpho Challenge 2007. Unless an 
algorithm has been given an explicit name, mor-
phology induction algorithms will be denoted in 
this paper by the name of their lead author. The 
five algorithms which participated in the 2007 
Challenge are: Bernhard (2007), Bordag (2007), 
Zeman (2007), Creutz?s (2006) Morfessor, and Pa-
raMor (2007b). 
Morpho Challenge 2007 had participating algo-
rithms analyze words in four languages: English, 
German, Finnish, and Turkish. The Challenge 
evaluated each algorithm?s morphological analyses 
in two ways. First, a linguistic evaluation measured 
each algorithm?s precision, recall, and F1 at mor-
pheme identification against an answer key of mor-
phologically analyzed word forms. Scores were 
normalized when a system proposed multiple 
analyses of a single word, as our combined Pa-
raMor-Morfessor submissions do. For further de-
tails on the linguistic evaluation in Morpho Chal-
lenge 2007, see Kurimo et al (2007a). The second 
evaluation of Morpho Challenge 2007 was a task 
based evaluation. Each algorithm?s analyses were 
imbedded in an information retrieval (IR) system. 
The IR evaluation consisted of queries over a lan-
guage specific collection of newswire articles. All 
word forms in all queries and all documents were 
replaced with the morphological decompositions of 
each individual analysis algorithm. Separate IR 
tasks were run for English, German, and Finnish, 
but not Turkish. For additional details on the IR 
evaluation of Morpho Challenge 2007 please refer-
ence Kurimo et al (2007b). 
Tables 2 and 3 present, respectively, the lin-
guistic and IR evaluation results. In these two ta-
bles, the top two rows contain results for segmen-
tations produced by versions of ParaMor that in-
clude our extensions. The topmost row in each ta-
ble, labeled ?+P +Seg?, gives the results for our 
fully augmented version of ParaMor, which in-
cludes our two extensions designed to improve 
precision as well as our new segmentation model 
which can propose multiple morpheme boundaries 
in a single analysis of a word form. The second 
row of each table, labeled ?+P ?Seg?, augments Pa-
raMor only with the two enhancements designed to 
improve precision. The third row of each table 
gives the Challenge results for the ParaMor base-
line algorithm. Rows four through seven of each 
table give scores from Morpho Challenge 2007 for 
the best performing unsupervised systems. If mul-
tiple versions of a single algorithm competed in the 
Challenge, the scores reported here are the highest 
F1 or Average Precision score of any algorithm 
variant at a particular task. In all test scenarios but 
Finnish IR, we produced Morfessor segmentations 
to augment ParaMor that are independent of the 
Morfessor runs which competed in Morpho Chal-
lenge. If our Morfessor runs gave a higher F1 or 
Average Precision, then we report this higher 
score. Finally, scores reported on rows eight and 
beyond are from reference algorithms that are not 
unsupervised. Reference algorithms appear in ital-
ics. A double line bisects both Table 2 and Table 3 
horizontally. All results which appear above the 
double line were evaluated after the final deadline 
of Morpho Challenge 2007. In particular, ParaMor 
officially competed only in the English and Ger-
man tracks of the Challenge.  
The Linguistic Evaluation 
Table 2 contains the results from the linguistic 
evaluation of Morpho Challenge. The Morpho 
Challenge Organizing Committee did not provide 
us with data on the statistical significance of the 
results for the enhanced versions of ParaMor. But 
most score differences are statistically signifi-
cant?All F1 differences of more than 0.5 between 
systems which officially competed in Morpho 
Challenge 2007 were statistically significant (Ku-
rimo et al, 2007a).  
In German, Finnish, and Turkish our fully en-
hanced version of ParaMor achieves a higher F1 
than any system that competed in Morpho Chal-
lenge 2007. In English, ParaMor?s precision score 
drags F1 under that of the first place system, Bern-
hard; In Finnish, the Bernhard system?s F1 is likely 
not statistically different from that of our system. 
Our final segmentation algorithm demonstrates 
consistent performance across all four languages. 
In Turkish, where the morpheme recall of other 
unsupervised systems is anomalously low, our al-
gorithm achieves a recall in a range similar to its 
recall scores for the other languages. ParaMor?s ul-
timate recall is double that of any other unsuper-
55
vised Turkish system, leading to an improvement 
in F1 over the next best system, Morfessor alone, 
of 13.5% absolute or 22.0% relative.  
In all four languages, as expected, the combina-
tion of removing short types from the training data, 
and the additional filtering of scheme clusters, 
?+P?, significantly improves precision scores over 
the ParaMor baseline. Allowing multiple mor-
pheme boundaries in a single word, ?+Seg?, in-
creases the number of words ParaMor believes 
share a morpheme. Some of these new words do in 
fact share a morpheme, some, in reality do not. 
Hence, our extension of ParaMor to agglutinative 
sequences of morphemes increases recall but low-
ers precision across all four languages. The effect 
of agglutinative segmentations on F1, however, dif-
fers with language. For the two languages which 
make limited use of suffix sequences, English and 
German, a model which hypothesizes multiple 
morpheme boundaries can only moderately in-
crease recall and does not justify, by F1, the many 
incorrect segmentations which result. On the other 
hand, an agglutinative model significantly im-
proves recall for true agglutinative languages like 
Finnish and Turkish, more than compensating in F1 
for the drop in precision over these languages. But 
in all four languages, the agglutinative version of 
ParaMor outperforms the baseline unenhanced ver-
sion at F1. 
The final row of Table 2 is the evaluation of a 
reference algorithm submitted by Tepper (2007). 
While not an unsupervised algorithm, Tepper?s 
reference parallels ParaMor in augmenting seg-
mentations produced by Morfessor. Where Pa-
raMor augments Morfessor with special attention 
to inflectional morphology, Tepper augments Mor-
fessor with hand crafted morphophonology rules 
that conflate multiple surface forms of the same 
underlying suffix. Like ParaMor, Tepper?s algo-
rithm significantly improves on Morfessor?s recall. 
With two examples of successful system augmen-
tation, we suggest that future research take a closer 
look at building on existing unsupervised mor-
phology induction systems. 
The IR Evaluation 
Turn now to results from the IR evaluation in Ta-
ble 3. Although ParaMor does not fair as well in 
Finnish, in German, the fully enhanced version of 
ParaMor places above the best system from the 
2007 Challenge, Bernhard, while our score on 
English rivals this same best system. Morpho Chal-
lenge 2007 did not measure the statistical signifi-
cance of uninterpolated average precision scores in 
the IR evaluation. It is not clear what feature of Pa-
raMor?s Finnish analyses causes comparatively 
low average precision. Perhaps it is simply that Pa-
raMor attains a lower morpheme recall over Fin-
nish than over English or German. And unfortu-
nately, Morpho Challenge 2007 did not run IR ex-
periments over the other agglutinative language in 
the competition, Turkish. When ParaMor does not 
combine multiple morpheme boundaries into a sin-
gle analysis, as in the baseline and ?+P ?Seg? sce-
Table 2. Unsupervised morphology induction systems evaluated for precision (P), recall (R), and F1 at morpheme 
identification using the methodology of the linguistic competition of Morpho Challenge 2007. 
English German Finnish Turkish 
 P R F1 P R F1 P R F1 P R F1 
 +P +Seg 50.6 63.3 56.3 49.5 59.5 54.1 49.8 47.3 48.5 51.9 52.1 52.0 
 +P ?Seg 56.2 60.9 58.5 57.4 53.5 55.4 60.5 33.9 43.5 62.0 38.2 47.3 
ParaMor  
&        
Morfessor 
Baseline 41.6 65.1 50.7 51.5 55.6 53.4 55.0 35.6 43.2 53.2 41.6 46.7 
Bernhard 61.6 60.0 60.8 49.1 57.4 52.9 59.7 40.4 48.2 73.7 14.8 24.7 
Bordag 59.7 32.1 41.8 60.5 41.6 49.3 71.3 24.4 36.4 81.3 17.6 28.9 
Morfessor 82.2 33.1 47.2 67.6 36.9 47.8 76.8 27.5 40.6 73.9 26.1 38.5 
Zeman 53.0 42.1 46.9 52.8 28.5 37.0 58.8 20.9 30.9 65.8 18.8 29.2 
Tepper 69.2 52.6 59.8 - - - 62.0 46.2 53.0 70.3 43.0 53.3 
 
56
narios, average precision is comparatively poor. 
Where the linguistic evaluation did not always pe-
nalize a system for proposing multiple partial 
analyses, real NLP applications, such as IR, can. 
The reference algorithms for the IR evaluation 
are: Dummy, no morphological analysis; Oracle, 
where all words in the queries and documents for 
which the linguistic answer key contains an entry 
are replaced with that answer; Porter, the standard 
English Porter stemmer; and Tepper described 
above. While the hand built Porter stemmer still 
outperforms the best unsupervised systems on Eng-
lish, these same best unsupervised systems outper-
form both the Dummy and Oracle references for all 
three evaluated languages?strong evidence that 
unsupervised induction algorithms are not only 
better than no morphological analysis, but that they 
are better than incomplete analysis as well.  
4 Conclusions and Future Directions 
Augmenting ParaMor with an agglutinative model 
of segmentation produces an unsupervised mor-
phology induction system with consistent and 
strong performance at morpheme identification 
across all four languages of Morpho Challenge 
2007. By first cleaning up the paradigm models 
that ParaMor learns, we raise ParaMor?s segmenta-
tion precision and allow the agglutinative model to 
significantly improve ParaMor?s morpheme recall.  
Looking forward to future improvements, we 
examined by hand the final set of scheme clusters 
that the current version of ParaMor produces over 
our newswire corpus of 50,000 Spanish types. Pa-
raMor?s paradigm identification stage outputs 41 
separate clusters. Among these final scheme clus-
ters are those which model all major productive 
paradigms of Spanish. In fact, there are often mul-
tiple scheme clusters which model portions of the 
same true paradigm. As an extreme case, 12 sepa-
rate scheme clusters contain suffixes from the 
Spanish ar verbal paradigm. Relaxing restrictions 
on ParaMor?s clustering algorithm (Monson et al, 
2007a) may address this paradigm fragmentation.  
The second significant shortcoming which sur-
faces among ParaMor?s 41 final scheme clusters is 
that ParaMor currently does not address morpho-
phonology. Among the final scheme clusters, 12 
attempt to model morphophonological change by 
incorporating the phonological change either into 
the stems or into the suffixes of the scheme cluster. 
But ParaMor currently has no mechanism for de-
tecting when a cluster is modeling morphophonol-
ogy. Perhaps ideas on morphophonology from 
Goldsmith (2006) could be adapted to work with 
the ParaMor algorithm. Finally, we plan to look at 
scaling the size of the vocabulary used both during 
paradigm induction and during morpheme segmen-
tation. We are particularly interested in the possi-
bility that ParaMor may  be able to identify para-
digms from much less data than 50,000 types. 
Acknowledgements 
We kindly thank Mikko Kurimo, Ville Turunen, 
Matti Varjokallio, and the full Organizing Com-
mittee of Morpho Challenge 2007, for running the 
evaluations of ParaMor. These dedicated workers 
produced impressively fast turn around for evalua-
tions on sometimes rather short notice. 
The research described in this paper was sup-
ported by NSF grants IIS-0121631 (AVENUE) and 
IIS-0534217 (LETRAS), with supplemental fund-
ing from NSF?s Office of Polar Programs and Of-
fice of International Science and Education. 
Table 3. Unsupervised morphology induction sys-
tems evaluated for uninterpolated average precision 
using the methodology of the IR competition of 
Morpho Challenge 2007. These results use Okapi 
term weighting (Kurimo et al, 2008b). 
*Only a subset of the words which occurred in the 
IR evaluation of this language was analyzed by this 
system.  
 Eng. Ger. Finn. Tur. 
 +P +Seg 39.3 48.4 42.6 - 
 +P ?Seg 35.1 43.1 37.1 - 
ParaMor 
&        
Morfessor 
Baseline 34.4 40.1 35.9 - 
Bernhard 39.4 47.3 49.2 - 
Bordag 34.0 43.1 43.1 - 
Morfessor 38.8 46.0 44.1 - 
Zeman  26.7*  25.7*  28.1* - 
Dummy 31.2 32.3 32.7 - 
Oracle 37.7 34.7 43.1 - 
Porter 40.8 - - - 
Tepper  37.3* - - - 
 
57
References 
Bernhard, Delphine. Simple Morpheme Labeling in Un-
supervised Morpheme Analysis. Working Notes for 
the CLEF 2007 Workshop. Budapest, Hungary, 2007. 
Bordag, Stefan. Unsupervised and Knowledge-free 
Morpheme Segmentation and Analysis. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007. 
Brent, Michael R., Sreerama K. Murthy, and Andrew 
Lundberg. Discovering Morphemic Suffixes: A Case 
Study in MDL Induction. The Fifth International 
Workshop on Artificial Intelligence and Statistics. 
Fort Lauderdale, Florida, 1995.  
Creutz, Mathias. Induction of the Morphology of Natu-
ral Language: Unsupervised Morpheme Segmenta-
tion with Application to Automatic Speech Recogni-
tion. Ph.D. Thesis. Computer and Information Sci-
ence, Report D13. Helsinki: University of Technol-
ogy, Espoo, Finland, 2006. 
Demberg, Vera, Helmut Schmid, and Gregor M?hler. 
Phonological Constraints and Morphological Pre-
processing for Grapheme-to-Phoneme Conversion. 
Association for Computational Linguistics. Prague, 
Czech Republic, 2007. 
Dryer, Matthew S. Prefixing vs. Suffixing in Inflec-
tional Morphology.  In The World Atlas of Language 
Structures. Eds. Martin Haspelmath, Matthew S. 
Dryer, David Gil, and Bernard Comrie. 2005. 
Goldsmith, John. Unsupervised Learning of the Mor-
phology of a Natural Language. Computational Lin-
guistics. 27.2:153-198. 2001. 
Goldsmith, John. An Algorithm for the Unsupervised 
Learning of Morphology. Natural Language Engi-
neering. 12.4:335-351. 2006. 
Goldsmith, John, and Yu Hu. From Signatures to Finite 
State Automata. Paper presented at the Midwest 
Computational Linguistics Colloquium. Blooming-
ton, Indiana, 2004. 
Goldwater, Sharon, and David McClosky. Improving 
Statistic MT through Morphological Analysis. Em-
pirical Methods in Natural Language Processing. 
Vancouver, Canada, 2005. 
Hafer, Margaret A. and Stephen F. Weiss. Word Seg-
mentation by Letter Successor Varieties. Information 
Storage and Retrieval, 10:371-385. 1974. 
Harris, Zellig. From Phoneme to Morpheme. Language 
31.2:190-222. 1955. Reprinted in Harris (1970). 
Harris, Zellig. Papers in Structural and Transforma-
tional Linguists. Ed. D. Reidel, Dordrecht. 1970. 
Johnson, Howard, and Joel Martin. Unsupervised 
Learning of Morphology for English and Inuktitut. 
Human Language Technology Conference / North 
American Chapter of the Association for Computa-
tional Linguistics. Edmonton, Canada, 2003. 
Kurimo, Mikko, Mathias Creutz, and Matti Varjokallio. 
Unsupervised Morpheme Analysis Evaluation by a 
Comparison to a Linguistic Gold Standard ? Morpho 
Challenge 2007. Working Notes for the CLEF 2007 
Workshop. Budapest, Hungary, 2007a. 
Kurimo, Mikko, Mathias Creutz, and Ville Turunen. 
Unsupervised Morpheme Analysis Evaluation by IR 
Experiments ? Morpho Challenge 2007. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007b. 
Kurimo, Mikko, Mathias Creutz, and Matti Varjokallio. 
Unsupervised Morpheme Analysis -- Morpho Chal-
lenge 2007. January 10, 2008. <http://www.cis.hut.-
fi/morphochallenge2007/>. 2008. 
Monson, Christian, Jaime Carbonell, Alon Lavie, and 
Lori Levin. ParaMor: Minimally Supervised Induc-
tion of Paradigm Structure and Morphological 
Analysis. Computing and Historical Phonology: The 
Ninth Meeting of the ACL Special Interest Group in 
Computational Morphology and Phonology. Prague, 
Czech Republic, 2007a. 
Monson, Christian, Jaime Carbonell, Alon Lavie, and 
Lori Levin. ParaMor: Finding Paradigms across 
Morphology. Working Notes for the CLEF 2007 
Workshop. Budapest, Hungary, 2007b. 
Schone, Patrick, and Daniel Jurafsky. Knowledge-Free 
Induction of Inflectional Morphologies. North 
American Chapter of the Association for Computa-
tional Linguistics. Pittsburgh, Pennsylvania, 2001. 
Snover, Matthew G. An Unsupervised Knowledge Free 
Algorithm for the Learning of Morphology in Natural 
Languages. M.S. Thesis. Computer Science, Sever 
Institute of Technology, Washington University, 
Saint Louis, Missouri, 2002. 
Tepper, Michael A. Using Hand-Written Rewrite Rules 
to Induce Underlying Morphology. Working Notes 
for the CLEF 2007 Workshop. Budapest, Hungary, 
2007. 
Wicentowski, Richard. Modeling and Learning Multi-
lingual Inflectional Morphology in a Minimally Su-
pervised Framework. Ph.D. Thesis. Johns Hopkins 
University, Baltimore, Maryland, 2002. 
Zeman, Daniel. Unsupervised Acquiring of Morpho-
logical Paradigms from Tokenized Text. Working 
Notes for the CLEF 2007 Workshop. Budapest, Hun-
gary, 2007. 
 
 
58
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 56?60,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Machine Translation System Combination with Flexible Word Ordering
Kenneth Heafield, Greg Hanneman, Alon Lavie
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
{kheafiel,ghannema,alavie}@cs.cmu.edu
Abstract
We describe a synthetic method for com-
bining machine translations produced by
different systems given the same input.
One-best outputs are explicitly aligned
to remove duplicate words. Hypotheses
follow system outputs in sentence order,
switching between systems mid-sentence
to produce a combined output. Experi-
ments with the WMT 2009 tuning data
showed improvement of 2 BLEU and 1
METEOR point over the best Hungarian-
English system. Constrained to data pro-
vided by the contest, our system was sub-
mitted to the WMT 2009 shared system
combination task.
1 Introduction
Many systems for machine translation, with dif-
ferent underlying approaches, are of competitive
quality. Nonetheless these approaches and sys-
tems have different strengths and weaknesses. By
offsetting weaknesses with strengths of other sys-
tems, combination can produce higher quality than
does any component system.
One approach to system combination uses con-
fusion networks (Rosti et al, 2008; Karakos et
al., 2008). In the most common form, a skele-
ton sentence is chosen from among the one-best
system outputs. This skeleton determines the or-
dering of the final combined sentence. The re-
maining outputs are aligned with the skeleton, pro-
ducing a list of alternatives for each word in the
skeleton, which comprises a confusion network. A
decoder chooses from the original skeleton word
and its alternatives to produce a final output sen-
tence. While there are a number of variations on
this theme, our approach differs fundamentally in
that the effective skeleton changes on a per-phrase
basis.
Our system is an enhancement of our previous
work (Jayaraman and Lavie, 2005). A hypothesis
uses words from systems in order, switching be-
tween systems at phrase boundaries. Alignments
and a synchronization method merge meaning-
equivalent output from different systems. Hy-
potheses are scored based on system confidence,
alignment support, and a language model.
We contribute a few enhancements to this pro-
cess. First, we introduce an alignment-sensitive
method for synchronizing available hypothesis ex-
tensions across systems. Second, we pack similar
partial hypotheses, which allows greater diversity
in our beam search while maintaining the accuracy
of n-best output. Finally, we describe an improved
model selection process that determined our sub-
missions to the WMT 2009 shared system combi-
nation task.
The remainder of this paper is organized as fol-
lows. Section 2 describes the system with empha-
sis on our modifications. Tuning, our experimen-
tal setup, and submitted systems are described in
Section 3. Section 4 concludes.
2 System
The system consists of alignment (Section 2.1)
and phrase detection (Section 2.2) followed by de-
coding. The decoder constructs hypothesis sen-
tences one word at a time, starting from the left. A
partially constructed hypothesis comprises:
Word The most recently decoded word. Initially,
this is the beginning of sentence marker.
Used The set of used words from each system.
Initially empty.
Phrase The current phrase constraint from Sec-
tion 2.2, if any. The initial hypothesis is not
in a phrase.
Features Four feature values defined in Section
2.4 and used in Section 2.5 for beam search
56
and hypothesis ranking. Initially, all features
are 1.
Previous A set of preceding hypothesis pointers
described in Section 2.5. Initially empty.
The leftmost unused word from each system
corresponds to a continuation of the partial hy-
pothesis. Therefore, for each system, we extend a
partial hypothesis by appending that system?s left-
most unused word, yielding several new hypothe-
ses. The appended word, and those aligned with it,
are marked as used in the new hypothesis. Since
systems do not align perfectly, too few words may
be marked as used, a problem addressed in Sec-
tion 2.3. As described in Section 2.4, hypotheses
are scored using four features based on alignment,
system confidence, and a language model. Since
the search space is quite large, we use these partial
scores for a beam search, where the beam contains
hypotheses of equal length. This space contains
hypotheses that extend in precisely the same way,
which we exploit in Section 2.5 to increase diver-
sity. Finally, a hypothesis is complete when the
end of sentence marker is appended.
2.1 Alignment
Sentences from different systems are aligned in
pairs using a modified version of the METEOR
(Banerjee and Lavie, 2005) matcher. This iden-
tifies alignments in three phases: exact matches
up to case, WordNet (Fellbaum, 1998) morphol-
ogy matches, and shared WordNet synsets. These
sources of alignments are quite precise and unable
to pick up on looser matches such as ?mentioned?
and ?said? that legitimately appear in output from
different systems. Artificial alignments are in-
tended to fill gaps by using surrounding align-
ments as clues. If a word is not aligned to any
word in some other sentence, we search left and
right for words that are aligned into that sentence.
If these alignments are sufficiently close to each
other in the other sentence, words between them
are considered for artificial alignment. An arti-
ficial alignment is added if a matching part of
speech is found. The algorithm is described fully
by Jayaraman and Lavie (2005).
2.2 Phrases
Switching between systems is permitted outside
phrases or at phrase boundaries. We find phrases
in two ways. Alignment phrases are maximally
long sequences of words which align, in the same
order and without interruption, to a word se-
quence from at least one other system. Punctua-
tion phrases place punctuation in a phrase with the
preceding word, if any. When the decoder extends
a hypothesis, it considers the longest phrase in
which no word is used. If a punctuation phrase is
partially used, the decoder marks the entire phrase
as used to avoid extraneous punctuation.
2.3 Synchronization
While phrases address near-equal pieces of trans-
lation output, we must also deal with equally
meaningful output that does not align. The im-
mediate effect of this issue is that too few words
are marked as used by the decoder, leading to du-
plication in the combined output. In addition, par-
tially aligned system output results in lingering un-
used words between used words. Often these are
function words that, with language model scoring,
make output unnecessarily verbose. To deal with
this problem, we expire lingering words by mark-
ing them as used. Specifically, we consider the
frontier of each system, which is the leftmost un-
used word. If a frontier lags behind, words as used
to advance the frontier. Our two methods for syn-
chronization differ in how frontiers are compared
across systems and the tightness of the constraint.
Previously, we measured frontiers from the be-
ginning of sentence. Based on this measurement,
the synchronization constraint requires that the
frontiers of each system differ by at most s. Equiv-
alently, a frontier is lagging if it is more than s
words behind the rightmost frontier. Lagging fron-
tiers are advanced until the synchronization con-
straint becomes satisfied. We found this method
can cause problems in the presence of variable
length output. When the variability in output
length exceeds s, proper synchronization requires
distances between frontiers greater than s, which
this constraint disallows.
Alignments indicate where words are syn-
chronous. Words near an alignment are also likely
to be synchronous even without an explicit align-
ment. For example, in the fragments ?even more
serious, you? and ?even worse, you? from WMT
2008, ?serious? and ?worse? do not align but
do share relative position from other alignments,
suggesting these are synchronous. We formalize
this by measuring the relative position of fron-
tiers from alignments on each side. For example,
57
if the frontier itself is aligned then relative posi-
tion is zero. For each pair of systems, we check
if these relative positions differ by at most s un-
der an alignment on either side. Confidence in a
system?s frontier is the sum of the system?s own
confidence plus confidence in systems for which
the pair-wise constraint is satisfied. If confidence
in any frontier falls below 0.8, the least confident
lagging frontier is advanced. The process repeats
until the constraint becomes satisfied.
2.4 Scores
We score partial and complete hypotheses using
system confidence, alignments, and a language
model. Specifically, we have four features which
operate at the word level:
Alignment Confidence in the system from which
the word came plus confidence in systems to
which the word aligns.
Language Model Score from a suffix array lan-
guage model (Zhang and Vogel, 2006)
trained on English from monolingual and
French-English data provided by the contest.
N -Gram
(
1
3
)order?ngram
using language model
order and length of ngram found.
Overlap overlaporder?1 where overlap is the length of
intersection between the preceding and cur-
rent n-grams.
The N -Gram and Overlap features are intended to
improve fluency across phrase boundaries. Fea-
tures are combined using a log-linear model
trained as discussed in Section 3. Hypotheses are
scored using the geometric average score of each
word in the hypothesis.
2.5 Search
Of note is that a word?s score is impacted only by
its alignments and the n-gram found by the lan-
guage model. Therefore two partial hypotheses
that differ only in words preceding the n-gram and
in their average score are in some sense duplicates.
With the same set of used words and same phrase
constraint, they extend in precisely the same way.
In particular, the highest scoring hypothesis will
never use a lower scoring duplicate.
We use duplicate detecting beam search to ex-
plore our hypothesis space. A beam contains par-
tial hypotheses of the same length. Duplicate
hypotheses are detected on insertion and packed,
with the combined hypothesis given the highest
score of those packed. Once a beam contains the
top scoring partial hypotheses of length l, these
hypotheses are extended to length l+1 and placed
in another beam. Those hypotheses reaching end
of sentence are placed in a separate beam, which is
equivalent to packing them into one final hypoth-
esis. Once we remove partial hypothesis that did
not extend to the final hypothesis, the hypotheses
are a lattice connected by parent pointers.
While we submitted only one-best hypotheses,
accurate n-best hypotheses are important for train-
ing as explained in Section 3. Unpacking the hy-
pothesis lattice into n-best hypotheses is guided
by scores stored in each hypothesis. For this task,
we use an n-best beam of paths from the end of
sentence hypothesis to a partial hypothesis. Paths
are built by induction, starting with a zero-length
path from the end of sentence hypothesis to itself.
The top scoring path is removed and its terminal
hypothesis is examined. If it is the beginning of
sentence, the path is output as a complete hypoth-
esis. Otherwise, we extend the path to each parent
hypothesis, adjusting each path score as necessary,
and insert into the beam. This process terminates
with n complete hypotheses or an empty beam.
3 Tuning
Given the 502 sentences made available for tun-
ing by WMT 2009, we selected feature weights for
scoring, a set of systems to combine, confidence in
each selected system, and the type and distance s
of synchronization. Of these, only feature weights
can be trained, for which we used minimum error
rate training with version 1.04 of IBM-style BLEU
(Papineni et al, 2002) in case-insensitive mode.
We treated the remaining parameters as a model
selection problem, using 402 randomly sampled
sentences for training and 100 sentences for eval-
uation. This is clearly a small sample on which
to evaluate, so we performed two folds of cross-
validation to obtain average scores over 200 un-
trained sentences. We chose to do only two folds
due to limited computational time and a desire to
test many models.
We scored systems and our own output using
case-insensitive IBM-style BLEU 1.04 (Papineni
et al, 2002), METEOR 0.6 (Lavie and Agarwal,
2007) with all modules, and TER 5 (Snover et
al., 2006). For each source language, we ex-
58
In Sync s BLEU METE TER Systems and Confidences
cz length 8 .236 .507 59.1 google .46 cu-bojar .27 uedin .27
cz align 5 .226 .499 57.8 google .50 cu-bojar .25 uedin .25
cz align 7 .211 .508 65.9 cu-bojar .60 google .20 uedin .20
cz .231 .504 57.8 google
de length 7 .255 .531 54.2 google .40 uka .30 stuttgart .15 umd .15
de length 6 .260 .532 55.2 google .50 systran .25 umd .25
de align 9 .256 .533 55.5 google .40 uka .30 stuttgart .15 umd .15
de align 6 .200 .514 54.2 google .31 uedin .22 systran .18 umd .16 uka .14
de .244 .523 57.5 google
es align 8 .297 .560 52.7 google .75 uedin .25
es length 5 .289 .548 52.1 google .50 talp-upc .17 uedin .17 rwth .17
es .297 .558 52.7 google
fr align 6 .329 .574 49.9 google .70 lium1 .30
fr align 8 .314 .596 48.6 google .50 lium1 .30 limsi1 .20
fr length 8 .323 .570 48.5 google .50 lium1 .25 limsi1 .25
fr .324 .576 48.7 google
hu length 5 .162 .403 69.2 umd .50 morpho .40 uedin .10
hu length 8 .158 .407 69.5 umd .50 morpho .40 uedin .10
hu align 7 .153 .392 68.0 umd .33 morpho .33 uedin .33
hu .141 .391 66.1 umd
xx length 5 .326 .584 49.6 google-fr .61 google-es .39
xx align 4 .328 .580 49.5 google-fr .80 google-es .20
xx align 5 .324 .576 48.6 google-fr .61 google-es .39
xx align 7 .319 .587 51.1 google-fr .50 google-es .50
xx .324 .576 48.7 google-fr
Table 1: Combination models used for submission to WMT 2009. For each language, we list our pri-
mary combination, contrastive combinations, and a high-scoring system for comparison in italic. All
translations are into English. The xx source language combines translations from different languages,
in our case French and Spanish. Scores from BLEU, METEOR, and TER are the average of two cross-
validation folds with 100 evaluation sentences each. Numbers following system names indicate con-
trastive systems. More evaluation, including human scores, will be published by WMT.
perimented with various sets of high-scoring sys-
tems to combine. We also tried confidence val-
ues proportional to various powers of BLEU and
METEOR scores, as well as hand-picked values.
Finally we tried both variants of synchronization
with values of s ranging from 2 to 9. In total, 405
distinct models were evaluated. For each source
source language, our primary system was chosen
by performing well on all three metrics. Models
that scored well on individual metrics were sub-
mitted as contrastive systems. In Table 1 we report
the models underlying each submitted system.
4 Conclusion
We found our combinations are quite sensitive to
presence of and confidence in the underlying sys-
tems. Further, we show the most improvement
when these systems are close in quality, as is the
case with our Hungarian-English system. The
two methods of synchronization were surprisingly
competitive, a factor we attribute to short sentence
length compared with WMT 2008 Europarl sen-
tences. Opportunities for further work include per-
sentence system confidence, automatic training of
more parameters, and different alignment models.
We look forward to evaluation results from WMT
2009.
Acknowledgments
The authors wish to thank Jonathan Clark for
training the language model and other assistance.
This work was supported in part by the DARPA
GALE program and by a NSF Graduate Research
Fellowship.
59
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Proc.
ACLWorkshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization, pages 65?72.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Shyamsundar Jayaraman and Alon Lavie. 2005.
Multi-engine machine translation guided by explicit
word matching. In Proc. EAMT, pages 143?152.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL-08: HLT, Short Papers (Companion Vol-
ume), pages 81?84.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An automatic metric for MT evaluation with
high levels of correlation with human judgments.
In Proc. Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proc. 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, PA, July.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proc. Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proc. Seventh Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Ying Zhang and Stephan Vogel. 2006. Suffix array
and its applications in empirical natural language
processing. Technical Report CMU-LTI-06-010,
Language Technologies Institute, School of Com-
puter Science, Carnegie Mellon University, Pitts-
burgh, PA, Dec.
60
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 140?144,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
An Improved Statistical Transfer System for French?English
Machine Translation
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark, Alok Parlikar, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema,vamshi,jhclark,aup,alavie}@cs.cmu.edu
Abstract
This paper presents the Carnegie Mellon
University statistical transfer MT system
submitted to the 2009 WMT shared task
in French-to-English translation. We de-
scribe a syntax-based approach that incor-
porates both syntactic and non-syntactic
phrase pairs in addition to a syntactic
grammar. After reporting development
test results, we conduct a preliminary anal-
ysis of the coverage and effectiveness of
the system?s components.
1 Introduction
The statistical transfer machine translation group
at Carnegie Mellon University has been devel-
oping a hybrid approach combining a traditional
rule-based MT system and its linguistically ex-
pressive formalism with more modern techniques
of statistical data processing and search-based de-
coding. The Stat-XFER framework (Lavie, 2008)
provides a general environment for building new
MT systems of this kind. For a given language
pair or data condition, the framework depends on
two main resources extracted from parallel data: a
probabilistic bilingual lexicon, and a grammar of
probabilistic synchronous context-free grammar
rules. Additional monolingual data, in the form of
an n-gram language model in the target language,
is also used. The statistical transfer framework op-
erates in two stages. First, the lexicon and gram-
mar are applied to synchronously parse and trans-
late an input sentence; all reordering is applied
during this stage, driven by the syntactic grammar.
Second, a monotonic decoder runs over the lat-
tice of scored translation pieces produced during
parsing and assembles the highest-scoring overall
translation according to a log-linear feature model.
Since our submission to last year?s Workshop
on Machine Translation shared translation task
(Hanneman et al, 2008), we have made numerous
improvements and extensions to our resource ex-
traction and processing methods, resulting in sig-
nificantly improved translation scores. In Section
2 of this paper, we trace our current methods for
data resource management for the Stat-XFER sub-
mission to the 2009 WMT shared French?English
translation task. Section 3 explains our tuning pro-
cedure, and Section 4 gives our experimental re-
sults on various development sets and offers some
preliminary analysis.
2 System Construction
Because of the additional data resources provided
for the 2009 French?English task, our system this
year is trained on nearly eight times as much
data as last year?s. We used three officially pro-
vided data sets to make up a parallel corpus for
system training: version 4 of the Europarl cor-
pus (1.43 million sentence pairs), the News Com-
mentary corpus (0.06 million sentence pairs), and
the pre-release version of the new Giga-FrEn cor-
pus (8.60 million sentence pairs)1. The combined
corpus of 10.09 million sentence pairs was pre-
processed to remove blank lines, sentences of 80
words or more, and sentence pairs where the ra-
tio between the number of English and French
words was larger than 5 to 1 in either direction.
These steps removed approximately 3% of the cor-
pus. Given the filtered corpus, our data prepara-
tion pipeline proceeded according to the descrip-
tions below.
1Because of data processing time, we were unable to use
the larger verions 1 or 2 of Giga-FrEn released later in the
evaluation period.
140
2.1 Parsing and Word Alignment
We parsed both sides of our parallel corpus with
independent automatic constituency parsers. We
used the Berkeley parser (Petrov and Klein, 2007)
for both English and French, although we obtained
better results for French by tokenizing the data
with our own script as a preprocessing step and
not allowing the parser to change it. There were
approximately 220,000 English sentences that did
not return a parse, which further reduced the size
of our training corpus by 2%.
After parsing, we re-extracted the leaf nodes
of the parse trees and statistically word-aligned
the corpus using a multi-threaded implementa-
tion (Gao and Vogel, 2008) of the GIZA++ pro-
gram (Och and Ney, 2003). Unidirectional align-
ments were symmetrized with the ?grow-diag-
final? heuristic (Koehn et al, 2005).
2.2 Phrase Extraction and Combination
Phrase extraction for last year?s statistical transfer
system used automatically generated parse trees
on both sides of the corpus as absolute constraints:
a syntactic phrase pair was extracted from a given
sentence only when a contiguous sequence of En-
glish words exactly made up a syntactic con-
stituent in the English parse tree and could also
be traced though symmetric word alignments to a
constituent in the French parse tree. While this
?tree-to-tree? extraction method is precise, it suf-
fers from low recall and results in a low-coverage
syntactic phrase table. Our 2009 system uses an
extended ?tree-to-tree-string? extraction process
(Ambati and Lavie, 2008) in which, if no suit-
able equivalent is found in the French parse tree
for an English node, a copy of the English node is
projected into the French tree, where it spans the
French words aligned to the yield of the English
node. This method can result in a 50% increase
in the number of extracted syntactic phrase pairs.
Each extracted phrase pair retains a syntactic cat-
egory label; in our current system, the node label
in the English parse tree is used as the category for
both sides of the bilingual phrase pair, although we
subsequently map the full set of labels used by the
Berkeley parser down to a more general set of 19
syntactic categories.
We also ran ?standard? phrase extraction on the
same corpus using Steps 4 and 5 of the Moses sta-
tistical machine translation training script (Koehn
et al, 2007). The two types of phrases were then
merged in a syntax-prioritized combination that
removes all Moses-extracted phrase pairs that have
source sides already covered by the tree-to-tree-
string syntactic phrase extraction. The syntax pri-
oritization has the advantage of still including a se-
lection of non-syntactic phrases while producing a
much smaller phrase table than a direct combina-
tion of all phrase pairs of both types. Previous ex-
periments we conducted indicated that this comes
with only a minor drop in automatic metric scores.
In our current submission, we modify the proce-
dure slightly by removing singleton phrase pairs
from the syntactic table before the combination
with Moses phrases. The coverage of the com-
bined table is not affected ? our syntactic phrase
extraction algorithm produces a subset of the non-
syntactic phrase pairs extracted from Moses, up to
phrase length constraints ? but the removal al-
lows Moses-extracted versions of some phrases to
survive syntax prioritization. In effect, we are lim-
iting the set of category-labeled syntactic transla-
tions we trust to those that have been seen more
than once in our training data. For a given syn-
tactic phrase pair, we also remove all but the most
frequent syntactic category label for the pair; this
removes a small number of entries from our lexi-
con in order to limit label ambiguity, but does not
affect coverage.
From our training data, we extracted 27.6 mil-
lion unique syntactic phrase pairs after single-
ton removal, reducing this set to 27.0 million en-
tries after filtering for category label ambiguity.
Some 488.7 million unique phrase pairs extracted
from Moses were reduced to 424.0 million after
syntax prioritization. (The remaining 64.7 mil-
lion phrase pairs had source sides already covered
by the 27.0 million syntactically extracted phrase
pairs, so they were thrown out.) This means non-
syntactic phrases outnumber syntactic phrases by
nearly 16 to 1. However, when filtering the phrase
table to a particular development or test set, we
find the syntactic phrases play a larger role, as this
ratio drops to approximately 3 to 1.
Sample phrase pairs from our system are shown
in Figure 1. Each pair includes two rule scores,
which we calculate from the source-side syntac-
tic category (cs), source-side text (ws), target-side
category (ct), and target-side text (wt). In the
case of Moses-extracted phrase pairs, we use the
?dummy? syntactic category PHR. Rule score rt|s
is a maximum likelihood estimate of the distri-
141
cs ct ws wt rt|s rs|t
ADJ ADJ espagnols Spanish 0.8278 0.1141
N N repre?sentants officials 0.0653 0.1919
NP NP repre?sentants de la Commission Commission officials 0.0312 0.0345
PHR PHR haute importance a` very important to 0.0357 0.0008
PHR PHR est charge? de has responsibility for 0.0094 0.0760
Figure 1: Sample lexical entries, including non-syntactic phrases, with rule scores (Equations 1 and 2).
bution of target-language translations and source-
and target-language syntactic categories given the
source string (Equation 1). The rs|t score is simi-
lar, but calculated in the reverse direction to give a
source-given-target probability (Equation 2).
rt|s =
#(wt, ct, ws, cs)
#(ws) + 1
(1)
rs|t =
#(wt, ct, ws, cs)
#(wt) + 1
(2)
Add-one smoothing in the denominators counter-
acts overestimation of the rule scores of lexical en-
tries with very infrequent source or target sides.
2.3 Syntactic Grammar
Syntactic phrase extraction specifies a node-to-
node alignment across parallel parse trees. If these
aligned nodes are used as decomposition points,
a set of synchronous context-free rules that pro-
duced the trees can be collected. This is our pro-
cess of syntactic grammar extraction (Lavie et al,
2008). For our 2009 WMT submission, we ex-
tracted 11.0 million unique grammar rules, 9.1
million of which were singletons, from our paral-
lel parsed corpus. These rules operate on our syn-
tactically extracted phrase pairs, which have cat-
egory labels, but they may also be partially lexi-
calized with explicit source or target word strings.
Each extracted grammar rule is scored according
to Equations 1 and 2, where now the right-hand
sides of the rule are used as ws and wt.
As yet, we have made only minimal use of the
Stat-XFER framework?s grammar capabilities, es-
pecially for large-scale MT systems. For the cur-
rent submission, the syntactic grammar consisted
of 26 manually chosen high-frequency grammar
rules that carry out some reordering between En-
glish and French. Since rules for high-level re-
ordering (near the top of the parse tree) are un-
likely to be useful unless a large amount of parse
structure can first be built, we concentrate our
rules on low-level reorderings taking place within
or around small constituents. Our focus for this
selection is the well-known repositioning of adjec-
tives and adjective phrases when translating from
French to English, such as from le Parlement eu-
rope?en to the European Parliament or from l? in-
tervention forte et substantielle to the strong and
substantial intervention. Our grammar thus con-
sists of 23 rules for building noun phrases, two
rules for building adjective phrases, and one rule
for building verb phrases.
2.4 English Language Model
We built a suffix-array language model (Zhang and
Vogel, 2006) on approximately 700 million words
of monolingual data: the unfiltered English side of
our parallel training corpus, plus the 438 million
words of English monolingual news data provided
for the WMT 2009 shared task. With the relatively
large amount of data available, we made the some-
what unusual decision of building our language
model (and all other data resources for our system)
in mixed case, which adds approximately 12.3%
to our vocabulary size. This saves us the need to
build and run a recaser as a postprocessing step
on our output. Our mixed-case decision may also
be validated by preliminary test set results, which
show that our submission has the smallest drop in
BLEU score (0.0074) between uncased and cased
evaluation of any system in the French?English
translation task.
3 System Tuning
Stat-XFER uses a log-linear combination of seven
features in its scoring of translation fragments:
language model probability, source-given-target
and target-given-source rule probabilities, source-
given-target and target-given-source lexical prob-
abilities, a length score, and a fragmentation score
based on the number of parsed translation frag-
ments that make up the output sentence. We tune
the weights for these features with several rounds
of minimum error rate training, optimizing to-
142
Primary Contrastive
Data Set METEOR BLEU TER METEOR BLEU TER
news-dev2009a-425 0.5437 0.2299 60.45 ? ? ?
news-dev2009a-600 ? ? ? 0.5134 0.2055 63.46
news-dev2009b 0.5263 0.2073 61.96 0.5303 0.2104 61.74
nc-test2007 0.6194 0.3282 51.17 0.6195 0.3226 51.49
Figure 2: Primary and contrastive system results on tuning and development test sets.
wards the BLEU metric. For each tuning itera-
tion, we save the n-best lists output by the sys-
tem from previous iterations and concatenate them
onto the current n-best list in order to present the
optimizer with a larger variety of translation out-
puts and score values.
From the provided ?news-dev2009a? develop-
ment set we create two tuning sets: one using the
first 600 sentences of the data, and a second using
the remaining 425 sentences. We tuned our sys-
tem separately on each set, saving the additional
?news-dev2009b? set as a final development test to
choose our primary and contrastive submissions2.
At run time, our full system takes on average be-
tween four and seven seconds to translate each in-
put sentence, depending on the size of the final
bilingual lexicon.
4 Evaluation and Analysis
Figure 2 shows the results of our primary and con-
trastive systems on four data sets. First, we report
final (tuned) performance on our two tuning sets
? the last 425 sentences of news-dev2009a for the
primary system, and the first 600 sentences of the
same set for the contrastive. We also include our
development test (news-dev2009b) and, for addi-
tional comparison, the ?nc-test2007? news com-
mentary test set from the 2007 WMT shared task.
For each, we give case-insensitive scores on ver-
sion 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-
style BLEU (Papineni et al, 2002), and version 5
of TER (Snover et al, 2006).
From these results, we highlight two interest-
ing areas of analysis. First, the low tuning and
development test set scores bring up questions
about system coverage, given that the news do-
main was not strongly represented in our system?s
2Due to a data processing error, the choice of the primary
submission was based on incorrectly computed scores. In
fact, the contrastive system has better performance on our de-
velopment test set.
training data. We indeed find a significantly larger
proportion of out-of-vocabulary (OOV) words in
news-domain sets: the news-dev2009b set is trans-
lated by our primary submission with 402 of 6263
word types (6.42%) or 601 of 27,821 word tokens
(2.16%) unknown. The same system running on
the 2007 WMT ?test2007? set of Europarl-derived
data records an OOV rate of only 87 of 7514
word types (1.16%) or 105 of 63,741 word tokens
(0.16%).
Second, we turn our attention to the usefulness
of the syntactic grammar. Though small, we find
it to be both beneficial and precise. In the 1026-
sentence news-dev2009b set, for example, we find
351 rule applications ? the vast majority of them
(337) building noun phrases. The three most fre-
quently occurring rules are those for reordering the
sequence [DET N ADJ] to [DET ADJ N] (52 oc-
currences), the sequence [N ADJ] to [ADJ N] (51
occurrences), and the sequence [N1 de N2] to [N2
N1] (45 occurrences). We checked precision by
manually reviewing the 52 rule applications in the
first 150 sentences of news-dev2009b. There, 41
of the occurrences (79%) were judged to be cor-
rect and beneficial to translation output. Of the
remainder, seven were judged incorrect or detri-
mental and four were judged either neutral or of
unclear benefit.
We expect to continue to analyze the output and
effectiveness of our system in the coming months.
In particular, we would like to learn more about
the usefulness of our 26-rule grammar with the
view of using significantly larger grammars in fu-
ture versions of our system.
Acknowledgments
This research was supported in part by NSF grants
IIS-0121631 (AVENUE) and IIS-0534217 (LE-
TRAS), and by the DARPA GALE program. We
thank Yahoo! for the use of theM45 research com-
puting cluster, where we ran the parsing stage of
our data processing.
143
References
Vamshi Ambati and Alon Lavie. 2008. Improving
syntax driven translation models by re-structuring
divergent and non-isomorphic parse tree structures.
In Proceedings of the Eighth Conference of the As-
sociation for Machine Translation in the Americas,
pages 235?244, Waikiki, HI, October.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, OH,
June.
Greg Hanneman, Edmund Huber, Abhaya Agarwal,
Vamshi Ambati, Alok Parlikar, Erik Peterson, and
Alon Lavie. 2008. Statistical transfer systems
for French?English and German?English machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 163?166,
Columbus, OH, June.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proceedings of IWSLT 2005, Pittsburgh, PA, Oc-
tober.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the ACL 2007 Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, MA, August.
Ying Zhang and Stephan Vogel. 2006. Suffix array
and its applications in empirical natural language
processing. Technical Report CMU-LTI-06-010,
Carnegie Mellon University, Pittsburgh, PA, Decem-
ber.
144
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Decoding with Syntactic and Non-Syntactic Phrases in a Syntax-Based
Machine Translation System
Greg Hanneman and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema,alavie}@cs.cmu.edu
Abstract
A key concern in building syntax-based ma-
chine translation systems is how to improve
coverage by incorporating more traditional
phrase-based SMT phrase pairs that do not
correspond to syntactic constituents. At the
same time, it is desirable to include as much
syntactic information in the system as pos-
sible in order to carry out linguistically mo-
tivated reordering, for example. We apply
an extended and modified version of the ap-
proach of Tinsley et al (2007), extracting
syntax-based phrase pairs from a large parallel
parsed corpus, combining them with PBSMT
phrases, and performing joint decoding in a
syntax-based MT framework without loss of
translation quality. This effectively addresses
the low coverage of purely syntactic MT with-
out discarding syntactic information. Further,
we show the potential for improved transla-
tion results with the inclusion of a syntactic
grammar. We also introduce a new syntax-
prioritized technique for combining syntactic
and non-syntactic phrases that reduces overall
phrase table size and decoding time by 61%,
with only a minimal drop in automatic trans-
lation metric scores.
1 Introduction
The dominance of traditional phrase-based statisti-
cal machine translation (PBSMT) models (Koehn et
al., 2003) has recently been challenged by the de-
velopment and improvement of a number of new
models that explicity take into account the syntax
of the sentences being translated. One simple ap-
proach is to limit the phrases learned by a standard
PBSMT translation model to only those contiguous
sequences of words that additionally correspond to
constituents in a syntactic parse tree. However, a to-
tal reliance on such syntax-based phrases has been
shown to be detrimental to translation quality, as the
space of phrase segmentation of a parallel sentence
is heavily constrained by both the source-side and
target-side tree structures. Noting that the number
of phrase pairs extracted from a corpus is reduced by
around 80% when they are required to correspond to
syntactic constituents, Koehn et al (2003) observed
that many non-constituent phrase pairs that would
not be included in a syntax-only model are in fact
extremely important to system performance. Since
then, researchers have explored effective ways for
combining phrase pairs derived from syntax-aware
methods with those extracted from more traditional
PBSMT. Briefly stated, the goal is to retain the high
level of coverage provided by non-syntactic PBSMT
phrases while simultaneously incorporating and ex-
ploiting specific syntactic knowledge.
Zollmann and Venugopal (2006) overcome the re-
strictiveness of the syntax-only model by starting
with a complete set of phrases as produced by tra-
ditional PBSMT heuristics, then annotating the tar-
get side of each phrasal entry with the label of the
constituent node in the target-side parse tree that
subsumes the span. They then introduce new con-
stituent labels to handle the cases where the phrasal
entries do not exactly correspond to the syntactic
constituents. Liu et al (2006) also add non-syntactic
PBSMT phrases into their tree-to-string translation
system. Working from the other direction, Marton
and Resnik (2008) extend a hierarchical PBSMT
1
system with a number of features to prefer or dis-
prefer certain types of syntactic phrases in different
contexts. Restructuring the parse trees to ease their
restrictiveness is another recent approach: in partic-
ular, Wang et al (2007) binarize source-side parse
trees in order to provide phrase pair coverage for
phrases that are partially syntactic.
Tinsley et al (2007) showed an improvement over
a PBSMT baseline on four tasks in bidirectional
German?English and Spanish?English translation
by incorporating syntactic phrases derived from par-
allel trees into the PBSMT translation model. They
first word align and extract phrases from a parallel
corpus using the open-source Moses PBSMT toolkit
(Koehn et al, 2007), which provides a baseline SMT
system. Then, both sides of the parallel corpus are
parsed with independent automatic parsers, subtrees
from the resulting parallel treebank are aligned, and
an additional set of phrases (with each phrase corre-
sponding to a syntactic constituent in the parse tree)
is extracted. The authors report statistically signif-
icant improvements in translation quality, as mea-
sured by a variety of automatic metrics, when the
two types of phrases are combined in the Moses de-
coder.
Our approach in this paper is structurally similar
to that of Tinsley et al (2007), but we extend or
modify it in a number of key ways. First, we ex-
tract both non-syntactic PBSMT and syntax-driven
phrases from a parallel corpus that is two orders of
magnitude larger, making our system competitive
in size to state-of-the-art SMT systems elsewhere.
Second, we apply a different algorithm for subtree
alignment, proposed by Lavie et al (2008), which
proceeds bottom-up from existing statistical word
alignments, rather than inducing them top-down
from lexical alignment probabilities. Third, in addi-
tion to straightforwardly combining syntax-derived
phrases with traditional PBSMT phrases, we demon-
strate a new combination technique that removes
PBSMT phrases whose source-language strings are
already covered by a syntax-derived phrase. This
new syntax-prioritized technique results in a 61%
reduction in the size of the combined phrase table
with only a minimal decrease in automatic transla-
tion metric scores. Finally, and crucially, we carry
out the joint decoding over both syntactic and non-
syntactic phrase pairs in a syntax-aware MT sys-
tem, which allows a syntactic grammar to be put in
place on top of the phrase pairs to carry out linguis-
tically motivated reordering, hierarchical decoding,
and other operations.
After this introduction, we first describe the base
MT system we used, its formalism for specify-
ing translation rules, and the method for extract-
ing syntax-derived phrase pairs from a parallel cor-
pus (Section 2). Section 3 gives the two methods
for combining PBSMT phrases with our syntactic
phrases, and introduces our first steps with includ-
ing a grammar in the syntax-based translation frame-
work. The results of our experiments are described
in Section 4 and are further discussed in Section 5.
Finally, Section 6 offers some conclusions and di-
rections for future work.
2 Base Translation System
The base MT system used for our experiments is the
statistical transfer (?Stat-XFER?) framework (Lavie,
2008). The core of the framework is a transfer en-
gine using two language-pair-dependent resources:
a grammar of weighted synchronous context-free
rules, and a probabilistic bilingual lexicon. Once
the resources have been provided, the Stat-XFER
framework carries out translation in a two-stage pro-
cess, first applying the lexicon and grammar to syn-
chronously parse an input sentence, then running
a monotonic decoder over the resulting lattice of
scored translation pieces assembled during parsing
to produce a final string output. Reordering is ap-
plied only in the first stage, driven by the syntactic
grammar; the second-stage monotonic decoder only
assembles translation fragments into complete hy-
potheses.
2.1 Lexicon and Grammar Formalism
Each Stat-XFER bilingual lexicon entry has a syn-
chronous context-free grammar (SCFG) expression
of the source- and target-language production rules,
shown in abbreviated format below, where cs and ct
represent source- and target-side syntactic category
labels and ws and wt represent source- and target-
side word or phrase strings.
cs :: ct ? [ws] :: [wt]
2
Each entry in the lexicon is assigned a pair of rule
scores (rt|s and rs|t) based on cs, ws, ct, and wt1.
The rt|s score is a maximum-likelihood estimate
of the distribution of target-language translations
and source- and target-language syntactic categories
given the source string (Equation 1); this is similar
to the usual ?target-given-source? phrasal probabil-
ity in standard SMT systems. The rs|t score is sim-
ilar, but calculated in the reverse direction to give a
source-given-target probability (Equation 2).
rt|s = #(wt, ct, ws, cs)#(ws) + 1 (1)
rs|t = #(wt, ct, ws, cs)#(wt) + 1 (2)
The add-one smoothing in the denominators coun-
teracts overestimation of the rule scores of lexical
entries with very infrequent source or target sides.
Stat-XFER grammar rules have a similar form,
shown below via an example.
NP :: NP ? [DET1 N2 de N3] :: [DET1 N3 N2]
The SCFG backbone may include lexicalized items,
as above, as well as non-terminals and pre-terminals
from the grammar. Constituent alignment infor-
mation, shown here as co-indexes on the non-
terminals, specifies one-to-one correspondences be-
tween source-language and target-language con-
stituents on the right-hand side of the SCFG rule.
Rule scores rt|s and rs|t for grammar rules, if they
are learned from data, are calculated in the same way
as the scores for lexical entries.
2.2 Syntax-Based Phrase Extraction
In this section, we briefly summarize the automatic
resource extraction approach described by Lavie et
al. (2008) and recently extended by Ambati and
Lavie (2008), which we use here, specifically as ap-
plied to the extraction of syntax-based phrase pairs
for the bilingual lexicon.
The grammar and lexicon are extracted from a
large parallel corpus that has been statistically word-
aligned and independently parsed on both sides with
1If no syntactic category information is available, cs and ct
can be set to dummy values, but the rule score equations remain
unchanged.
automatic parsers. Word-level entries for the bilin-
gual lexicon are directly taken from the word align-
ments; corresponding syntactic categories for the
left-hand side of the SCFG rules are obtained from
the preterminal nodes of the parse trees. Phrase-
level entries for the lexicon are based on node-to-
node alignments in the parallel parse trees. In the
straightforward ?tree-to-tree? scenario, a given node
ns in one parse tree S will be aligned to a node nt
in the other parse tree T if the words in the yield of
ns are all either aligned to words within the yield of
nt or have no alignment at all. If there are multiple
nodes nt satisfying this constraint, the node in the
tree closest to the leaves is selected. Each aligned
node pair (ns, nt) produces a phrase-level entry in
the lexicon, where the left-hand sides of the SCFG
rule are the labels of ns and nt, and the right-hand
sides are the yields of those two nodes in their re-
spective trees. In the expanded ?tree-to-tree-string?
configuration, if no suitable node nt exists, a new
node n?s is introduced into T as a projection of ns,
spanning the yield of the words in T aligned to the
yield of ns. At the end of the extraction process in
either case, the entry counts are collected and scored
in the manner described in Section 2.1.
3 Combination with PBSMT Phrases
Conceptually, we take the opposite approach to that
of Tinsley et al (2007) by adding traditional PBSMT
phrases into a syntax-based MT system rather than
the other way around. We begin by running steps
3 through 5 of the Moses training script (Koehn et
al., 2007)2, which results in a list of phrase pair in-
stances for the same word-aligned corpus to which
we applied the syntax-based extraction methods in
Section 2.2. Given the two sets of phrases, we ex-
plore two methods of combining them.
? Direct Combination. Following the method of
Tinsley et al (2007), we directly combine the
counts of observed syntax-based phrase pairs
with the counts of observed PBSMT phrase
pairs. This results in a modified probability
model in which a higher likelihood is moved
onto syntactic phrase pairs that were also ex-
tractable using traditional PBSMT heuristics. It
2See also www.statmt.org/moses.
3
Decoder Phrase Type # Phrases METEOR BLEU TER
Stat-XFER Syntactic only, PHR 917,266 0.5654 0.2734 56.49
Stat-XFER Syntactic only, frag 1,081,233 0.5653 0.2741 56.54
Stat-XFER Syntactic only, gra 1,081,233 0.5665 0.2772 56.26
Stat-XFER PBSMT only 8,069,480 0.5835 0.3018 54.26
Stat-XFER Direct combination, PHR 8,071,773 0.5835 0.3009 54.21
Stat-XFER Direct combination, frag 9,150,713 0.5841 0.3026 54.52
Stat-XFER Direct combination, gra 9,150,713 0.5855 0.3034 54.28
Stat-XFER Syntax-prioritized, PHR 2,888,154 0.5800 0.2961 54.79
Stat-XFER Syntax-prioritized, frag 3,052,121 0.5802 0.2979 54.78
Stat-XFER Syntax-prioritized, gra 3,052,121 0.5813 0.2991 54.73
Moses PBSMT only, mono 8,145,083 0.5911 0.3139 53.77
Moses PBSMT only, lex RO 8,145,083 0.5940 0.3190 53.48
Figure 1: Results on the test set for all phrase table configurations. For BLEU, bold type indicates the best Stat-XFER
baseline and the configurations statistically equivalent to it (paired bootstrap resampling with n = 1000, p = 0.05).
also allows either extraction mechanism to in-
troduce new entries into the combined phrase
table that were not extracted by the other, thus
permitting the system to take full advantage of
complementary information provided by PB-
SMT phrases that do not correspond to syntac-
tic constituents.
? Syntax-Prioritized Combination. Under this
method, we take advantage of the fact that
syntax-based phrase pairs are likely to be
more precise translational equivalences than
traditional PBSMT phrase pairs, since con-
stituent boundaries are taken into account dur-
ing phrase extraction. PBSMT phrases whose
source-side strings are already covered by an
entry from the syntactic phrase table are re-
moved; the remaining PBSMT phrases are
combined as in the direct combination method
above. The effect on the overall system is
to trust the syntactic phrase pairs in the cases
where they exist, supplementing with PBSMT
phrase pairs for non-constituents.
For each type of phrase-pair combination, we test
three variants when jointly decoding syntax-based
phrases, which come with syntactic information,
along with PBSMT phrases, which do not. In the
first configuration (?PHR?), all extracted phrase la-
bels for syntactic phrases are mapped to a generic
?PHR? tag to simulate standard SMT monotonic de-
coding; this matches the treatment given throughout
to our extracted non-syntactic phrases. In the sec-
ond variant (?frag?), the phrase labels in the large
nonterminal sets used by our source- and target-side
parsers are mapped down to a smaller set of 19 la-
bels that we use for both sides. The same translation
phrase pair may occur with multiple category labels
in this case if it was extracted with different syn-
tactic categories from different trees in the corpus.
In a third variant (?gra?), a small manually devel-
oped grammar is additionally inserted into the sys-
tem. The Stat-XFER system behaves the same way
in each variant. All phrase pairs are applied jointly
to the input sentence during the parsing stage, get-
ting added to the translation according to their syn-
tactic category and scores, although phrases tagged
as PHR cannot participate in any grammar rules.
The second-stage decoder then receives the joint lat-
tice and assembles complete output hypotheses re-
gardless of syntactic category labels.
4 Experiments
We extracted the lexical resources for our MT sys-
tem from version 3 of the French?English Europarl
parallel corpus (Koehn, 2005), using the officially
released training set from the 2008 Workshop in
Statistical Machine Translation (WMT)3. This gives
us a corpus of approximately 1.2 million sentence
3www.statmt.org/wmt08/shared-task.html
4
Phrase Table # Entries # Source Sides Amb. Factor
Total syntax-prioritized table 3,052,121 113,988 26.8
Syntactic component 1,081,233 39,105 27.7
PBSMT component 1,970,888 74,883 26.3
Total baseline PBSMT table 8,069,480 113,972 70.8
Overlap with syntax-prioritized 6,098,592 39,089 156.0
Figure 2: Statistical characteristics of the syntax-prioritized phrase table (top) compared with the baseline PBSMT
phrase table (bottom). The ambiguity factor is the ratio of the number of unique entries to the number of unique
source sides, or the average number of target-language alternatives per source phrase.
pairs. Statistical word alignments are learned in both
directions with GIZA++ (Och and Ney, 2003), then
combined with the ?grow-diag-final? heuristic. For
the extraction of syntax-based phrase pairs, we ob-
tain English-side constituency parses using the Stan-
ford parser (Klein and Manning, 2003), and French-
side constituency parses using the Xerox XIP parser
(A??t-Mokhtar et al, 2001). In phrase extraction,
we concentrate on the expanded tree-to-tree-string
scenario described in Section 2.2, as it results in
a nearly 50% increase in the number of extracted
phrase pairs over the tree-to-tree method. For de-
coding, we construct a suffix-array language model
(Zhang and Vogel, 2006) from a corpus of 430 mil-
lion words, including the English side of our train-
ing data, the English side of the Hansard corpus, and
newswire data. The ?gra? variant uses a nine-rule
grammar that is meant to address the most common
low-level reorderings between French and English,
focusing mainly on the reordering between nouns or
noun phrases and adjectives or adjective phrases.
Our test set is the 2000-sentence ?test2007? data
set, also released as part of the WMT workshop
series. We report case-insensitive scores on ver-
sion 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-style
BLEU (Papineni et al, 2002), and version 5 of TER
(Snover et al, 2006).
Figure 1 gives an overall summary of our results
on the test2007 data. Overall, we train and test 10
different configurations of phrase pairs in the Stat-
XFER decoder. We begin by testing each type of
phrase separately, producing one set of baseline sys-
tems with only phrase pairs that correspond to syn-
tactic constituents (?Syntactic only?) and one base-
line system with only phrase pairs that were ex-
tracted from Moses (?PBSMT only?). We then test
our two combination techniques, and their variants,
as described in Section 3. Statistical significance
is tested on the BLEU metric using paired boot-
strap resampling (Koehn, 2004) with n = 1000 and
p = 0.05. In the figure, the best baseline system and
the configurations statistically equivalent to it are in-
dicated in bold type. In addition to automatic met-
ric scores, we also list the number of unique phrase
pairs extracted for each configuration. (Because of
the large number of phrase pairs, we pre-filter them
to only the set whose source sides appear in the test
data; these numbers are the ones reported.)
As an additional point of comparison, we build
and tune a Moses MT system on the same data
as our Stat-XFER experiments. The Moses system
with a 4-gram language model and a distance-6 lex-
ical reordering model (?lex RO?) scores similarly to
state-of-the-art systems of this type on the test2007
French?English data (Callison-Burch et al, 2007).
Without the reordering model (?mono?), the Moses
system is as comparable as possible in design and
resources to the Stat-XFER PBSMT-only configu-
ration. We do not propose in this paper a head-
to-head performance comparison between the Stat-
XFER and Moses decoders; rather, we report results
on both to gain a better understanding of the im-
pact of the non-syntactic lexical reordering model
in Moses compared with the impact of the syntactic
grammar in Stat-XFER.
5 Discussion
5.1 Phrasal Coverage and Precision
One observation apparent in Figure 1 is that we have
again confirmed that a total restriction to syntax-
5
Source: Il faut que l? opinion publique soit informe?e pleinement sur les caracte?ristiques du
test dont je parle .
Reference: Public opinion must be fully informed of the characteristics of the test I am talking
about .
Syntax only: It | is | that | the public | be informed | fully | on | the characteristics | of the test | I
am talking about | .
PBSMT only: We must | that public opinion gets noticed | fully | on the characteristics of the |
test | above .
Direct comb.: We must | that public opinion gets noticed | fully on | the characteristics of the |
test | above .
Syntax-prioritized: It is important that | the public | be informed | fully on | the characteristics | of the
test | I am talking about | .
Figure 3: A translation example from the test set showing the output?s division into phrases. In the syntax-prioritized
translation, English phrases that derived from syntax-based phrasal entries are shown in italics.
based phrases is detrimental to output quality. A
likely reason for this, as Tinsley et al (2007) sug-
gested, is that the improved precision and infor-
mativeness of the syntactic phrases is not enough
to overcome their relative scarcity when compared
to non-syntactic PBSMT phrases. (The syntactic
phrase table is only 11 to 13% of the size of the PB-
SMT phrase table.) It is important to note that this
scarcity occurs at the phrasal level: though there are
294 unknown word types in our test set when trans-
lating with only syntactic phrase pairs, this num-
ber only drops to 277 with the inclusion of PBSMT
phrases. The largest phrase table configuration, di-
rect combination, yields statistically equivalent per-
formance to the baseline system created using stan-
dard PBSMT extraction heuristics. Its key benefit
is that the inclusion of syntactic information in the
phrase pairs, where possible, leaves open the door to
further improvement in scores with the addition of a
larger syntactic grammar. We have thus addressed
the syntax-only phrase coverage problem without
giving up syntactic information.
An interesting conclusion is revealed in the anal-
ysis of the sizes and relative overlaps of the phrase
tables in each of our translation conditions. In
the absence of significant grammar, the equiva-
lence of scores between the PBSMT-only and direct-
combination scenarios is understandable given the
minimal change in the size of the phrase table. Out
of nearly 8.1 million entries, only 2293 entirely new
entries are provided by adding the syntactic phrase
table; further, these phrases are relatively rare long
phrases that do not have much effect on the trans-
lation of the overall test set. On the other hand, the
syntax-prioritized phrase table is extremely different
in nature ? and only 37.8% of the size of the base-
line PBSMT phrase table ? yet still attains nearly
the same automatic metric scores. There, we can
clearly see the effect of the syntactic phrases, since
the 3,052,121 phrases used in the fragmented vari-
ant of that scenario are more noticibly split between
1,970,888 PBSMT phrases (64.6%) and 1,081,233
syntax-based phrases (35.4%).
Some statistics for the makeup of the syntax-
prioritized phrase table, compared to the baseline
PBSMT phrase table, are shown in Figure 2. For
each, we calculate the ?ambiguity factor,? or the
average number of target-language alternatives for
each source-language phrase in the table. This anal-
ysis shows not only that the distribution of tradi-
tional PBSMT phrases is rather different from that
of the syntactic phrases, it is also different from the
non-syntactic PBSMT phrases that are preserved in
the syntax-prioritized table. In effect, given a base-
line PBSMT phrase table, the syntax prioritization
replaces phrase entries for 39,089 source-language
phrases, each with an average of 156 different target-
language translations, with 39,105 source phrases,
each with an average of 27.7 syntactically motivated
target translations ? a net savings of 5.0 million
6
Source: Je veux saluer , a` mon tour , l? intervention forte et substantielle du pre?sident Prodi .
Reference: I too would like to welcome Mr Prodi ?s forceful and meaningful intervention .
PHR
I welcome
S
, in turn ,
NP
the strong and substantial speech
ADJP
strong and substantial
ADJ
substantial
CON
and
ADJ
strong
N
speech
DET
the
PP
of President Prodi
PU
.
Figure 4: A translation example from the test set showing the result of including the nine-rule grammar in the syntax-
prioritized combination. The SMT-only translation of the noun phrase is the decisive intervention and substantial.
phrase pairs. This is a strong indication that, be-
cause of the more accurate phrase boundary detec-
tion, the syntactic phrases are a much more precise
representation of translational equivalence. An ad-
ditional benefit is a significant reduction in decoding
time, from an average of 27.3 seconds per sentence
with the baseline PBSMT phrase table to 10.7 sec-
onds per sentence with the syntax-prioritized table
with the grammar included.
Improved precision due to the inclusion of syn-
tactic phrases can be seen by examining a translation
example and the phrasal chunks that produce it (Fig-
ure 3). In the syntax-prioritized output, the English
phrases deriving from syntax-based phrase pairs are
shown in italic, while the phrases deriving from PB-
SMT pairs are in normal type. The example shows
an effective combination of on-target translations for
syntactic constituents, when they are available, with
non-syntactic phrases to handle constituent bound-
aries or places where parallel constituents are dif-
ficult to extract. The translation pieces be informed
and I am talking about, though they exist in the base-
line PBSMT phrase table, do not make it into the
top-best translation in the PBSMT-only scenario be-
cause of its high ambiguity factor.
5.2 Effect of Syntactic Information
Although our current experiments do not show a sig-
nificant increase in automatic metric scores with the
addition of a small grammar, we can see the po-
tential power of grammar in examining further sen-
tences from the output. For example, in Figure 4,
standard PBSMT phrase extraction is able to pick up
the adjective?noun reordering when translating from
intervention forte to decisive intervention. However,
in this sentence we have an adjective phrase follow-
ing the noun, and there is no pre-extracted phrase
pair for the entire constituent, so our system built
from only PBSMT phrases produces the incorrect
noun phrase translation the decisive intervention and
substantial. Our nine-rule grammar, specifically tar-
geted for this scenario, is able to correct the structure
of the sentence by applying two rules to produce the
strong and substantial speech.
Analysis of the entire test set further suggests that
even our small grammar produces correct and pre-
cise output across all phrase table configurations, al-
though the total number of applications of the nine
rules remains low. There are 590 rule applications
in the one-best output on the test set in the syntax-
only configuration, 472 applications in the syntax-
prioritized configuration, and 216 applications in the
direct combination. In each configuration, we man-
ually inspected all rule applications in the first 200
sentences and classified them as correctly reordering
words in the English output (?good?), incorrectly re-
ordering (?bad?), or ?null.? This last category de-
notes applications of monotonic structure-building
rules that did not feed into a higher-level reordering
rule. The results of this analysis are shown in Fig-
ure 5. Overall, we find that the grammar is 97% ac-
curate in its applications, making helpful reordering
changes 88% of the time.
Given the preceding analysis ? and the fact that
our inclusion of a lexicalized reordering model in
7
Phrase Table Good Bad Null
Syntactic only 47 3 8
Syntax-prioritized 45 1 3
Direct combination 25 0 0
Figure 5: Manual analysis of grammar rule applications
in the first 200 sentences of the test set.
Moses resulted in automatic metric gains of only
0.0051 BLEU, 0.0029 METEOR, and 0.29 TER ?
we believe that further experiments with a much
larger syntactic grammar will lead to a more signif-
icant improvement in automatic metric scores and
translation quality.
6 Conclusions and Future Work
We have extended and applied an algorithm for com-
bining syntax-based phrases from a parallel parsed
corpus with non-syntactic phrases from phrase-
based SMT within the context of a statistical syntax-
based translation framework. Using a much larger
corpus than has previously been employed for this
approach, we produce jointly decoded output sta-
tistically equivalent to a monotonic decoding using
standard PBSMT phrase-extraction heuristics, re-
taining syntactic information and setting the stage
for further improvements by incorporating a syntac-
tic grammar into the translation framework. Our
preliminary nine-rule grammar, targeted for two spe-
cific English?French linguistic phenomena, already
shows promise in performing linguistically moti-
vated reordering that cannot be captured formally by
a standard PBSMT model.
We present a syntax-prioritized method of com-
bining phrase types into a single phrase table by al-
ways selecting a syntax-based phrase pair when one
is available for a given source string. This new com-
bination style reduces the size of the resulting phrase
table and total decoding time by 61%, with only
a minor degradation in MT performance. We sug-
gest that this is because the syntax-derived phrases,
when they can be extracted, are a much more precise
method of describing correct translational equiva-
lences.
As yet, we have made only minimal use of the
Stat-XFER framework?s grammar capabilities. In
our experiments, the full tree-to-tree-string rule-
extraction process of Ambati and Lavie (2008) pro-
duces more than 2 million unique SCFG rules when
applied to a corpus the size of the Europarl. Not only
is translating with such a large set computationally
intractable, but empirically nearly 90% of the rules
were observed only once in the parallel parsed cor-
pus, making it difficult to separate rare but correct
rules from those due to noise in the parses and word
alignments. With the view of moving beyond our
manually written nine-rule grammar, but wanting to
get only the most useful rules from the entire auto-
matically extracted set, we are currently investigat-
ing methods for automatic scoring or selection of a
reasonable number of grammar rules for a particular
language pair. Given that the majority of our phrase
pairs, even in the syntax-prioritized combination, are
non-syntactic, we have also conducted preliminary
experiments with ?syntactifying? them so that they
may also be used by grammar rules to produce larger
translation fragments.
The experiments in this paper used the grow-diag-
final heuristic for word alignment combination be-
cause it has been shown to provide the highest preci-
sion on the subtree node alignment method by which
we extract syntax-based phrase pairs (Lavie et al,
2008). However, this is a trade-off that sacrifices
some amount of recall. Experimenting with differ-
ent symmetric alignment heuristics may lead to a
more optimal configuration for phrase-pair extrac-
tion or combination with PBSMT phrases. We also
suspect that the choice of source- and target-side
parsers plays a significant role in the number and
nature of phrase pairs we extract; to address this,
we are in the process of re-trying our line of exper-
iments using the Berkeley parser (Petrov and Klein,
2007) for English, French, or both.
Acknowledgments
This research was supported in part by NSF grant
IIS-0534217 (LETRAS) and the DARPA GALE
program. We thank the members of the Parsing and
Semantics group at Xerox Research Center Europe
for parsing the French data with their XIP parser.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
8
Proceedings of the Seventh International Workshop on
Parsing Technologies, Beijing, China, October.
Vamshi Ambati and Alon Lavie. 2008. Improving syntax
driven translation models by re-structuring divergent
and non-isomorphic parse tree structures. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 235?244,
Waikiki, HI, October.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10. MIT Press, Cambridge,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 48?54, Edmonton,
Alberta, May?June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the 10th
Machine Translation Summit, pages 79?86, Phuket,
Thailand, September.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 228?231, Prague, Czech Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 609?616, Sydney, Aus-
tralia, July.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrase-based translation.
In Proceedings of ACL-08: HLT, pages 1003?1011,
Columbus, OH, June.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
John Tinsley, Mary Hearne, and Andy Way. 2007. Ex-
ploiting parallel treebanks to improve phrase-based
statistical machine translation. In Proceedings of the
Sixth International Workshop on Treebanks and Lin-
guistic Theories, pages 175?187, Bergen, Norway, De-
cember.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 746?754, Prague, Czech Re-
public, June.
Ying Zhang and Stephan Vogel. 2006. Suffix array and
its applications in empirical natural language process-
ing. Technical Report CMU-LTI-06-010, Carnegie
Mellon University, Pittsburgh, PA, December.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY, June.
9
A Multi-Perspective Evaluation of the NESPOLE!
Speech-to-Speech Translation System
Alon Lavie
Carnegie Mellon University, Pittsburgh, PA, USA
alavie@cs.cmu.edu
Florian Metze
University of Karlsruhe, Germany
Roldano Cattoni
ITC-irst, Trento, Italy
Erica Costantini
University of Trieste, Trieste, Italy
Abstract
Performance and usability of real-
world speech-to-speech translation sys-
tems, like the one developed within
the Nespole! project, are aected by
several aspects that go beyond the
pure translation quality provided by
the underlying components of the sys-
tem. In this paper we describe these
aspects as perspectives along which
we have evaluated the Nespole! sys-
tem. Four main issues are investigated:
(1) assessing system performance un-
der various network trac conditions;
(2) a study on the usage and utility of
multi-modality in the context of multi-
lingual communication; (3) a compar-
ison of the features of the individual
speech recognition engines, and (4) an
end-to-end evaluation of the system.
1 Introduction
Nespole!1 is a speech-to-speech machine tran-
slation project designed to provide fully func-
tional speech-to-speech capabilities within real-
world settings of common users involved in e-
commerce applications. The project is a collab-
oration between three European research groups
1Nespole! { NEgotiation through SPOken Lan-
guage in E-commerce. See the project web-site at
http://nespole.itc.it for further details.
(IRST in Trento, Italy; ISL at Universita?t Karl-
sruhe (TH); and CLIPS at Universite Joseph
Fourier in Grenoble, France), one US research
group (ISL at Carnegie Mellon University in
Pittsburgh, PA) and two industrial partners
(APT; Trento, Italy { the Trentino provincial
tourism board, and AETHRA; Ancona, Italy {
a tele-communications company). The project is
funded jointly by the European Commission and
the US NSF. Over the past two years, we have
developed a fully functional showcase of the Ne-
spole! system within the domain of travel and
tourism, and have signicantly improved system
performance and usability based on a series of
studies and evaluations with real users. Our ex-
perience has shown that improving translation
quality is only one of several important issues
that must be addressed in achieving a practical
real-world speech-to-speech translation system.
This paper describes how we tackled these is-
sues and evaluates their eect on system per-
formance and usability. We focus on four main
issues: (1) assessing system performance under
various network trac conditions and architec-
tural congurations; (2) a study on the usage
and utility of multi-modality in the context of
multi-lingual communication; (3) a comparison
of the features of the individual speech recogni-
tion engines, and (4) an end-to-end evaluation
of the demonstration system.
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 121-128.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
2 The NESPOLE! System
The Nespole! system (Lazzari, 2001) uses a
client-server architecture to allow a common
user, who is initially browsing through the web
pages of a service provider on the Internet, to
connect seamlessly to a human agent of the ser-
vice provider who speaks another language, and
provides speech-to-speech translation service be-
tween the two parties. Standard commercially
available PC video-conferencing technology such
as Microsoft?s NetMeeting r? is used to connect
between the two parties in real-time.
In the rst showcase which we describe in this
paper, the scenario is the following: a client
is browsing through the web-pages of APT {
the tourism bureau of the province of Trentino
in Italy { in search of tour-packages in the
Trentino region. If more detailed information
is desired, the client can click on a dedicated
\button" within the web-page in order to estab-
lish a video-conferencing connection to a human
agent located at APT. The client is then pre-
sented with an interface consisting primarily of
a standard video-conferencing application win-
dow and a shared whiteboard application. Us-
ing this interface, the client can carry on a con-
versation with the agent, where the Nespole!
server provides two-way speech-to-speech trans-
lation between the parties. In the current setup,
the agent speaks Italian, while the client can
speak English, French or German.
2.1 System Architecture
The Nespole! system architecture is shown
in Figure 1. A key component in the Ne-
spole! system is the \Mediator" module,
which is responsible for mediating the commu-
nication channel between the two parties as
well as interfacing with the appropriate Human
Language Technology (HLT) speech-translation
servers. The HLT servers provide the actual
speech recognition and translation capabilities.
This system design allows for a very flexible and
distributed architecture: Mediators and HLT-
servers can be run in various physical locations,
so that the optimal conguration, given the lo-
cations of the client and the agent and antic-
Figure 1: The Nespole! System Architecture
ipated network trac, can be taken into ac-
count at any time. A well-dened API allows
the HLT servers to communicate with each other
and with the Mediator, while the HLT modules
within the servers for the dierent languages are
implemented using very dierent software pack-
ages. Further details of the design principles of
the system are described in (Lavie et al, 2001).
The computationally intensive part of speech
recognition and translation is done on dedicated
server machines, whose nature and location is
of no concern to the user. A wide range of
client-machines, even portable devices or pub-
lic information kiosks, are therefore able to run
the client software, so that the service can be
made available nearly everywhere.
The system architecture shown in Figure 1
contains two dierent types of Internet connec-
tions with dierent characteristics. The connec-
tion between Client/Agent PCs and the Media-
tor is a standard video-conferencing connection
that uses H323 and UDP protocols. In cases
of insucient network bandwidth, these proto-
cols compromise performance by allowing de-
layed or lost packets of data to be \dropped" on
the receiving side, in order to minimize delays
and ensure close to real-time performance. The
connection between the Mediator and the HLT
servers uses TCP over IP in order to achieve loss-
less communication between the Mediator and
the translation components. For practical rea-
sons, Mediator and HLT servers in our current
system usually run in separate and distant loca-
tions, which can introduce some additional time
delay. System response times in recent demon-
strations have been about three times real-time.
2.2 User interface
The user interface display is designed for
Windows r?and consists of four windows: (1)
a Microsoft r?Internet Explorer web browser;
(2) a Microsoft r?Windows NetMeeting video-
conferencing application; (3) the AeWhite-
board; and (4) the Nespole Monitor. Using
Internet Explorer, the client initiates the au-
dio and video call with an agent of the service
provider, by a simple click of a button on the
browser page. Microsoft Windows Netmeeting is
automatically opened and the audio and video
connection is established. The two additional
displays { the AeWhiteboard and the Nespole
Monitor are also launched at the same time.
Client and agent can then proceed in carrying
out a dialogue with the help of the speech trans-
lation system. For a screen snapshot of these
four displays, see (Metze et al, 2002).
We found it important to visually present as-
pects of the speech-translation process to the
end users. This is accomplished via the Ne-
spole Monitor display. Three textual represen-
tations are displayed in clearly identied elds:
(1) a transcript of their spoken input (the output
from the speech recognizer); (2) a paraphrase of
their input { the result of translating the recog-
nized input back into their own language; and
(3) the translated textual output of the utter-
ance spoken by the other party. These textual
representations provide the users with the capa-
bility to identify mis-translations and indicate
errors to the other party. A bad paraphrase is
often a good indicator of a signicant error in
the translation process. When a mis-translation
is detected, the user can press a dedicated but-
ton that informs the other party to ignore the
translation being displayed, by highlighting the
textual translation in red on the monitor display
of the other party. The user can then repeat the
turn. The current system also allows the partic-
ipants to correct speech recognition and transla-
tion errors via keyboard input, a feature which
is very eective when bandwidth limitations de-
grade the system performance.
3 Multi-Perspective Evaluations
Several dierent evaluation experiments have
been conducted, targeting dierent aspects of
our system: (1) the impact of network trac
and the consequences of real packet-loss on sys-
tem performance; (2) the impact and usability of
multi-modality; (3) a comparison of the features
of the various speech recognition engines, devel-
oped independently for dierent languages with
dierent techniques; and (4) end-to-end perfor-
mance evaluations. The data used in the evalu-
ations is part of a database collected during the
project (Burger et al, 2001).
3.1 Network Trac Impact
In our various user studies and demonstrations,
we have been forced to deal with the detrimental
eects of network congestion on the transmission
of Voice-over-IP in our system. The critical net-
work paths are the H323 connections between
the Mediator and the client and agent, which
rely on the UDP protocol in order to guaran-
tee real-time, but potentially lossy, human-to-
human communication. This can potentially be
very detrimental to the performance of speech
recognizers (Metze et al, 2001). The commu-
nication between the Mediator and HLT servers
can, in principle, be within a local network, al-
though we currently run the HLT servers at the
sites of the developing partners. This introduces
time delays, but no packet loss, due to the use
of TCP, rather than the UDP used for the H323
connections.
To quantify the influence of UDP packet-loss
on system performance, we ran a number of tests
with German client installations in the USA
(CMU at Pittsburgh) and Germany (UKA at
Karlsruhe) calling a Mediator in Italy (IRST),
which in turn contacted the German HLT server
located at UKA. The tests were conducted by
feeding a high-quality recording of the German
36
37
38
39
40
41
42
0 1 2 3 4 5 6
W
or
d 
Er
ro
r R
at
e 
(%
)
Packet Loss (%)
ITA-US
ITA-GER
Figure 2: Influence of packet loss on word accuracy of
the German Nespole! recognizer
development test-set collected at the beginning
of the project into a computer set-up for a video-
conference, i.e. we replaced the microphone by
a DAT recorder (or a computer) playing a tape,
while leaving everything else as it would be for
sessions with real subjects. In particular, seg-
mentation was based on silence detection per-
formed automatically by NetMeeting. Each test
consisted of several dialogues, lasting about an
hour. These tests (a total of more than 16 hours)
were conducted at dierent times of the day on
dierent days of the week, in an attempt to in-
vestigate a wide as possible variety of real-life
network conditions.
We were able to run 16 complete tests, re-
sulting in an average word accuracy of 60.4%,2
with single values in the 63% to 59% range for
packet-loss conditions between 0.1% and 5.2%.
The results of these tests are presented in graph-
ical from in Figure 2. On a couple of occasions
we experienced abnormally bad network condi-
tions for short periods of time. These led to a
breakdown of the Client-Mediator or Mediator-
HLT server link due to time-out conditions being
reached, or the inability to establish a connec-
tion at all. We were able, however, to record one
full test with 21.0% packet loss, which resulted
in a word accuracy of 50.3%. These dialogues
are very dicult to understand even for humans.
Our conclusion from the packet loss experi-
2The word accuracy on the clean 16kHz recording is
71.2%.
ment is that our speech recognition engine is
relatively robust to packet loss rates of up to
5%, since there is no clear degradation in the
word accuracy of the recognizer as a function
of packet loss rate (in this range). This is very
good news, since our experience indicates that
packet loss rates of over 5% are quite rare un-
der normal network trac conditions. For 20%
packet-loss, the increase in WER is signicant,
but the degradation is less severe than that re-
ported in (Milner and Semnani, 2000) on syn-
thetic data. We suspect that this is due to the
non-random distribution of lost packets.
The tests described above were the rst phase
of our research on the impact of network trac
on system performance. We are currently in the
process of conducting several further experimen-
tal investigations concerning dierent conditions
in which the system may run:
Transmission of video in addition to audio
through the video-conferencing communi-
cation channel: in this case we expect a sub-
stantial increase in UDP packet-loss rates due
to audio and video competing for the network
bandwidth over the H323 connections. It is not
clear, however, how this competition takes place
in practice and what are the resulting repercus-
sions on the audio quality (and consequently on
the recognizers? performance).
The use of low-bandwidth network con-
nections (such as standard 56Kbps
modems): This is the most common network
scenario for real client users using a home in-
stalled computer. We are currently exploring
how the bandwidth limitations in this setting
aect audio quality and system usability. In low
bandwidth conditions, NetMeeting supports en-
coding the speech with the G.723 codec, which
can consume a much lower bandwidth (less
then 6.4Kbps) compared to the G.711 codec
(64Kbps), which we currently use in our system.
We are in the process of testing the G.723 codec
within our system. Preliminary results indicate
that the recognizers used in the Nespole! sys-
tem are quite robust with respect to this new
front-end processing.
3.2 Experiments on Multi-Modality
The nature of the e-commerce scenario and ap-
plication in which our system is situated re-
quires that speech-translation be well-integrated
with additional modalities of communication
and information exchange between the agent
and client. Signicant eort has been devoted
to this issue within the project. The main
multi-modal component in the current version
of our system is the AeWhiteboard { a special
whiteboard, which allows users to share maps
and web-pages. The functionalities provided
by the AeWhiteboard include: image loading,
free-hand drawing, area selecting, color choos-
ing, scrolling the image loaded, zooming the im-
age loaded, URL opening, and Nespole! Monitor
activation. The most important feature of the
whiteboard is that each gesture performed by a
user is mirrored on the whiteboard of the other
user. Both users communicate while viewing the
same images and annotated whiteboards.
Typically, the client asks for spatial informa-
tion regarding locations, distances, and naviga-
tion directions (e.g., how to get from a hotel
to the ski slopes). By using the whiteboard,
the agent can indicate the locations and draw
routes on the map, point at areas, select items,
draw connections between dierent locations us-
ing a mouse or an optical pen, and accompany
his/her gestures with verbal explanations. Sup-
porting such combined verbal and gesture in-
teractions has required modications and exten-
sions of both HLT modules and the IF.
During July 2001, we conducted a detailed
study to evaluate the eect of multi-modality on
the communication eectiveness and usability of
our system. The goals of the experiment were
to test: (1) whether multi-modality increases
the probability of successful interaction, espe-
cially when spatial information is the focus of
the communicative exchange; (2) whether multi-
modality helps reduce mis-communications and
disfluencies; and (3) whether multi-modality
supports a faster recovery from recognition and
translation errors. For these purposes, two ex-
perimental conditions were devised: a speech-
only condition (SO), involving multilingual com-
munication and the sharing of images; and a
multi-modal condition (MM), where users could
additionally convey spatial information by pen-
based gestures on shared maps.
The setting for the experiment was the sce-
nario described earlier, involving clients search-
ing for winter tour-package information in the
Trentino province. The client?s task was to se-
lect an appropriate resort location and hotel
within the specied constraints concerning the
relevant geographical area, the available bud-
get, etc. The agent?s task was to provide the
necessary information. Novice subjects, previ-
ously unfamiliar with the system and task were
recruited to play the role of the clients. Subjects
wore a head-mounted microphone, using it in a
push-to-talk mode, and drew gestures on maps
by means of a table-pen device or a mouse. Each
subject could only hear the translated speech of
the other party (original audio was disabled in
this experiment). 28 dialogues were collected,
with 14 dialogues each for English and for Ger-
man clients, and Italian agents in all cases. Each
group contained 7 SO and 7 MM dialogues. The
dialogue transcriptions include: orthographical
transcription, annotations for spontaneous phe-
nomena and disfluencies, turn information and
annotations for gestures. Translated turns were
classied into successful, partially successful and
unsuccessful by comparing the translated turns
with the responses they generated. Repeated
turns were also counted.
The average duration of dialogues was 35 min-
utes (35.8 for SO and 35.5 for MM). On aver-
age, a dialogue contained 35 turns, 247 tokens
and 97 token types per speaker. Average val-
ues and variance of all measures are very similar
for agents and clients and across conditions and
Languages. ANOVA tests (p=0.05) on the num-
ber of turns and the number of spontaneous phe-
nomena and disfluencies, agents and customers
separately, did not produce any evidence that
modality or language aected these variables.
Hence the spoken input is homogeneous across
groups. Details on the experimental database
collected and the various statistical analyses per-
formed appear in (Costantini et al, 2002). The
analysis of the results indicated that both the
SO and MM versions of the system were eec-
tive for goal completion: 86% of the users were
able to complete the task?s goal by choosing a
hotel meeting the pre-specied budget and loca-
tion constraints.
In the MM dialogues, there were 7.6 gestures
per dialogue on average. The agents performed
almost all gestures (98%), with a clear prefer-
ence for area selections (61% of total gestures).
Most gestures (79%) followed a dialogue con-
tribution; none of the gestures were performed
during speech. Overall, few or no deictics were
used. We believe that these ndings are related
to the push-to-talk procedure and to the time
needed to transfer gestures across the network:
agents often preceded gestures with appropriate
verbal cues e.g., \I?ll show you the hotel on the
map", in order to notify the other party of an
upcoming gesture. These verbal cues indicate
that gestures were well integrated in the com-
munication.
We found signicant dierences between the
SO and MM dialogues in terms of unsuccessful
and repeated turns, particularly so in the spatial
segments of the dialogues. In the English-Italian
dialogues the MM dialogues contained 19% un-
successful turns versus 30% for the SO dialogues.
For German-Italian dialogues we found 18% in
MM versus 31% in SO. English-Italian MM dia-
logues contained 11% repeated turns versus 17%
for SO. For German-Italian dialogues repeated
turns amounted to 18% for MM versus 23% for
SO. In addition we found smoother dialogues
under MM condition, with fewer returns to al-
ready discussed topics for MM (one return every
19 turns in SO versus one return every 31 turns
in MM). MM also exhibited a lower number of
dialogue segments containing identiable misun-
derstandings between the two parties (one such
segment in each of 3 of the MM dialogues, ver-
sus a total of seven such segments in the SO dia-
logues { one dialogue with 3 segments, one with
two, and a third with a single segment of mis-
communication). Furthermore, the misunder-
standings in MM conditions were often immedi-
ately solved by resorting to MM resources, while
in case of SO ambiguous or mis-understood sub-
dialogues often remained unresolved. Finally,
the experiment subjects, given the choice be-
tween the MM and the SO system, expressed
a clear preference for the former. In summary,
we found strong supporting evidence that mul-
timodality has a positive eect on the quality
of interaction by reducing ambiguity, making it
easier to resolve ambiguous utterances and to re-
cover from system errors, improving the flow of
the dialogue, and enhancing the mutual compre-
hension between the parties, in particular when
spatial information is involved.
3.3 Features of Automatic Speech
Recognition Engines
The Speech Recognition modules of the Nespo-
le! system were developed separately at the dif-
ferent participating sites, using dierent toolk-
its, but communicate with the Mediator using
a standardized interface. The French and Ger-
man ASR modules are described in more detail
in (Vaufreydaz et al, 2001; Metze et al, 2001).
The German engine was derived from the UKA
recognizer developed for the German Verbmobil
Task (Soltau et al, 2001).
All systems were derived from existing
LVCSR recognizers and adapted to the Nespo-
le! task using less than 2 hours of adaptation
data. This data was collected during an initial
user-study, in which clients from all countries
communicated with an APT agent fluent in their
mother tongue through the Nespole! system,
but without recognition and translation compo-
nents in place. Segmentation of input speech is
done based on automatic silence detection per-
formed by NetMeeting at the site of the origi-
nating audio. The audio is encoded according
to the G.711 standard at a sampling frequency
of 8kHz. The characteristics of the dierent rec-
ognizers are summarized in Table 1. The word
accuracy rates of the recognizers are presented
in Section 3.4.
3.4 End-to-End System Evaluation
In December 2001, we conducted a large scale
multi-lingual end-to-end translation evaluation
of the Nespole! rst-showcase system. For
each of the three language pairs (English-Italian,
German-Italian and French-Italian), four previ-
English French German Italian
Vocabulary size 8,000 20,000 12,000 4,000
OOV rate 0.3% <1% 3.0%
LM training Verbmobil (E), C-Star Internet Verbmobil (D) C-Star
Data 550k words 1,500M words 500k words 100k words
+ adaptation Nespole none Nespole Nespole
Perplexity 33 98 150
Microphone type head-set head-set table-top head-set
Speaking style spontaneous read spontaneous read
Ac. training 16kHz G711 recoded 16kHz G711 recoded
Data 90h 12h 65h Verbmobil-II 11h C-Star
+ adaptation Up-sampling of G711 MLLR 80min. + FSA
Real-time factor 2.5, 1GHz P-III 1.1, 1GHz P-III 1.8, 650Mhz P-III
Memory consumption 280Mb 200Mb 100Mb 100Mb
WER on clean data 19.9% 28% 29.8% 31.5%
Table 1: Features of the Speech Recognition Engines
Language WARs SR Graded (% Acc)
English 61.9% 66.0%
German 63.5% 68.0%
French 71.2% 65.0%
Italian 76.5% 70.6%
Table 2: Speech Recognition Word Accuracy Rates and
Results of Human Grading (Percent Acceptable) of Recog-
nition Output as a Paraphrase
Language Transcribed Speech Rec.
English-to-English 58% 45%
German-to-German 46% 40%
French-to-French 54% 41%
Italian-to-Italian 61% 48%
Table 3: Monolingual End-to-End Translation Results
(Percent Acceptable) on Transcribed and Speech Recog-
nized Input
ously unseen test dialogues were used to evaluate
the performance of the translation system. The
dialogues included two scenarios: one covering
winter ski vacations, the other about summer
resorts. One or two of the dialogues for each lan-
guage contained multi-modal expressions. The
test data included a mixture of dialogues that
were collected mono-lingually prior to system
development (both client and agent spoke the
same language), and data collected bilingually
(during the July 2001 MM experiment), using
the actual translation system. This mixture of
data conditions was intended primarily for com-
prehensiveness and not for comparison of the dif-
ferent conditions.
We performed an extensive suite of evalua-
Language Transcribed Speech Rec.
English-to-Italian 55% 43%
German-to-Italian 32% 27%
French-to-Italian 44% 34%
Italian-to-English 47% 37%
Italian-to-German 47% 31%
Italian-to-French 40% 27%
Table 4: Cross-lingual End-to-End Translation Results
(Percent Acceptable) on Transcribed and Speech Recog-
nized Input
tions on the above data. The evaluations were
all end-to-end, from input to output, not as-
sessing individual modules or components. We
performed both mono-lingual evaluation (where
generated output language was the same as the
input language), as well as cross-lingual evalu-
ation. For cross-lingual evaluations, translation
from English German and French to Italian was
evaluated on client utterances, and translation
from Italian to each of the three languages was
evaluated on agent utterances. We evaluated on
both manually transcribed input as well as on
actual speech-recognition of the original audio.
We also graded the speech recognized output as
a \paraphrase" of the transcriptions, to measure
the levels of semantic loss of information due
to recognition errors. Speech recognition word
accuracies and the results of speech graded as
a paraphrase appear in Table 2. Translations
were graded by multiple human graders at the
level of Semantic Dialogue Units (SDUs). For
each data set, one grader rst manually seg-
mented each utterance into SDUs. All graders
then used this segmentation in order to assign
scores for each SDU present in the utterance.
We followed the three-point grading scheme pre-
viously developed for the C-STAR consortium,
as described in (Levin et al, 2000). Each SDU is
graded as either \Perfect" (meaning translated
correctly and output is fluent), \OK" (meaning
is translated reasonably correct but output may
be disfluent), or \Bad" (meaning not properly
translated). We calculate the percent of SDUs
that are graded with each of the above cate-
gories. \Perfect" and \OK" percentages are also
summed together into a category of \Accept-
able" translations. Average percentages are cal-
culated for each dialogue, each grader, and sep-
arately for client and agent utterances. We then
calculated combined averages for all graders and
for all dialogues for each language pair.
Table 3 shows the results of the monolingual
end-to-end translation for the four languages,
and Table 4 shows the results of the cross-
lingual evaluations. The results indicate accept-
able translations in the range of 27{43% of SDUs
(interlingua units) with speech recognized in-
puts. While this level of translation accuracy
cannot be considered impressive, our user stud-
ies and system demonstrations indicate that it is
already sucient for achieving eective commu-
nication with real users. We expect performance
levels to reach a range of 60{70% within the next
year of the project.
Acknowledgements
Additional Authors: S. Burger, D. Gates, C.
Langley, K. Laskowski, L. Levin, K. Peterson, T.
Schultz, A. Waibel, D. Wallace, Carnegie Mel-
lon University; J. McDonough, H. Soltau, Uni-
versity of Karlsruhe, Germany; G. Lazzari, N.
Manna, F. Pianesi, E. Pianta, ITC-irst, Trento,
Italy; L. Besacier, H. Blanchon, D. Vaufreydaz,
Universite Joseph Fourier, Grenoble, France; L.
Taddei, AETHRA, Ancona, Italy.
This work was supported by NSF Grant
9982227 and EU Grant IST 1999-11562 as part
of the joint EU/NSF MLIAM research initiative.
References
Susanne Burger, Laurent Besacier, Paolo Coletti,
Florian Metze, and Celine Morel. 2001. The NE-
SPOLE! VoIP Dialogue Database. In Proc. Eu-
roSpeech 2001, Aalborg, Denmark. ISCA.
Erica Costantini, Susanne Burger, and Fabio Pianesi.
2002. Nespole!?s multilingual and multimodal cor-
pus. In Proceedings of the Third International
Conference on Language Resources and Evalua-
tion (LREC-2002), Grand Canary Island, Spain,
June. To appear.
Alon Lavie, Fabio Pianesi, and al. 2001. Architec-
ture and Design Considerations in NESPOLE!: a
Speech Translation System for E-Commerce Ap-
plications. In Proc. of the HLT2001, San Diego,
CA. ACM.
Gianni Lazzari. 2001. Spoken translation: chal-
lenges and opportunities. In Proc. ICSLP 2001,
Beijing, China, 10.
Lori Levin, Donna Gates, Fabio Pianesi, Donna Wal-
lace, Takeshi Watanabe, and Monika Woszczyna.
2000. Evaluation of a Practical Interlingua for
Task-Oriented Dialogues. In Proceedings NAACL-
2000 Workshop On Interlinguas and Interlingual
Approaches, Seattle, WA. AMTA.
Florian Metze, John McDonough, and Hagen Soltau.
2001. Speech Recognition over NetMeeting Con-
nections. In Proc. EuroSpeech 2001, Aalborg,
Denmark. ISCA.
Florian Metze, John McDonough, Hagen Soltau,
Alex Waibel, Alon Lavie, Susan Burger, Chad
Langley, Kornel Laskowski, Lori Levin, Tanja
Schultz, Fabio Pianesi, Roldano Cattoni, Gianni
Lazzari, Nadia Mana, Emanuele Pianta, Laurent
Besacier, Herve Blanchon, Dominique Vaufreydaz,
and Loredana Taddei. 2002. The NESPOLE!
Speech-to-Speech Translation System. In Proc.
HLT 2002, San Diego, CA, 3.
Ben Milner and Sharam Semnani. 2000. Robust
Speech Recognition over IP Networks. In Pro-
ceedings of International Conference on Acoustics
Speech and Signal Processing (ICASSP-00), Istan-
bul, Turkey, June.
Hagen Soltau, Thomas Schaaf, Florian Metze, and
Alex Waibel. 2001. The ISL Evaluation System
for Verbmobil - II. In Proc. ICASSP 2001, Salt
Lake City, USA, 5.
D. Vaufreydaz, L. Besacier, C. Bergamini, and
R. Lamy. 2001. presented at ISCA ITRW Work-
shop on Adaptation Methods for Speech Recogni-
tion, August. Sophia-Antipolis, France.
Balancing Expressiveness and Simplicity
in an Interlingua for Task Based Dialogue
Lori Levin, Donna Gates, Dorcas Wallace,
Kay Peterson, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
email: lsl@cs.cmu.edu
Fabio Pianesi, Emanuele Pianta,
Roldano Cattoni, Nadia Mana
IRST-itc, Italy
Abstract
In this paper we compare two interlin-
gua representations for speech transla-
tion. The basis of this paper is a distri-
butional analysis of the C-star II and
Nespole databases tagged with inter-
lingua representations. The C-star II
database has been partially re-tagged
with the Nespole interlingua, which
enables us to make comparisons on the
same data with two types of interlin-
guas and on two types of data (C-
star II and Nespole) with the same
interlingua. The distributional infor-
mation presented in this paper show
that the Nespole interlingua main-
tains the language-independence and
simplicity of the C-star II speech-act-
based approach, while increasing se-
mantic expressiveness and scalability.
1 Introduction
Several speech translation projects have chosen
interlingua-based approaches because of its con-
venience (especially in adding new languages)
in multi-lingual projects. However, interlingua
design is notoriously dicult and inexact. The
main challenge is deciding on the grain size of
meaning to represent and what facets of mean-
ing to include. This may depend on the do-
main and the contexts in which the translation
system is used. For projects that take place at
multiple research sites, another factor becomes
important in interlingua design: if the interlin-
gua is too complex, it cannot be used reliably by
researchers at remote sites. Furthermore, the in-
terlingua should not be biased toward one fam-
ily of languages. Finally, an interlingua should
clearly distinguish general and domain specic
components for easy scalability and portability
between domains.
Sections 2 and 3 describe how we balanced
the factors of grain-size, language independence,
and simplicity in two interlinguas for speech
translation projects | the C-star II Inter-
change Format (Levin et al, 1998) and the Ne-
spole Interchange Format. Both interlinguas
are based in the framework of domain actions
as described in (Levin et al, 1998). We will
show that the Nespole interlingua has a ner
grain-size of meaning, but is still simple enough
for collaboration across multiple research sites,
and still maintains language-independence.
Section 4 will address the issue of scalabil-
ity of interlinguas based on domain actions to
larger domains. The basis of Section 4 is a dis-
tributional analysis of the C-star II and Ne-
spole databases tagged with interlingua repre-
sentations. The C-star II database has been
partially re-tagged with the Nespole interlin-
gua, which enables us to make comparisons on
the same data with two types of interlinguas and
on two types of data (C-star II and Nespole)
with the same type of interlingua.
2 The C-star II Domain, Database,
and Interlingua
The C-star II interlingua (Levin et al, 1998)
was developed between 1997 and 1999 for use
in the C-star II 1999 demo (www.c-star.org).
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 53-60.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
c: can I have some flight times
that would leave some time around June sixth
a: the there are several flights leaving D C
there?d be one at one twenty four
there?s a three fifty nine flight
that arrives at four fifty eight
...
what time would you like to go
c: I would take the last one that you mentioned
...
a: what credit card number would you like
to reserve this with
c: I have a visa card
and the number is double oh five three
three one one six
ninety nine eighty seven
a okay
c: the expiration date is eleven ninety seven
...
a okay they should be ready tomorrow
c: okay thank you very much
Figure 1: Excerpt from a C-star II dialogue
with six participating research sites. The seman-
tic domain was travel, including reservations
and payments for hotels, tours, and transporta-
tion. Figure 1 shows a sample dialogue from
the C-star II database. (C is the client and a
is the travel agent.) The C-star II database
contains 2278 English sentences and 7148 non-
English (Japanese, Italian, Korean) sentences
tagged with interlingua representations. Most
of the database consists of transcripts of role-
playing conversations.
The driving concept behind the C-star II
interlingua is that there are a limited num-
ber of actions in the domain | requesting the
price of a room, telling the price of a room,
requesting the time of a flight, giving a credit
card number, etc. | and that each utter-
ance can be classied as an instance of one
of these domain actions . Figure 2 illustrates
the components of the C-star II interlingua:
(1) the speaker tag, in this case c for client,
(2) a speech act (request-action), (3) a list
of concepts (reservation, temporal, hotel),
(4) arguments (e.g., time), and (5) values of ar-
guments. The C-star II interlingua specica-
tion document contains denitions for 44 speech
acts, 93 concepts, and 117 argument names.
The domain action is the part of the interlin-
gua consisting of the speech act and concepts, in
this case request-action+reservation+tem-
poral+hotel. The domain action does not in-
clude the list of argument-value pairs.
First it is important to point out that do-
main actions are created compositionally. A do-
main action consists of a speech act followed by
zero or more concepts. (Recall that argument-
value pairs are not part of the domain action.)
The Nespole interlingua includes 65 speech
acts and 110 concepts. An interlingua speci-
cation document denes the legal combinations
of speech acts and arguments.
The linguistic justication for an interlingua
based on domain-actions is that many travel do-
main utterances contain xed, formulaic phrases
(e.g., can you tell me; I was wondering; how
about; would you mind, etc.) that signal domain
actions, but either do not translate literally into
other languages or have a meaning that is su-
ciently indirect that the literal meaning is irrele-
vant for translation. To take two examples, how
about as a signal of a suggestion does not trans-
late into other languages with the words corre-
sponding to how and about . Also, would you
mind might translate literally into some Euro-
pean languages as a way of signaling a request,
but the literal meaning of minding is not rel-
evant to the translation, only the fact that it
signals politeness.
The measure of success for the domain-action
based interlingua (as described in (Levin et al,
2000a)) is that (1) it covers the data in the C-
star II database with less than 8% no-tag rate,
(2) inter-coder agreement across research sites
is reasonably high: 82% for speech acts, 88%
for concepts, and 65% for domain actions, and
(3) end-to-end translation results using an an-
alyzer and generator written at dierent sites
were about the same as end-to-end translation
results using an analyzer and generator written
at the same site.
3 The Nespole Domain, Database,
and Interlingua
The Nespole interlingua has been under devel-
opment for the last two years as part of the Ne-
spole project (http://nespole.itc.it). Fig-
I would like to make a hotel reservation for the fourth through
the seventh of july
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Figure 2: Example of a C-star II interlingua representation
ure 3 shows a Nespole dialogue. The Ne-
spole domain does not include reservations and
payments, but includes more detailed inquiries
about hotels and facilities for ski vacations and
summer vacations in Val di Fiemme, Italy. (The
tourism board of the Trentino area is a partner
of the Nespole project.) Most of the database
consists of transcripts of dialogues between an
Italian-speaking travel agent and an English or
German speaker playing the role of a traveller.
There are fewer xed, formulaic phrases in the
Nespole domain, prompting us to move toward
domain actions that are more general, and also
requiring more detailed interlingua representa-
tions. Changes from the C-star II interlingua
fall into several categories:
1. Extending semantic expressivity and
syntactic coverage: Increased coverage of
modality, tense, aspect, articles, fragments,
coordinate structures, number, and rhetor-
ical relations. In addition, we have added
more explicit representation of grammati-
cal relations and improved capabilities for
representing modication and embedding.
2. Additional Domain-Specic Con-
cepts: New concepts include giving
directions, describing sizes and dimensions
of objects, traveling routes, equipment and
gear, airports, tourist services, facilities,
vehicles, information objects (brochures,
web pages, rules and regulations), hours
of operation of businesses and attractions,
etc.
3. Utterances that accompany multi-
modal gestures: The Nespole system
includes capabilities to share web pages
and draw marks such as circles and arrows
on web pages. The interlingua was ex-
tended to cover colord, descriptions of two-
dimensional objects, and actions of show-
ing.
4. General concept names from Word-
Net: The Nespole interlingua includes
conventions for making new concept names
based on WordNet synsets.
5. More general domain actions replac-
ing specic ones: For example, replacing
hotel with accommodation.
Interlinguas based on domain actions con-
trast with interlinguas based on lexical seman-
tics (Dorr, 1993; Lee et al, 2001; Goodman and
Nirenburg, 1991). A lexical-semantic interlingua
includes a representation of predicates and their
arguments. For example, the sentence I want to
take a vacation has a predicate want with two
arguments I and to take a vacation, which in
turn has a predicate take and two arguments, I
and a vacation. Of course, predicates like take
may be represented as word senses that are less
language-dependent like participate-in. The
strength and weakness of the lexical-semantic
approach is that it is less domain dependent
than the domain-action approach.
In order to cover the less formulaic utterances
of the Nespole domain, we have taken a step
closer to the lexical-semantic approach. How-
ever, we have maintained the overall framework
of the domain-action approach because there are
still many formulaic utterances that are better
represented in a non-literal way. Also, in or-
der to abstract away from English syntax, con-
cepts such as disposition, eventuality, and obli-
gation are not represented in the interlingua as
argument-taking main verbs in order to accom-
modate languages in which these meanings are
c: and I have some questions about coming about a trip I?m gonna be taking to Trento
a: okay what are your questions
c: I currently have a hotel booking at the
Panorama-Hotel in Panchia but at the moment I have no idea how to get to my hotel from Trento
and I wanted to ask what would be the best way for me to get there
a: okay I?m gonna show you a map that and then describe the directions to you
okay so right so you will arrive in the train station in Trento
the that is shown in the middle of the map stazione FFSS
and just below that here is a bus stop labeled number forty
so okay on the map that I?m showing you here
the hotel is the orange building off on the right hand side
...
c: I also wanted to ask about skiing in the area once I?m in Panchia
a: all right just a moment and I?ll show you another map
c: okay
a: okay so on the map you see now Panchia is right in the center of the map
c: I see it
Figure 3: Excerpt from a Nespole dialogue
represented as adverbs or suxes on verbs. Fig-
ure 4 shows the Nespole interlingua represen-
tation corresponding to the C-star II interlin-
gua in Figure 2. The specication document for
the Nespole interlingua denes 65 speech acts,
110 concepts, 292 arguments, and 7827 values
grouped into 222 value classes. As in the C-
star II interlingua, domain actions are dened
compositionally from speech acts and arguments
in combinations that are allowed by the interlin-
gua specication.
3.1 Comparison of Nespole and
C-star II Interlinguas
It is useful to compare the Nespole and C-
star II Interlinguas in expressivity, language in-
dependence, and simplicity.
Expressivity of the Nespole interlingua,
Argument 1: The metric we use for expres-
sivity is the no-tag rate in the databases. The
no-tag rate is the percentage of sentences that
cannot be assigned an interlingua representation
by a human expert. The C-star II database
tagged with C-star II interlingua had a no-
tag rate of 7.3% (Levin et al, 2000a). The
C-star II database tagged with Nespole in-
terlingua has a no-tag rate of 2.4%. More than
300 English sentences in the C-star II database
that were not covered by the C-star II interlin-
gua are now covered by the Nespole interlin-
gua. (See Table 2.) We conclude from this that
the Nespole interlingua is more expressive in
that it covers more data.
Language-independence of the Nespole
interlingua: We do not have a numerical
measure of language-independence, but we note
that interlinguas based on domain actions are
particularly suitable for avoiding translation
mismatches (Dorr, 1994), particularly head-
switching mismatches (e.g., I just arrived and
Je vient d?arriver where the meaning of recent
past is expressed by an adverb just or a syn-
tactic verb vient (venir).) Interlinguas based
on domain actions resolve head-switching mis-
matches by identifying the types of meanings
that are often involved in mismatches | modal-
ity, evidentiality, disposition, and so on | and
assigning them a representation that abstracts
away from predicate argument structure. In-
terlinguas based on domain actions also neu-
tralize the dierent ways of expressing indirect
speech acts within and across languages (for ex-
ample, Would you mind..., I was wondering if
you could...., and Please.... as ways of request-
ing an action). Although Nespole domain ac-
tions are more general than C-star II domain
actions, they maintain language independence
by abstracting away from predicate-argument
structure.
Simplicity and cross-site reliability of the
Nespole interlingua: Simplicity of an inter-
lingua is measured by cross-site reliability in
I would like to make a hotel reservation for the fourth through
the seventh of july
C-star II Interlingua:
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Nespole Interlingua:
c:give-information+disposition+reservation+accommodation
(disposition=(who=i, desire),
reservation-spec=(reservation, identifiability=no),
accommodation-spec=hotel,
object-time=(start-time=(md=4), end-time=(md=7, month=7, incl-excl=inclusive)))}
Figure 4: Example of Nespole interlingua representation
inter-coder agreement and end-to-end transla-
tion performance. At the time of writing this pa-
per we have not conducted cross-site inter-coder
agreement experiments using the Nespole in-
terlingua. We have, however, conducted cross-
site evaluations (Lavie et al, 2002), in which the
analyzer and generator were written at dier-
ent sites. Experiments at the end of C-star II
showed that cross-site evaluations were compa-
rable to intra-site evaluations (analyzer and gen-
erator written at the same site) (Levin et al,
2000b). Nespole evaluations so far show a loss
of cross-site reliability: intra-site evaluations are
noticeably better than cross-site evaluations, as
reported in (Lavie et al, 2002). This seems to
indicate that developers at dierent sites have
a lower level of agreement on the Nespole in-
terlingua. However there are other possible ex-
planations for the discrepancy | for example
developers at dierent sites may have focused
their development on dierent sub-domains |
that are currently under investigation.
4 Scalability of the Nespole
Interlingua
The rest of this paper addresses the scalability
of the Nespole interlingua. A possible criti-
cism of domain actions is that they are domain
dependent and that the number of domain ac-
tions might increase too quickly with the size
of the domain. In this section, we will examine
the rate of increase in the number of domain ac-
tions as a function of the amount of data and
the diversity of the data.
Dierences in the C-star and Nespole Do-
mains: We will rst show that the C-star
and Nespole domains are signicantly dierent
even though they both pertain to travel. The
combination of the two domains is therefore sig-
nicantly larger than either domain alone.
In order to demonstrate the dierences be-
tween the C-star travel domain and the Ne-
spole travel domain, we measured the overlap
in vocabulary. The numbers in Table 4 are based
on the rst 7900 word tokens in the C-star En-
glish database and the rst 7900 word tokens
in the Nespole English database. The table
shows the number of unique word types in each
database, the number of word types that occur
in both databases, and the number of word types
that occur in one of the databases, but not in the
other. In each database, about half of the word
types overlap with the other database. The non-
overlapping vocabulary (402 C-star word types
and 344 Nespole word types) indicates that the
two databases cover quite dierent aspects of the
travel domain.
Scalability: Argument 1: We will now be-
gin to address the issue of scalability of the
domain action approach to interlingua design.
Our rst argument concerns the number of
Number of unique word types
CSTAR English 745
Nespole English 687
Word types in both CSTAR and Nespole 343
Words types in CSTAR not in Nespole 402
Words types n Nespole not in CSTAR 344
Table 1: Number of overlapping word types in the C-star English and Nespole English
databases
SA Con. Snts. Domain Ac-
tions
Old C-star English 44 93 2278 358
New C-star English 65 110 2564 452
Nespole English 65 110 1446 337
Nespole German 65 110 3298 427
Nespole Italian 65 110 1063 206
Table 2: Number of unique domain actions in interlingua databases
speech acts and concepts in the combined C-
star/Nespole domain. The C-star II in-
terlingua, designed for coverage of the C-star
travel domain, included 44 speech acts and 93
concepts. The Nespole interlingua, designed
for coverage of the combined C-star and Ne-
spole domains, has 65 speech acts and 110 con-
cepts. Thus a relatively small increase in the
number of speech acts and concepts is required
to cover a signicantly larger domain.
The increased size of the C-star/Nepsole
domain is reflected in the number of arguments
and values. The C-star II interlingua contained
denitions for 117 arguments, whereas the Ne-
spole interlingua contains denitions for 292 ar-
guments. The number of values for arguments
also has increased signicantly in the Nespole
domain. There are 7827 values grouped into 222
classes (airport names, days of the week, etc.).
Distributional Data: number of domain
actions in each database: Next we will
present distributional data concerning the num-
ber of domain actions as a function of database
size. We will compare several databases: Old
C-star English (around 2278 sentences tagged
with C-star II interlingua), New C-star En-
glish (2564 sentences tagged with Nespole in-
terlingua, including the 2278 sentences from Old
C-star English), Nespole English, Nespole
German, and Nespole Italian. Table 2 shows
the number of sentences and the number of do-
main actions in each database. The number of
domain actions refers to the number of types,
not tokens, of domain actions.
Distributional data: Coverage of the top
50 domain actions: Table 3 shows the per-
centage of each database that is covered by the
5, 10, 20, and 50 most frequent domain actions
in that database. For each database, the do-
main actions were ordered by frequency. The
percentage of sentences covered by the top-n
domain actions was then calculated. For this
experiment, we separated sentences spoken by
the traveller (client) and sentences spoken by
the travel agent (agent). C-star data in Ta-
ble 3 refers to 2564 English sentences from the
C-star database that were tagged with Ne-
spole interlingua. Nespole data refers to the
English portion of the Nespole database (1446
sentences). Combined data refers to the combi-
nation of the two (4014 sentences).
Two points are worth noting about Table 3.
First, the Nespole agent data has a higher cov-
erage rate than the Nespole client data. That
is, more data is covered by the top-n domain
actions. This may be because there was was
Domain Actions Top 5 Top 10 Top 20 Top 50
Client
C-star data 33.6 42.7 53.1 66.7
Nespole data 31.7 43.5 53.9 66.5
Combined data 31.6 40.0 50.3 62.9
Agent
C-star data 33.8 42.8 54.1 67.3
Nespole data 39.0 47.8 56.1 71.4
Combined data 33.6 41.5 51.7 64.0
Table 3: DA Coverage using Nespole interlingua on English data for both C-star and
Nespole
only a small amount of English agent data and
it was spoken by non-native speakers. Second,
the combined data has a slightly lower cover-
age rate than either the C-star or Nespole
databases alone. This is expected because, as
shown above, the combined domain is signi-
cantly more diverse than either domain by itself.
Scalability: Argument 2: Table 3 provides
additional evidence for the scalability of the Ne-
spole interlingua to larger domains. In the
combined C-star and Nespole domain, the
top 50 domain actions cover only slightly less
data than the top 50 domain actions in either
domain separately. There is not, in fact, an ex-
plosion of domain actions when the two C-star
and Nespole domains are combined.
Distributional Data: domain actions as a
function of database size: Table 3 shows
that in each of our databases, the 50 most fre-
quent domain actions cover approximately 65%
of the sentences. The next issue we address is
the nature of the \tail" of less frequent domain
actions covering the remainder of the data.
Figure 5 shows the number of domain actions
as a function of data set size. Sampling was done
for intervals of 25 sentences starting at 100 sen-
tences. For each sample size s there was ten-fold
cross-validation. Ten random samples of size s
were chosen, and the number of dierent domain
actions in each sample was counted. The aver-
age of the number of domain actions in each of
the ten samples of size s are plotted in Figure 5.
The four databases represented in Figure 5 are
IF Coverage of Four Datasets
0
100
200
300
400
500
600
700
10
0
70
0
13
00
19
00
25
00
31
00
number of SDUs in sample
av
er
ag
e 
nu
m
be
r o
f u
ni
qu
e 
DA
s 
o
ve
r 
10
 ra
nd
om
 s
am
pl
es
Old CSTAR
New CSTAR
NESPOLE
Combined
Figure 5: Number of domain actions as a function of
database size
the C-star English database tagged with C-
star II interlingua, the C-star II database
tagged with Nespole interlingua, the Nespole
English database, and the combined C-star
and Nespole English databases.
Expressivity, Argument 2: Figure 5 pro-
vides evidence for the increased expressivity of
the Nespole interlingua. In contrast to Ta-
ble 3, which deals with samples containing the
most frequent domain actions, the samples plot-
ted in Figure 5 contain random mixtures of fre-
quent and non-frequent domain actions. The
curve representing the C-star data with C-
star II interlingua is the slowest growing of the
four curves. This is because the grain-size of
meaning represented in the C-star II interlin-
gua was larger than in the Nespole interlin-
gua. Also many infrequent domain actions were
not covered by the C-star II interlingua. The
faster growth of the curve representing the C-
star data with Nespole interlingua indicates
improved expressivity of the Nespole interlin-
gua | it covers more of the infrequent domain
actions. The highest curve in Figure 5 repre-
sents the combined C-star and Nespole do-
mains. This curve is higher than the others be-
cause, as shown above, the two travel domains
are signicantly dierent from each other.
Expressivity and Simplicity, the right bal-
ance: Comparing Table 3 and Figure 5, we ar-
gue that the Nespole interlingua strikes a good
balance between expressivity and simplicity. Ta-
ble 3 shows evidence for the simplicity of the Ne-
spole interlingua: Only 50 domain actions are
needed to cover 60-70% of the sentences in the
database. Figure 5 shows evidence for expressiv-
ity: because domain actions are compositionally
formed from speech acts and concepts, it is pos-
sible to form a large number of low-frequency
domain actions in order to cover the domain.
Over 600 domain actions are used in the com-
bined C-star and Nespole domains.
5 Conclusions
We have presented a comparison of a purely
domain-action-based interlingua (the C-star II
interlingua) and a more expressive, but still
domain-action-based interlingua (the Nespole
interlingua). The data that we have presented
show that the more expressive interlingua has
better coverage of the domain (a decrease from
7.3% to 2.4% uncovered data in the C-star II
domain) and can also scale up to larger domains
without an explosion of domain actions. Thus
we have a reasonable compromise between sim-
plicity and expressiveness of the interlingua.
Acknowledgments
We would like to acknowledge Hans-Ulrich Block
for rst proposing the domain-action-based in-
terlingua to the C-star consortium. We would
also like to thank all of the C-star and Ne-
spole partners who have participated in the de-
sign of the interlingua. This work was supported
by NSF Grant 9982227 and EU Grant IST 1999-
11562 as part of the joint EU/NSF MLIAM re-
search initiative.
References
Bonnie J. Dorr. 1993. Machine Translation: A View
from the Lexicon. The MIT Press, Cambridge,
Massachusetts.
Bonnie J. Dorr. 1994. Machine Translation Diver-
gences: A Formal Description and Proposed Solu-
tion. Computational Linguistics, 20(4):597{633.
Kenneth Goodman and Sergei Nirenburg. 1991.
The KBMT Project: A Case Study in Knowledge-
Based Machine Translation. Morgan Kaufmann,
San Mateo, CA.
Alon Lavie, Florian Metze, Roldano Cattoni, and Er-
ica Constantini. 2002. A Multi-Perspective Eval-
uation of the NESPOLE! Speech-to-Speech Trans-
lation System. In Proceedings of Speech-to-Speech
Translation: Algorithms and Systems.
Young-Suk Lee, W. Yi, Cliord Weinstein, and
Stephanie Sene. 2001. Interlingua-based broad-
coverage korean-to-english translation. In Pro-
ceedings of HLT, San Diego.
Lori Levin, Donna Gates, Alon Lavie, and Alex
Waibel. 1998. An Interlingua Based on Domain
Actions for Machine Translation of Task-Oriented
Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (IC-
SLP?98), pages Vol. 4, 1155{1158, Sydney, Aus-
tralia.
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi,
Dorcas Wallace, Taro Watanabe, and Monika
Woszczyna. 2000a. Evaluation of a Practical In-
terlingua for Task-Oriented Dialogue. In Work-
shop on Applied Interlinguas: Practical Applica-
tions of Interlingual Approaches to NLP, Seattle.
Lori Levin, Alon Lavie, Monika Woszczyna, Donna
Gates, Marsal Gavalda, Detlef Koll, and Alex
Waibel. 2000b. The Janus-III Translation Sys-
tem. Machine Translation.
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1169?1178, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Language Model Rest Costs and Space-Efficient Storage
Kenneth Heafield?,? Philipp Koehn? Alon Lavie?
? Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{heafield,alavie}@cs.cmu.edu
? School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB, UK
pkoehn@inf.ed.ac.uk
Abstract
Approximate search algorithms, such as cube
pruning in syntactic machine translation, rely
on the language model to estimate probabili-
ties of sentence fragments. We contribute two
changes that trade between accuracy of these
estimates and memory, holding sentence-level
scores constant. Common practice uses lower-
order entries in an N -gram model to score
the first few words of a fragment; this vio-
lates assumptions made by common smooth-
ing strategies, including Kneser-Ney. Instead,
we use a unigram model to score the first
word, a bigram for the second, etc. This im-
proves search at the expense of memory. Con-
versely, we show how to save memory by col-
lapsing probability and backoff into a single
value without changing sentence-level scores,
at the expense of less accurate estimates for
sentence fragments. These changes can be
stacked, achieving better estimates with un-
changed memory usage. In order to interpret
changes in search accuracy, we adjust the pop
limit so that accuracy is unchanged and re-
port the change in CPU time. In a German-
English Moses system with target-side syntax,
improved estimates yielded a 63% reduction
in CPU time; for a Hiero-style version, the
reduction is 21%. The compressed language
model uses 26% less RAM while equivalent
search quality takes 27% more CPU. Source
code is released as part of KenLM.
1 Introduction
Language model storage is typically evaluated in
terms of speed, space, and accuracy. We introduce
a fourth dimension, rest cost quality, that captures
how well the model scores sentence fragments for
purposes of approximate search. Rest cost quality is
distinct from accuracy in the sense that the score of
a complete sentence is held constant. We first show
how to improve rest cost quality over standard prac-
tice by using additional space. Then, conversely, we
show how to compress the language model by mak-
ing a pessimistic rest cost assumption1.
Language models are designed to assign probabil-
ity to sentences. However, approximate search algo-
rithms use estimates for sentence fragments. If the
language model has order N (an N -gram model),
then the first N ? 1 words of the fragment have in-
complete context and the last N ? 1 words have not
been completely used as context. Our baseline is
common practice (Koehn et al 2007; Dyer et al
2010; Li et al 2009) that uses lower-order entries
from the language model for the first words in the
fragment and no rest cost adjustment for the last few
words. Formally, the baseline estimate for sentence
fragment wk1 is
(
N?1?
n=1
pN (wn|w
n?1
1 )
)(
k?
n=N
pN (wn|w
n?1
n?N+1)
)
where each wn is a word and pN is an N -gram lan-
guage model.
The problem with the baseline estimate lies in
lower order entries pN (wn|w
n?1
1 ). Commonly used
Kneser-Ney (Kneser and Ney, 1995) smoothing,
1Here, the term rest cost means an adjustment to the score of
a sentence fragment but not to whole sentences. The adjustment
may be good or bad for approximate search.
1169
including the modified version (Chen and Good-
man, 1998), assumes that a lower-order entry will
only be used because a longer match could not
be found2. Formally, these entries actually eval-
uate pN (wn|w
n?1
1 , did not find w
n
0 ). For purposes
of scoring sentence fragments, additional context is
simply indeterminate, and the assumption may not
hold.
As an example, we built 5-gram and unigram lan-
guage models with Kneser-Ney smoothing on the
same data. Sentence fragments frequently begin
with ?the?. Using a lower-order entry from the 5-
gram model, log10 p5(the) = ?2.49417. The uni-
gram model does not condition on backing off, as-
signing log10 p1(the) = ?1.28504. Intuitively, the
5-gram model is surprised, by more than an order of
magnitude, to see ?the? without matching words that
precede it.
To remedy the situation, we train N language
models on the same data. Each model pn is an n-
gram model (it has order n). We then use pn to
score the nth word of a sentence fragment. Thus,
a unigram model scores the first word of a sentence
fragment, a bigram model scores the second word,
and so on until either the n-gram is not present in
the model or the first N?1 words have been scored.
Storing probabilities from these models requires one
additional value per n-gram in the model, except for
N -grams where this probability is already stored.
Conversely, we can lower memory consumption
relative to the baseline at the expense of poorer rest
costs. Baseline models store two entries per n-gram:
probability and backoff. We will show that the prob-
ability and backoff values in a language model can
be collapsed into a single value for each n-gram
without changing sentence probability. This trans-
formation saves memory by halving the number of
values stored per entry, but it makes rest cost esti-
mates worse. Specifically, the rest cost pessimisti-
cally assumes that the model will back off to uni-
grams immediately following the sentence fragment.
The two modifications can be used independently
or simultaneously. To measure the impact of their
different rest costs, we experiment with cube prun-
ing (Chiang, 2007) in syntactic machine transla-
2Other smoothing techniques, including Witten-Bell (Witten
and Bell, 1991), do not make this assumption.
tion. Cube pruning?s goal is to find high-scoring
sentence fragments for the root non-terminal in the
parse tree. It does so by going bottom-up in the parse
tree, searching for high-scoring sentence fragments
for each non-terminal. Within each non-terminal, it
generates a fixed number of high-scoring sentence
fragments; this is known as the pop limit. Increasing
the pop limit therefore makes search more accurate
but costs more time. By moderating the pop limit,
improved accuracy can be interpreted as a reduction
in CPU time and vice-versa.
2 Related Work
Vilar and Ney (2011) study several modifications to
cube pruning and cube growing (Huang and Chiang,
2007). Most relevant is their use of a class-based
language model for the first of two decoding passes.
This first pass is cheaper because translation alter-
natives are likely to fall into the same class. Entries
are scored with the maximum probability over class
members (thereby making them no longer normal-
ized). Thus, paths that score highly in this first pass
may contain high-scoring paths under the lexicalized
language model, so the second pass more fully ex-
plores these options. The rest cost estimates we de-
scribe here could be applied in both passes, so our
work is largely orthogonal.
Zens and Ney (2008) present rest costs for phrase-
based translation. These rest costs are based on fac-
tors external to the sentence fragment, namely out-
put that the decoder may generate in the future. Our
rest costs examine words internal to the sentence
fragment, namely the first and last few words. We
also differ by focusing on syntactic translation.
A wide variety of work has been done on language
model compression. While data structure compres-
sion (Raj and Whittaker, 2003; Heafield, 2011) and
randomized data structures (Talbot and Osborne,
2007; Guthrie and Hepple, 2010) are useful, here
we are concerned solely with the values stored by
these data structures. Quantization (Whittaker and
Raj, 2001; Federico and Bertoldi, 2006) uses less
bits to store each numerical value at the expense
of model quality, including scores of full sentences,
and is compatible with our approach. In fact, the
lower-order probabilities might be quantized further
than normal since these are used solely for rest cost
1170
purposes. Our compression technique reduces stor-
age from two values, probability and backoff, to one
value, theoretically halving the bits per value (ex-
cept N -grams which all have backoff 1). This makes
the storage requirement for higher-quality modified
Kneser-Ney smoothing comparable to stupid back-
off (Brants et al 2007). Whether to use one smooth-
ing technique or the other then becomes largely an
issue of training costs and quality after quantization.
3 Contribution
3.1 Better Rest Costs
As alluded to in the introduction, the first few words
of a sentence fragment are typically scored us-
ing lower-order entries from an N -gram language
model. However, Kneser-Ney smoothing (Kneser
and Ney, 1995) conditions lower-order probabilities
on backing off. Specifically, lower-order counts are
adjusted to represent the number of unique exten-
sions an n-gram has:
a(wn1 ) =
{
|{w0 : c(wn0 ) > 0}| if n < N
c(wn1 ) if n = N
where c(wn1 ) is the number of times w
n
1 appears in
the training data3. This adjustment is also performed
for modified Kneser-Ney smoothing. The intuition
is based on the fact that the language model will
base its probability on the longest possible match. If
an N -gram was seen in the training data, the model
will match it fully and use the smoothed count. Oth-
erwise, the full N -gram was not seen in the train-
ing data and the model resorts to a shorter n-gram
match. Probability of this shorter match is based on
how often the n-gram is seen in different contexts.
Thus, these shorter n-gram probabilities are not rep-
resentative of cases where context is short simply
because additional context is unknown at the time of
scoring.
In some cases, we are able to determine that
the model will back off and therefore the lower-
order probability makes the appropriate assumption.
Specifically, if vwn1 does not appear in the model for
any word v, then computing p(wn|vw
n?1
1 ) will al-
3Counts are not modified for n-grams bound to the begin-
ning of sentence, namely those with w1 = <s>.
ways back off to wn?11 or fewer words
4. This crite-
rion is the same as used to minimize the length of left
language model state (Li and Khudanpur, 2008) and
can be retrieved for each n-gram without using addi-
tional memory in common data structures (Heafield
et al 2011).
Where it is unknown if the model will back off,
we use a language model of the same order to pro-
duce a rest cost. Specifically, there are N language
models, one of each order from 1 to N . The mod-
els are trained on the same corpus with the same
smoothing parameters to the extent that they apply.
We then compile these into one data structure where
each n-gram record has three values:
1. Probability pn from the n-gram language
model
2. Probability pN from the N -gram language
model
3. Backoff b from the N -gram language model
For N -grams, the two probabilities are the same and
backoff is always 1, so only one value is stored.
Without pruning, the n-gram model contains the
same n-grams as the N -gram model. With prun-
ing, the two sets may be different, so we query the
n-gram model in the normal way to score every n-
gram in the N -gram model. The idea is that pn is the
average conditional probability that will be encoun-
tered once additional context becomes known. We
also tried more complicated estimates by addition-
ally interpolating upper bound, lower bound, and pN
with weights trained on cube pruning logs; none of
these improved results in any meaningful way.
Formalizing the above, let wk1 be a sentence frag-
ment. Choose the largest s so that vws1 appears in
the model for some v; equivalently ws1 is the left
state described in Li and Khudanpur (2008). The
4Usually, this happens because wn1 does not appear, though
it can also happen that wn1 appears but all vw
n
1 were removed
by pruning or filtering.
1171
baseline estimate is
pb(w
k
1) =
(
s?
n=1
pN (wn|w
n?1
1 )
)
?
(
N+1?
n=s+1
pN (wn|w
n?1
1 )
)
? (1)
(
k?
n=N
pN (wn|w
n?1
n?N+1)
)
while our improved estimate is
pr(w
k
1) =
(
s?
n=1
pn(wn|w
n?1
1 )
)
?
(
N+1?
n=s+1
pN (wn|w
n?1
1 )
)
? (2)
(
k?
n=N
pN (wn|w
n?1
n?N+1)
)
The difference between these equations is that pn is
used for words in the left state i.e. 1 ? n ? s.
We have also abused notation by using pN to denote
both probabilities stored explicitly in the model and
the model?s backoff-smoothed probabilities when
not present. It is not necessary to store backoffs for
pn because s was chosen such that all queried n-
grams appear in the model.
This modification to the language model improves
rest costs (and therefore quality or CPU time) at the
expense of using more memory to store pn. In the
next section, we do the opposite: make rest costs
worse to reduce storage size.
3.2 Less Memory
Many language model smoothing strategies, includ-
ing modified Kneser-Ney smoothing, use the back-
off algorithm shown in Figure 1. Given an n-gram
wn1 , the backoff algorithm bases probability on as
much context as possible. Equivalently, it finds
the minimum f so that wnf is in the model then
uses p(wn|w
n?1
f ) as a basis. Backoff penalties b
are charged because a longer match was not found,
forming the product
p(wn|w
n?1
1 ) = p(wn|w
n?1
f )
f?1?
j=1
b(wn?1j ) (3)
Notably, the backoff penalties {b(wn?1j )}
n?1
j=1 are in-
dependent of wn, though which backoff penalties are
charged depends on f and therefore wn.
backoff? 1
for f = 1? n do
if wnf is in the model then
return p(wn|wn?1f ) ? backoff
else
if wn?1f is in the model then
backoff? backoff ? b(wn?1f )
end if
end if
end for
Figure 1: The baseline backoff algorithm to com-
pute p(wn|w
n?1
1 ). It always terminates with a prob-
ability because even unknown words are treated as a
unigram.
for f = 1? n do
if wnf is in the model then
return q(wn|wn?1f )
end if
end for
Figure 2: The run-time pessimistic backoff algo-
rithm to find q(wn|w
n?1
1 ). It assumes that q has been
computed at model building time.
In order to save memory, we propose to account
for backoff in a different way, defining q
q(wn|w
n?1
1 ) =
p(wn|w
n?1
f )
?n
j=f b(w
n
j )
?n?1
j=f b(w
n?1
j )
where again wnf is the longest matching entry in the
model. The idea is that q is a term in the telescop-
ing series that scores a sentence fragment, shown
in equation (1) or (2). The numerator pessimisti-
cally charges all backoff penalties, as if the next
word wn+1 will only match a unigram. When wn+1
is scored, the denominator of q(wn+1|wn1 ) cancels
out backoff terms that were wrongly charged. Once
these terms are canceled, all that is left is p, the cor-
rect backoff penalties, and terms on the edge of the
series.
1172
Proposition 1. The terms of q telescope. Formally,
let wk1 be a sentence fragment and f take the mini-
mum value so that wkf is in the model. Then,
q(wk1) = p(w
k
1)
k?
j=f
b(wkj )
Proof. By induction on k. When k = 1, f = 1 since
the word w1 is either in the vocabulary or mapped to
<unk> and treated like a unigram.
q(w1) =
p(w1)
?1
j=1 b(w
1
j )
?0
j=1 b(w
0
j )
= p(w1)b(w1)
For k > 1,
q(wk1) = q(w
k?1
1 )q(wk|w
k?1
1 )
=
q(wk?11 )p(wk|w
k?1
f )
?k
j=f b(w
k
j )
?k?1
j=f b(w
k?1
j )
where f has the lowest value such that wkf is in the
model. Applying the inductive hypothesis to expand
q(wk?11 ), we obtain
p(wk?11 )
(?k?1
j=e b(w
k?1
j )
)
p(wk|w
k?1
f )
?k
j=f b(w
k
j )
?k?1
j=f b(w
k?1
j )
where e has the lowest value such that wk?1e is in the
model. The backoff terms cancel to yield
p(wk?11 )
?
?
f?1?
j=e
b(wk?1j )
?
? p(wk|w
k?1
f )
k?
j=f
b(wkj )
By construction of e, wk?1j is not in the model for all
j < e. Hence, b(wk?1j ) = 1 implicitly for all j < e.
Multiplying by 1,
p(wk?11 )
?
?
f?1?
j=1
b(wk?1j )
?
? p(wk|w
k?1
f )
k?
j=f
b(wkj )
Recognizing the backoff equation (3) to simplify,
p(wk?11 )p(wk|w
k?1
1 )
k?
j=f
b(wkj )
Finally, the conditional probability folds as desired
q(wk1) = p(w
k
1)
k?
j=f
b(wkj )
We note that entries ending in </s> have back-
off 1, so it follows from Proposition 1 that sentence-
level scores are unchanged.
q(<s> wk1 </s>) = p(<s> w
k
1 </s>)
Proposition 1 characterizes q as a pessimistic rest
cost on sentence fragments that scores sentences in
exactly the same way as the baseline using p and
b. To save memory, we simply store q in lieu of
p and b. Compared with the baseline, this halves
number of values from two to one float per n-gram,
except N -grams that already have one value. The
impact of this reduction is substantial, as seen in
Section 4.3. Run-time scoring is also simplified
as shown in Figure 2 since the language model lo-
cates the longest match wnf then returns the value
q(wn|w
n?1
1 ) = q(wn|w
n?1
f ) without any calcula-
tion or additional lookup. Baseline language mod-
els either retrieve backoffs values with additional
lookups (Stolcke, 2002; Federico et al 2008) or
modify the decoder to annotate sentence fragments
with backoff information (Heafield, 2011); we have
effectively moved this step to preprocessing. The
disadvantage is that q is not a proper probability and
it produces worse rest costs than does the baseline.
Language models are actually applied at two
points in syntactic machine translation: scoring lexi-
cal items in grammar rules and during cube pruning.
Grammar scoring is an offline and embarrassingly
parallel process where memory is not as tight (since
the phrase table is streamed) and fewer queries
are made, so slow non-lossy compression and even
network-based sharding can be used. We there-
fore use an ordinary language model for grammar
scoring and only apply the compressed model dur-
ing cube pruning. Grammar scoring impacts gram-
mar pruning (by selecting only top-scoring grammar
rules) and the order in which rules are tried during
cube pruning.
1173
3.3 Combined Scheme
Our two language model modifications can be triv-
ially combined by using lower-order probabilities on
the left of a fragment and by charging all backoff
penalties on the right of a fragment. The net result is
a language model that uses the same memory as the
baseline but has better rest cost estimates.
4 Experiments
To measure the impact of different rest costs, we
use the Moses chart decoder (Koehn et al 2007)
for the WMT 2011 German-English translation task
(Callison-Burch et al 2011). Using the Moses
pipeline, we trained two syntactic German-English
systems, one with target-side syntax and the other
hierarchical with unlabeled grammar rules (Chiang,
2007). Grammar rules were extracted from Europarl
(Koehn, 2005) using the Collins parser (Collins,
1999) for syntax on the English side. The language
model interpolates, on the WMT 2010 test set, sep-
arate models built on Europarl, news commentary,
and the WMT news data for each year. Models were
built and interpolated using SRILM (Stolcke, 2002)
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1998) and the de-
fault pruning settings. In all scenarios, the primary
language model has order 5. For lower-order rest
costs, we also built models with orders 1 through 4
then used the n-gram model to score n-grams in the
5-gram model. Feature weights were trained with
MERT (Och, 2003) on the baseline using a pop limit
of 1000 and 100-best output. Since final feature val-
ues are unchanged, we did not re-run MERT in each
condition. Measurements were collected by running
the decoder on the 3003-sentence test set.
4.1 Rest Costs as Prediction
Scoring the first few words of a sentence fragment
is a prediction task. The goal is to predict what
the probability will be when more context becomes
known. In order to measure performance on this
task, we ran the decoder on the hierarchical system
with a pop limit of 1000. Every time more context
became known, we logged5 the prediction error (es-
timated log probability minus updated log probabil-
5Logging was only enabled for this experiment.
Lower Baseline
n Mean Bias MSE Var Bias MSE Var
1 -3.21 .10 .84 .83 -.12 .87 .86
2 -2.27 .04 .18 .17 -.14 .23 .24
3 -1.80 .02 .07 .07 -.09 .10 .09
4 -1.29 .01 .04 .04 -.10 .09 .08
Table 1: Bias (mean error), mean squared error, and
variance (of the error) for the lower-order rest cost
and the baseline. Error is the estimated log prob-
ability minus the final probability. Statistics were
computed separately for the first word of a fragment
(n = 1), the second word (n = 2), etc. The lower-
order estimates are better across the board, reducing
error in cube pruning. All numbers are in log base
ten, as is standard for ARPA-format language mod-
els. Statistics were only collected for words with
incomplete context.
ity) for both lower-order rest costs and the baseline.
Table 1 shows the results.
Cube pruning uses relative scores, so bias mat-
ters less, though positive bias will favor rules with
more arity. Variance matters the most because lower
variance means cube pruning?s relative rankings are
more accurate. Our lower-order rest costs are bet-
ter across the board in terms of absolute bias, mean
squared error, and variance.
4.2 Pop Limit Trade-Offs
The cube pruning pop limit is a trade-off between
search accuracy and CPU time. Here, we mea-
sure how our rest costs improve (or degrade) that
trade-off. Search accuracy is measured by the aver-
age model score of single-best translations. Model
scores are scale-invariant and include a large con-
stant factor; higher is better. We also measure over-
all performance with uncased BLEU (Papineni et al
2002). CPU time is the sum of user and system time
used by Moses divided by the number of sentences
(3003). Timing includes time to load, though files
were forced into the disk cache in advance. Our test
machine has 64 GB of RAM and 32 cores. Results
are shown in Figures 3 and 4.
Lower-order rest costs perform better in both sys-
tems, reaching plateau model scores and BLEU with
less CPU time. The gain is much larger for tar-
1174
Baseline Lower Order Pessimistic Combined
Pop CPU Model BLEU CPU Model BLEU CPU Model BLEU CPU Model BLEU
2 3.29 -105.56 20.45 3.68 -105.44 20.79 3.74 -105.62 20.01 3.18 -105.49 20.43
10 5.21 -104.74 21.13 5.50 -104.72 21.26 5.43 -104.77 20.85 5.67 -104.75 21.10
50 23.30 -104.31 21.36 23.51 -104.24 21.38 23.68 -104.33 21.25 24.29 -104.22 21.34
500 54.61 -104.25 21.33 55.92 -104.15 21.38 54.23 -104.26 21.31 55.74 -104.15 21.40
700 64.08 -104.25 21.34 87.02 -104.14 21.42 68.74 -104.25 21.29 78.84 -104.15 21.41
(a) Numerical results for select pop limits.
-104.6
-104.5
-104.4
-104.3
-104.2
-104.1
0 10 20 30 40 50 60 70 80 90
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
Lower
Combined
Baseline
Pessimistic
21
21.05
21.1
21.15
21.2
21.25
21.3
21.35
21.4
0 10 20 30 40 50 60 70 80 90
U
nc
as
ed
B
L
E
U
CPU seconds/sentence
Lower
Combined
Baseline
Pessimistic
(b) Model and BLEU scores near the plateau.
Figure 3: Target-syntax performance. CPU time and model score are averaged over 3003 sentences.
get syntax, where a pop limit of 50 outperforms the
baseline with pop limit 700. CPU time per sen-
tence is reduced to 23.5 seconds from 64.0 seconds,
a 63.3% reduction. The combined setting, using the
same memory as the baseline, shows a similar 62.1%
reduction in CPU time. We attribute this differ-
ence to improved grammar rule scoring that impacts
pruning and sorting. In the target syntax model,
the grammar is not saturated (i.e. less pruning will
still improve scores) but we nonetheless prune for
tractability reasons. The lower-order rest costs are
particularly useful for grammar pruning because lex-
ical items are typically less than five words long (and
frequently only word).
The hierarchical grammar is nearly saturated with
respect to grammar pruning, so improvement there is
due mostly to better search. In the hierarchical sys-
tem, peak BLEU 22.34 is achieved under the lower-
order condition with pop limits 50 and 200, while
other scenarios are still climbing to the plateau. With
a pop limit of 1000, the baseline?s average model
score is -101.3867. Better average models scores
are obtained from the lower-order model with pop
limit 690 using 79% of baseline CPU, the combined
model with pop limit 900 using 97% CPU, and the
pessimistic model with pop limit 1350 using 127%
CPU.
Pessimistic compression does worsen search, re-
quiring 27% more CPU in the hierarchical system to
achieve the same quality. This is worthwhile to fit
large-scale language models in memory, especially
if the alternative is a remote language model.
4.3 Memory Usage
Our rest costs add a value (for lower-order prob-
abilities) or remove a value (pessimistic compres-
sion) for each n-gram except those of highest order
(n = N ). The combined condition adds one value
1175
Baseline Lower Order Pessimistic Combined
Pop CPU Model BLEU CPU Model BLEU CPU Model BLEU CPU Model BLEU
2 2.96 -101.85 21.19 2.44 -101.80 21.63 2.71 -101.90 20.85 3.05 -101.84 21.37
10 2.80 -101.60 21.90 2.42 -101.58 22.20 2.95 -101.63 21.74 2.69 -101.60 21.98
50 3.02 -101.47 22.18 3.11 -101.46 22.34 3.46 -101.48 22.08 2.67 -101.47 22.14
690 10.83 -101.39 22.28 11.45 -101.39 22.25 10.88 -101.40 22.25 11.19 -101.39 22.23
900 13.41 -101.39 22.27 14.00 -101.38 22.24 13.38 -101.39 22.25 14.09 -101.39 22.22
1000 14.50 -101.39 22.27 15.17 -101.38 22.25 15.09 -101.39 22.26 15.23 -101.39 22.23
1350 18.52 -101.38 22.27 19.16 -101.38 22.23 18.46 -101.39 22.25 18.61 -101.38 22.23
5000 59.67 -101.38 22.24 61.41 -101.38 22.22 59.76 -101.38 22.27 61.38 -101.38 22.22
(a) Numerical results for select pop limits.
-101.42
-101.415
-101.41
-101.405
-101.4
-101.395
-101.39
-101.385
-101.38
0 5 10 15 20 25
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
Lower
Combined
Baseline
Pessimistic
21.95
22
22.05
22.1
22.15
22.2
22.25
22.3
22.35
0 5 10 15 20 25
U
nc
as
ed
B
L
E
U
CPU seconds/sentence
Lower
Combined
Baseline
Pessimistic
(b) Model and BLEU scores near the plateau.
Figure 4: Hierarchical system performance. All values are averaged over 3003 sentences.
and removes another, so it uses the same memory
as the baseline. The memory footprint of adding or
removing a value depends on the number of such n-
grams, the underlying data structure, and the extent
of quantization. Our test language model has 135
million n-grams for n < 5 and 56 million 5-grams.
Memory usage was measured for KenLM data struc-
tures (Heafield, 2011) and minimal perfect hashing
(Guthrie and Hepple, 2010). For minimal perfect
hashing, we assume the Compress, Hash and Dis-
place algorithm (Belazzougui et al 2008) with 8-bit
signatures and 8-bit quantization. Table 2 shows the
results. Storage size of the smallest model is reduced
by 26%, bringing higher-quality smoothed models
in line with stupid backoff models that also store one
value per n-gram.
Structure Baseline Change %
Probing 4,072 517 13%
Trie 2,647 506 19%
8-bit quantized trie 1,236 140 11%
8-bit minimal perfect hash 540 140 26%
Table 2: Size in megabytes of our language model,
excluding operating system overhead. Change is the
cost of adding an additional value to store lower-
order probabilities. Equivalently, it is the savings
from pessimistic compression.
1176
5 Conclusion
Our techniques reach plateau-level BLEU scores
with less time or less memory. Efficiently stor-
ing lower-order probabilities and using them as rest
costs improves both cube pruning (21% CPU reduc-
tion in a hierarchical system) and model filtering
(net 63% CPU time reduction with target syntax) at
the expense of 13-26% more RAM for the language
model. This model filtering improvement is surpris-
ing both in the impact relative to changing the pop
limit and simplicity of implementation, since it can
be done offline. Compressing the language model to
halve the number of values per n-gram (except N -
grams) results in a 13-26% reduction in RAM with
26% over the smallest model, costing 27% more
CPU and leaving overall sentence scores unchanged.
This compression technique is likely to have more
general application outside of machine translation,
especially where only sentence-level scores are re-
quired. Source code is being released6 under the
LGPL as part of KenLM (Heafield, 2011).
Acknowledgements
This work was supported by the National Sci-
ence Foundation under grants DGE-0750271, IIS-
0713402, and IIS-0915327; by the EuroMatrixPlus
project funded by the European Commission (7th
Framework Programme), and by the DARPA GALE
program. Benchmarks were run on Trestles at the
San Diego Supercomputer Center under allocation
TG-CCR110017. Trestles is part of the Extreme
Science and Engineering Discovery Environment
(XSEDE), which is supported by National Science
Foundation grant number OCI-1053575.
References
Djamal Belazzougui, Fabiano C. Botelho, and Martin Di-
etzfelbinger. 2008. Hash, displace, and compress. In
Proceedings of the 35th international colloquium on
Automata, Languages and Programming (ICALP ?08),
pages 385?396.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
6http://kheafield.com/code/kenlm/
Language Processing and Computational Language
Learning, pages 858?867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228, June.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A decoder, alignment, and learning framework for
finite-state and context-free translation models. In
Proceedings of the ACL 2010 System Demonstrations,
ACLDemos ?10, pages 7?12.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of the Workshop on
Statistical Machine Translation, pages 94?101, New
York City, June.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Proceedings of Inter-
speech, Brisbane, Australia.
David Guthrie and Mark Hepple. 2010. Storing the web
in memory: Space efficient language models with con-
stant time retrieval. In Proceedings of EMNLP 2010,
Los Angeles, CA.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
model state for syntactic machine translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, San Francisco, CA, USA, De-
cember.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
1177
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181?
184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the Second ACL Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
10?18, Columbus, Ohio, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 135?139, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, PA, July.
Bhiksha Raj and Ed Whittaker. 2003. Lossless compres-
sion of language model structure and word identifiers.
In Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 388?
391.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proceedings of ACL, pages 512?519, Prague, Czech
Republic.
David Vilar and Hermann Ney. 2011. Cardinality prun-
ing and language model heuristics for hierarchical
phrase-based translation. Machine Translation, pages
1?38, November. DOI 10.1007/s10590-011-9119-4.
Ed Whittaker and Bhiksha Raj. 2001. Quantization-
based language model compression. In Proceedings
of EUROSPEECH, pages 33?36, September.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
Richard Zens and Hermann Ney. 2008. Improvements in
dynamic programming beam search for phrase-based
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), Honolulu, Hawaii, October.
1178
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 395?404,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Learning from Post-Editing:
Online Model Adaptation for Statistical Machine Translation
Michael Denkowski Chris Dyer Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{mdenkows,cdyer,alavie}@cs.cmu.edu
Abstract
Using machine translation output as a
starting point for human translation has
become an increasingly common applica-
tion of MT. We propose and evaluate three
computationally efficient online methods
for updating statistical MT systems in a
scenario where post-edited MT output is
constantly being returned to the system:
(1) adding new rules to the translation
model from the post-edited content, (2)
updating a Bayesian language model of
the target language that is used by the
MT system, and (3) updating the MT
system?s discriminative parameters with
a MIRA step. Individually, these tech-
niques can substantially improve MT qual-
ity, even over strong baselines. Moreover,
we see super-additive improvements when
all three techniques are used in tandem.
1 Introduction
Using machine translation outputs as a starting
point for human translators is becoming increas-
ingly common and is now arguably one of the most
commercially important applications of MT. Con-
siderable evidence has accumulated showing that
human translators are more productive and accu-
rate when post-editing MT output than when trans-
lating from scratch (Guerberof, 2009; Carl et al.,
2011; Koehn, 2012; Zhechev, 2012, inter alia).
An important (if unsurprising) insight from prior
research in this area is that translators become
more productive as MT quality improves (Tat-
sumi, 2009). While general improvements to MT
continue to lead to further productivity gains, we
explore how MT quality can be improved specifi-
cally in an online post-editing scenario in which
sentence-level MT outputs are constantly being
presented to human experts, edited, and then re-
turned to the system for immediate learning. This
task is challenging in two regards. First, from a
technical perspective, post-edited outputs must be
processed rapidly: a productive post-editor cannot
wait for a standard batch MT training pipeline to
be rerun after each sentence is corrected! Sec-
ond, from a methodological perspective, it is ex-
pensive to run many human subject experiments,
in particular when the human subjects must have
translation expertise. We therefore use a sim-
ulated post-editing paradigm in which either
non-post-edited reference translations or manually
post-edited translations from a similar MT system
are used in lieu of human post-editors (?2). This
paradigm allows us to efficiently develop and eval-
uate systems that can go on to function in real-time
post-editing scenarios without modification.
We present and evaluate three online methods
for improving translation models using feedback
from editors: adding new translations rules to
the translation grammar (?3), updating a Bayesian
language model with observations of the post-
edited output (?4), and using an online discrimi-
native parameter update to minimize model error
(?5). These techniques are computationally effi-
cient and make minimal use of approximation or
heuristics, handling initial and incremental data in
a uniform way. We evaluate these techniques in a
variety of language and data scenarios that mimic
the demands of real-world translation tasks. Com-
pared to a competitive baseline, we show substan-
tial improvement from updating the translation
grammar or language model independently and
super-additive gains from combining these tech-
niques with a MIRA update (?6). We then discuss
how our techniques relate to prior work (?7) and
conclude (?8).
2 Simulated Post-Editing Paradigm
In post-editing scenarios, humans continuously
edit machine translation outputs into production-
quality translations, providing an additional, con-
395
stant stream of data absent in batch translation.
This data consists of highly domain-relevant ref-
erence translations that are minimally different
from MT outputs, making them ideal for learn-
ing. However, true post-editing data is infeasi-
ble to collect during system development and in-
ternal testing as standard MT pipelines require
tens of thousands of sentences to be translated
with low latency. To address this problem, Hardt
and Elming (2010) formulate the task of sim-
ulated post-editing, wherein pre-generated refer-
ence translations are used as a stand-in for actual
post-editing. This approximation is equivalent to
the case where humans edit each translation hy-
pothesis to be identical to the reference rather than
simply correcting the MT output to be grammat-
ical and meaning-equivalent to the source. Our
work uses this approximation for tuning and eval-
uation. We also introduce a more accurate approx-
imation wherein MT output from the target sys-
tem (or a similar system) is post-edited in advance,
creating ?offline? post-edited data that is similar
to expected system outputs and should thus min-
imize unnecessary edits. An experiment in ?6.4
compares the two approximations.
In our simulated post-editing tasks, decoding
(for both the test corpus and each pass over the
development corpus during optimization) begins
with baseline models trained on standard bilin-
gual and monolingual data. After each sentence
is translated, the following take place in order:
First, MIRA uses the new source?reference pair
to update weights for the current models. Second,
the source is aligned to the reference and used to
update the translation grammar. Third, the refer-
ence is added to the Bayesian language model. As
sentences are translated, the models gain valuable
context information, allowing them to zero in on
the target document and translator. Context is re-
set at the start of each development or test corpus.
1
This setup, which allows a uniform approach to
tuning and decoding, is visualized in Figure 1.
3 Translation Grammar Adaptation
Translation models (either phrase tables or syn-
chronous grammars) are typically generated of-
fline from large bilingual text. This is reasonable
in scenarios where available training data is fixed
over long periods of time. However, this approach
1
Initial experiments show this to outperform resetting
models on more fine-grained document boundaries, although
further investigation is warranted.
Hola contestadora ...
Hello voicemail, ...
He llamado a servicio ... I?ve called for tech ...
Ignor?e la advertencia ... I ignored my boss? ...
Ahora anochece, ...
Now it?s evening, and ...
Todav??a sigo en espera ...
I?m still on hold ...
No creo que me hayas ... I don?t think you ...
Ya he presionado cada ... I punched every touch ...
Incremental training data
Source
Target (Reference)
Figure 1: Context when translating an input sen-
tence (bold) with simulated post-editing. Previ-
ous sentences and references (shaded) are added
to the training data. After the current sentence is
translated, it is aligned to the reference (italic) and
added to the context for the next sentence.
does not allow adding new data without repeating
model estimation in its entirety, which may take
hours or days. In this section, we describe a simple
technique for incorporating new bilingual training
data as soon as it is available. Our approach is
an extension of the on-demand grammar extractor
described by Lopez (2008a). We extend the work
initially designed for on-the-fly grammar extrac-
tion from static data (to mitigate the expense of
storing large translation grammars), to specifically
handle incremental data from post-editing.
3.1 Suffix Array Grammar Extraction
Lopez (2008a) introduces an alternative to tradi-
tional model estimation for hierarchical phrase-
based statistical machine translation (Chiang,
2007). Rather than estimating a single grammar
from all training data, the aligned bitext is indexed
using a source-side suffix array (Manber and My-
ers, 1993). When an input sentence is to be trans-
lated, a grammar extraction program samples in-
stances of aligned phrase pairs from the suffix ar-
ray that match the source side of the sentence.
Using statistics from these samples rather than
the entire bitext, a sentence-specific grammar is
rapidly generated. In addition to speed gains from
sampling, indexing the source side of the bitext fa-
cilitates a more powerful feature set. Rules in on-
demand grammars are generated using a sample S
for each source phrase f in the input sentence. The
sample, containing pairs ?f, e?, is used to calculate
the following statistics:
396
Feature Baseline Adaptive
coherent
p(e|f)
C
S
(f, e)
|S|
C
S
(f, e) + C
L
(f, e)
|S|+ |L|
sample size |S| |S|+ |L|
co-occur-
rence ?f, e?
C
S
(f, e) C
S
(f, e)+C
L
(f, e)
singleton f
C
S
(f)
= 1
C
S
(f) + C
L
(f) =
1
singleton
?f, e?
C
S
(f, e)
= 1
C
S
(f, e) + C
L
(f, e)
= 1
post-edit sup-
port ?f, e?
0 C
L
(f, e) > 0
Table 1: Phrase feature definitions for baseline and
adaptive translation models.
? C
S
(f, e): count of instances in S where f
aligns to e (phrase co-occurrence count).
? C
S
(f): count of instances in S where f aligns
to any target phrase.
? |S|: total number of instances in S, equal to
number of occurrences of f in training data,
capped by the sample size limit.
These statistics are used to instantiate translation
rules X??f, e? and calculate scores for the phrase
feature set shown in the ?Baseline? column of Ta-
ble 1. Notably, the coherent phrase translation
probability that conditions on f occurring in the
data (|S|) rather than f being extracted as part of a
phrase pair (C
S
(f)) is shown by Lopez (2008b) to
yield significant improvement over the traditional
translation probability.
3.2 Online Grammar Extraction
When a human translator post-edits MT output, a
new bilingual sentence pair is created. However,
in typical settings, it can be weeks or months be-
fore these training instances are incorporated into
bilingual data and models retrained. Our exten-
sion to on-demand grammar extraction incorpo-
rates these new training instances into the model
immediately. In addition to a static suffix array
that indexes initial data, our system maintains a
dynamic lookup table. Each new sentence pair is
word-aligned with the model estimated from the
initial data (a process often called forced align-
ment). This makes a generally insignificant ap-
proximation with respect to the original alignment
model. Extractable phrase pairs are stored in the
lookup table and phrase occurrences are counted
on the source side. When subsequent grammars
are extracted, the suffix array sample S for each
f is accompanied by an exhaustive lookup L from
the lookup table. Matching statistics are calculated
from L:
? C
L
(f, e): count of instances in L where f
aligns to e.
? C
L
(f): count of instances inLwhere f aligns
to any target phrase.
? |L|: total number of instances of f in post-
editing data (no size limit).
We use combined statistics from S and L to calcu-
late scores for the ?Adaptive? feature set defined in
Table 1. In addition to updating existing features,
we introduce a new indicator feature that identi-
fies rules supported by post-editor feedback. Fur-
ther, our approach allows us to extract rules that
encode translations (phrase mappings and reorder-
ings) only observed in the incremental post-editing
data. This process, which can be seen as influ-
encing the distribution from which grammars are
sampled over time, produces comparable results
to the infeasible process of rebuilding the transla-
tion model after every sentence is translated with
the added benefit of allowing an optimizer to learn
a weight for the post-edited data via the post-edit
support feature. The simple aggregation of statis-
tics allows our model to handle initial and incre-
mental data in a formally consistent way. Further,
any additional features that can be calculated on a
suffix array sample can be matched by an incre-
mental data lookup, making our translation model
a viable platform for further exploration in online
learning for MT.
4 Language Model Adaptation
Adapting language models in an online manner
based on the content they are generating has long
been seen as a promising technique for improving
automatic speech recognition and machine transla-
tion (Kuhn and de Mori, 1990; Zhao et al., 2004;
Sanchis-Trilles, 2012, inter alia). The post-editing
scenario we are considering simplifies this process
somewhat since rather than only having a poste-
rior distribution over machine-generated outputs
(any of which may be ungrammatical), the out-
puts, once edited by human translators, may be
presumed to be grammatical.
We thus take a novel approach to language
model adaptation, building on recent work show-
ing that state-of-the-art language models can be
397
inferred as the posterior predictive distribution
of a Bayesian language model with hierarchi-
cal Pitman-Yor process priors, conditioned on the
training corpus (Teh, 2006). The Bayesian formu-
lation provides a natural way to incorporate pro-
gressively more data: by updating the posterior
distribution given subsequent observations. Fur-
thermore, the nonparametric nature of the model
means that the model is well suited to poten-
tially unbounded growth of vocabulary. Unfortu-
nately, in general, Bayesian techniques are com-
putationally difficult to work with. However, hi-
erarchical Pitman-Yor process language models
(HPYPLMs) are convenient in this regard since
(1) inference can be carried out efficiently in a
convenient collapsed representation (the ?Chinese
restaurant franchise?) and (2) the posterior predic-
tive distribution from a single sample provides a
high quality language model.
We thus use the following procedure. Using
the target side of the bitext as observations, we
run the Gibbs sampling procedure described by
Teh (2006) for 100 iterations in a 3-gram HPY-
PLM. The inferred ?seating configuration? defines
a posterior predictive distribution over words in 2-
gram contexts (as with any 3-gram LM) as well
as a posterior distribution over how the model will
generate subsequent observations. We use the for-
mer as a language model component of a transla-
tion model. And, as post-edited sentences become
available, we add their n-grams to the model us-
ing the later. We do not run any Gibbs sampling.
Just updating the language model in this way, we
obtain the results shown in Table 2 for the experi-
mental conditions described in ?6.
5 Learning Feature Weights
MT system parameter optimization (learning fea-
ture weights for the decoder) is also typically con-
ducted as a batch process. Discriminative learn-
ing techniques such as minimum error rate train-
ing (Och, 2003) are used to find feature weights
that maximize automatic metric score on a small
development corpus. The resulting weight vector
is then used to decode given input sentences. Us-
ing this approach with post-editing tasks presents
two major issues. First, reference translation are
only considered after all sentences are translated,
a mismatch with post-editing where references are
available incrementally. Second, despite the fact
that adaptive feature sets become more powerful
as post-editing data increases, an optimizer must
Spanish?English WMT10 WMT11 TED1 TED2
HPYPLM 25.5 24.8 29.4 26.6
+data 25.8 25.2 29.5 27.0
English?Spanish WMT10 WMT11 TED1 TED2
HPYPLM 25.1 26.8 26.0 24.3
+data 25.4 27.2 26.2 25.0
Arabic?English MT08 MT09 TED1 TED2
HPYPLM 19.3 24.7 9.5 10.0
+data 19.6 24.9 9.8 10.5
Table 2: BLEU scores for systems with trigram
HPYPLM (no large language model), with and
without incremental updates from simulated post-
editing data. Scores are averages over 3 optimizer
runs. Bold scores indicate statistically significant
improvement. Tuning set scores are italicized.
learn a single corpus-level weight for each fea-
ture. This forces an averaging effect that can lead
to decoding individual sentences with suboptimal
weights. We address the first issue by using ref-
erence translations to simulate post-editing (Hardt
and Elming, 2010) at tuning time and the second
by using a version of the margin-infused relaxed
algorithm (Crammer et al., 2006; Eidelman, 2012)
to make online parameter updates during decod-
ing. The result is a consistent approach to tuning
and decoding that brings out the potential of adap-
tive models.
5.1 Parameter Optimization
In order to make our decoding process fully con-
sistent with tuning, we introduce an online dis-
criminative parameter update that allows our adap-
tive translation and language models be weighted
appropriately as more data is available. This re-
quires an optimization algorithm that can func-
tion as an online learner during decoding as well
as a batch optimizer during tuning. Popular opti-
mizers such as MERT (Och, 2003) and pairwise
rank optimization (Hopkins and May, 2011) can-
not be used due to their reliance on corpus-level
optimization. We select the cutting-plane variant
of the margin-infused relaxed algorithm (Chiang,
2012; Crammer et al., 2006) with additional exten-
sions described by Eidelman (2012). MIRA is an
online large-margin learner that makes a param-
eter update after each model prediction with the
objective of choosing the correct output over the
incorrect output by a margin at least as large as the
cost of predicting the incorrect output. Applied
398
to MT system optimization on a development cor-
pus, MIRA proceeds as follows. The MT system
generates a list of the k best translations for a sin-
gle input sentence. From the list, a ?hope? hy-
pothesis is selected as a translation with both high
model score and high automatic metric score. A
?fear? hypothesis is selected as a translation with
high model score but low metric score. Parameters
are updated away from the fear hypothesis, toward
the hope hypothesis, and the system processes the
next input sentence. This process continues for a
set number of passes over the development corpus.
All adaptive systems used in our work are opti-
mized with this variant of MIRA using the param-
eter settings described by Eidelman (2012). For
each pass over the data, translation and language
models have incremental access to reference trans-
lations (simulated post-editing data) as input sen-
tences are translated. Translation and language
models reset to using background data only at the
beginning of each MIRA iteration.
2
5.2 Online Parameter Updates
Our optimization strategy allows us to treat de-
coding as if it were simply the next iteration of
MIRA (or alternatively that MIRA makes a single
pass over an input corpus that consists of the de-
velopment data concatenated n times followed by
unseen input data). After each sentence is trans-
lated, a reference translation (resulting from ac-
tual human post-editing in production or simulated
post-editing for our experiments) is provided to
the models and MIRA makes a parameter update.
In the only departure from our optimization setup,
we decrease the maximum step size for MIRA (de-
scribed in ?6.2), effectively increasing regulariza-
tion strength. This allows us to prefer small ad-
justments to already optimized decoding parame-
ters over the large changes needed during tuning.
It is also important to note that by using MIRA
for updating weights during both tuning and de-
coding, we avoid scaling issues between multiple
optimizers (such as when tuning with MERT and
updating with a passive-aggressive algorithm).
6 Experiments
We evaluate our online extensions to standard
machine translation systems in a series of sim-
2
Resetting translation and language models prevents con-
tamination. If models retained state from previous passes
over the development set, they would include data for input
sentences before they were translated, rather than after as in
post-editing.
Spanish?English WMT10 WMT11 TED1 TED2
Base MERT 29.1 27.9 32.8 29.6
Base MIRA 29.2 28.0 32.7 29.7
G 29.8 28.3 34.2 30.7
L 29.2 28.1 33.0 29.8
M 29.2 28.1 33.1 29.8
G+L+M 30.0 28.8 35.2 31.3
English?Spanish WMT10 WMT11 TED1 TED2
Base MERT 27.8 29.4 26.5 25.7
Base MIRA 27.7 29.6 26.8 26.7
G 28.1 29.8 27.9 27.5
L 27.9 29.7 26.8 26.5
M 27.9 29.7 27.2 26.6
G+L+M 28.4 30.4 28.6 27.9
Arabic?English MT08 MT09 TED1 TED2
Base MERT 21.5 25.0 10.4 10.5
Base MIRA 21.2 25.9 10.6 10.9
G 21.8 26.2 11.0 11.7
L 20.6 25.7 10.6 10.9
M 21.3 25.7 10.8 11.0
G+L+M 21.8 26.5 11.4 11.8
Table 3: BLEU scores for baseline and adap-
tive systems. Scores are averages over three opti-
mizer runs. Highest scores are bold and tuning set
scores are italicized. All fully adaptive systems
(G+L+M) show statistically significant improve-
ment over both MERT and MIRA baselines.
ulated post-editing experiments that cover high-
traffic languages and challenging domains. We
show incremental improvement from our adaptive
models and significantly larger gains when pair-
ing our models with an online parameter update.
We finally validate our adaptive system on actual
post-edited data.
6.1 Data
We conduct a series of simulated post-editing
experiments in three full scale language sce-
narios: Spanish?English, English?Spanish, and
Arabic?English. Spanish?English and English?
Spanish systems are trained on the 2012 NAACL
WMT (Callison-Burch et al., 2012) constrained
resources (2 million bilingual sentences, 300 mil-
lion words of monolingual Spanish, and 1.1 billion
words of monolingual English). Arabic?English
systems are trained on the 2012 NIST OpenMT
(Przybocki, 2012) constrained bilingual resources
plus a selection from the English Gigaword cor-
pus (Parker et al., 2011) (5 million bilingual sen-
tences and 650 million words of monolingual En-
399
glish). We tune and evaluate on standard news
sets: WMT10 and WMT11 for Spanish?English
and English?Spanish, and MT08 and MT09 for
Arabic?English. To simulate real-world post edit-
ing where one translator works on a document at a
time, we use only one of the four available refer-
ence translation sets for MT08 and MT09.
We also evaluate on a blind domain adapta-
tion scenario that mimics the demands placed
on MT systems in real-world translation tasks.
The Web Inventory of Transcribed and Translated
Talks (WIT
3
) corpus (Cettolo et al., 2012) makes
transcriptions of TED talks
3
available in several
languages, including English, Spanish, and Ara-
bic. For each language pair, we select two sets of
10 talk transcripts each (2000-3000 sentences) as
blind evaluation sets. These sets consist of spoken
language covering a broad range of topics. Sys-
tems have no access to any training or develop-
ment data in this domain prior to translation.
6.2 Translation Systems
For each language scenario, we first construct a
competitive baseline system. Bilingual data is
word aligned using the model described by Dyer
et al. (2013) and suffix array-backed transla-
tion grammars are extracted using the method
described by Lopez (2008a). We add the stan-
dard lexical and derivation features
4
from Lopez
(2008b) and Dyer et al. (2010). An unpruned,
modified Kneser-Ney-smoothed 4-gram language
model is estimated using the KenLM toolkit
(Heafield et al., 2013). Feature weights are op-
timized using the lattice-based variant of MERT
(Macherey et al., 2008; Och, 2003) on either
WMT10 or MT08. Evaluation sets are translated
using the cdec decoder (Dyer et al., 2010) and
evaluated with the BLEU metric (Papineni et al.,
2002). These results are listed as ?Base MERT?
in Table 3. To establish a baseline for our adap-
tive systems, we tune the same baseline system
using cutting-plane MIRA with 500-best lists, the
pseudo-document approximation described by Ei-
delman (2012), and a maximum update size of
0.01. We begin with uniform weights and make
20 passes over the development corpus. Results
for this system are listed as ?Base MIRA?.
To evaluate the impact of each online model
adaptation technique, we report the results for the
3
http://www.ted.com/talks
4
Derivation features consist of word count, discretized
rule-level non-terminal count (0, 1, or 2), glue rule count,
and out-of-vocabulary pass-through count.
News TED Talks
New Supp New Supp
Spanish?English 15% 19% 14% 18%
English?Spanish 12% 16% 9% 13%
Arabic?English 9% 12% 23% 28%
Table 5: Percentages of new rules (only seen
in incremental data) and post-edit supported rules
(Rules from all data for which the ?post-edit sup-
port ?f, e?? feature fires) in grammars by domain.
following systems in Table 3:
? G: Baseline MIRA system with online gram-
mar extraction, including incrementally up-
dating existing phrase features plus an addi-
tional indicator feature for post-edit support.
? L: Baseline MIRA with a trigram hierarchi-
cal Pitman-Yor process language model that
is incrementally updated, including a sepa-
rate out-of-vocabulary feature.
? M: Baseline MIRA with online feature
weight updates from cutting-plane MIRA.
Finally, we report results for a fully adaptive
system that includes online grammar, language
model, and feature weight updates. This system
is reported as ?G+L+M?. To account for optimizer
instability, all systems are tuned (consisting of
running either MERT or MIRA) and evaluated 3
times. We report average scores over optimizer
runs and conduct statistical significance tests us-
ing the methods described by Clark et al. (2011).
6.3 Results
Our simulated translation post-editing experi-
ments are summarized in Table 3. Simply mov-
ing from MERT to cutting-plane MIRA for pa-
rameter optimization yields improvement in most
cases, corroborating existing work (Eidelman,
2012). Using incremental post-editing data to up-
date translation grammars (G) yields further im-
provement in all cases evaluated. Gains are signif-
icantly larger for TED talks where translator feed-
back can bridge the gap between domains. Table 5
shows the aggregate percentages of rules in online
grammars that are entirely new (extracted from
post-editing instances only) or post-edit supported
(superset of new rules). While percentages vary
by data set, the overall trend is a combination of
learning new vocabulary and reordering and dis-
ambiguating existing translation choices.
The introduction of a trigram Bayesian lan-
guage model (L) yields mixed results: in some
400
Base MERT and changing the definition of what the Zona Cero is .
G+L+M and the changing definition of what the Ground Zero is .
Reference and the changing definition of what Ground Zero is .
Base MERT was that when we side by side comparisons with coal , timber
G+L+M was that when we did side-by-side comparisons with wood charcoal ,
Reference was when we did side-by-side comparisons with wood charcoal ,
Base MERT There was a way ? there was one ?
G+L+M There was a way ? there had to be a way ?
Reference There was a way ? there had to be a way ?
Table 4: Translation examples from baseline and fully adaptive systems of Spanish TED talks into En-
glish. Examples illustrate (from top to bottom) learning translations for new vocabulary items, selecting
correct translation candidates for the domain, and learning domain-appropriate phrasing.
cases it leads to slight improvement and in oth-
ers, degradation. It appears that a static but large
4-gram language model often outperforms an in-
crementally updated but smaller trigram model.
Further, learning a single weight for the Bayesian
model can lead to a harmful mismatch. As a tun-
ing pass over the development corpus proceeds,
the model incorporates additional data and MIRA
learns a weight corresponding to its predictive
ability at the end of the corpus. During decod-
ing, all sentences are translated with this language
model weight, even before the model can ade-
quately adapt itself to the target domain. This
problem is alleviated in our fully adaptive system.
Using cutting-plane MIRA to incrementally up-
date weights during decoding (M) also leads to
mixed results, frequently resulting in both small
increases and decreases in score. This could be
due to the noise incurred when making small ad-
justments to static features after each sentence:
depending on the similarity between the previous
and current sentence and the limit of the step size
(regularization strength), a parameter update may
slightly improve or degrade translation.
Finally, we see significantly larger gains for
our fully adaptive system (G+L+M) that com-
bines adaptive translation grammars and language
models with online parameter updates. In many
cases, the difference between the baseline sys-
tems and our adaptive system is greater than the
sum of the differences from our individual tech-
niques, demonstrating the effectiveness of com-
bining online learning methods. Our final sys-
tem has two key advantages over any individual
extension. First, incremental updates from MIRA
can rescale weights for features that change over
time, keeping the model consistent. Second, the
Bayesian language model?s out-of-vocabulary fea-
ture can discriminate between true OOV items
and vocabulary items in the post-editing data not
present in the monolingual data. By contrast, the
only OOVs in the baseline system are untranslated
items, as the target side of the bitext is included in
the language model training data. This interplay
between the adaptive components in our transla-
tion system leads to significant gains over MERT
and MIRA baselines. Table 4 contains examples
from our system?s output that exemplify key im-
provements in translation quality. With respect to
performance, our fully adaptive system translates
an average of 1.5 sentences per second per CPU
core. The additional cost incurred updating trans-
lation grammars and language models is less than
one second per sentence (though the baseline cost
of on-demand grammar extraction can be up to a
few seconds). In total, the system is well within
the acceptable speed range needed to function in
real-time human translation scenarios.
6.4 Evaluation Using Post-Edited References
The 2012 ACL Workshop on Machine Translation
(Callison-Burch et al., 2012) makes available a set
of 1832 English?Spanish parallel news source sen-
tences, independent references, initial MT outputs,
and post-edited MT outputs. The employed MT
system is trained on largely the same resources as
our own English?Spanish system, granting the op-
portunity for a much closer approximation to an
actual post-editing task; our system configurations
score between 54 and 56 BLEU against the sam-
ple MT, indicating that humans post-edited trans-
lations similar but not identical to our own. We
split the data into development and test sets, each
916 sentences, and run 3 iterations of optimizing
on the development set and evaluating on the test
set with both the MERT baseline and our G+L+M
401
system on both types of references. Using inde-
pendent references for tuning and evaluation (as
before), our system yields an improvement of 0.6
BLEU (23.3 to 23.9). With post-edited references,
our system yields an improvement of 1.3 BLEU
(43.0 to 44.3). This provides strong evidence that
our adaptive systems would provide better trans-
lations (both in terms of absolute quality and im-
provement over a standard baseline) for real-world
post-editing scenarios.
7 Related Work
Prior work has led to the extension of standard
phrase-based translation systems to make use of
incrementally available data.
5
Approaches gen-
erally fall into categories of adding new data to
translation models and of using incremental data
to adjust model parameters (feature weights). In
the first case, Nepveu et al. (2004) use cache-based
translation and language models to incorporate
data from the current document into a computer-
aided translation scenario. Ortiz-Mart??nez et al.
(2010) augment a standard translation model by
storing sufficient statistics in addition to feature
scores for phrase pairs, allowing feature values to
be incrementally updated as new sentence pairs
are available for phrase extraction. Hardt and Elm-
ing (2010) demonstrate the benefit of maintain-
ing a distinction between background and post-
editing data in an adaptive model with simulated
post-editing. Though not targeted at post-editing
applications, the most similar work to our online
grammar adaptation is the stream-based transla-
tion model described by Levenberg et al. (2010).
The authors introduce a dynamic suffix array that
can incorporate new training text as it becomes
available. Sanchis-Trilles (2012) proposes a strat-
egy for online language model adaptation wherein
several smaller domain-specific models are built
and their scores interpolated for each sentence
translated based on the target domain.
Focusing on incrementally updating model pa-
rameters with post-editing data, Mart??nez-G?omez
et al. (2012) and L?opez-Salcedo et al. (2012)
show improvement under some conditions when
using techniques including passive-aggressive al-
gorithms, perceptron, and discriminative ridge re-
gression to adapt feature weights for systems ini-
tially tuned using MERT. This work also uses ref-
erence translations to simulate post-editing. Saluja
5
Prior to phrase-based systems, NISHIDA et al. (1988)
use post-editing data to correct errors in transfer-based MT.
et al. (2012) introduce a support vector machine-
based algorithm capable of learning from binary-
labeled examples. This learning algorithm is used
to incrementally adjust feature weights given user
feedback on whether a translation is ?good? or
?bad?. As with our work, this strategy can be used
during both optimization and decoding.
Finally, Simard and Foster (2013) apply a
pipeline solution to the post-editing task wherein
a second stage automatic post-editor (APE) sys-
tem learns to replicate the corrections made to ini-
tial MT output by human translators. As incre-
mental data accumulates, the APE (itself a statisti-
cal phrase-based system) attempts to ?correct? the
MT output before it is shown to humans.
8 Conclusion
Casting machine translation for post-editing as
an online learning task, we have presented three
methods for incremental model adaptation: adding
data to the indexed bitext from which gram-
mars are extracted, updating a Bayesian language
model with incremental data, and using an on-
line discriminative parameter update during de-
coding. These methods, which allow the sys-
tem to handle all data in a uniform way, are ap-
plied to a strong baseline system optimized using
MIRA in conjunction with simulated post-editing.
In addition to showing gains for individual meth-
ods under various circumstances, we report super-
additive improvement from combining our tech-
niques to produce a fully adaptive system. Im-
provements generalize over language and data sce-
narios, with the greatest gains realized in blind
out-of-domain tasks where the system must rely
heavily on post-editor feedback to improve qual-
ity. Gains are also more significant when using of-
fline post-edited references, showing promise for
applying our techniques to real-world post-editing
tasks. All software used for our online model
adaptation experiments is freely available under an
open source license as part of the cdec toolkit.
6
Acknowledgements
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
6
http://www.cs.cmu.edu/
?
mdenkows/
cdec-realtime.html
402
References
[Callison-Burch et al.2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada, June. Association for Computational
Linguistics.
[Carl et al.2011] Michael Carl, Barbara Dragsted,
Jakob Elming, Daniel Hardt, and Arnt Lykke
Jakobsen. 2011. The process of post-editing: A
pilot study. Copenhagen Studies in Language,
41:131?142.
[Cettolo et al.2012] Mauro Cettolo, Christian Girardi,
and Marcello Federico. 2012. Wit
3
: Web inventory
of transcribed and translated talks. In Proceedings
of the Sixteenth Annual Conference of the European
Association for Machine Translation.
[Chiang2007] David Chiang. 2007. Hierarchical
phrase-based translation. Computational Linguis-
tics, 33.
[Chiang2012] David Chiang. 2012. Hope and fear for
discriminative training of statistical translation mod-
els. Journal of Machine Learning Research, pages
1159?1187, April.
[Clark et al.2011] Jonathan H. Clark, Chris Dyer, Alon
Lavie, and Noah A. Smith. 2011. Better hypothe-
sis testing for statistical machine translation: Con-
trolling for optimizer instability. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 176?181, Portland, Oregon, USA, June.
Association for Computational Linguistics.
[Crammer et al.2006] Koby Crammer, Ofer Dekel,
Joseph Keshet, Shai Shalev-Shwartz, and Yoram
Singer. 2006. Online passive-aggressive algo-
rithms. Journal of Machine Learning Research,
pages 551?558, March.
[Dyer et al.2010] Chris Dyer, Adam Lopez, Juri Gan-
itkevitch, Jonathan Weese, Ferhan Ture, Phil Blun-
som, Hendra Setiawan, Vladimir Eidelman, and
Philip Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proceedings of the ACL
2010 System Demonstrations, pages 7?12, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
[Dyer et al.2013] Chris Dyer, Victor Chahuneau, and
Noah A. Smith. 2013. A simple, fast, and effective
reparameterization of IBM model 2. In The 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
[Eidelman2012] Vladimir Eidelman. 2012. Optimiza-
tion strategies for online large-margin learning in
machine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
480?489, Montr?eal, Canada, June. Association for
Computational Linguistics.
[Guerberof2009] Ana Guerberof. 2009. Productivity
and quality in mt post-editing. In Proceedings of MT
Summit XII - Workshop: Beyond Translation Memo-
ries: New Tools for Translators MT.
[Hardt and Elming2010] Daniel Hardt and Jakob Elm-
ing. 2010. Incremental re-training for post-editing
smt. In Proceedings of the Ninth Conference of the
Association for Machine Translation in the Ameri-
cas.
[Heafield et al.2013] Kenneth Heafield, Ivan
Pouzyrevsky, Jonathan H. Clark, and Philipp
Koehn. 2013. Scalable modified Kneser-Ney
language model estimation. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics, Sofia, Bulgaria, August.
[Hopkins and May2011] Mark Hopkins and Jonathan
May. 2011. Tuning as ranking. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1352?1362, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
[Koehn2012] Philipp Koehn. 2012. Computer-aided
translation. Machine Translation Marathon.
[Kuhn and de Mori1990] Roland Kuhn and Renato
de Mori. 1990. A cache-based natural language
model for speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
12(6).
[Levenberg et al.2010] Abby Levenberg, Chris
Callison-Burch, and Miles Osborne. 2010.
Stream-based translation models for statistical
machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 394?402, Los Angeles,
California, June. Association for Computational
Linguistics.
[Lopez2008a] Adam Lopez. 2008a. Machine transla-
tion by pattern matching. In Dissertation, Univer-
sity of Maryland, March.
[Lopez2008b] Adam Lopez. 2008b. Tera-scale transla-
tion models via pattern matching. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 505?512,
Manchester, UK, August. Coling 2008 Organizing
Committee.
[L?opez-Salcedo et al.2012] Francisco-Javier L?opez-
Salcedo, Germ?an Sanchis-Trilles, and Francisco
Casacuberta. 2012. Online learning of log-linear
weights in interactive machine translation. Ad-
vances in Speech and Language Technologies for
Iberian Languages, pages 277?286.
403
[Macherey et al.2008] Wolfgang Macherey, Franz Och,
Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-
based minimum error rate training for statistical ma-
chine translation. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 725?734, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
[Manber and Myers1993] Udi Manber and Gene My-
ers. 1993. Suffix arrays: A new method for on-
line string searches. SIAM Journal of Computing,
22:935?948.
[Mart??nez-G?omez et al.2012] Pascual Mart??nez-
G?omez, Germ?an Sanchis-Trilles, and Francisco
Casacuberta. 2012. Online adaptation strategies
for statistical machine translation in post-editing
scenarios. Pattern Recognition, 45:3193?3203.
[Nepveu et al.2004] Laurent Nepveu, Guy Lapalme,
Philippe Langlais, and George Foster. 2004. Adap-
tive language and translation models for interactive
machine translation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 190?
197, Barcelona, Spain, July. Association for Com-
putational Linguistics.
[NISHIDA et al.1988] Fujio NISHIDA, Shinobu
TAKAMATSU, Tadaaki TANI, and Tsunehisa
DOI. 1988. Feedback of correcting information
in postediting to a machine translation system. In
Proc. of COLING.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 160?167,
Sapporo, Japan, July. Association for Computational
Linguistics.
[Ortiz-Mart??nez et al.2010] Daniel Ortiz-Mart??nez, Is-
mael Garc??a-Varea, and Francisco Casacuberta.
2010. Online learning for interactive statistical ma-
chine translation. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 546?554, Los Ange-
les, California, June. Association for Computational
Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 311?318, Philadelphia, Pennsylvania, USA,
July. Association for Computational Linguistics.
[Parker et al.2011] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2011. En-
glish Gigaword Fifth Edition, June. Linguistic Data
Consortium, LDC2011T07.
[Przybocki2012] Mark Przybocki. 2012. Nist open
machine translation 2012 evaluation (openmt12).
http://www.nist.gov/itl/iad/mig/openmt12.cfm.
[Saluja et al.2012] Avneesh Saluja, Ian Lane, and Ying
Zhang. 2012. Machine translation with binary feed-
back: a large-margin approach. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas.
[Sanchis-Trilles2012] Germ?an Sanchis-Trilles. 2012.
Building task-oriented machine translation systems.
In Ph.D. Thesis, Universitat Politcnica de Valncia.
[Simard and Foster2013] Michel Simard and George
Foster. 2013. PEPr: Post-edit propagation using
phrase-based statistical machine translation. In Pro-
ceedings of the XIV Machine Translation Summit,
pages 191?198,, September.
[Tatsumi2009] Midori Tatsumi. 2009. Correlation
between automatic evaluation metric scores, post-
editing speed, and some other factors. In Proceed-
ings of the Twelfth Machine Translation Summit.
[Teh2006] Yee Whye Teh. 2006. A hierarchical
Bayesian language model based on Pitman-Yor pro-
cesses. In Proc. of ACL.
[Zhao et al.2004] Bing Zhao, Matthias Eck, and
Stephan Vogel. 2004. Language model adaptation
for statistical machine translation with structured
query models. In Proc. of COLING.
[Zhechev2012] Ventsislav Zhechev. 2012. Machine
Translation Infrastructure and Post-editing Perfor-
mance at Autodesk. In AMTA 2012 Workshop
on Post-Editing Technology and Practice (WPTP
2012), pages 87?96, San Diego, USA, October. As-
sociation for Machine Translation in the Americas
(AMTA).
404
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 250?253,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extending the METEOR Machine Translation Evaluation Metric to the
Phrase Level
Michael Denkowski and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper presents METEOR-NEXT, an ex-
tended version of the METEOR metric de-
signed to have high correlation with post-
editing measures of machine translation qual-
ity. We describe changes made to the met-
ric?s sentence aligner and scoring scheme as
well as a method for tuning the metric?s pa-
rameters to optimize correlation with human-
targeted Translation Edit Rate (HTER). We
then show that METEOR-NEXT improves cor-
relation with HTER over baseline metrics, in-
cluding earlier versions of METEOR, and ap-
proaches the correlation level of a state-of-the-
art metric, TER-plus (TERp).
1 Introduction
Recent focus on the need for accurate automatic
metrics for evaluating the quality of machine trans-
lation output has spurred much development in the
field of MT. Workshops such as WMT09 (Callison-
Burch et al, 2009) and the MetricsMATR08 chal-
lenge (Przybocki et al, 2008) encourage the devel-
opment of new MT metrics and reliable human judg-
ment tasks.
This paper describes our work extending the ME-
TEOR metric to improve correlation with human-
targeted Translation Edit Rate (HTER) (Snover et
al., 2006), a semi-automatic post-editing based met-
ric which measures the distance between MT out-
put and a targeted reference. We identify several
limitations of the original METEOR metric and de-
scribe our modifications to improve performance on
this task. Our extended metric, METEOR-NEXT, is
then tuned to maximize segment-level correlation
with HTER scores and tested against several base-
line metrics. We show that METEOR-NEXT outper-
forms earlier versions of METEOR when tuned to the
same HTER data and approaches the performance of
a state-of-the-art TER-based metric, TER-plus.
2 The METEOR-NEXT Metric
2.1 Traditional METEOR Scoring
Given a machine translation hypothesis and a refer-
ence translation, the traditional METEOR metric cal-
culates a lexical similarity score based on a word-
to-word alignment between the two strings (Baner-
jee and Lavie, 2005). When multiple references are
available, the hypothesis is scored against each and
the reference producing the highest score is used.
Alignments are built incrementally in a series of
stages using the following METEOR matchers:
Exact: Words are matched if and only if their sur-
face forms are identical.
Stem: Words are stemmed using a language-
appropriate Snowball Stemmer (Porter, 2001) and
matched if the stems are identical.
Synonym: Words are matched if they are both
members of a synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database. This
matcher is limited to translations into English.
At each stage, one of the above matchers iden-
tifies all possible word matches between the two
translations using words not aligned in previous
stages. An alignment is then identified as the largest
subset of these matches in which every word in each
sentence aligns to zero or one words in the other sen-
250
tence. If multiple such alignments exist, the align-
ment is chosen that best preserves word order by
having the fewest crossing alignment links. At the
end of each stage, matched words are fixed so that
they are not considered in future stages. The final
alignment is defined as the union of all stage align-
ments.
Once an alignment has been constructed, the to-
tal number of unigram matches (m), the number of
words in the hypothesis (t), and the number of words
in the reference (r) are used to calculate precision
(P = m/t) and recall (R = m/r). The parame-
terized harmonic mean of P and R (van Rijsbergen,
1979) is then calculated:
Fmean =
P ?R
? ? P + (1 ? ?) ?R
To account for differences in word order, the min-
imum number of ?chunks? (ch) is calculated where a
chunk is defined as a series of matched unigrams that
is contiguous and identically ordered in both sen-
tences. The fragmentation (frag = ch/m) is then
used to calculate a fragmentation penalty:
Pen = ? ? frag?
The final METEOR score is then calculated:
Score = (1 ? Pen) ? Fmean
The free parameters ?, ?, and ? can be tuned to
maximize correlation with various types of human
judgments (Lavie and Agarwal, 2007).
2.2 Extending the METEOR Aligner
Traditional METEOR is limited to unigram matches,
making it strictly a word-level metric. By focus-
ing on only one match type per stage, the aligner
misses a significant part of the possible alignment
space. Further, selecting partial alignments based
only on the fewest number of per-stage crossing
alignment links can in practice lead to missing full
alignments with the same number of matches in
fewer chunks. Our extended aligner addresses these
limitations by introducing support for multiple-word
phrase matches and considering all possible matches
in a single alignment stage.
We introduce an additional paraphrase matcher
which matches phrases (one or more successive
words) if one phrase is considered a paraphrase of
the other by a paraphrase database. For English, we
use the paraphrase database developed by Snover et
al. (2009), using techniques presented by Bannard
and Callison-Burch (2005).
The extended aligner first constructs a search
space by applying all matchers in sequence to iden-
tify all possible matches between the hypothesis and
reference. To reduce redundant matches, stem and
synonym matches between pairs of words which
have already been identified as exact matches are not
considered. Matches have start positions and lengths
in both sentences; a word occurring less than length
positions after a match start is said to be covered by
the match. As exact, stem, and synonym matches
will always have length one in both sentences, they
can be considered phrase matches of length one.
Since other matches can cover phrases of different
lengths in the two sentences, matches are now said
to be one-to-one at the phrase level rather than the
word level.
Once all possible matches have been identified,
the aligner identifies the final alignment as the
largest subset of these matches meeting the follow-
ing criteria in order of importance:
1. Each word in each sentence is covered by zero
or one matches
2. Largest number of covered words across both
sentences
3. Smallest number of chunks, where a chunk is
now defined as a series of matched phrases that
is contiguous and identically ordered in both
sentences
4. Smallest sum of absolute distances between
match start positions in the two sentences (pre-
fer to align words and phrases that occur at sim-
ilar positions in both sentences)
The resulting alignment is selected from the full
space of possible alignments and directly optimizes
the statistics on which the the final score will be cal-
culated.
2.3 Extended METEOR Scoring
Once an alignment has been chosen, the METEOR-
NEXT score is calculated using extended versions of
251
the traditional METEOR statistics. We also introduce
a tunable weight vector used to dictate the relative
contribution of each match type. The extended ME-
TEOR score is calculated as follows.
The number of words in the hypothesis (t) and
reference (r) are counted. For each of the match-
ers (mi), count the number of words covered by
matches of this type in the hypothesis (mi(t)) and
reference (mi(r)) and apply the appropriate module
weight (wi). The weighted Precision and Recall are
then calculated:
P =
?
iwi ?mi(t)
t
R =
?
iwi ?mi(r)
r
The minimum number of chunks (ch) is then cal-
culated using the new chunk definition. Once P , R,
and ch are calculated, the remaining statistics and
final score can be calculated as in Section 2.1.
3 Tuning for Post-Editing Measures of
Quality
Human-targeted Translation Edit Rate (HTER)
(Snover et al, 2006), is a semi-automatic assessment
of machine translation quality based on the number
of edits required to correct translation hypotheses. A
human annotator edits each MT hypothesis so that it
is meaning-equivalent with a reference translation,
with an emphasis on making the minimum possible
number of edits. The Translation Edit Rate (TER)
is then calculated using the human-edited transla-
tion as a targeted reference for the MT hypothe-
sis. The resulting scores are shown to correlate well
with other types of human judgments (Snover et al,
2006).
3.1 Tuning Toward HTER
The GALE (Olive, 2005) Phase 2 unsequestered
data includes HTER scores for multiple Arabic-to-
English and Chinese-to-English MT systems. We
used HTER scores for 10838 segments from 1045
documents from this data set to tune both the orig-
inal METEOR and METEOR-NEXT. Both were ex-
haustively tuned to maximize the length-weighted
segment-level Pearson?s correlation with the HTER
scores. This produced globally optimal ?, ?, and ?
values for METEOR and optimal ?, ?, ? values plus
stem, synonym, and paraphrase match weights for
Task ? ? ?
Adequacy & Fluency 0.81 0.83 0.28
Ranking 0.95 0.50 0.50
HTER 0.70 1.95 0.50
HTER (extended) 0.65 1.95 0.45
Stem Syn Par
0 0.4 0.9
Table 1: Parameter values for various METEOR tasks for
translations into English.
METEOR-NEXT (with the weight of exact matches
fixed at 1). Table 1 compares the new HTER pa-
rameters to those tuned for other tasks including ad-
equacy and fluency (Lavie and Agarwal, 2007) and
ranking (Agarwal and Lavie, 2008).
As observed by Snover et al (2009), HTER
prefers metrics which are more balanced between
precision and recall: this results in the lowest values
of ? for any task. Additionally, non-exact matches
receive lower weights, with stem matches receiving
zero weight. This reflects a weakness in HTER scor-
ing where words with matching stems are treated as
completely dissimilar, requiring full word substitu-
tions (Snover et al, 2006).
4 Experiments
The GALE (Olive, 2005) Phase 3 unsequestered
data includes HTER scores for Arabic-to-English
MT output. We created a test set from HTER scores
of 2245 segments from 195 documents in this data
set. Our evaluation metric (METEOR-NEXT-hter)
was tested against the following established metrics:
BLEU (Papineni et al, 2002) with a maximum N -
gram length of 4, TER (Snover et al, 2006), versions
of METEOR based on release 0.7 tuned for adequacy
and fluency (METEOR-0.7-af) (Lavie and Agarwal,
2007), ranking (METEOR-0.7-rank) (Agarwal and
Lavie, 2008), and HTER (METEOR-0.7-hter). Also
included is the HTER-tuned version of TER-plus
(TERp-hter), a metric with state-of-the-art perfor-
mance in recent evaluations (Snover et al, 2009).
Length-weighted Pearson?s and Spearman?s correla-
tion are shown for all metrics at both the segment
(Table 2) and document level (Table 3). System level
correlations are not shown as the Phase 3 data only
contained the output of 2 systems.
252
Metric Pearson?s r Spearman?s ?
BLEU-4 -0.496 -0.510
TER 0.539 0.510
METEOR-0.7-af -0.573 -0.561
METEOR-0.7-rank -0.561 -0.556
METEOR-0.7-hter -0.574 -0.562
METEOR-NEXT-hter -0.600 -0.581
TERp-hter 0.627 0.610
Table 2: Segment level correlation with HTER.
Metric Pearson?s r Spearman?s ?
BLEU-4 -0.689 -0.686
TER 0.675 0.679
METEOR-0.7-af -0.696 -0.699
METEOR-0.7-rank -0.691 -0.693
METEOR-0.7-hter -0.704 -0.705
METEOR-NEXT-hter -0.719 -0.713
TERp-hter 0.738 0.747
Table 3: Document level correlation with HTER.
METEOR-NEXT-hter outperforms all baseline
metrics at both the segment and document level.
Bootstrap sampling indicates that the segment-level
correlation improvements of 0.026 in Pearson?s r
and 0.019 in Spearman?s ? over METEOR-0.7-hter
are statistically significant at the 95% level. TERp?s
correlation with HTER is still significantly higher
across all categories. Our metric does run signifi-
cantly faster than TERp, scoring approximately 120
segments per second to TERp?s 3.8.
5 Conclusions
We have presented an extended METEOR metric
which shows higher correlation with HTER than
baseline metrics, including traditional METEOR
tuned on the same data. Our extensions are not
specific to HTER tasks; improved alignments and
additional features should improve performance on
any task having sufficient tuning data. Although our
metric does not outperform TERp, it should be noted
that HTER incorporates TER alignments, providing
TER-based metrics a natural advantage. Our metric
also scores segments relatively quickly, making it a
viable choice for tuning MT systems.
Acknowledgements
This work was funded in part by NSF grants IIS-
0534932 and IIS-0915327.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, m-bleu
and m-ter: Evaluation Metrics for High-Correlation
with Human Rankings of Machine Translation Output.
In Proc. of WMT08, pages 115?118.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
of the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL05, pages 597?604.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Proc.
of WMT09, pages 1?28.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Lev-
els of Correlation with Human Judgments. In Proc. of
WMT07, pages 228?231.
George Miller and Christiane Fellbaum. 2007. WordNet.
http://wordnet.princeton.edu/.
Joseph Olive. 2005. Global Autonomous Language Ex-
ploitation (GALE). DARPA/IPTO Proposer Informa-
tion Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL02,
pages 311?318.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
M. Przybocki, K. Peterson, and S Bronsart. 2008.
Official results of the NIST 2008 "Metrics for
MAchine TRanslation" Challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA-2006, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with a
Tunable MT Metric. In Proc. of WMT09, pages 259?
268.
C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.
253
Proceedings of NAACL-HLT 2013, pages 288?297,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving Syntax-Augmented Machine Translation by
Coarsening the Label Set
Greg Hanneman and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema,alavie}@cs.cmu.edu
Abstract
We present a new variant of the Syntax-
Augmented Machine Translation (SAMT) for-
malism with a category-coarsening algorithm
originally developed for tree-to-tree gram-
mars. We induce bilingual labels into the
SAMT grammar, use them for category coars-
ening, then project back to monolingual la-
beling as in standard SAMT. The result is a
?collapsed? grammar with the same expres-
sive power and format as the original, but
many fewer nonterminal labels. We show that
the smaller label set provides improved trans-
lation scores by 1.14 BLEU on two Chinese?
English test sets while reducing the occur-
rence of sparsity and ambiguity problems
common to large label sets.
1 Introduction
The formulation of statistical machine translation in
terms of synchronous parsing has become both the-
oretically and practically successful. In a parsing-
based MT formalism, synchronous context-free
grammar rules that match a source-language input
can be hierarchically composed to produce a corre-
sponding target-language output. SCFG translation
grammars can be extracted automatically from data.
While formally syntactic approaches with a single
grammar nonterminal have often worked well (Chi-
ang, 2007), the desire to exploit linguistic knowl-
edge has motivated the use of translation grammars
with richer, linguistically syntactic nonterminal in-
ventories (Galley et al, 2004; Liu et al, 2006; Lavie
et al, 2008; Liu et al, 2009).
Linguistically syntactic MT systems can derive
their label sets, either monolingually or bilingually,
from parallel corpora that have been annotated with
source- and/or target-side parse trees provided by
a statistical parser. The MT system may exactly
adopt the parser?s label set or modify it in some way.
Larger label sets are able to represent more precise,
fine-grained categories. On the other hand, they also
exacerbate a number of computational and modeling
problems by increasing grammar size, derivational
ambiguity, and data sparsity.
In this paper, we focus on the Syntax-Augmented
MT formalism (Zollmann and Venugopal, 2006), a
monolingually labeled version of Hiero that can cre-
ate up to 4000 ?extended? category labels based on
pairs of parse nodes. We take a standard SAMT
grammar with target-side labels and extend its label-
ing to a bilingual format (Zollmann, 2011). We then
coarsen the bilingual labels following the ?label col-
lapsing? algorithm of Hanneman and Lavie (2011).
This represents a novel extension of the tree-to-tree
collapsing algorithm to the SAMT formalism. Af-
ter removing the source-side labels, we obtain a new
SAMT grammar with coarser target-side labels than
the original.
Coarsened grammars provide improvement of up
to 1.14 BLEU points over the baseline SAMT results
on two Chinese?English test sets; they also outper-
form a Hiero baseline by up to 0.60 BLEU on one
of the sets. Aside from improved translation quality,
in analysis we find significant reductions in deriva-
tional ambiguity and rule sparsity, two problems that
make large nonterminal sets difficult to work with.
Section 2 provides a survey of large syntax-based
288
MT label sets, their associated problems of deriva-
tional ambiguity and rule sparsity, and previous at-
tempts at addressing those problems. The section
also summarizes the tree-to-tree label collapsing al-
gorithm and the process of SAMT rule extraction.
We then describe our method of label collapsing in
SAMT grammars in Section 3. Experimental results
are presented in Section 4 and analyzed in Section
5. Finally, Section 6 offers some conclusions and
avenues for future work.
2 Background
2.1 Working with Large Label Sets
Aside from the SAMT method of grammar extrac-
tion, which we treat more fully in Section 2.3, sev-
eral other lines of work have explored increasing
the nonterminal set for syntax-based MT. Huang and
Knight (2006), for example, augmented the standard
Penn Treebank labels for English by adding lexi-
calization to certain types of nodes. Chiang (2010)
and Zollmann (2011) worked with a bilingual exten-
sion of SAMT that used its notion of ?extended cat-
egories? on both the source and target sides. Taking
standard monolingual SAMT as a baseline, Baker et
al. (2012) developed a tagger to augment syntactic
labels with some semantically derived information.
Ambati et al (2009) extracted tree-to-tree rules with
similar extensions for sibling nodes, resulting again
in a large number of labels.
Extended categories allow for the extraction of
a larger number of rules, increasing coverage and
translation performance over systems that are lim-
ited to exact constituent matches only. However,
the gains in coverage come with a corresponding
increase in computational and modeling complexity
due to the larger label set involved.
Derivational ambiguity ? the condition of hav-
ing multiple derivations for the same output string
? is a particular problem for parsing-based MT sys-
tems. The same phrase pair may be represented with
a large number of different syntactic labels. Fur-
ther, new hierarchical rules are created by abstract-
ing smaller phrase pairs out of larger ones; each of
these substitutions must also be marked by a label
of some kind. Keeping variantly labeled copies of
the same rules fragments probabilities during gram-
mar scoring and creates redundant hypotheses in the
decoder at run time.
A complementary problem ? when a desired rule
application is impossible because its labels do not
match ? has been variously identified as ?data spar-
sity,? the ?matching constraint,? and ?rule sparsity?
in the grammar. It arises from the definition of
SCFG rule application: in order to compose two
rules, the left-hand-side label of the smaller rule
must match a right-hand-side label in the larger rule
it is being plugged in to. With large label sets, it
becomes less likely that two arbitrarily chosen rules
can compose, making the grammar less flexible for
representing new sentences.
Previous research has attempted to address both
of these problems in different ways. Preference
grammars (Venugopal et al, 2009) are a technique
for reducing derivational ambiguity by summing
scores over labeled variants of the same deriva-
tion during decoding. Chiang (2010) addressed rule
sparsity by introducing a soft matching constraint:
the decoder may pay a learned label-pair-specific
penalty for substituting a rule headed by one label
into a substitution slot marked for another. Combin-
ing properties of both of the above methods, Huang
et al (2010) modeled monolingual labels as distribu-
tions over latent syntactic categories and calculated
similarity scores between them for rule composition.
2.2 Label Collapsing in Tree-to-Tree Rules
Aiming to reduce both derivational ambiguity and
rule sparsity, we previously presented a ?label col-
lapsing? algorithm for systems in which bilingual
labels are used (Hanneman and Lavie, 2011). It
coarsens the overall label set by clustering monolin-
gual labels based on which labels they appear joined
with in the other language.
The label collapsing algorithm takes as its input
a set of SCFG rule instances extracted from a par-
allel corpus. Each time a tree-to-tree rule is ex-
tracted, its left-hand side is a label of the form s::t,
where s is a label from the source-language cate-
gory set S and t is a label from the target-language
category set T . Operationally, the joint label means
that a source-side subtree rooted at s was the trans-
lational equivalent of a target-side subtree rooted at
t in a parallel sentence. Figure 1 shows several such
subtrees, highlighted in grey and numbered. Joint
left-hand-side labels for the collapsing algorithm,
289
Figure 1: Sample extraction of bilingual nonterminals for
label collapsing. Labels extracted from this tree pair in-
clude VBD::VV and NP::AD.
such as VBD::VV and NP::AD, can be assembled
by matching co-numbered nodes.
From the counts of the extracted rules, it is thus
straightforward to compute for all values of s and
t the observed P (s | t) and P (t | s), the probability
of one half of a joint nonterminal label appearing
in the grammar given the other half. In the figure,
for example, P (JJ |NN) = 0.5. The conditional
probabilities accumulated over the whole grammar
give rise to a simple L1 distance metric over any pair
of monolingual labels:
d(s1, s2) =
?
t?T
|P (t | s1)? P (t | s2)| (1)
d(t1, t2) =
?
s?S
|P (s | t1)? P (s | t2)| (2)
An agglomerative clustering algorithm then com-
bines labels in a series of greedy iterations. At each
step, the algorithm finds the pair of labels that is cur-
rently the closest together according to the distance
metrics of Equations (1) and (2), combines those two
labels into a new one, and updates the set of P (s | t)
and P (t | s) values appropriately. The choice of la-
bel pair to collapse in each iteration can be expressed
formally as
argmin
(si,sj)?S2,(tk,t`)?T 2
{d(si, sj), d(tk, t`)} (3)
That is, either a source label pair or a target label pair
may be chosen by the algorithm in each iteration.
2.3 SAMT Rule Extraction
SAMT grammars pose a challenge to the label col-
lapsing algorithm described above because their la-
bel sets are usually monolingual. The classic SAMT
formulation (Zollmann and Venugopal, 2006) pro-
duces a grammar labeled on the target side only.
Nonterminal instances that exactly match a target-
language syntactic constituent in a parallel sentence
are given labels of the form t. Labels of the form
t1+t2 are assigned to nonterminals that span exactly
two contiguous parse nodes. Categorial grammar la-
bels such as t1/t2 and t1\t2 are given to nontermi-
nals that span an incomplete t1 constituent missing
a t2 node to its right or left, respectively. Any non-
terminal that cannot be labeled by one of the above
three schemes is assigned the default label X.
Figure 2(a) shows the extraction of a VP-level
SAMT grammar rule from part of a parallel sen-
tence. At the word level, the smaller English phrase
supported each other (and its Chinese equivalent) is
being abstracted as a nonterminal within the larger
phrase supported each other in international affairs.
The larger phrase corresponds to a parsed VP node
on the target side; this will become the label of
the extracted rule?s left-hand side. Since the ab-
stracted sub-phrase does not correspond to a single
constituent, the SAMT labeling conventions assign
it the label VBD+NP. We can thus write the ex-
tracted rule as:
(4)
While the SAMT label formats can be trivially
converted into joint labels X::t, X::t1+t2, X::t1/t2,
X::t1\t2, and X::X, they cannot be usefully fed into
the label collapsing algorithm because the necessary
conditional label probabilities are meaningless. To
acquire meaningful source-side labels, we turn to a
290
(a) (b)
Figure 2: Sample extraction of an SAMT grammar rule: (a) with monolingual syntax and (b) with bilingual syntax.
bilingual SAMT extension used by Chiang (2010)
and Zollmann (2011). Both a source- and a target-
side parse tree are used to extract rules from a par-
allel sentence; two SAMT-style labels are worked
out independently on each side for each nonterminal
instance, then packed into a joint label. It is there-
fore possible for a nonterminal instance to be labeled
s::t, s1\s2::t, s1+s2::t1/t2, or various other combi-
nations depending on what parse nodes the nonter-
minal spans in each tree.
Such a bilingually labeled rule is extracted in Fig-
ure 2(b). The target-side labels from Figure 2(a) are
now paired with source-side labels extracted from an
added Chinese parse tree. In this case, the abstracted
sub-phrase supported each other is given the joint
label VP::VBD+NP, while the rule?s left-hand side
becomes LCP+VP::VP.
We implement bilingual SAMT grammar extrac-
tion by modifying Thrax (Weese et al, 2011), an
open-source, Hadoop-based framework for extract-
ing standard SAMT grammars. By default, Thrax
can produce grammars labeled either on the source
or target side, but not both. It also outputs rules
that are already scored according to a user-specified
set of translation model features, meaning that the
raw rule counts needed to compute the label condi-
tional probabilities P (s | t) and P (t | s) are not di-
rectly available. We implement a new subclass of
grammar extractor with logic for independently la-
beling both sides of an SAMT rule in order to get the
necessary bilingual labels; an adaptation to the exist-
ing Thrax ?rarity? feature provides the rule counts.
3 Label Collapsing in SAMT Rules
Our method of producing label-collapsed SAMT
grammars is shown graphically in Figure 3.
We first obtain an SAMT grammar with bilingual
labels, together with the frequency count for each
rule, using the modified version of Thrax described
in Section 2.3. The rules can be grouped according
to the target-side label of their left-hand sides (Fig-
ure 3(a)).
The rule counts are then used to compute label-
ing probabilities P (s | t) and P (t | s) over left-hand-
side usages of each source label s and each target
label t. These are simple maximum-likelihood es-
timates: if #(si, tj) represents the combined fre-
quency counts of all rules with si::tj on the left-hand
291
(a) (b) (c) (d)
Figure 3: Stages of preparing label-collapsed rules for SAMT grammars. (a) SAMT rules with bilingual nonterminals
are extracted and collected based on their target left-hand sides. (b) Probabiliites P (t | s) and P (s | s) are computed. (c)
Nonterminals are clustered according to the label collapsing algorithm. (d) Source sides of nonterminals are removed
to create a standard SAMT grammar.
side, the source-given-target labeling probability is:
P (si | tj) =
#(si::tj)
?
t?T #(si::t)
(5)
The computation for target given source is analo-
gous. Each monolingual label can thus be repre-
sented as a distribution over the labels it is aligned
to in the opposite language (Figure 3(b)).
Such distributions over labels are the input to the
label-collapsing algorithm, as described in Section
2.2. As shown in Figure 3(c), the algorithm results
in the original target-side labels being combined into
different groups, denoted in this case as new labels
CA and CB. We run label collapsing for varying
numbers of iterations to produce varying degrees of
coarsened label sets.
Given a mapping from original target-side labels
to collapsed groups, all nonterminals in the original
SAMT grammar are overwritten accordingly. The
source-side labels are dropped at this point: we use
them only for the purpose of label collapsing, but not
in assembling or scoring the final grammar. The re-
sulting monolingual SAMT-style grammar with col-
lapsed labels (Figure 3(d)) can now be scored and
used for decoding in the usual way.
For constructing a baseline SAMT grammar with-
out label collapsing, we merely extract a bilingual
grammar as in the first step of Figure 3, immediately
remove the source-side labels from it, and proceed
to grammar scoring.
All grammars are scored according to a set of
eight features. For an SCFG rule with left-hand-side
label t, source right-hand side f , and target right-
hand side e, they are:
? Standard maximum-likelihood phrasal transla-
tion probabilities P (f | e) and P (e | f)
? Maximum-likelihood labeling probability
P (t | f, e)
? Lexical translation probabilities Plex(f | e) and
Plex(e | f), as calculated by Thrax
? Rarity score
exp( 1c )?1
exp(1)?1 for a rule with extracted
count c
? Binary indicator features that mark phrase pair
(as opposed to hierarchical) rules and glue rules
Scored grammars are filtered down to the sen-
tence level, retaining only those rules whose source-
side terminals match an individual tuning or testing
sentence. In addition to losslessly filtering gram-
mars in this way, we also carry out two types of
lossy pruning in order to reduce overall grammar
292
System Labels Rules Per Sent.
SAMT 4181 69,401,006 48,444
Collapse 1 913 64,596,618 35,004
Collapse 2 131 60,526,479 24,510
Collapse 3 72 58,483,310 20,445
Hiero 1 36,538,657 7,738
Table 1: Grammar statistics for different degrees of label
collapsing: number of target-side labels, unique rules in
the whole grammar, and average number of pruned rules
after filtering to individual sentences.
size. One pruning pass keeps only the 80 most fre-
quently observed target right-hand sides for each
source right-hand side. A second pass globally re-
moves hierarchical rules that were extracted fewer
than six times in the training data.
4 Experiments
We conduct experiments on Chinese-to-English MT,
using systems trained from the FBIS corpus of ap-
proximately 302,000 parallel sentence pairs. We
parse both sides of the training data with the Berke-
ley parsers (Petrov and Klein, 2007) for Chinese
and English. The English side is lowercased after
parsing; the Chinese side is segmented beforehand.
Unidirectional word alignments are obtained with
GIZA++ (Och and Ney, 2003) and symmetrized, re-
sulting in a parallel parsed corpus with Viterbi word
alignments for each sentence pair. Our modified ver-
sion of Thrax takes the parsed and aligned corpus as
input and returns a list of rules, which can then be
label-collapsed and scored as previously described.
In Thrax, we retain most of the default settings for
Hiero- and SAMT-style grammars as specified in the
extractor?s configuration file. Inheriting from Hiero,
we require the right-hand side of all rules to con-
tain at least one pair of aligned terminals, no more
than two nonterminals, and no more than five termi-
nals and nonterminal elements combined. Nonter-
minals are not allowed to be adjacent on the source
side, and they may not contain unaligned boundary
words. Rules themselves are not extracted from any
span in the training data longer than 10 tokens.
Our initial bilingual SAMT grammar uses 2699
unique source-side labels and 4181 unique target-
side labels, leading to the appearance of 29,088 joint
bilingual labels in the rule set. We provide the joint
labels (along with their counts) to the label collaps-
ing algorithm, while we strip out the source-side
labels to create the baseline SAMT grammar with
4181 unique target-side labels. Table 1 summarizes
how the number of target labels, unique extracted
rules, and the average number of pruned rules avail-
able per sentence change as the initial grammar is
label-collapsed to three progressively coarser de-
grees. Once the collapsing process has occurred ex-
haustively, the original SAMT grammar becomes a
Hiero-format grammar with a single nonterminal.
Each of the five grammars in Table 1 is used to
build an MT system. All systems are tuned and de-
coded with cdec (Dyer et al, 2010), an open-source
decoder for SCFG-based MT with arbitrary rule for-
mats and nonterminal labels. We tune the systems
on the 1664-sentence NIST Open MT 2006 data set,
optimizing towards the BLEU metric. Our test sets
are the NIST 2003 data set of 919 sentences and the
NIST 2008 data set of 1357 sentences. The tun-
ing set and both test sets all have four English ref-
erences.
We evaluate systems on BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011), and
TER (Snover et al, 2006), as calculated in all three
cases by MultEval version 0.5.0.1 These scores for
the MT ?03 test set are shown in Table 2, and those
for the MT ?08 test set in Table 3, combined by Mult-
Eval over three optimization runs on the tuning set.
MultEval also implements statistical significance
testing between systems based on multiple optimizer
runs and approximate randomization. This process
(Clark et al, 2011) randomly swaps outputs between
systems and estimates the probability that the ob-
served score difference arose by chance. We report
these results in the tables as well for three MERT
runs and a p-value of 0.05. Systems that were judged
statistically different from the SAMT baseline have
triangles in the appropriate ?Sig. SAMT?? columns;
systems judged different from the Hiero baseline
have triangles under the ?Sig. Hiero?? columns. An
up-triangle (N) indicates that the system was better,
while a down-triangle (O) means that the baseline
was better.
1https://github.com/jhclark/multeval
293
Metric Scores Sig. SAMT? Sig. Hiero?
System BLEU MET TER B M T B M T
SAMT 31.18 30.64 61.02 O O O
Collapse 1 31.42 31.31 60.95 N O O
Collapse 2 31.90 31.73 60.98 N N O N O
Collapse 3 32.32 31.75 60.54 N N N N O
Hiero 32.30 31.42 60.10 N N N
Table 2: MT ?03 test set results. The first section gives automatic metric scores; the remaining sections indicate
whether each system is statistically significantly better (N) or worse (O) than the SAMT and Hiero baselines.
Metric Scores Sig. SAMT? Sig. Hiero?
System BLEU MET TER B M T B M T
SAMT 22.10 24.94 63.78 O O O
Collapse 1 23.01 26.03 63.35 N N N N
Collapse 2 23.53 26.50 63.29 N N N N N
Collapse 3 23.61 26.37 63.07 N N N N N N
Hiero 23.01 25.72 63.53 N N N
Table 3: MT ?08 test set results. The first section gives automatic metric scores; the remaining sections indicate
whether each system is statistically significantly better (N) or worse (O) than the SAMT and Hiero baselines.
Figure 4: Extracted frequency of each target-side label, with labels arranged in order of decreasing frequency count.
Note the log?log scale of the plot.
294
5 Analysis
Tables 2 and 3 show that the coarsened grammars
significantly improve translation performance over
the SAMT baseline. This is especially true for the
?Collapse 3? setting of 72 labels, which scores 1.14
BLEU higher on MT ?03 and 1.51 BLEU higher on
MT ?08 than the uncollapsed system.
On the easier MT ?03 set, label-collapsed systems
do not generally outperform Hiero, although Col-
lapse 3 achieves a statistical tie according to BLEU
(+0.02) and a statistical improvement over Hiero ac-
cording to METEOR (+0.33). MT ?08 appears as
a significantly harder test set: metric scores for all
systems are drastically lower, and we find approxi-
mately 7% to 8% fewer phrase pair matches per sen-
tence. In this case the label-collapsed systems per-
form better, with all three of them achieving statisti-
cal significance over Hiero in at least one metric and
statistical ties in the other. The coarsened systems?
comparatively better performance on the harder test
set suggests that the linguistic information encoded
in multiple-nonterminal grammars helps the systems
more accurately parse new types of input.
Table 1 already showed at a global scale the strong
effect of label collapsing on reducing derivational
ambiguity, as labeled variants of the same basic
structural rule were progressively combined. Since
category coarsening is purely a relabeling operation,
any reordering pattern implemented in the original
SAMT grammar still exists in the collapsed ver-
sions; therefore, any reduction in the size of the
grammar is a reduction in variant labelings. Figure
4 shows this process in more detail for the baseline
SAMT grammar and the three collapsed grammars.
For each grammar, labels are arranged in decreas-
ing order of extracted frequency, and the frequency
count of each label is plotted. The long tail of rare
categories in the SAMT grammar (1950 labels seen
fewer than 100 times each) is combined into a pro-
gressively sharper distribution at each step. Not only
are there fewer rare labels, but these hard-to-model
categories consume a proportionally smaller fraction
of the total label set: from 47% in the baseline gram-
mar down to 26% in Collapse 3.
We find that label collapsing disproportionately
affects frequently extracted and hierarchical rules
over rarer rules and phrase pairs. The 15.7% re-
duction in total grammar size between the SAMT
baseline and the Collapse 3 system affects 18.0% of
the hierarchical rules, but only 1.6% of the phrase
pairs. If rules are counted separately each time they
match another source sentence, the average reduc-
tion in size of a sentence-filtered grammar is 57.8%.
Intuitively, hierarchical rules are more affected by
label collapsing because phrase pairs do not have
many variant left-hand-side labels to begin with,
while the same hierarchical rule pattern may be in-
stantiated in the grammar by a large number of vari-
ant labelings. We can see this situation in more de-
tail by counting variants of a particular set of rules.
Labeled forms of the Hiero-style rule
X ? [X1 X2] :: [the X2 of X1] (6)
are among the most frequently used rules in all five
of our systems. The way they are treated by label
collapsing thus has a strong impact on the results of
runtime decoding.
In the SAMT baseline, Rule (6) appears in the
grammar with 221 different labels in the X1 nonter-
minal slot, 53 labels for the X2 slot, and 90 choices
of left-hand side ? a total of 1330 different label-
ings all together. More than three-fourths of these
variants were extracted three times or fewer from the
training data; even if they can be used in a test sen-
tence, statistical features for such low-count rules
are poorly estimated. During label collapsing, the
number of labeled variations of Rule (6) drops from
1330 to 325, to 96, and finally to 63 in the Collapse
3 grammar. There, the pattern is instantiated with 14
possible X1 labels, five X2 labels, and three different
left-hand sides.
It is difficult to measure rule sparsity directly (i.e.
to count the number of rules that are missing during
decoding), but a reduction in rule sparsity between
systems should be manifested as an increased num-
ber of hierarchical rule applications. Figure 5 shows
the average number of hierarchical rules applied per
sentence, distinguishing syntactic rules from glue
rules, on both test sets. The collapsed grammars al-
low for approximately one additional syntactic rule
application per sentence compared to the SAMT
baseline, or three additional applications compared
to Hiero. This shows an implicit reduction in miss-
ing syntactic rules in the collapsed grammars. In the
295
MT 2003 MT 2008
Figure 5: Average number of hierarchical rules (both syntactic and glue rules) applied per sentence on each test set.
glue rule columns, we note that label collapsing also
promotes a shift away from generic glue rules, pos-
sibly via the creation of more permissive ? but still
meaningfully labeled ? syntactic rules.
6 Conclusion
We demonstrated a viable technique for reducing the
label set size in SAMT grammars by temporarily in-
ducing bilingual syntax and using it in an existing
tree-to-tree category coarsening algorithm. In col-
lapsing SAMT category labels, we were able to sig-
nificantly improve translation quality while using a
grammar less than half the size of the original. We
believe it is also more robust to test-set or domain
variation than a single-nonterminal Hiero grammar.
Collapsed grammars confer practical benefits during
both model estimation and runtime decoding. We
showed that, in particular, they suffer less from rule
sparsity and derivational ambiguity problems that
are common to larger label sets.
We can highlight two areas for potential improve-
ments in future work. In our current implementation
of label collapsing, we indiscriminately allow either
source labels or target labels to be collapsed at each
iteration of the algorithm (see Equation 3). This is
an intuitively sensible setting when collapsing bilin-
gual labels, but it is perhaps less obviously so for a
monolingually labeled system such as SAMT. An al-
ternative would be to collapse target-side labels only,
leaving the source-side labels alone since they do not
appear in the final grammar anyway. In this case, the
target labels would be represented and clustered as
distributions over a static set of latent categories.
A larger area of future concern is the stopping
point of the collapsing algorithm. In our previ-
ous work (Hanneman and Lavie, 2011), we manu-
ally identified iterations in our run of the algorithm
where the L1 distance between the most recently
collapsed label pair was markedly lower than the
L1 difference of the pair in the previous iteration.
Such an approach is more feasible in our previous
runs of 120 iterations than in ours here of nearly
2100, where it is not likely that three manually cho-
sen stopping points represent the optimal collapsing
results. In future work, we plan to work towards the
development of an automatic stopping criterion, a
more principled test for whether each successive it-
eration of label collapsing provides some useful ben-
efit to the underlying grammar.
Acknowledgments
This research work was supported in part by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
Thanks to Chris Dyer for providing the word-
aligned and preprocessed corpus we used in our ex-
periments. We also thank the anonymous reviewers
for helpful comments and suggestions for analysis.
References
Vamshi Ambati, Alon Lavie, and Jaime Carbonell. 2009.
Extraction of syntactic translation models from paral-
lel data using syntax from source and target languages.
296
In Proceedings of the 12th Machine Translation Sum-
mit, pages 190?197, Ottawa, Canada, August.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Chris Callison-Burch, Nathaniel W. Filardo, Christine
Piatko, Lori Levin, and Scott Miller. 2012. Use of
modality and negation in semantically-informed syn-
tactic MT. Computational Linguistics, 38(2):411?
438.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Crontrolling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Short
Papers, pages 176?181, Portland, OR, June.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 85?91, Edinburgh, United Kingdom, July.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7?12, Uppsala, Sweden, July.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
HLT-NAACL 2004: Main Proceedings, pages 273?
280, Boston, MA, May.
Greg Hanneman and Alon Lavie. 2011. Automatic cate-
gory label coarsening for syntax-based machine trans-
lation. In Proceedings of SSST-5: Fifth Workshop on
Syntax, Semantics, and Structure in Statistical Trans-
lation, pages 98?106, Portland, OR, June.
Bryant Huang and Kevin Knight. 2006. Relabeling syn-
tax trees to improve syntax-based machine translation
quality. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the ACL, pages 240?247, New York, NY, June.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 138?147, Cambridge, MA, October.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 609?616, Sydney, Aus-
tralia, July.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the 47th Annual Meeting of the ACL and
the Fourth IJCNLP of the AFNLP, pages 558?566,
Suntec, Singapore, August.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the ACL, pages 236?244, Boulder, CO,
June.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 478?
484, Edinburgh, United Kingdom, July.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY, June.
Andreas Zollmann. 2011. Learning Multiple-
Nonterminal Synchronous Grammars for Machine
Translation. Ph.D. thesis, Carnegie Mellon University.
297
Proceedings of NAACL-HLT 2013, pages 958?968,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Grouping Language Model Boundary Words to Speed K?Best Extraction
from Hypergraphs
Kenneth Heafield?,? Philipp Koehn? Alon Lavie?
? School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB, UK
pkoehn@inf.ed.ac.uk
? Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{heafield,alavie}@cs.cmu.edu
Abstract
We propose a new algorithm to approximately
extract top-scoring hypotheses from a hyper-
graph when the score includes an N?gram
language model. In the popular cube prun-
ing algorithm, every hypothesis is annotated
with boundary words and permitted to recom-
bine only if all boundary words are equal.
However, many hypotheses share some, but
not all, boundary words. We use these com-
mon boundary words to group hypotheses and
do so recursively, resulting in a tree of hy-
potheses. This tree forms the basis for our
new search algorithm that iteratively refines
groups of boundary words on demand. Ma-
chine translation experiments show our algo-
rithm makes translation 1.50 to 3.51 times as
fast as with cube pruning in common cases.
1 Introduction
This work presents a new algorithm to search a
packed data structure for high-scoring hypothe-
ses when the score includes an N?gram language
model. Many natural language processing systems
have this sort of problem e.g. hypergraph search
in hierarchical and syntactic machine translation
(Mi et al, 2008; Klein and Manning, 2001), lat-
tice rescoring in speech recognition, and confusion
network decoding in optical character recognition
(Tong and Evans, 1996). Large language models
have been shown to improve quality, especially in
machine translation (Brants et al, 2007; Koehn and
Haddow, 2012). However, language models make
search computationally expensive because they ex-
amine surface words without regard to the structure
at North Korea
in North Korea
with North Korea
with the DPRK
at
?
?
?
North Koreain
with
{
the DPRK
Figure 1: Hypotheses are grouped by common prefixes
and suffixes.
of the packed search space. Prior work, including
cube pruning (Chiang, 2007), has largely treated the
language model as a black box. Our new search
algorithm groups hypotheses by common prefixes
and suffixes, exploiting the tendency of the language
model to score these hypotheses similarly. An exam-
ple is shown in Figure 1. The result is a substantial
improvement over the time-accuracy trade-off pre-
sented by cube pruning.
The search spaces mentioned in the previous para-
graph are special cases of a directed acyclic hyper-
graph. As used here, the difference from a nor-
mal graph is that an edge can go from one vertex
to any number of vertices; this number is the arity
of the edge. Lattices and confusion networks are
hypergraphs in which every edge happens to have
arity one. We experiment with parsing-based ma-
chine translation, where edges represent grammar
rules that may have any number of non-terminals,
including zero.
Hypotheses are paths in the hypergraph scored by
a linear combination of features. Many features are
additive: they can be expressed as weights on edges
that sum to form hypothesis features. However, log
probability from anN?gram language model is non-
958
additive because it examines surface strings across
edge and vertex boundaries. Non-additivity makes
search difficult because locally optimal hypotheses
may not be globally optimal.
In order to properly compute the language model
score, each hypothesis is annotated with its bound-
ary words, collectively referred to as its state (Li
and Khudanpur, 2008). Hypotheses with equal state
may be recombined, so a straightforward dynamic
programming approach (Bar-Hillel et al, 1964) sim-
ply treats state as an additional dimension in the dy-
namic programming table. However, this approach
quickly becomes intractable for large language mod-
els where the number of states is too large.
Beam search (Chiang, 2005; Lowerre, 1976) ap-
proximates the straightforward algorithm by remem-
bering a beam of up to k hypotheses1 in each vertex.
It visits each vertex in bottom-up order, each time
calling a beam filling algorithm to select k hypothe-
ses. The parameter k is a time-accuracy trade-off:
larger k increases both CPU time and accuracy.
We contribute a new beam filling algorithm that
improves the time-accuracy trade-off over the popu-
lar cube pruning algorithm (Chiang, 2007) discussed
in ?2.3. The algorithm is based on the observation
that competing hypotheses come from the same im-
put, so their language model states are often similar.
Grouping hypotheses by these similar words enables
our algorithm to reason over multiple hypotheses at
once. The algorithm is fully described in ?3.
2 Related Work
2.1 Alternatives to Bottom-Up Search
Beam search visits each vertex in the hypergraph
in bottom-up (topological) order. The hypergraph
can also be searched in left-to-right order (Watanabe
et al, 2006; Huang and Mi, 2010). Alternatively,
hypotheses can be generated on demand with cube
growing (Huang and Chiang, 2007), though we note
that it showed little improvement in Moses (Xu and
Koehn, 2012). All of these options are compatible
with our algorithm. However, we only experiment
with bottom-up beam search.
1We use K to denote the number of fully-formed hypotheses
requested by the user and k to denote beam size.
2.2 Exhaustive Beam Filling
Originally, beam search was used with an exhaustive
beam filling algorithm (Chiang, 2005). It generates
every possible hypothesis (subject to the beams in
previous vertices), selects the top k by score, and
discards the remaining hypotheses. This is expen-
sive: just one edge of arity a encodes O(1 + ak)
hypotheses and each edge is evaluated exhaustively.
In the worst case, our algorithm is exhaustive and
generates the same number of hypotheses as beam
search; in practice, we are concerned with the aver-
age case.
2.3 Baseline: Cube Pruning
Cube pruning (Chiang, 2007) is a fast approximate
beam filling algorithm and our baseline. It chooses
k hypotheses by popping them off the top of a prior-
ity queue. Initially, the queue is populated with hy-
potheses made from the best (highest-scoring) parts.
These parts are an edge and a hypothesis from each
vertex referenced by the edge. When a hypothesis
is popped, several next-best alternatives are pushed.
These alternatives substitute the next-best edge or a
next-best hypothesis from one of the vertices.
Our work follows a similar pattern of popping one
queue entry then pushing multiple entries. However,
our queue entries are a group of hypotheses while
cube pruning?s entries are a single hypothesis.
Hypotheses are usually fully scored before being
placed in the priority queue. An alternative priori-
tizes hypotheses by their additive score. The addi-
tive score is the edge?s score plus the score of each
component hypothesis, ignoring the non-additive as-
pect of the language model. When the additive score
is used, the language model is only called k times,
once for each hypothesis popped from the queue.
Cube pruning can produce duplicate queue en-
tries. Gesmundo and Henderson (2010) modified the
algorithm prevent duplicates instead of using a hash
table. We include their work in the experiments.
Hopkins and Langmead (2009) characterized
cube pruning as A* search (Hart et al, 1968) with an
inadmissible heuristic. Their analysis showed deep
and unbalanced search trees. Our work can be inter-
preted as a partial rebalancing of these search trees.
959
2.4 Exact Algorithms
A number of exact search algorithms have been de-
veloped. We are not aware of an exact algorithm that
tractably scales to the size of hypergraphs and lan-
guage models used in many modern machine trans-
lation systems (Callison-Burch et al, 2012).
The hypergraph and language model can be com-
piled into an integer linear program. The best hy-
pothesis can then be recovered by taking the dual
and solving by Lagrangian relaxation (Rush and
Collins, 2011). However, that work only dealt with
language models up to order three.
Iglesias et al (2011) represent the search space
as a recursive transition network and the language
model as a weighted finite state transducer. Using
standard finite state algorithms, they intersect the
two automatons then exactly search for the highest-
scoring paths. However, the intersected automaton
is too large. The authors suggested removing low
probability entries from the language model, but this
form of pruning negatively impacts translation qual-
ity (Moore and Quirk, 2009; Chelba et al, 2010).
Their work bears some similarity to our algorithm
in that partially overlapping state will be collapsed
and efficiently handled together. However, the key
advatage to our approach is that groups have a score
that can be used for pruning before the group is ex-
panded, enabling pruning without first constructing
the intersected automaton.
2.5 Coarse-to-Fine
Coarse-to-fine (Petrov et al, 2008) performs mul-
tiple pruning passes, each time with more detail.
Search is a subroutine of coarse-to-fine and our work
is inside search, so the two are compatible. There are
several forms of coarse-to-fine search; the closest to
our work increases the language model order each
iteration. However, by operating inside search, our
algorithm is able to handle hypotheses at different
levels of refinement and use scores to choose where
to further refine hypotheses. Coarse-to-fine decod-
ing cannot do this because it determines the level of
refinement before calling search.
3 Our New Beam Filling Algorithm
In our algorithm, the primary idea is to group hy-
potheses with similar language model state. The
following sections formalize what these groups are
(partial state), that the groups have a recursive struc-
ture (state tree), how groups are split (bread crumbs),
using groups with hypergraph edges (partial edge),
prioritizing search (scoring) and best-first search
(priority queue).
3.1 Partial State
An N?gram language model (with order N ) com-
putes the probability of a word given the N ? 1 pre-
ceding words. The left state of a hypothesis is the
first N ? 1 words, which have insufficient context
to be scored. Right state is the last N ? 1 words;
these might become context for another hypothesis.
Collectively, they are known as state. State mini-
mization (Li and Khudanpur, 2008) may reduce the
size of state due to backoff in the language model.
For example, the hypothesis ?the few nations that
have diplomatic relations with North Korea? might
have left state ?the few? and right state ?Korea?
after state minimization determined that ?North?
could be elided. Collectively, the state is denoted
(the few a  ` Korea). The diamond  is a stand-in
for elided words. Terminators a and ` indicate when
left and right state are exhausted, respectively2.
Our algorithm is based on partial state. Par-
tial state is simply state with more inner words
elided. For example, (the  Korea) is a partial state
for (the few a  ` Korea). Terminators a and ` can
be elided just like words. Empty state is denoted
using the customary symbol for empty string, . For
example, (  ) is the empty partial state. The termi-
nators serve to distinguish a completed state (which
may be short due to state minimization) from an in-
complete partial state.
3.2 State Tree
States (the few a  ` Korea) and (the a  ` Korea)
have words in common, so the partial state
(the  Korea) can be used to reason over both of
them. Generalizing this notion to the set of hypothe-
ses in a beam, we build a state tree. The root of
the tree is the empty partial state (  ) that reasons
2A corner case arises for hypotheses with less than N ? 1
words. For these hypotheses, we still attempt state minimiza-
tion and, if successful, the state is treated normally. If state
minimization fails, a flag is set in the state. For purposes of the
state tree, the flag acts like a different terminator symbol.
960
(  )
(a  ) (a  Korea) (a a  Korea)
(a a  ` Korea)
(a a  in Korea) (a a  ` in Korea)
(some  ) (some  DPRK) (some a  DPRK) (some a  ` DPRK)
(the  ) (the  Korea)
(the a  Korea) (the a  ` Korea)
(the few  Korea) (the few  ` Korea) (the few a  ` Korea)
Figure 2: A state tree containing five states: (the few a  ` Korea), (the a  ` Korea), (some a  ` DPRK),
(a a  ` in Korea), and (a a  ` Korea). Nodes of the tree are partial states. The branching order is the first word,
the last word, the second word, and so on. If the left or right state is exhausted, then branching continues with the
remaining state. For purposes of branching, termination symbols a and ` act like normal words.
(  )
(a a  Korea)
(a a  ` Korea)
(a a  ` in Korea)
(some a  ` DPRK)
(the  Korea)
(the a  ` Korea)
(the few a  ` Korea)
Figure 3: The optimized version of Figure 2. Nodes
immediately reveal the longest shared prefix and suffix
among hypotheses below them.
over all hypotheses. From the root, the tree branches
by the first word of state, the last word, the second
word, the second-to-last word, and so on. If left or
right state is exhausted, then branching continues us-
ing the remaining state. The branching order priori-
tizes the outermost words because these can be used
to update the language model probability. The deci-
sion to start with left state is arbitrary. An example
tree is shown in Figure 2.
As an optimization, each node determines the
longest shared prefix and suffix of the hypotheses
below it. The node reports these words immedi-
ately, rendering some other nodes redundant. This
makes our algorithm faster because it will then only
encounter nodes when there is a branching decision
to be made. The original tree is shown in Figure 2
and the optimized version is shown in Figure 3. As
a side effect of branching by left state first, the al-
gorithm did not notice that states (the  Korea) and
(  )[1+]
(a a  Korea)
(a a  ` Korea)
(a a  ` in Korea)
(some a  ` DPRK)
(the  Korea)
(the a  ` Korea)
(the few a  ` Korea)
(the  Korea)[0+]
(the a  ` Korea)
(the few a  ` Korea)
Figure 4: Visiting the root node partitions the tree into
best child (the  Korea)[0+] and bread crumb (  )[1+].
The data structure remains intact for use elsewhere.
(a a  Korea) both end with Korea. We designed the
tree building algorithm for speed and plan to exper-
iment with alternatives as future work.
The state tree is built lazily. A node initially holds
a flat array of all the hypotheses below it. When its
children are first needed, the hypotheses are grouped
by the branching word and an array of child nodes
is built. In turn, these newly created children each
initially hold an array of hypotheses. CPU time is
saved because nodes containing low-scoring nodes
may never construct their children.
Each node has a score. For leaves, this score is
copied from the underlying hypothesis (or best hy-
pothesis if some other feature prevented recombina-
tion). The score of an internal node is the maximum
score of its children. As an example, the root node?s
score is the same as the highest-scoring hypothesis
in the tree. Children are sorted by score.
961
3.3 Bread Crumbs
The state tree is explored in a best-first manner.
Specifically, when the algorithm visits a node, it
considers that node?s best child. The best child re-
veals more words, so the score may go up or down
when the language model is consulted. Therefore,
simply following best children may lead to a poor
hypothesis. Some backtracking mechanism is re-
quired, for which we use bread crumbs. Visiting a
node results in two items: the best child and a bread
crumb. The bread crumb encodes the node that was
visited and how many children have already been
considered. Figure 4 shows an example.
More formally, each node has an array of chil-
dren sorted by score, so it suffices for the bread
crumb to keep an index in this array. An in-
dex of zero denotes that no child has been vis-
ited. Continuing the example from Figure 3,
(  )[0+] denotes the root partial state with chil-
dren starting at index 0 (i.e. all of them). Visit-
ing (  )[0+] yields best child (the  Korea)[0+]
and bread crumb (  )[1+]. Later, the search al-
gorithm may return to (  )[1+], yielding best
child (some a  ` DPRK)[0+] and bread crumb
(  )[2+]. If there is no remaining sibling, visit-
ing yields only the best child.
The index serves to restrict the array of children
to those with that index or above. Formally, let d
map from a node or bread crumb to the set of leaves
descended from it. The descendants of a node n are
those of its children
d(n) =
|n|?1?
i=0
d(n[i])
where unionsq takes the union of disjoint sets and n[i] is
the ith child. In a bread crumb with index c, only de-
scendents by the remaining children are considered
d(n[c+]) =
|n|?1?
i=c
d(n[i])
It follows that the set of descendants is partitioned
into two disjoint sets
d(n[c+]) = d(n[c])
?
d(n[c+ 1+])
3.4 Partial Edge
The beam filling algorithm is tasked with selecting
hypotheses given a number of hypergraph edges.
Hypergraph edges are strings comprised of words
and references to vertices (in parsing, terminals and
non-terminals). A hypergraph edge is converted to a
partial edge by replacing each vertex reference with
the root node from that vertex. For example, the hy-
pergraph edge ?is v .? referencing vertex v becomes
partial edge ?is (  )[0+] .?
Partial edges allow our algorithm to reason over
a large set of hypotheses at once. Visiting a
partial edge divides that set into two as follows.
A heuristic chooses one of the non-leaf nodes to
visit. Currently, this heuristic picks the node with
the fewest words revealed. As a tie breaker, it
chooses the leftmost node. The chosen node is
visited (partitioned), yielding the best child and
bread crumb as described in the previous section.
These are substituted into separate copies of the par-
tial edge. Continuing our example with the vertex
shown in Figure 3, ?is (  )[0+] .? partitions into
?is (the  Korea)[0+] .? and ?is (  )[1+] .?
3.5 Scoring
Every partial edge has a score that determines its
search priority. Initially, this score is the sum of the
edge?s score and the scores of each bread crumb (de-
fined below). As words are revealed, the score is
updated to account for new language model context.
Each edge score includes a log language model
probability and possibly additive features. When-
ever there is insufficient context to compute the lan-
guage model probability of a word, an estimate r is
used. For example, edge ?is v .? incorporates esti-
mate
log r(is)r(.)
into its score. The same applies to hypotheses:
(the few a  ` Korea) includes estimate
log r(the)r(few | the)
because the words in left state are those with insuf-
ficient context.
In common practice (Chiang, 2007; Hoang et al,
2009; Dyer et al, 2010), the estimate is taken from
the language model: r = p. However, querying
the language model with incomplete context leads
962
Kneser-Ney smoothing (Kneser and Ney, 1995) to
assume that backoff has occurred. An alternative is
to use average-case rest costs explicitly stored in the
language model (Heafield et al, 2012). Both options
are used in the experiments3.
The score of a bread crumb is the maximum score
of its descendants as defined in ?3.3. For example,
the bread crumb (  )[1+] has a lower score than
(  )[0+] because the best child (the  Korea)[0+]
and its descendants no longer contribute to the max-
imum.
The score of partial edge ?is (  )[0+] .? is
the sum of scores from its two parts: edge
?is v .? and bread crumb (  )[0+]. The
edge?s score includes estimated log probability
log r(is)r(.) as explained earlier. The bread crumb?s
score comes from its highest-scoring descendent
(the few a  ` Korea) and therefore includes esti-
mate log r(the)r(few | the).
Estimates are updated as words are revealed.
Continuing the example, ?is (  )[0+] .? has best
child ?is (the  Korea)[0+] .? In this best child, the
estimate r(.) is updated to r(. | Korea). Similarly,
r(the) is replaced with r(the | is). Updates exam-
ine only words that have been revealed: r(few | the)
remains unrevised.
Updates are computed efficiently by using point-
ers (Heafield et al, 2011) with KenLM. To summa-
rize, the language model computes
r(wn|w
n?1
1 )
r(wn|w
n?1
i )
in a single call. In the popular reverse trie data struc-
ture, the language model visits wni while retrieving
wn1 , so the cost is the same as a single query. More-
over, when the language model earlier provided es-
timate r(wn|w
n?1
i ), it also returned a data-structure
pointer t(wni ). Pointers are retained in hypotheses,
edges, and partial edges for each word with an esti-
mated probability. When context is revealed, our al-
gorithm queries the language model with new con-
text wi?11 and pointer t(w
n
i ). The language model
uses this pointer to immediately retrieve denomina-
tor r(wn|w
n?1
i ) and as a starting point to retrieve nu-
merator r(wn|w
n?1
1 ). It can therefore avoid looking
3We also tested upper bounds (Huang et al, 2012; Carter et
al., 2012) but the result is still approximate due to beam pruning
and initial experiments showed degraded performance.
up r(wn), r(wn|wn?1), . . . , r(wn|w
n?1
i+1 ) as would
normally be required with a reverse trie.
3.6 Priority Queue
Our beam filling algorithm is controlled by a priority
queue containing partial edges. The queue is popu-
lated by converting all outgoing hypergraph edges
into partial edges and pushing them onto the queue.
After this initialization, the algorithm loops. Each
iteration begins by popping the top-scoring partial
edge off the queue. If all nodes are leaves, then the
partial edge is converted to a hypothesis and placed
in the beam. Otherwise, the partial edge is parti-
tioned as described in ?3.3. The two resulting partial
edges are pushed onto the queue. Looping continues
with the next iteration until the queue is empty or the
beam is full. After the loop terminates, the beam is
given to the root node of the state tree; other nodes
will be built lazily as described in ?3.2.
Overall, the algorithm visits hypergraph vertices
in bottom-up order. Our beam filling algorithm runs
in each vertex, making use of state trees in vertices
below. The top of the tree contains full hypotheses.
If a K-best list is desired, packing and extraction
works the same way as with cube pruning.
4 Experiments
Performance is measured by translating the 3003-
sentence German-English test set from the 2011
Workshop on Machine Translation (Callison-Burch
et al, 2011). Two translation models were built, one
hierarchical (Chiang, 2007) and one with target syn-
tax. The target-syntax system is based on English
parses from the Collins (1999) parser. Both were
trained on Europarl (Koehn, 2005). The language
model interpolates models built on Europarl, news
commentary, and news data provided by the evalua-
tion. Interpolation weights were tuned on the 2010
test set. Language models were built with SRILM
(Stolcke, 2002), modified Kneser-Ney smoothing
(Kneser and Ney, 1995; Chen and Goodman, 1998),
default pruning, and order 5. Feature weights were
tuned with MERT (Och, 2003), beam size 1000,
100-best output, and cube pruning. Systems were
built with the Moses (Hoang et al, 2009) pipeline.
Measurements were collected by running the de-
coder on all 3003 sentences. For consistency, all
963
-101.6
-101.5
-101.4
0 1 2
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
This work
Additive cube pruning
Cube pruning
Figure 5: Hierarchial system in Moses with our algo-
rithm, cube pruning with additive scores, and cube prun-
ing with full scores (?2.3). The two baselines overlap.
relevant files were forced into the operating system
disk cache before each run. CPU time is the to-
tal user and system time taken by the decoder mi-
nus loading time. Loading time was measured by
running the decoder with empty input. In partic-
ular, CPU time includes the cost of parsing. Our
test system has 32 cores and 64 GB of RAM; no
run came close to running out of memory. While
multi-threaded experiments showed improvements
as well, we only report single-threaded results to re-
duce noise and to compare with cdec (Dyer et al,
2010). Decoders were compiled with the optimiza-
tion settings suggested in their documentation.
Search accuracy is measured by average model
score; higher is better. Only relative comparisons
are meaningful because model scores have arbitrary
scale and include constant factors. Beam sizes start
at 5 and rise until a time limit determined by running
the slowest algorithm with beam size 1000.
4.1 Comparison Inside Moses
Figure 5 shows Moses performance with this work
and with cube pruning. These results used the hi-
erarchical system with common-practice estimates
(?3.5). The two cube pruning variants are explained
in ?2.3. Briefly, the queue can be prioritized using
-101.6
-101.5
-101.4
0 1 2
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
This work
Gesmundo 1
Gesmundo 2
Cube pruning
Figure 6: Hierarchical system in cdec with our algorithm,
similarly-performing variants of cube pruning defined in
Gesmundo and Henderson (2010), and the default.
additive or full scores. Performance with additive
scores is roughly the same as using full scores with
half the beam size.
Our algorithm is faster for every beam size tested.
It is also more accurate than additive cube pruning
with the same beam size. However, when compared
with full scores cube pruning, it is less accurate for
beam sizes below 300. This makes sense because
our algorithm starts with additive estimates and iter-
atively refines them by calling the language model.
Moreover, when beams are small, there are fewer
chances to group hypotheses. With beams larger
than 300, our algorithm can group more hypotheses,
overtaking both forms of cube pruning.
Accuracy improvements can be interpreted as
speed improvements by asking how much time each
algorithm takes to achieve a set level of accuracy.
By this metric, our algorithm is 2.04 to 3.37 times as
fast as both baselines.
4.2 Comparison Inside cdec
We also implemented our algorithm in cdec (Dyer
et al, 2010). Figure 6 compares with two enhanced
versions of cube pruning (Gesmundo and Hender-
son, 2010) and the cdec baseline. The model scores
964
-101.6
-101.5
-101.4
0 1 2
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
Rest+This work
This work
Rest+Cube pruning
Cube pruning
21.4
21.6
21.8
22
0 1 2
U
nc
as
ed
B
L
E
U
CPU seconds/sentence
Rest+This work
This work
Rest+Cube pruning
Cube pruning
Figure 7: Effect of rest costs on our algorithm and on cube pruning in Moses. Noisy BLEU scores reflect model errors.
are comparable with Moses4.
Measuring at equal accuracy, our algorithm
makes cdec 1.56 to 2.24 times as fast as the best
baseline. At first, this seems to suggest that cdec is
faster. In fact, the opposite is true: comparing Fig-
ures 5 and 6 reveals that cdec has a higher parsing
cost than Moses5, thereby biasing the speed ratio to-
wards 1. In subsequent experiments, we use Moses
because it more accurately reflects search costs.
4.3 Average-Case Rest Costs
Previous experiments used the common-practice
probability estimate described in ?3.5. Figure 7
shows the impact of average-case rest costs on our
algorithm and on cube pruning in Moses. We also
looked at uncased BLEU (Papineni et al, 2002)
scores, finding that our algorithm attains near-peak
BLEU in less time. The relationship between model
score and BLEU is noisy due to model errors.
4The glue rule builds hypotheses left-to-right. In Moses,
glued hypotheses start with <s> and thus have empty left state.
In cdec, sentence boundary tokens are normally added last, so
intermediate hypotheses have spurious left state. Running cdec
with the Moses glue rule led to improved time-accuracy perfor-
mance. The improved version is used in all results reported. We
accounted for constant-factor differences in feature definition
i.e. whether <s> is part of the word count.
5In-memory phrase tables were used with both decoders.
The on-disk phrase table makes Moses slower than cdec.
Average-case rest costs impact our algorithm
more than they impact cube pruning. For small beam
sizes, our algorithm becomes more accurate, mostly
eliminating the disadvantage reported in ?4.1. Com-
pared to the common-practice estimate with beam
size 1000, rest costs made our algorithm 1.62 times
as fast and cube pruning 1.22 times as fast.
Table 1 compares our best result with the best
baseline: our algorithm and cube pruning, both with
rest costs inside Moses. In this scenario, our algo-
rithm is 2.59 to 3.51 times as fast as cube pruning.
4.4 Target-Syntax
We took the best baseline and best result from previ-
ous experiments (Moses with rest costs) and ran the
target-syntax system. Results are shown in Figure
8. Parsing and search are far more expensive. For
beam size 5, our algorithm attains equivalent accu-
racy 1.16 times as fast. Above 5, our algorithm is
1.50 to 2.00 times as fast as cube pruning. More-
over, our algorithm took less time with beam size
6900 than cube pruning took with beam size 1000.
A small bump in model score occurs around 15
seconds. This is due to translating ?durchzoge-
nen? as ?criss-crossed? instead of passing it through,
which incurs a severe penalty (-100). The only rule
capable of doing so translates ?X durchzogenen? as
?criss-crossed PP?; a direct translation rule was not
965
-105
-104.8
-104.6
-104.4
-104.2
0 10 20
A
ve
ra
ge
m
od
el
sc
or
e
CPU seconds/sentence
Rest+This work
Rest+Cube pruning
21
21.2
21.4
0 10 20
U
nc
as
ed
B
L
E
U
CPU seconds/sentence
Rest+This work
Rest+Cube pruning
Figure 8: Performance of Moses with the target-syntax system.
extracted due to reordering. An appropriate prepo-
sitional phrase (PP) was pruned with smaller beam
sizes because it is disfluent.
4.5 Memory
Peak virtual memory usage was measured before
each process terminated. Compared with cube prun-
ing at a beam size of 1000, our algorithm uses 160
MB more RAM in Moses and 298 MB less RAM in
cdec. The differences are smaller with lower beam
sizes and minor relative to 12-13 GB total size, most
of which is the phrase table and language model.
Rest+This work Rest+Cube pruning
k CPU Model BLEU CPU Model BLEU
5 0.068 -1.698 21.59 0.243 -1.667 21.75
10 0.076 -1.593 21.89 0.255 -1.592 21.97
50 0.125 -1.463 22.07 0.353 -1.480 22.04
75 0.157 -1.446 22.06 0.408 -1.462 22.05
100 0.176 -1.436 22.03 0.496 -1.451 22.05
500 0.589 -1.408 22.00 1.356 -1.415 22.00
750 0.861 -1.405 21.96 1.937 -1.409 21.98
1000 1.099 -1.403 21.97 2.502 -1.407 21.98
Table 1: Numerical results from the hierarchical system
for select beam sizes k comparing our best result with the
best baseline, both in Moses with rest costs enabled. To
conserve space, model scores are shown with 100 added.
5 Conclusion
We have described a new search algorithm that
achieves equivalent accuracy 1.16 to 3.51 times as
fast as cube pruning, including two implementations
and four variants. The algorithm is based on group-
ing similar language model feature states together
and dynamically expanding these groups. In do-
ing so, it exploits the language model?s ability to
estimate with incomplete information. Our imple-
mentation is available under the LGPL as a stand-
alone from http://kheafield.com/code/
and distributed with Moses and cdec.
Acknowledgements
This research work was supported in part by the Na-
tional Science Foundation under grant IIS-0713402,
by a NPRP grant (NPRP 09-1140-1-177) from the
Qatar National Research Fund (a member of the
Qatar Foundation), and by computing resources pro-
vided by the NSF-sponsored XSEDE program under
grant TG-CCR110017. The statements made herein
are solely the responsibility of the authors. The re-
search leading to these results has received funding
from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreements
287576 (CASMACAT), 287658 (EU BRIDGE),
287688 (MateCat), and 288769 (ACCEPT).
966
References
Yehoshua Bar-Hillel, Micha Perles, and Eli Shamir.
1964. On Formal Properties of Simple Phrase Struc-
ture Grammars. Hebrew University Students? Press.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Language
Learning, pages 858?867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Simon Carter, Marc Dymetman, and Guillaume
Bouchard. 2012. Exact sampling and decoding in
high-order hidden Markov models. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1125?1134, Jeju
Island, Korea, July.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and Kneser-Ney smoothing. In Proceedings of In-
terspeech, pages 2242?2245.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270, Ann Arbor, Michi-
gan, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228, June.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A decoder, alignment, and learning framework for
finite-state and context-free translation models. In
Proceedings of the ACL 2010 System Demonstrations,
ACLDemos ?10, pages 7?12.
Andrea Gesmundo and James Henderson. 2010. Faster
cube pruning. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT),
pages 267?274.
Peter Hart, Nils Nilsson, and Bertram Raphael. 1968. A
formal basis for the heuristic determination of mini-
mum cost paths. IEEE Transactions on Systems Sci-
ence and Cybernetics, 4(2):100?107, July.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
model state for syntactic machine translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, San Francisco, CA, USA, De-
cember.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language model rest costs and space-efficient storage.
In Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, Jeju Is-
land, Korea.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 152?159, Tokyo, Japan.
Mark Hopkins and Greg Langmead. 2009. Cube pruning
as heuristic search. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 62?71, Singapore, August.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273?283, Cambridge,
MA, October.
Zhiheng Huang, Yi Chang, Bo Long, Jean-Francois Cre-
spo, Anlei Dong, Sathiya Keerthi, and Su-Lin Wu.
2012. Iterative Viterbi A* algorithm for k-best se-
quential decoding. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1125?1134, Jeju Island, Korea,
July.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`
de Gispert, and Michael Riley. 2011. Hierarchical
phrase-based translation representations. In Proceed-
ings of the 2011 Conference on Empirical Methods in
967
Natural Language Processing, pages 1373?1383, Ed-
inburgh, Scotland, UK, July. Association for Compu-
tational Linguistics.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the Seventh Inter-
national Workshop on Parsing Technologies, Beijing,
China, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181?
184.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the Second ACL Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
10?18, Columbus, Ohio, June.
Bruce Lowerre. 1976. The Harpy Speech Recognition
System. Ph.D. thesis, Carnegie Mellon University.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June.
Robert C. Moore and Chris Quirk. 2009. Less is more:
Significance-based n-gram selection for smaller, better
language models. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 746?755, August.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, PA, July.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 108?116, Honolulu, HI, USA, October.
Alexander Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 72?82, Portland, Oregon, USA,
June.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904.
Xiang Tong and David A. Evans. 1996. A statistical
approach to automatic OCR error correction in con-
text. In Proceedings of the Fourth Workshop on Very
Large Corpora, pages 88?100, Copenhagen, Den-
mark, April.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pages 777?
784, Sydney, Australia, July.
Wenduan Xu and Philipp Koehn. 2012. Extending hiero
decoding in Moses with cube growing. The Prague
Bulletin of Mathematical Linguistics, 98:133?142.
968
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 409?419,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Word Alignment with Arbitrary Features
Chris Dyer Jonathan Clark Alon Lavie Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{cdyer,jhclark,alavie,nasmith}@cs.cmu.edu
Abstract
We introduce a discriminatively trained, glob-
ally normalized, log-linear variant of the lex-
ical translation models proposed by Brown
et al (1993). In our model, arbitrary, non-
independent features may be freely incorpo-
rated, thereby overcoming the inherent limita-
tion of generative models, which require that
features be sensitive to the conditional inde-
pendencies of the generative process. How-
ever, unlike previous work on discriminative
modeling of word alignment (which also per-
mits the use of arbitrary features), the param-
eters in our models are learned from unanno-
tated parallel sentences, rather than from su-
pervised word alignments. Using a variety
of intrinsic and extrinsic measures, including
translation performance, we show our model
yields better alignments than generative base-
lines in a number of language pairs.
1 Introduction
Word alignment is an important subtask in statis-
tical machine translation which is typically solved
in one of two ways. The more common approach
uses a generative translation model that relates bilin-
gual string pairs using a latent alignment variable to
designate which source words (or phrases) generate
which target words. The parameters in these models
can be learned straightforwardly from parallel sen-
tences using EM, and standard inference techniques
can recover most probable alignments (Brown et al,
1993). This approach is attractive because it only
requires parallel training data. An alternative to the
generative approach uses a discriminatively trained
alignment model to predict word alignments in the
parallel corpus. Discriminative models are attractive
because they can incorporate arbitrary, overlapping
features, meaning that errors observed in the predic-
tions made by the model can be addressed by engi-
neering new and better features. Unfortunately, both
approaches are problematic, but in different ways.
In the case of discriminative alignment mod-
els, manual alignment data is required for train-
ing, which is problematic for at least three reasons.
Manual alignments are notoriously difficult to cre-
ate and are available only for a handful of language
pairs. Second, manual alignments impose a commit-
ment to a particular preprocessing regime; this can
be problematic since the optimal segmentation for
translation often depends on characteristics of the
test set or size of the available training data (Habash
and Sadat, 2006) or may be constrained by require-
ments of other processing components, such parsers.
Third, the ?correct? alignment annotation for differ-
ent tasks may vary: for example, relatively denser or
sparser alignments may be optimal for different ap-
proaches to (downstream) translation model induc-
tion (Lopez, 2008; Fraser, 2007).
Generative models have a different limitation: the
joint probability of a particular setting of the ran-
dom variables must factorize according to steps in a
process that successively ?generates? the values of
the variables. At each step, the probability of some
value being generated may depend only on the gen-
eration history (or a subset thereof), and the possible
values a variable will take must form a locally nor-
malized conditional probability distribution (CPD).
While these locally normalized CPDs may be pa-
409
rameterized so as to make use of multiple, overlap-
ping features (Berg-Kirkpatrick et al, 2010), the re-
quirement that models factorize according to a par-
ticular generative process imposes a considerable re-
striction on the kinds of features that can be incor-
porated. When Brown et al (1993) wanted to in-
corporate a fertility model to create their Models 3
through 5, the generative process used in Models 1
and 2 (where target words were generated one by
one from source words independently of each other)
had to be abandoned in favor of one in which each
source word had to first decide how many targets it
would generate.1
In this paper, we introduce a discriminatively
trained, globally normalized log-linear model of lex-
ical translation that can incorporate arbitrary, over-
lapping features, and use it to infer word alignments.
Our model enjoys the usual benefits of discrimina-
tive modeling (e.g., parameter regularization, well-
understood learning algorithms), but is trained en-
tirely from parallel sentences without gold-standard
word alignments. Thus, it addresses the two limita-
tions of current word alignment approaches.
This paper is structured as follows. We begin by
introducing our model (?2), and follow this with a
discussion of tractability, parameter estimation, and
inference using finite-state techniques (?3). We then
describe the specific features we used (?4) and pro-
vide experimental evaluation of the model, showing
substantial improvements in three diverse language
pairs (?5). We conclude with an analysis of related
prior work (?6) and a general discussion (?8).
2 Model
In this section, we develop a conditional model
p(t | s) that, given a source language sentence s with
length m = |s|, assigns probabilities to a target sen-
tence t with length n, where each word tj is an el-
ement in the finite target vocabulary ?. We begin
by using the chain rule to factor this probability into
two components, a translation model and a length
model.
p(t | s) = p(t, n | s) = p(t | s, n)
? ?? ?
translation model
? p(n | s)
? ?? ?
length model
1Moore (2005) likewise uses this example to motivate the
need for models that support arbitrary, overlapping features.
In the translation model, we then assume that each
word tj is a translation of one source word, or a
special null token. We therefore introduce a latent
alignment variable a = ?a1, a2, . . . , an? ? [0,m]n,
where aj = 0 represents a special null token.
p(t | s, n) =
?
a
p(t, a | s, n)
So far, our model is identical to that of (Brown et
al., 1993); however, we part ways here. Rather than
using the chain rule to further decompose this prob-
ability and motivate opportunities to make indepen-
dence assumptions, we use a log-linear model with
parameters ? ? Rk and feature vector function H
that maps each tuple ?a, s, t, n? into Rk to model
p(t, a | s, n) directly:
p?(t, a | s, n) =
exp?>H(t, a, s, n)
Z?(s, n)
, where
Z?(s, n) =
?
t???n
?
a?
exp?>H(t?, a?, s, n)
Under some reasonable assumptions (a finite target
vocabulary ? and that all ?k < ?), the partition
function Z?(s, n) will always take on finite values,
guaranteeing that p(t, a | s, n) is a proper probability
distribution.
So far, we have said little about the length model.
Since our intent here is to use the model for align-
ment, where both the target length and target string
are observed, it will not be necessary to commit to
any length model, even during training.
3 Tractability, Learning, and Inference
The model introduced in the previous section is
extremely general, and it can incorporate features
sensitive to any imaginable aspects of a sentence
pair and their alignment, from linguistically in-
spired (e.g., an indicator feature for whether both
the source and target sentences contain a verb), to
the mundane (e.g., the probability of the sentence
pair and alignment under Model 1), to the absurd
(e.g., an indicator if s and t are palindromes of each
other).
However, while our model can make use of arbi-
trary, overlapping features, when designing feature
functions it is necessary to balance expressiveness
and the computational complexity of the inference
410
algorithms used to reason under models that incor-
porate these features.2 To understand this tradeoff,
we assume that the random variables being modeled
(t, a) are arranged into an undirected graph G such
that the vertices represent the variables and the edges
are specified so that the feature function H decom-
poses linearly over all the cliques C in G,
H(t, a, s, n) =
?
C
h(tC , aC , s, n) ,
where tC and aC are the components associated with
subgraph C and h(?) is a local feature vector func-
tion. In general, exact inference is exponential in
the width of tree-decomposition of G, but, given a
fixed width, they can be solved in polynomial time
using dynamic programming. For example, when
the graph has a sequential structure, exact infer-
ence can be carried out using the familiar forward-
backward algorithm (Lafferty et al, 2001). Al-
though our features look at more structure than this,
they are designed to keep treewidth low, meaning
exact inference is still possible with dynamic pro-
gramming. Figure 1 gives a graphical representation
of our model as well as the more familiar genera-
tive (directed) variants. The edge set in the depicted
graph is determined by the features that we use (?4).
3.1 Parameter Learning
To learn the parameters of our model, we select the
?? that minimizes the `1 regularized conditional log-
likelihood of a set of training data T :
L(?) =?
?
?s,t??T
log
?
a
p?(t, a | s, n) + ?
?
k
|?k| .
Because of the `1 penalty, this objective is not every-
where differentiable, but the gradient with respect to
the parameters of the log-likelihood term is as fol-
lows.
?L
??
=
?
?s,t??T
Ep?(a|s,t,n)[H(?)]? Ep?(t,a|s,n)[H(?)]
(1)
To optimize L, we employ an online method that
approximates `1 regularization and only depends on
2One way to understand expressiveness is in terms of inde-
pendence assumptions, of course. Research in graphical models
has done much to relate independence assumptions to the com-
plexity of inference algorithms (Koller and Friedman, 2009).
the gradient of the unregularized objective (Tsu-
ruoka et al, 2009). This method is quite attrac-
tive since it is only necessary to represent the active
features, meaning impractically large feature spaces
can be searched provided the regularization strength
is sufficiently high. Additionally, not only has this
technique been shown to be very effective for opti-
mizing convex objectives, but evidence suggests that
the stochasticity of online algorithms often results
in better solutions than batch optimizers for non-
convex objectives (Liang and Klein, 2009). On ac-
count of the latent alignment variable in our model,
L is non-convex (as is the likelihood objective of the
generative variant).
To choose the regularization strength ? and the
initial learning rate ?0,3 we trained several mod-
els on a 10,000-sentence-pair subset of the French-
English Hansards, and chose values that minimized
the alignment error rate, as evaluated on a 447 sen-
tence set of manually created alignments (Mihalcea
and Pedersen, 2003). For the remainder of the ex-
periments, we use the values we obtained, ? = 0.4
and ?0 = 0.3.
3.2 Inference with WFSAs
We now describe how to use weighted finite-state
automata (WFSAs) to compute the quantities neces-
sary for training. We begin by describing the ideal
WFSA representing the full translation search space,
which we call the discriminative neighborhood, and
then discuss strategies for reducing its size in the
next section, since the full model is prohibitively
large, even with small data sets.
For each training instance ?s, t?, the contribution
to the gradient (Equation 1) is the difference in two
vectors of expectations. The first term is the ex-
pected value of H(?) when observing ?s, n, t? and
letting a range over all possible alignments. The
second is the expectation of the same function, but
observing only ?s, n? and letting t? and a take on
any possible values (i.e., all possible translations
of length n and all their possible alignments to s).
To compute these expectations, we can construct
a WFSA representing the discriminative neighbor-
hood, the set ?n?[0,m]n, such that every path from
the start state to goal yields a pair ?t?, a? with weight
3For the other free parameters of the algorithm, we use the
default values recommended by Tsuruoka et al (2009).
411
a1
a
2
a
3
a
n
t
1
t
2
t
3
t
n
s
n
Fully directed model (Brown et al, 1993;
Vogel et al, 1996; Berg-Kirkpatrick et al, 2010)
Our model
...
a
1
a
2
a
3
a
n
t
1
t
2
t
3
t
n
s
n
...
......
s
s s s s
ss s
Figure 1: A graphical representation of a conventional generative lexical translation model (left) and our model with
an undirected translation model. For clarity, the observed node s (representing the full source sentence) is drawn in
multiple locations. The dashed lines indicate a dependency on a deterministic mapping of tj (not its complete value).
H(t?, a, s, n). With our feature set (?4), number of
states in this WFSA isO(m?n) since at each target
index j, there is a different state for each possible in-
dex of the source word translated at position j ? 1.4
Once the WFSA representing the discriminative
neighborhood is built, we use the forward-backward
algorithm to compute the second expectation term.
We then intersect the WFSA with an unweighted
FSA representing the target sentence t (because of
the restricted structure of our WFSA, this amounts
to removing edges), and finally run the forward-
backward algorithm on the resulting WFSA to com-
pute the first expectation.
3.3 Shrinking the Discriminative
Neighborhood
The WFSA we constructed requires m? |?| transi-
tions between all adjacent states, which is impracti-
cally large. We can reduce the number of edges by
restricting the set of words that each source word can
translate into. Thus, the model will not discriminate
4States contain a bit more information than the index of the
previous source word, for example, there is some additional in-
formation about the previous translation decision that is passed
forward. However, the concept of splitting states to guarantee
distinct paths for different values of non-local features is well
understood by NLP and machine translation researchers, and
the necessary state structure should be obvious from the feature
description.
among all candidate target strings in ?n, but rather
in ?ns , where ?s =
?m
i=1 ?si , and where ?s is the
set of target words that s may translate into.5
We consider four different definitions of ?s: (1)
the baseline of the full target vocabulary, (2) the set
of all target words that co-occur in sentence pairs
containing s, (3) the most probable words under
IBM Model 1 that are above a threshold, and (4) the
same Model 1, except we add a sparse symmetric
Dirichlet prior (? = 0.01) on the translation distri-
butions and use the empirical Bayes (EB) method to
infer a point estimate, using variational inference.
Table 1: Comparison of alternative definitions ?s (arrows
indicate whether higher or lower is better).
?s time (s) ?
?
s |?s| ? AER ?
= ? 22.4 86.0M 0.0
co-occ. 8.9 0.68M 0.0
Model 1 0.2 0.38M 6.2
EB-Model 1 1.0 0.15M 2.9
Table 1 compares the average per-sentence time
required to run the inference algorithm described
5Future work will explore alternative formulations of the
discriminative neighborhood with the goal of further improving
inference efficiency. Smith and Eisner (2005) show that good
performance on unsupervised syntax learning is possible even
when learning from very small discriminative neighborhoods,
and we posit that the same holds here.
412
above under these four different definitions of ?s on
a 10,000 sentence subset of the Hansards French-
English corpus that includes manual word align-
ments. While our constructions guarantee that all
references are reachable even in the reduced neigh-
borhoods, not all alignments between source and tar-
get are possible. The last column is the oracle AER.
Although EB variant of Model 1 neighborhood is
slightly more expensive to do inference with than
regular Model 1, we use it because it has a lower
oracle AER.6
During alignment prediction (rather than during
training) for a sentence pair ?s, t?, it is possible to
further restrict ?s to be just the set of words occur-
ring in t, making extremely fast inference possible
(comparable to that of the generative HMM align-
ment model).
4 Features
Feature engineering lets us encode knowledge about
what aspects of a translation derivation are useful in
predicting whether it is good or not. In this section
we discuss the features we used in our model. Many
of these were taken from the discriminative align-
ment modeling literature, but we also note that our
features can be much more fine-grained than those
used in supervised alignment modeling, since we
learn our models from a large amount of parallel
data, rather than a small number of manual align-
ments.
Word association features. Word association fea-
tures are at the heart of all lexical translation models,
whether generative or discriminative. In addition to
fine-grained boolean indicator features ?saj , tj? for
pair types, we have several orthographic features:
identity, prefix identity, and an orthographic simi-
larity measure designed to be informative for pre-
dicting the translation of named entities in languages
that use similar alphabets.7 It has the property that
source-target pairs of long words that are similar are
given a higher score than word pairs that are short
and similar (dissimilar pairs have a score near zero,
6We included all translations whose probability was within
a factor of 10?4 of the highest probability translation.
7In experiments with Urdu, which uses an Arabic-derived
script, the orthographic feature was computed after first ap-
plying a heuristic Romanization, which made the orthographic
forms somewhat comparable.
regardless of length). We also include ?global? asso-
ciation scores that are precomputed by looking at the
full training data: Dice?s coefficient (discretized),
which we use to measure association strength be-
tween pairs of source and target word types across
sentence pairs (Dice, 1945), IBM Model 1 forward
and reverse probabilities, and the geometric mean of
the Model 1 forward and reverse probabilities. Fi-
nally, we also cluster the source and target vocab-
ularies (Och, 1999) and include class pair indicator
features, which can learn generalizations that, e.g.,
?nouns tend to translate into nouns but not modal
verbs.?
Positional features. Following Blunsom and
Cohn (2006), we include features indicating
closeness to the alignment matrix diagonal,
h(aj , j,m, n) =
?
?
?
aj
m ?
j
n
?
?
?. We also conjoin this
feature with the source word class type indicator to
enable the model to learn that certain word types
are more or less likely to favor a location on the
diagonal (e.g. Urdu?s sentence-final verbs).
Source features. Some words are functional el-
ements that fulfill purely grammatical roles and
should not be the ?source? of a translation. For ex-
ample, Romance languages require a preposition in
the formation of what could be a noun-noun com-
pound in English, thus, it may be useful to learn not
to translate certain words (i.e. they should not par-
ticipate in alignment links), or to have a bias to trans-
late others. To capture this intuition we include an
indicator feature that fires each time a source vocab-
ulary item (and source word class) participates in an
alignment link.
Source path features. One class of particularly
useful features assesses the goodness of the align-
ment ?path? through the source sentence (Vogel et
al., 1996). Although assessing the predicted path
requires using nonlocal features, since each aj ?
[0,m] and m is relatively small, features can be sen-
sitive to a wider context than is often practical.
We use many overlapping source path features,
some of which are sensitive to the distance and di-
rection of the jump between aj?1 and aj , and oth-
ers which are sensitive to the word pair these two
points define, and others that combine all three el-
ements. The features we use include a discretized
413
jump distance, the discretized jump conjoined with
an indicator feature for the target length n, the dis-
cretized jump feature conjoined with the class of saj ,
and the discretized jump feature conjoined with the
class of saj and saj?1 . To discretize the features we
take a log transform (base 1.3) of the jump width and
let an indicator feature fire for the closest integer.
In addition to these distance-dependent features, we
also include indicator features that fire on bigrams
?saj?1 , saj ? and their word classes. Thus, this fea-
ture can capture our intuition that, e.g., adjectives
are more likely to come before or after a noun in
different languages.
Target string features. Features sensitive to mul-
tiple values in the predicted target string or latent
alignment variable must be handled carefully for the
sake of computational tractability. While features
that look at multiple source words can be computed
linearly in the number of source words considered
(since the source string is always observable), fea-
tures that look at multiple target words require ex-
ponential time and space!8 However, by grouping
the tj?s into coarse equivalence classes and looking
at small numbers of variables, it is possible to incor-
porate such features. We include a feature that fires
when a word translates as itself (for example, a name
or a date, which occurs in languages that share the
same alphabet) in position j, but then is translated
again (as something else) in position j ? 1 or j + 1.
5 Experiments
We now turn to an empirical assessment of our
model. Using various datasets, we evaluate the
performance of the models? intrinsic quality and
theirtheir alignments? contribution to a standard ma-
chine translation system. We make use of parallel
corpora from languages with very different typolo-
gies: a small (0.8M words) Chinese-English corpus
from the tourism and travel domain (Takezawa et al,
2002), a corpus of Czech-English news commen-
tary (3.1M words),9 and an Urdu-English corpus
(2M words) provided by NIST for the 2009 Open
MT Evaluation. These pairs were selected since
each poses different alignment challenges (word or-
8This is of course what makes history-based language model
integration an inference challenge in translation.
9http://statmt.org/wmt10
der in Chinese and Urdu, morphological complex-
ity in Czech, and a non-alphabetic writing system in
Chinese), and confining ourselves to these relatively
small corpora reduced the engineering overhead of
getting an implementation up and running. Future
work will explore the scalability characteristics and
limits of the model.
5.1 Methodology
For each language pair, we train two log-linear
translation models as described above (?3), once
with English as the source and once with English
as the target language. For a baseline, we use
the Giza++ toolkit (Och and Ney, 2003) to learn
Model 4, again in both directions. We symmetrize
the alignments from both model types using the
grow-diag-final-and heuristic (Koehn et al,
2003) producing, in total, six alignment sets. We
evaluate them both intrinsically and in terms of their
performance in a translation system.
Since we only have gold alignments for Czech-
English (Bojar and Prokopova?, 2006), we can re-
port alignment error rate (AER; Och and Ney, 2003)
only for this pair. However, we offer two further
measures that we believe are suggestive and that
do not require gold alignments. One is the aver-
age alignment ?fertility? of source words that occur
only a single time in the training data (so-called ha-
pax legomena). This assesses the impact of a typical
alignment problem observed in generative models
trained to maximize likelihood: infrequent source
words act as ?garbage collectors?, with many target
words aligned to them (the word dislike in the Model
4 alignment in Figure 2 is an example). Thus, we ex-
pect lower values of this measure to correlate with
better alignments. The second measure is the num-
ber of rule types learned in the grammar induction
process used for translation that match the transla-
tion test sets.10 While neither a decrease in the aver-
age singleton fertility nor an increase in the number
of rules induced guarantees better alignment quality,
we believe it is reasonable to assume that they are
positively correlated.
For the translation experiments in each language
pair, we make use of the cdec decoder (Dyer et al,
10This measure does not assess whether the rule types are
good or bad, but it does suggest that the system?s coverage is
greater.
414
2010), inducing a hierarchical phrase based trans-
lation grammar from two sets of symmetrized align-
ments using the method described by Chiang (2007).
Additionally, recent work that has demonstrated that
extracting rules from n-best alignments has value
(Liu et al, 2009; Venugopal et al, 2008). We
therefore define a third condition where rules are
extracted from the corpus under both the Model 4
and discriminative alignments and merged to form
a single grammar. We incorporate a 3-gram lan-
guage model learned from the target side of the
training data as well as 50M supplemental words
of monolingual training data consisting of sentences
randomly sampled from the English Gigaword, ver-
sion 4. In the small Chinese-English travel domain
experiment, we just use the LM estimated from the
bitext. The parameters of the translation model were
tuned using ?hypergraph? minimum error rate train-
ing (MERT) to maximize BLEU on a held-out de-
velopment set (Kumar et al, 2009). Results are
reported using case-insensitive BLEU (Papineni et
al., 2002), METEOR11 (Lavie and Denkowski, 2009),
and TER (Snover et al, 2006), with the number of
references varying by task. Since MERT is a non-
deterministic optimization algorithm and results can
vary considerably between runs, we follow Clark et
al. (2011) and report the average score and stan-
dard deviation of 5 independent runs, 30 in the case
of Chinese-English, since observed variance was
higher.
5.2 Experimental Results
Czech-English. Czech-English poses problems
for word alignment models since, unlike English,
Czech words have a complex inflectional morphol-
ogy, and the syntax permits relatively free word or-
der. For this language pair, we evaluate alignment
error rate using the manual alignment corpus de-
scribed by Bojar and Prokopova? (2006). Table 2
summarizes the results.
Chinese-English. Chinese-English poses a differ-
ent set of problems for alignment. While Chinese
words have rather simple morphology, the Chinese
writing system renders our orthographic features
useless. Despite these challenges, the Chinese re-
11Meteor 1.0 with exact, stem, synonymy, and paraphrase
modules and HTER parameters.
Table 2: Czech-English experimental results. ??sing. is the
average fertility of singleton source words.
AER ? ??sing. ? # rules ?
Model 4 e | f 24.8 4.1
f | e 33.6 6.6
sym. 23.4 2.7 993,953
Our model e | f 21.9 2.3
f | e 29.3 3.8
sym. 20.5 1.6 1,146,677
Alignment BLEU ? METEOR ? TER ?
Model 4 16.3?0.2 46.1?0.1 67.4?0.3
Our model 16.5?0.1 46.8?0.1 67.0?0.2
Both 17.4?0.1 47.7?0.1 66.3?0.5
sults in Table 3 show the same pattern of results as
seen in Czech-English.
Table 3: Chinese-English experimental results.
??sing. ? # rules ?
Model 4 e | f 4.4
f | e 3.9
sym. 3.6 52,323
Our model e | f 3.5
f | e 2.6
sym. 3.1 54,077
Alignment BLEU ? METEOR ? TER ?
Model 4 56.5?0.3 73.0?0.4 29.1?0.3
Our model 57.2?0.8 73.8?0.4 29.3?1.1
Both 59.1?0.6 74.8?0.7 27.6?0.5
Urdu-English. Urdu-English is a more challeng-
ing language pair for word alignment than the pre-
vious two we have considered. The parallel data is
drawn from numerous genres, and much of it was ac-
quired automatically, making it quite noisy. So our
models must not only predict good translations, they
must cope with bad ones as well. Second, there has
been no previous work on discriminative modeling
of Urdu, since, to our knowledge, no manual align-
ments have been created. Finally, unlike English,
Urdu is a head-final language: not only does it have
SOV word order, but rather than prepositions, it has
post-positions, which follow the nouns they modify,
meaning its large scale word order is substantially
415
different from that of English. Table 4 demonstrates
the same pattern of improving results with our align-
ment model.
Table 4: Urdu-English experimental results.
??sing. ? # rules ?
Model 4 e | f 6.5
f | e 8.0
sym. 3.2 244,570
Our model e | f 4.8
f | e 8.3
sym. 2.3 260,953
Alignment BLEU ? METEOR ? TER ?
Model 4 23.3?0.2 49.3?0.2 68.8?0.8
Our model 23.4?0.2 49.7?0.1 67.7?0.2
Both 24.1?0.2 50.6?0.1 66.8?0.5
5.3 Analysis
The quantitative results presented in this section
strongly suggest that our modeling approach pro-
duces better alignments. In this section, we try to
characterize how the model is doing what it does
and what it has learned. Because of the `1 regular-
ization, the number of active (non-zero) features in
the inferred models is small, relative to the number
of features considered during training. The num-
ber of active features ranged from about 300k for
the small Chinese-English corpus to 800k for Urdu-
English, which is less than one tenth of the available
features in both cases. In all models, the coarse fea-
tures (Model 1 probabilities, Dice coefficient, coarse
positional features, etc.) typically received weights
with large magnitudes, but finer features also played
an important role.
Language pair differences manifested themselves
in many ways in the models that were learned.
For example, orthographic features were (unsurpris-
ingly) more valuable in Czech-English, with their
largely overlapping alphabets, than in Chinese or
Urdu. Examining the more fine-grained features is
also illuminating. Table 5 shows the most highly
weighted source path bigram features on the three
models where English was the source language, and
in each, we may observe some interesting character-
istics of the target language. Left-most is English-
Czech. At first it may be surprising that words like
since and that have a highly weighted feature for
transitioning to themselves. However, Czech punc-
tuation rules require that relative clauses and sub-
ordinating conjunctions be preceded by a comma
(which is only optional or outright forbidden in En-
glish), therefore our model translates these words
twice, once to produce the comma, and a second
time to produce the lexical item. The middle col-
umn is the English-Chinese model. In the training
data, many of the sentences are questions directed to
a second person, you. However, Chinese questions
do not invert and the subject remains in the canon-
ical first position, thus the transition from the start
of sentence to you is highly weighted. Finally, Fig-
ure 2 illustrates how Model 4 (left) and our discrimi-
native model (right) align an English-Urdu sentence
pair (the English side is being conditioned on in both
models). A reflex of Urdu?s head-final word order
is seen in the list of most highly weighted bigrams,
where a path through the English source where verbs
that transition to end-of-sentence periods are predic-
tive of good translations into Urdu.
Table 5: The most highly weighted source path bigram
features in the English-Czech, -Chinese, and -Urdu mod-
els.
Bigram ?k
. ?/s? 3.08
like like 1.19
one of 1.06
? . 0.95
that that 0.92
is but 0.92
since since 0.84
?s? when 0.83
, how 0.83
, not 0.83
Bigram ?k
. ?/s? 2.67
? ? 2.25
?s? please 2.01
much ? 1.61
?s? if 1.58
thank you 1.47
?s? sorry 1.46
?s? you 1.45
please like 1.24
?s? this 1.19
Bigram ?k
. ?/s? 1.87
?s? this 1.24
will . 1.17
are . 1.16
is . 1.09
is that 1.00
have . 0.97
has . 0.96
was . 0.91
will ?/s? 0.88
6 Related Work
The literature contains numerous descriptions of dis-
criminative approaches to word alignment motivated
by the desire to be able to incorporate multiple,
overlapping knowledge sources (Ayan et al, 2005;
Moore, 2005; Taskar et al, 2005; Blunsom and
Cohn, 2006; Haghighi et al, 2009; Liu et al, 2010;
DeNero and Klein, 2010; Setiawan et al, 2010).
This body of work has been an invaluable source
of useful features. Several authors have dealt with
the problem training log-linear models in an unsu-
416
IBM Model 4 alignment Our model's alignment
Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model
4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model
does not exhibit these problems, and in fact, makes no mistakes in the alignment.
pervised setting. The contrastive estimation tech-
nique proposed by Smith and Eisner (2005) is glob-
ally normalized (and thus capable of dealing with ar-
bitrary features), and closely related to the model we
developed; however, they do not discuss the problem
of word alignment. Berg-Kirkpatrick et al (2010)
learn locally normalized log-linear models in a gen-
erative setting. Globally normalized discriminative
models with latent variables (Quattoni et al, 2004)
have been used for a number of language processing
problems, including MT (Dyer and Resnik, 2010;
Blunsom et al, 2008a). However, this previous
work relied on translation grammars constructed us-
ing standard generative word alignment processes.
7 Future Work
While we have demonstrated that this model can be
substantially useful, it is limited in some important
ways which are being addressed in ongoing work.
First, training is expensive, and we are exploring al-
ternatives to the conditional likelihood objective that
is currently used, such as contrastive neighborhoods
advocated by (Smith and Eisner, 2005). Addition-
ally, there is much evidence that non-local features
like the source word fertility are (cf. IBM Model 3)
useful for translation and alignment modeling. To be
truly general, it must be possible to utilize such fea-
tures. Unfortunately, features like this that depend
on global properties of the alignment vector, a, make
the inference problem NP-hard, and approximations
are necessary. Fortunately, there is much recent
work on approximate inference techniques for incor-
porating nonlocal features (Blunsom et al, 2008b;
Gimpel and Smith, 2009; Cromie`res and Kurohashi,
2009; Weiss and Taskar, 2010), suggesting that this
problem too can be solved using established tech-
niques.
8 Conclusion
We have introduced a globally normalized, log-
linear lexical translation model that can be trained
discriminatively using only parallel sentences,
which we apply to the problem of word alignment.
Our approach addresses two important shortcomings
of previous work: (1) that local normalization of
generative models constrains the features that can be
used, and (2) that previous discriminatively trained
word alignment models required supervised align-
ments. According to a variety of measures in a vari-
ety of translation tasks, this model produces superior
alignments to generative approaches. Furthermore,
the features learned by our model reveal interesting
characteristics of the language pairs being modeled.
Acknowledgments
This work was supported in part by the DARPA GALE
program; the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
417
ber W911NF-10-1-0533; and the National Science Foun-
dation through grants IIS-0844507, IIS-0915187, IIS-
0713402, and IIS-0915327 and through TeraGrid re-
sources provided by the Pittsburgh Supercomputing Cen-
ter under grant number TG-DBS110003. We thank
Ondr?ej Bojar for providing the Czech-English alignment
data, and three anonymous reviewers for their detailed
suggestions and comments on an earlier draft of this pa-
per.
References
N. F. Ayan, B. J. Dorr, and C. Monz. 2005. NeurAlign:
combining word alignments using neural networks. In
Proc. of HLT-EMNLP.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2006. Discriminative word
alignment with conditional random fields. In Proc. of
ACL.
P. Blunsom, T. Cohn, and M. Osborne. 2008a. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2008b. Proba-
bilistic inference for machine translation. In Proc. of
EMNLP 2008.
O. Bojar and M. Prokopova?. 2006. Czech-English word
alignment. In Proc. of LREC.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Bet-
ter hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proc. of
ACL.
F. Cromie`res and S. Kurohashi. 2009. An alignment al-
gorithm using belief propagation and a structure-based
distortion model. In Proc. of EACL.
J. DeNero and D. Klein. 2010. Discriminative modeling
of extraction sets for machine translation. In Proc. of
ACL.
L. R. Dice. 1945. Measures of the amount of eco-
logic association between species. Journal of Ecology,
26:297?302.
C. Dyer and P. Resnik. 2010. Context-free reordering,
finite-state translation. In Proc. of NAACL.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL (demonstration session).
A. Fraser. 2007. Improved Word Alignments for Statis-
tical Machine Translation. Ph.D. thesis, University of
Southern California.
K. Gimpel and N. A. Smith. 2009. Cube summing, ap-
proximate inference with non-local features, and dy-
namic programming without semirings. In Proc. of
EACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL-IJCNLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL-IJCNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. Lavie and M. Denkowski. 2009. The METEOR metric
for automatic evaluation of machine translation. Ma-
chine Translation Journal, 23(2?3):105?115.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
Y. Liu, T. Xia, X. Xiao, and Q. Liu. 2009. Weighted
alignment matrices for statistical machine translation.
In Proc. of EMNLP.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3):303?339.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proc. of the Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proc. of HLT-EMNLP.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 1999. An efficient method for determining bilin-
gual word classes. In Proc. of EACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
418
A. Quattoni, M. Collins, and T. Darrell. 2004. Condi-
tional random fields for object recognition. In NIPS
17.
H. Setiawan, C. Dyer, and P. Resnik. 2010. Discrimina-
tive word alignment with a function word reordering
model. In Proc. of EMNLP.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
M. Snover, B. J. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proc. of LREC.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. of HLT-EMNLP.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL-IJCNLP.
A. Venugopal, A. Zollmann, N. A. Smith, and S. Vogel.
2008. Wider pipelines: n-best alignments and parses
in MT training. In Proc. of AMTA.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS.
419
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176?181,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Better Hypothesis Testing for Statistical Machine Translation:
Controlling for Optimizer Instability
Jonathan H. Clark Chris Dyer Alon Lavie Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jhclark,cdyer,alavie,nasmith}@cs.cmu.edu
Abstract
In statistical machine translation, a researcher
seeks to determine whether some innovation
(e.g., a new feature, model, or inference al-
gorithm) improves translation quality in com-
parison to a baseline system. To answer this
question, he runs an experiment to evaluate the
behavior of the two systems on held-out data.
In this paper, we consider how to make such
experiments more statistically reliable. We
provide a systematic analysis of the effects of
optimizer instability?an extraneous variable
that is seldom controlled for?on experimen-
tal outcomes, and make recommendations for
reporting results more accurately.
1 Introduction
The need for statistical hypothesis testing for ma-
chine translation (MT) has been acknowledged since
at least Och (2003). In that work, the proposed
method was based on bootstrap resampling and was
designed to improve the statistical reliability of re-
sults by controlling for randomness across test sets.
However, there is no consistently used strategy that
controls for the effects of unstable estimates of
model parameters.1 While the existence of opti-
mizer instability is an acknowledged problem, it is
only infrequently discussed in relation to the relia-
bility of experimental results, and, to our knowledge,
there has yet to be a systematic study of its effects on
1We hypothesize that the convention of ?trusting? BLEU
score improvements of, e.g., > 1, is not merely due to an ap-
preciation of what qualitative difference a particular quantita-
tive improvement will have, but also an implicit awareness that
current methodology leads to results that are not consistently
reproducible.
hypothesis testing. In this paper, we present a series
of experiments demonstrating that optimizer insta-
bility can account for substantial amount of variation
in translation quality,2 which, if not controlled for,
could lead to incorrect conclusions. We then show
that it is possible to control for this variable with a
high degree of confidence with only a few replica-
tions of the experiment and conclude by suggesting
new best practices for significance testing for ma-
chine translation.
2 Nondeterminism and Other
Optimization Pitfalls
Statistical machine translation systems consist of a
model whose parameters are estimated to maximize
some objective function on a set of development
data. Because the standard objectives (e.g., 1-best
BLEU, expected BLEU, marginal likelihood) are
not convex, only approximate solutions to the op-
timization problem are available, and the parame-
ters learned are typically only locally optimal and
may strongly depend on parameter initialization and
search hyperparameters. Additionally, stochastic
optimization and search techniques, such as mini-
mum error rate training (Och, 2003) and Markov
chain Monte Carlo methods (Arun et al, 2010),3
constitute a second, more obvious source of noise
in the optimization procedure.
This variation in the parameter vector affects the
quality of the model measured on both development
2This variation directly affects the output translations, and
so it will propagate to both automated metrics as well as human
evaluators.
3Online subgradient techniques such as MIRA (Crammer et
al., 2006; Chiang et al, 2008) have an implicit stochastic com-
ponent as well based on the order of the training examples.
176
data and held-out test data, independently of any ex-
perimental manipulation. Thus, when trying to de-
termine whether the difference between two mea-
surements is significant, it is necessary to control for
variance due to noisy parameter estimates. This can
be done by replication of the optimization procedure
with different starting conditions (e.g., by running
MERT many times).
Unfortunately, common practice in reporting ma-
chine translation results is to run the optimizer once
per system configuration and to draw conclusions
about the experimental manipulation from this sin-
gle sample. However, it could be that a particu-
lar sample is on the ?low? side of the distribution
over optimizer outcomes (i.e., it results in relatively
poorer scores on the test set) or on the ?high? side.
The danger here is obvious: a high baseline result
paired with a low experimental result could lead to a
useful experimental manipulation being incorrectly
identified as useless. We now turn to the question of
how to reduce the probability falling into this trap.
3 Related Work
The use of statistical hypothesis testing has grown
apace with the adoption of empirical methods in
natural language processing. Bootstrap techniques
(Efron, 1979; Wasserman, 2003) are widespread
in many problem areas, including for confidence
estimation in speech recognition (Bisani and Ney,
2004), and to determine the significance of MT re-
sults (Och, 2003; Koehn, 2004; Zhang et al, 2004;
Zhang and Vogel, 2010). Approximate randomiza-
tion (AR) has been proposed as a more reliable tech-
nique for MT significance testing, and evidence sug-
gests that it yields fewer type I errors (i.e., claiming
a significant difference where none exists; Riezler
and Maxwell, 2005). Other uses in NLP include
the MUC-6 evaluation (Chinchor, 1993) and pars-
ing (Cahill et al, 2008). However, these previous
methods assume model parameters are elements of
the system rather than extraneous variables.
Prior work on optimizer noise in MT has fo-
cused primarily on reducing optimizer instability
(whereas our concern is how to deal with optimizer
noise, when it exists). Foster and Kuhn (2009) mea-
sured the instability of held-out BLEU scores across
10 MERT runs to improve tune/test set correlation.
However, they only briefly mention the implications
of the instability on significance. Cer et al (2008)
explored regularization of MERT to improve gener-
alization on test sets. Moore and Quirk (2008) ex-
plored strategies for selecting better random ?restart
points? in optimization. Cer et al (2010) analyzed
the standard deviation over 5 MERT runs when each
of several metrics was used as the objective function.
4 Experiments
In our experiments, we ran the MERT optimizer to
optimize BLEU on a held-out development set many
times to obtain a set of optimizer samples on two dif-
ferent pairs of systems (4 configurations total). Each
pair consists of a baseline system (System A) and an
?experimental? system (System B), which previous
research has suggested will perform better.
The first system pair contrasts a baseline phrase-
based system (Moses) and experimental hierarchi-
cal phrase-based system (Hiero), which were con-
structed from the Chinese-English BTEC corpus
(0.7M words), the later of which was decoded with
the cdec decoder (Koehn et al, 2007; Chiang, 2007;
Dyer et al, 2010). The second system pair con-
trasts two German-English Hiero/cdec systems con-
structed from the WMT11 parallel training data
(98M words).4 The baseline system was trained on
unsegmented words, and the experimental system
was constructed using the most probable segmenta-
tion of the German text according to the CRF word
segmentation model of Dyer (2009). The Chinese-
English systems were optimized 300 times, and the
German-English systems were optimized 50 times.
Our experiments used the default implementation
of MERT that accompanies each of the two de-
coders. The Moses MERT implementation uses 20
random restart points per iteration, drawn uniformly
from the default ranges for each feature, and, at each
iteration, 200-best lists were extracted with the cur-
rent weight vector (Bertoldi et al, 2009). The cdec
MERT implementation performs inference over the
decoder search space which is structured as a hyper-
graph (Kumar et al, 2009). Rather than using restart
points, in addition to optimizing each feature inde-
pendently, it optimizes in 5 random directions per it-
eration by constructing a search vector by uniformly
sampling each element of the vector from (?1, 1)
and then renormalizing so it has length 1. For all
systems, the initial weight vector was manually ini-
tialized so as to yield reasonable translations.
4http://statmt.org/wmt11/
177
Metric System Avg ssel sdev stest
BTEC Chinese-English (n = 300)
BLEU ?
System A 48.4 1.6 0.2 0.5
System B 49.9 1.5 0.1 0.4
MET ?
System A 63.3 0.9 - 0.4
System B 63.8 0.9 - 0.5
TER ?
System A 30.2 1.1 - 0.6
System B 28.7 1.0 - 0.2
WMT German-English (n = 50)
BLEU ?
System A 18.5 0.3 0.0 0.1
System B 18.7 0.3 0.0 0.2
MET ?
System A 49.0 0.2 - 0.2
System B 50.0 0.2 - 0.1
TER ?
System A 65.5 0.4 - 0.3
System B 64.9 0.4 - 0.4
Table 1: Measured standard deviations of different au-
tomatic metrics due to test-set and optimizer variability.
sdev is reported only for the tuning objective function
BLEU.
Results are reported using BLEU (Papineni et
al., 2002), METEOR5 (Banerjee and Lavie, 2005;
Denkowski and Lavie, 2010), and TER (Snover et
al., 2006).
4.1 Extraneous variables in one system
In this section, we describe and measure (on the ex-
ample systems just described) three extraneous vari-
ables that should be considered when evaluating a
translation system. We quantify these variables in
terms of standard deviation s, since it is expressed
in the same units as the original metric. Refer to
Table 1 for the statistics.
Local optima effects sdev The first extraneous
variable we discuss is the stochasticity of the opti-
mizer. As discussed above, different optimization
runs find different local maxima. The noise due to
this variable can depend on many number of fac-
tors, including the number of random restarts used
(in MERT), the number of features in a model, the
number of references, the language pair, the portion
of the search space visible to the optimizer (e.g. 10-
best, 100-best, a lattice, a hypergraph), and the size
of the tuning set. Unfortunately, there is no proxy to
estimate this effect as with bootstrap resampling. To
control for this variable, we must run the optimizer
multiple times to estimate the spread it induces on
the development set. Using the n optimizer samples,
with mi as the translation quality measurement of
5METEOR version 1.2 with English ranking parameters and
all modules.
the development set for the ith optimization run, and
m is the average of all mis, we report the standard
deviation over the tuning set as sdev:
sdev =
?
?
?
?
n?
i=1
(mi ?m)
2
n? 1
A high sdev value may indicate that the optimizer is
struggling with local optima and changing hyperpa-
rameters (e.g. more random restarts in MERT) could
improve system performance.
Overfitting effects stest As with any optimizer,
there is a danger that the optimal weights for a tuning
set may not generalize well to unseen data (i.e., we
overfit). For a randomized optimizer, this means that
parameters can generalize to different degrees over
multiple optimizer runs. We measure the spread in-
duced by optimizer randomness on the test set met-
ric score stest, as opposed to the overfitting effect in
isolation. The computation of stest is identical to sdev
except that the mis are the translation metrics cal-
culated on the test set. In Table 1, we observe that
stest > sdev, indicating that optimized parameters are
likely not generalizing well.
Test set selection ssel The final extraneous vari-
able we consider is the selection of the test set it-
self. A good test set should be representative of
the domain or language for which experimental ev-
idence is being considered. However, with only a
single test corpus, we may have unreliable results
because of idiosyncrasies in the test set. This can
be mitigated in two ways. First, replication of ex-
periments by testing on multiple, non-overlapping
test sets can eliminate it directly. Since this is not
always practical (more test data may not be avail-
abile), the widely-used bootstrap resampling method
(?3) also controls for test set effects by resampling
multiple ?virtual? test sets from a single set, making
it possible to infer distributional parameters such as
the standard deviation of the translation metric over
(very similar) test sets.6 Furthermore, this can be
done for each of our optimizer samples. By averag-
ing the bootstrap-estimated standard deviations over
6Unlike actually using multiple test sets, bootstrap resam-
pling does not help to re-estimate the mean metric score due to
test set spread (unlike actually using multiple test sets) since the
mean over bootstrap replicates is approximately the aggregate
metric score.
178
optimizer samples, we have a statistic that jointly
quantifies the impact of test set effects and optimizer
instability on a test set. We call this statistic ssel.
Different values of this statistic can suggest method-
ological improvements. For example, a large ssel in-
dicates that more replications will be necessary to
draw reliable inferences from experiments on this
test set, so a larger test set may be helpful.
To compute ssel, assume we have n indepen-
dent optimization runs which produced weight vec-
tors that were used to translate a test set n times.
The test set has ` segments with references R =
?R1, R2, . . . , R`?. Let X = ?X1,X2, . . . ,Xn?
where each Xi = ?Xi1, Xi2, . . . , Xi`? is the list of
translated segments from the ith optimization run
list of the ` translated segments of the test set. For
each hypothesis output Xi, we construct k bootstrap
replicates by drawing ` segments uniformly, with re-
placement, from Xi, together with its corresponding
reference. This produces k virtual test sets for each
optimization run i. We designate the score of the jth
virtual test set of the ith optimization run with mij .
If mi = 1k
?k
j=1 mij , then we have:
si =
?
?
?
?
k?
j=1
(mij ?mi)
2
k ? 1
ssel =
1
n
n?
i=1
si
4.2 Comparing Two Systems
In the previous section, we gave statistics about
the distribution of evaluation metrics across a large
number of experimental samples (Table 1). Because
of the large number of trials we carried out, we can
be extremely confident in concluding that for both
pairs of systems, the experimental manipulation ac-
counts for the observed metric improvements, and
furthermore, that we have a good estimate of the
magnitude of that improvement. However, it is not
generally feasible to perform as many replications
as we did, so here we turn to the question of how
to compare two systems, accounting for optimizer
noise, but without running 300 replications.
We begin with a visual illustration how opti-
mizer instability affects test set scores when com-
paring two systems. Figure 1 plots the histogram
of the 300 optimizer samples each from the two
BTEC Chinese-English systems. The phrase-based
46 47 48 49 50 51
BLEU
0
5
10
15
20
25
30
35
40
Ob
se
rv
at
ion
 C
ou
nt
Figure 1: Histogram of test set BLEU scores for the
BTEC phrase-based system (left) and BTEC hierarchical
system (right). While the difference between the systems
is 1.5 BLEU in expectation, there is a non-trivial region
of overlap indicating that some random outcomes will re-
sult in little to no difference being observed.
0.6 0.3 0.0 0.3 0.6 0.9
BLEU difference
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Pr
ob
ab
ilit
y 
of
 o
bs
er
va
tio
n 1 sample
3 samples
5 samples
10 samples
50 samples
Figure 2: Relative frequencies of obtaining differences
in BLEU scores on the WMT system as a function of the
number of optimizer samples. The expected difference
is 0.2 BLEU. While there is a reasonably high chance of
observing a non-trivial improvement (or even a decline)
for 1 sample, the distribution quickly peaks around the
expected value given just a few more samples.
system?s distribution is centered at the sample
mean 48.4, and the hierarchical system is centered
at 49.9, a difference of 1.5 BLEU, correspond-
ing to the widely replicated result that hierarchi-
cal phrase-based systems outperform conventional
phrase-based systems in Chinese-English transla-
tion. Crucially, although the distributions are dis-
tinct, there is a non-trivial region of overlap, and
experimental samples from the overlapping region
could suggest the opposite conclusion!
To further underscore the risks posed by this over-
lap, Figure 2 plots the relative frequencies with
which different BLEU score deltas will occur, as a
function of the number of optimizer samples used.
When is a difference significant? To determine
whether an experimental manipulation results in a
179
statistically reliable difference for an evaluation met-
ric, we use a stratified approximate randomization
(AR) test. This is a nonparametric test that approxi-
mates a paired permutation test by sampling permu-
tations (Noreen, 1989). AR estimates the probability
(p-value) that a measured difference in metric scores
arose by chance by randomly exchanging sentences
between the two systems. If there is no significant
difference between the systems (i.e., the null hypoth-
esis is true), then this shuffling should not change
the computed metric score. Crucially, this assumes
that the samples being analyzed are representative
of all extraneous variables that could affect the out-
come of the experiment. Therefore, we must include
multiple optimizer replications. Also, since metric
scores (such as BLEU) are in general not compa-
rable across test sets, we stratify, exchanging only
hypotheses that correspond to the same sentence.
Table 2 shows the p-values computed by AR, test-
ing the significance of the differences between the
two systems in each pair. The first three rows illus-
trate ?single sample? testing practice. Depending on
luck with MERT, the results can vary widely from
insignificant (at p > .05) to highly significant.
The last two lines summarize the results of the test
when a small number of replications are performed,
as ought to be reasonable in a research setting. In
this simulation, we randomly selected n optimizer
outputs from our large pool and ran the AR test to
determine the significance; we repeated this proce-
dure 250 times. The p-values reported are the p-
values at the edges of the 95% confidence interval
(CI) according to AR seen in the 250 simulated com-
parison scenarios. These indicate that we are very
likely to observe a significant difference for BTEC
at n = 5, and a very significant difference by n = 50
(Table 2). Similarly, we see this trend in the WMT
system: more replications leads to more significant
results, which will be easier to reproduce. Based on
the average performance of the systems reported in
Table 1, we expect significance over a large enough
number of independent trials.
5 Discussion and Recommendations
No experiment can completely control for all pos-
sible confounding variables. Nor are metric scores
(even if they are statistically reliable) a substitute
for thorough human analysis. However, we believe
that the impact of optimizer instability has been ne-
p-value
n System A System B BTEC WMT
1 high low 0.25 0.95
1 median median 0.15 0.13
1 low high 0.0003 0.003
p-value (95% CI)
5 random random 0.001?0.034 0.001?0.38
50 random random 0.001?0.001 0.001?0.33
Table 2: Two-system analysis: AR p-values for three
different ?single sample? scenarios that illustrate differ-
ent pathological scenarios that can result when the sam-
pled weight vectors are ?low? or ?high.? For ?random,?
we simulate an experiments with n optimization replica-
tions by drawing n optimized system outputs from our
pool and performing AR; this simulation was repeated
250 times and the 95% CI of the AR p-values is reported.
glected by standard experimental methodology in
MT research, where single-sample measurements
are too often used to assess system differences. In
this paper, we have provided evidence that optimizer
instability can have a substantial impact on results.
However, we have also shown that it is possible to
control for it with very few replications (Table 2).
We therefore suggest:
? Replication be adopted as standard practice in
MT experimental methodology, especially in
reporting results;7
? Replication of optimization (MERT) and test
set evaluation be performed at least three times;
more replications may be necessary for experi-
mental manipulations with more subtle effects;
? Use of the median system according to a trusted
metric when manually analyzing system out-
put; preferably, the median should be deter-
mined based on one test set and a second test
set should be manually analyzed.
Acknowledgments
We thank Michael Denkowski, Kevin Gimpel, Kenneth
Heafield, Michael Heilman, and Brendan O?Connor for
insightful feedback. This research was supported in part
by the National Science Foundation through TeraGrid re-
sources provided by Pittsburgh Supercomputing Center
under TG-DBS110003; the National Science Foundation
under IIS-0713402, IIS-0844507, IIS-0915187, and IIS-
0915327; the DARPA GALE program, the U. S. Army
Research Laboratory, and the U. S. Army Research Of-
fice under contract/grant number W911NF-10-1-0533.
7Source code to carry out the AR test for multiple optimizer
samples on the three metrics in this paper is available from
http://github.com/jhclark/multeval.
180
References
A. Arun, B. Haddow, P. Koehn, A. Lopez, C. Dyer,
and P. Blunsom. 2010. Monte Carlo techniques
for phrase-based translation. Machine Translation,
24:103?121.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for mt evaluation with improved corre-
lation with human judgments. In Proc. of ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization.
N. Bertoldi, B. Haddow, and J.-B. Fouet. 2009. Im-
proved minimum error rate training in Moses. Prague
Bulletin of Mathematical Linguistics, No. 91:7?16.
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluation.
In Proc. of ICASSP.
A. Cahill, M. Burke, R. O?Donovan, S. Riezler, J. van
Genabith, and A. Way. 2008. Wide-coverage deep
statistical parsing using automatic dependency struc-
ture annotation. Computational Linguistics, 34(1):81?
124.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regular-
ization and search for minimum error rate training. In
Proc. of WMT.
D. Cer, C. D. Manning, and D. Jurafsky. 2010. The best
lexical metric for phrase-based statistical mt system
optimization. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 555?563. Proc. of ACL, June.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
N. Chinchor. 1993. The statistical significance of the
MUC-5 results. Proc. of MUC.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
M. Denkowski and A. Lavie. 2010. Extending the
METEOR machine translation evaluation metric to the
phrase level. In Proc. of NAACL.
C. Dyer, J. Weese, A. Lopez, V. Eidelman, P. Blunsom,
and P. Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proc. of ACL.
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
B. Efron. 1979. Bootstrap methods: Another look at the
jackknife. The Annals of Statistics, 7(1):1?26.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
P. Koehn, A. Birch, C. Callison-burch, M. Federico,
N. Bertoldi, B. Cowan, C. Moran, C. Dyer, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. of ACL-IJCNLP.
R. C. Moore and C. Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proc. of COLING, Manchester, UK.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-j. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
MT. In Proc. of the Workshop on Intrinsic and Extrin-
sic Evaluation Methods for Machine Translation and
Summarization.
M. Snover, B. Dorr, C. Park, R. Schwartz, L. Micciulla,
and J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
L. Wasserman. 2003. All of Statistics: A Concise Course
in Statistical Inference. Springer.
Y. Zhang and S. Vogel. 2010. Significance tests of auto-
matic machine translation metrics. Machine Transla-
tion, 24:51?65.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
BLEU/NIST scores: How much improvement do we
need to have a better system? In Proc. of LREC.
181
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 57?61,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Exploring Normalization Techniques for Human Judgments of Machine
Translation Adequacy Collected Using Amazon Mechanical Turk
Michael Denkowski and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper discusses a machine translation
evaluation task conducted using Amazon Me-
chanical Turk. We present a translation ade-
quacy assessment task for untrained Arabic-
speaking annotators and discuss several tech-
niques for normalizing the resulting data. We
present a novel 2-stage normalization tech-
nique shown to have the best performance on
this task and further discuss the results of all
techniques and the usability of the resulting
adequacy scores.
1 Introduction
Human judgments of translation quality play a vital
role in the development of effective machine trans-
lation (MT) systems. Such judgments can be used
to measure system quality in evaluations (Callison-
Burch et al, 2009) and to tune automatic metrics
such as METEOR (Banerjee and Lavie, 2005) which
act as stand-ins for human evaluators. However, col-
lecting reliable human judgments often requires sig-
nificant time commitments from expert annotators,
leading to a general scarcity of judgments and a sig-
nificant time lag when seeking judgments for new
tasks or languages.
Amazon?s Mechanical Turk (MTurk) service fa-
cilitates inexpensive collection of large amounts of
data from users around the world. However, Turk-
ers are not trained to provide reliable annotations for
natural language processing (NLP) tasks, and some
Turkers attempt to game the system by submitting
random answers. For these reasons, NLP tasks must
be designed to be accessible to untrained users and
data normalization techniques must be employed to
ensure that the data collected is usable.
This paper describes a MT evaluation task for
translations of English into Arabic conducted us-
ing MTurk and compares several data normaliza-
tion techniques. A novel 2-stage normalization tech-
nique is demonstrated to produce the highest agree-
ment between Turkers and experts while retaining
enough judgments to provide a robust tuning set for
automatic evaluation metrics.
2 Data Set
Our data set consists of human adequacy judgments
for automatic translations of 1314 English sentences
into Arabic. The English source sentences and Ara-
bic reference translations are taken from the Arabic-
English sections of the NIST Open Machine Trans-
lation Evaluation (Garofolo, 2001) data sets for 2002
through 2005. Selected sentences are between 10
and 20 words in length on the Arabic side. Arabic
machine translation (MT) hypotheses are obtained
by passing the English sentences through Google?s
free online translation service.
2.1 Data Collection
Human judgments of translation adequacy are col-
lected for each of the 1314 Arabic MT output hy-
potheses. Given a translation hypothesis and the
corresponding reference translation, annotators are
asked to assign an adequacy score according to the
following scale:
4 ? Hypothesis is completely meaning equivalent
with the reference translation.
57
3 ? Hypothesis captures more than half of meaning
of the reference translation.
2 ? Hypothesis captures less than half of meaning
of the reference translation.
1 ? Hypothesis captures no meaning of the refer-
ence translation.
Adequacy judgments are collected from untrained
Arabic-speaking annotators using Amazon?s Me-
chanical Turk (MTurk) service. We create a human
intelligence task (HIT) type that presents Turkers
with a MT hypothesis/reference pair and asks for
an adequacy judgment. To make this task accessi-
ble to non-experts, the traditional definitions of ad-
equacy scores are replaced with the following: (4)
excellent, (3) good, (2) bad, (1) very bad. Each rat-
ing is accompanied by an example from the data set
which fits the corresponding criteria from the tradi-
tional scale. To make this task accessible to the Ara-
bic speakers we would like to complete the HITs,
the instructions are provided in Arabic as well as En-
glish.
To allow experimentation with various data nor-
malization techniques, we collect judgments from
10 unique Turkers for each of the translations. We
also ask an expert to provide ?gold standard? judg-
ments for 101 translations drawn uniformly from the
data. These 101 translations are recombined with the
data and repeated such that every 6th translation has
a gold standard judgment, resulting in a total of 1455
HITs. We pay Turkers $0.01 per HIT and Ama-
zon fees of $0.005 per HIT, leading to a total cost
of $218.25 for data collection and an effective cost
of $0.015 per judgment. Despite requiring Arabic
speakers, our HITs are completed at a rate of 1000-
3000 per day. It should be noted that the vast ma-
jority of Turkers working on our HITs are located in
India, with fewer in Arabic-speaking countries such
as Egypt and Syria.
3 Normalization Techniques
We apply multiple normalization techniques to the
data set and evaluate their relative performance.
Several techniques use the following measures:
? ?: For judgments (J = j1...jn) and gold stan-
dard (G = g1...gn), we define average distance:
?(J,G) =
?n
i=1 |gi ? ji|
n
? K: For two annotators, Cohen?s kappa coeffi-
cient (Smeeton, 1985) is defined:
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that an-
notators agree and P (E) is the proportion of
times that agreement is expected by chance.
3.1 Straight Average
The baseline approach consists of keeping all judg-
ments and taking the straight average on a per-
translation basis without additional normalization.
3.2 Removing Low-Agreement Judges
Following Callison-Burch et al (2009), we calcu-
late pairwise inter-annotator agreement (P (A)) of
each annotator with all others and remove judgments
from annotators with P (A) below some threshold.
We set this threshold such that the highest overall
agreement can be achieved while retaining at least
one judgment for each translation.
3.3 Removing Outlying Judgments
For a given translation and human judgments
(j1...jn), we calculate the distance (?) of each judg-
ment from the mean (j?):
?(ji) = |ji ? j?|
We then remove outlying judgments with ?(ji) ex-
ceeding some threshold. This threshold is also set
such that the highest agreement is achieved while
retaining at least one judgment per translation.
3.4 Weighted Voting
Following Callison-Burch (2009), we treat evalua-
tion as a weighted voting problem where each anno-
tator?s contribution is weighted by agreement with
either a gold standard or with other annotators. For
this evaluation, we weigh contribution by P (A) with
the 101 gold standard judgments.
58
3.5 Scaling Judgments
To account for the notion that some annotators judge
translations more harshly than others, we apply per-
annotator scaling to the adequacy judgments based
on annotators? signed distance from gold standard
judgments. For judgments (J = j1...jn) and gold
standard (G = g1...gn), an additive scaling factor is
calculated:
?+(J,G) =
?n
i=1 gi ? ji
n
Adding this scaling factor to each judgment has the
effect of shifting the judgments? center of mass to
match that of the gold standard.
3.6 2-Stage Technique
We combine judgment scaling with weighted vot-
ing to produce a 2-stage normalization technique
addressing two types of divergence in Turker judg-
ments from the gold standard. Divergence can be
either consistent, where Turkers regularly assign
higher or lower scores than experts, or random,
where Turkers guess blindly or do not understand
the task.
Stage 1: Given a gold standard (G = g1...gn),
consistent divergences are corrected by calculat-
ing ?+(J,G) for each annotator?s judgments (J =
ji...jn) and applying ?+(J,G) to each ji to produce
adjusted judgment set J ?. If ?(J ?, G) < ?(J,G),
where ?(J,G) is defined in Section 3, the annotator
is considered consistently divergent and J ? is used
in place of J . Inconsistently divergent annotators?
judgments are unaffected by this stage.
Stage 2: All annotators are considered in a
weighted voting scenario. In this case, annotator
contribution is determined by a distance measure
similar to the kappa coefficient. For judgments (J =
j1...jn) and gold standard (G = g1...gn), we define:
K?(J,G) =
(max ???(J,G))? E(?)
max ?? E(?)
where max ? is the average maximum distance be-
tween judgments and E(?) is the expected distance
between judgments. Perfect agreement with the gold
standard produces K? = 1 while chance agreement
produces K? = 0. Annotators with K? ? 0 are re-
moved from the voting pool and final scores are cal-
culated as the weighted averages of judgments from
all remaining annotators.
Type ? K?
Uniform-a 1.02 0.184
Uniform-b 1.317 -0.053
Gaussian-2 1.069 0.145
Gaussian-2.5 0.96 0.232
Gaussian-3 1.228 0.018
Table 2: Weights assigned to random data
4 Results
Table 1 outlines the performance of all normaliza-
tion techniques. To calculate P (A) and K with the
gold standard, final adequacy scores are rounded to
the nearest whole number. As shown in the table, re-
moving low-agreement annotators or outlying judg-
ments greatly improves Turker agreement and, in
the case of removing judgments, decreases distance
from the gold standard. However, these approaches
remove a large portion of the judgments, leaving a
skewed data set. When removing judgments, 1172
of the 1314 translations receive a score of 3, making
tasks such as tuning automatic metrics infeasible.
Weighing votes by agreement with the gold stan-
dard retains most judgments, though neither Turker
agreement nor agreement with the gold standard im-
proves. The scaling approach retains all judgments
and slightly improves correlation and ?, though K
decreases. As scaled judgments are not whole num-
bers, Turker P (A) and K are not applicable.
The 2-stage approach outperforms all other tech-
niques when compared against the gold standard,
being the only technique to significantly raise cor-
relation. Over 90% of the judgments are used, as
shown in Figure 1. Further, the distribution of fi-
nal adequacy scores (shown in Figure 2) resembles
a normal distribution, allowing this data to be used
for tuning automatic evaluation metrics.
4.1 Resistance to Randomness
To verify that our 2-stage technique handles prob-
lematic data properly, we simulate user data from
5 unreliable Turkers. Turkers ?Uniform-a? and
?Uniform-b? draw answers randomly from a uni-
form distribution. ?Gaussian? Turkers draw answers
randomly from Gaussian distributions with ? = 1
and ? according to name. Each ?Turker? contributes
one judgment for each translation. As shown in Ta-
59
Gold Standard Turker
Technique Retained Correlation ? P (A) K P (A) K
Straight Average 14550 0.078 0.988 0.356 0.142 0.484 0.312
Remove Judges 6627 -0.152 1.002 0.347 0.129 0.664 0.552
Remove Judgments 9250 0 0.891 0.356 0.142 0.944 0.925
Weighted Voting 14021 0.152 0.968 0.356 0.142 0.484 0.312
Scale Judgments 14550 0.24 0.89 0.317 0.089 N/A N/A
2-Stage Technique 13621 0.487 0.836 0.366 0.155 N/A N/A
Table 1: Performance of normalization techniques
0 0.25 0.5 0.75 1
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Vote Weight
Nu
mb
er 
of J
ud
gm
en
ts
Figure 1: Distribution of weights for judgments
ble 2, only Gaussian-2.5 receives substantial weight
while the others receive low or zero weight. This fol-
lows from the fact that the actual data follows a sim-
ilar distribution, and thus the random Turkers have
negligible impact on the final distribution of scores.
5 Conclusions and Future Work
We have presented an Arabic MT evaluation task
conducted using Amazon MTurk and discussed
several possibilities for normalizing the collected
data. Our 2-stage normalization technique has been
shown to provide the highest agreement between
Turkers and experts while retaining enough judg-
ments to avoid problems of data sparsity and appro-
priately down-weighting random data. As we cur-
rently have a single set of expert judgments, our fu-
ture work involves collecting additional judgments
from multiple experts against which to further test
our techniques. We then plan to use normalized
0 . 2 5
7
17
077
017
.77
.17
34Vote Wig hNV
utm
bV
Nih
rigV
fm
VJ
dn
Figure 2: Distribution of adequacy scores after 2-stage
normalization
Turker adequacy judgments to tune an Arabic ver-
sion of the METEOR (Banerjee and Lavie, 2005) MT
evaluation metric.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
ACL WIEEMMTS.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of WMT09. In
Proc. WMT09.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. EMNLP09.
John Garofolo. 2001. NIST Open Machine Translation
Evaluation. http://www.itl.nist.gov/iad/mig/tests/mt/.
N. C. Smeeton. 1985. Early History of the Kappa Statis-
tic. In Biometrics, volume 41.
60
Fi
gu
re
3:
E
xa
m
pl
e
H
IT
as
se
en
by
T
ur
ke
rs
61
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 66?70,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Turker-Assisted Paraphrasing for English-Arabic Machine Translation
Michael Denkowski and Hassan Al-Haj and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,hhaj,alavie}@cs.cmu.edu
Abstract
This paper describes a semi-automatic para-
phrasing task for English-Arabic machine
translation conducted using Amazon Me-
chanical Turk. The method for automatically
extracting paraphrases is described, as are
several human judgment tasks completed by
Turkers. An ideal task type, revised specif-
ically to address feedback from Turkers, is
shown to be sophisticated enough to identify
and filter problem Turkers while remaining
simple enough for non-experts to complete.
The results of this task are discussed along
with the viability of using this data to combat
data sparsity in MT.
1 Introduction
Many language pairs have large amounts of paral-
lel text that can be used to build statistical machine
translation (MT) systems. For such language pairs,
resources for system tuning and evaluation tend to
be disproportionately abundant in the language typ-
ically used as the target. For example, the NIST
Open Machine Translation Evaluation (OpenMT)
2009 (Garofolo, 2009) constrained Arabic-English
development and evaluation data includes four En-
glish translations for each Arabic source sentence,
as English is the usual target language. However,
when considering this data to tune and evaluate
an English-to-Arabic system, each English sentence
has a single Arabic translation and such translations
are often identical. With at most one reference trans-
lation for each source sentence, standard minimum
error rate training (Och, 2003) to the BLEU met-
ric (Papineni et al, 2002) becomes problematic, as
BLEU relies on the availability of multiple refer-
ences.
We describe a semi-automatic paraphrasing
technique that addresses this problem by identifying
paraphrases that can be used to create new reference
translations based on valid phrase substitutions on
existing references. Paraphrases are automatically
extracted from a large parallel corpus and filtered by
quality judgments collected from human annotators
using Amazon Mechanical Turk. As Turkers are
not trained to complete natural language processing
(NLP) tasks and can dishonestly submit random
judgments, we develop a task type that is able to
catch problem Turkers while remaining simple
enough for untrained annotators to understand.
2 Data Set
The parallel corpus used for paraphrasing con-
sists of all Arabic-English sentence pairs in the
NIST OpenMT Evaluation 2009 (Garofolo, 2009)
constrained training data. The target corpus to be
paraphrased consists of the 728 Arabic sentences
from the OpenMT 2002 (Garofolo, 2002) develop-
ment data.
2.1 Paraphrase Extraction
We conduct word alignment and phrase extraction
on the parallel data to produce a phrase table con-
taining Arabic-English phrase pairs (a, e) with trans-
lation probabilities P (a|e) and P (e|a). Follow-
66
ing Bannard and Callison-Burch (2005), we iden-
tify Arabic phrases (a1) in the target corpus that are
translated by at least one English phrase (e). We
identify paraphrase candidates as alternate Arabic
phrases (a2) that translate e. The probability of a2
being a paraphrase of a1 given foreign phrases e is
defined:
P (a2|a1) =
?
e
P (e|a1)P (a2|e)
A language model trained on the Arabic side of the
parallel corpus is used to further score the possi-
ble paraphrases. As each original phrase (a1) oc-
curs in some sentence (s1) in the target corpus, a
paraphrased sentence (s2) can be created by replac-
ing a1 with one of its paraphrases (a2). The final
paraphrase score considers context, scaling the para-
phrase probability proportionally to the change in
log-probability of the sentence:
F (a2, s2|a1, s1) = P (a2|a1)
logP (s1)
logP (s2)
These scores can be combined for each pair (a1, a2)
to obtain overall paraphrase scores, however we
use the F scores directly as our task considers the
sentences in which paraphrases occur.
3 Turker Paraphrase Assessment
To determine which paraphrases to use to trans-
form the development set references, we elicit bi-
nary judgments of quality from human annotators.
While collecting this data from experts would be ex-
pensive and time consuming, Amazon?s Mechani-
cal Turk (MTurk) service facilitates the rapid collec-
tion of large amounts of inexpensive data from users
around the world. As these users are not trained
to work on natural language processing tasks, any
work posted on MTurk must be designed such that
it can be understood and completed successfully by
untrained annotators. Further, some Turkers attempt
to dishonestly profit from entering random answers,
creating a need for tasks to have built-in measures
for identifying and filtering out problem Turkers.
Our original evaluation task consists of eliciting
two yes/no judgments for each paraphrase and cor-
responding sentence. Shown the original phrase
(a1) and the paraphrase (a2), annotators are asked
whether or not these two phrases could have the
same meaning in some possible context. Annotators
are then shown the original sentence (s1) and the
paraphrased sentence (s2) and asked whether these
two sentences have the same meaning. This task has
the attractive property that if s1 and s2 have the same
meaning, a1 and a2 can have the same meaning. An-
notators assigning ?yes? to the sentence pair should
always assign ?yes? to the phrase pair.
To collect these judgments from MTurk, we de-
sign a human intelligence task (HIT) that presents
Turkers with two instances of the above task along
with a text area for optional feedback. The task
description asks skilled Arabic speakers to evalu-
ate paraphrases of Arabic text. For each HIT, we
pay Turkers $0.01 and Amazon fees of $0.005 for
a total label cost of $0.015. For our initial test,
we ask Turkers to evaluate the 400 highest-scoring
paraphrases, collecting 3 unique judgments for each
paraphrase in and out of context. These HITs were
completed at a rate of 200 per day.
Examining the results, we notice that most
Turkers assign ?yes? to the sentence pairs more
often than to the phrase pairs, which should not be
possible. To determine whether quality of Turkers
might be an issue, we run another test for the same
400 paraphrases, this time paying Turkers $0.02 per
HIT and requiring a worker approval rate of 98% to
work on this task. These HITs, completed by high
quality Turkers at a rate of 100 per day, resulted
in similarly impossible data. However, we also
received valuable feedback from one of the Turkers.
3.1 Turker Feedback
We received a comment from one Turker that
our evaluation task was causing confusion. The
Turker would select ?no? for some paraphrase in
isolation due to missing information. However, the
Turker would then select ?yes? for the paraphrased
sentence, as the context surrounding the phrase
rendered the missing information unnecessary.
This illustrates the point that untrained annotators
understand the idea of ?possible context? differently
from experts and allows us to restructure our HITs
to be ideal for untrained Turkers.
67
3.2 Revised Main Task
We simplify our task to eliminate as many sources
of ambiguity as possible. Our revised task simply
presents annotators with the original sentence la-
beled ?sentence 1? and the paraphrased sentence la-
beled ?sentence 2?, and asks whether or not the two
sentences have the same meaning. Each HIT, titled
?Evaluate Arabic Sentences?, presents Turkers with
2 such tasks, pays $0.02, and costs $0.005 in Ama-
zon fees.
Without additional consideration, this task re-
mains highly susceptible to random answers from
dishonest or unreliable Turkers. To ensure that such
Turkers are identified and removed, we intersperse
absolute positive and negative examples with the
sentence pairs from our data set. Absolute posi-
tives consist of the same original sentence s1 re-
peated twice and should always receive a ?yes? judg-
ment. Absolute negatives consist of some origi-
nal s1 and a different, randomly selected original
sentence s?1 with several words dropped to obscure
meaning. Absolute negatives should always receive
a ?no? judgment. Positive and negative control cases
can be inserted with a frequency based either on de-
sired confidence that enough cases are encountered
for normalization or on the availability of funds.
Inserting either a positive or negative control
case every 5th task increases the per-label cost to
$0.0156. We use this task type to collect 3 unique
judgments for each of the 1280 highest-scoring
paraphrases at a total cost of $60.00 for 2400 HITs.
These HITs were completed substantially faster at a
rate of 500-1000 per day. The results of this task are
discussed in section 4.
3.3 Editing Task
We conduct an additional experiment to see if Turk-
ers will fix paraphrases judged to be incorrect. The
task extends the sentence evaluation task described
in the previous section by asking Turkers who select
?no? to edit the paraphrase text in the second sen-
tence such that the sentences have the same mean-
ing. While the binary judgment task is used for fil-
tering only, this editing task ensures a usable data
point for every HIT completed. As such, fewer total
HITs are required and high quality Turkers can be
0 0.25 0.5 0.75 1
0
2
4
6
8
10
12
14
16
18
20
Accuracy of judgments of control cases
Nu
mb
er 
of T
urk
ers
Figure 1: Turker accuracy classifying control cases
paid more for each HIT. We run 3 sequential tests
for this task, offering $0.02, $0.04, and $0.10 per
paraphrase approved or edited.
Examining the results, we found that regardless
of price, very few paraphrases were actually edited,
even when Turkers selected ?no? for sentence
equality. While this allows us to easily identify and
remove problem Turkers, it does not solve the issue
that honest Turkers either cannot or will not provide
usable paraphrase edits for this price range. A brief
examination by an expert indicates that the $0.02
per HIT edits are actually better than the $0.10 per
HIT edits.
4 Results
Our main task of 2400 HITs was completed through
the combined effort of 47 unique Turkers. As shown
Figure 1, these Turkers have varying degrees of ac-
curacy classifying the control cases. The two most
common classes of Turkers include (1) those spend-
ing 15 or more seconds per judgment and scoring
above 0.9 accuracy on the control cases and (2) those
spending 5-10 seconds per judgment and scoring be-
tween 0.4 and 0.6 accuracy as would be expected by
chance. As such, we accept but do not consider the
judgments of Turkers scoring between 0.7 and 0.9
accuracy on the control set, and reject all HITs for
Turkers scoring below 0.7, republishing them to be
completed by other workers.
68
Decision Confirm Reject Undec.
Paraphrases 726 423 131
Table 1: Turker judgments of top 1280 paraphrases
Figure 2: Paraphrases confirmed by Turkers
After removing judgments from below-threshold
annotators, all remaining judgments are used to
confirm or reject the covered paraphrases. If a
paraphrase has at least 2 remaining judgments, it is
confirmed if at least 2 annotators judge it positively
and rejected otherwise. Paraphrases with fewer than
2 remaining judgments are considered undecidable.
Table 1 shows the distribution of results for the 1280
top-scoring paraphrases. As shown in the table,
726 paraphrases are confirmed as legitimate phrase
substitutions on reference translations, providing
an average of almost one paraphrase per reference.
Figures 2 and 3 show example Arabic paraphrases
filtered by Turkers.
5 Conclusions
We have presented a semi-automatic paraphrasing
technique for creating additional reference transla-
tions. The paraphrase extraction technique provides
a ranked list of paraphrases and their contexts which
can be incrementally filtered by human judgments.
Our judgment task is designed to address specific
Turker feedback, remaining simple enough for
non-experts while successfully catching problem
users. The $60.00 worth of judgments collected
produces enough paraphrases to apply an average
Figure 3: Paraphrases rejected by Turkers
of one phrase substitution to each reference. Our
future work includes collecting sufficient data to
substitute multiple paraphrases into each Arabic
reference in our development set, producing a full
additional set of reference translations for use tuning
our English-to-Arabic MT system. The resulting
individual paraphrases can also be used for other
tasks in MT and NLP.
Acknowledgements
This work was supported by a $100 credit from
Amazon.com, Inc. as part of a shared task for the
NAACL 2010 workshop ?Creating Speech and Lan-
guage Data With Amazon?s Mechanical Turk?.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proc. of
ACL.
John Garofolo. 2002. NIST OpenMT Eval. 2002.
http://www.itl.nist.gov/iad/mig/tests/mt/2002/.
John Garofolo. 2009. NIST OpenMT Eval. 2009.
http://www.itl.nist.gov/iad/mig/tests/mt/2009/.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL.
69
Fi
gu
re
4:
E
xa
m
pl
e
H
IT
as
se
en
by
T
ur
ke
rs
70
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 82?87,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improved Features and Grammar Selection for Syntax-Based MT
Greg Hanneman and Jonathan Clark and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, jhclark, alavie}@cs.cmu.edu
Abstract
We present the Carnegie Mellon Univer-
sity Stat-XFER group submission to the
WMT 2010 shared translation task. Up-
dates to our syntax-based SMT system
mainly fell in the areas of new feature for-
mulations in the translation model and im-
proved filtering of SCFG rules. Compared
to our WMT 2009 submission, we report
a gain of 1.73 BLEU by using the new
features and decoding environment, and a
gain of up to 0.52 BLEU from improved
grammar selection.
1 Introduction
From its earlier focus on linguistically rich ma-
chine translation for resource-poor languages, the
statistical transfer MT group at Carnegie Mellon
University has expanded in recent years to the in-
creasingly successful domain of syntax-based sta-
tistical MT in large-data scenarios. Our submis-
sion to the 2010 Workshop on Machine Transla-
tion is a syntax-based SMT system with a syn-
chonous context-free grammar (SCFG), where the
SCFG rules are derived from full constituency
parse trees on both the source and target sides of
parallel training sentences. We participated in the
French-to-English shared translation task.
This year, we focused our efforts on making
more and better use of syntactic grammar. Much
of the work went into formulating a more expan-
sive feature set in the translation model and a new
method of assigning scores to phrase pairs and
grammar rules. Following a change of decoder
that allowed us to experiment with systems using
much larger syntactic grammars than previously,
we also adapted a technique to more intelligently
pre-filter grammar rules to those most likely to be
useful.
2 System Overview
We built our system on a partial selection of
the provided French?English training data, us-
ing the Europarl, News Commentary, and UN
sets, but ignoring the Giga-FrEn data. After
tokenization and some pruning of our training
data, this left us with a corpus of approximately
8.6 million sentence pairs. We word-aligned the
corpus with MGIZA++ (Gao and Vogel, 2008),
a multi-threaded implementation of the standard
word alignment tool GIZA++ (Och and Ney,
2003). Word alignments were symmetrized with
the ?grow-diag-final-and? heuristic. We automati-
cally parsed the French side of the corpus with the
Berkeley parser (Petrov and Klein, 2007), while
we used the fast vanilla PCFG model of the Stan-
ford parser (Klein and Manning, 2003) for the
English side. These steps resulted in a parallel
parsed corpus from which to extract phrase pairs
and grammar rules.
Phrase extraction involves three distinct steps.
In the first, we perform standard (non-syntactic)
phrase extraction according to the heuristics of
phrase-based SMT (Koehn et al, 2003). In the
second, we obtain syntactic phrase pairs using
the tree-to-tree matching method of Lavie et al
(2008). Briefly, this method aligns nodes in par-
allel parse trees by projecting up from the word
alignments. A source-tree node s will be aligned
to a target-tree node t if the word alignments in the
yield of s all land within the yield of t, and vice
versa. This node alignment is similar in spirit to
the subtree alignment method of Zhechev and Way
(2008), except our method is based on the spe-
cific Viterbi word alignment links found for each
82
sentence rather than on the general word trans-
lation probabilities computed for the corpus as a
whole. This enables us to use efficient dynamic
programming to infer node alignments, rather than
resorting to a greedy search or the enumeration of
all possible alignments. Finally, in the third step,
we use the node alignments from syntactic phrase
pair extraction to extract grammar rules. Each
aligned node in a tree pair specifies a decompo-
sition point for breaking the parallel trees into a
series of SCFG rules. Like Galley et al (2006),
we allow ?composed? (non-minimal) rules when
they build entirely on lexical items. However, to
control the size of the grammar, we do not produce
composed rules that build on other non-terminals,
nor do we produce multiple possible rules when
we encounter unaligned words. Another differ-
ence is that we discard internal structure of com-
posed lexical rules so that we produce SCFG rules
rather than synchronous tree substitution grammar
rules.
The extracted phrase pairs and grammar rules
are collected together and scored according to a
variety of features (Section 3). Instead of decod-
ing with the very large complete set of extracted
grammar rules, we select only a small number of
rules meeting certain criteria (Section 4).
In contrast to previous years, when we used the
Stat-XFER decoder, this year we switched to the
the Joshua decoder (Li et al, 2009) to take advan-
tage of its more efficient architecture and imple-
mentation of modern decoding techniques, such as
cube pruning and multi-threading. We also man-
aged system-building workflows with LoonyBin
(Clark and Lavie, 2010), a toolkit for managing
multi-step experiments across different servers or
computing clusters. Section 5 details our experi-
mental results.
3 Translation Model Construction
One major improvement in our system this year
is the feature scores we applied to our grammar
and phrase pairs. Inspired largely by the Syntax-
Augmented MT system (Zollmann and Venu-
gopal, 2006), our translation model contains 22
features in addition to the language model. In con-
trast to earlier formulations of our features (Han-
neman and Lavie, 2009), our maximum-likelihood
features are now based on a strict separation be-
tween counts drawn from non-syntactic phrase ex-
traction heuristics and our syntactic rule extractor;
no feature is estimated from counts in both spaces.
We define an aggregate rule instance as a 5-
tuple r = (L,S, T,Cphr, Csyn) that contains a
left-hand-side label L, a sequence of terminals
and non-terminals for the source (S) and target
(T ) right-hand sides, and aggregated counts from
phrase-based SMT extraction heuristics Cphr and
the syntactic rule extractor Csyn.
In preparation for feature scoring, we:
1. Run phrase instance extraction using stan-
dard phrase-based SMT heuristics to obtain
tuples (PHR, S, T,Cphr, ?) where S and T
never contain non-terminals
2. Run syntactic rule instance extraction as de-
scribed in Section 2 above to obtain tuples
(L,S, T, ?, Csyn)
3. Share non-syntactic counts such that, for
any two tuples r1 = (PHR, S, T,Cphr, ?)
and r2 = (L2, S, T, ?, Csyn) with equiv-
alent S and T values, we produce r2 =
(L2, S, T,Cphr, Csyn)
Note that there is no longer any need to retain
PHR rules (PHR, S, T ) that have syntactic equiv-
alents (L 6= PHR, S, T ) since they have the same
features In addition, we assume there will be no
tuples where S and T contain non-terminals while
Cphr = 0 and Csyn > 0. That is, the syntactic
phrases are a subset of non-syntactic phrases.
3.1 Maximum-Likelihood Features
Our most traditional features are Pphr(T |S) and
Pphr(S |T ), estimated using only counts Cphr.
These features apply only to rules not con-
taining any non-terminals. They are equiva-
lent to the phrase P (T |S) and P (S |T ) fea-
tures from the Moses decoder, even when L 6=
PHR. In contrast, we used Psyn?phr(L,S |T ) and
Psyn?phr(L, T |S) last year, which applied to all
rules. The new features are no longer subject to
increased sparsity as the number of non-terminals
in the grammar increases.
We also have grammar rule probabili-
ties Psyn(T |S), Psyn(S |T ), Psyn(L |S),
Psyn(L |T ), and Psyn(L |S, T ) estimated using
Csyn; these apply only to rules where S and T
contain non-terminals. By no longer including
counts from phrase-based SMT extraction heuris-
tics in these features, we encourage rules where
L 6= PHR since the smaller counts from the rule
learner would have otherwise been overshadowed
83
by the much larger counts from the phrase-based
SMT heuristics.
Finally, we estimate ?not labelable? (NL) fea-
tures Psyn(NL |S) and Psyn(NL |T ). With R de-
noting the set of all extracted rules,
Psyn(NL |S) =
Csyn
?
r??R s.t. S?=S C ?syn
(1)
Psyn(NL |T ) =
Csyn
?
r??R s.t. T ?=T C ?syn
(2)
We use additive smoothing (with n = 1 for our ex-
periments) to avoid a probability of 0 when there
is no syntactic label for an (S, T ) pair. These fea-
tures can encourage syntactic rules when syntax
is likely given a particular string since probability
mass is often distributed among several different
syntactic labels.
3.2 Instance Features
We add several features that use sufficient statis-
tics local to each rule. First, we add three binary
low-count features that take on the value 1 when
the frequency of the rule is exactly 1, 2, or 3. There
are also two indicator features related to syntax:
one each that fires when L = PHR and when
L 6= PHR. Other indicator features analyze the
abstractness of grammar rules: AS = 1 when the
source side contains only non-terminals, AT = 1
when the target side contains only non-terminals,
TGTINSERTION = 1 when AS = 1, AT = 0,
SRCDELETION = 1 when AS = 0, AT = 1, and
INTERLEAVED = 1 when AS = 0, AT = 0.
Bidirectional lexical probabilities for each rule
are calculated from a unigram lexicon MLE-
estimated over aligned word pairs in the training
corpus, as is the default in Moses.
Finally, we include a glue rule indicator feature
that fires whenever a glue rule is applied during
decoding. In the Joshua decoder, these monotonic
rules stitch syntactic parse fragments together at
no model cost.
4 Grammar Selection
With extracted grammars typically reaching tens
of millions of unique rules ? not to mention
phrase pairs ? our systems clearly face an en-
gineering challenge when attempting to include
the full grammar at decoding time. Iglesias et al
(2009) classified SCFG rules according to the pat-
tern of terminals and non-terminals on the rules?
right-hand sides, and found that certain patterns
could be entirely left out of the grammar without
loss of MT quality. In particular, large classes of
monotonic rules could be removed without a loss
in automatic metric scores, while small classes of
reordering rules contributed much more to the suc-
cess of the system. Inspired by that approach, we
passed our full set of extracted grammar rule in-
stances through a filter after scoring. Using the
rule notation from Section 3, the filter retained
only those rules that matched one of the follow-
ing patterns:
S = X1 w, T = w X1
S = w X1, T = X1 w
S = X1 X2, T = X2 X1
S = X1 X2, T = X1 X2
where X represents any non-terminal and w rep-
resents any span of one or more terminals. The
choice of the specific reordering patterns above
captures our intuition that binary swaps are a fun-
damental ordering divergence between languages,
while the inclusion of the abstract monotonic pat-
tern (X1 X2,X1 X2) ensures that the decoder is
not disproportionately biased towards applying re-
ordering rules without supporting lexical evidence
merely because in-order rules are left out.
Orthogonally to the pattern-based pruning, we
also selected grammars by sorting grammar rules
in decreasing order of frequency count and using
the top n in the decoder. We experimented with
n = 0, 100, 1000, and 10,000. In all cases of
grammar selection, we disallowed rules that in-
serted unaligned target-side terminals unless the
inserted terminals were among the top 100 most
frequent unigrams in the target-side vocabulary.
5 Results and Analysis
5.1 Comparison with WMT 2009 Results
We performed our initial development work on
an updated version of our previous WMT sub-
mission (Hanneman et al, 2009) so that the ef-
fects of our changes could be directly compared.
Our 2009 system was trained from the full Eu-
roparl and News Commentary data available that
year, plus the pre-release version of the Giga-FrEn
data, for a total of 9.4 million sentence pairs. We
used the news-dev2009a set for minimum error-
rate training and tested system performance on
news-dev2009b. To maintain continuity with our
previously reported scores, we report new scores
here using the same training, tuning, and test-
ing sets, using the uncased versions of IBM-style
84
System Configuration METEOR BLEU
1. WMT ?09 submission 0.5263 0.2073
2. Joshua decoder 0.5231 0.2158
3. New TM features 0.5348 0.2246
Table 1: Dev test results (on news-dev2009b) from
our WMT 2009 system when updating decoding
environment and feature formulations.
System Configuration METEOR BLEU
1. n = 100 0.5314 0.2200
2. n = 100, filtered 0.5341 0.2242
3. n = 1000 0.5324 0.2206
4. n = 1000, filtered 0.5330 0.2233
5. n = 10,000 0.5332 0.2198
6. n = 10,000, filtered 0.5350 0.2250
Table 2: Dev test results (on news-dev2009b) from
our WMT 2009 system with and without pattern-
based grammar selection.
BLEU 1.04 (Papineni et al, 2002) and METEOR
0.6 (Lavie and Agarwal, 2007).
Table 1 shows the effect of our new scoring and
decoding environment. Line 2 uses the same ex-
tracted phrase pairs and grammar rules as line 1,
but the system is tuned and tested with the Joshua
decoder instead of Stat-XFER. For line 3, we re-
scored the extracted phrase pairs from lines 1 and
2 using the updated features discussed in Sec-
tion 3.1 The difference in automatic metric scores
shows a significant benefit from both the new de-
coder and the updated feature formulations: 0.8
BLEU points from the change in decoder, and 0.9
BLEU points from the expanded set of 22 transla-
tion model features.
Our next test was to examine the usefulness of
the pattern-based grammar selection described in
Section 4. For various numbers of rules n, Ta-
ble 2 shows the scores obtained with and without
filtering the grammar before the n most frequent
rules are skimmed off for use. We observe a small
but consistent gain in scores from the grammar se-
lection process, up to half a BLEU point in the
largest-grammar systems (lines 5 and 6).
1In line 2, we did not control for difference in formulation
of the translation length feature: Stat-XFER uses a length
ratio, while Joshua uses a target word count. Line 3 does
not include 26 manually selected grammar rules present in
lines 1 and 2; this is because our new feature scoring requires
information from the grammar rules that was not present in
our 2009 extracted resources.
Source Target
un ro?le AP1 ADJP1 roles
l? instabilite? AP1 ADJP1 instability
l? argent PP1 NP1 money
une pression AP1 ADJP1 pressure
la gouvernance AP1 ADJP1 governance
la concurrence AP1 ADJP1 competition
des preuves AP1 ADJP1 evidence
les outils AP1 ADJP1 tools
des changements AP1 ADJP1 changes
Table 3: Rules fitting the pattern (S = w X1, T =
X1 w) that applied on the news-test2010 test set.
5.2 WMT 2010 Results and Analysis
We built the WMT 2010 version of our system
from the training data described in Section 2. (The
system falls under the strictly constrained track:
we used neither the Giga-FrEn data for training
nor the LDC Gigaword corpora for language mod-
eling.) We used the provided news-test2008 set
for system tuning, while news-test2009 served
as our 2010 dev test set. Based on the results
in Table 2, our official submission to this year?s
shared task was constructed as in line 6, with
10,000 syntactic grammar rules chosen after a
pattern-based grammar selection step. On the
news-test2010 test set, this system scored 0.2327
on case-insensitive IBM-style BLEU 1.04, 0.5614
on METEOR 0.6, and 0.5519 on METEOR 1.0
(Lavie and Denkowski, 2009).
The actual application of grammar rules in the
system is quite surprising. Despite having a gram-
mar of 10,000 rules at its disposal, the decoder
chose to only apply a total of 20 unique rules
in 392 application instances in the 2489-sentence
news-test2010 set. On a per-sentence basis, this
is actually fewer rule applications than our sys-
tem performed last year with a 26-rule handpicked
grammar! The most frequently applied rules are
fully abstract, monotonic structure-building rules,
such as for stitching together compound noun
phrases with adverbial phrases or prepositional
phrases. Nine of the 20 rules, listed in Table 3,
demonstrate the effect of our pattern-based gram-
mar selection. These partially lexicalized rules fit
the pattern (S = w X1, T = X1 w) and han-
dle cases of lexicalized binary reordering between
French and English. Though the overall impact of
these rules on automatic metric scores is presum-
85
ably quite small, we believe that the key to effec-
tive syntactic grammars in our MT approach lies
in retaining precise rules of this type for common
linguistically motivated reordering patterns.
The above pattern of rule applications is also
observed in our dev test set, news-test2009, where
16 distinct rules apply a total of 352 times. Seven
of the fully abstract rules and three of the lexical-
ized rules that applied on news-test2009 also ap-
plied on news-test2010, while a further two ab-
stract and four lexicalized rules applied on news-
test2009 alone. We thus have a general trend of a
set of general rules applying with higher frequency
across test sets, while the set of lexicalized rules
used varies according to the particular set.
Since, overall, we still do not see as much gram-
mar application in our systems as we would like,
we plan to concentrate future work on further im-
proving this aspect. This includes a more detailed
study of grammar filtering or refinement to select
the most useful rules. We would also like to ex-
plore the effect of the features of Section 3 individ-
ually, on different language pairs, and using differ-
ent grammar types.
Acknowledgments
This research was supported in part by NSF grant
IIS-0534217 (LETRAS) and the DARPA GALE
program. We thank Yahoo! for the use of the M45
research computing cluster, where we ran many
steps of our experimental pipeline.
References
Jonathan Clark and Alon Lavie. 2010. LoonyBin:
Keeping language technologists sane through
automated management of experimental (hy-
per)workflows. In Proceedings of the Seventh
International Language Resources and Evaluation
(LREC ?10), Valletta, Malta, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, OH,
June.
Greg Hanneman and Alon Lavie. 2009. Decoding
with syntactic and non-syntactic phrases in a syntax-
based machine translation system. In Proceedings of
the Third Workshop on Syntax and Structure in Sta-
tistical Translations, pages 1?9, Boulder, CO, June.
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,
Alok Parlikar, and Alon Lavie. 2009. An improved
statistical transfer systems for French?English ma-
chine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
140?144, Athens, Greece, March.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the ACL, pages 380?388, Athens, Greece, March?
April.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15, pages 3?10. MIT Press,
Cambridge, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 48?54, Ed-
monton, Alberta, May?June.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June.
Alon Lavie and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In Proceed-
ings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 135?139, Athens, Greece,
March.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
86
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Ventsislav Zhechev and Andy Way. 2008. Automatic
generation of parallel treebanks. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics, pages 1105?1112, Manchester,
England, August.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, NY,
June.
87
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 301?306,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CMU Multi-Engine Machine Translation for WMT 2010
Kenneth Heafield
Carnegie Mellon University
Pittsburgh, PA, USA.
heafield@cs.cmu.edu
Alon Lavie
Carnegie Mellon University
Pittsburgh, PA, USA.
alavie@cs.cmu.edu
Abstract
This paper describes our submission,
cmu-heafield-combo, to the WMT
2010 machine translation system combi-
nation task. Using constrained resources,
we participated in all nine language pairs,
namely translating English to and from
Czech, French, German, and Spanish as
well as combining English translations
from multiple languages. Combination
proceeds by aligning all pairs of system
outputs then navigating the aligned out-
puts from left to right where each path is
a candidate combination. Candidate com-
binations are scored by their length, agree-
ment with the underlying systems, and a
language model. On tuning data, improve-
ment in BLEU over the best system de-
pends on the language pair and ranges
from 0.89% to 5.57% with mean 2.37%.
1 Introduction
System combination merges the output of sev-
eral machine translation systems into a sin-
gle improved output. Our system combina-
tion scheme, submitted to the Workshop on Sta-
tistical Machine Translation (WMT) 2010 as
cmu-heafield-combo, is an improvement
over our previous system (Heafield et al, 2009),
called cmu-combo in WMT 2009. The scheme
consists of aligning 1-best outputs from each sys-
tem using the METEOR (Denkowski and Lavie,
2010) aligner, identifying candidate combinations
by forming left-to-right paths through the aligned
system outputs, and scoring these candidates us-
ing a battery of features. Improvements this year
include unigram paraphrase alignment, support for
all target languages, new features, language mod-
eling without pruning, and more parameter opti-
mization. This paper describes our scheme with
emphasis on improved areas.
2 Related Work
Confusion networks (Rosti et al, 2008) are the
most popular form of system combination. In this
approach, a single system output acts as a back-
bone to which the other outputs are aligned. This
backbone determines word order while other out-
puts vote for substitution, deletion, and insertion
operations. Essentially, the backbone is edited
to produce a combined output which largely pre-
serves word order. Our approach differs in that
we allow paths to switch between sentences, effec-
tively permitting the backbone to switch at every
word.
Other system combination techniques typically
use TER (Snover et al, 2006) or ITGs (Karakos
et al, 2008) to align system outputs, meaning
they depend solely on positional information to
find approximate matches; we explicitly use stem,
synonym, and paraphrase data to find alignments.
Our use of paraphrases is similar to Leusch et al
(2009), though they learn a monolingual phrase
table while we apply cross-lingual pivoting (Ban-
nard and Callison-Burch, 2005).
3 Alignment
System outputs are aligned at the token level using
a variant of the METEOR (Denkowski and Lavie,
2010) aligner. This identifies, in decreasing order
of priority: exact, stem, synonym, and unigram
paraphrase matches. Stems (Porter, 2001) are
available for all languages except Czech, though
this is planned for future work and expected
to produce significant improvement. Synonyms
come from WordNet (Fellbaum, 1998) and are
only available in English. Unigram paraphrases
are automatically generated using phrase table piv-
oting (Bannard and Callison-Burch, 2005). The
phrase tables are trained using parallel data from
Europarl (fr-en, es-en, and de-en), news commen-
tary (fr-en, es-en, de-en, and cz-en), United Na-
301
tions (fr-en and es-en), and CzEng (cz-en) (Bojar
and Z?abokrtsky?, 2009) sections 0?8. The German
and Spanish tables also use the German-Spanish
Europarl corpus released for WMT08 (Callison-
Burch et al, 2008). Currently, the generated para-
phrases are filtered to solely unigram matches;
full use of this table is planned for future work.
When alignment is ambiguous (i.e. ?that? appears
twice in a system output), an alignment is chosen
to minimize crossing with other alignments. Fig-
ure 1 shows an example alignment. Compared to
our previous system, this replaces heuristic ?arti-
ficial? alignments with automatically learned uni-
gram paraphrases.
Twice that produced by nuclear plants
Double that that produce nuclear power stations
Figure 1: Alignment generated by METEOR
showing exact (that?that and nuclear?nuclear),
stem (produced?produce), synonym (twice?
double), and unigram paraphrase (plants?stations)
alignments.
4 Search Space
A candidate combination consists of a string of to-
kens (words and punctuation) output by the under-
lying systems. Unconstrained, the string could re-
peat tokens and assemble them in any order. We
therefore have several constraints:
Sentence The string starts with the beginning of
sentence token and finishes with the end of
sentence token. These tokens implicitly ap-
pear in each system?s output.
Repetition A token may be used at most once.
Tokens that METEOR aligned are alterna-
tives and cannot both be used.
Weak Monotonicity This prevents the scheme
from reordering too much. Specifically, the
path cannot jump backwards more than r to-
kens, where positions are measured relative
to the beginning of sentence. It cannot make
a series of smaller jumps that add up to more
than r either. Equivalently, once a token
in the ith position of some system output is
used, all tokens before the i? rth position in
their respective system outputs become un-
usable. The value of r is a hyperparameter
considered in Section 6.
Completeness Tokens may not be skipped unless
the sentence ends or another constraint would
be violated. Specifically, when a token from
some system is used, it must be the first (left-
most in the system output) available token
from that system. For example, the first de-
coded token must be the first token output by
some system.
Together, these define the search space. The candi-
date starts at the beginning of sentence by choos-
ing the first token from any system. Then it can
either continue with the next token from the same
system or switch to another one. When it switches
to another system, it does so to the first available
token from the new system. The repetition con-
straint requires that the token does not repeat con-
tent. The weak monotonicity constraint ensures
that the jump to the new system goes at most r
words back. The process repeats until the end of
sentence token is encountered.
The previous version (Heafield et al, 2009) also
had a hard phrase constraint and heuristics to de-
fine a phrase; this has been replaced with new
match features.
Search is performed using beam search where
the beam contains partial candidates of the same
length, each of which starts with the beginning of
sentence token. In our experiments, the beam size
is 500. When two partial candidates will extend
in the same way (namely, the set of available to-
kens is the same) and have the same feature state
(i.e. language model history), they are recom-
bined. The recombined partial candidate subse-
quently acts like its highest scoring element, until
k-best list extraction when it is lazily unpacked.
5 Scoring Features
Candidates are scored using three feature classes:
Length Number of tokens in the candidate. This
compensates, to first order, for the impact of
length on other features.
Match For each system s and small n, feature
ms,n is the number of n-grams in the candi-
date matching the sentence output by system
s. This is detailed in Section 5.1.
302
Language Model Log probability from a n-gram
language model and backoff statistics. Sec-
tion 5.2 details our training data and backoff
features.
Features are combined into a score using a linear
model. Equivalently, the score is the dot product
of a weight vector with the vector of our feature
values. The weight vector is a parameter opti-
mized in Section 6.
5.1 Match Features
The n-gram match features reward agreement be-
tween the candidate combination and underlying
system outputs. For example, feature m1,1 counts
tokens in the candidate that also appear in sys-
tem 1?s output for the sentence being combined.
Featurem1,2 counts bigrams appearing in both the
candidate and the translation suggested by system
1. Figure 2 shows example feature values.
System 1: Supported Proposal of France
System 2: Support for the Proposal of France
Candidate: Support for Proposal of France
Unigram Bigram Trigram
System 1 4 2 1
System 2 5 3 1
Figure 2: Example match feature values with two
systems and matches up to length three. Here,
?Supported? counts because it aligns with ?Sup-
port?.
The match features count n-gram matches be-
tween the candidate and each system. These
matches are defined in terms of alignments. A to-
ken matches the system that supplied it as well as
the systems to which it aligns. This can be seen in
Figure 2 where System 1?s unigram match count
includes ?Supported? even though the candidate
chose ?Support?. Longer matches are defined sim-
ilarly: a bigram match consists of two consecutive
alignments without reordering. Since METEOR
generates several types of alignments as shown in
Figure 1, we wonder whether all alignment types
should count as matches. If we count all types
of alignment, then the match features are blind to
lexical choice, leaving only the language model to
discriminate. If only exact alignments count, then
less systems are able to vote on a word order deci-
sion mediated by the bigram and trigram features.
We find that both versions have their advantages,
and therefore include two sets of match features:
one that counts only exact alignments and another
that counts all alignments. We also tried copies of
the match features at the stem and synonym level
but found these impose additional tuning cost with
no measurable improvement in quality.
Since systems have different strengths and
weaknesses, we avoid assigning a single system
confidence (Rosti et al, 2008) or counting n-gram
matches with uniform system confidence (Hilde-
brand and Vogel, 2009). The weight on match
feature ms,n corresponds to our confidence in n-
grams from system s. These weights are fully tun-
able. However, there is another hyperparameter:
the maximum length of n-gram considered; we
typically use 2 or 3 with little gain seen above this.
5.2 Language Model
We built language models for each of the five tar-
get languages with the aim of using all constrained
data. For each language, we used the provided
Europarl (Koehn, 2005) except for Czech, News
Commentary, and News monolingual corpora. In
addition, we used:
Czech CzEng (Bojar and Z?abokrtsky?, 2009) sec-
tions 0?7
English Gigaword Fourth Edition (Parker et al,
2009), Giga-FrEn, and CzEng (Bojar and
Z?abokrtsky?, 2009) sections 0?7
French Gigaword Second Edition (Mendonca et
al., 2009a), Giga-FrEn
Spanish Gigaword Second Edition (Mendonca et
al., 2009b)
Paragraphs in the Gigaword corpora were split
into sentences using the script provided with
Europarl (Koehn, 2005); parenthesized format-
ting notes were removed from the NYT portion.
We discarded Giga-FrEn lines containing invalid
UTF8, control characters, or less than 90% Latin
characters or punctuation. Czech training data
and system outputs were preprocessed using Tec-
toMT (Z?abokrtsky? and Bojar, 2008) following the
CzEng 0.9 pipeline (Bojar and Z?abokrtsky?, 2009).
English training data and system outputs were to-
kenized with the IBM tokenizer. French, Ger-
man, and Spanish used the provided tokenizer.
303
Czech words were truecased based on automati-
cally identified lemmas marking names; for other
languages, training data was lowercased and sys-
tems voted, with uniform weight, on capitalization
of each character in the final output.
With the exception of Czech (for which we used
an existing model), all models were built with no
lossy pruning whatsoever, including our English
model with 5.8 billion tokens (i.e. after IBM to-
kenization). Using the stock SRILM (Stolcke,
2002) toolkit with modified Kneser-Ney smooth-
ing, the only step that takes unbounded memory is
final model estimation from n-gram counts. Since
key parameters have already been estimated at this
stage, this final step requires only counts for the
desired n-grams and all of their single token ex-
tensions. We can therefore filter the n-grams on
all but the last token. Our scheme will only query
an n-gram if all of the tokens appear in the union
of system outputs for some sentence; this strict fil-
tering criterion is further described and released
as open source in Heafield and Lavie (2010). The
same technique applies to machine translation sys-
tems, with phrase table expansion taking the place
of system outputs.
For each language, we built one model by ap-
pending all data. Another model interpolates
smaller models built on the individual sources
where each Gigaword provider counts as a distinct
source. Interpolation weights were learned on the
WMT 2009 references. For English, we also tried
an existing model built solely on Gigaword using
interpolation. The choice of model is a hyperpa-
rameter we consider in Section 6.
In the combination scheme, we use the log lan-
guage model probability as a feature. Another
feature reports the length of the n-gram matched
by the model; this exposes limited tunable con-
trol over backoff behavior. For Czech, the model
was built with a closed vocabulary; when an out-
of-vocabulary (OOV) word is encountered, it is
skipped for purposes of log probability and a
third feature counts how often this happens. This
amounts to making the OOV probability a tunable
parameter.
6 Parameter Optimization
6.1 Feature Weights
Feature weights are tuned using Minimum Error
Rate Training (MERT) (Och, 2003) on the 455
provided references. Our largest submission, xx-
en primary, combines 17 systems with five match
features each plus three other features for a total of
88 features. This immediately raises two concerns.
First, there is overfitting and we expect to see a
loss in the test results, although our experience in
the NIST Open MT evaluation is that the amount
of overfitting does not significantly increase at this
number of parameters. Second, MERT is poor at
fitting this many feature weights. We present one
modification to MERT that addresses part of this
problem, leaving other tuning methods as future
work.
MERT is prone to local maxima, so we apply
a simple form of simulated annealing. As usual,
the zeroth iteration decodes with some initial fea-
ture weights. Afterward, the weights {?f} learned
from iteration 0 ? j < 10 are perturbed to pro-
duce new feature weights
?f ? U
[
j
10
?f ,
(
2?
j
10
)
?f
]
where U is the uniform distribution. This sam-
pling is done on a per-sentence basis, so the first
sentence is decoded with different weights than
the second sentence. The amount of random per-
turbation decreases linearly each iteration until
the 10th and subsequent iterations whose learned
weights are not perturbed. We emphasize that
the point is to introduce randomness in sentences
decoded during MERT, and therefore considered
during parameter tuning, and not on the spe-
cific formula presented in this system description.
In practice, this technique increases the number
of iterations and decreases the difference in tun-
ing scores following MERT. In our experiments,
weights are tuned towards uncased BLEU (Pap-
ineni et al, 2002) or the combined metric TER-
BLEU (Snover et al, 2006).
6.2 Hyperparameters
In total, we tried 1167 hyperparameter configura-
tions, limited by CPU time during the evaluation
period. For each of these configurations, the fea-
ture weights were fully trained with MERT and
scored on the same tuning set, which we used to
select the submitted combinations. Because these
configurations represent a small fraction of the
hyperparameter space, we focused on values that
work well based on prior experience and tuning
scores as they became available:
Set of systems Top systems by BLEU. The num-
ber of top systems included ranged from 3 to
304
Pair Entry #Sys r Match LM Objective ?BLEU ?TER ?METE
cz-en main 5 4 2 Append BLEU 2.38 0.99 1.50
de-en
main 6 4 2 Append TER-BLEU 2.63 -2.38 1.36
contrast 7 3 2 Append BLEU 2.60 -2.62 1.09
es-en
main 7 5 3 Append BLEU 1.22 -0.74 0.70
contrast 5 6 2 Gigaword BLEU 1.08 -0.80 0.97
fr-en
main 9 5 3 Append BLEU 2.28 -2.26 0.78
contrast 8 5 3 Append BLEU 2.19 -1.81 0.63
xx-en
main 17 5 3 Append BLEU 5.57 -5.60 4.33
contrast 16 5 3 Append BLEU 5.45 -5.38 4.22
en-cz main 7 5 3 Append TER-BLEU 0.74 -0.26 0.68
en-de
main 6 6 2 Interpolate BLEU 1.26 0.16 1.14
contrast 5 4 2 Interpolate BLEU 1.26 0.30 1.00
en-es
main 8 5 3 Interpolate BLEU 2.38 -2.20 0.96
contrast 6 7 2 Append BLEU 2.40 -1.85 1.02
en-fr main 6 7 2 Append BLEU 2.64 -0.50 1.55
Table 1: Submitted combinations chosen from among 1167 hyperparameter settings by tuning data
scores. Uncased BLEU, uncased TER, and METEOR 1.0 with adequacy-fluency parameters are shown
relative to top system by BLEU. Improvement is seen in all pairs on all metrics except for TER on cz-en
and en-de where the top systems are 5% and 2% shorter than the references, respectively. TER has a well
known preference for shorter hypotheses. The #Sys column indicates the number of systems combined,
using the top scoring systems by BLEU. The Match column indicates the maximum n-gram length con-
sidered for matching on all alignments; we separately counted unigram and bigram exact matches. In
some cases, we made a contrastive submission where metrics disagreed or length behavior differed near
the top; contrastive submissions are not our 2009 scheme.
all of them, except on xx-en where we com-
bined up to 17.
Jump limit Mostly r = 5, with some experi-
ments ranging from 3 to 7.
Match features Usually unigram and bigram fea-
tures, sometimes trigrams as well.
Language model Balanced between the ap-
pended and interpolated models, with the
occasional baseline Gigaword model for
English.
Tuning objective Usually BLEU for speed rea-
sons; occasional TER-BLEU with typical
values for other hyperparameters.
7 Conclusion
Table 1 shows the submitted combinations and
their performance. Our submissions this year im-
prove over last year (Heafield et al, 2009) in
overall performance and support for multiple lan-
guages. The improvement in performance we pri-
marily attribute to the new match features, which
account for most of the gain and allowed us to in-
clude lower quality systems. We also trained lan-
guage models without pruning, replaced heuristic
alignments with unigram paraphrases, tweaked the
other features, and improved the parameter opti-
mization process. We hope that the improvements
seen on tuning scores generalize to significantly
improved test scores, especially human evaluation.
Acknowledgments
Ondr?ej Bojar made the Czech language model
and preprocessed Czech system outputs. Michael
Denkowski provided the paraphrase tables and
wrote the version of METEOR used. This work
was supported in part by the DARPA GALE pro-
gram and by a NSF Graduate Research Fellow-
ship.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings ACL.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
305
0.9, building a large Czech-English automatic paral-
lel treebank. The Prague Bulletin of Mathematical
Linguistics, (92):63?83.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Denkowski and Alon Lavie. 2010. Extend-
ing the METEOR machine translation metric to the
phrase level. In Proceedings NAACL 2010, Los An-
geles, CA, June.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Kenneth Heafield and Alon Lavie. 2010. Combining
machine translation output with open source: The
Carnegie Mellon multi-engine machine translation
scheme. In The Prague Bulletin of Mathematical
Linguistics, number 93, pages 27?36, Dublin.
Kenneth Heafield, Greg Hanneman, and Alon Lavie.
2009. Machine translation system combination
with flexible word ordering. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 56?60, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Almut Silja Hildebrand and Stephan Vogel. 2009.
CMU system combination for WMT?09. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 47?50, Athens, Greece,
March. Association for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proceedings ACL-08: HLT, Short Papers (Compan-
ion Volume), pages 81?84.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2009. The RWTH system combination system for
WMT 2009. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 51?
55, Athens, Greece, March. Association for Compu-
tational Linguistics.
Angelo Mendonca, David Graff, and Denise DiPer-
sio. 2009a. French gigaword second edition.
LDC2009T28.
Angelo Mendonca, David Graff, and Denise DiPer-
sio. 2009b. Spanish gigaword second edition.
LDC2009T21.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proceed-
ings of 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English gigaword fourth
edition. LDC2009T13.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proceedings Third Workshop on Statistical
Machine Translation, pages 183?186.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings Seventh Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901?904.
Zdene?k Z?abokrtsky? and Ondr?ej Bojar. 2008. TectoMT,
Developer?s Guide. Technical Report TR-2008-39,
Institute of Formal and Applied Linguistics, Faculty
of Mathematics and Physics, Charles University in
Prague, December.
306
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 339?342,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
METEOR-NEXT and the METEOR Paraphrase Tables: Improved
Evaluation Support for Five Target Languages
Michael Denkowski and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper describes our submission to
the WMT10 Shared Evaluation Task and
MetricsMATR10. We present a version
of the METEOR-NEXT metric with para-
phrase tables for five target languages. We
describe the creation of these paraphrase
tables and conduct a tuning experiment
that demonstrates consistent improvement
across all languages over baseline ver-
sions of the metric without paraphrase re-
sources.
1 Introduction
Workshops such as WMT (Callison-Burch et al,
2009) and MetricsMATR (Przybocki et al, 2008)
focus on the need for accurate automatic met-
rics for evaluating the quality of machine transla-
tion (MT) output. While these workshops evalu-
ate metric performance on many target languages,
most metrics are limited to English due to the rel-
ative lack of lexical resources for other languages.
This paper describes a language-independent
method for adding paraphrase support to the
METEOR-NEXT metric for all WMT10 target lan-
guages. Taking advantage of the large parallel cor-
pora released for the translation tasks often accom-
panying evaluation tasks, we automatically con-
struct paraphrase tables using the pivot method
(Bannard and Callison-Burch, 2005). We use the
WMT09 human evaluation data to tune versions
of METEOR-NEXT with and without paraphrases
and report significantly better performance for ver-
sions with paraphrase support.
2 The METEOR-NEXT Metric
The METEOR-NEXT metric (Denkowski and
Lavie, 2010) evaluates a machine translation hy-
pothesis against a reference translation by calcu-
lating a similarity score based on an alignment be-
tween the two strings. When multiple references
are provided, the hypothesis is scored against each
and the reference producing the highest score is
used. Alignments are formed in two stages: search
space construction and alignment selection.
For a single hypothesis-reference pair, the space
of possible alignments is constructed by identify-
ing all possible word and phrase matches between
the strings according to the following matchers:
Exact: Words are matched if and only if their sur-
face forms are identical.
Stem: Words are stemmed using a language-
appropriate Snowball Stemmer (Porter, 2001) and
matched if the stems are identical.
Synonym: Words are matched if they are both
members of a synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database.
Paraphrase: Phrases are matched if they are
listed as paraphrases in a paraphrase table. The
tables used are described in Section 3.
Previously, full support has been limited to En-
glish, with French, German, and Spanish having
exact and stem match support only, and Czech
having exact match support only.
Although the exact, stem, and synonym match-
ers identify word matches while the paraphrase
matcher identifies phrase matches, all matches can
be generalized to phrase matches with a start po-
sition and phrase length in each string. A word
occurring less than length positions after a match
start is considered covered by the match. Ex-
act, stem, and synonym matches always cover one
word in each string.
Once the search space is constructed, the final
alignment is identified as the largest possible sub-
set of all matches meeting the following criteria in
order of importance:
1. Each word in each sentence is covered by
zero or one matches
2. Largest number of covered words across both
339
sentences
3. Smallest number of chunks, where a chunk
is defined as a series of matched phrases that
is contiguous and identically ordered in both
sentences
4. Smallest sum of absolute distances between
match start positions in the two sentences
(prefer to align words and phrases that occur
at similar positions in both sentences)
Once an alignment is selected, the METEOR-
NEXT score is calculated as follows. The num-
ber of words in the translation hypothesis (t) and
reference (r) are counted. For each of the match-
ers (mi), count the number of words covered by
matches of this type in the hypothesis (mi(t)) and
reference (mi(r)) and apply matcher weight (wi).
The weighted Precision and Recall are then calcu-
lated:
P =
?
iwi ?mi(t)
|t|
R =
?
iwi ?mi(r)
|r|
The parameterized harmonic mean of P and R
(van Rijsbergen, 1979) is then calculated:
Fmean =
P ?R
? ? P + (1? ?) ?R
To account for gaps and differences in word or-
der, a fragmentation penalty (Lavie and Agar-
wal, 2007) is calculated using the total number of
matched words (m) and number of chunks (ch):
Pen = ? ?
(
ch
m
)?
The final METEOR-NEXT score is then calculated:
Score = (1? Pen) ? Fmean
The parameters ?, ?, ?, and wi...wn can be
tuned to maximize correlation with various types
of human judgments.
3 The METEOR Paraphrase Tables
To extend support for WMT10 target languages,
we use released parallel corpora to construct para-
phrase tables for English, Czech, German, Span-
ish, and French. These tables are used by the
METEOR-NEXT paraphrase matcher to identify
additional phrase matches in each language.
3.1 Paraphrasing with Parallel Corpora
Following Bannard and Callison-Burch (2005),
we extract paraphrases automatically from bilin-
gual corpora using a pivot phrase method. For a
given language pair, word alignment, phrase ex-
traction, and phrase scoring are conducted on par-
allel corpora to build a single bilingual phrase ta-
ble for the language pair. For each native phrase
(n1) in the table, we identify each foreign phrase
(f ) that translates n1. Each alternate native phrase
(n2 6= n1) that translates f is considered a para-
phrase of n1 with probability P (f |n1) ? P (n2|f).
The total probability of n2 paraphrasing n1 is
given as the sum over all f :
P (n2|n1) =
?
f
P (f |n1) ? P (n2|f)
The same method can be used to identify foreign
paraphrases (f1, f2) given native pivot phrases
n. To merge same-language paraphrases ex-
tracted from different parallel corpora, we take the
mean of the corpus-specific paraphrase probabili-
ties (PC) weighted by the size of the corpora (C)
used for paraphrase extraction:
P (n2|n1) =
?
C |C| ? PC(n2|n1)?
C |C|
To improve paraphrase accuracy, we apply mul-
tiple filtering techniques during paraphrase extrac-
tion. The following are applied to each paraphrase
instance (n1, f, n2):
1. Discard paraphrases with very low probabil-
ity (P (f |n1) ? P (n2|f) < 0.001)
2. Discard paraphrases for which n1, f , or n2
contain any punctuation characters.
3. Discard paraphrases for which n1, f , or
n2 contain only common words. Common
words are defined as having relative fre-
quency of 0.001 or greater in the parallel cor-
pus.
Remaining phrase instances are summed to con-
struct corpus-specific paraphrase tables. Same-
language paraphrase tables are selectively merged
as part of the tuning process described in Sec-
tion 4.2. Final paraphrase tables are further fil-
tered to include only paraphrases with probabili-
ties above a final threshold (0.01).
340
Language Pair Corpus Phrase Table
Target Source Sentences Phrase Pairs
English Czech 7,321,950 128,326,269
English German 1,630,132 84,035,599
English Spanish 7,965,250 363,714,779
English French 8,993,161 404,883,736
German Spanish 1,305,650 70,992,157
Table 1: Sizes of training corpora and phrase ta-
bles used for paraphrase extraction
Language Pivot Languages Phrase Pairs
English German, Spanish, 6,236,236
French
Czech English 756,113
German English, Spanish 3,521,052
Spanish English, German 6,352,690
French English 3,382,847
Table 2: Sizes of final paraphrase tables
3.2 Available Data
We conduct paraphrase extraction using parallel
corpora released for the WMT10 Shared Trans-
lation Task. This includes Europarl corpora
(French-English, Spanish-English, and German-
English), news commentary (French-English,
Spanish-English, German-English, and Czech-
English), United Nations corpora (French-English
and Spanish-English), and the CzEng (Bojar and
Z?abokrtsky?, 2009) corpus sections 0-8 (Czech-
English). In addition, we use the German-Spanish
Europarl corpus released for WMT08 (Callison-
Burch et al, 2008).
3.3 Paraphrase Table Construction
Using all available data for each language pair,
we create bilingual phrase tables for the follow-
ing: French-English, Spanish-English, German-
English, Czech-English, and German-Spanish.
The full training corpora and resulting phrase ta-
bles are described in Table 1. For each phrase ta-
ble, both foreign and native paraphrases are ex-
tracted. Same-language paraphrases are selec-
tively merged as described in Section 4.2 to pro-
duce the final paraphrase tables described in Ta-
ble 2. To keep table size reasonable, we only ex-
tract paraphrases for phrases occurring in target
corpora consisting of the pooled development data
from the WMT08, WMT09, and WMT10 trans-
lation tasks (10,158 sentences for Czech, 20,258
sentences for all other languages).
Target Systems Usable Judgments
English 45 20,357
Czech 5 11,242
German 11 6,563
Spanish 9 3,249
French 12 2,967
Table 3: Human ranking judgment data from
WMT09
4 Tuning METEOR-NEXT
4.1 Development Data
As part of the WMT10 Shared Evaluation Task,
data from WMT09 (Callison-Burch et al, 2009),
including system output, reference translations,
and human judgments, is available for metric de-
velopment. As metrics are evaluated primarily
on their ability to rank system output on the seg-
ment level, we select the human ranking judg-
ments from WMT09 as our development set (de-
scribed in Table 3).
4.2 Tuning Procedure
Tuning a version of METEOR-NEXT consists of
selecting parameters (?, ?, ?, wi...wn) that opti-
mize an objective function for a given language.
If multiple paraphrase tables exist for a language,
tuning also requires selecting the optimal set of ta-
bles to merge.
For WMT10, we tune to rank consistency on the
WMT09 data. Following Callison-Burch et. al
(2009), we discard judgments where system out-
puts are deemed equivalent and calculate the pro-
portion of remaining judgments preserved when
system outputs are ranked by automatic metric
scores. For each target language, tuning is con-
ducted as an exhaustive grid search over metric pa-
rameters and possible paraphrase tables, resulting
in global optima for both.
5 Experiments
To evaluate the impact of our paraphrase ta-
bles on metric performance, we tune versions of
METEOR-NEXT with and without the paraphrase
matchers for each language. For further compar-
ison, we tune a version of METEOR-NEXT using
the TERp English paraphrase table (Snover et al,
2009) used by previous versions of the metric.
As shown in Table 4, the addition of paraphrases
leads to a better tuning point for every target lan-
guage. The best scoring subset of paraphrase ta-
341
Language Paraphrases Rank Consistency ? ? ? wexact wstem wsyn wpar
English none 0.619 0.85 2.35 0.45 1.00 0.80 0.60 ?
TERp 0.625 0.70 1.40 0.25 1.00 0.80 0.80 0.60
de+es+fr 0.629 0.75 0.60 0.35 1.00 0.80 0.80 0.60
Czech none 0.564 0.95 0.20 0.70 1.00 ? ? ?
en 0.574 0.95 2.15 0.35 1.00 ? ? 0.40
German none 0.550 0.20 0.75 0.25 1.00 0.80 ? ?
en+es 0.576 0.75 0.80 0.90 1.00 0.20 ? 0.80
Spanish none 0.586 0.95 0.55 0.90 1.00 0.80 ? ?
en+de 0.608 0.15 0.25 0.75 1.00 0.80 ? 0.40
French none 0.696 0.95 0.80 0.35 1.00 0.60 ? ?
en 0.707 0.90 0.85 0.45 1.00 0.00 ? 0.60
Table 4: Optimal METEOR-NEXT parameters with and without paraphrases for WMT10 target languages
bles for English also outperforms the TERp para-
phrase table.
Analysis of the phrase matches contributed by
the paraphrase matchers reveals an interesting
point about the task of paraphrasing for MT eval-
uation. Despite filtering techniques, the final para-
phrase tables include some unusual, inaccurate,
or highly context-dependent paraphrases. How-
ever, the vast majority of matches identified be-
tween actual system output and reference trans-
lations correspond to valid paraphrases. In many
cases, the evaluation task itself acts as a final filter;
to produce a phrase that can match a spurious para-
phrase, not only must a MT system produce incor-
rect output, but it must produce output that over-
laps exactly with an obscure paraphrase of some
phrase in the reference translation. As systems
are far more likely to produce phrases with similar
words to those in reference translations, far more
valid paraphrases exist in typical system output.
6 Conclusions
We have presented versions of METEOR-NEXT
and paraphrase tables for five target languages.
Tuning experiments indicate consistent improve-
ments across all languages over baseline versions
of the metric. Created for MT evaluation, the ME-
TEOR paraphrase tables can also be used for other
tasks in MT and natural language processing. Fur-
ther, the techniques used to build the paraphrase
tables are language-independent and can be used
to improve evaluation support for other target lan-
guages. METEOR-NEXT, the METEOR paraphrase
tables, and the software used to generate para-
phrases are released under an open source license
and made available via the METEOR website.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL05.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9: Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proc. of WMT08.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of WMT09. In
Proc. of WMT09.
Michael Denkowski and Alon Lavie. 2010. Extend-
ing the METEOR Machine Translation Metric to
the Phrase Level for Improved Correlation with Hu-
man Post-Editing Judgments. In Proc. NAACL/HLT
2010.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proc. of WMT07.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
M. Przybocki, K. Peterson, and S Bronsart. 2008. Offi-
cial results of the NIST 2008 ?Metrics for MAchine
TRanslation? Challenge (MetricsMATR08).
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. In Proc. of WMT09.
C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.
342
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 98?106,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Automatic Category Label Coarsening for Syntax-Based Machine
Translation
Greg Hanneman and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, alavie}@cs.cmu.edu
Abstract
We consider SCFG-basedMT systems that get
syntactic category labels from parsing both
the source and target sides of parallel train-
ing data. The resulting joint nonterminals of-
ten lead to needlessly large label sets that are
not optimized for an MT scenario. This pa-
per presents a method of iteratively coarsening
a label set for a particular language pair and
training corpus. We apply this label collaps-
ing on Chinese?English and French?English
grammars, obtaining test-set improvements of
up to 2.8 BLEU, 5.2 TER, and 0.9 METEOR
on Chinese?English translation. An analysis
of label collapsing?s effect on the grammar
and the decoding process is also given.
1 Introduction
A common modeling choice among syntax-based
statistical machine translation systems is the use of
synchronous context-free grammar (SCFG), where a
source-language string and a target-language string
are produced simultaneously by applying a series of
re-write rules. Given a parallel corpus that has been
statistically word-aligned and annotated with con-
stituency structure on one or both sides, SCFG mod-
els for MT can be learned via a variety of methods.
Parsing may be applied on the source side (Liu et al,
2006), on the target side (Galley et al, 2004), or on
both sides of the parallel corpus (Lavie et al, 2008;
Zhechev and Way, 2008).
In any of these cases, using the raw label set from
source- and/or target-side parsers can be undesir-
able. Label sets used in statistical parsers are usu-
ally inherited directly from monolingual treebank
projects, where the inventory of category labels was
designed by independent teams of human linguists.
These labels sets are not necessarily ideal for sta-
tistical parsing, let alne for bilingual syntax-based
translation models. Further, the side(s) on which
syntax is represented defines the nonterminal label
space used by the resulting SCFG. A pair of aligned
adjectives, for example, may be labeled ADJ if only
source-side syntax is used, JJ if only target-side syn-
tax is used, or ADJ::JJ if syntax from both sides
is used in the grammar. Beyond such differences,
however, most existing SCFG-based MT systems
do not further modify the nonterminal label set in
use. Those that do require either specialized de-
coders or complicated parameter tuning, or the la-
bel set may be unsatisfactory from a computational
point of view (Section 2).
We believe that representing both source-side and
target-side syntax is important. Even assuming two
monolingually perfect label sets for the source and
target languages, using label information from only
one side ignores any meaningful constraints ex-
pressed in the labels of the other. On the other hand,
using the default node labels from both sides gener-
ates a joint nonterminal set of thousands of unique
labels, not all of which may be useful. Our real pref-
erence is to use a joint nonterminal set adapted to
our particular language pair or translation task.
In this paper, we present the first step towards
a tailored label set: collapsing syntactic categories
to remove the most redundant labels and shrink the
overall source?target nonterminal set.1 There are
1The complementary operation, splitting existing labels, is
beyond the scope of this paper and is left for future work.
98
two problems with an overly large label set:
First, it encourages labeling ambiguity among
rules, a well-known practical problem in SCFG-
based MT. Most simply, the same right-hand side
may be observed in rule extraction with a variety of
left-hand-side labels, each leading to a unique rule
in the grammar. The grammar may further contain
many rules with the same structure and reordering
pattern that differ only with respect to the actual la-
bels in use. Together, these properties can cause an
SCFG-based MT system to process a large number
of alternative syntactic derivations that use different
rules but produce identical output strings. Limiting
the possible number of variant labelings cuts down
on ambiguous derivations.
Second, a large label set leads to rule sparsity. A
rule whose right-hand side can only apply on a very
tightly specified set of labels is unlikely to be es-
timated reliably from a parallel corpus or to apply
in all needed cases at test time. However, a coarser
version of its application constraints may be more
frequently observed in training data and more likely
to apply on test data.
We therefore introduce a method for automati-
cally clustering and collapsing category labels, on
either one or both sides of SCFG rules, for any lan-
guage pair and choice of statistical parsers (Section
3). Turning to alignments between source and tar-
get parse nodes as an additional source of informa-
tion, we calculate a distance metric between any
two labels in one language based on the difference
in alignment probabilities to labels in the other lan-
guage. We then apply a greedy label collapsing al-
gorithm that repeatedly merges the two labels with
the closest distance until some stopping criterion is
reached. The resulting coarsened labels are used in
the SCFG rules of a syntactic machine translation
system in place of the original labels.
In experiments on Chinese?English translation
(Section 4), we find significantly improved perfor-
mance of up to 2.8 BLEU points, 5.2 TER points,
and 0.9 METEOR points by applying varying de-
grees of label collapsing to a baseline syntax-based
MT system (Section 5). In our analysis of the results
(Section 6), we find that the largest immediate effect
of coarsening the label set is to reduce the number of
fully abstract hierarchical SCFG rules present in the
grammar. These rules? increased permissiveness, in
turn, directs the decoder?s search into a largely dis-
joint realm from the search space explored by the
baseline system. A full summary and ideas for fu-
ture work are given in Section 7.
2 Related Work
One example of modifying the SCFG nonterminal
set is seen in the Syntax-Augmented MT (SAMT)
system of Zollmann and Venugopal (2006). In
SAMT rule extraction, rules whose left-hand sides
correspond exactly to a target-side parse node t re-
tain that label in the grammar. Additional nontermi-
nal labels of the form t1+ t2 are created for rules
spanning two adjacent parse nodes, while catego-
rial grammar?style nonterminals t1/t2 and t1\t2 are
used for rules spanning a partial t1 node that is miss-
ing a t2 node to its right or left.
These compound nonterminals in practice lead to
a very large label set. Probability estimates for rules
with the same structure up to labeling can be com-
bined with the use of a preference grammar (Venu-
gopal et al, 2009), which replaces the variant label-
ings with a single SCFG rule using generic ?X? la-
bels. The generic rule?s ?preference? over possible
labelings is stored as a probability distribution inside
the rule for use at decoding time. Preference gram-
mars thus reduce the label set size to one for the pur-
poses of some feature calculations ? which avoids
the fragmentation of rule scores due to labeling am-
biguity ? but the original labels persist for specify-
ing which rules may combine with which others.
Chiang (2010) extended SAMT-style labels to
both source- and target-side parses, also introducing
a mechanism by which SCFG rules may apply at run
time even if their labels do not match. Under Chi-
ang?s soft matching constraint, a rule headed by a la-
bel A::Z may still plug into a substitution site labeled
B::Y by paying additional model costs substB?A
and substY?Z . This is an on-the-fly method of
coarsening the effective label set on a case-by-case
basis. Unfortunately, it also requires tuning a sep-
arate decoder feature for each pair of source-side
and each pair of target-side labels. This tuning can
become prohibitively complex when working with
standard parser label sets, which typically contain
between 30 and 70 labels on each side.
99
JJ JJR JJS
Figure 1: Alignment distributions over French labels for the English adjective labels JJ, JJR, and JJS.
3 Label Collapsing Algorithm
We begin with an initial set of SCFG rules extracted
from a parallel parsed corpus, where S denotes the
set of labels used on the source side and T denotes
the set of labels used on the target side. Each rule has
a left-hand side of the form s :: t, where s ? S and
t ? T , meaning that a node labeled s was aligned to
a node labeled t in a parallel sentence. From the left-
hand sides of all extracted rule instances, we com-
pute label alignment distribution P (s | t) by simple
counting and normalizing:
P (s | t) =
#(s :: t)
#(t)
(1)
We use an analogous equation to calculate P (t | s).
For two target-language labels t1 and t2, we have
an equally simple metric of alignment distribution
difference d: the total of the absolute differences in
likelihood for each aligned source-language label.
d(t1, t2) =
?
s?S
|P (s | t1) ? P (s | t2)| (2)
Again, the calculation for d(s1, s2) is analogous.
If t1 and t2 are plotted as points in |S|-
dimensional space such that each point?s position in
dimension s is equal to P (s | t), then this metric is
equivalent to the L1 distance between t1 and t2.
Sample alignment distributions into French for
three English adjective labels are shown in Figure
1. Bars in the chart represent alignment probabili-
ties between French and English according to Equa-
tion 1, with the various French labels as s and JJ,
JJR, or JJS as t. To compute an L1 alignment dis-
tribution difference between a pair of English ad-
jective tags, we sum the absolute differences in bar
heights for each column of two graphs, as in Equa-
tion 2. It is already visually clear from Figure 1
that all three English labels are somewhat related
in terms of distribution, but it appears that JJR and
JJS are more closely related to each other than either
is to JJ. This is reflected in the actual L1 distances:
d(JJ, JJR) = 0.9941 and d(JJ, JJS) = 0.8730, but
d(JJR, JJS) = 0.3996.
Given the above method for computing an align-
ment distribution difference for any pair of labels,
we develop an iterative greedy method for label col-
lapsing. At each step, we compute the L1 distance
between all pairs of labels, then collapse the pair
with the smallest distance into a single label. Then
L1 distances are recomputed over the new, smaller
label set, and again the label pair with the smallest
distance is collapsed. This process continues until
some stopping criterion is reached. Label pairs be-
ing considered for collapsing may be only source-
side labels, only target-side labels, or both. In gen-
eral, we choose to allow label collapsing to apply on
either side during each iteration of our algorithm.
In the limit, label collapsing can be applied it-
eratively until all syntactic categories on both the
source and target sides have been collapsed into a
single label. In Section 5, we explore several earlier
and more meaningful stopping points.
4 Experimental Setup
Experiments are conducted on Chinese-to-English
translation using approximately 300,000 sentence
pairs from the FBIS corpus. To obtain parse trees
over both sides of each parallel corpus, we used
the English and Chinese grammars of the Berkeley
100
parser (Petrov and Klein, 2007).
Given a parsed and word-aligned parallel sen-
tence, we extract SCFG rules from it following the
procedure of Lavie et al (2008). The method first
identifies node alignments between the two parse
trees according to support from the word alignments.
A node in the source parse tree will be aligned to
a node in the target parse tree if all the words in
the yield of the source node are either all aligned to
words within the yield of the target node or have no
alignments at all. Then SCFG rules can be extracted
from adjacent levels of aligned nodes, which spec-
ify points at which the tree pair can be decomposed
into minimal SCFG rules. In addition to producing
a minimal rule, each decomposition point also pro-
duces a phrase pair rule with the node pair?s yields
as the right-hand side, as long as the length of the
yield is less than a specified threshold.
Following grammar extraction, labels are option-
ally clustered and collapsed according to the algo-
rithm in Section 3. The grammar is re-written with
the modified nonterminals, then scored as usual ac-
cording to our translation model features. Feature
weights themselves are learned via minimum error
rate training as implemented in Z-MERT (Zaidan,
2009) with the BLEU metric (Papineni et al, 2002).
Decoding is carried out with Joshua (Li et al, 2009),
an open-source platform for SCFG-based MT.
Due to engineering limitations in decoding with
a large grammar, we apply three additional error-
correction and filtering steps to every system. First,
we observed that the syntactic parsers were most
likely to make labeling errors for cardinal numbers
in English and punctuation marks in all languages.
We thus post-process the parses of our training data
to tag all English cardinal numbers as CD and to
overwrite the labels of various punctuation marks
with the correct labels as defined by each language?s
label set. Second, after rule extraction, we com-
pute the distribution of left-hand-side labels for each
unique labeled right-hand side in the grammar, and
we remove the labels in the least frequent 10% of the
distribution. This puts a general-purpose limit on la-
beling ambiguity. Third, we filter and prune the final
scored grammar to each individual development and
test set before decoding: all matching phrase pairs
are retained, along with the most frequent 10,000 hi-
erarchical grammar rules.
5 Experiments and Results
In our first set of experiments, we sought to explore
the effect of increasing degrees of label collapsing
on a baseline system and to determine a reasonable
stopping point. Starting with the baseline grammar,
we ran the label collapsing algorithm of Section 3
until all the constituent labels on each side had been
collapsed into a single category. We next examined
the L1 distances between the label pairs that had
been merged in each iteration of the algorithm. This
data is shown in Figure 2 as a plot of L1 distance
versus iteration number. The distances between the
successive labels merged in the first 29 iterations of
the algorithm are nearly monotonically increasing,
followed by a much larger discontinuity at iteration
30. Similar patterns emerge for iterations 30 to 45
and for iterations 46 to 60. The next regions of the
graph, from iterations 61 to 81 and from iterations
82 to 99, show an increasing prevalence of disconti-
nuities. Finally, from iterations 100 to 123, the suc-
cessive L1 distances entirely alternate between very
high and very low values.
Discontinuities are merely the result of a label
pair in one language suddenly scoring much lower
on the distribution difference metric than previously,
thanks to some change that has occurred in the la-
bel set of the other language. Looking back to Fig-
ure 1, for example, we could bring the distributions
for JJ and JJS much closer together by merging A
and ADV on the French side. Although such sudden
drops in distribution difference value are expected,
they may provide an indication of when the label
collapsing algorithm has progressed too far, since
we have so reduced the label set that categories pre-
viously very different have become much less dis-
tinguishable. On the other hand, further reduction of
the label set may have a variety of pratical benefits.
We tested this trade-off empirically by building
five Chinese?English MT systems, each exhibiting
an increasing degree of label collapsing compared to
the original label set, which serves as our baseline.
The degree of label collapsing in each of the five
systems corresponds to one of the major discontinu-
ity features highlighted in the right-hand side Figure
2. The systems were tuned on the NIST MT 2006
data set, and we evaluated performance on the NIST
MT 2003 and 2008 sets. (All data sets have four
101
Iter. L1 Dist.
29 0.3646
45 0.5607
60 0.6155
81 0.8665
99 1.1303
Figure 2: Observed L1 distance values for the labels merged in each iteration of our algorithm on a Chinese?English
SCFG. We divide the graph into six distinct regions using the cutoffs at right.
Chinese?English MT 2003 Test Set MT 2008 Test Set
System METEOR BLEU TER METEOR BLEU TER
Baseline 54.35 24.39 68.01 45.68 18.27 69.18
Collapsed, 29 iterations 55.24 27.03 63.77 46.25 19.78 65.88
Collapsed, 45 iterations 54.65 26.69 62.76 46.02 19.60 64.88
Collapsed, 60 iterations 55.11 27.23 63.06 46.30 20.19 65.18
Collapsed, 81 iterations 54.87 26.87 64.92 45.70 20.48 66.75
Collapsed, 99 iterations 54.86 26.16 64.17 45.87 19.52 65.61
Table 1: Results of applying increasing degrees of label collapsing on our Chinese?English baseline system. Bold
figures indicate the best score in each column.
references.) Table 1 reports automatic metric results
for version 1.0 of METEOR (Lavie and Denkowski,
2009) using the default settings, uncased IBM-style
BLEU (Papineni et al, 2002), and uncased TER ver-
sion 0.7 (Snover et al, 2006).
No matter the degree of label collapsing, we find
significant improvements in BLEU and TER scores
on both test sets. On the MT 2003 set, label-
collapsed systems score 1.77 to 2.84 BLEU points
and 3.09 to 5.25 TER points better than the baseline.
OnMT 2008, improvements range from 1.25 to 2.21
points on BLEU and from 2.43 to 4.30 points on
TER. Improvements on both sets according to ME-
TEOR, though smaller, are still noticable (up to 0.89
points). In the case of BLEU, we verified the sig-
nificance of the improvements by conducting paired
bootstrap resampling (Koehn, 2004) on theMT 2003
output. With n = 1000 and p < 0.05, all five label-
collapsed systems were statistically significant im-
provements over the baseline, and all other collapsed
systems were significant improvements over the 99-
iteration system.
Thus, though the system that provides the highest
score changes across metrics and test sets, the over-
all pattern of scores suggests that over-collapsing la-
bels may start to weaken results. A more moderate
stopping point is thus preferable, but beyond that we
suspect the best result is determined more by the test
set, automatic metric choice, and MERT instability
than systematic changes in the label set.
6 Analysis
Table 1 showed a strong practical benefit to running
the label collapsing algorithm. In this section, we
102
seek to further understand where this benefit comes
from, tracing the effects of label collapsing via its
modification of labels themselves, the differences in
the resulting grammars, and collapsing?s effect on
decoding and output.
6.1 Labels Selected for Collapsing
Our first concern is for the size of the grammar?s
overall nonterminal set. The baseline system uses a
total of 55 labels on the Chinese side and 71 on the
English side, leading to an observed joint nontermi-
nal set of 1556 unique labels. After 29 iterations
of label collapsing, this is reduced to 46 Chinese,
51 English, and 1035 joint labels ? a reduction of
33%. In the grammar of our most collapsed gram-
mar variant (99 iterations), the nonterminal set is re-
duced to 14 English and 14 Chinese labels, for a to-
tal of 106 joint labels and a reduction of 93% from
the baseline grammar. This demonstrates one facet
of our introductory claim from Section 1: since we
have improved translation results by removing the
vast majority of our grammar nonterminals, most of
the initial joint Chinese?English syntactic categories
were not necessary for Chinese?English translation.
We identify three broad trends in the sets of labels
that are collapsed:
? Full Subtype Collapsing. The Chinese-side
parses include six phrase-level tags for various
types of verb compounds. As label collapsing
progresses, these labels are all combined with
each other at relatively low L1 distances.
? Partial Subtype Collapsing. In English, three
of the four noun labels (NN, NNS, and NNPS)
form a cohesive cluster early on in Chinese?
English collapsing. However, the fourth tag
(NNP, for singular proper nouns) remains sep-
arate, then later joins a cluster for more
adjective-like labels.
? Combination by Syntactic Function. In
French?English label collapsing (see below),
we find the creation of a combined label in
English for reduced relative clauses (RRC),
adjective phrases headed by a wh-adjective
(WHADJP), and interjections (INTJ). Even
though these tags are unrelated in surface form,
at some level they all represent parenthetical in-
sertions or explanatory phrases.
The formulation of the L1 distance metric in Sec-
tion 3 means that our label collapsing algorithm will
naturally produce different label clusters for differ-
ent input grammars ? any change in the Viterbi
word alignments, underlying parallel corpus, initial
label set, or choice of automatic parser will neces-
sarily change the label alignment distributions on
which the collapsing algorithm is based. In par-
ticular, the label clusters formed in one language
are likely to be markedly different depending on
which other language it is paired with. We exam-
ine these differences in more detail for the case of
English when paired with either Chinese or with
French. Our 29-iteration run of label collapsing for
Chinese?English merged labels on the English side
19 times. For an exact comparison, we run iterations
of label collapsing on a large-scale French?English
grammar, extracted in the same way as the Chinese?
English grammar, until the same number of English-
side merges have been carried out, then examine the
results.
Table 2 shows the English label clusters cre-
ated from the Chinese?English and French?English
grammars, arranged by broad syntactic categories.
The differences in English label clusters hint at dif-
ferences in the source-side label sets, as well as
structural divergences relevant for translating Chi-
nese versus French into English.
For example, Table 2 shows partial subtype col-
lapsing of the English verb tags when paired with
French. The French Berkeley parser has a single tag,
V, to represent all verbs, and most English verb tags
as well as the tag for modals very consistently align
to it. The exception is VBG, for present-progressive
or gerundive verb forms, which is more easily con-
flatable in French?English translation with a noun or
an adjective. In translation from Chinese, however,
it is VBG that is combined early on with a smaller
selection of English verb labels that correspond most
strongly to a basic Chinese verb. Other English verb
tags are more likely to align to Chinese copulas, ex-
istential verbs, and nouns; they are not combined
with the group for more ?typical? verbs until itera-
tion 67. The adverb series presents another example
of translational divergence between language pairs.
103
Cluster Chinese?English French?English
Nouns NN NNS NNPS # NN NNS $
Verbs VB VBG VBN VB VBD VBN VBP VBZ MD
Adverbs RB RBR RBR RBS
Punctuation LRB RRB ? ? , . ? ?
Prepositions IN TO SYM
Determiners DT PRP$
Noun phrases NP NX QP UCP NAC NP WHNP NX WHADVP NAC
Adjective phrases ADJP WHADJP
Adverb phrases ADVP WHADVP
Prepositional phrases PP WHPP
Sentences S SINV SBARQ FRAG S SQ SBARQ
Table 2: English-side label clusters created after partial label collapsing of a Chinese?English and a French?English
grammar. In each case, the algorithm has been run until merges have occurred 19 times on the English side.
6.2 Effect on the Grammar
With a smaller label set, we also expect a reduc-
tion in the overall size of our various label-collapsed
grammars as labeling ambiguity is removed. In the
aggregate, however, even 99 iterations of Chinese?
English label collapsing has a minimal effect on
the total number of unique rules in the resulting
SCFG. A clearer picture emerges when we sepa-
rate rules according to their form. Figure 3 parti-
tions the grammar into three parts: one for phrase
pairs, where the rules? right-hand sides are made up
entirely of terminals (?P-type? rules); one for hier-
archical rules whose right-hand sides are made up
entirely of nonterminals (abstract or ?A-type? rules);
and one for hierarchical rules whose right-hand sides
include a mix of terminals and nonterminals (re-
maining grammar or ?G-type? rules).
This separation reveals two interesting facts.
First, although the size of the label set continues
to shrink considerably between iterations 29 and 81,
the number of unique rules in the grammar remains
relatively unchanged. Second, the reduction in the
size of the grammar is largely due to a reduction in
the number of fully abstract grammar rules, rather
than phrase pairs or partially lexicalized grammar
rules. From these observations, we infer that the ma-
jor practical benefit of label collapsing is a reduction
in rule sparsity rather than a reduction in left-hand-
side labeling ambiguity. Many highly ambiguous
rules have had their possible left-hand-side labels ef-
fectively pruned down by the pre-processing steps
we described in Section 4, which in preliminary ex-
Figure 3: The effect of label collapsing on the number of
unique phrase pairs, partially lexicalized grammar rules,
and fully abstract grammar rules.
periments had a larger effect on the overall size of
the grammar than label collapsing. As a more com-
plementary technique, increasing the applicability of
the fully abstract rules via label collapsing is impor-
tant for performance. Such rules make up 49% to
59% of the hierarchical rules retained at decoding
time, and they account for 76% to 87% of the rule
application instances on the MT 2003 test set.
6.3 Effect on Decoding and Output
Interestingly, the label collapsing algorithm does
not owe its success at decoding time to a signif-
icant increase in the number of rule applications.
Among our systems, both the 45-iteration and the
104
60-iteration collapsed versions scored highly ac-
cording to automatic metrics. Nevertheless, the 45-
iteration system used 32% and 38% more rule appli-
cations than the baseline on the MT 2003 and MT
2008 test sets, respectively, while the 60-iteration
system used 15% and 11% fewer. The number of
unique rule types and the number of reordering rules
applied on a test set may also go up or down.
Instead, the practical effect of making the gram-
mar more permissive seems to be a significant
change in the search space explored during decod-
ing. This can be seen superficially via an exam-
ination of output n-best lists. On both test sets
combined (2276 sentences), the 60-iteration label-
collapsed system?s top-best output appears in the
baseline?s 100-best list in only 81 sentences. When
it does appear in the baseline, the improved system?s
translation is ranked fairly highly ? always 30th
place or higher. Conversely, the baseline?s top-best
output tends to be ranked lower in the improved sys-
tem?s n-best list: among the 114 times it appears, it
is placed as low as 87th.
We ran a small follow-up analysis on the transla-
tion fragments explored during decoding. Using a
modified version of the Joshua decoder, we dumped
lists of hypergraph entries that were explored by
cube pruning during Joshua?s lazy generation of a
100-best list. These entries represent the decoder?s
approximative search through the larger space of
translations licenced by the grammar for each test
sentence. We then compared the hypergraph entries,
excluding glue rules, produced on the first 100 sen-
tences of the MT 2003 test set by both the baseline
and the 60-iteration label-collapsed system.
A full 90% of the entries produced by the label-
collapsed system had no analogue in the baseline
system. The average length of the entries that do
match is 2.3 source words, compared with an aver-
age of 6.2 words for the non-matched entries. We
believe that the increased permissiveness of the hi-
erarchical grammar rules is again the root cause of
these results. Low-level constituents are more likely
to be matched in both the baseline and the label-
collapsed system, but different applications of the
grammar rules, perhaps combined with retuned fea-
ture weights, leads the search for larger translation
fragments into new areas.
7 Conclusions and Future Work
This paper has presented a language-specific method
for automatically coarsening the label set used in
an SCFG-based MT system. Our motivation for
collapsing labels comes from the intuition that the
full cross-product of joint source?target labels, as
produced by statistical parsers, is too large and not
specifically created for bilingual MT modeling. The
greedy collapsing algorithm we developed is based
on iterative merging of the two single-language la-
bels whose alignment distributions are most similar
according to a simple L1 distance metric.
In applying varying degrees of label collapsing to
a baseline MT system, we found significantly im-
proved automatic metric results even when the size
of the joint label set had been reduced by 93%. The
best results, however, were obtained with more mod-
erate coarsening. The coarser labels that our method
produces are syntactically meaningful and represent
specific cross-language behaviors of the language
pair involved. At the grammar level, label collaps-
ing primarily caused a reduction in the number of
rules whose right-hand sides are made up entirely of
nonterminals. The coarser labels made the grammar
more permissive, cutting down on the problem of
rule sparsity. Labeling ambiguity, on the other hand,
was more effectively addressed by pre-processing
we applied to the grammar beforehand. At run time,
the more permissive collapsed grammar allowed the
decoder to search a markedly different region of the
allowable translation space than in the baseline sys-
tem, generally leading to improved output.
One shortcoming of our current algorithm is that
it is based entirely on label alignment distribution
without regard to the different contexts in which la-
bels occur. It thus cannot distinguish between two
labels that align similarly but appear in very different
rules. For example, singular common nouns (NN)
and plural proper nouns (NNPS) in English both
most frequently align to French nouns (N) and are
thus strong candidates for label collapsing under our
algorithm. However, when building noun phrases,
an N::NNPS will more likely require a rule to delete
a French-side determiner, while an N::NN will typ-
ically require a determiner in both French and En-
glish. Thus, collapsing NN and NNPS may lead to
additional ambiguity or incorrect choices when ap-
105
plying larger rules.
Another dimension to be explored is the trade-off
between greedy collapsing and other methods that
cluster all labels at once. K-means clustering could
be a reasonable contrast in this respect; its down-
side would be that all labels in one language must
be assigned to clusters without knowledge of what
clusters are being formed in the other language.
Finally, label collapsing is only the first step in a
broader exploration of SCFG labeling for MT. We
also plan to investigate methods for refining exist-
ing category labels in order to find finer-grained sub-
types that are useful for translating a particular lan-
guage pair. By running label collapsing and refining
together, our end goal is to be able to adapt standard
parser labels to individual translation scenarios.
Acknowledgments
This research was supported in part by U.S. Na-
tional Science Foundation grants IIS-0713402 and
IIS-0915327 and by the DARPA GALE program.
Thanks to Chris Dyer for providing the word-
aligned and preprocessed FBIS corpus we used in
our Chinese?English experiments, and to Jon Clark
for suggesting and setting up the hypergraph com-
parison analysis. We also thank Yahoo! for the use
of the M45 research computing cluster, where we
ran many steps of our experimental pipeline.
References
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
HLT-NAACL 2004: Main Proceedings, pages 273?
280, Boston, MA, May.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 609?616, Sydney, Aus-
tralia, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the ACL, pages 236?244, Boulder, CO,
June.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Ventsislav Zhechev and Andy Way. 2008. Automatic
generation of parallel treebanks. In Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 1105?1112, Manchester, England, Au-
gust.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY, June.
106
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 135?144,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
A General-Purpose Rule Extractor for SCFG-Based Machine Translation
Greg Hanneman and Michelle Burroughs and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, mburroug, alavie}@cs.cmu.edu
Abstract
We present a rule extractor for SCFG-based
MT that generalizes many of the contraints
present in existing SCFG extraction algo-
rithms. Our method?s increased rule coverage
comes from allowing multiple alignments, vir-
tual nodes, and multiple tree decompositions
in the extraction process. At decoding time,
we improve automatic metric scores by signif-
icantly increasing the number of phrase pairs
that match a given test set, while our experi-
ments with hierarchical grammar filtering in-
dicate that more intelligent filtering schemes
will also provide a key to future gains.
1 Introduction
Syntax-based machine translation systems, regard-
less of the underlying formalism they use, depend
on a method for acquiring bilingual rules in that for-
malism to build the system?s translation model. In
modern syntax-based MT, this formalism is often
synchronous context-free grammar (SCFG), and the
SCFG rules are obtained automatically from parallel
data through a large variety of methods.
Some SCFG rule extraction techniques require
only Viterbi word alignment links between the
source and target sides of the input corpus (Chi-
ang, 2005), while methods based on linguistic con-
stituency structure require the source and/or target
side of the input to be parsed. Among such tech-
niques, most retain the dependency on Viterbi word
alignments for each sentence (Galley et al, 2004;
Zollmann and Venugopal, 2006; Lavie et al, 2008;
Chiang, 2010) while others make use of a general,
corpus-level statistical lexicon instead of individual
alignment links (Zhechev and Way, 2008). Each
method may also place constraints on the size, for-
mat, or structure of the rules it returns.
This paper describes a new, general-purpose rule
extractor intended for cases in which two parse trees
and Viterbi word alignment links are provided for
each sentence, although compatibility with single-
parse-tree extraction methods can be achieved by
supplying a flat ?dummy? parse for the missing tree.
Our framework for rule extraction is thus most sim-
ilar to the Stat-XFER system (Lavie et al, 2008;
Ambati et al, 2009) and the tree-to-tree situation
considered by Chiang (2010). However, we signif-
icantly broaden the scope of allowable rules com-
pared to the Stat-XFER heuristics, and our approach
differs from Chiang?s system in its respect of the lin-
guistic constituency constraints expressed in the in-
put tree structure. In summary, we attempt to extract
the greatest possible number of syntactically moti-
vated rules while not allowing them to violate ex-
plicit constituent boundaries on either the source or
target side. This is achieved by allowing creation of
virtual nodes, by allowing multiple decompositions
of the same tree pair, and by allowing extraction of
SCFG rules beyond the minimial set required to re-
generate the tree pair.
After describing our extraction method and com-
paring it to a number of existing SCFG extraction
techniques, we present a series of experiments ex-
amining the number of rules that may be produced
from an input corpus. We also describe experiments
on Chinese-to-English translation that suggest that
filtering a very large extracted grammar to a more
135
Figure 1: Sample input for our rule extraction algorithm. It consists of a source-side parse tree (French) and a target-
side parse tree (English) connected by a Viterbi word alignment.
moderate-sized translation model is an important
consideration for obtaining strong results. Finally,
this paper concludes with some suggestions for fu-
ture work.
2 Rule Extraction Algorithm
We begin with a parallel sentence consisting of a
source-side parse tree S, a target-side parse tree T ,
and a Viterbi word alignment between the trees?
leaves. A sample sentence of this type is shown in
Figure 1. Our goal is to extract a number of SCFG
rules that are licensed by this input.
2.1 Node Alignment
Our algorithm first computes a node alignment be-
tween the parallel trees. A node s in tree S is aligned
to a node t in tree T if the following constraints are
met. First, all words in the yield of s must either
be aligned to words within the yield of t, or they
must be unaligned. Second, the reverse must also
hold: all words in the yield of t must be aligned to
words within the yield of s or again be unaligned.
This is analogous to the word-alignment consistency
constraint of phrase-based SMT phrase extraction
(Koehn et al, 2003). In Figure 1, for example, the
NP dominating the French words les voitures bleues
is aligned to the equivalent English NP node domi-
nating blue cars.
As in phrase-based SMT, where a phrase in one
language may be consistent with multiple possible
phrases in the other language, we allow parse nodes
in both trees to have multiple node alignments. This
is in contrast to one-derivation rule extractors such
as that of Lavie et al (2008), in which each node
136
in S may only be aligned to a single node in T and
vice versa. The French NP node Ma me`re, for exam-
ple, aligns to both the NNP and NP nodes in English
producing Mother.
Besides aligning existing nodes in both parse trees
to the extent possible, we also permit the introduc-
tion of ?virtual? nodes into either tree. Virtual nodes
are created when two or more contiguous children of
an existing node are aligned consistently to a node or
a similar set of two or more contiguous children of
a node in the opposite parse tree. Virtual nodes may
be aligned to ?original? nodes in the opposite tree or
to other virtual nodes.
In Figure 1, the existing English NP node blue
cars can be aligned to a new virtual node in French
that dominates the N node voitures and the AP node
bleues. The virtual node is inserted as the parent
of N and AP, and as the child of the NP node di-
rectly above. In conjunction with node alignments
between existing nodes, this means that the English
NP blue cars is now aligned twice: once to the orig-
inal French NP node and once to the virtual node
N+AP. We thus replicate the behavior of ?growing
into the gaps? from phrase-based SMT in the pres-
ence of unaligned words. As another example, a vir-
tual node in French covering the V node avait and
the ADV node toujours could be created to align
consistently with a virtual node in English covering
the VBD node had and the ADVP node always.
Since virtual nodes are always created out of chil-
dren of the same node, they are always consis-
tent with the existing syntactic structure of the tree.
Within the constraints of the existing tree structure
and word alignments, however, all possible virtual
nodes are considered. This is in keeping with our
philosophy of allowing multiple alignments with-
out violating constituent boundaries. Near the top
of the trees in Figure 1, for example, French virtual
nodes NP+VN+NP (aligned to English NP+VP) and
VN+NP+PU (aligned to VP+PU) both exist, even
though they overlap. In our procedure, we do allow a
limit to be placed the number of child nodes that can
be combined into a virtual node. Setting this limit
to two, for instance, will constrain node alignment
to the space of possible synchronous binarizations
consistent with the Viterbi word alignments.
2.2 Grammar Extraction
Given the final set of node alignments between the
source tree and the target tree, SCFG rules are ob-
tained via a grammar extraction step. Rule extrac-
tion proceeds in a depth-first manner, such that rules
are extracted and cached for all descendents of a
source node s before rules in which s is the left-hand
side are considered. Extracting rules where source
node s is the left-hand side consists of two phases:
decomposition and combination.
The first phase is decomposition of node s into
all distinct sets D = {d1, d2, . . . , dn} of descendent
nodes such that D spans the entire yield of node s,
where di ? D is node-aligned or is an unaligned ter-
minal for all i, and di has no ancestor a where a is a
descendent of s and a is node-aligned. Each D thus
represents the right-hand side of a minimal SCFG
rule rooted at s. Due to the introduction of overlap-
ping virtual nodes, the decomposition step may in-
volve finding multiple sets of decomposition points
when there are multiple nodes with the same span at
the same level of the tree.
The second phase involves composition of all
rules derived from each element of D subject to cer-
tain constraints. Rules are constructed using s, the
set of nodes Ts = {t | s is aligned to t}, and each
decomposed node set D. The set of left-hand sides
is {s} ? Ts, but there may be many right-hand sides
for a given t and D. Define rhs(d) as the set of
right-hand sides of rules that are derived from d, plus
all alignments of d to its aligned set Td. If d is a
terminal, word alignments are used in the place of
node alignments. To create a set of right-hand sides,
we generate the set R = rhs(d1) ? . . . ? rhs(dn).
For each r ? R, we execute a combine operation
such that combine(r) creates a new right-hand side
by combining the component right-hand sides and
recalculating co-indexes between the source- and
target-side nonterminals. Finally, we insert any un-
aligned terminals on either side.
We work through a small example of grammar ex-
traction using Figure 2, which replicates a fragment
of Figure 1 with virtual nodes included. The En-
glish node JJ is aligned to the French nodes A and
AP, the English node NNS is aligned to the French
node N and the virtual node D+N, and the English
node NP is aligned to the French node NP and the
137
Figure 2: A fragment of Figure 1 with virtual nodes (sym-
bolized by dashed lines) added on the French side. Nodes
D, N, and AP are all original children of the French NP.
virtual node N+AP. To extract rules from the French
node NP, we consider two potential decompositions:
D1 = {D+N,AP} and D2 = {les,N+AP}. Since
the French NP is aligned only to the English NP, the
set of left-hand sides is {NP::NP}, where we use the
symbol ?::? to separate the source and target sides
of joint nonterminal label or a rule.
In the next step, we use cached rules and
alignments to generate all potential right-hand-side
pieces from these top-level nodes:
rhs(D+N) =
{
[D+N1] :: [NNS1],
[les voitures] :: [cars]
}
rhs(AP) =
?
?
?
[AP1] :: [JJ1],
[A1] :: [JJ1],
[bleues] :: [blue]
?
?
?
rhs(les) = ?
rhs(N+AP) =
?
?????????
?????????
[N+AP1] :: [NP1],
[N1 AP2] :: [JJ2 NNS1],
[N1 A2] :: [JJ2 NNS1],
[voitures AP1] :: [JJ1 cars],
[voitures A1] :: [JJ1 cars],
[N1 bleues] :: [blue NNS1],
[voitures bleues] :: [blue cars]
?
?????????
?????????
Next we must combine these pieces. For example,
from D1 we derive the full right-hand sides
1. combine([les voitures]::[cars], [bleues]::[blue])
= [les voitures bleues]::[blue cars]
2. combine([les voitures]::[cars], [A1]::[JJ1])
= [les voitures A1]::[JJ1 cars]
3. combine([les voitures]::[cars], [AP1]::[JJ1])
= [les voitures AP1]::[JJ1 cars]
4. combine([D+N1]::[NNS1], [bleues]::[blue])
= [D+N1 bleues]::[blue NNS1]
5. combine([D+N1]::[NNS1], [A1]::[JJ1])
= [D+N1 A2]::[JJ2 NNS1]
6. combine([D+N1]::[NNS1], [AP1]::[JJ1])
= [D+N1 AP2]::[JJ2 NNS1]
Similarly, we derive seven full right-hand sides from
D2. Since rhs(les) is empty, rules derived have
right-hand sides equivalent to rhs(N+AP) with the
unaligned les added on the source side to com-
plete the span of the French NP. For example,
combine([N+AP1]::[NP1]) = [les N+AP1]::[NP1].
In the final step, the left-hand side is added to each
full right-hand side. Thus,
NP :: NP? [les voitures A1] :: [JJ1 cars]
is one example rule extracted from this tree.
The number of rules can grow rapidly: if the parse
tree has a branching factor of b and a depth of h,
there are potentially O(2b
h
) rules extracted. To con-
trol this, we allow certain constraints on the rules ex-
tracted that can short-circuit right-hand-side forma-
tion. We allow separate restrictions on the number
of items that may appear on the right-hand side of
phrase pair rules (maxp) and hierarchical grammar
rules (maxg). We also optionally allow the exclu-
sion of parallel unary rules ? that is, rules whose
right-hand sides consist solely of a pair of aligned
nonterminals.
138
Tree Multiple Virtual Multiple
System Constraints Alignments Nodes Derivations
Hiero No ? ? Yes
Stat-XFER Yes No Some No
GHKM Yes No No Yes
SAMT No No Yes Yes
Chiang (2010) No No Yes Yes
This work Yes Yes Yes Yes
Table 1: Comparisons between the rule extractor described in this paper and other SCFG rule extraction methods.
3 Comparison to Other Methods
Table 1 compares the rule extractor described in Sec-
tion 2 to other SCFG extraction methods described
in the literature. We include comparisons of our
work against the Hiero system (Chiang, 2005), the
Stat-XFER system rule learner most recently de-
scribed by Ambati et al (2009), the composed ver-
sion of GHKM rule extraction (Galley et al, 2006),
the so-called Syntax-Augmented MT (SAMT) sys-
tem (Zollmann and Venugopal, 2006), and a Hiero?
SAMT extension with source- and target-side syntax
described by Chiang (2010). Note that some of these
methods make use of only target-side parse trees ?
or no parse trees at all, in the case of Hiero ? but
our primary interest in comparison is the constraints
placed on the rule extraction process rather than the
final output form of the rules themselves. We high-
light four specific dimensions along these lines.
Tree Constraints. As we mentioned in this pa-
per?s introduction, we do not allow any part of our
extracted rules to violate constituent boundaries in
the input parse trees. This is in contrast to Hiero-
derived techniques, which focus on expanding gram-
mar coverage by extracting rules for all spans in
the input sentence pair that are consistently word-
aligned, regardless of their correspondence to lin-
guistic constituents. Practitioners of both phrase-
based and syntax-based SMT have reported severe
grammar coverage issues when rules are required to
exactly match parse constituents (Koehn et al, 2003;
Chiang, 2010). In our work, we attempt to improve
the coverage of the grammar by allowing multiple
node alignments, virtual nodes, and multiple tree
decompositions rather than ignoring structure con-
straints.
Multiple Alignments. In contrast to all other ex-
traction methods in Table 1, ours allows a node in
one parse tree to be aligned with multiple nodes
in the other tree, as long as the word-alignment
and structure constraints are satisfied. However, we
do not allow a node to have multiple simultaneous
alignments ? a single node alignment must be cho-
sen for extracting an individual rule. In practice,
this prevents extraction of ?triangle? rules where the
same node appears on both the left- and right-hand
side of the same rule.1
Virtual Nodes. In keeping with our philosophy
of representing multiple alignments, our use of mul-
tiple and overlapping virtual nodes is less restrictive
than the single-alignment constraint of Stat-XFER.
Another key difference is that Stat-XFER requires
all virtual nodes to be aligned to original nodes in
the other language, while we permit virtual?virtual
node alignments. In respecting existing tree struc-
ture constraints, our virtual node placement is more
restrictive than SAMT or Chiang, where extracted
nodes may cross existing constituent boundaries.
Multiple Derivations. Galley et al (2006) ar-
gued that breaking a single tree pair into multiple
decompositions is important for correct probability
modeling. We agree, and we base our rule extrac-
tor?s acquisition of multiple derivations per tree pair
on techniques from both GHKM and Hiero. More
specifically, we borrow from Hiero the idea of cre-
ating hierarchical rules by subtracting and abstract-
ing all possible subsets of smaller phrases (aligned
nodes in our case) from larger phrases. Like GHKM,
1Figure 2 includes a potential triangle rule, D+N :: NNS ?
[les N1] :: [NNS1], where the English NNS node appears on
both sides of the rule. It is simultaneously aligned to the French
D+N and N nodes.
139
we do this exhaustively within some limit, although
in our case we use a rank limit on a rule?s right-hand
side rather than a limit on the depth of the subn-
ode subtractions. Our constraint achieves the goal
of controlling the size of the rule set while remaining
flexibile in terms of depth depending on the shape of
the parse trees.
4 Experiments
We conducted experiments with our rule extrac-
tor on the FBIS corpus, made up of approximately
302,000 Chinese?English sentence pairs. We parsed
the corpus with the Chinese and English grammars
of the Berkeley parser (Petrov and Klein, 2007) and
word-aligned it with GIZA++ (Och and Ney, 2003).
The parsed and word-aligned FBIS corpus served as
the input to our rule extractor, which we ran with a
number of different settings.
First, we acquired a baseline rule extraction
(?xfer-orig?) from our corpus using an implementa-
tion of the basic Stat-XFER rule learner (Lavie et al,
2008), which decomposes each input tree pair into a
single set of minimal SCFG rules2 using only origi-
nal nodes in the parse trees. Next, we tested the ef-
fect of allowing multiple decompositions by running
our own rule learner, but restricting its rules to also
only make use of original nodes (?compatible?). Fi-
nally, we investigated the total number of extractable
rules by allowing the creation of virtual nodes from
up to four adjacent sibling nodes and placing two
different limits on the length of the right-hand side
(?full-short? and ?full-long?). These configurations
are summarized in Table 2.
Rule Set maxp maxg Virtual Unary
xfer-orig 10 ? No Yes
compatible 10 5 No Yes
full-short 5 5 Yes No
full-long 7 7 Yes No
Table 2: Rule sets considered by a Stat-XFER baseline
(?xfer-orig?) and our own rule extractor.
2In practice, some Stat-XFER aligned nodes produce two
rules instead of one: a minimal hierarchical SCFG rule is al-
ways produced, and a phrase pair rule will also be produced for
node yields within the maxp cutoff.
4.1 Rules Extracted
As expected, we find that allowing multiple decom-
positions of each tree pair has a significant effect on
the number of extracted rules. Table 3 breaks the ex-
tracted rules for each configuration down into phrase
pairs (all terminals on the right-hand side) and hier-
archical rules (containing at least one nonterminal
on the right-hand side). We also count the num-
ber of extracted rule instances (tokens) against the
number of unique rules (types). The results show
that multiple decomposition leads to a four-fold in-
crease in the number of extracted grammar rules,
even when the length of the Stat-XFER baseline
rules is unbounded. The number of extracted phrase
pairs shows a smaller increase, but this is expected:
the number of possible phrase pairs is proportional
to the square of the sentence length, while the num-
ber of possible hierarchical rules is exponential, so
there is more room for coverage improvement in the
hierarchical grammar.
With virtual nodes included, there is again a large
jump in both the number of extracted rule tokens and
types, even at relatively short length limits. When
both maxp and maxg are set to 7, our rule ex-
tractor produces 1.5 times as many unique phrase
pairs and 20.5 times as many unique hierarchical
rules as the baseline Stat-XFER system, and nearly
twice the number of hierarchical rules as when us-
ing length limits of 5. Ambati et al (2009) showed
the usefulness of extending rule extraction from ex-
act original?original node alignments to cases in
which original?virtual and virtual?original align-
ments were also permitted. Our experiments con-
firm this, as only 60% (full-short) and 54% (full-
long) of our extracted rule types are made up of only
original?original node alignments. Further, we find
a contribution from the new virtual?virtual case: ap-
proximately 8% of the rules extracted in the ?full-
long? configuration from Table 3 are headed by a
virtual?virtual alignment, and a similar number have
a virtual?virtual alignment on their right-hand sides.
All four of the extracted rule sets show Zipfian
distributions over rule frequency counts. In the xfer-
orig, full-short, and full-long configurations, be-
tween 82% and 86% of the extracted phrase pair
rules, and between 88% and 92% of the extracted
hierarchical rules, were observed only once. These
140
Extracted Instances Unique Rules
Rule Set Phrase Hierarchical Phrase Hierarchical
xfer-orig 6,646,791 1,876,384 1,929,641 767,573
compatible 8,709,589 6,657,590 2,016,227 3,590,184
full-short 10,190,487 14,190,066 2,877,650 8,313,690
full-long 10,288,731 22,479,863 2,970,403 15,750,695
Table 3: The number of extracted rule instances (tokens) and unique rules (types) produced by the Stat-XFER system
(?xfer-orig?) and three configurations of our rule extractor.
percentages are remarkably consistent despite sub-
stantial changes in grammar size, meaning that our
more exhaustive method of rule extraction does not
produce a disproportionate number of singletons.3
On the other hand, it does weaken the average count
of an extracted hierarchical rule type. From Table 3,
we can compute that the average phrase pair count
remains at 3.5 when we move from xfer-orig to the
two full configurations; however, the average hier-
archical rule count drops from 2.4 to 1.7 (full-short)
and finally 1.4 (full-long). This likely again reflects
the exponential increase in the number of extractable
hierarchical rules compared to the quadratic increase
in the phrase pairs.
4.2 Translation Results
The grammars obtained from our rule extractor can
be filtered and formatted for use with a variety of
SCFG-based decoders and rule formats. We car-
ried out end-to-end translation experiments with the
various extracted rule sets from the FBIS corpus us-
ing the open-source decoder Joshua (Li et al, 2009).
Given a source-language string, Joshua translates by
producing a synchronous parse of it according to a
scored SCFG and a target-side language model. A
significant engineering challenge in building a real
MT system of this type is selecting a more moderate-
sized subset of all extracted rules to retain in the final
translation model. This is an especially important
consideration when dealing with expanded rule sets
derived from virtual nodes and multiple decomposi-
tions in each input tree.
In our experiments, we pass all grammars through
3The compatible configuration is somewhat of an outlier. It
has proportionally fewer singleton phrase pairs (80%) than the
other variants, likely because it allows multiple alignments and
multiple decompositions without allowing virtual nodes.
two preprocessing steps before any translation
model scoring. First, we noticed that English car-
dinal numbers and punctuation marks in many lan-
guages tend to receive incorrect nonterminal labels
during parsing, despite being closed-class items with
clearly defined tags. Therefore, before rule extrac-
tion, we globally correct the nodel labels of all-
numeral terminals in English and certain punctua-
tion marks in both English and Chinese. Second,
we attempt to reduce derivational ambiguity in cases
where the same SCFG right-hand side appears in
the grammar after extraction with a large number of
possible left-hand-side labels. To this end, we sort
the possible left-hand sides by frequency for each
unique right-hand side, and we remove the least fre-
quent 10 percent of the label distribution.
Our translation model scoring is based on the fea-
ture set of Hanneman et al (2010). This includes
the standard bidirectional conditional maximum-
likelihood scores at both the word and phrase level
on the right-hand side of rules. We also include
maximum-likelihood scores for the left-hand-side
label given all or part of the right-hand side. Using
statistics local to each rule, we set binary indicator
features for rules whose frequencies are ? 3, plus
five additional indicator features according to the
format of the rule?s right-hand side, such as whether
it is fully abstract. Since the system in this paper
is not constructed using any non-syntactic rules, we
do not include the Hanneman et al (2010) ?not la-
belable? maximum-likelihood features or the indica-
tor features related to non-syntactic labels.
Beyond the above preprocessing and scoring
common to all grammars, we experiment with three
different solutions to the more difficult problem of
selecting a final translation grammar. In any case,
we separate phrase pair rules from hierarchical rules
141
Rule Set Filter BLEU TER MET
xfer-orig 10k 24.39 68.01 54.35
xfer-orig 5k+100k 25.95 66.27 54.77
compatible 10k 24.28 65.30 53.58
full-short 10k 25.16 66.25 54.33
full-short 100k 25.51 65.56 54.15
full-short 5k+100k 26.08 64.32 54.58
full-long 10k 25.74 65.52 54.55
full-long 100k 25.53 66.24 53.68
full-long 5k+100k 25.83 64.55 54.35
Table 4: Automatic metric results using different rule
sets, as well as different grammar filtering methods.
and include in the grammar all phrase pair rules
matching a given tuning or testing set. Any im-
provement in phrase pair coverage during the extrac-
tion stage is thus directly passed along to decoding.
For hierarchical rules, we experiment with retain-
ing the 10,000 or 100,000 most frequently extracted
unique rules. We also separate fully abstract hier-
archical rules from partially lexicalized hierarchical
rules, and in a further selection technique we retain
the 5,000 most frequent abstract and 100,000 most
frequent partially lexicalized rules.
Given these final rule sets, we tune our MT sys-
tems on the NIST MT 2006 data set using the min-
imum error-rate training package Z-MERT (Zaidan,
2009), and we test on NIST MT 2003. Both sets
have four reference translations. Table 4 presents
case-insensitive evaluation results on the test set ac-
cording to the automatic metrics BLEU (Papineni et
al., 2002), TER (Snover et al, 2006), and METEOR
(Lavie and Denkowski, 2009).4 The trend in the
results is that including a larger grammar is gener-
ally better for performance, but filtering techniques
also play a substantial role in determining how well
a given grammar will perform at run time.
We first compare the results in Table 4 for dif-
ferent rule sets all filtered the same way at decod-
ing time. With only 10,000 hierarchical rules in use
(?10k?), the improvements in scores indicate that an
important contribution is being made by the addi-
tional phrase pair coverage provided by each suc-
4For METEOR scoring we use version 1.0 of the metric,
tuned to HTER with the exact, stemming, and synonymy mod-
ules enabled.
cessive rule set. The original Stat-XFER rule ex-
traction provides 244,988 phrase pairs that match
the MT 2003 test set. This is already increased to
520,995 in the compatible system using multiple de-
compositions. With virtual nodes enabled, the full
system produces 766,379 matching phrase pairs up
to length 5 or 776,707 up to length 7. These systems
both score significantly higher than the Stat-XFER
baseline according to BLEU and TER, and the ME-
TEOR scores are likely statistically equivalent.
Across all configurations, we find that changing
the grammar filtering technique ? possibly com-
bined with retuned decoder feature weights ? also
has a large influence on automatic metric scores.
Larger hierarchical grammars tend to score better, in
some cases to the point of erasing the score differ-
ences between rule sets. From this we conclude that
making effective use of the extracted grammar, no
matter its size, with intelligent filtering techniques
is at least as important as the number and type of
rules extracted overall. Though the filtering results
in Table 4 are still somewhat inconclusive, the rel-
ative success of the ?5k+100k? setting shows that
filtering fully abstract and partially lexicalized rules
separately is a reasonable starting approach. While
fully abstract rules do tend to be more frequently ob-
served in grammar extraction, and thus more reliably
scored in the translation model, they also have the
ability to overapply at decoding time because their
use is not restricted to any particular lexical context.
5 Conclusions and Future Work
We demonstrated in Section 4.1 that the general
SCFG extraction algorithm described in this paper
is capable of producing very large linguistically mo-
tivated rule sets. These rule sets can improve auto-
matic metric scores at decoding time. At the same
time, we see the results in Section 4.2 as a spring-
board to more advanced and more intelligent meth-
ods of grammar filtering. Our major research ques-
tion for future work is to determine how to make the
best runtime use of the grammars we can extract.
As we saw in Section 2, multiple decompositions
of a single parse tree allow the same constituent to
be built in a variety of ways. This is generally good
for coverage, but its downside at run time is that the
decoder must manage a larger number of competing
142
derivations that, in the end, produce the same output
string. Grammar filtering that explicitly attempts to
limit the derivational ambiguity of the retained rules
may prevent the translation model probabilities of
correct outputs from getting fragmented into redun-
dant derivations. So far we have only approximated
this by using fully abstract rules as a proxy for the
most derivationally ambiguous rules.
Filtering based on the content of virtual nodes
may also be a reasonable strategy for selecting use-
ful grammar rules and discarding those whose con-
tributions are less necessary. For example, we find
in our current output many applications of rules
involving virtual nodes that consist of an open-
class category and a mark of punctuation, such as
VBD+COMMA and NN+PU. While there is noth-
ing technically wrong with these rules, they may not
be as helpful in translation as rules for nouns and
adjectives such as JJ+NNP+NN or NNP+NNP in flat
noun phrase structures such as former U.S. president
Bill Clinton.
A final concern in making use of our large ex-
tracted grammars is the effect virtual nodes have
on the size of the nonterminal set. The Stat-XFER
baseline grammar from our ?xfer-orig? configura-
tion uses a nonterminal set of 1,577 unique labels.
In our rule extractor so far, we have adopted the con-
vention of naming virtual nodes with a concatena-
tion of their component sibling labels, separated by
?+?s. With the large number of virtual node labels
that may be created, this gives our ?full-short? and
?full-long? extracted grammars nonterminal sets of
around 73,000 unique labels. An undesirable conse-
quence of such a large label set is that a particular
SCFG right-hand side may acquire a large variety
of left-hand-side labels, further contributing to the
derivational ambiguity problems discussed above.
In future work, the problem could be addressed by
reconsidering our naming scheme for virtual nodes,
by allowing fuzzy matching of labels at translation
time (Chiang, 2010), or by other techniques aimed
at reducing the size of the overall nonterminal set.
Acknowledgments
This research was supported in part by U.S. National
Science Foundation grants IIS-0713402 and IIS-
0915327 and the DARPA GALE program. We thank
Vamshi Ambati and Jon Clark for helpful discus-
sions regarding implementation details of the gram-
mar extraction algorithm. Thanks to Chris Dyer for
providing the word-aligned and preprocessed FBIS
corpus. Finally, we thank Yahoo! for the use of
the M45 research computing cluster, where we ran
many steps of our experimental pipeline.
References
Vamshi Ambati, Alon Lavie, and Jaime Carbonell. 2009.
Extraction of syntactic translation models from paral-
lel data using syntax from source and target languages.
In Proceedings of the 12th Machine Translation Sum-
mit, pages 190?197, Ottawa, Canada, August.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, MI, June.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
HLT-NAACL 2004: Main Proceedings, pages 273?
280, Boston, MA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
ACL, pages 961?968, Sydney, Australia, July.
Greg Hanneman, Jonathan Clark, and Alon Lavie. 2010.
Improved features and grammar selection for syntax-
based MT. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 82?87, Uppsala, Sweden, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 48?54, Edmonton,
Alberta, May?June.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
143
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Ventsislav Zhechev and Andy Way. 2008. Automatic
generation of parallel treebanks. In Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 1105?1112, Manchester, England, Au-
gust.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY, June.
144
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 85?91,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of
Machine Translation Systems
Michael Denkowski and Alon Lavie
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper describes Meteor 1.3, our submis-
sion to the 2011 EMNLP Workshop on Sta-
tistical Machine Translation automatic evalua-
tion metric tasks. New metric features include
improved text normalization, higher-precision
paraphrase matching, and discrimination be-
tween content and function words. We include
Ranking and Adequacy versions of the metric
shown to have high correlation with human
judgments of translation quality as well as a
more balanced Tuning version shown to out-
perform BLEU in minimum error rate training
for a phrase-based Urdu-English system.
1 Introduction
The Meteor1 metric (Banerjee and Lavie, 2005;
Denkowski and Lavie, 2010b) has been shown to
have high correlation with human judgments in eval-
uations such as the 2010 ACL Workshop on Statisti-
cal Machine Translation and NIST Metrics MATR
(Callison-Burch et al, 2010). However, previous
versions of the metric are still limited by lack of
punctuation handling, noise in paraphrase matching,
and lack of discrimination between word types. We
introduce new resources for all WMT languages in-
cluding text normalizers, filtered paraphrase tables,
and function word lists. We show that the addition of
these resources to Meteor allows tuning versions of
the metric that show higher correlation with human
translation rankings and adequacy scores on unseen
1The metric name has previously been stylized as ?ME-
TEOR? or ?METEOR?. As of version 1.3, the official stylization
is simply ?Meteor?.
test data. The evaluation resources are modular, us-
able with any other evaluation metric or MT soft-
ware.
We also conduct a MT system tuning experiment
on Urdu-English data to compare the effectiveness
of using multiple versions of Meteor in minimum
error rate training. While versions tuned to various
types of human judgments do not perform as well
as the widely used BLEU metric (Papineni et al,
2002), a balanced Tuning version of Meteor consis-
tently outperforms BLEU over multiple end-to-end
tune-test runs on this data set.
The versions of Meteor corresponding to the
translation evaluation task submissions, (Ranking
and Adequacy), are described in Sections 3 through
5 while the submission to the tunable metrics task,
(Tuning), is described in Section 6.
2 New Metric Resources
2.1 Meteor Normalizer
Whereas previous versions of Meteor simply strip
punctuation characters prior to scoring, version 1.3
includes a new text normalizer intended specifi-
cally for translation evaluation. The normalizer first
replicates the behavior of the tokenizer distributed
with the Moses toolkit (Hoang et al, 2007), includ-
ing handling of non-breaking prefixes. After tok-
enization, we add several rules for normalization,
intended to reduce meaning-equivalent punctuation
styles to common forms. The following two rules
are particularly helpful:
? Remove dashes between hyphenated words.
(Example: far-off ? far off)
85
? Remove full stops in acronyms/initials. (Exam-
ple: U.N. ? UN)
Consider the behavior of the Moses tokenizer
and Meteor normalizers given a reference trans-
lation containing the phrase ?U.S.-based
organization?:
Moses: U.S.-based organization
Meteor ?1.2: U S based organization
Meteor 1.3: US based organization
Of these, only the Meteor 1.3 normalization
allows metrics to match all of the following
stylizations:
U.S.-based organization
US-based organization
U.S. based organization
US based organization
While intended for Meteor evaluation, use of this
normalizer is a suitable preprocessing step for other
metrics to improve accuracy when reference sen-
tences are stylistically different from hypotheses.
2.2 Filtered Paraphrase Tables
The original Meteor paraphrase tables (Denkowski
and Lavie, 2010b) are constructed using the phrase
table ?pivoting? technique described by Bannard
and Callison-Burch (2005). Many paraphrases suf-
fer from word accumulation, the appending of un-
aligned words to one or both sides of a phrase rather
than finding a true rewording from elsewhere in par-
allel data. To improve the precision of the para-
phrase tables, we filter out all cases of word accumu-
lation by removing paraphrases where one phrase is
a substring of the other. Table 1 lists the number of
phrase pairs found in each paraphrase table before
and after filtering. In addition to improving accu-
racy, the reduction of phrase table sizes also reduces
the load time and memory usage of the Meteor para-
phrase matcher. The tables are a modular resource
suitable for other MT or NLP software.
2.3 Function Word Lists
Commonly used metrics such as BLEU and ear-
lier versions of Meteor make no distinction between
content and function words. This can be problem-
atic for ranking-based evaluations where two system
Language Phrase Pairs After Filtering
English 6.24M 5.27M
Czech 756K 684K
German 3.52M 3.00M
Spanish 6.35M 5.30M
French 3.38M 2.84M
Table 1: Sizes of paraphrase tables before and after filter-
ing
Language Corpus Size (sents) FW Learned
English 836M 93
Czech 230M 68
French 374M 85
German 309M 92
Spanish 168M 66
Table 2: Monolingual corpus size (words) and number of
function words learned for each language
outputs can differ by a single word, such as mistrans-
lating either a main verb or a determiner. To improve
Meteor?s discriminative power in such cases, we in-
troduce a function word list for each WMT language
and a new ? parameter to adjust the relative weight
given to content words (any word not on the list) ver-
sus function words (see Section 3). Function word
lists are estimated according to relative frequency in
large monolingual corpora. For each language, we
pool freely available WMT 2011 data consisting of
Europarl (Koehn, 2005), news (sentence-uniqued),
and news commentary data. Any word with relative
frequency of 10?3 or greater is added to the func-
tion word list. Table 2 lists corpus size and number
of function words learned for each language. In ad-
dition to common words, punctuation symbols con-
sistently rise to the tops of function word lists.
3 Meteor Scoring
Meteor evaluates translation hypotheses by align-
ing them to reference translations and calculating
sentence-level similarity scores. This section de-
scribes our extended version of the metric.
For a hypothesis-reference pair, the search space
of possible alignments is constructed by identifying
all possible matches between the two sentences ac-
cording to the following matchers:
Exact: Match words if their surface forms are iden-
86
tical.
Stem: Stem words using a language-appropriate
Snowball Stemmer (Porter, 2001) and match if the
stems are identical.
Synonym: Match words if they share member-
ship in any synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database.
Paraphrase: Match phrases if they are listed as
paraphrases in the paraphrase tables described in
Section 2.2.
All matches are generalized to phrase matches
with a start position and phrase length in each sen-
tence. Any word occurring less than length posi-
tions after a match start is considered covered by
the match. The exact and paraphrase matchers sup-
port all five WMT languages while the stem matcher
is limited to English, French, German, and Spanish
and the synonym matcher is limited to English.
Once matches are identified, the final alignment is
resolved as the largest subset of all matches meeting
the following criteria in order of importance:
1. Require each word in each sentence to be cov-
ered by zero or one matches.
2. Maximize the number of covered words across
both sentences.
3. Minimize the number of chunks, where a chunk
is defined as a series of matches that is contigu-
ous and identically ordered in both sentences.
4. Minimize the sum of absolute distances be-
tween match start positions in the two sen-
tences. (Break ties by preferring to align words
and phrases that occur at similar positions in
both sentences.)
Given an alignment, the metric score is calculated
as follows. Content and function words are iden-
tified in the hypothesis (hc, hf ) and reference (rc,
rf ) according to the function word lists described in
Section 2.3. For each of the matchers (mi), count
the number of content and function words covered
by matches of this type in the hypothesis (mi(hc),
mi(hf )) and reference (mi(rc), mi(rf )). Calculate
weighted precision and recall using matcher weights
(wi...wn) and content-function word weight (?):
P =
?
iwi ? (? ?mi(hc) + (1? ?) ?mi(hf ))
? ? |hc|+ (1? ?) ? |hf |
Target WMT09 WMT10 Combined
English 20,357 24,915 45,272
Czech 11,242 9,613 20,855
French 2,967 5,904 7,062
German 6,563 10,892 17,455
Spanish 3,249 3,813 7,062
Table 3: Human ranking judgment data from 2009 and
2010 WMT evaluations
R =
?
iwi ? (? ?mi(rc) + (1? ?) ?mi(rf ))
? ? |rc|+ (1? ?) ? |rf |
The parameterized harmonic mean of P and R (van
Rijsbergen, 1979) is then calculated:
Fmean =
P ?R
? ? P + (1? ?) ?R
To account for gaps and differences in word order,
a fragmentation penalty is calculated using the total
number of matched words (m, average over hypoth-
esis and reference) and number of chunks (ch):
Pen = ? ?
(
ch
m
)?
The Meteor score is then calculated:
Score = (1? Pen) ? Fmean
The parameters ?, ?, ?, ?, and wi...wn are tuned
to maximize correlation with human judgments.
4 Parameter Optimization
4.1 Development Data
The 2009 and 2010 WMT shared evaluation data
sets are made available as development data for
WMT 2011. Data sets include MT system outputs,
reference translations, and human rankings of trans-
lation quality. Table 3 lists the number of judgments
for each evaluation and combined totals.
4.2 Tuning Procedure
To evaluate a metric?s performance on a data set, we
count the number of pairwise translation rankings
preserved when translations are re-ranked by met-
ric score. We then compute Kendall?s ? correlation
coefficient as follows:
? =
concordant pairs?discordant pairs
total pairs
87
Tune ? (WMT09) Test ? (WMT10)
Lang Met1.2 Met1.3 Met1.2 Met1.3
English 0.258 0.276 0.320 0.343
Czech 0.148 0.162 0.220 0.215
French 0.414 0.437 0.370 0.384
German 0.152 0.180 0.170 0.155
Spanish 0.216 0.240 0.310 0.326
Table 5: Meteor 1.2 and 1.3 correlation with ranking
judgments on tune and test data
For each WMT language, we learn Meteor pa-
rameters that maximize ? over the combined 2009
and 2010 data sets using an exhaustive parametric
sweep. The resulting parameters, listed in Table 4,
are used in the default Ranking version of Meteor
1.3.
For each language, the ? parameter is above 0.5,
indicating a preference for content words over func-
tion words. In addition, the fragmentation penalties
are generally less severe across languages. The ad-
ditional features in Meteor 1.3 allow for more bal-
anced parameters that distribute responsibility for
penalizing various types of erroneous translations.
5 Evaluation Experiments
To compare Meteor 1.3 against previous versions of
the metric on the task of evaluating MT system out-
puts, we tune a version for each language on 2009
WMT data and evaluate on 2010 data. This repli-
cates the 2010 WMT shared evaluation task, allow-
ing comparison to Meteor 1.2. Table 5 lists correla-
tion of each metric version with ranking judgments
on tune and test data. Meteor 1.3 shows significantly
higher correlation on both tune and test data for En-
glish, French, and Spanish while Czech and German
demonstrate overfitting with higher correlation on
tune data but lower on test data. This overfitting ef-
fect is likely due to the limited number of systems
providing translations into these languages and the
difficulty of these target languages leading to sig-
nificantly noisier translations skewing the space of
metric scores. We believe that tuning to combined
2009 and 2010 data will counter these issues for the
official Ranking version.
Meteor-1.2 r Meteor-1.3 r
Tune / Test MT08 MT09 MT08 MT09
MT08 0.620 0.625 0.650 0.636
MT09 0.612 0.630 0.642 0.648
Tune / Test P2 P3 P2 P3
P2 -0.640 -0.596 -0.642 -0.594
P3 -0.638 -0.600 -0.625 -0.612
Table 6: Meteor 1.2 and 1.3 correlation with adequacy
and H-TER scores on tune and test data
5.1 Generalization to Other Tasks
To evaluate the impact of new features on other
evaluation tasks, we follow Denkowski and Lavie
(2010a), tuning versions of Meteor to maximize
length-weighted sentence-level Pearson?s r correla-
tion coefficient with adequacy and H-TER (Snover
et al, 2006) scores of translations. Data sets in-
clude 2008 and 2009 NIST Open Machine Trans-
lation Evaluation adequacy data (Przybocki, 2009)
and GALE P2 and P3 H-TER data (Olive, 2005).
For each type of judgment, metric versions are tuned
and tested on each year and scores are compared.
We compare Meteor 1.3 results with those from ver-
sion 1.2 with results shown in Table 6. For both
adequacy data sets, Meteor 1.3 significantly outper-
forms version 1.2 on both tune and test data. The
version tuned on MT09 data is selected as the official
Adequacy version of Meteor 1.3. H-TER versions
either show no improvement or degradation due to
overfitting. Examination of the optimal H-TER pa-
rameter sets reveals a mismatch between evalua-
tion metric and human judgment type. As H-TER
evaluation is ultimately limited by the TER aligner,
there is no distinction between content and function
words, and words sharing stems are considered non-
matches. As such, these features do not help Meteor
improve correlation, but rather act as a source of ad-
ditional possibility for overfitting.
6 MT System Tuning Experiments
The 2011 WMT Tunable Metrics task consists of
using Z-MERT (Zaidan, 2009) to tune a pre-built
Urdu-English Joshua (Li et al, 2009) system to a
new evaluation metric on a tuning set with 4 refer-
ence translations and decoding a test set using the re-
sulting parameter set. As this task does not provide a
88
Language ? ? ? ? wexact wstem wsyn wpar
English 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60
Czech 0.95 0.20 0.60 0.80 1.00 ? ? 0.40
French 0.90 1.40 0.60 0.65 1.00 0.20 ? 0.40
German 0.95 1.00 0.55 0.55 1.00 0.80 ? 0.20
Spanish 0.65 1.30 0.50 0.80 1.00 0.80 ? 0.60
Table 4: Optimal Meteor parameters for WMT target languages on 2009 and 2010 data (Meteor 1.3 Ranking)
devtest set, we select a version of Meteor by explor-
ing the effectiveness of using multiple versions of
the metric to tune phrase-based translation systems
for the same language pair.
We use the 2009 NIST Open Machine Transla-
tion Evaluation Urdu-English parallel data (Przy-
bocki, 2009) plus 900M words of monolingual data
from the English Gigaword corpus (Parker et al,
2009) to build a standard Moses system (Hoang et
al., 2007) as follows. Parallel data is word aligned
using the MGIZA++ toolkit (Gao and Vogel, 2008)
and alignments are symmetrized using the ?grow-
diag-final-and? heuristic. Phrases are extracted us-
ing standard phrase-based heuristics (Koehn et al,
2003) and used to build a translation table and lex-
icalized reordering model. A standard SRI 5-gram
language model (Stolke, 2002) is estimated from
monolingual data. Using Z-MERT, we tune this sys-
tem to baseline metrics as well as the versions of
Meteor discussed in previous sections. We also tune
to a balanced Tuning version of Meteor designed to
minimize bias. This data set provides a single set
of reference translations for MERT. To account for
the variance of MERT, we run end-to-end tuning 3
times for each metric and report the average results
on two unseen test sets: newswire and weblog. Test
set translations are evaluated using BLEU, TER, and
Meteor 1.2. The parameters for each Meteor version
are listed in Table 7 while the results are listed in
Table 8.
The results are fairly consistent across both test
sets: the Tuning version of Meteor outperforms
BLEU across all metrics while versions of Meteor
that perform well on other tasks perform poorly in
tuning. This illustrates the differences between eval-
uation and tuning tasks. In evaluation tasks, metrics
are engineered to score 1-best translations from sys-
tems most often tuned to BLEU. As listed in Table 7,
Newswire
Tuning Metric BLEU TER Met1.2
BLEU 23.67 72.48 50.45
TER 25.35 59.72 48.60
TER-BLEU/2 26.25 61.66 49.69
Meteor-tune 24.89 69.54 51.29
Meteor-rank 19.28 94.64 49.78
Meteor-adq 22.86 77.27 51.40
Meteor-hter 25.23 66.71 50.90
Weblog
Tuning Metric BLEU TER Met1.2
BLEU 17.10 76.28 41.86
TER 17.07 64.32 39.75
TER-BLEU/2 18.14 65.77 40.68
Meteor-tune 18.07 73.83 42.78
Meteor-rank 14.34 98.86 42.75
Meteor-adq 16.76 81.63 43.43
Meteor-hter 18.12 70.47 42.28
Table 8: Average metric scores for Urdu-English systems
tuned to baseline metrics and versions of Meteor
these parameters are often skewed to emphasize the
differences between system outputs. In the tuning
scenario, MERT optimizes translation quality with
respect to the tuning metric. If a metric is biased (for
example, assigning more weight to recall than preci-
sion), it will guide the MERT search toward patho-
logical translations that receive lower scores across
other metrics. Balanced between precision and re-
call, content and function words, and word choice
versus fragmentation, the Tuning version of Meteor
is significantly less susceptible to gaming. Chosen
as the official submission for WMT 2011, we be-
lieve that this Tuning version of Meteor will further
generalize to other tuning scenarios.
89
Task ? ? ? ? wexact wstem wsyn wpar
Ranking 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60
Adequacy 0.75 1.40 0.45 0.70 1.00 1.00 0.60 0.80
H-TER 0.40 1.50 0.35 0.55 1.00 0.20 0.60 0.80
Tuning 0.50 1.00 0.50 0.50 1.00 0.50 0.50 0.50
Table 7: Parameters for Meteor 1.3 tasks
7 Conclusions
We have presented Ranking, Adequacy, and Tun-
ing versions of Meteor 1.3. The Ranking and Ad-
equacy versions are shown to have high correlation
with human judgments except in cases of overfitting
due to skewed tuning data. We believe that these
overfitting issues are lessened when tuning to com-
bined 2009 and 2010 data due to increased variety
in translation characteristics. The Tuning version of
Meteor is shown to outperform BLEU in minimum
error rate training of a phrase-based system on small
Urdu-English data and we believe that it will gener-
alize well to other tuning scenarios. The source code
and all resources for Meteor 1.3 and the version of
Z-MERT with Meteor integration will be available
for download from the Meteor website.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
of ACL WIEEMMTS 2005.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL2005.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 Joint Workshop on Sta-
tistical Machine Translation and Metrics for Machine
Translation. In Proc. of ACL WMT/MetricsMATR
2010.
Michael Denkowski and Alon Lavie. 2010a. Choosing
the Right Evaluation for Machine Translation: an Ex-
amination of Annotator and Automatic Metric Perfor-
mance on Human Judgment Tasks. In Proc. of AMTA
2010.
Michael Denkowski and Alon Lavie. 2010b. METEOR-
NEXT and the METEOR Paraphrase Tables: Improve
Evaluation Support for Five Target Languages. In
Proc. of ACL WMT/MetricsMATR 2010.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Proc. of ACL
WSETQANLP 2008.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and On-
dej Bojar. 2007. Moses: Open Source Toolkit for Sta-
tistical Machine Translation. In Proc. of ACL 2007.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of NAACL/HLT 2003.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of MT Sum-
mit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An Open Source Toolkit for Parsing-based
Machine Translation. In Proc. of WMT 2009.
George Miller and Christiane Fellbaum. 2007. WordNet.
http://wordnet.princeton.edu/.
Joseph Olive. 2005. Global Autonomous Language Ex-
ploitation (GALE). DARPA/IPTO Proposer Informa-
tion Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL 2002.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth Edi-
tion. Linguistic Data Consortium, LDC2009T13.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
Mark Przybocki. 2009. NIST Open
Machine Translation 2009 Evaluation.
http://www.itl.nist.gov/iad/mig/tests/mt/2009/.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA 2006.
Andreas Stolke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002.
C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.
90
Omar F. Zaidan. 2009. Z-MERT: A Fully Configurable
Open Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague Bulletin
of Mathematical Linguistics.
91
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 145?151,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
CMU System Combination in WMT 2011
Kenneth Heafield and Alon Lavie
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA, USA
{heafield,alavie}@cs.cmu.edu
Abstract
This paper describes our submissions,
cmu-heafield-combo, to the ten tracks
of the 2011 Workshop on Machine Transla-
tion?s system combination task. We show how
the combination scheme operates by flexibly
aligning system outputs then searching a
space constructed from the alignments.
Humans judged our combination the best on
eight of ten tracks.
1 Introduction
We participated in all ten tracks of the 2011 Work-
shop on Machine Translation system combination
task as cmu-heafield-combo. This uses a sys-
tem combination scheme that builds on our prior
work (Heafield and Lavie, 2010), especially with
respect to language modeling and handling non-
English languages. We present a summary of
the system, describe improvements, list the data
used (all of the constrained monolingual data), and
present automatic results in anticipation of human
evaluation by the workshop.
2 Our Combination Scheme
Given single-best outputs from each system, the
scheme aligns system outputs then searches a space
based on these alignments. The scheme is a contin-
uation of our previous system (Heafield and Lavie,
2010) so we describe unchanged parts of the sys-
tem in less detail, preferring instead to focus on new
components.
2.1 Alignment
We run the METEOR matcher (Denkowski and
Lavie, 2010) on every pair of system outputs for a
given sentence. It identifies exact matches, identi-
cal stems (Porter, 2001) except for Czech, WordNet
synonym matches for English (Fellbaum, 1998), and
automatically extracted matches for all five target
languages. The automatic matches come from piv-
oting (Bannard and Callison-Burch, 2005) on con-
strained data. An example METEOR alignment is
shown in Figure 1, though it need not be monotone.
Twice that produced by nuclear plants
Double that that produce nuclear power stations
Figure 1: Alignment generated by METEOR showing
exact (that?that and nuclear?nuclear), stem (produced?
produce), synonym (twice?double), and unigram para-
phrase (plants?stations) alignments.
2.2 Search
The search space is unchanged from Heafield and
Lavie (2010), so we give a summary here. The gen-
eral idea is to generate a combined sentence one
word at a time, going from left to right. As the
scheme creates an output, it also steps through the
system outputs from left to right. Stepping through
systems is synchronized with the partial output, so
that words to the left are already captured in the hy-
pothesis and the next word from any of the systems
represents a meaningful extension of the partial out-
put. All of these options are considered by hypothe-
sis branching.
145
Thus far, we have assumed that system outputs are
monotone: they agree on word order, so it is possi-
ble to step through all of them simultaneously. On
the left are words captured in the partial output and
on the right are the words whose meaning remains
to be captured in the output. When systems disagree
on word order, the partial output corresponds to dis-
joint pieces of a system?s output. We still retain that
notion that a word is either captured in the partial
output or not captured, but do not have a single di-
viding line between them. In this case, we still pro-
ceed from left to right, considering the first uncap-
tured word for extension. Then, we skip over parts
of a system?s output that have already been captured.
Here, we have used the informal notion of words
whose meaning is ?captured? or ?uncaptured? by the
partial output. The system interprets words aligned
to the partial output as captured while those not
aligned to the hypothesis are considered uncaptured.
A heuristic also cleans up excess words in order
to keep the stepping process loosely synchronized
across system outputs.
2.3 Features
We use three feature categories to guide search:
Length The length of the hypothesis in tokens.
Language Model Log probability and OOV count
from an N -gram language model. Details are
in Section 4.1.
Match Counts Counts of n-gram matches between
systems outputs and the hypothesis.
The match count features report n-gram matches
between each system and the hypothesis. Specifi-
cally, feature ms,n reports n-gram overlap between
the hypothesis and system s. We track n-gram
counts up to length N , typically 2 or 3, finding that
tracking longer lengths adds little. An example is
shown in Figure 2.
These match counts may be exact, in which case
every word of the n-gram must be the same (up
to case) or approximate, in which case any aligned
word found by METEOR may be substituted. Be-
cause exact matches handle lexical choice and in-
exact matches collect more votes that better handle
word order, we use both sets of features. However,
the limit N may be different i.e. Ne = 2 counts
exact matches up to length 2 and Na = 3 counts
inexact matches up to length 3.
System 1: Supported Proposal of France
System 2: Support for the Proposal of France
Candidate: Support for Proposal of France
Unigram Bigram Trigram
System 1 4 2 1
System 2 5 3 1
Figure 2: Example match feature values with two systems
and matches up to length three. Here, ?Supported? counts
because it aligns with ?Support?.
3 Related Work
Hypothesis selection (Hildebrand and Vogel, 2009)
selects an entire sentence at a time instead of picking
and merging words. This makes the approach less
flexible, in that it cannot synthesize new sentences,
but also less risky by avoiding matching and related
problems entirely.
While our alignment is based on METEOR, other
techniques are based on TER (Snover et al, 2006),
Inversion Transduction Grammars (Narsale, 2010),
and other alignment methods. These use exact
alignments and positional information to infer align-
ments, ignoring the content-based method used by
METEOR. This means they might align content
words to function words, while we never do. In prac-
tice, using both signals would likely work better.
Confusion networks (Rosti et al, 2010; Narsale,
2010) are the dominant method for system combi-
nation. These base their word order on one system,
dubbed the backbone, and have all systems vote on
editing the backbone. Word order is largely fixed to
that of one system; by contrast, ours can piece to-
gether word orders taken from multiple systems. In
a loose sense, our approach is a confusion network
where the backbone is permitted to switch after each
word.
Interestingly, BBN (Rosti et al, 2010) this year
added a novel-bigram penalty that penalizes bigrams
in the output if they do not appear in one of the sys-
146
tem outputs. This is the complement of our bigram
match count features (and, since, we have a length
feature, the same up to rearranging weights). How-
ever, they threshold it to indicate whether the bigram
appears at all instead of how many systems support
the bigram.
4 Resources
The resources we use are constrained to those pro-
vided for the shared task.
For the paraphrase matches described in Sec-
tion 2.1, METEOR (Denkowski and Lavie, 2010)
trains its paraphrase tables via pivoting (Bannard
and Callison-Burch, 2005). The phrase tables are
trained using parallel data from Europarl v6 (Koehn,
2005) (fr-en, es-en, de-en, and es-de), news com-
mentary (fr-en, es-en, de-en, and cz-en), United Na-
tions (fr-en and es-en), and CzEng (cz-en) (Bojar
and Z?abokrtsky?, 2009) sections 0?8.
4.1 Language Modeling
As with previous versions of the system, we use
language model log probability as a feature to bias
translations towards fluency. We add a second fea-
ture per language model that counts OOVs, allow-
ing MERT to independently tune the OOV penalty.
Language models often have poor OOV estimates
for translation because they come not from new text
in the same language but from new text in a differ-
ent language. The distribution is even more biased
in system combination, where most systems have al-
ready applied a language model. The new OOV fea-
ture replaces a previous feature that reported the av-
erage n-gram length matched by the model.
We added support for multiple language mod-
els so that their probabilities, OOV penalties, and
all other features are dynamically interpolated using
MERT. This we use for the Haitian Creole-English
tasks, where the first language model is a large
model built on the monolingual data except SMS
messages and the second small language model is
built on the SMS messages. The OOV features play
an important role here because frequent anonymiza-
tion markers such as ?[firstname]? do not appear in
the large language model.
To scale to larger language models, we use
BigFatLM1, an open-source builder of large un-
pruned models with modified Kneser-Ney smooth-
ing. Then, we filter the models to the system out-
puts. In order for an n-gram to be queried, all of the
words must appear in system outputs for the same
sentence. This enables a filtering constraint stronger
than normal vocabulary filtering, which permits n-
grams supported only by words in different sen-
tences. Finally, we use KenLM (Heafield, 2011) for
inference at runtime.
Our primary use of data is for language model-
ing. We used essientially every constrained resource
available and appended them together to build one
large model. For every language, we used the pro-
vided Europarl v6 (Koehn, 2005), News Crawl, and
News Commentary corpora. In addition, we used:
English Gigaword Fourth Edition (Parker et al,
2009) and the English parts of United Na-
tions documents, Giga-FrEn, and CzEng (Bojar
and Z?abokrtsky?, 2009) sections 0?7. For the
Haitian Creole-English tasks, we built a sepa-
rate language model on the SMS messages and
used it alongside the large English model.
Czech CzEng (Bojar and Z?abokrtsky?, 2009) sec-
tions 0?7
French Gigaword Second Edition (Mendonc?a et
al., 2009a) and the French parts of Giga-FrEn
and United Nations documents.
German There were no additional corpora avail-
able.
Spanish Gigaword Second Edition (Mendonc?a et
al., 2009b) and the Spanish parts of United Na-
tions documents.
4.2 Preprocessing
Many corpora contained excessive duplicate text.
We wrote a deduplicator that removes all but the
first instance of each line. Clean corpora generally
reduced line count by 10-25% when deduplicated,
resulting from naturally-occuring duplicates such as
?yes .? We left the duplicate lines in these corpora.
The News Crawl corpus showed a 72.6% reduction
in line count due mainly to boilerplace, such as the
1https://github.com/jhclark/bigfatlm
147
Reuters comment section header and Fark headlines
that appear in a box on many pages. We dedupli-
cated the News Crawl corpus, United Nations docu-
ments, and New York Times and LA Times portions
of English Gigaword.
The Giga-FrEn corpus is noisy. We removed lines
from Giga-FrEn if any of the following conditions
held:
? Invalid UTF8 or control characters.
? Less than 90% of characters are in the Latin
alphabet (including diacritics) or punctuation.
We did not count ?<? and ?>? as punctuation
to limit the amount of HTML code.
? Less than half the characters are Latin letters.
System outputs and language model training data
were normalized using the provided punctuation
normalization script, Unicode codepoint collaps-
ing, the provided Moses (Koehn et al, 2007) to-
kenizer, and several custom rules. These remove
formatting-related tokens from Gigaword, rejoin
some French words with internal apostrophes, and
threshold repetitive punctuation. In addition, Ger-
man words were segmented as explained in Section
4.3. Text normalization is more difficult for system
combination because the system outputs, while theo-
retically detokenized, contain errors that result from
different preprocessing at each site.
4.3 German Segmentation
German makes extensive use of compounding, cre-
ating words that do not cleanly align to English and
have less reliable statistics. German-English trans-
lation systems therefore typically segment German
compounds as a preprocessing step. In our case,
we are concerned with combining translations into
German that may be segmented differently. These
can be due to stylistic choices; for example both
?jahrzehnte lang? and ?jahrzehntelang? appear with
approximately equal frequency as shown in Table 1.
Translation systems add additional biases due to the
various preprocessing approaches taken by individ-
ual sites and inherent biases in models such as word
alignment.
In order to properly align differently segmented
words, we normalize by segmenting all system out-
puts and our language model training data using
Words Separate Compounded
jahrzehnte lang 554 542
klar gemacht 840 802
unter anderem 49538 4
wieder herzustellen 513 1532
Table 1: Counts of separate or compounded versions of
select words in the lowercased German monolingual data.
Compounding can be optional or biased in either way.
the single-best segmentation from cdec (Dyer et
al., 2010). Running our system therefore produces
segmented German output. Internally, we tuned
towards segmented references but for final output
it is desirable to rejoin compound words. Since
the cdec segmentation was designed for German-
English translation, no corresponding desegmenter
was provided.
We created a German desegmenter in the natural
way: segment German words then invert the map-
ping to identify words that should be rejoined. To do
so, we ran every word from the German monolingual
data and system outputs through the cdec segmenter,
counted both the compounded and segmented ver-
sions in the monolingual data, and removed those
that appear segmented more often. Desegmenting is
a mildly ambiguous process because n-grams to re-
join may overlap. When an n-gram compounded to
one word, we gave that a score of n2. The total score
is a sum of these squares, favoring compounds that
cover more words. Maximizing the score is a fast
and exact dynamic programming algorithm. Casing
of unchanged words comes from equally-weighted
system votes at the character level while casing of
rejoined words is based on the majority appearance
in the corpus; this is almost always initial capital.
We ran our desegmenter followed by the workshop?s
provided detokenizer to produce the submitted out-
put.
5 Results
We tried many variations on the scheme, such as se-
lecting different systems, tuning to BLEU (Papineni
et al, 2002) or METEOR (Denkowski and Lavie,
2010), and changing the structure of the match count
features from Section 2.3. To try these, we ran
MERT 242 times, or about 24 times for each of the
ten tasks in which we participated. Then we selected
148
the best performing systems on the tuning set and
submitted them, with the secondary system chosen
to meaningfully differ from the primary while still
scoring well. Once the evaluation released refer-
ences, we scored against them to generate Table 2.
On the featured Haitian Creole task, we show no
and sometimes even negative improvement. This we
attribute to the gap between the top system, bm-i2r,
and the second place system. For htraw-en, where
training data is noisy, the bm-i2r is 3.65 BLEU
higher than the second place system at 28.53 BLEU.
On htclean-en, the gap is 4.44 points to the second
place cmu-denkowski-contrastive.
The main tasks were quite competitive and many
systems were within a BLEU point of the top. This
is an ideal scenario for system combination, and we
show corresponding improvements. The English-
Czech task is difficult for our scheme because we do
not properly handle Czech morpology in alignment.
On Czech-English, online-B beat other systems by
a substantial (6.21 BLEU) margin, so we see little
gain. On English-German, the gain is small but this
is consistent with a general observation that more
improvement is seen on higher-quality systems. Fur-
ther, strength in this year?s submission comes from
language modeling, but only limited German data
was available; segmenting German improved our
scores. Translations into Spanish and French show
the impact of Gigaword in those languages.
The evaluation?s official metric is human rank-
ing judgments. On this metric, our submissions
score highest on eight of ten tracks: Czech-English,
German-English, English-Czech, English-German,
English-Spanish, English-French, the clean Haitian
Creole-English task, and the raw Haitian Creole-
English task. For Spanish-English, humans pre-
ferred RWTH?s submission. For French-English,
humans preferred RWTH and BBN. However, sys-
tem combinations were ranked against other system
combinations, but not against underlying systems,
so we suspect that the bm-i2r submission still per-
forms better than combinations on the Haitian Cre-
ole tasks. The human judges also preferred our
translations more than BLEU (where we lead on
three language pairs: English to German, Span-
ish, and French). We attribute this to the tendency
of confusion networks to drop words supported by
many systems due to position-based alignment er-
Track Entry BLEU TER MET
htraw-en
primary 32.30 56.57 61.05
contrast 31.76 56.69 60.81
bm-i2r 32.18 57.01 60.85
htclean-en
primary 36.39 51.16 63.72
contrast 36.49 51.15 63.78
bm-i2r 36.97 51.06 64.01
cz-en
primary 29.85 53.20 62.50
contrast 29.88 53.19 62.40
online-B 29.59 52.15 61.77
de-en
primary 26.21 56.19 60.56
contrast 26.11 56.42 60.54
online-B 24.30 57.95 59.63
es-en
primary 33.90 48.88 65.72
contrast 33.47 49.41 66.41
online-A 30.26 51.56 63.83
fr-en
primary 32.41 48.93 65.72
contrast 32.15 49.12 65.71
kit 30.36 50.74 64.32
en-cz
primary 20.80 61.17 41.68
contrast 20.74 61.29 41.69
online-B 20.37 61.38 41.40
en-de
primary 18.45 64.15 22.91
contrast 18.27 64.48 22.75
online-B 17.92 64.01 22.95
en-es
primary 36.47 47.08 34.96
contrast 35.82 47.52 34.64
online-B 33.85 50.09 33.96
en-fr
primary 36.42 48.28 24.29
contrast 36.31 48.56 24.12
online-B 35.34 48.68 23.53
Table 2: Automatic scores for our submissions. For com-
parison, the top individual system by BLEU is shown
in the third row of each track. Test data and references
were preprocessed prior to scoring. Metrics are uncased
and METEOR 1.0 uses adequacy-fluency parameters. We
show improvement on all tasks except Haitian Creole-
English.
149
rors; our content-based alignment method avoids
many of these errors. BLEU penalizes the missing
word the same as missing punctuation while human
judges will penalize heavily for missing content. For
full results, we refer to the simultaneously published
Workshop on Machine Translation findings paper.
6 Conclusion
We participated in the all ten tracks of the sys-
tem combination, prioritizing participation and lan-
guage support over optimizing for one particular
language pair. Nonetheless, we show improvement
on several tasks, including wins by BLEU on three
tracks. The Haitian Creole and Czech-English tasks
proved challenging due to the gap between top sys-
tems. However, other tracks show a variety of
high-performing systems that make our scheme per-
form well. Unlike most other system combination
schemes, our code is open source2 so that these re-
sults may be replicated and brought to bear on simi-
lar problems.
Acknowledgements
Jon Clark assisted with language model construction
and wrote BigFatLM. This material is based upon
work supported by the National Science Founda-
tion Graduate Research Fellowship under Grant No.
0750271 and by the DARPA GALE program.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings ACL.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng 0.9,
building a large Czech-English automatic parallel tree-
bank. The Prague Bulletin of Mathematical Linguis-
tics, (92):63?83.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evalua-
tion support for five target languages. In Proceed-
ings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 339?342,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
2http://kheafield.com/code/mt
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A decoder, alignment, and learning framework for
finite-state and context-free translation models. In
Proceedings of the ACL 2010 System Demonstrations,
ACLDemos ?10, pages 7?12.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Kenneth Heafield and Alon Lavie. 2010. CMU multi-
engine machine translation for WMT 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR. Association for
Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Almut Silja Hildebrand and Stephan Vogel. 2009. CMU
system combination for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 47?50, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
A?ngelo Mendonc?a, David Graff, and Denise DiPer-
sio. 2009a. French gigaword second edition.
LDC2009T28.
A?ngelo Mendonc?a, David Graff, and Denise DiPer-
sio. 2009b. Spanish gigaword second edition.
LDC2009T21.
Sushant Narsale. 2010. JHU system combination
scheme for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 311?314, Uppsala, Sweden,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English gigaword fourth edi-
tion. LDC2009T13.
150
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2010. BBN system description for
wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 321?326, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), pages 223?231, Cambridge, MA, Au-
gust.
151
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 365?371,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
CMU Syntax-Based Machine Translation at WMT 2011
Greg Hanneman and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, alavie}@cs.cmu.edu
Abstract
We present the Carnegie Mellon University
Stat-XFER group submission to the WMT
2011 shared translation task. We built a hy-
brid syntactic MT system for French?English
using the Joshua decoder and an automati-
cally acquired SCFG. New work for this year
includes training data selection and grammar
filtering. Expanded training data selection
significantly increased translation scores and
lowered OOV rates, while results on grammar
filtering were mixed.
1 Introduction
During the past year, the statistical transfer ma-
chine translation group at Carnegie Mellon Univer-
sity has continued its work on large-scale syntactic
MT systems based on automatically acquired syn-
chronous context-free grammars (SCFGs). For the
2011 Workshop on Machine Translation, we built
a hybrid MT system, including both syntactic and
non-syntactic rules, and submitted it as a constrained
entry to the French?English translation task. This
is our fourth yearly submission to the WMT shared
translation task.
In design and construction, the system is sim-
ilar to our submission from last year?s workshop
(Hanneman et al, 2010), with changes in the meth-
ods we employed for training data selection and
SCFG filtering. Continuing WMT?s general trend,
we worked with more data than in previous years,
basing our 2011 system on 13.9 million sentences
of parallel French?English training data and an En-
glish language model of 1.8 billion words. Decod-
ing was carried out in Joshua (Li et al, 2009), an
open-source framework for parsing-based MT. We
managed our experiments with LoonyBin (Clark and
Lavie, 2010), an open-source tool for defining, mod-
ifying, and running complex experimental pipelines.
We describe our system-building process in more
detail in Section 2. In Section 3, we evaluate the sys-
tem?s performance on WMT development sets and
examine the aftermath of training data selection and
grammar filtering. Section 4 concludes with possi-
ble directions for future work.
2 System Construction
2.1 Training Data Selection
WMT 2011?s provided French?English training data
consisted of 36.8 million sentence pairs from the Eu-
roparl, news commentary, UN documents, and Giga-
FrEn corpora (Table 1). The first three of these are,
for the most part, clean data resources that have been
successfully employed as MT corpora for a number
of years. The Giga-FrEn corpus, though the largest,
is also the least precise, as its Web-crawled data
sources are less homogeneous and less structured
than the other corpora. Nevertheless, Pino et al
(2010) found significant improvements in French?
English MT output quality by including it. Our goal
for this year was to strike a middle ground: to avoid
computational difficulties in using the entire 36.8
million sentence pairs of training data, but to mine
the Giga-FrEn corpus for sentences to increase our
system?s vocabulary coverage.
Our method of training data selection proceeded
as follows. We first tokenized all the parallel training
365
Corpus Released Used
Europarl 1,825,077 1,614,111
News commentary 115,562 95,138
UN documents 12,317,600 9,352,232
Giga-FrEn 22,520,400 2,839,466
Total 36,778,639 13,900,947
Table 1: Total number of training sentence pairs released,
by corpus, and the number used in building our system.
data using the Stanford parser?s tokenizer (Klein and
Manning, 2003) for English and our own in-house
script for French. We then passed the Europarl, news
commentary, and UN data through a filtering script
that removed lines longer than 95 tokens in either
language, empty lines, lines with excessively imbal-
anced length ratios, and lines containing tokens of
more than 25 characters in either language. From
the filtered data, we computed a list of the source-
side vocabulary words along with their frequency
counts. Next, we searched the Giga-FrEn corpus for
relatively short lines on the source side (up to 50 to-
kens long) that contained either a new vocabulary
word or a word that had been previously seen fewer
than 20 times. Such lines were added to the filtered
training data to make up our system?s final parallel
training corpus.
The number of sentences retained from each data
source is listed in Table 1; in the end, we trained our
system from 13.9 million parallel sentences. With
the Giga-FrEn data included, the source side of our
parallel corpus had a vocabulary of just over 1.9
million unique words, compared with a coverage of
545,000 words without using Giga-FrEn.
We made the decision to leave the training data in
mixed case for our entire system-building process.
At the cost of slightly sparser estimates for word
alignments and translation probabilities, a mixed-
case system avoids the extra step of building a sta-
tistical recaser to treat our system?s output.
2.2 Grammar Extraction and Scoring
Once we had assembled the final training corpus,
we annotated it with statistical word alignments and
constituent parse trees on both sides. Unidirec-
tional word alignments were provided by MGIZA++
(Gao and Vogel, 2008), then symmetrized with the
grow-diag-final-and heuristic (Koehn et al, 2005).
For generating parse trees, we used the French and
English grammars of the Berkeley statistical parser
(Petrov and Klein, 2007).
Except for minor bug fixes, our method for ex-
tracting and scoring a translation grammar remains
the same as in our WMT 2010 submission. We ex-
tracted both syntactic and non-syntactic portions of
the translation grammar. The non-syntactic gram-
mar was extracted from the parallel corpus and
word alignments following the standard heuristics
of phrase-based SMT (Koehn et al, 2003). The
syntactic grammar was produced using the method
of Lavie et al (2008), which decomposes each pair
of word-aligned parse trees into a series of minimal
SCFG rules. The word alignments are first gener-
alized to node alignments, where nodes s and t are
aligned between the source and target parse trees if
all word alignments in the yield of s land within
the yield of t and vice versa. Minimal SCFG rules
are derived from adjacent levels of node alignments:
the labels from each pair of aligned nodes forms a
rule?s left-hand side, and the right-hand side is made
up of the labels from the frontier of aligned nodes
encountered when walking the left-hand side?s sub-
trees. Within a phrase length limit, each aligned
node pair generate an all-terminal phrase pair rule
as well.
Since both grammars are extracted from the same
Viterbi word alignments using similar alignment
consistency constraints, the phrase pair rules from
the syntactic grammar make up a subset of the rules
extracted according to phrase-based SMT heuristics.
We thus share instance counts between identical
phrases extracted in both grammars, then delete the
non-syntactic versions. Remaining non-syntactic
phrase pairs are converted to SCFG rules, with the
phrase pair forming the right-hand side and the
dummy label PHR::PHR as the left-hand side. Ex-
cept for the dummy label, all nonterminals in the fi-
nal SCFG are made up of a syntactic category label
from French joined with a syntactic category label
from English, as extracted in the syntactic grammar.
A sampling of extracted SCFG rules is shown in Fig-
ure 1.
The combined grammar was scored according to
the 22 translation model features we used last year.
For a generic SCFG rule of the form ?s :: ?t ?
366
PHR :: PHR ? [, ainsi qu?] :: [as well as]
V :: VBN ? [modifie?es] :: [modified]
NP :: NP ? [les conflits arme?s] :: [armed conflict]
AP :: SBAR ? [tel qu? VPpart1] :: [as VP1]
NP :: NP ? [D1 N2 A3] :: [CD1 JJ3 NNS2]
Figure 1: Sample extracted SCFG rules. They include
non-syntactic phrase pairs, single-word and multi-word
syntactic phrase pairs, partially lexicalized hierarchical
rules, and fully abstract hierarchical rules.
[rs ] :: [rt ], we computed 11 maximum-likelihood
features as follows:
? Phrase translation scores P (rs | rt) and
P (rt | rs) for phrase pair rules, using the larger
non-syntactic instance counts for rules that
were also extracted syntactically.
? Hierarchical translation scores P (rs | rt) and
P (rt | rs) for syntactic rules with nonterminals
on the right-hand side.
? Labeling scores P (?s :: ?t | rs), P (?s :: ?t | rt),
and P (?s :: ?t | rs, rt) for syntactic rules.
? ?Not syntactically labelable? scores P (?s ::
?t = PHR :: PHR | rs) and P (?s :: ?t =
PHR :: PHR | rt), with additive smoothing
(n = 1), for all rules.
? Bidirectional lexical scores for all rules with
lexical items, calculated from a unigram lexi-
con over Viterbi-aligned word pairs as in the
Moses decoder (Koehn et al, 2007).
We also included the following 10 binary indicator
features using statistics local to each rule:
? Three low-count features that equal 1 when the
extracted frequency of the rule is exactly equal
to 1, 2, or 3.
? A syntactic feature that equals 1 when the rule?s
label is syntactic, and a corresponding non-
syntactic feature that equals 1 when the rule?s
label is PHR::PHR.
? Five rule format features that equal 1 when the
rule?s right-hand side has a certain composi-
tion. If as and at are true when the source and
target sides contain only nonterminals, respec-
tively, our rule format features are equal to as,
at, as ? a?t, a?s ? at, and a?s ? a?t.
Finally, our model includes a glue rule indicator fea-
ture that equals 1 when the rule is a generic glue
rule. In the Joshua decoder, glue rules monotoni-
cally stitch together adjacent parsed translation frag-
ments at no model cost.
2.3 Language Modeling
This year, our constrained-track system made use of
part of the English Gigaword data, along with other
provided text, in its target-side language model.
From among the data released directly for WMT
2011, we used the English side of the Europarl,
news commentary, French?English UN document,
and English monolingual news corpora. From the
English Gigaword corpus, we included the entire
Xinhua portion and the most recent 13 million sen-
tences of the AP Wire portion. Some of these cor-
pora contain many lines that are repeated a dispro-
portionate number of times ? the monolingual news
corpus in particular, when filtered to only one oc-
currence of each sentence, reaches only 27% of its
original line count. As part of preparing our lan-
guage modeling data, we deduplicated both the En-
glish news and the UN documents, the corpora with
the highest percentages of repeated sentences. We
also removed lines containing more than 750 char-
acters (about 125 average English words) before to-
kenization.
The final prepared corpus was made up of approx-
imately 1.8 billion words of running text. We built
a 5-gram language model from it with the SRI lan-
guage modeling toolkit (Stolcke, 2002). To match
the treatment given to the training data, the language
model was also built in mixed case.
2.4 Grammar Filtering for Decoding
As is to be expected from a training corpus of 13.9
million sentence pairs, the grammars we extract ac-
cording to the procedure of Section 2.2 are quite
large: approximately 2.53 billion non-syntactic and
440 million syntactic rule instances, for a combined
grammar of 1.26 billion unique rules. In preparation
for tuning or decoding, we are faced with the engi-
neering challenge of selecting a subset of the gram-
367
mar that contains useful rules and fits in a reasonable
amount of memory.
Before even extracting a syntactic grammar, we
passed the automatically generated parse trees on the
training corpus through a small tag-correction script
as a pre-step. In previous experimentation, we no-
ticed that a surprising proportion of cardinal num-
bers in English had been tagged with labels other
than CD, their correct tag. We also found errors in
labeling marks of punctuation in both English and
French, when again the canonical labels are unam-
biguous. To fix these errors, we forcibly overwrote
the labels of English tokens made up of only digits
with CD, and we overwrote the labels of 25 English
and 24 French marks of punctuation or other sym-
bols with the appropriate tag as defined by the rele-
vant treebank tagging guidelines.
After grammar extraction and combination of
syntactic and non-syntactic rules, we ran an addi-
tional filtering step to reduce derivational ambiguity
in the case where the same SCFG right-hand side ap-
peared with more than one left-hand-side label. For
each right-hand side, we sorted its possible labels by
extracted frequency, then threw out the labels in the
bottom 10% of the left-hand-side distribution.
Finally, we ran a main grammar filtering step prior
to tuning or decoding, experimenting with two dif-
ferent filtering methods. In both cases, the phrase
pair rules in the grammar were split off and filtered
so that only those whose source sides completely
matched the tuning or test set were retained.
The first, more naive grammar filtering method
sorted all hierarchical rules by extracted frequency,
then retained the most frequent 10,000 rules to join
all matching phrase pair rules in the final translation
grammar. This is similar to the basic grammar filter-
ing we performed for our WMT 2010 submission.
It is based on the rationale that the most frequently
extracted rules in the parallel training data are likely
to be the most reliably estimated and also frequently
used in translating a new data set. However, it also
passes through a disproportionate number of fully
abstract rules ? that is, rules whose right-hand sides
are made up entirely of nonterminals ? which can
apply more recklessly on the test set because they
are not lexically grounded.
Our second, more advanced method of filtering
made two improvements over the naive approach.
First, it controlled for the imbalance of hierarchi-
cal rules by splitting the grammar?s partially lexical-
ized rules into a separate group that can be filtered
independently. Second, it applied a lexical-match
filter such that a partially lexicalized rule was re-
tained only if all its lexicalized source phrases up
to bigrams matched the intended tuning or testing
set. The final translation grammar in this case was
made up of three parts: all phrase pair rules match-
ing the test set (as before), the 100,000 most fre-
quently extracted partially lexicalized rules whose
bigrams match the test set, and the 2000 most fre-
quently extracted fully abstract rules.
3 Experimental Results and Analysis
We tuned each system variant on the newstest2008
data set, using the Z-MERT package (Zaidan, 2009)
for minimum error-rate training to the BLEU metric.
We ran development tests on the newstest2009 and
newstest2010 data sets; Table 2 reports the results
obtained according to various automatic metrics.
The evaluation consists of case-insensitive scoring
according to METEOR 1.0 (Lavie and Denkowski,
2009) tuned to HTER with the exact, stemming,
and synonymy modules enabled, case-insensitive
BLEU (Papineni et al, 2002) as implemented by
the NIST mteval-v13 script, and case-insensitive
TER 0.7.25 (Snover et al, 2006).
Table 2 gives comparative results for two major
systems: one based on our WMT 2011 data selec-
tion as outlined in Section 2.1, and one based on
the smaller WMT 2010 training data that we used
last year (8.6 million sentence pairs). Each system
was run with the two grammar filtering variants de-
scribed in Section 2.4: the 10,000 most frequently
extracted hierarchical rules of any type (?10k?), and
a combination of the 2000 most frequently extracted
abstract rules and the 100,000 most frequently ex-
tracted partially lexicalized rules that matched the
test set (?2k+100k?). Our primary submission to the
WMT 2011 shared task was the fourth line of Ta-
ble 2 (?WMT 2011 2k+100k?); we also made a con-
strastive submission with the system from the sec-
ond line (?WMT 2010 2k+100k?).
Using part of the Giga-FrEn data ? along with
the additions to the Europarl, news commentary,
and UN document courses released since last year
368
newstest2009 newstest2010
System METEOR BLEU TER METEOR BLEU TER
WMT 2010 10k 54.94 24.77 56.53 56.66 25.78 55.06
WMT 2010 2k+100k 55.16 24.88 56.19 56.89 26.05 54.66
WMT 2011 10k 55.82 26.02 54.77 58.13 27.71 52.96
WMT 2011 2k+100k 55.77 26.01 54.70 57.88 27.38 53.04
Table 2: Development test results for systems based on WMT 2010 data (without the Giga-FrEn corpus) and WMT
2011 data (with some Giga-FrEn). The fourth line is our primary shared-task submission.
Applications 10k 2k+100k
Unique rules 1,305 1,994
Rule instances 14,539 12,130
Table 3: Summary of 2011 system syntactic rule applica-
tions on both test sets.
? is beneficial to translation quality, as there is
a clear improvement in metric scores between the
2010 and 2011 systems. Our BLEU score improve-
ments of 1.2 to 1.9 points are statistically significant
according to the paired bootstrap resampling method
(Koehn, 2004) with n = 1000 and p < 0.01. They
are also larger than the 0.7- to 1.1-point gains re-
ported by Pino et al (2010) when the full Giga-FrEn
was added. The 2011 system also shows a signifi-
cant reduction in the out-of-vocabulary (OOV) rate
on both test sets: 38% and 47% fewer OOV types,
and 44% and 45% fewer OOV tokens, when com-
pared to the 2010 system.
Differences between grammar filtering tech-
niques, on the other hand, are much less signifi-
cant according to all three metrics. Under paired
bootstrap resampling on the newstest2009 set, the
grammar variants in both the 2010 and 2011 systems
are statistically equivalent according to BLEU score.
On newstest2010, the 2k+100k grammar improves
over the 10k version (p < 0.01) in the 2010 system,
but the situation is reversed in the 2011 system.
We investigated differences in grammar use with
an analysis of rule applications in the two variants
of the 2011 system, the results of which are summa-
rized in Table 3. Though the configuration with the
2k+100k grammar does apply syntactic rules 20%
more frequently than its 10k counterpart, the 10k
system uses overall 53% more unique rules. One
contributing factor to this situation could be that the
fully abtract rule cutoff is set too low compared to
the increase in partially lexicalized rules. The ef-
fect of the 2k+100k filtering is to reduce the number
of abstract rules from 4000 to 2000 while increas-
ing the number of partially lexicalized rules from
6000 to 100,000. However, we find that the 10k
system makes heavy use of some short, meaningful
abstract rules that were excluded from the 2k+100k
system. The 2k+100k grammar, by contrast, in-
cludes a long tail of less frequently used partially
lexicalized grammar rules.
In practice, there is a balance between the use
of syntactic and non-syntactic grammar rules dur-
ing decoding. We highlight an example of how
both types of rules work together in Figure 2, which
shows our primary system?s translation of part of
newstest2009 sentence 2271. The French source
text is given in italics and segmented into phrases.
The SCFG rules used in translation are shown
above each phrase, where numerical superscripts on
the nonterminal labels indicate those constituents?
relative ordering in the original French sentence.
(Monotonic glue rules are not shown.) While non-
syntactic rules can be used for short-distance re-
ordering and fixed phrases, such as te?le?phones mo-
biles ? mobile phones, the model prefers syntac-
tic translations for more complicated patterns, such
as the head?children reversal in appareils musicaux
portables ? portable music devices.
4 Conclusions and Future Work
Compared to last year, the two main differences in
our current WMT submission are: (1) a new train-
ing data selection strategy aimed at increasing sys-
tem vocabulary without hugely increasing corpus
size, and (2) a new method of grammar filtering that
emphasizes partially lexicalized rules over fully ab-
369
PHR::PHR
young people who
PHR::PHR
frequently use
NP::NP
N::NNS1
devices
A::NN2
music
A::JJ3
portable
PHR::PHR
and mobile phones
jeunes qui utilisent fre?quemment des appareils musicaux portables et des te?le?phones mobiles
PHR::PHR
at full
N::NN
volume
,::,
,
V::MD
can
VPpart::VP
NP::NP3
N::NN2
hearing
D::PRP$1
their
V::VBG1
damaging
ADV::RB2
unknowingly
a` plein volume , puissent endommager inconsciemment leur audition
Figure 2: Our primary submission?s translation of a partial sentence from the newstest2009 set, showing a combination
of syntactic and non-syntactic rules.
stract ones.
Based on the results presented in Section 3, we
feel confident in declaring vocabulary-based filter-
ing of the Giga-FrEn corpus a success. By increas-
ing the size of our parallel corpus by 26%, we more
than tripled the number of unique words appearing
in the source text. In conjunction with supplements
to the Europarl, news commentary, and UN docu-
ment corpora, this improvement led to 44% fewer
OOV tokens at decoding time on two different test
sets, as well as a boost in automatic metric scores
of 0.6 METEOR, 1.2 BLEU, and 1.5 TER points
compared to last year?s system. We expect to em-
ploy similar data selection techniques when building
future systems, especially as the amount of parallel
data available continues to increase.
We did not, however, find significant improve-
ments in translation quality by changing the gram-
mar filtering method. As discussed in Section 3, lim-
iting the grammar to only 2000 fully abstract rules
may not have been enough, since additional abstract
rules applied fairly frequently in test data if they
were available. We plan to experiment with larger
filtering cutoffs in future work. A complementary
solution could be to increase the number of par-
tially lexicalized rules. Although we found mixed
results in their application within our current sys-
tem, the success of Hiero-derived MT systems (Chi-
ang, 2005; Chiang, 2010) shows that high transla-
tion quality can be achieved with rules that are only
partially abstract. A major difference between such
systems and our current implementation is that ours,
at 102,000 rules, has a much smaller grammar.
Acknowledgments
This research was supported in part by U.S. Na-
tional Science Foundation grants IIS-0713402 and
IIS-0915327, as well as by the DARPA GALE pro-
gram. Thanks to Kenneth Heafield for processing
the English monolingual data and building the lan-
guage model file, and to Jonathan Clark for Loony-
Bin support and bug fixes. We also thank Yahoo!
for the use of the M45 research computing clus-
ter, where we ran many steps of our experimental
pipeline.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, MI, June.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July.
370
Jonathan Clark and Alon Lavie. 2010. LoonyBin: Keep-
ing language technologists sane through automated
management of experimental (hyper)workflows. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation, pages 1301?
1308, Valletta, Malta, May.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engineer-
ing, Testing, and Quality Assurance for Natural Lan-
guage Processing, pages 49?57, Columbus, OH, June.
Greg Hanneman, Jonathan Clark, and Alon Lavie. 2010.
Improved features and grammar selection for syntax-
based MT. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 82?87, Uppsala, Sweden, July.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10. MIT Press, Cambridge,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 48?54, Edmonton,
Alberta, May?June.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of IWSLT 2005, Pittsburgh, PA, October.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Juan Pino, Gonzalo Iglesias, Adria` de Gispert, Graeme
Blackwood, Jaime Brunning, and William Byrne.
2010. The CUED HiFST system for the WMT10
translation shared task. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 155?160, Uppsala, Sweden,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Seventh
International Conference on Spoken Language Pro-
cessing, pages 901?904, Denver, CO, September.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
371
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 261?266,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The CMU-Avenue French-English Translation System
Michael Denkowski Greg Hanneman Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mdenkows,ghannema,alavie}@cs.cmu.edu
Abstract
This paper describes the French-English trans-
lation system developed by the Avenue re-
search group at Carnegie Mellon University
for the Seventh Workshop on Statistical Ma-
chine Translation (NAACL WMT12). We
present a method for training data selection,
a description of our hierarchical phrase-based
translation system, and a discussion of the im-
pact of data size on best practice for system
building.
1 Introduction
We describe the French-English translation sys-
tem constructed by the Avenue research group at
Carnegie Mellon University for the shared trans-
lation task in the Seventh Workshop on Statistical
Machine Translation. The core translation system
uses the hierarchical phrase-based model described
by Chiang (2007) with sentence-level grammars ex-
tracted and scored using the methods described by
Lopez (2008). Improved techniques for data selec-
tion and monolingual text processing significantly
improve the performance of the baseline system.
Over half of all parallel data for the French-
English track is provided by the Giga-FrEn cor-
pus (Callison-Burch et al, 2009). Assembled from
crawls of bilingual websites, this corpus is known to
be noisy, containing sentences that are either not par-
allel or not natural language. Rather than simply in-
cluding or excluding the resource in its entirety, we
use a relatively simple technique inspired by work in
machine translation quality estimation to select the
best portions of the corpus for inclusion in our train-
ing data. Including around 60% of the Giga-FrEn
chosen by this technique yields an improvement of
0.7 BLEU.
Prior to model estimation, we process all parallel
and monolingual data using in-house tokenization
and normalization scripts that detect word bound-
aries better than the provided WMT12 scripts. After
translation, we apply a monolingual rule-based post-
processing step to correct obvious errors and make
sentences more acceptable to human judges. The
post-processing step alone yields an improvement of
0.3 BLEU to the final system.
We conclude with a discussion of the impact of
data size on important decisions for system building.
Experimental results show that ?best practice? deci-
sions for smaller data sizes do not necessarily carry
over to systems built with ?WMT-scale? data, and
provide some explanation for why this is the case.
2 Training Data
Training data provided for the French-English trans-
lation task includes parallel corpora taken from Eu-
ropean Parliamentary proceedings (Koehn, 2005),
news commentary, and United Nations documents.
Together, these sets total approximately 13 million
sentences. In addition, a large, web-crawled parallel
corpus termed the ?Giga-FrEn? (Callison-Burch et
al., 2009) is made available. While this corpus con-
tains over 22 million parallel sentences, it is inher-
ently noisy. Many parallel sentences crawled from
the web are neither parallel nor sentences. To make
use of this large data source, we employ data se-
lection techniques discussed in the next subsection.
261
Corpus Sentences
Europarl 1,857,436
News commentary 130,193
UN doc 11,684,454
Giga-FrEn 1stdev 7,535,699
Giga-FrEn 2stdev 5,801,759
Total 27,009,541
Table 1: Parallel training data
Parallel data used to build our final system totals 27
million sentences. Precise figures for the number of
sentences in each data set, including selections from
the Giga-FrEn, are found in Table 1.
2.1 Data Selection as Quality Estimation
Drawing inspiration from the workshop?s featured
task, we cast the problem of data selection as one
of quality estimation. Specia et al (2009) report
several estimators of translation quality, the most ef-
fective of which detect difficult-to-translate source
sentences, ungrammatical translations, and transla-
tions that align poorly to their source sentences. We
can easily adapt several of these predictive features
to select good sentence pairs from noisy parallel cor-
pora such as the Giga-FrEn.
We first pre-process the Giga-FrEn by removing
lines with invalid Unicode characters, control char-
acters, and insufficient concentrations of Latin char-
acters. We then score each sentence pair in the re-
maining set (roughly 90% of the original corpus)
with the following features:
Source language model: a 4-gram modified
Kneser-Ney smoothed language model trained on
French Europarl, news commentary, UN doc, and
news crawl corpora. This model assigns high scores
to grammatical source sentences and lower scores to
ungrammatical sentences and non-sentences such as
site maps, large lists of names, and blog comments.
Scores are normalized by number of n-grams scored
per sentence (length + 1). The model is built using
the SRILM toolkit (Stolke, 2002).
Target language model: a 4-gram modified
Kneser-Ney smoothed language model trained on
English Europarl, news commentary, UN doc, and
news crawl corpora. This model scores grammati-
cality on the target side.
Word alignment scores: source-target and
target-source MGIZA++ (Gao and Vogel, 2008)
force-alignment scores using IBM Model 4 (Och
and Ney, 2003). Model parameters are estimated
on 2 million words of French-English Europarl and
news commentary text. Scores are normalized by
the number of alignment links. These features mea-
sure the extent to which translations are parallel with
their source sentences.
Fraction of aligned words: source-target and
target-source ratios of aligned words to total words.
These features balance the link-normalized align-
ment scores.
To determine selection criteria, we use this feature
set to score the news test sets from 2008 through
2011 (10K parallel sentences) and calculate the
mean and standard deviation of each feature score
distribution. We then select two subsets of the Giga-
FrEn, ?1stdev? and ?2stdev?. The 1stdev set in-
cludes sentence pairs for which the score for each
feature is above a threshold defined as the develop-
ment set mean minus one standard deviation. The
2stdev set includes sentence pairs not included in
1stdev that meet the per-feature threshold of mean
minus two standard deviations. Hard, per-feature
thresholding is motivated by the notion that a sen-
tence pair must meet al the criteria discussed above
to constitute good translation. For example, high
source and target language model scores are irrel-
evant if the sentences are not parallel.
As primarily news data is used for determining
thresholds and building language models, this ap-
proach has the added advantage of preferring par-
allel data in the domain we are interested in translat-
ing. Our final translation system uses data from both
1stdev and 2stdev, corresponding to roughly 60% of
the Giga-FrEn corpus.
2.2 Monolingual Data
Monolingual English data includes European Parlia-
mentary proceedings (Koehn, 2005), news commen-
tary, United Nations documents, news crawl, the En-
glish side of the Giga-FrEn, and the English Giga-
word Fourth Edition (Parker et al, 2009). We use all
available data subject to the following selection de-
cisions. We apply the initial filter to the Giga-FrEn
to remove non-text sections, leaving approximately
90% of the corpus. We exclude the known prob-
262
Corpus Words
Europarl 59,659,916
News commentary 5,081,368
UN doc 286,300,902
News crawl 1,109,346,008
Giga-FrEn 481,929,410
Gigaword 4th edition 1,960,921,287
Total 3,903,238,891
Table 2: Monolingual language modeling data (uniqued)
lematic New York Times section of the Gigaword.
As many data sets include repeated boilerplate text
such as copyright information or browser compat-
ibility notifications, we unique sentences from the
UN doc, news crawl, Giga-FrEn, and Gigaword sets
by source. Final monolingual data totals 4.7 billion
words before uniqueing and 3.9 billion after. Word
counts for all data sources are shown in Table 2.
2.3 Text Processing
All monolingual and parallel system data is run
through a series of pre-processing steps before
construction of the language model or translation
model. We first run an in-house normalization script
over all text in order to convert certain variably en-
coded characters to a canonical form. For example,
thin spaces and non-breaking spaces are normalized
to standard ASCII space characters, various types of
?curly? and ?straight? quotation marks are standard-
ized as ASCII straight quotes, and common French
and English ligatures characters (e.g. ?, fi) are re-
placed with standard equivalents.
English text is tokenized with the Penn Treebank-
style tokenizer attached to the Stanford parser (Klein
and Manning, 2003), using most of the default op-
tions. We set the tokenizer to Americanize vari-
ant spellings such as color vs. colour or behavior
vs. behaviour. Currency-symbol normalization is
avoided.
For French text, we use an in-house tokenization
script. Aside from the standard tokenization based
on punctuation marks, this step includes French-
specific rules for handling apostrophes (French eli-
sion), hyphens in subject-verb inversions (includ-
ing the French t euphonique), and European-style
numbers. When compared to the default WMT12-
provided tokenization script, our custom French
rules more accurately identify word boundaries, par-
ticularly in the case of hyphens. Figure 1 highlights
the differences in sample phrases. Subject-verb in-
versions are broken apart, while other hyphenated
words are unaffected; French aujourd?hui (?today?)
is retained as a single token to match English.
Parallel data is run through a further filtering step
to remove sentence pairs that, by their length char-
acteristics alone, are very unlikely to be true parallel
data. Sentence pairs that contain more than 95 to-
kens on either side are globally discarded, as are sen-
tence pairs where either side contains a token longer
than 25 characters. Remaining pairs are checked for
length ratio between French and English, and sen-
tences are discarded if their English translations are
either too long or too short given the French length.
Allowable ratios are determined from the tokenized
training data and are set such that approximately the
middle 95% of the data, in terms of length ratio, is
kept for each French length.
3 Translation System
Our translation system uses cdec (Dyer et al,
2010), an implementation of the hierarchical phrase-
based translation model (Chiang, 2007) that uses the
KenLM library (Heafield, 2011) for language model
inference. The system translates from cased French
to cased English; at no point do we lowercase data.
The Parallel data is aligned in both directions us-
ing the MGIZA++ (Gao and Vogel, 2008) imple-
mentation of IBM Model 4 and symmetrized with
the grow-diag-final heuristic (Och and Ney,
2003). The aligned corpus is then encoded as a
suffix array to facilitate sentence-level grammar ex-
traction and scoring (Lopez, 2008). Grammars are
extracted using the heuristics described by Chiang
(Chiang, 2007) and feature scores are calculated ac-
cording to Lopez (2008).
Modified Knesser-Ney smoothed (Chen and
Goodman, 1996) n-gram language models are built
from the monolingual English data using the SRI
language modeling toolkit (Stolke, 2002). We ex-
periment with both 4-gram and 5-gram models.
System parameters are optimized using minimum
error rate training (Och, 2003) to maximize the
corpus-level cased BLEU score (Papineni et al,
263
Base: Y a-t-il un colle`gue pour prendre la parole
Custom: Y a -t-il un colle`gue pour prendre la parole
Base: Peut-e?tre , a` ce sujet , puis-je dire a` M. Ribeiro i Castro
Custom: Peut-e?tre , a` ce sujet , puis -je dire a` M. Ribeiro i Castro
Base: le proce`s-verbal de la se?ance d? aujourd? hui
Custom: le proce`s-verbal de la se?ance d? aujourd?hui
Base: s? e?tablit environ a` 1,2 % du PIB
Custom: s? e?tablit environ a` 1.2 % du PIB
Figure 1: Customized French tokenization rules better identify word boundaries.
pre?-e?l?ectoral ? pre-electoral
mosa??que ? mosaique
de?ragulation ? deragulation
Figure 2: Examples of cognate translation
2002) on news-test 2008 (2051 sentences). This de-
velopment set is chosen for its known stability and
reliability.
Our baseline translation system uses Viterbi de-
coding while our final system uses segment-level
Minimum Bayes-Risk decoding (Kumar and Byrne,
2004) over 500-best lists using 1 - BLEU as the loss
function.
3.1 Post-Processing
Our final system includes a monolingual rule-based
post-processing step that corrects obvious transla-
tion errors. Examples of correctable errors include
capitalization, mismatched punctuation, malformed
numbers, and incorrectly split compound words. We
finally employ a coarse cognate translation system
to handle out-of-vocabulary words. We assume that
uncapitalized French source words passed through
to the English output are cognates of English words
and translate them by removing accents. This fre-
quently leads to (in order of desirability) fully cor-
rect translations, correct translations with foreign
spellings, or correct translations with misspellings.
All of the above are generally preferable to untrans-
lated foreign words. Examples of cognate transla-
tions for OOV words in newstest 2011 are shown in
Figure 2.1
1Some OOVs are caused by misspellings in the dev-test
source sentences. In these cases we can salvage misspelled En-
glish words in place of misspelled French words
BLEU (cased) Meteor TER
base 5-gram 28.4 27.4 33.7 53.2
base 4-gram 29.1 28.1 34.0 52.5
+1stdev GFE 29.3 28.3 34.2 52.1
+2stdev GFE 29.8 28.9 34.5 51.7
+5g/1K/MBR 29.9 29.0 34.5 51.5
+post-process 30.2 29.2 34.7 51.3
Table 3: Newstest 2011 (dev-test) translation results
4 Experiments
Beginning with a baseline translation system, we in-
crementally evaluate the contribution of additional
data and components. System performance is eval-
uated on newstest 2011 using BLEU (uncased and
cased) (Papineni et al, 2002), Meteor (Denkowski
and Lavie, 2011), and TER (Snover et al, 2006).
For full consistency with WMT11, we use the NIST
scoring script, TER-0.7.25, and Meteor-1.3 to eval-
uate cased, detokenized translations. Results are
shown in Table 3, where each evaluation point is the
result of a full tune/test run that includes MERT for
parameter optimization.
The baseline translation system is built from 14
million parallel sentences (Europarl, news commen-
tary, and UN doc) and all monolingual data. Gram-
mars are extracted using the ?tight? heuristic that
requires phrase pairs to be bounded by word align-
ments. Both 4-gram and 5-gram language models
are evaluated. Viterbi decoding is conducted with a
cube pruning pop limit (Chiang, 2007) of 200. For
this data size, the 4-gram model is shown to signifi-
cantly outperform the 5-gram.
Adding the 1stdev and 2stdev sets from the Giga-
FrEn increases the parallel data size to 27 million
264
BLEU (cased) Meteor TER
587M tight 29.1 28.1 34.0 52.5
587M loose 29.3 28.3 34.0 52.5
745M tight 29.8 28.9 34.5 51.7
745M loose 29.6 28.6 34.3 52.0
Table 4: Results for extraction heuristics (dev-test)
sentences and further improves performance. These
runs require new grammars to be extracted, but
use the same 4-gram language model and decoding
method as the baseline system. With large training
data, moving to a 5-gram language model, increas-
ing the cube pruning pop limit to 1000, and using
Minimum Bayes-Risk decoding (Kumar and Byrne,
2004) over 500-best lists collectively show a slight
improvement. Monolingual post-processing yields
further improvement. This decoding/processing
scheme corresponds to our final translation system.
4.1 Impact of Data Size
The WMT French-English track provides an oppor-
tunity to experiment in a space of data size that is
generally not well explored. We examine the impact
of data sizes of hundreds of millions of words on
two significant system building decisions: grammar
extraction and language model estimation. Compar-
ative results are reported on the newstest 2011 set.
In the first case, we compare the ?tight? extrac-
tion heuristic that requires phrases to be bounded
by word alignments to the ?loose? heuristic that al-
lows unaligned words at phrase edges. Lopez (2008)
shows that for a parallel corpus of 107 million
words, using the loose heuristic produces much
larger grammars and improves performance by a full
BLEU point. However, even our baseline system
is trained on substantially more data (587 million
words on the English side) and the addition of the
Giga-FrEn sets increases data size to 745 million
words, seven times that used in the cited work. For
each data size, we decode with grammars extracted
using each heuristic and a 4-gram language model.
As shown in Table 4, the differences are much
smaller and the tight heuristic actually produces the
best result for the full data scenario. We believe
this to be directly linked to word alignment quality:
smaller training data results in sparser, noisier word
BLEU (cased) Meteor TER
587M 4-gram 29.1 28.1 34.0 52.5
587M 5-gram 28.4 27.4 33.7 53.2
745M 4-gram 29.8 28.9 34.5 51.7
745M 5-gram 29.8 28.9 34.4 51.7
Table 5: Results for language model order (dev-test)
alignments while larger data results in denser, more
accurate alignments. In the first case, accumulating
unaligned words can make up for shortcomings in
alignment quality. In the second, better rules are ex-
tracted by trusting the stronger alignment model.
We also compare 4-gram and 5-gram language
model performance with systems using tight gram-
mars extracted from 587 million and 745 million
sentences. As shown in Table 5, the 4-gram sig-
nificantly outperforms the 5-gram with smaller data
while the two are indistinguishable with larger data2.
With modified Kneser-Ney smoothing, a lower or-
der model will outperform a higher order model if
the higher order model constantly backs off to lower
orders. With stronger grammars learned from larger
parallel data, the system is able to produce output
that matches longer n-grams in the language model.
5 Summary
We have presented the French-English translation
system built for the NAACL WMT12 shared transla-
tion task, including descriptions of our data selection
and text processing techniques. Experimental re-
sults have shown incremental improvement for each
addition to our baseline system. We have finally
discussed the impact of the availability of WMT-
scale data on system building decisions and pro-
vided comparative experimental results.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Proc.
of ACL WMT 2009.
2We find that for the full data system, also increasing the
cube pruning pop limit and using MBR decoding yields a very
slight improvement with the 5-gram model over the same de-
coding scheme with the 4-gram.
265
Stanley F. Chen and Joshua Goodman. 1996. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. In Proc. of ACL 1996.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proc. of
the EMNLP WMT 2011.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models. In
Proc. of ACL 2010.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Proc. of ACL
WSETQANLP 2008.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proc. of EMNLP WMT
2011.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proc. of ACL 2003.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of MT Sum-
mit 2005.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-Risk Decoding for Statistical Machine Transla-
tion. In Proc. of NAACL/HLT 2004.
Adam Lopez. 2008. Tera-Scale Translation Models via
Pattern Matching. In Proc. of COLING 2008.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of ACL
2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL 2002.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth Edi-
tion. Linguistic Data Consortium, LDC2009T13.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA 2006.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
Confidence of Machine Translation Quality Estimates.
In Proc. of MT Summit XII.
Andreas Stolke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002.
266
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70?77,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2013:
Syntax, Synthetic Translation Options, and Pseudo-References
Waleed Ammar Victor Chahuneau Michael Denkowski Greg Hanneman
Wang Ling Austin Matthews Kenton Murray Nicola Segall Yulia Tsvetkov
Alon Lavie Chris Dyer?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submit-
ted to the 2013 WMT shared task in ma-
chine translation. We participated in three
language pairs, French?English, Russian?
English, and English?Russian. Our
particular innovations include: a label-
coarsening scheme for syntactic tree-to-
tree translation and the use of specialized
modules to create ?synthetic translation
options? that can both generalize beyond
what is directly observed in the parallel
training data and use rich source language
context to decide how a phrase should
translate in context.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute par-
ticipated in three language pairs for the 2013
Workshop on Machine Translation shared trans-
lation task: French?English, Russian?English,
and English?Russian. Our French?English sys-
tem (?3) showcased our group?s syntactic sys-
tem with coarsened nonterminal types (Hanne-
man and Lavie, 2011). Our Russian?English and
English?Russian system demonstrate a new multi-
phase approach to translation that our group is us-
ing, in which synthetic translation options (?4)
to supplement the default translation rule inven-
tory that is extracted from word-aligned training
data. In the Russian-English system (?5), we used
a CRF-based transliterator (Ammar et al, 2012)
to propose transliteration candidates for out-of-
vocabulary words, and used a language model
to insert or remove common function words in
phrases according to an n-gram English language
model probability. In the English?Russian system
(?6), we used a conditional logit model to predict
the most likely inflectional morphology of Rus-
sian lemmas, conditioning on rich source syntac-
tic features (?6.1). In addition to being able to
generate inflected forms that were otherwise unob-
served in the parallel training data, the translations
options generated in this matter had features re-
flecting their appropriateness given much broader
source language context than usually would have
been incorporated in current statistical MT sys-
tems.
For our Russian?English system, we addition-
ally used a secondary ?pseudo-reference? transla-
tion when tuning the parameters of our Russian?
English system. This was created by automatically
translating the Spanish translation of the provided
development data into English. While the output
of an MT system is not always perfectly gram-
matical, previous work has shown that secondary
machine-generated references improve translation
quality when only a single human reference is
available when BLEU is used as an optimization
criterion (Madnani, 2010; Dyer et al, 2011).
2 Common System Components
The decoder infrastructure we used was cdec
(Dyer et al, 2010). Only the constrained data
resources provided for the shared task were used
for training both the translation and language
models. Word alignments were generated us-
ing the Model 2 variant described in Dyer et al
(2013). Language models used modified Kneser-
Ney smoothing estimated using KenLM (Heafield,
2011). Translation model parameters were dis-
criminatively set to optimize BLEU on a held-out
development set using an online passive aggres-
sive algorithm (Eidelman, 2012) or, in the case of
70
the French?English system, using the hypergraph
MERT algorithm and optimizing towards BLEU
(Kumar et al, 2009). The remainder of the paper
will focus on our primary innovations in the vari-
ous system pairs.
3 French-English Syntax System
Our submission for French?English is a tree-to-
tree translation system that demonstrates several
innovations from group?s research on SCFG-based
translation.
3.1 Data Selection
We divided the French?English training data into
two categories: clean data (Europarl, News Com-
mentary, UN Documents) totaling 14.8 million
sentence pairs, and web data (Common Crawl,
Giga-FrEn) totaling 25.2 million sentence pairs.
To reduce the volume of data used, we filtered
non-parallel and other unhelpful segments accord-
ing to the technique described by Denkowski et al
(2012). This procedure uses a lexical translation
model learned from just the clean data, as well as
source and target n-gram language models to com-
pute the following feature scores:
? French and English 4-gram log likelihood (nor-
malized by length);
? French?English and English?French lexical
translation log likelihood (normalized by
length); and,
? Fractions of aligned words under the French?
English and English?French models.
We pooled previous years? WMT news test sets
to form a reference data set. We computed the
same features. To filter the web data, we retained
only sentence for which each feature score was
no lower than two standard deviations below the
mean on the reference data. This reduced the web
data from 25.2 million to 16.6 million sentence
pairs. Parallel segments from all parts of the data
that were blank on either side, were longer than 99
tokens, contained a token of more than 30 charac-
ters, or had particularly unbalanced length ratios
were also removed. After filtering, 30.9 million
sentence pairs remained for rule extraction: 14.4
million from the clean data, and 16.5 million from
the web data.
3.2 Preprocessing and Grammar Extraction
Our French?English system uses parse trees in
both the source and target languages, so tokeniza-
tion in this language pair was carried out to match
the tokenizations expected by the parsers we used
(English data was tokenized with the Stanford to-
kenizer for English and an in-house tokenizer for
French that targets the tokenization used by the
Berkeley French parser). Both sides of the par-
allel training data were parsed using the Berkeley
latent variable parser.
Synchronous context-free grammar rules were
extracted from the corpus following the method of
Hanneman et al (2011). This decomposes each
tree pair into a collection of SCFG rules by ex-
haustively identifying aligned subtrees to serve as
rule left-hand sides and smaller aligned subtrees
to be abstracted as right-hand-side nonterminals.
Basic subtree alignment heuristics are similar to
those by Galley et al (2006), and composed rules
are allowed. The computational complexity is held
in check by a limit on the number of RHS elements
(nodes and terminals), rather than a GHKM-style
maximum composition depth or Hiero-style max-
imum rule span. Our rule extractor also allows
?virtual nodes,? or the insertion of new nodes in
the parse tree to subdivide regions of flat struc-
ture. Virtual nodes are similar to the A+B ex-
tended categories of SAMT (Zollmann and Venu-
gopal, 2006), but with the added constraint that
they may not conflict with the surrounding tree
structure.
Because the SCFG rules are labeled with non-
terminals composed from both the source and tar-
get trees, the nonterminal inventory is quite large,
leading to estimation difficulties. To deal with
this, we automatically coarsening the nonterminal
labels (Hanneman and Lavie, 2011). Labels are
agglomeratively clustered based on a histogram-
based similarity function that looks at what tar-
get labels correspond to a particular source label
and vice versa. The number of clusters used is de-
termined based on spikes in the distance between
successive clustering iterations, or by the number
of source, target, or joint labels remaining. Start-
ing from a default grammar of 877 French, 2580
English, and 131,331 joint labels, we collapsed
the label space for our WMT system down to 50
French, 54 English, and 1814 joint categories.1
1Selecting the stopping point still requires a measure of
intuition. The label set size of 1814 chosen here roughly cor-
responds to the number of joint labels that would exist in the
grammar if virtual nodes were not included. This equivalence
has worked well in practice in both internal and published ex-
periments on other data sets (Hanneman and Lavie, 2013).
71
Extracted rules each have 10 features associated
with them. For an SCFG rule with source left-
hand side `s, target left-hand side `t, source right-
hand side rs, and target right-hand side rt, they
are:
? phrasal translation log relative frequencies
log f(rs | rt) and log f(rt | rs);
? labeling relative frequency log f(`s, `t | rs, rt)
and generation relative frequency
log f(rs, rt | `s, `t);
? lexical translation log probabilities log plex(rs |
rt) and log plex(rt | rs), defined similarly to
Moses?s definition;
? a rarity score exp( 1c )?1exp(1)?1 for a rule with frequency
c (this score is monotonically decreasing in the
rule frequency); and,
? three binary indicator features that mark
whether a rule is fully lexicalized, fully abstract,
or a glue rule.
Grammar filtering. Even after collapsing la-
bels, the extracted SCFGs contain an enormous
number of rules ? 660 million rule types from just
under 4 billion extracted instances. To reduce the
size of the grammar, we employ a combination of
lossless filtering and lossy pruning. We first prune
all rules to select no more than the 60 most fre-
quent target-side alternatives for any source RHS,
then do further filtering to produce grammars for
each test sentence:
? Lexical rules are filtered to the sentence level.
Only phrase pairs whose source sides match the
test sentence are retained.
? Abstract rules (whose RHS are all nontermi-
nals) are globally pruned. Only the 4000 most
frequently observed rules are retained.
? Mixed rules (whose RHS are a mix of terminals
and nonterminals) must match the test sentence,
and there is an additional frequency cutoff.
After this filtering, the number of completely lex-
ical rules that match a given sentence is typically
low, up to a few thousand rules. Each fully ab-
stract rule can potentially apply to every sentence;
the strict pruning cutoff in use for these rules is
meant to focus the grammar to the most important
general syntactic divergences between French and
English. Most of the latitude in grammar pruning
comes from adjusting the frequency cutoff on the
mixed rules since this category of rule is by far the
most common type. We conducted experiments
with three different frequency cutoffs: 100, 200,
and 500, with each increase decreasing the gram-
mar size by 70?80 percent.
3.3 French?English Experiments
We tuned our system to the newstest2008 set of
2051 segments. Aside from the official new-
stest2013 test set (3000 segments), we also col-
lected test-set scores from last year?s newstest2012
set (3003 segments). Automatic metric scores
are computed according to BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011),
and TER (Snover et al, 2006), all computed ac-
cording to MultEval v. 0.5 (Clark et al, 2011).
Each system variant is run with two independent
MERT steps in order to control for optimizer in-
stability.
Table 1 presents the results, with the metric
scores averaged over both MERT runs. Quite in-
terestingly, we find only minor differences in both
tune and test scores despite the large differences in
filtered/pruned grammar size as the cutoff for par-
tially abstract rules increases. No system is fully
statistically separable (at p < 0.05) from the oth-
ers according to MultEval?s approximate random-
ization algorithm. The closest is the variant with
cutoff 200, which is generally judged to be slightly
worse than the other two. METEOR claims full
distinction on the 2013 test set, ranking the sys-
tem with the strictest grammar cutoff (500) best.
This is the version that we ultimately submitted to
the shared translation task.
4 Synthetic Translation Options
Before discussing our Russian?English and
English?Russian systems, we introduce the
concept of synthetic translation options, which
we use in these systems. We provide a brief
overview here; for more detail, we refer the reader
to Tsvetkov et al (2013).
In language pairs that are typologically similar,
words and phrases map relatively directly from
source to target languages, and the standard ap-
proach to learning phrase pairs by extraction from
parallel data can be very effective. However, in
language pairs in which individual source lan-
guage words have many different possible transla-
tions (e.g., when the target language word could
have many different inflections or could be sur-
rounded by different function words that have no
72
Dev (2008) Test (2012) Test (2013)
System BLEU METR TER BLEU METR TER BLEU METR TER
Cutoff 100 22.52 31.44 59.22 27.73 33.30 53.25 28.34 * 33.19 53.07
Cutoff 200 22.34 31.40 59.21 * 27.33 33.26 53.23 * 28.05 * 33.07 53.16
Cutoff 500 22.80 31.64 59.10 27.88 * 33.58 53.09 28.27 * 33.31 53.13
Table 1: French?English automatic metric scores for three grammar pruning cutoffs, averaged over two
MERT runs each. Scores that are statistically separable (p < 0.05) from both others in the same column
are marked with an asterisk (*).
direct correspondence in the source language), we
can expect the standard phrasal inventory to be
incomplete, except when very large quantities of
parallel data are available or for very frequent
words. There simply will not be enough exam-
ples from which to learn the ideal set of transla-
tion options. Therefore, since phrase based trans-
lation can only generate input/output word pairs
that were directly observed in the training corpus,
the decoder?s only hope for producing a good out-
put is to find a fluent, meaning-preserving transla-
tion using incomplete translation lexicons. Syn-
thetic translation option generation seeks to fill
these gaps using secondary generation processes
that produce possible phrase translation alterna-
tives that are not directly extractable from the
training data. By filling in gaps in the transla-
tion options used to construct the sentential trans-
lation search space, global discriminative transla-
tion models and language models can be more ef-
fective than they would otherwise be.
From a practical perspective, synthetic transla-
tion options are attractive relative to trying to build
more powerful models of translation since they
enable focus on more targeted translation prob-
lems (for example, transliteration, or generating
proper inflectional morphology for a single word
or phrase). Since they are translation options and
not complete translations, many of them may be
generated.
In the following system pairs, we use syn-
thetic translation options to augment hiero gram-
mar rules learned in the usual way. The synthetic
phrases we include augment draw from several
sources:
? transliterations of OOV Russian words (?5.3);
? English target sides with varied function words
(for example, given a phrase that translates into
cat we procedure variants like the cat, a cat and
of the cat); and,
? when translating into Russian, we generate
phrases by first predicting the most likely Rus-
sian lemma for a source word or phrase, and
then, conditioned on the English source context
(including syntactic and lexical features), we
predict the most likely inflection of the lemma
(?6.1).
5 Russian?English System
5.1 Data
We used the same parallel data for both the
Russian?English and English Russian systems.
Except for filtering to remove sentence pairs
whose log length ratios were statistical outliers,
we only filtered the Common Crawl corpus to re-
move sentence pairs with less than 50% concentra-
tion of Cyrillic characters on the Russian side. The
remaining data was tokenized and lower-cased.
For language models, we trained 4-gram Markov
models using the target side of the bitext and any
available monolingual data (including Gigaword
for English). Additionally, we trained 7-gram lan-
guage models using 600-class Brown clusters with
Witten-Bell smoothing.2
5.2 Baseline System
Our baseline Russian?English system is a hierar-
chical phrase-based translation model as imple-
mented in cdec (Chiang, 2007; Dyer et al, 2010).
SCFG translation rules that plausibly match each
sentence in the development and deftest sets were
extracted from the aligned parallel data using the
suffix array indexing technique of Lopez (2008).
A Russian morphological analyzer was used to
lemmatize the training, development, and test
data, and the ?noisier channel? translation ap-
proach of Dyer (2007) was used in the Russian?
English system to let unusually inflected surface
forms back off to per-lemma translations.
2http://www.ark.cs.cmu.edu/cdyer/ru-600/.
73
5.3 Synthetic Translations: Transliteration
Analysis revealed that about one third of the un-
seen Russian tokens in the development set con-
sisted of named entities which should be translit-
erated. We used individual Russian-English word
pairs in Wikipedia parallel headlines 3 to train a
linear-chained CRF tagger which labels each char-
acter in the Russian token with a sequence of zero
or more English characters (Ammar et al, 2012).
Since Russian names in the training set were in
nominative case, we used a simple rule-based mor-
phological generator to produce possible inflec-
tions and filtered out the ones not present in the
Russian monolingual corpus. At decoding, un-
seen Russian tokens are fed to the transliterator
which produces the most probable 20 translitera-
tions. We add a synthetic translation option for
each of the transliterations with four features: an
indicator feature for transliterations, the CRF un-
normalized score, the trigram character-LM log-
probability, and the divergence from the average
length-ratio between an English name and its Rus-
sian transliteration.
5.4 Synthetic Translations: Function Words
Slavic languages like Russian have a large number
of different inflected forms for each lemma, repre-
senting different cases, tenses, and aspects. Since
our training data is rather limited relative to the
number of inflected forms that are possible, we use
an English language model to generate a variety
of common function word contexts for each con-
tent word phrase. These are added to the phrase
table with a feature indicating that they were not
actually observed in the training data, but rather
hallucinated using SRILM?s disambig tool.
5.5 Summary
Table 5.5 summarizes our Russian-English trans-
lation results. In the submitted system, we addi-
tionally used MBR reranking to combine the 500-
best outputs of our system, with the 500-best out-
puts of a syntactic system constructed similarly to
the French?English system.
6 English?Russian System
The bilingual training data was identical to the
filtered data used in the previous section. Word
alignments was performed after lemmatizing the
3We contributed the data set to the shared task participants
at http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
Table 2: Russian-English summary.
Condition BLEU
Baseline 30.8
Function words 30.9
Transliterations 31.1
Russian side of the training corpus. An unpruned,
modified Kneser-Ney smoothed 4-gram language
model (Chen and Goodman, 1996) was estimated
from all available Russian text (410 million words)
using the KenLM toolkit (Heafield et al, 2013).
A standard hierarchical phrase-based system
was trained with rule shape indicator features, ob-
tained by replacing terminals in translation rules
by a generic symbol. MIRA training was per-
formed to learn feature weights.
Additionally, word clusters (Brown et al, 1992)
were obtained for the complete monolingual Rus-
sian data. Then, an unsmoothed 7-gram language
model was trained on these clusters and added as
a feature to the translation system. Indicator fea-
tures were also added for each cluster and bigram
cluster occurence. These changes resulted in an
improvement of more than a BLEU point on our
held-out development set.
6.1 Predicting Target Morphology
We train a classifier to predict the inflection of
each Russian word independently given the cor-
responding English sentence and its word align-
ment. To do this, we first process the Russian
side of the parallel training data using a statisti-
cal morphological tagger (Sharoff et al, 2008) to
obtain lemmas and inflection tags for each word
in context. Then, we obtain part-of-speech tags
and dependency parses of the English side of the
parallel data (Martins et al, 2010), as well as
Brown clusters (Brown et al, 1992). We extract
features capturing lexical and syntactical relation-
ships in the source sentence and train structured
linear logistic regression models to predict the tag
of each English word independently given its part-
of-speech.4 In practice, due to the large size of
the corpora and of the feature space dimension,
we were only able to use about 10% of the avail-
able bilingual data, sampled randomly from the
Common Crawl corpus. We also restricted the
4We restrict ourselves to verbs, nouns, adjectives, adverbs
and cardinals since these open-class words carry most inflec-
tion in Russian.
74
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
aux
????????_V*+*mis/sfm/e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
Figure 1: The classifier is trained to predict the verbal inflection mis-sfm-e based on the linear and
syntactic context of the words aligned to the Russian word; given the stem ???????? (pytat?sya), this
inflection paradigm produces the observed surface form ???????? (pytalas?).
set of possible inflections for each word to the set
of tags that were observed with its lemma in the
full monolingual training data. This was neces-
sary because of our choice to use a tagger, which
is not able to synthesize surface forms for a given
lemma-tag pair.
We then augment the standard hierarchical
phrase-base grammars extracted for the baseline
systems with new rules containing inflections not
necessarily observed in the parallel training data.
We start by training a non-gappy phrase transla-
tion model on the bilingual data where the Russian
has been lemmatized.5 Then, before translating an
English sentence, we extract translation phrases
corresponding to this specific sentence and re-
inflect each word in the target side of these phrases
using the classifier with features extracted from
the source sentence words and annotations. We
keep the original phrase-based translation features
and add the inflection score predicted by the clas-
sifier as well as indicator features for the part-of-
speech categories of the re-inflected words.
On a held-out development set, these synthetic
phrases produce a 0.3 BLEU point improvement.
Interestingly, the feature weight learned for using
these phrases is positive, indicating that useful in-
flections might be produced by this process.
7 Conclusion
The CMU systems draws on a large number of
different research directions. Techniques such as
MBR reranking and synthetic phrases allow dif-
ferent contributors to focus on different transla-
5We keep intact words belonging to non-predicted cate-
gories.
tion problems that are ultimately recombined into
a single system. Our performance, in particular,
on English?Russian machine translation was quite
satisfying, we attribute our biggest gains in this
language pair to the following:
? Our inflection model that predicted how an En-
glish word ought best be translated, given its
context. This enabled us to generate forms that
were not observed in the parallel data or would
have been rare independent of context with pre-
cision.
? Brown cluster language models seem to be quite
effective at modeling long-range morphological
agreement patterns quite reliably.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
75
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computional Linguistics, 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California, USA,
June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2007. The ?noiser channel?: Translation
from morphologically complex languages. In Pro-
ceedings of WMT.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. of
ACL-IJCNLP.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. of COLING.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
76
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a russian tagset. In Proc. of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
77
Workshop on Humans and Computer-assisted Translation, pages 72?77,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Real Time Adaptive Machine Translation for Post-Editing with
cdec and TransCenter
Michael Denkowski Alon Lavie Isabel Lacruz
?
Chris Dyer
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA
?
Institute for Applied Linguistics, Kent State University, Kent, OH 44242 USA
{mdenkows,alavie,cdyer}@cs.cmu.edu ilacruz@kent.edu
Abstract
Using machine translation output as a
starting point for human translation has
recently gained traction in the transla-
tion community. This paper describes
cdec Realtime, a framework for build-
ing adaptive MT systems that learn from
post-editor feedback, and TransCenter, a
web-based translation interface that con-
nects users to Realtime systems and logs
post-editing activity. This combination
allows the straightforward deployment of
MT systems specifically for post-editing
and analysis of human translator produc-
tivity when working with these systems.
All tools, as well as actual post-editing
data collected as part of a validation exper-
iment, are freely available under an open
source license.
1 Introduction
This paper describes the end-to-end machine
translation post-editing setup provided by cdec
Realtime and TransCenter. As the quality of MT
systems continues to improve, the idea of using
automatic translation as a primary technology in
assisting human translators has become increas-
ingly attractive. Recent work has explored the
possibilities of integrating MT into human transla-
tion workflows by providing MT-generated trans-
lations as a starting point for translators to cor-
rect, as opposed to translating source sentences
from scratch. The motivation for this process is
to dramatically reduce human translation effort
while improving translator productivity and con-
sistency. This computer-aided approach is directly
applicable to the wealth of scenarios that still re-
quire precise human-quality translation that MT
is currently unable to deliver, including an ever-
increasing number of government, commercial,
and community-driven projects.
The software described in the following sec-
tions enables users to translate documents with
the assistance of an adaptive MT system using
a web-based interface. The system learns from
user feedback, improving translation quality as
users work. All user interaction is logged, al-
lowing post-editing sessions to be replayed and
analyzed. All software is freely available under
an open source license, allowing anyone to eas-
ily build, deploy, and evaluate MT systems specif-
ically for post-editing. We first describe the under-
lying adaptive MT paradigm (?2) and the Realtime
implementation (?3). We then describe Trans-
Center (?4) and the results of an end-to-end post-
editing experiment with human translators (?5).
All data collected as part of this validation experi-
ment is also publicly available.
2 Adaptive Machine Translation
Traditional machine translation systems operate in
batch mode: statistical translation models are es-
timated from large volumes of sentence-parallel
bilingual text and then used to translate new text.
Incorporating new data requires a full system re-
build, an expensive operation taking up to days of
time. As such, MT systems in production scenar-
ios typically remain static for large periods of time
(months or even indefinitely). Recently, an adap-
tive MT paradigm has been introduced specifi-
cally for post-editing (Denkowski et al., 2014).
Three major MT system components are extended
to support online updates, allowing human post-
editor feedback to be immediately incorporated:
? An online translation model is updated to in-
clude new translations extracted from post-
editing data.
? A dynamic language model is updated to in-
clude post-edited target language text.
? An online update is made to the system?s
feature weights after each sentence is post-
edited.
72
These extensions allow the MT system to gener-
ate improved translations that require significantly
less effort to correct for later sentences in the doc-
ument. This paradigm is now implemented in
the freely available cdec (Dyer et al., 2010) ma-
chine translation toolkit as Realtime, part of the
pycdec (Chahuneau et al., 2012) Python API.
Standard MT systems use aggregate statistics
from all training text to learn a single large
translation grammar (in the case of cdec?s hi-
erarchical phrase-based model (Chiang, 2007), a
synchronous context-free grammar) consisting of
rules annotated with feature scores. As an alter-
native, the bitext can be indexed using a suffix ar-
ray (Lopez, 2008), a data structure allowing fast
source-side lookups. When a new sentence is to be
translated, training sentences that share spans of
text with the input sentence are sampled from the
suffix array. Statistics from the sample are used to
learn a small, sentence-specific grammar on-the-
fly. The adaptive paradigm extends this approach
to support online updates by also indexing the
new bilingual sentences generated as a post-editor
works. When a new sentence is translated, match-
ing sentences are sampled from the post-editing
data as well as the suffix array. All feature scores
that can be computed on a suffix array sample can
be identically computed on the combined sample,
allowing uniform handling of all data. An addi-
tional ?post-edit support? feature is included that
indicates whether a grammar rule was extracted
from the post-editing data. This allows an opti-
mizer to learn to prefer translations that originate
from human feedback. This adaptation approach
also serves as a platform for exploring expanded
post-editing-aware feature sets; any feature that
can be computed from standard text can be added
to the model and will automatically include post-
editing data. Implementationally, feature scoring
is broken out into a single Python source file con-
taining a single function for each feature score.
New feature functions can be added easily.
The adaptive paradigm uses two language mod-
els. A standard (static) n-gram language model es-
timated on large monolingual text allows the sys-
tem to prefer translations more similar to human-
generated text in the target language. A (dy-
namic) Bayesian n-gram language model (Teh,
2006) can be updated with observations of the
post-edited output in a straightforward way. This
smaller model exactly covers the training bitext
and all post-editing data, letting the system up-
weight translations with newly learned vocabu-
lary and phrasing absent in the large monolingual
text. Finally, the margin-infused relaxed algorithm
(MIRA) (Crammer et al., 2006; Eidelman, 2012)
is used to make an online parameter update after
each sentence is post-edited, minimizing model er-
ror. This allows the system to continuously rescale
weights for translation and language model fea-
tures that adapt over time.
Since true post-editing data is infeasible to col-
lect during system development and internal test-
ing, as standard MT pipelines require tens of thou-
sands of sentences to be translated with low la-
tency, a simulated post-editing paradigm (Hardt
and Elming, 2010) can be used, wherein pre-
generated reference translations act as a stand-in
for actual post-editing. This approximation is ef-
fective for tuning and internal evaluation when
real post-editing data is unavailable. In simulated
post-editing tasks, decoding (for both the test cor-
pus and each pass over the development corpus
during optimization) begins with baseline mod-
els trained on standard bilingual and monolingual
text. After each sentence is translated, the fol-
lowing take place in order: First, MIRA uses the
new source?reference pair to update weights for
the current models. Second, the source is aligned
to the reference using word-alignment models
learned from the initial data and used to update the
translation grammar. Third, the reference is added
to the Bayesian language model. As sentences are
translated, the models gain valuable context infor-
mation, allowing them to adapt to the specific tar-
get document and translator. Context is reset at the
start of each development or test corpus. Systems
optimized with simulated post-editing can then be
deployed to serve real human translators without
further modification.
3 cdec Realtime
Now included as part of the free, open source
cdec machine translation toolkit (Dyer et al.,
2010), Realtime
1
provides an efficient implemen-
tation of the adaptive MT paradigm that can serve
an arbitrary number of unique post-editors concur-
rently. A full Realtime tutorial, including step-
by-step instructions for installing required soft-
ware and building full adaptive systems, is avail-
1
https://github.com/redpony/cdec/tree/
master/realtime
73
import rt
# Start new Realtime translator using a Spanish--English
# system and automatic, language-independent text normalization
# (pre-tokenization and post-detokenization)
translator = rt.RealtimeTranslator(?es-en.d?, tmpdir=?/tmp?, cache_size=5,
norm=True)
# Translate a sentence for user1
translation = translator.translate(?Muchas gracias Chris.?, ctx_name=?user1?)
# Learn from user1?s post-edited transaltion
translator.learn(?Muchas gracias Chris.?, ?Thank you so much, Chris.?,
ctx_name=?user1?)
# Save, free, and reload state for user1
translator.save_state(file_or_stringio=?user1.state?, ctx_name=?user1?)
translator.drop_ctx(ctx_name=?user1?)
translator.load_state(file_or_stringio=?user1.state?, ctx_name=?user1?)
Figure 1: Sample code using the Realtime Python API to translate and learn from post-editing.
able online.
2
Building an adaptive system begins
with the usual MT pipeline steps: word alignment,
bitext indexing (for suffix array grammar extrac-
tion), and standard n-gram language model esti-
mation. Additionally, the cpyp
3
package, also
freely available, is used to estimate a Bayesian
n-gram language model on the target side of the
bitext. The cdec grammar extractor and dy-
namic language model implementations both in-
clude support for efficient inclusion of incremental
data, allowing optimization with simulated post-
editing to be parallelized. The resulting system,
optimized for post-editing, is then ready for de-
ployment with Realtime.
At runtime, a Realtime system operates as fol-
lows. A single instance of the indexed bitext is
loaded into memory for grammar extraction. Sin-
gle instances of the directional word alignment
models are loaded into memory for force-aligning
post-edited data. When a new user requests a
translation, a new context is started. The follow-
ing are loaded into memory: a table of all post-
edited data from the user, a user-specific dynamic
language model, and a user-specific decoder (in
this case an instance of MIRA that has a user-
specific decoder and set of weights). Each user
also requires an instance of the large static lan-
guage model, though all users effectively share a
single instance through the memory mapped im-
plementation of KenLM (Heafield, 2011). When a
2
http://www.cs.cmu.edu/
?
mdenkows/
cdec-realtime.html
3
https://github.com/redpony/cpyp
new sentence is to be translated, the grammar ex-
tractor samples from the shared background data
plus the user-specific post-editing data to generate
a sentence-specific grammar incorporating data
from all prior sentences translated by the same
user. The sentence is then decoded using the user
and time-specific grammar, current weights, and
current dynamic language model. When a post-
edited sentence is available as feedback, the fol-
lowing happen in order: (1) the source-reference
pair is used to update feature weights with MIRA,
(2) the source-reference pair is force-aligned and
added to the indexed post-editing data, and (3) the
dynamic language model is updated with the ref-
erence. User state (current weights and indexed
post-edited data for grammars and the language
model) can be saved and loaded, allowing mod-
els to be loaded and freed from memory as trans-
lators start and stop their work. Figure 1 shows
a minimal example of the above using the Real-
time package. While this paper describes integra-
tion with TransCenter, a tool primarily targeting
data collection and analysis, the Realtime Python
API allows straightforward integration with other
computer-assisted translation tools such as full-
featured translation workbench environments.
4 TransCenter: Web-Based Translation
Research Suite
The TransCenter software (Denkowski and Lavie,
2012) dramatically lowers barriers in post-editing
data collection and increases the accuracy and de-
scriptiveness of the collected data. TransCenter
74
Figure 2: Example of editing and rating machine translations with the TransCenter web interface.
Figure 3: Example TransCenter summary report for a single user on a document.
provides a web-based translation editing interface
that remotely monitors and records user activity.
The ?live? version
4
now uses cdec Realtime to
provide on-demand MT that automatically learns
from post-editor feedback. Translators use a web
browser to access a familiar two-column editing
environment (shown in Figure 2) from any com-
puter with an Internet connection. The left column
displays the source sentences, while the right col-
umn, initially empty, is incrementally populated
with translations from the Realtime system as the
user works. For each sentence, the translator ed-
its the MT output to be grammatically correct and
convey the same information as the source sen-
tence. During editing, all user actions (key presses
and mouse clicks) are logged so that the full edit-
ing process can be replayed and analyzed. After
editing, the final translation is reported to the Re-
altime system for learning and the next transla-
tion is generated. The user is additionally asked
to rate the amount of work required to post-edit
each sentence immediately after completing it,
yielding maximally accurate feedback. The rating
scale ranges from 5 (no post-editing required) to
1 (requires total re-translation). TransCenter also
records the number of seconds each sentence is
focused, allowing for exact timing measurements.
A pause button is available if the translator needs
to take breaks. TransCenter can generate reports
4
https://github.com/mjdenkowski/
transcenter-live
of translator effort as measured by (1) keystroke,
(2) exact timing, and (3) actual translator post-
assessment. Final translations are also available
for calculating edit distance. Millisecond-level
timing of all user actions further facilitates time
sequence analysis of user actions and pauses. Fig-
ure 3 shows an example summary report gener-
ated by TransCenter showing a user?s activity on
each sentence in a document. This information
is also output in a simple comma-separated value
format for maximum interoperability with other
standards-compliant tools.
TransCenter automatically handles resource
management with Realtime. When a TransCenter
server is started, it loads a Realtime system with
zero contexts into memory. As users log in to work
on documents, new contexts are created to deliver
on-demand translations. As users finish work-
ing or take extended breaks, contexts automati-
cally time out and resources are freed. Translator
and document-specific state is automatically saved
when contexts time out and reloaded when transla-
tors resume work with built-in safeguards against
missing or duplicating any post-editing data due
to timeouts or Internet connectivity issues. This
allows any number of translators to work on trans-
lation tasks at their convenience.
5 Experiments
In a preliminary experiment to evaluate the impact
of adaptive MT in real-world post-editing scenar-
75
HTER Rating
Baseline 19.26 4.19
Adaptive 17.01 4.31
Table 1: Aggregate HTER scores and average
translator self-ratings (5 point scale) of post-
editing effort for translations of TED talks from
Spanish into English.
ios, we compare a static Spanish?English MT sys-
tem to a comparable adaptive system on a blind
out-of-domain test. Competitive with the current
state-of-the-art, both systems are trained on the
2012 NAACL WMT (Callison-Burch et al., 2012)
constrained resources (2 million bilingual sen-
tences) using the cdec toolkit (Dyer et al., 2010).
Blind post-editing evaluation sets are drawn from
the Web Inventory of Transcribed and Translated
Talks (WIT
3
) corpus (Cettolo et al., 2012) that
makes transcriptions of TED talks
5
available in
several languages, including English and Spanish.
We select 4 excerpts from Spanish talk transcripts
(totaling 100 sentences) to be translated into En-
glish. Five students training to be professional
translators post-edit machine translations of these
excerpts using TransCenter. Translations are pro-
vided by either the static or fully adaptive system.
Tasks are divided such that each user translates
2 excerpts with the static system and 2 with the
adaptive system and each excerpt is post-edited ei-
ther 2 or 3 times with each system. Users do not
know which system is providing the translations.
Using the data collected by TransCenter, we
evaluate post-editing effort with the established
human-targeted translation edit rate (HTER) met-
ric (Snover et al., 2006). HTER computes an
edit distance score between initial MT outputs and
the ?targeted? references created by human post-
editing, with lower scores being better. Results
for the two systems are aggregated over all users
and documents. Shown in Table 1, introducing
an adaptive MT system results in a significant re-
duction in editing effort. We additionally aver-
age the user post-ratings for each translation by
system to evaluate user perception of the adap-
tive system compared to the static baseline. Also
shown in Table 1, we see a slight preference for
the adaptive system. This data, as well as precise
keystroke, mouse click, and timing information is
5
http://www.ted.com/talks
made freely available for further analysis.
6
Trans-
Center records all data necessary for more sophis-
ticated editing time analysis (Koehn, 2012) as well
as analysis of translator behavior, including pauses
(used as an indicator of cognitive effort) (Lacruz et
al., 2012).
6 Related Work
There has been a recent push for new computer-
aided translation (CAT) tools that leverage adap-
tive machine translation. The CASMACAT
7
project (Alabau et al., 2013) focuses on building
state-of-the-art tools for computer-aided transla-
tion. This includes translation predictions backed
by machine translation systems that incrementally
update model parameters as users edit translations
(Mart??nez-G?omez et al., 2012; L?opez-Salcedo et
al., 2012). The MateCat
8
project (Cattelan, 2013)
specifically aims to integrate machine translation
(including online model adaptation and translation
quality estimation) into a web-based CAT tool.
Bertoldi et al. (2013) show improvements in trans-
lator productivity when using the MateCat tool
with an adaptive MT system that uses cache-based
translation and language models.
7 Conclusion
This paper describes the free, open source MT
post-editing setup provided by cdec Realtime
and TransCenter. All software and the data col-
lected for a preliminary post-editing experiment
are all freely available online. A live demon-
stration of adaptive MT post-editing powered by
Realtime and TransCenter is scheduled for the
2014 EACL Workshop on Humans and Computer-
assisted Translation (HaCaT 2014).
Acknowledgements
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
6
www.cs.cmu.edu/
?
mdenkows/
transcenter-round1.tar.gz
7
http://casmacat.eu/
8
http://www.matecat.com/
76
References
Vicent Alabau, Ragnar Bonk, Christian Buck, Michael
Carl, Francisco Casacuberta, Mercedes Garc??a-
Mart??nez, Jes?us Gonz?alez-Rubio, Philipp Koehn,
Luis A. Leiva, Bartolom?e Mesa-Lao, Daniel Ortiz-
Mart??nez, Herv?e Saint-Amand, Germ?an Sanchis-
Trilles, and Chara Tsoukala. 2013. Casmacat:
An open source workbench for advanced computer
aided translation. In The Prague Bulletin of Mathe-
matical Linguistics, pages 101?112.
Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-
erico. 2013. Cache-based online adaptation for ma-
chine translation enhanced computer assisted trans-
lation. In Proceedings of the XIV Machine Transla-
tion Summit, pages 35?42.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Alessandro Cattelan. 2013. Second version of Mate-
Cat tool. Deliverable 4.2.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of the Sixteenth
Annual Conference of the European Association for
Machine Translation.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2012. pycdec: A python interface to cdec. The
Prague Bulletin of Mathematical Linguistics, 98:51?
61.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, pages 551?558, March.
Michael Denkowski and Alon Lavie. 2012. Trans-
Center: Web-based translation research suite. In
AMTA 2012 Workshop on Post-Editing Technology
and Practice Demo Session.
Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from post-editing: Online model
adaptation for statistical machine translation. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 480?489, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Daniel Hardt and Jakob Elming. 2010. Incremental
re-training for post-editing smt. In Proceedings of
the Ninth Conference of the Association for Machine
Translation in the Americas.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, United Kingdom,
July.
Philipp Koehn. 2012. Computer-aided translation.
Machine Translation Marathon.
Isabel Lacruz, Gregory M. Shreve, and Erik Angelone.
2012. Average Pause Ratio as an Indicator of Cogni-
tive Effort in Post-Editing: A Case Study. In AMTA
2012 Workshop on Post-Editing Technology and
Practice (WPTP 2012), pages 21?30, San Diego,
USA, October. Association for Machine Translation
in the Americas (AMTA).
Adam Lopez. 2008. Machine translation by pattern
matching. In Dissertation, University of Maryland,
March.
Francisco-Javier L?opez-Salcedo, Germ?an Sanchis-
Trilles, and Francisco Casacuberta. 2012. On-
line learning of log-linear weights in interactive ma-
chine translation. Advances in Speech and Lan-
guage Technologies for Iberian Languages, pages
277?286.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45:3193?
3203.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the 7th Conference of the.
Association for Machine Translation of the Ameri-
cas, pages 223?231.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. of ACL.
77
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142?149,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2014
Austin Matthews Waleed Ammar Archna Bhatia Weston Feely
Greg Hanneman Eva Schlinger Swabha Swayamdipta Yulia Tsvetkov
Alon Lavie Chris Dyer
?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?
Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submitted
to the 2014 WMT shared translation task.
We participated in two language pairs,
German?English and Hindi?English. Our
innovations include: a label coarsening
scheme for syntactic tree-to-tree transla-
tion, a host of new discriminative features,
several modules to create ?synthetic trans-
lation options? that can generalize beyond
what is directly observed in the training
data, and a method of combining the out-
put of multiple word aligners to uncover
extra phrase pairs and grammar rules.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute partici-
pated in two language pairs for the 2014 Workshop
on Machine Translation shared translation task:
German?English and Hindi?English. Our systems
showcase our multi-phase approach to translation,
in which synthetic translation options supple-
ment the default translation rule inventory that is
extracted from word-aligned training data.
In the German?English system, we used our
compound splitter (Dyer, 2009) to reduce data
sparsity, and we allowed the translator to back
off to translating lemmas when it detected case-
inflected OOVs. We also demonstrate our group?s
syntactic system with coarsened nonterminal types
(Hanneman and Lavie, 2011) as a contrastive
German?English submission.
In both the German?English and Hindi?English
systems, we used an array of supplemental ideas to
enhance translation quality, ranging from lemma-
tization and synthesis of inflected phrase pairs to
novel reordering and rule preference features.
2 Core System Components
The decoder infrastructure we used was cdec
(Dyer et al., 2010). For our primary systems,
all data was tokenized using cdec?s tokenization
tool. Only the constrained data resources pro-
vided for the shared task were used for training
both the translation and language models. Word
alignments were generated using both FastAlign
(Dyer et al., 2013) and GIZA++ (Och and Ney,
2003). All our language models were estimated
using KenLM (Heafield, 2011). Translation model
parameters were chosen using MIRA (Eidelman,
2012) to optimize BLEU (Papineni et al., 2002)
on a held-out development set.
Our data was filtered using qe-clean
(Denkowski et al., 2012), with a cutoff of
two standard deviations from the mean. All
data was left in fully cased form, save the first
letter of each segment, which was changed to
whichever form the first token more commonly
used throughout the data. As such, words like The
were lowercased at the beginning of segments,
while words like Obama remained capitalized.
Our primary German?English and Hindi?
English systems were Hiero-based (Chiang,
2007), while our contrastive German?English sys-
tem used cdec?s tree-to-tree SCFG formalism.
Before submitting, we ran cdec?s implementa-
tion of MBR on 500-best lists from each of our
systems. For both language pairs, we used the
Nelder?Mead method to optimize the MBR pa-
rameters. In the German?English system, we ran
MBR on 500 hypotheses, combining the output of
the Hiero and tree-to-tree systems.
The remainder of the paper will focus on our
primary innovations in the two language pairs.
142
3 Common System Improvements
A number of our techniques were used for both our
German?English and Hindi?English primary sub-
missions. These techniques each fall into one of
three categories: those that create translation rules,
those involving language models, or those that add
translation features. A comparison of these tech-
niques and their performance across the two lan-
guage pairs can be found in Section 6.
3.1 Rule-Centric Enhancements
While many of our methods of enhancing the
translation model with extra rules are language-
specific, three were shared between language
pairs.
First, we added sentence-boundary tokens <s>
and </s> to the beginning and end of each line in
the data, on both the source and target sides.
Second, we aligned all of our training data us-
ing both FastAlign and GIZA++ and simply con-
catenated two copies of the training corpus, one
aligned with each aligner, and extracted rules from
the resulting double corpus.
Third, we hand-wrote a list of rules that trans-
form numbers, dates, times, and currencies into
well-formed English equivalents, handling differ-
ences such as the month and day reversal in dates
or conversion from 24-hour time to 12-hour time.
3.2 Employed Language Models
Each of our primary systems uses a total of three
language models.
The first is a traditional 4-gram model gen-
erated by interoplating LMs built from each of
the available monolingual corpora. Interpolation
weights were calculated used the SRILM toolkit
(Stolcke, 2002) and 1000 dev sentences from the
Hindi?English system.
The second is a model trained on word clus-
ters instead of surface forms. For this we mapped
the LM vocabulary into 600 clusters based on the
algorithm of Brown et al. (1992) and then con-
structed a 7-gram LM over the resulting clusters,
allowing us to capture more context than our tra-
ditional surface-form language model.
The third is a bigram model over the source side
of each language?s respective bitext. However, at
run time this LM operates on the target-side out-
put of the translator, just like the other two. The
intuition here is that if a source-side LM likes our
output, then we are probably passing through more
than we ought to.
Both source and target surface-form LM used
modified Kneser-Ney smoothing (Kneser and Ney,
1995), while the model over Brown clusters
(Brown et al., 1992) used subtract-0.5 smoothing.
3.3 New Translation Features
In addition to the standard array of features, we
added four new indicator feature templates, lead-
ing to a total of nearly 150,000 total features.
The first set consists of target-side n-gram fea-
tures. For each bigram of Brown clusters in the
output string generated by our translator, we fire
an indicator feature. For example, if we have the
sentence, Nato will ihren Einfluss im Osten st?arken
translating as NATO intends to strengthen its influ-
ence in the East, we will fire an indicator features
NGF C367 C128=1, NGF C128 C31=1, etc.
The second set is source-language n-gram fea-
tures. Similar to the previous feature set, we fire
an indicator feature for each ngram of Brown clus-
ters in the output. Here, however, we use n = 1,
and we use the map of source language words to
Brown clusters, rather than the target language?s,
despite the fact that this is examining target lan-
guage output. The intuition here is to allow this
feature to penalize passthroughs differently de-
pending on their source language Brown cluster.
For example, passing through the German word
zeitung (?newspaper?) is probably a bad idea, but
passing through the German word Obama proba-
bly should not be punished as severely.
The third type of feature is source path features.
We can imagine translation as a two-step process
in which we first permute the source words into
some order, then translate them phrase by phrase.
This set of features examines that intermediate
string in which the source words have been per-
muted. Again, we fire an indicator feature for each
bigram in this intermediate string, this time using
surface lexical forms directly instead of first map-
ping them to Brown clusters.
Lastly, we create a new type of rule shape fea-
ture. Traditionally, rule shape features have indi-
cated, for each rule, the sequence of terminal and
non-terminal items on the right-hand side. For ex-
ample, the rule [X] ? der [X] :: the [X] might
have an indicator feature Shape TN TN, where
T represents a terminal and N represents a non-
terminal. One can also imagine lexicalizing such
rules by replacing each T with its surface form.
We believe such features would be too sparse, so
instead of replacing each terminal by its surface
form, we instead replace it with its Brown cluster,
143
creating a feature like Shape C37 N C271 N.
4 Hindi?English Specific Improvements
In addition to the enhancements common to the
two primary systems, our Hindi?English system
includes improved data cleaning of development
data, a sophisticated linguistically-informed tok-
enization scheme, a transliteration module, a syn-
thetic phrase generator that improves handling of
function words, and a synthetic phrase generator
that leverages source-side paraphrases. We will
discuss each of these five in turn.
4.1 Development Data Cleaning
Due to a scarcity of clean development data, we
augmented the 520 segments provided with 480
segments randomly drawn from the training data
to form our development set, and drew another
random 1000 segments to serve as a dev test set.
After observing large discrepencies between the
types of segments in our development data and the
well-formed news domain sentences we expected
to be tested on, we made the decision to prune our
tuning set by removing any segment that did not
appear to be a full sentence on both the Hindi and
English sides. While this reduced our tuning set
from 1000 segments back down to 572 segments,
we believe it to be the single largest contributor to
our success on the Hindi?English translation task.
4.2 Nominal Normalization
Another facet of our system was normalization of
Hindi nominals. The Hindi nominal system shows
much more morphological variation than English.
There are two genders (masculine and feminine)
and at least six noun stem endings in pronuncia-
tion and 10 in writing.
The pronominal system also is much richer than
English with many variants depending on whether
pronouns appear with case markers or other post-
positions.
Before normalizing the nouns and pronouns, we
first split these case markers / postpositions from
the nouns / pronouns to result in two words in-
stead of the original combined form. If the case
marker was n (ne), the ergative case marker in
Hindi, we deleted it as it did not have any trans-
lation in English. All the other postpositions were
left intact while splitting from and normalizing the
nouns and pronouns.
These changes in stem forms contribute to the
sparsity in data; hence, to reduce this sparsity, we
construct for each input segment an input lattice
that allows the decoder to use the split or original
forms of all nouns or pronouns, as well as allowing
it to keep or delete the case marker ne.
4.3 Transliteration
We used the 12,000 Hindi?English transliteration
pairs from the ACL 2012 NEWS workshop on
transliteration to train a linear-chained CRF tag-
ger
1
that labels each character in the Hindi token
with a sequence of zero or more English characters
(Ammar et al., 2012). At decoding, unseen Hindi
tokens are fed to the transliterator, which produces
the 100 most probable transliterations. We add
a synthetic translation option for each candidate
transliteration.
In addition to this sophisticated transliteration
scheme, we also employ a rule-based translitera-
tor that specifically targets acronyms. In Hindi,
many acronyms are spelled out phonetically, such
as NSA being rendered as enese (en.es.e). We
detected such words in the input segments and
generated synthetic translation options both with
and without periods (e.g. N.S.A. and NSA).
4.4 Synthetic Handling of Function Words
In different language pairs, individual source
words may have many different possible trans-
lations, e.g., when the target language word has
many different morphological inflections or is sur-
rounded by different function words that have no
direct counterpart in the source language. There-
fore, when very large quantities of parallel data
are not available, we can expect our phrasal inven-
tory to be incomplete. Synthetic translation option
generation seeks to fill these gaps using secondary
generation processes that exploit existing phrase
pairs to produce plausible phrase translation alter-
natives that are not directly extractable from the
training data (Tsvetkov et al., 2013; Chahuneau et
al., 2013).
To generate synthetic phrases, we first remove
function words from the source and target sides
of existing non-gappy phrase pairs. We manually
constructed English and Hindi lists of common
function words, including articles, auxiliaries, pro-
nouns, and adpositions. We then employ the
SRILM hidden-ngram utility (Stolcke, 2002) to re-
store missing function words according to an n-
gram language model probability, and add the re-
sulting synthetic phrases to our phrase table.
1
https://github.com/wammar/transliterator
144
4.5 Paraphrase-Based Synthetic Phrases
We used a graph-based method to obtain transla-
tion distributions for source phrases that are not
present in the phrase table extracted from the par-
allel corpus. Monolingual data is used to construct
separate similarity graphs over phrases (word se-
quences or n-grams), using distributional features
extracted from the corpora. The source similar-
ity graph consists of phrase nodes representing se-
quences of words in the source language. In our
instance, we restricted the phrases to bigrams, and
the bigrams come from both the phrase table (the
labeled phrases) and from the evaluation set but
not present in the phrase table (unlabeled phrases).
The labels for these source phrases, namely the
target phrasal inventory, can also be represented
in a graph form, where the distributional features
can also be computed from the target monolingual
data. Translation information is then propagated
from the labeled phrases to the unlabeled phrases
in the source graph, proportional to how similar
the phrases are to each other on the source side,
as well as how similar the translation candidates
are to each other on the target side. The newly
acquired translation distributions for the unlabeled
phrases are written out to a secondary phrase table.
For more information, see Saluja et al. (2014).
5 German?English Specific
Improvements
Our German?English system also had its own
suite of tricks, including the use of ?pseudo-
references? and special handling of morphologi-
cally inflected OOVs.
5.1 Pseudo-References
The development sets provided have only a sin-
gle reference, which is known to be sub-optimal
for tuning of discriminative models. As such,
we use the output of one or more of last year?s
top performing systems as pseudo-references dur-
ing tuning. We experimented with using just one
pseudo-reference, taken from last year?s Spanish?
English winner (Durrani et al., 2013), and with
using four pseudo-references, including the out-
put of last year?s winning Czech?English, French?
English, and Russian?English systems (Pino et al.,
2013).
5.2 Morphological OOVs
Examination of the output of our baseline sys-
tems lead us to conclude that the majority of our
system?s OOVs were due to morphologically in-
flected nouns in the input data, particularly those
in genitive case. As such, for each OOV in the
input, we attempt to remove the German genitive
case marker -s or -es. We then run the resulting
form f through our baseline translator to obtain a
translation e of the lemma. Finally, we add two
translation rules to our translation table: f ? e,
and f ? e?s.
6 Results
As we added each feature to our systems, we
first ran a one-off experiment comparing our base-
line system with and without each individual fea-
ture. The results of that set of experiments are
shown in Table 1 for Hindi?English and Table 2
for German?English. Features marked with a *
were not included in our final system submission.
The most surprising result is the strength of
our Hindi?English baseline system. With no extra
bells or whistles, it is already half a BLEU point
ahead of the second best system submitted to this
shared task. We believe this is due to our filter-
ing of the tuning set, which allowed our system to
generate translations more similar in length to the
final test set.
Another interesting result is that only one fea-
ture set, namely our rule shape features based on
Brown clusters, helped on the test set in both lan-
guage pairs. No feature hurt the BLEU score on
the test set in both language pairs, meaning the
majority of features helped in one language and
hurt in the other.
If we compare results on the tuning sets, how-
ever, some clearer patterns arise. Brown cluster
language models, n-gram features, and our new
rule shape features all helped.
Furthermore, there were a few features, such as
the Brown cluster language model and tuning to
Meteor (Denkowski and Lavie, 2011), that helped
substantially in one language pair while just barely
hurting the other. In particular, the fact that tuning
to Meteor instead of BLEU can actually help both
BLEU and Meteor scores was rather unexpected.
7 German?English Syntax System
In addition to our primary German?English sys-
tem, we also submitted a contrastive German?
English system showcasing our group?s tree-to-
tree syntax-based translation formalism.
145
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 15.7 25.3 68.0 11.4 22.9 70.3
*Meteor Tuning 15.2 25.8 71.3 12.8 23.7 71.3
Sentence Boundaries 15.2 25.4 69.1 12.1 23.4 70.0
Double Aligners 16.1 25.5 66.6 11.9 23.1 69.2
Manual Number Rules 15.7 25.4 68.5 11.6 23.0 70.3
Brown Cluster LM 15.6 25.1 67.3 11.5 22.7 69.8
*Source LM 14.2 25.1 72.1 11.3 23.0 72.3
N-Gram Features 15.6 25.2 67.9 12.2 23.2 69.2
Src N-Gram Features 15.3 25.2 68.9 12.0 23.4 69.5
Src Path Features 15.8 25.6 68.8 11.9 23.3 70.4
Brown Rule Shape 15.9 25.4 67.2 11.8 22.9 69.6
Lattice Input 15.2 25.8 71.3 11.4 22.9 70.3
CRF Transliterator 15.7 25.7 69.4 12.1 23.5 70.1
Acronym Translit. 15.8 25.8 68.8 12.4 23.4 70.2
Synth. Func. Words 15.7 25.3 67.8 11.4 22.8 70.4
Source Paraphrases 15.6 25.2 67.7 11.5 22.7 69.9
Final Submission 16.7
Table 1: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero Hindi?
English system. Each line is the baseline plus that one feature, non-cumulatively. Lines marked with a *
were not included in our final WMT submission.
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 25.3 30.4 52.6 26.2 31.3 53.6
*Meteor Tuning 26.2 31.3 53.1 26.9 32.2 54.4
Sentence Boundaries 25.4 30.5 52.2 26.1 31.4 53.3
Double Aligners 25.2 30.4 52.5 26.0 31.3 53.6
Manual Number Rules 25.3 30.3 52.5 26.1 31.4 53.4
Brown Cluster LM 26.4 31.0 51.9 27.0 31.8 53.2
*Source LM 25.8 30.6 52.4 26.4 31.5 53.4
N-Gram Features 25.4 30.4 52.6 26.7 31.6 53.0
Src N-Gram Features 25.3 30.5 52.5 26.2 31.5 53.4
Src Path Features 25.0 30.1 52.6 26.0 31.2 53.3
Brown Rule Shape 25.5 30.5 52.4 26.3 31.5 53.2
One Pseudo Ref 25.5 30.4 52.6 34.4 32.7 49.3
*Four Psuedo Refs 22.6 29.2 52.6 49.8 35.0 46.1
OOV Morphology 25.5 30.5 52.4 26.3 31.5 53.3
Final Submission 27.1
Table 2: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero
German?English system. Each line is the baseline plus that one feature, non-cumulatively.
Dev (2013) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 20.98 29.81 58.47 18.65 28.72 61.80
+ Label coarsening 23.07 30.71 56.46 20.43 29.34 60.16
+ Meteor tuning 23.48 30.90 56.18 20.96 29.60 59.87
+ Brown LM + Lattice + Synthetic 24.46 31.41 56.66 21.50 30.28 60.51
+ Span limit 15 24.20 31.25 55.48 21.75 29.97 59.18
+ Pseudo-references 24.55 31.30 56.22 22.10 30.12 59.73
Table 3: BLEU, Meteor, and TER results for experiments conducted in the tree-to-tree German?English
system. The system in the bottom line was submitted to WMT as a contrastive entry.
7.1 Basic System Construction
Since all training data for the tree-to-tree system
must be parsed in addition to being word-aligned,
we prepared separate copies of the training, tun-
ing, and testing data that are more suitable for in-
put into constituency parsing. Importantly, we left
the data in its original mixed-case format. We used
the Stanford tokenizer to replicate Penn Treebank
tokenization on the English side. On the German
side, we developed new in-house normalization
and tokenization script.
We filtered tokenized training sentences by sen-
146
tence length, token length, and sentence length ra-
tio. The final corpus for parsing and word align-
ment contained 3,897,805 lines, or approximately
86 percent of the total training resources released
under the WMT constrained track. Word align-
ment was carried out using FastAlign (Dyer et
al., 2013), while for parsing we used the Berke-
ley parser (Petrov et al., 2006).
Given the parsed and aligned corpus, we ex-
tracted synchronous context-free grammar rules
using the method of Hanneman et al. (2011).
In addition to aligning subtrees that natively ex-
ist in the input trees, our grammar extractor also
introduces ?virtual nodes.? These are new and
possibly overlapping constituents that subdivide
regions of flat structure by combining two adja-
cent sibling nodes into a single nonterminal for
the purposes of rule extraction. Virtual nodes
are similar in spirit to the ?A+B? extended cate-
gories of SAMT (Zollmann and Venugopal, 2006),
and their nonterminal labels are constructed in the
same way, but with the added restriction that they
do not violate any existing syntactic structure in
the parse tree.
7.2 Improvements
Nonterminals in our tree-to-tree grammar are
made up of pairs of symbols: one from the source
side and one from the target side. With virtual
nodes included, this led to an initial German?
English grammar containing 153,219 distinct non-
terminals ? a far larger set than is used in SAMT,
tree-to-string, string-to-tree, or Hiero systems. To
combat the sparsity introduce by this large nonter-
minal set, we coarsened the label set with an ag-
glomerative label-clustering technique(Hanneman
and Lavie, 2011; Hanneman and Lavie, 2013).
The stopping point was somewhat arbitrarily cho-
sen to be a grammar of 916 labels.
Table 3 shows a significant improvement in
translation quality due to coarsening the label set:
approximately +1.8 BLEU, +0.6 Meteor, and ?1.6
TER on our dev test set, newtest2012.
2
In the MERT runs, however, we noticed that the
length of the MT output can be highly variable,
ranging on the tuning set from a low of 92.8% of
the reference length to a high of 99.1% in another.
We were able to limit this instability by tuning to
Meteor instead of BLEU. Aside from a modest
2
We follow the advice of Clark et al. (2011) and eval-
uate our tree-to-tree experiments over multiple independent
MERT runs. All scores in Table 3 are averages of two or
three runs, depending on the row.
score improvement, we note that the variability in
length ratio is reduced from 6.3% to 2.8%.
Specific difficulties of the German?English lan-
guage pair led to three additional system compo-
nents to try to combat them.
First, we introduced a second language model
trained on Brown clusters instead of surface forms.
Next we attempted to overcome the sparsity
of German input by making use of cdec?s lattice
input functionality introduce compound-split ver-
sions of dev and test sentences.
Finally, we attempted to improve our grammar?s
coverage of new German words by introducing
synthetic rules for otherwise out-of-vocabulary
items. Each token in a test sentence that the gram-
mar cannot translate generates a synthetic rule al-
lowing the token to be translated as itself. The left-
hand-side label is decided heuristically: a (coars-
ened) ?noun? label if the German OOV starts with
a capital letter, a ?number? label if the OOV con-
tains only digits and select punctuation characters,
an ?adjective? label if the OOV otherwise starts
with a lowercase letter or a number, or a ?symbol?
label for anything left over.
The effect of all three of these improvements
combined is shown in the fourth row of Table 3.
By default our previous experiments were per-
formed with a span limit of 12 tokens. Increasing
this limit to 15 has a mixed effect on metric scores,
as shown in the fifth row of Table 3. Since two out
of three metrics report improvement, we left the
longer span limit in effect in our final system.
Our final improvement was to augment our tun-
ing set with the same set of pseudo-references
as our Hiero systems. We found that using one
pseudo-reference versus four pseudo-references
had negligible effect on the (single-reference) tun-
ing scores, but four produced a better improve-
ment on the test set.
The best MERT run of this final system (bottom
line of Table 3) was submitted to the WMT 2014
evaluation as a contrastive entry.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
147
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine transla-
tion systems for european language pairs.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 406?414. Association for Computational Lin-
guistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gis-
pert, Federico Flego, and William Byrne. 2013.
The university of cambridge russian-english system
at wmt13.
148
Avneesh Saluja, Hany Hassan, Kristina Toutanova, and
Chris Quirk. 2014. Graph-based semi-supervised
learning of translation models from monolingual
data. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Baltimore, Maryland, June.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
149
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376?380,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Meteor Universal: Language Specific Translation Evaluation
for Any Target Language
Michael Denkowski Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{mdenkows,alavie}@cs.cmu.edu
Abstract
This paper describes Meteor Universal, re-
leased for the 2014 ACL Workshop on
Statistical Machine Translation. Meteor
Universal brings language specific evalu-
ation to previously unsupported target lan-
guages by (1) automatically extracting lin-
guistic resources (paraphrase tables and
function word lists) from the bitext used to
train MT systems and (2) using a univer-
sal parameter set learned from pooling hu-
man judgments of translation quality from
several language directions. Meteor Uni-
versal is shown to significantly outperform
baseline BLEU on two new languages,
Russian (WMT13) and Hindi (WMT14).
1 Introduction
Recent WMT evaluations have seen a variety of
metrics employ language specific resources to
replicate human translation rankings far better
than simple baselines (Callison-Burch et al., 2011;
Callison-Burch et al., 2012; Mach?a?cek and Bojar,
2013; Snover et al., 2009; Denkowski and Lavie,
2011; Dahlmeier et al., 2011; Chen et al., 2012;
Wang and Manning, 2012, inter alia). While the
wealth of linguistic resources for the WMT lan-
guages allows the development of sophisticated
metrics, most of the world?s 7,000+ languages lack
the prerequisites for building advanced metrics.
Researchers working on low resource languages
are usually limited to baseline BLEU (Papineni et
al., 2002) for evaluating translation quality.
Meteor Universal brings language specific eval-
uation to any target language by combining lin-
guistic resources automatically learned from MT
system training data with a universal metric pa-
rameter set that generalizes across languages.
Given only the bitext used to build a standard
phrase-based translation system, Meteor Universal
learns a paraphrase table and function word list,
two of the most consistently beneficial language
specific resources employed in versions of Me-
teor. Whereas previous versions of Meteor require
human ranking judgments in the target language
to learn parameters, Meteor Universal uses a sin-
gle parameter set learned from pooling judgments
from several languages. This universal parameter
set captures general preferences shown by human
evaluators across languages. We show this ap-
proach to significantly outperform baseline BLEU
for two new languages, Russian and Hindi. The
following sections review Meteor?s scoring func-
tion (?2), describe the automatic extraction of lan-
guage specific resources (?3), discuss training of
the universal parameter set (?4), report experimen-
tal results (?5), describe released software (?6),
and conclude (?7).
2 Meteor Scoring
Meteor evaluates translation hypotheses by align-
ing them to reference translations and calculating
sentence-level similarity scores. For a hypothesis-
reference pair, the space of possible alignments is
constructed by exhaustively identifying all possi-
ble matches between the sentences according to
the following matchers:
Exact: Match words if their surface forms are
identical.
Stem: Stem words using a language appropriate
Snowball Stemmer (Porter, 2001) and match if the
stems are identical.
Synonym: Match words if they share member-
ship in any synonym set according to the WordNet
database (Miller and Fellbaum, 2007).
Paraphrase: Match phrases if they are listed as
376
paraphrases in a language appropriate paraphrase
table (described in ?3.2).
All matches are generalized to phrase matches
with a span in each sentence. Any word occur-
ring within the span is considered covered by the
match. The final alignment is then resolved as the
largest subset of all matches meeting the following
criteria in order of importance:
1. Require each word in each sentence to be
covered by zero or one matches.
2. Maximize the number of covered words
across both sentences.
3. Minimize the number of chunks, where a
chunk is defined as a series of matches that
is contiguous and identically ordered in both
sentences.
4. Minimize the sum of absolute distances be-
tween match start indices in the two sen-
tences. (Break ties by preferring to align
phrases that occur at similar positions in both
sentences.)
Alignment resolution is conducted as a beam
search using a heuristic based on the above cri-
teria.
The Meteor score for an aligned sentence pair is
calculated as follows. Content and function words
are identified in the hypothesis (h
c
, h
f
) and ref-
erence (r
c
, r
f
) according to a function word list
(described in ?3.1). For each of the matchers
(m
i
), count the number of content and function
words covered by matches of this type in the hy-
pothesis (m
i
(h
c
), m
i
(h
f
)) and reference (m
i
(r
c
),
m
i
(r
f
)). Calculate weighted precision and re-
call using matcher weights (w
i
...w
n
) and content-
function word weight (?):
P =
?
i
w
i
? (? ?m
i
(h
c
) + (1? ?) ?m
i
(h
f
))
? ? |h
c
|+ (1? ?) ? |h
f
|
R =
?
i
w
i
? (? ?m
i
(r
c
) + (1? ?) ?m
i
(r
f
))
? ? |r
c
|+ (1? ?) ? |r
f
|
The parameterized harmonic mean of P and R
(van Rijsbergen, 1979) is then calculated:
F
mean
=
P ?R
? ? P + (1? ?) ?R
To account for gaps and differences in word order,
a fragmentation penalty is calculated using the to-
tal number of matched words (m, averaged over
hypothesis and reference) and number of chunks
(ch):
Pen = ? ?
(
ch
m
)
?
The Meteor score is then calculated:
Score = (1? Pen) ? F
mean
The parameters?, ?, ?, ?, andw
i
...w
n
are tuned
to maximize correlation with human judgments.
3 Language Specific Resources
Meteor uses language specific resources to dra-
matically improve evaluation accuracy. While
some resources such as WordNet and the Snowball
stemmers are limited to one or a few languages,
other resources can be learned from data for any
language. Meteor Universal uses the same bitext
used to build statistical translation systems to learn
function words and paraphrases. Used in con-
junction with the universal parameter set, these re-
sources bring language specific evaluation to new
target languages.
3.1 Function Word Lists
The function word list is used to discriminate be-
tween content and function words in the target lan-
guage. Meteor Universal counts words in the tar-
get side of the training bitext and considers any
word with relative frequency above 10
?3
to be a
function word. This list is used only during the
scoring stage of evaluation, where the tunable ?
parameter controls the relative weight of content
versus function words. When tuned to match hu-
man judgments, this parameter usually reflects a
greater importance for content words.
3.2 Paraphrase Tables
Paraphrase tables allow many-to-many matches
that can encapsulate any local language phenom-
ena, including morphology, synonymy, and true
paraphrasing. Identifying these matches allows
far more sophisticated evaluation than is possible
with simple surface form matches. In Meteor Uni-
versal, paraphrases act as the catch-all for non-
exact matches. Paraphrases are automatically ex-
tracted from the training bitext using the transla-
tion pivot approach (Bannard and Callison-Burch,
2005). First, a standard phrase table is learned
from the bitext (Koehn et al., 2003). Paraphrase
extraction then proceeds as follows. For each tar-
get language phrase (e
1
) in the table, find each
377
source phrase f that e
1
translates. Each alternate
phrase (e
2
6= e
1
) that translates f is considered
a paraphrase with probability P (f |e
1
) ? P (e
2
|f).
The total probability of e
2
being a paraphrase of
e
1
is the sum over all possible pivot phrases f :
P (e
2
|e
1
) =
?
f
P (f |e
1
) ? P (e
2
|f)
To improve paraphrase precision, we apply
several language independent pruning techniques.
The following are applied to each paraphrase in-
stance (e
1
, f , e
2
):
? Discard instances with very low probability
(P (f |e
1
) ? P (e
2
|f) < 0.001).
? Discard instances where e
1
, f , or e
2
contain
punctuation characters.
? Discard instances where e
1
, f , or e
2
con-
tain only function words (relative frequency
above 10
?3
in the bitext).
The following are applied to each final paraphrase
(e
1
, e
2
) after summing over all instances:
? Discard paraphrases with very low probabil-
ity (P (e
2
|e
1
) < 0.01).
? Discard paraphrases where e
2
is a sub-phrase
of e
1
.
This constitutes the full Meteor paraphrasing
pipeline that has been used to build tables for
fully supported languages (Denkowski and Lavie,
2011). Paraphrases for new languages have the
added advantage of being extracted from the same
bitext that MT systems use for phrase extraction,
resulting in ideal paraphrase coverage for evalu-
ated systems.
4 Universal Parameter Set
Traditionally, building a version of Meteor for a
new target language has required a set of human-
scored machine translations, most frequently in
the form of WMT rankings. The general lack of
availability of these judgments has severely lim-
ited the number of languages for which Meteor
versions could be trained. Meteor Universal ad-
dresses this problem with the introduction of a
?universal? parameter set that captures general hu-
man preferences that apply to all languages for
Direction Judgments
cs-en 11,021
de-en 11,934
es-en 9,796
fr-en 11,594
en-cs 18,805
en-de 14,553
en-es 11,834
en-fr 11,562
Total 101,099
Table 1: Binary ranking judgments per language
direction used to learn parameters for Meteor Uni-
versal
which judgment data does exist. We learn this pa-
rameter set by pooling over 100,000 binary rank-
ing judgments from WMT12 (Callison-Burch et
al., 2012) that cover 8 language directions (de-
tails in Table 1). Data for each language is scored
using the same resources (function word list and
paraphrase table only) and scoring parameters are
tuned to maximize agreement (Kendall?s ? ) over
all judgments from all languages, leading to a sin-
gle parameter set. The universal parameter set en-
codes the following general human preferences:
? Prefer recall over precision.
? Prefer word choice over word order.
? Prefer correct translations of content words
over function words.
? Prefer exact matches over paraphrase
matches, while still giving significant credit
to paraphrases.
Table 2 compares the universal parameters to those
learned for specific languages in previous versions
of Meteor. Notably, the universal parameter set is
more balanced, showing a normalizing effect from
generalizing across several language directions.
5 Experiments
We evaluate the Universal version of Meteor
against full language dedicated versions of Meteor
and baseline BLEU on the WMT13 rankings. Re-
sults for English, Czech, German, Spanish, and
French are biased in favor of Meteor Universal
since rankings for these target languages are in-
cluded in the training data while Russian consti-
tutes a true held out test. We also report the re-
sults of the WMT14 Hindi evaluation task. Shown
378
Language ? ? ? ? w
exact
w
stem
w
syn
w
par
English 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60
Czech 0.95 0.20 0.60 0.80 1.00 ? ? 0.40
German 0.95 1.00 0.55 0.55 1.00 0.80 ? 0.20
Spanish 0.65 1.30 0.50 0.80 1.00 0.80 ? 0.60
French 0.90 1.40 0.60 0.65 1.00 0.20 ? 0.40
Universal 0.70 1.40 0.30 0.70 1.00 ? ? 0.60
Table 2: Comparison of parameters for language specific and universal versions of Meteor.
WMT13 ? M-Full M-Universal BLEU
English 0.214 0.206 0.124
Czech 0.092 0.085 0.044
German 0.163 0.157 0.097
Spanish 0.106 0.101 0.068
French 0.150 0.137 0.099
Russian ? 0.128 0.068
WMT14 ? M-Full M-Universal BLEU
Hindi ? 0.264 0.227
Table 3: Sentence-level correlation with human
rankings (Kendall?s ? ) for Meteor (language spe-
cific versions), Meteor Universal, and BLEU
in Table 3, Meteor Universal significantly out-
performs baseline BLEU in all cases while suf-
fering only slight degradation compared to ver-
sions of Meteor tuned for individual languages.
For Russian, correlation is nearly double that of
BLEU. This provides substantial evidence that
Meteor Universal will further generalize, bringing
improved evaluation accuracy to new target lan-
guages currently limited to baseline language in-
dependent metrics.
For the WMT14 evaluation, we use the tradi-
tional language specific versions of Meteor for all
language directions except Hindi. This includes
Russian, for which additional language specific re-
sources (a Snowball word stemmer) help signifi-
cantly. For Hindi, we use the release version of
Meteor Universal to extract linguistic resources
from the constrained training bitext provided for
the shared translation task. These resources are
used with the universal parameter set to score all
system outputs for the English?Hindi direction.
6 Software
Meteor Universal is included in Meteor version
1.5 which is publicly released for WMT14.
Meteor 1.5 can be downloaded from the official
webpage
1
and a full tutorial for Meteor Universal
is available online.
2
Building a version of Meteor
for a new language requires a training bitext
(corpus.f, corpus.e) and a standard Moses format
phrase table (phrase-table.gz) (Koehn et al.,
2007). To extract linguistic resources for Meteor,
run the new language script:
$ python scripts/new_language.py out \
corpus.f corpus.e phrase-table.gz
To use the resulting files to score translations with
Meteor, use the new language option:
$ java -jar meteor-
*
.jar test ref -new \
out/meteor-files
Meteor 1.5, including Meteor Universal, is free
software released under the terms of the GNU
Lesser General Public License.
7 Conclusion
This paper describes Meteor Universal, a version
of the Meteor metric that brings language specific
evaluation to any target language using the same
resources used to build statistical translation sys-
tems. Held out tests show Meteor Universal to sig-
nificantly outperform baseline BLEU on WMT13
Russian and WMT14 Hindi. Meteor version 1.5 is
freely available open source software.
Acknowledgements
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
1
http://www.cs.cmu.edu/~alavie/METEOR/
2
http://www.cs.cmu.edu/~mdenkows/meteor-
universal.html
379
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
597?604, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an mt evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 59?63, Montr?eal, Canada,
June. Association for Computational Linguistics.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.
2011. Tesla at wmt 2011: Translation evaluation and
tunable metric. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 78?
84, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL/HLT 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45?51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259?268, Athens, Greece, March. Association for
Computational Linguistics.
C. J. van Rijsbergen, 1979. Information Retrieval,
chapter 7. Butterworths, London, UK, 2nd edition.
Mengqiu Wang and Christopher Manning. 2012.
Spede: Probabilistic edit distance metrics for mt
evaluation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 76?
83, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
380
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196?206,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Domain and Dialect Adaptation
for Machine Translation into Egyptian Arabic
Serena Jeblee
1
, Weston Feely
1
, Houda Bouamor
2
Alon Lavie
1
, Nizar Habash
3
and Kemal Oflazer
2
1
Carnegie Mellon University
{sjeblee, wfeely, alavie}@cs.cmu.edu
2
Carnegie Mellon University in Qatar
hbouamor@qatar.cmu.edu, ko@cs.cmu.edu
3
New York University Abu Dhabi
nizar.habash@nyu.edu
Abstract
In this paper, we present a statistical ma-
chine translation system for English to Di-
alectal Arabic (DA), using Modern Stan-
dard Arabic (MSA) as a pivot. We cre-
ate a core system to translate from En-
glish to MSA using a large bilingual par-
allel corpus. Then, we design two separate
pathways for translation from MSA into
DA: a two-step domain and dialect adap-
tation system and a one-step simultane-
ous domain and dialect adaptation system.
Both variants of the adaptation systems are
trained on a 100k sentence tri-parallel cor-
pus of English, MSA, and Egyptian Arabic
generated by a rule-based transformation.
We test our systems on a held-out Egyp-
tian Arabic test set from the 100k sen-
tence corpus and we achieve our best per-
formance using the two-step domain and
dialect adaptation system with a BLEU
score of 42.9.
1 Introduction
While MSA is the shared official language of cul-
ture, media and education in the Arab world, it is
not the native language of any speakers of Ara-
bic. Most native speakers are unable to produce
sustained spontaneous discourse in MSA - they
usually resort to repeated code-switching between
their dialect and MSA (Abu-Melhim, 1991). Ara-
bic speakers are quite aware of the contextual fac-
tors and the differences between their dialects and
MSA, although they may not always be able to
pinpoint exact linguistic differences. In the con-
text of natural language processing (NLP), some
Arabic dialects have started receiving increas-
ing attention, particularly in the context of ma-
chine translation (Zbib et al., 2012; Salloum and
Habash, 2013; Salloum et al., 2014; Al-Mannai
et al., 2014) and in terms of data collection (Cot-
terell and Callison-Burch, 2014; Bouamor et al.,
2014; Salama et al., 2014) and basic enabling
technologies (Habash et al., 2012; Pasha et al.,
2014). However, the focus is on a small number
of iconic dialects, (e.g., Egyptian). The Egyptian
media industry has traditionally played a dominant
role in the Arab world, making the Egyptian di-
alect the most widely understood and used dialect.
DA is now emerging as the language of informal
communication online. DA differs phonologically,
lexically, morphologically, and syntactically from
MSA. And while MSA has an established stan-
dard orthography, the dialects do not: people write
words reflecting their phonology and sometimes
use roman script. Thus, MSA tools cannot ef-
fectively model DA; for instance, over one-third
of Levantine verbs cannot be analyzed using an
MSA morphological analyzer (Habash and Ram-
bow, 2006). These differences make the direct use
of MSA NLP tools and applications for handling
dialects impractical.
In this work, we design an MT system for En-
glish to Egyptian Arabic translation by using MSA
as an intermediary step. This includes different
challenges from those faced when translating into
English. Because MSA is the formal written va-
riety of Arabic, there is an abundance of written
data, including parallel corpora from sources like
the United Nations and newspapers, as well as var-
ious treebanks. Using these resources, many re-
searchers have created fairly reliable MSA trans-
lation systems. However, these systems are not
designed to deal with the other Arabic variants.
Egyptian Arabic is much closer to MSA than
it is to English, so one can get a system bet-
196
ter performance by translating first into MSA and
then translating from MSA to Egyptian Arabic,
which are far more similar. Our approach consists
of a core MT system trained on a large amount
of out-of-domain English-MSA parallel data, fol-
lowed by an adaptation system. We design and im-
plement two adaptation systems: a two-step sys-
tem first adapts to in-domain MSA and then sep-
arately adapts from MSA to Egyptian Arabic, and
a one-step system that adapts directly from out-of-
domain MSA to in-domain Egyptian Arabic.
Our research contributions are summarized as
follows:
(a) We build a machine translation system to
translate into, rather than out of, dialectal Ara-
bic (from English), using MSA as a pivot
point.
(b) We apply a domain adaptation technique to
improve the MSA results on our in-domain
dataset.
(c) We automatically generate the Egyptian side
of a 100k tri-parallel corpus covering MSA,
English and Egyptian.
(d) We use this domain adaptation technique to
adapt MSA into dialectal Arabic.
The remainder of this paper is structured as fol-
lows. We first review the main previous efforts
for dealing with DA in NLP, in Section 2. In Sec-
tion 3,we give a general description about using
phrase-based MT as an adaptation system. Sec-
tion 4 presents the dataset used in the different ex-
periments. Our approach for translating English
text into Egyptian Arabic is explained in Section 5.
Section 6 presents our experimental setup and the
results obtained. Then, we give an analysis of our
system output in Section 7. Finally, we conclude
and describe our future work in Section 8.
2 Related work
Machine translation (MT) for dialectal Arabic
(DA) is quite challenging given the limited re-
sources to build rule-based models or train statis-
tical models for MT. While there has been a con-
siderable amount of work in the context of stan-
dard Arabic NLP (Habash, 2010), DA is impov-
erished in terms of available tools and resources
compared to MSA, e.g., there are few parallel DA-
English corpora (Zbib et al., 2012; Bouamor et
al., 2014). The majority of DA resources are for
speech recognition, although more and more re-
sources for machine translation and enabling tech-
nologies such as morphological analyzers are be-
coming available for specific dialects (Habash et
al., 2012; Habash et al., 2013).
For Arabic and its dialects, several researchers
have explored the idea of exploiting existing MSA
rich resources to build tools for DA NLP. Differ-
ent research work successfully translated DA to
MSA as a bridge to translate to English (Sawaf,
2010; Salloum and Habash, 2013), or to enhance
the performance of Arabic-based information re-
trieval systems (Shatnawi et al., 2012). Among the
efforts on translation from DA to MSA, Abo Bakr
et al. (2008) introduced a hybrid approach to trans-
fer a sentence from Egyptian Arabic to MSA.
Sajjad et al. (2013) used a dictionary of Egyp-
tian/MSA words to transform Egyptian to MSA
and showed improvement in the quality of ma-
chine translation. A similar but rule-based work
was done by Mohamed et al. (2012). Boujel-
bane et al. (2013) and Hamdi et al. (2014) built
a bilingual dictionary using explicit knowledge
about the relation between Tunisian Arabic and
MSA. These works are limited to a dictionary or
rules that are not available for all dialects. Zbib
et al. (2012) used crowdsourcing to translate sen-
tences from Egyptian and Levantine into English,
and thus built two bilingual corpora. The dialec-
tal sentences were selected from a large corpus
of Arabic web text. Then, they explored several
methods for dialect/English MT. Their best Egyp-
tian/English system was trained on dialect/English
parallel data. They argued that differences in genre
between MSA and DA make bridging through
MSA of limited value. For this reason, while piv-
oting through MSA, it is important to consider the
domain and add an additional step: domain adap-
tation.
The majority of previous efforts in DA MT has
been focusing on translating from dialectal Arabic
into other languages (mainly MSA or English). In
contrast, in this work we focus on building a sys-
tem to translate from English and MSA into DA.
Furthermore, to the best of our knowledge, this is
the first work in which we adapt the domain in ad-
dition to the dialect (Egyptian specifically).
3 Using Phrase-Based MT as an
Adaptation System
For commercial use, MT output is usually post-
edited by a human translator in order to fix the er-
rors generated by the MT system. This is often
faster and cheaper than having a human translate
197
the document from scratch. However, we can ap-
ply statistical phrase-based MT to create an auto-
matic machine post-editor (what we refer to in this
paper as an adaptation system) to improve the out-
put of an MT system, and make it more closely
resemble the references. Simard et al. (2007) used
a phrase-based MT system as an automatic post-
editor for the output of a commercial rule-based
MT system, showing that it produced better results
than both the rule-based system alone and a sin-
gle pass phrase-based MT system. This technique
is also useful for adapting to a specific domain or
dataset. Isabelle et al. (2007) used a statistical MT
system to automatically post-edit the output of a
generic rule-based MT system, to avoid manually
customizing a system dictionary and to reduce the
amount of manual post-editing required.
For our adaptation systems, we build a core
phrase-based MT system with a large amount of
out-of-domain data, which allows us to have better
coverage of the target language. For an adaptation
system, we then build a second phrase-based MT
system by translating the in-domain train, tune,
and test sets through the core translation system,
then using that data to build the second system.
This system uses only in-domain data: parallel
MT output from the core and the references. In
this system, instead of learning to translate one
language into another, the model learns to trans-
late erroneous MT output into more fluent output
of the same language, which more closely resem-
bles the references.
In this work, we apply this technique for
domain and dialect adaptation, treating Egyp-
tian Arabic as the target language, and the MT-
generated MSA as the erroneous MT output. We
use this approach to adapt to the domain of the
MSA data, and also to adapt to the Egyptian di-
alect. What we refer to as the ?one-step? system is
a core system plus one adaptation system, whereas
the ?two-step? system consists of the core plus two
subsequent adaptation systems. We describe the
systems in more detail in Section 5.
4 Data
For the core English to MSA system, we use
the 5 million parallel sentences of English and
MSA from NIST 2012 as the training set. The
tuning set consists of 1,356 sentences from the
NIST 2008 Open Machine Translation Evalua-
tion (MT08) data (NIST Multimodal Information
Group, 2010a), and the test set consists of 1,313
sentences from NIST MT09 (NIST Multimodal
Information Group, 2010b).
We use a 5-gram MSA language model built
using the SRILM toolkit (Stolcke, 2002) on 260
million words of MSA from the Arabic Gigaword
(Parker et al., 2011). All our MSA parallel data
and monolingual MSA language modeling data
were tokenized with MADA v3.1 (Habash and
Rambow, 2005) using the ATB (Arabic Treebank)
tokenization scheme.
For the adaptation systems, we build a 100k
tri-parallel corpus Egyptian-MSA-English corpus.
The MSA and English parts are extracted from
the NIST corpus distributed by the Linguistic Data
Consortium. The Egyptian sentences are obtained
automatically by extending Mohamed et al. (2012)
method for generating Egyptian Arabic from mor-
phologically disambiguated MSA sentences. This
rule-based method relies on 103 transformation
rules covering essentially nouns, verbs and pro-
nouns as well as certain lexical items. For each
MSA sentence, this method provides more than
one possible candidate, in its original version, the
Egyptian sentence kept was chosen randomly. We
extend the selection algorithm by scoring the dif-
ferent sentences using a language model. For
this, we use SRILM with modified Kneser-Ney
smoothing to build a 5-gram language model. The
model is trained on a corpus including articles ex-
tracted from the Egyptian version of Wikipedia
1
and the Egyptian side of the AOC corpus (Zaidan
and Callison-Burch, 2011). We chose to include
Egyptian Wikipedia for the formal level of sen-
tences in it different from the regular DA written
in blogs or microblogging websites (e.g., Twitter)
and closer to the ones generated by our system.
We split this data into train, tune, and test sets
of 98,027, 960, and 961 sentences respectively,
after removing duplicates across sets. The MSA
corpus was tokenized using MADA and the Egyp-
tian Arabic data was tokenized with MADA-ARZ
v0.4 (Habash et al., 2013), both using the ATB to-
kenization scheme, with alif/ya normalization.
5 System Design
Figure 1 shows a diagram of our three English to
Egyptian Arabic MT systems: (1) the baseline MT
system, (2) the one-step adaptation MT system,
and (3) the two-step adaptation MT system. We
describe each system below.
1
http://arz.wikipedia.org/
198
System Design
Egyptian ArabicEnglish
English
English Egyptian Arabic
Egyptian Arabic
MSA
Translation
Translation
Translation
Domain & Dialect Adaptation
Domain Adaptation Dialect AdaptationIn-domain MSAMSA
100K sent.
100K sent. 100K sent.
100K sent.5M sent.
5M sent.
Baseline MT System
One-Step Adaptation MT System
Two-Step Adaptation MT System
Figure 1: An overview of the different system architectures.
Baseline System
Our baseline system is a single phrase-based En-
glish to Egyptian Arabic MT system, built using
Moses (Koehn et al., 2007) on the 100k corpus de-
scribed in Section 4. This system does not include
any MSA data, nor does it have an adaptation sys-
tem; it is a typical, one-pass MT system that trans-
lates English directly into Egyptian Arabic. We
will show that using adaptation systems improves
the results significantly.
Core System
We base our systems on a core system built us-
ing Moses with the NIST data, a large amount of
parallel English-MSA data from different sources
than our in-domain data (the 100k dataset). Our
core system is also built using Moses. We use
this core system to translate the English side of our
100k train, tune, and test sets into MSA, the output
of which we refer to as MSA?. This MSA? data is
what we use as the source side for the adaptation
systems.
One-Step Adaptation System
To adapt to the domain and dialect of the 100k cor-
pus, we first build a single adaptation system that
translates the MSA? output of the core directly into
Egyptian Arabic using the 100k corpus. The train-
ing data consists of parallel MSA? (the output of
the core) and the Egyptian Arabic from the 100k
dataset. With this system, we can take an English
test set, translate it through the core to get MSA?
output, which we can translate through the adap-
tation system to get Egyptian Arabic.
Two-Step Adaptation System
We also build a two-step adaptation system that
consists of two adaptation steps: one to adapt the
MSA output of the core system to the domain of
the MSA in the 100k corpus, and a second system
to translate the MSA output of the domain adap-
tation system into Egyptian Arabic. We use the
first adaptation system to translate the MSA? train,
tune, and test sets (the output of the core, which is
out-of-domain MSA), into in-domain MSA. This
system is trained on the MSA? output parallel with
the MSA references from the 100k dataset. We
refer to the output of this system as MSA?, be-
cause it has been translated from English into out-
of-domain MSA (MSA?), and then from out-of-
domain MSA to in-domain MSA.
The first adaptation system is used to translate
the MSA? train, tune, and test sets into MSA?.
Then we use these MSA? sets with their parallel
Egyptian Arabic from the 100k dataset to build the
second adaptation system from in-domain MSA to
Egyptian Arabic. We do not use the dialect trans-
formation from (Mohamed et al., 2012) because it
is designed to work with gold-standard annotation
of the MSA text, which we do not have.
System Variants
Since MSA and Egyptian are more similar to each
other than they are to English, we tried several dif-
ferent reordering window sizes to find the optimal
reordering distance for adapting MSA to Egyptian
Arabic, including the typical reordering window
of length 7, a smaller window of length 4, and no
reordering at all. We found a reordering window
199
size of 7 to work best for all our systems, except
for the one-step adaptation system, where no re-
ordering produced the best result.
We also tested two different heuristics for sym-
metrizing the word alignments: grow-diag and
grow-diag-final-and (Och and Ney, 2003). We
found that using grow-diag as our symmetriza-
tion heuristic produced slightly better scores on
the 100k datasets. For the baseline and adaptation
systems we built 5-gram language models with
KenLM (Heafield et al., 2013) using the target side
of the training set, and for the core system we used
the large MSA language model described in sec-
tion 4. We use KenLM because it has been shown
(Heafield, 2011) to be faster and use less memory
than SRILM (Stolcke, 2002) and IRSTLM (Fed-
erico et al., 2008).
6 Evaluation and Results
For evaluation we use multeval (Clark et al.,
2011) to calculate BLEU (Papineni et al.,
2002), METEOR (Denkowski and Lavie, 2011),
TER (Snover et al., 2006), and length of the test set
for each system. We evaluate the core and adap-
tation systems on the MSA and Egyptian sides of
the test set drawn from the 100k corpus, which we
refer to as the 100k sets. The data used for evalua-
tion is a genuine Egyptian Arabic generated from
MSA, just like the data the systems were trained
on. It is not practical to evaluate on naturally-
generated Egyptian Arabic in this case because the
domain of our datasets is very formal, since most
of the text comes from news sources, and dialectal
Arabic is generally used in informal situations.
2
Below we report BLEU scores from our evalua-
tion using tokenized and detokenized system out-
put. We separate our results into the baseline sys-
tem results, the results of the core, the results of
the adaptation systems, and a comparison section.
We specify scores of intermediate system output,
such as MSA, as BLEU (A), and the scores of fi-
nal system output as BLEU (B). For error analysis,
we use METEOR X-ray (Denkowski and Lavie,
2011) to visualize the alignments of our system
results with the references and each other.
For all MT systems we used grow-diag as our
symmetrization heuristic. For each system, we re-
port only the BLEU score of the best reordering
window variant, which is specified in the caption
2
It is important to note that the Egyptian Arabic data we
use is more MSA-like than typical Egyptian because it was
generated directly from MSA.
below each table. The difference in scores be-
tween the different reordering window sizes (7, 4,
and 0) we tried for the adaptation systems was not
large (between 0 and 0.7 BLEU). In the following
tables we present the best results for each adapta-
tion system, which is a reordering window size of
7 for all systems, except for the phrase-based one-
step domain and dialect adaptation system, which
performs better with no reordering (0.2 BLEU bet-
ter than a window of 7, 0.6 BLEU better than a
window of 4), but these small differences in BLEU
scores are within noise. The greatest difference
in scores from the reordering windows was in the
two-step systems domain adaptation step (MSA to
MSA) on top of the phrase-based core, where a re-
ordering window of 7 was 0.7 BLEU better than a
window of 0.
6.1 Baseline System Results
BLEU (B)
Tokenized Detokenized
100k EGY Tune 22.6 22.3
100k EGY Test 21.5 21.1
Table 1: Baseline results (English ? EGY) with a
reordering window size of 7.
The baseline system demonstrates the results of
building a basic MT system directly from English
to Egyptian Arabic. The goal of the core and adap-
tation systems is to achieve better scores than this
initial approach.
6.2 Core System Results
In Table 2, we report BLEU scores for our core
system on its own tuning set, NIST MT08, and
NIST MT09 as a held-out MSA test set. We
also report scores on the tune and test sets used
to build our adaptation systems, both MSA and
Egyptian Arabic. This is not the final system out-
put, but rather these scores are for intermediate
output only, which becomes the input for our ada-
patation systems.
We notice that unsurprisingly the core system
performs much better on the 100k MSA test set
than on the 100k Egyptian Arabic test set, which
is to be expected because the core system is not
trained on any Egyptian Arabic data. This shows
the impact that the dialectal differences make on
MT output. The results on the Egyptian test
set here are the result of evaluating MSA output
against Egyptian Arabic references.
200
BLEU (A)
Tokenized Detokenized
NIST MT08 (Tune) 23.6 22.8
NIST MT09 (Test) 29.3 28.5
100k MSA Tune 39.8 39.3
100k MSA Test 39.4 39.0
100k EGY Tune 28.1 28.1
100k EGY Test 27.7 27.7
Table 2: Core system (English ? MSA) results
using a reordering window size of 7.
6.3 Adaptation System Results
The adaptation systems take as input the MSA out-
put of the core and attempt to improve the scores
on the Egyptian test set by adapting to the domain
of the 100k dataset, as well as to Egyptian Arabic,
in either one or two steps.
BLEU (B)
Tokenized Detokenized
100k EGY Tune 40.8 40.5
100k EGY Test 40.3 40.1
Table 3: One-Step Adaptation system (MSA? ?
Egyptian Arabic) results using a reordering win-
dow size of 0.
Table 3 shows the results of the single adapta-
tion system, which adapts directly from the MSA
output of the core to Egyptian Arabic. These
BLEU scores are already much better than the core
systems performance on the same test sets, im-
proving from 28.1 BLEU to 40.5 BLEU on the
Egyptian Arabic tuning set (a difference of 12.4
BLEU) and improving from 22.7 BLEU to 40.1
BLEU on the Egyptian Arabic test set (a differ-
ence of 17.4 BLEU).
Tables 4 and 5 below illustrate the results of the
first and second steps of the two-step adaptation
system: Table 4 contains the results of the first do-
main adaptation step from out-of-domain MSA to
in-domain MSA and Table 5 contains the results of
the second dialect adaptation step from in-domain
MSA to Egyptian Arabic.
An example of our system output for an English
sentence is given in Table 6. Its METEOR X-ray
alignment is illustrated in Figure 2.
6.4 System Comparisons on 100k Test Sets
In Table 7, we compare the results from the core
and the results from the first step of the two-step
BLEU (A)
Tokenized Detokenized
100k MSA Tune 45.2 44.6
100k MSA Test 44.8 44.2
100k EGY Tune 32.2 32.2
100k EGY Test 32.0 32.0
Table 4: Domain Adaptation system (MSA? ?
MSA?) for Two-Step Adaptation System Results
using a reordering window size of 7.
BLEU (B)
Tokenized Detokenized
100k EGY Tune 43.3 43.2
100k EGY Test 43.1 42.9
Table 5: Dialect Adaptation system (MSA? ?
Egyptian) for Two-Step Adaptation System Re-
sults using a reordering window size of 7.
? ? ????? ??????
?
????? ?????? ?????? ?? ??????? ??????
??
????? ????? ??????? ? ???????????? ? ???????????? ? ????????? ? ????????????? ????????? ? ???? ????? ? ?? ????????????? ? ????????????? ? ?????????? ? ?????Segment 314
P: 0.700 vs 0.900 : 0.200R: 0.700 vs 0.900 : 0.200Frag: 0.214 vs 0.085 : 0.129-Score: 0.550 vs 0.823 : 0.273
Figure 2: METEOR X-ray alignment of the sen-
tence in table 6. The left side is the output of the
one-step system, the right side is the output of the
two-step system, and the top is the reference. The
shaded cells represent matches between the refer-
ence and the one-step system, and the dots repre-
sent matches between the reference and the two-
step system.
adaptation system on the MSA test set and we
see that adapting to the domain improves BLEU
scores on MSA.
Since our goal is to improve the output for
1
One-Step System: Core + Domain and Dialect Adapta-
tion (MSA? ? EGY)
2
Two Step Adaptation System (Step 1): Core + Domain
Adaptation (MSA? ? MSA?)
3
Two Step Adaptation System (Step 2): Core + Domain
Adaptation (MSA??MSA?) + Dialect Adaptation (MSA??
EGY)
201
English UN closes old office in Liberia in preparation for new mission
Egyptian Reference

?YK


Yg
.

????
?
@X @Y?

J?@

?K


Q



J
.
J


? ?


	
?

?K
.
A??@ A?D
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktbhA AAlsAbq fy lybyryp AAstEdAdA lmhmp jdydp
1-Step System

?YK


Yg
.

????
?
@X @Y?

J?@ AK


Q



J
.
J


? ?


	
?

??
?

Y

??@ I
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktb AAlqdymp fy lybyryA AAstEdAdA lmhmp jdydp
2-Step System (step2)

?YK


Yg
.

????
?
@X @Y?

J?@

?K


Q



J
.
J


? ?


	
?

??
?

Y

??@ A?D
.

J??

??
	
?

JK
.

?Yj

J?
?
@ ??B@
AAlAmm AAlmtHdp btglq mktbhA AAlqdymp fy lybyryp AAstEdAdA lmhmp jdydp
Table 6: An example of system output from the Egyptian test set.
BLEU (A)
Tokenized Detokenized
Core (English ? MSA?) 39.4 39.0
Core + Domain Adaptation (MSA? ? MSA?) 44.8 44.2
Table 7: Comparison of results on 100k MSA test set.
BLEU (A/B)
Tokenized Detokenized
Baseline (English ? EGY) 21.5 (B) 21.1
Core (English ?MSA
?
) 27.7 (A) 27.7
One-Step Adaptation System
1
40.3 (B) 40.1
Two-Step Adaptation System (Step 1)
2
32.0 (A) 32.0
Two-Step Adaptation System (Step 2)
3
43.1 (B) 42.9
Table 8: Comparison of results on 100k EGY test data.
BLEU (B) METEOR TER Length
Baseline System 21.1 38.5 66.1 102.7
One-Step System 40.1 53.4 51.3 100.0
Two-Step System: Step 2 (Dialect) 42.9 55.2 50.4 100.1
Table 9: Detokenized BLEU, METEOR, TER, and length scores for the best system results.
Egyptian Arabic, we examine the improvement of
scores through different steps of the system in Ta-
ble 8. These scores are all based on the same
Egyptian Arabic references, even though some of
the systems are designed to produce MSA output.
It is important to note that although the first step
of the two-step adaptation system (domain adap-
tation) is still producing MSA output, it performs
better on the Egyptian test set than the out-of-
domain MSA core. The domain adaptation sys-
tem built on top of the core performs better than
the core alone on the 100k corpus MSA test set
(+5.2 BLEU), as well as the 100k corpus Egyptian
Arabic test set (+4.3 BLEU). The best score we
achieve on the 100k corpus MSA test set is 44.2
BLEU, from the core plus the domain adaptation
system.
Table 9 shows the other detokenized scores
from multeval (Clark et al., 2011) from the final
output on the EGY test set from each system, and
Table 10 shows BLEU-1 through BLEU-4 scores
on the same detokenized results, which shows an
improvement at different n-gram levels in unigram
coverage from the baseline system to the adapta-
tion systems.
Overall, the two-step adaptation system built on
top of the core performs 15.2 BLEU better than
the core alone on the 100k corpus Egyptian Ara-
bic test set and the one-step adaptation system per-
forms 12.4 BLEU better than the core on the same
test set. The best score on the 100k EGY test set
is from the two-step adaptation system with 42.9
BLEU, which outperforms the one-step adaptation
system by 2.8 BLEU points. We consider possible
causes of these results in section 7.
202
BLEU-1 BLEU-2 BLEU-3 BLEU-4
Baseline System 53.4 26.6 15.3 9.1
One-Step System 64.3 43.5 33.5 27.1
Two-Step System: Step 2 (Dialect) 65.2 46.0 36.8 30.7
Table 10: Detokenized BLEU (B) scores on the 100k EGY test set at different n-gram levels.
English US , Indonesia commit to closer trade , investment ties
Egyptian Reference

?

K?@

?K


PA?

J

?@?

?K


PAm
.
