Domain Portability in Speech-to-Speech Translation
Alon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin Han
Alicia Tribble, Donna Gates, Dorcas Wallace and Kay Peterson
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
1. INTRODUCTION
Speech-to-speech translation has made significant advances over
the past decade, with several high-visibility projects (C-STAR, Verb-
mobil, the Spoken Language Translator, and others) significantly
advancing the state-of-the-art. While speech recognition can cur-
rently effectively deal with very large vocabularies and is fairly
speaker independent, speech translation is currently still effective
only in limited, albeit large, domains. The issue of domain porta-
bility is thus of significant importance, with several current research
efforts designed to develop speech-translation systems that can be
ported to new domains with significantly less time and effort than
is currently possible.
This paper reports on three experiments on portability of a speech-
to-speech translation system between semantic domains.1 The ex-
periments were conducted with the JANUS system [5, 8, 12], ini-
tially developed for a narrow travel planning domain, and ported
to the doctor-patient domain and an extended tourism domain. The
experiments cover both rule-based and statistical methods, and hand-
written as well as automatically learned rules. For rule-based sys-
tems, we have investigated the re-usability of rules and other knowl-
edge sources from other domains. For statistical methods, we have
investigated how much additional training data is needed for each
new domain. We are also experimenting with combinations of
hand-written and automatically learned components. For speech
recognition, we have conducted studies of what parameters change
when a recognizer is ported from one domain to another, and how
these changes affect recognition performance.
2. DESCRIPTION OF THE INTERLINGUA
The first two experiments concern the analysis component of our
interlingua-based MT system. The analysis component takes a sen-
tence as input and produces an interlingua representation as output.
We use a task-oriented interlingua [4, 3] based on domain actions.
Examples of domain actions are giving information about the on-
set of a symptom (e.g., I have a headache) or asking a patient
1We have also worked on the issue of portability across languages
via our interlingua approach to translation [3] and on portability of
speech recognition across languages [10].
.
to perform some action (e.g., wiggle your fingers). The interlin-
gua, shown in the example below, has five main components: (1) a
speaker tag such as a: for doctor (agent) and c: for a patient (cus-
tomer), (2) a speech act, in this case, give-information (3)
some concepts (+body-state and+existence), and (4) some
arguments (body-state-spec= andbody-location=), and
(5) some sub-arguments (identifiability=no and
inside=head).
I have a pain in my head.
c:give-information+existence+body-state
(body-state-spec=(pain,identifiability=no),
body-location=(inside=head))
3. EXPERIMENT 1:
EXTENSION OF SEMANTIC GRAMMAR
RULES BY HAND AND BY AUTOMATIC
LEARNING
Experiment 1 concerns extension of the coverage of semantic
grammars in the medical domain. Semantic grammars are based
on semantic constituents such as request information phrases (e.g.,
I was wondering : : : ) and location phrases (e.g., in my right arm)
rather than syntactic constituents such as noun phrases and verb
phrases. In other papers [12, 5], we have described how our mod-
ular grammar design enhances portability across domains. The
portable grammar modules are the cross-domain module, contain-
ing rules for things like greetings, and the shared module, contain-
ing rules for things like times, dates, and locations. Figure 1 shows
a parse tree for the sentence How long have you had this pain? XDM
indicates nodes that were produced by cross-domain rules. MED in-
dicates nodes that were produced by rules from the new medical
domain grammar.
The preliminary doctor-patient grammar focuses on three med-
ical situations: give-information+existence ? giving
information about the existence of a symptom (I have been get-
ting headaches); give-information+onset ? giving infor-
mation about the onset of a symptom (The headaches started three
months ago); and give-information+occurrence ? giv-
ing information about the onset of an instance of the symptoms
(The headaches start behind my ears). Symptoms are expressed
as body-state (e.g., pain), body-object (e.g., rash), and
body-event (e.g., bleeding).
Our experiment on extendibility was based on a hand written
seed grammar that was extended by hand and by automatic learn-
ing. The seed grammar covered the domain actions mentioned
above, but did not cover very many ways to phrase each domain
action. For example, it might have covered The headaches started
[request-information+existence+body-state]::MED
( WH-PHRASES::XDM
( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )
HAVE-GET-FEEL::MED ( GET ( have ) ) you
HAVE-GET-FEEL::MED ( HAS ( had ) )
[super_body-state-spec=]::MED
( [body-state-spec=]::MED
( ID-WHOSE::MED
( [identifiability=]
( [id:non-distant] ( this ) ) )
BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )
Figure 1: Parser output with nodes produced by medical and cross-domain grammars.
Seed Extended Learned
IF 37.2 37.2 31.3
Domain Action 37.2 37.2 31.3
Speech Act
Recall 43.3 48.2 49.3
Precision 71.0 75.0 45.8
Concept List
Recall 2.2 10.1 32.5
Precision 12.5 42.2 25.1
Top-Level Arguments
Recall 0.0 7.2 29.6
Precision 0.0 42.2 34.4
Top-Level Values
Recall 0.0 8.3 29.8
Precision 0.0 50.0 39.2
Sub-Level Arguments
Recall 0.0 28.3 14.1
Precision 0.0 48.2 12.6
Sub-level Values
Recall 1.2 28.3 14.1
Precision 6.2 48.2 12.9
Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data
three months ago but not I started getting the headaches three months
ago. The seed grammar was extended by hand and by automatic
learning to cover a development set of 133 utterances. The re-
sult was two new grammars, a human-extended grammar and a
machine-learned grammar, referred to as the extended and learned
grammars in Table 1. The two new grammars were then tested on
132 unseen sentences in order to compare generality of the rules.
Results are reported only for 83 of the 132 sentences which were
covered by the current interlingua design. The remaining 49 sen-
tences were not covered by the current interlingua design and were
not scored. Results are shown in Table 1.
The parsed test sentences were scored in comparison to a hand-
coded correct interlingua representation. Table 1 separates results
for six components of the interlingua: speech act, concepts, top-
level arguments, top-level values, sub-level arguments, and sub-
level values, in addition to the total interlingua, and the domain
action (speech act and concepts combined). The components of the
interlingua were described in Section 2.
The scores for the total interlingua and domain action are re-
ported as percent correct. The scores for the six components of the
interlingua are reported as average percent precision and recall. For
example, if the correct interlingua for a sentence has two concepts,
and the parser produces three, two of which are correct and one of
which is incorrect, the precision is 66% and the recall is 100%.
Several trends are reflected in the results. Both the human-ex-
tended grammar and the machine-learned grammar show improved
performance over the seed grammar. However, the human extended
grammar tended to outperform the automatically learned grammar
in precision, whereas the automatically learned grammar tended to
outperform the human extended grammar in recall. This result is to
be expected: humans are capable of formulating correct rules, but
may not have time to analyze the amount of data that a machine can
analyze. (The time spent on the human extended grammar after the
seed grammar was complete was only five days.)
Grammar Induction: Our work on automatic grammar induc-
tion for Experiment 1 is still in preliminary stages. At this point,
we have experimented with completely automatic induction (no in-
teraction with a user)2 of new grammar rules starting from a core
grammar and using a development set of sentences that are not
parsable according to the core grammar. The development sen-
tences are tagged with the correct interlingua, and they do not stray
from the concepts covered by the core grammar ? they only cor-
respond to alternative (previously unseen) ways of expressing the
same set of covered concepts. The automatic induction is based
on performing tree matching between a skeletal tree representation
obtained from the interlingua, and a collection of parse fragments
2Previous work on our project [2] investigated learning of grammar
rules with user interaction.
[give-information+onset+symptom]
[manner=]
[sudden]
suddenly
[symptom-location=]
DETP
DET
POSS
my
BODYLOCATION
BODYFLUID
[urine]
urine
became [adj:symptom-name=]
ADJ-SYMPTOM
FUNCTION-ADJ-VALS [attribute=]
[color_attribute]
colored
[abnormal]
dis
Parse chunk #1 Parse chunk #2 Parse chunk #3
Original interlingua:
give-information+onset+symptom
(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,
manner=sudden)
Learned Grammar Rule:
s[give-information+onset+symptom]
( [manner=] [symptom-location=] *+became [adj:symptom-name=] )
Figure 2: A reconstructed parse tree from the Interlingua
that is derived from parsing the new sentence with the core gram-
mar. Extensions to the existing rules are hypothesized in a way that
would produce the correct interlingua representation for the input
utterance.
Figure 2 shows a tree corresponding to an automatically learned
rule. The input to the learning algorithm is the interlingua (shown
in bold boxes in the figure) and three parse chunks (circled in the
figure). The dashed edges are augmented by the learning algorithm.
4. EXPERIMENT 2:
PORTING TO A NEW DOMAIN
USING A HYBRID RULE-BASED AND
STATISTICAL ANALYSIS APPROACH
We are in the process of developing a new alternative analysis
approach for our interlingua-based speech-translation systems that
combines rule-based and statistical methods and we believe inher-
ently supports faster porting into new domains. The main aspects
of the approach are the following. Rather than developing com-
plete semantic grammars for analyzing utterances into our interlin-
gua (either completely manually, or using grammar induction tech-
niques), we separate the task into two main levels. We continue to
develop and maintain rule-based grammars for phrases that corre-
spond to argument-level concepts of our interlingua representation
(e.g., time expressions, locations, symptom-names, etc.). However,
instead of developing grammar rules for assembling the argument-
level phrases into appropriate domain actions, we apply machine
learning and classification techniques [1] to learn these mappings
from a corpus of interlingua tagged utterances. (Earlier work on
this task is reported in [6].)
We believe this approach should prove to be more suitable for
fast porting into new domains for the following reasons. Many of
the required argument-level phrase grammars for a new domain are
likely to be covered by already existing grammar modules, as can
be seen by examining the XDM (cross-domain) nodes in Figure 1.
The remaining new phrase grammars are fairly fast and straightfor-
ward to develop. The central questions, however, are whether the
statistical methods used for classifying strings of arguments into
domain actions are accurate enough, and what amounts of tagged
data are required to obtain reasonable levels of performance. To
assess this last question, we tested the performance of the current
speech-act and concept classifiers for the expanded travel-domain
when trained with increasing amounts of training data. The results
of these experiments are shown in Figure 3. We also report the
performance of the domain-action classification derived from the
combined speech-act and concepts. As can be seen, performance
reaches a relative plateau at around 4000-5000 utterances. We see
these results as indicative that this approach should indeed prove to
be significantly easier to port to new domains. Creating a tagged
database of this order of magnitude can be done in a few weeks,
rather than the months required for complete manual grammar de-
velopment time.
5. EXPERIMENT 3:
PORTING THE SPEECH RECOGNIZER
TO NEW DOMAINS
When the speech recognition components (acoustic models, pro-
nunciation dictionary, vocabulary, and language model) are ported
across domains and languages mainly three types of mismatches
Speech Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Concept Sequence Classification Accuracy for 16-
fold Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Dialog Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Figure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training Data
Baseline Systems WER on Different Tasks [%]
BN (Broadcast News) h4e98 1, all F-conditions 18.5
ESST (scheduling and travel planning domain) 24.3
BN+ESST 18.4
C-STAR (travel planning domain) 20.2
Adaptation!Meeting Recognition
ESST on meeting data 54.1
BN on meeting data 44.2
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (16 meetings) 38.7
BN+ESST on meeting data 42.2
+ language model interpolation (16 meetings) 39.0
Adaptation! Doctor-Patient Domain
C-STAR on doctor-patient data 34.1
+ language model interpolation ( 34 dialogs) 25.1
Table 2: Recognition Results
occur: (1) mismatches in recording condition; (2) speaking style
mismatches; as well as (3) vocabulary and language model mis-
matches. In the past these problems have mostly been solved by
collecting large amounts of acoustic data for training the acoustic
models and development of the pronunciation dictionary, as well
as large text data for vocabulary coverage and language model cal-
culation. However, especially for highly specialized domains and
conversational speaking styles, large databases cannot always be
provided. Therefore, our research has focused on the problem of
how to build LVCSR systems for new tasks and languages [7, 9]
using only a limited amount of data. In this third experiment we
investigate the results of porting the speech recognition component
of our MT system to different new domains. The experiments and
improvements were conducted with the Janus Speech Recognition
Toolkit JRTk [13].
Table 2 shows the results of porting four baseline speech recog-
nition systems to the doctor-patient domain, and to the meeting do-
main. The four baseline systems are trained on Broadcast News
(BN), English SpontaneousScheduling Task (ESST), combined BN
and ESST, and the travel planning domain of the C-STAR consor-
tium (http://www.c-star.org). The given tasks illustrate
a variety of domain size, speaking styles and recording conditions
ranging from clean spontaneous speech in a very limited domain
(ESST, C-STAR) to highly conversational multi-party speech in an
extremely broad domain (Meeting). As a consequence the error
rates on the meeting data are quite high but using MAP (Maximum
A Posteriori) acoustic model adaptation and language model adap-
tation the error rate can be reduced by about 10.2% relative over the
BN baseline system. With the doctor-patient data the drop in error
rate was less severe which can be explained by the similar speaking
style and recording conditions for C-STAR and doctor-patient data.
Details about the applied recognition engine can be found in [10]
for ESST and [11] for the BN system.
6. ACKNOWLEDGMENTS
The research work reported here was funded in part by the DARPA
TIDES Program and supported in part by the National Science
Foundation under Grant number 9982227. Any opinions, findings
and conclusions or recomendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the
National Science Foundation (NSF) or DARPA.
7. REFERENCES
[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. TiMBL: Tilburg Memory Based Learner, version 3.0
Reference Guide. Technical Report Technical Report 00-01,
ILK, 2000. Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`. Epiphenomenal Grammar Acquisition with
GSG. In Proceedings of the Workshop on Conversational
Systems of the 6th Conference on Applied Natural Language
Processing and the 1st Conference of the North American
Chapter of the Association for Computational Linguistics
(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,
D. Koll, and A. Waibel. The Janus-III Translation System.
Machine Translation. To appear.
[6] M. Munk. Shallow statistical parsing for machine translation.
Master?s thesis, University of Karlsruhe, Karlsruhe,
Germany, 1999. http://www.is.cs.cmu.edu/papers/
speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel. Polyphone Decision Tree
Specialization for Language Adaptation. In Proceedings of
the ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel. Interactive Translation of Conversational Speech.
Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,
and M. Woszczyna. Multilinguality in Speech and Spoken
Language Systems. Proceedings of the IEEE, Special Issue
on Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.
Multilingual Speech Recognition, chapter From Speech Input
to Augmented Word Lattices, pages 33?45. Springer Verlag,
Berlin, Heidelberg, New York, artificial Intelligence edition,
2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,
F. Metze, and M. Bett. Advances in Meeting Recognition.
Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,
A. Lavie, L. Levin, and A. Waibel. A Modular Approach to
Spoken Language Translation for Large Domains. In
Proceedings of Conference of the Association for Machine
Translation in the Americas (AMTA?98), Langhorn, PA,
October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.
Recognition of Conversational Telephone Speech using the
Janus Speech Engine. In Proceedings of the ICASSP?97,
Mu?nchen, Germany, 1997.
A THAI SPEECH TRANSLATION SYSTEM  FOR MEDICAL DIALOGS
Tanja Schultz, Dorcas Alexander, Alan W Black, Kay Peterson, Sinaporn Suebvisai, Alex Waibel
Language Technologies Institute, Carnegie Mellon University
E-mail: tanja@cs.cmu.edu
1. Introduction
In this paper we present our activities towards a Thai
Speech-to-Speech translation system. We investigated in
the design and implementation of a prototype system. For
this purpose we carried out research on bootstrapping a
Thai speech recognition system, developing a translation
component, and building an initial Thai synthesis system
using our existing tools.
2. Speech Recognition
The language adaptation techniques developed in our lab
[5] enables us to rapidly bootstrap a speech recognition
system in a new target language given very limited amount
of training data. The Thailand?s National Electronics and
Technology Center gave us the permission to use their
Thai speech data collected in the hotel reservation domain.
They provided us with a 6 hours text and speech database
recorded from native Thai speakers. We divided the data
into three speaker disjoint sets, 34 speakers were used for
training, 4 speakers for development, and another 4
speakers for evaluation. The provided transcriptions were
manually pre-segmented and given in Thai script. We
transformed the Thai script into a Roman script
representation by concatenating the phoneme
representation of the Thai word given in the pronunciation
dictionary. The motivation for this romanization step was
threefold: (1) it makes it easier for non-Thai researchers to
work with the Roman representation like in the grammar
development, (2) the romanized output basically provides
the pronunciation which makes things easier for the speech
synthesis component, and (3) our speech engine currently
does not handle Thai characters.
In our first Thai speech engine we decided to disregard the
tone information. Since tone is a distinctive feature in the
Thai language, disregarding the tone increases the number
of homographs. In order to limit this number, we
distinguished those word candidates by adding a tag that
represents the tone. The resulting dictionary consists of
734 words which cover the given 6-hours database.
Building on our earlier studies which showed that
multilingual seed models outperform monolingual ones
[5], we applied phonemes taken from seven languages,
namely Chinese, Croatian, French, German, Japanese,
Spanish, and Turkish as seed models for the Thai phone
set. Table 1 describes the performance of the Thai speech
recognition component for different acoustic model sizes
(context-independent vs. 500 and 1000 tri-phone models).
The results indicate that a Thai speech recognition engine
can be built by using the bootstrapping approach with a
reasonable amount of speech data. Even the very initial
system bootstrapped from multilingual seed models gives
a performance above 80% word accuracy. The good
performance might be an artifact from the very limited
domain with a compact and closed vocabulary.
System Dev Test Eval Test
Context-Independent 85.62% 83.63%
Context-Dependent (500) 86.99% 84.44%
Context-Dependent (1000) 84.63% 82.71%
Table1: Word accuracy [%] in Thai language
3. Machine Translation
The Machine Translation (MT) component of our current
Thai system is based on an interlingua called the
Interchange Format (IF). The IF developed by CMU has
been expanded and now encompasses concepts in both the
travel and medical domains, as well as many general-use
or cross-domain concepts in many different languages [4].
Interlingua-based MT has several advantages, namely: (1)
it abstracts away from variations in syntax across
languages, providing potentially deep analysis of meaning
without relying on information pertinent only to one
particular language pair, (2) modules for analysis and
generation can be developed monolingually, with
additional reference only to the second "language" of the
interlingua, (3) the speaker can be given a paraphrase in
his or her own language, which can help verify the
accuracy of the analysis and be used to alert the listener to
inaccurate translations, and (4) translation systems can be
extended to new languages simply by hooking up new
monolingual modules for analysis and/or generation,
eliminating the need to develop a completely new system
for each new language pair.
Thai has some particular characteristics which we
addressed in IF and appear in the grammars as follows:
1) The use of a term to indicate the gender of the person:
Thai: zookhee kha1
Eng: okay (ending)
s[acknowledge] (zookhee *[speaker=])
2) An affirmation that means more than simply "yes."
Thai: saap khrap
Eng: know (ending)
s[affirm+knowledge](saap *[speaker=])
3) The separation from the main verb of terms for
feasibility and other modalities.
Thai: rvv khun ca paj dooj thxksii
kyydaaj
Eng: or you will go by taxi [can too]
s[give-information+feasibility+trip]
(*DISC-RHET [who=] ca paj
[locomotion=] [feasibility=])
4. Language Generation
For natural language generation from interlingua for Thai
and English, we are currently investigating two options: a
knowledge-based generation with the pseudo-unification
based GenKit generator developed at CMU, which
employs manually written semantic/syntactic grammars
and lexicons, and a statistical generation operating on a
training corpus of aligned interlingua and natural language
correspondences. Performance tests as well as the amount
and quality of training data will decide which approach
will be pursued in the future.
5. Speech Synthesis
First, we built a limited domain Thai voice in the Festival
Speech Synthesis System [1]. Limited Domain voices can
achieve very high quality voice output [2], and can be easy
to construct if the domain is constrained. Our initial voice
targeted the Hotel Reservation domain and we constructed
235 sentence that covered the aspects of our immediate
interest. Using the tools provided in FestVox [1], we
recorded, auto-labeled, and built a synthetic voice.
In supporting any new language in synthesis, a number of
language specific issues first had to be addressed. As with
our other speech-to-speech translation projects we share
the phoneme set between the recognizer and the
synthesizer. The second important component is the
lexicon. The pronunciation of Thai words from Thai script
is not straightforward, but there is a stronger relationship
between the orthography and pronunciation than in
English. For this small set of initial words we constructed
an explicit lexicon by hand with the output vocabulary of
522 words. The complete Thai limited domain voice uses
unit selection concatenative synthesis. Unlike our other
limited domain synthesizers, where they have a limited
vocabulary, we tag each phone with syllable and tone
information in selection making the result more fluent, and
a little more general.
Building on our previous Thai work in pronunciation of
Thai words [3], we have used the lexicon and statistically
trained letter to sound rules to bootstrap the required word
coverage. With a pronunciation model we can select
suitable phonetically balanced text (both general and in-
domain) from which we are able to record and build a
more general voice.
6. Demonstration Prototype System
Our current version is a two-way speech-to-speech
translation system between Thai and English for dialogs in
the medical domain where the English speaker is a doctor
and the Thai speaker is a patient. The translated speech
input will be spoken using the built voice. At the moment,
the coverage is very limited due to the simplicity of the
used grammars. The figure shows the interface of our
prototype system.
Acknowledgements
This work was partly funded by LASER-ACTD. The
authors thank Thailand?s National Electronics and
Computer Technology Center for giving the permission to
use their database and dictionary for this task.
References
[1] Black, A. and Lenzo, K. (2000) "Building Voices in the
Festival Speech Synthesis System", http://festvox.org
[2] Black, A. and Lenzo, K. (2000) "Limited Domain Synthesis",
ICSLP2000, Beijing, China.
[3] Chotmongkol, A. and Black, A. (2000) "Statistically trained
orthographic to sound models for Thai", ICSLP2000,
Beijing, China.
[4] Lavie A. and Levin L. and Schultz T. and Langley C. and
Han B., Tribble, A., Gates D., Wallace D. and Peterson K.
(2001) ?Domain Portability in Speech-to-speech
Translation?,  HLT, San Diego, March 2001.
[5] Schultz, T. and Waibel, A. (2001) ?Language Independent
and Language Adaptive Acoustic Modeling for Speech
Recognition?, Speech Communication, Volume 35, Issue 1-
2, pp. 31-51, August 2001.
Spoken Language Parsing Using Phrase-Level Grammars and Trainable 
Classifiers 
Chad Langley, Alon Lavie, Lori Levin, Dorcas Wallace, Donna Gates, and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, USA 
{clangley|alavie|lsl|dorcas|dmg|kay}@cs.cmu.edu 
 
Abstract 
In this paper, we describe a novel 
approach to spoken language analysis 
for translation, which uses a combination 
of grammar-based phrase-level parsing 
and automatic classification. The job of 
the analyzer is to produce a shallow 
semantic interlingua representation for 
spoken task-oriented utterances. The 
goal of our hybrid approach is to provide 
accurate real-time analyses while 
improving robustness and portability to 
new domains and languages. 
1 Introduction 
Interlingua-based approaches to Machine 
Translation (MT) are highly attractive in systems 
that support a large number of languages. For each 
source language, an analyzer that converts the 
source language into the interlingua is required. 
For each target language, a generator that converts 
the interlingua into the target language is needed. 
Given analyzers and generators for all supported 
languages, the system simply connects the source 
language analyzer with the target language 
generator to perform translation. 
Robust and accurate analysis is critical in 
interlingua-based translation systems. In speech-to-
speech translation systems, the analyzer must be 
robust to speech recognition errors, spontaneous 
speech, and ungrammatical inputs as described by 
Lavie (1996). Furthermore, the analyzer should run 
in (near) real time. 
In addition to accuracy, speed, and robustness, 
the portability of the analyzer with respect to new 
domains and new languages is an important 
consideration. Despite continuing improvements in 
speech recognition and translation technologies, 
restricted domains of coverage are still necessary 
in order to achieve reasonably accurate machine 
translation. Porting translation systems to new 
domains or even expanding the coverage in an 
existing domain can be very difficult and time-
consuming.  This creates significant challenges in 
situations where translation is needed for a new 
domain within relatively short notice. Likewise, 
demand can be high for translation systems that 
can be rapidly expanded to include new languages 
that were not previously considered important. 
Thus, it is important that the analysis approach 
used in a translation system be portable to new 
domains and languages. 
One approach to analysis in restricted domains 
is to use semantic grammars, which focus on 
parsing semantic concepts rather than syntactic 
structure. Semantic grammars can be especially 
useful for parsing spoken language because they 
are less susceptible to syntactic deviations caused 
by spontaneous speech effects. However, the focus 
on meaning rather than syntactic structure 
generally makes porting to a new domain quite 
difficult. Since semantic grammars do not exploit 
syntactic similarities across domains, completely 
new grammars must usually be developed. 
While grammar-based parsing can provide very 
accurate analyses on development data, it is 
difficult for a grammar to completely cover a 
domain, a problem that is exacerbated by spoken 
input. Furthermore, it generally takes a great deal 
of effort by human experts to develop a high-
coverage grammar. On the other hand, machine 
learning approaches can generalize beyond training 
data and tend to degrade gracefully in the face of 
noisy input. Machine learning methods may, 
however, be less accurate on clearly in-domain 
input than grammars and may require a large 
amount of training data. 
We describe a prototype version of an analyzer 
that combines phrase-level parsing and machine 
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 15-22.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
learning techniques to take advantage of the 
benefits of each. Phrase-level semantic grammars 
and a robust parser are used to extract low-level 
interlingua arguments from an utterance. Then, 
automatic classifiers assign high-level domain 
actions to semantic segments in the utterance. 
2 MT System Overview 
The analyzer we describe is used for English and 
German in several multilingual human-to-human 
speech-to-speech translation systems, including the 
NESPOLE! system (Lavie et al, 2002). The goal 
of NESPOLE! is to provide translation for 
common users within real-world e-commerce 
applications. The system currently provides 
translation in the travel and tourism domain 
between English, French, German and Italian.  
NESPOLE! employs an interlingua-based 
translation approach that uses four basic steps to 
perform translation. First, an automatic speech 
recognizer processes spoken input. The best-
ranked hypothesis from speech recognition is then 
passed through the analyzer to produce interlingua. 
Target language text is then generated from the 
interlingua. Finally, the target language text is 
synthesized into speech. 
This interlingua-based translation approach 
allows for distributed development of the 
components for each language. The components 
for each language are assembled into a translation 
server that accepts speech, text, or interlingua as 
input and produces interlingua, text, and 
synthesized speech. In addition to the analyzer 
described here, the English translation server uses 
the JANUS Recognition Toolkit for speech 
recognition, the GenKit system (Tomita & Nyberg, 
1988) for generation, and the Festival system 
(Black et al, 1999) for synthesis. 
NESPOLE! uses a client-server architecture 
(Lavie et al, 2001) to enable users who are 
browsing the web pages of a service provider (e.g. 
a tourism bureau) to seamlessly connect to a 
human agent who speaks a different language. 
Using commercially available software such as 
Microsoft NetMeeting?, a user is connected to the 
NESPOLE! Mediator, which establishes 
connections with the agent and with translation 
servers for the appropriate languages. During a 
dialogue, the Mediator transmits spoken input from 
the users to the translation servers and synthesized 
translations from the servers to the users. 
3 The Interlingua 
The interlingua used in the NESPOLE! system is 
called Interchange Format (IF) (Levin et al, 1998; 
Levin et al, 2000). The IF defines a shallow 
semantic representation for task-oriented 
utterances that abstracts away from language-
specific syntax and idiosyncrasies while capturing 
the meaning of the input. Each utterance is divided 
into semantic segments called semantic dialog 
units (SDUs), and an IF is assigned to each SDU. 
An IF representation consists of four parts: a 
speaker tag, a speech act, an optional sequence of 
concepts, and an optional set of arguments. The 
representation takes the following form: 
 
speaker : speech act +concept* (argument*) 
 
The speaker tag indicates the role of the speaker 
in the dialogue. The speech act captures the 
speaker?s intention. The concept sequence, which 
may contain zero or more concepts, captures the 
focus of an SDU. The speech act and concept 
sequence are collectively referred to as the domain 
action (DA). The arguments use a feature-value 
representation to encode specific information from 
the utterance. Argument values can be atomic or 
complex. The IF specification defines all of the 
components and describes how they can be legally 
combined. Several examples of utterances with 
corresponding IFs are shown below. 
 
Thank you very much. 
a:thank 
Hello. 
c:greeting (greeting=hello) 
How far in advance do I need to book a room for the Al-
Cervo Hotel? 
c:request-suggestion+reservation+room ( 
   suggest-strength=strong, 
   time=(time-relation=before, 
     time-distance=question), 
   who=i, 
   room-spec=(room, identifiability=no, 
     location=(object-name=cervo_hotel))) 
4 The Hybrid Analysis Approach 
Our hybrid analysis approach uses a combination 
of grammar-based parsing and machine learning 
techniques to transform spoken utterances into the 
IF representation described above. The speaker tag 
is assumed to be given. Thus, the goal of the 
analyzer is to identify the DA and arguments.  
The hybrid analyzer operates in three stages. 
First, semantic grammars are used to parse an 
utterance into a sequence of arguments. Next, the 
utterance is segmented into SDUs. Finally, the DA 
is identified using automatic classifiers. 
4.1 Argument Parsing 
The first stage in analysis is parsing an utterance 
for arguments. During this stage, utterances are 
parsed with phrase-level semantic grammars using 
the robust SOUP parser (Gavald?, 2000). 
4.1.1 The Parser 
The SOUP parser is a stochastic, chart-based, top-
down parser that is designed to provide real-time 
analysis of spoken language using context-free 
semantic grammars. One important feature 
provided by SOUP is word skipping. The amount 
of skipping allowed is configurable and a list of 
unskippable words can be defined. Another feature 
that is critical for phrase-level argument parsing is 
the ability to produce analyses consisting of 
multiple parse trees. SOUP also supports modular 
grammar development (Woszczyna et al, 1998). 
Subgrammars designed for different domains or 
purposes can be developed independently and 
applied in parallel during parsing. Parse tree nodes 
are then marked with a subgrammar label. When 
an input can be parsed in multiple ways, SOUP can 
provide a ranked list of interpretations. 
In the prototype analyzer, word skipping is only 
allowed between parse trees. Only the best-ranked 
argument parse is used for further processing. 
4.1.2 The Grammars 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument 
grammar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains phrase-
level rules for parsing arguments defined in the IF. 
Top-level argument grammar nonterminals 
correspond to top-level arguments in the IF. 
The pseudo-argument grammar contains top-
level nonterminals that do not correspond to 
interlingua concepts. These rules are used for 
parsing common phrases that can be grouped into 
classes to capture more useful information for the 
classifiers. For example, all booked up, full, and 
sold out might be grouped into a class of phrases 
that indicate unavailability. In addition, rules in the 
pseudo-argument grammar can be used for 
contextual anchoring of ambiguous arguments. For 
example, the arguments [who=] and [to-whom=] 
have the same values. To parse these arguments 
properly in a sentence like ?Can you send me the 
brochure??, we use a pseudo-argument grammar 
rule, which refers to the arguments [who=] and [to-
whom=] within the appropriate context.  
The cross-domain grammar contains rules for 
parsing whole DAs that are domain-independent. 
For example, this grammar contains rules for 
greetings (Hello, Good bye, Nice to meet you, etc.). 
Cross-domain grammar rules do not cover all 
possible domain-independent DAs. Instead, the 
rules focus on DAs with simple or no argument 
lists. Domain-independent DAs with complex 
argument lists are left to the classifiers. Cross-
domain rules play an important role in the 
prediction of SDU boundaries. 
Finally, the shared grammar contains common 
grammar rules that can be used by all other 
subgrammars. These include definitions for most 
of the arguments, since many can also appear as 
sub-arguments. RHSs in the argument grammar 
contain mostly references to rules in the shared 
grammar. This method eliminates redundant rules 
in the argument and shared grammars and allows 
for more accurate grammar maintenance. 
4.2 Segmentation 
The second stage of processing in the hybrid 
analysis approach is segmentation of the input into 
SDUs. The IF representation assigns DAs at the 
SDU level. However, since dialogue utterances 
often consist of multiple SDUs, utterances must be 
segmented into SDUs before DAs can be assigned. 
Figure 1 shows an example utterance containing 
four arguments segmented into two SDUs. 
 
SDU1  SDU2  
greeting= disposition= visit-spec= location= 
hello i would like to take a vacation in val di fiemme 
Figure 1. Segmentation of an utterance into SDUs. 
The argument parse may contain trees for cross-
domain DAs, which by definition cover a complete 
SDU. Thus, there must be an SDU boundary on 
both sides of a cross-domain tree. Additionally, no 
SDU boundaries are allowed within parse trees. 
The prototype analyzer drops words skipped 
between parse trees, leaving only a sequence of 
trees. The parse trees on each side of a potential 
boundary are examined, and if either tree was 
constructed by the cross-domain grammar, an SDU 
boundary is inserted. Otherwise, a simple statistical 
model similar to the one described by Lavie et al 
(1997) estimates the likelihood of a boundary. 
The statistical model is based only on the root 
labels of the parse trees immediately preceding and 
following the potential boundary position. Suppose 
the position under consideration looks like 
[A1?A2], where there may be a boundary between 
arguments A1 and A2. The likelihood of an SDU 
boundary is estimated using the following formula: 
 
])C([A  ])C([A
])AC([  ])C([A])AF([A
21
21
21
+
?+?
??  
 
The counts C([A1?]), C([?A2]), C([A1]), C([A2]) 
are computed from the training data. An evaluation 
of this baseline model is presented in section 6.  
4.3 DA Classification 
The third stage of analysis is the identification of 
the DA for each SDU using automatic classifiers. 
After segmentation, a cross-domain parse tree may 
cover an SDU. In this case, analysis is complete 
since the parse tree contains the DA. Otherwise, 
automatic classifiers are used to assign the DA. In 
the prototype analyzer, the DA classification task 
is split into separate subtasks of classifying the 
speech act and concept sequence. This reduces the 
complexity of each subtask and allows for the 
application of specialized techniques to identify 
each component. 
One classifier is used to identify the speech act, 
and a second classifier identifies the concept 
sequence. Both classifiers are implemented using 
TiMBL (Daelemans et al, 2000), a memory-based 
learner. Speech act classification is performed first. 
Input to the speech act classifier is a set of binary 
features that indicate whether each of the possible 
argument and pseudo-argument labels is present in 
the argument parse for the SDU. No other features 
are currently used. Concept sequence classification 
is performed after speech act classification. The 
concept sequence classifier uses the same feature 
set as the speech act classifier with one additional 
feature: the speech act assigned by the speech act 
classifier. We present an evaluation of this baseline 
DA classification scheme in section 6. 
4.4 Using the IF Specification 
The IF specification imposes constraints on how 
elements of the IF representation can legally 
combine. DA classification can be augmented with 
knowledge of constraints from the IF specification, 
providing two advantages over otherwise na?ve 
classification. First, the analyzer must produce 
valid IF representations in order to be useful in a 
translation system. Second, using knowledge from 
the IF specification can improve the quality of the 
IF produced, and thus the translation. 
Two elements of the IF specification are 
especially relevant to DA classification. First, the 
specification defines constraints on the 
composition of DAs. There are constraints on how 
concepts are allowed to pair with speech acts as 
well as ordering constraints on how concepts are 
allowed to combine to form a valid concept 
sequence. These constraints can be used to 
eliminate illegal DAs during classification. The 
second important element of the IF specification is 
the definition of how arguments are licensed by 
speech acts and concepts. In order for an IF to be 
valid, at least one speech act or concept in the DA 
must license each argument. 
The prototype analyzer uses the IF specification 
to aid classification and guarantee that a valid IF 
representation is produced. The speech act and 
concept sequence classifiers each provide a ranked 
list of possible classifications. When the best 
speech act and concept sequence combine to form 
an illegal DA or form a legal DA that does not 
license all of the arguments, the analyzer attempts 
to find the next best legal DA that licenses the 
most arguments. Each of the alternative concept 
sequences (in ranked order) is combined with each 
of the alternative speech acts (in ranked order). For 
each possible legal DA, the analyzer checks if all 
of the arguments found during parsing are licensed. 
If a legal DA is found that licenses all of the 
arguments, then the process stops. If not, one 
additional fallback strategy is used. The analyzer 
then tries to combine the best classified speech act 
with each of the concept sequences that occurred in 
the training data, sorted by their frequency of 
occurrence. Again, the analyzer checks if each 
legal DA licenses all of the arguments and stops if 
such a DA is found. If this step fails to produce a 
legal DA that licenses all of the arguments, the 
best-ranked DA that licenses the most arguments is 
returned. In this case, any arguments that are not 
licensed by the selected DA are removed. This 
approach is used because it is generally better to 
select an alternative DA and retain more arguments 
than to keep the best DA and lose the information 
represented by the arguments. An evaluation of 
this strategy is presented in the section 6. 
5 Grammar Development and 
Classifier Training 
During grammar development, it is generally 
useful to see how changes to the grammar affect 
the IF representations produced by the analyzer. In 
a purely grammar-based analysis approach, full 
interlingua representations are produced as the 
result of parsing, so testing new grammars simply 
requires loading them into the parser. Because the 
grammars used in our hybrid approach parse at the 
argument level, testing grammar modifications at 
the complete IF level requires retraining the 
segmentation model and the DA classifiers. 
 When new grammars are ready for testing, 
utterance-IF pairs for the appropriate language are 
extracted from the training database. Each 
utterance-IF pair in the training data consists of a 
single SDU with a manually annotated IF. Using 
the new grammars, the argument parser is applied 
to each utterance to produce an argument parse. 
The counts used by the segmentation model are 
then recomputed based on the new argument 
parses. Since each utterance contains a single 
SDU, the counts C([?A2]) and C([A1?]) can be 
computed directly from the first and last arguments 
in the parse respectively. 
Next, the training examples for the DA 
classifiers are constructed. Each training example 
for the speech act classifier consists of the speech 
act from the annotated IF and a vector of binary 
features with a positive value set for each argument 
or pseudo-argument label that occurs in the 
argument parse. The training examples for the 
concept sequence classifiers are similar with the 
addition of the annotated speech act to the feature 
vector. After the training examples are constructed, 
new classifiers are trained. 
Two tools are available to support easy testing 
during grammar development. First, the entire 
training process can be run using a single script. 
Retraining for a new grammar simply requires 
running the script with pointers to the new 
grammars. Then, a special development mode of 
the translation servers allows the grammar writers 
to load development grammars and their 
corresponding segmentation model and DA 
classifiers. The translation server supports input in 
the form of individual utterances or files and 
allows the grammar developers to look at the 
results of each stage of the analysis process. 
6 Evaluation 
We present the results from recent experiments to 
measure the performance of the analyzer 
components and of end-to-end translation using the 
analyzer. We also report the results of an ablation 
experiment that used earlier versions of the 
analyzer and IF specification. 
6.1 Translation Experiment 
 
Acceptable Perfect 
SR Hypotheses 66% 56% 
Translation from 
Transcribed Text 58% 43% 
Translation from 
SR Hypotheses 45% 32% 
Table 1. English-to-English end-to-end translation 
 
Acceptable Perfect 
Translation from 
Transcribed Text 55% 38% 
Translation from 
SR Hypotheses 43% 27% 
Table 2. English-to-Italian end-to-end translation 
Tables 1 and 2 show end-to-end translation 
results of the NESPOLE! system. In this 
experiment, the input was a set of English 
utterances. The utterances were paraphrased back 
into English via the interlingua (Table 1) and 
translated into Italian (Table 2). The data used to 
train the DA classifiers consisted of 3350 SDUs 
annotated with IF representations. The test set 
contained 151 utterances consisting of 332 SDUs 
from 4 unseen dialogues. Translations were 
compared to human transcriptions and graded as 
described in (Levin et al, 2000). A grade of 
perfect, ok, or bad was assigned to each 
translation by human graders. A grade of perfect 
or ok is considered acceptable. The table shows the 
average of grades assigned by three graders. 
The row in Table 1 labeled SR Hypotheses 
shows the grades when the speech recognizer 
output is compared directly to human transcripts. 
As these grades show, recognition errors can be a 
major source of unacceptable translations. These 
grades provide a rough bound on the translation 
performance that can be expected when using input 
from the speech recognizer since meaning lost due 
to recognition errors cannot be recovered. The 
rows labeled Translation from Transcribed Text 
show the results when human transcripts are used 
as input. These grades reflect the combined 
performance of the analyzer and generator. The 
rows labeled Translation from SR Hypotheses 
show the results when the speech recognizer 
produces the input utterances. As expected, 
translation performance was worse with the 
introduction of recognition errors. 
 
Precision Recall 
70% 54% 
Table 3. SDU boundary detection performance 
Table 3 shows the performance of the 
segmentation model on the test set. The SDU 
boundary positions assigned automatically were 
compared with manually annotated positions. 
 
 
Classifier Accuracy 
Speech Act 65% 
Concept Sequence 54% 
Domain Action 43% 
Table 4. Classifier accuracy on transcription 
 
Frequency 
Speech Act 33% 
Concept Sequence 40% 
Domain Action 14% 
Table 5. Frequency of most common DA elements 
Table 4 shows the performance of the DA 
classifiers, and Table 5 shows the frequency of the 
most common DA, speech act, and concept 
sequence in the test set. Transcribed utterances 
were used as input and were segmented into SDUs 
before analysis. This experiment is based on only 
293 SDUs. For the remaining SDUs in the test set, 
it was not possible to assign a valid representation 
based on the current IF specification. 
These results demonstrate that it is not always 
necessary to find the canonical DA to produce an 
acceptable translation. This can be seen by 
comparing the Domain Action accuracy from Table 
4 with the Transcribed grades from Table 1. 
Although the DA classifiers produced the 
canonical DA only 43% of the time, 58% of the 
translations were graded as acceptable. 
 
 
Changed 
Speech Act 5% 
Concept Sequence 26% 
Domain Action 29% 
Table 6. DA elements changed by IF specification 
In order to examine the effects of using IF 
specification constraints, we looked at the 182 
SDUs which were not parsed by the cross-domain 
grammar and thus required DA classification. 
Table 6 shows how many DAs, speech acts, and 
concept sequences were changed as a result of 
using the constraints. DAs were changed either 
because the DA was illegal or because the DA did 
not license some of the arguments. Without the IF 
specification, 4% of the SDUs would have been 
assigned an illegal DA, and 29% of the SDUs 
(those with a changed DA) would have been 
assigned an illegal IF. Furthermore, without the IF 
specification, 0.38 arguments per SDU would have 
to be dropped while only 0.07 arguments per SDU 
were dropped when using the fallback strategy. 
The mean number of arguments per SDU was 1.47. 
6.2 Ablation Experiment 
Classification Accuracy (16-fold Cross 
Validation)
0
0.2
0.4
0.6
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
e
a
n
 
A
cc
ur
a
c
y
Speech Act
Concept
Sequence
Domain Action
 
Figure 2: DA classifier accuracy with varying 
amounts of data 
Figure 2 shows the results of an ablation 
experiment that examined the effect of varying the 
training set size on DA classification accuracy. 
Each point represents the average accuracy using a 
16-fold cross validation setup. 
The training data contained 6409 SDU-
interlingua pairs. The data were randomly divided 
into 16 test sets containing 400 examples each. In 
each fold, the remaining data were used to create 
training sets containing 500, 1000, 2000, 3000, 
4000, 5000, and 6009 examples. 
The performance of the classifiers appears to 
begin leveling off around 4000 training examples. 
These results seem promising with regard to the 
portability of the DA classifiers since a data set of 
this size could be constructed in a few weeks. 
7 Related Work 
Lavie et al (1997) developed a method for 
identifying SDU boundaries in a speech-to-speech 
translation system. Identifying SDU boundaries is 
also similar to sentence boundary detection. 
Stevenson and Gaizauskas (2000) use TiMBL 
(Daelemans et al, 2000) to identify sentence 
boundaries in speech recognizer output, and Gotoh 
and Renals (2000) use a statistical approach to 
identify sentence boundaries in automatic speech 
recognition transcripts of broadcast speech. 
Munk (1999) attempted to combine grammars 
and machine learning for DA classification. In 
Munk?s SALT system, a two-layer HMM was used 
to segment and label arguments and speech acts. A 
neural network identified the concept sequences. 
Finally, semantic grammars were used to parse 
each argument segment. One problem with SALT 
was that the segmentation was often inaccurate and 
resulted in bad parses. Also, SALT did not use a 
cross-domain grammar or interlingua specification. 
Cattoni et al (2001) apply statistical language 
models to DA classification. A word bigram model 
is trained for each DA in the training data. To label 
an utterance, the most likely DA is assigned. 
Arguments are identified using recursive transition 
networks. IF specification constraints are used to 
find the most likely valid DA and arguments. 
8 Discussion and Future Work 
One of the primary motivations for developing the 
hybrid analysis approach described here is to 
improve the portability of the analyzer to new 
domains and languages. We expect that moving 
from a purely grammar-based parsing approach to 
this hybrid approach will help attain this goal. 
The SOUP parser supports portability to new 
domains by allowing separate grammar modules 
for each domain and a grammar of rules shared 
across domains (Woszczyna et al, 1998). This 
modular grammar design provides an effective 
method for adding new domains to existing 
grammars. Nevertheless, developing a full 
semantic grammar for a new domain requires 
significant effort by expert grammar writers. 
The hybrid approach reduces the manual labor 
required to port to new domains by incorporating 
machine learning. The most labor-intensive part of 
developing full semantic grammars for producing 
IF is writing DA-level rules. This is exactly the 
work eliminated by using automatic DA classifiers. 
Furthermore, the phrase-level argument grammars 
used in the analyzer contain fewer rules than a full 
semantic grammar. The argument-level grammars 
are also less domain-dependent than the full 
grammars and thus more reusable. The DA 
classifiers should also be more tolerant than full 
grammars of deviations from the domain. 
We analyzed the grammars from a previous 
version of the translation system, which produced 
complete IFs using strictly grammar-based parsing, 
to estimate what portion of the grammar was 
devoted to the identification of domain actions. 
Approximately 2200 rules were used to cover 400 
DAs. Nonlexical rules made up about half of the 
grammar, and the DA rules accounted for about 
20% of the nonlexical rules. Using these figures, 
we can project the number of DA rules that would 
have to be added to the current system, which uses 
our hybrid analysis approach. The database for the 
new system contains approximately 600 DAs. 
Assuming the average number of rules per DA is 
the same as before, roughly 3300 DA-level rules 
would have to be added to the current grammar, 
which has about 17500 nonlexical rules, to cover 
the DAs in the database. 
Our hybrid approach should also improve the 
portability of the analyzer to new languages. Since 
grammars are language specific, adding a new 
language still requires writing new argument 
grammars. Then the DA classifiers simply need to 
be retrained on data for the new language. If 
training data for the new language were not 
available, DA classifiers using only language-
independent features, from the IF for example, 
could be trained on data for existing languages and 
used for the new language. Such classifiers could 
be used as a starting point until training data was 
available in the new language. 
The experimental results indicate the promise 
of the analysis approach we have described. The 
level of performance reported here was achieved 
using a simple segmentation model and simple DA 
classifiers with limited feature sets. We expect that 
performance will substantially improve with a 
more informed design of the segmentation model 
and DA classifiers. We plan to examine various 
design options, including richer feature sets and 
alternative classification techniques. We are also 
planning experiments to evaluate robustness and 
portability when the coverage of the NESPOLE! 
system is expanded to the medical domain later 
this year. In these experiments, we will measure 
the effort needed to write new argument grammars, 
the extent to which existing argument grammars 
are reusable, and the effort required to expand the 
argument grammar to include DA-level rules. 
9 Acknowledgements 
The research work reported here was supported by 
the National Science Foundation under Grant 
number 9982227. Special thanks to Alex Waibel 
and everyone in the NESPOLE! group for their 
support on this work. 
References 
Black, A., P. Taylor, and R. Caley. 1999. The 
Festival Speech Synthesis System: System 
Documentation. Human Computer Research 
Centre, University of Edinburgh, Scotland. 
http://www.cstr.ed.ac.uk/projects/festival/ma
nual 
Cattoni, R., M. Federico, and A. Lavie. 2001. 
Robust Analysis of Spoken Input Combining 
Statistical and Knowledge-Based Information 
Sources. In Proceedings of the IEEE Automatic 
Speech Recognition and Understanding 
Workshop, Trento, Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2000. TiMBL: Tilburg Memory 
Based Learner, version 3.0, Reference Guide. 
ILK Technical Report 00-01. 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz 
Gavald?, M. 2000. SOUP: A Parser for Real-
World Spontaneous Speech. In Proceedings of 
the IWPT-2000, Trento, Italy. 
Gotoh, Y. and S. Renals. Sentence Boundary 
Detection in Broadcast Speech Transcripts. 2000. 
In Proceedings on the International Speech 
Communication Association Workshop: 
Automatic Speech Recognition: Challenges for 
the New Millennium, Paris. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. 
Enhancing the Usability and Performance of 
NESPOLE! ? a Real-World Speech-to-Speech 
Translation System. In Proceedings of HLT-
2002, San Diego, CA. 
Lavie, A., C. Langley, A. Waibel, et al 2001. 
Architecture and Design Considerations in 
NESPOLE!: a Speech Translation System for E-
commerce Applications. In Proceedings of HLT-
2001, San Diego, CA. 
Lavie, A., D. Gates, N. Coccaro, and L. Levin. 
1997. Input Segmentation of Spontaneous Speech 
in JANUS: a Speech-to-speech Translation 
System. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-
96 Workshop, E. Maier, M. Mast, and S. 
Luperfoy (eds.), LNCS series, Springer Verlag. 
Lavie, A. 1996. GLR*: A Robust Grammar-
Focused Parser for Spontaneously Spoken 
Language. PhD dissertation, Technical Report 
CMU-CS-96-126, Carnegie Mellon University, 
Pittsburgh, PA. 
Levin, L., D. Gates, A. Lavie, et al 2000. 
Evaluation of a Practical Interlingua for Task-
Oriented Dialogue. In Workshop on Applied 
Interlinguas: Practical Applications of 
Interlingual Approaches to NLP, Seattle. 
Levin, L., D. Gates, A. Lavie, and A. Waibel. 
1998. An Interlingua Based on Domain Actions 
for Machine Translation of Task-Oriented 
Dialogues. In Proceedings of ICSLP-98, Vol. 4, 
pp. 1155-1158, Sydney, Australia. 
Munk, M. 1999. Shallow Statistical Parsing for 
Machine Translation. Diploma Thesis, Karlsruhe 
University. 
Stevenson, M. and R. Gaizauskas. Experiments on 
Sentence Boundary Detection. 2000. In 
Proceedings of ANLP and NAACL-2000, Seattle. 
Tomita, M. and E. H. Nyberg. 1988. Generation 
Kit and Transformation Kit, Version 3.2: User?s 
Manual. Technical Report CMU-CMT-88-
MEMO, Carnegie Mellon University, Pittsburgh, 
PA. 
Woszczyna, M., M. Broadhead, D. Gates, et al 
1998. A Modular Approach to Spoken Language 
Translation for Large Domains. In Proceedings 
of AMTA-98, Langhorne, PA. 
Domain Specific Speech Acts for Spoken Language Translation 
Lori Levin, Chad Langley, Alon Lavie,  
Donna Gates, Dorcas Wallace and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, United States 
{lsl,clangley,alavie,dmg,dorcas,kay+}@cs.cmu.edu 
Abstract 
We describe a coding scheme for ma-
chine translation of spoken task-
oriented dialogue. The coding scheme 
covers two levels of speaker intention ? 
domain independent speech acts and 
domain dependent domain actions. Our 
database contains over 14,000 tagged 
sentences in English, Italian, and Ger-
man. We argue that domain actions, and 
not speech acts, are the relevant dis-
course unit for improving translation 
quality. We also show that, although 
domain actions are domain specific, the 
approach scales up to large domains 
without an explosion of domain actions 
and can be coded with high inter-coder 
reliability across research sites. Fur-
thermore, although the number of do-
main actions is on the order of ten times 
the number of speech acts, sparseness is 
not a problem for the training of classi-
fiers for identifying the domain action. 
We describe our work on developing 
high accuracy speech act and domain 
action classifiers, which is the core of 
the source language analysis module of 
our NESPOLE machine translation sys-
tem. 
1 Introduction 
The NESPOLE and C-STAR machine translation 
projects use an interlingua representation based 
on speaker intention rather than literal meaning. 
The speaker's intention is represented as a 
domain independent speech act followed by do-
main dependent concepts. We use the term 
domain action to refer to the combination of a 
speech act with domain specific concepts. Exam-
ples of domain actions and speech acts are shown 
in Figure 1. 
 
c:give-information+party  
?I will be traveling with my husband and 
our two children ages two and eleven? 
 
c:request-information+existence+facility  
?Do they have parking available?" 
?Is there someplace to go ice skating?" 
 
c:give-information+view+information-
object  
?I see the bus icon?  
 
Figure 1: Examples of Speech Acts and Domain 
Actions. 
 
Domain actions are constructed compositionally 
from an inventory of speech acts and an inven-
tory of concepts. The allowable combinations of 
speech acts and concepts are formalized in a hu-
man- and machine-readable specification docu-
ment. The specification document is supported 
by a database of over 14,000 tagged sentences in 
English, German, and Italian. 
The discourse community has long recog-
nized the potential for improving NLP systems 
by identifying speaker intention. It has been hy-
pothesized that predicting speaker intention of 
the next utterance would improve speech recog-
nition (Reithinger et al, Stolcke et al), or reduce 
ambiguity for machine translation (Qu et al, 
1996, Qu et al, 1997). Identifying speaker inten-
tion is also critical for sentence generation. 
We argue in this paper that the explicit repre-
sentation of speaker intention using domain ac-
tions can serve as the basis for an effective 
language-independent representation of meaning 
for speech-to-speech translation and that the 
relevant units of speaker intention are the domain 
specific domain action as well as the domain in-
dependent speech act. After a brief description of 
our database, we present linguistic motivation for 
domain actions. We go on to show that although 
domain actions are domain specific, there is not 
an explosion or exponential growth of domain 
actions when we scale up to a larger domain or 
port to a new domain. Finally we will show that, 
although the number of domain actions is on the 
order of ten times the number of speech acts, 
data sparseness is not a problem in training a 
domain action classifier. We present extensive 
work on developing a high-accuracy classifier 
for domain actions using a variety of classifica-
tion approaches and conclusions on the adequacy 
of these approaches to the task of domain action 
classification.  
2 Data Collection Scenario and Data-
base 
Our study is based on data that was collected for 
the NESPOLE and C-STAR speech-to-speech 
translation projects. Three domains are included. 
The NESPOLE travel domain covers inquiries 
about vacation packages. The C-STAR travel 
domain consists largely of reservation and pay-
ment dialogues and overlaps only about 50% in 
vocabulary with the NESPOLE travel domain. 
The medical assistance domain includes dia-
logues about chest pain and flu-like symptoms. 
There were two data collection protocols for 
the NESPOLE travel domain ? monolingual and 
bilingual. In the monolingual protocol, an Eng-
lish speaker in the United States had a conversa-
tion with an Italian travel agent speaking (non-
native) English in Italy. Monolingual data was 
also collected for German, French and Italian. 
Bilingual data was collected during user studies 
with, for example, an English speaker in the 
United States talking to an Italian-speaking travel 
agent in Italy, with the NESPOLE system pro-
viding the translation between the two parties. 
The C-STAR data consists of only monolingual 
role-playing dialogues with both speakers at the 
same site. The medical dialogues are monolin-
gual with doctors playing the parts of both doctor 
and patient. 
The dialogues were transcribed and multi-
sentence utterances were broken down into mul-
tiple Semantic Dialogue Units (SDUs) that each 
correspond to one domain action. Some SDUs 
have been translated into other NESPOLE or C-
STAR languages. Over 14,000 SDUs have been 
tagged with interlingua representations including 
domain actions as well as argument-value pairs. 
Table 1 summarizes the number of tagged SDUs 
in complete dialogues in the interlingua database. 
There are some additional tagged dialogue frag-
ments that are not counted. Figure 2 shows an 
excerpt from the database. 
 
 
 
English NESPOLE Travel 4691 
English C-STAR Travel 2025 
German NESPOLE Travel 1538 
Italian NESPOLE Travel 2248 
English Medical Assistance 2001 
German Medical Assistance 1152 
Italian Medical Assistance 935 
Table 1: Tagged SDUs in the Interlingua Data-
base. 
 
e709wa.19.0  comments: DATA from 
e709_1_0018_ITAGOR_00 
 
e709wa.19.1  olang ITA  lang ITA Prv CMU   
?hai in mente una localita specifica?" 
e709wa.19.1  olang ITA  lang GER  Prv CMU   
?haben Sie einen bestimmten Ort im Sinn?" 
e709wa.19.1  olang ITA  lang FRE  Prv 
CLIPS ?" 
e709wa.19.1  olang ITA  lang ENG  Prv CMU   
?do you have a specific place in mind" 
e709wa.19.1                   IF  Prv CMU   
a:request-information+disposition+object  
(object-spec=(place, modifier=specific, 
identifiability=no), disposi-
tion=(intention, who=you)) 
e709wa.19.1  comments: Tagged by dmg 
 
Figure 2: Excerpt from the Interlingua Database. 
3 Linguistic Argument for Domain Ac-
tions 
Proponents of Construction Grammar (Fillmore 
et. al. 1988, Goldberg 1995) have argued that 
human languages consist of constructional units 
that include a syntactic structure along with its 
associated semantics and pragmatics. Some con-
structions follow the typical syntactic rules of the 
language but have a semantic or pragmatic focus 
that is not compositionally predictable from the 
parts. Other constructions do not even follow the 
typical syntax of the language (e.g., Why not go? 
with no tensed verb). 
Our work with multilingual machine transla-
tion of spoken language shows that fixed expres-
sions cannot be translated literally. For example, 
Why not go to the meeting? can be translated 
into Japanese as Kaigi ni itte mitara doo? (meet-
ing to going see/try-if how), which differs from 
the English in several ways. It does not have a 
word corresponding to not; it has a word that 
means see/try that does not appear in the English 
sentence; and so on. In order to produce an ac-
ceptable translation, we must find a common 
ground between the English fixed expression 
Why not V-inf? and the Japanese fixed expression 
-te mittara doo?. The common ground is the 
speaker's intention (in this case, to make a sug-
gestion) rather than the syntax or literal meaning. 
Speaker intention is partially captured with a 
direct or indirect speech act. However, whereas 
speech acts are generally domain independent, 
task-oriented language abounds with fixed ex-
pressions that have domain specific functions. 
For example, the phrases We have? or There 
are? in the hotel reservation domain express 
availability of rooms in addition to their more 
literal meanings of possession and existence. In 
the past six years, we have been successful in 
using domain specific domain actions as the ba-
sis for translation of limited-domain task-
oriented spoken language (Levin et al, 1998, 
Levin et al 2002; Langley and Lavie, 2003) 
4 Scalability and Portability of Domain 
Actions 
Domain actions, like speech acts, convey speaker 
intention. However, domain actions also repre-
sent components of meaning and are therefore 
more numerous than domain independent speech 
acts. 1168 unique domain actions are used in our 
NESPOLE database, in contrast to only 72 
speech acts. We show in this section that domain 
actions yield good coverage of task-oriented do-
mains, that domain actions can be coded effec-
tively by humans, and that scaling up to larger 
domains or porting to new domains is feasible 
without an explosion of domain actions.  
 
Coverage of Task-Oriented Domains: Our 
NESPOLE domain action database contains dia-
logues from two task-oriented domains: medical 
assistance and travel. Table 2 shows the number 
of speech acts and concepts that are used in the 
travel and medical domains.  The 1168 unique 
domain actions that appear in our database are 
composed of the 72 speech acts and 125 con-
cepts. 
 
 Travel Medical Combined 
DAs 880 459 1168 
SAs 67 44 72 
Concepts 91 74 125 
Table 2: DA component counts in NESPOLE 
data. 
 
Our domain action based interlingua has quite 
high coverage of the travel and medical dia-
logues we have collected. To measure how well 
the interlingua covers a domain, we define the 
no-tag rate as the percent of sentences that are 
not covered by the interlingua, according to a 
human expert. The no-tag rate for the English 
NESPOLE travel dialogues is 4.3% for dialogues 
that have been used for system development.  
We have also estimated the domain action no-
tag rate for unseen data using the NESPOLE 
travel database (English, German, and Italian 
combined). We randomly selected 100 SDUs as 
seen data and extracted their domain actions. We 
then randomly selected 100 additional SDUs 
from the remaining data and estimated the no-tag 
rate by counting the number of SDUs not cov-
ered by the domain actions in the seen data. We 
then added the unseen data to the seen data set 
and randomly selected 100 new SDUs. We re-
peated this process until the entire database had 
been seen, and we repeated the entire sampling 
process 10 times. Although the number of do-
main actions increases steadily with the database 
size (Figure 4), the no-tag rate for unseen data 
stabilizes at less than 10%.  
We also randomly selected half of the SDUs 
(4200) from the database as seen data and ex-
tracted the domain actions. Holding the seen data 
set fixed, we then estimated the no-tag rates in 
increasing amounts of unseen data from the re-
maining half of the database. We repeated this 
process 10 times. With a fixed amount of seen 
data, the no-tag rate remains stable for increasing 
amounts of unseen data. We observed similar no-
tag rate results for the medical assistance domain 
and for the combination of travel and medical 
domains. 
It is also important to note that although there 
is a large set of uncommon domain actions, the 
top 105 domain actions cover 80% of the sen-
tences in the travel domain database. Thus do-
main actions are practical for covering task-
oriented domains. 
 
Intercoder Agreement: Intercoder agreement is 
another indicator of manageability of the domain 
action based interlingua. We calculate intercoder 
agreement as percent agreement. Three interlin-
gua experts at one NESPOLE site achieved 94% 
agreement (average pairwise agreement) on 
speech acts and 88% agreement on domain ac-
tions. Across sites, expert agreement on speech 
acts is still quite high (89%), although agreement 
on domain actions is lower (62%). Since many 
domain actions are similar in meaning, some dis-
agreement can be tolerated without affecting 
translation quality. 
 
Figure 3: DAs to cover data (English). 
Figure 4: DAs to cover data (All languages). 
 
Scalability and Portability: The graphs in Figure 
3 and Figure 4 illustrate growth in the number of 
domain actions as the database size increases and 
as new domains are added. The x-axis represents 
the sample size randomly selected from the data-
base. The y-axis shows the number of unique 
domain actions (types) averaged over 10 samples 
of each size. Figure 3 shows the growth in do-
main actions for three English databases 
(NESPOLE travel, C-STAR travel, and medical 
assistance) as well as the growth in domain ac-
tions for a database consisting of equal amounts 
of data from each domain. Figure 4 shows the 
growth in domain actions for combined English, 
German, and Italian data in the NESPOLE travel 
and medical domains.  
Figure 3 and Figure 4 show that the number 
of domain actions increases steadily as the data-
base grows. However, closer examination reveals 
that scalability to larger domains and portability 
to new domains are in fact feasible.  The curves 
representing combined domains (travel plus 
medical in Figure 4 and NESPOLE travel, C-
STAR travel, and medical in Figure 3) show only 
a small increase in the number of domain actions 
when two domains are combined. In fact, there is 
a large overlap between domains.  In Table 3 the 
Overlap columns show the number of DA types 
and tokens that are shared between the travel and 
medical domains. We can see around 70% of DA 
tokens are covered by DA types that occur in 
both domains. 
 
 
DA 
Types 
Type 
Overlap 
DA 
Tokens 
Token 
Overlap 
NESPOLE 
Travel 880 171 8477 
6004 
(70.8%) 
NESPOLE 
Medical 459 171 4088 
2743 
(67.1%) 
Table 3: DA Overlap (All languages). 
5 A Hybrid Analysis Approach for Pars-
ing Domain Actions 
Langley et al (2002; Langley and Lavie, 2003) 
describe the hybrid analysis approach that is used 
in the NESPOLE! system (Lavie et al, 2002). 
The hybrid analysis approach combines gram-
mar-based phrasal parsing and machine learning 
techniques to transform utterances into our inter-
lingua representation. Our analyzer operates in 
three stages to identify the domain action and 
arguments. 
First, an input utterance is parsed into a se-
quence of arguments using phrase-level semantic 
grammars and the SOUP parser (Gavald?, 2000). 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument gram-
mar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains 
phrase-level rules for parsing arguments defined 
in the interlingua. The pseudo-argument gram-
mar contains rules for parsing common phrases 
that are not covered by interlingua arguments. 
For example, all booked up, full, and sold out 
might be grouped into a class of phrases that in-
dicate unavailability. The cross-domain grammar 
contains rules for parsing complete DAs that are 
domain independent. For example, this grammar 
contains rules for greetings (Hello, Good bye, 
Nice to meet you, etc.). Finally, the shared 
grammar contains low-level rules that can be 
used by all other subgrammars. 
After argument parsing, the utterance is seg-
mented into SDUs using memory-based learning 
(k-nearest neighbor) techniques. Spoken utter-
ances often consist of several SDUs. Since DAs 
are assigned at the SDU level, it is necessary to 
segment utterances before assigning DAs. 
0
100
200
300
400
500
600
700
800
900
0 1000 2000 3000 4000 5000 6000 7000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical C-STAR
Nespole Travel+Medical C-STAR + Nespole Travel+Medical
0
100
200
300
400
500
600
700
800
900
1000
0 1000 2000 3000 4000 5000 6000 7000 8000 9000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical Nespole Travel+Medical
The final stage in the hybrid analysis ap-
proach is domain action classification.  
6 Domain Action Classification 
Identifying the domain action is a critical step in 
the analysis process for our interlingua-based 
translation systems. One possible approach 
would be to manually develop grammars de-
signed to parse input utterances all the way to the 
domain action level. However, while grammar-
based parsing may provide very accurate analy-
ses, it is generally not feasible to develop a 
grammar that completely covers a domain. This 
problem is exacerbated with spoken input, where 
disfluencies and deviations from the grammar are 
very common. Furthermore, a great deal of effort 
by human experts is generally required to de-
velop a wide-coverage grammar. 
An alternative to writing full domain action 
grammars is to train classifiers to identify the 
DA. Machine learning approaches allow the ana-
lyzer to generalize beyond training data and tend 
to degrade gracefully in the face of noisy input. 
Machine learning methods may, however, be less 
accurate than grammars, especially on common 
in-domain input, and may require a large amount 
of training data in order to achieve adequate lev-
els of performance. In the hybrid analyzer de-
scribed above, classifiers are used to identify the 
DA for domain specific portions of utterances 
that are not covered by the cross-domain gram-
mar. 
We tested classifiers trained to classify com-
plete DAs. We also split the DA classification 
task into two subtasks: speech act classification 
and concept sequence classification. This simpli-
fies the task of each classifier, allows for the use 
of different approaches and/or feature sets for 
each task, and reduces data sparseness. Our hy-
brid analyzer uses the output of each classifier 
along with the interlingua specification to iden-
tify the DA (Langley et al, 2002; Langley and 
Lavie, 2003). 
7 Experimental Setup 
We conducted experiments to assess the per-
formance of several machine-learning ap-
proaches on the DA classification tasks. We 
evaluated all of the classifiers on English and 
German input in the NESPOLE travel domain.  
7.1 Corpus 
The corpus used in all of the experiments was the 
NESPOLE! travel and tourism database. Since 
our goal was to evaluate the SA and concept se-
quence classifiers and not segmentation, we cre-
ated training examples for each SDU in the 
database rather than for each utterance. Table 4 
contains statistics regarding the contents of the 
corpus for our classification tasks. Table 5 shows 
the frequency of the most common domain ac-
tion, speech act, and concept sequence in the 
corpus. These frequencies provide a baseline that 
would be achieved by a simple classifier that al-
ways returned the most common class. 
 
 English German 
SDUs 8289 8719 
Domain Actions 972 1001 
Speech Acts 70 70 
Concept Sequences 615 638 
Vocabulary Size 1946 2815 
Table 4: Corpus Statistics. 
 
 English German 
DA (acknowledge) 19.2% 19.7% 
SA (give-information) 41.4% 40.7% 
Concept Sequence 
(No concepts) 
38.9% 40.3% 
Table 5: Most frequent DAs, SAs, and CSs. 
 
All of the results presented in this paper were 
produced using a 20-fold cross validation setup. 
The corpus was randomly divided into 20 sets of 
equal size. Each of the sets was held out as the 
test set for one fold with the remaining 19 sets 
used as training data. Within each language, the 
same random split was used for all of the classi-
fication experiments. Because the same split of 
the data was used for different classifiers, the 
results of two classifiers on the same test set are 
directly comparable. Thus, we tested for signifi-
cance using two-tailed matched pair t-tests. 
7.2 Machine Learning Approaches 
We evaluated the performance of four different 
machine-learning approaches on the DA classifi-
cation tasks: memory-based learning (k-Nearest-
Neighbor), decision trees, neural networks, and 
na?ve Bayes n-gram classifiers. We selected 
these approaches because they vary substantially 
in the their representations of the training data 
and their methods for selecting the best class. 
Our purpose was not to implement each ap-
proach from scratch but to test the approach for 
our particular task. Thus, we chose to use exist-
ing software for each approach ?off the shelf.? 
The ease of acquiring and setting up the software 
influenced our choice. Furthermore, the ease of 
incorporating the software into our online trans-
lation system was also a factor. 
Our memory-based classifiers were imple-
mented using TiMBL (Daelemans et al, 2002). 
We used C4.5 (Quinlan, 1993) for our decision 
tree classifiers. Our neural network classifiers 
were implemented using SNNS (Zell et al, 
1998). We used Rainbow (McCallum, 1996) for 
our na?ve Bayes n-gram classifiers. 
8 Experiments 
In our first experiment, we compared the per-
formance of the four machine learning ap-
proaches. Each SDU was parsed using the 
argument and pseudo-argument grammars de-
scribed above. The feature set for the DA and SA 
classifiers consisted of binary features indicating 
the presence or absence of labels from the 
grammars in the parse forest for the SDU. The 
feature set included 212 features for English and 
259 features for German. The concept sequence 
classifiers used the same feature set with the ad-
dition of the speech act. 
In the SA classification experiment, the 
TiMBL classifier used the IB1 (k-NN) algorithm 
with 1 neighbor and gain ratio feature weighting. 
The C4.5 classifier required at least one instance 
per branch and used node post-pruning. Both the 
TiMBL and C4.5 classifiers used the binary fea-
tures described above and produced the single 
best class as output. The SNNS classifier used a 
simple feed-forward network with 1 input unit 
for each binary feature, 1 hidden layer containing 
15 units, and 1 output unit for each speech act. 
The network was trained using backpropagation. 
The order of presentation of the training exam-
ples was randomized in each epoch, and the 
weights were updated after each training exam-
ple presentation. In order to simulate the binary 
features used by the other classifiers as closely as 
possible, the Rainbow classifier used a simple 
unigram model whose vocabulary was the set of 
labels included in the binary feature set. The 
setup for the DA classification experiment was 
identical except that the neural network had 50 
hidden units. 
The setup of the classifiers for the concept se-
quence classification experiment was very simi-
lar. The TiMBL and C4.5 classifiers were set up 
exactly as in the DA and SA experiments with 
one extra feature whose value was the speech act. 
The SNNS concept sequence classifier used a 
similar network with 50 hidden units. The SA 
feature was represented as a set of binary input 
units. The Rainbow classifier was set up exactly 
as in the DA and SA experiments. The SA fea-
ture was not included. 
As mentioned above, both experiments used a 
20-fold cross-validation setup. In each fold, the 
TiMBL, C4.5, and Rainbow classifiers were sim-
ply trained on 19 subsets of the data and tested 
on the remaining set. The SNNS classifiers re-
quired a more complex setup to determine the 
number of epochs to train the neural network for 
each test set. Within each fold, a cross-validation 
setup was used to determine the number of train-
ing epochs. Each of the 19 training subsets for a 
fold was used as a validation set. The network 
was trained on the remaining 18 subsets until the 
accuracy on the validation set did not improve 
for 50 consecutive epochs. The network was then 
trained on all 19 training subsets for the average 
number of epochs from the validation sets. This 
process was used for all 20-folds in the SA clas-
sification experiment. For the DA and concept 
sequence experiments, this process ran for ap-
proximately 1.5 days for each fold. Thus, this 
process was run for the first two folds, and the 
average number of epochs from those folds was 
used for training. 
 
 English German 
TiMBL 49.69% 46.51% 
C4.5 48.90% 46.58% 
SNNS 49.39% 46.21% 
Rainbow 39.74% 38.32% 
Table 6: Domain Action classifier accuracy. 
 
 English German 
TiMBL 69.82% 67.57% 
C4.5 70.41% 67.90% 
SNNS 71.52% 67.61% 
Rainbow 51.39% 46.00% 
Table 7: Speech Act classifier accuracy. 
 
 English German 
TiMBL 69.59% 67.08% 
C4.5 68.47% 66.45% 
SNNS 71.35% 68.67% 
Rainbow 51.64% 51.50% 
Table 8: Concept Sequence classifier accuracy. 
 Table 6, Table 7, and Table 8 show the aver-
age accuracy of each learning approach on the 
20-fold cross validation experiments for domain 
action, speech act, and concept classification re-
spectively. For DA classification, there were no 
significant differences between the TiMBL, 
C4.5, and SNNS classifiers for English or Ger-
man. In the SA experiment, the difference be-
tween the TiMBL and C4.5 classifiers for 
English was not significant. The SNNS classifier 
was significantly better than both TiMBL and 
C4.5 (at least p=0.0001). For German SA classi-
fication, there were no significant differences 
between the TiMBL, C4.5, and SNNS classifiers. 
For concept sequence classification, SNNS was 
significantly better than TiMBL and C4.5 (at 
least p=0.0001) for both English and German. 
For English only, TiMBL was significantly better 
than C4.5 (p=0.005). 
For both languages, the Rainbow classifier 
performed much worse than the other classifiers. 
However, the unigram model over arguments did 
not exploit the strengths of the n-gram classifica-
tion approach. Thus, we ran another experiment 
in which the Rainbow classifier was trained on 
simple word bigrams. No stemming or stop 
words were used in building the bigram models. 
 
 English German 
Domain Action 48.59% 48.09% 
Speech Act 79.00% 77.46% 
Concept Sequence 56.87% 57.77% 
Table 9: Rainbow accuracy with word bigrams. 
 
Table 9 shows the average accuracy of the 
Rainbow word bigram classifiers using the same 
20-fold cross-validation setup as in the previous 
experiments. As we expected, using word bi-
grams rather than parse label unigrams improved 
the performance of the Rainbow classifiers. For 
German DA classification, the word bigram clas-
sifier was significantly better than all of the pre-
vious German DA classifiers (at least p=0.005). 
Furthermore, the Rainbow word bigram SA clas-
sifiers for both languages outperformed all of the 
SA classifiers that used only the parse labels. 
Although the argument parse labels provide 
an abstraction of the words present in an SDU, 
the words themselves also clearly provided use-
ful information for classification, at least for the 
SA task. Thus, we conducted additional experi-
ments to examine whether combining parse and 
word information could further improve per-
formance. 
We chose to incorporate word information 
into the TiMBL classifiers used in the first ex-
periment. Although the SNNS SA classifier per-
formed significantly better than the TiMBL SA 
classifier for English, there was no significant 
difference for SA classification in German. Fur-
thermore, because of the complexity and time 
required for training with SNNS, we preferred 
working with TiMBL. 
We tested two approaches to adding word in-
formation to the TiMBL classifier. In both ap-
proaches, the word-based information for each 
fold was computed only based on the data in the 
training set. In our first approach, we added bi-
nary features for the 250 words that had the 
highest mutual information with the class. Each 
feature indicated the presence or absence of the 
word in the SDU. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 3 neighbors, and unweighted voting. The 
second approach we tested combined the Rain-
bow word bigram classifier with the TiMBL 
classifier. We added one input feature for each 
possible speech act to the TiMBL classifier. The 
value of each SA feature was the probability of 
the speech act computed by the Rainbow word 
bigram classifier. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 11 neighbors, and inverse linear distance 
weighted voting. 
 
 English German 
TiMBL + words 78.59% 75.98% 
TiMBL + Rainbow 81.25% 78.93% 
Table 10: Word+Parse SA classifier accuracy. 
 
Table 10 shows the average accuracy of the 
SA classifiers that combined parse and word in-
formation using the same 20-fold cross-
validation setup as the previous experiments. 
Although adding binary features for individual 
words improved performance over the classifiers 
with no word information, it did not allow the 
combined classifiers to outperform the Rainbow 
word bigram classifiers. However, for both 
languages, adding the probabilities computed by 
the Rainbow bigram model resulted in a SA clas-
sifier that outperformed all previous classifiers. 
The improvement in accuracy was highly signifi-
cant for both languages. 
We conducted a similar experiment for com-
bining parse and word information in the concept 
sequence classifiers. The first condition was 
analogous to the first condition in the combined 
SA classification experiment. The second condi-
tion was slightly different. A concept sequence 
can be broken down into a set of individual con-
cepts. The set of individual concepts is much 
smaller than the set of concept sequences (110 
for English and 111 for German). Thus, we used 
a Rainbow word bigram classifier to compute the 
probability of each individual concept rather than 
the complete concept sequence. The probabilities 
for the individual concepts were added to the 
parse label features for the combined classifier. 
In both conditions, the performance of the com-
bined classifiers was roughly the same as the 
classifiers that used only parse labels as features. 
 
 English German 
TiMBL + words 56.48% 54.98% 
Table 11: Word+Parse DA classifier accuracy. 
 
Table 11 shows the average accuracy of DA 
classifiers for English and German using a setup 
similar to the first approach in the combined SA 
experiment. In this experiment, we added binary 
features for the 250 words that the highest mu-
tual information with the class. We used a 
TiMBL classifier with gain ratio feature weight-
ing and one neighbor. The improvement in accu-
racy for both languages was highly significant. 
 
 English German 
TiMBL SA 
+ TiMBL CS 49.63% 46.50% 
TiMBL+Rainbow SA 
+ TiMBL CS 57.74% 53.93% 
Table 12: DA accuracy of SA+CS classifiers. 
 
Finally, Table 12 shows the results from two 
tests to compare the performance of combining 
the best output of the SA and concept sequence 
classifiers with the performance of the complete 
DA classifiers. In the first test, we combined the 
output from the TiMBL SA and CS classifiers 
shown in Table 7 and Table 8. The performance 
of the combined SA+CS classifiers was almost 
identical to that of the TiMBL DA classifiers 
shown in Table 6. In the second test, we com-
bined our best SA classifier (TiMBL+Rainbow, 
shown in Table 10) with the TiMBL CS classi-
fier. In this case, we had mixed results. The per-
formance of the combined classifiers was better 
than our best DA classifier for English and worse 
for German. 
9 Discussion 
One of our main goals was to determine the fea-
sibility of automatically classifying domain ac-
tions. As the data in Table 4 show, DA 
classification is a challenging problem with ap-
proximately 1000 classes. Even when the task is 
divided into subproblems of identifying the SA 
and concept sequence, the subtasks remain diffi-
cult. The difficulty is compounded by relatively 
sparse training data with unevenly distributed 
classes. Although the most common classes in 
our training corpus had over 1000 training exam-
ples, many of the classes had only 1 or 2 exam-
ples. 
Despite these difficulties, our results indicate 
that domain action classification is feasible. For 
SA classification in particular we were able to 
achieve very strong performance. Although per-
formance on concept sequence and DA classifi-
cation is not as high, it is still quite strong, 
especially given that there are an order of magni-
tude more classes than in SA classification. 
Based on our experiments, it appears that all of 
the learning approaches we tested were able to 
cope with data sparseness at the level found in 
our data, with the possible exception of the na?ve 
Bayes n-gram approach (Rainbow) for the con-
cept sequence task. 
One additional point worth noting is that there 
is evidence that domain action classification 
could be performed reasonably well using only 
word-based information. Although our best-
performing classifiers combined word and argu-
ment parse information, the na?ve Bayes word 
bigram classifier (Rainbow) performed very well 
on the SA classification task. With additional 
data, the performance of the concept sequence 
and DA word bigram classifiers could be ex-
pected to improve. Cattoni et al (2001) also ap-
ply statistical language models to DA 
classification. A word bigram model is trained 
for each DA, and the DA with the highest likeli-
hood is assigned to each SDU. Arguments are 
identified using recursive transition networks, 
and interlingua specification constraints are used 
to find the most likely valid interlingua represen-
tation. Although it is clear that argument infor-
mation is useful for the task, it appears that 
words alone can be used to achieve reasonable 
performance. 
Another goal of our experiments was to help 
in the selection of a machine learning approach 
to be used in our hybrid analyzer. Certainly one 
of the most important considerations is how well 
the learning approach performs the task. For SA 
classification, the combination of parse features 
and word bigram probabilities clearly gave the 
best performance. For concept sequence classifi-
cation, no learning approach clearly outper-
formed any other (with the exception that the 
na?ve Bayes n-gram approach performed worse 
than other approaches). However, the perform-
ance of the classifiers is not the only considera-
tion to be made in selecting the classifier for our 
hybrid analyzer. 
Several additional factors are also important 
in selecting the particular machine learning ap-
proach to be used. One important attribute of the 
learning approach is the speed of both classifica-
tion and training. Since the classifiers are part of 
a translation system designed for use between 
two humans to facilitate (near) real-time com-
munication, the DA classifiers must classify in-
dividual utterances online very quickly. 
Furthermore, since humans must write and test 
the argument grammars, training and batch 
classification should be fast so that the grammar 
writers can update the grammars, retrain the clas-
sifiers, and test efficiently. 
The machine learning approach should also 
be able to easily accommodate both continuous 
and discrete features from a variety of sources. 
Possible sources for features include words 
and/or phrases in an utterance, the argument 
parse, the interlingua representation of the argu-
ments, and properties of the dialogue (e.g. 
speaker tag). The classifier should be able to eas-
ily combine features from any or all of these 
sources. 
Another desirable attribute for the machine 
learning approach is the ability to produce a 
ranked list of possible classes. Our interlingua 
specification defines how speech acts and con-
cepts are allowed to combine as well as how ar-
guments are licensed by the domain action. 
These constraints can be used to select an alter-
native DA if the best DA violates the specifica-
tion. 
Based on all of these considerations, the 
TiMBL+Rainbow classifier, which combines 
parse label features with word bigram probabili-
ties, seems like an excellent choice for speech act 
classification. It was the most accurate classifier 
that we tested. Furthermore, the main TiMBL 
classifier meets all of the requirements discussed 
above except the ability to produce a complete 
ranked list of the classes for each instance. How-
ever, such a list could be produced as a backup 
from the Rainbow probability features. Adding 
new features to the combined classifier would 
also be very easy because TiMBL was the pri-
mary classifier in the combination. Finally, since 
both TiMBL and Rainbow provide an online 
server mode for classifying single instances, in-
corporating the combined classifier into an 
online translation system would not be difficult. 
Since there were no significant differences in the 
performance of most of the concept sequence 
classifiers, this combined approach is probably 
also a good option for that task. 
10 Conclusion 
We have described a representation of 
speaker intention that includes domain independ-
ent speech acts as well as domain dependent do-
main actions. We have shown that domain 
actions are a useful level of abstraction for ma-
chine translation of task-oriented dialogue, and 
that, in spite of their domain specificity, they are 
scalable to larger domains and portable to new 
domains.  
We have also presented classifiers for domain 
actions that have been comparatively tested and 
used successfully in the NESPOLE speech-to-
speech translation system. We experimentally 
compared the effectiveness of several machine-
learning approaches for classification of domain 
actions, speech acts, and concept sequences on 
two input languages. Despite the difficulty of the 
classification tasks due to a large number of 
classes and relatively sparse data, the classifiers 
exhibited strong performance on all tasks. We 
also demonstrated how the combination of two 
learning approaches could be used to improve 
performance and overcome the weaknesses of 
the individual approaches. 
Acknowledgements: NESPOLE was funded 
by NSF (Grant number 9982227) and the EU. 
The NESPOLE partners are ITC-irst, Universite 
Joseph Fourrier, Universitat Karlsruhe, APT 
Trentino travel board, and AETHRA telecom-
munications. We would like to acknowledge the 
contribution of the following people in particu-
lar: Fabio Pianesi, Emanuele Pianta, Nadia 
Mana, and Herve Blanchon. 
References 
Cattoni, R., M. Federico, and A. Lavie. 2001. Robust 
Analysis of Spoken Input Combining Statistical 
and Knowledge-Based Information Sources. In 
Proceedings of the IEEE ASRU Workshop, Trento, 
Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2002. TiMBL: Tilburg Memory 
Based Learner, version 4.3, Reference Guide. ILK 
Technical Report 02-10. Available from 
http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.
gz. 
Fillmore, C.J., Kay, P. and O'Connor, M.C. 1988. 
Regularity and Idiomaticity in Grammatical Con-
structions. Language, 64(3), 501-538. 
Gavald?, M. 2000. SOUP: A Parser for Real-World 
Spontaneous Speech. In Proceedings of IWPT-
2000, Trento, Italy. 
Goldberg, Adele E. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. 
Chicago University Press. 
Langley, C. and A. Lavie. 2003. Parsing Domain Ac-
tions with Phrase-Level Grammars and Memory-
Based Learners. To appear in Proceedings of 
IWPT-2003. Nancy, France. 
Langley, C., A. Lavie, L. Levin, D. Wallace, D. 
Gates, and K. Peterson. 2002. Spoken Language 
Parsing Using Phrase-Level Grammars and Train-
able Classifiers. In Workshop on Algorithms for 
Speech-to-Speech Machine Translation at ACL-02. 
Philadelphia, PA. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. Enhancing 
the Usability and Performance of NESPOLE! ? a 
Real-World Speech-to-Speech Translation System. 
In Proceedings of HLT-2002. San Diego, CA. 
Levin, L., D. Gates, A. Lavie, A. Waibel. 1998. An 
Interlingua Based on Domain Actions for Machine 
Translation of Task-Oriented Dialogues. In Pro-
ceedings of ICSLP 98, Vol. 4, pages 1155-1158, 
Sydney, Australia. 
Levin, L., D. Gates, D. Wallace, K. Peterson, A. La-
vie F. Pianesi, E. Pianta, R. Cattoni, N. Mana. 
2002. Balancing Expressiveness and Simplicity in 
an Interlingua for Task Based Dialogue. In Pro-
ceedings of Workshop on Spoken Language Trans-
lation. ACL-02, Philadelphia. 
McCallum, A. K. 1996. Bow: A toolkit for statistical 
language modeling, text retrieval, classification and 
clustering. 
http://www.cs.cmu.edu/~mccallum/bow. 
Qu, Y., B. DiEugenio, A. Lavie, L. Levin and C.P. 
Rose. 1997. Minimizing Cumulative Error in Dis-
course Context. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-96 
Workshop, E. Maier, M. Mast and S. LuperFoy 
(eds.), LNCS series, Springer Verlag. 
Qu, Y., C. P. Rose, and B. DiEugenio. 1996. Using 
Discourse Predictions for Ambiguity Resolution. In 
Proceedings of COLING-1996. 
Quinlan, J. R. 1993. C4.5: Programs for Machine 
Learning. San Mateo: Morgan Kaufmann. 
Reithinger, N., R. Engel, M. Kipp, M. Klesen. 1996. 
Predicting Dialogue Acts for a Speech-To-Speech 
Translation System. DFKI GmbH Saarbruecken. 
Verbmobil-Report 151. 
http://verbmobil.dfki.de/cgi-
bin/verbmobil/htbin/doc-access.cgi 
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. 
Bates, D. Jurafsky, P. Taylor, R. Martin, M. 
Meteer, and C. Van Ess-Dykema. 2000. Dialogue 
Act Modeling for Automatic Tagging and Recogni-
tion of Conversational Speech. Computational Lin-
guistics 26:3, 339-371.  
Zell, A., G. Mamier, M. Vogt, et al 1998. SNNS: 
Stuttgart Neural Network Simulator User Manual, 
Version 4.2. 
Balancing Expressiveness and Simplicity
in an Interlingua for Task Based Dialogue
Lori Levin, Donna Gates, Dorcas Wallace,
Kay Peterson, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
email: lsl@cs.cmu.edu
Fabio Pianesi, Emanuele Pianta,
Roldano Cattoni, Nadia Mana
IRST-itc, Italy
Abstract
In this paper we compare two interlin-
gua representations for speech transla-
tion. The basis of this paper is a distri-
butional analysis of the C-star II and
Nespole databases tagged with inter-
lingua representations. The C-star II
database has been partially re-tagged
with the Nespole interlingua, which
enables us to make comparisons on the
same data with two types of interlin-
guas and on two types of data (C-
star II and Nespole) with the same
interlingua. The distributional infor-
mation presented in this paper show
that the Nespole interlingua main-
tains the language-independence and
simplicity of the C-star II speech-act-
based approach, while increasing se-
mantic expressiveness and scalability.
1 Introduction
Several speech translation projects have chosen
interlingua-based approaches because of its con-
venience (especially in adding new languages)
in multi-lingual projects. However, interlingua
design is notoriously dicult and inexact. The
main challenge is deciding on the grain size of
meaning to represent and what facets of mean-
ing to include. This may depend on the do-
main and the contexts in which the translation
system is used. For projects that take place at
multiple research sites, another factor becomes
important in interlingua design: if the interlin-
gua is too complex, it cannot be used reliably by
researchers at remote sites. Furthermore, the in-
terlingua should not be biased toward one fam-
ily of languages. Finally, an interlingua should
clearly distinguish general and domain specic
components for easy scalability and portability
between domains.
Sections 2 and 3 describe how we balanced
the factors of grain-size, language independence,
and simplicity in two interlinguas for speech
translation projects | the C-star II Inter-
change Format (Levin et al, 1998) and the Ne-
spole Interchange Format. Both interlinguas
are based in the framework of domain actions
as described in (Levin et al, 1998). We will
show that the Nespole interlingua has a ner
grain-size of meaning, but is still simple enough
for collaboration across multiple research sites,
and still maintains language-independence.
Section 4 will address the issue of scalabil-
ity of interlinguas based on domain actions to
larger domains. The basis of Section 4 is a dis-
tributional analysis of the C-star II and Ne-
spole databases tagged with interlingua repre-
sentations. The C-star II database has been
partially re-tagged with the Nespole interlin-
gua, which enables us to make comparisons on
the same data with two types of interlinguas and
on two types of data (C-star II and Nespole)
with the same type of interlingua.
2 The C-star II Domain, Database,
and Interlingua
The C-star II interlingua (Levin et al, 1998)
was developed between 1997 and 1999 for use
in the C-star II 1999 demo (www.c-star.org).
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 53-60.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
c: can I have some flight times
that would leave some time around June sixth
a: the there are several flights leaving D C
there?d be one at one twenty four
there?s a three fifty nine flight
that arrives at four fifty eight
...
what time would you like to go
c: I would take the last one that you mentioned
...
a: what credit card number would you like
to reserve this with
c: I have a visa card
and the number is double oh five three
three one one six
ninety nine eighty seven
a okay
c: the expiration date is eleven ninety seven
...
a okay they should be ready tomorrow
c: okay thank you very much
Figure 1: Excerpt from a C-star II dialogue
with six participating research sites. The seman-
tic domain was travel, including reservations
and payments for hotels, tours, and transporta-
tion. Figure 1 shows a sample dialogue from
the C-star II database. (C is the client and a
is the travel agent.) The C-star II database
contains 2278 English sentences and 7148 non-
English (Japanese, Italian, Korean) sentences
tagged with interlingua representations. Most
of the database consists of transcripts of role-
playing conversations.
The driving concept behind the C-star II
interlingua is that there are a limited num-
ber of actions in the domain | requesting the
price of a room, telling the price of a room,
requesting the time of a flight, giving a credit
card number, etc. | and that each utter-
ance can be classied as an instance of one
of these domain actions . Figure 2 illustrates
the components of the C-star II interlingua:
(1) the speaker tag, in this case c for client,
(2) a speech act (request-action), (3) a list
of concepts (reservation, temporal, hotel),
(4) arguments (e.g., time), and (5) values of ar-
guments. The C-star II interlingua specica-
tion document contains denitions for 44 speech
acts, 93 concepts, and 117 argument names.
The domain action is the part of the interlin-
gua consisting of the speech act and concepts, in
this case request-action+reservation+tem-
poral+hotel. The domain action does not in-
clude the list of argument-value pairs.
First it is important to point out that do-
main actions are created compositionally. A do-
main action consists of a speech act followed by
zero or more concepts. (Recall that argument-
value pairs are not part of the domain action.)
The Nespole interlingua includes 65 speech
acts and 110 concepts. An interlingua speci-
cation document denes the legal combinations
of speech acts and arguments.
The linguistic justication for an interlingua
based on domain-actions is that many travel do-
main utterances contain xed, formulaic phrases
(e.g., can you tell me; I was wondering; how
about; would you mind, etc.) that signal domain
actions, but either do not translate literally into
other languages or have a meaning that is su-
ciently indirect that the literal meaning is irrele-
vant for translation. To take two examples, how
about as a signal of a suggestion does not trans-
late into other languages with the words corre-
sponding to how and about . Also, would you
mind might translate literally into some Euro-
pean languages as a way of signaling a request,
but the literal meaning of minding is not rel-
evant to the translation, only the fact that it
signals politeness.
The measure of success for the domain-action
based interlingua (as described in (Levin et al,
2000a)) is that (1) it covers the data in the C-
star II database with less than 8% no-tag rate,
(2) inter-coder agreement across research sites
is reasonably high: 82% for speech acts, 88%
for concepts, and 65% for domain actions, and
(3) end-to-end translation results using an an-
alyzer and generator written at dierent sites
were about the same as end-to-end translation
results using an analyzer and generator written
at the same site.
3 The Nespole Domain, Database,
and Interlingua
The Nespole interlingua has been under devel-
opment for the last two years as part of the Ne-
spole project (http://nespole.itc.it). Fig-
I would like to make a hotel reservation for the fourth through
the seventh of july
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Figure 2: Example of a C-star II interlingua representation
ure 3 shows a Nespole dialogue. The Ne-
spole domain does not include reservations and
payments, but includes more detailed inquiries
about hotels and facilities for ski vacations and
summer vacations in Val di Fiemme, Italy. (The
tourism board of the Trentino area is a partner
of the Nespole project.) Most of the database
consists of transcripts of dialogues between an
Italian-speaking travel agent and an English or
German speaker playing the role of a traveller.
There are fewer xed, formulaic phrases in the
Nespole domain, prompting us to move toward
domain actions that are more general, and also
requiring more detailed interlingua representa-
tions. Changes from the C-star II interlingua
fall into several categories:
1. Extending semantic expressivity and
syntactic coverage: Increased coverage of
modality, tense, aspect, articles, fragments,
coordinate structures, number, and rhetor-
ical relations. In addition, we have added
more explicit representation of grammati-
cal relations and improved capabilities for
representing modication and embedding.
2. Additional Domain-Specic Con-
cepts: New concepts include giving
directions, describing sizes and dimensions
of objects, traveling routes, equipment and
gear, airports, tourist services, facilities,
vehicles, information objects (brochures,
web pages, rules and regulations), hours
of operation of businesses and attractions,
etc.
3. Utterances that accompany multi-
modal gestures: The Nespole system
includes capabilities to share web pages
and draw marks such as circles and arrows
on web pages. The interlingua was ex-
tended to cover colord, descriptions of two-
dimensional objects, and actions of show-
ing.
4. General concept names from Word-
Net: The Nespole interlingua includes
conventions for making new concept names
based on WordNet synsets.
5. More general domain actions replac-
ing specic ones: For example, replacing
hotel with accommodation.
Interlinguas based on domain actions con-
trast with interlinguas based on lexical seman-
tics (Dorr, 1993; Lee et al, 2001; Goodman and
Nirenburg, 1991). A lexical-semantic interlingua
includes a representation of predicates and their
arguments. For example, the sentence I want to
take a vacation has a predicate want with two
arguments I and to take a vacation, which in
turn has a predicate take and two arguments, I
and a vacation. Of course, predicates like take
may be represented as word senses that are less
language-dependent like participate-in. The
strength and weakness of the lexical-semantic
approach is that it is less domain dependent
than the domain-action approach.
In order to cover the less formulaic utterances
of the Nespole domain, we have taken a step
closer to the lexical-semantic approach. How-
ever, we have maintained the overall framework
of the domain-action approach because there are
still many formulaic utterances that are better
represented in a non-literal way. Also, in or-
der to abstract away from English syntax, con-
cepts such as disposition, eventuality, and obli-
gation are not represented in the interlingua as
argument-taking main verbs in order to accom-
modate languages in which these meanings are
c: and I have some questions about coming about a trip I?m gonna be taking to Trento
a: okay what are your questions
c: I currently have a hotel booking at the
Panorama-Hotel in Panchia but at the moment I have no idea how to get to my hotel from Trento
and I wanted to ask what would be the best way for me to get there
a: okay I?m gonna show you a map that and then describe the directions to you
okay so right so you will arrive in the train station in Trento
the that is shown in the middle of the map stazione FFSS
and just below that here is a bus stop labeled number forty
so okay on the map that I?m showing you here
the hotel is the orange building off on the right hand side
...
c: I also wanted to ask about skiing in the area once I?m in Panchia
a: all right just a moment and I?ll show you another map
c: okay
a: okay so on the map you see now Panchia is right in the center of the map
c: I see it
Figure 3: Excerpt from a Nespole dialogue
represented as adverbs or suxes on verbs. Fig-
ure 4 shows the Nespole interlingua represen-
tation corresponding to the C-star II interlin-
gua in Figure 2. The specication document for
the Nespole interlingua denes 65 speech acts,
110 concepts, 292 arguments, and 7827 values
grouped into 222 value classes. As in the C-
star II interlingua, domain actions are dened
compositionally from speech acts and arguments
in combinations that are allowed by the interlin-
gua specication.
3.1 Comparison of Nespole and
C-star II Interlinguas
It is useful to compare the Nespole and C-
star II Interlinguas in expressivity, language in-
dependence, and simplicity.
Expressivity of the Nespole interlingua,
Argument 1: The metric we use for expres-
sivity is the no-tag rate in the databases. The
no-tag rate is the percentage of sentences that
cannot be assigned an interlingua representation
by a human expert. The C-star II database
tagged with C-star II interlingua had a no-
tag rate of 7.3% (Levin et al, 2000a). The
C-star II database tagged with Nespole in-
terlingua has a no-tag rate of 2.4%. More than
300 English sentences in the C-star II database
that were not covered by the C-star II interlin-
gua are now covered by the Nespole interlin-
gua. (See Table 2.) We conclude from this that
the Nespole interlingua is more expressive in
that it covers more data.
Language-independence of the Nespole
interlingua: We do not have a numerical
measure of language-independence, but we note
that interlinguas based on domain actions are
particularly suitable for avoiding translation
mismatches (Dorr, 1994), particularly head-
switching mismatches (e.g., I just arrived and
Je vient d?arriver where the meaning of recent
past is expressed by an adverb just or a syn-
tactic verb vient (venir).) Interlinguas based
on domain actions resolve head-switching mis-
matches by identifying the types of meanings
that are often involved in mismatches | modal-
ity, evidentiality, disposition, and so on | and
assigning them a representation that abstracts
away from predicate argument structure. In-
terlinguas based on domain actions also neu-
tralize the dierent ways of expressing indirect
speech acts within and across languages (for ex-
ample, Would you mind..., I was wondering if
you could...., and Please.... as ways of request-
ing an action). Although Nespole domain ac-
tions are more general than C-star II domain
actions, they maintain language independence
by abstracting away from predicate-argument
structure.
Simplicity and cross-site reliability of the
Nespole interlingua: Simplicity of an inter-
lingua is measured by cross-site reliability in
I would like to make a hotel reservation for the fourth through
the seventh of july
C-star II Interlingua:
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Nespole Interlingua:
c:give-information+disposition+reservation+accommodation
(disposition=(who=i, desire),
reservation-spec=(reservation, identifiability=no),
accommodation-spec=hotel,
object-time=(start-time=(md=4), end-time=(md=7, month=7, incl-excl=inclusive)))}
Figure 4: Example of Nespole interlingua representation
inter-coder agreement and end-to-end transla-
tion performance. At the time of writing this pa-
per we have not conducted cross-site inter-coder
agreement experiments using the Nespole in-
terlingua. We have, however, conducted cross-
site evaluations (Lavie et al, 2002), in which the
analyzer and generator were written at dier-
ent sites. Experiments at the end of C-star II
showed that cross-site evaluations were compa-
rable to intra-site evaluations (analyzer and gen-
erator written at the same site) (Levin et al,
2000b). Nespole evaluations so far show a loss
of cross-site reliability: intra-site evaluations are
noticeably better than cross-site evaluations, as
reported in (Lavie et al, 2002). This seems to
indicate that developers at dierent sites have
a lower level of agreement on the Nespole in-
terlingua. However there are other possible ex-
planations for the discrepancy | for example
developers at dierent sites may have focused
their development on dierent sub-domains |
that are currently under investigation.
4 Scalability of the Nespole
Interlingua
The rest of this paper addresses the scalability
of the Nespole interlingua. A possible criti-
cism of domain actions is that they are domain
dependent and that the number of domain ac-
tions might increase too quickly with the size
of the domain. In this section, we will examine
the rate of increase in the number of domain ac-
tions as a function of the amount of data and
the diversity of the data.
Dierences in the C-star and Nespole Do-
mains: We will rst show that the C-star
and Nespole domains are signicantly dierent
even though they both pertain to travel. The
combination of the two domains is therefore sig-
nicantly larger than either domain alone.
In order to demonstrate the dierences be-
tween the C-star travel domain and the Ne-
spole travel domain, we measured the overlap
in vocabulary. The numbers in Table 4 are based
on the rst 7900 word tokens in the C-star En-
glish database and the rst 7900 word tokens
in the Nespole English database. The table
shows the number of unique word types in each
database, the number of word types that occur
in both databases, and the number of word types
that occur in one of the databases, but not in the
other. In each database, about half of the word
types overlap with the other database. The non-
overlapping vocabulary (402 C-star word types
and 344 Nespole word types) indicates that the
two databases cover quite dierent aspects of the
travel domain.
Scalability: Argument 1: We will now be-
gin to address the issue of scalability of the
domain action approach to interlingua design.
Our rst argument concerns the number of
Number of unique word types
CSTAR English 745
Nespole English 687
Word types in both CSTAR and Nespole 343
Words types in CSTAR not in Nespole 402
Words types n Nespole not in CSTAR 344
Table 1: Number of overlapping word types in the C-star English and Nespole English
databases
SA Con. Snts. Domain Ac-
tions
Old C-star English 44 93 2278 358
New C-star English 65 110 2564 452
Nespole English 65 110 1446 337
Nespole German 65 110 3298 427
Nespole Italian 65 110 1063 206
Table 2: Number of unique domain actions in interlingua databases
speech acts and concepts in the combined C-
star/Nespole domain. The C-star II in-
terlingua, designed for coverage of the C-star
travel domain, included 44 speech acts and 93
concepts. The Nespole interlingua, designed
for coverage of the combined C-star and Ne-
spole domains, has 65 speech acts and 110 con-
cepts. Thus a relatively small increase in the
number of speech acts and concepts is required
to cover a signicantly larger domain.
The increased size of the C-star/Nepsole
domain is reflected in the number of arguments
and values. The C-star II interlingua contained
denitions for 117 arguments, whereas the Ne-
spole interlingua contains denitions for 292 ar-
guments. The number of values for arguments
also has increased signicantly in the Nespole
domain. There are 7827 values grouped into 222
classes (airport names, days of the week, etc.).
Distributional Data: number of domain
actions in each database: Next we will
present distributional data concerning the num-
ber of domain actions as a function of database
size. We will compare several databases: Old
C-star English (around 2278 sentences tagged
with C-star II interlingua), New C-star En-
glish (2564 sentences tagged with Nespole in-
terlingua, including the 2278 sentences from Old
C-star English), Nespole English, Nespole
German, and Nespole Italian. Table 2 shows
the number of sentences and the number of do-
main actions in each database. The number of
domain actions refers to the number of types,
not tokens, of domain actions.
Distributional data: Coverage of the top
50 domain actions: Table 3 shows the per-
centage of each database that is covered by the
5, 10, 20, and 50 most frequent domain actions
in that database. For each database, the do-
main actions were ordered by frequency. The
percentage of sentences covered by the top-n
domain actions was then calculated. For this
experiment, we separated sentences spoken by
the traveller (client) and sentences spoken by
the travel agent (agent). C-star data in Ta-
ble 3 refers to 2564 English sentences from the
C-star database that were tagged with Ne-
spole interlingua. Nespole data refers to the
English portion of the Nespole database (1446
sentences). Combined data refers to the combi-
nation of the two (4014 sentences).
Two points are worth noting about Table 3.
First, the Nespole agent data has a higher cov-
erage rate than the Nespole client data. That
is, more data is covered by the top-n domain
actions. This may be because there was was
Domain Actions Top 5 Top 10 Top 20 Top 50
Client
C-star data 33.6 42.7 53.1 66.7
Nespole data 31.7 43.5 53.9 66.5
Combined data 31.6 40.0 50.3 62.9
Agent
C-star data 33.8 42.8 54.1 67.3
Nespole data 39.0 47.8 56.1 71.4
Combined data 33.6 41.5 51.7 64.0
Table 3: DA Coverage using Nespole interlingua on English data for both C-star and
Nespole
only a small amount of English agent data and
it was spoken by non-native speakers. Second,
the combined data has a slightly lower cover-
age rate than either the C-star or Nespole
databases alone. This is expected because, as
shown above, the combined domain is signi-
cantly more diverse than either domain by itself.
Scalability: Argument 2: Table 3 provides
additional evidence for the scalability of the Ne-
spole interlingua to larger domains. In the
combined C-star and Nespole domain, the
top 50 domain actions cover only slightly less
data than the top 50 domain actions in either
domain separately. There is not, in fact, an ex-
plosion of domain actions when the two C-star
and Nespole domains are combined.
Distributional Data: domain actions as a
function of database size: Table 3 shows
that in each of our databases, the 50 most fre-
quent domain actions cover approximately 65%
of the sentences. The next issue we address is
the nature of the \tail" of less frequent domain
actions covering the remainder of the data.
Figure 5 shows the number of domain actions
as a function of data set size. Sampling was done
for intervals of 25 sentences starting at 100 sen-
tences. For each sample size s there was ten-fold
cross-validation. Ten random samples of size s
were chosen, and the number of dierent domain
actions in each sample was counted. The aver-
age of the number of domain actions in each of
the ten samples of size s are plotted in Figure 5.
The four databases represented in Figure 5 are
IF Coverage of Four Datasets
0
100
200
300
400
500
600
700
10
0
70
0
13
00
19
00
25
00
31
00
number of SDUs in sample
av
er
ag
e 
nu
m
be
r o
f u
ni
qu
e 
DA
s 
o
ve
r 
10
 ra
nd
om
 s
am
pl
es
Old CSTAR
New CSTAR
NESPOLE
Combined
Figure 5: Number of domain actions as a function of
database size
the C-star English database tagged with C-
star II interlingua, the C-star II database
tagged with Nespole interlingua, the Nespole
English database, and the combined C-star
and Nespole English databases.
Expressivity, Argument 2: Figure 5 pro-
vides evidence for the increased expressivity of
the Nespole interlingua. In contrast to Ta-
ble 3, which deals with samples containing the
most frequent domain actions, the samples plot-
ted in Figure 5 contain random mixtures of fre-
quent and non-frequent domain actions. The
curve representing the C-star data with C-
star II interlingua is the slowest growing of the
four curves. This is because the grain-size of
meaning represented in the C-star II interlin-
gua was larger than in the Nespole interlin-
gua. Also many infrequent domain actions were
not covered by the C-star II interlingua. The
faster growth of the curve representing the C-
star data with Nespole interlingua indicates
improved expressivity of the Nespole interlin-
gua | it covers more of the infrequent domain
actions. The highest curve in Figure 5 repre-
sents the combined C-star and Nespole do-
mains. This curve is higher than the others be-
cause, as shown above, the two travel domains
are signicantly dierent from each other.
Expressivity and Simplicity, the right bal-
ance: Comparing Table 3 and Figure 5, we ar-
gue that the Nespole interlingua strikes a good
balance between expressivity and simplicity. Ta-
ble 3 shows evidence for the simplicity of the Ne-
spole interlingua: Only 50 domain actions are
needed to cover 60-70% of the sentences in the
database. Figure 5 shows evidence for expressiv-
ity: because domain actions are compositionally
formed from speech acts and concepts, it is pos-
sible to form a large number of low-frequency
domain actions in order to cover the domain.
Over 600 domain actions are used in the com-
bined C-star and Nespole domains.
5 Conclusions
We have presented a comparison of a purely
domain-action-based interlingua (the C-star II
interlingua) and a more expressive, but still
domain-action-based interlingua (the Nespole
interlingua). The data that we have presented
show that the more expressive interlingua has
better coverage of the domain (a decrease from
7.3% to 2.4% uncovered data in the C-star II
domain) and can also scale up to larger domains
without an explosion of domain actions. Thus
we have a reasonable compromise between sim-
plicity and expressiveness of the interlingua.
Acknowledgments
We would like to acknowledge Hans-Ulrich Block
for rst proposing the domain-action-based in-
terlingua to the C-star consortium. We would
also like to thank all of the C-star and Ne-
spole partners who have participated in the de-
sign of the interlingua. This work was supported
by NSF Grant 9982227 and EU Grant IST 1999-
11562 as part of the joint EU/NSF MLIAM re-
search initiative.
References
Bonnie J. Dorr. 1993. Machine Translation: A View
from the Lexicon. The MIT Press, Cambridge,
Massachusetts.
Bonnie J. Dorr. 1994. Machine Translation Diver-
gences: A Formal Description and Proposed Solu-
tion. Computational Linguistics, 20(4):597{633.
Kenneth Goodman and Sergei Nirenburg. 1991.
The KBMT Project: A Case Study in Knowledge-
Based Machine Translation. Morgan Kaufmann,
San Mateo, CA.
Alon Lavie, Florian Metze, Roldano Cattoni, and Er-
ica Constantini. 2002. A Multi-Perspective Eval-
uation of the NESPOLE! Speech-to-Speech Trans-
lation System. In Proceedings of Speech-to-Speech
Translation: Algorithms and Systems.
Young-Suk Lee, W. Yi, Cliord Weinstein, and
Stephanie Sene. 2001. Interlingua-based broad-
coverage korean-to-english translation. In Pro-
ceedings of HLT, San Diego.
Lori Levin, Donna Gates, Alon Lavie, and Alex
Waibel. 1998. An Interlingua Based on Domain
Actions for Machine Translation of Task-Oriented
Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (IC-
SLP?98), pages Vol. 4, 1155{1158, Sydney, Aus-
tralia.
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi,
Dorcas Wallace, Taro Watanabe, and Monika
Woszczyna. 2000a. Evaluation of a Practical In-
terlingua for Task-Oriented Dialogue. In Work-
shop on Applied Interlinguas: Practical Applica-
tions of Interlingual Approaches to NLP, Seattle.
Lori Levin, Alon Lavie, Monika Woszczyna, Donna
Gates, Marsal Gavalda, Detlef Koll, and Alex
Waibel. 2000b. The Janus-III Translation Sys-
tem. Machine Translation.
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17?53,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Findings of the 2010 Joint Workshop on
Statistical Machine Translation and Metrics for Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb@cs.jhu.edu
Philipp Koehn
University of Edinburgh
pkoehn@inf.ed.ac.uk
Christof Monz
University of Amsterdam
c.monz@uva.nl
Kay Peterson and Mark Przybocki
National Institute of Standards and Technology
kay.peterson,mark.przybocki@nist.gov
Omar F. Zaidan
Johns Hopkins University
ozaidan@cs.jhu.edu
Abstract
This paper presents the results of the
WMT10 and MetricsMATR10 shared
tasks,1 which included a translation task,
a system combination task, and an eval-
uation task. We conducted a large-scale
manual evaluation of 104 machine trans-
lation systems and 41 system combina-
tion entries. We used the ranking of these
systems to measure how strongly auto-
matic metrics correlate with human judg-
ments of translation quality for 26 metrics.
This year we also investigated increasing
the number of human judgments by hiring
non-expert annotators through Amazon?s
Mechanical Turk.
1 Introduction
This paper presents the results of the shared
tasks of the joint Workshop on statistical Ma-
chine Translation (WMT) and Metrics for MA-
chine TRanslation (MetricsMATR), which was
held at ACL 2010. This builds on four previ-
ous WMT workshops (Koehn and Monz, 2006;
Callison-Burch et al, 2007; Callison-Burch et al,
2008; Callison-Burch et al, 2009), and one pre-
vious MetricsMATR meeting (Przybocki et al,
2008). There were three shared tasks this year:
a translation task between English and four other
European languages, a task to combine the out-
put of multiple machine translation systems, and
a task to predict human judgments of translation
quality using automatic evaluation metrics. The
1The MetricsMATR analysis was not complete in time for
the publication deadline. An updated version of paper will be
made available on http://statmt.org/wmt10/ prior
to July 15, 2010.
performance on each of these shared task was de-
termined after a comprehensive human evaluation.
There were a number of differences between
this year?s workshop and last year?s workshop:
? Non-expert judgments ? In addition to hav-
ing shared task participants judge translation
quality, we also collected judgments from
non-expert annotators hired through Ama-
zon?s Mechanical Turk. By collecting a large
number of judgments we hope to reduce the
burden on shared task participants, and to in-
crease the statistical significance of our find-
ings. We discuss the feasibility of using non-
experts evaluators, by analyzing the cost, vol-
ume and quality of non-expert annotations.
? Clearer results for system combination ?
This year we excluded Google translations
from the systems used in system combina-
tion. In last year?s evaluation, the large mar-
gin between Google and many of the other
systems meant that it was hard to improve on
when combining systems. This year, the sys-
tem combinations perform better than their
component systems more often than last year.
? Fewer rule-based systems ? This year there
were fewer rule-based systems submitted. In
past years, University of Saarland compiled a
large set of outputs from rule-based machine
translation (RBMT) systems. The RBMT
systems were not submitted this year. This
is unfortunate, because they tended to outper-
form the statistical systems for German, and
they were often difficult to rank properly us-
ing automatic evaluation metrics.
The primary objectives of this workshop are to
evaluate the state of the art in machine transla-
17
tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. As with past years, all of the
data, translations, and human judgments produced
for our workshop are publicly available.2 We hope
they form a valuable resource for research into sta-
tistical machine translation, system combination,
and automatic evaluation of translation quality.
2 Overview of the shared translation and
system combination tasks
The workshop examined translation between En-
glish and four other languages: German, Span-
ish, French, and Czech. We created a test set for
each language pair by translating newspaper arti-
cles. We additionally provided training data and
two baseline systems.
2.1 Test data
The test data for this year?s task was created
by hiring people to translate news articles that
were drawn from a variety of sources from mid-
December 2009. A total of 119 articles were se-
lected, in roughly equal amounts from a variety
of Czech, English, French, German and Spanish
news sites:3
Czech: iDNES.cz (5), iHNed.cz (1), Lidov-
ky (16)
French: Les Echos (25)
Spanish: El Mundo (20), ABC.es (4), Cinco
Dias (11)
English: BBC (5), Economist (2), Washington
Post (12), Times of London (3)
German: Frankfurter Rundschau (11), Spie-
gel (4)
The translations were created by the profes-
sional translation agency CEET4. All of the trans-
lations were done directly, and not via an interme-
diate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
2http://statmt.org/wmt10/results.html
3For more details see the XML test files. The docid
tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.
4http://www.ceet.eu/
train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.
2.3 Baseline systems
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits
for phrase-based and parsing-based statistical ma-
chine translation (Koehn et al, 2007; Li et al,
2009).
2.4 Submitted systems
We received submissions from 33 groups from 29
institutions, as listed in Table 1, a 50% increase
over last year?s shared task.
We also evaluated 2 commercial off the shelf
MT systems, and two online statistical machine
translation systems. We note that these companies
did not submit entries themselves. The entries for
the online systems were done by translating the
test data via their web interfaces. The data used
to train the online systems is unconstrained. It is
possible that part of the reference translations that
were taken from online news sites could have been
included in the online systems? language models.
2.5 System combination
In total, we received 153 primary system submis-
sions along with 28 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year?s system combina-
tion task, we provided two additional resources to
participants:
? Development set: We reserved 25 articles
to use as a dev set for system combination.
These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.
? n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 20 n-best lists accompa-
nying the system submissions.
Table 2 lists the 9 participants in the system
combination task.
3 Human evaluation
As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
18
Europarl Training Corpus
Spanish? English French? English German? English
Sentences 1,650,152 1,683,156 1,540,549
Words 47,694,560 46,078,122 50,964,362 47,145,288 40,756,801 43,037,967
Distinct words 173,033 95,305 123,639 95,846 316,365 92,464
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 98,598 84,624 100,269 94,742
Words 2,724,141 2,432,064 2,405,082 2,101,921 2,505,583 2,443,183 2,050,545 2,290,066
Distinct words 69,410 46,918 53,763 43,906 101,529 47,034 125,678 45,306
United Nations Training Corpus
Spanish? English French? English
Sentences 6,222,450 7,230,217
Words 213,877,170 190,978,737 243,465,100 216,052,412
Distinct words 441,517 361,734 402,491 412,815
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 7,227,409
Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770
Europarl Language Model Data
English Spanish French German
Sentence 1,843,035 1,822,021 1,855,589 1,772,039
Words 50,132,615 51,223,902 54,273,514 43,781,217
Distinct words 99,206 178,934 127,689 328,628
News Language Model Data
English Spanish French German Czech
Sentence 48,653,884 3,857,414 15,670,745 17,474,133 13,042,040
Words 1,148,480,525 106,716,219 382,563,246 321,165,206 205,614,201
Distinct words 1,451,719 548,169 998,595 1,855,993 1,715,376
News Test Set
English Spanish French German Czech
Sentences 2489
Words 62,988 65,654 68,107 62,390 53,171
Distinct words 9,457 11,409 10,775 12,718 15,825
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and
the number of distinct words is based on the provided tokenizer.
19
ID Participant
AALTO Aalto University, Finland (Virpioja et al, 2010)
CAMBRIDGE Cambridge University (Pino et al, 2010)
CMU Carnegie Mellon University?s Cunei system (Phillips, 2010)
CMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2010)
COLUMBIA Columbia University
CU-BOJAR Charles University Bojar (Bojar and Kos, 2010)
CU-TECTO Charles University Tectogramatical MT (Z?abokrtsky? et al, 2010)
CU-ZEMAN Charles University Zeman (Zeman, 2010)
DCU Dublin City University (Penkale et al, 2010)
DFKI Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (Federmann et al, 2010)
EU European Parliament, Luxembourg (Jellinghaus et al, 2010)
EUROTRANS commercial MT provider from the Czech Republic
FBK Fondazione Bruno Kessler (Hardmeier et al, 2010)
GENEVA University of Geneva
HUICONG Shanghai Jiao Tong University (Cong et al, 2010)
JHU Johns Hopkins University (Schwartz, 2010)
KIT Karlsruhe Institute for Technology (Niehues et al, 2010)
KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al, 2010)
LIMSI LIMSI (Allauzen et al, 2010)
LIU Linko?ping University (Stymne et al, 2010)
LIUM University of Le Mans (Lambert et al, 2010)
NRC National Research Council Canada (Larkin et al, 2010)
ONLINEA an online machine translation system
ONLINEB an online machine translation system
PC-TRANS commercial MT provider from the Czech Republic
POTSDAM Potsdam University
RALI RALI - Universite? de Montre?al (Huet et al, 2010)
RWTH RWTH Aachen (Heger et al, 2010)
SFU Simon Fraser University (Sankaran et al, 2010)
UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010)
UEDIN University of Edinburgh (Koehn et al, 2010)
UMD University of Maryland (Eidelman et al, 2010)
UPC Universitat Polite`cnica de Catalunya (Henr??quez Q. et al, 2010)
UPPSALA Uppsala University (Tiedemann, 2010)
UPV Universidad Polite?cnica de Valencia (Sanchis-Trilles et al, 2010)
UU-MS Uppsala University - Saers (Saers et al, 2010)
Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.
20
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2010)
CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010)
CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010)
DCU-COMBO Dublin City University system combination (Du et al, 2010)
JHU-COMBO Johns Hopkins University system combination (Narsale, 2010)
KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIUM-COMBO University of Le Mans system combination (Barrault, 2010)
RWTH-COMBO RWTH Aachen system combination (Leusch and Ney, 2010)
UPV-COMBO Universidad Polite?cnica de Valencia (Gonza?lez-Rubio et al, 2010)
Table 2: Participants in the system combination task.
Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 5,212 830 824
English-German 6,847 755 751
Spanish-English 5,653 845 845
English-Spanish 2,587 920 690
French-English 4,147 925 921
English-French 3,981 1,325 1,223
Czech-English 2,688 490 488
English-Czech 6,769 1,165 1,163
Totals 37,884 7,255 6,905
Table 3: The number of items that were collected for each task during the manual evaluation. An item
is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no
judgment in the judgment task.
21
that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 120 people partic-
ipated in the manual evaluation5, with 89 people
putting in more than an hour?s worth of effort, and
29 putting in more than four hours. A collective
total of 337 hours of labor was invested.6
We asked people to evaluate the systems? output
in two different ways:
? Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.
? Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.
The total number of judgments collected for the
different modes of annotation is given in Table 3.
In all cases, the output of the various translation
systems were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:
Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).
5We excluded data from three errant annotators, identified
as follows. We considered annotators completing at least 3
screens, whose P (A) with others (see 3.2) is less than 0.33.
Out of seven such annotators, four were affiliated with shared
task teams. The other three had no apparent affiliation, and
so we discarded their data, less than 5% of the total data.
6Whenever an annotator appears to have spent more than
ten minutes on a single screen, we assume they left their sta-
tion and left the window open, rather than actually needing
more than ten minutes. In those cases, we assume the time
spent to be ten minutes.
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions. For each of the lan-
guage pairs, there were more than 5 submissions.
We did not attempt to get a complete ordering over
the systems, and instead relied on random selec-
tion and a reasonably large sample size to make
the comparisons fair.
Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
We were interested in determining the inter- and
intra-annotator agreement for the ranking task,
since a reasonable degree of agreement must ex-
ist to support our process as a valid evaluation
setup. To ensure we had enough data to measure
agreement, we purposely designed the sampling of
source segments shown to annotators so that items
were likely to be repeated, both within an annota-
tor?s assigned tasks and across annotators. We did
so by assigning an annotator a batch of 20 screens
(each with three ranking sets; see 3.1) that were to
be completed in full before generating new screens
for that annotator.
Within each batch, the source segments for nine
of the 20 screens (45%) were chosen from a small
pool of 60 source segments, instead of being sam-
pled from the larger pool of 1,000 source segments
designated for the ranking task.7 The larger pool
was used to choose source segments for nine other
screens (also 45%). As for the remaining two
screens (10%), they were chosen randomly from
the set of eighteen screens already chosen. Fur-
thermore, in the two ?local repeat? screens, the
system choices were also preserved.
Heavily sampling from a small pool of source
segments ensured we had enough data to measure
inter-annotator agreement, while purposely mak-
ing 10% of each annotator?s screens repeats of pre-
viously seen sets in the same batch ensured we
7Each language pair had its own 60-sentence pool, dis-
joint from other language pairs? pools, but ach of the 60-
sentence pools was a subset of the 1,000-sentence pool.
22
INTER-ANNOTATOR AGREEMENT
P (A) K
With references 0.658 0.487
Without references 0.626 0.439
WMT ?09 0.549 0.323
INTRA-ANNOTATOR AGREEMENT
P (A) K
With references 0.755 0.633
Without references 0.734 0.601
WMT ?09 0.707 0.561
Table 4: Inter- and intra-annotator agreement for
the sentence ranking task. In this task, P (E) is
0.333.
had enough data to measure intra-annotator agree-
ment.
We measured pairwise agreement among anno-
tators using the kappa coefficient (K), which is de-
fined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance.
For inter-annotator agreement for the ranking
tasks we calculated P (A) by examining all pairs
of systems which had been judged by two or more
judges, and calculated the proportion of time that
they agreed thatA > B, A = B, orA < B. Intra-
annotator agreement was computed similarly, but
we gathered items that were annotated on multiple
occasions by a single annotator.
Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The exact interpretation
of the kappa coefficient is difficult, but according
to Landis and Koch (1977), 0? .2 is slight, .2? .4
is fair, .4 ? .6 is moderate, .6 ? .8 is substantial
and the rest is almost perfect.
Based on these interpretations the agreement
for sentence-level ranking is moderate for inter-
annotator agreement and substantial for intra-
annotator agreement. These levels of agreement
are higher than in previous years, partially due to
the fact that that year we randomly included the
references along the system outputs. In general,
judges tend to rank the reference as the best trans-
lation, so people have stronger levels of agreement
when it is included. That said, even when compar-
isons involving reference are excluded, we still see
an improvement in agreement levels over last year.
3.3 Editing machine translation output
In addition to simply ranking the output of sys-
tems, we also had people edit the output of MT
systems. We did not show them the reference
translation, which makes our edit-based evalu-
ation different from the Human-targeted Trans-
lation Edit Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people?s understanding of the out-
put.
The instructions given to our judges were as fol-
lows:
Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select ?No corrections
needed.? If you cannot understand the
sentence well enough to correct it, select
?Unable to correct.?
A screenshot is shown in Figure 2. This year,
judges were shown the translations of 5 consec-
utive source sentences, all produced by the same
machine translation system. In last year?s WMT
evaluation they were shown only one sentence at a
time, which made the task more difficult because
the surrounding context could not be used as an
aid to understanding.
Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system?s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system?s output.
3.4 Judging the acceptability of edited output
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:
23
Edit Machine Translation Outputs
Instructions:
You are shown several machine translation outputs.
Your task is to edit each translation to make it as fluent as possible.
It is possible that the translation is already fluent. In that case, select No corrections needed.
If you cannot understand the sentence well enough to correct it, select Unable to correct.
The sentences are all from the same article. You can use the earlier and later sentences
to help understand a confusing sentence.
Your edited translations           The machine translations
   
The shortage of snow in mountain worries the hoteliers
Edited     No corrections needed     Unable to
correct         Reset
 
The shortage of snow in mountain
worries the hoteliers
   
The deserted tracks are not putting down problem only at the exploitants 
of skilift.
Edited     No corrections needed     Unable to
correct         Reset
 
The deserted tracks are not
putting down problem only at the
exploitants of skilift.
   
The lack of snow deters the people to reserving their stays at the ski in 
the hotels and pension.
Edited     No corrections needed     Unable to
correct         Reset
 
The lack of snow deters the people
to reserving their stays at the ski
in the hotels and pension.
   
Thereby, is always possible to track free bedrooms for all the dates in 
winter, including Christmas and Nouvel An.
Edited     No corrections needed     Unable to
correct         Reset
 
Thereby, is always possible to
track free bedrooms for all the
dates in winter, including
Christmas and Nouvel An.
   
We have many of visit on our site
Figure 2: This screenshot shows what an annotator sees when beginning to edit the output of a machine
translation system.
24
Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.
In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item.
4 Translation task results
We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:
? Which systems produced the best translation
quality for each language pair?
? Did the system combinations produce better
translations than individual systems?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 5 shows the best individual systems. We
define the best systems as those which had no
other system that was statistically significantly
better than them under the Sign Test at p ? 0.1.
Multiple systems are listed as the winners for
many language pairs because it was not possible to
draw a statistically significant difference between
the systems. There is no individual system clearly
outperforming all other systems across the differ-
ent language pairs. With the exception of French-
English and English-French one can observe that
top-performing constrained systems did as well as
the unconstrained system ONLINEB.
Table 6 shows the best combination systems.
For all language directions, except Spanish-
English, one can see that the system combina-
tion runs outperform the individual systems and
that in most cases the differences are statistically
significant. While this is to be expected, system
combination is not guaranteed to improve perfor-
mance as some of the lower ranked combination
runs show, which are outperformed by individual
systems. Also note that except for Czech-English
translation the online systems ONLINEA and ON-
LINEB where not included for the system combi-
nation runs
Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system?s output was under-
standable. Figure 3 gives the percentage of times
that each system?s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).
This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:
? There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.
? The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.
5 Shared evaluation task overview
In addition to allowing the analysis of subjective
translation quality measures for different systems,
the judgments gathered during the manual evalu-
ation may be used to evaluate how well the au-
tomatic evaluation metrics serve as a surrogate to
the manual evaluation processes. NIST began run-
ning a ?Metrics for MAchine TRanslation? chal-
lenge (MetricsMATR), and presented their find-
ings at a workshop at AMTA (Przybocki et al,
2008). This year we conducted a joint Metrics-
MATR and WMT workshop, with NIST running
the shared evaluation task and analyzing the re-
sults.
In this year?s shared evaluation task 14 different
research groups submitted a total of 26 different
automatic metrics for evaluation:
Aalto University of Science and Technology
(Dobrinkat et al, 2010)
? MT-NCD ? A machine translation metric
based on normalized compression distance
(NCD), a general information-theoretic mea-
sure of string similarity. MT-NCD mea-
sures the surface level similarity between two
strings with a general compression algorithm.
More similar strings can be represented with
25
French-English
551?755 judgments per system
System C? ?others
LIUM ?? Y 0.71
ONLINEB ? N 0.71
NRC ?? Y 0.66
CAMBRIDGE ?? Y +GW 0.66
LIMSI ? Y +GW 0.65
UEDIN Y 0.65
RALI ?? Y +GW 0.65
JHU Y 0.59
RWTH ?? Y +GW 0.55
LIG Y 0.53
ONLINEA N 0.52
CMU-STATXFER Y 0.51
HUICONG Y 0.51
DFKI N 0.42
GENEVA Y 0.27
CU-ZEMAN Y 0.21
English-French
664?879 judgments per system
System C? ?others
UEDIN ?? Y 0.70
ONLINEB ? N 0.68
RALI ?? Y +GW 0.66
LIMSI ?? Y +GW 0.66
RWTH ?? Y +GW 0.63
CAMBRIDGE ? Y +GW 0.63
LIUM Y 0.63
NRC Y 0.62
ONLINEA N 0.55
JHU Y 0.53
DFKI N 0.40
GENEVA Y 0.35
EU N 0.32
CU-ZEMAN Y 0.26
KOC Y 0.26
Czech-English
788?868 judgments per system
System C? ?others
ONLINEB ? N 0.7
UEDIN ? Y 0.61
CMU Y 0.55
CU-BOJAR N 0.55
AALTO Y 0.43
ONLINEA N 0.37
CU-ZEMAN Y 0.22
German-English
723?879 judgments per system
System C? ?others
ONLINEB ? N 0.73
KIT ?? Y +GW 0.72
UMD ?? Y 0.68
UEDIN ? Y 0.66
FBK ? Y +GW 0.66
ONLINEA ? N 0.63
RWTH Y +GW 0.62
LIU Y 0.59
UU-MS Y 0.55
JHU Y 0.53
LIMSI Y +GW 0.52
UPPSALA Y 0.51
DFKI N 0.50
HUICONG Y 0.47
CMU Y 0.46
AALTO Y 0.42
CU-ZEMAN Y 0.36
KOC Y 0.23
English-German
1284?1542 judgments per system
System C? ?others
ONLINEB ? N 0.70
DFKI ? N 0.62
UEDIN ?? Y 0.62
KIT ? Y 0.60
ONLINEA N 0.59
FBK ? Y 0.56
LIU Y 0.55
RWTH Y 0.51
LIMSI Y 0.51
UPPSALA Y 0.47
JHU Y 0.46
SFU Y 0.34
KOC Y 0.30
CU-ZEMAN Y 0.28
English-Czech
1375?1627 judgments per system
System C? ?others
ONLINEB ? N 0.70
CU-BOJAR ? N 0.66
PC-TRANS ? N 0.62
UEDIN ?? Y 0.62
CU-TECTO Y 0.60
EUROTRANS N 0.54
CU-ZEMAN Y 0.50
SFU Y 0.45
ONLINEA N 0.44
POTSDAM Y 0.44
DCU N 0.38
KOC Y 0.33
Spanish-English
1448?1577 judgments per system
System C? ?others
ONLINEB ? N 0.70
UEDIN ?? Y 0.69
CAMBRIDGE Y +GW 0.61
JHU Y 0.61
ONLINEA N 0.54
UPC ? Y 0.51
HUICONG Y 0.50
DFKI N 0.45
COLUMBIA Y 0.45
CU-ZEMAN Y 0.27
English-Spanish
540?722 judgments per system
System C? ?others
ONLINEB ? N 0.71
ONLINEA ? N 0.69
UEDIN ? Y 0.61
DCU N 0.61
DFKI ? N 0.55
JHU ? Y 0.55
UPV ? Y 0.55
CAMBRIDGE ? Y +GW 0.54
UHC-UPV ? Y 0.54
SFU Y 0.40
CU-ZEMAN Y 0.23
KOC Y 0.19
Systems are listed in the order of how often their translations were ranked higher than or equal to any other system. Ties are
broken by direct comparison.
C? indicates constrained condition, meaning only using the supplied training data, standard monolingual linguistic tools, and
optionally the LDC?s GigaWord, which was allowed this year (entries that used the GigaWord are marked +GW).
? indicates a win in the category, meaning that no other system is statistically significantly better at p-level?0.1 in pairwise
comparison.
? indicates a constrained win, no other constrained system is statistically better.
For all pairwise comparisons between systems, please check the appendix.
Table 5: Official results for the WMT10 translation task, based on the human evaluation (ranking trans-
lations relative to each other)
26
French-English
589?716 judgments per combo
System ?others
RWTH-COMBO ? 0.77
CMU-HYP-COMBO ? 0.77
DCU-COMBO ? 0.72
LIUM ? 0.71
CMU-HEA-COMBO ? 0.70
UPV-COMBO ? 0.68
NRC 0.66
CAMBRIDGE 0.66
UEDIN ? 0.65
LIMSI ? 0.65
JHU-COMBO 0.65
RALI 0.65
LIUM-COMBO 0.64
BBN-COMBO 0.64
RWTH 0.55
English-French
740?829 judgments per combo
System ?others
RWTH-COMBO ? 0.75
CMU-HEA-COMBO ? 0.74
UEDIN 0.70
KOC-COMBO ? 0.68
UPV-COMBO 0.66
RALI ? 0.66
LIMSI 0.66
RWTH 0.63
CAMBRIDGE 0.63
Czech-English
766?843 judgments per combo
System ?others
CMU-HEA-COMBO ? 0.71
ONLINEB ? 0.7
BBN-COMBO ? 0.70
RWTH-COMBO ? 0.65
UPV-COMBO ? 0.63
JHU-COMBO 0.62
UEDIN 0.61
German-English
743?835 judgments per combo
System ?others
BBN-COMBO ? 0.77
RWTH-COMBO ? 0.75
CMU-HEA-COMBO 0.73
KIT ? 0.72
UMD ? 0.68
JHU-COMBO 0.67
UEDIN ? 0.66
FBK 0.66
CMU-HYP-COMBO 0.65
UPV-COMBO 0.64
RWTH 0.62
KOC-COMBO 0.59
English-German
1340?1469 judgments per combo
System ?others
RWTH-COMBO ? 0.65
DFKI ? 0.62
UEDIN ? 0.62
KIT ? 0.60
CMU-HEA-COMBO ? 0.59
KOC-COMBO 0.59
FBK ? 0.56
UPV-COMBO 0.55
English-Czech
1405?1496 judgments per combo
System ?others
DCU-COMBO ? 0.75
ONLINEB ? 0.70
RWTH-COMBO 0.70
CMU-HEA-COMBO 0.69
UPV-COMBO 0.68
CU-BOJAR 0.66
KOC-COMBO 0.66
PC-TRANS 0.62
UEDIN 0.62
Spanish-English
1385?1535 judgments per combo
System ?others
UEDIN ? 0.69
CMU-HEA-COMBO ? 0.66
UPV-COMBO ? 0.66
BBN-COMBO 0.62
JHU-COMBO 0.55
UPC 0.51
English-Spanish
516?673 judgments per combo
System ?others
CMU-HEA-COMBO ? 0.68
KOC-COMBO 0.62
UEDIN ? 0.61
UPV-COMBO 0.60
RWTH-COMBO 0.59
DFKI ? 0.55
JHU 0.55
UPV 0.55
CAMBRIDGE ? 0.54
UPV-NNLM ? 0.54
System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system.
Ties are broken by direct comparison. We show the best individual systems alongside the system combinations, since the goal
of combination is to produce better quality translation than the component systems.
? indicates a win for the system combination meaning that no other system or system combination is statistically signifi-
cantly better at p-level?0.1 in pairwise comparison.
? indicates an individual system that none of the system combinations beat by a statistically significant margin at p-
level?0.1.
For all pairwise comparisons between systems, please check the appendix.
Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks,
except in the Czech-English and English-Czech conditions, where ONLINEB was included.
Table 6: Official results for the WMT10 system combination task, based on the human evaluation (rank-
ing translations relative to each other)
27
System % Yes Yes count No count N/A count Total count *** en-cz ***
ref 0.97 63 2 0 65 en-cz
dcu-c 0.58 29 21 0 50 en-cz
onlineB 0.55 22 18 0 40 en-cz
rwth-c 0.49 56 59 0 115 en-cz
koc-c 0.45 29 36 0 65 en-cz
pc-trans 0.43 26 34 0 60 en-cz
upv-c 0.42 23 32 0 55 en-cz
cu-bojar 0.4 20 30 0 50 en-cz
eurotrans 0.4 18 27 0 45 en-cz
uedin 0.34 24 46 0 70 en-cz
cu-tecto 0.34 29 55 1 85 en-cz
cmu-hea-c 0.29 13 32 0 45 en-cz
sfu 0.24 14 44 0 58 en-cz
potsdam 0.24 13 42 0 55 en-cz
cu-zeman 0.21 15 55 0 70 en-cz
koc 0.21 21 79 0 100 en-cz
onlineA 0.2 13 52 0 65 en-cz
dcu 0.19 13 57 0 70 en-cz
0.1260077028
*** en-de ***
ref 0.94 47 3 0 50 en-de
onlineA 0.8 20 5 0 25 en-de
koc-c 0.68 17 8 0 25 en-de
uppsala 0.65 26 14 0 40 en-de
uedin 0.62 50 30 0 80 en-de
kit 0.62 37 23 0 60 en-de
upv-c 0.57 30 23 0 53 en-de
onlineB 0.52 21 19 0 40 en-de
dfki 0.52 13 12 0 25 en-de
koc 0.51 18 17 0 35 en-de
limsi 0.51 18 16 1 35 en-de
liu 0.51 28 27 0 55 en-de
rwth 0.5 15 15 0 30 en-de
rwth-c 0.49 22 23 0 45 en-de
jhu 0.48 12 13 0 25 en-de
cmu-hea-c 0.47 14 16 0 30 en-de
fbk 0.4 4 6 0 10 en-de
sfu 0.31 11 24 0 35 en-de
cu-zeman 0.19 10 40 3 53 en-de
0.1364453014
System % Yes Yes count No count N/A count Total count *** en-es ***
ref 0.83 48 10 0 58 en-es
onlineB 0.58 25 18 0 43 en-es
upv 0.5 20 20 0 40 en-es
rwth-c 0.46 13 15 0 28 en-es
dcu 0.42 16 22 0 38 en-es
koc 0.4 17 24 1 42 en-es
upv-nnlm 0.39 15 23 0 38 en-es
onlineA 0.38 11 18 0 29 en-es
jhu 0.38 17 27 1 45 en-es
koc-c 0.38 20 33 0 53 en-es
uedin 0.36 12 21 0 33 en-es
upb-c 0.32 13 27 0 40 en-es
cmu-hea-c 0.32 16 34 0 50 en-es
camb 0.3 12 27 1 40 en-es
dfki 0.29 7 17 0 24 en-es
cu-zeman 0.29 16 39 0 55 en-es
sfu 0.26 9 25 0 34 en-es
0.0845946216
System % Yes Yes count No count N/A count Total count *** en-fr ***
ref 0.91 64 4 2 70 en-fr
rwth-c 0.54 27 23 0 50 en-fr
onlineB 0.52 47 42 1 90 en-fr
upv-c 0.51 34 33 0 67 en-fr
koc-c 0.48 32 34 0 66 en-fr
uedin 0.48 30 32 1 63 en-fr
rali 0.47 21 24 0 45 en-fr
rwth 0.45 25 30 0 55 en-fr
lium 0.43 20 27 0 47 en-fr
camb 0.42 26 36 0 62 en-fr
onlineA 0.41 15 22 0 37 en-fr
limsi 0.37 26 44 0 70 en-fr
jhu 0.37 27 46 0 73 en-fr
nrc 0.36 13 23 0 36 en-fr
cmu-hea-c 0.32 22 47 0 69 en-fr
geneva 0.31 32 70 0 102 en-fr
eu 0.3 13 30 0 43 en-fr
dfki 0.28 16 42 0 58 en-fr
koc 0.21 12 44 1 57 en-fr
cu-zeman 0.17 11 52 0 63 en-fr
0.1045877454
System % Yes Yes count No count N/A count Total count *** cz-en ***
ref 1.00 33 0 0 33 cz-en
cu-bojar 0.6 3 2 0 5 cz-en
upv-c 0.43 15 20 0 35 cz-en
cmu-hea-c 0.35 14 26 0 40 cz-en
rwth-c 0.32 16 34 0 50 cz-en
onlineB 0.3 12 28 0 40 cz-en
bbn-c 0.28 17 43 0 60 cz-en
uedin 0.28 11 28 1 40 cz-en
aalto 0.27 8 22 0 30 cz-en
jhu-c 0.26 13 37 0 50 cz-en
onlineA 0.2 6 24 0 30 cz-en
cmu 0.17 5 25 0 30 cz-en
cu-zeman 0.09 4 40 1 45 cz-en
0.1292958787
System % Yes Yes count No count N/A count Total count *** de-en ***
ref 0.98 44 1 0 45 de-en
umd 0.8 8 2 0 10 de-en
bbn-c 0.67 10 5 0 15 de-en
onlineB 0.65 13 7 0 20 de-en
cmu-hea-c 0.52 12 11 0 23 de-en
jhu-c 0.51 18 17 0 35 de-en
upv-c 0.51 18 16 1 35 de-en
fbk 0.5 20 20 0 40 de-en
uppsala 0.5 20 19 1 40 de-en
limsi 0.46 30 34 1 65 de-en
kit 0.45 18 22 0 40 de-en
liu 0.44 19 24 0 43 de-en
uedin 0.44 11 14 0 25 de-en
dfki 0.4 12 18 0 30 de-en
onlineA 0.4 6 9 0 15 de-en
rwth 0.4 14 21 0 35 de-en
cmu-hyp-c 0.37 11 19 0 30 de-en
huicong 0.36 9 16 0 25 de-en
koc-c 0.36 9 14 2 25 de-en
rwth-c 0.36 10 18 0 28 de-en
koc 0.31 11 23 1 35 de-en
cu-zeman 0.3 12 28 0 40 de-en
uu-ms 0.26 13 37 0 50 de-en
jhu 0.26 9 26 0 35 de-en
cmu 0.24 6 19 0 25 de-en
aalto 0.07 1 14 0 15 de-en
0.1512635669
System % Yes Yes count No count N/A count Total count *** es-en ***
ref 0.98 39 0 1 40 es-en
onlineB 0.71 39 15 1 55 es-en
onlineA 0.64 32 18 0 50 es-en
upv-c 0.6 36 24 0 60 es-en
huicong 0.54 27 23 0 50 es-en
jhu 0.54 35 30 0 65 es-en
cmu-hea-c 0.52 26 23 1 50 es-en
bbn-c 0.51 36 33 1 70 es-en
uedin 0.51 33 30 2 65 es-en
jhu-c 0.47 28 31 1 60 es-en
dfki 0.46 16 18 1 35 es-en
upc 0.43 28 36 1 65 es-en
cu-zeman 0.4 18 26 1 45 es-en
camb 0.36 25 45 0 70 es-en
columbia 0.29 19 46 0 65 es-en
0.1104436607
System % Yes Yes count No count N/A count Total count *** fr-en ***
ref 0.91 32 3 0 35 fr-en
cmu-hyp-c 0.7 21 9 0 30 fr-en
uedin 0.58 23 17 0 40 fr-en
bbn-c 0.56 14 10 1 25 fr-en
rwth-c 0.53 16 14 0 30 fr-en
onlineB 0.51 28 27 0 55 fr-en
camb 0.5 20 19 1 40 fr-en
rali 0.48 31 34 0 65 fr-en
lium 0.46 23 27 0 50 fr-en
dcu-c 0.45 15 16 2 33 fr-en
lig 0.45 9 11 0 20 fr-en
cmu-statxfer 0.44 11 14 0 25 fr-en
nrc 0.43 15 20 0 35 fr-en
dfki 0.4 8 12 0 20 fr-en
jhu 0.4 10 14 1 25 fr-en
jhu-c 0.4 22 30 3 55 fr-en
upv-c 0.4 14 20 1 35 fr-en
lium-c 0.4 27 41 0 68 fr-en
cmu-hea-c 0.35 14 26 0 40 fr-en
limsi 0.35 14 26 0 40 fr-en
onlineA 0.33 20 40 0 60 fr-en
huicong 0.32 13 25 2 40 fr-en
cu-zeman 0.24 6 19 0 25 fr-en
geneva 0.24 6 19 0 25 fr-en
rwth 0.2 1 4 0 5 fr-en
0.1143475506
0
0.25
0.5
0.75
1
r
e
f
d
c
u
-
c
o
n
l
i
n
e
B
r
w
t
h
-
c
k
o
c
-
c
p
c
-
t
r
a
n
s
u
p
v
-
c
c
u
-
b
o
j
a
r
e
u
r
o
t
r
a
n
s
u
e
d
i
n
c
u
-
t
e
c
t
o
c
m
u
-
h
e
a
-
c
s
f
u
p
o
t
s
d
a
m
c
u
-
z
e
m
a
n
k
o
c
o
n
l
i
n
e
A
d
c
u
.19.2.21.21.24.24.29.34.34.4.4.42.43.45.49.55.58.97
English-Czech
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
A
k
o
c
-
c
u
p
p
s
a
l
a
u
e
d
i
n
k
i
t
u
p
v
-
c
o
n
l
i
n
e
B
d
f
k
i
k
o
c
l
i
m
s
i
l
i
u
r
w
t
h
r
w
t
h
-
c
j
h
u
c
m
u
-
h
e
a
-
c
f
b
k
s
f
u
c
u
-
z
e
m
a
n
.19.31.4.47.48.49.5.51.51.51.52.52.57.62.62.65.68.8.94
English-German
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
B
u
p
v
r
w
t
h
-
c
d
c
u
k
o
c
u
p
v
-
n
n
l
m
o
n
l
i
n
e
A
j
h
u
k
o
c
-
c
u
e
d
i
n
u
p
b
-
c
c
m
u
-
h
e
a
-
c
c
a
m
b
d
f
k
i
c
u
-
z
s
f
u
.26.29.29.3.32.32.36.38.38.38.39.4.42.46.5.58.83
English-Spanish
0
0.25
0.5
0.75
1
r
e
f
r
w
t
h
-
c
o
n
l
i
n
e
B
u
p
v
-
c
k
o
c
-
c
u
e
d
i
n
r
a
l
i
r
w
t
h
l
i
u
m
c
a
m
b
o
n
l
i
n
e
A
l
i
m
s
i
j
h
u
n
r
c
c
m
u
-
h
e
a
-
c
g
e
n
e
v
a
e
u
d
f
k
i
k
o
c
c
u
-
z
e
m
a
n
.17.21.28.3.31.32.36.37.37.41.42.43.45.47.48.48.51.52.54.91
English-French
0
0.25
0.5
0.75
1
r
e
f
u
m
d
b
b
n
-
c
o
n
l
i
n
e
B
c
m
u
-
h
e
a
-
c
j
h
u
-
c
u
p
v
-
c
f
b
k
u
p
p
s
a
l
a
l
i
m
s
i
k
i
t
l
i
u
u
e
d
i
n
d
f
k
i
o
n
l
i
n
e
A
r
w
t
h
c
m
u
-
h
y
p
-
c
h
u
i
c
o
n
g
k
o
c
-
c
r
w
t
h
-
c
k
o
c
c
u
-
z
e
m
a
n
u
u
-
m
s
j
h
u
c
m
u
a
a
l
t
o
.07.24.26.26.3.31.36.36.36.37.4.4.4.44.44.45.46.5.5.51.51.52.65.67.8.98
German-English
0
0.25
0.5
0.75
1
r
e
f
c
u
-
b
o
j
a
r
u
p
v
-
c
c
m
u
-
h
e
a
-
c
r
w
t
h
-
c
o
n
l
i
n
e
B
b
b
n
-
c
u
e
d
i
n
a
a
l
t
o
j
h
u
-
c
o
n
l
i
n
e
A
c
m
u
c
u
-
z
e
m
a
n
.09.17.2.26.27.28.28.3.32.35.43.61.0
Czech-English
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
B
o
n
l
i
n
e
A
u
p
v
-
c
h
u
i
c
o
n
g
j
h
u
c
m
u
-
h
e
a
-
c
b
b
n
-
c
4
5
j
h
u
-
c
d
f
k
i
u
p
c
c
u
-
z
e
m
a
n
c
a
m
b
c
o
l
u
m
b
i
a
.29.36.4.43.46.47.51.51.52.54.54.6.64.71.98
Spanish-English
0
0.25
0.5
0.75
1
r
e
f
c
m
u
-
h
y
p
-
c
u
e
d
i
n
b
b
n
-
c
r
w
t
h
-
c
o
n
l
i
n
e
B
c
a
m
b
r
a
l
i
l
i
u
m
d
c
u
-
c
l
i
g
c
m
u
-
s
t
a
t
x
f
e
r
n
r
c
d
f
k
i
j
h
u
j
h
u
-
c
u
p
v
-
c
l
i
u
m
-
c
c
m
u
-
h
e
a
-
c
l
i
m
s
i
o
n
l
i
n
e
A
h
u
i
c
o
n
g
c
u
-
z
e
m
a
n
g
e
n
e
v
a
r
w
t
h
.2.24.24.32.33.35.35.4.4.4.4.4.43.44.45.45.46.48.5.51.53.56.58.7.91
French-English
Figure 3: The percent of time that each system?s edited output was judged to be an acceptable translation.
These numbers also include judgments of the system?s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.
28
a shorter description when concatenated be-
fore compression than when concatenated af-
ter compression. MT-NCD does not require
any language specific resources.
? MT-mNCD ? Enhances MT-NCD with flex-
ible word matching provided by stemming
and synonyms. It works analogously to
M-BLEU and M-TER and uses METEOR?s
aligner module to find relaxed word-to-word
alignments. MT-mNCD exploits English
WordNet data and increases correlation to hu-
man judgments for English over MT-NCD.
Due to a processing issue inherent to the metric,
the scores reported were generated excluding the
first segment of each document. Also, a separate
issue was found for the MT-mNCD metric, and ac-
cording to the developer the scores reported here
would like change with a correction of the issue.
BabbleQuest International8
? Badger 2.0 full ? Uses the Smith-Waterman
alignment algorithm with Gotoh improve-
ments to measure segment similarity. The
full version uses a multilingual knowledge
base to assign a substitution cost which sup-
ports normalization of word infection and
similarity.
? Badger 2.0 lite ? The lite version uses default
gap, gap extension and substitution costs.
City University of Hong Kong (Wong and Kit,
2010)
? ATEC 2.1 ? This version of ATEC extends
the measurement of word choice and word or-
der by various means. The former is assessed
by matching word forms at linguistic levels,
including surface form, stem, sense and se-
mantic similarity, and further by weighting
the informativeness of both matched and un-
matched words. The latter is quantified in
term of the discordance of word position and
word sequence between an MT output and its
reference.
Due to a version discrepancy of the metric, final
scores for ATECD-2.1 differ from those reported
here, but only minimally.
8http://www.babblequest.com/badger2
Carnegie Mellon University (Denkowski and
Lavie, 2010)
? METEOR-NEXT-adq ? Evaluates a machine
translation hypothesis against one or more
reference translations by calculating a simi-
larity score based on an alignment between
the hypothesis and reference strings. Align-
ments are based on exact, stem, synonym,
and paraphrase matches between words and
phrases in the strings. Metric parameters are
tuned to maximize correlation with human
judgments of translation quality (adequacy
judgments).
? METEOR-NEXT-hter ? METEOR-NEXT
tuned to HTER.
? METEOR-NEXT-rank ? METEOR-NEXT
tuned to human judgments of rank.
Columbia University9
? SEPIA ? A syntactically-aware machine
translation evaluation metric designed with
the goal of assigning bigger weight to gram-
matical structural bigrams with long surface
spans that cannot be captured with surface n-
gram metrics. SEPIA uses a dependency rep-
resentation produced for both hypothesis and
reference(s). SEPIA is configurable to allow
using different combinations of structural n-
grams, surface n-grams, POS tags, depen-
dency relations and lemmatization. SEPIA is
a precision-based metric and as such employs
clipping and length penalty to minimize met-
ric gaming.
Charles University Prague (Bojar and Kos,
2010)
? SemPOS ? Computes overlapping of autose-
mantic (content-bearing) word lemmas in the
candidate and reference translations given a
fine-grained semantic part of speech (sem-
pos) and outputs average overlapping score
over all sempos types. The overlapping is de-
fined as the number of matched lemmas di-
vided by the total number of lemmas in the
candidate and reference translations having
the same sempos type.
9http://www1.ccls.columbia.edu/?SEPIA/
29
? SemPOS-BLEU ? A linear combination of
SemPOS and BLEU with equal weights.
BLEU is computed on surface forms of au-
tosemantic words that are used by SemPOS,
i.e. auxiliary verbs or prepositions are not
taken into account.
Dublin City University (He et al, 2010)
? DCU-LFG ? A combination of syntactic and
lexical information. It measures the similar-
ity of the hypothesis and reference in terms
of matches of Lexical Functional Grammar
(LFG) dependency triples. The matching
module can also access the WordNet syn-
onym dictionary and Snover?s paraphrase
database10.
University of Edinburgh (Birch and Osborne,
2010)
? LRKB4 ? A novel metric which directly mea-
sures reordering success using Kendall?s tau
permutation distance metrics. The reordering
component is combined with a lexical metric,
capturing the two most important elements
of translation quality. This simple combined
metric only has one parameter, which makes
its scores easy to interpret. It is also fast
to run and language-independent. It uses
Kendall?s tau permutation.
? LRHB4 ? LRKB4, replacing Kendall?s tau
permutation distance metric with the Ham-
ming distance permutation distance metric.
Due to installation issues, the reported submitted
scores for these two metrics have not been verified
to produce identical scores at NIST.
Harbin Institute of Technology, China
? I-letter-BLEU ? Normal BLEU based on let-
ters. Moreover, the maximum length of N-
gram is decided by the average length for
each sentence, respectively.
? I-letter-recall ? A geometric mean of N-gram
recall based on letters. Moreover, the maxi-
mum length of N-gram is decided by the av-
erage length for each sentence, respectively.
10Available at http://www.umiacs.umd.edu/
?snover/terp/.
? SVM-RANK ? Uses support vector ma-
chines rank models to predict an ordering
over a set of system translations with lin-
ear kernel. Features include Meteor-exact,
BLEU-cum-1, BLEU-cum-2, BLEU-cum-5,
BLEU-ind-1, BLEU-ind-2, ROUGE-L re-
call, letter-based TER, letter-based BLEU-
cum-5, letter-based ROUGE-L recall, and
letter-based ROUGE-S recall.
National University of Singapore (Liu et al,
2010)
? TESLA-M ? Based on matching of bags of
unigrams, bigrams, and trigrams, with con-
sideration of WordNet synonyms. The match
is done in the framework of real-valued lin-
ear programming to enable the discounting of
function words.
? TESLA ? Built on TESLA-M, this metric
also considers bilingual phrase tables to dis-
cover phrase-level synonyms. The feature
weights are tuned on the development data
using SVMrank.
Stanford University
? Stanford ? A discriminatively trained
string-edit distance metric with various
similarity-matching, synonym-matching, and
dependency-parse-tree-matching features.
The model resembles a Conditional Random
Field, but performs regression instead of
classification. It is trained on Arabic, Chi-
nese, and Urdu data from the MT-Eval 2008
dataset.
Due to installation issues, the reported scores for
this metric have not been verified to produce iden-
tical scores at NIST.
University of Maryland11
? TER-plus (TERp) ? An extension of the
Translation Edit Rate (TER) metric that mea-
sures the number of edits between a hypoth-
esized translation and a reference translation.
TERp extends TER by using stemming, syn-
onymy, and paraphrases as well as tunable
edit costs to better measure the distance be-
tween the two translations. This version
of TERp improves upon prior versions by
adding brevity and length penalties.
11http://www.umiacs.umd.edu/?snover/
terp
30
Scores were not submitted along with this metric,
and due to installation issues were not produced at
NIST in time to be included in this report.
University Polite`cnica de Catalunya/University
de Barcelona (Comelles et al, 2010)
? DR ? An arithmetic mean over a set of
three metrics based on discourse representa-
tions, respectively computing lexical overlap,
morphosyntactic overlap, and semantic tree
matching.
? DRdoc ? Is analogous to DR but, instead of
operating at the segment level, it analyzes
similarities over whole document discourse
representations.
? ULCh ? An arithmetic mean over a
heuristically-defined set of metrics operat-
ing at different linguistic levels (ROUGE,
METEOR, and measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations).
University of Southern California, ISI
? BEwT-E ? Basic Elements with Transfor-
mations for Evaluation, is a recall-oriented
metric that compares basic elements, small
portions of contents, between the two trans-
lations. The basic elements (BEs) consist
of content words and various combinations
of syntactically-related words. A variety of
transformations are performed to allow flexi-
ble matching so that words and syntactic con-
structions conveying similar content in dif-
ferent manners may be matched. The trans-
formations cover synonymy, preposition vs.
noun compounding, differences in tenses,
etc. BEwT-E was originally created for sum-
marization evaluation and is English-specific.
? Bkars ? Measures overlap between character
trigrams in the system and reference trans-
lations. It is heavily weighted toward recall
and contains a fragmentation penalty. Bkars
produces a score both with and without stem-
ming (using the Snowball package of stem-
mers) and averages the results together. It is
not English-specific.
Scores were not submitted for BEwT-E; the run-
time required for this metric to process the WMT-
10 data set prohibited the production of scores in
time for publication.
6 Evaluation task results
The results reported here are preliminary; a final
release of results will be published on the WMT10
website before July 15, 2010. Metric developers
submitted metrics for installation at NIST; they
were also asked to submit metric scores on the
WMT10 test set alng with their metrics. Not
all developers submitted scores, and not all met-
rics were verified to produce the same scores as
submitted at NIST in time for publication. Any
such caveats are reported with the description of
the metrics above.
The results reported here are limited to a com-
parison of metric scores on the full WMT10
test set with human assessments on the human-
assessed subset. An analysis comparing the hu-
man assessments with the automatic metrics run
only on the human-assessed subset will follow at
a later date.
The WMT10 system output used to generate
the reported metric scores was found to have im-
properly escaped characters for a small number of
segments. While we plan to regenerate the met-
ric scores with this issue resolved, we do not ex-
pect this to significantly alter the results, given the
small number of segments affected.
6.1 System Level Metric Scores
The tables in Appendix B list the metric scores
for the language pairs processed by each metric.
These first four tables present scores for transla-
tions out of English into Czech, French, German
and Spanish. In addition to the metric scores of
the submitted metrics identified above, we also
present (1) the ranking of the system as deter-
mined by the human assessments; and (2) the
metrics scores for two popular baseline metrics,
BLEU as calculated by NIST?s mteval software12
and the NIST score. For each method of system
measurement the absolute highest score is identi-
fied by being outlined in a box.
Similarly, the remaining tables in Appendix B
list the metric scores for the submitted metrics and
the two baseline metrics, and the ranking based
on the human assessments for translations into En-
glish from Czech, French, German and Spanish.
As some metrics employ language-specific re-
sources, not all metrics produced scores for all lan-
guage pairs.
12ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz
31
cz-
en
fr-
en
de-
en
es-
en
avg
SemPOS .78 .77 .60 .95 .77
IQmt-DRdoc .61 .79 .65 .98 .76
SemPOS-BLEU .75 .70 .61 .96 .75
i-letter-BLEU .71 .70 .60 .98 .75
NIST .85 .72 .55 .86 .74
TESLA .70 .70 .60 .97 .74
MT-NCD .71 .72 .58 .95 .74
Bkars .71 .67 .58 .98 .74
ATEC-2.1 .71 .67 .59 .96 .73
meteor-next-rank .69 .68 .60 .96 .73
IQmt-ULCh .70 .64 .60 .99 .73
IQmt-DR .68 .67 .60 .97 .73
meteor-next-hter .71 .66 .59 .95 .73
meteor-next-adq .69 .67 .60 .96 .73
badger-2.0-lite .70 .70 .56 .94 .73
DCU-LFG .69 .69 .58 .96 .73
badger-2.0-full .69 .70 .57 .94 .73
SEPIA .71 .70 .57 .92 .73
SVM-rank .66 .65 .61 .98 .73
i-letter-recall .65 .64 .61 .98 .72
TESLA-M .67 .67 .57 .95 .72
BLEU-4-v13a .69 .68 .52 .90 .70
LRKB4 .63 .62 .53 .89 .67
LRHB4 .62 .65 .50 .87 .66
MT-mNCD .69 .64 .52 .70 .64
Stanford .58 .19 .60 .46 .46
Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.
It is noticeable that system combinations are of-
ten among those achieving the highest scores.
6.2 System-Level Correlations
To assess the performance of the automatic met-
rics, we correlated the metrics? scores with the hu-
man rankings at the system level. We assigned a
consolidated human-assessment rank to each sys-
tem based on the number of times that the given
system?s translations were ranked higher than or
equal to the translations of any other system in
the manual evaluation of the given language pair.
We then compared the ranking of systems by the
human assessments to that provided by the au-
tomatic metric system level scores on the com-
plete WMT10 test set for each language pair, us-
ing Spearman?s ? rank correlation coefficient. The
correlations are shown in Table 7 for translations
to English, and Table 8 out of English, with base-
line metrics listed at the bottom. The highest cor-
relation for each language pair and the highest
overall average are bolded.
Overall, correlations are higher for translations
to English than compared to translations from En-
glish. For all language pairs, there are a number
of new metrics that yield noticeably higher corre-
en-
cz
en-
fr
en-
de
en-
es
avg
SVM-rank .29 .54 .68 .67 .55
TESLA-M .27 .49 .74 .66 .54
LRKB4 .39 .58 .47 .71 .54
i-letter-recall .28 .51 .61 .66 .52
LRHB4 .39 .59 .41 .63 .51
i-letter-BLEU .26 .49 .56 .65 .49
ATEC-2.1 .38 .52 .44 .62 .49
badger-2.0-full .37 .58 .41 .59 .49
Bkars .22 .54 .52 .66 .48
BLEU-4-v13a .35 .58 .39 .57 .47
badger-2.0-lite .32 .57 .41 .59 .47
TESLA .09 .62 .66 .50 .47
meteor-next-rank .34 .59 .39 .51 .46
Stanford .34 .48 .70 .32 .46
MT-NCD .17 .54 .51 .61 .46
NIST .30 .52 .41 .50 .43
MT-mNCD .26 .49 .17 .43 .34
SemPOS .31 n/a n/a n/a .31
SemPOS-BLEU .29 n/a n/a n/a .29
Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.
lations with human assessments than either of the
two included baseline metrics. In particular, Bleu
performed in the bottom half of the into-English
and out-of-English directions.
6.3 Segment-Level Metric Analysis
The method employed to collect human judgments
of rank preferences at the segment level produces
a sparse matrix of decision points. It is unclear
whether attempts to normalize the segment level
rankings to 0.0?1.0 values, representing the rela-
tive rank of a system per segment given the num-
ber of comparisons it is involved with, is proper.
An intuitive display of how well metrics mirror the
human judgments may be shown via a confusion
matrix. We compare the human ranks to the ranks
as determined by a metric. Below, we show an ex-
ample of the confusion matrix for the SVM-rank
metric which had the highest summed diagonal
(occurrences when a particular rank by the met-
ric?s score exactly matches the human judgments)
for all segments translated into English. The num-
bers provided are percentages of the total count.
The summed diagonal constitutes 39.01% of all
counts in this example matrix. The largest cell is
the 1/1 ranking cell (top left). We included the
reference translation as a system in this analysis,
which is likely to lead to a lot of agreement on the
highest rank between humans and automatic met-
rics.
32
Metric Human Rank
Rank 1 2 3 4 5
1 12.79 4.48 2.75 1.82 0.92
2 2.77 7.94 5.55 3.79 2.2
3 1.57 4.29 6.74 5.4 4.46
4 0.97 2.42 3.76 4.99 6.5
5 0.59 1.54 1.84 3.38 6.55
No allowances for ties were made in this analy-
sis. That is, if a human ranked two system transla-
tions the same, this analysis expects the metrics to
provide the same score in order to get them both
correct. Future analysis could relax this constraint.
As not all human rankings start with the highest
possible rank of ?1? (due to ties and withholding
judgment on a particular system output being al-
lowed), we set the highest automatic metric rank
to the highest human rank and shifted the lower
metric ranks down accordingly.
Table 9 shows the summed diagonal percent-
ages of the total count of all datapoints for all met-
rics that WMT10 scores were available for, both
combined for all languages to English (X-English)
and separately for each language into English.
The results are ordered by the highest percent-
age for the summed diagonal on all languages
to English combined. There are quite noticeable
changes in ranking of the metrics for the separate
language pairs; further analysis into the reasons
for this will be necessary.
We plan to also analyze metric performance for
translation into English.
7 Feasibility of Using Non-Expert
Annotators in Future WMTs
In this section we analyze the data that we col-
lected data by posting the ranking task on Ama-
zon?s Mechanical Turk (MTurk). Although we did
not use this data when creating the official results,
our hope was that it may be useful in future work-
shops in two ways. First, if we find that it is pos-
sible to obtain a sufficient amount of data of good
quality, then we might be able to reduce the time
commitment expected from the system develop-
ers in future evaluations. Second, the additional
collected labels might enable us to detect signifi-
cant differences between systems that would oth-
erwise be insignificantly different using only the
data from the volunteers (which we will now refer
to as the ?expert? data).
7.1 Data collection
To that end, we prepared 600 ranking sets for each
of the eight language pairs, with each set con-
taining five MT outputs to be ranked, using the
same interface used by the volunteers. We posted
the data to MTurk and requested, for each one,
five redundant assignments, from different work-
ers. Had all the 5? 8? 600 = 24,000 assignments
been completed, we would have obtained 24,000
? 5 = 120,000 additional rank labels, compared
to the 37,884 labels we collected from the volun-
teers (Table 3). In actuality, we collected closer to
55,000 rank labels, as we discuss shortly.
To minimize the amount of data that is of poor
quality, we placed two requirements that must be
satisfied by any worker before completing any of
our tasks. First, we required that a worker have an
existing approval rating of at least 85%. Second,
we required a worker to reside in a country where
the target language of the task can be assumed to
be the spoken language. Finally, anticipating a
large pool of workers located in the United States,
we felt it possible for us to add a third restriction
for the *-to-English language pairs, which is that a
worker must have had at least five tasks previously
approved on MTurk.13 We organized the ranking
sets in groups of 3 per screen, with a monetary re-
ward of $0.05 per screen.
When we created our tasks, we had no expecta-
tion that all the assignments would be completed
over the tasks? lifetime of 30 days. This was in-
deed the case (Table 10), especially for language
pairs with a non-English target language, due to
workers being in short supply outside the US.
Overall, we see that the amount of data collected
from non-US workers is relatively small (left half
of Table 10), whereas the pool of US-based work-
ers is much larger, leading to much higher com-
pletion rates for language pairs with English as the
target language (right half of Table 10). This is in
spite of the additional restriction we placed on US
workers.
13We suspect that newly registered workers on MTurk al-
ready start with an ?approval rating? of 100%, and so requir-
ing a high approval rating alone might not guard against new
workers. It is not entirely clear if our suspicion is true, but our
past experiences with MTurk usually involved a noticeably
faster completion rate than what we experienced this time
around, indicating our suspicion might very well be correct.
33
Metric *-English Czech-English French-English German-English Spanish-English
SVM-rank 39.01 41.21 36.07 38.81 40.3
i-letter-recall 38.85 41.71 36.19 38.8 39.5
MT-NCD 38.77 42.55 35.31 38.7 39.48
i-letter-BLEU 38.69 40.54 36.05 38.82 39.64
meteor-next-rank 38.5 40.1 34.41 39.25 40.05
meteor-next-adq 38.27 39.58 34.41 39.5 39.35
meteor-next-hter 38.21 38.61 34.1 39.13 40.18
Bkars 37.98 40.1 35.08 38.6 38.52
Stanford 37.97 39.87 36.19 38.27 38.09
ATEC-2.1 37.95 40.06 34.96 38.6 38.53
TESLA 37.57 38.68 34.38 38.67 38.36
NIST 37.47 39.54 35.54 37.13 38.2
SemPOS 37.21 38.8 37.39 35.73 37.69
SemPOS-BLEU 37.16 38.05 36.57 37.11 37.21
badger-2.0-full 37.12 37.5 36 36.21 38.62
badger-2.0-lite 37.08 37.2 35.88 36.23 38.69
SEPIA 37.06 38.98 34.6 36.46 38.52
BLEU-4-v13a 36.71 37.83 34.84 36.44 37.81
LRHB4 36.14 38.35 34.65 34.24 37.93
TESLA-M 36.13 37.01 34 35.79 37.6
LRKB4 36.12 38.72 33.47 35.25 37.63
IQmt-ULCh 35.86 37.64 33.95 35.81 36.45
IQmt-DR 35.77 36.27 34.43 34.43 37.74
DCU-LFG 34.72 36.38 32.29 33.87 36.49
MT-mNCD 34.51 34.93 31.78 35.73 35.13
IQmt-DRdoc 31.9 33.85 28.99 32.9 32.18
Table 9: The segment-level performance for metrics for the into-English direction.
en-de en-es en-fr en-cz de-en es-en fr-en cz-en
Location DE ES/MX FR CZ US US US US
Completed 1 time 37% 38% 29% 19% 3.5% 1.5% 14% 2.0%
Completed 2 times 18% 14% 12% 1.5% 6.0% 5.5% 19% 4.5%
Completed 3 times 2.5% 4.5% 0.5% 0.0% 8.5% 11% 20% 10%
Completed 4 times 1.5% 0.5% 0.5% 0.0% 22% 19% 23% 17%
Completed 5 times 0.0% 0.5% 0.0% 0.0% 60% 63% 22% 67%
Completed ? once 59% 57% 42% 21% 100% 99% 96% 100%
Label count 2,583 2,488 1,578 627 12,570 12,870 9,197 13,169
(% of expert data) (38%) (96%) (40%) (9%) (241%) (228%) (222%) (490%)
Table 10: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were
collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and
we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have
potentially obtained 600 ? 5 ? 5 = 15,000 labels for each language pair. The Label count row indicates
to what extent that potential was met (over the 30-day lifetime of our tasks), and the ?Completed...? rows
give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group,
2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5
workers, with 100% of the sets completed at least once. The total cost of this data collection effort was
roughly $200.
34
INTER-ANNOTATOR AGREEMENT
P (A) K K?
With references 0.466 0.198 0.487
Without references 0.441 0.161 0.439
INTRA-ANNOTATOR AGREEMENT
P (A) K K?
With references 0.539 0.309 0.633
Without references 0.538 0.307 0.601
Table 11: Inter- and intra-annotator agreement for
the MTurk workers on the sentence ranking task.
(As before, P (E) is 0.333.) For comparison, we
repeat here the kappa coefficients of the experts
(K?), taken from Table 4.
7.2 Quality of MTurk data
It is encouraging to see that we can collect a large
amount of rank labels from MTurk. That said, we
still need to guard against data from bad work-
ers, who are either not being faithful and click-
ing randomly, or who might simply not be compe-
tent enough. Case in point, if we examine inter-
and intra-annotator agreement on the MTurk data
(Table 11), we see that the agreement rates are
markedly lower than their expert counterparts.
Another indication of the presence of bad work-
ers is a low reference preference rate (RPR),
which we define as the proportion of time a ref-
erence translation wins (or ties in) a comparison
when it appears in one. Intuitively, the RPR
should be quite high, since it is quite rare that an
MT output ought to be judged better than the refer-
ence. This rate is 96.5% over the expert data, but
only 83.7% over the MTurk data. Compare this
to a randomly-clicking RPR of 66.67% (because
the two acceptable answers are that the reference
is either better than a system?s output or tied with
it).
Also telling would be the rate at which MTurk
workers agree with experts. To ensure that we ob-
tain enough overlapping data to calculate such a
rate, we purposely select one-sixth14 of our rank-
ing sets so that the five-system group is exactly one
that has been judged by an expert. This way, at
least one-sixth of the comparisons obtained from
an MTurk worker?s labels are comparisons for
14This means that on average Turkers ranked a set of sys-
tem outputs that had been ranked by experts on every other
screen, since each screen?s worth of work had three sets.
which we already have an expert judgment. When
we calculate the rate of agreement on this data,
we find that MTurk workers agree with the ex-
pert workers 53.2% of the time, or K = 0.297, and
when references are excluded, the agreement rate
is 50.0%, or K = 0.249. Ideally, we would want
those values to be in the 0.4?0.5 range, since that
is where the inter-annotator kappa coefficient lies
for the expert annotators.
7.3 Filtering MTurk data by agreement with
experts
We can use the agreement rate with experts to
identify MTurk workers who are not performing
the task as required. For each worker w of the
669 workers for whom we have such data, we
compute the worker?s agreement rate with the ex-
perts, and from it a kappa coefficient Kexp(w) for
that worker. (Given that P (E) is 0.333, Kexp(w)
ranges between?0.5 and +1.0.) We sort the work-
ers based on Kexp(w) in ascending order, and ex-
amine properties of the MTurk data as we remove
the lowest-ranked workers one by one (Figure 4).
We first note that the amount of data we ob-
tained from MTurk is so large, that we could af-
ford to eliminate close to 30% of the labels, and
we would still have twice as much data than us-
ing the expert data alone. We also note that two
workers in particular (the 103rd and 130th to be
removed) are likely responsible for the majority
of the bad data, since removing their data leads to
noticeable jumps in the reference preference rate
and the inter-annotator agreement rate (right two
curves of Figure 4). Indeed, examining the data for
those two workers, we find that their RPR values
are 55.7% and 51.9%, which is a clear indication
of random clicking.15
Looking again at those two curves shows de-
grading values as we continue to remove workers
in large droves, indicating a form of ?overfitting?
to agreement with experts (which, naturally, con-
tinues to increase until reaching 1.0; bottom left
curve). It is therefore important, if one were to fil-
ter out the MTurk data by removing workers this
way, to choose a cutoff carefully so that no crite-
rion is degraded dramatically.
In Appendix A, after reporting head-to-head
comparisons using only the expert data, we also
report head-to-head comparisons using the expert
15In retrospect, we should have performed this type of
analysis as the data was being collected, since such workers
could have been identified early on and blocked.
35
-20
40
60
80
100
120
140
160
0 100 200 300 400 500 600 700
# Workers Removed
M
T
u
r
k
 
D
a
t
a
 
R
e
m
a
i
n
i
n
g
(
%
 
o
f
 
E
x
p
e
r
t
 
D
a
t
a
)
-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0 100 200 300 400 500 600 700
# Workers Removed
A
g
r
e
e
m
e
n
t
 
w
i
t
h
 
E
x
p
e
r
t
 
D
a
t
a
 
(
k
a
p
p
a
)
82%
84%
86%
88%
90%
92%
94%
96%
98%
100%
0 100 200 300 400 500 600 700
# Workers Removed
R
e
f
e
r
e
n
c
e
 
P
r
e
f
e
r
e
n
c
e
 
R
a
t
e
0.10
0.15
0.20
0.25
0.30
0 100 200 300 400 500 600 700
# Workers Removed
I
n
t
e
r
-
A
n
n
o
t
a
t
o
r
 
A
g
r
e
e
m
e
n
t
 
(
k
a
p
p
a
)
Figure 4: The effect of removing an increasing number of MTurk workers. The order in which workers
are removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).
data combined with the MTurk data, in order to
be able to detect more significant differences be-
tween the systems. We choose the 300-worker
point as a reasonable cutoff point before combin-
ing the MTurk data with the expert data, based
on the characteristics of the MTurk data at that
point: a high reference preference rate, high inter-
annotator agreement, and, critically, a kappa co-
efficient vs. expert data of 0.449, which is close
to the expert inter-annotator kappa coefficient of
0.439.
7.4 Feasibility of using only MTurk data
In the previous subsection, we outlined an ap-
proach by which MTurk data can be filtered out
using expert data. Since we were to combine the
filtered MTurk data with the expert data to ob-
tain more significant differences, it was reason-
able to use agreement with experts to quantify the
MTurk workers? competency. However, we also
would like to know whether it is feasible to use the
MTurk data alone. Our aim here is not to boost the
differences we see by examining expert data, but
to eliminate our reliance on obtaining expert data
in the first place.
We briefly examined some simple ways of fil-
tering/combining the MTurk data, and measured
the Spearman rank correlations obtained from the
MTurk data (alone), as compared to the rankings
obtained using the expert data (alone), and report
them in Table 12. (These correlations do not in-
clude the references.)
We first see that even when using the MTurk
data untouched, we already obtain relatively high
correlation with expert ranking (?Unfiltered?).
This is especially true for the *-to-English lan-
guage pairs, where we collected much more data
than English-to-*. In fact, the relationship be-
tween the amount of data and the correlation val-
ues is very strong, and it is reasonable to expect
the correlation numbers for English-to-* to catch
up had more data been collected.
We also measure rank correlations when apply-
ing some simple methods of cleaning/weighting
MTurk data. The first method (?Voting?) is per-
forming a simple vote whenever redundant com-
parisons (i.e. from different workers) are avail-
able. The second method (?Kexp-filtered?) first re-
moves labels from the 300 worst workers accord-
ing to agreement with experts. The third method
36
(?RPR-filtered?) first removes labels from the 62
worst workers according to their RPR. The num-
bers 300 and 62 were chosen since those are the
points at which the MTurk data reaches the level
of expert data in the inter-annotator agreement and
RPR of the experts.
The fourth and fifth methods (?Weighted by
Kexp? and ?Weighted by K(RPR)?) do not re-
move any data, instead assigning weights to work-
ers based on their agreement with experts and their
RPR, respectively. Namely, for each worker, the
weight assigned by the fourth method is Kexp for
that worker, and the weight assigned by the fifth
method is K(RPR) for that worker.
Examining the correlation coefficients obtained
from those methods (Table 12), we see mixed re-
sults, and there is no clear winner among those
methods. It is also difficult to draw any conclusion
as to which method performs best when. However,
it is encouraging to see that the two RPR-based
methods perform well. This is noteworthy, since
there is no need to use expert data to weight work-
ers, which means that it is possible to evaluate a
worker using inherent, ?built-in? properties of that
worker?s own data, without resorting to making
comparisons with other workers or with experts.
8 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
and vice versa.
The number of participants grew substantially
compared to previous editions of the WMT work-
shop, with 33 groups from 29 institutions partic-
ipating in WMT10. Most groups participated in
the translation task only, while the system combi-
nation task attracted a somewhat smaller number
of participants
Unfortunately, fewer rule-based systems partic-
ipated in this year?s edition of WMT, compared
to previous editions. We hope to attract more
rule-based systems in future editions as they in-
crease the variation of translation output and for
some language pairs, such as German-English,
tend to outperform statistical machine translation
systems.
This was the first time that the WMT workshop
was held as a joint workshop with NIST?s Metric-
sMATR evaluation initiative. This joint effort was
very productive as it allowed us to focus more on
the two evaluation dimensions: manual evaluation
of MT performance and the correlation between
manual metrics and automated metrics.
This year was also the first time we have in-
troduced quality assessments by non-experts. In
previous years all assessments were carried out
through peer evaluation exclusively consisting of
developers of machine translation systems, and
thereby people who are used to machine transla-
tion output. This year we have facilitated Ama-
zon?s Mechanical Turk to investigate two as-
pects of manual evaluation: How stable are man-
ual assessments across different assessor profiles
(experts vs. non-experts) and how reliable are
quality judgments of non-expert users? While
the intra- and inter-annotator agreements between
non-expert assessors are considerably lower than
for their expert counterparts, the overall rankings
of translation systems exhibit a high degree of cor-
relation between experts and non-experts. This
correlation can be further increased by applying
various filtering strategies reducing the impact of
unreliable non-expert annotators.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.16
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-
0022, and the US National Science Foundation un-
der grant IIS-0713448.
References
Alexandre Allauzen, Josep M. Crego, lknur Durgar El-
Kahlout, and Francois Yvon. 2010. Limsi?s statisti-
cal translation systems for wmt?10. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 29?34, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Lo??c Barrault. 2010. Many: Open source mt system
combination at wmt?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
16http://www.statmt.org/wmt09/results.
html
37
Label Unfiltered Voting Kexp-filtered RPR-filtered Weighted by Weighted by
count Kexp K(RPR)
en-de 2,583 0.862 0.779 0.818 0.862 0.868 0.862
en-es 2,488 0.759 0.785 0.797 0.797 0.768 0.806
en-fr 1,578 0.826 0.840 0.791 0.814 0.802 0.814
en-cz 627 0.833 0.818 0.354 0.833 0.851 0.828
de-en 12,570 0.914 0.925 0.920 0.931 0.933 0.926
es-en 12,870 0.934 0.969 0.965 0.987 0.978 0.987
fr-en 9,197 0.880 0.865 0.920 0.919 0.907 0.917
cz-en 13,169 0.951 0.909 0.965 0.944 0.930 0.944
Table 12: Spearman rank coefficients for the MTurk data across the various language pairs, using differ-
ent methods to clean the data or weight workers. (These correlations were computed after excluding the
references.) Kexp is the kappa coefficient of the worker?s agreement rate with experts, with P (A) = 0.33.
K(RPR) is the kappa coefficient of the worker?s RPR (see 7.2), with P (A) = 0.66. In Kexp-filtering,
42% of labels remain, after removing 300 workers. In K(RPR)-filtering, 69% of labels remain, after
removing 62 workers.
and MetricsMATR, pages 252?256, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 257?262, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 263?270, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
302?307, Uppsala, Sweden, July. Association for
Computational Linguistics.
Ondrej Bojar and Kamil Kos. 2010. 2010 failures
in english-czech phrase-based mt. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 35?41, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with amazons mechan-
ical turk. In Proceedings NAACL-2010 Workshop on
Creating Speech and Language Data With Amazons
Mechanical Turk, Los Angeles.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, , Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the findings
of the 2009 workshop on statistical machine trans-
lation. In Proceedings of the Fourth Workshop on
Statistical Machine Translation (WMT09), Athens,
Greece.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2009), Singapore.
Elisabet Comelles, Jesus Gimenez, Lluis Marquez,
Irene Castellon, and Victoria Arranz. 2010.
Document-level automatic mt evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 308?313, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Hui Cong, Zhao Hai, Lu Bao-Liang, and Song Yan.
2010. An empirical study on development set se-
lection strategy for machine translation learning.
In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 42?46, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2010. Meteor-
next and the meteor paraphrase tables: Improved
38
evaluation support for five target languages. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 314?
317, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Marcus Dobrinkat, Tero Tapiovaara, Jaakko Va?yrynen,
and Kimmo Kettunen. 2010. Normalized compres-
sion distance based measures for metricsmatr 2010.
In Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
318?323, Uppsala, Sweden, July. Association for
Computational Linguistics.
Jinhua Du, Pavel Pecina, and Andy Way. 2010. An
augmented three-pass system combination frame-
work: Dcu combination system for wmt 2010. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
271?276, Uppsala, Sweden, July. Association for
Computational Linguistics.
Vladimir Eidelman, Chris Dyer, and Philip Resnik.
2010. The university of maryland statistical ma-
chine translation system for the fifth workshop on
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 47?51, Uppsala, Sweden,
July. Association for Computational Linguistics.
Christian Federmann, Andreas Eisele, Yu Chen, Sabine
Hunsicker, Jia Xu, and Hans Uszkoreit. 2010.
Further experiments with shallow hybrid mt sys-
tems. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 52?56, Uppsala, Sweden, July. Association
for Computational Linguistics.
Jesu?s Gonza?lez-Rubio, Germa?n Sanchis-Trilles, Joan-
Andreu Sa?nchez, Jesu?s Andre?s-Ferrer, Guillem
Gasco?, Pascual Mart??nez-Go?mez, Martha-Alicia
Rocha, and Francisco Casacuberta. 2010. The upv-
prhlt combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 277?
281, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Greg Hanneman, Jonathan Clark, and Alon Lavie.
2010. Improved features and grammar selection for
syntax-based mt. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 57?62, Uppsala, Sweden, July.
Association for Computational Linguistics.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. Fbk at wmt 2010: Word lattices for
morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 63?67, Uppsala, Sweden, July. Association
for Computational Linguistics.
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 324?328, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Kenneth Heafield and Alon Lavie. 2010. Cmu multi-
engine machine translation for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 68?
73, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor
Leusch, Saab Mansour, Daniel Stein, and Hermann
Ney. 2010. The rwth aachen machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 74?78, Uppsala, Sweden,
July. Association for Computational Linguistics.
Carlos A. Henr??quez Q., Marta Ruiz Costa-jussa`, Vi-
das Daudaravicius, Rafael E. Banchs, and Jose? B.
Marin?o. 2010. Using collocation segmentation to
augment the phrase table. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 79?83, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Almut Silja Hildebrand and Stephan Vogel. 2010.
Cmu system combination via hypothesis selection
for wmt?10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 282?285, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry,
and Philippe Langlais. 2010. The rali machine
translation system for wmt 2010. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 84?90, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Michael Jellinghaus, Alexandros Poulis, and David
Kolovratn??k. 2010. Exodus - exploring smt for eu
institutions. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 91?95, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
2006 Workshop on Statistical Machine Translation,
New York, New York.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions, Prague, Czech Republic.
39
Philipp Koehn, Barry Haddow, Philip Williams, and
Hieu Hoang. 2010. More linguistic annotation for
statistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 96?101, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger
Schwenk. 2010. Lium smt machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 102?107, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Samuel Larkin, Boxing Chen, George Foster, Ulrich
Germann, Eric Joanis, Howard Johnson, and Roland
Kuhn. 2010. Lessons from nrcs portage system at
wmt 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 108?113, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 290?
295, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 114?118, Uppsala, Sweden, July.
Association for Computational Linguistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation evaluation of sentences
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 329?
334, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Sushant Narsale. 2010. Jhu system combination
scheme for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 286?289, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Jan Niehues, Teresa Herrmann, Mohammed Mediani,
and Alex Waibel. 2010. The karlsruhe institute
for technology translation system for the acl-wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 119?123, Uppsala, Sweden, July. Association
for Computational Linguistics.
NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat,
Pratyush Banerjee, Ankit K. Srivastava, Jinhua Du,
Pavel Pecina, Sudip Kumar Naskar, Mikel L. For-
cada, and Andy Way. 2010. Matrex: The dcu mt
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 124?129, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Aaron Phillips. 2010. The cunei machine translation
platform for wmt ?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 130?135, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Juan Pino, Gonzalo Iglesias, Adria` de Gispert, Graeme
Blackwood, Jamie Brunning, and William Byrne.
2010. The cued hifst system for the wmt10 trans-
lation shared task. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 136?141, Uppsala, Sweden,
July. Association for Computational Linguistics.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The lig machine translation system for wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 142?147, Uppsala, Sweden, July. Association
for Computational Linguistics.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. Bbn system descrip-
tion for wmt10 system combination task. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 296?
301, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Linear inversion transduction grammar alignments
as a second translation path. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 148?152, Uppsala,
40
Sweden, July. Association for Computational Lin-
guistics.
Germa?n Sanchis-Trilles, Jesu?s Andre?s-Ferrer, Guillem
Gasco?, Jesu?s Gonza?lez-Rubio, Pascual Mart??nez-
Go?mez, Martha-Alicia Rocha, Joan-Andreu
Sa?nchez, and Francisco Casacuberta. 2010.
Upv-prhlt english?spanish system for wmt10. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
153?157, Uppsala, Sweden, July. Association for
Computational Linguistics.
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 197?204, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Lane Schwartz. 2010. Reproducible results in parsing-
based machine translation: The jhu shared task sub-
mission. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 158?163, Uppsala, Sweden, July.
Association for Computational Linguistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and oovs: Two problems for translation
between german and english. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 164?169, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Jo?rg Tiedemann. 2010. To cache or not to cache?
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 170?175, Uppsala, Sweden,
July. Association for Computational Linguistics.
Sami Virpioja, Jaakko Va?yrynen, Andre Man-
sikkaniemi, and Mikko Kurimo. 2010. Apply-
ing morphological decompositions to statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 176?181, Uppsala, Sweden,
July. Association for Computational Linguistics.
Zdene?k Z?abokrtsky?, Martin Popel, and David Marec?ek.
2010. Maximum entropy translation model in
dependency-based mt framework. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 182?187, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Billy Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 335?
339, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Francisco Zamora-Martinez and Germa?n Sanchis-
Trilles. 2010. Uch-upv english?spanish system for
wmt10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 188?192, Uppsala, Sweden, July.
Association for Computational Linguistics.
Daniel Zeman. 2010. Hierarchical phrase-based mt
at the charles university for the wmt 2010 shared
task. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 193?196, Uppsala, Sweden, July. Association
for Computational Linguistics.
41
A Pairwise system comparisons by human judges
Tables 13?20 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
B Automatic scores
The tables on pages 33?32 give the automatic scores for each of the systems.
C Pairwise system comparisons for combined expert and non-expert data
Tables 21?20 show pairwise comparisons between systems for the into English direction when non-
expert judgments have been added.
The number of pairwise comparisons at the ? level of significance increases from 48 to 50, and the
number at the ? level of significants increases from 79 to 80 (basically same number). However, the
? level of significance went up considerably, from 280 to 369. That?s a 31% increase. 75 of ? are
comparisons involving the reference, then the non-reference ? count went up from 205 to 294, a 43%
increase.
R
E
F
C
A
M
B
R
ID
G
E
C
M
U
-S
T
A
T
X
F
E
R
C
U
-Z
E
M
A
N
D
F
K
I
G
E
N
E
V
A
H
U
IC
O
N
G
JH
U
L
IG
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
C
M
U
-H
Y
P
O
S
E
L
-C
O
M
B
O
D
C
U
-C
O
M
B
O
JH
U
-C
O
M
B
O
L
IU
M
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .00? .00? .00? .00? .04? .03? .00? .00? .00? .00? .04? .00? .04? .00? .00? .00? .00? .05? .06? .03? .09? .04? .04?
CAMBRIDGE .79? ? .36 .16? .12? .23? .27 .43 .26? .38 .24 .3 .28 .51 .34 .23 .37 .24 .32 .46 .24 .29 .45 .59? .44
CMU-STATXFER .84? .58 ? .16? .48 .14? .19 .39 .33 .54 .54? .50? .36 .50 .70? .55? .50 .46 .58? .67? .50 .56? .48 .58? .52?
CU-ZEMAN 1.00? .77? .72? ? .76? .37 .73? .74? .79? .77? .77? .81? .75? .94? .86? .77? .89? .67 .77? .79? .81? .81? .77? .96? .86?
DFKI 1.00? .72? .45 .12? ? .32 .48 .50 .52 .53 .56 .65 .53 .62 .55 .43 .61? .50 .68? .73? .70? .60 .59? .72? .71?
GENEVA 1.00? .69? .76? .48 .56 ? .47 .71? .79? .72? .79? .71? .68? .76? .83? .57 .86? .72? .71? .69? .76? .65? .88? .96? .70
HUICONG .86? .54 .29 .12? .26 .37 ? .48 .31 .43 .63? .62? .53 .55 .53? .44 .50 .55 .52 .68? .52? .51 .52? .57 .53
JHU .83? .39 .42 .13? .33 .19? .3 ? .3 .36 .56? .56? .47 .52 .46 .29 .36 .42 .42 .59? .50 .31 .43 .29 .37
LIG .97? .63? .36 .15? .37 .18? .40 .60 ? .62? .57? .39 .35 .54? .46 .33 .34 .38 .54? .48? .42 .44 .50 .61? .56
LIMSI .96? .41 .23 .19? .31 .17? .32 .50 .28? ? .35 .42 .21 .62? .25 .21 .33 .22 .42 .35 .43 .32 .26 .35 .41
LIUM .83? .33 .21? .13? .41 .05? .13? .15? .09? .3 ? .39 .19 .36 .43 .26 .23? .28 .29 .45 .28 .26 .28 .33 .28
NRC .96? .3 .10? .10? .32 .24? .15? .22? .22 .33 .43 ? .26 .58 .26 .24 .3 .50 .36 .45 .47? .23 .38 .36? .35
ONLINEA .96? .55 .57 .14? .42 .16? .42 .4 .39 .53 .52 .47 ? .52? .46 .36 .64 .57 .59 .50 .59 .42 .46 .43 .48
ONLINEB .87? .37 .33 .03? .29 .12? .31 .26 .16? .12? .39 .35 .20? ? .33 .38 .17? .36 .29 .21 .33 .3 .3 .32 .21?
RALI .89? .45 .15? .06? .35 .04? .12? .42 .35 .46 .32 .42 .39 .52 ? .32 .31 .26 .43 .41 .27 .43 .40 .63? .26
RWTH .91? .46 .21? .05? .51 .36 .44 .46 .53 .39 .48 .48 .39 .48 .48 ? .39 .38 .39 .52 .46 .53? .52 .50? .25
UEDIN .96? .40 .33 .03? .28? .03? .28 .29 .49 .38 .61? .3 .32 .50? .34 .24 ? .42 .33 .43 .48 .18? .13 .27 .38
BBN-C .90? .48 .46 .29 .39 .22? .27 .27 .46 .43 .28 .35 .33 .39 .29 .34 .26 ? .28 .44? .33 .26 .62? .36 .28
CMU-HEA-C .89? .50 .23? .14? .30? .21? .26 .25 .17? .33 .43 .16 .36 .43 .26 .29 .24 .24 ? .48 .27 .13 .25 .30 .15
CMU-HYP-C .81? .17 .19? .11? .19? .19? .14? .14? .19? .40 .23 .18 .29 .46 .35 .29 .21 .15? .17 ? .26 .18 .07? .32 .21
DCU-C .88? .27 .25 .11? .22? .24? .20? .28 .21 .35 .50 .10? .31 .44 .27 .29 .22 .21 .2 .30 ? .12? .26 .26 .08
JHU-C .86? .48 .16? .16? .33 .21? .35 .41 .32 .44 .39 .35 .39 .37 .26 .19? .50? .23 .32 .43 .40? ? .36 .27 .39
LIUM-C .87? .41 .36 .13? .31? .08? .21? .48 .31 .47 .44 .24 .39 .52 .28 .28 .33 .27? .25 .67? .26 .44 ? .54? .48
RWTH-C .88? .18? .13? .04? .22? .04? .14 .24 .25? .3 .33 .05? .43 .50 .30? .13? .23 .14 .18 .21 .19 .23 .11? ? .24
UPV-C .92? .25 .12? .10? .16? .3 .25 .34 .29 .31 .34 .29 .39 .65? .39 .36 .3 .45 .27 .36 .23 .16 .24 .28 ?
> others .90 .44 .31 .13 .33 .18 .29 .37 .34 .42 .44 .38 .37 .51 .41 .31 .38 .35 .38 .48 .39 .36 .40 .46 .37
>= others .98 .66 .51 .21 .42 .27 .51 .59 .53 .65 .71 .66 .52 .71 .65 .55 .65 .64 .70 .77 .72 .65 .64 .77 .68
Table 13: Sentence-level ranking for the WMT10 French-English News Task
42
R
E
F
C
A
M
B
R
ID
G
E
C
U
-Z
E
M
A
N
D
F
K
I
E
U
G
E
N
E
V
A
JH
U
K
O
C
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .08? .02? .00? .04? .08? .13? .06? .09? .09? .07? .16? .11? .12? .12? .12? .05? .07? .08? .09?
CAMBRIDGE .82? ? .16? .24? .15? .07? .35 .10? .42 .36 .43 .27 .67? .46 .39 .44 .40 .46 .48? .40
CU-ZEMAN .98? .82? ? .47 .54? .62? .71? .41 .79? .82? .70? .67? .85? .90? .75? .72? .92? .82? .88? .82?
DFKI .95? .66? .31 ? .46 .25? .78? .36 .59 .62? .75? .65? .45 .56? .75? .69? .71? .63? .57 .65?
EU .96? .78? .30? .41 ? .55 .68? .16? .76? .72? .82? .67? .63? .86? .78? .78? .76? .76? .75? .71?
GENEVA .86? .81? .23? .55? .34 ? .65? .25? .65? .70? .69? .66? .77? .71? .70? .89? .75? .63? .84? .75?
JHU .77? .42 .15? .22? .22? .22? ? .06? .58? .47 .52? .49 .70? .61? .53 .64? .53? .65? .68? .50
KOC .85? .67? .4 .58 .55? .69? .82? ? .76? .85? .81? .72? .86? .82? .86? .85? .77? .77? .74? .79?
LIMSI .84? .23 .08? .29 .09? .30? .21? .08? ? .33 .37 .17? .51 .40 .29 .45 .49 .40 .61? .28
LIUM .85? .39 .07? .32? .11? .21? .44 .07? .46 ? .44 .4 .32 .44 .37 .64? .35 .40 .35 .42
NRC .91? .43 .15? .20? .11? .25? .21? .09? .31 .45 ? .32 .48 .44 .49 .61? .52? .30 .58? .40
ONLINEA .80? .51 .21? .33? .23? .15? .41 .14? .60? .42 .54 ? .52? .56? .36 .67? .61? .45 .50 .44
ONLINEB .87? .23? .08? .43 .23? .11? .12? .08? .27 .36 .43 .25? ? .38 .31 .33 .52 .33? .46 .29
RALI .83? .38 .05? .27? .11? .15? .22? .10? .36 .44 .49 .31? .50 ? .38 .44 .42 .37 .38 .34
RWTH .76? .33 .11? .12? .15? .17? .34 .05? .34 .44 .29 .42 .49 .40 ? .56 .48 .44 .53? .50
UEDIN .84? .29 .20? .17? .12? .09? .19? .07? .33 .23? .24? .24? .56 .31 .3 ? .36? .27 .51 .18?
CMU-HEAFIELD-COMBO .90? .23 .04? .23? .18? .12? .22? .11? .32 .41 .20? .23? .28 .31 .31 .11? ? .29 .24 .3
KOC-COMBO .91? .26 .08? .31? .17? .28? .20? .07? .23 .26 .19 .36 .57? .37 .32 .32 .42 ? .38 .34
RWTH-COMBO .85? .21? .02? .36 .16? .07? .12? .07? .16? .3 .30? .4 .34 .32 .06? .26 .35 .16 ? .21?
UPV-COMBO .87? .38 .08? .30? .19? .19? .37 .11? .39 .24 .33 .37 .44 .27 .34 .46? .35 .28 .50? ?
> others .87 .43 .15 .30 .22 .25 .38 .13 .44 .45 .46 .41 .53 .49 .44 .52 .53 .45 .53 .45
>= others .92 .63 .26 .40 .32 .35 .53 .26 .66 .63 .62 .55 .68 .66 .63 .70 .74 .68 .75 .66
Table 14: Sentence-level ranking for the WMT10 English-French News Task
43
R
E
F
A
A
L
T
O
C
M
U
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
H
U
IC
O
N
G
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
U
E
D
IN
U
M
D
U
P
P
S
A
L
A
U
U
-M
S
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
C
M
U
-H
Y
P
O
S
E
L
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .03? .00? .06? .03? .00? .00? .05? .00? .00? .03? .06? .09? .06? .00? .09? .03? .03? .14? .03? .06? .03? .03? .06? .00?
AALTO 1.00? ? .50 .31 .60 .69? .39 .41 .71? .31 .45 .60? .59? .65? .66? .64? .81? .45 .41 .69? .72? .75? .55 .55? .76? .57?
CMU .93? .31 ? .29 .49 .57? .38 .50 .74? .13? .44 .59? .57? .59? .60? .67? .59? .41 .50 .68? .67? .46 .64? .55? .67? .54?
CU-ZEMAN 1.00? .44 .56 ? .58 .64? .17 .44 .75? .38 .50 .54? .76? .79? .73? .72? .72? .50? .73? .78? .80? .68? .72? .62? .68? .73?
DFKI .92? .25 .32 .27 ? .53 .36 .46 .65? .07? .50 .47 .47 .69? .56 .35 .55 .58 .47 .67? .61? .52 .47 .38 .67? .51
FBK .97? .20? .16? .14? .38 ? .11? .31 .45 .10? .22? .36 .50 .57? .37 .43 .40 .12? .17? .48? .43 .35 .38 .22 .38 .39
HUICONG .93? .35 .28 .46 .43 .75? ? .52 .69? .16? .39 .42 .64? .79? .31 .51? .78? .27 .41 .49 .74? .68? .60? .37 .68? .56?
JHU .86? .34 .29 .16 .43 .31 .26 ? .61? .15? .35 .36 .45 .69? .52? .56? .64? .27 .36 .70? .53 .47 .66? .52 .68? .44
KIT .89? .21? .10? .14? .29? .33 .19? .14? ? .03? .27 .21? .36 .46 .17? .29 .24 .25? .25? .48 .23? .31 .38 .2 .36 .12?
KOC .96? .58 .77? .48 .70? .77? .58? .71? .97? ? .77? .90? .72? .82? .76? .84? .81? .84? .66? .83? .87? .79? .77? .75? .93? .71?
LIMSI 1.00? .23 .28 .35 .35 .53? .33 .45 .41 .19? ? .49 .48 .63? .49 .63? .52 .36 .29 .73? .53? .45 .59? .29 .56? .59?
LIU .88? .12? .15? .16? .39 .21 .46 .36 .61? .00? .27 ? .44 .63? .49 .45 .53 .27? .33 .67? .55? .46 .44 .32 .37 .55
ONLINEA .92? .15? .23? .24? .42 .34 .21? .35 .50 .10? .32 .36 ? .41 .4 .44 .37 .32 .34 .36 .4 .47 .3 .26 .48 .41
ONLINEB .68? .18? .29? .17? .26? .24? .18? .23? .33 .18? .23? .27? .34 ? .3 .15? .29 .24? .15? .44 .28 .33? .20? .21? .38 .3
RWTH .88? .17? .20? .20? .37 .49 .41 .23? .61? .16? .4 .3 .43 .56 ? .39 .50 .26 .49 .37 .29 .34 .41 .26 .44 .2
UEDIN .89? .14? .22? .13? .62 .34 .18? .22? .39 .03? .17? .3 .44 .67? .42 ? .39 .15? .14? .52? .40 .36 .43 .26 .41 .38
UMD .91? .07? .14? .08? .36 .34 .11? .25? .48 .16? .24 .34 .52 .56 .41 .45 ? .16? .21? .41 .28 .29 .43 .29 .25 .23
UPPSALA .97? .32 .34 .17? .36 .54? .23 .37 .70? .00? .41 .62? .56 .68? .57 .64? .59? ? .2 .63? .69? .51? .60? .33 .69? .63?
UU-MS .82? .22 .43 .14? .45 .51? .19 .21 .68? .14? .39 .52 .60 .64? .44 .53? .61? .28 ? .36 .58? .52? .53? .30 .64? .44
BBN-C .86? .25? .10? .07? .27? .17? .23 .18? .35 .07? .15? .12? .32 .41 .3 .19? .22 .15? .27 ? .39 .06? .23? .11? .21 .18?
CMU-HEA-C .87? .14? .15? .08? .29? .33 .04? .26 .53? .00? .20? .24? .44 .31 .46 .23 .53 .15? .13? .27 ? .40 .2 .14? .22 .28
CMU-HYP-C .94? .25? .24 .14? .44 .3 .15? .26 .47 .08? .45 .31 .42 .67? .24 .36 .46 .14? .21? .50? .32 ? .43 .28 .51? .42
JHU-C .97? .34 .11? .20? .29 .34 .29? .03? .38 .12? .07? .29 .55 .67? .34 .32 .23 .24? .24? .48? .40 .32 ? .27 .37 .31
KOC-C .88? .00? .23? .21? .53 .44 .29 .22 .43 .08? .36 .50 .53 .63? .39 .37 .39 .28 .19 .64? .61? .38 .55 ? .48? .46
RWTH-C .82? .09? .06? .29? .25? .25 .18? .18? .24 .03? .19? .26 .36 .54 .25 .26 .33 .06? .14? .29 .22 .23? .3 .17? ? .13?
UPV-C .97? .17? .21? .17? .36 .36 .23? .19 .67? .20? .18? .29 .41 .40 .40 .38 .48 .17? .31 .50? .43 .27 .27 .27 .65? ?
> others .91 .23 .25 .20 .39 .42 .24 .30 .53 .11 .31 .38 .47 .59 .42 .43 .48 .27 .30 .53 .49 .42 .44 .31 .51 .41
>= others .96 .42 .46 .36 .50 .66 .47 .53 .72 .23 .52 .59 .63 .73 .62 .66 .68 .51 .55 .77 .73 .65 .67 .59 .75 .64
Table 15: Sentence-level ranking for the WMT10 German-English News Task
R
E
F
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
S
F
U
U
E
D
IN
U
P
P
S
A
L
A
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .03? .06? .01? .02? .05? .00? .00? .01? .04? .03? .01? .01? .01? .02? .01? .01? .05? .06?
CU-ZEMAN .97? ? .85? .67? .62? .78? .58? .70? .64? .80? .85? .64? .52 .80? .61? .79? .69? .76? .73?
DFKI .89? .14? ? .36? .24? .38 .30? .27? .36? .36? .55 .35? .21? .41 .39 .46 .38? .47 .37?
FBK .97? .30? .59? ? .35? .42 .12? .36 .48 .48 .64? .39 .29? .46 .30? .44 .46 .48 .38
JHU .98? .27? .72? .57? ? .59? .30? .51 .53 .56? .65? .43 .39 .66? .45 .56 .61? .52 .47
KIT .92? .18? .55 .42 .29? ? .23? .32 .32? .43 .53? .41 .27? .43 .23? .41 .41 .42 .37
KOC 1.00? .37? .64? .82? .62? .70? ? .74? .74? .74? .82? .63? .48 .62? .65? .73? .67? .81? .71?
LIMSI .95? .27? .68? .39 .45 .49 .17? ? .49 .74? .70? .51 .28? .58? .32 .51 .53? .52? .31
LIU .95? .32? .59? .4 .36 .58? .21? .37 ? .39 .74? .33? .23? .55? .36? .49 .42 .46 .38
ONLINEA .95? .16? .55? .4 .36? .45 .21? .23? .50 ? .56? .38 .23? .41 .23? .48 .4 .50 .33?
ONLINEB .92? .12? .42 .26? .27? .33? .14? .23? .21? .32? ? .24? .14? .39 .19? .29? .27? .36 .32?
RWTH .98? .33? .61? .51 .47 .46 .30? .33 .52? .55 .71? ? .33? .57? .45 .40 .51? .47 .46
SFU .98? .42 .77? .66? .51 .69? .48 .68? .69? .72? .77? .56? ? .82? .53 .65? .69? .73? .62?
UEDIN .94? .17? .51 .4 .31? .49 .34? .25? .30? .52 .52 .36? .10? ? .33? .31 .42 .38 .22?
UPPSALA .97? .36? .55 .51? .47 .70? .25? .46 .57? .67? .71? .41 .38 .54? ? .53? .42 .58? .40
CMU-HEAFIELD-COMBO .96? .17? .49 .36 .36 .37 .21? .35 .49 .42 .64? .38 .28? .48 .28? ? .35 .46 .35
KOC-COMBO .99? .27? .56? .32 .27? .32 .23? .32? .41 .55 .64? .30? .21? .37 .36 .41 ? .34 .36
RWTH-COMBO .92? .17? .50 .34 .35 .41 .09? .25? .38 .4 .54 .38 .20? .42 .19? .28 .35 ? .16?
UPV-COMBO .93? .23? .58? .38 .36 .51 .23? .50 .49 .57? .60? .42 .28? .51? .3 .38 .46 .48? ?
> others .95 .24 .57 .44 .37 .48 .24 .39 .45 .51 .63 .40 .27 .51 .34 .45 .44 .49 .39
>= others .98 .28 .62 .56 .46 .60 .30 .51 .55 .59 .70 .51 .34 .62 .47 .59 .59 .65 .55
Table 16: Sentence-level ranking for the WMT10 English-German News Task
44
R
E
F
C
A
M
B
R
ID
G
E
C
O
L
U
M
B
IA
C
U
-Z
E
M
A
N
D
F
K
I
H
U
IC
O
N
G
JH
U
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
U
P
C
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .01? .01? .01? .00? .00? .00? .00? .00? .01? .02? .05? .01? .04?
CAMBRIDGE .95? ? .23? .14? .34? .31? .41 .34 .62? .45? .35 .40? .42 .22? .44
COLUMBIA .97? .58? ? .25? .52 .45 .59? .53? .65? .60? .47 .56? .55? .45 .58?
CU-ZEMAN .96? .71? .59? ? .60? .68? .79? .66? .75? .80? .66? .79? .78? .69? .75?
DFKI .97? .51? .37 .23? ? .43 .59? .52? .66? .62? .48 .53? .55? .55? .64?
HUICONG .95? .50? .34 .21? .41 ? .45 .50 .66? .61? .39 .50? .59? .40 .52?
JHU .98? .39 .22? .12? .30? .33 ? .37 .56? .51? .34 .39 .34? .22? .34
ONLINEA .96? .46 .37? .23? .32? .38 .44 ? .59? .53? .4 .50 .36 .30? .54?
ONLINEB .88? .25? .21? .16? .23? .21? .27? .23? ? .35 .24? .28? .34? .22? .36
UEDIN .96? .31? .28? .10? .25? .19? .25? .31? .48 ? .23? .27? .31 .23? .2
UPC .94? .47 .4 .20? .41 .33 .43 .46 .66? .56? ? .50? .52? .48? .49?
BBN-COMBO .95? .26? .31? .09? .32? .34? .33 .37 .54? .44? .33? ? .35 .24? .34
CMU-HEAFIELD-COMBO .91? .39 .21? .08? .34? .22? .16? .42 .57? .45 .31? .31 ? .14? .27
JHU-COMBO .95? .40? .32 .15? .36? .31 .44? .50? .66? .50? .32? .47? .43? ? .43?
UPV-COMBO .92? .35 .28? .16? .27? .23? .38 .28? .47 .30 .28? .26 .35 .25? ?
> others .95 .41 .30 .15 .33 .32 .39 .39 .56 .48 .34 .41 .43 .32 .43
>= others .99 .61 .45 .27 .45 .50 .61 .54 .70 .69 .51 .62 .66 .55 .66
Table 17: Sentence-level ranking for the WMT10 Spanish-English News Task
R
E
F
C
A
M
B
R
ID
G
E
C
U
-Z
E
M
A
N
D
C
U
D
F
K
I
JH
U
K
O
C
O
N
L
IN
E
A
O
N
L
IN
E
B
S
F
U
U
E
D
IN
U
P
V
U
C
H
-U
P
V
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .02? .07? .15? .07? .02? .11? .14? .07? .07? .03? .06? .09? .06? .03? .07?
CAMBRIDGE .91? ? .28? .45 .38 .45 .11? .52 .61? .21? .52 .47 .35 .54 .51 .39 .49
CU-ZEMAN .95? .70? ? .79? .75? .85? .49 .83? .82? .74? .87? .67? .85? .81? .80? .70? .74?
DCU .93? .32 .21? ? .45 .32 .09? .70? .59 .24? .48 .38 .29 .32 .36 .24 .14?
DFKI .80? .41 .15? .45 ? .38 .12? .64? .57 .4 .57 .31 .41 .59 .50 .48 .47
JHU .90? .37 .10? .52 .56 ? .17? .67? .67? .26? .34 .3 .49 .54 .53? .47 .35
KOC .98? .87? .47 .88? .73? .76? ? .76? .87? .67? .83? .86? .90? .87? .90? .86? .86?
ONLINEA .82? .42 .08? .30? .18? .24? .20? ? .49 .36 .25? .17? .25? .45 .30? .29 .18?
ONLINEB .76? .26? .10? .32 .37 .22? .10? .34 ? .21? .28 .24? .32 .33 .22? .19? .27?
SFU .91? .54? .19? .67? .51 .63? .27? .64 .72? ? .74? .57? .68? .77? .71? .64? .46
UEDIN .91? .3 .08? .4 .38 .34 .14? .71? .49 .09? ? .34 .4 .58 .33 .3 .31
UPV .94? .34 .07? .41 .53 .54 .07? .73? .61? .27? .45 ? .37 .51 .44 .38 .48?
UCH-UPV .90? .55 .07? .58 .51 .41 .08? .69? .52 .24? .51 .46 ? .47 .41 .49 .49
CMU-HEAFIELD-COMBO .83? .29 .13? .37 .38 .35 .07? .48 .54 .08? .29 .26 .28 ? .17? .21? .21
KOC-COMBO .88? .27 .15? .40 .42 .24? .03? .62? .60? .15? .41 .27 .34 .53? ? .3 .40
RWTH-COMBO .92? .36 .21? .52 .33 .31 .10? .55 .65? .14? .37 .22 .41 .52? .48 ? .31
UPV-COMBO .91? .32 .13? .69? .4 .32 .09? .76? .52? .36 .38 .19? .31 .45 .35 .28 ?
> others .89 .39 .15 .48 .44 .41 .14 .61 .58 .29 .46 .36 .42 .51 .44 .39 .40
>= others .93 .54 .23 .61 .55 .55 .19 .69 .71 .40 .61 .55 .54 .68 .62 .59 .60
Table 18: Sentence-level ranking for the WMT10 English-Spanish News Task
45
R
E
F
A
A
L
T
O
C
M
U
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .04? .02? .03? .00? .02? .00? .03? .03? .04? .01? .04? .02?
AALTO .88? ? .49 .51 .22? .38 .64? .55? .57? .71? .64? .65? .59?
CMU .97? .35 ? .4 .14? .18? .59? .49? .45? .57? .50? .34 .43
CU-BOJAR .90? .33 .43 ? .12? .20? .64? .45 .45 .54? .42 .42 .41
CU-ZEMAN .99? .60? .77? .75? ? .56? .81? .78? .88? .79? .84? .84? .76?
ONLINEA .92? .46 .68? .59? .28? ? .65? .54? .72? .75? .58? .57? .66?
ONLINEB .97? .27? .28? .21? .10? .17? ? .25? .32 .22 .21? .32 .28
UEDIN .95? .28? .26? .38 .07? .22? .49? ? .60? .52? .33 .31 .32
BBN-COMBO .92? .31? .20? .39 .08? .15? .41 .16? ? .27 .25 .3 .26
CMU-HEAFIELD-COMBO .90? .13? .23? .25? .07? .15? .31 .23? .34 ? .18? .35 .28
JHU-COMBO .93? .20? .19? .33 .08? .25? .48? .39 .38 .52? ? .37 .42
RWTH-COMBO .92? .18? .37 .38 .13? .25? .34 .28 .43 .40 .26 ? .25
UPV-COMBO .96? .25? .36 .41 .11? .27? .45 .35 .37 .44 .31 .34 ?
> others .93 .28 .36 .38 .11 .23 .49 .38 .47 .48 .38 .40 .40
>= others .98 .43 .55 .55 .22 .37 .70 .61 .70 .71 .62 .65 .63
Table 19: Sentence-level ranking for the WMT10 Czech-English News Task
R
E
F
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
C
U
-Z
E
M
A
N
D
C
U
E
U
R
O
T
R
A
N
S
K
O
C
O
N
L
IN
E
A
O
N
L
IN
E
B
P
C
-T
R
A
N
S
P
O
T
S
D
A
M
S
F
U
U
E
D
IN
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
D
C
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .04? .04? .03? .01? .05? .03? .08? .04? .04? .03? .02? .02? .04? .08? .04? .07? .04?
CU-BOJAR .87? ? .46 .27? .12? .28? .16? .17? .44 .4 .11? .27? .41 .28 .52? .28 .42 .43
CU-TECTO .88? .36 ? .30? .23? .38 .17? .28? .56? .44 .29? .27? .36 .45 .51? .4 .58? .35
CU-ZEMAN .91? .58? .51? ? .38 .49 .19? .39 .62? .63? .36 .41 .48 .51? .58? .48? .54? .55?
DCU .98? .73? .52? .43 ? .59? .22? .47 .74? .63? .47? .53? .56? .77? .77? .62? .76? .71?
EUROTRANS .88? .61? .47 .33 .30? ? .10? .33 .51 .54? .25? .27? .49 .57? .59? .49 .57? .60?
KOC .93? .69? .67? .54? .49? .77? ? .54? .71? .70? .51? .55? .64? .72? .78? .65? .76? .78?
ONLINEA .91? .62? .57? .51 .39 .44 .24? ? .66? .62? .39 .43 .55? .60? .61? .59? .73? .61?
ONLINEB .91? .31 .29? .27? .13? .33 .14? .19? ? .44 .22? .09? .39 .19 .34 .24? .22? .39
PC-TRANS .88? .45 .43 .24? .26? .29? .21? .24? .49 ? .22? .27? .37 .43 .55? .33? .49 .41
POTSDAM .88? .60? .51? .40 .27? .59? .25? .47 .63? .64? ? .45 .52? .56? .69? .61? .70? .68?
SFU .95? .52? .56? .4 .30? .61? .27? .39 .65? .64? .29 ? .55? .54? .76? .53? .70? .60?
UEDIN .94? .39 .44 .33 .23? .32 .20? .26? .32 .49 .25? .26? ? .43 .57? .18 .46? .42
CMU-HEAFIELD-COMBO .91? .42 .39 .23? .10? .27? .14? .19? .23 .35 .24? .19? .28 ? .48? .28 .34 .29
DCU-COMBO .84? .23? .27? .23? .03? .31? .10? .21? .42 .31? .15? .10? .16? .20? ? .18? .27? .22?
KOC-COMBO .91? .37 .49 .25? .10? .39 .17? .32? .42? .55? .17? .27? .26 .33 .41? ? .32 .22
RWTH-COMBO .88? .29 .34? .28? .05? .26? .10? .17? .48? .43 .16? .15? .24? .33 .46? .36 ? .29
UPV-COMBO .92? .37 .52 .22? .09? .25? .10? .19? .28 .47 .15? .25? .33 .24 .49? .34 .39 ?
> others .91 .45 .44 .32 .20 .39 .16 .29 .49 .49 .25 .28 .40 .43 .54 .39 .50 .45
>= others .96 .66 .60 .50 .38 .54 .33 .44 .70 .62 .44 .45 .62 .69 .75 .66 .70 .68
Table 20: Sentence-level ranking for the WMT10 English-Czech News Task
46
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-. +,/- +,/. +,-0 +,/+ +,-1 +,-0 +,-. +,-1 +,/+ +,-/ +,2- +,3/ +,24 +,./ +,/3 +,--''567*8'* +,4+ +,/1 !"#$ !"#% !"#% !"#% !"#% !"#! !"## !"#& !"#' !"() !"'& !"*& !"%% !"#$ !"%)78&69:67*8'* !"&' +,/1 +,// +,/- +,/- +,/. +,/. +,/+ +,/- !"#& +,/+ !"() !"'& +,.; !"%% !"#$ +,-178& +,// +,// +,/. +,/2 +,/2 +,/3 +,/2 +,-; +,/2 +,/- +,-1 +,24 !"'& +,.. +,-3 +,/- +,-;7&6'*<"= +,// +,/; +,/. +,/+ +,/3 +,-0 +,/+ +,-/ +,/3 +,/. +,-4 +,2/ +,3/ +,.+ +,.4 +,/. +,-;7&6>?8"5 +,22 +,-0 +,/2 +,-/ +,-; +,-/ +,-/ +,.0 +,-/ +,-; +,-2 +,23 +,3. +,2+ +,.+ +,/+ +,-3<@&67*8'* +,;2 +,/4 !"#$ !"#% !"#% +,/. !"#% +,-1 +,/. +,// +,-0 +,21 !"'& +,./ +,-2 +,/. +,-4*5#A5?B +,.4 +,/- +,/2 +,-0 +,/+ +,-1 +,/+ +,-2 +,-4 +,-0 +,-- +,2- +,3- +,24 +,.- +,-0 +,-2*5#A5?C +,4+ !"#) !"#$ !"#% +,/. +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,.; +,-. +,/. +,-;=D)@67*8'* +,;/ +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/. +,/; +,-0 !"() !"'& +,./ +,-2 +,// +,-1&?EA5 +,;3 +,/4 +,/- +,/2 +,/. +,/3 +,/2 +,-1 +,/. +,// +,-0 +,21 +,3; +,.- +,-2 +,// +,-1&FG67*8'* +,;. +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,./ +,-. +,// +,-1!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-0 +,/2 -,04 !"%' +,+2 +,+4 +,24 +,-2 +,2/ +,/2 +,/2 +,31 ;,2+''567*8'* +,/- !"#& /,-; !"%+ !"!* +,33 !"** !"%+ !"*! !"#+ !"#+ !"(# &"'!78&69:67*8'* +,/- +,/4 /,-. !"%+ !"!* +,+0 !"** +,-1 !"*! +,/4 +,/4 !"(# ;,0178& +,/+ +,/. /,3+ +,-/ +,+2 +,+0 +,.3 +,-; +,21 +,/- +,/- +,22 ;,;;7&6'*<"= +,/2 +,// /,2. +,-/ +,+2 +,+/ +,.+ +,-/ +,24 +,// +,// +,30 ;,2-7&6>?8"5 +,-4 +,-0 -,44 +,.; +,+2 +,+. +,2. +,.4 +,22 +,-0 +,-0 +,3- /,-/<@&67*8'* +,/. +,// /,2/ +,-; +,+2 +,+. +,.2 +,-; +,20 +,/; +,/; +,2. 4,++*5#A5?B +,-0 +,/3 -,14 +,-2 +,+2 +,+4 +,24 +,-. +,2/ +,/2 +,/2 +,31 ;,3.*5#A5?C !"## !"#& /,-3 !"%+ !"!* !"'$ +,.2 +,-4 !"*! !"#+ !"#+ +,2- ;,3.=D)@67*8'* +,/- +,/; /,-+ !"%+ !"!* +,+4 +,.2 +,-4 +,20 +,/4 +,/4 +,2- 4,+;&?EA5 +,/. !"#& /,-+ !"%+ !"!* +,3- +,.2 +,-4 +,20 +,/4 +,/4 +,2. ;,;-&FG67*8'* +,/- !"#& #"%& !"%+ !"!* +,+4 !"** +,-4 +,20 !"#+ !"#+ +,2- ;,00
="5H
IJK)"5L*=E MNJFO(#?P?=(=?7"## KQR(J"5H MNKSB CNDM6N CH"=%
RNMNTJ("EU RNMNTJ(="5HRNMNTJ(@)?= KNVOB K?8VTK,-./012345670888((66((((((R?)=A7%(%&'8A)?E()*(WOKM(R?)=A7%RBMJ(2+3+X(F#&%()D*('"%?#A5?(8?)=A7%(!CSNY("5E(WOKM$,((K7*=?%(L*=(Z"##Z(!?5[=?(\RM3+()?%)%?)$("5E(Z%&'Z(!%&'%?)(*L()@?(@&8"5#]("%%?%%?E(E")"$("=?(%@*D5, SJ^C- SJ9C-
WOKMCSNY(G3."
RM6W_I RM68W_I C"E`?=(L&## C"E`?=(#A)? BMN_(2,3
I=E*7 YS_@O(#?P?=(CSNY
K?8VTK(CSNY I_Y6S:a
MNKSB(R
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./01 ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 %&'% %&'* ./2.+"-'56789 ./00 ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* ./1. ./1; %&'* ./2.+-&*<=*+,-', ./>. ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 ./1; %&'* ./2.+-&*?@A*+,-', %&(( ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* %&-/ %&-) ./2> ./2.+-&*%B"BCD95 ./24 ./2: ./22 ./22 ./21 ./1; ./20 ./2: ./24 ./E; %&/( %&.( %&-. ./20 ./1:+&*F9-") ./E4 ./2E ./22 ./24 ./2. ./10 ./2E ./21 ./1: ./E1 %&/' %&,. %&.( ./23 ./117+&*+,-', ./>E %&+/ %&'( %&') %&'* %&'- %&') ./0E %&'- %&.. %&/* %&-. %&'% %&'* %&'/7G6 ./1E ./20 ./23 ./2E ./24 ./10 ./23 ./21 ./1; ./E2 ./4> ./3E ./1. ./22 ./1089)9H" ./E> ./22 ./2E ./1; ./1; ./11 ./2. ./24 ./10 ./E3 ./41 ./E> ./3> ./23 ./11?&6+,)8 ./24 ./20 ./22 ./22 ./21 ./2. ./20 ./2: ./24 ./E: ./4> ./3E ./11 ./22 ./1>I?&*+,-', ./02 ./0. %&'( ./2: %&'* ./23 ./2: ./04 ./23 ./3E %&/* ./1. ./1: %&'* ./2.I?& ./2; ./0. %&'( ./2> ./20 ./2E ./2: ./0. ./23 ./34 %&/* ./3: ./1> ./2> ./1;#68 ./23 ./2; ./20 ./22 ./22 ./24 ./20 ./2; ./2E ./3. ./4> ./30 ./12 ./20 ./1:#6-%6 ./02 ./2; ./20 ./2> ./20 ./2E ./2> ./0. ./23 ./34 ./4> ./3: ./1> ./2> ./1:#6&-*+,-', ./01 ./0. %&'( ./2: ./2> ./23 ./2: ./04 ./23 %&.. %&/* ./14 ./1; ./2> ./1;#6&- ./>4 ./0. %&'( ./2: ./2> ./23 %&') ./04 %&'- ./3E %&/* ./14 ./1; %&'* ./2.)5+ ./00 ./0. %&'( ./2> ./20 ./2E ./2: ./04 ./23 ./3E %&/* ./3; ./1: ./2> ./1;,)#6)9J ./2E ./2; ./20 ./22 ./22 ./2. ./20 ./2: ./24 ./3. ./4> ./30 ./11 ./21 ./10,)#6)9K ./>4 ./0. %&'( ./2: %&'* ./23 ./2: ./04 %&'- ./3E %&/* ./1E ./1; ./2> ./1;5"#6 ./02 ./0. ./20 ./2> ./20 ./2E ./2: ./04 %&'- ./3E %&/* ./1. ./1: ./2> ./1;5LB?*+,-', %&(( ./0. %&'( ./2; %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* ./2.5LB? ./22 ./2; ./20 ./2> ./20 ./2E ./2: ./0. ./23 ./34 ./4> ./3: ./10 %&'( ./1;&976) ./02 ./0. ./20 ./2> ./2> ./23 ./2: ./04 %&'- ./3E %&/* ./1. ./1; %&'* ./1;&AH*+,-', ./0: ./0. %&'( ./2: %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* %&'/!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', %&'* ./0. 2/:. ./24 %&%. ./.E %&.( ./2. %&.- %&+/ ./3. (&*/+"-'56789 ./2> ./0. 2/:. ./24 %&%. ./.2 %&.( ./2. ./33 %&+/ ./3. >/02+-&*<=*+,-', %&'* ./0. 2/:4 %&', %&%. ./.3 %&.( %&'/ ./31 %&+/ ./3. >/0;+-&*?@A*+,-', %&'* ./0. 2/:1 %&', %&%. ./.3 %&.( %&'/ %&.- %&+/ ./3. >/0;+-&*%B"BCD95 ./22 ./2> 2/20 ./1: %&%. ./.3 ./31 ./1; ./34 ./2: ./E2 >/40+&*F9-") ./24 ./21 2/3. ./1. ./.E ./.E ./E> ./1. ./E> ./22 ./4; 0/4E7+&*+,-', %&'* %&+/ 2/:> ./2E %&%. ./.3 %&.( %&'/ %&.- %&+/ %&./ >/>:7G6 ./24 ./20 2/10 ./10 ./.E ./.1 ./34 ./12 ./E; ./22 ./4; 0/.489)9H" ./1; ./22 2/E; ./12 ./.E ./.2 ./E; ./13 ./E> ./21 ./4> 2/0>?&6+,)8 ./22 ./2: 2/0. ./1: %&%. ./.3 ./33 ./1> ./34 ./2> ./E1 0/;;I?&*+,-', ./2> ./0. 2/>> ./24 %&%. ./.4 ./30 ./2. ./33 ./0. ./E; >/04I?& ./2> ./2; 2/>1 ./2. %&%. ./.: ./32 ./2. ./3E ./0. ./E: >/11#68 ./22 ./2: 2/0E ./1: %&%. ./.2 ./31 ./1; ./34 ./2: ./E0 >/43#6-%6 ./20 ./2; 2/>E ./24 %&%. ./.2 ./30 ./2. ./33 ./2; ./E> >/34#6&-*+,-', ./2> ./0. 2/>> ./24 %&%. ./.3 %&.( %&'/ ./33 ./0. ./E; >/>.#6&- %&'* %&+/ '&** %&', %&%. ./.0 %&.( %&'/ %&.- %&+/ ./E; >/2;)5+ ./2> ./0. 2/>; ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E; >/23,)#6)9J ./22 ./2: 2/21 ./2. %&%. ./.: ./33 ./1: ./34 ./2: ./E2 >/E;,)#6)9K %&'* ./0. 2/:. %&', %&%. %&// %&.( %&'/ %&.- %&+/ ./E; >/>35"#6 ./2> ./0. 2/:. %&', %&%. ./.0 ./30 ./2. ./33 ./0. ./E: >/115LB?*+,-', %&'* %&+/ 2/:> %&', %&%. ./.4 %&.( %&'/ %&.- %&+/ ./3. >/>35LB? ./20 ./2; 2/>3 ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E: >/3;&976) ./2> ./0. 2/:E ./24 %&%. ./.1 ./30 ./2. ./33 %&+/ ./E: >/11&AH*+,-', %&'* %&+/ '&** %&', %&%. ./.3 %&.( ./2. %&.- %&+/ ./3. >/01
0123456738#9:5;(((**(((((M9B56+%(%&'-6N97(B,(OPQR(M9B56+%MJRS(E.4.T(A#&%(BL,('"%9#6)9(-9B56+%(!KUVW(")7(OPQR$/((Q+,59%(D,5(X"##X(!9)Y59(ZMR4.(B9%B%9B$(")7(X%&'X(!%&'%9B(,D(B?9(?&-")#@("%%9%%97(7"B"$("59(%?,L)/5")[ MR*O\] MR*-O\] K"7895(D&## K"7895(#6B9 ]\W*U=^ US_K1 US<K1Q9-`aQ Q9-`aQ(KUVW
P(#9N95(KUVW P(#9N95(59+"## QbM(S")[ RVQUJ(M RVQUJ QB")D,57
MVRVaS("7c MVRVaS(?B95 MVRVaS(5")[ QV`PJJRV\(E/4
KUVW(43" OPQRRVSA ]S ]57,+ WU\? KVLR*V K["5%
47
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-. +,/- +,/0 +,/+ +,-1 +,-0 +,/+ +,/. +,-2 +,./ +,3/ +,./ +,0/ +,-4 +,02''567*8'* !"## !"$% +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. !"'( +,39 +,02 +,-2 !"$& !"&)78&6:;67*8'* +,90 +,/4 +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& !"&)78&6<=>67*8'* +,2/ +,/4 +,// +,/- +,/0 +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-378& +,-2 +,/- +,/3 +,/+ +,/+ +,-- +,/3 +,/0 +,-9 +,.2 +,32 +,0+ +,04 +,/3 +,047&6?@8"5 +,02 +,/3 +,/0 +,-9 +,-9 +,-. +,-4 +,/+ +,-/ +,.0 +,3- +,.+ +,0. +,/+ +,02ABC +,/+ +,/- +,/. +,-1 +,-4 +,-- +,/3 +,/. +,-9 +,.- +,32 +,.1 +,0/ +,/+ +,02DE +,22 +,/4 +,/- +,/3 +,/3 +,-1 +,// +,/9 +,/+ +,.9 +,32 +,03 +,-. +,/. +,01<&C7*5F +,-9 +,/0 +,/3 +,-1 +,-4 +,-0 +,/+ +,/. +,-9 +,./ +,3/ +,./ +,02 +,/+ +,02G<&67*8'* +,29 +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3G<& +,/0 +,// +,/. +,/. +,/3 +,-/ +,/3 +,/- +,-9 +,.9 +,32 +,.1 +,09 +,-1 +,09EC) +,9. +,/4 +,/- +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-+E*767*8'* +,/1 +,/2 +,/- +,/0 +,/. +,-4 +,/0 +,/2 +,-1 +,.4 +,32 +,03 +,-3 +,/. +,-+E*7 +,.0 +,-1 +,-9 +,-/ +,-/ +,09 +,-0 +,-/ +,-3 +,.3 +,30 +,.3 +,.4 +,-1 +,0/#C8%C +,/. +,/9 +,/- +,/. +,/3 +,-4 +,/- +,/2 +,/+ +,.4 +,32 +,0. +,-3 +,/. +,01#C& +,/1 +,/9 +,/- +,/3 +,/+ +,-9 +,/0 +,// +,-1 +,.9 +,32 +,0. +,-3 +,/. +,01*5#C5@H +,20 +,/9 +,/- +,/. +,/3 +,-4 +,/0 +,// +,-1 +,.9 +,32 +,03 +,-+ +,-1 +,02*5#C5@I +,90 !"$% !"$* !"$$ !"$& !"$) +,/9 !"*! !"$' !"'( !"(+ !"'# !"&# !"$& +,-3JK)<67*8'* +,9/ +,/4 +,// +,/- !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& +,-3JK)< +,2. +,/4 +,// +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-3&@AC5 +,22 +,/4 +,/- +,/0 +,/. +,/+ +,// +,/9 +,/3 +,.1 +,32 +,00 +,-0 +,/0 +,01&8A +,24 +,/4 +,// +,/0 +,/. +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,00 +,-- +,/0 +,-+&>>%"#" +,/3 +,// +,/0 +,/+ +,/+ +,-/ +,/3 +,/- +,-4 +,.2 +,3/ +,.4 +,04 +,/3 +,04&>L67*8'* +,2- +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3&&68% +,// +,// +,/. +,/+ +,-1 +,-/ +,/3 +,/0 +,-9 +,.2 +,3/ +,.1 +,09 +,/3 +,04!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,/+ +,/. /,+3 +,-3 +,+. +,+3 +,.1 +,-. +,./ +,/0 +,31 2,04''567*8'* +,// +,/9 /,-2 +,-4 !"!' +,+- +,0/ +,-9 +,0+ +,/4 !")* #")+78&6:;67*8'* +,// +,/9 /,-9 +,-4 !"!' +,+0 +,0- +,-2 +,0+ +,/4 !")* 9,3178&6<=>67*8'* +,/- +,/9 /,-2 +,-9 !"!' +,+- +,00 +,-2 +,.1 +,/4 +,./ 9,+178& +,/+ +,/0 /,+4 +,-. +,+. +,+0 +,0+ +,-- +,.2 +,/. +,.3 2,-97&6?@8"5 +,-4 +,/3 -,14 +,09 +,+. +,+. +,.2 +,01 +,.0 +,/3 +,32 /,93ABC +,-1 +,/0 /,3. +,-/ +,+. +,+2 +,03 +,-/ +,.4 +,/0 +,39 /,1/DE +,/- +,/9 /,-0 +,-9 !"!' +,+. +,00 +,-2 +,.4 +,/4 +,.. 2,29<&C7*5F +,/+ +,/0 /,33 +,-. +,+. +,+. +,0+ +,-0 +,.2 +,/. +,31 2,30G<&67*8'* +,/- +,/9 /,-0 +,-9 !"!' +,+. +,0- +,-2 +,.1 +,/4 +,./ 9,30G<& +,/3 +,/0 /,3. +,-. +,+. +,+/ +,03 +,-- +,.9 +,/- +,.3 2,90EC) +,/- +,/4 /,/3 +,-4 !"!' +,+/ +,0- +,-9 +,0+ +,/4 +,.- 2,13E*767*8'* +,/. +,// /,03 +,-/ +,+. +,+. +,00 +,-/ +,.4 +,/2 +,.- 2,44E*7 +,-0 +,-4 -,9. +,02 +,+. +,+3 +,.9 +,-+ +,.- +,-9 +,32 /,01#C8%C +,/0 +,/2 /,0/ +,-9 +,+. +,+- +,0. +,-2 +,.4 +,/9 +,.0 2,90#C& +,/0 +,/2 /,0- +,-2 +,+. +,+0 +,0. +,-2 +,.4 +,/2 +,.. 2,2+*5#C5@H +,/0 +,// /,.- +,-2 +,+. +,3+ +,00 +,-2 +,.1 +,/2 +,.3 2,44*5#C5@I !"$* !"*! $"#( !"$) !"!' !"(# !"'* !"&% !"') !"*( !")* 9,34JK)<67*8'* +,// +,/4 /,/+ +,-4 !"!' +,+. +,0/ +,-9 +,0+ +,/1 !")* 9,.+JK)< +,/- +,/9 /,-- +,-9 !"!' +,+0 +,00 +,-2 +,.1 +,/4 +,.- 2,12&@AC5 +,/- +,/4 /,/0 +,-1 !"!' +,+2 +,0- +,-4 +,0+ +,/4 +,.0 2,4+&8A +,// +,/4 /,/+ +,-4 !"!' +,+9 +,00 +,-9 +,.1 +,/4 +,.0 2,99&>>%"#" +,/3 +,/- /,.3 +,-0 +,+. +,+. +,03 +,-- +,.9 +,/- +,.3 2,/3&>L67*8'* +,/- +,/9 /,-/ +,-4 !"!' +,+0 +,0- +,-/ +,.1 +,/4 +,./ 9,3/&&68% +,/+ +,/- /,34 +,-- +,+. +,+. +,03 +,-/ +,.9 +,/- +,.+ 2,0.
,-./0123145678(((((66(((((M@)JC7%(%&'8CN@A()*(OPQR(M@)JC7%MHRS(.+3+T(>#&%()K*('"%@#C5@(8@)JC7%(!IUVW("5A(OPQR$,((Q7*J@%(X*J(Y"##Y(!@5ZJ@([MR3+()@%)%@)$("5A(Y%&'Y(!%&'%@)(*X()<@(<&8"5#=("%%@%%@A(A")"$("J@(%<*K5,J"5E MR6O\] MR68O\] I"AF@J(X&## I"AF@J(#C)@ ]\W6U;^ US_I- US:I-Q@8`aQ Q@8`aQ(IUVW
P(#@N@J(IUVW P(#@N@J(J@7"## QbM(S"5E RVQUH(M RVQUH Q)"5X*JA
MVRVaS("Ac MVRVaS(<)@J MVRVaS(J"5E QV`PHHRV\(.,3
IUVW(30" OPQRRVS> ]S ]JA*7 WU\< IVKR6V IE"J%
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./01 ./02 ./34 ./35 ./35 ./36 ./0. ./07 ./33 ./76 ./24 ./67 ./37 ./35 !"#$+"-'89:;< ./02 ./0. ./34 ./34 ./3= ./37 ./35 ./01 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+-&*>?*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+,#&-'9" ./63 ./34 ./30 ./33 ./36 ./3. ./30 ./35 ./31 ./7. ./2= ./73 ./6= ./3= ./64+&*@<-") ./1= ./33 ./34 ./31 ./31 ./64 ./36 ./30 ./3. ./1= ./23 ./13 ./62 ./33 ./63:A9 ./63 ./34 ./33 ./36 ./37 ./65 ./30 ./34 ./32 ./14 ./24 ./73 ./60 ./30 ./6=B&9+,); ./3. ./3= ./30 ./30 ./33 ./32 ./34 ./02 ./37 ./72 ./2= ./73 ./6= ./3= ./64CB&*+,-', ./33 ./0. ./34 ./34 ./34 ./37 ./35 ./01 ./36 ./77 ./24 ./6. ./32 ./34 ./3.CB& ./02 ./0. ./3= ./3= ./3= ./37 ./35 ./01 ./36 ./77 ./24 ./75 ./3. ./34 ./65,)#9)"D ./36 ./0. ./3= ./3= ./3= ./37 ./34 ./02 ./36 ./71 ./2= ./75 ./64 ./33 ./60,)#9)<E !"%! !"&' !"#( !"&) !"&$ !"#& !"&) !"&# !"#% !"'& !")! !"*% !"## ./35 ./3.&<:9) ./05 ./02 ./3= ./34 ./34 ./36 ./0. ./07 ./33 ./77 ./24 ./61 ./31 ./35 !"#$&F+ ./32 ./35 ./30 ./33 ./33 ./32 ./34 ./0. ./37 ./72 ./2= ./7= ./64 ./34 ./65&FG*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./61 ./37 !"&! !"#$!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./35 ./02 3/51 ./31 !"!' ./.6 ./74 ./3. ./76 ./01 ./71 =/56+"-'89:;< ./34 ./0. 3/43 ./32 !"!' ./.= ./7= ./3. ./77 ./02 ./72 =/==+-&*>?*+,-', ./35 ./02 3/5. ./31 !"!' ./.6 ./7= ./3. ./76 ./01 ./72 =/4=+,#&-'9" ./33 ./3= 3/37 ./6= ./.1 ./.6 ./76 ./60 ./7. ./3= ./1= =/25+&*@<-") ./36 ./30 3/60 ./61 ./.1 ./.7 ./14 ./62 ./1= ./30 ./11 0/03:A9 ./36 ./34 3/07 ./3. ./.1 ./.4 ./76 ./6= ./72 ./34 ./17 0/0.B&9+,); ./3= ./35 3/=7 ./65 !"!' ./.3 ./76 ./64 ./72 ./35 ./10 =/7=CB&*+,-', ./34 ./0. 3/47 ./32 !"!' ./.2 ./7= ./3. ./77 ./02 ./7. =/42CB& ./34 ./0. 3/47 ./32 !"!' ./.4 ./70 ./65 ./77 ./02 ./15 =/=1,)#9)"D ./3= ./35 3/=. ./31 !"!' ./22 ./70 ./3. ./71 ./02 ./14 =/==,)#9)<E !"&$ !"&' &"!' !"#& !"!' !")# !"*! !"#' !"'& !"&* !"'' +"'(&<:9) ./35 ./01 3/5= ./36 !"!' ./21 ./74 ./32 ./76 ./01 ./7. =/04&F+ ./30 ./0. 3/=3 ./3. !"!' ./.7 ./73 ./64 ./71 ./0. ./10 =/13&FG*+,-', ./35 ./01 3/5= ./37 !"!' ./.6 ./74 ./3. ./76 ./01 ./72 =/41
,-./01234/560127((((**(((((H<I89+%(%&'-9J<:(I,(KLMN(H<I89+%HDNO(1.2.P(F#&%(IQ,('"%<#9)<(-<I89+%(!ERST("):(KLMN$/((M+,8<%(U,8(V"##V(!<)W8<(XHN2.(I<%I%<I$("):(V%&'V(!%&'%<I(,U(IB<(B&-")#Y("%%<%%<:(:"I"$("8<(%B,Q)/8")Z HN*K[\ HN*-K[\ E":;<8(U&## E":;<8(#9I< \[T*R?] RO^E6 RO>E6M<-_`M M<-_`M(ERST
L(#<J<8(ERST L(#<J<8(8<+"## MaH(O")Z NSMRD(H NSMRD MI")U,8:
HSNS`O(":b HSNS`O(BI<8 HSNS`O(8")Z MS_LDDNS[(1/2
ERST(27" KLMNNSOF \O \8:,+ TR[B ESQN*S EZ"8%
48
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. /012 /034 /035 /03/ /056 /076 !"#$ /076 /052 /038 /055)&+'.9": /011 /036 /035 /052 /051 /071 /057 /071 /056 /03/ /057)&+;<);. /01/ /036 /037 /051 /057 /075 /05/ /077 /05= /054 /058)&+><*"? /03/ /033 /03= /051 /057 /078 /072 /075 /05= /051 /072@)&+).*'. !"%$ !"$& !"$$ !"$' !"#( !")& !"#$ !")( !"$' !"$* !"#+@)& /074 /035 /038 /051 /057 /07= /072 /077 /058 /052 /057<&:.;:"?% /035 /035 /038 /057 /058 /0=2 /076 /078 /074 /054 /05=A.)+).*'. /011 /036 /035 /052 /051 /076 /057 /076 /056 /038 /053A.) /077 /03= /052 /055 /058 /0=2 /071 /077 /072 /052 /05=.?#B?<C /055 /033 /038 /053 /058 /078 /074 /077 /058 /054 /05=.?#B?<D /06/ /034 /035 /052 /051 /076 /055 /076 /054 /038 /053E)+;:"?% /01= /033 /038 /055 /058 /078 /076 /0=2 /071 /052 /05=E.;%@"* /055 /033 /038 /053 /05= /078 /074 /075 /05= /052 /05=:F;G+).*'. /06/ /036 /037 /03/ /056 /074 /055 /076 /052 /038 /053%H& /053 /033 /03= /055 /058 /078 /074 /077 /058 /052 /057&<@B? /01= /036 /037 /054 /053 /071 /05= /076 /056 /03/ /055&EI+).*'. /014 /034 /033 /03/ /056 /074 !"#$ /076 /052 /038 /053!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. !"#+ /056 504/ !"$$ +/0/= /0/3 /037 /086 301=)&+'.9": /053 /051 506/ /037 ,!"!' /0/1 /038 /081 3056)&+;<);. /057 /053 501= /037 ,!"!' /0/2 /03/ /087 30=3)&+><*"? /058 /05= 5071 /03/ ,!"!' /0/3 /056 /08= 5021@)&+).*'. /056 !"#( #"&! !"$$ ,!"!' !"'# !"$# !"'& $"&+@)& /05/ /058 50=3 /056 ,!"!' /0/3 /051 /087 5023<&:.;:"?% /05/ /05= 505= /038 ,!"!' /0/5 /051 /08/ 5055A.)+).*'. /053 /051 506/ /037 ,!"!' /0/= /038 /086 3034A.) /074 /05/ 50=5 /057 ,!"!' /0/7 /055 /08= 5037.?#B?<C /058 /057 5054 /038 ,!"!' /0/1 /056 /08= 5013.?#B?<D !"#+ !"#( 5046 /035 ,!"!' /08/ /037 /086 3031E)+;:"?% /058 /057 503= /03/ ,!"!' /0/3 /056 /08/ 5031E.;%@"* /05/ /05= 5074 /052 ,!"!' /0/7 /051 /08= 5065:F;G+).*'. !"#+ /056 5048 !"$$ ,!"!' /0/5 /038 /084 304=%H& /058 /057 5055 /03/ ,!"!' /0/5 /056 /088 5014&<@B? /053 /051 5067 /035 ,!"!' /088 /03= /081 3057&EI+).*'. !"#+ !"#( 5045 /035 ,!"!' /0/1 /037 /084 3044
:"?A
JKL;"?H.:@ MNKEO(#<P<:(:<)"## LQR(K"?A MNLSC DNFM+N DA":%
RNMNTK("@U RNMNTK(:"?ARNMNTK(G;<: LNVOC L<*VTL-./0123,45673888((++((((((R<;:B)%(%&'*B;<@(;.(WOLM(R<;:B)%RCMK(=/8/X(E#&%(;F.('"%<#B?<(*<;:B)%(!DSNY("?@(WOLM$0((L).:<%(H.:(Z"##Z(!<?[:<(\RM8/(;<%;%<;$("?@(Z%&'Z(!%&'%<;(.H(;G<(G&*"?#]("%%<%%<@(@";"$(":<(%G.F?0 SK^D5 SK,D5
WOLMDSNY(I87"
RM+W_J RM+*W_J D"@`<:(H&## D"@`<:(#B;< CMN_(=08
J:@.) YS_GO(#<P<:(DSNY
L<*VTL(DSNY J_Y+S-a
MNLSC(R
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0123 0145 0146 !"#$ 0144 0164 0167 0148 0140)*&9:;9)<*'< 0156 0148 !"## !"#$ !"#% !"&$ !"#' !"%! !"#()&9=/*"> 01?2 014@ 0143 0165 0162 0138 0163 0146 0163-A, 0160 0146 014@ 0140 0167 0137 0166 0144 0162/& 013? 0143 014@ 0168 0165 0138 0166 0142 0162./>/B" 0134 0144 014@ 0167 0168 0135 016? 0146 0166CD& 0143 0145 0143 0146 0143 0166 0167 0148 0168E<)9)<*'< 0128 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(E<) 01?2 0143 014@ 0167 0168 0135 0163 0144 0162#,*%, 0122 0148 0146 0144 0146 0164 0167 0147 0140#,&* 0123 0148 0146 0142 0144 0162 !"#' !"%! 014@>+) 012? 0145 0146 0146 0143 0164 0167 0148 0167<>#,>/F 0144 0145 0143 0146 0143 0166 0168 0148 0167<>#,>/G 0128 0148 0146 !"#$ 0144 0162 !"#' !"%! 014@+"#, 0122 0148 0146 0142 0144 0162 0140 !"%! 014@+HID9)<*'< !"$# !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(+HID 0123 0148 0146 0144 0146 0164 0140 0147 014@&/-,> 0150 0148 0146 0142 0144 0164 0140 !"%! 014@&JB9)<*'< 0122 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0144 0145 4143 014? 9010? 0102 0148 01?8 5165)*&9:;9)<*'< !"#$ 0147 4156 0146 *!"!' 0103 !"%! !"+! 514?)&9=/*"> 0140 014? 4103 0160 90103 0103 014? 01@2 4163-A, 014@ 0146 41?7 0162 9010? 0102 0146 01?0 21@2/& 0140 0146 41?@ 0162 9010? 0103 0143 01@7 4176./>/B" 0140 014? 41@0 0168 90103 0104 014? 01@8 2106CD& 0144 0145 414@ 0140 9010? 0104 0145 01?6 2124E<)9)<*'< !"#$ 0147 4152 0146 *!"!' 0100 !"%! 01?7 5143E<) 0167 014@ 4102 0164 9010? 010? 014? 01?0 21@6#,*%, 0142 0148 4125 0146 9010? 0102 0148 01?5 51?0#,&* !"#$ 0147 4152 0146 *!"!' 0105 0147 01?7 5138>+) 0142 0148 412@ 014? *!"!' 0102 0148 01?5 51??<>#,>/F 0144 0145 4148 014? 9010? 0105 0148 01?4 2177<>#,>/G !"#$ !"%! #",' 0146 9010? !"'& !"%! 01?8 513?+"#, !"#$ 0147 4153 0146 *!"!' 0102 0147 01?8 513?+HID9)<*'< !"#$ 0147 4157 !"## *!"!' 0106 !"%! !"+! 512?+HID 0142 0148 4127 0143 *!"!' 0102 0147 01?8 51?8&/-,> 0142 0148 4150 0146 *!"!' 0107 0147 01?8 51?6&JB9)<*'< !"#$ 0147 4158 !"## *!"!' 0104 !"%! !"+! $"%#
-./0123*456.738(((99(((((K/I+,)%(%&'*,L/-(I<(MNOP(K/I+,)%KFPQ(?0@0R(J#&%(IH<('"%/#,>/(*/I+,)%(!GSTU(">-(MNOP$1((O)<+/%(V<+(W"##W(!/>X+/(YKP@0(I/%I%/I$(">-(W%&'W(!%&'%/I(<V(ID/(D&*">#Z("%%/%%/-(-"I"$("+/(%D<H>1+">E KP9M[\ KP9*M[\ G"-./+(V&## G"-./+(#,I/ \[U9S;] SQ^G6 SQ:G6O/*_`O O/*_`O(GSTU
N(#/L/+(GSTU N(#/L/+(+/)"## OaK(Q">E PTOSF(K PTOSF OI">V<+-
KTPT`Q("-b KTPT`Q(DI/+ KTPT`Q(+">E OT_NFFPT[(?1@
GSTU(@3" MNOPPTQJ \Q \+-<) US[D GTHP9T GE"+%
49
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. /012 /011 /034 /034 /035 /064 /074 !"#$ !"%&)&+89*": /07; /035 !"&' /037 /03< /06/ /077 /036 /06<=>? /057 /016 /035 /033 /033 /063 /073 /031 /066@A /015 /011 /034 /031 /031 /065 /075 /031 /063BC& /035 /016 /034 /035 /031 /063 /071 /036 /06<A?D /05/ /011 /034 /034 /035 /065 /075 /031 /066A.)+).*'. /012 /011 /034 /034 /035 /064 /075 /035 !"%&A.) /06/ /01/ /031 /037 /037 /06< /077 /033 /067#?*%? /01< /013 /035 /031 /033 /061 /071 /031 /066#?& /011 /013 /034 /035 /031 /061 /071 /033 /066.:#?:9E /012 /013 /035 /035 /031 /061 /071 /033 /067.:#?:9F !"$! !"&( /03; !"#) !"#$ !"%* !"') !"#$ !"%&GHDC+).*'. /051 /011 /034 /034 /035 /064 /074 /035 !"%&GHDC /01< /013 /034 /035 /031 /061 /075 /031 /063%I& /063 /01< /031 /033 /036 /072 /07< /062 /07;&9=?: /057 /011 /034 /035 /031 /065 /075 /035 /063&JJ%"#" /034 /013 /034 /031 /033 /061 /071 /031 /063&JK+).*'. /011 /011 /034 /034 !"#$ /064 /074 /031 /063!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+C9"L9#=+).*'. /034 /03; 30;5 /031 /0/6 /0/1 /016 /0<; 102;)&+89*": /03< /037 3067 /066 /0/6 /0/6 /034 /0<7 10/6=>? /031 /034 3042 /035 !"!# !"+' /01/ /0<3 107;@A /035 /034 3045 /031 /0/6 /0/6 /017 /0<5 10;1BC& /033 /031 3017 /037 /0/6 /0/4 /01/ /0<1 1042A?D /034 /03; 30;6 /031 /0/6 /0/5 /016 /0<4 102;A.)+).*'. /035 /034 3044 /031 /0/6 /0// /016 /0<4 102;A.) /03< /037 303/ /062 /0/6 /0/7 /035 /0<3 10<2#?*%? /031 /035 3055 /036 /0/6 /0/1 /01< /0<5 104/#?& /035 /03; 3045 /031 /0/6 /0/3 /017 /0<5 10;1.:#?:9E /035 /03; 30;7 /035 !"!# /0/2 /017 /0<1 1051.:#?:9F !"#* !"&! &"!& !"#) !"!# /07/ !"&& !"+* ("+)GHDC+).*'. /035 /03; 3044 /031 /0/6 /0/6 /016 /0<4 50/5GHDC /031 /034 304< /033 /0/6 /0/1 /017 /0<5 10;5%I& /06; /062 6023 /06; /0/6 /0/6 /031 /0<< 30;7&9=?: /034 /03; 30;1 /035 /0/6 /0/; /016 /0<4 1023&JJ%"#" /031 /035 3051 /036 /0/6 /0/6 /01< /0<5 1043&JK+).*'. /035 /034 3041 /033 /0/6 /0/6 /016 /0<4 50</
,-./012345678-(((((++(((((M9DG?)%(%&'*?N9=(D.(OPQR(M9DG?)%MERS(7/</T(J#&%(DH.('"%9#?:9(*9DG?)%(!FUVW(":=(OPQR$0((Q).G9%(I.G(X"##X(!9:YG9(ZMR</(D9%D%9D$(":=(X%&'X(!%&'%9D(.I(DC9(C&*":#[("%%9%%9=(="D"$("G9(%C.H:0G":A MR+O\] MR+*O\] F"=^9G(I&## F"=^9G(#?D9 ]\W+U-_ US`F3 US,F3Q9*abQ Q9*abQ(FUVW
P(#9N9G(FUVW P(#9N9G(G9)"## QcM(S":A RVQUE(M RVQUE QD":I.G=
MVRVbS("=d MVRVbS(CD9G MVRVbS(G":A QVaPEERV\(70<
FUVW(<6" OPQRRVSJ ]S ]G=.) WU\C FVHR+V FA"G%
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0123 0124 0125 0126 0122 0132 0172 0126 0120)*&89:8);*'; 015< 0150 !"#$ 0124 012< 013< !"%& 012< 0127)&8=/*"> 017? 0122 !"#$ 012@ 0120 013@ 0177 0127 013?-)& 015@ 0150 !"#$ 012< 0126 0136 0172 012< 012@-A, 0122 0126 0123 0123 0127 0137 017? 0123 0136BC& 0122 0124 0125 0126 0122 0132 0172 0125 013<D;)8);*'; 0157 0150 !"#$ 012< 0126 0136 !"%& 012< 012@D;) 01@4 0123 012? 012@ 0134 01?< 017@ 012? 0135;>#,>/E 0154 0150 !"#$ 012< 0126 013< 0172 012< 012@;>#,>/F !"$' !"&' !"#$ !"&! !"#( !")( !"%& !"&! !"#*+GHC8);*'; 0124 0124 0125 0124 012< 013< !"%& 012< 012@%I& 0130 012< 0122 0122 0123 0137 0173 012? 0132&/-,> 015@ 0150 0125 012< 0126 0136 0172 012< 012@&J'8);*'; 0150 0150 !"#$ 0124 012< 013< !"%& 012< 012@&JK8>>#* 0123 0124 0125 0125 0122 0132 0173 0126 0134&JK 0122 0124 0125 0126 0125 0135 0172 0126 0120!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0126 0124 2162 0132 !"!' 0100 0150 0174 6120)*&89:8);*'; 0124 015@ 2140 013< !"!' 0100 0157 !"*% 6145)&8=/*"> 0123 0125 213@ 0130 0100 0100 0125 017@ 5123-)& 0124 015@ 2140 0136 !"!' 0100 015@ 01?0 6167-A, 0123 0126 2122 0137 0100 0100 0126 0173 51<?BC& 0126 0124 2167 0132 !"!' 0100 0150 017< 612<D;)8);*'; 012< 0150 21<3 0135 !"!' 0100 015@ 01?0 6164D;) 012@ 012? 21@4 01?5 0100 0100 012? 017@ 51?6;>#,>/E 0124 0157 2146 013< !"!' 0100 0157 01?0 6152;>#,>/F !"&! !"&* &"'% !")( !"!' 0100 0153 !"*% 61<6+GHC8);*'; 0124 015@ 21<4 0136 !"!' 0100 0150 01?@ $"($%I& 0123 0122 21?2 0137 0100 0100 0125 0173 61@<&/-,> 012< 015@ 21<5 0136 !"!' !"!' 015@ 01?0 6155&J'8);*'; 012< 0150 21<< 0136 !"!' 0100 015@ 01?@ 614?&JK8>>#* 0126 0124 216@ 0133 !"!' 0100 0124 017< 61?4&JK 012< 0150 21<0 0132 !"!' 0100 0150 0174 613<
+,-./012345,/016((((88(((((L/H+,)%(%&'*,M/-(H;(NOPQ(L/H+,)%LEQR(70@0S(J#&%(HG;('"%/#,>/(*/H+,)%(!FTUV(">-(NOPQ$1((P);+/%(I;+(W"##W(!/>X+/(YLQ@0(H/%H%/H$(">-(W%&'W(!%&'%/H(;I(HC/(C&*">#Z("%%/%%/-(-"H"$("+/(%C;G>1+">D LQ8N[\ LQ8*N[\ F"-./+(I&## F"-./+(#,H/ \[V8T:] TR^F3 TR9F3P/*_`P P/*_`P(FTUV
O(#/M/+(FTUV O(#/M/+(+/)"## PaL(R">D QUPTE(L QUPTE PH">I;+-
LUQU`R("-b LUQU`R(CH/+ LUQU`R(+">D PU_OEEQU[(71@
FTUV(@?" NOPQQURJ \R \+-;) VT[C FUGQ8U FD"+%
50
R
E
F
A
A
L
T
O
C
M
U
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
B
B
N
-C
C
M
U
-H
E
A
-C
JH
U
-C
R
W
T
H
-C
U
P
V
-C
REF ? .03? .02? .03? .01? .03? .02? .05? .02? .06? .03? .05? .03?
AALTO .93? ? .54? .54? .23? .36 .58? .56? .65? .69? .64? .67? .62?
CMU .94? .30? ? .47 .14? .22? .52? .41 .50? .57? .45? .44 .38
CU-BOJAR .94? .26? .38 ? .10? .22? .61? .47? .46 .55? .42 .49? .44
CU-ZEMAN .98? .58? .73? .77? ? .55? .79? .71? .84? .80? .77? .79? .75?
ONLINEA .94? .41 .61? .57? .23? ? .68? .63? .71? .71? .63? .54? .61?
ONLINEB .93? .30? .31? .26? .10? .17? ? .32? .35 .31 .22? .29? .38
UEDIN .91? .27? .35 .34? .11? .18? .47? ? .54? .50? .35 .29 .35
BBN-C .95? .21? .22? .36 .06? .17? .38 .26? ? .32 .24? .31? .26?
CMU-HEA-C .90? .17? .19? .23? .09? .18? .32 .27? .34 ? .31? .31? .30?
JHU-C .93? .19? .30? .35 .09? .24? .50? .34 .47? .45? ? .41? .36
RWTH-C .91? .16? .35 .29? .12? .27? .41? .37 .42? .42? .23? ? .24?
UPV-C .94? .24? .40 .36 .09? .28? .39 .32 .46? .47? .33 .36? ?
> others .93 .26 .37 .38 .11 .24 .47 .40 .49 .49 .38 .41 .40
>= others .97 .42 .56 .55 .25 .39 .67 .62 .70 .70 .61 .65 .62
Table 21: Sentence-level ranking for the WMT10 Czech-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
51
R
E
F
A
A
L
T
O
C
M
U
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
H
U
IC
O
N
G
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
U
E
D
IN
U
M
D
U
P
P
S
A
L
A
U
U
-M
S
B
B
N
-C
C
M
U
-H
E
A
-C
C
M
U
-H
Y
P
O
-C
JH
U
-C
K
O
C
-C
R
W
T
H
-C
U
P
V
-C
REF ? .00? .02? .00? .07? .04? .03? .00? .06? .04? .00? .02? .07? .07? .07? .02? .09? .03? .03? .10? .04? .04? .03? .02? .07? .06?
AALTO 1.00? ? .43 .39 .48 .60? .38 .41 .74? .18? .42 .57? .50? .63? .55? .68? .79? .42 .33 .71? .61? .66? .54 .51? .66? .56?
CMU .95? .34 ? .19? .45 .52? .38 .50 .63? .17? .51? .55? .56? .66? .55? .60? .56? .30 .40 .62? .64? .49? .58? .46 .64? .46?
CU-ZEMAN 1.00? .44 .64? ? .43 .72? .31 .45? .69? .36 .55 .62? .75? .75? .78? .75? .75? .48? .56? .79? .82? .72? .68? .63? .67? .84?
DFKI .92? .29 .33 .35 ? .37 .40 .34 .59 .08? .42 .50 .49 .64? .35 .44 .44 .50 .41 .70? .61? .57 .46 .47 .62? .44
FBK .93? .26? .23? .17? .49 ? .12? .30 .52? .08? .20? .45? .41 .62? .44 .44 .48? .18? .25? .53? .47 .38 .38 .22? .41 .51?
HUICONG .92? .34 .39 .37 .38 .71? ? .53? .67? .18? .51? .47 .60? .65? .49? .55? .78? .35 .41 .56? .77? .74? .58? .41 .65? .57?
JHU .92? .35 .30 .17? .52 .45 .25? ? .58? .16? .43 .38 .57? .60? .54? .60? .70? .29 .25 .65? .75? .56? .62? .49? .66? .48?
KIT .90? .14? .16? .14? .35 .28? .19? .16? ? .03? .29? .20? .35 .53? .21? .24? .30 .20? .22? .44 .29 .38 .35 .24 .40 .24?
KOC .95? .66? .71? .51 .75? .80? .58? .68? .93? ? .75? .87? .72? .74? .74? .81? .81? .78? .66? .89? .85? .80? .80? .72? .91? .73?
LIMSI .99? .26 .24? .32 .45 .61? .25? .38 .50? .10? ? .50? .55? .69? .52? .57? .57? .29? .22? .60? .52? .42 .47? .37 .60? .56?
LIU .87? .17? .20? .14? .34 .22? .31 .38 .66? .04? .27? ? .51? .53? .52? .53? .51 .20? .33 .64? .59? .48? .48 .51 .37 .53?
ONLINEA .90? .25? .29? .18? .34 .43 .23? .28? .49 .08? .32? .30? ? .44 .38 .40 .42 .32? .35? .39 .47 .51 .27? .35 .43 .40
ONLINEB .76? .22? .24? .14? .27? .27? .25? .25? .32? .22? .21? .28? .32 ? .27? .21? .30? .23? .15? .41 .31 .40 .23? .16? .42 .29
RWTH .89? .22? .23? .13? .49 .35 .29? .21? .62? .15? .32? .29? .46 .57? ? .39 .49 .25 .38 .41 .27 .34 .36 .27 .48? .22?
UEDIN .91? .15? .20? .12? .49 .35 .24? .22? .49? .04? .22? .30? .46 .62? .43 ? .39 .11? .15? .45 .33 .40 .45 .33 .34 .33
UMD .91? .12? .23? .06? .35 .29? .11? .16? .47 .14? .23? .35 .40 .55? .36 .47 ? .16? .17? .44 .29? .27 .37 .26 .27 .24?
UPPSALA .94? .30 .41 .23? .35 .53? .26 .37 .66? .03? .54? .71? .57? .65? .45 .72? .67? ? .25 .59? .69? .49? .63? .33 .60? .64?
UU-MS .83? .28 .42 .24? .41 .49? .28 .42 .68? .10? .55? .48 .55? .63? .49 .56? .60? .32 ? .52? .58? .61? .64? .46? .64? .50?
BBN-C .90? .15? .16? .10? .22? .17? .22? .18? .41 .06? .16? .21? .35 .45 .30 .26 .34 .13? .20? ? .42? .14? .27 .11? .25 .21?
CMU-HEA-C .83? .20? .18? .07? .29? .32 .06? .10? .49 .05? .26? .21? .41 .33 .37 .43 .58? .10? .14? .18? ? .33 .32 .11? .34 .24?
CMU-HYPO-C .96? .24? .20? .07? .37 .33 .12? .21? .40 .10? .41 .26? .40 .54 .25 .37 .44 .13? .17? .49? .31 ? .34 .23? .51? .45
JHU-C .97? .33 .22? .18? .31 .30 .27? .18? .33 .12? .19? .33 .59? .60? .39 .32 .30 .19? .20? .44 .29 .34 ? .21? .36 .23
KOC-C .93? .11? .31 .17? .41 .50? .25 .27? .44 .11? .42 .36 .47 .68? .43 .41 .40 .33 .18? .59? .57? .46? .47? ? .52? .43
RWTH-C .87? .20? .10? .21? .25? .27 .15? .23? .24 .02? .20? .30 .34 .47 .27? .34 .36 .14? .20? .33 .26 .21? .24 .20? ? .17?
UPV-C .93? .14? .20? .10? .42 .29? .25? .25? .57? .20? .22? .33? .39 .45 .47? .40 .50? .24? .28? .44? .42? .27 .34 .28 .56? ?
> others .92 .25 .28 .18 .39 .41 .25 .30 .52 .12 .34 .39 .47 .57 .42 .46 .51 .27 .28 .52 .49 .45 .44 .34 .50 .42
>= others .96 .46 .49 .35 .53 .62 .45 .51 .71 .24 .54 .58 .63 .72 .63 .66 .70 .50 .51 .75 .73 .68 .67 .59 .74 .64
Table 22: Sentence-level ranking for the WMT10 German-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
R
E
F
C
A
M
B
R
ID
G
E
C
O
L
U
M
B
IA
C
U
-Z
E
M
A
N
D
F
K
I
H
U
IC
O
N
G
JH
U
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
U
P
C
B
B
N
-C
C
M
U
-H
E
A
-C
JH
U
-C
U
P
V
-C
REF ? .05? .01? .02? .03? .03? .01? .02? .04? .03? .04? .03? .07? .05? .04?
CAMBRIDGE .90? ? .24? .11? .35? .26? .43 .35 .50? .45? .33? .40 .46 .28? .41
COLUMBIA .97? .61? ? .25? .47 .44 .61? .53? .62? .59? .48? .59? .57? .45? .57?
CU-ZEMAN .92? .73? .59? ? .62? .66? .71? .65? .75? .79? .58? .75? .78? .71? .72?
DFKI .95? .50? .41 .21? ? .46 .56? .52? .65? .62? .47 .52? .56? .52? .60?
HUICONG .93? .57? .34 .21? .36 ? .47? .43 .67? .58? .40 .51? .62? .46? .52?
JHU .94? .39 .22? .16? .30? .32? ? .41 .52? .47? .37 .41 .33? .28 .35
ONLINEA .92? .45 .35? .24? .34? .41 .41 ? .60? .58? .38 .55? .46 .36 .57?
ONLINEB .87? .34? .24? .15? .21? .19? .33? .25? ? .34? .26? .34? .37? .24? .40
UEDIN .94? .33? .26? .12? .24? .22? .25? .25? .50? ? .25? .28? .32? .25? .26
UPC .89? .45? .36? .23? .39 .37 .42 .48 .62? .57? ? .54? .51? .50? .53?
BBN-C .91? .33 .25? .11? .32? .30? .34 .31? .51? .41? .30? ? .36 .26? .31
CMU-HEA-C .89? .37 .20? .10? .29? .23? .23? .35 .50? .44? .31? .34 ? .23? .31
JHU-C .89? .39? .31? .17? .37? .33? .38 .42 .63? .47? .31? .42? .42? ? .37?
UPV-C .91? .35 .30? .16? .29? .26? .32 .28? .44 .35 .27? .27 .30 .24? ?
> others .92 .42 .29 .16 .33 .32 .39 .37 .54 .48 .34 .42 .44 .35 .43
>= others .97 .62 .45 .29 .46 .50 .61 .52 .68 .68 .51 .64 .65 .58 .66
Table 23: Sentence-level ranking for the WMT10 Spanish-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
52
R
E
F
C
A
M
B
R
ID
G
E
C
M
U
-S
T
A
T
X
F
E
R
C
U
-Z
E
M
A
N
D
F
K
I
G
E
N
E
V
A
H
U
IC
O
N
G
JH
U
L
IG
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
B
B
N
-C
C
M
U
-H
E
A
-C
C
M
U
-H
Y
P
O
-C
D
C
U
-C
JH
U
-C
L
IU
M
-C
R
W
T
H
-C
U
P
V
-C
REF ? .02? .00? .00? .00? .00? .05? .02? .00? .00? .00? .02? .06? .02? .04? .02? .04? .03? .02? .05? .05? .04? .05? .06? .02?
CAMBRIDGE .82? ? .42 .16? .12? .35 .31 .45 .21? .47 .29 .38 .28? .54 .43 .33 .38 .28 .39 .45? .24 .25 .34 .54? .37
CMU-STATXFER .91? .50 ? .17? .41 .17? .28 .44 .36 .48? .56? .57? .47 .56? .70? .49 .50 .47 .61? .68? .55? .50 .42 .52? .51?
CU-ZEMAN 1.00? .74? .71? ? .74? .46 .67? .73? .73? .74? .75? .76? .75? .89? .78? .66? .83? .74? .87? .73? .80? .83? .77? .95? .82?
DFKI 1.00? .77? .48 .17? ? .27? .49 .52 .48 .64? .69? .67? .47 .62? .53 .47 .64? .60? .73? .72? .79? .58? .66? .73? .74?
GENEVA .98? .58 .70? .44 .59? ? .55? .67? .70? .70? .77? .73? .63? .81? .81? .69? .77? .73? .62? .66? .75? .60? .73? .88? .67?
HUICONG .89? .53 .34 .13? .34 .30? ? .41 .36 .43 .70? .56? .57 .59? .56? .43 .55? .45 .51? .64? .48 .49 .49 .53? .57?
JHU .88? .36 .38 .11? .34 .25? .35 ? .33? .46 .49? .48 .40 .50 .40 .34 .36 .39 .33 .59? .54? .41 .42 .40 .41
LIG .98? .65? .34 .18? .44 .26? .39 .56? ? .60? .55? .51? .45 .54? .53 .39 .38 .52? .54? .53? .51? .53? .55 .51 .58?
LIMSI .98? .40 .24? .23? .23? .15? .29 .38 .25? ? .28 .38 .27? .64? .35 .30 .41 .27 .33 .49 .45 .37 .28 .45 .39
LIUM .90? .40 .19? .12? .30? .11? .11? .26? .15? .36 ? .36 .25? .37 .39 .26 .29 .24 .34 .49? .34 .33 .34 .31 .38
NRC .93? .31 .06? .15? .29? .23? .20? .32 .16? .38 .36 ? .23? .53 .36 .24? .31 .44 .37 .47? .45? .29 .39 .38 .42
ONLINEA .92? .60? .47 .15? .44 .22? .32 .46 .34 .57? .52? .60? ? .52? .34 .44 .57? .56 .51 .51 .64? .46 .51 .41 .60
ONLINEB .85? .35 .32? .09? .33? .10? .29? .31 .25? .17? .40 .34 .24? ? .38 .32? .28 .39 .30 .42 .37 .41 .35 .32 .22?
RALI .90? .31 .19? .10? .38 .10? .17? .47 .35 .38 .33 .38 .48 .48 ? .29? .31 .29 .38 .40 .38 .34 .31 .57? .21?
RWTH .93? .43 .33 .12? .47 .26? .39 .40 .47 .35 .45 .49? .44 .53? .54? ? .44? .42 .48 .51? .54? .48? .49 .50? .26
UEDIN .92? .42 .32 .10? .22? .10? .28? .30 .42 .30 .55 .36 .23? .43 .33 .20? ? .41 .24 .52? .46 .25 .22 .27 .37
BBN-C .92? .49 .33 .24? .28? .18? .40 .39 .28? .45 .27 .27 .36 .39 .35 .35 .31 ? .26 .45? .43 .26 .58? .36 .28
CMU-HEA-C .90? .41 .21? .06? .23? .29? .28? .27 .22? .39 .40 .22 .39 .43 .29 .30 .40 .28 ? .43 .28 .15? .25 .26 .16
CMU-HYPO-C .84? .18? .20? .14? .20? .22? .21? .19? .16? .31 .22? .21? .36 .38 .34 .27? .22? .16? .24 ? .36 .23 .10? .33 .24
DCU-C .92? .27 .24? .12? .17? .23? .30 .29? .24? .32 .43 .22? .28? .41 .23 .27? .28 .22 .23 .25 ? .23 .23 .24 .17
JHU-C .88? .47 .26 .10? .33? .24? .36 .34 .24? .41 .39 .40 .42 .39 .34 .25? .42 .28 .37? .38 .39 ? .37 .32 .38?
LIUM-C .90? .48 .42 .13? .25? .20? .33 .50 .30 .44 .37 .34 .37 .52 .43 .34 .33 .22? .34 .56? .33 .43 ? .49? .44
RWTH-C .89? .22? .19? .03? .23? .12? .19? .23 .27 .30 .36 .19 .47 .54 .26? .16? .27 .19 .26 .28 .16 .22 .16? ? .22
UPV-C .89? .27 .15? .10? .16? .29? .30? .31 .25? .36 .42 .24 .32 .64? .46? .34 .27 .44 .33 .44 .23 .17? .31 .24 ?
> others .91 .43 .32 .14 .31 .21 .31 .39 .31 .42 .44 .40 .38 .52 .43 .33 .40 .37 .40 .49 .43 .38 .4 .44 .39
>= others .97 .64 .51 .24 .40 .31 .50 .59 .50 .63 .68 .65 .51 .68 .65 .55 .66 .63 .69 .75 .71 .64 .62 .74 .67
Table 24: Sentence-level ranking for the WMT10 French-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
53
