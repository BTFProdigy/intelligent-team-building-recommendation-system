Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 208?217, Prague, June 2007. c?2007 Association for Computational Linguistics
Joint Morphological and Syntactic Disambiguation?
Shay B. Cohen and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{scohen,nasmith}@cs.cmu.edu
Abstract
In morphologically rich languages, should morphological and
syntactic disambiguation be treated sequentially or as a sin-
gle problem? We describe several efficient, probabilistically-
interpretable ways to apply joint inference to morphological
and syntactic disambiguation using lattice parsing. Joint infer-
ence is shown to compare favorably to pipeline parsing methods
across a variety of component models. State-of-the-art perfor-
mance on Hebrew Treebank parsing is demonstrated using the
new method. The benefits of joint inference are modest with
the current component models, but appear to increase as com-
ponents themselves improve.
1 Introduction
As the field of statistical NLP expands to handle
more languages and domains, models appropriate
for standard benchmark tasks do not always work
well in new situations. Take, for example, pars-
ing the Wall Street Journal Penn Treebank, a long-
standing task for which highly accurate context-free
models stabilized by the year 2000 (Collins, 1999;
Charniak, 2000). On this task, the Collins model
achieves 90% F1-accuracy. Extended for new lan-
guages by Bikel (2004), it achieves only 75% on
Arabic and 72% on Hebrew.1
It should come as no surprise that Semitic parsing
lags behind English. The Collins model was care-
fully designed and tuned for WSJ English. Many of
the features in the model depend on English syntax
or Penn Treebank annotation conventions. Inherent
in its crafting is the assumption that a million words
of training text are available. Finally, for English, it
need not handle morphological ambiguity.
Indeed, the figures cited above for Arabic and
Hebrew are achieved using gold-standard morpho-
logical disambiguation and part-of-speech tagging.
?The authors acknowledge helpful feedback from the
anonymous reviewers, Sharon Goldwater, Rebecca Hwa, Alon
Lavie, and Shuly Wintner.
1Compared to the Penn Treebank, the Arabic Treebank
(Maamouri et al, 2004) has 60% as many word tokens, and
the Hebrew Treebank (Sima?an et al, 2001) has 6%.
Given only surface words, Arabic performance
drops by 1.5 F1 points. The Hebrew Treebank (un-
like Arabic) is built over morphemes, a convention
we view as sensible, though it complicates parsing.
This paper considers parsing for morphologically
rich languages, with Hebrew as a test case. Mor-
phology and syntax are two levels of linguistic de-
scription that interact. This interaction, we argue,
can affect disambiguation, so we explore here the
matter of joint disambiguation. This involves the
comparison of a pipeline (where morphology is in-
ferred first and syntactic parsing follows) with joint
inference. We present a generalization of the two,
and show new ways to do joint inference for this task
that does not involve a computational blow-up.
The paper is organized as follows. ?2 describes
the state of the art in NLP for Hebrew and some
phenomena it exhibits that motivate joint inference
for morphology and syntax. ?3 describes our ap-
proach to joint inference using lattice parsing, and
gives three variants of weighted lattice parsing with
their probabilistic interpretations. The different fac-
tor models and their stand-alone performance are
given in ?4. ?5 presents experiments on Hebrew
parsing and explores the benefits of joint inference.
2 Background
In this section we discuss prior work on statistical
morphological and syntactic processing of Hebrew
and motivate the joint approach.
2.1 NLP for Modern Hebrew
Wintner (2004) reviews work in Hebrew NLP, em-
phasizing that the challenges stem from the writ-
ing system, rich morphology, unique word forma-
tion process of roots and patterns, and relative lack
of annotated corpora.
We know of no publicly available statistical parser
designed specifically for Hebrew. Sima?an et al
208
h.
Figure 1: (a.) A sentence in Hebrew (to be read right to left), with (b.) one morphological analysis, (c.) English glosses, and (d.)
natural translation; and (e.) a different morphological analysis, (f.) English glosses, and (g.) less natural translation. (h.) shows a
morphological ?sausage? lattice that encodes the morpheme-sequence analyses L(~x) possible for a shortened sentence (unmodified
?meadow?). Shaded states are word boundaries, white states are intra-word morpheme boundaries; in practice we add POS tags to
the arcs, to permit disambiguation. According to both native speakers we polled, both interpretations are grammatical?note the
long-distance agreement required for grammaticality.
(2001) built a Hebrew Treebank of 88,747 words
(4,783 sentences) and parsed it using a probabilis-
tic model. However, they assumed that the input to
the parser was already (perfectly) morphologically
disambiguated. This assumption is very common in
multilingual parsing (see, for example, Cowan et al,
2005, and Buchholz et al, 2006).
Until recently, the NLP literature on morpho-
logical processing was dominated by the largely
non-probabilistic application of finite-state trans-
ducers (Kaplan and Kay, 1981; Koskenniemi, 1983;
Beesley and Karttunen, 2003) and the largely unsu-
pervised discovery of morphological patterns in text
(Goldsmith, 2001; Wicentowski, 2002); Hebrew
morphology receives special attention in Levinger
et al (1995), Daya et al (2004), and Adler and El-
hadad (2006). Lately a few supervised disambigua-
tion methods have come about, including hidden
Markov models (Hakkani-Tu?r et al, 2000; Hajic? et
al., 2001), conditional random fields (Kudo et al,
2004; Smith et al, 2005b), and local support vector
machines (Habash and Rambow, 2005). There are
also morphological disambiguators designed specif-
ically for Hebrew (Segal, 2000; Bar-Haim et al,
2005).
2.2 Why Joint Inference?
In NLP, the separation of syntax and morphology is
understandable when the latter is impoverished, as
in English. When both involve high levels of am-
biguity, this separation becomes harder to justify,
as argued by Tsarfaty (2006). To our knowledge,
that is the only study to move toward joint inference
of syntax and morphology, presenting joint models
and testing approximation of these models with two
parsers: one a pipeline (segmentation ? tagging ?
parsing), the other involved joint inference of seg-
mentation and tagging, with the result piped to the
parser. The latter was slightly more accurate. Tsar-
faty discussed but did not carry out joint inference.
In a morphologically rich language, the different
morphemes that make up a word can play a variety
of different syntactic roles. A reasonable linguistic
analysis might not make such morphemes immedi-
ate sisters in the tree. Indeed, the convention of the
Hebrew Treebank is to placemorphemes (rather than
words) at the leaves of the parse tree, allowing mor-
phemes of a word to attach to different nonterminal
parents.2
Generating parse trees over morphemes requires
the availability of morphological information when
parsing. Because this analysis is not in general re-
ducible to sequence labeling (tagging), the problem
is different from POS tagging. Figure 1 gives an
2The Arabic Treebank, by contrast, annotates words mor-
phologically but keeps the morphemes together as a single node
tagged with a POS sequence. In Bikel?s Arabic parser, complex
POS tags are projected to a small atomic set; it is unclear how
much information is lost.
209
example from Hebrew that illustrates the interaction
between morphology and syntax. In this example,
we show two interpretations of the surface text, with
the first being a more common natural analysis for
the sentence. The first and third-to-last words? anal-
yses depend on each other if the resulting analysis
is to be the more natural one: for this analysis the
first seven words have to be a noun phrase, while for
the less common analysis (?lying there nicely?) only
the first six words compose a noun phrase, with the
last two words composing a verb phrase. Consis-
tency depends on a long-distance dependency that a
finite-state morphology model cannot capture, but a
model that involves syntactic information can. Dis-
ambiguating the syntax aids in disambiguating the
morphology, suggesting that a joint model will per-
form both more accurately.
In sum, joint inference of morphology and syntax
is expected to allow decisions of both kinds to influ-
ence each other, enforce adherence to constraints at
both levels, and to diminish the propagation of errors
inherent in pipelines.
3 Joint Inference of Morphology and
Syntax
We now formalize the problem and supply the nec-
essary framework for performing joint morphologi-
cal disambiguation and syntactic parsing.
3.1 Notation and Morphological Sausages
Let X be the language?s word vocabulary and M be
its morpheme inventory. The set of valid analyses
for a surface word is defined using a morphologi-
cal lexicon L, which defines L(x) ? M+. L(~x) ?
(M+)+ (sequence of sequences) is the set of whole-
sentence analyses for sentence ~x = ?x1, x2, ..., xn?,
produced by concatenating elements of L(xi) in or-
der. L(~x) can be represented as an acyclic lattice
with a ?sausage? shape familiar from speech recog-
nition (Mangu et al, 1999) and machine translation
(Lavie et al, 2004). Fig. 1h shows a sausage lat-
tice for a sentence in Hebrew. We use ~m to denote
an element of L(~x) and ~mi to denote an element of
L(xi); in general, ~m = ?~m1, ~m2, ..., ~mn?.
We are interested in a function f :X+ ?
(M+)+ ? T, where T is the set of syntactic trees
for the language. f can be viewed as a structured
classifier. We use DG(~m) ? T to denote the set of
valid trees under a grammar G (here, a PCFG with
terminal alphabetM) for morpheme sequence ~m. To
be precise, f(~x) selects a mutually consistent mor-
phological and syntactic analysis from
GEN(~x) = {?~m, ?? | ~m ? L(~x), ? ? DG(~m)}
3.2 Product of Experts
Our mapping f(~x) is based on a joint probability
model p(?, ~m | ~x) which combines two probabil-
ity models pG(?, ~m) (a PCFG built on the gram-
mar G) and pL(~m | ~x) (a morphological disam-
biguation model built on the lexicon L). Factoring
the joint model into sub-models simplifies training,
since we can train each model separately, and in-
ference (parsing), as we will see later in this sec-
tion. Factored estimation has been quite popular in
NLP of late (Klein and Manning, 2003b; Smith and
Smith, 2004; Smith et al, 2005a, inter alia).
The most obvious joint parser uses pG as a condi-
tional model over trees given morphemes and maxi-
mizes the joint likelihood:
flik(~x)
= argmax
?~m,???GEN(~x)
pG(? | ~m) ? pL(~m | ~x) (1)
= argmax
?~m,???GEN(~x)
pG(?, ~m)
?
? ?
pG(?
?, ~m)
?
pL(~m, ~x)
?
~m?
pL(~m
?, ~x)
This is not straightforward, because it involves sum-
ming up the trees for each ~m to compute pG(~m),
which calls for the O(|~m|3)-Inside algorithm to
be called on each ~m. Instead, we use the joint,
pG(?, ~m), which, strictly speaking, makes the model
deficient (?leaky?), but permits a dynamic program-
ming solution.
Our models will be parametrized using either un-
normalized weights (a log-linear model) or multino-
mial distributions. Either way, both models define
scores over parts of analyses, and it may be advanta-
geous to give one model relatively greater strength,
especially since we often ignore constant normal-
izing factors. This is known as a product of ex-
perts (Hinton, 1999), where a new combined distri-
bution over events is defined by multiplying compo-
nent distributions together and renormalizing. In the
210
present setting, for some value ? ? 0,
fpoe,?(~x) = argmax
?~m,???GEN(~x)
pG(?, ~m) ? pL(~m | ~x)?
Z(~x, ?)
(2)
where Z(~x, ?) need not be computed (since it is a
constant in ~m and ? ). ? tunes the relative weight
of the morphology model with respect to the pars-
ing model. The higher ? is, the more we trust the
morphology model over the parser to correctly dis-
ambiguate the sentence. We might trust one model
more than the other for a variety of reasons: it could
be more robustly or discriminatively estimated, or it
could be known to come from a more appropriate
family.
This formulation also generalizes two more na??ve
parsing methods. If ? = 0, the morphology is mod-
eled only through the PCFG and pL is ignored ex-
cept as a constraint on which analyses L(~x) are al-
lowed (i.e., on the definition of the set GEN(~x)). At
the other extreme, as ? ? +?, pL becomes more
important. Because pL does not predict trees, pG
still ?gets to choose? the syntax tree, but in the limit
it must find a tree for argmax~m?L(~x) pL(~m | ~x).
This is effectively the morphology-first pipeline.3
3.3 Parsing Algorithms
To parse, we apply a dynamic programming algo-
rithm in the ?max, +? semiring to solve the fpoe,?
problem shown in Eq. 4. If pL is a unigram-factored
model, such that for some single-word morphologi-
cal model ? we have
pL(~m | ~x) =
?n
i=1 ?(~mi | xi) (3)
then we can implement morpho-syntactic parsing by
weighting the sausage lattice. Let the weight of each
arc that starts an analysis ~mi ? L(xi) be equal to
log ?(~mi | xi), and let other arcs have weight 0.
In the parsing algorithm, the weight on an arc is
summed in when the arc is first used to build a con-
stituent.
In general, we would like to define a joint model
that assigns (unnormalized) probabilities to ele-
ments of GEN(~x). If pG is a PCFG and pL can
3There is a slight difference. If no parse tree exists for the
pL-best morphological analysis, then a less probable ~m may be
chosen. So as ? ? +?, we can view flik,? as finding the best
grammatical ~m and its best tree?not exactly a pipeline.
be described as a weighted finite-state transducer,
then this joint model is their weighted composition,
which is a weighted CFG; call the composed gram-
mar I and its (unnormalized) distribution pI . Com-
pared to G, I will have many more nonterminals if
pL has a Markov order greater than 0 (unigram, as
above). Because parsing runtime depends heavily on
the grammar constant (at best, quadratic in the num-
ber of nonterminals), parsing with pI is not compu-
tationally attractive.4 fpoe,? is not, then, a scalable
solution when we wish to use a morphology model
pL that can make interdependent decisions about dif-
ferent words in ~x in context. We propose two new,
efficient dynamic programming solutions for joint
parsing.
In the first, we approximate the distribution
pL( ~M | ~x) using a unigram-factored model of the
form in Eq. 3:
p?L(~m | ~x) =
?n
i=1 pL( ~Mi = ~mi | ~x)? ?? ?
posterior, depends on all of ~x
(7)
Similar methods were applied by Matsuzaki et al
(2005) and Petrov and Klein (2007) for parsing un-
der a PCFG with nonterminals with latent anno-
tations. Their approach was variational, approxi-
mating the true posterior over coarse parses using
a sentence-specific PCFG on the coarse nontermi-
nals, created directly out of the true fine-grained
PCFG. In our case, we approximate the full distri-
bution over morphological analyses for the sentence
by a simpler, sentence-specific unigram model that
assumes each word?s analysis is to be chosen inde-
pendently of the others. Note that our model (pL)
does not make such an assumption, only the ap-
proximate model p?L does, and the approximation is
per-sentence. The idea resembles a mean-field vari-
ational approximation for graphical models. Turn-
ing to implementation, we can solve for pL(~mi | ~x)
exactly using the forward-backward algorithm. We
will call this method fvari,? (see Eq. 5).
A closely related method, applied by Goodman
(1996) is called minimum-risk decoding. Good-
man called it ?maximum expected recall? when ap-
plying it to parsing. In the HMM community it
4In prior work involving factored syntax models?
lexicalized (Klein and Manning, 2003b) and bilingual (Smith
and Smith, 2004)?fpoe,1 was applied, and the asymptotic run-
time went to O(n5) and O(n7).
211
fpoe,?(~x) = argmax
?~m,???GEN(~x)
log pG(?, ~m) + ? log pL(~m | ~x) (4)
fvari,?(~x) = argmax
?~m,???GEN(~x)
log pG(?, ~m) + ?
?n
i=1 log pL(~mi | ~x) (5)
frisk,?(~x) = argmax
?~m,???GEN(~x)
log pG(?, ~m) + ?
?n
i=1 pL(~mi | ~x) (6)
is sometimes called ?posterior decoding.? Mini-
mum risk decoding is attributable to Goel and Byrne
(2000). Applied to a single model, it factors the
parsing decision by penalizable errors, and chooses
the solution that minimizes the risk (expected num-
ber of errors under the model). This factors into a
sum of expectations, one per potential mistake. This
method is expensive for parsing models (since it re-
quires the Inside algorithm to compute expected re-
call mistakes), but entirely reasonable for sequence
labeling models. The idea is to score each word-
analysis ~mi in the morphological lattice by the ex-
pected value (under pL) that ~mi is present in the fi-
nal analysis ~m. This is, of course pL( ~Mi = ~mi | ~x),
the same quantity computed for fvari,?, except the
score of a path in the lattice is now a sum of pos-
teriors rather than a product. Our second approxi-
mate joint parser tries to maximize the probability
of the parse (as before) and at the same time to min-
imize the risk of the morphological analysis. See
frisk,? in Eq. 6; the only difference between frisk,?
and fvari,? is whether posteriors are added (frisk,?)
or multiplied (fvari,?).
To summarize this section, fvari,? and frisk,?
are two approximations to the expensive-in-general
fpoe,? that boil down to parsing over weighted lat-
tices. The only difference between them is how
the lattice is weighted: using ? log pL(~mi | ~x) for
fvari,? or using ?pL(~mi | ~x) for frisk,?.5 In case of
a unigram pL, fpoe,? is equivalent to fvari,?; other-
wise fpoe,? is likely to be too expensive.
3.4 Lattice Parsing
To parse the weighted lattices using fvari,? and
frisk,? in the previous section, we use lattice parsing.
Lattice parsing is a straightforward generalization of
5Until now, we have talked about weighting word analyses,
which may cover several arcs, rather than arcs. In practice we
apply the weight to the first arc of a word analysis, and weight
the remaining arcs of that analysis with 0 (no cost or benefit),
giving the desired effect.
string parsing that indexes constituents by states in
the lattice rather than word interstices. At parsing
time, a ?max, +? lattice parser finds the best com-
bined parse tree and path through the lattice. Im-
portantly, the data structures that are used in chart
parsing need not change in order to accommodate
lattices. The generalization over classic Earley or
CKY parsing is simple: keep in the parsing chart
constituents created over a pair of start state and end
state (instead of start position and end position), and
(if desired) factor in weights on lattice arcs; see Hall
(2005).
4 Factored Models
A fair comparison of joint and pipeline parsing must
make some attempt to control for the component
models. We describe here two PCFGs we used for
pG(?, ~m) and two finite-state morphological models
we used for pL(~m | ~x). We show how these mod-
els perform in stand-alone evaluations. For all ex-
periments, we used the Hebrew Treebank (Sima?an
et al, 2001). After removing traces and removing
functional information from the nonterminals, we
had 3,770 sentences in the training set, 371 sen-
tences in the development set (used primarily to se-
lect the value of ?) and 370 sentences in the test set.
4.1 Syntax Model
Our first syntax model is an unbinarized PCFG
trained using relative frequencies. Preterminal (POS
tag ? morpheme) rules are smoothed using back-
off to a model that predicts the morpheme length
and letter sequence. The PCFG is not binarized.
This grammar is remarkably good, given the lim-
ited effort that went into it. The rules in the train-
ing set had high coverage with respect to the de-
velopment set: an oracle experiment in which we
maximized the number of recovered gold-standard
constituents (on the development set) gave F1 ac-
curacy of 93.7%. In fact, its accuracy supersedes
212
more complex, lexicalized, models: given gold-
standard morphology, it achieves 81.2% (compared
to 72.0% by Bikel?s parser, with head rules specified
by a native speaker). This is probably attributable
to the dataset?s size, which makes training with
highly-parameterized lexicalized models precarious
and prone to overfitting. With first-order vertical
markovization (i.e., annotating each nonterminal
with its parent as in Johnson, 1998), accuracy is also
at 81.2%. Tuning the horizontal markovization of
the grammar rules (Klein and Manning, 2003a) had
a small, adverse effect on this dataset.
Since the PCFG model was relatively successful
compared to lexicalized models, and is faster to run,
we decided to use a vanilla PCFG, denoted Gvan,
and a parent-annotated version of that PCFG (John-
son, 1998), denoted Gv=2.
4.2 Morphology Model
Both of our morphology models use the same mor-
phological lexicon L, which we describe first.
4.2.1 Morphological Lexicon
In this work, a morphological analysis of a word
is a sequence of morphemes, possibly with a tag for
each morpheme. There are several available analyz-
ers for Hebrew, including Yona and Wintner (2005)
and Segal (2000). We use instead an empirically-
constructed generative lexicon that has the advan-
tage of matching the Treebank data and conventions.
If the Treebank is enriched, this would then directly
benefit the lexicon and our models.
Starting with the training data from the Hebrew
Treebank, we first create a set of prefixesMp ? M;
this set includes any morpheme seen in a non-final
position within any word. We also create a set of
stems Ms ? M that includes any morpheme seen
in a final position in a word. This effectively cap-
tures the morphological analysis convention in the
Hebrew Treebank, where a stem is prefixed by a rel-
atively dominant low-entropy sequence of 0?5 prefix
morphemes. For example,MHKLB (?from the dog?)
is analyzed as M+H+KLB with prefixes M (?from?)
and H (?the?) and KLB (?dog?) is the stem. In prac-
tice, |Mp| = 124 (including some conventions for
numerals) and |Ms| = 13,588. The morphological
lexicon is then defined as any analysis givenMp and
Ms:
L(x) = {mk1 ? M
?
p ?Ms | concat(m
k
1) = x)}
?{mk1 | count(m
k
1, x) ? 1} (9)
where mk1 denotes ?m1, ...,mk? and count(m
k
1, x)
denotes the number of occurrences of x disam-
biguated as mk1 in the training set. Note that L(x)
also includes any analysis of x observed in the train-
ing data. This permits the memorization of any
observed analysis that is more involved than sim-
ple segmentation (4% of word tokens in the train-
ing set; e.g., LXDR (?to the room?) is analyzed as
L+H+XDR). This will have an effect on evaluation
(see ?5.1). On the development data, L has 98.6%
coverage.
4.2.2 Unigram Baseline
The baseline morphology model, puniL , first de-
fines a joint distribution following Eq. 8. The word
model factors out when we conditionalize to form
puniL (?m1, ...,mk? | x). The prefix sequence model
is multinomial estimated by MLE. The stem model
(conditioned on the prefix sequence) is smoothed to
permit any stem that is a sequence of Hebrew char-
acters. On the development data, puniL is 88.8% ac-
curate (by word).
4.2.3 Conditional Random Field
The second morphology model, pcrfL , which is
based on the same morphological lexicon L, uses
a second-order conditional random field (Lafferty et
al., 2001) to disambiguate the full sentence by mod-
eling local contexts (Kudo et al, 2004; Smith et al,
2005b). Space does not permit a full description; the
model uses all the features of Smith et al (2005b)
except the ?lemma? portion of the model, since the
Hebrew Treebank does not provide lemmas. The
weights are trained to maximize the probability of
the correct path through the morphological lattice,
conditioned on the lattice. This is therefore a dis-
criminative model that defines pL(~m | ~x) directly,
though we ignore the normalization factor in pars-
ing.
Until now we have described pL as a model of
morphemes, but this CRF is trained to predict POS
tags as well?we can either use the tags (i.e., label
the morphological lattice with tag/morpheme pairs,
213
puniL (?m1,m2, ...,mk?, x) = p(x | ?m1,m2, ...,mk?)? ?? ?
word
? p(mk | ?m1, ...,mk?1?)
? ?? ?
stem
? p(?m1, ...,mk?1?)
? ?? ?
prefix sequence
(8)
so that the lattice parser finds a parse that is con-
sistent under both models), or sum the tags out and
let the parser do the tagging. One subtlety is the
tagging of words not seen in the training data; for
such words an unsegmented hypothesis with tag UN-
KNOWN is included in the lattice and may therefore
be selected by the CRF. On the development data,
pcrfL is 89.8% accurate on morphology, with 74.9%
fine-grained POS-tagging F1-accuracy (see ?5.1).
Note on generative and discriminative models.
The reader may be skeptical of our choice to com-
bine a generative PCFG with a discrimative CRF.
We point out that both are used to define conditional
distributions over desired ?output? structures given
?input? sequences. Notwithstanding the fact that the
factors can be estimated in very different ways, our
combination in an exact or approximate product-of-
experts is a reasonable and principled approach.
5 Experiments
In this section we evaluate parsing performance, but
an evaluation issue is resolved first.
5.1 Evaluation Measures
The ?Parseval? measures (Black et al, 1991) are
used to evaluate a parser?s phrase-structure trees
against a gold standard. They compute precision and
recall of constituents, each indexed by a label and
two endpoints. As pointed out by Tsarfaty (2006),
joint parsing of morphology and syntax renders this
indexing inappropriate, since it assumes the yields
of the trees are identical?that assumption is vio-
lated if there are any errors in the hypothesized ~m.
Tsarfaty (2006) instead indexed by non-whitespace
character positions, to deal with segmentation mis-
matches. In general (and in this work) that is still
insufficient, since L(~x) may include ~m that are not
simply segmentations of ~x (see ?4.2.1).
Roark et al (2006) propose an evaluation met-
ric for comparing a parse tree over a sentence gen-
erated by a speech recognizer to a gold-standard
parse. As in our case, the hypothesized tree could
have a different yield than the original gold-standard
parse tree, because of errors made by the speech
recognizer. The metric is based on an alignment
between the hypothesized sentence and the gold-
standard sentence. We used a similar evaluation
metric, which takes into account the information
about parallel word boundaries as well, a piece of
information that does not appear naturally in speech
recognition. Given the correct ~m? and the hypothe-
sis ~?m, we use dynamic programming to find an op-
timal many-to-many monotonic alignment between
the atomic morphemes in the two sequences. The
algorithm penalizes each violation (by a morpheme)
of a one-to-one correspondence,6 and each character
edit required to transform one side of a correspon-
dence into the other (without whitespace). Word
boundaries are (here) known and included as index
positions. In the case where ~?m = ~m? (or equal up to
whitespace) the method is identical to Parseval (and
also to Tsarfaty, 2006). POS tag accuracy is evalu-
ated the same way, for the same reasons; we report
F1-accuracy for tagging and parsing.
5.2 Experimental Comparison
In our experiment we vary four settings:
? Decoding algorithm: fpoe,?, frisk,?, or fvari,?
(?3.3).
? Syntax model: Gvan or Gv=2 (?4.1).
? Morphology model: puniL or p
crf
L (?4.2). In the lat-
ter case, we can use the scores over morpheme
sequences only (summing out tags before lattice
parsing; denoted m.-pcrfL ) or the full model over
morphemes and tags, denoted t.-pcrfL .
7
? ?, the relative strength given to the morphol-
ogy model (see ?3). We tested values of ? in
{0,+?} ? {10q | q ? {0, 1, ..., 16}}. Recall
that ? = 0 ignores the morphology model prob-
abilities altogether (using an unweighted lattice),
6That is, in a correspondence of a morphemes in one string
with b in the other, the penalty is a+ b?2, since the morpheme
on each side is not in violation.
7One subtlety is that any arc with the UNKNOWN POS
tag can be relabeled?to any other tag?by the syntax model,
whose preterminal rules are smoothed. This was crucial for
? = +? (pipeline) parsing with t.-pcrfL as the morphology
model, since the parser does not recognize UNKNOWN as a tag.
214
tuned ? pipeline (? ? +?)
pa
rs
er
m
or
ph
. m
od
el
sy
nt
ax
m
od
el
se
g.
ac
c
fin
e
PO
S
F 1
co
ar
se
PO
S
F 1
pa
rs
e
F 1
se
g.
ac
c
fin
e
PO
S
F 1
co
ar
se
PO
S
F 1
pa
rs
e
F 1
puniL pGvan 88.0 70.6 75.5 59.5 88.5 71.5 76.1 59.8
pGv=2 88.0 70.7 75.8 60.4 88.6 70.8 75.7 59.9
m.-pcrfL pGvan ? ? ? ? 90.9 75.6 80.2 63.7
f p
o
e,
?
pGv=2 ? ? ? ? 90.9 75.3 80.2 64.2
t.-pcrfL pGvan ? ? ? ? 90.9 77.2
?81.5 63.0
pGv=2 ? ? ? ? 90.9 77.2
?81.5 64.0
puniL pGvan 87.9 70.9 75.3 58.9 88.5 71.5 76.1 59.8
pGv=2 87.8 70.9 75.6 59.5 88.6 70.8 75.6 59.9
m.-pcrfL pGvan 89.8 74.5 78.9 62.5 89.8 74.5 78.9 62.4
f r
is
k
,?
pGv=2 89.8 74.3 79.1 63.0 89.8 74.3 79.1 63.0
t.-pcrfL pGvan 90.2 76.6 80.5 62.4 89.9 76.4 80.4 61.6
pGv=2 90.2 76.6 80.5 63.1 89.9 76.4 80.4 62.2
puniL pGvan 88.0 70.6 75.5 59.5 88.5 71.5 76.1 59.8
pGv=2 88.0 70.7 75.8 60.4 88.6 70.8 75.7 59.9
m.-pcrfL pGvan
?91.1 75.6 80.4 64.0 90.9 74.8 79.3 62.9
pGv=2 90.9 75.4 80.5
?64.4 90.1 74.6 79.5 63.2
f v
a
ri
,?
t.-pcrfL pGvan
?91.3 ?77.7 ?81.7 63.0 90.9 77.0 ?81.3 62.6
pGv=2
?91.3 ?77.6 ?81.6 63.6 90.9 77.0 ?81.3 63.6
Table 1: Results of experi-
ments on Hebrew (test data,
max. length 40). This table
shows the performance of
joint parsing (finite ?; left)
and a pipeline (? ? +?;
right). Joint parsing with a non-
unigram morphology model
is too expensive (marked ?).
Morphological analysis accu-
racy (by word), fine-grained
(full tags) and coarse-grained
(only parts of speech) POS
tagging accuracy (F1), and gen-
eralized constituent accuracy
(F1) are reported; ? was tuned
for each of these separately.
Boldface denotes that figures
were significantly better than
their counterparts in the same
row, under a binomial sign test
(p < 0.05). ? marks the best
overall accuracy and figures
that are not significantly worse
(binomial sign test, p < 0.05).
and as ? ? +? a morphology-first pipeline is
approached.
We measured four outcome values: segmentation
accuracy (fraction of word tokens segmented cor-
rectly), fine- and coarse-grained tagging accuracy,8
and parsing accuracy. For tagging and parsing, F1-
measures are given, according to the generalized
evaluation measure described in ?5.1.
5.3 Results
Tab. 1 compares parsing with tuned ? values to the
pipeline.
The best results were achieved using fvari,?, us-
ing the CRF and joint disambiguation. Without the
CRF (using puniL ), the difference between the decod-
ing algorithms is less apparent, suggesting an inter-
action between the sophistication of the components
and the best way to decode with them. These re-
sults suggest that fvari,?, which permits pL to ?veto?
any structure involving a morphological analysis for
any word that is a posteriori unlikely (note that
8Although the Hebrew Treebank is small, the size of its POS
tagset is large (four times larger than the Penn Treebank), be-
cause the tags encode morphological features (gender, person,
and number). These features have either been ignored in prior
work or encoded differently. In order for our POS-tagging fig-
ures to be reasonably comparable to previous work, we include
accuracy for coarse-grained tags (only the core part of speech)
tags as well as the detailed Hebrew Treebank tags.
log pL(~mi | ~x) can be an arbitrarily large negative
number), is beneficial as a ?filter? on parses.9 frisk,?,
on the other hand, is only allowed to give ?bonuses?
of up to ? to each morphological analysis that pL
believes in; its influence is therefore weaker. This
result is consistent with the findings of Petrov et al
(2007) for another approximate parsing task.
The advantage of the parent-annotated PCFG is
also more apparent when the CRF is used for mor-
phology, and when ? is tuned. All other things
equal, then, pcrfL led to higher accuracy all around.
Letting the CRF help predict the POS tags helped
tagging accuracy but not parsing accuracy.
While the gains over the pipeline are modest,
the segmentation, fine POS, and parsing accuracy
scores achieved by joint disambiguation with fvari,?
with the CRF are significantly better than any of the
pipeline conditions.
Interestingly, if we had not tested with the CRF,
we might have reached a very different conclusion
about the usefulness of tuning ? as opposed to a
pipeline. With the unigram morphology model,
joint parsing frequently underperforms the pipeline,
sometimes even signficantly. The explanation, we
9Another way to describe this combination is to call it a
product of |~x|+1 experts: one for the morphological analysis of
each word, plus the grammar. The morphology experts (softly)
veto any analysis that is dubious based on surface criteria, and
the grammar (softly) vetoes less-grammatical parses.
215
pa
rs
er
m
or
ph
. m
od
el
sy
nt
ax
m
od
el
se
g.
ac
c
fin
e
PO
S
F 1
co
ar
se
PO
S
F 1
pa
rs
e
F 1
puniL pGvan 90.7 73.4 78.5 64.3
pGv=2 90.2 73.0 78.5 64.9
m.-pcrfL pGvan 90.7 75.4 80.0 65.2
f r
is
k
,?
pGv=2 90.8 75.1 80.2 65.4
t.-pcrfL pGvan 91.2 78.1 82.4 65.7
pGv=2 91.1 78.0 82.2 66.2
puniL pGvan 90.6 73.2 78.3 63.5
pGv=2 90.2 72.8 78.4 64.4
m.-pcrfL pGvan 92.0 76.6 81.5 66.9
pGv=2 91.9 76.2 81.6 66.9
f v
a
ri
,?
t.-pcrfL pGvan 91.8 79.1 83.2 66.5
pGv=2 91.7 78.7 83.0 67.4
Table 2: Oracle results of experiments on Hebrew (test data,
max. length 40). This table shows the performance of mor-
phological segmentation, part-of-speech tagging, coarse part-
of-speech tagging and parsing when using an oracle to select
the best ? for each sentence. The notation and interpretation of
the numbers are the same as in Tab. 1.
believe, has to do with the ability of the unigram
model to estimate a good distribution over analy-
ses. While the unigram model is nearly as good
as the CRF at picking the right segmentation for a
word, joint parsing demands much more. In case
the best segmentation does not lead to a grammat-
ical morpheme sequence (under the syntax model),
the morphology model needs to be able to give rela-
tive strengths to the alternatives. The unigrammodel
is less able to do this, because it ignores the context
of the word, and so the benefit of joint parsing is lost.
Most commonly the tuned value of ? is around
10 (not shown, to preserve clarity). Because of ig-
nored normalization constants, this does not mean
that morphology is ?10? more important than syn-
tax,? but it does mean that, for a particular pL and
pG, tuning their relative importance in decoding can
improve accuracy. In Tab. 2 we show how perfor-
mance would improve if the oracle value of ? was
selected for each test-set sentence; this further high-
lights the potential impact of perfecting the tradeoff
between models. Of course, selecting ? automati-
cally at test-time, per sentence, is an open problem.
To our knowledge, the parsers we have described
represent the state-of-the-art in Modern Hebrew
parsing. The closest result is Tsarfaty (2006), which
we have not directly replicated. Tsarfaty?s model is
essentially a pipeline application of fpoe,? with a
grammar like pGvan . Her work focused more on the
interplay between the segmentation and POS tag-
ging models and the amount of information passed
to the parser. Some key differences preclude direct
comparison: we modeled fine-grained tags (though
we report both kinds of tagging accurcy), we em-
ployed a richer morphological lexicon (permitting
analyses that are not just segmentation), and a dif-
ferent training/test split and length filter (we used
longer sentences). Nonetheless, our conclusions
support the argument in Tsarfaty (2006) for more in-
tegrated parsing methods.
We conclude that tuning the relative importance
of the two models?rather than pipelining to give
one infinitely more importance?can provide an im-
provement on segmentation, tagging, and parsing
accuracy. This suggests that future parsing efforts
for languages with rich morphology might con-
tinue to assume separately-trained (and separately-
improved) morphology and syntax components,
which would stand to gain from joint decoding. In
our experiments, better morphological disambigua-
tion was crucial to getting any benefit from joint
decoding. Our result also suggests that exploring
new, fully-integrated models (and training methods
for them) may be advantageous.
6 Conclusion
We showed that joint morpho-syntactic parsing can
improve the accuracy of both kinds of disambigua-
tion. Several efficient parsing methods were pre-
sented, using factored state-of-the-art morphology
and syntax models for the language under considera-
tion. We demonstrated state-of-the-art performance
on and consistent improvements across many set-
tings for Modern Hebrew, a morphologically-rich
language with a relatively small treebank.
References
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological
disambiguation. In Proc. of COLING-ACL.
R. Bar-Haim, K. Sima?an, and Y. Winter. 2005. Choos-
ing an optimal architecture for segmentation and POS-
tagging of Modern Hebrew. In Proc. of ACLWorkshop
on Computational Approaches to Semitic Languages.
K. R. Beesley and L. Karttunen. 2003. Finite State Mor-
phology. CSLI.
216
D. Bikel. 2004. Multilingual statistical pars-
ing engine. http://www.cis.upenn.edu/
?dbikel/software.html#stat-parser.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proc. of DARPA Workshop on
Speech and Natural Language.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, U. Penn.
B. Cowan and M. Collins. 2005. Morphology and
reranking for the statistical parsing of Spanish. In
Proc. of HLT-EMNLP.
E. Daya, D. Roth, and S. Wintner. 2004. Learning
Hebrew roots: Machine learning with linguistic con-
straints. In Proc. of EMNLP.
V. Goel and W. Byrne. 2000. Minimum Bayes risk auto-
matic speech recognition. Computer Speech and Lan-
guage, 14(2):115?135.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of natural language. Comp. Ling., 27(2):153?
198.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging, and morphological dis-
ambiguation in one fell swoop. In Proc. of ACL.
J. Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petkevic?.
2001. Serial combination of rules and statistics: A
case study in Czech tagging. In Proc. of ACL.
D. Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2000. Statis-
tical morphological disambiguation for agglutinative
languages. In Proc. of COLING.
K. Hall. 2005. Best-first Word-lattice Parsing: Tech-
niques for Integrated Syntactic Language Modeling.
Ph.D. thesis, Brown University.
G. E. Hinton. 1999. Products of experts. In Proc. of
ICANN.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Comp. Ling., 24(4):613?632.
R. M. Kaplan and M. Kay. 1981. Phonological rules and
finite-state transducers. Presented at LSA.
D. Klein and C. D. Manning. 2003a. Accurate unlexical-
ized parsing. In Proc. of ACL, pages 423?430.
D. Klein and C. D. Manning. 2003b. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
K. Koskenniemi. 1983. A general computational model
of word-form recognition and production. Technical
Report 11, University of Helsinki.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
plying conditional random fields to Japanese morpho-
logical analysis. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. Lavie, S. Wintner, Y. Eytani, E. Peterson, and
K. Probst. 2004. Rapid prototyping of a transfer-
based Hebrew-to-English machine translation system.
In Proc. of TMI.
M. Levinger, U. Ornan, and A. Itai. 1995. Learning mor-
pholexical probabilities from an untagged corpus with
an application to Hebrew. Comp. Ling., 21:383?404.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proc. of NEMLAR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words: Lattice-based word error mini-
mization. In Proc. of ECSCT.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proc. of ACL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
B. Roark, M. Harper, E. Charniak, B. Dorr, M. Johnson,
J. Kahn, Y. Liu, M. Ostendorf, J. Hale, A. Krasnyan-
skaya, M. Lease, I. Shafran, M. Snover, R. Stewart,
and Lisa Yung. 2006. Sparseval: Evaluation metrics
for parsing speech. In Proc. of LREC.
E. Segal. 2000. A probabilistic morphological analyzer
for Hebrew undotted texts. Master?s thesis, Technion.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Na-
tiv. 2001. Building a treebank of modern Hebrew text.
Journal Traitement Automatique des Langues. Avail-
able at http://mila.cs.technion.ac.il.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proc. of EMNLP, pages 49?56.
A. Smith, T. Cohn, and M. Osborne. 2005a. Logarithmic
opinion pools for conditional random fields. In Proc.
of ACL.
N. A. Smith, D. A. Smith, and R. W. Tromble.
2005b. Context-based morphological disambiguation
with random fields. In Proc. of HLT-EMNLP.
R. Tsarfaty. 2006. Integrated morphological and syn-
tactic disambiguation for Modern Hebrew. In Proc. of
COLING-ACL Student Research Workshop.
R. Wicentowski. 2002. Modeling and Learning Mul-
tilingual Inflectional Morphology in a Minimally Su-
pervised Framework. Ph.D. thesis, Johns Hopkins U.
S. Wintner. 2004. Hebrew computational linguistics:
Past and future. Art. Int. Rev., 21(2):113?138.
S. Yona and S. Wintner. 2005. A finite-state morpholog-
ical grammar of Hebrew. In Proc. of ACL Workshop
on Computational Approaches to Semitic Languages.
217
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 74?82,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Shared Logistic Normal Distributions for Soft Parameter Tying
in Unsupervised Grammar Induction
Shay B. Cohen and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,nasmith}@cs.cmu.edu
Abstract
We present a family of priors over probabilis-
tic grammar weights, called the shared logistic
normal distribution. This family extends the
partitioned logistic normal distribution, en-
abling factored covariance between the prob-
abilities of different derivation events in the
probabilistic grammar, providing a new way
to encode prior knowledge about an unknown
grammar. We describe a variational EM al-
gorithm for learning a probabilistic grammar
based on this family of priors. We then experi-
ment with unsupervised dependency grammar
induction and show significant improvements
using our model for both monolingual learn-
ing and bilingual learning with a non-parallel,
multilingual corpus.
1 Introduction
Probabilistic grammars have become an important
tool in natural language processing. They are most
commonly used for parsing and linguistic analy-
sis (Charniak and Johnson, 2005; Collins, 2003),
but are now commonly seen in applications like ma-
chine translation (Wu, 1997) and question answer-
ing (Wang et al, 2007). An attractive property of
probabilistic grammars is that they permit the use
of well-understood parameter estimation methods
for learning?both from labeled and unlabeled data.
Here we tackle the unsupervised grammar learning
problem, specifically for unlexicalized context-free
dependency grammars, using an empirical Bayesian
approach with a novel family of priors.
There has been an increased interest recently
in employing Bayesian modeling for probabilistic
grammars in different settings, ranging from putting
priors over grammar probabilities (Johnson et al,
2007) to putting non-parametric priors over deriva-
tions (Johnson et al, 2006) to learning the set of
states in a grammar (Finkel et al, 2007; Liang et al,
2007). Bayesian methods offer an elegant frame-
work for combining prior knowledge with data.
The main challenge in Bayesian grammar learning
is efficiently approximating probabilistic inference,
which is generally intractable. Most commonly vari-
ational (Johnson, 2007; Kurihara and Sato, 2006)
or sampling techniques are applied (Johnson et al,
2006).
Because probabilistic grammars are built out of
multinomial distributions, the Dirichlet family (or,
more precisely, a collection of Dirichlets) is a natural
candidate for probabilistic grammars because of its
conjugacy to the multinomial family. Conjugacy im-
plies a clean form for the posterior distribution over
grammar probabilities (given the data and the prior),
bestowing computational tractability.
Following work by Blei and Lafferty (2006) for
topic models, Cohen et al (2008) proposed an alter-
native to Dirichlet priors for probabilistic grammars,
based on the logistic normal (LN) distribution over
the probability simplex. Cohen et al used this prior
to softly tie grammar weights through the covariance
parameters of the LN. The prior encodes informa-
tion about which grammar rules? weights are likely
to covary, a more intuitive and expressive represen-
tation of knowledge than offered by Dirichlet distri-
butions.1
The contribution of this paper is two-fold. First,
from the modeling perspective, we present a gen-
eralization of the LN prior of Cohen et al (2008),
showing how to extend the use of the LN prior to
1Although the task, underlying model, and weights being
tied were different, Eisner (2002) also showed evidence for the
efficacy of parameter tying in grammar learning.
74
tie between any grammar weights in a probabilistic
grammar (instead of only allowing weights within
the same multinomial distribution to covary). Sec-
ond, from the experimental perspective, we show
how such flexibility in parameter tying can help in
unsupervised grammar learning in the well-known
monolingual setting and in a new bilingual setting
where grammars for two languages are learned at
once (without parallel corpora).
Our method is based on a distribution which we
call the shared logistic normal distribution, which
is a distribution over a collection of multinomials
from different probability simplexes. We provide a
variational EM algorithm for inference.
The rest of this paper is organized as follows. In
?2, we give a brief explanation of probabilistic gram-
mars and introduce some notation for the specific
type of dependency grammar used in this paper, due
to Klein and Manning (2004). In ?3, we present our
model and a variational inference algorithm for it. In
?4, we report on experiments for both monolingual
settings and a bilingual setting and discuss them. We
discuss future work (?5) and conclude in ?6.
2 Probabilistic Grammars and
Dependency Grammar Induction
A probabilistic grammar defines a probability dis-
tribution over grammatical derivations generated
through a step-by-step process. HMMs, for exam-
ple, can be understood as a random walk through
a probabilistic finite-state network, with an output
symbol sampled at each state. Each ?step? of the
walk and each symbol emission corresponds to one
derivation step. PCFGs generate phrase-structure
trees by recursively rewriting nonterminal symbols
as sequences of ?child? symbols (each itself either
a nonterminal symbol or a terminal symbol analo-
gous to the emissions of an HMM). Each step or
emission of an HMM and each rewriting operation
of a PCFG is conditionally independent of the other
rewriting operations given a single structural ele-
ment (one HMM or PCFG state); this Markov prop-
erty permits efficient inference for the probability
distribution defined by the probabilistic grammar.
In general, a probabilistic grammar defines the
joint probability of a string x and a grammatical
derivation y:
p(x,y | ?) =
K?
k=1
Nk?
i=1
?fk,i(x,y)k,i (1)
= exp
K?
k=1
Nk?
i=1
fk,i(x,y) log ?k,i
where fk,i is a function that ?counts? the number
of times the kth distribution?s ith event occurs in
the derivation. The ? are a collection of K multi-
nomials ??1, ...,?K?, the kth of which includes Nk
events. Note that there may be many derivations y
for a given string x?perhaps even infinitely many
in some kinds of grammars.
2.1 Dependency Model with Valence
HMMs and PCFGs are the best-known probabilis-
tic grammars, but there are many others. In this
paper, we use the ?dependency model with va-
lence? (DMV), due to Klein and Manning (2004).
DMV defines a probabilistic grammar for unla-
beled, projective dependency structures. Klein and
Manning (2004) achieved their best results with a
combination of DMV with a model known as the
?constituent-context model? (CCM). We do not ex-
periment with CCM in this paper, because it does
not fit directly in a Bayesian setting (it is highly defi-
cient) and because state-of-the-art unsupervised de-
pendency parsing results have been achieved with
DMV alone (Smith, 2006).
Using the notation above, DMV defines x =
?x1, x2, ..., xn? to be a sentence. x0 is a special
?wall? symbol, $, on the left of every sentence. A
tree y is defined by a pair of functions yleft and
yright (both {0, 1, 2, ..., n} ? 2{1,2,...,n}) that map
each word to its sets of left and right dependents,
respectively. Here, the graph is constrained to be a
projective tree rooted at x0 = $: each word except $
has a single parent, and there are no cycles or cross-
ing dependencies. yleft(0) is taken to be empty, and
yright(0) contains the sentence?s single head. Let
y(i) denote the subtree rooted at position i. The
probability P (y(i) | xi,?) of generating this sub-
tree, given its head word xi, is defined recursively,
as described in Fig. 1 (Eq. 2).
The probability of the entire tree is given by
p(x,y | ?) = P (y(0) | $,?). The ? are the multi-
nomial distributions ?s(? | ?, ?, ?) and ?c(? | ?, ?). To
75
P (y(i) | xi,?) = ?D?{left ,right} ?s(stop | xi,D , [yD(i) = ?]) (2)
??j?yD (i) ?s(?stop | xi,D ,firsty(j))? ?c(xj | xi,D)? P (y(j) | xj ,?)
Figure 1: The ?dependency model with valence? recursive equation. firsty(j) is a predicate defined to be true iff xj is
the closest child (on either side) to its parent xi. The probability of the tree p(x,y | ?) = P (y(0) | $,?).
follow the general setting of Eq. 1, we index these
distributions as ?1, ...,?K .
Headden et al (2009) extended DMV so that the
distributions ?c condition on the valence as well,
with smoothing, and showed significant improve-
ments for short sentences. Our experiments found
that these improvements do not hold on longer sen-
tences. Here we experiment only with DMV, but
note that our techniques are also applicable to richer
probabilistic grammars like that of Headden et al
2.2 Learning DMV
Klein and Manning (2004) learned the DMV prob-
abilities ? from a corpus of part-of-speech-tagged
sentences using the EM algorithm. EM manipulates
? to locally optimize the likelihood of the observed
portion of the data (here, x), marginalizing out the
hidden portions (here, y). The likelihood surface
is not globally concave, so EM only locally opti-
mizes the surface. Klein and Manning?s initializa-
tion, though reasonable and language-independent,
was an important factor in performance.
Various alternatives to EM were explored by
Smith (2006), achieving substantially more accu-
rate parsing models by altering the objective func-
tion. Smith?s methods did require substantial hyper-
parameter tuning, and the best results were obtained
using small annotated development sets to choose
hyperparameters. In this paper, we consider only
fully unsupervised methods, though we the Bayesian
ideas explored here might be merged with the bias-
ing approaches of Smith (2006) for further benefit.
3 Parameter Tying in the Bayesian Setting
As stated above, ? comprises a collection of multi-
nomials that weights the grammar. Taking the
Bayesian approach, we wish to place a prior on those
multinomials, and the Dirichlet family is a natural
candidate for such a prior because of its conjugacy,
which makes inference algorithms easier to derive.
For example, if we make a ?mean-field assumption,?
with respect to hidden structure and weights, the
variational algorithm for approximately inferring the
distribution over ? and trees y resembles the tradi-
tional EM algorithm very closely (Johnson, 2007).
In fact, variational inference in this case takes an ac-
tion similar to smoothing the counts using the exp-?
function during the E-step. Variational inference can
be embedded in an empirical Bayes setting, in which
we optimize the variational bound with respect to the
hyperparameters as well, repeating the process until
convergence.
3.1 Logistic Normal Distributions
While Dirichlet priors over grammar probabilities
make learning algorithms easy, they are limiting.
In particular, as noted by Blei and Lafferty (2006),
there is no explicit flexible way for the Dirichlet?s
parameters to encode beliefs about covariance be-
tween the probabilities of two events. To illustrate
this point, we describe how a multinomial ? of di-
mension d is generated from a Dirichlet distribution
with parameters ? = ??1, ..., ?d?:
1. Generate ?j ? ?(?j , 1) independently for j ?
{1, ..., d}.
2. ?j ? ?j/?i ?i.
where ?(?, 1) is a Gamma distribution with shape ?
and scale 1.
Correlation among ?i and ?j , i 6= j, cannot be
modeled directly, only through the normalization
in step 2. In contrast, LN distributions (Aitchison,
1986) provide a natural way to model such correla-
tion. The LN draws a multinomial ? as follows:
1. Generate ? ? Normal(?,?).
2. ?j ? exp(?j)/?i exp(?i).
76
I1 = {1:2, 3:6, 7:9} = { I1,1, I1,2, I1,L1 }
I2 = {1:2, 3:6} = { I2,1, I2,L2 }
I3 = {1:4, 5:7} = { I3,1, I3,L3 }
IN = {1:2} = { I4,L4 }
J1 J2 JK
?
?????
?????
partition struct. S
?1 = ??1,1, ?1,2, ?1,3, ?1,4, ?1,5, ?1,6, ?1,7, ?1,8, ?1,`1? ? Normal(?1,?1)
?2 = ??2,1, ?2,2, ?2,3, ?2,4, ?2,5, ?2,`2? ? Normal(?2,?2)
?3 = ??3,1, ?3,2, ?3,3, ?3,4, ?3,5, ?3,6, ?3,`3? ? Normal(?3,?3)
?4 = ??4,1, ?4,`4? ? Normal(?4,?4)
?
???
???
sample ?
??1 = 13 ??1,1 + ?2,1 + ?4,1, ?1,2 + ?2,2 + ?4,2?
??2 = 13 ??1,3 + ?2,3 + ?3,1, ?1,4 + ?2,4 + ?3,2, ?1,5 + ?2,5 + ?3,3, ?1,6 + ?2,6 + ?3,4?
??3 = 12 ??1,7 + ?3,5, ?1,8 + ?3,6, ?1,9 + ?3,7?
?
?
? combine ?
?1 = (exp ??1)
/?N1
i?=1 exp ??1,i?
?2 = (exp ??2)
/?N2
i?=1 exp ??2,i?
?3 = (exp ??3)
/?N3
i?=1 exp ??3,i?
?
????
????
softmax
Figure 2: An example of a shared logistic normal distribution, illustrating Def. 1. N = 4 experts are used to sample
K = 3 multinomials; L1 = 3, L2 = 2, L3 = 2, L4 = 1, `1 = 9, `2 = 6, `3 = 7, `4 = 2, N1 = 2, N2 = 4, and
N3 = 3. This figure is best viewed in color.
Blei and Lafferty (2006) defined correlated topic
models by replacing the Dirichlet in latent Dirich-
let alocation models (Blei et al, 2003) with a LN
distribution. Cohen et al (2008) compared Dirichlet
and LN distributions for learning DMV using em-
pirical Bayes, finding substantial improvements for
English using the latter.
In that work, we obtained improvements even
without specifying exactly which grammar proba-
bilities covaried. While empirical Bayes learning
permits these covariances to be discovered without
supervision, we found that by initializing the covari-
ance to encode beliefs about which grammar prob-
abilities should covary, further improvements were
possible. Specifically, we grouped the Penn Tree-
bank part-of-speech tags into coarse groups based
on the treebank annotation guidelines and biased
the initial covariance matrix for each child distri-
bution ?c(? | ?, ?) so that the probabilities of child
tags from the same coarse group covaried. For ex-
ample, the probability that a past-tense verb (VBD)
has a singular noun (NN) as a right child may be
correlated with the probability that it has a plu-
ral noun (NNS) as a right child. Hence linguistic
knowledge?specifically, a coarse grouping of word
classes?can be encoded in the prior.
A per-distribution LN distribution only permits
probabilities within a multinomial to covary. We
will generalize the LN to permit covariance among
any probabilities in ?, throughout the model. For
example, the probability of a past-tense verb (VBD)
having a noun as a right child might correlate with
the probability that other kinds of verbs (VBZ, VBN,
etc.) have a noun as a right child.
The partitioned logistic normal distribution
(PLN) is a generalization of the LN distribution
that takes the first step towards our goal (Aitchison,
1986). Generating from PLN involves drawing a
random vector from a multivariate normal distribu-
tion, but the logistic transformation is applied to dif-
ferent parts of the vector, leading to sampled multi-
nomial distributions of the required lengths from
different probability simplices. This is in principle
what is required for arbitrary covariance between
grammar probabilities, except that DMV has O(t2)
weights for a part-of-speech vocabulary of size t, re-
quiring a very large multivariate normal distribution
with O(t4) covariance parameters.
77
3.2 Shared Logistic Normal Distributions
To solve this problem, we suggest a refinement of
the class of PLN distributions. Instead of using a
single normal vector for all of the multinomials, we
use several normal vectors, partition each one and
then recombine parts which correspond to the same
multinomial, as a mixture. Next, we apply the lo-
gisitic transformation on the mixed vectors (each of
which is normally distributed as well). Fig. 2 gives
an example of a non-trivial case of using a SLN
distribution, where three multinomials are generated
from four normal experts.
We now formalize this notion. For a natural num-
ber N , we denote by 1:N the set {1, ..., N}. For a
vector in v ? RN and a set I ? 1:N , we denote
by vI to be the vector created from v by using the
coordinates in I . Recall that K is the number of
multinomials in the probabilistic grammar, and Nk
is the number of events in the kth multinomial.
Definition 1. We define a shared logistic nor-
mal distribution with N ?experts? over a collec-
tion of K multinomial distributions. Let ?n ?
Normal(?n,?n) be a set of multivariate normal
variables for n ? 1:N , where the length of ?n
is denoted `n. Let In = {In,j}Lnj=1 be a parti-
tion of 1:`n into Ln sets, such that ?Lnj=1In,j =1:`n and In,j ? In,j? = ? for j 6= j?. Let Jk
for k ? 1:K be a collection of (disjoint) sub-
sets of {In,j | n ? 1:N, j ? 1:`n, |In,j | =
Nk}, such that all sets in Jk are of the same size,
Nk. Let ??k = 1|Jk|
?
In,j?Jk ?n,In,j , and ?k,i =
exp(??k,i)
/?
i? exp(??k,i?) . We then say ? distributes
according to the shared logistic normal distribution
with partition structure S = ({In}Nn=1, {Jk}Kk=1)
and normal experts {(?n,?n)}Nn=1 and denote it by
? ? SLN(?,?, S).
The partitioned LN distribution in Aitchison
(1986) can be formulated as a shared LN distribution
where N = 1. The LN collection used by Cohen et
al. (2008) is the special case where N = K, each
Ln = 1, each `k = Nk, and each Jk = {Ik,1}.
The covariance among arbitrary ?k,i is not defined
directly; it is implied by the definition of the nor-
mal experts ?n,In,j , for each In,j ? Jk. We notethat a SLN can be represented as a PLN by relying
on the distributivity of the covariance operator, and
merging all the partition structure into one (perhaps
sparse) covariance matrix. However, if we are inter-
ested in keeping a factored structure on the covari-
ance matrices which generate the grammar weights,
we cannot represent every SLN as a PLN.
It is convenient to think of each ?i,j as a weight
associated with a unique event?s probability, a cer-
tain outcome of a certain multinomial in the prob-
abilistic grammar. By letting different ?i,j covary
with each other, we loosen the relationships among
?k,j and permit the model?at least in principle?
to learn patterns from the data. Def. 1 also implies
that we multiply several multinomials together in a
product-of-experts style (Hinton, 1999), because the
exponential of a mixture of normals becomes a prod-
uct of (unnormalized) probabilities.
Our extension to the model in Cohen et al (2008)
follows naturally after we have defined the shared
LN distribution. The generative story for this model
is as follows:
1. Generate ? ? SLN(?,?, S), where ? is a col-
lection of vectors ?k, k = 1, ...,K.
2. Generate x and y from p(x,y | ?) (i.e., sample
from the probabilistic grammar).
3.3 Inference
In this work, the partition structure S is known, the
sentences x are observed, the trees y and the gram-
mar weights ? are hidden, and the parameters of the
shared LN distribution ? and ? are learned.2
Our inference algorithm aims to find the poste-
rior over the grammar probabilities ? and the hidden
structures (grammar trees y). To do that, we use
variational approximation techniques (Jordan et al,
1999), which treat the problem of finding the pos-
terior as an optimization problem aimed to find the
best approximation q(?,y) of the posterior p(?,y |
x,?,?, S). The posterior q needs to be constrained
to be within a family of tractable and manageable
distributions, yet rich enough to represent good ap-
proximations of the true posterior. ?Best approx-
imation? is defined as the KL divergence between
q(?,y) and p(?,y | x,?,?, S).
Our variational inference algorithm uses a mean-
field assumption: q(?,y) = q(?)q(y). The distri-
bution q(?) is assumed to be a LN distribution with
2In future work, we might aim to learn S.
78
log p(x | ?,?, S) ?
(?N
n=1 Eq [log p(?k | ?k,?k)]
)
+
(?K
k=1
?Nk
i=1 f?k,i??k,i
)
+H(q)
? ?? ?
B
(3)
f?k,i , ?y q(y)fk,i(x,y) (4)
??k,i , ??Ck,i ? log ??k + 1? 1??k
?Nk
i?=1 exp
(
??Ck,i +
(??Ck,i)2
2
)
(5)
??Ck , 1|Jk|
?
Ir,j?Jk ??r,Ir,j (6)
(??Ck )2 , 1|Jk|2
?
Ir,j?Jk ??
2
r,Ir,j (7)
Figure 3: Variational inference bound. Eq. 3 is the bound itself, using notation defined in Eqs. 4?7 for clarity. Eq. 4
defines expected counts of the grammar events under the variational distribution q(y), calculated using dynamic pro-
gramming. Eq. 5 describes the weights for the weighted grammar defined by q(y). Eq. 6 and Eq. 7 describe the mean
and the variance, respectively, for the multivariate normal eventually used with the weighted grammar. These values
are based on the parameterization of q(?) by ??i,j and ??2i,j . An additional set of variational parameters is ??k, which
helps resolve the non-conjugacy of the LN distribution through a first order Taylor approximation.
all off-diagonal covariances fixed at zero (i.e., the
variational parameters consist of a single mean ??k,i
and a single variance ??2k,i for each ?k,i). There is
an additional variational parameter, ??k per multino-
mial, which is the result of an additional variational
approximation because of the lack of conjugacy of
the LN distribution to the multinomial distribution.
The distribution q(y) is assumed to be defined by a
DMV with unnormalized probabilities ??.
Inference optimizes the bound B given in Fig. 3
(Eq. 3) with respect to the variational parameters.
Our variational inference algorithm is derived simi-
larly to that of Cohen et al (2008). Because we wish
to learn the values of? and?, we embed variational
inference as the E step within a variational EM algo-
rithm, shown schematically in Fig. 4. In our exper-
iments, we use this variational EM algorithm on a
training set, and then use the normal experts? means
to get a point estimate for ?, the grammar weights.
This is called empirical Bayesian estimation. Our
approach differs from maximum a posteriori (MAP)
estimation, since we re-estimate the parameters of
the normal experts. Exact MAP estimation is prob-
ably not feasible; a variational algorithm like ours
might be applied, though better performance is ex-
pected from adjusting the SLN to fit the data.
4 Experiments
Our experiments involve data from two treebanks:
the Wall Street Journal Penn treebank (Marcus et
al., 1993) and the Chinese treebank (Xue et al,
2004). In both cases, following standard practice,
sentences were stripped of words and punctuation,
leaving part-of-speech tags for the unsupervised in-
duction of dependency structure. For English, we
train on ?2?21, tune on ?22 (without using annotated
data), and report final results on ?23. For Chinese,
we train on ?1?270, use ?301?1151 for development
and report testing results on ?271?300.3
To evaluate performance, we report the fraction
of words whose predicted parent matches the gold
standard corpus. This performance measure is also
known as attachment accuracy. We considered two
parsing methods after extracting a point estimate
for the grammar: the most probable ?Viterbi? parse
(argmaxy p(y | x,?)) and the minimum Bayes risk
(MBR) parse (argminy Ep(y?|x,?)[`(y;x,y?)]) with
dependency attachment error as the loss function
(Goodman, 1996). Performance with MBR parsing
is consistently higher than its Viterbi counterpart, so
we report only performance with MBR parsing.
4.1 Nouns, Verbs, and Adjectives
In this paper, we use a few simple heuristics to de-
cide which partition structure S to use. Our heuris-
3Unsupervised training for these datasets can be costly,
and requires iteratively running a cubic-time inside-outside dy-
namic programming algorithm, so we follow Klein and Man-
ning (2004) in restricting the training set to sentences of ten or
fewer words in length. Short sentences are also less structurally
ambiguous and may therefore be easier to learn from.
79
Input: initial parameters ?(0), ?(0), partition
structure S, observed data x, number of
iterations T
Output: learned parameters ?, ?
t? 1 ;
while t ? T do
E-step (for ` = 1, ...,M ) do: repeat
optimize B w.r.t. ??`,(t)r , r = 1, ..., N ;
optimize B w.r.t. ??`,(t)r , r = 1, ..., N ;
update ??`,(t)r , r = 1, ..., N ;
update ??`,(t)r , r = 1, ..., N ;
compute counts f? `,(t)r , r = 1, ..., N ;until convergence of B ;
M-step: optimize B w.r.t. ?(t) and ?(t);
t? t+ 1;
end
return ?(T ), ?(T )
Figure 4: Main details of the variational inference EM
algorithm with empirical Bayes estimation of ? and ?.
B is the bound defined in Fig. 3 (Eq. 3). N is the number
of normal experts for the SLN distribution defining the
prior. M is the number of training examples. The full
algorithm is given in Cohen and Smith (2009).
tics rely mainly on the centrality of content words:
nouns, verbs, and adjectives. For example, in the En-
glish treebank, the most common attachment errors
(with the LN prior from Cohen et al, 2008) happen
with a noun (25.9%) or a verb (16.9%) parent. In
the Chinese treebank, the most common attachment
errors happen with noun (36.0%) and verb (21.2%)
parents as well. The errors being governed by such
attachments are the direct result of nouns and verbs
being the most common parents in these data sets.
Following this observation, we compare four dif-
ferent settings in our experiments (all SLN settings
include one normal expert for each multinomial on
its own, equivalent to the regular LN setting from
Cohen et al):
? TIEV: We add normal experts that tie all proba-
bilities corresponding to a verbal parent (any par-
ent, using the coarse tags of Cohen et al, 2008).
Let V be the set of part-of-speech tags which be-
long to the verb category. For each direction D
(left or right), the set of multinomials of the form
?c(? | v,D), for v ? V , all share a normal expert.
For each direction D and each boolean value B
of the predicate firsty(?), the set of multinomials
?s(? | x,D , v), for v ? V share a normal expert.
? TIEN: This is the same as TIEV, only for nominal
parents.
? TIEV&N: Tie both verbs and nouns (in separate
partitions). This is equivalent to taking the union
of the partition structures of the above two set-
tings.
? TIEA: This is the same as TIEV, only for adjecti-
val parents.
Since inference for a model with parameter tying
can be computationally intensive, we first run the in-
ference algorithm without parameter tying, and then
add parameter tying to the rest of the inference algo-
rithm?s execution until convergence.
Initialization is important for the inference al-
gorithm, because the variational bound is a non-
concave function. For the expected values of the
normal experts, we use the initializer from Klein and
Manning (2004). For the covariance matrices, we
follow the setting in Cohen et al (2008) in our ex-
periments also described in ?3.1. For each treebank,
we divide the tags into twelve disjoint tag families.4
The covariance matrices for all dependency distri-
butions were initialized with 1 on the diagonal, 0.5
between tags which belong to the same family, and
0 otherwise. This initializer has been shown to be
more successful than an identity covariance matrix.
4.2 Monolingual Experiments
We begin our experiments with a monolingual set-
ting, where we learn grammars for English and Chi-
nese (separately) using the settings described above.
The attachment accuracy for this set of experi-
ments is described in Table 1. The baselines include
right attachment (where each word is attached to the
word to its right), MLE via EM (Klein and Man-
ning, 2004), and empirical Bayes with Dirichlet and
LN priors (Cohen et al, 2008). We also include a
?ceiling? (DMV trained using supervised MLE from
the training sentences? trees). For English, we see
that tying nouns, verbs or adjectives improves per-
formance compared to the LN baseline. Tying both
nouns and verbs improves performance a bit more.
4These are simply coarser tags: adjective, adverb, conjunc-
tion, foreign word, interjection, noun, number, particle, prepo-
sition, pronoun, proper noun, verb.
80
attachment acc. (%)
? 10 ? 20 all
En
gli
sh
Attach-Right 38.4 33.4 31.7
EM (K&M, 2004) 46.1 39.9 35.9
Dirichlet 46.1 40.6 36.9
LN (CG&S, 2008) 59.4 45.9 40.5
SLN, TIEV 60.2 46.2 40.0
SLN, TIEN 60.2 46.7 40.9
SLN, TIEV&N 61.3 47.4 41.4
SLN, TIEA 59.9 45.8 40.9
Biling. SLN, TIEV ?61.6 47.6 41.7
Biling. SLN, TIEN ?61.8 48.1 ?42.1
Biling. SLN, TIEV&N 62.0 ?48.0 42.2
Biling. SLN, TIEA 61.3 47.6 41.7
Supervised MLE 84.5 74.9 68.8
Ch
ine
se
Attach-Right 34.9 34.6 34.6
EM (K&M, 2004) 38.3 36.1 32.7
Dirichlet 38.3 35.9 32.4
LN 50.1 40.5 35.8
SLN, TIEV ?51.9 42.0 35.8
SLN, TIEN 43.0 38.4 33.7
SLN, TIEV&N 45.0 39.2 34.2
SLN, TIEA 47.4 40.4 35.2
Biling. SLN, TIEV ?51.9 42.0 35.8
Biling. SLN, TIEN 48.0 38.9 33.8
Biling. SLN, TIEV&N ?51.5 ?41.7 35.3
Biling. SLN, TIEA 52.0 41.3 35.2
Supervised MLE 84.3 66.1 57.6
Table 1: Attachment accuracy of different models, on test
data from the Penn Treebank and the Chinese Treebank
of varying levels of difficulty imposed through a length
filter. Attach-Right attaches each word to the word on
its right and the last word to $. Bold marks best overall
accuracy per length bound, and ? marks figures that are
not significantly worse (binomial sign test, p < 0.05).
4.3 Bilingual Experiments
Leveraging information from one language for the
task of disambiguating another language has re-
ceived considerable attention (Dagan, 1991; Smith
and Smith, 2004; Snyder and Barzilay, 2008; Bur-
kett and Klein, 2008). Usually such a setting re-
quires a parallel corpus or other annotated data that
ties between those two languages.5
Our bilingual experiments use the English and
Chinese treebanks, which are not parallel corpora,
to train parsers for both languages jointly. Shar-
5Haghighi et al (2008) presented a technique to learn bilin-
gual lexicons from two non-parallel monolingual corpora.
ing information between those two models is done
by softly tying grammar weights in the two hidden
grammars.
We first merge the models for English and Chi-
nese by taking a union of the multinomial fami-
lies of each and the corresponding prior parame-
ters. We then add a normal expert that ties be-
tween the parts of speech in the respective parti-
tion structures for both grammars together. Parts
of speech are matched through the single coarse
tagset (footnote 4). For example, with TIEV, let
V = V Eng ?V Chi be the set of part-of-speech tags
which belong to the verb category for either tree-
bank. Then, we tie parameters for all part-of-speech
tags in V . We tested this joint model for each of
TIEV, TIEN, TIEV&N, and TIEA. After running
the inference algorithm which learns the two mod-
els jointly, we use unseen data to test each learned
model separately.
Table 1 includes the results for these experiments.
The performance on English improved significantly
in the bilingual setting, achieving highest perfor-
mance with TIEV&N. Performance with Chinese is
also the highest in the bilingual setting, with TIEA
and TIEV&N.
5 Future Work
In future work we plan to lexicalize the model, in-
cluding a Bayesian grammar prior that accounts for
the syntactic patterns ofwords. Nonparametric mod-
els (Teh, 2006) may be appropriate. We also believe
that Bayesian discovery of cross-linguistic patterns
is an exciting topic worthy of further exploration.
6 Conclusion
We described a Bayesian model that allows soft pa-
rameter tying among any weights in a probabilistic
grammar. We used this model to improve unsuper-
vised parsing accuracy on two different languages,
English and Chinese, achieving state-of-the-art re-
sults. We also showed how our model can be effec-
tively used to simultaneously learn grammars in two
languages from non-parallel multilingual data.
Acknowledgments
This research was supported by NSF IIS-0836431. The
authors thank the anonymous reviewers and Sylvia Reb-
holz for helpful comments.
81
References
J. Aitchison. 1986. The Statistical Analysis of Composi-
tional Data. Chapman and Hall, London.
D. M. Blei and J. D. Lafferty. 2006. Correlated topic
models. In Proc. of NIPS.
D. M. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In Proc. of EMNLP.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. of ACL.
S. B. Cohen and N. A. Smith. 2009. Inference for proba-
bilistic grammars with shared logistic normal distribu-
tions. Technical report, Carnegie Mellon University.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic
normal priors for unsupervised probabilistic grammar
induction. In NIPS.
M. Collins. 2003. Head-driven statistical models for nat-
ural language processing. Computational Linguistics,
29:589?637.
I. Dagan. 1991. Two languages are more informative
than one. In Proc. of ACL.
J. Eisner. 2002. Transformational priors over grammars.
In Proc. of EMNLP.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In Proc. of ACL.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proc. of ACL.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL-
HLT.
G. E. Hinton. 1999. Products of experts. In Proc. of
ICANN.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying com-
positional nonparameteric Bayesian models. In NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. EMNLP-CoNLL.
M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37(2):183?
233.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Proc. of
ICGI.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proc. of EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proc. of EMNLP, pages 49?56.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proc. of ACL.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
COLING-ACL.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous grammar
for question answering. In Proc. of EMNLP.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comp. Ling., 23(3):377?404.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
10(4):1?30.
82
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 1?4,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Variational Inference for Grammar Induction with Prior Knowledge
Shay B. Cohen and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,nasmith}@cs.cmu.edu
Abstract
Variational EM has become a popular
technique in probabilistic NLP with hid-
den variables. Commonly, for computa-
tional tractability, we make strong inde-
pendence assumptions, such as the mean-
field assumption, in approximating pos-
terior distributions over hidden variables.
We show how a looser restriction on the
approximate posterior, requiring it to be a
mixture, can help inject prior knowledge
to exploit soft constraints during the varia-
tional E-step.
1 Introduction
Learning natural language in an unsupervised way
commonly involves the expectation-maximization
(EM) algorithm to optimize the parameters of a
generative model, often a probabilistic grammar
(Pereira and Schabes, 1992). Later approaches in-
clude variational EM in a Bayesian setting (Beal
and Gharamani, 2003), which has been shown to
obtain even better results for various natural lan-
guage tasks over EM (e.g., Cohen et al, 2008).
Variational EM usually makes the mean-field
assumption, factoring the posterior over hidden
variables into independent distributions. Bishop et
al. (1998) showed how to use a less strict assump-
tion: a mixture of factorized distributions.
In other work, soft or hard constraints on the
posterior during the E-step have been explored
in order to improve performance. For example,
Smith and Eisner (2006) have penalized the ap-
proximate posterior over dependency structures
in a natural language grammar induction task to
avoid long range dependencies between words.
Grac?a et al (2007) added linear constraints on ex-
pected values of features of the hidden variables in
an alignment task.
In this paper, we use posterior mixtures to inject
bias or prior knowledge into a Bayesian model.
We show that empirically, injecting prior knowl-
edge improves performance on an unsupervised
Chinese grammar induction task.
2 Variational Mixtures with Constraints
Our EM variant encodes prior knowledge in an ap-
proximate posterior by constraining it to be from
a mixture family of distributions. We will use x to
denote observable random variables, y to denote
hidden structure, and ? to denote the to-be-learned
parameters of the model (coming from a subset of
R
`
for some `). ? will denote the parameters of
a prior over ?. The mean-field assumption in the
Bayesian setting assumes that the posterior has a
factored form:
q(?,y) = q(?)q(y) (1)
Traditionally, variational inference with the mean-
field assumption alternates between an E-step
which optimizes q(y) and then an M-step which
optimizes q(?).
1
The mean-field assumption
makes inference feasible, at the expense of op-
timizing a looser lower bound on the likelihood
(Bishop, 2006). The lower bound that the algo-
rithm optimizes is the following:
F (q(?,y),?) = E
q(?,y)
[log p(x,y,? | ?)]+H(q)
(2)
where H(q) denotes the entropy of distribution q.
We focus on changing the E-step and as a result,
changing the underlying bound, F (q(?,y),?).
Similarly to Bishop et al (1998), instead of mak-
ing the strict mean-field assumption, we assume
that the variational model is a mixture. One com-
ponent of the mixture might take the traditional
form, but others will be used to encourage certain
1
This optimization can be nested inside another EM al-
gorithm that optimizes ?; this is our approach. q(?) is tra-
ditionally conjugate to the likelihood for computational rea-
sons, but our method is not limited to that kind of prior, as
seen in the experiments.
1
tendencies considered a priori to be appropriate.
Denoting the probability simplex of dimension r
4
r
= {??
1
, ..., ?
r
? ? R
r
: ?
i
? 0,
?
r
i=1
?
i
=
1}, we require that:
q(?,y | ?) =
?
r
i=1
?
i
q
i
(y)q
i
(?) (3)
for ? ? 4
r
. Q
i
will denote the family of distri-
butions for the ith mixture component, and Q(4
r
)
will denote the family implied by the mixture of
Q
1
, . . . ,Q
r
where the mixture coefficients ? ?
4
r
. ? comprise r additional variational param-
eters, in addition to parameters for each q
i
(y) and
q
i
(?).
When one of the mixture components q
i
is suf-
ficiently expressive, ? will tend toward a degener-
ate solution. In order to force all mixture compo-
nents to play a role?even at the expense of the
tightness of the variational bound?we will im-
pose hard constraints on ?: ? ?
?
4
r
? 4
r
. In
our experiments (?3),
?
4
r
will be mostly a line seg-
ment corresponding to two mixture coefficients.
The role of the variational EM algorithm is to
optimize the variational bound in Eq. 2 with re-
spect to q(y), q(?), and ?. Keeping this intention
in mind, we can replace the E-step and M-step in
the original variational EM algorithm with 2r + 1
coordinate ascent steps, for 1 ? i ? r:
E-step: For each i ? {1, ..., r}, optimize the
bound given ? and q
i
?
(y)|
i
?
?{1,...,r}\{i}
and
q
i
?
(?)|
i
?
?{1,...,r}
by selecting a new distribution
q
i
(y).
M-step: For each i ? {1, ..., r}, optimize the
bound given ? and q
i
?
(?)|
i
?
?{1,...,r}\{i}
and
q
i
?
(y)|
i
?
?{1,...,r}
by selecting a new distribution
q
i
(?).
C-step: Optimize the bound by selecting a new set
of coefficients ? ?
?
4
r
in order to optimize the
bound with respect to the mixture coefficients.
We call the revised algorithm constrained mix-
ture variational EM.
For a distribution r(h), we denote by KL(Q
i
?r)
the following:
KL(Q
i
?r) = min
q?Q
i
KL(q(h)?r)) (4)
where KL(???) denotes the Kullback-Leibler di-
vergence.
The next proposition, which is based on a result
in Grac?a et al (2007), gives an intuition of how
modifying the variational EM algorithm with Q =
Q(
?
4
r
) affects the solution:
Proposition 1. Constrained mixture variational
EM finds local maxima for a function G(q,?)
such that
log p(x | ?)? min
??
?
4
r
L(?,?) ? G(q,?) ? log p(x | ?)
(5)
where L(?,?) =
r
?
i=1
?
i
KL(Q
i
?p(?,y | x,?)).
We can understand mixture variational EM as
penalizing the likelihood with a term bounded by
a linear function of the ?, minimized over
?
4
r
. We
will exploit that bound in ?2.2 for computational
tractability.
2.1 Simplex Annealing
The variational EM algorithm still identifies only
local maxima. Different proposals have been for
pushing EM toward a global maximum. In many
cases, these methods are based on choosing dif-
ferent initializations for the EM algorithm (e.g.,
repeated random initializations or a single care-
fully designed initializer) such that it eventually
gets closer to a global maximum.
We follow the idea of annealing proposed in
Rose et al (1990) and Smith and Eisner (2006) for
the ? by gradually loosening hard constraints on ?
as the variational EM algorithm proceeds. We de-
fine a sequence of
?
4
r
(t) for t = 0, 1, ... such that
?
4
r
(t) ?
?
4
r
(t+1). First, we have the inequality:
KL(Q(
?
4
r
(t))?p(?,y | x,?) (6)
? KL(Q(
?
4
r
(t + 1))?p(?,y | x,?))
We say that the annealing schedule is ? -separated
if we have for any ?:
KL(Q(
?
4
r
(t))?p(?,y | x,?)) (7)
? KL(Q(
?
4
r
(t + 1))?p(?,y | x,?)) ?
?
2
(t+1)
? -separation requires consecutive families
Q(
?
4
r
(t)) and Q(
?
4
r
(t + 1)) to be similar.
Proposition 1 stated the bound we optimize,
which penalizes the likelihood by subtracting a
positive KL divergence from it. With the ? -
separation condition we can show that even though
we penalize likelihood, the variational EM algo-
rithm will still increase likelihood by a certain
amount. Full details are omitted for space and can
be found in ?).
2
Input: initial parameters ?
(0)
, observed data x,
annealing schedule
?
4
r
: N? 2
4
r
Output: learned parameters ? and approximate
posterior q(?,y)
t? 1;
repeat
E-step: repeat
E-step: forall i ? [r] do: q
(t+1)
i
(y)? argmax
q(y)?Q
i
F
?
(
P
j 6=i
?
j
q
(t)
i
(?)q(y) + ?
i
q
(t)
i
q(y),?
(t)
)
M-step: forall i ? [r] do: q
(t+1)
i
(?)? argmax
q(?)?Q
i
F
?
(
P
j 6=i
?
j
q(?)q
(t)
i
(y) + ?
i
q
(t)
i
q(y),?
(t)
)
C-step: ?
(t+1)
?
argmax
??
?
4
r
(t)
F
?
(
P
r
j=1
?
j
q
(t)
i
(?)q
(t)
i
(y),?
(t)
)
until convergence ;
M-step: ?
(t+1)
?
argmax
?
F
?
(
P
r
i=1
?
i
q
(t+1)
i
(?)q
(t+1)
i
(y),?)
t? t + 1;
until convergence ;
return ?
(t)
,
P
r
i=1
?
i
q
(t)
i
(?)q
(t)
i
(y)
Figure 1: The constrained variational mixture EM algorithm.
[n] denotes {1, ..., n}.
2.2 Tractability
We now turn to further alterations of the bound in
Eq. 2 to make it more tractable. The main problem
is the entropy term which is not easy to compute,
because it includes a log term over a mixture of
distributions from Q
i
. We require the distributions
in Q
i
to factorize over the hidden structure y, but
this only helps with the first term in Eq. 2.
We note that because the entropy function is
convex, we can get a lower bound on H(q):
H(q) ?
?
r
i=1
?
i
H(q
i
) =
?
r
i=1
?
i
H(q
i
(?,y))
Substituting the modified entropy term into
Eq. 2 still yields a lower bound on the likeli-
hood. This change makes the E-step tractable,
because each distribution q
i
(y) can be computed
separately by optimizing a bound which depends
only on the variational parameters in that distribu-
tion. In fact, the bound on the left hand side in
Proposition 1 becomes the function that we opti-
mize instead of G(q,?).
Without proper constraints, the ? update can be
intractable as well. It requires maximizing a lin-
ear objective (in ?) while constraining the ? to
be from a particular subspace of the probability
simplex,
?
4
r
(t). To solve this issue, we require
that
?
4
r
(t) is polyhedral, making it possible to ap-
ply linear programming (Boyd and Vandenberghe,
2004).
The bound we optimize is:
2
F
?
(
r
?
i=1
?
i
q
i
(?,y),?
)
(8)
=
r
?
i=1
?
i
(
E
q
i
(?,y)
[log p(?,y,x | m)] + H(q
i
(?,y))
)
with ? ?
?
4
r
(t
final
) and (q
i
(?,y)) ? Q
i
. The
algorithm for optimizing this bound is in Fig. 1,
which includes an extra M-step to optimize? (see
extended report).
3 Experiments
We tested our method on the unsupervised learn-
ing problem of dependency grammar induction.
For the generative model, we used the dependency
model with valence as it appears in Klein andMan-
ning (2004). We used the data from the Chi-
nese treebank (Xue et al, 2004). Following stan-
dard practice, sentences were stripped of words
and punctuation, leaving part-of-speech tags for
the unsupervised induction of dependency struc-
ture, and sentences of length more than 10 were
removed from the set. We experimented with
a Dirichlet prior over the parameters and logis-
tic normal priors over the parameters, and found
the latter to still be favorable with our method, as
in Cohen et al (2008). We therefore report results
with our method only for the logistic normal prior.
We do inference on sections 1?270 and 301?1151
of CTB10 (4,909 sentences) by running the EM al-
gorithm for 20 iterations, for which all algorithms
have their variational bound converge.
To evaluate performance, we report the fraction
of words whose predicted parent matches the gold
standard (attachment accuracy). For parsing, we
use the minimum Bayes risk parse.
Our mixture componentsQ
i
are based on simple
linguistic tendencies of Chinese syntax. These ob-
servations include the tendency of dependencies to
(a) emanate from the right of the current position
and (b) connect words which are nearby (in string
distance). We experiment with six mixture com-
ponents: (1) RIGHTATTACH: Each word?s parent
is to the word?s right. The root, therefore, is al-
ways the rightmost word; (2) ALLRIGHT: The
rightmost word is the parent of all positions in the
sentence (there is only one such tree); (3) LEFT-
CHAIN: The tree forms a chain, such that each
2
This is a less tight bound than the one in Bishop et al
(1998), but it is easier to handle computationally.
3
l
e
a
r
n
i
n
g
s
e
t
t
i
n
g
LEFTCHAIN 34.9
vanilla EM 38.3
LN, mean-field 48.9
This paper: I II III
RIGHTATTACH 49.1 47.1 49.8
ALLRIGHT 49.4 49.4 48.4
LEFTCHAIN 47.9 46.5 49.9
VERBASROOT 50.5 50.2 49.4
NOUNSEQUENCE 48.9 48.9 49.9
SHORTDEP 49.5 48.4 48.4
RA+VAR+SD 50.5 50.6 50.1
Table 1: Results (attachment accuracy). The baselines are
LEFTCHAIN as a parsing model (attaches each word to the
word on its right), non-Bayesian EM, and mean-field vari-
ational EM without any constraints. These are compared
against the six mixture components mentioned in the text. (I)
corresponds to simplex annealing experiments (?
(0)
1
= 0.85);
(II?III) correspond to fixed values, 0.85 and 0.95, for the
mixture coefficients. With the last row, ?
2
to ?
4
are always
(1? ?
1
)/3. Boldface denotes the best result in each row.
word is governed by the word to its right; (4) VER-
BASROOT: Only verbs can attach to the wall node
$; (5) NOUNSEQUENCE: Every sequence of nNN
(nouns) is assumed to be a noun phrase, hence the
first n?1 NNs are attached to the last NN; and (6)
SHORTDEP: Allow only dependencies of length
four or less. This is a strict model reminiscent
of the successful application of structural bias to
grammar induction (Smith and Eisner, 2006).
These components are added to a variational
DMV model without the sum-to-1 constraint on
?. This complements variational techniques which
state that the optimal solution during the E-step
for the mean-field variational EM algorithm is a
weighted grammar of the same form of p(x,y | ?)
(DMV in our case). Using the mixture compo-
nents this way has the effect of smoothing the esti-
mated grammar event counts during the E-step, in
the direction of some prior expectations.
Let ?
1
correspond to the component of the orig-
inal DMV model, and let ?
2
correspond to one of
the components from the above list. Variational
techniques show that if we let ?
1
obtain the value
1, then the optimal solution will be ?
1
= 1 and
?
2
= 0. We therefore restrict ?
1
to be smaller than
1. More specifically, we use an annealing process
which starts by limiting ?
1
to be ? s = 0.85 (and
hence limits ?
2
to be ? 0.15) and increases s at
each step by 1% until s reaches 0.95. In addition,
we also ran the algorithm with ?
1
fixed at 0.85 and
?
1
fixed at 0.95 to check the effectiveness of an-
nealing on the simplex.
Table 1 describes the results of our experi-
ments. In general, using additional mixture com-
ponents has a clear advantage over the mean-field
assumption. The best result with a single mix-
ture is achieved with annealing, and the VERBAS-
ROOT component. A combination of the mix-
tures (RIGHTATTACH) together with VERBAS-
ROOT and SHORTDEP led to an additional im-
provement, implying that proper selection of sev-
eral mixture components together can achieve a
performance gain.
4 Conclusion
We described a variational EM algorithm that uses
a mixture model for the variational model. We
refined the algorithm with an annealing mecha-
nism to avoid local maxima. We demonstrated
the effectiveness of the algorithm on a dependency
grammar induction task. Our results show that
with a good choice of mixture components and
annealing schedule, we achieve improvements for
this task over mean-field variational inference.
References
M. J. Beal and Z. Gharamani. 2003. The variational
Bayesian EM algorithm for incomplete data: with appli-
cation to scoring graphical model structures. In Proc. of
Bayesian Statistics.
C. Bishop, N. Lawrence, T. S. Jaakkola, and M. I. Jordan.
1998. Approximating posterior distributions in belief net-
works using mixtures. In Advances in NIPS.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
S. Boyd and L. Vandenberghe. 2004. Convex Optimization.
Cambridge Press.
S. B. Cohen and N. A. Smith. 2009. Variational inference
with prior knowledge. Technical report, Carnegie Mellon
University.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logis-
tic normal priors for unsupervised probabilistic grammar
induction. In Advances in NIPS.
J. V. Grac?a, K. Ganchev, and B. Taskar. 2007. Expectation
maximization and posterior constraints. In Advances in
NIPS.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. of ACL.
F. C. N. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In Proc. of ACL.
K. Rose, E. Gurewitz, and G. C. Fox. 1990. Statistical me-
chanics and phrase transitions in clustering. Physical Re-
view Letters, 65(8):945?948.
N. A. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In Proc. of
COLING-ACL.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn
Chinese Treebank: Phrase structure annotation of a large
corpus. Natural Language Engineering, 10(4):1?30.
4
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50?61,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Structure Prediction
with Non-Parallel Multilingual Guidance
Shay B. Cohen Dipanjan Das Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,dipanjan,nasmith}@cs.cmu.edu
Abstract
We describe a method for prediction of lin-
guistic structure in a language for which only
unlabeled data is available, using annotated
data from a set of one or more helper lan-
guages. Our approach is based on a model
that locally mixes between supervised mod-
els from the helper languages. Parallel data
is not used, allowing the technique to be ap-
plied even in domains where human-translated
texts are unavailable. We obtain state-of-the-
art performance for two tasks of structure pre-
diction: unsupervised part-of-speech tagging
and unsupervised dependency parsing.
1 Introduction
A major focus of recent NLP research has involved
unsupervised learning of structure such as POS
tag sequences and parse trees (Klein and Manning,
2004; Johnson et al, 2007; Berg-Kirkpatrick et al,
2010; Cohen and Smith, 2010, inter alia). In its
purest form, such research has improved our un-
derstanding of unsupervised learning practically and
formally, and has led to a wide range of new algo-
rithmic ideas. Another strain of research has sought
to exploit resources and tools in some languages (es-
pecially English) to construct similar resources and
tools for other languages, through heuristic ?projec-
tion? (Yarowsky and Ngai, 2001; Xi and Hwa, 2005)
or constraints in learning (Burkett and Klein, 2008;
Smith and Eisner, 2009; Das and Petrov, 2011; Mc-
Donald et al, 2011) or inference (Smith and Smith,
2004). Joint unsupervised learning (Snyder and
Barzilay, 2008; Naseem et al, 2009; Snyder et al,
2009) is yet another research direction that seeks to
learn models for many languages at once, exploiting
linguistic universals and language similarity. The
driving force behind all of this work has been the
hope of building NLP tools for languages that lack
annotated resources.1
In this paper, we present an approach to using
annotated data from one or more languages (helper
languages) to learn models for another language that
lacks annotated data (the target language). Unlike
the previous work mentioned above, our framework
does not rely on parallel data in any form. This is
advantageous because parallel text exists only in a
few text domains (e.g., religious texts, parliamentary
proceedings, and news).
We focus on generative probabilistic models pa-
rameterized by multinomial distributions. We be-
gin with supervised maximum likelihood estimates
for models of the helper languages. In the second
stage, we learn a model for the target language using
unannotated data, maximizing likelihood over inter-
polations of the helper language models? distribu-
tions. The tying is performed at the parameter level,
through coarse, nearly-universal syntactic categories
(POS tags). The resulting model is then used to ini-
tialize learning of the target language?s model using
standard unsupervised parameter estimation.
Some previous multilingual research, such as
Bayesian parameter tying across languages (Co-
hen and Smith, 2009) or models of parameter
1Although the stated objective is often to build systems for
resource-poor languages and domains, for evaluation purposes,
annotated treebank test data figure prominently in this research
(including in this paper).
50
drift down phylogenetic trees (Berg-Kirkpatrick and
Klein, 2010) is comparable, but the practical as-
sumption of supervised helper languages is new to
this work. Naseem et al (2010) used universal
syntactic categories and rules to improve grammar
induction, but their model required expert hand-
written rules as constraints.
Herein, we specifically focus on two problems
in linguistic structure prediction: unsupervised POS
tagging and unsupervised dependency grammar in-
duction. Our experiments demonstrate that the pre-
sented method outperforms strong state-of-the-art
unsupervised baselines for both tasks. Our approach
can be applied to other problems in which a sub-
set of the model parameters can be linked across
languages. We also experiment with unsupervised
learning of dependency structures from words, by
combining our tagger and parser. Our results show
that combining our tagger and parser with joint
inference outperforms pipeline inference, and, in
several cases, even outperforms models built using
gold-standard part-of-speech tags.
2 Overview
For each language `, we assume the presence of a
set of fine-grained POS tags F`, used to annotate the
language?s treebank. Furthermore, we assume that
there is a set of universal, coarse-grained POS tags
C such that, for every language `, there is a determin-
istic mapping from fine-grained to coarse-grained
tags, ?` : F` ? C. Our approach can be summa-
rized using the following steps for a given task:
1. Select a set of L helper languages for which there
exists annotated data ?D1, . . . ,DL?. Here, we use
treebanks in these languages.
2. For all ` ? {1, . . . , L}, convert the examples in
D` by applying ?` to every POS tag in the data,
resulting in D?`. Estimate the parameters of a
probabilistic model using D?`. In this work, such
models are generative probabilistic models based
on multinomial distributions,2 including an HMM
and the dependency model with valence (DMV)
of Klein and Manning (2004). Denote the subset
of parameters that are unlexicalized by ?(`). (Lex-
icalized parameters will be denoted ?(`).)
2In ?4 we also consider a feature-based parametrization.
3. For the target language, define the set of valid un-
lexicalized parameters
? =
{
?
??????k =
L?
`=1
?`,k?(`)k ,
L?
`=1
?`,k = 1,? ? 0
}
,
(1)
for each group of parameters k, and maximize
likelihood over that set, using the target-language
unannotated data U . Because the syntactic cate-
gories referenced by each ?(`) and all models in ?
are in C, the models will be in the same parametric
family. (Figure 1 gives a graphical interpretation
of ?.) Let the resulting model be ?.
4. Transform ? by expanding the coarse-grained
syntactic categories into the target language?s
fine-grained categories. Use the resulting model
to initialize parameter estimation, this time over
fine-grained tags, again using the unannotated
target-language data U . Initialize lexicalized pa-
rameters ? for the target language using standard
methods (e.g., uniform initialization with random
symmetry breaking).
The main idea in the approach is to estimate a
certain model family for one language, while using
supervised models from other languages. The link
between the languages is achieved through coarse-
grained categories, which are now now common-
place (and arguably central to any theory of natural
language syntax). A key novel contribution is the
use of helper languages for initialization, and of un-
supervised learning to learn the contribution of each
helper language to that initialization (step 3). Addi-
tional treatment is required in expanding the coarse-
grained model to the fine-grained one (step 4).
3 Interpolated Multilingual Probabilistic
Context-Free Grammars
Our focus in this paper is on models that consist
of multinomial distributions that have relationships
between them through a generative process such as
a probabilistic context-free grammar (PCFG). More
specifically, we assume that we have a model defin-
ing a probability distribution over observed surface
forms x and derivations y parametrized by ?:
51
(0,1,0)
(0,0,1) (1,0,0)
English
Czech
German
Italian
Figure 1: A simple case of interpolation within the 3-
event probability simplex. The shaded area corresponds
to a convex hull inside the probability simplex, indicating
a mixture of the parameters of the four languages shown
in the figure.
p(x,y | ?) =
K?
k=1
Nk?
i=1
?fk,i(x,y)k,i (2)
= exp
K?
k=1
Nk?
i=1
fk,i(x,y) log ?k,i (3)
where fk,i is a function that ?counts? the number
of times the kth distribution?s ith event occurs in
the derivation. The parameters ? are a collection
of K multinomials ??1, . . . ,?K?, the kth of which
includes Nk events. Letting ?k = ??k,1, . . . , ?k,Nk?,
each ?k,i is a probability, such that ?k,?i, ?k,i ? 0
and ?k,?Nki=1 ?k,i = 1.
3.1 Multilingual Interpolation
Our framework places additional, temporary con-
straints on the parameters ?. More specifically, we
assume that we have L existing, parameter estimates
for the multinomial families from Eq. 3. Each such
estimate ?(`), for 1 ? ` ? L, corresponds to a the
maximum likelihood estimate based on annotated
data for the `th helper language. Then, to create a
model for new language, we define a new set of pa-
rameters ? as:
?k,i =
L?
`=1
?`,k?(`)k,i , (4)
where ? is the set of coefficients that we will now
be interested in estimating (instead of directly esti-
mating ?). Note that for each k,?L`=1 ?`,k = 1 and
?`,k ? 0.
3.2 Grammatical Interpretation
We now give an interpretation of our approach relat-
ing it to PCFGs. We assume familiarity with PCFGs.
For a PCFG ?G,?? we denote the set of nontermi-
nal symbols by N , the set of terminal symbols by
?, and the set of rewrite rules for each nonterminal
A ? N by R(A). Each r ? R(A) has the form
A ? ? where ? ? (N ? ?)?. In addition, there is
a probability attached to each rule ?A?? such that
?A ? N ,??:(A??)?R(A) ?A?? = 1. A PCFG can
be framed as a model using Eq. 3, where ? corre-
spond to K = |N | multinomial distributions, where
each distribution attaches probabilities to rules with
a specific left hand symbol.
We assume that the model we are trying to
estimate (over coarse part-of-speech tags) can be
framed as a PCFG ?G,??. This is indeed the case
for part-of-speech tagging and dependency grammar
induction we experiment with in ?6. In that case,
our approach can be framed for PCFGs as follow-
ing. We assume that there exists L set of parameters
for this PCFG ?(1), . . . ,?(L), each corresponding to
a helper language. We then create a new PCFG G?
with parameters ?? and ? as follows:
1. G? contains all nonterminal and terminal symbols
inG, and none of the rules inG.
2. For each nonterminal A in G, we create a new
nonterminal aA,` for ` ? {1, . . . , L}.
3. For each nonterminal A in G, we create rules
A ? aA,` for ` ? {1, . . . , L} which have proba-
bilities ?A?aA,` .
4. For each rule A? ? inG, we add toG? the rule
aA,` ? ? with
??aA,`?? = ?
(`)
A??. (5)
where ?(`)A?? is the probability associated with
rule A? ? in the `th helper language.
At each point, the derivational process of this
PCFG uses the nonterminal?s specific ? coefficients
52
to choose one of the helper languages. It then se-
lects a rule according to the multinomial from that
language. This step is repeated until a whole deriva-
tion is generated.
This PCFG representation of the approach in ?3
points to a possible generalization. Instead of using
an identical CFG backbone for each language, we
can use a set of PCFGs, ?G(`),?(`)? with an iden-
tical nonterminal set and alphabet, and repeat the
same construction as above, replacing step 4 with
the addition of rules of the form aA,` ? ? for each
rule A ? ? in G(`). Such a construction allows
more syntactic variability in the language we are try-
ing to estimate, originating in the syntax of the var-
ious helper languages. In this paper, we do not use
this generalization, and always use the same PCFG
backbone for all languages.
Note that the interpolated model can still be un-
derstood in terms of the exponential model of Eq. 3.
For a given collection of multinomials and base
models of the form of Eq. 3, we can analogously
define a new log-linear model over a set of ex-
tended derivations. These derivations will now in-
clude L ? K features of the form g`,k(x,y), cor-
responding to a count of the event of choosing the
`th mixture component for multinomial k. In addi-
tion, the feature set fk,i(x,y) will be extended to
a feature set of the form f`,k,i(x,y), analogous to
step 4 in constructed PCFG above. The model pa-
rameterized according to Eq. 4 can be recovered by
marginalizing out the ?g? features. We will refer to
the model with these new set of features as ?the ex-
tended model.?
4 Inference and Parameter Estimation
The main building block commonly required for un-
supervised learning in NLP is that of computing fea-
ture expectations for a given model. These feature
expectations can be used with an algorithm such as
expectation-maximization (where the expectations
are normalized to obtain a new set of multinomial
weights) or with other gradient based log-likelihood
optimization algorithms such as L-BFGS (Liu and
Nocedal, 1989) for feature-rich models.
Estimating Multinomial Distributions Given a
surface form x, a multinomial k and an event i in the
multinomial, ?feature expectation? refers to the cal-
culation of the following quantities (in the extended
model):
E[f`,k,i(x,y)] = ?y p(x,y | ?)f`,k,i(x,y) (6)
E[g`,k(x,y)] = ?y p(x,y | ?)g`,k(x,y) (7)
These feature expectations can usually be computed
using algorithms such as the forward-backward al-
gorithm for hidden Markov models, or more gener-
ally, the inside-outside algorithm for PCFGs. In this
paper, however, the task of estimation is different
than the traditional task. As mentioned in ?2, we are
interested in estimating ? from Eq. 4, while fixing
?(`). Therefore, we are only interested in computing
expectations of the form of Eq. 7.
As explained in ?3.2, any model interpolating
with the ? parameters can be reduced to a new log-
linear model with additional features representing
the mixture coefficients of ?. We can then use the
inside-outside algorithm to obtain the necessary fea-
ture expectations for features of the form g`,k(x,y),
expectations which assist in the estimation of the ?
parameters.
These feature expectations can readily be used
in estimation algorithms such as expectation-
maximization (EM). With EM, the update at itera-
tion t would be:
?(t)`,k =
E[g`,k(x,y)]?
` E[g`,k(x,y)]
, (8)
where the expectations are taken with respect to
?(t?1) and the fixed ?(l) for ` = 1, . . . , L.
Estimating Feature-Rich Directed Models Re-
cently Berg-Kirkpatrick et al (2010) found that
replacing traditional multinomial parameterizations
with locally normalized, feature-based log-linear
models was advantageous. This can be understood
as parameterizing ?:
?k,i =
exp?>h(k, i)?
i?
exp?>h(k, i?)
(9)
where h(k, i) are a set of features looking at event i
in context k. For such a feature-rich model, our mul-
tilingual modeling framework still substitutes ? with
a mixture of supervised multinomials for L helper
languages as in Eq. 4. However, for computational
53
convenience, we also reparametrize the mixture co-
efficients ?:
?`,k =
exp ?`,k?L
`?=1 exp ?`?,k
(10)
Here, each ?`,k is an unconstrained parameter, and
the above ?softmax? transformation ensures that ?
lies within the probability simplex for context k.
This is done so that a gradient-based optimization
method like L-BFGS (Liu and Nocedal, 1989) can
be used to estimate ? without having to worry about
additional simplex constraints. For optimization,
derivatives of the data log-likelihood with respect to
? need to be computed. We calculate the derivatives
following Berg-Kirkpatrick et al (2010, ?3.1), mak-
ing use of feature expectations, calculated exactly as
before.
In addition to these estimation techniques, which
are based on the optimization of the log-likelihood,
we also consider a trivially simple technique for es-
timating ?: setting ?l,k to the uniform weight L?1,
where L is the number of helper languages.
5 Coarse-to-Fine Multinomial Expansion
To expand these multinomials involving coarse-
grained categories into multinomials over fine-
grained categories specific to the target language t,
we do the following:
? Whenever a multinomial conditions on a coarse
category c ? C, we make copies of it for each fine-
grained category in ??1t (c) ? Ft.3 If the multino-
mial does not condition on coarse categories, it is
simply copied.
? Whenever a probability ?i within a multinomial
distribution involves a coarse-grained category c
as an event (i.e., it is on the left side of the condi-
tional bar), we expand the event into |??1t (c)| new
events, one per corresponding fine-grained cate-
gory, each assigned the value ?i|??1t (c)| .
4
3We note that in the models we experiment with, we always
condition on at most one fine-grained category.
4During this expansion process for a coarse event, we tried
adding random noise to ?i|??1t (c)| and renormalizing, to breaksymmetry between the fine events, but that was found to be
harmful in preliminary experiments.
The result of this expansion is a model in the
desired family; we use it to initialize conventional
unsupervised parameter estimation. Lexical param-
eters, if any, do not undergo this expansion pro-
cess, and they are estimated anew in the fine grained
model during unsupervised learning, and are initial-
ized using standard methods.
6 Experiments and Results
In this section, we describe the experiments under-
taken and the results achieved. We first note the
characteristics of the datasets and the universal POS
tags used in multilingual modeling.
6.1 Data
For our experiments, we fixed a set of four helper
languages with relatively large amounts of data,
displaying nontrivial linguistic diversity: Czech
(Slavic), English (West-Germanic), German (West-
Germanic), and Italian (Romance). The datasets are
the CoNLL-X shared task data for Czech and Ger-
man (Buchholz and Marsi, 2006),5 the Penn Tree-
bank for English (Marcus et al, 1993), and the
CoNLL 2007 shared task data for Italian (Monte-
magni et al, 2003). This was the only set of helper
languages we tested; improvements are likely pos-
sible. We leave an exploration of helper language
choice (a subset selection problem) to future re-
search, instead demonstrating that the concept has
merit.
We considered ten target languages: Bulgarian
(Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese
(Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es),
Swedish (Sv), and Turkish (Tr). The data come
from the CoNLL-X and CoNLL 2007 shared tasks
(Buchholz and Marsi, 2006; Nivre et al, 2007). For
all the experiments conducted, we trained models
on the training section of a language?s treebank and
tested on the test set. Table 1 shows the number of
sentences in the treebanks and the size of fine POS
tagsets for each language.
Following standard practice, in unsupervised
grammar induction experiments we remove punctu-
ation and then eliminate sentences from the data of
length greater than 10.
5These are based on the Prague Dependency Treebank
(Hajic?, 1998) and the Tiger treebank (Brants et al, 2002) re-
spectively.
54
Pt Tr Bg Jp El Sv Es Sl Nl Da
Training sentences 9,071 4,997 12,823 17,044 2,705 11,042 3,306 1,534 13,349 5,190
Test sentences 288 623 398 709 197 389 206 402 386 322
Size of POS tagset 22 31 54 80 38 41 47 29 12 25
Table 1: The first two rows show the sizes of the training and test datasets for each language. The third row shows the
number of fine POS tags in each language including punctuations.
6.2 Universal POS Tags
Our coarse-grained, universal POS tag set consists
of the following 12 tags: NOUN, VERB, ADJ
(adjective), ADV (adverb), PRON (pronoun), DET
(determiner), ADP (preposition or postposition),
NUM (numeral), CONJ (conjunction), PRT (parti-
cle), PUNC (punctuation mark) and X (a catch-all
for other categories such as abbreviations or foreign
words). These follow recent work by Das and Petrov
(2011) on unsupervised POS tagging in a multilin-
gual setting with parallel data, and have been de-
scribed in detail by Petrov et al (2011).
While there might be some controversy about
what an appropriate universal tag set should include,
these 12 categories (or a subset) cover the most fre-
quent parts of speech and exist in one form or an-
other in all of the languages that we studied. For
each language in our data, a mapping from the
fine-grained treebank POS tags to these universal
POS tags was constructed manually by Petrov et al
(2011).
6.3 Part-of-Speech Tagging
Our first experimental task is POS tagging, and here
we describe the specific details of the model, train-
ing and inference and the results attained.
6.3.1 Model
The model is a hidden Markov model (HMM),
which has been popular for unsupervised tagging
tasks (Merialdo, 1994; Elworthy, 1994; Smith and
Eisner, 2005; Berg-Kirkpatrick et al, 2010).6 We
use a bigram model and a locally normalized log-
linear parameterization, like Berg-Kirkpatrick et al
(2010). These locally normalized log-linear mod-
els can look at various aspects of the observation x
given a tag y, or the pair of tags in a transition, in-
corporating overlapping features. In basic monolin-
6HMMs can be understood as a special case of PCFGs.
gual experiments, we used the same set of features
as Berg-Kirkpatrick et al (2010). For the transi-
tion log-linear model, Berg-Kirkpatrick et al (2010)
used only a single indicator feature of a tag pair, es-
sentially equating to a traditional multinomial dis-
tribution. For the emission log-linear model, sev-
eral features were used: an indicator feature con-
joining the state y and the word x, a feature checking
whether x contains a digit conjoined with the state y,
another feature indicating whether x contains a hy-
phen conjoined with y, whether the first letter of x is
upper case along with the state y, and finally indica-
tor features corresponding to suffixes up to length 3
present in x conjoined with the state y.
Since only the unlexicalized transition distribu-
tions are common across multiple languages, assum-
ing that they all use a set of universal POS tags, akin
to Eq. 4, we can have a multilingual version of the
transition distributions, by incorporating supervised
helper transition probabilities. Thus, we can write:
?y?y? =
L?
`=1
?`,y?(`)y?y? (11)
We use the above expression to replace the transi-
tion distributions, obtaining a multilingual mixture
version of the model. Here, the transition probabili-
ties ?(`)y?y? for the `th helper language are fixed afterbeing estimated using maximum likelihood estima-
tion on the helper language?s treebank.
6.3.2 Training and Inference
We trained both the basic feature-based HMM
model as well as the multilingual mixture model by
optimizing the following objective function:7
L(?) =
N?
i=1
log
?
y
p(x(i),y | ?)? C???22
7Note that in the objective function, for brevity, we abuse
notation by using ? for both models ? monolingual and multi-
lingual; the latter model is also parameterized by ?.
55
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform+DG 45.7 43.6 38.0 60.4 36.7 37.7 31.8 35.9 43.7 36.2 41.0
Mixture+DG 51.5 38.6 35.8 61.7 38.9 39.9 40.5 36.0 50.2 39.9 43.3
DG (B-K et al, 2010) 53.5 27.9 34.7 52.3 35.3 34.4 40.0 33.4 45.4 48.8 40.6
(a)
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform+DG 83.8 50.4 81.3 77.9 80.3 69.0 82.3 82.8 79.3 82.0 76.9
Mixture+DG 84.7 50.0 82.6 79.9 80.3 67.0 83.3 82.8 80.0 82.0 77.3
DG (B-K et al, 2010) 75.4 50.4 80.7 83.4 88.0 61.5 82.3 75.6 79.2 82.3 75.9
(b)
Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con-
structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient
method of Berg-Kirkpatrick et al (2010) using a monolingual feature-based HMM. ?Mixture+DG? is the model where
multilingual mixture coefficients ? of helper languages are estimated using coarse tags (?4), followed by expansion
(?5), and then initializing DG with the expanded transition parameters. ?Uniform+DG? is the case where ? are set
to 1/4, transitions of helper languages are mixed, expanded, and then DG is initialized with the result. For (a), eval-
uation is performed using one-to-one mapping accuracy. In case of (b), the tag dictionary solves the problem of tag
identification and performance is measured using per word POS accuracy. ?Avg? denotes macro-average across the
ten languages.
Note that this involves marginalizing out all possible
state configurations y for a sentence x, resulting in
a non-convex objective. As described in ?4, we opti-
mized this function using L-BFGS. For the mono-
lingual model, derivatives of the feature weights
took the exact same form as Berg-Kirkpatrick et al
(2010), while for the mixture case, we computed
gradients with respect to ?, the unconstrained pa-
rameters used to express the mixture coefficients ?
(see Eq. 10). The regularization constant C was set
to 1.0 for all experiments, and L-BFGS was run till
convergence.
During training, for the basic monolingual
feature-based HMM model, we initialized all param-
eters using small random real values, sampled from
N (0, 0.01). For estimation of the mixture parame-
ters ? for our multilingual model (step 3 in ?2), we
similarly sampled real values fromN (0, 0.01) as an
initialization point. Moreover, during this stage, the
emission parameters also go through parameter es-
timation, but they are monolingual, and are initial-
ized with real values sampled from N (0, 0.01); as
explained in ?2, coarse universal tags are used both
in the transitions and emissions during multilingual
estimation.
After the mixture parameters ? are estimated, we
compute the mixture probabilities ? using Eq. 10.
Next, for each tag pair y, y?, we compute ?y?y? ,
which are the coarse transition probabilities inter-
polated using ?, given the helper languages. We
then expand these transition probabilities (see ?5) to
result in transition probabilities based on fine tags.
Finally, we train a feature-HMM by initializing its
transition parameters with natural logarithms of the
expanded ? parameters, and the emission parame-
ters using small random real values sampled from
N (0, 0.01). This implies that the lexicalized emis-
sion parameters ? that were previously estimated in
the coarse multilingual model are thrown away and
not used for initialization; instead standard initial-
ization is used.
For inference at the testing stage, we use min-
imum Bayes-risk decoding (or ?posterior decod-
ing?), by choosing the most probable tag for each
word position, given the entire observation x. We
chose this strategy because it usually performs
slightly better than Viterbi decoding (Cohen and
Smith, 2009; Ganchev et al, 2010).
6.3.3 Experimental Setup
For experiments, we considered three configura-
tions, and for each, we implemented two variants of
POS induction, one without any kind of supervision,
and the other with a tag dictionary. Our baseline is
56
the direct gradient approach of Berg-Kirkpatrick et
al. (2010), which is the current state of the art for this
task, outperforming classical HMMs. Because this
model achieves strong performance using straight-
forward MLE, it also serves as the core model within
our approach. This model has also been applied in
a multilingual setting with parallel data (Das and
Petrov, 2011). In this baseline, we set the number
of HMM states to the number of fine-grained tree-
bank tags for the given language.
We test two versions of our model. The first ini-
tializes training of the target language?s POS model
using a uniform mixture of the helper language mod-
els (i.e., each ?`,y = 1L = 14 ), and expansion fromcoarse-grained to fine-grained POS tags as described
in ?5. We call this model ?Uniform+DG.?
The second version estimates the mixture coeffi-
cients to maximize likelihood, then expands the POS
tags (?5), using the result to initialize training of the
final model. We call this model ?Mixture+DG.?
No Tag Dictionary For each of the above configura-
tions, we ran purely unsupervised training without a
tag dictionary, and evaluated using one-to-one map-
ping accuracy constraining at most one HMM state
to map to a unique treebank tag in the test data, us-
ing maximum bipartite matching. This is a variant of
the greedy one-to-one mapping scheme of Haghighi
and Klein (2006).8
With a Tag Dictionary We also ran a second ver-
sion of each experimental configuration, where we
used a tag dictionary to restrict the possible path se-
quences of the HMM during both learning and infer-
ence. This tag dictionary was constructed only from
the training section of a given language?s treebank.
It is widely known that such knowledge improves
the quality of the model, though it is an open debate
whether such knowledge is realistic to assume. For
this experiment we removed punctuation from the
training and test data, enabling direct use within the
dependency grammar induction experiments.
8We also evaluated our approach using the greedy version of
this evaluation metric, and results followed the same trends with
only minor differences. We did not choose the other variant,
many-to-one mapping accuracy, because quite often the metric
mapped several HMM states to one treebank tag, leaving many
treebank tags unaccounted for.
6.3.4 Results
All results for POS induction are shown in Ta-
ble 2. Without a tag dictionary, in eight out of ten
cases, either Uniform+DG or Mixture+DG outper-
forms the monolingual baseline (Table 2a). For six
of these eight languages, the latter model where the
mixture coefficients are learned automatically fares
better than uniform weighting. With a tag dictionary,
the multilingual variants outperform the baseline in
seven out of ten cases, and the learned mixture out-
performs or matches the uniform mixture in five of
those seven (Table 2b).
6.4 Dependency Grammar Induction
We next describe experiments for dependency gram-
mar induction. As the basic grammatical model,
we adopt the dependency model with valence (Klein
and Manning, 2004), which forms the basis for state-
of-the-art results for dependency grammar induc-
tion in various settings (Cohen and Smith, 2009;
Spitkovsky et al, 2010; Gillenwater et al, 2010;
Berg-Kirkpatrick and Klein, 2010). As shown in Ta-
ble 3, DMV obtains much higher accuracy in the su-
pervised setting than the unsupervised setting, sug-
gesting that more can be achieved with this model
family.9 For this reason, and because DMV is eas-
ily interpreted as a PCFG, it is our starting point and
baseline.
We consider four conditions. The independent
variables are (1) whether we use uniform? (all set to
1
4 ) or estimate them using EM (as described in ?4),and (2) whether we simply use the mixture model to
decode the test data, or to initialize EM for the DMV.
The four settings are denoted ?Uniform,? ?Mixture,?
?Uniform+EM,? and ?Mixture+EM.?
The results are given in Table 3. In general, the
use of data from other languages improves perfor-
mance considerably; all of our methods outperform
the Klein and Manning (2004) initializer, and we
achieve state-of-the-art performance for eight out of
ten languages. Uniform and Mixture behave simi-
larly, with a slight advantage to the trained mixture
setting. Using EM to train the mixture coefficients
more often hurts than helps (six languages out of
ten). It is well known that likelihood does not cor-
9Its supervised performance is still far from the supervised
state of the art in dependency parsing.
57
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform 78.6 45.0 75.6 56.3 57.0 74.0 73.2 46.1 50.7 59.2 61.6
Mixture 76.8 45.3 75.5 58.3 59.5 73.2 75.9 46.0 51.1 59.9 62.2
Uniform+EM 78.7 43.9 74.7 59.8 73.0 70.5 75.5 41.3 45.9 51.3 61.5
Mixture+EM 79.8 44.1 72.8 63.9 72.3 68.7 76.7 41.0 46.0 55.2 62.1
EM (K & M, 2004) 42.5 36.3 54.3 43.0 41.0 42.3 38.1 37.0 38.6 41.4 41.4
PR (G et al, ?10) 47.8 53.4 54.0 60.2 - 42.2 62.4 50.3 37.9 44.0 -
Phylo. (B-K & K, ?10) 63.1 - - - - 58.3 63.8 49.6 45.1 41.6 -
Supervised (MLE) 81.7 75.7 83.0 89.2 81.8 83.2 79.0 74.5 64.8 80.8 79.3
Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy
(fraction of parents which are correct). The three existing methods are: our replication of EM with the initializer from
Klein and Manning (2004), denoted ?EM?; reported results from Gillenwater et al (2010) for posterior regularization
(?PR?); and reported results from Berg-Kirkpatrick and Klein (2010), denoted ?Phylo.? ?Supervised (MLE)? are oracle
results of estimating parameters from gold-standard annotated data using maximum likelihood estimation. ?Avg?
denotes macro-average across the ten languages.
Figure 2: Projection of the learned mixture coefficients
through PCA. In green, Japanese. In red, Dutch, Danish
and Swedish. In blue, Bulgarian and Slovene. In ma-
genta, Portuguese and Spanish. In black, Greek. In cyan,
Turkish.
relate with the true accuracy measurement, and so
it is unsurprising that this holds in the constrained
mixture family as well. In future work, a different
parametrization of the mixture coefficients, through
features, or perhaps a Bayesian prior on the weights,
might lead to an objective that better simulates ac-
curacy.
Table 3 shows that even uniform mixture coef-
ficients are sufficient to obtain accuracy which su-
percedes most unsupervised baselines. We were in-
terested in testing whether the coefficients which are
learned actually reflect similarities between the lan-
guages. To do that, we projected the learned vectors
? for each tested language using principal compo-
nent analysis and plotted the result in Figure 2. It
is interesting to note that languages which are closer
phylogenetically tend to appear closer to each other
in the plot.
Our experiments also show that multilingual
learning performs better for dependency grammar
induction than part-of-speech tagging. We believe
that this happens because of the nature of the mod-
els and data we use. The transition matrix in part-
of-speech tagging largely depends on word order in
the various helper languages, which differs greatly.
This means that a mixture of transition matrices will
not necessarily yield a meaningful transition matrix.
However, for dependency grammar, there are certain
universal dependencies which appear in all helper
languages, and therefore, a mixture between multi-
nomials for these dependencies still yields a useful
multinomial.
6.5 Inducing Dependencies from Words
Finally, we combine the models for POS tagging and
grammar induction to perform grammar induction
directly from words, instead of gold-standard POS
tags. Our approach is as follows:
1. With a tag dictionary, learn a fine-grained POS
tagging model unsupervised, using either DG or
Mixture+DG as described in ?6.3 and shown in
Table 2b.
58
Method Tags Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Joint DG 68.4 52.4 62.4 61.4 63.5 58.2 67.7 47.2 48.3 50.4 57.9
Joint Mixture+DG 62.2 47.4 67.0 69.5 52.2 49.1 69.3 36.8 52.2 50.1 55.6
Pipeline DG 60.0 50.8 57.7 64.2 68.2 57.9 65.8 45.8 49.9 48.9 56.9
Pipeline Mixture+DG 59.8 47.1 62.9 68.6 50.0 47.6 68.1 36.4 51.2 48.3 54.0
Gold-standard tags 79.8 45.3 75.6 63.9 73.0 74.0 76.7 46.1 50.7 59.9 64.5
Table 4: Results for dependency grammar induction over words. ?Joint?/?Pipeline? refers to joint/pipeline decoding
of tags and dependencies as described in the text. See ?6.3 for a description of DG and Mixture+DG. For the induction
of dependencies we use the Mixture+EM setting as described in ?6.4. All tag induction uses a dictionary as specified
in ?6.3. The last row in this table indicates the best results using multilingual guidance taken from our methods in
Table 3. ?Avg? denotes macro-average across the ten languages.
2. Apply the fine-grained tagger to the words in the
training data for the dependency parser. We con-
sider two variants: the most probable assignment
of tags to words (denoted ?Pipeline?), and the pos-
terior distribution over tags for each word, repre-
sented as a weighted ?sausage? lattice (denoted
?Joint?). This idea was explored for joint infer-
ence by Cohen and Smith (2007).
3. We apply the Mixture+EM unsupervised parser
learning method from ?6.4 to the automatically
tagged sentences, or the lattices.
4. Given the two models, we infer POS tags on the
test data using DG or Mixture+DG to get a lattice
(Joint) or a sequence (Pipeline) and then parse us-
ing the model from the previous step.10 The re-
sulting dependency trees are evaluated against the
gold standard.
Results are reported in Table 4. In almost all cases,
joint decoding of tags and trees performs better than
the pipeline. Even though our part-of-speech tagger
with multilingual guidance outperforms the com-
pletely unsupervised baseline, there is not always an
advantage of using this multilingually guided part-
of-speech tagger for dependency grammar induc-
tion. For Turkish, Japanese, Slovene and Dutch, our
unsupervised learner from words outperforms unsu-
pervised parsing using gold-standard part-of-speech
tags.
We note that some recent work gives a treatment
to unsupervised parsing (but not of dependencies)
10The decoding method on test data (Joint or Pipeline) was
matched to the training method, though they are orthogonal in
principle.
directly from words (Seginer, 2007). Earlier work
that induced part-of-speech tags and then performed
unsupervised parsing in a pipeline includes Klein
and Manning (2004) and Smith (2006). Headden
et al (2009) described the use of a lexicalized vari-
ant of the DMV model, with the use of gold part-of-
speech tags.
7 Conclusion
We presented an approach to exploiting annotated
data in helper languages to infer part-of-speech tag-
ging and dependency parsing models in a different,
target language, without parallel data. Our approach
performs well in many cases. We also described a
way to do joint decoding of part-of-speech tags and
dependencies which performs better than a pipeline.
Future work might consider exploiting a larger num-
ber of treebanks, and more powerful techniques for
combining models than simple local mixtures.
Acknowledgments
We thank Ryan McDonald and Slav Petrov for help-
ful comments on an early draft of the paper. This re-
search has been funded by NSF grants IIS-0844507 and
IIS-0915187 and by U.S. Army Research Office grant
W911NF-10-1-0533.
References
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic
grammar induction. In Proceedings of ACL.
T. Berg-Kirkpatrick, A. B. Co?te?, J. DeNero, and D. Klein.
2010. Painless unsupervised learning with features. In
Proceedings of NAACL-HLT.
59
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL.
D. Burkett and D. Klein. 2008. Two languages are bet-
ter than one (for syntactic parsing). In Proceedings of
EMNLP.
S. B. Cohen and N. A. Smith. 2007. Joint morpholog-
ical and syntactic disambiguation. In Proceedings of
EMNLP-CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proceedings of HLT-
NAACL.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proceedings of ACL-HLT.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceedings of ACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11:2001?2049.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar
induction. In Proceedings of ACL.
A. Haghighi and D. Klein. 2006. Prototype driven learn-
ing for sequence models. In Proceedings of HLT-
NAACL.
J. Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning. Studies in Honor of
Jarmila Panevova?. Prague Karolinum, Charles Univer-
sity Press.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of
NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proceedings of ACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn treebank. Computational Linguistics, 19.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source
transfer of delexicalized dependency parsers. In Pro-
ceedings of EMNLP.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Compulational Lingustics, 20(2):155?
72.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Zampolli, F. Fanciulli, M. Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino,
F. Zanzotto, N. Mana, F. Pianesi, and R. Delmonte.
2003. Building the Italian Syntactic-Semantic Tree-
bank. In Building and using Parsed Corpora, Lan-
guage and Speech Series. Kluwer, Dordrecht.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. JAIR, 36.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proceedings of EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of CoNLL.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. ArXiv:1104.2086.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Proceedings of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of ACL.
D. A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous grammar features.
In Proceedings of EMNLP.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proceedings of EMNLP.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proceedings of ACL.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsuper-
vised multilingual grammar induction. In Proceedings
of ACL-IJCNLP.
V. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010. From
baby steps to leapfrog: How ?less is more? in unsuper-
vised dependency parsing. In Proceedings of NAACL.
C. Xi and R. Hwa. 2005. A backoff model for bootstrap-
ping resources for non-English languages. In Proceed-
ings of HLT-EMNLP.
60
D. Yarowsky and G. Ngai. 2001. Inducing multilingual
POS taggers and NP bracketers via robust projection
across aligned corpora. In Proceedings of NAACL.
61
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234?1245,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exact Inference for Generative Probabilistic
Non-Projective Dependency Parsing
Shay B. Cohen
School of Computer Science
Carnegie Mellon University, USA
scohen@cs.cmu.edu
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We describe a generative model for non-
projective dependency parsing based on a sim-
plified version of a transition system that has
recently appeared in the literature. We then
develop a dynamic programming parsing al-
gorithm for our model, and derive an inside-
outside algorithm that can be used for unsu-
pervised learning of non-projective depend-
ency trees.
1 Introduction
Dependency grammars have received considerable
attention in the statistical parsing community in
recent years. These grammatical formalisms of-
fer a good balance between structural expressiv-
ity and processing efficiency. Most notably, when
non-projectivity is supported, these formalisms can
model crossing syntactic relations that are typical in
languages with relatively free word order.
Recent work has reduced non-projective parsing
to the identification of a maximum spanning tree in a
graph (McDonald et al, 2005; Koo et al, 2007; Mc-
Donald and Satta, 2007; Smith and Smith, 2007).
An alternative to this approach is to use transition-
based parsing (Yamada and Matsumoto, 2003; Nivre
and Nilsson, 2005; Attardi, 2006; Nivre, 2009;
Go?mez-Rodr??guez and Nivre, 2010), where there is
an incremental processing of a string with a model
that scores transitions between parser states, condi-
tioned on the parse history. This paper focuses on
the latter approach.
The above work on transition-based parsing has
focused on greedy algorithms set in a statistical
framework (Nivre, 2008). More recently, dynamic
programming has been successfully used for pro-
jective parsing (Huang and Sagae, 2010; Kuhlmann
et al, 2011). Dynamic programming algorithms for
parsing (also known as chart-based algorithms) al-
low polynomial space representations of all parse
trees for a given input string, even in cases where
the size of this set is exponential in the length of
the string itself. In combination with appropriate
semirings, these packed representations can be ex-
ploited to compute many values of interest for ma-
chine learning, such as best parses and feature ex-
pectations (Goodman, 1999; Li and Eisner, 2009).
In this paper we move one step forward with re-
spect to Huang and Sagae (2010) and Kuhlmann et
al. (2011) and present a polynomial dynamic pro-
gramming algorithm for non-projective transition-
based parsing. Our algorithm is coupled with a
simplified version of the transition system from At-
tardi (2006), which has high coverage for the type
of non-projective structures that appear in various
treebanks. Instead of an additional transition oper-
ation which permits swapping of two elements in
the stack (Titov et al, 2009; Nivre, 2009), Attardi?s
system allows reduction of elements at non-adjacent
positions in the stack. We also present a generat-
ive probabilistic model for transition-based parsing.
The implication for this, for example, is that one can
now approach the problem of unsupervised learning
of non-projective dependency structures within the
transition-based framework.
Dynamic programming algorithms for non-
projective parsing have been proposed by Kahane et
al. (1998), Go?mez-Rodr??guez et al (2009) and Kuhl-
mann and Satta (2009), but they all run in exponen-
tial time in the ?gap degree? of the parsed structures.
To the best of our knowledge, this paper is the first to
1234
introduce a dynamic programming algorithm for in-
ference with non-projective structures of unbounded
gap degree.
The rest of this paper is organized as follows. In
?2 and ?3 we outline the transition-based model we
use, together with a probabilistic generative inter-
pretation. In ?4 we give the tabular algorithm for
parsing, and in ?5 we discuss statistical inference
using expectation maximization. We then discuss
some other aspects of the work in ?6 and conclude
in ?7.
2 Transition-based Dependency Parsing
In this section we briefly introduce the basic defini-
tions for transition-based dependency parsing. For a
more detailed presentation of this subject, we refer
the reader to Nivre (2008). We then define a spe-
cific transition-based model for non-projective de-
pendency parsing that we investigate in this paper.
2.1 General Transition Systems
Assume an input alphabet ? with a special symbol
$ ? ? , which we use as the root of our parse struc-
tures. Throughout this paper we denote the input
string as w = a0 ? ? ? an?1, n ? 1, where a0 = $ and
ai ? ? \ {$} for each i with 1 ? i ? n? 1.
A dependency tree for w is a directed tree Gw =
(Vw, Aw), where Vw = {0, . . . , n ? 1} is the set of
nodes, and Aw ? Vw ? Vw is the set of arcs. The
root of Gw is the node 0. The intended meaning
is that each node in Vw encodes the position of a
token in w. Furthermore, each arc in Aw encodes a
dependency relation between two tokens. We write
i ? j to denote a directed arc (i, j) ? Aw, where
node i is the head and node j is the dependent.
A transition system (for dependency parsing) is a
tuple S = (C, T, I, Ct), whereC is a set of configur-
ations, defined below, T is a finite set of transitions,
which are partial functions t:C ? C, I is a total
initialization function mapping each input string to
a unique initial configuration, and Ct ? C is a set of
terminal configurations.
A configuration is defined relative to some input
string w, and is a triple (?, ?,A), where ? and ? are
disjoint lists called stack and buffer, respectively,
and A ? Vw ? Vw is a set of arcs. Elements of
? and ? are nodes from Vw and, in the case of the
stack, a special symbol ? that we will use as initial
stack symbol. If t is a transition and c1, c2 are con-
figurations such that t(c1) = c2, we write c1 `t c2,
or simply c1 ` c2 if t is understood from the context.
Given an input string w, a parser based on S in-
crementally processes w from left to right, starting
in the initial configuration I(w). At each step, the
parser nondeterministically applies one transition, or
else it stops if it has reached some terminal config-
uration. The dependency graph defined by the arc
set associated with a terminal configuration is then
returned as one possible analysis for w.
Formally, a computation of S is a sequence ? =
c0, . . . , cm, m ? 1, of configurations such that, for
every iwith 1 ? i ? m, ci?1 `ti ci for some ti ? T .
In other words, each configuration in a computa-
tion is obtained as the value of the preceding con-
figuration under some transition. A computation is
called complete whenever c0 = I(w) for some in-
put string w, and cm ? Ct.
We can view a transition-based dependency
parser as a device mapping strings into graphs (de-
pendency trees). Without any restriction on trans-
ition functions in T , these functions might have an
infinite domain, and could thus encode even non-
recursively enumerable languages. However, in
standard practice for natural language parsing, trans-
itions are always specified by some finite mean. In
particular, the definition of each transition depends
on some finite window at the top of the stack and
some finite window at the beginning of the buffer
in each configuration. In this case, we can view a
transition-based dependency parser as a notational
variant of a push-down transducer (Hopcroft et al,
2000), whose computations output sequences that
directly encode dependency trees. These transducers
are nondeterministic, meaning that several trans-
itions can be applied to some configurations. The
transition systems we investigate in this paper fol-
low these principles.
We close this subsection with some additional
notation. We denote the stack with its topmost ele-
ment to the right and the buffer with its first ele-
ment to the left. We indicate concatenation in the
stack and buffer by a vertical bar. For example, for
k ? Vw, ?|k denotes some stack with topmost ele-
ment k and k|? denotes some buffer with first ele-
ment k. For 0 ? i ? n ? 1, ?i denotes the buffer
1235
[i, i + 1, . . . , n ? 1]; for i ? n, ?i denotes [] (the
empty buffer).
2.2 A Non-projective Transition System
We now turn to give a description of our trans-
ition system for non-projective parsing. While a
projective dependency tree satisfies the requirement
that, for every arc in the tree, there is a direc-
ted path between its headword and each of the
words between the two endpoints of the arc, a non-
projective dependency tree may violate this condi-
tion. Even though some natural languages exhibit
syntactic phenomena which require non-projective
expressive power, most often such a resource is used
in a limited way.
This idea is demonstrated by Attardi (2006), who
proposes a transition system whose individual trans-
itions can deal with non-projective dependencies
only to a limited extent, depending on the distance
in the stack of the nodes involved in the newly con-
structed dependency. The author defines this dis-
tance as the degree of the transition, with transitions
of degree one being able to handle only projective
dependencies. This formulation permits parsing a
subset of the non-projective trees, where this subset
depends on the degree of the transitions. The repor-
ted coverage in Attardi (2006) is already very high
when the system is restricted to transitions of degree
two or three. For instance, on training data for Czech
containing 28,934 non-projective relations, 27,181
can be handled by degree two transitions, and 1,668
additional dependencies can be handled by degree
three transitions. Table 1 gives additional statistics
for treebanks from the CoNLL-X shared task (Buch-
holz and Marsi, 2006).
We now turn to describe our variant of the trans-
ition system of Attardi (2006), which is equivalent to
the original system restricted to transitions of degree
two. Our results are based on such a restriction. It is
not difficult to extend our algorithms (?4) to higher
degree transitions, but this comes at the expense of
higher complexity. See ?6 for more discussion on
this issue.
Let w = a0 ? ? ? an?1 be an input string over ?
defined as in ?2.1, with a0 = $. Our transition sys-
tem for non-projective dependency parsing is
S(np) = (C, T (np), I(np), C(np)t ),
Language Deg. 2 Deg. 3 Deg. 4
Arabic 180 21 7
Bulgarian 961 41 10
Czech 27181 1668 85
Danish 876 136 53
Dutch 9072 2119 171
German 15827 2274 466
Japanese 1484 143 9
Portuguese 3104 424 37
Slovene 601 48 13
Spanish 66 7 0
Swedish 1566 226 79
Turkish 579 185 8
Table 1: The number of non-projective relations of vari-
ous degrees for several treebanks (training sets), as repor-
ted by the parser of Attardi (2006). Deg. stands for ?de-
gree.? The parser did not detect non-projective relations
of degree higher than 4.
where C is the same set of configurations defined
in ?2.1. The initialization function I(np) maps each
string w to the initial configuration ([?], ?0, ?). The
set of terminal configurationsC(np)t contains all con-
figurations of the form ([?, 0], [], A), for any set of
arcs A.
The set of transition functions is defined as
T (np) = {shb | b ? ?} ? {la1, ra1, la2, ra2},
where each transition is specified below. We let vari-
ables i, j, k, l range over Vw, and variable ? is a list
of stack elements from Vw ? {?}:
shb : (?, k|?,A) ` (?|k, ?,A) if ak = b;
la1 : (?|i|j, ?,A) ` (?|j, ?,A ? {j ? i});
ra1 : (?|i|j, ?,A) ` (?|i, ?, A ? {i? j});
la2 : (?|i|j|k, ?,A) ` (?|j|k, ?,A ? {k ? i});
ra2 : (?|i|j|k, ?,A) ` (?|i|j, ?,A ? {i? k}).
Each of the above transitions is undefined on config-
urations that do not match the forms specified above.
As an example, transition la2 is not defined for a
configuration (?, ?,A) with |?| ? 2, and transition
shb is not defined for a configuration (?, k|?,A)
with b 6= ak, or for a configuration (?, [], A).
Transition shb removes the first node from the buf-
fer, in case this node represents symbol b ? ? ,
1236
and pushes it into the stack. These transitions are
called shift transitions. The remaining four trans-
itions are called reduce transitions, i.e., transitions
that consume nodes from the stack. Notice that in
the transition system at hand all the reduce trans-
itions decrease the size of the stack by one ele-
ment. Transition la1 creates a new arc with the top-
most node on the stack as the head and the second-
topmost node as the dependent, and removes the
latter from the stack. Transition ra1 is symmetric
with respect to la1. Transitions la1 and ra1 have
degree one, as already explained. When restricted
to these three transitions, the system is equivalent
to the so-called stack-based arc-standard model of
Nivre (2004). Transition la2 and transition ra2 are
very similar to la1 and ra1, respectively, but with
the difference that they create a new arc between
the topmost node in the stack and a node which is
two positions below the topmost node. Hence, these
transitions have degree two, and are the key com-
ponents in parsing of non-projective dependencies.
We turn next to describe the equivalence between
our system and the system in Attardi (2006). The
transition-based parser presented by Attardi pushes
back into the buffer elements that are in the top pos-
ition of the stack. However, a careful analysis shows
that only the first position in the buffer can be af-
fected by this operation, in the sense that elements
that are pushed back from the stack are never found
in buffer positions other than the first. This means
that we can consider the first element of the buffer
as an additional stack element, always sitting on the
top of the top-most stack symbol.
More formally, we can define a function mc :
C ? C that maps configurations in the original al-
gorithm to those in our variant as follows:
mc((?, k|?,A)) = (?|k, ?,A)
By applying this mapping to the source and target
configuration of each transition in the original sys-
tem, it is easy to check that c1 ` c2 in that parser if
and only if mc(c1) ` mc(c2) in our variant. We ex-
tend this and define an isomorphism between com-
putations in both systems, such that a computation
c0, . . . , cm in the original parser is mapped to a com-
putation mc(c0), . . . ,mc(cm) in the variant, with
both generating the same dependency graph A. This
???
??? 2n2n? 12n? 21 2 3
Figure 1: A dependency structure of arbitrary gap degree
that can be parsed with Attardi?s parser.
proves that our notational variant is in fact equival-
ent to Attardi?s parser.
A relevant property of the set of dependency
structures that can be processed by Attardi?s parser,
even when restricted to transitions of degree two, is
that the number of discontinuities present in each of
their subtrees, defined as the gap degree by Bod-
irsky et al (2005), is not bounded. For example, the
dependency graph in Figure 1 has gap degree n? 1,
and it can be parsed by the algorithm for any arbit-
rary n ? 1 by applying 2n shb transitions to push
all the nodes into the stack, followed by (2n ? 2)
ra2 transitions to create the crossing arcs, and finally
one ra1 transition to create the dependency 1? 2.
As mentioned in ?1, the computational complex-
ity of the dynamic programming algorithm that will
be described in later sections does not depend on the
gap degree, contrary to the non-projective depend-
ency chart parsers presented by Go?mez-Rodr??guez et
al. (2009) and by Kuhlmann and Satta (2009), whose
running time is exponential in the maximum gap de-
gree allowed by the grammar.
3 A Generative Probabilistic Model
In this section we introduce a generative probabil-
istic model based on the transition system of ?2.2.
In formal language theory, there is a standard way
of giving a probabilistic interpretation to a non-
deterministic parser whose computations are based
on sequences of elementary operations such as trans-
itions. The idea is to define conditional probability
distributions over instances of the transition func-
tions, and to ?combine? these probabilities to assign
probabilities to computations and strings.
One difficulty we have to face with when dealing
with transition systems is that the notion of compu-
tation, defined in ?2.1, depends on the input string,
because of the buffer component appearing in each
configuration. This is a pitfall to generative model-
1237
ing, where we are interested in a system whose com-
putations lead to the generation of any string. To
overcome this problem, we observe that each com-
putation, defined as a sequence of stacks and buffers
(the configurations) can equivalently be expressed as
a sequence of stacks and transitions.
More precisely, consider a computation ? =
c0, . . . , cm, m ? 1. Let ?i, be the stack associated
with ci, for each i with 0 ? i ? m. Let alo C? be
the set of all stacks associated with configurations in
C. We can make explicit the transitions that have
been used in the computation by rewriting ? in the
form ?0 `t1 ?1 ? ? ??m?1 `tm ?m. In this way, ?
generates a string that is composed by all symbols
that are pushed into the stack by transitions shb, in
the left to right order.
We can now associate a probability to (our repres-
entation of) sequence ? by setting
p(?) =
m?
i=1
p(ti | ?i?1). (1)
To assign probabilities to complete computations we
should further multiply p(?) by factors ps(?0) and
pe(?m), where ps and pe are start and end probabil-
ity distributions, respectively, both defined over C?.
Note however that, as defined in ?2.2, all initial con-
figurations are associated with stack [?] and all final
configurations are associated with stack [?, 0], thus
ps and pe are deterministic. Note that the Markov
chain represented in Eq. 1 is homogeneous, i.e., the
probabilities of the transition operations do not de-
pend on the time step.
As a second step we observe that, according to the
definition of transition system, each t ? T has an in-
finite domain. A commonly adopted solution is to
introduce a special function, called history function
and denoted by H , defined over the set C? and tak-
ing values over some finite set. For each t ? T and
?, ?? ? C?, we then impose the condition
p(t | ?) = p(t | ??)
whenever H(?) = H(??). Since H is finitely val-
ued, and since T is a finite set, the above condition
guarantees that there will only be a finite number of
parameters p(t | ?) in our model.
So far we have presented a general discussion of
how to turn a transition-based parser into a gener-
ative probabilistic model, and have avoided further
specification of the history function. We now turn
our attention to the non-projective transition system
of ?2.2. To actually transform that system into a
parametrized probabilistic model, and to develop an
associated efficient inference procedure as well, we
need to balance between the amount of information
we put into the history function and the computa-
tional complexity which is required for inference.
We start the discussion with a na??ve model using a
history function defined by a fixed size window over
the topmost portion of the stack. More precisely,
each transition is conditioned on the lexical form of
the three symbols at the top of the stack ?, indic-
ated as b3, b2, b1 ? ? below, with b1 referring to the
topmost symbol. The parameters of the model are
defined as follows.
p(shb | b3, b2, b1) = ?shbb3,b2,b1 , ?b ? ? ,
p(la1 | b3, b2, b1) = ?la1b3,b2,b1 ,
p(ra1 | b3, b2, b1) = ?ra1b3,b2,b1 ,
p(la2 | b3, b2, b1) = ?la2b3,b2,b1 ,
p(ra2 | b3, b2, b1) = ?ra2b3,b2,b1 .
The parameters above are subject to the follow-
ing normalization conditions, for every choice of
b3, b2, b1 ? ? :
?la1b3,b2,b1 + ?
ra1
b3,b2,b1 + ?
la2
b3,b2,b1+
?ra2b3,b2,b1 +
?
b??
?shbb3,b2,b1 = 1 .
This na??ve model presents two practical problems.
The first problem relates to the efficiency of an in-
ference algorithm, which has a quite high computa-
tional complexity, as it will be discussed in ?5. A
second problem arises in the probabilistic setting.
Using this model would require estimating many
parameters which are based on trigrams. This leads
to higher sample complexity to avoid sparse counts:
we would need more samples to accurately estimate
the model.
We therefore consider a more elaborated model,
which tackles both of the above problems. Again,
let b3, b2, b1 ? ? indicate the lexical form of the
three symbols at the top of the stack. We define the
1238
distributions p(t | ?) as follows:
p(shb | b1) = ?shbb1 , ?b ? ? ,
p(la1 | b2, b1) = ?rdb1 ? ?
la1
b2,b1 ,
p(ra1 | b2, b1) = ?rdb1 ? ?
ra1
b2,b1 ,
p(la2 | b3, b2, b1) = ?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1 ,
p(ra2 | b3, b2, b1) = ?rdb1 ? ?
rd2
b2,b1 ? ?
ra2
b3,b2,b1 .
The parameters above are subject to the following
normalization conditions, for every b3, b2, b1 ? ? :
?
b??
?shbb1 + ?
rd
b1 = 1 , (2)
?la1b2,b1 + ?
ra1
b2,b1 + ?
rd2
b2,b1 = 1 , (3)
?la2b3,b2,b1 + ?
ra2
b3,b2,b1 = 1 . (4)
Intuitively, parameter ?rdb denotes the probability
that we perform a reduce transition instead of a shift
transition, given that we have seen lexical form b at
the top of the stack. Similarly, parameter ?rd2b2,b1 de-notes the probability that we perform a reduce trans-
ition of degree 2 (see ?2.2) instead of a reduce trans-
ition of degree 1, given that we have seen lexical
forms b1 and b2 at the top of the stack.
We observe that the above model has a num-
ber of parameters |? | + 4 ? |? |2 + 2 ? |? |3 (not
all independent). This should be contrasted with
the na??ve model, that has a number of parameters
4 ? |? |3 + |? |4.
4 Tabular parsing
We present here a dynamic programming algorithm
for simulating the computations of the system from
?2?3. Given an input string w, our algorithm pro-
duces a compact representation of the set ? (w),
defined as the set of all possible computations of
the model when processing w. In combination with
the appropriate semirings, this method can provide
for instance the highest probability computation in
? (w), or else the probability of w, defined as the
sum of all probabilities of computations in ? (w).
We follow a standard approach in the literature
on dynamic programming simulation of stack-based
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989). More recently, this approach has also
been applied by Huang and Sagae (2010) and by
??????
c 0
c 1
c m
? h 1
h 1? i
? h 2 h 3
minimum
stack
length in
c 1 , . . . , cm
i i + 1
i + 1
buffer size
stack size
st
ac
k
b
u
ff
er
j
Figure 2: Schematic representation of the computations
? associated with item [h1, i, h2h3, j].
Kuhlmann et al (2011) to the simulation of pro-
jective transition-based parsers. The basic idea in
this approach is to decompose computations of the
parser into smaller parts, group them into equival-
ence classes and recombine to obtain larger parts of
computations.
Let w = a0 ? ? ? an?1, Vw and S(np) be defined as
in ?2. We use a structure called item, defined as
[h1, i, h2h3, j],
where 0 ? i < j ? n and h1, h2, h3 ? Vw must
satisfy h1 < i and i ? h2 < h3 < j. The intended
interpretation of an item can be stated as follows; see
also Figure 2.
? There exists a computation ? of S(np) on w hav-
ing the form c0, . . . , cm, m ? 1, with c0 =
(?|h1, ?i, A) and cm = (?|h2|h3, ?j , A?) for
some stack ? and some arc sets A and A?;
? For each iwith 1 ? i < m, the stack ?i associated
with configuration ci has the list ? at the bottom
and satisfies |?i| ? |?|+ 2.
Some comments on the above conditions are in
order here. Let t1, ? ? ? , tm be the sequence of trans-
itions in T (np) associated with computation ?. Then
we have t1 = shai , since |?1| ? |?| + 2. Thus we
conclude that |?1| = |?|+ 2.
The most important consequence of the definition
of item is that each transition ti with 2 ? i ? m
does not depend on the content of the ? portion of
the stack ?i. To see this, consider transition ci?1 `ti
ci. If ti = shai , the content of ? is irrelevant at
1239
this step, since in our model shai is conditioned only
on the topmost stack symbol of ?i?1, and we have
|?i?1| ? |?|+ 2.
Consider now the case of ti = la2. From |?i| ?
|?| + 2 we have that |?i?1| ? |?| + 3. Again, the
content of ? is irrelevant at this step, since in our
model la2 is conditioned only on the three topmost
stack symbols of ?i?1. A similar argument applies
to the cases of ti ? {ra2, la1, ra1}.
From the above, we conclude that if we apply the
transitions t1, . . . , tm to stacks of the form ?|h1, the
resulting computations have all identical probabilit-
ies, independently of the choice of ?.
Each computation satisfying the two conditions
above will be called an I-computation associ-
ated with item [h1, i, h2h3, j]. Notice that an I-
computation has the overall effect of replacing node
h1 sitting above a stack ? with nodes h2 and h3.
This is the key property in the development of our
algorithm below.
We specify our dynamic programming algorithm
as a deduction system (Shieber et al, 1995). The
deduction system starts with axiom [?, 0, ?0, 1], cor-
responding to an initial stack [?] and to the shift of
a0 = $ from the buffer into the stack. The set ? (w)
is non-empty if and only if item [?, 0, ?0, n] can be
derived using the inference rules specified below.
Each inference rule is annotated with the type of
transition it simulates, along with the arc constructed
by the transition itself, if any.
[h1, i, h2h3, j]
[h3, j, h3j, j + 1]
(shaj )
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h5, j]
(la1;h5 ? h4)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h4, j]
(ra1;h4 ? h5)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h4h5, j]
(la2;h5 ? h2)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h4, j]
(ra2;h2 ? h5)
The above deduction system infers items in a
bottom-up fashion. This means that longer compu-
tations over substrings of w are built by combining
shorter ones. In particular, the inference rule shaj
asserts the existence of I-computations consisting of
a single shaj transition. Such computations are rep-
resented by the consequent item [h3, j, h3j, j + 1],
indicating that the index of the shifted word aj is
added to the stack by pushing it on top of h3.
The remaining four rules implement the reduce
transitions of the model. We have already ob-
served in ?2.2 that all available reduce transitions
shorten the size of the stack by one unit. This al-
lows us to combine pairs of I-computations with
a reduce transition, resulting in a computation that
is again an I-computation. More precisely, if we
concatenate an I-computation asserted by an item
[h1, i, h2h3, k] with an I-computation asserted by an
item [h3, k, h4h5, j], we obtain a computation that
has the overall effect of increasing the size of the
stack by 2, replacing the topmost stack element h1
with stack elements h2, h4 and h5. If we now apply
any of the reduce transitions from the inventory of
the model, we will remove one of these three nodes
from the stack, and the overall result will be again
an I-computation, which can then be asserted by a
certain item. For example, if we apply the reduce
transition la1, the consequent item is [h1, i, h2h5, j],
since an la1 transition removes the second topmost
element from the stack (h4). The other reduce trans-
itions remove a different element, and thus their
rules produce different consequent items.
The above argument shows the soundness of the
deduction system, i.e., an item I = [h1, i, h2h3, j]
is only generated if there exists an I-computation
? = c0, . . . , cm with c0 = (?|h1, ?i, A) and cm =
(?|h2|h3, ?j , A?). To prove completeness, we must
show the converse result, i.e., that the existence of
an I-computation ? implies that item I is inferred.
We first do this under the assumption that the infer-
ence rule for the shift transitions do not have an ante-
cedent, i.e., items [h1, j, h1j, j + 1] are considered
as axioms. We proceed by using strong induction on
the length m of the computation ?.
For m = 1, ? consists of a single transition shaj ,
and the corresponding item I = [h1, j, h1j, j + 1]
is constructed as an axiom. For m > 1, let ? be
as specified above. The transition that produced
1240
cm must have been a reduce transition, otherwise
? would not be an I-computation. Let ck be the
rightmost configuration in c0, . . . , cm?1 whose stack
size is |?| + 2. Then it can be shown that the com-
putations ?1 = c0, . . . , ck and ?2 = ck, . . . , cm?1
are again I-computations. Since ?1 and ?2 have
strictly fewer transitions than ?, by the induction hy-
pothesis, the system constructs items [h1, i, h2h3, k]
and [h3, k, h4h5, j], where h2 and h3 are the stack
elements at the top of ck. Applying to these items
the inference rule corresponding to the reduce trans-
ition at hand, we can construct item I .
When the inference rule for the shift transition has
an antecedent [h1, i, h2h3, j], as indicated above, we
have the overall effect that I-computations consist-
ing of a single transition shifting aj on the top of h3
are simulated only in case there exists a computation
starting with configuration ([?], ?0) and reaching a
configuration of the form (?|h2|h3, ?j). This acts as
a filter on the search space of the algorithm, but does
not invalidate the completeness property. However,
in this case the proof is considerably more involved,
and we do not report it here.
An important property of the deduction system
above, which will be used in the next section, is
that the system is unambiguous, that is, each I-
computation is constructed by the system in a
unique way. This can be seen by observing that, in
the sketch of the completeness proof reported above,
there always is an unique choice of ck that decom-
poses I-computation ? into I-computations ?1 and
?2. In fact, if we choose a configuration ck? other
than ck with stack size |?| + 2, the computation
??2 = ck? , . . . , cm?1 will contain ck as an interme-
diate configuration, which violates the definition of
I-computation because of an intervening stack hav-
ing size not larger than the size of the stack associ-
ated with the initial configuration.
As a final remark, we observe that we can keep
track of all inference rules that have been applied
in the computation of each item by the above al-
gorithm, by encoding each application of a rule as
a reference to the pair of items that were taken as
antecedent in the inference. In this way, we ob-
tain a parse forest structure that can be viewed as a
hypergraph or as a non-recursive context-free gram-
mar, similar to the case of parsing based on context-
free grammars. See for instance Klein and Manning
(2001) or Nederhof (2003). Such a parse forest en-
codes all valid computations in ? (w), as desired.
The algorithm runs in O(n8) time. Using meth-
ods similar to those specified in Eisner and Satta
(1999), we can reduce the running time to O(n7).
However, we do not further pursue this idea here,
and proceed with the discussion of exact inference,
found in the next section.
5 Inference
We turn next to specify exact inference with our
model, for computing feature expectations. Such
inference enables, for example, the derivation of
an expectation-maximization algorithm for unsuper-
vised parsing.
Here, a feature is a function over computations,
providing the count of a pattern related to a para-
meter. We denote by f la2b3,b2,b1(?), for instance,the number of occurrences of transition la2 within
? with topmost stack symbols having word forms
b3, b2, b1 ? ? , with b1 associated with the topmost
stack symbol.
Feature expectations are computed by using an
inside-outside algorithm for the items in the tabu-
lar algorithm. More specifically, given a string w,
we associate each item [h1, i, h2h3, j] defined as in
?4 with two quantities:
I([h1, i, h2h3, j]) =
?
?=([h1],?i),...,([h2,h3],?j)
p(?) ; (5)
O([h1, i, h2h3, j]) =
?
?,?=([?],?0),...,(?|h1,?i)
??=(?|h2|h3,?j),...,([?,0],?n)
p(?) ? p(??) . (6)
I([h1, i, h2h3, j]) and O([h1, i, h2h3, j]) are called
the inside and the outside probabilities, respect-
ively, of item [h1, i, h2h3, j]. The tabular algorithm
of ?4 can be used to compute the inside probabilit-
ies. Using the gradient transformation (Eisner et al,
2005), a technique for deriving outside probabilities
from a set of inference rules, we can also compute
O([h1, i, h2h3, j]). The use of the gradient trans-
formation is valid in our case because the tabular al-
gorithm is unambiguous (see ?4).
Using the inside and outside probabilities, we can
now efficiently compute feature expectations for our
1241
Ep(?|w)[f la2b3,b2,b1(?)] =
?
??? (w)
p(? | w) ? f la2b3,b2,b1(?) =
1
p(w) ?
?
??? (w)
p(?) ? f la2b3,b2,b1(?)
= 1p(w) ?
?
?,i,k,j,
h1,h2,h3,h4,h5,
s.t. ah2=b3,
ah4=b2, ah5=b1
?
?0=([?],?0),...,(?|h1,?i),
?1=(?|h1,?i),...,(?|h2|h3,?k),
?2=(?|h2|h3,?k),...,(?|h2|h4|h5,?j),
?3=(?|h2|h5,?j),...,([?,0],?n)
p(?0) ? p(?1) ? p(?2) ? p(la2 | b3, b2, b1) ? p(?3)
=
?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1
p(w) ?
?
?,i,j,
h1,h2,h5, s.t.
ah2=b3, ah5=b1
?
?0=([?],?0),...,(?|h1,?i),
?3=(?|h2|h5,?j),...,([?,0],?n)
p(?0) ? p(?3) ?
?
?
k,h3,h4,
s.t. ah4=b2
?
?1=(?|h1,?i),...,(?|h2|h3,?k)
p(?1) ?
?
?2=(?|h2|h3,?k),...,(?|h2|h4|h5,?j)
p(?2)
Figure 3: Decomposition of the feature expectationEp(?|w)[f la2b3,b2,b1(?)] into a finite summation. Quantity p(w) aboveis the sum over all probabilities of computations in ? (w).
model. Figure 3 shows how to express the expect-
ation of feature f la2b3,b2,b1(?) by means of a finitesummation. Using Eq. 5 and 6 and the relation
p(w) = I([?, 0, ?0, n]) we can then write:
Ep(?|w)[f la2b3,b2,b1(?)] =
?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1
I([?, 0, ?0, n]) ?
?
?
i,j,h1,h4,h5,
s.t. ah4=b2, ah5=b1
O([h1, i, h4h5, j]) ?
?
?
k,h2,h3,
s.t. ah2=b3
I([h1, i, h2h3, k]) ? I([h3, k, h4h5, j]) .
Very similar expressions can be derived for the ex-
pectations for features f ra2b3,b2,b1(?), f la1b2,b1(?), and
f ra1b2,b1(?). As for feature f shbb1 (?), b ? ? , the aboveapproach leads to
Ep(?|w)[f shbb1 (?)] =
=
?shbb1
I([?, 0, ?0, n]) ?
?
?,i,h, s.t.
ah=b1, ai=b
O([h, i, hi, i+ 1]) .
As mentioned above, these expectations can be
used, for example, to derive an EM algorithm for our
model. The EM algorithm in our case is not com-
pletely straightforward because of the way we para-
metrize the model. We give now the re-estimation
steps for such an EM algorithm. We assume that all
expectations below are taken with respect to a set of
parameters ? from iteration s ? 1 of the algorithm,
and we are required to update these ?. To simplify
notation, let us assume that there is only one stringw
in the training corpus. For each b1 ? ? , we define:
Zb1 =
?
b2??
Ep(?|w)
[
f la1b2,b1(?) + f
ra1
b2,b1(?)
]
+
?
b3,b2??
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
]
;
Zb2,b1 =
?
b3??
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
]
.
We then have, for every b ? ? :
?shbb1 (s)?
Ep(?|w)[f shbb1 (?)]
Zb1 +
?
b??? Ep(?|w)[f
shb?
b1 (?)]
.
1242
Furthermore, we have:
?la1b2,b1(s)?
Ep(?|w)[f la1b2,b1(?)]
Zb2,b1 + Ep(?|w)
[
f la1b2,b1(?) + f
ra1
b2,b1(?)
] ,
and:
?la2b3,b2,b1(s)?
Ep(?|w)[f la2b3,b2,b1(?)]
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
] .
The rest of the parameter updates can easily be de-
rived using the above updates because of the sum-
to-1 constraints in Eq. 2?4.
6 Discussion
We note that our model inherits spurious ambigu-
ity from Attardi?s model. More specifically, we can
have different derivations, corresponding to differ-
ent system computations, that result in identical de-
pendency graphs and strings. While running our
tabular algorithm with the Viterbi semiring effi-
ciently computes the highest probability computa-
tion in ? (w), spurious ambiguity means that find-
ing the highest probability dependency tree is NP-
hard. This latter result can be shown using proof
techniques similar to those developed by Sima?an
(1996). We leave it for future work how to eliminate
spurious ambiguity from the model.
While in the previous sections we have described
a tabular method for the transition system of Attardi
(2006) restricted to transitions of degree up to two, it
is possible to generalize the model to include higher-
degree transitions. In the general formulation of At-
tardi parser, transitions of degree d create links in-
volving nodes located d positions beneath the top-
most position in the stack:
lad : (?|i1|i2| . . . |id+1, ?, A) `
(?|i2| . . . |id+1, ?, A ? {id+1 ? i1});
rad : (?|i1|i2| . . . |id+1, ?, A) `
(?|i1|i2| . . . |id, ?, A ? {i1 ? id+1}).
To define a transition system that supports trans-
itions up to degree D, we use a set of
items of the form [s1 . . . sD?1, i, e1 . . . eD, j], cor-
responding (in the sense of ?4) to compu-
tations of the form c0, . . . , cm, m ? 1,
with c0 = (?|s1| . . . |sD?1, ?i, A) and cm =
(?|e1| . . . |eD, ?j , A?). The deduction steps corres-
ponding to reduce transitions in this general system
have the general form
[s1 . . . sD?1, i, e1m1 . . .mD?1, j]
[m1 . . .mD?1, j, e2 . . . eD+1, w]
[s1 . . . sD?1, i, e1 . . . ec?1ec+1 . . . eD+1, w]
(ep ? ec)
where the values of p and c differ for each transition:
to obtain the inference rule corresponding to a lad
transition, we make p = D + 1 and c = D + 1? d;
and to obtain the rule for a rad transition, we make
p = D + 1? d and c = D + 1. Note that the parser
runs in timeO(n3D+2), whereD stands for the max-
imum transition degree, so each unit increase in the
transition degree adds a cubic factor to the parser?s
polynomial time complexity. This is in contrast to a
previous tabular formulation of the Attardi parser by
Go?mez-Rodr??guez et al (2011), which ran in expo-
nential time.
The model for the transition system we give in this
paper is generative. It is not hard to naturally extend
this model to the discriminative setting. In this case,
we would condition the model on the input string to
get a conditional distribution over derivations. It is
perhaps more natural in this setting to use arbitrary
weights for the parameter values, since the compu-
tation of a normalization constant (the probability of
a string) is required in any case. Arbitrary weights
in the generative setting could be more problematic,
because it would require computing a normalization
constant corresponding to a sum over all strings and
derivations.
7 Conclusion
We presented in this paper a generative probabilistic
model for non-projective parsing, together with the
description of an efficient tabular algorithm for pars-
ing and doing statistical inference with the model.
Acknowledgments
The authors thank Marco Kuhlmann for helpful
comments on an early draft of the paper. The authors
also thank Giuseppe Attardi for the help received to
extract the parsing statistics. The second author has
been partially supported by Ministerio de Ciencia e
Innovacio?n and FEDER (TIN2010-18552-C03-02).
1243
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166?170,
New York, USA.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143?151,
Vancouver, Canada.
Manuel Bodirsky, Marco Kuhlmann, and Mathias Mo?hl.
2005. Well-nested drawings as models of syntactic
structure. In Tenth Conference on Formal Gram-
mar and Ninth Meeting on Mathematics of Language,
pages 195?203, Edinburgh, UK.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
149?164, New York, USA.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and Head Auto-
maton Grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 457?464, College Park, MD,
USA.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling Comp Ling: Practical weighted dynamic
programming and the Dyna language. In Proceedings
of HLT-EMNLP, pages 281?290.
Carlos Go?mez-Rodr??guez and Joakim Nivre. 2010. A
transition-based parser for 2-planar dependency struc-
tures. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1492?1501, Uppsala, Sweden.
Carlos Go?mez-Rodr??guez, David J. Weir, and John Car-
roll. 2009. Parsing mildly non-projective dependency
structures. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 291?299, Athens, Greece.
Carlos Go?mez-Rodr??guez, John Carroll, and David Weir.
2011. Dependency parsing schemata and mildly non-
projective dependency parsing. Computational Lin-
guistics (in press), 37(3).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2000. Introduction to Automata Theory.
Addison-Wesley, 2nd edition.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1077?1086,
Uppsala, Sweden.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In 36th Annual
Meeting of the Association for Computational Lin-
guistics and 18th International Conference on Compu-
tational Linguistics (COLING-ACL), pages 646?652,
Montre?al, Canada.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the IWPT, pages
123?134.
Terry Koo, Amir Globerson, Xavier Carreras, and Mi-
chael Collins. 2007. Structured prediction models
via the matrix-tree theorem. In Proceedings of the
EMNLP-CoNLL, pages 141?150.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective depend-
ency parsing. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 478?486, Athens, Greece.
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), Portland, Oregon,
USA.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Jacques Loecx,
editor, Automata, Languages and Programming, 2nd
Colloquium, University of Saarbru?cken, July 29?
August 2, 1974, number 14 in Lecture Notes in Com-
puter Science, pages 255?269. Springer.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 40?51, Singa-
pore.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Tenth International Conference on Parsing
Technologies (IWPT), pages 121?132, Prague, Czech
Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Human Language
Technology Conference (HLT) and Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 523?530, Vancouver, Canada.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and knuth?s algorithm. Computational Linguistics,
29(1):135?143.
1244
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 99?106, Ann Arbor, USA.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Workshop on Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 50?57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguist-
ics, 34(4):513?553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the 47th An-
nual Meeting of the ACL and the Fourth International
Joint Conference on Natural Language Processing of
the AFNLP, pages 351?359, Singapore.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1?2):3?
36.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of COLING, pages 1175?
1180.
David A. Smith and Noah A. Smith. 2007. Probab-
ilistic models of nonprojective dependency trees. In
Proceedings of the EMNLP-CoNLL, pages 132?140.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of IJCAI, pages 281?290.
Masaru Tomita. 1986. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies (IWPT), pages 195?206, Nancy,
France.
1245
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953?1964,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Latent-Variable Synchronous CFGs for Hierarchical Translation
Avneesh Saluja and Chris Dyer
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{avneesh,cdyer}@cs.cmu.edu
Shay B. Cohen
University of Edinburgh
Edinburgh EH8 9AB, UK
scohen@inf.ed.ac.uk
Abstract
Data-driven refinement of non-terminal
categories has been demonstrated to be
a reliable technique for improving mono-
lingual parsing with PCFGs. In this pa-
per, we extend these techniques to learn
latent refinements of single-category syn-
chronous grammars, so as to improve
translation performance. We compare two
estimators for this latent-variable model:
one based on EM and the other is a spec-
tral algorithm based on the method of mo-
ments. We evaluate their performance on a
Chinese?English translation task. The re-
sults indicate that we can achieve signifi-
cant gains over the baseline with both ap-
proaches, but in particular the moments-
based estimator is both faster and performs
better than EM.
1 Introduction
Translation models based on synchronous context-
free grammars (SCFGs) treat the translation prob-
lem as a context-free parsing problem. A parser
constructs trees over the input sentence by pars-
ing with the source language projection of a syn-
chronous CFG, and each derivation induces trans-
lations in the target language (Chiang, 2007).
However, in contrast to syntactic parsing, where
linguistic intuitions can help elucidate the ?right?
tree structure for a grammatical sentence, no such
intuitions are available for synchronous deriva-
tions, and so learning the ?right? grammars is a
central challenge.
Of course, learning synchronous grammars
from parallel data is a widely studied problem
(Wu, 1997; Blunsom et al., 2008; Levenberg et
al., 2012, inter alia). However, there has been
less exploration of learning rich non-terminal cat-
egories, largely because previous efforts to learn
such categories have been coupled with efforts
to learn derivation structures?a computationally
formidable challenge. One popular approach has
been to derive categories from source and/or target
monolingual grammars (Galley et al., 2004; Zoll-
mann and Venugopal, 2006; Hanneman and Lavie,
2013). While often successful, accurate parsers
are not available in many languages: a more ap-
pealing approach is therefore to learn the category
structure from the data itself.
In this work, we take a different approach to
previous work in synchronous grammar induc-
tion by assuming that reasonable tree structures
for a parallel corpus can be chosen heuristically,
and then, fixing the trees (thereby enabling us to
sidestep the worst of the computational issues), we
learn non-terminal categories as latent variables to
explain the distribution of these synchronous trees.
This technique has a long history in monolingual
parsing (Petrov et al., 2006; Liang et al., 2007;
Cohen et al., 2014), where it reliably yields state-
of-the-art phrase structure parsers based on gen-
erative models, but we are the first to apply it to
translation.
We first generalize the concept of latent PCFGs
to latent-variable SCFGs (?2). We then follow
by a presentation of the tensor-based formulation
for our parameters, a representation that makes it
convenient to marginalize over latent states. Sub-
sequently, two methods for parameter estimation
are presented (?4): a spectral approach based on
the method of moments, and an EM-based likeli-
hood maximization. Results on a Chinese?English
evaluation set (?5) indicate significant gains over
baselines and point to the promise of using latent-
variable synchronous grammars in conjunction
with a smaller, simpler set of rules instead of un-
wieldy and bloated grammars extracted via exist-
ing heuristics, where a large number of context-
independent but un-generalizable rules are uti-
lized. Hence, the hope is that this work pro-
1953
motes the move towards translation models that
directly model the conditional likelihood of trans-
lation rules via (potentially feature-rich) latent-
variable models which leverage information con-
tained in the synchronous tree structure, instead
of relying on a heuristic set of features based on
empirical relative frequencies (Koehn et al., 2003)
from non-hierarchical phrase-based translation.
2 Latent-Variable SCFGs
Before discussing parameter learning, we in-
troduce latent-variable synchronous context-free
grammars (L-SCFGs) and discuss an inference al-
gorithm for marginalizing over latent states.
We extend the definition of L-PCFGs (Mat-
suzaki et al., 2005; Petrov et al., 2006) to syn-
chronous grammars as used in machine transla-
tion (Chiang, 2007). A latent-variable SCFG (L-
SCFG) is a 6-tuple (N ,m, n
s
, n
t
, pi, t) where:
? N is a set of non-terminal (NT) symbols in the
grammar. For hierarchical phrase-based transla-
tion (HPBT), the set consists of only two sym-
bols, X and a goal symbol S.
? [m] is the set of possible hidden states associ-
ated with NTs. Aligned pairs of NTs across the
source and target languages share the same hid-
den state.
? [n
s
] is the set of source side words, i.e., the
source-side vocabulary, with [n
s
] ?N = ?.
? [n
t
] is the set of target side words, i.e., the
target-side vocabulary, with [n
t
] ?N = ?.
? The synchronous production rules compose a
setR = R
0
?R
1
?R
2
:
? Arity 2 (binary) rules (R
2
):
a(h
1
)? ??
1
b(h
2
)?
2
c(h
3
)?
3
, ?
1
b(h
2
)?
2
c(h
3
)?
3
?
or
a(h
1
)? ??
1
b(h
2
)?
2
c(h
3
)?
3
, ?
1
c(h
2
)?
2
b(h
3
)?
3
?
where a, b, c ? N , h
1
, h
2
, h
3
? [m],
?
1
, ?
2
, ?
3
? [n
s
]
?
and ?
1
, ?
2
, ?
3
? [n
t
]
?
.
? Arity 1 (unary) rules (R
1
):
a(h
1
)? ??
1
b(h
2
)?
2
, ?
1
b(h
2
)?
2
?
where a, b ? N , h
1
, h
2
? [m], ?
1
, ?
2
? [n
s
]
?
and ?, ?
2
? [n
t
]
?
.
? Pre-terminal rules (R
0
): a(h
1
) ? ??, ??
where a ? N , ? ? [n
t
]
?
and ? ? [n
s
]
?
.
Each of these rules is associated with a proba-
bility t(a(h
1
) ? ?|a, h
1
) where ? is the right-
hand side (RHS) of the rule.
? For a ? N , h ? [m], pi(a, h) is a parameter
specifying the root probability of a(h).
A skeletal tree (s-tree) for a sentence is the set
of rules in the synchronous derivation of that sen-
tence, without any additional latent state informa-
tion or decoration. A full tree consists of an s-
tree r
1
, . . . , r
N
together with values h
1
, . . . , h
N
for every NT in the tree. An important point to
keep in mind in comparison to L-PCFGs is that
the right-hand side (RHS) non-terminals of syn-
chronous rules are aligned pairs across the source
and target languages.
In this work, we refine the one-category gram-
mar introduced by Chiang (2007) for HPBT in or-
der to learn additional latent NT categories. Thus,
the following discussion is restricted to these kinds
of grammars, although the method is equally ap-
plicable in other scenarios, e.g., the extended tree-
to-string transducer (xRs) formalism (Huang et
al., 2006; Graehl et al., 2008) commonly used in
syntax-directed translation, and phrase-based MT
(Koehn et al., 2003).
Marginal Inference with L-SCFGs. For a pa-
rameter t of rule r, the latent state h
1
attached to
the left-hand side (LHS) NT of r is associated with
the outside tree for the sub-tree rooted at the LHS,
and the states attached to the RHS NTs are asso-
ciated with the inside trees of that NT. Since we
do not assume conditional independence of these
states, we need to consider all possible interac-
tions, which can be compactly represented as a
3
rd
-order tensor in the case of a binary rule, a ma-
trix (i.e., a 2
nd
-order tensor) for unary rules, and
a vector for pre-terminal (lexical) rules. Prefer-
ences for certain outside-inside tree combinations
are reflected in the values contained in these tensor
structures. In this manner, we intend to capture in-
teractions between non-local context of a phrase,
which can typically be represented via features de-
fined over outside trees of the node spanning the
phrase, and the interior context, correspondingly
defined via features over the inside trees. We re-
fer to these tensor structures collectively as C
r
for
rules r ? R, which encompass the parameters t.
For r ? R
0
: C
r
? R
m?1
; similarly for
r ? R
1
: C
r
? R
m?m
and r ? R
2
: C
r
?
R
m?m?m
. We also maintain a vector C
S
? R
1?m
corresponding to the parameters pi(S, h) for the
1954
Inputs: Sentence f
1
. . . f
N
, L-SCFG (N , S,m, n), param-
eters C
r
? R
(m?m?m)
, ? R
(m?m)
, or ? R
(m?1)
for all
r ? R, C
S
? R
(1?m)
, hypergraphH.
Data structures:
For each node q ? H:
? ?(q) ? Rm?1 is a column vector of inside terms.
? ?(q) ? R1?m is a row vector of outside terms.
? For each incoming edge e ? B(q) to node q, ?(e) is a
marginal probability for edge (rule) e.
Algorithm:
. Inside Computation
For nodes q in topological order inH,
?(q) = 0
For each incoming edge e ? B(q),
tail = t(e), rule = r(e)
if |tail| = 0, then ?(q) = ?(q) + C rule
else if |tail| = 1, then ?(q) = ?(q) +
C
rule
?
1
?(tail
0
)
else if |tail| = 2, then ?(q) = ?(q) +
C
rule
?
2
?(tail
1
)?
1
?(tail
0
)
. Outside Computation
For q ? H,
?(q) = 0
?(goal) = CS
For q in reverse topological order inH,
For each incoming edge e ? B(q),
tail = t(e), rule = r(e)
if |tail| = 1, then
?(tail
0
) = ?(tail
0
) + ?(q)?
0
C
rule
else if |tail| = 2, then
?(tail
0
) = ?(tail
0
) +
?(q)?
0
C
rule
?
2
?(tail
1
)
?(tail
1
) = ?(tail
1
) +
?(q)?
0
C
rule
?
1
?(tail
0
)
.Edge Marginals
Sentence probability g = ?(goal)? ?(goal)
For edge e ? H,
head = h(e), tail = t(e), rule = r(e)
if |tail| = 0, then ?(e) = (?(head)?
0
C
rule
)/g
else if |tail| = 1, then ?(e) = (?(head) ?
0
C
rule
?
1
?(tail
0
))/g
else if |tail| = 2, then ?(e) = (?(head) ?
0
C
rule
?
2
?(tail
1
)?
1
?(tail
0
))/g
Figure 1: The tensor form of the hypergraph inside-
outside algorithm, for calculation of rule marginals ?(e). A
slight simplification in the marginal computation yields NT
marginals for spans ?(X, i, j). B(q) returns the incoming hy-
peredges for node q, and h(e), t(e), r(e) return the head node,
tail nodes, and rule for hyperedge e.
goal node (root). These parameters participate in
tensor-vector operations: a 3
rd
-order tensor C
r
2
can be multiplied along each of its three modes
(?
0
,?
1
,?
2
), and if multiplied by an m ? 1 vec-
tor, will produce an m?m matrix.
1
Note that ma-
trix multiplication can be represented by ?
1
when
multiplying on the right and ?
0
when multiplying
on the left of the matrix. The decoder computes
marginal probabilities for each skeletal rule in the
1
This operation is sometimes called a contraction.
parse forest of a source sentence by marginaliz-
ing over the latent states, which in practice corre-
sponds to simple tensor-vector products. This op-
eration is not dependent on the manner in which
the parameters were estimated.
Figure 1 presents the tensor version of the
inside-outside algorithm for decoding L-SCFGs.
The algorithm takes as input the parse forest of
the source sentence represented as a hypergraph
(Klein and Manning, 2001), which is computed
using a bottom-up parser with Earley-style rules
similar to the algorithm in Chiang (2007). Hyper-
graphs are a compact way to represent a forest of
multiple parse trees. Each node in the hypergraph
corresponds to an NT span, and can have multiple
incoming and outgoing hyperedges. Hyperedges,
which connect one or more tail nodes to a single
head node, correspond exactly to rules, and tail or
head nodes correspond to children (RHS NTs) or
parent (LHS NT). The function B(q) returns all in-
coming hyperedges to a node q, i.e., all rules such
that the LHS NT of the rule corresponds to the NT
span of the node q. The algorithm computes inside
and outside probabilities over the hypergraph us-
ing the tensor representations, and converts these
probabilities to marginal rule probabilities. It is
similar to the version presented in Cohen et al.
(2014), but adapted to hypergraph parse forests.
The complexity of this decoding algorithm is
O(n
3
m
3
|G|) where n is the length of the input
sentence, m is the number of latent states, and |G|
is the number of production rules in the grammar
without latent-variable annotations (i.e., m = 1).
2
The bulk of the computation is a series of tensor-
vector products of relatively small size (each di-
mension is of length m), which can be computed
very quickly and in parallel. The tensor computa-
tions can be significantly sped up using techniques
described by Cohen and Collins (2012), so that
they are linear in m and not cubic.
3 Derivation Trees for Parallel Sentences
To estimate the parameters t and pi of an L-
SCFG (discussed in detail in the next section),
we assume the existence of a dataset composed
of synchronous s-trees, which can be acquired
from word alignments. Normally in phrase-based
translation models, we consider all possible phrase
2
In practice, the term m
3
|G| can be replaced with a
smaller term, which separates the rules inG by the number of
NTs on the RHS. This idea relates to the notion of ?effective
grammar size? which we discuss in ?5.
1955
pairs consistent with the word alignments and es-
timate features based on surface statistics associ-
ated with the phrase pairs or rules. The weights of
these features are then learned using a discrimina-
tive training algorithm (Och, 2003; Chiang, 2012,
inter alia). In contrast, in this work we restrict
the number of possible synchronous derivations
for each sentence pair to just one; thus, derivation
forests do not have to be considered, making pa-
rameter estimation more tractable.
3
To achieve this objective, for each sentence in
the training data we extract the minimal set of
synchronous rules consistent with the word align-
ments, as opposed to the composed set of rules
(Galley et al., 2006). Composed rules are ones that
can be formed from smaller rules in the grammar;
with these rules, there are multiple synchronous
trees consistent with the alignments for a given
sentence pair, and thus the total number of applica-
ble rules can be combinatorially larger than if we
just consider the set of rules that cannot be formed
from other rules, namely the minimal rules. The
rule types across all sentence pairs are combined
to form a minimal grammar.
4
To extract a set of
minimal rules, we use the linear-time extraction
algorithm of Zhang et al. (2008). We give a rough
description of their method below, and refer the
reader to the original paper for additional details.
The algorithm returns a complete minimal
derivation tree for each word-aligned sentence
pair, and generalizes an approach for finding all
common intervals (pairs of phrases such that no
word pair in the alignment links a word inside
the phrase to a word outside the phrase) between
two permutations (Uno and Yagiura, 2000) to se-
quences with many-to-many alignment links be-
tween the two sides, as in word alignment. The
key idea is to encode all phrase pairs of a sen-
tence alignment in a tree of size proportional to
the source sentence length, which they call the
normalized decomposition tree. Each node cor-
responds to a phrase pair, with larger phrase spans
represented by higher nodes in the tree. Construct-
ing the tree is analogous to finding common in-
tervals in two permutations, a property that they
leverage to propose a linear-time algorithm for tree
3
For future work, we will consider efficient algorithms for
parameter estimation over derivation forests, since there may
be multiple valid ways to explain the sentence pair via a syn-
chronous tree structure.
4
Table 2 presents a comparison of grammar sizes for our
experiments (?5.1).
extraction. Converting the tree to a set of minimal
SCFG rules for the sentence pair is straightfor-
ward, by replacing nodes corresponding to spans
with lexical items or NTs in a bottom-up manner.
5
By using minimal rules as a starting point
instead of the traditional heuristically-extracted
rules (Chiang, 2007) or arbitrary compositions of
minimal rules (Galley et al., 2006), we are also
able to explore the transition from minimal rules
to composed ones in a principled manner by en-
coding contextual information through the latent
states. Thus, a beneficial side effect of our re-
finement process is the creation of more context-
specific rules without increasing the overall size
of the baseline grammar, instead holding this in-
formation in our parameters C
r
.
4 Parameter Estimation for L-SCFGs
We explore two methods for estimating the param-
eters C
r
of the model: a likelihood-maximization
approach based on EM (Dempster et al., 1977),
and a spectral approach based on the method of
moments (Hsu et al., 2009; Cohen et al., 2014),
where we identify a subspace using a singular
value decomposition (SVD) of the cross-product
feature space between inside and outside trees and
estimate parameters in this subspace.
Figure 2 presents a side-by-side comparison of
the two algorithms, which we discuss in this sec-
tion. In the spectral approach, we base our pa-
rameter estimates on low-rank representations of
moments of features, while EM explicitly maxi-
mizes a likelihood criterion. The parameter es-
timation algorithms are relatively similar, but in
lieu of sparse feature functions in the spectral case,
EM uses partial counts estimated with the current
set of parameters. The nature of EM allows it to
be susceptible to local optima, while the spectral
approach comes with guarantees on obtaining the
global optimum (Cohen et al., 2014). Lastly, com-
puting the SVD and estimating parameters in the
low-rank space is a one-shot operation, as opposed
to the iterative procedure of EM, and therefore is
much more computationally efficient.
4.1 Estimation with Spectral Method
We generalize the parameter estimation algorithm
presented in Cohen et al. (2013) to the syn-
5
We filtered rules with arity 3 and above (i.e., containing
more than 3 NTs on the RHS). While the L-SCFG formalism
is perfectly capable of handling such cases, it would have re-
sulted in higher order tensors for our parameter structures.
1956
Inputs:
Training examples (r
(i)
, t
(i,1)
, t
(i,2)
, t
(i,3)
, o
(i)
, b
(i)
)
for i ? {1 . . .M}, where r
(i)
is a context free rule;
t
(i,1)
, t
(i,2)
, and t
(i,3)
are inside trees; o
(i)
is an out-
side tree; and b
(i)
= 1 if the rule is at the root of tree,
0 otherwise. A function ? that maps inside trees t to
feature-vectors ?(t) ? R
d
. A function ? that maps
outside trees o to feature-vectors ?(o) ? R
d
?
.
Algorithm:
. Step 0: Singular Value Decomposition
? Compute the SVD of Eq. 1 to calculate matri-
ces
?
U ? R
(d?m)
and
?
V ? R
(d
?
?m)
.
. Step 1: Projection
Y (t) = U
>
?(t)
Z(o) = ?
?1
V
>
?(o)
. Step 2: Calculate Correlations
?
E
r
=
?
?
??
?
??
?
o?Q
r
Z(o)
|Q
r
|
if r ? R
0
?
(o,t)?Q
r
Z(o)?Y (t)
|Q
r
|
if r ? R
1
?
(
o,t
2
,t
3
)
?Q
r
Z(o)?Y (t
2
)?Y (t
3
)
|Q
r
|
if r ? R
2
Q
r
is the set of outside-inside tree triples for binary
rules, outside-inside tree pairs for unary rules, and
outside trees for pre-terminals.
. Step 3: Compute Final Parameters
? For all r ? R,
?
C
r
=
count(r)
M
?
?
E
r
? For all r
(i)
? {1, . . . ,M} such that b
(i)
is 1,
?
C
S
=
?
C
S
+
Y (t
(i,1)
)
|Q
S
|
Q
S
is the set of trees at the root.
(a) The spectral learning algorithm for estimating pa-
rameters of an L-SCFG.
Inputs:
Training examples (r
(i)
, t
(i,1)
, t
(i,2)
, t
(i,3)
, o
(i)
, b
(i)
) for i ?
{1 . . .M}, where r
(i)
is a context free rule; t
(i,1)
, t
(i,2)
, and
t
(i,3)
are inside trees; o
(i)
is an outside tree; b
(i)
= 1 if the rule
is at the root of tree, 0 otherwise; and MAX ITERATIONS.
Algorithm:
. Step 0: Parameter Initialization
For rule r ? R,
? if r ? R
0
: initialize
?
C
r
? R
m?1
? if r ? R
1
: initialize
?
C
r
R
m?m
? if r ? R
2
: initialize
?
C
r
R
m?m?m
Initialize
?
C
S
? R
m?1
?
C
r
0
=
?
C
r
,
?
C
S
0
=
?
C
S
For iteration t = 1, . . . ,MAX ITERATIONS,
? Expectation Step:
. Estimate Y and Z
Compute partial counts and total tree probabili-
ties g for all t and o using Fig. 1 and parameters
?
C
r
t?1
,
?
C
S
t?1
.
. Calculate Correlations
?
E
r
=
?
?
?
??
?
?
??
?
o,g?Q
r
Z(o)
g
if r ? R
0
?
(o,t,g)?Q
r
Z(o)?Y (t)
g
if r ? R
1
?
(
o,t
2
,t
3
,g
)
?Q
r
Z(o)?Y (t
2
)?Y (t
3
)
g
if r ? R
2
. Update Parameters
For all r ? R,
?
C
r
t
=
?
C
r
t?1

?
E
r
For all r
(i)
? {1, . . . ,M} such that b
(i)
is 1,
?
C
S
t
=
?
C
S
t
+ (
?
C
S
t?1
 Y (r
(i)
))/g
Q
S
is the set of trees at the root.
? Maximization Step
if r ? R
0
: ?h
1
:
?
C
r
(h
1
) =
?
C
r
(h
1
)
?
r
?
=r
?
h
1
?
C
r
?
(h
1
)
if r ? R
1
: ?h
1
, h
2
:
?
C
r
(h
1
, h
2
) =
?
C
r
(h
1
,h
2
)
?
r
?
=r
?
h
2
?
C
r
?
(h
1
,h
2
)
if r ? R
2
: ?h
1
, h
2
, h
3
:
?
C
r
(h
1
, h
2
, h
3
) =
?
C
r
(h
1
,h
2
,h
3
)
?
r
?
=r
?
h
2
,h
3
?
C
r
?
(h
1
,h
2
,h
3
)
if LHS(r) = S: ?h
1
:
?
C
r
(h
1
) =
?
C
r
(h
1
)
?
r
?
=r
?
h
1
?
C
r
?
(h
1
)
(b) The EM-based algorithm for estimating parameters of an L-
SCFG.
Figure 2: The two parameter estimation algorithms proposed for L-SCFGs; (a) method of moments; (b) expectation maxi-
mization.  is the element-wise multiplication operator.
chronous or bilingual case. The central concept
of the spectral parameter estimation algorithm is
to learn an m-dimensional representation of in-
side and outside trees by defining these trees in
terms of features, in combination with a projection
step (SVD), with the hope being that the lower-
dimensional space captures the syntactic and se-
mantic regularities among rules from the sparse
feature space. Every NT in an s-tree has an as-
sociated inside and outside tree; the inside tree
contains the entire sub-tree at and below the NT,
and the outside tree is everything else in the syn-
chronous s-tree except the inside tree. The inside
feature function ? maps the domain of inside tree
1957
fragments to a d-dimensional Euclidean space,
and the outside feature function ? maps the do-
main of outside tree fragments to a d
?
-dimensional
space. The specific features we used are discussed
in ?5.2.
Let O be the set of all tuples of inside-outside
trees in our training corpus, whose size is equiva-
lent to the number of rule tokens (occurrences in
the corpus)M , and let ?(t) ? R
d?1
, ?(o) ? R
d
?
?1
be the inside and outside feature functions for in-
side tree t and outside tree o. By computing the
outer product ? between the inside and outside
feature vectors for each pair and aggregating, we
obtain the empirical inside-outside feature covari-
ance matrix:
?
? =
1
|O|
?
(o,t)?O
?(t) (?(o))
>
(1)
If m is the desired latent space dimension, we
compute an m-rank truncated SVD of the empir-
ical covariance matrix
?
? ? U?V
>
, where U ?
R
d?m
and V ? R
d
?
?m
are the matrices containing
the left and right singular vectors, and ? ? R
m?m
is a diagonal matrix containing the m-largest sin-
gular values along its diagonal.
Figure 2a provides the remaining steps in the
algorithm. The M training examples are obtained
by considering all nodes in all of the synchronous
s-trees given as input. In step 1, for each inside
and outside tree, we project its high-dimensional
representation to the m-dimensional latent space.
Using the m-dimensional representations for in-
side and outside trees, in step 2 for each rule type r
we compute the covariance between the inside tree
vectors and the outside tree vector using the ten-
sor product, a generalized outer product to com-
pute covariances between more than two random
vectors. For binary rules, with two child inside
vectors and one outside vector, the result
?
E
r
is a
3-mode tensor; for unary rules, a regular matrix,
and for pre-terminal rules with no right-hand side
non-terminals, a vector. The final parameter es-
timate is then the associated tensor/matrix/vector,
scaled by the maximum likelihood estimate of the
rule r, as in step 3.
The corresponding theoretical guarantees from
Cohen et al. (2014) can also be generalized to
the synchronous case.
?
? is an empirical esti-
mate of the true covariance matrix ?, and if ?
has rank m, then the marginals computed using
the spectrally-estimated parameters will converge
to the true marginals, with the sample complexity
for convergence inversely proportional to a poly-
nomial function of the m
th
largest singular value
of ?.
4.2 Estimation with EM
A likelihood maximization approach can also be
used to learn the parameters of an L-SCFG. Pa-
rameters are initialized by sampling each param-
eter value
?
C
r
(h
1
, h
2
, h
3
) from the interval [0, 1]
uniformly at random.
6
We first decode the train-
ing corpus using an existing set of parameters to
compute the inside and outside probability vectors
associated with NTs for every rule in each s-tree,
constrained to the tree structure of the training ex-
ample. These probabilities can be computed us-
ing the decoding algorithm in Figure 1 (where ?
and ? correspond to the inside and outside proba-
bilities respectively), except the parse forest con-
sists of a single tree only. These vectors repre-
sent partial counts over latent states. We then de-
fine functions Y and Z (analogous to the spectral
case) which map inside and outside tree instances
to m-dimensional vectors containing these partial
counts. In the spectral case, Y and Z are estimated
just once, while in the case of EM they have to be
re-estimated at each iteration.
The expectation step thus consists of comput-
ing the partial counts of inside and outside trees t
and o, i.e., recovering the functions Y and Z, and
updating parameters C
r
by computing correla-
tions, which involves summing over partial counts
(across all occurrences of a rule in the corpus).
Each partial count?s contribution is divided by a
normalization factor g, which is the total probabil-
ity of the tree which t or o is part of. Note that
unlike the spectral case, there is a specific normal-
ization factor for each inside-outside tuple. Lastly,
the correlations are scaled by the existing parame-
ter estimates.
To obtain the next set of parameters, in the max-
imization step we normalize
?
C
r
for r ? R such
that for every h
1
,
?
r
?
=r,h
2
,h
3
?
C
r
?
(h
1
, h
2
, h
3
) = 1
for r ? R
2
,
?
r
?
=r,h
2
?
C
r
?
(h
1
, h
2
) = 1 for r ? R
1
,
and
?
r
?
=r,h
2
?
C
r
?
(h
2
) = 1 for r ? R
0
. We
also normalize the root rule parameters
?
C
r
where
LHS(r) = S. It is also possible to add sparse,
overlapping features to an EM-based estimation
6
In our experiments, we also tried the initialization
scheme described in Matsuzaki et al. (2005), but found that it
provided little benefit.
1958
procedure (Berg-Kirkpatrick et al., 2010) and we
leave this extension for future work.
5 Experiments
The goal of the experimental section is to evalu-
ate the performance of the latent-variable SCFG
in comparison to a baseline without any additional
NT annotations (MIN-GRAMMAR), and to com-
pare the performance of the two parameter esti-
mation algorithms. We also compare L-SCFGs to
a HIERO baseline (Chiang, 2007). The language
pair of evaluation is Chinese?English (ZH-EN).
We score translations using BLEU (Papineni
et al., 2002). The latent-variable model is inte-
grated into the standard MT pipeline by comput-
ing marginal probabilities for each rule in the parse
forest of a source sentence using the algorithm in
Figure 1 with the parameters estimated through
the algorithms in Figure 2, and is added as a fea-
ture for the rule during MERT (Och, 2003). These
probabilities are conditioned on the LHS (X), and
are thus joint probabilities for a source-target RHS
pair. We also write out as features the condi-
tional relative frequencies
?
P (e|f) and
?
P (f |e) as
estimated by our latent-variable model, i.e., con-
ditioned on the source and target RHS.
Overall, we find that both the spectral and
the EM-based estimators improve upon a mini-
mal grammar baseline with only a single cate-
gory, but the spectral approach does better. In fact,
it matches the performance of the standard HI-
ERO baseline, despite learning on top of a minimal
grammar.
5.1 Data and Baselines
The ZH-EN data is the BTEC parallel corpus
(Paul, 2009); we combine the first and second
development sets in one, and evaluate on the third
development set. The development and test sets
are evaluated with 16 references. Statistics for
the data are shown in Table 1. We used the CDEC
decoder (Dyer et al., 2010) to extract word align-
ments and the baseline hierarchical grammars,
MERT tuning, and decoding. We used a 4-gram
language model built from the target-side of the
parallel training data. The Python-based imple-
mentation of the tensor-based decoder, as well as
the parameter estimation algorithms is available at
github.com/asaluja/spectral-scfg/.
The baseline HIERO system uses a grammar ex-
tracted by applying the commonly used heuris-
ZH-EN
TRAIN (SRC) 334K
TRAIN (TGT) 366K
DEV (SRC) 7K
DEV (TGT) 7.6K
TEST (SRC) 3.8K
TEST (TGT) 3.9K
Table 1: Corpus statistics (in words). For the target DEV and
TEST statistics, we take the first reference.
tics (Chiang, 2007). Each rule is decorated with
two lexical and phrasal features corresponding to
the forward (e|f) and backward (f |e) conditional
log frequencies, along with the log joint frequency
(e, f), the log frequency of the source phrase (f),
and whether the phrase pair or the source phrase
is a singleton. Weights for the language model
(and language model OOV), glue rule, and word
penalty are also tuned. The MIN-GRAMMAR
baseline
7
maintains the same set of weights.
Grammar Number of Rules
HIERO 1.69M
MIN-GRAMMAR 59K
LV m = 1 27.56K
LV m = 8 3.18M
LV m = 16 22.22M
Table 2: Grammar sizes for the different systems; for the
latent-variable models, effective grammar sizes are provided.
Grammar sizes are presented in Table 2. For
the latent-variable models, we provide the effec-
tive grammar size, where the number of NTs on
the RHS of a rule is taken into account when com-
puting the grammar size, by assuming each possi-
ble latent variable configuration amongst the NTs
generates a different rule. Furthermore, all single-
tons are mapped to the OOV rule, while we in-
clude singletons in MIN-GRAMMAR.
8
Hence, ef-
fective grammar size can be computed as m(1 +
|R
>1
0
|) +m
2
|R
1
|+m
3
|R
2
|, whereR
>1
0
is the set
of pre-terminal rules that occur more than once.
5.2 Spectral Features
We use the following set of sparse, binary features
in the spectral learning process:
7
Code to extract the minimal derivation trees is available
at www.cs.rochester.edu/u/gildea/mt/.
8
This OOV mapping is done so that the latent-variable
model can handle unknown tokens.
1959
? Rule Indicator. For the inside features, we con-
sider the rule production containing the current
non-terminal on the left-hand side, as well as
the rules of the children (distinguishing between
left and right children for binary rules). For
the outside features, we consider the parent rule
production along with the rule production of the
sibling (if it exists).
? Lexical. for both the inside and outside fea-
tures, any lexical items that appear in the rule
productions are recorded. Furthermore, we con-
sider the first and last words of spans (left and
right child spans for inside features, distinguish-
ing between the two if both exist, and sibling
span for outside features). Source and target
words are treated separately.
? Length. the span length of the tree and each
of its children for inside features, and the span
length of the parent and sibling for outside fea-
tures.
In our experiments, we instantiated a total of
170,000 rule indicator features, 155,000 lexical
features, and 80 length features.
5.3 Chinese?English Experiments
Table 3 presents a comprehensive evaluation of the
ZH-EN experimental setup. The first section con-
sists of the various baselines we consider. In ad-
dition to the aforementioned baselines, we eval-
uated a setup where the spectral parameters sim-
ply consist of the joint maximum likelihood esti-
mates of the rules. This baseline should perform
en par with MIN-GRAMMAR, which we see is the
case on the development set. The performance
on the test set is better though, primarily because
we also include the reverse log relative frequency
(f |e) computed from the latent-variable model as
an additional feature in MERT. Furthermore, in
line with previous work (Galley et al., 2006) which
compares minimal and composed rules, we find
that minimal grammars take a hit of more than 2.5
BLEU points on the development set, compared to
composed (HIERO) grammars. The m = 1 spec-
tral baseline with only rule indicator features per-
forms slightly better than the minimal grammar
baseline, since it overtly takes into account inside-
outside tree combination preferences in the param-
eters, but improvement is minimal with one latent
state naturally and the performance on the test set
is in line with the MLE baseline.
On top of the baselines, we looked at a number
BLEU
Setup Dev Test
Baselines
HIERO 46.08 55.31
MIN-
GRAMMAR
43.38 51.78
MLE 43.24 52.80
Spectral
m = 1 RI 44.18 52.62
m = 8 RI 44.60 53.63
m = 16 RI 46.06 55.83
m=16 RI+Lex+Sm 46.08 55.22
m=16 RI+Lex+Len 45.70 55.29
m=24 RI+Lex 43.00 51.28
m=32 RI+Lex 43.06 52.16
EM
m = 8 40.53 (0.2) 49.78 (0.5)
m = 16 42.85 (0.2) 52.93 (0.9)
m = 32 41.07 (0.4) 49.95 (0.7)
Table 3: Results for the ZH-EN corpus, comparing across
the baselines and the two parameter estimation techniques.
RI, Lex, and Len correspond to the rule indicator, lexical,
and length features respectively, and Sm denotes smoothing.
For the EM experiments, we selected the best scoring iter-
ation by tuning weights for parameters obtained after 25 it-
erations and evaluating other parameters with these weights.
Results for EM are averaged over 5 starting points, with stan-
dard deviation given in parentheses. Spectral, EM, and MLE
performances compared to the MIN-GRAMMAR baseline are
statistically significant (p < 0.01).
of feature combinations and latent states for the
spectral and EM-estimated latent-variable models.
For the spectral models, we tuned MERT parame-
ters separately for each rank on a set of parameters
estimated from rule indicator features only; subse-
quent variations within a given rank, e.g., the ad-
dition of lexical or length features or smoothing,
were evaluated with the same set of rank-specific
weights from MERT. For EM, we ran parame-
ter estimation with 5 randomly initialized starting
points for 50 iterations; we tuned the MERT pa-
rameters with EM parameters obtained after 25
th
iterations. Similar to the spectral experiments,
we fixed the MERT weight values and evaluated
BLEU performance with parameters after every 5
iterations and chose the iteration with the highest
score on the development set. The results are av-
eraged over the 5 initializations, with standard de-
viation in parentheses.
Firstly, we can see a clear dependence on rank,
with peak performance for the spectral and EM
models occurring at m = 16. In this instance, the
spectral model roughly matches the performance
of the HIERO baseline, but it only uses rules ex-
tracted from a minimal grammar, whose size is a
fraction of the HIERO grammar. The gains seem
to level off at this rank; additional ranks seem to
add noise to the parameters. Feature-wise, addi-
tional lexical and length features add little, prob-
1960
ably because much of this information is encap-
sulated in the rule indicator features. For EM,
m = 16 outperforms the minimal grammar base-
line, but is not at the level of the spectral results.
All EM, spectral, and MLE results are statistically
significant (p < 0.01) with respect to the MIN-
GRAMMAR baseline (Zhang et al., 2004), and the
improvement over the HIERO baseline achieved by
them = 16 rule indicator configuration is also sta-
tistically significant.
The two estimation algorithms differ signifi-
cantly in their estimation time. Given a feature
covariance matrix, the spectral algorithm (SVD,
which was done with Matlab, and correlation com-
putation steps) for m = 16 took 7 minutes, while
the EM algorithm took 5 minutes for each iteration
with this rank.
5.4 Analysis
Figure 3 presents a comparison of the non-
terminal span marginals for two sentences in the
development set. We visualize these differences
through a heat map of the CKY parse chart, where
the starting word of the span is on the rows, and
the span end index is on the columns. Each cell is
shaded to represent the marginal of that particular
non-terminal span, with higher likelihoods in blue
and lower likelihoods in red.
For the most part, marginals at the leaves (i.e.,
pre-terminal marginals) tend to score relatively
similarly across different setups. Higher up in the
chart, the latent SCFG marginals look quite dif-
ferent than the MLE parameters. Most noticeably,
spans starting at the beginning of the sentence are
much more favored. It is these rules that allow
the right translation to be preferred since the MLE
chooses not to place the object of the sentence in
the subject?s span. However, the spectral param-
eters seem to discriminate between these higher-
level rules better than EM, which scores spans
starting with the first word uniformly highly. An-
other interesting point is that the range of likeli-
hoods is much larger in the EM case compared to
the MLE and spectral variants. For the second sen-
tence (row), the 1-best hypothesis produced by all
systems are the same, but the heat map accentuates
the previous observation.
6 Related Work
The goal of refining single-category HPBT gram-
mars or automatically learning the NT categories
in a grammar, instead of relying on noisy parser
outputs, has been explored from several different
angles in the MT literature. Blunsom et al. (2008)
present a Bayesian model for synchronous gram-
mar induction, and place an appropriate nonpara-
metric prior on the parameters. However, their
starting point is to estimate a synchronous gram-
mar with multiple categories from parallel data
(using the word alignments as a prior), while we
aim to refine a fixed grammar with additional la-
tent states. Furthermore, their estimation proce-
dure is extremely expensive and is restricted to
learning up to five NT categories, via a series of
mean-field approximations.
Another approach is to explicitly attach a real-
valued vector to each NT: Huang et al. (2010) use
an external source-language parser for this pur-
pose and score rules based on the similarity be-
tween a source sentence parse and the information
contained in this vector, which explicitly requires
the integration of a good-quality source-language
parser. The EM-based algorithm that we propose
here is similar to what they propose, except that we
need to handle tensor structures. Mylonakis and
Sima?an (2011) select among linguistically moti-
vated non-terminal labels with a cross-validated
version of EM. Although they consider a restricted
hypothesis space, they do marginalize over dif-
ferent derivations therefore their inside-outside al-
gorithm is O(n
6
). In the syntax-directed trans-
lation literature, there have been efforts to relax
or coarsen the hard labels provided by a syntactic
parser in an automatic manner to promote param-
eter sharing (Venugopal et al., 2009; Hanneman
and Lavie, 2013), which is the complement of our
aim in this paper.
The idea of automatically learned grammar re-
finements comes from the monolingual parsing lit-
erature, where phenomena like head lexicalization
can be modeled through latent variables. Mat-
suzaki et al. (2005) look at a likelihood-based
method to split the NT categories of a gram-
mar into a fixed number of sub-categories, while
Petrov et al. (2006) learn a variable number of
sub-categories per NT. The latter?s extension may
be useful for finding the optimal number of latent
states from the data in our case.
The question of whether we can incorporate ad-
ditional contextual information in minimal rule
grammars in MT via auxiliary models instead of
using longer, composed rules has been investi-
gated before as well. n-gram translation mod-
1961
0 1 2 3 4Span End
Span
 start
ing a
t wor
d: 
1.05
0.90
0.75
0.60
0.45
0.30
0.15
0.00
ln(su
m)
I go away .
(a) MLE
0 1 2 3 4Span End
Span
 start
ing a
t wor
d: 
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
ln(su
m)
I ?ll bring it .
(b) Spectral m = 16 RI
0 1 2 3 4Span End
Span
 start
ing a
t wor
d: 
9
8
7
6
5
4
3
2
1
0
ln(su
m)
I ?ll bring it .
(c) EM m = 16
0 1 2 3 4 5 6 7Span End
Span
 start
ing a
t wor
d: 
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
ln(su
m)
I ?d like a shampoo and style .
(d) MLE
0 1 2 3 4 5 6 7Span End
Span
 start
ing a
t wor
d: 
3.2
2.8
2.4
2.0
1.6
1.2
0.8
0.4
0.0
ln(su
m)
I ?d like a shampoo and style .
(e) Spectral m = 16 RI
0 1 2 3 4 5 6 7Span End
Span
 start
ing a
t wor
d: 
10.5
9.0
7.5
6.0
4.5
3.0
1.5
0.0
ln(su
m)
I ?d like a shampoo and style .
(f) EM m = 16
Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans ?(X, i, j) for the MLE,
spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue,
lower likelihoods in red. The hypotheses produced by each setup are below the heat maps.
els (Mari?no et al., 2006; Durrani et al., 2011)
seek to model long-distance dependencies and re-
orderings through n-grams. Similarly, Vaswani
et al. (2011) use a Markov model in the context
of tree-to-string translation, where the parameters
are smoothed with absolute discounting (Ney et
al., 1994), while in our instance we capture this
smoothing effect through low rank or latent states.
Feng and Cohn (2013) also utilize a Markov model
for MT, but learn the parameters through a more
sophisticated estimation technique that makes use
of Pitman-Yor hierarchical priors.
Hsu et al. (2009) presented one of the initial
efforts at spectral-based parameter estimation (us-
ing SVD) of observed moments for latent-variable
models, in the case of Hidden Markov models.
This idea was extended to L-PCFGs (Cohen et al.,
2014), and our approach can be seen as a bilingual
or synchronous generalization.
7 Conclusion
In this work, we presented an approach to re-
fine synchronous grammars used in MT by in-
ferring the latent categories for the single non-
terminal in our grammar rules, and proposed two
algorithms to estimate parameters for our latent-
variable model. By fixing the synchronous deriva-
tions of each parallel sentence in the training data,
it is possible to avoid many of the computational
issues associated with synchronous grammar in-
duction. Improvements over a minimal grammar
baseline and equivalent performance to a hierar-
chical phrase-based baseline are achieved by the
spectral approach. For future work, we will seek
to relax this consideration and jointly reason about
non-terminal categories and derivation structures.
Acknowledgements
The authors would like to thank Daniel Gildea
for sharing his code to extract minimal derivation
trees, Stefan Riezler for useful discussions, Bren-
dan O?Connor for the CKY visualization advice,
and the anonymous reviewers for their feedback.
This work was supported by a grant from eBay
Inc. (Saluja), the U. S. Army Research Laboratory
and the U. S. Army Research Office under con-
tract/grant number W911NF-10-1-0533 (Dyer).
1962
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian Synchronous Grammar Induction. In Pro-
ceedings of NIPS.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
David Chiang. 2012. Hope and Fear for Dis-
criminative Training of Statistical Translation Mod-
els. Journal of Machine Learning Research, pages
1159?1187.
Shay B. Cohen and Michael Collins. 2012. Tensor
decomposition for fast parsing with latent-variable
PCFGs. In Proceedings of NIPS.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Pro-
ceedings of NAACL.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2014. Spectral learning
of latent-variable PCFGs: Algorithms and sample
complexity. Journal of Machine Learning Research.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39(1):1?38.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proceedings of ACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL.
Yang Feng and Trevor Cohn. 2013. A Markov
model of machine translation using non-parametric
bayesian inference. In Proceedings of ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427, September.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009.
A Spectral Algorithm for Learning Hidden Markov
Models. In Proceedings of COLT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Zhongqiang Huang, Martin
?
Cmejrek, and Bowen
Zhou. 2010. Soft syntactic constraints for hierar-
chical phrase-based translation using latent syntactic
distributions. In Proceedings of EMNLP.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of EMNLP-CoNLL.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
dirichlet processes. In Proceedings of EMNLP.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonol-
losa, and Marta R. Costa-juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549, December.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of ACL.
Markos Mylonakis and Khalil Sima?an. 2011. Learn-
ing hierarchical translation structure with linguistic
annotations. In Proceedings of ACL.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On Structuring Probabilistic Dependencies in
Stochastic Language Modelling. Computer Speech
and Language, 8:1?38.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Michael Paul. 2009. Overview of the IWSLT 2009
evaluation campaign. In Proceedings of IWSLT.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
1963
Takeaki Uno and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of two
permutations. Algorithmica, 26(2):290?309.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov models for fast tree-to-
string translation. In Proceedings of ACL.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
Softening syntactic constraints to improve statistical
machine translation. In Proceedings of NAACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system. In
In Proceedings LREC.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
COLING.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, StatMT ?06, pages 138?141.
Association for Computational Linguistics.
1964
Empirical Risk Minimization for
Probabilistic Grammars: Sample
Complexity and Hardness of Learning
Shay B. Cohen?
Columbia University
Noah A. Smith??
Carnegie Mellon University
Probabilistic grammars are generative statistical models that are useful for compositional and
sequential structures. They are used ubiquitously in computational linguistics. We present a
framework, reminiscent of structural risk minimization, for empirical risk minimization of prob-
abilistic grammars using the log-loss. We derive sample complexity bounds in this framework
that apply both to the supervised setting and the unsupervised setting. By making assumptions
about the underlying distribution that are appropriate for natural language scenarios, we are able
to derive distribution-dependent sample complexity bounds for probabilistic grammars. We also
give simple algorithms for carrying out empirical risk minimization using this framework in both
the supervised and unsupervised settings. In the unsupervised case, we show that the problem of
minimizing empirical risk is NP-hard. We therefore suggest an approximate algorithm, similar
to expectation-maximization, to minimize the empirical risk.
1. Introduction
Learning from data is central to contemporary computational linguistics. It is in com-
mon in such learning to estimate a model in a parametric family using the maximum
likelihood principle. This principle applies in the supervised case (i.e., using anno-
tated data) as well as semisupervised and unsupervised settings (i.e., using unan-
notated data). Probabilistic grammars constitute a range of such parametric families
we can estimate (e.g., hidden Markov models, probabilistic context-free grammars).
These parametric families are used in diverse NLP problems ranging from syntactic
and morphological processing to applications like information extraction, question
answering, and machine translation.
Estimation of probabilistic grammars, in many cases, indeed starts with the prin-
ciple of maximum likelihood estimation (MLE). In the supervised case, and with
traditional parametrizations based on multinomial distributions, MLE amounts to
? Department of Computer Science, Columbia University, New York, NY 10027, United States.
E-mail: scohen@cs.columbia.edu. This research was completed while the first author was at Carnegie
Mellon University.
?? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States.
E-mail: nasmith@cs.cmu.edu.
Submission received: 1 November 2010; revised submission received: 21 June 2011; accepted for publication:
3 August 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
normalization of rule frequencies as they are observed in data. In the unsupervised case,
on the other hand, algorithms such as expectation-maximization are available. MLE is
attractive because it offers statistical consistency if some conditions are met (i.e., if the
data are distributed according to a distribution in the family, then we will discover the
correct parameters if sufficient data is available). In addition, under some conditions it
is also an unbiased estimator.
An issue that has been far less explored in the computational linguistics literature
is the sample complexity of MLE. Here, we are interested in quantifying the number of
samples required to accurately learn a probabilistic grammar either in a supervised
or in an unsupervised way. If bounds on the requisite number of samples (known as
?sample complexity bounds?) are sufficiently tight, then they may offer guidance to
learner performance, given various amounts of data and a wide range of parametric
families. Being able to reason analytically about the amount of data to annotate, and
the relative gains in moving to a more restricted parametric family, could offer practical
advantages to language engineers.
We note that grammar learning has been studied in formal settings as a problem of
grammatical inference?learning the structure of a grammar or an automaton (Angluin
1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008,
among others). Our setting in this article is different. We assume that we have a fixed
grammar, and our goal is to estimate its parameters. This approach has shown great
empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005)
and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and
Manning 2004; Cohen and Smith 2010a) settings. There has also been some discus-
sion of sample complexity bounds for statistical parsing models, in a distribution-free
setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis
of natural language, as it has to account for pathological cases of distributions that
generate data.
We develop a framework for deriving sample complexity bounds using the max-
imum likelihood principle for probabilistic grammars in a distribution-dependent
setting. Distribution dependency is introduced here by making empirically justified
assumptions about the distributions that generate the data. Our framework uses and
significantly extends ideas that have been introduced for deriving sample complexity
bounds for probabilistic graphical models (Dasgupta 1997). Maximum likelihood esti-
mation is put in the empirical risk minimization framework (Vapnik 1998) with the loss
function being the log-loss. Following that, we develop a set of learning theoretic tools
to explore rates of estimation convergence for probabilistic grammars. We also develop
algorithms for performing empirical risk minimization.
Much research has been devoted to the problem of learning finite state automata
(which can be thought of as a class of grammars) in the Probably Approximately Correct
setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989;
Pitt 1989; Terwijn 2002). Typically, the setting in these cases is different from our setting:
Error is measured as the probability mass of strings that are not identified correctly by
the learned finite state automaton, instead of measuring KL divergence between the
automaton and the true distribution. In addition, in many cases, there is also a focus on
the distribution-free setting. To the best of our knowledge, it is still an open problem
whether finite state automata are learnable in the distribution-dependent setting when
measuring the error as the fraction of misidentified strings. Other work (Ron 1995; Ron,
Singer, and Tishby 1998; Clark and Thollard 2004; Palmer and Goldberg 2007) also gives
treatment to probabilistic automata with an error measure which is more suitable for
the probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance.
480
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
These also focus on learning the structure of finite state machines. As mentioned earlier,
in our setting we assume that the grammar is fixed, and that our goal is to estimate its
parameters.
We note an important connection to an earlier study about the learnability of
probabilistic automata and hidden Markov models by Abe and Warmuth (1992). In
that study, the authors provided positive results for the sample complexity for learning
probabilistic automata?they showed that a polynomial sample is sufficient for MLE.
We demonstrate positive results for the more general class of probabilistic grammars
which goes beyond probabilistic automata. Abe and Warmuth also showed that the
problem of finding or even approximating the maximum likelihood solution for a two-
state probabilistic automaton with an alphabet of an arbitrary size is hard. Even though
these results extend to probabilistic grammars to some extent, we provide a novel proof
that illustrates the NP-hardness of identifying the maximum likelihood solution for
probabilistic grammars in the specific framework of ?proper approximations? that we
define in this article. Whereas Abe and Warmuth show that the problem of maximum
likelihood maximization for two-state HMMs is not approximable within a certain
factor in time polynomial in the alphabet and the length of the observed sequence, we
show that there is no polynomial algorithm (in the length of the observed strings) that
identifies the maximum likelihood estimator in our framework. In our reduction, from
3-SAT to the problem of maximum likelihood estimation, the alphabet used is binary
and the grammar size is proportional to the length of the formula. In Abe andWarmuth,
the alphabet size varies, and the number of states is two.
This article proceeds as follows. In Section 2 we review the background necessary
from Vapnik?s (1988) empirical risk minimization framework. This framework is re-
duced to maximum likelihood estimation when a specific loss function is used: the log-
loss.1 There are some shortcomings in using the empirical risk minimization framework
in its simplest form. In its simplest form, the ERM framework is distribution-free, which
means that we make no assumptions about the distribution that generated the data.
Naively attempting to apply the ERM framework to probabilistic grammars in the
distribution-free setting does not lead to the desired sample complexity bounds. The
reason for this is that the log-loss diverges whenever small probabilities are allocated in
the learned hypothesis to structures or strings that have a rather large probability in the
probability distribution that generates the data. With a distribution-free assumption,
therefore, we would have to give treatment to distributions that are unlikely to be
true for natural language data (e.g., where some extremely long sentences are very
probable).
To correct for this, we move to an analysis in a distribution-dependent setting, by
presenting a set of assumptions about the distribution that generates the data. In Sec-
tion 3 we discuss probabilistic grammars in a general way and introduce assumptions
about the true distribution that are reasonable when our data come from natural lan-
guage examples. It is important to note that this distribution need not be a probabilistic
grammar.
The next stepwe take, in Section 4, is approximating the set of probabilistic grammars
over which we maximize likelihood. This is again required in order to overcome the
divergence of the log-loss for probabilities that are very small. Our approximations are
1 It is important to remember that minimizing the log-loss does not equate to minimizing the error of a
linguistic analyzer or natural language processing application. In this article we focus on the log-loss
case because we believe that probabilistic models of language phenomena have inherent usefulness
as explanatory tools in computational linguistics, aside from their use in systems.
481
Computational Linguistics Volume 38, Number 3
based on bounded approximations that have been used for deriving sample complexity
bounds for graphical models in a distribution-free setting (Dasgupta 1997).
Our approximations have two important properties: They are, by themselves, prob-
abilistic grammars from the family we are interested in estimating, and they become a
tighter approximation around the family of probabilistic grammars we are interested in
estimating as more samples are available.
Moving to the distribution-dependent setting and defining proper approximations
enables us to derive sample complexity bounds. In Section 5 we present the sample
complexity results for both the supervised and unsupervised cases. A question that
lingers at this point is whether it is computationally feasible to maximize likelihood
in our framework even when given enough samples.
In Section 6, we describe algorithms we use to estimate probabilistic grammars
in our framework, when given access to the required number of samples. We show
that in the supervised case, we can indeed maximize likelihood in our approximation
framework using a simple algorithm. For the unsupervised case, however, we show that
maximizing likelihood is NP-hard. This fact is related to a notion known in the learning
theory literature as inherent unpredictability (Kearns and Vazirani 1994): Accurate
learning is computationally hard evenwith enough samples. To overcome this difficulty,
we adapt the expectation-maximization algorithm (Dempster, Laird, and Rubin 1977)
to approximately maximize likelihood (or minimize log-loss) in the unsupervised case
with proper approximations.
In Section 7 we discuss some related ideas. These include the failure of an alter-
native kind of distributional assumption and connections to regularization by maxi-
mum a posteriori estimation with Dirichlet priors. Longer proofs are included in the
appendices. A table of notation that is used throughout is included as Table D.1 in
Appendix D.
This article builds on two earlier papers. In Cohen and Smith (2010b) we presented
the main sample complexity results described here; the present article includes signifi-
cant extensions, a deeper analysis of our distributional assumptions, and a discussion of
variants of these assumptions, as well as related work, such as that about the Tsybakov
noise condition. In Cohen and Smith (2010c) we proved NP-hardness for unsupervised
parameter estimation of probalistic context-free grammars (PCFGs) (without approxi-
mate families). The present article uses a similar type of proof to achieve results adapted
to empirical risk minimization in our approximation framework.
2. Empirical Risk Minimization and Maximum Likelihood Estimation
We begin by introducing some notation. We seek to construct a predictive model that
maps inputs from space X to outputs from space Z. In this work, X is a set of strings
using some alphabet ? (X ? ??), and Z is a set of derivations allowed by a grammar
(e.g., a context-free grammar). We assume the existence of an unknown joint probability
distribution p(x, z) over X? Z. (For the most part, we will be discussing discrete input
and output spaces. This means that p will denote a probability mass function.) We are
interested in estimating the distribution p from examples, either in a supervised setting,
where we are provided with examples of the form (x, z) ? X? Z, or in the unsupervised
setting, where we are provided only with examples of the form x ? X. We first consider
the supervised setting and return to the unsupervised setting in Section 5. We will use
q to denote the estimated distribution.
482
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
In order to estimate p as accurately as possible using q(x, z), we are interested in
minimizing the log-loss, that is, in finding qopt, from a fixed family of distributions Q
(also called ?the concept space?), such that
qopt = argmin
q?Q
Ep
[
? log q
]
= argmin
q?Q
?
?
x,z
p(x, z) log q(x, z) (1)
Note that if p ? Q, then this quantity achieves theminimumwhen qopt = p, in which case
the value of the log-loss is the entropy of p. Indeed, more generally, this optimization is
equivalent to finding q such that it minimizes the KL divergence from p to q.
Because p is unknown, we cannot hope to minimize the log-loss directly. Given a
set of examples (x1, z1), . . . , (xn, zn), however, there is a natural candidate, the empirical
distribution p?n, for use in Equation (1) instead of p, defined as:
p?n(x, z) = n
?1
n
?
i=1
I {(x, z) = (xi, zi)}
where I {(x, z) = (xi, zi)} is 1 if (x, z) = (xi, zi) and 0 otherwise.2 We then set up the
problem as the problem of empirical risk minimization (ERM), that is, trying to find q
such that
q? = argmin
q?Q
Ep?n
[
? log q
]
(2)
= argmin
q?Q
?n?1
n
?
i=1
log q(xi, zi)
= argmax
q?Q
n?1
n
?
i=1
log q(xi, zi) (3)
Equation (3) immediately shows that minimizing empirical risk using the log-loss is
equivalent to the maximizing likelihood, which is a common statistical principle used
for estimating a probabilistic grammar in computational linguistics (Charniak 1993;
Manning and Schu?tze 1999).3
As mentioned earlier, our goal is to estimate the probability distribution p while
quantifying how accurate our estimate is. One way to quantify the estimation accuracy
is by bounding the excess risk, which is defined as
Ep(q;Q) = Ep(q)  Ep
[
? log q
]
?min
q??Q
Ep
[
? log q?
]
(4)
We are interested in bounding the excess risk for q?, Ep(q
?). The excess risk is
reduced to KL divergence between p and q if p ? Q, because in this case the quantity
minq??Q E
[
? log q?
]
is minimized with q? = p, and equals the entropy of p. In a typical
2 We note that p?n itself is a random variable, because it depends on the sample drawn from p.
3 We note that being able to attain the minimum through an hypothesis q? is not necessarily possible in
the general case. In our instantiations of ERM for probabilistic grammars, however, the minimum can be
attained. In fact, in the unsupervised case the minimum can be attained by more than a single hypothesis.
In these cases, q? is arbitrarily chosen to be one of these minimizers.
483
Computational Linguistics Volume 38, Number 3
case, where we do not necessarily have p ? Q, then the excess risk of q is bounded from
above by the KL divergence between p and q.
We can bound the excess risk by showing the double-sided convergence of the
empirical process Rn(Q), defined as follows:
Rn(Q)  sup
q?Q
?
?Ep?n
[
? log q
]
? Ep
[
? log q
]
?
?? 0 (5)
as n? ?. For any  > 0, if, for large enough n it holds that
sup
q?Q
?
?Ep?n
[
? log q
]
? Ep
[
? log q
]
?
? <  (6)
(with high probability), then we can ?sandwich? the following quantities:
Ep
[
? log qopt
]
? Ep
[
? log q?
]
(7)
? Ep?n
[
? log q?
]
+ 
? Ep?n
[
? log qopt
]
+ 
? Ep
[
? log qopt
]
+ 2 (8)
where the inequalities come from the fact that qopt minimizes the expected risk
Ep
[
? log q
]
for q ? Q, and q? minimizes the empirical risk Ep?n
[
? log q
]
for q ? Q. The
consequence of Equations (7) and (8) is that the expected risk of q? is at most 2 away
from the expected risk of qopt, and as a result, we find the excess risk Ep(q
?), for large
enough n, is smaller than 2. Intuitively, this means that, under a large sample, q? does
not give much worse results than qopt under the criterion of the log-loss.
Unfortunately, the regularity conditions which are required for the convergence of
Rn(Q) do not hold because the log-loss can be unbounded. This means that a modifi-
cation is required for the empirical process in a way that will actually guarantee some
kind of convergence. We give a treatment to this in the next section.
We note that all discussion of convergence in this section has been about conver-
gence in probability. For example, we want Equation (6) to hold with high probability?
for most samples of size n. We will make this notion more rigorous in Section 2.2.
2.1 Empirical Risk Minimization and Structural Risk Minimization Methods
It has been noted in the literature (Vapnik 1998; Koltchinskii 2006) that often the class Q
is too complex for empirical risk minimization using a fixed number of data points.
It is therefore desirable in these cases to create a family of subclasses {Q? | ? ? A}
that have increasing complexity. The more data we have, the more complex our Q?
can be for empirical risk minimization. Structural risk minimization (Vapnik 1998) and
the method of sieves (Grenander 1981) are examples of methods that adopt such an
approach. Structural risk minimization, for example, can be represented in many cases
as a penalization of the empirical risk method, using a regularization term.
In our case, the level of ?complexity? is related to allocation of small probabilities to
derivations in the grammar by a distribution q ? Q. The basic problem is this: Whenever
we have a derivation with a small probability, the log-loss becomes very large (in
absolute value), and this makes it hard to show the convergence of the empirical process
484
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Rn(Q). Because grammars can define probability distributions over infinitely many
discrete outcomes, probabilities can be arbitrarily small and log-loss can be arbitrarily
large.
To solve this issuewith the complexity ofQ, we define in Section 4 a series of approx-
imations {Qn | n ? N} for probabilistic grammars such that
?
n Qn = Q. Our framework
for empirical riskminimization is then set up tominimize the empirical risk with respect
to Qn, where n is the number of samples we draw for the learner:
q?n = argmin
q?Qn
Ep?n
[
? log q
]
(9)
We are then interested in the convergence of the empirical process
Rn(Qn) = sup
q?Qn
?
?Ep?n
[
? log q
]
? Ep
[
? log q
]
?
? (10)
In Section 4 we show that the minimizer q?n is an asymptotic empirical risk minimizer
(in our specific framework), which means that Ep
[
? log q?n
]
? Ep
[
? log q?
]
. Because
we have
?
n Qn = Q, the implication of having asymptotic empirical risk minimization
is that we have Ep(q
?
n ;Qn) ? Ep(q
?;Q).
2.2 Sample Complexity Bounds
Knowing that we are interested in the convergence of Rn(Qn) = supq?Qn |Ep?n
[
? log q
]
?
Ep
[
? log q
]
|, a natural question to ask is: ?At what rate does this empirical process
converge??
Because the quantity Rn(Qn) is a random variable, we need to give a probabilistic
treatment to its convergence. More specifically, we ask the question that is typically
asked when learnability is considered (Vapnik 1998): ?How many samples n are re-
quired so that with probability 1? ? we have Rn(Qn) < ?? Bounds on this number
of samples are also called ?sample complexity bounds,? and in a distribution-free
setting they are described as a function N(, ?,Q), independent of the distribution p that
generates the data.
A complete distribution-free setting is not appropriate for analyzing natural lan-
guage. This setting poses technical difficulties with the convergence ofRn(Qn) and needs
to take into account pathological cases that can be ruled out in natural language data.
Instead, we will make assumptions about p, parametrize these assumptions in several
ways, and then calculate sample complexity bounds of the form N(, ?,Q, p), where the
dependence on the distribution is expressed as dependence on the parameters in the
assumptions about p.
The learning setting, then, can be described as follows. The user decides on a level
of accuracy () which the learning algorithm has to reach with confidence (1? ?). Then,
N(, ?,Q, p) samples are drawn from p and presented to the learning algorithm. The
learning algorithm then returns an hypothesis according to Equation (9).
3. Probabilistic Grammars
We begin this section by discussing the family of probabilistic grammars. A probabilistic
grammar defines a probability distribution over a certain kind of structured object (a
derivation of the underlying symbolic grammar) explained step-by-step as a stochastic
485
Computational Linguistics Volume 38, Number 3
process. Hidden Markov models (HMMs), for example, can be understood as a random
walk through a probabilistic finite-state network, with an output symbol sampled at
each state. PCFGs generate phrase-structure trees by recursively rewriting nonterminal
symbols as sequences of ?child? symbols (each itself either a nonterminal symbol or a
terminal symbol analogous to the emissions of an HMM).
Each step or emission of an HMM and each rewriting operation of a PCFG is
conditionally independent of the others given a single structural element (one HMM
or PCFG state); this Markov property permits efficient inference over derivations given
a string.
In general, a probabilistic grammar ?G,?? defines the joint probability of a string x
and a grammatical derivation z:
q(x, z | ?,G) =
K
?
k=1
Nk
?
i=1
?
?k,i(x,z)
k,i
= exp
K
?
k=1
Nk
?
i=1
?k,i(x, z) log?k,i (11)
where ?k,i is a function that ?counts? the number of times the kth distribution?s
ith event occurs in the derivation. The parameters ? are a collection of K multi-
nomials ??1, . . . ,?K?, the kth of which includes Nk competing events. If we let ?k =
??k,1, . . . ,?k,Nk?, each ?k,i is a probability, such that
?k,?i, ?k,i ? 0
?k,
Nk
?
i=1
?k,i = 1
We denote by ?G this parameter space for ?. The grammar G dictates the support
of q in Equation (11). As is often the case in probabilistic modeling, there are differ-
ent ways to carve up the random variables. We can think of x and z as correlated
structure variables (often x is known if z is known), or the derivation event counts
?(x, z) = ??k,i(x, z)?1?k?K,1?i?Nk as an integer-vector random variable. In this article,
we assume that x is always a deterministic function of z, so we use the distribution
p(z) interchangeably with p(x, z).
Note that there may be many derivations z for a given string x?perhaps even
infinitely many in some kinds of grammars. For HMMs, there are three kinds of multi-
nomials: a starting state multinomial, a transitionmultinomial per state and an emission
multinomial per state. In that case K = 2s+ 1, where s is the number of states. The value
of Nk depends on whether the kth multinomial is the starting state multinomial (in
which case Nk = s), transition multinomial (Nk = s), or emission multinomial (Nk = t,
with t being the number of symbols in the HMM). For PCFGs, each multinomial
among the K multinomials corresponds to a set of Nk context-free rules headed by
the same nonterminal. The parameter ?k,i is then the probability of the ith rule for the
kth nonterminal.
We assume that G denotes a fixed grammar, such as a context-free or regular gram-
mar. We let N =
?K
k=1Nk denote the total number of derivation event types. We use
D(G) to denote the set of all possible derivations of G. We define Dx(G) = {z ? D(G) |
yield(z) = x}. We use deg(G) to denote the ?degree? of G, i.e., deg(G) = maxk Nk. We
let |x| denote the length of the string x, and |z| =
?K
k=1
?Nk
i=1 ?k,i(z) denote the ?length?
(number of event tokens) of the derivation z.
486
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Going back to the notation in Section 2, Q would be a collection of probabilistic
grammars, parametrized by ?, and q would be a specific probabilistic grammar with
a specific ?. We therefore treat the problem of ERM with probabilistic grammars as the
problem of parameter estimation?identifying ? from complete data or incomplete data
(strings x are visible but the derivations z are not). We can also view parameter esti-
mation as the identification of a hypothesis from the concept space Q = H(G) = {h?(z) |
? ? ?G} (where h? is a distribution of the form of Equation [11]) or, equivalently, from
negated log-concept space F(G) = {? log h?(z) | ? ? ?G}. For simplicity of notation, we
assume that there is a fixed grammar G and use H to refer to H(G) and F to refer
to F(G).
3.1 Distributional Assumptions about Language
In this section, we describe a parametrization of assumptions we make about the dis-
tribution p(x, z), the distribution that generates derivations from D(G) (note that p does
not have to be a probabilistic grammar). We first describe empirical evidence about the
decay of the frequency of long strings x.
Figure 1 shows the frequency of sentence length for treebanks in various lan-
guages.4 The trend in the plots clearly shows that in the extended tail of the curve, all
languages have an exponential decay of probabilities as a function of sentence length. To
test this, we performed a simple regression of frequencies using an exponential curve.
We estimated each curve for each language using a curve of the form f (l; c,?) = cl?.
This estimation was done by minimizing squared error between the frequency ver-
sus sentence length curve and the approximate version of this curve. The data points
used for the approximation are (li, pi), where li denotes sentence length and pi denotes
frequency, selected from the extended tail of the distribution. Extended tail here refers
to all points with length longer than l1, where l1 is the length with the highest frequency
in the treebank. The goal of focusing on the tail is to avoid approximating the head
of the curve, which is actually a monotonically increasing function. We plotted the
approximate curve together with a length versus frequency curve for new syntactic
data. It can be seen (Figure 1) that the approximation is rather accurate in these corpora.
As a consequence of this observation, we make a few assumptions about G and
p(x, z):
 Derivation length proportional to sentence length: There is an ? ? 1 such
that, for all z, |z| ? ?|yield(z)|. Further, |z| ? |x|. (This prohibits unary
cycles.)
 Exponential decay of derivations: There is a constant r < 1 and a constant
L ? 0 such that p(z) ? Lr|z|. Note that the assumption here is about the
frequency of length of separate derivations, and not the aggregated
frequency of all sentences of a certain length (cf. the discussion above
referring to Figure 1).
4 Treebanks offer samples of cleanly segmented sentences. It is important to note that the distributions
estimated may not generalize well to samples from other domains in these languages. Our argument
is that the family of the estimated curve is reasonable, not that we can correctly estimate the curve?s
parameters.
487
Computational Linguistics Volume 38, Number 3
Figure 1
A plot of the tail of frequency vs. sentence length in treebanks for English, German, Bulgarian,
Turkish, Spanish, and Chinese. Red lines denote data from the treebank, blue lines denote an
approximation which uses an exponential function of the form f (l; c,?) = cl? (the blue line uses
data which is different from the data used to estimate the curve parameters, c and ?). The
parameters (c,?) are (0.19, 0.92) for English, (0.06, 0.94) for German, (0.26, 0.89) for Bulgarian,
(0.26, 0.83) for Turkish, (0.11, 0.93) for Spanish, and (0.03, 0.97) for Chinese. Squared errors are
0.0005, 0.0003, 0.0007, 0.0003, 0.001, and 0.002 for English, German, Bulgarian, Turkish, Spanish,
and Chinese, respectively.
 Exponential decay of strings: Let ?(k) = |{z ? D(G) | |z| = k}| be the
number derivations of length k in G. We assume that ?(k) is an increasing
function, and complete it such that it is defined over positive numbers by
taking ?(t)  ?(t). Taking r as before, we assume there exists a constant
q < 1, such that ?2(k)rk ? qk (and as a consequence, ?(k)rk ? qk). This
implies that the number of derivations of length kmay be exponentially
large (e.g., as with many PCFGs), but is bounded by (q/r)k.
488
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
 Bounded expectations of rules: There is a B < ? such that Ep
[
?k,i(z)
]
? B
for all k and i.
These assumptions must hold for any pwhose support consists of a finite set. These
assumptions also hold in many cases when p itself is a probabilistic grammar. Also, we
note that the last requirement of bounded expectations is optional, and it can be inferred
from the rest of the requirements: B = L/(1? q)2. We make this requirement explicit for
simplicity of notation later. We denote the family of distributions that satisfy all of these
requirements by P(?,L, r, q,B,G).
There are other cases in the literature of language learning where additional as-
sumptions are made on the learned family of models in order to obtain positive learn-
ability results. For example, Clark and Thollard (2004) put a bound on the expected
length of strings generated from any state of probabilistic finite state automata, which
resembles the exponential decay of strings we have for p in this article.
An immediate consequence of these assumptions is that the entropy of p is finite
and bounded by a quantity that depends on L, r and q.5 Bounding entropy of labels
(derivations) given inputs (sentences) is a common way to quantify the noise in a
distribution. Here, both the sentential entropy (Hs(p) = ?
?
x p(x) log p(x)) is bounded
as well as the derivational entropy (Hd(p) = ?
?
x,z p(x, z) log p(x, z)). This is stated in the
following result.
Proposition 1
Let p ? P(?,L, r, q,B,G) be a distribution. Then, we have
Hs(p) ? Hd(p) ? ? log L+
L log r
(1? q)2
log 1r +
(1+ log L)/ log 1r 
e ?
(?
1+ log L
log 1r
?)
Proof
First note that Hs(p) ? Hd(p) holds by the data processing inequality (Cover and
Thomas 1991) because the sentential probability distribution p(x) is a coarser version
of the derivational probability distribution p(x, z). Now, consider p(x, z). For simplicity
of notation, we use p(z) instead of p(x, z). The yield of z, x, is a function of z, and therefore
can be omitted from the distribution. It holds that
Hd(p) = ?
?
z
p(z) log p(z)
= ?
?
z?Z1
p(z) log p(z)?
?
z?Z2
p(z) log p(z)
= Hd(p,Z1)+Hd(p,Z2)
where Z1 = {z | p(z) > 1/e} and Z2 = {z | p(z) ? 1/e}. Note that the function ?? log?
reaches its maximum for ? = 1/e. We therefore have
Hd(p,Z1) ?
|Z1|
e
5 For simplicity and consistency with the log-loss, we measure entropy in nats, which means we use the
natural logarithm when computing entropy.
489
Computational Linguistics Volume 38, Number 3
We give a bound on |Z1|, the number of ?high probability? derivations. Because we have
p(x, z) ? Lr|z|, we can find the maximum length of a derivation that has a probability of
more than 1/e (and hence, it may appear in Z1) by solving 1/e ? Lr|z| for |z|, which leads
to |z| ? log(1/eL)/ log r. Therefore, there are at most
?(1+logL)/ log 1r 
k=1
?(k) derivations in
|Z1| and therefore we have
|Z1| ?
?
(1+ log L)/ log 1r
?
?
(?
(1+ logL)/ log 1r
?)
Hd(p,Z1) ?
?
(1+ log L)/ log 1r
?
e ?
(?
(1+ logL)/ log 1r
?)
(12)
where we use the monotonicity of ?. Consider Hd(p,Z2) (the ?low probability? deriva-
tions). We have:
Hd(p,Z2) ? ?
?
z?Z2
Lr|z| log
(
Lr|z|
)
? ? log L?
(
L log r
)
?
z?Z2
|z|r|z|
? ? log L?
(
L log r
)
?
?
k=1
?(k)krk
? ? log L?
(
L log r
)
?
?
k=1
kqk (13)
= ? log L+
L log r
(1? q)2
log 1q (14)
where Equation (13) holds from the assumptions about p. Putting Equation (12) and
Equation (14) together, we obtain the result. 
We note that another common way to quantify the noise in a distribution is through
the notion of Tsybakov noise (Tsybakov 2004; Koltchinskii 2006). We discuss this further
in Section 7.1, where we show that Tsybakov noise is too permissive, and probabilistic
grammars do not satisfy its conditions.
3.2 Limiting the Degree of the Grammar
When approximating a family of probabilistic grammars, it is much more convenient
when the degree of the grammar is limited. In this article, we limit the degree of the
grammar by making the assumption that all Nk ? 2. This assumption may seem, at first
glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence,
other formalisms), this assumption does not limit the total generative capacity that we
can have across all context-free grammars.
We first show that any context-free grammar with arbitrary degree can be mapped
to a corresponding grammar with all Nk ? 2 that generates derivations equivalent to
derivations in the original grammar. Such a grammar is also called a ?covering gram-
mar? (Nijholt 1980; Leermakers 1989). Let G be a CFG. Let A be the kth nonterminal.
Consider the rules A? ?i for i ? Nk where A appears on the left side. For each rule
490
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Figure 2
Example of a context-free grammar and its equivalent binarized form.
A? ?i, i < Nk, we create a new nonterminal in G
? such that Ai has two rewrite rules:
Ai ? ?i and Ai ? Ai+1. In addition, we create rules A? A1 and ANk ? ?Nk . Figure 2
demonstrates an example of this transformation on a small context-free grammar.
It is easy to verify that the resulting grammar G? has an equivalent capacity to
the original CFG, G. A simple transformation that converts each derivation in the
new grammar to a derivation in the old grammar would involve collapsing any path
of nonterminals added to G? (i.e., all Ai for nonterminal A) so that we end up with
nonterminals from the original grammar only. Similarly, any derivation in G can be
converted to a derivation in G? by adding new nonterminals through unary application
of rules of the form Ai ? Ai+1. Given a derivation z in G, we denote by ?G
?G? (z) the
corresponding derivation in G? after adding the new non-terminals Ai to z. Throughout
this article, we will refer to the normalized form of G? as a ?binary normal form.?6
Note that K?, the number of multinomials in the binary normal form, is a func-
tion of both the number of nonterminals in the original grammar and the number of
rules in that grammar. More specifically, we have that K? =
?K
k=1Nk + K. To make the
equivalence complete, we need to show that any probabilistic context-free grammar can
be translated to a PCFG with maxk Nk ? 2 such that the two PCFGs induce the same
equivalent distributions over derivations.
Utility Lemma 1
Let ai ? [0, 1], i ? {1, . . . ,N} such that
?
i ai = 1. Define b1 = a1, c1 = 1? a1, bi =
(
ai
ai?1
)
(
bi?1
ci?1
)
, and ci = 1? bi for i ? 2. Then ai =
?
?
i?1
?
j=1
cj
?
? bi.
See Appendix A for the proof of Utility Lemma 1.
Theorem 1
Let ?G,?? be a probabilistic context-free grammar. Let G? be the binarizing transforma-
tion of G as defined earlier. Then, there exists ?? for G? such that for any z ? D(G) we
have p(z | ?,G) = p(?G 
?G? (z) | ?
?,G?).
6 We note that this notion of binarization is different from previous types of binarization appearing in
computational linguistics for grammars. Typically in previous work about binarized grammars such as
CFGs, the grammars are constrained to have at most two nonterminals in the right side in Chomsky
normal form. Another form of binarization for linear context-free rewriting systems is restriction of the
fan-out of the rules to two (Go?mez-Rodr??guez and Satta 2009; Gildea 2010). We, however, limit the
number of rules for each nonterminal (or more generally, the number of elements in each multinomial).
491
Computational Linguistics Volume 38, Number 3
Proof
For the grammar G, index the set {1, ...,K} with nonterminals ranging from A1 to AK.
DefineG? as before.We need to define ??. Index themultinomials inG? by (k, i), each hav-
ing two events. Let ?(k,i),1 = ?k,i, ?(k,i),2 = 1? ?k,i for i = 1 and set ?k,i,1 = ?k,i/?(k,i?1),2,
and ?(k,i?1),2 = 1? ?(k,i?1),2.
?G?,?? is a weighted context-free grammar such that the ?(k,i),1 corresponds to the
ith event in the k multinomial of the original grammar. Let z be a derivation in G and
z? = ?G 
?G? (z). Then, from Utility Lemma 1 and the construction of g
?, we have that:
p(z | ?,G) =
K
?
k=1
Nk
?
i=1
?
?k,i(z)
k,i
=
K
?
k=1
Nk
?
i=1
?k,i(z)
?
l=1
?k,i
=
K
?
k=1
Nk
?
i=1
?k,i(z)
?
l=1
?
?
i?1
?
j=1
?(k,j),2
?
??(k,i),1
=
K
?
k=1
Nk
?
i=1
?
?
i?1
?
j=1
?
?k,i(z)
(k,j),2
?
??
?k,i(z)
(k,i),1
=
K
?
k=1
Nk
?
j=1
2
?
i=1
?
?k,j(z
? )
(k,j),i
= p(z? | ?,G?)
From Chi (1999), we know that the weighted grammar ?G?,?? can be converted to
a probabilistic context-free grammar ?G?,???, through a construction of ?? based on ?,
such that p(z? | ?,G?) = p(z? | ??,G?). 
The proof for Theorem 1 gives a construction the parameters ?? ofG? such that ?G,??
is equivalent to ?G?,???. The construction of ?? can also be reversed: Given ?? for G?, we
can construct ? for G so that again we have equivalence between ?G,?? and ?G?,???.
In this section, we focused on presenting parametrized, empirically justified distri-
butional assumptions about language data that will make the analysis in later sections
moremanageable.We showed that these assumptions bound the amount of entropy as a
function of the assumption parameters. We also made an assumption about the structure
of the grammar family, and showed that it entails no loss of generality for CFGs. Many
other formalisms can follow similar arguments to show that the structural assumption
is justified for them as well.
4. Proper Approximations
In order to follow the empirical risk minimization described in Section 2.1, we have
to define a series of approximations for F, which we denote by the log-concept spaces
F1,F2, . . . . We also have to replace two-sided uniform convergence (Equation [6]) with
convergence on the sequence of concept spaces we defined (Equation [10]). The concept
spaces in the sequence vary as a function of the number of samples we have. We next
492
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
construct the sequence of concept spaces, and in Section 5 we return to the learning
model. Our approximations are based on the concept of bounded approximations (Abe,
Takeuchi, and Warmuth 1991; Dasgupta 1997), which were originally designed for
graphical models.7 A bounded approximation is a subset of a concept space which is
controlled by a parameter that determines its tightness. Here we use this idea to define
a series of subsets of the original concept space F as approximations, while having two
asymptotic properties that control the series? tightness.
Let Fm (for m ? {1, 2, . . .}) be a sequence of concept spaces. We consider three
properties of elements of this sequence, which should hold for m > M for a fixedM.
The first is containment in F:
Fm ? F
The second property is boundedness:
?Km ? 0,?f ? Fm, E
[
| f | ? I {| f | ? Km}
]
? bound(m)
where bound is a non-increasing function such that bound(m) ??
m??
0. This states that
the expected values of functions from Fm on values larger than some Km is small.
This is required to obtain uniform convergence results in the revised empirical risk
minimization model from Section 2.1. Note that Km can grow arbitrarily large.
The third property is tightness:
?Cm ? F ? Fm, p
?
?
?
f?F
{z | Cm( f )(z)? f (z) ? tail(m)}
?
? ? tail(m)
where tail is a non-increasing function such that tail(m) ??
m??
0, and Cm denotes an
operator that maps functions in F to Fm. This ensures that our approximation actually
converges to the original concept space F. We will show in Section 4.3 that this is
actually a well-motivated characterization of convergence for probabilistic grammars
in the supervised setting.
We say that the sequence Fm properly approximates F if there exist tail(m), bound(m),
and Cm such that, for all m larger than some M, containment, boundedness, and tight-
ness all hold.
In a good approximation, Km would increase at a fast rate as a function of m and
tail(m) and bound(m) decrease quickly as a function ofm. As wewill see in Section 5, we
cannot have an arbitrarily fast convergence rate (by, for example, taking a subsequence
of Fm), because the size of Km has a great effect on the number of samples required to
obtain accurate estimation.
7 There are other ways to manage the unboundedness of KL divergence in the language learning literature.
Clark and Thollard (2004), for example, decompose the KL divergence between probabilistic finite-state
automata into several terms according to a decomposition of Carrasco (1997) and then bound each term
separately.
493
Computational Linguistics Volume 38, Number 3
Table 1
Example of a PCFG where there is more than a single way to approximate it by truncation with
? = 0.1, because it has more than two rules. Any value of ? ? [0,?] will lead to a different
approximation.
Rule ? General ? = 0 ? = 0.01 ? = 0.005
S? NP VP 0.09 0.01 0.1 0.1 0.1
S? NP 0.11 0.11? ? 0.11 0.1 0.105
S? VP 0.8 0.8? ?+ ? 0.79 0.8 0.795
4.1 Constructing Proper Approximations for Probabilistic Grammars
Wenow focus on constructing proper approximations for probabilistic grammarswhose
degree is limited to 2. Proper approximations could, in principle, be used with losses
other than the log-loss, though their main use is for unbounded losses. Starting from
this point in the article, we focus on using such proper approximations with the
log-loss.
We construct Fm. For each f ? F we define a transformation T( f,?) that shifts every
binomial parameter ?k = ??k,1,?k,2? in the probabilistic grammar by at most ?:
??k,1,?k,2? ?
?
?
?
??, 1? ?? if ?k,1 < ?
?1? ?, ?? if ?k,1 > 1? ?
??k,1, ?k,2? otherwise
Note that T( f,?) ? F for any ? ? 1/2. Fix a constant s > 1.8 We denote by T(?,?) the
same transformation on ? (which outputs the new shifted parameters) and we denote
by ?G(?) = ?(?) the set {T(?,?) | ? ? ?G}. For each m ? N, define Fm = {T( f,m?s) |
f ? F}.
When considering our approach to approximate a probabilistic grammar by in-
creasing its parameter probabilities to be over a certain threshold, it becomes clear
why we are required to limit the grammar to have only two rules and why we are
required to use the normal from Section 3.2 with grammars of degree 2. Consider the
PCFG rules in Table 1. There are different ways to move probability mass to the rule
with small probability. This leads to a problem with identifability of the approximation:
How does one decide how to reallocate probability to the small probability rules? By
binarizing the grammar in advance, we arrive at a single way to reallocate mass when
required (i.e., move mass from the high-probability rule to the low-probability rule).
This leads to a simpler proof for sample complexity bounds and a single bound (rather
than different bounds depending on different smoothing operators). We note, however,
that the choices made in binarizing the grammar imply a particular way of smoothing
the probability across the original rules.
We now describe how this construction of approximations satisfies the proper-
ties mentioned in Section 4, specifically, the boundedness property and the tightness
property.
8 By varying swe get a family of approximations. The larger s is, the tighter the approximation is. Also,
the larger s is, as we see later, the looser our sample complexity bound will be.
494
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Proposition 2
Let p ? P(?,L, r, q,B,G) and let Fm be as defined earlier. There exists a constant ? =
?(L, q, p,N) > 0 such that Fm has the boundedness property with Km = sN log
3m and
bound(m) = m
?? logm.
See Appendix A for the proof of Proposition 2.
Next, Fm is tight with respect to F with tail(m) =
N log2m
ms ? 1 .
Proposition 3
Let p ? P(?,L, r, q,B,G) and let Fm as defined earlier. There exists anM such that for any
m > Mwe have
p
?
?
?
f?F
{z | Cm( f )(z)? f (z) ? tail(m)}
?
? ? tail(m)
for tail(m) =
N log2m
ms ? 1 and Cm( f ) = T( f,m
?s).
See Appendix A for the proof of Proposition 3.
We now have proper approximations for probabilistic grammars. These approx-
imations are defined as a series of probabilistic grammars, related to the family of
probabilistic grammars we are interested in estimating. They consist of three prop-
erties: containment (they are a subset of the family of probabilistic grammars we
are interested in estimating), boundedness (their log-loss does not diverge to infinity
quickly), and they are tight (there is a small probability mass at which they are not tight
approximations).
4.2 Coupling Bounded Approximations with Number of Samples
At this point, the number of samples n is decoupled from the bounded approximation
(Fm) that we choose for grammar estimation. To couple between these two, we need
to define m as a function of the number of samples, m(n). As mentioned earlier, there
is a clear trade-off between choosing a fast rate for m(n) (such as m(n) = nk for some
k > 1) and a slower rate (such as m(n) = logn). The faster the rate is, the tighter the
family of approximations that we use for n samples. If the rate is too fast, however,
then Km grows quickly as well. In that case, because our sample complexity bounds are
increasing functions of such Km, the bounds will degrade.
To balance the trade-off, we choose m(n) = n. As we see later, this gives sample
complexity bounds which are asymptotically interesting for both the supervised and
unsupervised case.
495
Computational Linguistics Volume 38, Number 3
4.3 Asymptotic Empirical Risk Minimization
It would be compelling to determine whether the empirical risk minimizer over Fn is
an asymptotic empirical risk minimizer. This would mean that the risk of the empirical
risk minimizer over Fn converges to the risk of the maximum likelihood estimate. As a
conclusion to this section about proper approximations, we motivate the three re-
quirements that we posed on proper approximations by showing that this is indeed
true. We now unify n, the number of samples, and m, the index of the approxima-
tion of the concept space F. Let f ?n be the minimizer of the empirical risk over F,
( f ?n = argminf?F Ep?n
[
f
]
) and let gn be the minimizer of the empirical risk over Fn
(gn = argminf?Fn Ep?n
[
f
]
).
Let D = {z1, ..., zn} be a sample from p(z). The operator (gn =) argminf?Fn Ep?n [ f ] is
an asymptotic empirical risk minimizer if E
[
Ep?n
[
gn
]
? Ep?n [ f
?
n ]
]
? 0 as n? ? (Shalev-
Shwartz et al 2009). Then, we have the following
Lemma 1
Denote by Z,n the set
?
f?F{z | Cn( f )(z)? f (z) ? }. Denote by A,n the event ?one of
zi ? D is in Z,n.? If Fn properly approximates F, then:
E
[
Ep?n
[
gn
]
? Ep?n
[
f ?n
]
]
(15)
?
?
?
?
E
[
Ep?n
[
Cn( f
?
n )
]
| A,n
]
?
?
?
p(A,n)+
?
?
?
E
[
Ep?n
[
f ?n
]
| A,n
]
?
?
?
p(A,n)+ tail(n)
where the expectations are taken with respect to the data set D.
See Appendix A for the proof of Lemma 1.
Proposition 4
Let D = {z1, ..., zn} be a sample of derivations from G. Then gn = argminf?Fn Ep?n
[
f
]
is
an asymptotic empirical risk minimizer.
Proof
Let f0 ? F be the concept that puts uniform weights over ?, namely, ?k = ? 12 ,
1
2 ? for all k.
Note that
|E
[
Ep?n
[
f ?n
]
| A,n
]
|p(A,n)
? |E
[
Ep?n
[
f0
]
| A,n
]
|p(A,n) =
log 2
n
?n
l=1
?
k,i E[?k,i(zl) | A,n]p(A,n)
496
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Let Aj,,n for j ? {1, . . . ,n} be the event ?zj ? Z,n?. Then A,n =
?
j Aj,,n. We have
that
E[?k,i(zl) | A,n]p(A,n) ?
?
j
?
zl
p(zl,Aj,,n)|zl|
?
?
j =l
?
zl
p(zl)p(Aj,,n)|zl|+
?
zl
p(zl,Al,,n)|zl| (16)
?
?
?
?
j =l
p(Aj,,n)
?
?B+ E[?k,i(z) | z ? Z,n]p(z ? Z,n)
? (n? 1)Bp(z ? Z,n)+ E[?k,i(z) | z ? Z,n]p(z ? Z,n)
where Equation (16) comes from zl being independent. Also, B is the constant from
Section 3.1. Therefore, we have:
1
n
n
?
l=1
?
k,i
E[?k,i(zl) | A,n]p(A,n)
?
?
k,i
(
E[?k,i(z) | z ? Z,n]p(z ? Z,n)+ (n? 1)Bp(z ? Z,n)
)
From the construction of our proper approximations (Proposition 3), we know that only
derivations of length log2 n or greater can be in Z,n. Therefore
E[?k,i | Z,n]p(Z,n) ?
?
z:|z|>log2 n
p(z)?k,i(z) ?
?
?
l>log2 n
L?(l)rll ? ?qlog
2 n = o(1)
where ? > 0 is a constant. Similarly, we have p(z ? Z,n) = o(n?1). This means that
|E[Ep?n[? log?f
?
n ] | A,n]|p(A,n) ??
n??
0. In addition, it can be shown that |E[Ep?n[Cn( f
?
n ) |
A,n]|p(A,n) ??
n??
0 using the same proof technique we used here, while relying on the
fact that Cn( f
?
n ) ? Fn, and therefore Cn( f
?
n )(z) ? sN|z| log n. 
5. Sample Complexity Bounds
Equipped with the framework of proper approximations as described previously, we
now give our main sample complexity results for probabilistic grammars. These results
hinge on the convergence of supf?Fn |Ep?n
[
f
]
? Ep
[
f
]
|. Indeed, proper approximations
replace the use of F in these convergence results. The rate of this convergence can be
fast, if the covering numbers for Fn do not grow too fast.
5.1 Covering Numbers and Bounds on Covering Numbers
We next give a brief overview of covering numbers. A cover provides a way to reduce
a class of functions to a much smaller (finite, in fact) representative class such that each
function in the original class is represented using a function in the smaller class. Let G
497
Computational Linguistics Volume 38, Number 3
be a class of functions. Let d(f, g) be a distance measure between two functions f, g from
G. An -cover is a subset of G, denoted by G?, such that for every f ? G there exists an
f ? ? G? such that d( f, f ?) < . The covering number N(,G, d) is the size of the smallest
-cover of G for the distance measure d.
We are interested in a specific distancemeasure which is dependent on the empirical
distribution p?n that describes the data z1, ..., zn. Let f, g ? G. We will use
dp?n ( f, g) = Ep?n
[
| f ? g|
]
=
?
z?D(G)
| f (z)? g(z)| p?n(z)
= 1n
?n
i=1
| f (zi)? g(zi)|
Instead of using N(,G, dp?n ) directly, we bound this quantity with N(,G) = supp?n
N(,G, dp?n ), where we consider all possible samples (yielding p?n). The following is the
key result regarding the connection between covering numbers and the double-sided
convergence of the empirical process supf?Fn |Ep?n
[
f
]
? Ep
[
f
]
| as n? ?. This result
is a general-purpose result that has been used frequently to prove the convergence of
empirical processes of the type we discuss in this article.
Lemma 2
Let Fn be a permissible class
9 of functions such that for every f ? Fn we have E[| f | ?
I {| f | ? Kn}] ? bound(n). Let Ftruncated,n = {f ? I {f ? Kn} | f ? Fm}, namely, the set of
functions from Fn after being truncated by Kn. Then for  > 0 we have
p
(
sup
f?Fn
|Ep?n
[
f
]
? Ep
[
f
]
| > 2
)
? 8N(/8,Ftruncated,n) exp
(
? 1
128
n2/K2n
)
+ bound(n)/
provided n ? K2n/4
2 and bound(n) < .
See Pollard (1984; Chapter 2, pages 30?31) for the proof of Lemma 2. See also Ap-
pendix A.
Covering numbers are rather complex combinatorial quantities which are hard
to compute directly. Fortunately, they can be bounded using the pseudo-dimension
(Anthony and Bartlett 1999), a generalization of the Vapnik-Chervonenkis (VC)
dimension for real functions. In the case of our ?binomialized? probabilistic grammars,
the pseudo-dimension of Fn is bounded by N, because we have Fn ? F, and the
functions in F are linear with N parameters. Hence, Ftruncated,n also has pseudo-
dimension that is at most N. We then have the following.
9 The ?permissible class? requirement is a mild regularity condition regarding measurability that holds for
proper approximations. We refer the reader to Pollard (1984) for more details.
498
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Lemma 3
(From Pollard [1984] and Haussler [1992].) Let Fn be the proper approximations for
probabilistic grammars, for any 0 <  < Kn we have:
N(,Ftruncated,n) < 2
(
2eKn
 log
2eKn

)N
5.2 Supervised Case
We turn to give an analysis for the supervised case. This analysis is mostly described as a
preparation for the unsupervised case. In general, the families of probabilistic grammars
we give a treatment to are parametric families, and the maximum likelihood estimator
for these families is a consistent estimator in the supervised case. In the unsupervised
case, however, lack of identifiability prevents us from getting these traditional consis-
tency results. Also, the traditional results about the consistency of MLE are based on the
assumption that the sample is generated from the parametric family we are trying to
estimate. This is not the case in our analysis, where the distribution that generates the
data does not have to be a probabilistic grammar.
Lemmas 2 and 3 can be combined to get the following sample complexity result.
Theorem 2
LetG be a grammar. Let p ? P(?,L, r, q,B,G) (Section 3.1). Let Fn be a proper approxima-
tion for the corresponding family of probabilistic grammars. Let z1, . . . , zn be a sample
of derivations. Then there exists a constant ?(L, q, p,N) and constantM such that for any
0 < ? < 1 and 0 <  < Kn and any n > M and if
n ? max
{
128K2n
2
(
2N log(16eKn/)+ log
32
?
)
,
log 4/?+ log 1/
?(L, q, p,N)
}
then we have
P
(
sup
f?Fn
|Ep?n
[
f
]
? Ep
[
f
]
| ? 2
)
? 1? ?
where Kn = sN log
3 n.
Proof Sketch
?(L, q, p,N) is the constant from Proposition 2. The main idea in the proof is to solve for
n in the following two inequalities (based on Equation [17] [see the following]) while
relying on Lemma 3:
8N(/8,Ftruncated,n) exp
(
? 1
128
n2/K2n
)
? ?/2
bound(n)/ ? ?/2

499
Computational Linguistics Volume 38, Number 3
Theorem 2 gives little intuition about the number of samples required for accurate
estimation of a grammar because it considers the ?additive? setting: The empirical risk
is within  from the expected risk. More specifically, it is not clear how we should pick
 for the log-loss, because the log-loss can obtain arbitrary values.
We turn now to converting the additive bound in Theorem 2 to a multiplicative
bound. Multiplicative bounds can be more informative than additive bounds when the
range of the values that the log-loss can obtain is not known a priori. It is important
to note that the two views are equivalent (i.e., it is possible to convert a multiplicative
bound to an additive bound and vice versa). Let ? ? (0, 1) and choose  = ?Kn. Then,
substituting this  in Theorem 2, we get that if
n ? max
{
128
?2
(
2N log 16e? + log
32
?
)
,
log 4/?+ log 1/?
?(L, q, p,N)
}
then, with probability 1? ?,
sup
f?Fn
?
?
?
?
?
1?
Ep?n
[
f
]
Ep
[
f
]
?
?
?
?
?
?
?? 2sN log3(n)
H(p)
(17)
where H(p) is the Shannon entropy of p. This stems from the fact that Ep
[
f
]
? H(p) for
any f . This means that if we are interested in computing a sample complexity bound
such that the ratio between the empirical risk and the expected risk (for log-loss) is
close to 1 with high probability, we need to pick up ? such that the righthand side of
Equation (17) is smaller than the desired accuracy level (between 0 and 1). Note that
Equation (17) is an oracle inequality?it requires knowing the entropy of p or some
upper bound on it.
5.3 Unsupervised Case
In the unsupervised setting, we have n yields of derivations from the grammar, x1, ..., xn,
and our goal again is to identify grammar parameters ? from these yields. Our concept
classes are now the sets of log marginalized distributions from Fn. For each f? ? Fn, we
define f ?? as
f ??(x) = ? log
?
z?Dx(G)
exp(?f?(z)) = ? log
?
z?Dx(G)
exp
?
?
K
?
k=1
Nk
?
i=1
?i,k(z)?i,k
?
?
We denote the set of { f ??} by F
?
n. Analogously, we define F
?. Note that we also need to
define the operator C?n( f
?) as a first step towards defining F?n as proper approximations
(for F?) in the unsupervised setting. Let f ? ? F?. Let f be the concept in F such that
f ?(x) =
?
z f (x, z). Then we define C
?
n( f
?)(x) =
?
z Cn( f )(x, z).
It does not immediately follow that F?n is a proper approximation for F
?. It is not
hard to show that the boundedness property is satisfied with the same Kn and the same
form of bound(n) as in Proposition 2 (we would have 
?
bound(m) = m
??? logm for some
??(L, q, p,N) = ?? > 0). This relies on the property of bounded derivation length of p (see
Appendix A, Proposition 7). The following result shows that we have tightness as well.
500
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Utility Lemma 2
For ai, bi ? 0, if ? log
?
i ai + log
?
i bi ?  then there exists an i such that ? log ai +
log bi ? .
Proposition 5
There exists anM such that for any n > Mwe have
p
?
?
?
f ??F?
{x | C?n(f
?)(x)? f ?(x) ? tail(n)}
?
? ? tail(n)
for tail(n) =
N log2 n
ns ? 1 and the operator C
?
n( f ) as defined earlier.
Proof Sketch
From Utility Lemma 2 we have
p
?
?
?
f ??F?
{x | C?n( f
?)(x)? f ?(x) ? tail(n)}
?
? ? p
?
?
?
f?F
{x | ?zCn( f )(z)? f (z) ? tail(n)}
?
?
Define X(n) to be all x such that there exists a z with yield(z) = x and |z| ? log2 n.
From the proof of Proposition 3 and the requirements on p, we know that there exists
an ? ? 1 such that
p
(
?
f?F{x | ?z s.t.Cn( f )(z)? f (z) ? tail(n)}
)
?
?
x?X(n)
p(x)
?
?
x:|x|?log2 n/?
p(x) ?
?
?
k=log2 n/?
L?(k)rk ? tail(n)
where the last inequality happens for some n larger than a fixedM. 
Computing either the covering number or the pseudo-dimension of F?n is a hard
task, because the function in the classes includes the ?log-sum-exp.? Dasgupta (1997)
overcomes this problem for Bayesian networks with fixed structure by giving a bound
on the covering number for (his respective) F? which depends on the covering number
of F.
Unfortunately, we cannot fully adopt this approach, because the derivations of
a probabilistic grammar can be arbitrarily large. Instead, we present the following
proposition, which is based on the ?Hidden Variable Rule? from Dasgupta (1997). This
proposition shows that the covering number of F? (or more accurately, its bounded
approximations) can be bounded in terms of the covering number of the bounded
501
Computational Linguistics Volume 38, Number 3
approximations of F, and the constants which control the underlying distribution p
mentioned in Section 3.
Utility Lemma 3
For any two positive-valued sequences (a1, . . . , an) and (b1, . . . , bn) we have that
?
i | log ai/bi| ? | log (
?
ai/
?
bi) |.
Proposition 6 (Hidden Variable Rule for Probabilistic Grammars)
Let m =
log
4Kn
(1? q)
log 1q
. Then, N(,F?truncated,n) ? N
(

2?(m)
,Ftruncated,n
)
.
Proof
Let Z(m) = {z | |z| ? m} be the subset of derivations of length shorter than m. Consider
f, f0 ? Ftruncated,n. Let f ? and f ?0 be the corresponding functions in F
?
truncated,n. Then, for any
distribution p,
dp( f ?, f ?0) =
?
x
| f ?(x)? f ?0(x)| p(x) ?
?
x
?
z
| f (x, z)? f0(x, z)| p(x)
=
?
x
?
z?Z(m)
| f (x, z)? f0(x, z)| p(x)+
?
x
?
z/?Z(m)
| f (x, z)? f0(x, z)| p(x)
?
?
x
?
z?Z(m)
| f (x, z)? f0(x, z)| p(x)+
?
x
?
z/?Z(m)
2Knp(x) (18)
?
?
x
?
z?Z(m)
| f (x, z)? f0(x, z)| p(x)+ 2Kn
?
x : |x|?m
|Dx(G)|p(x)
?
?
x
?
z?Z(m)
| f (x, z)? f0(x, z)| p(x)+ 2Kn
?
?
k=m
?2(k)rk
? dp
?
( f, f0)|Z(m)|+ 2Kn
qm
1? q
where p?(x, z) is a probability distribution that uniformly divides the probability mass
p(x) across all derivations for the specific x, that is:
p?(x, z) =
p(x)
|Dx(G)|
The inequality in Equation (18) stems from Utility Lemma 3.
Set m to be the quantity that appears in the proposition to get the necessary result
( f ? and f are arbitrary functions in F?truncated,n and Ftruncated,n respectively. Then consider
f ?0 and f0 to be functions from the respective covers.). 
For the unsupervised case, then, we get the following sample complexity result.
502
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Theorem 3
Let G be a grammar. Let F?n be a proper approximation for the corresponding family of
probabilistic grammars. Let p(x, z) be a distribution over derivations which satisfies the
requirements in Section 3.1. Let x1, . . . , xn be a sample of strings from p(x). Then there
exists a constant ??(L, q, p,N) and constant M such that for any 0 < ? < 1, 0 <  < Kn,
any n > M, and if
n ? max
{
128K2n
2
(
2N log
(
32eKn?(m)

)
+ log 32
?
)
,
log 4/?+ log 1/
??(L, q, p,N)
}
(19)
where m =
log
4Kn
(1? q)
log 1q
, we have that
p
(
sup
f?F?n
|Ep?n
[
f
]
? Ep
[
f
]
| ? 2
)
? 1? ?
where Kn = sN log
3 n.
Theorem 3 states that the number of samples we require in order to accurately esti-
mate a probabilistic grammar from unparsed strings depends on the level of ambiguity
in the grammar, represented as ?(m). We note that this dependence is polynomial, and
we consider this a positive result for unsupervised learning of grammars. More specif-
ically, if ? is an exponential function (such as the case with PCFGs), when compared to
the supervised learning, there is an extra multiplicative factor in the sample complexity
in the unsupervised setting that behaves like O(log log
Kn
 ).
We note that the following Equation (20) can again be reduced to a multiplicative
case, similarly to the way we described it for the supervised case. Setting  = ?Kn (? ?
(0, 1)), we get the following requirement on n:
n ? max
{
128
?2
(
2N log
(
32e? t(?)
?
)
+ log 32
?
)
,
log 4/?+ log 1/
??(L, q, p,N)
}
(20)
where t(?) =
log 4
?(1? q)
log 1q
.
6. Algorithms for Empirical Risk Minimization
We turn now to describing algorithms and their properties for minimizing empirical
risk using the framework described in Section 4.
6.1 Supervised Case
ERM with proper approximations leads to simple algorithms for estimating the proba-
bilities of a probabilistic grammar in the supervised setting. Given an  > 0 and a ? > 0,
we draw n examples according to Theorem 2. We then set ? = n?s. To minimize the
log-loss with respect to these n examples, we use the proper approximation Fn.
503
Computational Linguistics Volume 38, Number 3
Note that the value of the empirical log-loss for a probabilistic grammar param-
etrized by ? is
Ep?n
[
? log h(x, z | ?)
]
= ?
?
x,z
p?n(x, z) log h(x, z | ?)
= ?
?
x,z
p?n(x, z)
K
?
k=1
Nk
?
i=1
?k,i(x, z) log(?k,i)
= ?
K
?
k=1
Nk
?
i=1
log(?k,i)Ep?n
[
?k,i
]
Because we make the assumption that deg(G) ? 2 (Section 3.2), we have
Ep?n
[
? log h(x, z | ?)
]
= ?
K
?
k=1
(
log(?k,1)Ep?n
[
?k,1
]
+ log(1? ?k,1)Ep?n
[
?k,2
])
(21)
To minimize the log-loss with respect to Fn, we need to minimize Equation (21) under
the constraint that ? ? ?k,i ? 1? ? and ?k1 + ?k,2 = 1. It can be shown that the solution
for this optimization problem is
?k,i = min
?
?
?
1? ?,max
?
?
?
?,
?
?
n
?
j=1
??j,k,i
?
?
/
?
?
n
?
j=1
2
?
i?=1
??j,k,i?
?
?
?
?
?
?
?
?
(22)
where ??j,k,i is the number of times that ?k,i fires in Example j. (We include a full
derivation of this result in Appendix B.) The interpretation of Equation (22) is simple:
We count the number of times a rule appears in the samples and then normalize this
value by the total number of times rules associated with the same multinomial appear
in the samples. This frequency count is the maximum likelihood solution with respect
to the full hypothesis class H (Corazza and Satta 2006; see Appendix B). Because we
constrain ourselves to obtain a value away from 0 or 1 by a margin of ?, we need to
truncate this solution, as done in Equation (22).
This truncation to amargin ? can be thought of as a smoothing factor that enables us
to compute sample complexity bounds. We explore this connection to smoothing with
a Dirichlet prior in a Maximum a posteriori (MAP) Bayesian setting in Section 7.2.
6.2 Unsupervised Case
Similarly to the supervised case, minimizing the empirical log-loss in the unsupervised
setting requires minimizing (with respect to ?) the following:
Ep?n
[
? log h(x | ?)
]
= ?
?
x
p?n(x) log
?
z
h(x, z | ?) (23)
with the constraint that ? ? ?k,i ? 1? ? (i.e., ? ? ?(?)) where ? = n?s. This is done
after drawing n examples according to Theorem 3.
504
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
6.2.1 Hardness of ERM with Proper Approximations. It turns out that minimizing Equa-
tion (23) under the specified constraints is actually an NP-hard problem when G is a
PCFG. This result follows using a similar proof to the one in Cohen and Smith (2010c)
for the hardness of Viterbi training and maximizing log-likelihood for PCFGs. We turn
to giving the full derivation of this hardness result for PCFGs and the modification
required for adapting the results from Cohen and Smith to the case of having an
arbitrary ?margin constraint.
In order to show an NP-hardness result, we need to ?convert? the problem of the
maximization of Equation (23) to a decision problem. We do so by stating the following
decision problem.
Problem 1 (Unsupervised Minimization of the Log-Loss with Margin)
Input: A binarized context-free grammar G, a set of sentences x1, . . . , xn, a value ? ?
[0, 12 ), and a value ? ? [0, 1].
Output: 1 if there exists ? ? ?(?) (and hence, h ? H(G)) such that
?
?
x
p?n(x) log
?
z
h(x, z | ?) ? ? log(?) (24)
and 0 otherwise.
We will show the hardness result both when ? is not restricted at all as well as when
we allow ? > 0. The proof of the hardness result is achieved by reducing the problem
3-SAT (Sipser 2006), known to be NP-complete, to Problem 1. The problem 3-SAT is
defined as follows:
Problem 2 (3-SAT)
Input: A formula ? =
?m
i=1 (ai ? bi ? ci) in conjunctive normal form, such that each
clause has three literals.
Output: 1 if there is a satisfying assignment for ?, and 0 otherwise.
Given an instance of the 3-SAT problem, the reduction will, in polynomial time,
create a grammar and a single string such that solving Problem 1 for this grammar and
string will yield a solution for the instance of the 3-SAT problem.
Let ? =
?m
i=1 (ai ? bi ? ci) be an instance of the 3-SAT problem, where ai, bi, and
ci are literals over the set of variables {Y1, . . . ,YN} (a literal refers to a variable Yj or
its negation, Y?j). Let Cj be the jth clause in ?, such that Cj = aj ? bj ? cj. We define the
following CFG G? and string to parse s?:
1. The terminals of G? are the binary digits ? = {0, 1}.
2. We create N nonterminals VYr , r ? {1, . . . ,N} and rules VYr ? 0 and
VYr ? 1.
3. We create N nonterminals VY?r , r ? {1, . . . ,N} and rules VY?r ? 0 and
VY?r ? 1.
4. We create UYr,1 ? VYrVY?r and UYr,0 ? VY?rVYr .
5. We create the rule S1 ? A1. For each j ? {2, . . . ,m}, we create a rule
Sj ? Sj?1Aj where Sj is a new nonterminal indexed by ?j 
?j
i=1 Ci
and Aj is also a new nonterminal indexed by j ? {1, . . . ,m}.
505
Computational Linguistics Volume 38, Number 3
6. Let Cj = aj ? bj ? cj be clause j in ?. Let Y(aj) be the variable that aj
mentions. Let (y1, y2, y3) be a satisfying assignment for Cj where yk ? {0, 1}
and is the value of Y(aj), Y(bj), and Y(cj), respectively, for k ? {1, 2, 3}. For
each such clause-satisfying assignment, we add the rule
Aj ? UY(aj ),y1UY(bj ),y2UY(cj ),y3
For each Aj, we would have at most seven rules of this form, because one
rule will be logically inconsistent with aj ? bj ? cj.
7. The grammar?s start symbol is Sn.
8. The string to parse is s? = (10)
3m, that is, 3m consecutive occurrences of
the string 10.
A parse of the string s? using G? will be used to get an assignment by setting
Yr = 0 if the rule VYr ? 0 or VY?r ? 1 is used in the derivation of the parse tree, and 1
otherwise. Notice that at this point we do not exclude ?contradictions? that come from
the parse tree, such as VY3 ? 0 used in the tree together with VY3 ? 1 or VY?3 ? 0. To
maintain the restriction on the degree of grammars, we convertG? to the binary normal
form described in Section 3.2. The following lemma gives a condition under which the
assignment is consistent (so that contradictions do not occur in the parse tree).
Lemma 4
Let ? be an instance of the 3-SAT problem, and let G? be a probabilistic CFG based on
the given grammar with weights ??. If the (multiplicative) weight of the Viterbi parse
(i.e., the highest scoring parse according to the PCFG) of s? is 1, then the assignment
extracted from the parse tree is consistent.
Proof
Because the probability of the Viterbi parse is 1, all rules of the form {VYr ,VY?r} ? {0, 1}
which appear in the parse tree have probability 1 as well. There are two possible types
of inconsistencies. We show that neither exists in the Viterbi parse:
1. For any r, an appearance of both rules of the form VYr ? 0 and VYr ? 1
cannot occur because all rules that appear in the Viterbi parse tree have
probability 1.
2. For any r, an appearance of rules of the form VYr ? 1 and VY?r ? 1 cannot
occur, because whenever we have an appearance of the rule VYr ? 0, we
have an adjacent appearance of the rule VY?r ? 1 (because we parse
substrings of the form 10), and then we again use the fact that all rules in
the parse tree have probability 1. The case of VYr ? 0 and VY?r ? 0 is
handled analogously.
Thus, both possible inconsistencies are ruled out, resulting in a consistent assignment.

Figure 3 gives an example of an application of the reduction.
506
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Figure 3
An example of a Viterbi parse tree which represents a satisfying assignment for
? = (Y1 ? Y2 ? Y?4) ? (Y?1 ? Y?2 ? Y3). In ??, all rules appearing in the parse tree have
probability 1. The extracted assignment would be Y1 = 0,Y2 = 1,Y3 = 1,Y4 = 0.
Note that there is no usage of two different rules for a single nonterminal.
Lemma 5
Define ? and G? as before. There exists ?? such that the Viterbi parse of s? is 1 if and
only if ? is satisfiable. Moreover, the satisfying assignment is the one extracted from the
parse tree with weight 1 of s? under ??.
Proof
(=?) Assume that there is a satisfying assignment. Each clause Cj = aj ? bj ? cj is sat-
isfied using a tuple (y1, y2, y3), which assigns values for Y(aj), Y(bj), and Y(cj). This
assignment corresponds to the following rule:
Aj ? UY(aj ),y1UY(bj ),y2UY(cj ),y3
Set its probability to 1, and set al other rules of Aj to 0. In addition, for each r, if
Yr = y, set the probabilities of the rules VYr ? y and VY?r ? 1? y to 1 and VY?r ? y and
VYr ? 1? y to 0. The rest of the weights for Sj ? Sj?1Aj are set to 1. This assignment of
rule probabilities results in a Viterbi parse of weight 1.
(?=) Assume that the Viterbi parse has probability 1. From Lemma 4, we know that we
can extract a consistent assignment from the Viterbi parse. In addition, for each clause
Cj we have a rule
Aj ? UY(aj ),y1UY(bj ),y2UY(cj ),y3
that is assigned probability 1, for some (y1, y2, y3). One can verify that (y1, y2, y3) are
the values of the assignment for the corresponding variables in clause Cj, and that
they satisfy this clause. This means that each clause is satisfied by the assignment we
extracted. 
We are now ready to prove the following result.
Theorem 4
Problem 1 is NP-hard when either requiring ? > 0 or when fixing ? = 0.
507
Computational Linguistics Volume 38, Number 3
Proof
We first describe the reduction for the case of ? = 0. In Problem 1, set ? = 0, ? = 1,
G = G?, ? = 0, and x1 = s?. If ? is satisfiable, then the left side of Equation (24) can get
value 0, by setting the rule probabilities according to Lemma 5, hence we would return
1 as the result of running Problem 1.
If ? is unsatisfiable, then we would still get value 0 only if L(G) = {s?}. If G? gen-
erates a single derivation for (10)3m, then we actually do have a satisfying assignment
from Lemma 4. Otherwise (more than a single derivation), the optimal ? would have
to give fractional probabilities to rules of the form VYr ? {0, 1} (or VY?r ? {0, 1}). In
that case, it is no longer true that (10)3m is the only generated sentence, and this is a
contradiction to getting value 0 for Problem 1.
We next show that Problem 1 is NP-hard even if we require ? > 0. Let ? < 120m .
Set ? = ?, and the rest of the inputs to Problem 1 the same as before. Assume that ?
is satisfiable. Let ? be the rule probabilities from Equation (5) after being shifted with a
margin of ?. Then, because there is a derivation that uses only rules that have probability
1? ?, we have
h(x1 | T(?,?),G?) =
?
z
p(x1, z | T(?,?),G?)
? (1? ?)10m
> ?
because the size of the parse tree for (10)3m is at most 10m (using the binarized G?)
and assuming ? = ? < (1? ?)10m. This inequality indeed holds whenever ? < 120m .
Therefore, we have ? log h(x1 | ?) > ? log?. Problem 1 would return 0 in this case.
Now, assume that ? is not satisfiable. That means that any parse tree for the string
(10)3m would have to contain two different rules headed by the same non-terminal. This
means that
h(x1 | T(?,?),G?) =
?
z
p(x1, z | T(?,?),G?)
? ?
and therefore ? log h(x1 | T(?,?)) ? ? log?, and Problem 1 would return 1. 
6.2.2 An Expectation-Maximization Algorithm. Instead of solving the optimization prob-
lem implied by Equation (21), we propose a rather simple modification to the
expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to ap-
proximate the optimal solution?this algorithm finds a local maximum for the max-
imum likelihood problem using proper approximations. The modified algorithm is
given in Algorithm 1.
The modification from the usual expectation-maximization algorithm is done in the
M-step: Instead of using the expected value of the sufficient statistics by counting and
normalizing, we truncate the values by ?. It can be shown that if ?(0) ? ?(?), then the
likelihood is guaranteed to increase (and hence, the log-loss is guaranteed to decrease)
after each iteration of the algorithm.
508
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Algorithm 1: Expectation-Maximization Algorithmwith Proper Approximations.
Input: grammar G in binary normal form, initial parameters ?(0),  > 0, ? > 0,
s > 1
Output: learned parameters ?
draw x = ?x1, ..., xn? from p following Theorem 3; t? 1 ;
? ? n?s;
repeat
// E
?(t?1)
[
?k,i(z) | xj
]
denotes the expected counts of event i in multinomial k
under the distribution p?n(x)p(z | x,?
(t?1))
Compute for each training example j ? {1, . . . ,n}, for each event i ? {1, 2} in each
multinomial k ? {1, . . . ,K}: ??j,k,i ? E?(t?1)
[
?k,i(z) | xj
]
;
Set ?
(t)
i,k
= min{1? ?,max{?,
(
?n
j=1 ??j,k,i
)
/
(
?n
j=1
?2
i?=1 ??j,k,i?
)
}};
t? t+ 1;
until convergence;
return ?(t)
The reason for this likelihood increase stems from the fact that the M-step solves
the optimization problem of minimizing the log-loss (with respect to ? ? ?(?)) when
the posterior calculate at the E-step as the base distribution is used. This means that the
M-step minimizes (in iteration t): Er
[
? log h(x, z | ?(t) )
]
where the expectation is taken
with respect to the distribution r(x, z) = p?n(x)p(z | x,?
(t?1)). With this notion in mind,
the likelihood increase after each iteration follows from principles similar to those
described in Bishop (2006) for the EM algorithm.
7. Discussion
Our framework can be specialized to improve the two main criteria which have a trade-
off: the tightness of the proper approximation and the sample complexity. For example,
we can improve the tightness of our proper approximations by taking a subsequence
of Fn. This will make the sample complexity bound degrade, however, because Kn will
grow faster. Table 2 shows the trade-offs between parameters in our model and the
effectiveness of learning.
We note that the sample complexity bounds that we give in this article give
insight about the asymptotic behavior of grammar estimation, but are not necessarily
Table 2
Trade-off between quantities in our learning model and effectiveness of different criteria. Kn is
the constant that satisfies the boundedness property (Theorems 2 and 3) and s is a fixed constant
larger than 1 (Section 4.1).
criterion as Kn increases . . . as s increases . . .
tightness of proper approximation improves improves
sample complexity bound degrades degrades
509
Computational Linguistics Volume 38, Number 3
sufficiently tight to be used in practice. It still remains an open problem to obtain
sample complexity bounds which are sufficiently tight in this respect. For a discussion
about the connection of grammar learning in theory and practice, we refer the reader
to Clark and Lappin (2010).
It is also important to note that MLE is not the only option for estimating finite
state probabilistic grammars. There has been some recent advances in learning finite
state models (HMMs and finite state transducers) by using spectral analysis of matrices
which consist of quantities estimated from observations only (Hsu, Kakade, and Zhang
2009; Balle, Quattoni, and Carreras 2011), based on the observable operator models of
Jaeger (1999). These algorithms are not prone to local minima, and converge to the
correct model as the number of samples increases, but require some assumptions about
the underlying model that generates the data.
7.1 Tsybakov Noise
In this article, we chose to introduce assumptions about distributions that generate
natural language data. The choice of these assumptions was motivated by observations
about properties shared among treebanks. The main consequence of making these
assumptions is bounding the amount of noise in the distribution (i.e., the amount of
variation in probabilities across labels given a fixed input).
There are other ways to restrict the noise in a distribution. One condition for such
noise restriction, which has received considerable recent attention in the statistical liter-
ature, is the Tsybakov noise condition (Tsybakov 2004; Koltchinskii 2006). Showing that
a distribution satisfies the Tsybakov noise condition enables the use of techniques (e.g.,
from Koltchinskii 2006) for deriving distribution-dependent sample complexity bounds
that depend on the parameters of the noise. It is therefore of interest to see whether
Tsybakov noise holds under the assumptions presented in Section 3.1. We show that
this is not the case, and that Tsybakov noise is too permissive. In fact, we show that p
can be a probabilistic grammar itself (and hence, satisfy the assumptions in Section 3.1),
and still not satisfy the Tsybakov noise conditions.
Tsybakov noise was originally introduced for classification problems (Tsybakov
2004), and was later extended to more general settings, such as the one we are facing in
this article (Koltchinskii 2006). We now explain the definition of Tsybakov noise in our
context.
Let C > 0 and ? ? 1. We say that a distribution p(x, z) satisfies the (C,?) Tsybakov
noise condition if for any  > 0 and h, g ? H such that h, g ? {h? | Ep(h?,H) ? }, we
have
dist(g, h) 
?
?
?
?Ep
[
(
log g
log h
)2
]
? C1/? (25)
This interpretation of Tsybakov noise implies that the diameter of the set of functions
from the concept class that has small excess risk should shrink to 0 at the rate in
Equation (25). Distribution-dependent bounds from Koltchinskii (2006) are monotone
with respect to the diameter of this set of functions, and therefore demonstrating that it
goes to 0 enables sharper derivations of sample complexity bounds.
510
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
We turn now to illustrating that the Tsybakov condition does not hold for proba-
bilistic grammars in most cases. Let G be a probabilistic grammar. Define A = AG(?) as
a matrix such that
(AG(?))(k,i),(k?,i? ) 
E
[
?k,i ??k?,i?
]
E[?k,i]E[?k?,i? ]
Theorem 5
Let G be a grammar with K ? 2 and degree 2. Assume that p is ?G,??? for some ??, such
that ??1,1 = ?
?
2,1 = ? and that c1 ? c2. If AG(?
?) is positive definite, then p does not satisfy
the Tsybakov noise condition for any (C,?), where C > 0 and ? ? 1.
See Appendix C for the proof of Theorem 5.
In Appendix C we show that AG(?) is positive semi-definite for any choice of ?.
The main intuition behind the proof is that given a probabilistic grammar p, we can
construct an hypothesis h such that the KL divergence between p and h is small, but
dist(p, h) is lower-bounded and is not close to 0.
We conclude that probabilistic grammars, as generative distributions of data, do
not generally satisfy the Tsybakov noise condition. This motivates an alternative choice
of assumptions that could lead to better understanding of rates of convergences and
bounds on the excess risk. Section 3.1 states such assumptions which were also justified
empirically.
7.2 Comparison to Dirichlet Maximum A Posteriori Solutions
The transformation T(?,?) from Section 4.1 can be thought of as a smoother for the
probabilities ?: It ensures that the probability of each rule is at least ? (and as a result,
the probabilities of all rules cannot exceed 1? ?). Adding pseudo-counts to frequency
counts is also a common way to smooth probabilities in models based on multinomial
distributions, including probabilistic grammars (Manning and Schu?tze 1999). These
pseudo-counts can be framed as a maximum a posteriori (MAP) alternative to the
maximum likelihood problem, with the choice of Bayesian prior over the parameters in
the form of a Dirichlet distribution. In comparison to our framework, with (symmetric)
Dirichlet smoothing, instead of truncating the probabilities with a margin ? we would
set the probability of each rule (in the supervised setting) to
??k,i =
?n
j=1 ??j,k,i + ?? 1
?n
j=1 ??j,k,1 +
?n
j=1 ??j,k,2 + 2(?? 1)
(26)
for i = 1, 2, where ??k,i are the counts in the data of event i in multinomial k for Example j.
Dirichlet smoothing can be formulated as the result of adding a symmetric Dirichlet
prior over the parameters ?k,i with hyperparameter ?. Then Equation (26) is the mode
of the posterior after observing ??k,i appearances of event i in multinomial k.
The effect of Dirichlet smoothing becomes weaker as we have more samples,
because the frequency counts ??j,k,i become dominant in both the numerator and the
denominator when there are more data. In this sense, the prior?s effect on learning
diminishes as we use more data. A similar effect occurs in our framework: ? = n?s
where n is the number of samples?the more samples we have, the more we trust the
511
Computational Linguistics Volume 38, Number 3
counts in the data to be reliable. There is a subtle difference, however. With the Dirichlet
MAP solution, the smoothing is less dominant only if the counts of the features are large,
regardless of the number of samples we have. With our framework, smoothing depends
only on the number of samples we have. These two scenarios are related, of course: The
more samples we have, the more likely it is that the counts of the events will grow large.
7.3 Other Derivations of Sample Complexity Bounds
In this section, we discuss other possible solutions to the problem of deriving sample
complexity bounds for probabilistic grammars.
7.3.1 Using Talagrand?s Inequality. Our bounds are based on VC theory together with
classical results for empirical processes (Pollard 1984). There have been some recent
developments to the derivation of rates of convergence in statistical learning theory
(Massart 2000; Bartlett, Bousquet, and Mendelson 2005; Koltchinskii 2006), most
prominently through the use of Talagrand?s inequality (Talagrand 1994), which is a
concentration of measure inequality, in the spirit of Lemma 2.
The bounds achieved with Talagrand?s inequality are also distribution-dependent,
and are based on the diameter of the -minimal set?the set of hypotheses which have
an excess risk smaller than . We saw in Section 7.1 that the diameter of the -minimal
set does not follow the Tsybakov noise condition, but it is perhaps possible to find
meaningful bounds for it, in which case we may be able to get tighter bounds using
Talagrand?s inequality. We note that it may be possible to obtain data-dependent bounds
for the diameter of the -minimal set, following Koltchinskii (2006), by calculating the
diameter of the -minimal set using p?n.
7.3.2 Simpler Bounds for the Supervised Case.As noted in Section 6.1, minimizing empirical
risk with the log-loss leads to a simple frequency count for calculating the estimated
parameters of the grammar. In Corazza and Satta (2006), it has been also noted that to
minimize the non-empirical risk, it is necessary to set the parameters of the grammar to
the normalized expected count of the features.
This means that we can get bounds on the deviation of a certain parameter from
the optimal parameter by applying modifications to rather simple inequalities such
as Hoeffding?s inequality, which determines the probability of the average of a set of
i.i.d. random variables deviating from its mean. The modification would require us
to split the event space into two cases: one in which the count of some features is
larger than some fixed value (which will happen with small probability because of the
bounded expectation of features), and one in which they are all smaller than that fixed
value. Handling these two cases separately is necessary because Hoeffding?s inequality
requires that the count of the rules is bounded.
The bound on the deviation from the mean of the parameters (the true probability)
can potentially lead to a bound on the excess risk in the supervised case. This formula-
tion of the problem would not generalize to the unsupervised case, however, where the
empirical risk minimization does not amount to simple frequency count.
7.4 Open Problems
We conclude the discussion with some directions for further exploration and future
work.
512
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
7.4.1 Sample Complexity Bounds with Semi-Supervised Learning. Our bounds focus on the
supervised case and the unsupervised case. There is a trivial extension to the semi-
supervised case. Consider the objective function to be the sum of the likelihood for the
labeled data together with the marginalized likelihood of the unlabeled data (this sum
could be a weighted sum). Then, use the sample complexity bounds for each summand
to derive a sample complexity bound on this sum.
It would be more interesting to extend our results to frameworks such as the one
described by Balcan and Blum (2010). In that case, our discussion of sample complexity
would attempt to identify how unannotated data can reduce the space of candidate
probabilistic grammars to a smaller set, after which we can use the annotated data
to estimate the final grammar. This reduction of the space is accomplished through a
notion of compatibility, a type of fitness that the learner believes the estimated grammar
should have given the distribution that generates the data. The key challenge in the
case of probabilistic grammars would be to properly define this compatibility notion
such that it fits the log-loss. If this is achieved, then similar machinery to that described
in this paper (with proper approximations) can be followed to derive semi-supervised
sample complexity bounds for probabilistic grammars.
7.4.2 Sharper Bounds for the Pseudo-Dimension of Probabilistic Grammars. The pseudo-
dimension of a probabilistic grammar with the log-loss is bounded by the number of
parameters in the grammar, because the logarithm of a distribution generated by a
probabilistic grammar is a linear function. Typically the set of counts for the feature
vectors of a probabilistic grammar resides in a subspace of a dimension which is smaller
than the full dimension specified by the number of parameters, however. The reason for
this is that there are usually relationships (which are often linear) between the elements
in the feature counts. For example, with HMMs, the total feature count for emissions
should equal the total feature count for transitions. With PCFGs, the total number of
times that nonterminal rules fire equals the total number of times that features with
that nonerminal in the right-hand side fired, again reducing the pseudo-dimension. An
open problem that remains is characterization of the exact value pseudo-dimension for
a given grammar, determined by consideration of various properties of that grammar.
We conjecture, however, that a lower bound on the pseudo-dimension would be rather
close to the full dimension of the grammar (the number of parameters).
It is interesting to note that there has been some work to identify the VC dimension
and pseudo-dimension for certain types of grammars. Bane, Riggle, and Sonderegger
(2010), for example, calculated the VC dimension for constraint-based grammars.
Ishigami and Tani (1993, 1997) computed the VC dimension for finite state automata
with various properties.
7.5 Conclusion
We presented a framework for performing empirical risk minimization for probabilis-
tic grammars, in which sample complexity bounds, for the supervised case and the
unsupervised case, can be derived. Our framework is based on the idea of bounded
approximations used in the past to derive sample complexity bounds for graphical
models.
Our framework required assumptions about the probability distribution that gener-
ates sentences or derivations in the language of the given grammar. These assumptions
were tested using corpora, and found to fit the data well.
513
Computational Linguistics Volume 38, Number 3
We also discussed algorithms that can be used for minimizing empirical risk in
our framework, given enough samples. We showed that directly trying to minimize
empirical risk in the unsupervised case is NP-hard, and suggested an approximation
based on an expectation-maximization algorithm.
Appendix A. Proofs
We include in this appendix proofs for several results in the article.
Utility Lemma 1
Let ai ? [0, 1], i ? {1, . . . ,N} such that
?
i ai = 1. Define b1 = a1, c1 = 1? a1, bi =
(
ai
ai?1
)
(
bi?1
ci?1
)
, and ci = 1? bi for i ? 2. Then ai =
?
?
i?1
?
j=1
cj
?
? bi.
Proof
Proof by induction on i ? {1, . . . ,N}. Clearly, the statement holds for i = 1. Assume it
holds for arbitrary i < N. Then:
ai+1 =
(
ai
ai
)
ai+1 =
?
?
?
?
i?1
?
j=1
cj
?
? bi
?
?
ai+1
ai
=
?
?
?
?
i?1
?
j=1
cj
?
? bi
?
?
cibi+1
bi
=
?
?
i
?
j=1
cj
?
? bi+1
and this completes the proof. 
Lemma 1
Denote by Z,n the set
?
f?F{z | Cn( f )(z)? f (z) ? }. Denote by A,n the event ?one of
zi ? D is in Z,n.? If Fn properly approximates F, then:
E
[
Ep?n
[
gn
]
? Ep?n
[
f ?n
]
]
(A.1)
?
?
?
?
E
[
Ep?n
[
Cn( f
?
n )
]
| A,n
]
?
?
?
p(A,n)+
?
?
?
E
[
Ep?n
[
f ?n
]
| A,n
]
?
?
?
p(A,n)+ tail(n)
where the expectations are taken with respect to the data set D.
Proof
Consider the following:
E
[
Ep?n
[
gn
]
? Ep?n
[
f ?n
]
]
= E
[
Ep?n
[
gn
]
? Ep?n
[
Cn( f
?
n )
]
+ Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
]
= E
[
Ep?n
[
gn
]
? Ep?n
[
Cn( f
?
n )
]
]
+ E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
]
514
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Note first that E
[
Ep?n
[
gn
]
? Ep?n
[
Cn( f
?
n )
]]
? 0, by the definition of gn as the mini-
mizer of the empirical risk. We next bound E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]]
. We know from
the requirement of proper approximation that we have
E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
]
= E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
| A,n
]
p(A,n)
+ E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
| ?A,n
]
(1? p(A,n))
? |E
[
Ep?n
[
Cn( f
?
n )
]
| A,n
]
|p(A,n)+ |E
[
Ep?n
[
f ?n
]
| A,n
]
|p(A,n)+ tail(n)
and that equals the right side of Equation (Appendix A.1). 
Proposition 2
Let p ? P(?,L, r, q,B,G) and let Fm be as defined earlier. There exists a constant ? =
?(L, q, p,N) > 0 such that Fm has the boundedness property with Km = sN log
3m and
bound(m) = m
?? logm.
Proof
Let f ? Fm. Let Z(m) = {z | |z| ? log
2m}. Then, for all z ? Z(m) we have | f (z)| =
?
?
i,k?(k, i) log?k,i ?
?
i,k?(k, i)(p logm) ? sN log
3m = Km, where the first inequality
follows from f ? Fm (?k,i ? m?s) and the second from |z| ? log
2m. In addition, from the
requirements on p we have
E
[
| f | ? I {| f | ? Km}
]
?
(
sN log3m
)
?
?
?
?
k>log2m
L?(k)rkk
?
? ?
(
? log3m
)
?
(
qlog
2m
)
for ? = sNL
(1? q)2
. Finally, for ?(L, q, p,N)  log?+ 1+ log 1q = ? > 0 and if m > 1 then
(
? log3m
)(
qlog
2m
)
? m?? logm. 
Utility Lemma 4
(From [Dasgupta 1997].) Let a ? [0, 1] and let b = a if a ? [?, 1? ?], b = ? if a ? ?,
and b = 1? ? if a ? 1? ?. Then for any  ? 1/2 such that ? ? /(1+ ) we have
log a/b ? .
Proposition 3
Let p ? P(?,L, r, q,B,G) and let Fm as defined earlier. There exists anM such that for any
m > Mwe have
p
?
?
?
f?F
{z | Cm( f )(z)? f (z) ? tail(m)}
?
? ? tail(m)
for tail(m) =
N log2m
ms ? 1 and Cm( f ) = T( f,m
?s).
515
Computational Linguistics Volume 38, Number 3
Proof
Let Z(m) be the set of derivations of size bigger than log2m. Let f ? F. Define f ? =
T(f,m?s). For any z /? Z(m) we have that
f ?(z)? f (z) = ?
K
?
k=1
(
?k,1(z) log?k,1 + ?k,2(z) log?k,2 ? ?k,1(z) log?
?
k,1 ? ?k,1(z) log?
?
k,2
)
?
K
?
k=1
log2m
(
max{0, log(??k,1/?k,1)}+max{0, log(?
?
k,2/?k,2)}
)
(A.2)
Without loss of generality, assume tail(n)/N log
2m ? 1/2. Let ? =
tail(m)/N log
2m
1+ tail(m)/N log
2m
=
1/ms. From Utility Lemma 4 we have that log(??k,i/?k,i) ? tail(m)/N logm. Plug this
into Equation A.2 (N = 2K) to get that for all z /? Z(m) we have f ?(z)? f (z) ? tail(m).
It remains to show that the measure p(Z(m)) ? tail(m). Note that
?
z?Z(m) p(z) ?
?
k>log2m
L?(k)rk ? L
?
k>log2m
qk = Lqlog
2m/(1? q) < tail(m) for m > M where M is
fixed. 
Proposition 7
There exists a ??(L, p, q,N) > 0 such that F?m has the boundedness property with Km =
sN log3m and bound(m) = m
??? logm.
Proof
From the requirement of p, we know that for any x we have a z such that yield(z) = x
and |z| ? ?|x|. Therefore, if we let X(m) = {x | |x| ? log2m/?}, then we have for any
f ? F?m and x ? X(m) that f (x) ? sN log
3m = Km (similarly to the proof of Proposition 2).
Denote by f1(x, z) the function in Fm such that f (x) = ? log
?
z exp(?f1(x, z)).
In addition, from the requirements on p and the definition of Km we have
E
[
| f | ? I {| f | ? Km}
]
=
?
x
p(x)f (x)I { f ? Km}
=
?
x:|x|>log2m/?
p(x)f (x)
?
?
x:|x|>log2m/?
p(x)f1(x, z(x))
516
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
where z(x) is some derivation for x. We have
?
x:|x|>log2m/?
p(x)f1(x, z(x)) ?
?
x:|x|?log2m/?
?
z?Dx(G)
p(x, z)f1(x, z(x))
? sN logm
?
x:|x|>log2m/?
?
z
p(x, z)|z(x)|
? sN logm
?
k>log2m
?(k)rkk
? sN logm
?
k>log2m
qkk ? ? logmqlog
2m
for some constant ? > 0. Finally, for some ??(L, p, q,N) = ?? > 0 and some constant M,
if m > M then ? logm
(
qlog
2m
)
? m??
? logm. 
Utility Lemma 2
For ai, bi ? 0, if ? log
?
i ai + log
?
i bi ?  then there exists an i such that ? log ai +
log bi ? .
Proof
Assume ? log ai + log bi <  for all i. Then, bi/ai < e, therefore
?
i bi/
?
i ai < e
, there-
fore ? log
?
i ai + log
?
i bi <  which is a contradiction to ? log
?
i ai + log
?
i bi ? .

The next lemma is the main concentation of measure result that we use. Its proof
requires some simple modification to the proof given for Theorem 24 in Pollard (1984,
pages 30?31).
Lemma 2
Let Fn be a permissible class of functions such that for every f ? Fn we have E[| f | ?
I {| f | ? Kn}] ? bound(n). Let Ftruncated,n = { f ? I { f ? Kn} | f ? Fm}, that is, the set of
functions from Fn after being truncated by Kn. Then for  > 0 we have
p
(
sup
f?Fn
|Ep?n
[
f
]
? Ep
[
f
]
| > 2
)
? 8N(/8,Ftruncated,n) exp
(
? 1
128
n2/K2n
)
+ bound(n)/
provided n ? K2n/4
2 and bound(n) < .
Proof
First note that
sup
f?Fn
|Ep?n
[
f
]
? Ep
[
f
]
| ? sup
f?Fn
|Ep?n
[
f I {| f | ? Kn}
]
? Ep
[
f I {| f | ? Kn}
]
|
+ sup
f?Fn
Ep?n
[
| f |I {| f | ? Kn}
]
+ sup
f?Fn
Ep
[
| f |I {| f | ? Kn}
]
517
Computational Linguistics Volume 38, Number 3
We have supf?Fn Ep
[
| f |I {| f | ? Kn}
]
? bound(n) < , and also, from Markov in-
equality, we have
P(sup
f?Fn
Ep?n
[
| f |I {| f | ? Kn}
]
> ) ? bound(n)/
At this point, we can follow the proof of Theorem 24 in Pollard (1984), and its
extension on pages 30?31 to get Lemma 2, using the shifted set of functions Ftruncated,n.

Appendix B. Minimizing Log-Loss for Probabilistic Grammars
Central to our algorithms for minimizing the log-loss (both in the supervised case and
the unsupervised case) is a convex optimization problem of the form
min
?
K
?
k=1
ck,1 log?k,1 + ck,2 log?k,2
such that ?k ? {1, . . . ,K} :
?k,1 + ?k,2 = 1
? ? ?k,1 ? 1? ?
? ? ?k,2 ? 1? ?
for constants ck,i which depend on p?n or some other intermediate distribution in the
case of the expectation-maximization algorithm and ? which is a margin determined
by the number of samples. This minimization problem can be decomposed into several
optimization problems, one for each k, each having the following form:
max
?
c1?1 + c2?2 (B.1)
such that exp(?1)+ exp(?2) = 1 (B.2)
? ? ?1 ? 1? ? (B.3)
? ? ?2 ? 1? ? (B.4)
where ci ? 0 and 1/2 > ? ? 0. Ignore for a moment the constraints ? ? ?i ? 1? ?. In
that case, this can be thought of as a regular maximum likelihood estimation problem,
so ?i = ci/(c1 + c2). We give a derivation of this result in this simple case for completion.
We use Lagranian multipliers to solve this problem. Let F(?1,?2) = c1?1 + c2?2. Define
the Lagrangian:
g(?) = inf
?
L(?,?)
= inf
?
c1?1 + c2?2 + ?(exp(?1)+ exp(?2)? 1)
518
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Taking the derivative of the term we minimize in the Lagrangian, we have
?L
??i
= ci + ? exp(?i)
Setting the derivatives to 0 for minimization, we have
g(?) = c1 log(?c1/?)+ c2 log(?c2/?)+ ?(?c1/?? c2/?? 1) (B.5)
g(?) is the objective function of the dual problem of Equation (B.1)?Equation (B.2).
We would like to minimize Equation (B.5) with respect to ?. The derivative of g(?) is
?g
??
= ?c1/?? c2/?? 1
hence when equating the derivative of g(?) to 0, we get ? = ?(c1 + c2), and therefore
the solution is ??i = log (ci/(c1 + c2)). We need to verify that the solution to the dual
problem indeed gets the optimal value for the primal. Because the primal problem is
convex, it is sufficient to verify that the Karush-Kuhn-Tucker (KKT) conditions hold
(Boyd and Vandenberghe 2004). Indeed, we have
?F
??i
(??)+ ? ?h
??i
(??) = ci ? (c1 + c2)?
ci
c1 + c2
= 0
where h(?)  exp(?)+ exp(?)? 1 stands for the equality constraint. The rest of the
KKT conditions trivially hold, therefore ?? is the optimal solution for Equations (B.1)?
(B.2).
Note that if 1? ? < ci/(c1 + c2) < ?, then this is the solution even when again
adding the constraints in Equation (B.3) and (B.4). When c1/(c1 + c2) < ?, then the
solution is ??1 = ? and ?
?
2 = 1? ?. Similarly, when c2/(c1 + c2) < ? then the solution is
??2 = ? and ?
?
1 = 1? ?. We describe why this is true for the first case. The second case
follows very similarly. Assume c1/(c1 + c2) < ?. We want to show that for any choice of
? ? [0, 1] such that ? > ?we have
c1 log?+ c2 log(1? ?) ? c1 log?+ c2 log(1? ?)
Divide both sides of the inequality by c1 + c2 and we get that we need to show that
c1
c1 + c2
log(?/?)+
c2
c1 + c2
log
(
1? ?
1? ?
)
? 0
Becausewehave? > ?, andwealsohave c1/(c1 + c2) < ?, it is sufficient to show that
? log(?/?)+ (1? ?) log
(
1? ?
1? ?
)
? 0 (B.6)
Equation (B.6) is precisely the definition of the KL divergence between the distribu-
tion of a coinwith probability ? of heads and the distribution of a coinwith probability?
519
Computational Linguistics Volume 38, Number 3
of heads, and therefore the right side in Equation (B.6) is positive, and we get what
we need.
Appendix C. Counterexample to Tsybakov Noise (Proofs)
Lemma 6
A = AG(?) is positive semi-definite for any probabilistic grammar ?G,??.
Proof
Let dk,i be a collection of constants. Define the random variable:
R(z) =
?
i,k
dk,i
E
[
?k,i
]?k,i(z)
We have that
E
[
R2
]
=
?
i,i?
?
k,k?
A(k,i),(k?,i? )dk,idk?,i?
which is always larger or equal to 0. Therefore, A is positive semi-definite. 
Lemma 7
Let 0 < ? < 1/2, c1, c2 ? 0. Let ?,C > 0. Also, assume that c1 ? c2. For any  > 0, define:
a = ?
(
exp
(
C1/? + /2
c1
))
= ?1?
b = ?
(
exp
(
?C1/? + /2
c2
))
= ?2?
t() = c1
(
1? ?
1? a
)
+ c2
(
1? ?
1? b
)
? (c1 + c2) exp(/2)
Then, for small enough , we have t() ? 0.
Proof
We have that t() ? 0 if
ac2 + bc1 ? ?
(c1 + c2)(1? a)(1? b)
1? ? exp(/2)+ c1 + c2
= (c1 + c2)
(
1?
(1? a)(1? b)
(1? ?) exp(?/2)
)
(C.1)
First, show that
(1? a)(1? b)
(1? ?) exp(?/2)
? 1? ? (C.2)
520
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
which happens if (after substituting a = ?1?, b = ?2?)
? ? (?1 + ?2 ? 2)/(1? ?1?2)
Note we have ?1?2 > 1 because c1 ? c2. In addition, we have ?1 + ?2 ? 2 ? 0 for small
enough  (can be shown by taking the derivative, with respect to  of?1 + ?2 ? 2, which
is always positive for small enough , and in addition, noticing that the value of ?1 +
?2 ? 2 is 0 when  = 0.) Therefore, Equation (C.2) is true.
Substituting Equation (C.2) in Equation (C.1), we have that t() ? 0 if
ac2 + bc1 ? (c1 + c2)?
which is equivalent to
c2?1 + c1?2 ? c1 + c2 (C.3)
Taking again the derivative of the left side of Equation (C.3), we have that it is an
increasing function of  (if c1 ? c2), and in addition at  = 0 it obtains the value c1 + c2.
Therefore, Equation (C.3) holds, and therefore t() ? 0 for small enough . 
Theorem 5
Let G be a grammar with K ? 2 and degree 2. Assume that p is ?G,??? for some ??, such
that ??1,1 = ?
?
2,1 = ? and that c1 ? c2. If AG(?
?) is positive definite, then p does not satisfy
the Tsybakov noise condition for any (C,?), where C > 0 and ? ? 1.
Proof
Define ? to be the eigenvalue of AG(?) with the smallest value (? is positive). Also,
define v(?) to be a vector indexed by k, i such that
vk,i(?) = E
[
?k,i
]
log
??k,i
?k,i
.
Simple algebra shows that for any h ? H(G) (and the fact that p ? H(G)), we have
Ep(h) = DKL(p?h) =
K
?
k=1
(
Ep
[
?k,1
]
log
??k,1
?k,1
+ Ep
[
?k,1
]
log
(
1? ??k,1
1? ?k,1
))
For a C > 0 and ? ? 1, define ? = C1/?. Let  < ?. First, we construct an h such
that DKL(p?h) < + /2 but dist(p, h) > C1/? as  ? 0. The construction follows.
Parametrize h by ? such that ? is identical to ?? except for k = 1, 2, in which case we
have
?1,1 = ?
?
1,1
(
exp
(
?+ /2
c1
))
= ?
(
exp
(
?+ /2
c1
))
(C.4)
?2,1 = ?
?
2,1
(
exp
(
??+ /2
c2
))
= ?
(
exp
(
??+ /2
c2
))
(C.5)
521
Computational Linguistics Volume 38, Number 3
Note that ? ? ?1,1 ? 1/2 and ?2,1 < ?. Then, we have that
DKL(p?h) =
K
?
k=1
(
Ep
[
?k,1
]
log
??k,1
?k,1
+ Ep
[
?k,1
]
log
(
1? ??k,1
1? ?k,1
))
= + c1 log
1? ??k,1
1? ?1,1
+ c2 log
1? ??k,2
1? ?2,1
= + c1 log
1? ?
1? ?1,1
+ c2 log
1? ?
1? ?2,1
We also have
c1 log
1? ?
1? ?1,1
+ c2 log
1? ?
1? ?2,1
? 0 (C.6)
if
c1 ?
1? ?
1? ?1,1
+ c2 ?
1? ?
1? ?2,1
? c1 + c2 (C.7)
(This can be shown by dividing Equation [C.6] by c1 + c2 and then using the concavity of
the logarithm function.) From Lemma 7, we have that Equation (C.7) holds. Therefore,
DKL(p?h) ? 2
Now, consider the following, which can be shown through algebraic manipulation:
dist(p, h) = E
[
(
log
p
h
)2
]
=
?
k,k?
?
i,i?
E
[
?k,i ??k?,i?
]
(
log
??k,i
?k,i
)(
log
??k?,i?
?k?,i?
)
Then, additional algebraic simplification shows that
E
[
(
log
p
h
)2
]
= v(?)Av(?)
A fact from linear algebra states that
v(?)Av(?) ? ?||v(?)||22
where ? is the smallest eigenvalue in A. From the construction of ? and Equation (C.4)?
(C.5), we have that ||v(?)||22 > ?
2. Therefore,
E
[
(
log
p
h
)2
]
? ??2
which means dist(p, h) ?
?
?C1/?. Therefore, p does not satisfy the Tsybakov noise
condition with parameters (D,?) for any D > 0. 
522
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Appendix D. Notation
Table D.1 gives a table of notation for symbols used throughout this article.
Table 1
Table of notation symbols used in this article.
Symbol Description 1st Mention
E
R
M
X Instance space (natural language sentences) Sec. 2
Z Output space (grammar derivations) Sec. 2
p Distribution generating the data Sec. 2
Q Concept space, a family of distributions Sec. 2
q An estimated distribution Sec. 2
qopt Risk minimizer Eq. 1
n Number of available samples Sec. 2
p?n Empirical distribution Sec. 2
q? Empirical risk minimizer Eq. 2
Ep(q;Q) Excess risk Eq. 4
Rn(Q) Empirical process for the log-loss Eq. 5
G
ra
m
m
a
rs
G Grammar (for example, CFG rules) Sec. 3
? Probabilistic grammar parameters Sec. 3
K Number of multinomials in the probabilistic grammar Eq. 11
Nk Size of the kth multinomial of the probabilistic grammar Eq. 11
N
?K
k=1Nk Sec. 3
x Sentence in the language of the grammar Sec. 3
z Derivation in the grammar Sec. 3
?k,i(x, z) Count of the ith event firing in the kth multinomial in x and z Eq. 11
?G Parameter space for a given probabilistic grammar G Eq. 11
? Parameters for a probabilistic grammar Eq. 11
deg(G) The degree of G, maxk Nk Sec. 3
Dx(G) The set of derivations for string x Sec. 3
H,H(G) Concept space, a set of probabilistic grammars Sec. 3
F,F(G) Negated log-concept space, {? log h | h ? H(G)} Sec. 3
L Constant determining distributional assumption Sec. 3.1
q Constant determining distributional assumption Sec. 3.1
r Constant determining distributional assumption Sec. 3.1
P
ro
p
e
r
A
p
p
ro
x
im
a
ti
o
n
s
Fn Element n in a proper approximation (contained in F) Sec. 4
tail(n) Convergence rate for the boundedness property Sec. 4
bound(n) Convergence rate for the tightness property Sec. 4
Cn( f ) A map for f ? F to f ? ? Fn Sec. 4
T(?,?) Parameters ?with shifted probabilities Sec. 4.1
T( f,?) f ? F with shifted probabilities Sec. 4.1
?G(?) Set of parameters {T(?,?) | ? ? ?G} for a given G Sec. 4.1
s A constant larger than 1 on which boundedness property
depends
Sec. 4.1
?(L, q, p,N) A constant on which sample complexity depends for the su-
pervised case
Prop. 2
F?n Element n in a proper approximation (contained in F) Sec. 4
C?n( f ) A map for f ? F to f
? ? Fn Sec. 4
?tail(n) Convergence rate for the soundness property Sec. 4
?bound(n) Convergence rate for the tightness property Sec. 4
??(L, q, p,N) A constant on which sample complexity depends for the
unsupervised case
Sec. 5.3
523
Computational Linguistics Volume 38, Number 3
Acknowledgments
The authors thank the anonymous reviewers
for their comments and Avrim Blum, Steve
Hanneke, Mark Johnson, John Lafferty, Dan
Roth, and Eric Xing for useful conversations.
This research was supported by National
Science Foundation grant IIS-0915187.
References
Abe, N., J. Takeuchi, and M. Warmuth.
1991. Polynomial learnability
of probabilistic concepts with
respect to the Kullback-Leiber
divergence. In Proceedings of the
Conference on Learning Theory,
pages 277?289.
Abe, N. and M. Warmuth. 1992. On
the computational complexity of
approximating distributions by
probabilistic automata.Machine
Learning, 2:205?260.
Angluin, D. 1987. Learning regular sets from
queries and counterexamples. Information
and Computation, 75:87?106.
Anthony, M. and P. L. Bartlett. 1999.
Neural Network Learning: Theoretical
Foundations. Cambridge
University Press.
Balcan, M. and A. Blum. 2010.
A discriminative model for semi-
supervised learning. Journal of the
Association for Computing Machinery,
57(3):1?46.
Balle, B., A. Quattoni, and X. Carreras.
2011. A spectral learning algorithm for
finite state transducers. In Proceedings
of the European Conference on Machine
Learning/the Principles and Practice of
Knowledge Discovery in Databases,
pages 156?171.
Bane, M., J. Riggle, and M. Sonderegger.
2010. The VC dimension of
constraint-based grammars.
Lingua, 120(5):1194?1208.
Bartlett, P., O. Bousquet, and S. Mendelson.
2005. Local Rademacher complexities.
Annals of Statistics, 33(4):1497?1537.
Bishop, C. M. 2006. Pattern Recognition and
Machine Learning. Springer, Berlin.
Boyd, S. and L. Vandenberghe. 2004.
Convex Optimization. Cambridge
University Press.
Carrasco, R. 1997. Accurate computation
of the relative entropy between
stochastic regular grammars.
Theoretical Informatics and Applications,
31(5):437?444.
Carroll, G. and E. Charniak. 1992. Two
experiments on learning probabilistic
dependency grammars from corpora.
Technical report, Brown University,
Providence, RI.
Charniak, E. 1993. Statistical Language
Learning. MIT Press, Cambridge, MA.
Charniak, E. and M. Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings of
the Association for Computational Linguistics,
pages 173?180.
Chi, Z. 1999. Statistical properties of
probabilistic context-free grammars.
Computational Linguistics, 25(1):131?160.
Clark, A., R. Eyraud, and A. Habrard. 2008.
A polynomial algorithm for the inference
of context free languages. In Proceedings of
the International Colloquium on Grammatical
Inference, pages 29?42.
Clark, A. and S. Lappin. 2010. Unsupervised
learning and grammar induction.
In Alexander Clark, Chris Fox, and
Shalom Lappin, editors, The Handbook
of Computational Linguistics and Natural
Language Processing. Wiley-Blackwell,
London, pages 197?220.
Clark, A. and F. Thollard. 2004.
PAC-learnability of probabilistic
deterministic finite state automata.
Journal of Machine Learning Research,
5:473?497.
Cohen, S. B. and N. A. Smith. 2010a.
Covariance in unsupervised learning of
probabilistic grammars. Journal of Machine
Learning Research, 11:3017?3051.
Cohen, S. B. and N. A. Smith. 2010b.
Empirical risk minimization with
approximations of probabilistic
grammars. In Proceedings of the
Advances in Neural Information
Processing Systems, pages 424?432.
Cohen, S. B. and N. A. Smith. 2010c. Viterbi
training for PCFGs: Hardness results and
competitiveness of uniform initialization.
In Proceedings of the Association for
Computational Linguistics, pages 1502?1511.
Collins, M. 2003. Head-driven statistical
models for natural language processing.
Computational Linguistics, 29:589?637.
Collins, M. 2004. Parameter estimation for
statistical parsing models: Theory and
practice of distribution-free methods.
In H. Bunt, J. Carroll, and G. Satta, Text,
Speech and Language Technology (New
Developments in Parsing Technology).
Kluwer, Dordrecht, pages 19?55.
Corazza, A. and G. Satta. 2006. Cross-entropy
and estimation of probabilistic context-free
524
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
grammars. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics, pages 335?342.
Cover, T. M. and J. A. Thomas. 1991.
Elements of Information Theory. Wiley,
London.
Dasgupta, S. 1997. The sample complexity
of learning fixed-structure bayesian
networks.Machine Learning,
29(2?3):165?180.
de la Higuera, C. 2005. A bibliographical
study of grammatical inference. Pattern
Recognition, 38:1332?1348.
Dempster, A., N. Laird, and D. Rubin. 1977.
Maximum likelihood estimation from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B,
39:1?38.
Gildea, D. 2010. Optimal parsing strategies
for linear context-free rewriting systems.
In Proceedings of the North American Chapter
of the Association for Computational
Linguistics, pages 769?776.
Go?mez-Rodr??guez, C. and G. Satta.
2009. An optimal-time binarization
algorithm for linear context-free
rewriting systems with fan-out two.
In Proceedings of the Association for
Computational Linguistics-International
Joint Conference on Natural Language
Processing, pages 985?993.
Grenander, U. 1981. Abstract Inference. Wiley,
New York.
Haussler, D. 1992. Decision-theoretic
generalizations of the PAC model
for neural net and other learning
applications. Information and
Computation, 100:78?150.
Hsu, D., S. M. Kakade, and T. Zhang.
2009. A spectral algorithm for
learning hidden Markov models.
In Proceedings of the Conference on
Learning Theory.
Ishigami, Y. and S. Tani. 1993. The
VC-dimensions of finite automata
with n states. In Proceedings of
Algorithmic Learning Theory,
pages 328?341.
Ishigami, Y. and S. Tani. 1997.
VC-dimensions of finite automata and
commutative finite automata with k letters
and n states. Applied Mathematics,
74(3):229?240.
Jaeger, H. 1999. Observable operator models
for discrete stochastic time series. Neural
Computation, 12:1371?1398.
Kearns, M. and L. Valiant. 1989.
Cryptographic limitations on learning
Boolean formulae and finite automata.
In Proceedings of the 21st Association
for Computing Machinery Symposium
on the Theory of Computing,
pages 433?444.
Kearns, M. J. and U. V. Vazirani. 1994.
An Introduction to Computational
Learning Theory. MIT Press,
Cambridge, MA.
Klein, D. and C. D. Manning. 2004.
Corpus-based induction of syntactic
structure: Models of dependency and
constituency. In Proceedings of the
Association for Computational Linguistics,
pages 478?487.
Koltchinskii, V. 2006. Local Rademacher
complexities and oracle inequalities
in risk minimization. The Annals of
Statistics, 34(6):2593?2656.
Leermakers, R. 1989. How to cover a
grammar. In Proceedings of the Association
for Computational Linguistics,
pages 135?142.
Manning, C. D. and H. Schu?tze. 1999.
Foundations of Statistical Natural
Language Processing. MIT Press,
Cambridge, MA.
Massart, P. 2000. Some applications of
concentration inequalities to statistics.
Annales de la Faculte? des Sciences de
Toulouse, IX(2):245?303.
Nijholt, A. 1980. Context-Free Grammars:
Covers, Normal Forms, and Parsing
(volume 93 of Lecture Notes in
Computer Science). Springer-Verlag,
Berlin.
Palmer, N. and P. W. Goldberg. 2007.
PAC-learnability of probabilistic
deterministic finite state automata
in terms of variation distance.
In Proceedings of Algorithmic Learning
Theory, pages 157?170.
Pereira, F. C. N. and Y. Schabes. 1992.
Inside-outside reestimation from partially
bracketed corpora. In Proceedings of the
Association for Computational Linguistics,
pages 128?135.
Pitt, L. 1989. Inductive inference, DFAs, and
computational complexity. Analogical and
Inductive Inference, 397:18?44.
Pollard, D. 1984. Convergence of Stochastic
Processes. Springer-Verlag, New York.
Ron, D. 1995. Automata Learning and Its
Applications. Ph.D. thesis, Hebrew
University of Jerusalem.
Ron, D., Y. Singer, and N. Tishby. 1998.
On the learnability and usage of acyclic
probabilistic finite automata. Journal
of Computer and System Sciences,
56(2):133?152.
525
Computational Linguistics Volume 38, Number 3
Shalev-Shwartz, S., O. Shamir, K. Sridharan,
and N. Srebro. 2009. Learnability and
stability in the general learning setting.
In Proceedings of the Conference on
Learning Theory.
Sipser, M. 2006. Introduction to the Theory of
Computation, Second Edition. Thomson
Course Technology, Boston, MA.
Talagrand, M. 1994. Sharper bounds for
Gaussian and empirical processes.
Annals of Probability, 22:28?76.
Terwijn, S. A. 2002. On the learnability of
hidden Markov models. In P. Adriaans,
H. Fernow, & M. van Zaane. Grammatical
Inference: Algorithms and Applications
(Lecture Notes in Computer Science).
Springer, Berlin, pages 344?348.
Tsybakov, A. 2004. Optimal aggregation of
classifiers in statistical learning. The Annals
of Statistics, 32(1):135?166.
Vapnik, V. N. 1998. Statistical Learning Theory.
Wiley-Interscience, New York.
526
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 564?572,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Variational Inference for Adaptor Grammars
Shay B. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
scohen@cs.cmu.edu
David M. Blei
Computer Science Department
Princeton University
Princeton, NJ 08540, USA
blei@cs.princeton.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
Adaptor grammars extend probabilistic
context-free grammars to define prior dis-
tributions over trees with ?rich get richer?
dynamics. Inference for adaptor grammars
seeks to find parse trees for raw text. This
paper describes a variational inference al-
gorithm for adaptor grammars, providing
an alternative to Markov chain Monte Carlo
methods. To derive this method, we develop
a stick-breaking representation of adaptor
grammars, a representation that enables us
to define adaptor grammars with recursion.
We report experimental results on a word
segmentation task, showing that variational
inference performs comparably to MCMC.
Further, we show a significant speed-up when
parallelizing the algorithm. Finally, we report
promising results for a new application for
adaptor grammars, dependency grammar
induction.
1 Introduction
Recent research in unsupervised learning for NLP
focuses on Bayesian methods for probabilistic gram-
mars (Goldwater and Griffiths, 2007; Toutanova and
Johnson, 2007; Johnson et al, 2007). Such meth-
ods have been made more flexible with nonparamet-
ric Bayesian (NP Bayes) methods, such as Dirichlet
process mixture models (Antoniak, 1974; Pitman,
2002). One line of research uses NP Bayes meth-
ods on whole tree structures, in the form of adaptor
grammars (Johnson et al, 2006; Johnson, 2008b;
Johnson, 2008a; Johnson and Goldwater, 2009), in
order to identify recurrent subtree patterns.
Adaptor grammars provide a flexible distribu-
tion over parse trees that has more structure than
a traditional context-free grammar. Adaptor gram-
mars are used via posterior inference, the compu-
tational problem of determining the posterior distri-
bution of parse trees given a set of observed sen-
tences. Current posterior inference algorithms for
adaptor grammars are based on MCMC sampling
methods (Robert and Casella, 2005). MCMC meth-
ods are theoretically guaranteed to converge to the
true posterior, but come at great expense: they are
notoriously slow to converge, especially with com-
plex hidden structures such as syntactic trees. John-
son (2008b) comments on this, and suggests the use
of variational inference as a possible remedy.
Variational inference provides a deterministic al-
ternative to sampling. It was introduced for Dirich-
let process mixtures by Blei and Jordan (2005) and
applied to infinite grammars by Liang et al (2007).
With NP Bayes models, variational methods are
based on the stick-breaking representation (Sethu-
raman, 1994). Devising a stick-breaking represen-
tation is a central challenge to using variational in-
ference in this setting.
The rest of this paper is organized as follows. In
?2 we describe a stick-breaking representation of
adaptor grammars, which enables variational infer-
ence (?3) and a well-defined incorporation of recur-
sion into adaptor grammars. In ?4 we give an em-
pirical comparison of the algorithm to MCMC in-
ference and describe a novel application of adaptor
grammars to unsupervised dependency parsing.
2 Adaptor Grammars
We review adaptor grammars and develop a stick-
breaking representation of the tree distribution.
2.1 Definition of Adaptor Grammars
Adaptor grammars capture syntactic regularities in
sentences by placing a nonparametric prior over the
distribution of syntactic trees that underlie them.
The model exhibits ?rich get richer? dynamics: once
a tree is generated, it is more likely to reappear.
Adaptor grammars were developed by Johnson et
al. (2006). An adaptor grammar is a tuple A =
?G,M,a, b,??, which contains: (i) a context-free
grammar G = ?W,N,R, S? where W is the set of
564
terminals, N is the set of nonterminals, R is a set of
production rules, and S ? N is the start symbol?we
denote byRA the subset ofR with left-hand sideA;
(ii) a set of adapted nonterminals, M ? N; and (iii)
parameters a, b and ?, which are described below.
An adaptor grammar assumes the following gen-
erative process of trees. First, the multinomial dis-
tributions ? for a PCFG based on G are drawn
from Dirichlet distributions. Specifically, multino-
mial ?A ? Dir(?A) where? is collection of Dirich-
let parameters, indexed by A ? N.
Trees are then generated top-down starting with
S. Any non-adapted nonterminal A ? N \ M is
expanded by drawing a rule from RA. There are
two ways to expand A ?M:
1. With probability (nz ? bA)/(nA + aA) we ex-
pand A to subtree z (a tree rooted at A with a
yield in W?), where nz is the number of times
the tree z was previously generated and nA is the
total number of subtrees (tokens) previously gen-
erated root being A. We denote by a the concen-
tration parameters and b the discount parameters,
both indexed by A ? M. We have aA ? 0 and
bA ? [0, 1].
2. With probability (aA + kAbA)/(nA + aA), A is
expanded as in a PCFG by a draw from ?A over
RA, where kA is the number of subtrees (types)
previously generated with root A.
For the expansion of adapted nonterminals, this
process can be explained using the Chinese restau-
rant process (CRP) metaphor: a ?customer? (cor-
responding to a partially generated tree) enters a
?restaurant? (corresponding to a nonterminal) and
selects a ?table? (corresponding to a subtree) to at-
tach to the partially generated tree. If she is the first
customer at the table, the PCFG ?G,?? produces the
new table?s associated ?dish? (a subtree).1
When adaptor grammars are defined using the
CRP, the PCFG G has to be non-recursive with re-
1We note that our construction deviates from the strict def-
inition of adaptor grammars (Johnson et al, 2006): (i) in our
construction, we assume (as prior work does in practice) that
the adaptors in A = ?G,M,a, b,?? follow the Pitman-Yor
(PY) process (Pitman and Yor, 1997), though in general other
stochastic processes might be used; and (ii) we place a sym-
metric Dirichlet over the parameters of the PCFG, ?, whereas
Johnson et al used a fixed PCFG for the definition (though they
experimented with a Dirichlet prior).
spect to the adapted nonterminals. More precisely,
for A ? N, denote by Reachable(G, A) all the non-
terminals that can be reached from A using a partial
derivation from G. Then we restrict G such that
for all A ? M, we have A /? Reachable(G, A).
Without this restriction, we might end up in a sit-
uation where the generative process is ill-defined:
in the CRP terminology, a customer could enter a
restaurant and select a table whose dish is still in
the process of being selected.2 In the more general
form of adaptor grammars with arbitrary adaptors,
the problem amounts to mutually dependent defini-
tions of distributions which rely on the others to be
defined. We return to this problem in ?3.1.
Inference The inference problem is to compute
the posterior distribution of parse trees given ob-
served sentences x = ?x1, . . . , xn?. Typically, in-
ference with adaptor grammars is done with Gibbs
sampling. Johnson et al (2006) use an embedded
Metropolis-Hastings sampler (Robert and Casella,
2005) inside a Gibbs sampler. The proposal distribu-
tion is a PCFG, resembling a tree substitution gram-
mar (TSG; Joshi, 2003). The sampler of Johnson et
al. is based on the representation of the PY process
as a distribution over partitions of integers. This rep-
resentation is not amenable to variational inference.
2.2 Stick-Breaking Representation
To develop a variational inference algorithm for
adaptor grammars, we require an alternative repre-
sentation of the model in ?2.1. The CRP-based def-
inition implicitly marginalizes out a random distri-
bution over trees. For variational inference, we con-
struct that distribution.
We first review the Dirichlet process and its stick-
breaking representation. The Dirichlet process de-
fines a distribution over distributions. Samples from
the Dirichlet process tend to deviate from a base
distribution depending on a concentration parame-
ter. Let G ? DP(G0, a) be a distribution sampled
from the Dirichlet process with base distribution G0
2Consider the simple grammar with rules { S ? S S, S ? a
}. Assume that a customer enters the restaurant for S. She sits
at a table, and selects a dish, a subtree, which starts with the rule
S ? S S. Perhaps the first child S is expanded by S ? a. For
the second child S, it is possible to re-enter the ?S restaurant?
and choose the first table, where the ?dish? subtree is still being
generated.
565
and concentration parameter a. The distribution G
is discrete, which means it puts positive mass on a
countable number of atoms drawn from G0. Re-
peated draws from G exhibit the ?clustering prop-
erty,? which means that they will be assigned to the
same value with positive probability. Thus, they ex-
hibit a partition structure. Marginalizing out G, the
distribution of that partition structure is given by a
CRP with parameter a (Pitman, 2002).
The stick-breaking process gives a constructive
definition of G (Sethuraman, 1994). With the stick-
breaking process (for the PY process), we first sam-
ple ?stick lengths? pi ? GEM(a, b) (in the case of
Dirichlet process, we have b = 0). The GEM par-
titions the interval [0, 1] into countably many seg-
ments. First, draw vi ? Beta(1 ? b, a + ib) for
i ? {1, . . .}. Then, define pii , vi
?i?1
j=1(1 ? vj).
In addition, we also sample infinitely many ?atoms?
independently zi ? G0. Define G as:
G(z) =
??
i=1 pii?(zi, z) (1)
where ?(zi, z) is 1 if zi = z and 0 otherwise. This
random variable is drawn from a Pitman-Yor pro-
cess. Notice the discreteness of G is laid bare in the
stick-breaking construction.
With the stick-breaking representation in hand,
we turn to a constructive definition of the distri-
bution over trees given by an adaptor grammar.
Let A1, . . . , AK be an enumeration of the nonter-
minals in M which satisfies: i ? j ? Aj /?
Reachable(G, Ai). (That this exists follows from
the assumption about the lack of recursiveness of
adapted nonterminals.) Let Yield(z) be the yield of
a tree derivation z. The process that generates ob-
served sentences x = ?x1, . . . , xn? from the adaptor
grammarA = ?G,M,a, b,?? is as follows:
1. For each A ? N, draw ?A ? Dir(?A).
2. For A from A1 to AK , define GA as follows:
(a) Draw piA | aA, bA ? GEM(aA, bA).
(b) For i ? {1, . . .}, grow a tree zA,i as follows:
i. Draw A? B1 . . . Bn fromRA.
ii. zA,i = A
HHH

B1 ? ? ? Bn
iii. While Yield(zA,i) has nonterminals:
A. Choose an unexpanded nonterminal B
from yield of zA,i.
B. If B ? M, expand B according to GB
(defined on previous iterations of step 2).
C. If B ? N \M, expand B with a rule from
RB according to Mult(?B).
(c) For i ? {1, . . .}, define GA(zA,i) = piA,i
3. For i ? {1, . . . , n} draw zi as follows:
(a) If S ?M, draw zi | GS ? GS .
(b) If S /? M, draw zi as in 2(b) (omitted for
space).
4. Set xi = Yield(zi) for i ? {1, . . . , n}.
Here, there are four collections of hidden variables:
the PCFG multinomials ? = {?A | A ? N}, the
stick length proportions v = {vA | A ? M} where
vA = ?vA,1, vA,2, . . .?, the adapted nonterminals?
subtrees zA = {zA,i | A ? M; i ? {1, . . .}} and
the derivations z1:n = z1, . . . , zn. The symbol z
refers to the collection of {zA | A ? M}, and z1:n
refers to the derivations of the data x.
Note that the distribution in 2(c) is defined with
the GEM distribution, as mentioned earlier. It is a
sample from the Pitman-Yor process (or the Dirich-
let process), which is later used in 3(a) to sample
trees for an adapted non-terminal.
3 Variational Inference
Variational inference is a deterministic alternative
to MCMC, which casts posterior inference as an
optimization problem (Jordan et al, 1999; Wain-
wright and Jordan, 2008). The optimized function
is a bound on the marginal likelihood of the obser-
vations, which is expressed in terms of a so-called
?variational distribution? over the hidden variables.
When the bound is tightened, that distribution is
close to the posterior of interest. Variational meth-
ods tend to converge faster than MCMC, and can be
more easily parallelized over multiple processors in
a framework such as MapReduce (Dean and Ghe-
mawat, 2004).
The variational bound on the likelihood of the
data is:
log p(x | a,?) ? H(q) +
?
A?M
Eq[log p(vA | aA)]
+
?
A?M
Eq[log p(?A | ?A)]
+
?
A?M
Eq[log p(zA | v,?)] + Eq[log p(z | vA)]
566
Expectations are taken with respect to the variational
distribution q(v,?, z) and H(q) is its entropy.
Before tightening the bound, we define the func-
tional form of the variational distribution. We use
the mean-field distribution in which all of the hid-
den variables are independent and governed by in-
dividual variational parameters. (Note that in the
true posterior, the hidden variables are highly cou-
pled.) To account for the infinite collection of ran-
dom variables, for which we cannot define a varia-
tional distribution, we use the truncated stick distri-
bution (Blei and Jordan, 2005). Hence, we assume
that, for all A ? M, there is some value NA such
that q(vA,NA = 1) = 1. The assigned probability to
parse trees in the stick will be 0 for i > NA, so we
can ignore zA,i for i > NA. This leads to a factor-
ized variational distribution:
q(v,?, z) = (2)
?
A?M
(
q(?A)
NA?
i=1
q(vA,i)? q(zA,i)
)
?
n?
i=1
q(zi)
It is natural to define the variational distributions
over ? and v to be Dirichlet distributions with pa-
rameters ?A and Beta distributions with parameters
?A,i, respectively. The two distributions over trees,
q(zA,i) and q(zi), are more problematic. For ex-
ample, with q(zi | ?), we need to take into ac-
count different subtrees that could be generated by
the model and use them with the proper probabilities
in the variational distribution q(zi | ?). We follow
and extend the idea from Johnson et al (2006) and
use grammatons for these distributions. Gramma-
tons are ?mini-grammars,? inspired by the grammar
G.
For two strings in s, t ? W?, we use ?t ? s?
to mean that t is a substring of s. In that case, a
grammaton is defined as follows:
Definition 1. LetA = ?G,M,a, b,?? be an adap-
tor grammar with G = ?W,N,R, S?. Let s be a fi-
nite string over the alphabet ofG andA ? N. Let U
be the set of nonterminals U , Reachable(G, A) ?
(N \M). The grammaton G(A, s) is the context-
free grammar with the start symbol A and the rules
RA?
(
?
B?U
RB
)
?
?
A?B1...Bn?RA
?
i?{i|Bi?M}
{Bi ?
t | t ? s}.
Using a grammaton, we define the distributions
q(zA,i | ?A) and q(zi | ?). This requires a pre-
processing step (described in detail in ?3.3) that de-
fines, for each A ? M, a list of strings sA =
?sA,1, . . . , sA,NA?. Then, for q(zA,i | ?A) we use
the grammaton G(A, sA,i) and for q(zi | ?) we
use the grammaton G(A, xi) where xi is the ith
observed sentence. We parametrize the grammaton
with weights ?A (or ?) for each rule in the gramma-
ton. This makes the variational distributions over the
trees for strings s (and trees for x) globally normal-
ized weighted grammars. Choosing such distribu-
tions is motivated by their ability to make the varia-
tional bound tight (similar to Cohen et al, 2008, and
Cohen and Smith, 2009). In practice we do not have
to use rewrite rules for all strings t ? s in the gram-
maton. It suffices to add rewrite rules only for the
strings t = sA,i that have some grammaton attached
to them,G(A, sA,i).
The variational distribution above yields a vari-
ational inference algorithm for approximating the
posterior by estimating ?A,i, ?A, ?A and ? it-
eratively, given a fixed set of hyperparameters
a, b and ?. Let r be a PCFG rule. Let
f?(r, sB,k) = Eq(zk|?B,k)[f(r; zk)], where f(r; zk)
counts the number of times that rule r is applied in
the derivation zk. Let A ? ? denote a rule from
G. The quantity f?(r, sB,k) is computed using the
inside-outside (IO) algorithm. Fig. 1 gives the vari-
ational inference updates.
Variational EM We use variational EM to fit the
hyperparameters. Variational EM is an EM algo-
rithm where the E step is replaced by variational in-
ference (Fig. 1). The M-step optimizes the hyperpa-
rameters (a, b and ?) with respect to expected suffi-
cient statistics under the variational distribution. We
use Newton-Raphson for each (Boyd and Vanden-
berghe, 2004); Fig. 2 gives the objectives.
3.1 Note about Recursive Grammars
With recursive grammars, the stick-breaking pro-
cess representation gives probability mass to events
which are ill-defined. In step 2(iii)(c) of the stick-
breaking representation, we assign nonzero proba-
bility to an event in which we choose to expand the
current tree using a subtree with the same index that
we are currently still expanding (see footnote 2). In
567
short, with recursive grammars, we can get ?loops?
inside the trees.
We would still like to use recursion in the cases
which are not ill-defined. In the case of recur-
sive grammars, there is no problem with the stick-
breaking representation and the order by which we
enumerate the nonterminals. This is true because the
stick-breaking process separates allocating the prob-
abilities for each index in the stick and allocating the
atoms for each index in the stick.
Our variational distributions give probability 0 to
any event which is ill-defined in the sense men-
tioned above. Optimizing the variational bound in
this case is equivalent to optimizing the same vari-
ational bound with a model p? that (i) starts with p,
(ii) assigns probability 0 to ill-defined events, and
(iii) renormalizes:
Proposition 2. Let p(x, z) be a probability distri-
bution, where z ? Z, and let S ? Z. Let Q = {q |
q(z) = 0, ?z ? S}, a set of distributions. Then:
argmax
q?Q
Eq[log p(x, z)] = argmax
q
Eq[log p?(x, z)]
where p?(x, z) is a probability distribution defined
as p?(x, z) = p(x, z)/
?
z?S p(x, z) for z ? S and
0 otherwise.
For this reason, our variational approximation al-
lows the use of recursive grammars. The use of re-
cursive grammars with MCMC methods is problem-
atic, since it has no corresponding probabilistic in-
terpretation, enabled by zeroing events that are ill-
defined in the variational distribution. There is no
underlying model such as p?, and thus the inference
algorithm is invalid.
3.2 Time Complexity
The algorithm in Johnson et al (2006) works by
sampling from a PCFG containing rewrite rules that
rewrite to a whole tree fragment. This requires
a procedure that uses the inside-outside algorithm.
Despite the grammar being bigger (because of the
rewrite rules to a string), the asymptotic complexity
of the IO algorithm stays O(|N|2|xi|3 + |N|3|xi|2)
where |xi| is the length of the ith sentence.3
3This analysis is true for CNF grammars augmented with
rules rewriting to a whole string, like those used in our study.
?1A,i = 1? bA +
?
B?M
?NB
k=1 f?(A? sA,i, sB,k)
?2A,i = aA + ibA
+
?i?1
j=1
?
B?M
?NB
k=1 f?(A? sA,j , sB,k)
?A,A?? =
?
B?M
?NB
k=1 f?(A? ?, sB,k)
?A,A?sA,i = ?(?
1
A,i)??(?
1
A,i + ?
2
A,i)
+
?i?1
j=1
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
?A,A?? = ?(?A,A??)??
(?
? ?A,A??
)
Figure 1: Updates for variational inference with adaptor
grammars. ? is the digamma function.
Our algorithm requires running the IO algorithm
for each yield in the variational distribution, for each
nonterminal, and for each sentence. However, IO
runs with much smaller grammars coming from the
grammatons. The cost of running the IO algorithm
on the yields in the sticks for A ? M can be taken
into account parsing a string that appears in the cor-
pus with the full grammars. This leads to an asymp-
totic complexity of O(|N|2|xi|3 + |N|3|xi|2) for the
ith sentence in the corpus each iteration.
Asymptotically, both sampling and variational
EM behave the same. However, there are different
constants that hide in these asymptotic runtimes: the
number of iterations that the algorithm takes to con-
verge (for which variational EM generally has an ad-
vantage over sampling) and the number of additional
rewrite rules that rewrite to a string representing a
tree (for which MCMC has a relative advantage, be-
cause it does not use a fixed set of strings; instead,
the size of the grammars it uses grow as sampling
proceeds). In ?4, we see that variational EM and
sampling methods are similar in the time it takes to
complete because of a trade-off between these two
constants. Simple parallelization, however, which
is possible only with variational inference, provides
significant speed-ups.4
3.3 Heuristics for Variational Inference
For the variational approximation from ?3, we need
to decide on a set of strings, sA,i (for A ? M and
i ? {1, . . . , NA}) to define the grammatons in the
4Newman et al (2009) show how to parallelize sampling al-
gorithms, but in general, parallelizing these algorithms is more
complicated than parallelizing variational algorithms and re-
quires further approximation.
568
max?A log ?(|RA|?A)? |RA| log ?(?A) + (?A ? 1)
(?
A???RA
?(?A??)??
(?
A???RA
?A??
))
maxaA
?NA
i=1 aA
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
+ log ?(aA + 1 + ibA)? log ?(ibA + aA)
maxbA
?NA
i=1 ibA
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
+ log ?(aA + 1 + ibA)? log ?(1? bA)? log ?(ibA + aA)
Figure 2: Variational M-step updates. ? is the gamma function.
nonparametric stick. Any set of strings will give
a valid approximation, but to make the variational
approximation as accurate as possible, we require
that: (i) the strings in the set must be likely to be
generated using the adaptor grammar as constituents
headed by the relevant nonterminal, and (ii) strings
that are more likely to be generated should be asso-
ciated with a lower index in the stick. The reason for
the second requirement is the exponential decay of
coefficients as the index increases.
We show that a simple heuristic leads to an order
over the strings generated by the adaptor grammars
that yields an accurate variational estimation. We
begin with a weighted context-free grammar Gheur
that has the same rules as in G, only the weight for
all of its rules is 1. We then compute the quantity:
c(A, s) =
1
n
(
n?
i=1
EGheur [fi(z;A, s)]
)
? ? log |s|
(3)
where fi(z;A, s) is a function computing the count
of constituents headed by A with yield s in the tree
z for the sentence xi. This quantity can be com-
puted by using the IO algorithm onGheur. The term
? log |s| is subtracted to avoid preference for shorter
constituents, similar to Mochihashi et al (2009).
While computing c(A, s) using the IO algorithm,
we sort the set of all substrings of s according to
their expected counts (aggregated over all strings s).
Then, we use the top NA strings in the sorted list for
the grammatons of A.5
3.4 Decoding
The variational inference algorithm gives a distribu-
tions over parameters and hidden structures (through
the grammatons). We experiment with two com-
monly used decoding methods: Viterbi decoding
5The requirement to select NA in advance is strict. We ex-
perimented with dynamic expansions of the stick, in the spirit
of Kurihara et al (2006) and Wang and Blei (2009), but we did
not achieve better performance and it had an adverse effect on
runtime. For completeness, we give these results in ?4.
and minimum Bayes risk decoding (MBR; Good-
man, 1996).
To parse a string with Viterbi (or MBR) decoding,
we find the tree with highest score for the gramma-
ton which is attached to that string. For all rules
which rewrite to strings in the resulting tree, we
again perform Viterbi (or MBR) decoding recur-
sively using other grammatons.
4 Experiments
We describe experiments with variational inference
for adaptor grammars for word segmentation and de-
pendency grammar induction.
4.1 Word Segmentation
We follow the experimental setting of Johnson and
Goldwater (2009), who present state-of-the-art re-
sults for inference with adaptor grammars using
Gibbs sampling on a segmentation problem. We
use the standard Brent corpus (Brent and Cartwright,
1996), which includes 9,790 unsegmented phone-
mic representations of utterances of child-directed
speech from the Bernstein-Ratner (1987) corpus.
Johnson and Goldwater (2009) test three gram-
mars for this segmentation task. The first grammar
is a character unigram grammar (GUnigram). The
second grammar is a grammar that takes into con-
sideration collocations (GColloc) which includes the
rules { Sentence? Colloc, Sentence? Colloc Sen-
tence, Colloc ? Word+, Word ? Char+ }. The
third grammar incorporates more prior knowledge
about the syllabic structure of English (GSyllable).
GUnigram and GSyllable can be found in Johnson
and Goldwater (2009). Once an utterance is parsed,
Word constituents denote segments.
The value of ? (penalty term for string length) had
little effect on our results and was fixed at ? = ?0.2.
When NA (number of strings used in the variational
distributions) is fixed, we use NA = 15,000. We re-
port results using Viterbi and MBR decoding. John-
son and Goldwater (2009) experimented with two
569
this paper J&G 2009
grammar model Vit. MBR SA MM
GU
ni
gr
am
Dir 0.49 0.84 0.57 0.54
PY 0.49 0.84 0.81 0.75
PY+inc 0.42 0.59 - -
GC
ol
lo
c Dir 0.40 0.86 0.75 0.72
PY 0.40 0.86 0.83 0.86
PY+inc 0.43 0.60 - -
G S
yl
la
bl
e Dir 0.77 0.83 0.84 0.84
PY 0.77 0.83 0.89 0.88
PY+inc 0.75 0.76 - -
Table 1: F1 performance for word segmentation on the
Brent corpus. Dir. stands for Dirichlet Process adaptor
(b = 0), PY stands for Pitman-Yor adaptor (b optimized),
and PY+inc. stands for Pitman-Yor with iteratively in-
creasing NA for A ? M (see footnote 5). J&G 2009 are
the results adapted from Johnson and Goldwater (2009);
SA is sample average decoding, and MM is maximum
marginal decoding.
Truncated stick length
F1 sc
ore
65
70
75
80
l
l
l
l
l l l
l l l l l l
l l
l
l
l
l
l
l l
l l
l l l l l l
2000 4000 6000 8000 10000 12000 14000
Figure 3: F1 performance of GUnigram as influenced by
the length of the stick, NWord.
decoding methods, sample average (SA) and maxi-
mal marginal decoding (MM), which are closely re-
lated to Viterbi and MBR, respectively. With MM,
we marginalize the tree structure, rather than the
word segmentation induced, similar to MBR decod-
ing. With SA, we compute the probability of a whole
tree, by averaging its count in the samples, an ap-
proximation to finding the tree with highest proba-
bility, like Viterbi.
Table 1 gives the results for our experiments. No-
tice that the results for the Pitman-Yor process and
the Dirichlet process are similar. When inspecting
the learned parameters, we noticed that the discount
parameters (b) learned by the variational inference
algorithm for the Pitman-Yor process are very close
to 0. In this case, the Pitman-Yor process is reduced
to the Dirichlet process.
Similar to Johnson and Goldwater?s comparisons,
we see superior performance when using minimum
Bayes risk over Viterbi decoding. Further notice that
the variational inference algorithm obtains signifi-
cantly superior performance for simpler grammars
than Johnson et al, while performance using the syl-
lable grammar is lower. The results also suggest that
it is better to decide ahead on the set of strings avail-
able in the sticks, instead of working gradually and
increase the size of the sticks as described in foot-
note 5. We believe that the reason is that the varia-
tional inference algorithm settles in a trajectory that
uses fewer strings, then fails to exploit the strings
that are added to the stick later. Given that select-
ing NA in advance is advantageous, we may inquire
if choosing NA to be too large can lead to degraded
performance, because of fragmention of the gram-
mar. Fig. 3 suggests it is not the case, and per-
formance stays steady after NA reaches a certain
value.
One of the advantages of variational approxima-
tion over sampling methods is the ability to run
for fewer iterations. For example, with GUnigram
convergence typically takes 40 iterations with vari-
ational inference, while Johnson and Goldwater
(2009) ran their sampler for 2,000 iterations, for
which 1,000 were for burning in. The inside-outside
algorithm dominates the iteration?s runtime, both
for sampling and variational EM. Each iteration
with sampling, however, takes less time, despite the
asymptotic analysis in ?3.2, because of different im-
plementations and the different number of rules that
rewrite to a string. We now give a comparison of
clock time for GUnigram for variational inference
and sampling as described in Johnson and Goldwa-
ter (2009).6 Replicating the experiment in Johnson
and Goldwater (first row in Table 1) took 2 hours
and 11 minutes. With the variational approximation,
we had the following: (i) the preprocessing (?3.3)
step took 114 seconds; (ii) each iteration took ap-
proximately 204 seconds, with convergence after 40
iterations, leading to 8,160 seconds of pure varia-
6We used the code and data available at http://www.
cog.brown.edu/?mj/Software.htm. The machine
used for this comparison is a 64-bit machine with 2.6GHz CPU,
4MB of cache memory and 8GB of RAM.
570
tional EM processing; (iii) parsing took another 952
seconds. The total time is 2 hours and 34 minutes.
At first glance it seems that variational inference
is slower than MCMC sampling. However, note that
the cost of the grammar preprocessing step is amor-
tized over all experiments with the specific gram-
mar, and the E-step with variational inference can be
parallelized, while sampling requires an update of a
global set of parameters after each tree update. We
ran our algorithm on a cluster of 20 1.86GHz CPUs
and achieved a significant speed-up: preprocessing
took 34 seconds, each variational EM iteration took
43 seconds and parsing took 208 seconds. The total
time was 47 minutes, which is 2.8 times faster than
sampling.
4.2 Dependency Grammar Induction
We conclude our experiments with preliminary re-
sults for unsupervised syntax learning. This is a new
application of adaptor grammars, which have so far
been used in segmentation (Johnson and Goldwater,
2009) and named entity recognition (Elsner et al,
2009).
The grammar we use is the dependency model
with valence (DMV Klein and Manning, 2004) rep-
resented as a probabilistic context-free grammar,
GDMV (Smith, 2006). We note that GDMV is re-
cursive; this is not a problem (?3.1).
We used part-of-speech sequences from the Wall
Street Journal Penn Treebank (Marcus et al, 1993),
stripped of words and punctuation. We follow stan-
dard parsing conventions and train on sections 2?
21 and test on section 23 (while using sentences of
length 10 or less). Because of the unsupervised na-
ture of the problem, we report results on the training
set, in addition to the test set.
The nonterminals that we adapted correspond to
nonterminals that define noun constituents. We then
use the preprocessing step defined in ?3.3 with a uni-
form grammar and take the top 3,000 strings for each
nonterminal of a noun constituent.
The results are in Table 4.2. We report attach-
ment accuracy, the fraction of parent-child relation-
ships that the algorithm classified correctly. Notice
that the results are not very different for Viterbi and
MBR decoding, unlike the case with word segmen-
tation. It seems like the DMV grammar, applied
to this task, is more robust to changes in decod-
model Vit. MBR
tr
ai
n
non-Bayesian 48.2 48.3
Dirichlet prior 48.3 48.6
Adaptor grammar 54.0 ?53.7
te
st
non-Bayesian 45.8 46.1
Dirichlet prior 45.9 46.1
Adaptor grammar 48.3 50.2
Table 2: Attachment accuracy for different models for
dependency grammar induction. Bold marks best overall
accuracy per evaluation set, and ? marks figures that are
not significantly worse (binomial sign test, p < 0.05).
ing mechanism. Adaptor grammars improve perfor-
mance over classic EM and variational EM with a
Dirichlet prior significantly.
We note that adaptor grammars are not limited to
a selection of a Dirichlet distribution as a prior for
the grammar rules. Our variational inference algo-
rithm, for example, can be extended to use the lo-
gistic normal prior instead of the Dirichlet, shown
successful by Cohen and Smith (2009).7
5 Conclusion
We described a variational inference algorithm for
adaptor grammars based on a stick-breaking process
representation, which solves a problem with adaptor
grammars and recursive PCFGs. We tested it for a
segmentation task, and showed results which are ei-
ther comparable or an imporvement of state of the
art. We showed that significant speed-ups can be
obtained using parallelization of the algorithm. We
also tested the algorithm on a novel task for adap-
tor grammars, dependency grammar induction. We
showed that an improvement can be obtained using
adaptor grammars over non-Bayesian and paramet-
ric baselines.
Acknowledgments
The authors would like to thank the anonymous review-
ers, Jordan Boyd-Graber, Reza Haffari, Mark Johnson,
and Chong Wang for their useful feedback and com-
ments. This work was supported by the following grants:
ONR 175-6343 and NSF CAREER 0745520 to Blei; NSF
IIS-0836431 and IIS-0915187 to Smith.
7The performance of Cohen and Smith (2009), like the per-
formance of Headden et al (2009), is greater than what we re-
port, but those developments are orthogonal to the contributions
of this paper.
571
References
C. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
N. Bernstein-Ratner. 1987. The phonology of parent
child speech. Children?s Language, 6.
D. Blei and M. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Journal of Bayesian Anal-
ysis, 1(1):121?144.
S. Boyd and L. Vandenberghe. 2004. Convex Optimiza-
tion. Cambridge Press.
M. Brent and T. Cartwright. 1996. Distributional reg-
ularity and phonotactic constraints are useful for seg-
mentation. Cognition, 6:93?125.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proc. of NAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic
normal priors for unsupervised probabilistic grammar
induction. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified data processing on large clusters. In Proc. of
OSDI.
M. Elsner, E. Charniak, and M. Johnson. 2009. Struc-
tured generative models for unsupervised named-
entity clustering. In Proc. of NAACL-HLT.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL-
HLT.
M. Johnson and S. Goldwater. 2009. Improving nonpa-
rameteric Bayesian inference experiments on unsuper-
vised word segmentation with adaptor grammars. In
Proc. of NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying com-
positional nonparameteric Bayesian models. In NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
M. Johnson. 2008a. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proceedings
of the Tenth Meeting of ACL Special Interest Group on
Computational Morphology and Phonology.
M. Johnson. 2008b. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Proc. of ACL.
M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37(2):183?
233.
A. Joshi. 2003. Tree adjoining grammars. In R. Mitkov,
editor, The Oxford Handbook of Computational Lin-
guistics, pages 483?501. Oxford University Press.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Kurihara, M. Welling, and N. A. Vlassis. 2006. Ac-
celerated variational Dirichlet process mixtures. In
NIPS.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proc. of EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
D. Mochihashi, T. Yamada, and N. Ueda. 2009.
Bayesian unsupervised word segmentation with nested
Pitman-Yor language modeling. In Proc. of ACL.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2009. Distributed algorithms for topic models. Jour-
nal of Machine Learning Research, 10:1801?1828.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25(2):855?900.
J. Pitman. 2002. Combinatorial Stochastic Processes.
Lecture Notes for St. Flour Summer School. Springer-
Verlag, New York, NY.
C. P. Robert and G. Casella. 2005. Monte Carlo Statisti-
cal Methods. Springer.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proc. of NIPS.
M. J. Wainwright and M. I. Jordan. 2008. Graphi-
cal models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1:1?305.
C. Wang and D. M. Blei. 2009. Variational inference for
the nested Chinese restaurant process. In NIPS.
572
Proceedings of NAACL-HLT 2013, pages 148?157,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Experiments with Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar3
1Dept. of Computer Science, Columbia University
2Dept. of Statistics/3Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
Abstract
Latent-variable PCFGs (L-PCFGs) are a
highly successful model for natural language
parsing. Recent work (Cohen et al, 2012)
has introduced a spectral algorithm for param-
eter estimation of L-PCFGs, which?unlike
the EM algorithm?is guaranteed to give con-
sistent parameter estimates (it has PAC-style
guarantees of sample complexity). This paper
describes experiments using the spectral algo-
rithm. We show that the algorithm provides
models with the same accuracy as EM, but is
an order of magnitude more efficient. We de-
scribe a number of key steps used to obtain
this level of performance; these should be rel-
evant to other work on the application of spec-
tral learning algorithms. We view our results
as strong empirical evidence for the viability
of spectral methods as an alternative to EM.
1 Introduction
Latent-variable PCFGS (L-PCFGs) are a highly suc-
cessful model for natural language parsing (Mat-
suzaki et al, 2005; Petrov et al, 2006). Recent
work (Cohen et al, 2012) has introduced a spectral
learning algorithm for L-PCFGs. A crucial prop-
erty of the algorithm is that it is guaranteed to pro-
vide consistent parameter estimates?in fact it has
PAC-style guarantees of sample complexity.1 This
is in contrast to the EM algorithm, the usual method
for parameter estimation in L-PCFGs, which has the
weaker guarantee of reaching a local maximum of
the likelihood function. The spectral algorithm is
relatively simple and efficient, relying on a singular
value decomposition of the training examples, fol-
lowed by a single pass over the data where parame-
ter values are calculated.
Cohen et al (2012) describe the algorithm, and
the theory behind it, but as yet no experimental re-
sults have been reported for the method. This paper
1under assumptions on certain singular values in the model;
see section 2.3.1.
describes experiments on natural language parsing
using the spectral algorithm for parameter estima-
tion. The algorithm provides models with slightly
higher accuracy than EM (88.05% F-measure on test
data for the spectral algorithm, vs 87.76% for EM),
but is an order of magnitude more efficient (9h52m
for training, compared to 187h12m, a speed-up of
19 times).
We describe a number of key steps in obtain-
ing this level of performance. A simple backed-off
smoothing method is used to estimate the large num-
ber of parameters in the model. The spectral algo-
rithm requires functions mapping inside and outside
trees to feature vectors?we make use of features
corresponding to single level rules, and larger tree
fragments composed of two or three levels of rules.
We show that it is important to scale features by their
inverse variance, in a manner that is closely related
to methods used in canonical correlation analysis.
Negative values can cause issues in spectral algo-
rithms, but we describe a solution to these problems.
In recent work there has been a series of results in
spectral learning algorithms for latent-variable mod-
els (Vempala and Wang, 2004; Hsu et al, 2009;
Bailly et al, 2010; Siddiqi et al, 2010; Parikh et
al., 2011; Balle et al, 2011; Arora et al, 2012;
Dhillon et al, 2012; Anandkumar et al, 2012). Most
of these results are theoretical (although see Luque
et al (2012) for empirical results of spectral learn-
ing for dependency parsing). While the focus of
our experiments is on parsing, our findings should
be relevant to the application of spectral methods to
other latent-variable models. We view our results as
strong empirical evidence for the viability of spec-
tral methods as an alternative to EM.
2 Background
In this section we first give basic definitions for L-
PCFGs, and then describe the spectral learning algo-
rithm of Cohen et al (2012).
148
2.1 L-PCFGs: Basic Definitions
We follow the definition in Cohen et al (2012)
of L-PCFGs. An L-PCFG is an 8-tuple
(N , I,P,m, n, pi, t, q) where:
? N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We as-
sume thatN = I?P , and I?P = ?. Hence we
have partitioned the set of non-terminals into
two subsets.
? [m] is the set of possible hidden states.2
? [n] is the set of possible words.
? For all a ? I, b, c ? N , h1, h2, h3 ?
[m], we have a context-free rule a(h1) ?
b(h2) c(h3). The rule has an associated pa-
rameter t(a? b c, h2, h3|a, h1).
? For all a ? P , h ? [m], x ? [n], we have a
context-free rule a(h) ? x. The rule has an
associated parameter q(a? x|a, h).
? For all a ? I, h ? [m], pi(a, h) is a parameter
specifying the probability of a(h) being at the
root of a tree.
A skeletal tree (s-tree) is a sequence of rules
r1 . . . rN where each ri is either of the form a? b c
or a? x. The rule sequence forms a top-down, left-
most derivation under a CFG with skeletal rules.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
For a given skeletal tree r1 . . . rN , define ai to be
the non-terminal on the left-hand-side of rule ri. For
any i ? [N ] such that ri is of the form a? b c, de-
fine h(2)i and h
(3)
i as the hidden state value of the left
and right child respectively. The model then defines
a probability mass function (PMF) as
p(r1 . . . rN , h1 . . . hN ) =
pi(a1, h1)
?
i:ai?I
t(ri, h
(2)
i , h
(3)
i |ai, hi)
?
i:ai?P
q(ri|ai, hi)
The PMF over skeletal trees is p(r1 . . . rN ) =?
h1...hN
p(r1 . . . rN , h1 . . . hN ).
2For any integer n, we use [n] to denote the set {1, 2, . . . n}.
The parsing problem is to take a sentence as in-
put, and produce a skeletal tree as output. A stan-
dard method for parsing with L-PCFGs is as follows.
First, for a given input sentence x1 . . . xn, for any
triple (a, i, j) such that a ? N and 1 ? i ? j ? n,
the marginal ?(a, i, j) is defined as
?(a, i, j) =
?
t:(a,i,j)?t
p(t) (1)
where the sum is over all skeletal trees t for
x1 . . . xn that include non-terminal a spanning
words xi . . . xj . A variant of the inside-outside
algorithm can be used to calculate marginals.
Once marginals have been computed, Good-
man?s algorithm (Goodman, 1996) is used to find
arg maxt
?
(a,i,j)?t ?(a, i, j).
3
2.2 The Spectral Learning Algorithm
We now give a sketch of the spectral learning algo-
rithm. The training data for the algorithm is a set
of skeletal trees. The output from the algorithm is a
set of parameter estimates for t, q and pi (more pre-
cisely, the estimates are estimates of linearly trans-
formed parameters; see Cohen et al (2012) and sec-
tion 2.3.1 for more details).
The algorithm takes two inputs in addition to the
set of skeletal trees. The first is an integer m, speci-
fying the number of latent state values in the model.
Typically m is a relatively small number; in our ex-
periments we test values such as m = 8, 16 or 32.
The second is a pair of functions ? and ?, that re-
spectively map inside and outside trees to feature
vectors in Rd and Rd
?
, where d and d? are integers.
Each non-terminal in a skeletal tree has an associ-
ated inside and outside tree. The inside tree for a
node contains the entire subtree below that node; the
outside tree contains everything in the tree excluding
the inside tree. We will refer to the node above the
inside tree that has been removed as the ?foot? of the
outside tree. See figure 1 for an example.
Section 3.1 gives definitions of ?(t) and ?(o)
used in our experiments. The definitions of ?(t) and
3In fact, in our implementation we calculate marginals
?(a? b c, i, k, j) for a, b, c ? N and 1 ? i ? k < j, and
?(a, i, i) for a ? N , 1 ? i ? n, then apply the CKY algorithm
to find the parse tree that maximizes the sum of the marginals.
For simplicity of presentation we will refer to marginals of the
form ?(a, i, j) in the remainder of this paper.
149
VP
V
saw
NP
D
the
N
dog
S
NP
D
the
N
cat
VP
Figure 1: The inside tree (shown left) and out-
side tree (shown right) for the non-terminal VP
in the parse tree [S [NP [D the ] [N cat]]
[VP [V saw] [NP [D the] [N dog]]]]
?(o) are typically high-dimensional, sparse feature
vectors, similar to those in log-linear models. For
example ? might track the rule immediately below
the root of the inside tree, or larger tree fragments;
? might include similar features tracking rules or
larger rule fragments above the relevant node.
The spectral learning algorithm proceeds in two
steps. In step 1, we learn an m-dimensional rep-
resentation of inside and outside trees, using the
functions ? and ? in combination with a projection
step defined through singular value decomposition
(SVD). In step 2, we derive parameter estimates di-
rectly from training examples.
2.2.1 Step 1: An SVD-Based Projection
For a given non-terminal a ? N , each instance of
a in the training data has an associated outside tree,
and an associated inside tree. We define Oa to be
the set of pairs of inside/outside trees seen with a in
the training data: each member of Oa is a pair (o, t)
where o is an outside tree, and t is an inside tree.
Step 1 of the algorithm is then as follows:
1. For each a ? N calculate ??a ? Rd?d
?
as
[??a]i,j =
1
|Oa|
?
(o,t)?Oa
?i(t)?j(o)
2. Perform an SVD on ??a. Define Ua ? Rd?m
(V a ? Rd
??m) to be a matrix containing the
m left (right) singular vectors corresponding
to the m largest singular values; define ?a ?
Rm?m to be the diagonal matrix with the m
largest singular values on its diagonal.
3. For each inside tree in the corpus with root la-
bel a, define
Y (t) = (Ua)>?(t)
For each outside tree with a foot node labeled
a, define
Z(o) = (?a)?1(V a)>?(o)
Note that Y (t) and Z(o) are both m-dimensional
vectors; thus we have used SVD to project inside
and outside trees to m-dimensional vectors.
2.3 Step 2: Parameter Estimation
We now describe how the functions Y (t) and Z(o)
are used in estimating parameters of the model.
First, consider the t(a? b c, h2, h3|a, h1) parame-
ters. Each instance of a given rule a? b c in the
training corpus has an outside tree o associated with
the parent labeled a, and inside trees t2 and t3 as-
sociated with the children labeled b and c. For any
rule a? b cwe defineQa?b c to be the set of triples
(o, t(2), t(3)) occurring with that rule in the corpus.
The parameter estimate is then
c?(a? b c, j, k|a, i) =
count(a? b c)
count(a)
? Ea?b ci,j,k
(2)
where
Ea?b ci,j,k =
?
(o,t(2),t(3))
?Qa?b c
Zi(o)? Yj(t(2))? Yk(t(3))
|Qa?b c|
Here we use count(a? b c) and count(a) to refer
to the count of the rule a? b c and the non-terminal
a in the corpus. Note that once the SVD step has
been used to compute representations Y (t) andZ(o)
for each inside and outside tree in the corpus, calcu-
lating the parameter value c?(a? b c, j, k|a, i) is a
very simple operation.
Similarly, for any rule a ? x, define Qa?x to
be the set of outside trees seen with that rule in the
training corpus. The parameter estimate is then
c?(a? x|a, i) =
count(a? x)
count(a)
? Ea?xi (3)
where Ea?xi =
?
o?Qa?x Zi(o)/|Q
a?x|.
A similar method is used for estimating parame-
ters c?(a, i) that play the role of the pi parameters (de-
tails omitted for brevity; see Cohen et al (2012)).
2.3.1 Guarantees for the Algorithm
Once the c?(a? b c, j, k|a, i), c?(a? x|a, i) and
c?(a, i) parameters have been estimated from the
150
training corpus, they can be used in place of the t,
q and pi parameters in the inside-outside algorithm
for computing marginals (see Eq. 1). Call the re-
sulting marginals ??(a, i, j). The guarantees for the
parameter estimation method are as follows:
? Define ?a = E[?(T )(?(O))>|A = a] where
A,O, T are random variables corresponding to
the non-terminal label at a node, the outside
tree, and the inside tree (see Cohen et al (2012)
for a precise definition). Note that ??a, as de-
fined above, is an estimate of ?a. Then if ?a
has rank m, the marginals ?? will converge to
the true values ? as the number of training ex-
amples goes to infinity, assuming that the train-
ing samples are i.i.d. samples from an L-PCFG.
? Define ? to be the m?th largest singular value
of ?a. Then the number of samples required
for ?? to be -close to ? with probability at least
1? ? is polynomial in 1/, 1/?, and 1/?.
Under the first assumption, (Cohen et al,
2012) show that the c? parameters converge to
values that are linear transforms of the orig-
inal parameters in the L-PCFG. For example,
define c(a? b c, j, k|a, i) to be the value that
c?(a? b c, j, k|a, i) converges to in the limit of infi-
nite data. Then there exist invertible matrices Ga ?
Rm?m for all a ? N such that for any a? b c, for
any h1, h2, h3 ? Rm,
t(a? b c, h2, h3|a, h1) =
?
i,j,k
[Ga]i,h1 [(G
b)?1]j,h2 [(G
c)?1]k,h3c(a? b c, j, k|a, i)
The transforms defined by the Ga matrices are be-
nign, in that they cancel in the inside-outside algo-
rithm when marginals ?(a, i, j) are calculated. Sim-
ilar relationships hold for the pi and q parameters.
3 Implementation of the Algorithm
Cohen et al (2012) introduced the spectral learning
algorithm, but did not perform experiments, leaving
several choices open in how the algorithm is imple-
mented in practice. This section describes a number
of key choices made in our implementation of the
algorithm. In brief, they are as follows:
The choice of functions ? and ?. We will de-
scribe basic features used in ? and ? (single-level
rules, larger tree fragments, etc.). We will also de-
scribe a method for scaling different features in ?
and ? by their variance, which turns out to be im-
portant for empirical results.
Estimation ofEa?b ci,j,k andE
a?x
i . There are a very
large number of parameters in the model, lead-
ing to challenges in estimation. The estimates in
Eqs. 2 and 3 are unsmoothed. We describe a simple
backed-off smoothing method that leads to signifi-
cant improvements in performance of the method.
Handling positive and negative values. As de-
fined, the c? parameters may be positive or negative;
as a result, the ?? values may also be positive or neg-
ative. We find that negative values can be a signif-
icant problem if not handled correctly; but with a
very simple fix to the algorithm, it performs well.
We now turn to these three issues in more detail.
Section 4 will describe experiments measuring the
impact of the different choices.
3.1 The Choice of Functions ? and ?
Cohen et al (2012) show that the choice of feature
definitions ? and ? is crucial in two respects. First,
for all non-terminals a ? N , the matrix ?a must
be of rank m: otherwise the parameter-estimation
algorithm will not be consistent. Second, the num-
ber of samples required for learning is polynomial
in 1/?, where ? = mina?N ?m(?a), and ?m(?a)
is the m?th smallest singular value of ?a. (Note that
the second condition is stronger than the first; ? > 0
implies that ?a is of rank m for all a.) The choice
of ? and ? has a direct impact on the value for ?:
roughly speaking, the value for ? can be thought of
as a measure of how informative the functions ? and
? are about the hidden state values.
With this in mind, our goal is to define a rel-
atively simple set of features, which nevertheless
provide significant information about hidden-state
values, and hence provide high accuracy under the
model. The inside-tree feature function ?(t) makes
use of the following indicator features (throughout
these definitions assume that a? b c is at the root
of the inside tree t):
? The pair of nonterminals (a, b). E.g., for the in-
side tree in figure 1 this would be the pair (VP, V).
151
? The pair (a, c). E.g., (VP, NP).
? The rule a? b c. E.g., VP ? V NP.
? The rule a? b c paired with the rule at the
root of t(i,2). E.g., for the inside tree in fig-
ure 1 this would correspond to the tree fragment
(VP (V saw) NP).
? The rule a? b c paired with the rule at
the root of t(i,3). E.g., the tree fragment
(VP V (NP D N)).
? The head part-of-speech of t(i,1) paired with a.4
E.g., the pair (VP, V).
? The number of words dominated by t(i,1) paired
with a (this is an integer valued feature).
In the case of an inside tree consisting of a single
rule a? x the feature vector simply indicates the
identity of that rule.
To illustrate the function ?, it will be useful to
make use of the following example outside tree:
S
NP
D
the
N
cat
VP
V
saw
NP
D N
dog
Note that in this example the foot node of the out-
side tree is labeled D. The features are as follows:
? The rule above the foot node. We take care
to mark which non-terminal is the foot, using a
* symbol. In the above example this feature is
NP ? D? N.
? The two-level and three-level rule fragments
above the foot node. In the above example these fea-
tures would be
VP
V NP
D? N
S
NP VP
V NP
D? N
? The label of the foot node, together with the
label of its parent. In the above example this is
(D, NP).
? The label of the foot node, together with the la-
bel of its parent and grandparent. In the above ex-
ample this is (D, NP, VP).
? The part of speech of the first head word along
the path from the foot of the outside tree to the root
of the tree which is different from the head node of
4We use the English head rules from the Stanford parser
(Klein and Manning, 2003).
the foot node. In the above example this is N.
? The width of the span to the left of the foot node,
paired with the label of the foot node.
? The width of the span to the right of the foot
node, paired with the label of the foot node.
Scaling of features. The features defined above
are almost all binary valued features. We scale the
features in the following way. For each feature ?i(t),
define count(i) to be the number of times the feature
is equal to 1, and M to be the number of training
examples. The feature is then redefined to be
?i(t)?
?
M
count(i) + ?
where ? is a smoothing term (the method is rela-
tively insensitive to the choice of ?; we set ? = 5 in
our experiments). A similar process is applied to the
? features. The method has the effect of decreasing
the importance of more frequent features in the SVD
step of the algorithm.
The SVD-based step of the algorithm is very
closely related to previous work on CCA (Hotelling,
1936; Hardoon et al, 2004; Kakade and Foster,
2009); and the scaling step is derived from previ-
ous work on CCA (Dhillon et al, 2011). In CCA
the ? and ? vectors are ?whitened? in a preprocess-
ing step, before an SVD is applied. This whiten-
ing process involves calculating covariance matrices
Cx = E[??>] and Cy = E[??>], and replacing ?
by (Cx)?1/2? and ? by (Cy)?1/2?. The exact cal-
culation of (Cx)?1/2 and (Cy)?1/2 is challenging in
high dimensions, however, as these matrices will not
be sparse; the transformation described above can
be considered an approximation where off-diagonal
members of Cx and Cy are set to zero. We will see
that empirically this scaling gives much improved
accuracy.
3.2 Estimation of Ea?b ci,j,k and E
a?x
i
The number of Ea?b ci,j,k parameters is very large,
and the estimation method described in Eqs. 2?3 is
unsmoothed. We have found significant improve-
ments in performance using a relatively simple back-
off smoothing method. The intuition behind this
method is as follows: given two random variablesX
and Y , under the assumption that the random vari-
ables are independent, E[XY ] = E[X] ? E[Y ]. It
152
makes sense to define ?backed off? estimates which
make increasingly strong independence assumptions
of this form.
Smoothing of binary rules For any rule a? b c
and indices i, j ? [m] we can define a second-order
moment as follows:
Ea?b ci,j,? =
?
(o,t(2),t(3))
?Qa?b c
Zi(o)? Yj(t(2))
|Qa?b c|
The definitions ofEa?b ci,?,k andE
a?b c
?,j,k are analogous.
We can define a first-order estimate as follows:
Ea?b c?,?,k =
?
(o,t(2),t(3))
?Qa?b c
Yk(t(3))
|Qa?b c|
Again, we have analogous definitions of Ea?b ci,?,? and
Ea?b c?,j,? . Different levels of smoothed estimate can
be derived from these different terms. The first is
E2,a?b ci,j,k =
Ea?b ci,j,? ? E
a?b c
?,?,k + E
a?b c
i,?,k ? E
a?b c
?,j,? + E
a?b c
?,j,k ? E
a?b c
i,?,?
3
Note that we give an equal weight of 1/3 to each of
the three backed-off estimates seen in the numerator.
A second smoothed estimate is
E3,a?b ci,j,k = E
a?b c
i,?,? ? E
a?b c
?,j,? ? E
a?b c
?,?,k
Using the definition of Oa given in section 2.2.1, we
also define
F ai =
?
(o,t)?Oa Yi(t)
|Oa|
Hai =
?
(o,t)?Oa Zi(o)
|Oa|
and our next smoothed estimate asE4,a?b ci,j,k = H
a
i ?
F bj ? F
c
k .
Our final estimate is
?Ea?b ci,j,k + (1? ?)
(
?E2,a?b ci,j,k + (1? ?)K
a?b c
i,j,k
)
where Ka?b ci,j,k = ?E
3,a?b c
i,j,k + (1? ?)E
4,a?b c
i,j,k .
Here ? ? [0, 1] is a smoothing parameter, set to?
|Qa?b c|/(C +
?
|Qa?b c|) in our experiments,
where C is a parameter that is chosen by optimiza-
tion of accuracy on a held-out set of data.
Smoothing lexical rules We define a similar
method for the Ea?xi parameters. Define
Eai =
?
x?
?
o?Qa?x? Zi(o)
?
x? |Q
a?x? |
hence Eai ignores the identity of x in making its es-
timate. The smoothed estimate is then defined as
?Ea?xi +(1??)E
a
i . Here, ? is a value in [0, 1] which
is tuned on a development set. We only smooth lex-
ical rules which appear in the data less than a fixed
number of times. Unlike binary rules, for which the
estimation depends on a high order moment (third
moment), the lexical rules use first-order moments,
and therefore it is not required to smooth rules with
a relatively high count. The maximal count for this
kind of smoothing is set using a development set.
3.3 Handling Positive and Negative Values
As described before, the parameter estimates may
be positive or negative, and as a result the
marginals computed by the algorithm may in some
cases themselves be negative. In early exper-
iments we found this to be a signficant prob-
lem, with some parses having a very large num-
ber of negatives, and being extremely poor in qual-
ity. Our fix is to define the output of the parser
to be arg maxt
?
(a,i,j)?t |?(a, i, j)| rather than
arg maxt
?
(a,i,j)?t ?(a, i, j) as defined in Good-
man?s algorithm. Thus if a marginal value ?(a, i, j)
is negative, we simply replace it with its absolute
value. This step was derived after inspection of the
parsing charts for bad parses, where we saw evi-
dence that in these cases the entire set of marginal
values had been negated (and hence decoding under
Eq. 1 actually leads to the lowest probability parse
being output under the model). We suspect that this
is because in some cases a dominant parameter has
had its sign flipped due to sampling error; more the-
oretical and empirical work is required in fully un-
derstanding this issue.
4 Experiments
In this section we describe parsing experiments us-
ing the L-PCFG estimation method. We give com-
parisons to the EM algorithm, considering both
speed of training, and accuracy of the resulting
model; we also give experiments investigating the
various choices described in the previous section.
153
We use the Penn WSJ treebank (Marcus et al,
1993) for our experiments. Sections 2?21 were
used as training data, and sections 0 and 22 were
used as development data. Section 23 is used as
the final test set. We binarize the trees in train-
ing data using the same method as that described in
Petrov et al (2006). For example, the non-binary
rule VP ? V NP PP SBAR would be converted
to the structure [VP [@VP [@VP V NP] PP]
SBAR] where @VP is a new symbol in the grammar.
Unary rules are removed by collapsing non-terminal
chains: for example the unary rule S ? VP would
be replaced by a single non-terminal S|VP.
For the EM algorithm we use the initialization
method described in Matsuzaki et al (2005). For ef-
ficiency, we use a coarse-to-fine algorithm for pars-
ing with either the EM or spectral derived gram-
mar: a PCFG without latent states is used to calcu-
late marginals, and dynamic programming items are
removed if their marginal probability is lower than
some threshold (0.00005 in our experiments).
For simplicity the parser takes part-of-speech
tagged sentences as input. We use automatically
tagged data from Turbo Tagger (Martins et al,
2010). The tagger is used to tag both the devel-
opment data and the test data. The tagger was re-
trained on sections 2?21. We use the F1 measure
according to the Parseval metric (Black et al, 1991).
For the spectral algorithm, we tuned the smoothing
parameters using section 0 of the treebank.
4.1 Comparison to EM: Accuracy
We compare models trained using EM and the spec-
tral algorithm using values form in {8, 16, 24, 32}.5
For EM, we found that it was important to use de-
velopment data to choose the number of iterations
of training. We train the models for 100 iterations,
then test accuracy of the model on section 22 (devel-
opment data) at different iteration numbers. Table 1
shows that a peak level of accuracy is reached for all
values of m, other than m = 8, at iteration 20?30,
with sometimes substantial overtraining beyond that
point.
The performance of a regular PCFG model, esti-
mated using maximum likelihood and with no latent
5Lower values of m, such as 2 or 4, lead to substantially
lower performance for both models.
section 22 section 23
EM spectral EM spectral
m = 8 86.87 85.60 ? ?
m = 16 88.32 87.77 ? ?
m = 24 88.35 88.53 ? ?
m = 32 88.56 88.82 87.76 88.05
Table 2: Results on the development data (section 22,
with machine-generated POS tags) and test data (section
23, with machine-generated POS tags).
states, is 68.62%.
Table 2 gives results for the EM-trained models
and spectral-trained models. The spectral models
give very similar accuracy to the EM-trained model
on the test set. Results on the development set with
varying m show that the EM-based models perform
better for m = 8, but that the spectral algorithm
quickly catches up as m increases.
4.2 Comparison to EM: Training Speed
Table 3 gives training times for the EM algorithm
and the spectral algorithm for m ? {8, 16, 24, 32}.
All timing experiments were done on a single Intel
Xeon 2.67GHz CPU. The implementations for the
EM algorithm and the spectral algorithm were writ-
ten in Java. The spectral algorithm also made use
of Matlab for several matrix calculations such as the
SVD calculation.
For EM we show the time to train a single iter-
ation, and also the time to train the optimal model
(time for 30 iterations of training for m = 8, 16, 24,
and time for 20 iterations for m = 32). Note that
this latter time is optimistic, as it assumes an oracle
specifying exactly when it is possible to terminate
EM training with no loss in performance. The spec-
tral method is considerably faster than EM: for ex-
ample, for m = 32 the time for training the spectral
model is just under 10 hours, compared to 187 hours
for EM, a factor of almost 19 times faster.6
The reason for these speed ups is as follows.
Step 1 of the spectral algorithm (feature calculation,
transfer + scaling, and SVD) is not required by EM,
but takes a relatively small amount of time (about
1.2 hours for all values of m). Once step 1 has been
completed, step 2 of the spectral algorithm takes a
6In practice, in order to overcome the speed issue with EM
training, we parallelized the E-step on multiple cores. The spec-
tral algorithm can be similarly parallelized, computing statistics
and parameters for each nonterminal separately.
154
10 20 30 40 50 60 70 80 90 100
m = 8 83.51 86.45 86.68 86.69 86.63 86.67 86.70 86.82 86.87 86.83
m = 16 85.18 87.94 88.32 88.21 88.10 87.86 87.70 87.46 87.34 87.24
m = 24 83.62 88.19 88.35 88.25 87.73 87.41 87.35 87.26 87.02 86.80
m = 32 83.23 88.56 88.52 87.82 87.06 86.47 86.38 85.85 85.75 85.57
Table 1: Results on section 22 for the EM algorithm, varying the number of iterations used. Best results in each row
are in boldface.
single EM spectral algorithm
EM iter. best model total feature transfer + scaling SVD a? b c a? x
m = 8 6m 3h 3h32m
22m 49m
36m 1h34m 10m
m = 16 52m 26h6m 5h19m 34m 3h13m 19m
m = 24 3h7m 93h36m 7h15m 36m 4h54m 28m
m = 32 9h21m 187h12m 9h52m 35m 7h16m 41m
Table 3: Running time for the EM algorithm and the various stages in the spectral algorithm. For EM we show the
time for a single iteration, and the time to train the optimal model (time for 30 iterations of training for m = 8, 16, 24,
time for 20 iterations of training for m = 32). For the spectral method we show the following: ?total? is the total
training time; ?feature? is the time to compute the ? and ? vectors for all data points; ?transfer + scaling? is time
to transfer the data from Java to Matlab, combined with the time for scaling of the features; ?SVD? is the time for
the SVD computation; a? b c is the time to compute the c?(a? b c, h2, h3|a, h1) parameters; a? x is the time to
compute the c?(a? x, h|a, h) parameters. Note that ?feature? and ?transfer + scaling? are the same step for all values
of m, so we quote a single runtime for these steps.
single pass over the data: in contrast, EM requires
a few tens of passes (certainly more than 10 passes,
from the results in table 1). The computations per-
formed by the spectral algorithm in its single pass
are relatively cheap. In contrast to EM, the inside-
outside algorithm is not required; however various
operations such as calculating smoothing terms in
the spectral method add some overhead. The net re-
sult is that form = 32 the time for training the spec-
tral method takes a very similar amount of time to a
single pass of the EM algorithm.
4.3 Smoothing, Features, and Negatives
We now describe experiments demonstrating the im-
pact of various components described in section 3.
The effect of smoothing (section 3.2) Without
smoothing, results on section 22 are 85.05% (m =
8, ?1.82), 86.84% (m = 16, ?1.48), 86.47%
(m = 24, ?1.88), 86.47% (m = 32, ?2.09) (in
each case we show the decrease in performance from
the results in table 2). Smoothing is clearly impor-
tant.
Scaling of features (section 3.1) Without scaling
of features, the accuracy on section 22 with m = 32
is 84.40%, a very significant drop from the 88.82%
accuracy achieved with scaling.
Handling negative values (section 3.3) Replac-
ing marginal values ?(a, i, j) with their absolute
values is also important: without this step, accu-
racy on section 22 decreases to 80.61% (m = 32).
319 sentences out of 1700 examples have different
parses when this step is implemented, implying that
the problem with negative values described in sec-
tion 3.3 occurs on around 18% of all sentences.
The effect of feature functions To test the effect
of features on accuracy, we experimented with a
simpler set of features than those described in sec-
tion 3.1. This simple set just includes an indicator
for the rule below a nonterminal (for inside trees)
and the rule above a nonterminal (for outside trees).
Even this simpler set of features achieves relatively
high accuracy (m = 8: 86.44 , m = 16: 86.86,
m = 24: 87.24 , m = 32: 88.07 ).
This set of features is reminiscent of a PCFG
model where the nonterminals are augmented their
parents (vertical Markovization of order 2) and bina-
rization is done while retaining sibling information
(horizontal Markovization of order 1). See Klein
and Manning (2003) for more information. The per-
155
formance of this Markovized PCFG model lags be-
hind the spectral model: it is 82.59%. This is prob-
ably due to the complexity of the grammar which
causes ovefitting. Condensing the sibling and parent
information using latent states as done in the spectral
model leads to better generalization.
It is important to note that the results for both
EM and the spectral algorithm are comparable to
state of the art, but there are other results previ-
ously reported in the literature which are higher.
For example, Hiroyuki et al (2012) report an ac-
curacy of 92.4 F1 on section 23 of the Penn WSJ
treebank using a Bayesian tree substitution gram-
mar; Charniak and Johnson (2005) report accuracy
of 91.4 using a discriminative reranking model; Car-
reras et al (2008) report 91.1 F1 accuracy for a dis-
criminative, perceptron-trained model; Petrov and
Klein (2007) report an accuracy of 90.1 F1, using
L-PCFGs, but with a split-merge training procedure.
Collins (2003) reports an accuracy of 88.2 F1, which
is comparable to the results in this paper.
5 Conclusion
The spectral learning algorithm gives the same level
of accuracy as EM in our experiments, but has sig-
nificantly faster training times. There are several ar-
eas for future work. There are a large number of pa-
rameters in the model, and we suspect that more so-
phisticated regularization methods than the smooth-
ing method we have described may improve perfor-
mance. Future work should also investigate other
choices for the functions ? and ?. There are natu-
ral ways to extend the approach to semi-supervised
learning; for example the SVD step, where repre-
sentations of outside and inside trees are learned,
could be applied to unlabeled data parsed by a first-
pass parser. Finally, the methods we have described
should be applicable to spectral learning for other
latent variable models.
Acknowledgements
Columbia University gratefully acknowledges the
support of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the view of DARPA,
AFRL, or the US government. Shay Cohen was
supported by the National Science Foundation un-
der Grant #1136996 to the Computing Research As-
sociation for the CIFellows Project. Dean Foster
was supported by National Science Foundation grant
1106743.
References
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and
M. Telgarsky. 2012. Tensor decompositions for learn-
ing latent-variable models. arXiv:1210.7559.
S. Arora, R. Se, and A. Moitra. 2012. Learning topic
models - going beyond SVD. In Proceedings of
FOCS.
R. Bailly, A. Habrar, and F. Denis. 2010. A spectral
approach for probabilistic grammatical inference on
trees. In Proceedings of ALT.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dy-
namic Programming, and the Perceptron for Efficient,
Feature-rich Parsing. In Proceedings of CoNLL, pages
9?16.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
M. Collins. 2003. Head-driven statistical models for nat-
ural language processing. Computational Linguistics,
29:589?637.
P. Dhillon, D. P. Foster, and L. H. Ungar. 2011. Multi-
view learning of word embeddings via CCA. In Pro-
ceedings of NIPS.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.
Ungar. 2012. Spectral dependency parsing with latent
variables. In Proceedings of EMNLP.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
156
D. Hardoon, S. Szedmak, and J. Shawe-Taylor. 2004.
Canonical correlation analysis: An overview with ap-
plication to learning methods. Neural Computation,
16(12):2639?2664.
S. Hiroyuki, M. Yusuke, F. Akinori, and N. Masaaki.
2012. Bayesian symbol-refined tree substitution gram-
mars for syntactic parsing. In Proceedings of ACL,
pages 440?448.
H. Hotelling. 1936. Relations between two sets of vari-
ants. Biometrika, 28:321?377.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
S. M. Kakade and D. P. Foster. 2009. Multi-view regres-
sion via canonical correlation analysis. In COLT.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL, pages 423?430.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. T.
Figueiredo, and M. Q. Aguiar. 2010. TurboParsers:
Dependency parsing by approximate variational infer-
ence. In Proceedings of EMNLP.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of COLING-ACL.
S. Siddiqi, B. Boots, and G. Gordon. 2010. Reduced-
rank hidden markov models. JMLR, 9:741?748.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841?860.
157
Proceedings of NAACL-HLT 2013, pages 487?496,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Approximate PCFG Parsing Using Tensor Decomposition
Shay B. Cohen
Department of Computer Science
Columbia University, USA
scohen@cs.columbia.edu
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Michael Collins
Department of Computer Science
Columbia University, USA
mcollins@cs.columbia.edu
Abstract
We provide an approximation algorithm for
PCFG parsing, which asymptotically im-
proves time complexity with respect to the in-
put grammar size, and prove upper bounds on
the approximation quality. We test our algo-
rithm on two treebanks, and get significant im-
provements in parsing speed.
1 Introduction
The problem of speeding-up parsing algorithms
based on probabilistic context-free grammars
(PCFGs) has received considerable attention in
recent years. Several strategies have been proposed,
including beam-search, best-first and A?. In this
paper we focus on the standard approach of approx-
imating the source PCFG in such a way that parsing
accuracy is traded for efficiency.
Nederhof (2000) gives a thorough presentation
of old and novel ideas for approximating non-
probabilistic CFGs by means of finite automata,
on the basis of specialized preprocessing of self-
embedding structures. In the probabilistic domain,
approximation by means of regular grammars is also
exploited by Eisner and Smith (2005), who filter
long-distance dependencies on-the-fly.
Beyond finite automata approximation, Charniak
et al (2006) propose a coarse-to-fine approach in
which an approximated (not necessarily regular)
PCFG is used to construct a parse forest for the in-
put sentence. Some statistical parameters are then
computed on such a structure, and exploited to filter
parsing with the non-approximated grammar. The
approach can also be iterated at several levels. In
the non-probabilistic setting, a similar filtering ap-
proach was also proposed by Boullier (2003), called
?guided parsing.?
In this paper we rely on an algebraic formulation
of the inside-outside algorithm for PCFGs, based on
a tensor formulation developed for latent-variable
PCFGs in Cohen et al (2012). We combine the
method with known techniques for tensor decompo-
sition to approximate the source PCFG, and develop
a novel algorithm for approximate PCFG parsing.
We obtain improved time upper bounds with respect
to the input grammar size for PCFG parsing, and
provide error upper bounds on the PCFG approxi-
mation, in contrast with existing heuristic methods.
2 Preliminaries
This section introduces the special representation for
probabilistic context-free grammars that we adopt in
this paper, along with the decoding algorithm that
we investigate. For an integer i ? 1, we let [i] =
{1, 2, . . . , i}.
2.1 Probabilistic Context-Free Grammars
We consider context-free grammars (CFGs) in
Chomsky normal form, and denote them as
(N ,L,R) where:
? N is the finite set of nonterminal symbols, with
m = |N |, and L is the finite set of words (lexi-
cal tokens), with L?N = ? and with n = |L|.
? R is a set of rules having the form a? b c,
a, b, c ? N , or the form a? x, a ? N and
x ? L.
A probabilistic CFG (PCFG) is a CFG associated
with a set of parameters defined as follows:
? For each (a? b c) ? R, we have a parameter
p(a? b c | a).
487
? For each (a? x) ? R, we have a parameter
p(a? x | a).
? For each a ? N , we have a parameter pia,
which is the probability of a being the root
symbol of a derivation.
The parameters above satisfy the following nor-
malization conditions:
?
(a?b c)?R
p(a? b c | a) +
?
(a?x)?R
p(a? x | a) = 1,
for each a ? N , and
?
a?N pia = 1.
The probability of a tree ? deriving a sentence in
the language, written p(?), is calculated as the prod-
uct of the probabilities of all rule occurrences in ? ,
times the parameter pia where a is the symbol at the
root of ? .
2.2 Tensor Form of PCFGs
A three-dimensional tensor C ? R(m?m?m) is a
set of m3 parameters Ci,j,k for i, j, k ? [m]. In what
follows, we associate with each tensor three func-
tions, each mapping a pair of vectors in Rm into a
vector in Rm.
Definition 1 Let C ? R(m?m?m) be a tensor.
Given two vectors y1, y2 ? Rm, we let C(y1, y2)
be them-dimensional row vector with components:
[C(y1, y2)]i =
?
j?[m],k?[m]
Ci,j,ky
1
j y
2
k .
We also let C(1,2)(y1, y2) be them-dimensional col-
umn vector with components:
[C(1,2)(y
1, y2)]k =
?
i?[m],j?[m]
Ci,j,ky
1
i y
2
j .
Finally, we let C(1,3)(y1, y2) be the m-dimensional
column vector with components:
[C(1,3)(y
1, y2)]j =
?
i?[m],k?[m]
Ci,j,ky
1
i y
2
k .
For two vectors x, y ? Rm we denote by x y ?
Rm the Hadamard product of x and y, i.e., [xy]i =
xiyi. Finally, for vectors x, y, z ? Rm, xy>z> is the
tensor D ? Rm?m?m where Di,j,k = xiyjzk (this
is analogous to the outer product: [xy>]i,j = xiyj).
We extend the parameter set of our PCFG such
that p(a? b c | a) = 0 for all a? b c not in R,
and p(a? x | a) = 0 for all a? x not in R. We
also represent each a ? N by a unique index in [m],
and we represent each x ? L by a unique index in
[n]: it will always be clear from the context whether
these indices refer to a nonterminal inN or else to a
word in L.
In this paper we assume a tensor representation
for the parameters p(a? b c | a), and we denote by
T ? Rm?m?m a tensor such that:
Ta,b,c , p(a? b c | a).
Similarly, we denote by Q ? Rm?n a matrix such
that:
Qa,x , p(a? x | a).
The root probabilities are denoted using a vector pi ?
Rm?1 such that pia is defined as before.
2.3 Minimum Bayes-Risk Decoding
Let z = x1 ? ? ?xN be some input sentence; we write
T (z) to denote the set of all possible trees for z. It
is often the case that parsing aims to find the high-
est scoring tree ?? for z according to the underlying
PCFG, also called the ?Viterbi parse:?
?? = argmax
??T (z)
p(?)
Goodman (1996) noted that Viterbi parsers do not
optimize the same metric that is usually used for
parsing evaluation (Black et al, 1991). He sug-
gested an alternative algorithm, which he called the
?Labelled Recall Algorithm,? which aims to fix this
issue.
Goodman?s algorithm has two phases. In the first
phase it computes, for each a ? N and for each sub-
string xi ? ? ?xj of z, the marginal ?(a, i, j) defined
as:
?(a, i, j) =
?
??T (z) : (a,i,j)??
p(?).
Here we write (a, i, j) ? ? if nonterminal a spans
words xi ? ? ?xj in the parse tree ? .
488
Inputs: Sentence x1 ? ? ?xN , PCFG (N ,L,R), pa-
rameters T ? R(m?m?m), Q ? R(m?n), pi ?
R(m?1).
Data structures:
? Each ?(a, i, j) ? R for a ? N , i, j ? [N ],
i ? j, is a marginal probability.
? Each ?i,j ? R for i, j ? [N ], i ? j, is the high-
est score for a tree spanning substring xi ? ? ?xj .
Algorithm:
(Marginals) ?a ? N ,?i, j ? [N ], i ? j, compute
the marginals ?(a, i, j) using the inside-outside
algorithm.
(Base case) ?i ? [N ],
?i,i = max
(a?xi)?R
?(a, i, i)
(Maximize Labelled Recall) ?i, j ? [N ], i < j,
?i,j = max
a?N
?(a, i, j) + max
i?k<j
(
?i,k + ?k+1,j
)
Figure 1: The labelled recall algorithm from Goodman
(1996). The algorithm in this figure finds the highest
score for a tree which maximizes labelled recall. The ac-
tual parsing algorithm would use backtrack pointers in
the score computation to return a tree. These are omitted
for simplicity.
The second phase includes a dynamic program-
ming algorithm which finds the tree ?? that maxi-
mizes the sum over marginals in that tree:
?? = argmax
??T (z)
?
(a,i,j)??
?(a, i, j).
Goodman?s algorithm is described in Figure 1.
As Goodman notes, the complexity of the second
phase (?Maximize Labelled Recall,? which is also
referred to as ?minimum Bayes risk decoding?) is
O(N3 +mN2). There are two nested outer loops,
each of order N , and inside these, there are two sep-
arate loops, one of order m and one of order N ,
yielding this computational complexity. The reason
for the linear dependence on the number of nonter-
minals is the lack of dependence on the actual gram-
mar rules, once the marginals are computed.
In its original form, Goodman?s algorithm does
not enforce that the output parse trees are included in
the tree language of the PCFG, that is, certain com-
binations of children and parent nonterminals may
violate the rules in the grammar. In our experiments
we departed from this, and changed Goodman?s al-
gorithm by incorporating the grammar into the dy-
namic programming algorithm in Figure 1. The rea-
son this is important for our experiments is that we
binarize the grammar prior to parsing, and we need
to enforce the links between the split nonterminals
(in the binarized grammar) that refer to the same
syntactic category. See Matsuzaki et al (2005) for
more details about the binarization scheme we used.
This step changes the dynamic programming equa-
tion of Goodman to be linear in the size of the gram-
mar (figure 1). However, empirically, it is the inside-
outside algorithm which takes most of the time to
compute with Goodman?s algorithm. In this paper
we aim to asymptotically reduce the time complex-
ity of the calculation of the inside-outside probabili-
ties using an approximation algorithm.
3 Tensor Formulation of the
Inside-Outside Algorithm
At the core of our approach lies the observation that
there is a (multi)linear algebraic formulation of the
inside-outside algorithm. It can be represented as a
series of tensor, matrix and vector products. A sim-
ilar observation has been made for latent-variable
PCFGs (Cohen et al, 2012) and hidden Markov
models, where only matrix multiplication is required
(Jaeger, 2000). Cohen and Collins (2012) use this
observation together with tensor decomposition to
improve the speed of latent-variable PCFG parsing.
The representation of the inside-outside algorithm
in tensor form is given in Figure 2. For example,
if we consider the recursive equation for the inside
probabilities (where ?i,j is a vector varying over the
nonterminals in the grammar, describing the inside
probability for each nonterminal spanning words i
to j):
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
489
Inputs: Sentence x1 ? ? ?xN , PCFG (N ,L,R), pa-
rameters T ? R(m?m?m), Q ? R(m?n), pi ?
R(m?1).
Data structures:
? Each ?i,j ? R1?m, i, j ? [N ], i ? j, is a row
vector of inside terms ranging over a ? N .
? Each ?i,j ? Rm?1, i, j ? [N ], i ? j, is a
column vector of outside terms ranging over
a ? N .
? Each ?(a, i, j) ? R for a ? N , i, j ? [N ],
i ? j, is a marginal probability.
Algorithm:
(Inside base case) ?i ? [N ], ?(a? xi) ? R,
[?i,i]a = Qa,x
(Inside recursion) ?i, j ? [N ], i < j,
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
(Outside base case) ?a ? N ,
[?1,N ]a = pia
(Outside recursion) ?i, j ? [N ], i ? j,
?i,j =
i?1?
k=1
T(1,2)(?
k,j , ?k,i?1)+
N?
k=j+1
T(1,3)(?
i,k, ?j+1,k)
(Marginals) ?a ? N ,?i, j ? [N ], i ? j,
?(a, i, j) = [?i,j ]a ? [?i,j ]a
Figure 2: The tensor form of the inside-outside algorithm,
for calculation of marginal terms ?(a, i, j).
and then apply the tensor product from Definition 1
to this equation, we get that coordinate a in ?i,j is
defined recursively as follows:
[?i,j ]a =
j?1?
k=i
?
b,c
Ta,b,c ? ?
i,k
b ? ?
k+1,j
c
=
j?1?
k=i
?
b,c
p(a? b c | a)? ?i,kb ? ?
k+1,j
c ,
which is exactly the recursive definition of the inside
algorithm. The correctness of the outside recursive
equations follows very similarly.
The time complexity of the algorithm in this case
is O(m3N3). To see this, observe that each tensor
application takes timeO(m3). Furthermore, the ten-
sor T is applied O(N) times in the computation of
each vector ?i,j and ?i,j . Finally, we need to com-
pute a total ofO(N2) inside and outside vectors, one
for each substring of the input sentence.
4 Tensor Decomposition for the
Inside-Outside Algorithm
In this section, we detail our approach to approxi-
mate parsing using tensor decomposition.
4.1 Tensor Decomposition
In the formulation of the inside-outside algorithm
based on tensor T , each vector ?i,j and ?i,j consists
of m elements, where computation of each element
requires timeO(m2). Therefore, the algorithm has a
O(m3) multiplicative factor in its time complexity,
which we aim to reduce by means of an approximate
algorithm.
Our approximate method relies on a simple ob-
servation. Given an integer r ? 1, assume that
the tensor T has the following special form, called
?Kruskal form:?
T =
r?
i=1
?iuiv>i w
>
i . (1)
In words, T is the sum of r tensors, where each
tensor is obtained as the product of three vectors
ui, vi and wi, together with a scalar ?i. Exact
Kruskal decomposition of a tensor is not necessarily
unique. See Kolda and Bader (2009) for discussion
of uniqueness of tensor decomposition.
490
Consider now two vectors y1, y2 ? Rm, associ-
ated with the inside probabilities for the left (y1) and
right child (y2) of a given node in a parse tree. Let
us introduce auxiliary arrays U, V,W ? Rr?m, with
the i-th row being ui, vi and wi, respectively. Let
also ? = (?1, . . . , ?r). Using the decomposition in
Eq. (1) within Definition 1 we can express the array
T (y1, y2) as:
T (y1, y2) =
[
r?
i=1
?iuiv>i w
>
i
]
(y1, y2) =
r?
i=1
?iui(v>i y
1)(w>i y
2) =
(
U>(? V y1 Wy2)
)
. (2)
The total complexity of the computation in Eq. (2)
is nowO(rm). It is well-known that an exact tensor
decomposition for T can be achieved with r = m2
(Kruskal, 1989). In this case, there is no computa-
tional gain in using Eq. (2) for the inside calculation.
The minimal r required for an exact tensor decom-
position can be smaller than m2. However, identify-
ing that minimal r is NP-hard (H?astad, 1990).
In this section we focused on the computa-
tion of the inside probabilities through vectors
T (?i,k, ?k+1,j). Nonetheless, the steps above can
be easily adapted for the computation of the outside
probabilities through vectors T(1,2)(?k,j , ?k,i?1)
and T(1,3)(?i,k, ?j+1,k).
4.2 Approximate Tensor Decomposition
The PCFG tensor T will not necessarily have the ex-
act decomposed form in Eq. (1). We suggest to ap-
proximate the tensor T by finding the closest tensor
according to some norm over Rm?m?m.
An example of such an approximate decom-
position is the canonical polyadic decomposition
(CPD), also known as CANDECOMP/PARAFAC
decomposition (Carroll and Chang, 1970; Harsh-
man, 1970; Kolda and Bader, 2009). Given an in-
teger r, least squares CPD aims to find the nearest
tensor in Kruskal form, minimizing squared error.
More formally, for a given tensor D ? Rm?m?m,
let ||D||F =
??
i,j,kD
2
i,j,k. Let the set of tensors in
Kruskal form Cr be:
Cr ={C ? Rm?m?m | C =
r?
i=1
?iuiv>i w
>
i
s.t. ?i ? R, ui, vi, wi ? Rm,
||ui||2 = ||vi||2 = ||wi||2 = 1}.
The least squares CPD of C is a tensor C? such
that C? ? argminC??Cr ||C ? C?||F . Here, we treat
the argmin as a set because there could be multiple
solutions which achieve the same accuracy.
There are various algorithms to perform CPD,
such as alternating least squares, direct linear de-
composition, alternating trilinear decomposition and
pseudo alternating least squares (Faber et al, 2003)
and even algorithms designed for sparse tensors (Chi
and Kolda, 2011). Most of these algorithms treat
the problem of identifying the approximate tensor as
an optimization problem. Generally speaking, these
optimization problems are hard to solve, but they
work quite well in practice.
4.3 Parsing with Decomposed Tensors
Equipped with the notion of tensor decomposition,
we can now proceed with approximate tensor pars-
ing in two steps. The first is approximating the ten-
sor using a CPD algorithm, and the second is apply-
ing the algorithms in Figure 1 and Figure 2 to do
parsing, while substituting all tensor product com-
putations with the approximate O(rm) operation of
tensor product.
This is not sufficient to get a significant speed-up
in parsing time. Re-visiting Eq. (2) shows that there
are additional ways to speed-up the tensor applica-
tion T in the context of the inside-outside algorithm.
The first thing to note is that the projections V y1
and Wy2 in Eq. (2) can be cached, and do not have
to be re-calculated every time the tensor is applied.
Here, y1 and y2 will always refer to an outside or
an inside probability vector over the nonterminals in
the grammar. Caching these projections means that
after each computation of an inside or outside proba-
bility, we can immediately project it to the necessary
r-dimensional space, and then re-use this computa-
tion in subsequent application of the tensor.
The second thing to note is that the U projection
in T can be delayed, because of rule of distributiv-
ity. For example, the step in Figure 2 that computes
491
the inside probability ?i,j can be re-formulated as
follows (assuming an exact decomposition of T ):
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
=
j?1?
k=1
U>(? V ?i,k W?k+1,j)
= U>
(j?1?
k=1
(? V ?i,k W?k+1,j)
)
. (3)
This means that projection through U can be done
outside of the loop over splitting points in the sen-
tence. Similar reliance on distributivity can be used
to speed-up the outside calculations as well.
The caching speed-up and the delayed projection
speed-up make the approximate inside-outside com-
putation asymptotically faster. While na??ve applica-
tion of the tensor yields an inside algorithm which
runs in time O(rmN3), the improved algorithm
runs in time O(rN3 + rmN2).
5 Quality of Approximate Tensor Parsing
In this section, we give the main approximation re-
sult, that shows that the probability distribution in-
duced by the approximate tensor is close to the orig-
inal probability distribution, if the distance between
the approximate tensor and the rule probabilities is
not too large.
Denote by T (N) the set of trees in the tree lan-
guage of the PCFG with N words (any nontermi-
nal can be the root of the tree). Let T (N) be the
set of pairs of trees ? = (?1, ?2) such that the to-
tal number of binary rules combined in ?1 and ?2 is
N ? 2 (this means that the total number of words
combined is N ). Let T? be the approximate ten-
sor for T . Denote the probability distribution in-
duced by T? by p?.1 Define the vector ?(?) such that
[?(?)]a = Ta,b,c ? p(?1 | b) ? p(?2 | c) where the root
?1 is nonterminal b and the root of ?2 is c. Similarly,
define [??(?)]a = T?a,b,c ? p?(?1 | b) ? p?(?2 | c).
Define Z(a,N) =
?
??T (N)[??(?)]a. In addition,
define D(a,N) =
?
??T (N) |[??(?)]a ? [?(?)]a|
1Here, p? does not have to be a distribution, because T? could
have negative values, in principle, and its slices do not have to
normalize to 1. However, we just treat p? as a function that maps
trees to products of values according to T? .
and define F (a,N) = D(a,N)/Z(a,N). De-
fine ? = ||T? ? T ||F . Last, define ? =
min(a?b c)?R p(a? b c | a). Then, the following
lemma holds:
Lemma 1 For any a and any N , it holds:
D(a,N) ? Z(a,N)
(
(1 + ?/?)N ? 1
)
Proof sketch: The proof is by induction on N .
Assuming that 1 + F (b, k) ? (1 + ?/?)k and
1 + F (c,N ? k ? 1) ? (1 + ?/?)N?k?1 for F
defined as above (this is the induction hypothesis), it
can be shown that the lemma holds. 
Lemma 2 The following holds for any N :
?
??T (N)
|p?(?)? p(?)| ? m
(
(1 + ?/?)N ? 1
)
Proof sketch: Using Ho?lder?s inequality and
Lemma 1 and the fact that Z(a,N) ? 1, it follows
that:
?
??T (N)
|p?(?)? p(?)| ?
?
??T (N),a
|[?(?)]a ? [??(?)]a|
?
(
?
a
Z(a,N)
)
(
(1 + ?/?)N ? 1
)
? m
(
(1 + ?/?)N ? 1
)

Then, the following is a result that explains how
accuracy changes as a function of the quality of the
tensor approximation:
Theorem 1 For anyN , and  < 1/4, it holds that if
? ?
?
2Nm
, then:
?
??T (N)
|p?(?)? p(?)| ? 
Proof sketch: This is the result of applying Lemma 2
together with the inequality (1 + y/t)t? 1 ? 2y for
any t > 0 and y ? 1/2. 
492
We note that Theorem 1 also implicitly bounds
the difference between a marginal ?(a, i, j) and its
approximate version. A marginal corresponds to a
sum over a subset of summands in Eq. (1).
A question that remains at this point is to decide
whether for a given grammar, the optimal ? that can
be achieved is large or small. We define:
??r = min
T??Cr
||T ? T? ||F (4)
The following theorem gives an upper bound on
the value of ??r based on intrinsic property of the
grammar, or more specifically T . It relies on the
fact that for three-dimensional tensors, where each
dimension is of length m, there exists an exact de-
composition of T using m2 components.
Theorem 2 Let:
T =
m2?
i=1
??iu
?
i (v
?
i )
>(w?i )
>
be an exact Kruskal decomposition of T such that
||u?i ||2 = ||v
?
i ||2 = ||w
?
i || = 1 and ?
?
i ? ?
?
i+1 for
i ? [m2 ? 1]. Then, for a given r, it holds:
??r ?
m2?
i=r+1
|??i |
Proof: Let T? be a tensor that achieves the minimum
in Eq. (4). Define:
T ?r =
r?
i=1
??iu
?
i (v
?
i )
>(w?i )
>
Then, noting that ??r is a minimizer of the norm
difference between T and T? and then applying the
triangle inequality and then Cauchy-Schwartz in-
equality leads to the following chain of inequalities:
??r = ||T ? T? ||F ? ||T ? T
?
r||F
= ||
m2?
i=r+1
??iu
?
i (v
?
i )
>(w?i )
>||F
?
m2?
i=r+1
|??i | ? ||u
?
i (v
?
i )
>(w?i )
>||F =
m2?
i=r+1
|??i |
as required. 
6 Experiments
In this section, we describe experiments that demon-
strate the trade-off between the accuracy of the ten-
sor approximation (and as a consequence, the accu-
racy of the approximate parsing algorithm) and pars-
ing time.
Experimental Setting We compare the tensor ap-
proximation parsing algorithm versus the vanilla
Goodman algorithm. Both algorithms were imple-
mented in Java, and the code for both is almost iden-
tical, except for the set of instructions which com-
putes the dynamic programming equation for prop-
agating the beliefs up in the tree. This makes the
clocktime comparison reliable for drawing conclu-
sions about the speed of the algorithms. Our im-
plementation of the vanilla parsing algorithm is lin-
ear in the size of the grammar (and not cubic in the
number of nonterminals, which would give a worse
running time).
In our experiments, we use the method described
in Chi and Kolda (2011) for tensor decomposition.2
This method is fast, even for large tensors, as long
as they are sparse. Such is the case with the tensors
for our grammars.
We use two treebanks for our comparison: the
Penn treebank (Marcus et al, 1993) and the Arabic
treebank (Maamouri et al, 2004). With the Penn
treebank, we use sections 2?21 for training a max-
imum likelihood model and section 22 for parsing,
while for the Arabic treebank we divide the data into
two sets, of size 80% and 20%, one is used for train-
ing a maximum likelihood model and the other is
used for parsing.
The number of binary rules in the treebank gram-
mar is 7,240. The number of nonterminals is 112
and the number of preterminals is 2593Unary rules
are removed by collapsing non-terminal chains. This
increased the number of preterminals. The number
of binary rules in the Arabic treebank is significantly
smaller and consists of 232 rules. We run all parsing
experiments on sentences of length ? 40. The num-
ber of nonterminals is 48 and the number of preter-
2We use the implementation given in Sandia?s Mat-
lab Tensor Toolbox, which can be downloaded at http:
//www.sandia.gov/?tgkolda/TensorToolbox/
index-2.5.html.
3.
493
rank (r) baseline 20 60 100 140 180 220 260 300 340
Ara
bic speed 0.57 0.04 0.06 0.1 0.12 0.16 0.19 0.22 0.26 0.28
F1 63.78 51.80 58.39 63.63 63.77 63.88 63.82 63.84 63.80 63.88
Eng
lish speed 3.89 0.15 0.21 0.30 0.37 0.44 0.52 0.60 0.70 0.79
F1 71.07 57.83 61.67 68.28 69.63 70.30 70.82 71.42 71.28 71.13
Table 1: Results for the Arabic and English treebank of parsing using a vanilla PCFG with and without tensor decom-
position. Speed is given in seconds per sentence.
minals is 81.
Results Table 1 describes the results of compar-
ing the tensor decomposition algorithm to the vanilla
PCFG parsing algorithm.
The first thing to note is that the running time of
the parsing algorithm is linear in r. This indeed
validates the asymptotic complexity of the inside-
outside component in Goodman?s algorithm with the
approximate tensors. It also shows that most of the
time during parsing is spent on the inside-outside al-
gorithm, and not on the dynamic programming algo-
rithm which follows it.
In addition, compared to the baseline which uses
a vanilla CKY algorithm (linear in the number of
rules), we get a speed up of a factor of 4.75 for
Arabic (r = 140) and 6.5 for English (r = 260)
while retaining similar performance. Perhaps more
surprising is that using the tensor approximation ac-
tually improves performance in several cases. We
hypothesize that the cause of this is that the tensor
decomposition requires less parameters to express
the rule probabilities in the grammar, and therefore
leads to better generalization than a vanilla maxi-
mum likelihood estimate.
We include results for a more complex model for
Arabic, which uses horizontal Markovization of or-
der 1 and vertical Markovization of order 2 (Klein
and Manning, 2003). This grammar includes 2,188
binary rules. Parsing exhaustively using this gram-
mar takes 1.30 seconds per sentence (on average)
with an F1 measure of 64.43. Parsing with tensor
decomposition for r = 280 takes 0.62 seconds per
sentence (on average) with an F1 measure of 64.05.
7 Discussion
In this section, we briefly touch on several other top-
ics related to tensor approximation.
7.1 Approximating the Probability of a String
The probability of a sentence z under a PCFG is de-
fined as p(z) =
?
??T (z) p(?), and can be approx-
imated using the algorithm in Section 4.3, running
in time O(rN3 + rmN2). Of theoretical interest,
we discuss here a time O(rN3 + r2N2) algorithm,
which is more convenient when r < m.
Observe that in Eq. (3) vector ?i,j always appears
within one of the two terms V ?i,j and W?i,j in
Rr?1, whose dimensions are independent of m.
We can therefore use Eq. (3) to compute V ?i,j as
V ?i,j = V U>
(?j?1
k=1(? V ?
i,k W?k+1,j)
)
,
where V U> is a Rr?r matrix that can be
computed off-line, i.e., independently of
z. A symmetrical relation can be used
to compute W?i,j . Finally, we can write
p(z) = pi>U
(?N?1
k=1 (? V ?
1,k W?k+1,N )
)
,
where pi>U is a R1?r vector that can again be
computed off-line. This algorithm then runs in time
O(rN3 + r2N2).
7.2 Applications to Dynamic Programming
The approximation method presented in this paper is
not limited to PCFG parsing. A similar approxima-
tion method has been used for latent-variable PCFGs
(Cohen and Collins, 2012), and in general, ten-
sor approximation can be used to speed-up inside-
outside algorithms for general dynamic program-
ming algorithms or weighted logic programs (Eisner
et al, 2004; Cohen et al, 2011). In the general case,
the dimension of the tensors will not be necessarily
just three (corresponding to binary rules), but can be
of a higher dimension, and therefore the speed gain
can be even greater. In addition, tensor approxima-
tion can be used for computing marginals of latent
variables in graphical models.
For example, the complexity of the forward-
494
backward algorithm for HMMs can be reduced to
be linear in the number of states (as opposed to
quadratic) and linear in the rank used in an approxi-
mate singular-value decomposition (instead of ten-
sor decomposition) of the transition and emission
matrices.
7.3 Tighter (but Slower) Approximation Using
Singular Value Decomposition
The accuracy of the algorithm depends on the ability
of the tensor decomposition algorithm to decompose
the tensor with a small reconstruction error. The de-
composition algorithm is performed on the tensor T
which includes all rules in the grammar.
Instead, one can approach the approximation by
doing a decomposition for each slice of T separately
using singular value decomposition. This will lead
to a more accurate approximation, but will also lead
to an extra factor of m during parsing. This factor
is added because now there is not a single U , V and
W , but instead there are such matrices for each non-
terminal in the grammar.
8 Conclusion
We described an approximation algorithm for prob-
abilistic context-free parsing. The approximation al-
gorithm is based on tensor decomposition performed
on the underlying rule table of the CFG grammar.
The approximation algorithm leads to significant
speed-up in PCFG parsing, with minimal loss in per-
formance.
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
P. Boullier. 2003. Guided earley parsing. In 8th In-
ternational Workshop on Parsing Technologies, pages
43?54.
J. D. Carroll and J. J. Chang. 1970. Analysis of indi-
vidual differences in multidimensional scaling via an
N-way generalization of Eckart-Young decomposition.
Psychometrika, 35:283?319.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil,
D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,
M. Pozar, and T. Vu. 2006. Multilevel coarse-to-fine
pcfg parsing. In Proceedings of HLT-NAACL.
E. C. Chi and T. G. Kolda. 2011. On tensors, spar-
sity, and nonnegative factorizations. arXiv:1112.2414
[math.NA], December.
S. B. Cohen and M. Collins. 2012. Tensor decomposi-
tion for fast latent-variable PCFG parsing. In Proceed-
ings of NIPS.
S. B. Cohen, R. J. Simmons, and N. A. Smith. 2011.
Products of weighted logic programs. Theory and
Practice of Logic Programming, 11(2?3):263?296.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In Proceed-
ings of IWPT, Parsing ?05.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A
declarative language for implementing dynamic pro-
grams. In Proc. of ACL (companion volume).
N. M. Faber, R. Bro, and P. Hopke. 2003. Recent devel-
opments in CANDECOMP/PARAFAC algorithms: a
critical review. Chemometrics and Intelligent Labora-
tory Systems, 65(1):119?137.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
R. A. Harshman. 1970. Foundations of the PARAFAC
procedure: Models and conditions for an ?explana-
tory? multi-modal factor analysis. UCLA working pa-
pers in phoentics, 16:1?84.
J. H?astad. 1990. Tensor rank is NP-complete. Algo-
rithms, 11:644?654.
H. Jaeger. 2000. Observable operator models for dis-
crete stochastic time series. Neural Computation,
12(6):1371?1398.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of ACL.
T. G. Kolda and B. W. Bader. 2009. Tensor decomposi-
tions and applications. SIAM Rev., 51:455?500.
J. B. Kruskal. 1989. Rank, decomposition, and unique-
ness for 3-way and N-way arrays. In R. Coppi and
S. Bolasco, editors, Multiway Data Analysis, pages 7?
18.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proceedings NEM-
LAR.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19(2):313?330.
495
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
M.-J. Nederhof. 2000. Practical experiments with regu-
lar approximation of context-free languages. Compu-
tational Linguistics, 26(1):17?44.
496
Tutorials, NAACL-HLT 2013, pages 13?15,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Spectral Learning Algorithms for
Natural Language Processing
Shay Cohen?, Michael Collins?, Dean P. Foster?, Karl Stratos?, Lyle Ungar?
?Columbia University
?University of Pennsylvania
scohen,mcollins,stratos@cs.columbia.edu
dean@foster.net
ungar@cis.upenn.edu
1 Introduction
Recent work in machine learning and NLP has developed spectral algorithms for
many learning tasks involving latent variables. Spectral algorithms rely on sin-
gular value decomposition as a basic operation, usually followed by some simple
estimation method based on the method of moments. From a theoretical point
of view, these methods are appealing in that they offer consistent estimators (and
PAC-style guarantees of sample complexity) for several important latent-variable
models. This is in contrast to the EM algorithm, which is an extremely success-
ful approach, but which only has guarantees of reaching a local maximum of the
likelihood function.
From a practical point of view, the methods (unlike EM) have no need for
careful initialization, and have recently been shown to be highly efficient (as one
example, in work under submission by the authors on learning of latent-variable
PCFGs, a spectral algorithm performs at identical accuracy to EM, but is around
20 times faster).
2 Outline
In this tutorial we will aim to give a broad overview of spectral methods, describing
theoretical guarantees, as well as practical issues. We will start by covering the
basics of singular value decomposition and describe efficient methods for doing
singular value decomposition. The SVD operation is at the core of most spectral
algorithms that have been developed.
13
We will then continue to cover canonical correlation analysis (CCA). CCA is an
early method from statistics for dimensionality reduction. With CCA, two or more
views of the data are created, and they are all projected into a lower dimensional
space which maximizes the correlation between the views. We will review the
basic algorithms underlying CCA, give some formal results giving guarantees for
latent-variable models and also describe how they have been applied recently to
learning lexical representations from large quantities of unlabeled data. This idea
of learning lexical representations can be extended further, where unlabeled data is
used to learn underlying representations which are subsequently used as additional
information for supervised training.
We will also cover how spectral algorithms can be used for structured predic-
tion problems with sequences and parse trees. A striking recent result by Hsu,
Kakade and Zhang (2009) shows that HMMs can be learned efficiently using a
spectral algorithm. HMMs are widely used in NLP and speech, and previous al-
gorithms (typically based on EM) were guaranteed to only reach a local maximum
of the likelihood function, so this is a crucial result. We will review the basic me-
chanics of the HMM learning algorithm, describe its formal guarantees, and also
cover practical issues.
Last, we will cover work about spectral algorithms in the context of natural
language parsing. We will show how spectral algorithms can be used to estimate
the parameter models of latent-variable PCFGs, a model which serves as the base
for state-of-the-art parsing models such as the one of Petrov et al (2007). We will
show what are the practical steps that are needed to be taken in order to make spec-
tral algorithms for L-PCFGs (or other models in general) practical and comparable
to state of the art.
3 Speaker Bios
Shay Cohen1 is a postdoctoral research scientist in the Department of Computer
Science at Columbia University. He is a computing innovation fellow. His re-
search interests span a range of topics in natural language processing and machine
learning. He is especially interested in developing efficient and scalable parsing
algorithms as well as learning algorithms for probabilistic grammars.
Michael Collins2 is the Vikram S. Pandit Professor of computer science at
Columbia University. His research is focused on topics including statistical pars-
ing, structured prediction problems in machine learning, and applications including
machine translation, dialog systems, and speech recognition. His awards include a
1http://www.cs.columbia.edu/?scohen/
2http://www.cs.columbia.edu/?mcollins/
14
Sloan fellowship, an NSF career award, and best paper awards at EMNLP (2002,
2004, and 2010), UAI (2004 and 2005), and CoNLL 2008.
Dean P. Foster3 is currently the Marie and Joseph Melone Professor of Statis-
tics at the Wharton School of the University of Pennsylvania. His current research
interests are machine learning, stepwise regression and computational linguistics.
He has been searching for new methods of finding useful features in big data sets.
His current set of hammers revolve around fast matrix methods (which decompose
2nd moments) and tensor methods for decomposing 3rd moments.
Karl Stratos4 is a Ph.D. student in the Department of Computer Science at
Columbia. His research is focused on machine learning and natural language pro-
cessing. His current research efforts are focused on spectral learning of latent-
variable models, or more generally, uncovering latent structure from data.
Lyle Ungar5 is a professor at the Computer and Information Science Depart-
ment at the University of Pennsylvania. His research group develops scalable ma-
chine learning and text mining methods, including clustering, feature selection,
and semi-supervised and multi-task learning for natural language, psychology, and
medical research. Example projects include spectral learning of language models,
multi-view learning for gene expression and MRI data, and mining social media to
better understand personality and well-being.
3http://gosset.wharton.upenn.edu/?foster/index.pl
4http://www.cs.columbia.edu/?stratos/
5http://www.cis.upenn.edu/?ungar/
15
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1502?1511,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Viterbi Training for PCFGs:
Hardness Results and Competitiveness of Uniform Initialization
Shay B. Cohen and Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,nasmith}@cs.cmu.edu
Abstract
We consider the search for a maximum
likelihood assignment of hidden deriva-
tions and grammar weights for a proba-
bilistic context-free grammar, the problem
approximately solved by ?Viterbi train-
ing.? We show that solving and even ap-
proximating Viterbi training for PCFGs is
NP-hard. We motivate the use of uniform-
at-random initialization for Viterbi EM as
an optimal initializer in absence of further
information about the correct model pa-
rameters, providing an approximate bound
on the log-likelihood.
1 Introduction
Probabilistic context-free grammars are an essen-
tial ingredient in many natural language process-
ing models (Charniak, 1997; Collins, 2003; John-
son et al, 2006; Cohen and Smith, 2009, inter
alia). Various algorithms for training such models
have been proposed, including unsupervised meth-
ods. Many of these are based on the expectation-
maximization (EM) algorithm.
There are alternatives to EM, and one such al-
ternative is Viterbi EM, also called ?hard? EM or
?sparse? EM (Neal and Hinton, 1998). Instead
of using the parameters (which are maintained in
the algorithm?s current state) to find the true pos-
terior over the derivations, Viterbi EM algorithm
uses a posterior focused on the Viterbi parse of
those parameters. Viterbi EM and variants have
been used in various settings in natural language
processing (Yejin and Cardie, 2007; Wang et al,
2007; Goldwater and Johnson, 2005; DeNero and
Klein, 2008; Spitkovsky et al, 2010).
Viterbi EM can be understood as a coordinate
ascent procedure that locally optimizes a function;
we call this optimization goal ?Viterbi training.?
In this paper, we explore Viterbi training for
probabilistic context-free grammars. We first
show that under the assumption that P 6= NP, solv-
ing and even approximating the Viterbi training
problem is hard. This result holds even for hid-
den Markov models. We extend the main hardness
result to the EM algorithm (giving an alternative
proof to this known result), as well as the problem
of conditional Viterbi training. We then describe
a ?competitiveness? result for uniform initializa-
tion of Viterbi EM: we show that initialization of
the trees in an E-step which uses uniform distri-
butions over the trees is optimal with respect to a
certain approximate bound.
The rest of this paper is organized as follows. ?2
gives background on PCFGs and introduces some
notation. ?3 explains Viterbi training, the declar-
ative form of Viterbi EM. ?4 describes a hardness
result for Viterbi training. ?5 extends this result to
a hardness result of approximation and ?6 further
extends these results for other cases. ?7 describes
the advantages in using uniform-at-random initial-
ization for Viterbi training. We relate these results
to work on the k-means problem in ?8.
2 Background and Notation
We assume familiarity with probabilistic context-
free grammars (PCFGs). A PCFGG consists of:
? A finite set of nonterminal symbols N;
? A finite set of terminal symbols ?;
? For each A ? N, a set of rewrite rules R(A) of
the form A ? ?, where ? ? (N ? ?)?, and
R = ?A?NR(A);
? For each rule A ? ?, a probability ?A??. The
collection of probabilities is denoted ?, and they
are constrained such that:
?(A? ?) ? R(A), ?A?? ? 0
?A ? N,
?
?:(A??)?R(A)
?A?? = 1
That is, ? is grouped into |N| multinomial dis-
tributions.
1502
Under the PCFG, the joint probability of a string
x ? ?? and a grammatical derivation z is1
p(x, z | ?) =
?
(A??)?R
(?A??)
fA??(z) (1)
= exp
?
(A??)?R
fA??(z) log ?A??
where fA??(z) is a function that ?counts? the
number of times the rule A ? ? appears in
the derivation z. fA(z) will similarly denote the
number of times that nonterminal A appears in z.
Given a sample of derivations z = ?z1, . . . , zn?,
let:
FA??(z) =
n?
i=1
fA??(zi) (2)
FA(z) =
n?
i=1
fA(zi) (3)
We use the following notation forG:
? L(G) is the set of all strings (sentences) x that
can be generated using the grammar G (the
?language ofG?).
? D(G) is the set of all possible derivations z that
can be generated using the grammarG.
? D(G, x) is the set of all possible derivations z
that can be generated using the grammarG and
have the yield x.
3 Viterbi Training
Viterbi EM, or ?hard? EM, is an unsupervised
learning algorithm, used in NLP in various set-
tings (Yejin and Cardie, 2007; Wang et al, 2007;
Goldwater and Johnson, 2005; DeNero and Klein,
2008; Spitkovsky et al, 2010). In the context of
PCFGs, it aims to select parameters ? and phrase-
structure trees z jointly. It does so by iteratively
updating a state consisting of (?, z). The state
is initialized with some value, then the algorithm
alternates between (i) a ?hard? E-step, where the
strings x1, . . . , xn are parsed according to a cur-
rent, fixed ?, giving new values for z, and (ii) an
M-step, where the ? are selected to maximize like-
lihood, with z fixed.
With PCFGs, the E-step requires running an al-
gorithm such as (probabilistic) CKY or Earley?s
1Note that x = yield(z); if the derivation is known, the
string is also known. On the other hand, there may be many
derivations with the same yield, perhaps even infinitely many.
algorithm, while the M-step normalizes frequency
counts FA??(z) to obtain the maximum likeli-
hood estimate?s closed-form solution.
We can understand Viterbi EM as a coordinate
ascent procedure that approximates the solution to
the following declarative problem:
Problem 1. ViterbiTrain
Input: G context-free grammar, x1, . . . , xn train-
ing instances from L(G)
Output: ? and z1, . . . , zn such that
(?, z1, . . . , zn) = argmax
?,z
n?
i=1
p(xi, zi | ?) (4)
The optimization problem in Eq. 4 is non-
convex and, as we will show in ?4, hard to op-
timize. Therefore it is necessary to resort to ap-
proximate algorithms like Viterbi EM.
Neal and Hinton (1998) use the term ?sparse
EM? to refer to a version of the EM algorithm
where the E-step finds the modes of hidden vari-
ables (rather than marginals as in standard EM).
Viterbi EM is a variant of this, where the E-
step finds the mode for each xi?s derivation,
argmaxz?D(G,xi) p(xi, z | ?).
We will refer to
L(?, z) =
n?
i=1
p(xi, zi | ?) (5)
as ?the objective function of ViterbiTrain.?
Viterbi training and Viterbi EM are closely re-
lated to self-training, an important concept in
semi-supervised NLP (Charniak, 1997; McClosky
et al, 2006a; McClosky et al, 2006b). With self-
training, the model is learned with some seed an-
notated data, and then iterates by labeling new,
unannotated data and adding it to the original an-
notated training set. McClosky et al consider self-
training to be ?one round of Viterbi EM? with su-
pervised initialization using labeled seed data. We
refer the reader to Abney (2007) for more details.
4 Hardness of Viterbi Training
We now describe hardness results for Problem 1.
We first note that the following problem is known
to be NP-hard, and in fact, NP-complete (Sipser,
2006):
Problem 2. 3-SAT
Input: A formula ? =
?m
i=1 (ai ? bi ? ci) in con-
junctive normal form, such that each clause has 3
1503
S?2
ccccc
ccccc
ccccc
ccccc
ccccc
ccccc
c
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
S?1
A1
eee
eee
eee
eee
eee
eee
e
YYY
YYY
YYY
YYY
YYY
YYY
Y
A2
eee
eee
eee
eee
eee
eee
e
YYY
YYY
YYY
YYY
YYY
YYY
Y
UY1,0
qq
qq
qq
q
MM
MM
MM
M
UY2,1
qq
qq
qq
q
MM
MM
MM
M
UY4,0
qq
qq
qq
q
MM
MM
MM
M
UY1,0
qq
qq
qq
q
MM
MM
MM
M
UY2,1
qq
qq
qq
q
MM
MM
MM
M
UY3,1
qq
qq
qq
q
MM
MM
MM
M
VY?1 VY1 VY2 VY?2 VY?4 VY4 VY?1 VY1 VY2 VY?2 VY3 VY?3
1 0 1 0 1 0 1 0 1 0 1 0
Figure 1: An example of a Viterbi parse tree which represents a satisfying assignment for ? = (Y1?Y2? Y?4)? (Y?1? Y?2?Y3).
In ??, all rules appearing in the parse tree have probability 1. The extracted assignment would be Y1 = 0, Y2 = 1, Y3 =
1, Y4 = 0. Note that there is no usage of two different rules for a single nonterminal.
literals.
Output: 1 if there is a satisfying assignment for ?
and 0 otherwise.
We now describe a reduction of 3-SAT to Prob-
lem 1. Given an instance of the 3-SAT problem,
the reduction will, in polynomial time, create a
grammar and a single string such that solving the
ViterbiTrain problem for this grammar and string
will yield a solution for the instance of the 3-SAT
problem.
Let ? =
?m
i=1 (ai ? bi ? ci) be an instance of
the 3-SAT problem, where ai, bi and ci are liter-
als over the set of variables {Y1, . . . , YN} (a literal
refers to a variable Yj or its negation, Y?j). Let Cj
be the jth clause in ?, such that Cj = aj ? bj ? cj .
We define the following context-free grammarG?
and string to parse s?:
1. The terminals of G? are the binary digits ? =
{0, 1}.
2. We create N nonterminals VYr , r ?
{1, . . . , N} and rules VYr ? 0 and VYr ? 1.
3. We create N nonterminals VY?r , r ?
{1, . . . , N} and rules VY?r ? 0 and VY?r ? 1.
4. We create UYr,1 ? VYrVY?r and UYr,0 ?
VY?rVYr .
5. We create the rule S?1 ? A1. For each j ?
{2, . . . ,m}, we create a rule S?j ? S?j?1Aj
where S?j is a new nonterminal indexed by
?j ,
?j
i=1Ci and Aj is also a new nonterminal
indexed by j ? {1, . . . ,m}.
6. Let Cj = aj ? bj ? cj be clause j in ?. Let
Y (aj) be the variable that aj mentions. Let
(y1, y2, y3) be a satisfying assignment for Cj
where yk ? {0, 1} and is the value of Y (aj),
Y (bj) and Y (cj) respectively for k ? {1, 2, 3}.
For each such clause-satisfying assignment, we
add the rule:
Aj ? UY (aj),y1UY (bj),y2UY (cj),y3 (6)
For each Aj , we would have at most 7 rules of
that form, since one rule will be logically incon-
sistent with aj ? bj ? cj .
7. The grammar?s start symbol is S?n .
8. The string to parse is s? = (10)3m, i.e. 3m
consecutive occurrences of the string 10.
A parse of the string s? using G? will be used
to get an assignment by setting Yr = 0 if the rule
VYr ? 0 or VY?r ? 1 are used in the derivation of
the parse tree, and 1 otherwise. Notice that at this
point we do not exclude ?contradictions? coming
from the parse tree, such as VY3 ? 0 used in the
tree together with VY3 ? 1 or VY?3 ? 0. The fol-
lowing lemma gives a condition under which the
assignment is consistent (so contradictions do not
occur in the parse tree):
Lemma 1. Let ? be an instance of the 3-SAT
problem, and letG? be a probabilistic CFG based
on the above grammar with weights ??. If the
(multiplicative) weight of the Viterbi parse of s?
is 1, then the assignment extracted from the parse
tree is consistent.
Proof. Since the probability of the Viterbi parse
is 1, all rules of the form {VYr , VY?r} ? {0, 1}
which appear in the parse tree have probability 1
as well. There are two possible types of inconsis-
tencies. We show that neither exists in the Viterbi
parse:
1504
1. For any r, an appearance of both rules of the
form VYr ? 0 and VYr ? 1 cannot occur be-
cause all rules that appear in the Viterbi parse
tree have probability 1.
2. For any r, an appearance of rules of the form
VYr ? 1 and VY?r ? 1 cannot occur, because
whenever we have an appearance of the rule
VYr ? 0, we have an adjacent appearance of
the rule VY?r ? 1 (because we parse substrings
of the form 10), and then again we use the fact
that all rules in the parse tree have probability 1.
The case of VYr ? 0 and VY?r ? 0 is handled
analogously.
Thus, both possible inconsistencies are ruled out,
resulting in a consistent assignment.
Figure 1 gives an example of an application of
the reduction.
Lemma 2. Define ?, G? as before. There exists
?? such that the Viterbi parse of s? is 1 if and only
if ? is satisfiable. Moreover, the satisfying assign-
ment is the one extracted from the parse tree with
weight 1 of s? under ??.
Proof. (=?) Assume that there is a satisfying as-
signment. Each clause Cj = aj ? bj ? cj is satis-
fied using a tuple (y1, y2, y3) which assigns value
for Y (aj), Y (bj) and Y (cj). This assignment cor-
responds the following rule
Aj ? UY (aj),y1UY (bj),y2UY (cj),y3 (7)
Set its probability to 1, and set al other rules of
Aj to 0. In addition, for each r, if Yr = y, set the
probabilities of the rules VYr ? y and VY?r ? 1?y
to 1 and VY?r ? y and VYr ? 1? y to 0. The rest
of the weights for S?j ? S?j?1Aj are set to 1.
This assignment of rule probabilities results in a
Viterbi parse of weight 1.
(?=) Assume that the Viterbi parse has prob-
ability 1. From Lemma 1, we know that we can
extract a consistent assignment from the Viterbi
parse. In addition, for each clause Cj we have a
rule
Aj ? UY (aj),y1UY (bj),y2UY (cj),y3 (8)
that is assigned probability 1, for some
(y1, y2, y3). One can verify that (y1, y2, y3)
are the values of the assignment for the corre-
sponding variables in clause Cj , and that they
satisfy this clause. This means that each clause is
satisfied by the assignment we extracted.
In order to show an NP-hardness result, we need
to ?convert? ViterbiTrain to a decision problem.
The natural way to do it, following Lemmas 1
and 2, is to state the decision problem for Viter-
biTrain as ?given G and x1, . . . , xn and ? ? 0,
is the optimized value of the objective function
L(?, z) ? ??? and use ? = 1 together with Lem-
mas 1 and 2. (Naturally, an algorithm for solving
ViterbiTrain can easily be used to solve its deci-
sion problem.)
Theorem 3. The decision version of the Viterbi-
Train problem is NP-hard.
5 Hardness of Approximation
A natural path of exploration following the hard-
ness result we showed is determining whether an
approximation of ViterbiTrain is also hard. Per-
haps there is an efficient approximation algorithm
for ViterbiTrain we could use instead of coordi-
nate ascent algorithms such as Viterbi EM. Recall
that such algorithms? main guarantee is identify-
ing a local maximum; we know nothing about how
far it will be from the global maximum.
We next show that approximating the objective
function of ViterbiTrain with a constant factor of ?
is hard for any ? ? (12 , 1] (i.e., 1/2 +  approxima-
tion is hard for any  ? 1/2). This means that, un-
der the P 6= NP assumption, there is no efficient al-
gorithm that, given a grammar G and a sample of
sentences x1, . . . , xn, returns ?? and z? such that:
L(??, z?) ? ? ?max
?,z
n?
i=1
p(xi, zi | ?) (9)
We will continue to use the same reduction from
?4. Let s? be the string from that reduction, and
let (?, z) be the optimal solution for ViterbiTrain
given G? and s?. We first note that if p(s?, z |
?) < 1 (implying that there is no satisfying as-
signment), then there must be a nonterminal which
appears along with two different rules in z.
This means that we have a nonterminal B ? N
with some rule B ? ? that appears k times,
while the nonterminal appears in the parse r ?
k + 1 times. Given the tree z, the ? that maxi-
mizes the objective function is the maximum like-
lihood estimate (MLE) for z (counting and nor-
malizing the rules).2 We therefore know that
the ViterbiTrain objective function, L(?, z), is at
2Note that we can only make p(z | ?, x) greater by using
? to be the MLE for the derivation z.
1505
most
(
k
r
)k
, because it includes a factor equal
to
(
fB??(z)
fB(z)
)fB??(z)
, where fB(z) is the num-
ber of times nonterminal B appears in z (hence
fB(z) = r) and fB??(z) is the number of times
B ? ? appears in z (hence fB??(z) = k). For
any k ? 1, r ? k + 1:
(
k
r
)k
?
(
k
k + 1
)k
?
1
2
(10)
This means that if the value of the objective func-
tion of ViterbiTrain is not 1 using the reduction
from ?4, then it is at most 12 . If we had an efficient
approximate algorithm with approximation coeffi-
cient ? > 12 (Eq. 9 holds), then in order to solve
3-SAT for formula ?, we could run the algorithm
on G? and s? and check whether the assignment
to (?, z) that the algorithm returns satisfies ? or
not, and return our response accordingly.
If ? were satisfiable, then the true maximal
value of L would be 1, and the approximation al-
gorithm would return (?, z) such that L(?, z) ?
? > 12 . z would have to correspond to a satisfy-
ing assignment, and in fact p(z | ?) = 1, because
in any other case, the probability of a derivation
which does not represent a satisfying assignment
is smaller than 12 . If ? were not satisfiable, then
the approximation algorithm would never return a
(?, z) that results in a satisfying assignment (be-
cause such a (?, z) does not exist).
The conclusion is that an efficient algorithm for
approximating the objective function of Viterbi-
Train (Eq. 4) within a factor of 12 +  is unlikely
to exist. If there were such an algorithm, we could
use it to solve 3-SAT using the reduction from ?4.
6 Extensions of the Hardness Result
An alternative problem to Problem 1, a variant of
Viterbi-training, is the following (see, for exam-
ple, Klein and Manning, 2001):
Problem 3. ConditionalViterbiTrain
Input: G context-free grammar, x1, . . . , xn train-
ing instances from L(G)
Output: ? and z1, . . . , zn such that
(?, z1, . . . , zn) = argmax
?,z
n?
i=1
p(zi | ?, xi) (11)
Here, instead of maximizing the likelihood, we
maximize the conditional likelihood. Note that
there is a hidden assumption in this problem def-
inition, that xi can be parsed using the grammar
G. Otherwise, the quantity p(zi | ?, xi) is not
well-defined. We can extend ConditionalViterbi-
Train to return ? in the case of not having a parse
for one of the xi?this can be efficiently checked
using a run of a cubic-time parser on each of the
strings xi with the grammarG.
An approximate technique for this problem is
similar to Viterbi EM, only modifying the M-
step to maximize the conditional, rather than joint,
likelihood. This new M-step will not have a closed
form and may require auxiliary optimization tech-
niques like gradient ascent.
Our hardness result for ViterbiTrain applies to
ConditionalViterbiTrain as well. The reason is
that if p(z, s? | ??) = 1 for a ? with a satisfying
assignment, thenL(G) = {s?} andD(G) = {z}.
This implies that p(z | ??, s?) = 1. If ? is unsat-
isfiable, then for the optimal ? of ViterbiTrain we
have z and z? such that 0 < p(z, s? | ??) < 1
and 0 < p(z?, s? | ??) < 1, and therefore p(z |
??, s?) < 1, which means the conditional objec-
tive function will not obtain the value 1. (Note
that there always exist some parameters ?? that
generate s?.) So, again, given an algorithm for
ConditionalViterbiTrain, we can discern between
a satisfiable formula and an unsatisfiable formula,
using the reduction from ?4 with the given algo-
rithm, and identify whether the value of the objec-
tive function is 1 or strictly less than 1. We get the
result that:
Theorem 4. The decision problem of Condition-
alViterbiTrain problem is NP-hard.
where the decision problem of ConditionalViter-
biTrain is defined analogously to the decision
problem of ViterbiTrain.
We can similarly show that finding the global
maximum of the marginalized likelihood:
max
?
1
n
n?
i=1
log
?
z
p(xi, z | ?) (12)
is NP-hard. The reasoning follows. Using the
reduction from before, if ? is satisfiable, then
Eq. 12 gets value 0. If ? is unsatisfiable, then we
would still get value 0 only if L(G) = {s?}. If
G? generates a single derivation for (10)3m, then
we actually do have a satisfying assignment from
1506
Lemma 1. Otherwise (more than a single deriva-
tion), the optimal ? would have to give fractional
probabilities to rules of the form VYr ? {0, 1} (or
VY?r ? {0, 1}). In that case, it is no longer true
that (10)3m is the only generated sentence, which
is a contradiction.
The quantity in Eq. 12 can be maximized ap-
proximately using algorithms like EM, so this
gives a hardness result for optimizing the objec-
tive function of EM for PCFGs. Day (1983) pre-
viously showed that maximizing the marginalized
likelihood for hidden Markov models is NP-hard.
We note that the grammar we use for all of our
results is not recursive. Therefore, we can encode
this grammar as a hidden Markov model, strength-
ening our result from PCFGs to HMMs.3
7 Uniform-at-Random Initialization
In the previous sections, we showed that solving
Viterbi training is hard, and therefore requires an
approximation algorithm. Viterbi EM, which is an
example of such algorithm, is dependent on an ini-
tialization of either ? to start with an E-step or z
to start with an M-step. In the absence of a better-
informed initializer, it is reasonable to initialize
z using a uniform distribution over D(G, xi) for
each i. If D(G, xi) is finite, it can be done effi-
ciently by setting ? = 1 (ignoring the normaliza-
tion constraint), running the inside algorithm, and
sampling from the (unnormalized) posterior given
by the chart (Johnson et al, 2007). We turn next
to an analysis of this initialization technique that
suggests it is well-motivated.
The sketch of our result is as follows: we
first give an asymptotic upper bound for the log-
likelihood of derivations and sentences. This
bound, which has an information-theoretic inter-
pretation, depends on a parameter ?, which de-
pends on the distribution from which the deriva-
tions were chosen. We then show that this bound
is minimized when we pick ? such that this distri-
bution is (conditioned on the sentence) a uniform
distribution over derivations.
Let q(x) be any distribution over L(G) and ?
some parameters for G. Let f(z) be some feature
function (such as the one that counts the number
of appearances of a certain rule in a derivation),
and then:
Eq,?[f ] ,
?
x?L(G)
q(x)
?
z?D(G,x)
p(z | ?, x)f(z)
3We thank an anonymous reviewer for pointing this out.
which gives the expected value of the feature func-
tion f(z) under the distribution q(x)?p(z | ?, x).
We will make the following assumption aboutG:
Condition 1. There exists some ?I such that
?x ? L(G),?z ? D(G, x), p(z | ?I , x) =
1/|D(G, x)|.
This condition is satisfied, for example, whenG
is in Chomsky normal form and for all A,A? ? N,
|R(A)| = |R(A?)|. Then, if we set ?A?? =
1/|R(A)|, we get that all derivations of x will
have the same number of rules and hence the same
probability. This condition does not hold for gram-
mars with unary cycles because |D(G, x)|may be
infinite for some derivations. Such grammars are
not commonly used in NLP.
Let us assume that some ?correct? parameters
?? exist, and that our data were drawn from a dis-
tribution parametrized by ??. The goal of this sec-
tion is to motivate the following initialization for
?, which we call UniformInit:
1. Initialize z by sampling from the uniform dis-
tribution over D(G, xi) for each xi.
2. Update the grammar parameters using maxi-
mum likelihood estimation.
7.1 Bounding the Objective
To show our result, we require first the following
definition due to Freund et al (1997):
Definition 5. A distribution p1 is within ? ? 1 of
a distribution p2 if for every event A, we have
1
?
?
p1(A)
p2(A)
? ? (13)
For any feature function f(z) and any two
sets of parameters ?2 and ?1 for G and for any
marginal q(x), if p(z | ?1, x) is within ? of
p(z | ?2, x) for all x then:
Eq,?1 [f ]
?
? Eq,?2 [f ] ? ?Eq,?1 [f ] (14)
Let ?0 be a set of parameters such that we perform
the following procedure in initializing Viterbi EM:
first, we sample from the posterior distribution
p(z | ?0, x), and then update the parameters with
maximum likelihood estimate, in a regular M-step.
Let ? be such that p(z | ?0, x) is within ? of
p(z | ??, x) (for all x ? L(G)). (Later we will
show that UniformInit is a wise choice for making
? small. Note that UniformInit is equivalent to the
procedure mentioned above with ?0 = ?I .)
1507
Consider p?n(x), the empirical distribution over
x1, . . . , xn. As n ? ?, we have that p?n(x) ?
p?(x), almost surely, where p? is:
p?(x) =
?
z
p?(x, z | ??) (15)
This means that as n ? ? we have Ep?n,?[f ] ?
Ep?,?[f ]. Now, let z0 = (z0,1, . . . , z0,n) be sam-
ples from p(z | ?0, xi) for i ? {1, . . . , n}. Then,
from simple MLE computation, we know that the
value
max
??
n?
i=1
p(xi, z0,i | ??) (16)
=
?
(A??)?R
(
FA??(z0)
FA(z0)
)FA??(z0)
We also know that for ?0, from the consistency of
MLE, for large enough samples:
FA??(z0)
FA(z0)
?
Ep?n,?0 [fA??]
Ep?n,?0 [fA]
(17)
which means that we have the following as n
grows (starting from the ViterbiTrain objective
with initial state z = z0):
max
??
n?
i=1
p(xi, z0,i | ??) (18)
(Eq. 16)
=
?
(A??)?R
(
FA??(z0)
FA(z0)
)FA??(z0)
(19)
(Eq. 17)
?
?
(A??)?R
(
Ep?n,?0 [fA??]
Ep?n,?0 [fA]
)FA??(z0)
(20)
We next use the fact that p?n(x) ? p?(x) for large
n, and apply Eq. 14, noting again our assumption
that p(z | ?0, x) is within ? of p(z | ??, x). We
also let B =
?
i
|zi|, where |zi| is the number of
nodes in the derivation zi. Note that FA(zi) ?
B. The above quantity (Eq. 20) is approximately
bounded above by
?
(A??)?R
1
?2B
(
Ep?,?? [fA??]
Ep?,?? [fA]
)FA??(z0)
(21)
=
1
?2|R|B
?
(A??)?R
(??A??)
FA??(z0) (22)
Eq. 22 follows from:
??A?? =
Ep?,?? [fA??]
Ep?,?? [fA]
(23)
If we continue to develop Eq. 22 and apply
Eq. 17 and Eq. 23 again, we get that:
1
?2|R|B
?
(A??)?R
(??A??)
FA??(z0)
=
1
?2|R|B
?
(A??)?R
(??A??)
FA??(z0)?
FA(z0)
FA(z0)
?
1
?2|R|B
?
(A??)?R
(??A??)
Ep?,?0
[fA??]
Ep?,?0
[fA]
?FA(z0)
?
1
?2|R|B
?
(A??)?R
(??A??)
?2??A??FA(z0)
?
1
?2|R|B
?
?
?
(A??)?R
(??A??)
n??A??
?
?
? ?? ?
T (??,n)
B?2/n
(24)
=
(
1
?2|R|B
)
T (??, n)B?
2/n (25)
, d(?;??, |R|, B) (26)
where Eq. 24 is the result of FA(z0) ? B.
For two series {an} and {bn}, let ?an ' bn?
denote that limn?? an ? limn?? bn. In other
words, an is asymptotically larger than bn. Then,
if we changed the representation of the objec-
tive function of the ViterbiTrain problem to log-
likelihood, for ?? that maximizes Eq. 18 (with
some simple algebra) we have:
1
n
n?
i=1
log2 p(xi, z0,i | ?
?) (27)
' ?
2|R|B
n
log2 ?+
B?2
n
(
1
n
log2 T (?
?, n)
)
= ?
2|R|B
n
log2 ?? |N|
B?2
|N|n
?
A?N
H(??, A)
(28)
where
H(??, A) = ?
?
(A??)?R(A)
??A?? log2 ?
?
A??
(29)
is the entropy of the multinomial for nonter-
minal A. H(??, A) can be thought of as the
minimal number of bits required to encode a
choice of a rule from A, if chosen independently
from the other rules. All together, the quantity
B
|N|n
(?
A?NH(?
?, A)
)
is the average number of
bits required to encode a tree in our sample using
1508
??, while removing dependence among all rules
and assuming that each node at the tree is chosen
uniformly.4 This means that the log-likelihood, for
large n, is bounded from above by a linear func-
tion of the (average) number of bits required to
optimally encode n trees of total size B, while as-
suming independence among the rules in a tree.
We note that the quantityB/nwill tend toward the
average size of a tree, which, under Condition 1,
must be finite.
Our final approximate bound from Eq. 28 re-
lates the choice of distribution, from which sample
z0, to ?. The lower bound in Eq. 28 is a monotone-
decreasing function of ?. We seek to make ? as
small as possible to make the bound tight. We next
show that the uniform distribution optimizes ? in
that sense.
7.2 Optimizing ?
Note that the optimal choice of ?, for a single x
and for candidate initializer ??, is
?opt(x,??;?0) = sup
z?D(G,x)
p(z | ?0, x)
p(z | ??, x)
(30)
In order to avoid degenerate cases, we will add an-
other condition on the true model, ??:
Condition 2. There exists ? > 0 such that, for
any x ? L(G) and for any z ? D(G, x), p(z |
??, x) ? ? .
This is a strong condition, forcing the cardinal-
ity of D(G) to be finite, but it is not unreason-
able if natural language sentences are effectively
bounded in length.
Without further information about ?? (other
than that it satisfies Condition 2), we may want
to consider the worst-case scenario of possible ?,
hence we seek initializer ?0 such that
?(x;?0) , sup
?
?opt(x,?;?0) (31)
is minimized. If ?0 = ?I , then we have that
p(z | ?I , x) = |D(G, x)|?1 , ?x. Together with
Condition 2, this implies that
p(z | ?I , x)
p(z | ??, x)
?
?x
?
(32)
4We note that Grenander (1967) describes a (lin-
ear) relationship between the derivational entropy and
H(??, A). The derivational entropy is defined as h(??, A) =
?
P
x,z p(x, z | ?
?) log p(x, z | ??), where z ranges over
trees that have nonterminal A as the root. It follows im-
mediately from Grenander?s result that
P
AH(?
?, A) ?P
A h(?
?, A).
and hence ?opt(x,??) ? ?x/? for any ??, hence
?(x;?I) ? ?x/? . However, if we choose ?0 6=
?I , we have that p(z? | ?0, x) > ?x for some z?,
hence, for ?? such that it assigns probability ? on
z?, we have that
sup
z?D(G,x)
p(z | ?0, x)
p(z | ??, x)
>
?x
?
(33)
and hence ?opt(x,??;??) > ?x/? , so ?(x;??) >
?x/? . So, to optimize for the worst-case scenario
over true distributions with respect to ?, we are
motivated to choose ?0 = ?I as defined in Con-
dition 1. Indeed, UniformInit uses ?I to initialize
the state of Viterbi EM.
We note that if ?I was known for a specific
grammar, then we could have used it as a direct
initializer. However, Condition 1 only guarantees
its existence, and does not give a practical way to
identify it. In general, as mentioned above, ? = 1
can be used to obtain a weighted CFG that sat-
isfies p(z | ?, x) = 1/|D(G, x)|. Since we re-
quire a uniform posterior distribution, the num-
ber of derivations of a fixed length is finite. This
means that we can converted the weighted CFG
with ? = 1 to a PCFG with the same posterior
(Smith and Johnson, 2007), and identify the ap-
propriate ?I .
8 Related Work
Viterbi training is closely related to the k-means
clustering problem, where the objective is to find
k centroids for a given set of d-dimensional points
such that the sum of distances between the points
and the closest centroid is minimized. The ana-
log for Viterbi EM for the k-means problem is the
k-means clustering algorithm (Lloyd, 1982), a co-
ordinate ascent algorithm for solving the k-means
problem. It works by iterating between an E-like-
step, in which each point is assigned the closest
centroid, and an M-like-step, in which the cen-
troids are set to be the center of each cluster.
?k? in k-means corresponds, in a sense, to the
size of our grammar. k-means has been shown to
be NP-hard both when k varies and d is fixed and
when d varies and k is fixed (Aloise et al, 2009;
Mahajan et al, 2009). An open problem relating to
our hardness result would be whether ViterbiTrain
(or ConditionalViterbiTrain) is hard even if we do
not permit grammars of arbitrarily large size, or
at least, constrain the number of rules that do not
rewrite to terminals (in our current reduction, the
1509
size of the grammar grows as the size of the 3-SAT
formula grows).
On a related note to ?7, Arthur and Vassilvit-
skii (2007) described a greedy initialization al-
gorithm for initializing the centroids of k-means,
called k-means++. They show that their ini-
tialization is O(log k)-competitive; i.e., it ap-
proximates the optimal clusters assignment by a
factor of O(log k). In ?7.1, we showed that
uniform-at-random initialization is approximately
O(|N|L?2/n)-competitive (modulo an additive
constant) for CNF grammars, where n is the num-
ber of sentences, L is the total length of sentences
and ? is a measure for distance between the true
distribution and the uniform distribution.5
Many combinatorial problems in NLP involv-
ing phrase-structure trees, alignments, and depen-
dency graphs are hard (Sima?an, 1996; Good-
man, 1998; Knight, 1999; Casacuberta and de la
Higuera, 2000; Lyngs? and Pederson, 2002;
Udupa and Maji, 2006; McDonald and Satta,
2007; DeNero and Klein, 2008, inter alia). Of
special relevance to this paper is Abe and Warmuth
(1992), who showed that the problem of finding
maximum likelihood model of probabilistic au-
tomata is hard even for a single string and an au-
tomaton with two states. Understanding the com-
plexity of NLP problems, we believe, is crucial as
we seek effective practical approximations when
necessary.
9 Conclusion
We described some properties of Viterbi train-
ing for probabilistic context-free grammars. We
showed that Viterbi training is NP-hard and, in
fact, NP-hard to approximate. We gave motivation
for uniform-at-random initialization for deriva-
tions in the Viterbi EM algorithm.
Acknowledgments
We acknowledge helpful comments by the anony-
mous reviewers. This research was supported by
NSF grant 0915187.
References
N. Abe and M. Warmuth. 1992. On the computational
complexity of approximating distributions by prob-
5Making the assumption that the grammar is in CNF per-
mits us to use L instead of B, since there is a linear relation-
ship between them in that case.
abilistic automata. Machine Learning, 9(2?3):205?
260.
S. Abney. 2007. Semisupervised Learning for Compu-
tational Linguistics. CRC Press.
D. Aloise, A. Deshpande, P. Hansen, and P. Popat.
2009. NP-hardness of Euclidean sum-of-squares
clustering. Machine Learning, 75(2):245?248.
D. Arthur and S. Vassilvitskii. 2007. k-means++: The
advantages of careful seeding. In Proc. of ACM-
SIAM symposium on Discrete Algorithms.
F. Casacuberta and C. de la Higuera. 2000. Com-
putational complexity of problems on probabilistic
grammars and transducers. In Proc. of ICGI.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. of AAAI.
S. B. Cohen and N. A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proc. of HLT-
NAACL.
M. Collins. 2003. Head-driven statistical models for
natural language processing. Computational Lin-
guistics, 29(4):589?637.
W. H. E. Day. 1983. Computationally difficult parsi-
mony problems in phylogenetic systematics. Jour-
nal of Theoretical Biology, 103.
J. DeNero and D. Klein. 2008. The complexity of
phrase alignment problems. In Proc. of ACL.
Y. Freund, H. Seung, E. Shamir, and N. Tishby. 1997.
Selective sampling using the query by committee al-
gorithm. Machine Learning, 28(2?3):133?168.
S. Goldwater and M. Johnson. 2005. Bias in learning
syllable structure. In Proc. of CoNLL.
J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
U. Grenander. 1967. Syntax-controlled probabilities.
Technical report, Brown University, Division of Ap-
plied Mathematics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying
compositional nonparameteric Bayesian models. In
Advances in NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
D. Klein and C. Manning. 2001. Natural lan-
guage grammar induction using a constituent-
context model. In Advances in NIPS.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615.
S. P. Lloyd. 1982. Least squares quantization in PCM.
In IEEE Transactions on Information Theory.
R. B. Lyngs? and C. N. S. Pederson. 2002. The con-
sensus string problem and the complexity of com-
paring hidden Markov models. Journal of Comput-
ing and System Science, 65(3):545?569.
M. Mahajan, P. Nimbhorkar, and K. Varadarajan. 2009.
The planar k-means problem is NP-hard. In Proc. of
International Workshop on Algorithms and Compu-
tation.
1510
D. McClosky, E. Charniak, and M. Johnson. 2006a.
Effective self-training for parsing. In Proc. of HLT-
NAACL.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
Proc. of COLING-ACL.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of IWPT.
R. M. Neal and G. E. Hinton. 1998. A view of the
EM algorithm that justifies incremental, sparse, and
other variants. In Learning and Graphical Models,
pages 355?368. Kluwer Academic Publishers.
K. Sima?an. 1996. Computational complexity of prob-
abilistic disambiguation by means of tree-grammars.
In In Proc. of COLING.
M. Sipser. 2006. Introduction to the Theory of Com-
putation, Second Edition. Thomson Course Tech-
nology.
N. A. Smith and M. Johnson. 2007. Weighted and
probabilistic context-free grammars are equally ex-
pressive. Computational Linguistics, 33(4):477?
491.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010. Viterbi training improves unsuper-
vised dependency parsing. In Proc. of CoNLL.
R. Udupa and K. Maji. 2006. Computational com-
plexity of statistical machine translation. In Proc. of
EACL.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous gram-
mar for question answering. In Proc. of EMNLP.
C. Yejin and C. Cardie. 2007. Structured local training
and biased potential functions for conditional ran-
dom fields with application to coreference resolu-
tion. In Proc. of HLT-NAACL.
1511
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223?231,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar3
1Dept. of Computer Science, Columbia University
2Dept. of Statistics/3Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
Abstract
We introduce a spectral learning algorithm for
latent-variable PCFGs (Petrov et al, 2006).
Under a separability (singular value) condi-
tion, we prove that the method provides con-
sistent parameter estimates.
1 Introduction
Statistical models with hidden or latent variables are
of great importance in natural language processing,
speech, and many other fields. The EM algorithm is
a remarkably successful method for parameter esti-
mation within these models: it is simple, it is often
relatively efficient, and it has well understood formal
properties. It does, however, have a major limitation:
it has no guarantee of finding the global optimum of
the likelihood function. From a theoretical perspec-
tive, this means that the EM algorithm is not guar-
anteed to give consistent parameter estimates. From
a practical perspective, problems with local optima
can be difficult to deal with.
Recent work has introduced polynomial-time
learning algorithms (and consistent estimation meth-
ods) for two important cases of hidden-variable
models: Gaussian mixture models (Dasgupta, 1999;
Vempala and Wang, 2004) and hidden Markov mod-
els (Hsu et al, 2009). These algorithms use spec-
tral methods: that is, algorithms based on eigen-
vector decompositions of linear systems, in particu-
lar singular value decomposition (SVD). In the gen-
eral case, learning of HMMs or GMMs is intractable
(e.g., see Terwijn, 2002). Spectral methods finesse
the problem of intractibility by assuming separabil-
ity conditions. For example, the algorithm of Hsu
et al (2009) has a sample complexity that is polyno-
mial in 1/?, where ? is the minimum singular value
of an underlying decomposition. These methods are
not susceptible to problems with local maxima, and
give consistent parameter estimates.
In this paper we derive a spectral algorithm
for learning of latent-variable PCFGs (L-PCFGs)
(Petrov et al, 2006; Matsuzaki et al, 2005). Our
method involves a significant extension of the tech-
niques from Hsu et al (2009). L-PCFGs have been
shown to be a very effective model for natural lan-
guage parsing. Under a separation (singular value)
condition, our algorithm provides consistent param-
eter estimates; this is in contrast with previous work,
which has used the EM algorithm for parameter es-
timation, with the usual problems of local optima.
The parameter estimation algorithm (see figure 4)
is simple and efficient. The first step is to take
an SVD of the training examples, followed by a
projection of the training examples down to a low-
dimensional space. In a second step, empirical av-
erages are calculated on the training example, fol-
lowed by standard matrix operations. On test ex-
amples, simple (tensor-based) variants of the inside-
outside algorithm (figures 2 and 3) can be used to
calculate probabilities and marginals of interest.
Our method depends on the following results:
? Tensor form of the inside-outside algorithm.
Section 5 shows that the inside-outside algorithm for
L-PCFGs can be written using tensors. Theorem 1
gives conditions under which the tensor form calcu-
lates inside and outside terms correctly.
? Observable representations. Section 6 shows
that under a singular-value condition, there is an ob-
servable form for the tensors required by the inside-
outside algorithm. By an observable form, we fol-
low the terminology of Hsu et al (2009) in referring
to quantities that can be estimated directly from data
where values for latent variables are unobserved.
Theorem 2 shows that tensors derived from the ob-
servable form satisfy the conditions of theorem 1.
? Estimating the model. Section 7 gives an al-
gorithm for estimating parameters of the observable
representation from training data. Theorem 3 gives a
sample complexity result, showing that the estimates
converge to the true distribution at a rate of 1/
?
M
where M is the number of training examples.
The algorithm is strikingly different from the EM
algorithm for L-PCFGs, both in its basic form, and
in its consistency guarantees. The techniques de-
223
veloped in this paper are quite general, and should
be relevant to the development of spectral methods
for estimation in other models in NLP, for exam-
ple alignment models for translation, synchronous
PCFGs, and so on. The tensor form of the inside-
outside algorithm gives a new view of basic calcula-
tions in PCFGs, and may itself lead to new models.
2 Related Work
For work on L-PCFGs using the EM algorithm, see
Petrov et al (2006), Matsuzaki et al (2005), Pereira
and Schabes (1992). Our work builds on meth-
ods for learning of HMMs (Hsu et al, 2009; Fos-
ter et al, 2012; Jaeger, 2000), but involves sev-
eral extensions: in particular in the tensor form of
the inside-outside algorithm, and observable repre-
sentations for the tensor form. Balle et al (2011)
consider spectral learning of finite-state transducers;
Lugue et al (2012) considers spectral learning of
head automata for dependency parsing. Parikh et al
(2011) consider spectral learning algorithms of tree-
structured directed bayes nets.
3 Notation
Given a matrix A or a vector v, we write A? or v?
for the associated transpose. For any integer n ? 1,
we use [n] to denote the set {1, 2, . . . n}. For any
row or column vector y ? Rm, we use diag(y) to
refer to the (m?m) matrix with diagonal elements
equal to yh for h = 1 . . . m, and off-diagonal ele-
ments equal to 0. For any statement ?, we use [[?]]
to refer to the indicator function that is 1 if ? is true,
and 0 if ? is false. For a random variable X, we use
E[X] to denote its expected value.
We will make (quite limited) use of tensors:
Definition 1 A tensor C ? R(m?m?m) is a set of
m3 parameters Ci,j,k for i, j, k ? [m]. Given a ten-
sor C , and a vector y ? Rm, we define C(y) to be
the (m ? m) matrix with components [C(y)]i,j =
?
k?[m]Ci,j,kyk. Hence C can be interpreted as a
function C : Rm ? R(m?m) that maps a vector
y ? Rm to a matrix C(y) of dimension (m?m).
In addition, we define the tensor C? ? R(m?m?m)
for any tensor C ? R(m?m?m) to have values
[C?]i,j,k = Ck,j,i
Finally, for vectors x, y, z ? Rm, xy?z? is the
tensor D ? Rm?m?m where Dj,k,l = xjykzl (this
is analogous to the outer product: [xy?]j,k = xjyk).
4 L-PCFGs: Basic Definitions
This section gives a definition of the L-PCFG for-
malism used in this paper. An L-PCFG is a 5-tuple
(N ,I,P,m, n) where:
? N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We assume
that N = I ? P, and I ? P = ?. Hence we have
partitioned the set of non-terminals into two subsets.
? [m] is the set of possible hidden states.
? [n] is the set of possible words.
? For all a ? I , b ? N , c ? N , h1, h2, h3 ? [m],
we have a context-free rule a(h1) ? b(h2) c(h3).
? For all a ? P, h ? [m], x ? [n], we have a
context-free rule a(h) ? x.
Hence each in-terminal a ? I is always the left-
hand-side of a binary rule a ? b c; and each pre-
terminal a ? P is always the left-hand-side of a
rule a ? x. Assuming that the non-terminals in
the grammar can be partitioned this way is relatively
benign, and makes the estimation problem cleaner.
We define the set of possible ?skeletal rules? as
R = {a ? b c : a ? I, b ? N , c ? N}. The
parameters of the model are as follows:
? For each a? b c ? R, and h ? [m], we have
a parameter q(a ? b c|h, a). For each a ? P,
x ? [n], and h ? [m], we have a parameter
q(a ? x|h, a). For each a ? b c ? R, and
h, h? ? [m], we have parameters s(h?|h, a ? b c)
and t(h?|h, a? b c).
These definitions give a PCFG, with rule proba-
bilities
p(a(h1) ? b(h2) c(h3)|a(h1)) =
q(a? b c|h1, a)? s(h2|h1, a? b c)? t(h3|h1, a? b c)
and p(a(h) ? x|a(h)) = q(a? x|h, a).
In addition, for each a ? I , for each h ? [m], we
have a parameter ?(a, h) which is the probability of
non-terminal a paired with hidden variable h being
at the root of the tree.
An L-PCFG defines a distribution over parse trees
as follows. A skeletal tree (s-tree) is a sequence of
rules r1 . . . rN where each ri is either of the form
a ? b c or a ? x. The rule sequence forms
a top-down, left-most derivation under a CFG with
skeletal rules. See figure 1 for an example.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for
224
S1
NP2
D3
the
N4
dog
VP5
V6
saw
P7
him
r1 = S ? NP VP
r2 = NP ? D N
r3 = D ? the
r4 = N ? dog
r5 = VP ? V P
r6 = V ? saw
r7 = P ? him
Figure 1: An s-tree, and its sequence of rules. (For con-
venience we have numbered the nodes in the tree.)
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
Define ai to be the non-terminal on the left-hand-
side of rule ri. For any i ? {2 . . . N} define pa(i)
to be the index of the rule above node i in the tree.
Define L ? [N ] to be the set of nodes in the tree
which are the left-child of some parent, and R ?
[N ] to be the set of nodes which are the right-child of
some parent. The probability mass function (PMF)
over full trees is then
p(r1 . . . rN , h1 . . . hN ) = ?(a1, h1)
?
N
?
i=1
q(ri|hi, ai)?
?
i?L
s(hi|hpa(i), rpa(i))
?
?
i?R
t(hi|hpa(i), rpa(i)) (1)
The PMF over s-trees is p(r1 . . . rN ) =
?
h1...hN p(r1 . . . rN , h1 . . . hN ).
In the remainder of this paper, we make use of ma-
trix form of parameters of an L-PCFG, as follows:
? For each a? b c ? R, we define Qa?b c ?
Rm?m to be the matrix with values q(a ? b c|h, a)
for h = 1, 2, . . . m on its diagonal, and 0 values for
its off-diagonal elements. Similarly, for each a ? P,
x ? [n], we define Qa?x ? Rm?m to be the matrix
with values q(a ? x|h, a) for h = 1, 2, . . . m on its
diagonal, and 0 values for its off-diagonal elements.
? For each a ? b c ? R, we define Sa?b c ?
Rm?m where [Sa?b c]h?,h = s(h?|h, a? b c).
? For each a ? b c ? R, we define T a?b c ?
Rm?m where [T a?b c]h?,h = t(h?|h, a? b c).
? For each a ? I , we define the vector ?a ? Rm
where [?a]h = ?(a, h).
5 Tensor Form of the Inside-Outside
Algorithm
Given an L-PCFG, two calculations are central:
Inputs: s-tree r1 . . . rN , L-PCFG (N , I,P ,m, n), parameters
? Ca?b c ? R(m?m?m) for all a? b c ? R
? c?a?x ? R(1?m) for all a ? P , x ? [n]
? c1a ? R(m?1) for all a ? I.
Algorithm: (calculate the f i terms bottom-up in the tree)
? For all i ? [N ] such that ai ? P , f i = c?ri
? For all i ? [N ] such that ai ? I, f i = f?Cri(f?) where
? is the index of the left child of node i in the tree, and ?
is the index of the right child.
Return: f1c1a1 = p(r1 . . . rN)
Figure 2: The tensor form for calculation of p(r1 . . . rN ).
1. For a given s-tree r1 . . . rN , calculate
p(r1 . . . rN ).
2. For a given input sentence x = x1 . . . xN , cal-
culate the marginal probabilities
?(a, i, j) =
?
??T (x):(a,i,j)??
p(?)
for each non-terminal a ? N , for each (i, j)
such that 1 ? i ? j ? N .
Here T (x) denotes the set of all possible s-trees for
the sentence x, and we write (a, i, j) ? ? if non-
terminal a spans words xi . . . xj in the parse tree ? .
The marginal probabilities have a number of uses.
Perhaps most importantly, for a given sentence x =
x1 . . . xN , the parsing algorithm of Goodman (1996)
can be used to find
arg max
??T (x)
?
(a,i,j)??
?(a, i, j)
This is the parsing algorithm used by Petrov et al
(2006), for example. In addition, we can calcu-
late the probability for an input sentence, p(x) =
?
??T (x) p(?), as p(x) =
?
a?I ?(a, 1, N).
Variants of the inside-outside algorithm can be
used for problems 1 and 2. This section introduces a
novel form of these algorithms, using tensors. This
is the first step in deriving the spectral estimation
method.
The algorithms are shown in figures 2 and 3. Each
algorithm takes the following inputs:
1. A tensor Ca?b c ? R(m?m?m) for each rule
a? b c.
2. A vector c?a?x ? R(1?m) for each rule a? x.
225
3. A vector c1a ? R(m?1) for each a ? I .
The following theorem gives conditions under
which the algorithms are correct:
Theorem 1 Assume that we have an L-PCFG with
parameters Qa?x, Qa?b c, T a?b c, Sa?b c, ?a, and
that there exist matrices Ga ? R(m?m) for all a ?
N such that each Ga is invertible, and such that:
1. For all rules a? b c, Ca?b c(y) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1
2. For all rules a? x, c?a?x = 1?Qa?x(Ga)?1
3. For all a ? I , c1a = Ga?a
Then: 1) The algorithm in figure 2 correctly com-
putes p(r1 . . . rN ) under the L-PCFG. 2) The algo-
rithm in figure 3 correctly computes the marginals
?(a, i, j) under the L-PCFG.
Proof: See section 9.1.
6 Estimating the Tensor Model
A crucial result is that it is possible to directly esti-
mate parameters Ca?b c, c?a?x and c1a that satisfy the
conditions in theorem 1, from a training sample con-
sisting of s-trees (i.e., trees where hidden variables
are unobserved). We first describe random variables
underlying the approach, then describe observable
representations based on these random variables.
6.1 Random Variables Underlying the Approach
Each s-tree with N rules r1 . . . rN has N nodes. We
will use the s-tree in figure 1 as a running example.
Each node has an associated rule: for example,
node 2 in the tree in figure 1 has the rule NP? D N.
If the rule at a node is of the form a? b c, then there
are left and right inside trees below the left child and
right child of the rule. For example, for node 2 we
have a left inside tree rooted at node 3, and a right
inside tree rooted at node 4 (in this case the left and
right inside trees both contain only a single rule pro-
duction, of the form a ? x; however in the general
case they might be arbitrary subtrees).
In addition, each node has an outside tree. For
node 2, the outside tree is
S
NP VP
V
saw
P
him
Inputs: Sentence x1 . . . xN , L-PCFG (N , I,P ,m, n), param-
eters Ca?b c ? R(m?m?m) for all a? b c ? R, c?a?x ?
R(1?m) for all a ? P , x ? [n], c1a ? R(m?1) for all a ? I.
Data structures:
? Each ?a,i,j ? R1?m for a ? N , 1 ? i ? j ? N is a
row vector of inside terms.
? Each ?a,i,j ? Rm?1 for a ? N , 1 ? i ? j ? N is a
column vector of outside terms.
? Each ?(a, i, j) ? R for a ? N , 1 ? i ? j ? N is a
marginal probability.
Algorithm:
(Inside base case) ?a ? P , i ? [N ], ?a,i,i = c?a?xi
(Inside recursion) ?a ? I, 1 ? i < j ? N,
?a,i,j =
j?1
?
k=i
?
a?b c
?c,k+1,jCa?b c(?b,i,k)
(Outside base case) ?a ? I, ?a,1,n = c1a
(Outside recursion) ?a ? N , 1 ? i ? j ? N,
?a,i,j =
i?1
?
k=1
?
b?c a
Cb?c a(?c,k,i?1)?b,k,j
+
N
?
k=j+1
?
b?a c
Cb?a c? (?c,j+1,k)?b,i,k
(Marginals) ?a ? N , 1 ? i ? j ? N,
?(a, i, j) = ?a,i,j?a,i,j =
?
h?[m]
?a,i,jh ?
a,i,j
h
Figure 3: The tensor form of the inside-outside algorithm,
for calculation of marginal terms ?(a, i, j).
The outside tree contains everything in the s-tree
r1 . . . rN , excluding the subtree below node i.
Our random variables are defined as follows.
First, we select a random internal node, from a ran-
dom tree, as follows:
? Sample an s-tree r1 . . . rN from the PMF
p(r1 . . . rN ). Choose a node i uniformly at ran-
dom from [N ].
If the rule ri for the node i is of the form a? b c,
we define random variables as follows:
? R1 is equal to the rule ri (e.g., NP ? D N).
? T1 is the inside tree rooted at node i. T2 is the
inside tree rooted at the left child of node i, and T3
is the inside tree rooted at the right child of node i.
? H1,H2,H3 are the hidden variables associated
with node i, the left child of node i, and the right
child of node i respectively.
226
? A1, A2, A3 are the labels for node i, the left
child of node i, and the right child of node i respec-
tively. (E.g., A1 = NP, A2 = D, A3 = N.)
? O is the outside tree at node i.
? B is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
If the rule ri for the selected node i is of
the form a ? x, we have random vari-
ables R1, T1,H1, A1, O,B as defined above, but
H2,H3, T2, T3, A2, and A3 are not defined.
We assume a function ? that maps outside trees o
to feature vectors ?(o) ? Rd? . For example, the fea-
ture vector might track the rule directly above the
node in question, the word following the node in
question, and so on. We also assume a function ?
that maps inside trees t to feature vectors ?(t) ? Rd.
As one example, the function ? might be an indica-
tor function tracking the rule production at the root
of the inside tree. Later we give formal criteria for
what makes good definitions of ?(o) of ?(t). One
requirement is that d? ? m and d ? m.
In tandem with these definitions, we assume pro-
jection matices Ua ? R(d?m) and V a ? R(d??m)
for all a ? N . We then define additional random
variables Y1, Y2, Y3, Z as
Y1 = (Ua1)??(T1) Z = (V a1)??(O)
Y2 = (Ua2)??(T2) Y3 = (Ua3)??(T3)
where ai is the value of the random variable Ai.
Note that Y1, Y2, Y3, Z are all in Rm.
6.2 Observable Representations
Given the definitions in the previous section, our
representation is based on the following matrix, ten-
sor and vector quantities, defined for all a ? N , for
all rules of the form a? b c, and for all rules of the
form a? x respectively:
?a = E[Y1Z?|A1 = a]
Da?b c = E
[
[[R1 = a? b c]]Y3Z?Y ?2 |A1 = a
]
d?a?x = E
[
[[R1 = a? x]]Z?|A1 = a
]
Assuming access to functions ? and ?, and projec-
tion matrices Ua and V a, these quantities can be es-
timated directly from training data consisting of a
set of s-trees (see section 7).
Our observable representation then consists of:
Ca?b c(y) = Da?b c(y)(?a)?1 (2)
c?a?x = d?a?x(?a)?1 (3)
c1a = E [[[A1 = a]]Y1|B = 1] (4)
We next introduce conditions under which these
quantities satisfy the conditions in theorem 1.
The following definition will be important:
Definition 2 For all a ? N , we define the matrices
Ia ? R(d?m) and Ja ? R(d??m) as
[Ia]i,h = E[?i(T1) | H1 = h,A1 = a]
[Ja]i,h = E[?i(O) | H1 = h,A1 = a]
In addition, for any a ? N , we use ?a ? Rm to
denote the vector with ?ah = P (H1 = h|A1 = a).
The correctness of the representation will rely on
the following conditions being satisfied (these are
parallel to conditions 1 and 2 in Hsu et al (2009)):
Condition 1 ?a ? N , the matrices Ia and Ja are
of full rank (i.e., they have rank m). For all a ? N ,
for all h ? [m], ?ah > 0.
Condition 2 ?a ? N , the matrices Ua ? R(d?m)
and V a ? R(d??m) are such that the matrices Ga =
(Ua)?Ia and Ka = (V a)?Ja are invertible.
The following lemma justifies the use of an SVD
calculation as one method for finding values for Ua
and V a that satisfy condition 2:
Lemma 1 Assume that condition 1 holds, and for
all a ? N define
?a = E[?(T1) (?(O))? |A1 = a] (5)
Then if Ua is a matrix of the m left singular vec-
tors of ?a corresponding to non-zero singular val-
ues, and V a is a matrix of the m right singular vec-
tors of ?a corresponding to non-zero singular val-
ues, then condition 2 is satisfied.
Proof sketch: It can be shown that ?a =
Iadiag(?a)(Ja)?. The remainder is similar to the
proof of lemma 2 in Hsu et al (2009).
The matrices ?a can be estimated directly from a
training set consisting of s-trees, assuming that we
have access to the functions ? and ?.
We can now state the following theorem:
227
Theorem 2 Assume conditions 1 and 2 are satisfied.
For all a ? N , define Ga = (Ua)?Ia. Then under
the definitions in Eqs. 2-4:
1. For all rules a? b c, Ca?b c(y) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1
2. For all rules a? x, c?a?x = 1?Qa?x(Ga)?1.
3. For all a ? N , c1a = Ga?a
Proof: The following identities hold (see sec-
tion 9.2):
Da?b c(y) = (6)
GcT a?b cdiag(yGbSa?b c)Qa?b cdiag(?a)(Ka)?
d?a?x = 1?Qa?xdiag(?a)(Ka)? (7)
?a = Gadiag(?a)(Ka)? (8)
c1a = Gapia (9)
Under conditions 1 and 2, ?a is invertible, and
(?a)?1 = ((Ka)?)?1(diag(?a))?1(Ga)?1. The
identities in the theorem follow immediately.
7 Deriving Empirical Estimates
Figure 4 shows an algorithm that derives esti-
mates of the quantities in Eqs 2, 3, and 4. As
input, the algorithm takes a sequence of tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i ? [M ].
These tuples can be derived from a training set
consisting of s-trees ?1 . . . ?M as follows:
? ?i ? [M ], choose a single node ji uniformly at
random from the nodes in ?i. Define r(i,1) to be the
rule at node ji. t(i,1) is the inside tree rooted at node
ji. If r(i,1) is of the form a? b c, then t(i,2) is the
inside tree under the left child of node ji, and t(i,3)
is the inside tree under the right child of node ji. If
r(i,1) is of the form a ? x, then t(i,2) = t(i,3) =
NULL. o(i) is the outside tree at node ji. b(i) is 1 if
node ji is at the root of the tree, 0 otherwise.
Under this process, assuming that the s-trees
?1 . . . ?M are i.i.d. draws from the distribution
p(?) over s-trees under an L-PCFG, the tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) are i.i.d. draws
from the joint distribution over the random variables
R1, T1, T2, T3, O,B defined in the previous section.
The algorithm first computes estimates of the pro-
jection matrices Ua and V a: following lemma 1,
this is done by first deriving estimates of ?a,
and then taking SVDs of each ?a. The matrices
are then used to project inside and outside trees
t(i,1), t(i,2), t(i,3), o(i) down to m-dimensional vec-
tors y(i,1), y(i,2), y(i,3), z(i); these vectors are used to
derive the estimates of Ca?b c, c?a?x, and c1a.
We now state a PAC-style theorem for the learning
algorithm. First, for a given L-PCFG, we need a
couple of definitions:
? ? is the minimum absolute value of any element
of the vectors/matrices/tensors c1a, d?a?x, Da?b c,
(?a)?1. (Note that ? is a function of the projec-
tion matrices Ua and V a as well as the underlying
L-PCFG.)
? For each a ? N , ?a is the value of the m?th
largest singular value of ?a. Define ? = mina ?a.
We then have the following theorem:
Theorem 3 Assume that the inputs to the algorithm
in figure 4 are i.i.d. draws from the joint distribution
over the random variables R1, T1, T2, T3, O,B, un-
der an L-PCFG with distribution p(r1 . . . rN ) over
s-trees. Define m to be the number of latent states
in the L-PCFG. Assume that the algorithm in fig-
ure 4 has projection matrices U?a and V? a derived as
left and right singular vectors of ?a, as defined in
Eq. 5. Assume that the L-PCFG, together with U?a
and V? a, has coefficients ? > 0 and ? > 0. In addi-
tion, assume that all elements in c1a, d?a?x, Da?b c,
and ?a are in [?1,+1]. For any s-tree r1 . . . rN de-
fine p?(r1 . . . rN ) to be the value calculated by the
algorithm in figure 3 with inputs c?1a, c??a?x, C?a?b c
derived from the algorithm in figure 4. Define R to
be the total number of rules in the grammar of the
form a? b c or a ? x. Define Ma to be the num-
ber of training examples in the input to the algorithm
in figure 4 where ri,1 has non-terminal a on its left-
hand-side. Under these assumptions, if for all a
Ma ?
128m2
( 2N+1?1 + ?? 1
)2 ?2?4
log
(2mR
?
)
Then
1? ? ?
?
?
?
?
p?(r1 . . . rN )
p(r1 . . . rN )
?
?
?
?
? 1 + ?
A similar theorem (omitted for space) states that
1? ? ?
?
?
?
??(a,i,j)
?(a,i,j)
?
?
?
? 1 + ? for the marginals.
The condition that U?a and V? a are derived from
?a, as opposed to the sample estimate ??a, follows
Foster et al (2012). As these authors note, similar
techniques to those of Hsu et al (2009) should be
228
applicable in deriving results for the case where ??a
is used in place of ?a.
Proof sketch: The proof is similar to that of Foster
et al (2012). The basic idea is to first show that
under the assumptions of the theorem, the estimates
c?1a, d??a?x, D?a?b c, ??a are all close to the underlying
values being estimated. The second step is to show
that this ensures that p?(r1...rN? )p(r1...rN? ) is close to 1.
The method described of selecting a single tuple
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for each s-tree en-
sures that the samples are i.i.d., and simplifies the
analysis underlying theorem 3. In practice, an im-
plementation should most likely use all nodes in all
trees in training data; by Rao-Blackwellization we
know such an algorithm would be better than the
one presented, but the analysis of how much better
would be challenging. It would almost certainly lead
to a faster rate of convergence of p? to p.
8 Discussion
There are several potential applications of the
method. The most obvious is parsing with L-
PCFGs.1 The approach should be applicable in other
cases where EM has traditionally been used, for ex-
ample in semi-supervised learning. Latent-variable
HMMs for sequence labeling can be derived as spe-
cial case of our approach, by converting tagged se-
quences to right-branching skeletal trees.
The sample complexity of the method depends on
the minimum singular values of ?a; these singular
values are a measure of how well correlated ? and
? are with the unobserved hidden variable H1. Ex-
perimental work is required to find a good choice of
values for ? and ? for parsing.
9 Proofs
This section gives proofs of theorems 1 and 2. Due
to space limitations we cannot give full proofs; in-
stead we provide proofs of some key lemmas. A
long version of this paper will give the full proofs.
9.1 Proof of Theorem 1
First, the following lemma leads directly to the cor-
rectness of the algorithm in figure 2:
1Parameters can be estimated using the algorithm in
figure 4; for a test sentence x1 . . . xN we can first
use the algorithm in figure 3 to calculate marginals
?(a, i, j), then use the algorithm of Goodman (1996) to find
argmax??T (x)
?
(a,i,j)?? ?(a, i, j).
Inputs: Training examples (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i))
for i ? {1 . . .M}, where r(i,1) is a context free rule; t(i,1),
t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; and
b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function
? that maps inside trees t to feature-vectors ?(t) ? Rd. A func-
tion ? that maps outside trees o to feature-vectors ?(o) ? Rd? .
Algorithm:
Define ai to be the non-terminal on the left-hand side of rule
r(i,1). If r(i,1) is of the form a? b c, define bi to be the non-
terminal for the left-child of r(i,1), and ci to be the non-terminal
for the right-child.
(Step 0: Singular Value Decompositions)
? Use the algorithm in figure 5 to calculate matrices U?a ?
R(d?m) and V? a ? R(d??m) for each a ? N .
(Step 1: Projection)
? For all i ? [M ], compute y(i,1) = (U?ai)??(t(i,1)).
? For all i ? [M ] such that r(i,1) is of the form
a? b c, compute y(i,2) = (U?bi)??(t(i,2)) and y(i,3) =
(U?ci)??(t(i,3)).
? For all i ? [M ], compute z(i) = (V? ai)??(o(i)).
(Step 2: Calculate Correlations)
? For each a ? N , define ?a = 1/
?M
i=1[[ai = a]]
? For each rule a? b c, compute D?a?b c = ?a ?
?M
i=1[[r(i,1) = a? b c]]y(i,3)(z(i))?(y(i,2))?
? For each rule a ? x, compute d??a?x = ?a ?
?M
i=1[[r(i,1) = a? x]](z(i))?
? For each a ? N , compute ??a = ?a ?
?M
i=1[[ai = a]]y(i,1)(z(i))?
(Step 3: Compute Final Parameters)
? For all a? b c, C?a?b c(y) = D?a?b c(y)(??a)?1
? For all a? x, c??a?x = d??a?x(??a)?1
? For all a ? I, c?1a =
?M
i=1[[ai=a and b(i)=1]]y(i,1)
?M
i=1[[b(i)=1]]
Figure 4: The spectral learning algorithm.
Inputs: Identical to algorithm in figure 4.
Algorithm:
? For each a ? N , compute ??a ? R(d??d) as
??a =
?M
i=1[[ai = a]]?(t(i,1))(?(o(i)))?
?M
i=1[[ai = a]]
and calculate a singular value decomposition of ??a.
? For each a ? N , define U?a ? Rm?d to be a matrix of the left
singular vectors of ??a corresponding to the m largest singular
values. Define V? a ? Rm?d? to be a matrix of the right singular
vectors of ??a corresponding to the m largest singular values.
Figure 5: Singular value decompositions.
229
Lemma 2 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 2 is an s-tree r1 . . . rN . Define ai for i ? [N ]
to be the non-terminal on the left-hand-side of rule
ri, and ti for i ? [N ] to be the s-tree with rule ri
at its root. Finally, for all i ? [N ], define the row
vector bi ? R(1?m) to have components
bih = P (Ti = ti|Hi = h,Ai = ai)
for h ? [m]. Then for all i ? [N ], f i = bi(G(ai))?1.
It follows immediately that
f1c1a1 = b
1(G(a1))?1Ga1?a1 = p(r1 . . . rN )
This lemma shows a direct link between the vec-
tors f i calculated in the algorithm, and the terms bih,
which are terms calculated by the conventional in-
side algorithm: each f i is a linear transformation
(through Gai) of the corresponding vector bi.
Proof: The proof is by induction.
First consider the base case. For any leaf?i.e., for
any i such that ai ? P?we have bih = q(ri|h, ai),
and it is easily verified that f i = bi(G(ai))?1.
The inductive case is as follows. For all i ? [N ]
such that ai ? I , by the definition in the algorithm,
f i = f?Cri(f?)
= f?Ga?T ridiag(f?Ga?Sri)Qri(Gai)?1
Assuming by induction that f? = b?(G(a? ))?1 and
f? = b?(G(a?))?1, this simplifies to
f i = ?rdiag(?l)Qri(Gai)?1 (10)
where ?r = b?T ri , and ?l = b?Sri . ?r is a row
vector with components ?rh =
?
h??[m] b
?
h?T
ri
h?,h =
?
h??[m] b
?
h?t(h?|h, ri). Similarly, ?l is a row vector
with components equal to ?lh =
?
h??[m] b
?
h?S
ri
h?,h =
?
h??[m] b
?
h?s(h?|h, ri). It can then be verified that
?rdiag(?l)Qri is a row vector with components
equal to ?rh?lhq(ri|h, ai).
But bih = q(ri|h, ai)?
(
?
h??[m] b
?
h?t(h?|h, ri)
)
?
(
?
h??[m] b
?
h?s(h?|h, ri)
)
= q(ri|h, ai)?rh?lh, hence
?rdiag(?l)Qri = bi and the inductive case follows
immediately from Eq. 10.
Next, we give a similar lemma, which implies the
correctness of the algorithm in figure 3:
Lemma 3 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 3 is a sentence x1 . . . xN . For any a ? N , for
any 1 ? i ? j ? N , define ??a,i,j ? R(1?m) to have
components ??a,i,jh = p(xi . . . xj|h, a) for h ? [m].
In addition, define ??a,i,j ? R(m?1) to have compo-
nents ??a,i,jh = p(x1 . . . xi?1, a(h), xj+1 . . . xN ) for
h ? [m]. Then for all i ? [N ], ?a,i,j = ??a,i,j(Ga)?1
and ?a,i,j = Ga??a,i,j . It follows that for all (a, i, j),
?(a, i, j) = ??a,i,j(Ga)?1Ga??a,i,j = ??a,i,j ??a,i,j
=
?
h
??a,i,jh ??
a,i,j
h =
?
??T (x):(a,i,j)??
p(?)
Thus the vectors ?a,i,j and ?a,i,j are linearly re-
lated to the vectors ??a,i,j and ??a,i,j , which are the
inside and outside terms calculated by the conven-
tional form of the inside-outside algorithm.
The proof is by induction, and is similar to the
proof of lemma 2; for reasons of space it is omitted.
9.2 Proof of the Identity in Eq. 6
We now prove the identity in Eq. 6, used in the proof
of theorem 2. For reasons of space, we do not give
the proofs of identities 7-9: the proofs are similar.
The following identities can be verified:
P (R1 = a? b c|H1 = h,A1 = a) = q(a? b c|h, a)
E [Y3,j|H1 = h,R1 = a? b c] = Ea?b cj,h
E [Zk|H1 = h,R1 = a? b c] = Kak,h
E [Y2,l|H1 = h,R1 = a? b c] = F a?b cl,h
where Ea?b c = GcT a?b c, F a?b c = GbSa?b c.
Y3, Z and Y2 are independent when conditioned
on H1, R1 (this follows from the independence as-
sumptions in the L-PCFG), hence
E [[[R1 = a? b c]]Y3,jZkY2,l | H1 = h,A1 = a]
= q(a? b c|h, a)Ea?b cj,h Kak,hF a?b cl,h
Hence (recall that ?ah = P (H1 = h|A1 = a)),
Da?b cj,k,l = E [[[R1 = a? b c]]Y3,jZkY2,l | A1 = a]
=
?
h
?ahE [[[R1 = a? b c]]Y3,jZkY2,l | H1 = h,A1 = a]
=
?
h
?ahq(a? b c|h, a)Ea?b cj,h Kak,hF a?b cl,h (11)
from which Eq. 6 follows.
230
Acknowledgements: Columbia University gratefully ac-
knowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of DARPA, AFRL, or the US
government. Shay Cohen was supported by the National
Science Foundation under Grant #1136996 to the Com-
puting Research Association for the CIFellows Project.
Dean Foster was supported by National Science Founda-
tion grant 1106743.
References
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
Proceedings of FOCS.
Dean P. Foster, Jordan Rodu, and Lyle H. Ungar.
2012. Spectral dimensionality reduction for hmms.
arXiv:1203.6130v1.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the 34th annual meeting on Associ-
ation for Computational Linguistics, pages 177?183.
Association for Computational Linguistics.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6).
F. M. Lugue, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 75?82. Association for
Computational Linguistics.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 128?135, Newark,
Delaware, USA, June. Association for Computational
Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 433?440, Sydney, Australia, July.
Association for Computational Linguistics.
S. A. Terwijn. 2002. On the learnability of hidden
markov models. In Grammatical Inference: Algo-
rithms and Applications (Amsterdam, 2002), volume
2484 of Lecture Notes in Artificial Intelligence, pages
261?268, Berlin. Springer.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841?860.
231
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1033?1041,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The effect of non-tightness on Bayesian estimation of PCFGs
Shay B. Cohen
Department of Computer Science
Columbia University
scohen@cs.columbia.edu
Mark Johnson
Department of Computing
Macquarie University
mark.johnson@mq.edu.au
Abstract
Probabilistic context-free grammars have
the unusual property of not always defin-
ing tight distributions (i.e., the sum of the
?probabilities? of the trees the grammar
generates can be less than one). This paper
reviews how this non-tightness can arise
and discusses its impact on Bayesian es-
timation of PCFGs. We begin by present-
ing the notion of ?almost everywhere tight
grammars? and show that linear CFGs fol-
low it. We then propose three different
ways of reinterpreting non-tight PCFGs to
make them tight, show that the Bayesian
estimators in Johnson et al (2007) are
correct under one of them, and provide
MCMC samplers for the other two. We
conclude with a discussion of the impact
of tightness empirically.
1 Introduction
Probabilistic Context-Free Grammars (PCFGs)
play a special role in computational linguistics be-
cause they are perhaps the simplest probabilistic
models of hierarchical structures. Their simplicity
enables us to mathematically analyze their prop-
erties to a detail that would be difficult with lin-
guistically more accurate models. Such analysis
is useful because it is reasonable to expect more
complex models to exhibit similar properties as
well.
The problem of inferring PCFG rule probabil-
ities from training data consisting of yields or
strings alone is interesting from both cognitive and
engineering perspectives. Cognitively it is implau-
sible that children can perceive the parse trees of
the language they are learning, but it is more rea-
sonable to assume that they can obtain the terminal
strings or yield of these trees. Unsupervised meth-
ods for learning a grammar from terminal strings
alone is also interesting from an engineering per-
spective because such training data is cheap and
plentiful, while the manually parsed data required
by supervised methods are expensive to produce
and relatively rare.
Cohen and Smith (2012) show that inferring
PCFG rule probabilities from strings alone is com-
putationally intractable, so we should not expect
to find an efficient, general-purpose algorithm for
the unsupervised problem. Instead, approxima-
tion algorithms are standardly used. For exam-
ple, the Inside-Outside (IO) algorithm efficiently
implements the Expectation-Maximization (EM)
procedure for approximating a Maximum Likeli-
hood estimator (Lari and Young, 1990). Bayesian
estimators for PCFG rule probabilities have also
been attracting attention because they provide a
theoretically-principled way of incorporating prior
information. Kurihara and Sato (2006) proposed
a Variational Bayes estimator based on a mean-
field approximation, and Johnson et al (2007) pro-
posed MCMC samplers for the posterior distribu-
tion over rule probabilities and the parse trees of
the training data strings.
PCFGs have the interesting property (which we
expect most linguistically more realistic models to
also possess) that the distributions they define are
not always properly normalized or ?tight?. In a
non-tight PCFG the partition function (i.e., sum
of the ?probabilities? of all the trees generated by
the PCFG) is less than one. (Booth and Thomp-
son, 1973, called such non-tight PCFGs ?incon-
sistent?, but we follow Chi and Geman (1998)
in calling them ?non-tight? to avoid confusion
with the consistency of statistical estimators). Chi
(1999) showed that renormalized non-tight PCFGs
(which he called ?Gibbs CFGs?) define the same
class of distributions over trees as do tight PCFGs
with the same rules, and provided an algorithm for
mapping any PCFG to a tight PCFG with the same
rules that defines the same distribution over trees.
An obvious question is then: how does tightness
affect the inference of PCFGs? Chi and Geman
(1998) studied the question for Maximum Likeli-
hood (ML) estimation, and showed that ML es-
1033
timates are always tight for both the supervised
case (where the input consists of parse trees) and
the unsupervised case (where the input consists of
yields or terminal strings). This means that ML
estimators can simply ignore issues of tightness,
and rest assured that the PCFGs they estimate are
in fact tight.
The situation is more subtle with Bayesian es-
timators. We show that for the special case of
linear PCFGs (which include HMMs) with non-
degenerate priors the posterior puts zero mass on
non-tight PCFGs, so tightness is not an issue with
Bayesian estimation of such grammars. However,
because all of the commonly used priors (such as
the Dirichlet or the logistic normal) assign non-
zero probability across the whole probability sim-
plex, in general the posterior may assign non-zero
probability to non-tight PCFGs. We discuss three
different possible approaches to this in this paper:
1. the only-tight approach, where we modify the
prior so it only assigns non-zero probability
to tight PCFGs,
2. the renormalization approach, where we
renormalize non-tight PCFGs so they define
a probability distribution over trees, and
3. the sink-element approach, where we reinter-
pret non-tight PCFGs as assigning non-zero
probability to a ?sink element?, so both tight
and non-tight PCFGs are properly normal-
ized.
We show how to modify the Gibbs sampler de-
scribed by Johnson et al (2007) so it produces
samples from the posterior distributions defined
by the only-tight and renormalization approaches.
Perhaps surprisingly, we show that Gibbs sampler
as defined by Johnson et al actually produces
samples from the posterior distributions defined by
the sink-element approach.
We conclude by studying the effect of requir-
ing tightness on the estimation of some simple
PCFGs. Because the Bayesian posterior converges
around the (tight) ML estimate as the size of
the data grows, requiring tightness only seems to
make a difference with highly biased priors or with
very small training corpora.
2 PCFGs and tightness
LetG = (T,N, S,R) be a Context-Free Grammar
in Chomsky normal form with no useless produc-
tions, where T is a finite set of terminal symbols,
N is a finite set of nonterminal symbols (disjoint
from T ), S ? N is a distinguished nonterminal
called the start symbol, andR is a finite set of pro-
ductions of the form A ? BC or A ? w, where
A,B,C ? N and w ? T . In what follows we use
? as a variable ranging over (N ?N) ? T .
A Probabilistic Context-Free Grammar (G,?)
is a pair consisting of a context-free grammar G
and a real-valued vector ? of length |R| indexed
by productions, where ?A?? is the production
probability associated with the production A ?
? ? R. We require that ?A?? ? 0 and that for
all nonterminals A ? N , ?A???RA ?A?? = 1,where RA is the subset of rules R expanding the
nonterminal A.
A PCFG (G,?) defines a measure ?? over
trees t as follows:
??(t) =
?
r?R
?fr(t)r
where fr(t) is the number of times the production
r = A? ? ? R is used in the derivation of t.
The partition function Z or measure of all pos-
sible trees is:
Z(?) =
?
t??T
?
r?R
?fr(t?)r
where T is the set of all (finite) trees generated
by G. A PCFG is tight iff the partition function
Z(?) = 1. In this paper we use ?? to denote the
set of rule probability vectors ? for which G is
non-tight. Nederhof and Satta (2008) survey sev-
eral algorithms for computing Z(?), and hence
for determining whether a PCFG is tight.1
Non-tightness can arise in very simple PCFGs,
such as the ?Catalan? PCFG S ? S S | a. This
grammar produces binary trees where all internal
nodes are labeled as S and the yield of these trees
is a sequence of as. If the probability of the rule
S ? S S is greater than 0.5 then this PCFG is
non-tight.
Perhaps the most straight-forward way to under-
stand this non-tightness is to view this grammar as
defining a branching process where an S can either
?reproduce? with probability ?S?S S or ?die out?
1We found out that finding whether a PCFG is tight by
directly inspecting the partition function value is less stable
than using the method in Wetherell (1980). For this reason,
we used Wetherell?s approach, which is based on finding the
principal eigenvalue of the matrix M .
1034
with probability ?S?a. When ?S?S S > ?S?a the
S nodes reproduce at a faster rate than they die
out, so the derivation has a non-zero probability of
endlessly rewriting (Atherya and Ney, 1972).
3 Bayesian inference for PCFGs
The goal of Bayesian inference for PCFGs is to in-
fer a posterior distribution over the rule probabil-
ity vectors ? given observed data D. This poste-
rior distribution is obtained by combining the like-
lihood P(D | ?) with a prior distribution P(?)
over ? using Bayes Rule.
P(? | D) ? P(D | ?) P(?)
We now formally define the three approaches to
handling non-tightness mentioned earlier:
the only-tight approach: we only permit priors
where P(??) = 0, i.e., we insist that the
prior assign zero mass to non-tight rule prob-
ability vectors, so Z = 1. This means we can
define:
P(t | ?) = ??(t)
the renormalization approach: we renormalize
non-tight PCFGs by dividing by the partition
function:
P(t | ?) = 1Z(?) ??(t) (1)
the sink-element approach: we redefine our
probability distribution so its domain is a set
T ? = T ? {?}, where T is the set of (finite)
trees generated by G and ? 6? T is a new
element that serves as a ?sink state? to which
the ?missing mass? 1 ? Z(?) is assigned.
Then we define:2
P(t | ?) =
{
??(t) if t ? T
1? Z(?) if t = ?
2This definition of a distribution over trees can be induced
by a tight PCFG with a special ? symbol in its vocabulary.
Given G, the first step is to create a tight grammar G0 using
the renormalization approach. Then, a new start symbol is
added to G0, S0, and also rules S0 ? S (where S is the
old start symbol in G0) and S0 ? ?. The first rule is given
probability Z(?) and the second rule is given probability 1?
Z(?). It can be then readily shown that the new tight PCFG
G0 induces a distribution over trees just like in Eq. 3, only
with additional S0 on top of all trees.
With this in hand, we can now define the likeli-
hood term. We consider two types of data D here.
In the supervised setting the data D consists of a
corpus of parse trees D = (t1, . . . , tn) where each
tree ti is generated by the PCFG G, so
P(D | ?) =
n?
i=1
P(ti | ?)
In the unsupervised setting the data D consists
of a corpus of strings D = (w1, . . . , wn) where
each string wi is the yield of one or more trees
generated by G. In this setting
P(D | ?) =
n?
i=1
P(wi | ?),where:
P(w | ?) =
?
t?T :yield(t)=w
P(t | ?)
4 The special case of linear PCFGs
One way to handle the issue of tightness is to iden-
tify a family of CFGs for which practically any pa-
rameter setting will yield a tight PCFG. This is the
focus of this section, in which we identify a sub-
set of CFGs, which are ?almost everywhere? tight.
This family of CFGs includes many of the CFGs
used in NLP applications.
We cannot expect that a CFG will yield a tight
PCFG for any assignment to the rule probabilities
(i.e. that ?? = ?). Even in simple cases, such as
the grammar S ? S|a, the assignment of proba-
bility 1 to S ? S and 0 to the other rule renders
the S nonterminal useless, and places all of the
probability mass on infinite structures of the form
S ? S ? S ? . . ..
However, we can weaken our requirement so
that the cases in which parameter assignment
yields a non-tight PCFG are rare, or have measure
zero. To put it more formally, we say that a prior
P(?) is ?tight almost everywhere for G? if
P(??) =
?
????
P(?) d? = 0.
We now provide a sufficient condition (linear-
ity) for CFGs under which they are tight almost
everywhere with any continuous prior.
For a nonterminal A ? N and ? ? (N ? T )?,
we use A?k ? to denote that A can be re-written
using a sequence of rules from R to the sentential
form ? in k derivation steps. We use A ?+ ? to
denote that there exists a k > 0 such thatA?k ?.
1035
Definition 1 A context-free grammarG is linear if
there are no A ? N such that
A?+ . . . A . . . A . . . .
Definition 2 A nonterminal A ? N in a proba-
bilistic context-free grammar G with parameters
? is nonterminating if
PG(A?+ . . . A . . . |?) = 1.
Here P(A?+ . . . A . . . |?) is defined as:
?
?:?=...A...
PG(A?+ ?|?).
Lemma 1 A linear PCFG G with parameters ?
which does not have any nonterminating nonter-
minals is tight.
Proof: Our proof relies on the properties of a cer-
tain |N | ? |N | matrix M where:
MAB =
?
A???RA
n(?,B) ?A??
where n(?,B) is the number of appearances of the
nonterminal B in the sequence ?. MAB is the ex-
pected number of B nonterminals generated from
an A nonterminal in one single derivational step,
so [Mk]AB is the expected number ofB nontermi-
nals generated from an A nonterminal in a k-step
derivation (Wetherell, 1980).
Since M is a non-negative matrix, under some
regularity conditions, the Frobenius-Perron theo-
rem states that the largest eigenvalue of this ma-
trix (in absolute value) is a real number. Let this
eigenvalue be denoted by ?.
A PCFG is called ?subcritical? if ? < 1 and
supercritical if ? > 1. Then, in turn, a PCFG is
tight if it is subcritical. It is not tight if it is su-
percritical. The case of ? = 1 is a borderline case
that does not give sufficient information to know
whether the PCFG is tight or not. In the Bayesian
case, for a continuous prior such as the Dirichlet
prior, this borderline case will have measure zero
under the prior.
Now let A ? N . Since the grammar is lin-
ear, there is no derivation A ?+ . . . A . . . A . . ..
Therefore, any derivation of the form A ?+
. . . A . . . includes A on the right hand-side exactly
once. Because the grammar has no useless non-
terminals, the probability of such a derivation is
strictly smaller than 1.
For each A ? N , define:
pA =
?
?=...A...
P(A?|N | ?|?).
Since A is not useless, then pA < 1. Therefore
q = maxA pA < 1. Since any derivation of length
k of the formA? . . . A . . . can be decomposed to
at least k2|N | cycles that start at a terminal B ? N
and end in the same nonterminal B ? N , it holds
that:
[Mk]AA ? q
k
2|N| k??? 0.
This means that trace(Mk) k??? 0. This means
that the eigenvalue of M is strictly smaller than 1
(linear algebra), and therefore the PCFG is tight.
Proposition 1 Any continuous prior P(?) on a
linear grammar G is tight almost everywhere for
G.
Proof: Let G be a linear grammar. With a contin-
uous prior, the probability ofG getting parameters
from the prior which yield a useless non-terminal
is 0 ? it would require setting at least one rule in
the grammar with rule probability which is exactly
1. Therefore, with probability 1, the parameters
taken from the prior yield a PCFG which is linear
and does not have nonterminating nonterminals.
According to Lemma 1, this means the PCFG is
tight. 
Deciding whether a grammar G is linear can
be done in polynomial time using the construction
from Bar-Hillel et al (1964). We can first elimi-
nate the differences between nonterminals and ter-
minal symbols by adding a rule A ? cA for each
nonterminal A ? N , after extending the set of
terminal symbols A with {cA|A ? N}. Let GA
be the grammar G with the start symbol being re-
placed with A. We can then intersect the grammar
GA with the regular language T ?cAT ?cAT ? (for
each nonterminal A ? N ). If for any nontermi-
nal A the intersection is not the empty set (with
respect to the language that the intersection gen-
erates), then the grammar is not linear. Checking
whether the intersection is the empty set or not can
be done in polynomial time.
We conclude this section by remarking that
many of the models used in computational lin-
guistics are in fact equivalent to linear PCFGs, so
continuous Bayesian priors are almost everywhere
tight. For example, HMMs and many kinds of
?stacked? finite-state machines are equivalent to
1036
linear PCFGs, as are the example PCFGs given in
Johnson et al (2007) to motivate the MCMC esti-
mation procedures.
5 Dirichlet priors
The first step in Bayesian inference is to specify a
prior on ?. In the rest of this paper we take P(?)
to be a product of Dirichlet distributions, with one
distribution for each non-terminal A ? N , as this
turns out to simplify the computations consider-
ably. The prior is parameterized by a positive real
valued vector ? indexed by productionsR, so each
production probability ?A?? has a corresponding
Dirichlet parameter ?A?? . As before, let RA be
the set of productions in R with left-hand side A,
and let ?A and ?A refer to the component subvec-
tors of ? and ? respectively indexed by produc-
tions in RA. The Dirichlet prior P(? | ?) is:
P(? | ?) =
?
A?N
PD(?A | ?A),
where
PD(?A | ?A) =
1
C(?A)
?
r?RA
??r?1r and
C(?A) =
?
r?RA ?(?r)
?(
?
r?RA ?r)
where ? is the generalized factorial function and
C(?) is a normalization constant that does not de-
pend on ?A.
Dirichlet priors are useful because they are con-
jugate to the multinomial distribution, which is
the building block of PCFGs. Ignoring issues of
tightness for the moment and setting P(t | ?) =
??(t), this means that in the supervised setting the
posterior distribution P(? | t, ?) given a set of
parse trees t = (t1, . . . , tn) is also a product of
Dirichlets distribution.
P(? | t, ?) ? P(t | ?) P(? | ?)
?
(?
r?R
?fr(t)r
)(?
r?R
??r?1r
)
=
?
r?R
?fr(t)+?r?1r
which is a product of Dirichlet distributions with
parameters f(t) + ?, where f(t) is the vector of
rule counts in t indexed by r ? R. We can thus
write:
P(? | t, ?) = P(? | f(t) + ?)
Input: Grammar G, vector of trees t, vector of
hyperparameters ?, previous parameters ?0.
Result: A vector of parameters ?
repeat
draw ? from products of Dirichlet with
hyperparameters ?+ f(t)
until ? is tight for G;
return ?
Algorithm 1: An algorithm for generating sam-
ples from P(? | t, ?) for the only-tight ap-
proach.
Input: Grammar G, vector of trees t, vector of
hyperparameters ?, previous rule parameters
?0.
Result: A vector of parameters ?
draw a proposal ?? from a product of Dirichlets with
parameters ?+ f(t).
draw a uniform number u from [0, 1].
if u < min{1,
(
Z(?(i?1))/Z(??)
)n
} return ??.
return ?0.
Algorithm 2: One step of Metropolis-Hastings
algorithm for generating samples from P(? |
t, ?) for the renormalization approach.
which makes it clear that the rule counts are di-
rectly added to the parameters of the prior to pro-
duce the parameters of the posterior.
6 Inference in the supervised setting
We first discuss Bayesian inference in the super-
vised setting, as inference in the unsupervised set-
ting is based on inference for the supervised set-
ting. For each of the three approaches to non-
tightness we provide an algorithm that character-
izes the posterior P(? | t), where t = (t1, . . . , tn)
is a sequence of trees, by generating samples from
that posterior. Our MCMC algorithms for the un-
supervised setting build on these samplers for the
supervised setting.
6.1 The only-tight approach
The ?only-tight? approach requires that the prior
assign zero mass to non-tight rule probability vec-
tors ??. One way to define such a distribution is
to restrict the domain of an existing prior distribu-
tion with the set of tight ? and renormalize. In
more detail, if P(?) is a prior over rule probabili-
ties, then its renormalization is the prior P? defined
as:
P?(?) = P(?)I(? /? ?
?)
Z(??) . (2)
where Z(??) = ?? P(?)I(? /? ??)d?.
1037
Input: Grammar G, vector of trees t, vector of
hyperparameters ?, previous parameters ?0.
Result: A vector of parameters ?
draw ? from products of Dirichlet with
hyperparameters ?+ f(t)
return ?
Algorithm 3: An algorithm for generating sam-
ples from P(? | t, ?) for the sink-state approach.
Perhaps surprisingly, it turns out that if P(?)
belongs to a family of conjugate priors, then P?(?)
also belongs to a (different) family of conjugate
priors as well.
Proposition 2 Let P(?|?) be a prior with hyper-
parameters ? over the parameters of G such that
P is conjugate to the grammar likelihood. Then
P?, defined in Eq. 2, is conjugate to the grammar
likelihood as well.
Proof: Assume that trees t are observed, and the
prior over the grammar parameters is the prior de-
fined in Eq. 2. Therefore, the posterior is:
P(?|t, ?) ? P?(?|?)p(t|?)
= P(?|?)p(t|?)I(? /? ?
?)
Z(??)
? P(?|t, ?)I(? /? ?
?)
Z(??) .
Since P(?|?) is a conjugate prior to the PCFG
likelihood, then there exists ?? = ??(t) such that
P(?|t, ?) = P?(?|??). Therefore:
P(?|t, ?) ? P(?|?
?)I(? /? ??)
Z(??) .
which exactly equals P?(?|??). 
Sampling from the posterior over the parame-
ters given a set of trees t is therefore quite sim-
ple when assuming the base prior being renormal-
ized is a product of Dirichlets. Algorithm 1 sam-
ples from a product of Dirichlets distribution with
hyperparameters ? + f(t) repeatedly, each time
checking and rejecting the sample until we obtain
a tight PCFG.
The more mass the Dirichlet distribution with
hyperparameters ? + f(t) puts on non-tight
PCFGs, the more rejections will happen. In gen-
eral, if the probability mass on non-tight PCFGs is
q?, then it would require, on average 1/(1 ? q?)
samples from this distribution in order to obtain a
tight PCFG.
6.2 The renormalization approach
The renormalization approach modifies the likeli-
hood function instead of the prior. Here we use a
product of Dirichlets prior P(? | ?) on rule prob-
ability vectors ?, but the presence of the partition
functionZ(?) in Eq. 1 means that the likelihood is
no longer conjugate to the prior. Instead we have:
P(? | t) =
n?
i=1
??(ti)
Z(?) P(? | ?)
? 1Z(?)n P(? | ?+ f(t)). (3)
Note that the factor Z(?) depends on ?, and
therefore cannot be absorbed into the constant. Al-
gorithm 2 describes a Metropolis-Hastings sam-
pler for sampling from the posterior in Eq. 3
that uses a product of Dirichlets with parameters
?+ f(t) as a proposal distribution.
In our experiments, we use the algorithm from
Nederhof and Satta (2008) to compute the parti-
tion function which is needed in Algorithm 2.
6.3 The ?sink element? approach
The ?sink element? approach does not affect the
likelihood (since the probability of a tree t is just
the product of the probabilities of the rules used
to generate it), nor does it require a change to the
prior. (The sink element ? is not a member of the
set of trees T , so it cannot appear in the data t).
This means that the conjugacy argument given
at the bottom of section 5 holds in this approach,
so the posterior P(? | t, ?) is a product of Dirich-
lets with parameters f(t) + ?. Algorithm 3 gives
a sampler for P(? | t, ?) for the sink element ap-
proach.
7 Inference in the unsupervised setting
Johnson et al (2007) provide two Markov chain
Monte Carlo algorithms for Bayesian inference for
PCFG rule probabilities in the unsupervised set-
ting (i.e., where the data consists of a corpus of
strings w = (w1, . . . , wn) alone). The algorithms
we give here are based on their Gibbs sampler,
which in each iteration first samples parse trees
t = (t1, . . . , tn), where each ti is a parse for
wi, from P(t | w,?), and then samples ? from
P(? | t, ?).
Notice that the conditional distribution P(t |
w,?) is unaffected in each of our three ap-
proaches (the partition functions cancel in the
1038
Input: Grammar G, vector of hyperparameters ?,
vector of strings w = (w1, . . . , wn), previous
rule parameters ?0.
Result: A vector of parameters ?
for i? 1 to n do
draw ti from P(ti|wi,?0)
end
use Algorithm 2 to sample ? given G, t, ? and ?0
return ?
Algorithm 4: One step of the Metropolis-within-
Gibbs sampler for the renormalization approach.
renormalization approach), so the algorithm for
sampling from P(t | w,?) given by Johnson et
al. applies in each of our three approaches as well.
Johnson et al ignored tightness and assumed
that P(? | t, ?) is a product of Dirichlets with
parameters f(t) + ?. As we noted in section 6.3,
this assumption holds for the sink-state approach
to non-tightness, so their sampler is in fact correct
for the sink-state approach.
In fact, we obtain samplers for the unsupervised
setting for each of our approaches by ?plugging
in? the corresponding sampling algorithm (Eq. 1?
3) for P(? | t, ?) into the generic Gibbs sampler
framework of Johnson et al
The one complication is that because we use a
Metropolis-Hastings procedure to generate sam-
ples from P(? | t, ?) in the renormalization ap-
proach, we use the Metropolis-within-Gibbs pro-
cedure given in Algorithm 4 (Robert and Casella,
2004).
8 The expressive power of the three
approaches
Probably the most important question to ask with
respect to the three different approaches to non-
tightness is whether they differ in terms of expres-
sive power. Clearly the three approaches differ in
terms of the grammars they admit (the only-tight
approach requires the prior to only assign non-zero
probability to tight PCFGs, while the other two ap-
proaches permit the prior to assign non-zero prob-
ability to non-tight PCFGs as well). However, if
we regard a grammar as merely a device for defin-
ing a distribution over trees and a prior as defining
a distribution over distributions over trees, it is rea-
sonable to ask whether the class of distributions
over distributions of trees that each of these ap-
proaches define are the same or differ. We believe,
but have not proved, that all three approaches de-
fine the same class of distributions over distribu-
tions of trees in the following sense: any prior used
with one of the approaches can be transformed
into a different prior that can be used with one of
the other approaches, and yield the same posterior
over trees conditioned on a string, marginalizing
out the parameters.
This does not mean that the three approaches
are equivalent, however. In this section we pro-
vide a grammar such that with a uniform prior over
rule probabilities, the conditional distribution over
trees given a fixed string varies under each of the
three different approaches.
The grammar we consider has three rules S ?
S S S|S S|a with probabilities ?1, ?2 and 1? ?1?
?2, respectively. The ? parameters are required to
satisfy ?1 + ?2 ? 1 and ?i ? 0 for i = 1, 2.
We compute the posterior distribution over
parse trees for the string w = a a a. The gram-
mar generates three parse trees for w1, namely:
t1 = S
S
a
S
a
S
a
t2 = S
S
a
S
S
a
S
a
t3 = S
S
S
a
S
a
S
a
The partition function Z for this grammar is the
smallest positive root of the cubic equation:
Z = ?1Z3 + ?2Z2 + (1? ?1 ? ?2)
We used Mathematica to find an analytic solution
for Z in this equation, obtaining not only an ex-
pression for the partition function Z(?) but also
identifying the non-tight region ??.
In order to compute P(t1|w), we used Mathe-
matica to first compute the following quantities:
qsinkElement(ti) =
?
?
??(ti) d?
qtightOnly(ti) =
?
?
??(ti) I(? /? ??) d?
qrenormalization(ti) =
?
?
??(ti)/Z(?) d?
where i ? {1, 2, 3}. We used Mathematica to ana-
lytically compute q(ti) for each approach and each
i ? {1, 2, 3}. Then it?s easy to show that:
P(ti | w) =
q(ti)?3
i?=1 q(ti?)
where the q used is based on the approach to
tightness desired. For the sink-element approach,
1039
010
20
30
0.35 0.40 0.45 0.50 0.55Average f?score
De
nsi
ty
Inference
only?tight
sink?state
renormalise
Figure 1: The density of the F1-scores with the
three approaches. The prior used is a symmetric
Dirichlet with ? = 0.1.
P(t1|w) = 711 ? 0.636364. For the only-tightapproach P(t1|w) = 1117917221 ? 0.649149. Forthe renormalization approach the analytic ex-
pression is too complex to include in this paper,
but it approximately equals 0.619893. A log
of our Mathematica calculations is available
at http://www.cs.columbia.edu/?scohen/
acl13tightness-mathematica.pdf, and we
confirmed these results to three decimal places us-
ing the samplers described above (which required
107 samples per approach).
While the differences between these conditional
probabilities are not great, the conditional prob-
abilities are clearly different, so the three ap-
proaches do in fact define different distributions
over trees under a uniform prior on rule probabili-
ties.
9 Empirical effects of the three
approaches in unsupervised grammar
induction
In this section we present experiments using the
three samplers just described in an unsupervised
grammar induction problem. Our goal here is
not to improve the state-of-the-art in unsupervised
grammar induction, but to try to measure empir-
ical differences in the estimates produced by the
three different approaches to tightness just de-
scribed. The bottom line of our experiments is that
we could not detect any significant difference in
the estimates produced by samplers for these three
different approaches.
In our experiments we used the English Penn
treebank (Marcus et al, 1993). We use the part-
of-speech tag sequences of sentences shorter than
11 words in sections 2?21. The grammar we use is
the PCFG version of the dependency model with
valence (Klein and Manning, 2004), as it appears
in Smith (2006).
We used a symmetric Dirichlet prior with hy-
perparameter ? = 0.1. For each of the three ap-
proaches for handling tightness, we ran 100 times
the samplers in ?7, each for 1,000 iterations. We
discarded the first 900 sweeps of each run, and cal-
culated the F1-scores of the sampled trees every
10th sweep from the last 100 sweeps. For each
run we calculated the average F1-score over the
10 sweeps we evaluated. We thus have 100 aver-
age F1-scores for each of the samplers.
Figure 1 plots the density of F1 scores (com-
pared to the gold standard) resulting from the
Gibbs sampler, using all three approaches. The
mean value for each of the approaches is 0.41
with standard deviation 0.06 (only-tight), 0.41
with standard deviation 0.05 (renormalization)
and 0.42 with standard deviation 0.06 (sink ele-
ment). In addition, the only-tight approach results
in an average of 437 (s.d., 142) rejected propos-
als in 1,000 samples, while the renormalization
approach results in an average of 232 (s.d., 114)
rejected proposals in 1,000 samples. (It?s not sur-
prising that the only-tight approach results in more
rejections as it keeps proposing new ? until a tight
proposal is found, while the renormalization ap-
proach simply uses the old ?).
We performed two-sample Kolmogorov-
Smirnov tests (which are non-parametric tests
designed to determine if two distributions are
different; see DeGroot, 1991) on each of the three
pairs of 100 F1-scores. None of the tests were
close to significant; the p-values were all above
0.5. Thus our experiments provided no evidence
that the samplers produced different distributions
over trees, although it?s reasonable to expect that
these distributions do indeed differ.
In terms of running time, our implementation
of the renormalization approach was several times
slower than our implementations of the other two
approaches because we used the naive fixed-point
algorithm to compute the partition function: per-
haps this could be improved using one of the
more sophisticated partition function algorithms
described in Nederhof and Satta (2008).
1040
10 Conclusion
In this paper we characterized the notion of an al-
most everywhere tight grammar in the Bayesian
setting and showed it holds for linear CFGs. For
non-linear CFGs, we described three different ap-
proaches to handle non-tightness. The ?only-
tight? approach restricts attention to tight PCFGs,
and perhaps surprisingly, we showed that conju-
gacy still obtains when the domain of a product
of Dirichlets prior is restricted to the subset of
tight grammars. The renormalization approach in-
volves renormalizing the PCFG measure ? over
trees when the grammar is non-tight, which de-
stroys conjugacy with a product of Dirichlets prior.
Perhaps most surprisingly of all, the sink-element
approach, which assigns the missing mass in non-
tight PCFG to a sink element ?, turns out to be
equivalent to existing practice where tightness is
ignored.
We studied the posterior distributions over trees
induced by the three approaches under a uniform
prior for a simple grammar and showed that they
differ. We leave for future work the important
question of whether the classes of distributions
over distributions over trees that the three ap-
proaches define are the same or different.
We described samplers for the supervised
and unsupervised settings for each of these ap-
proaches, and applied them to an unsupervised
grammar induction problem. (The code for the
unsupervised samplers is available from http://
web.science.mq.edu.au/?mjohnson).
We could not detect any difference in the pos-
terior distributions over trees produced by these
samplers, despite devoting considerable computa-
tional resources to the problem. This suggests that
for these kinds of problems at least, tightness is
not of practical concern for Bayesian inference of
PCFGs.
Acknowledgements
We thank the anonymous reviewers and Gior-
gio Satta for their valuable comments. Shay
Cohen was supported by the National Science
Foundation under Grant #1136996 to the Com-
puting Research Association for the CIFellows
Project, and Mark Johnson was supported by the
Australian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
References
K. B. Atherya and P. E. Ney. 1972. Branching Pro-
cesses. Dover Publications.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. Language and Information: Selected Essays
on Their Theory and Application, pages 116?150.
T. L. Booth and R. A. Thompson. 1973. Applying
probability measures to abstract languages. IEEE
Transactions on Computers, C-22:442?450.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguis-
tics, 24(2):299?305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
S. B. Cohen and N. A. Smith. 2012. Empirical risk
minimization for probabilistic grammars: Sample
complexity and hardness of learning. Computa-
tional Linguistics, 38(3):479?526.
M. H. DeGroot. 1991. Probability and Statistics (3rd
edition). Addison-Wesley.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proceedings of ACL.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In 8th In-
ternational Colloquium on Grammatical Inference.
K. Lari and S.J. Young. 1990. The estimation of
Stochastic Context-Free Grammars using the Inside-
Outside algorithm. Computer Speech and Lan-
guage, 4(35-56).
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
M.-J. Nederhof and G. Satta. 2008. Computing par-
tition functions of PCFGs. Research on Language
and Computation, 6(2):139?162.
C. P. Robert and G. Casella. 2004. Monte Carlo Sta-
tistical Methods. Springer-Verlag New York.
N. A. Smith. 2006. Novel Estimation Methods for Un-
supervised Discovery of Latent Structure in Natural
Language Text. Ph.D. thesis, Johns Hopkins Univer-
sity.
C. S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Surveys,
12:361?379.
1041
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644?654,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Lexical Inference over Multi-Word Predicates: A Distributional Approach
Omri Abend Shay B. Cohen Mark Steedman
School of Informatics, University of Edinburgh,
Edinburgh EH8 9AB, United Kingdom
{oabend,scohen,steedman}@inf.ed.ac.uk
Abstract
Representing predicates in terms of their
argument distribution is common practice
in NLP. Multi-word predicates (MWPs) in
this context are often either disregarded or
considered as fixed expressions. The lat-
ter treatment is unsatisfactory in two ways:
(1) identifying MWPs is notoriously diffi-
cult, (2) MWPs show varying degrees of
compositionality and could benefit from
taking into account the identity of their
component parts. We propose a novel
approach that integrates the distributional
representation of multiple sub-sets of the
MWP?s words. We assume a latent distri-
bution over sub-sets of the MWP, and esti-
mate it relative to a downstream prediction
task. Focusing on the supervised identi-
fication of lexical inference relations, we
compare against state-of-the-art baselines
that consider a single sub-set of an MWP,
obtaining substantial improvements. To
our knowledge, this is the first work to
address lexical relations between MWPs
of varying degrees of compositionality
within distributional semantics.
1 Introduction
Multi-word expressions (MWEs) constitute a
large part of the lexicon and account for much
of its growth (Jackendoff, 2002; Seaton and
Macaulay, 2002). However, despite their impor-
tance, MWEs remain difficult to define and model,
and consequently pose serious difficulties for NLP
applications (Sag et al, 2001). Multi-word Predi-
cates (MWPs; sometimes termed Complex Predi-
cates) form an important and much addressed sub-
class of MWEs and are the focus of this paper.
MWPs are informally defined as multiple words
that constitute a single predicate (Alsina et al,
1997). MWPs encompass a wide range of phe-
nomena, including causatives, light verbs, phrasal
verbs, serial verb constructions and many others,
and pose considerable challenges to both linguistic
theory and NLP applications (see Section 2). Part
of the difficulty in treating them stems from their
position on the borderline between syntax and the
lexicon. It is therefore often unclear whether they
should be treated as fixed expressions, as compo-
sitional phrases that reflect the properties of their
component parts or as both.
This work addresses the modelling of MWPs
within the context of distributional semantics (Tur-
ney and Pantel, 2010), in which predicates are
represented through the distribution of arguments
they may take. In order to collect meaningful
statistics, the predicate?s lexical unit should be suf-
ficiently frequent and semantically unambiguous.
MWPs pose a challenge to such models, as
na??vely collecting statistics over all instances of
highly ambiguous verbs is likely to result in noisy
representations. For instance, the verb ?take? may
appear in MWPs as varied as ?take time?, ?take
effect? and ?take to the hills?. This heterogene-
ity of ?take? is likely to have a negative effect on
downstream systems that use its distributional rep-
resentation. For instance, while ?take? and ?ac-
cept? are often considered lexically similar, the
high frequency in which ?take? participates in
non-compositional MWPs is likely to push the two
verbs? distributional representations apart.
A straightforward approach to this problem is
to represent the predicate as a conjunction of mul-
tiple words, thereby trading ambiguity for spar-
sity. For instance, the verb ?take? could be con-
joined with its object (e.g., ?take care?, ?take a
bus?). This approach, however, raises the chal-
lenge of identifying the sub-set of the predicate?s
words that should be taken to represent it (hence-
forth, its lexical components or LCs).
We propose a novel approach that addresses this
644
challenge in the context of identifying lexical in-
ference relations between predicates (Lin and Pan-
tel, 2001; Schoenmackers et al, 2010; Melamud et
al., 2013a, inter alia). A (lexical) inference rela-
tion p
L
? p
R
is said to hold if the relation denoted
by p
R
generally holds between a set of arguments
whenever the relation p
L
does. For instance, an in-
ference relation holds between ?annex? and ?con-
trol? since if a country annexes another, it gener-
ally controls it. Most works to this task use dis-
tributional similarity, either as their main compo-
nent (Szpektor and Dagan, 2008; Melamud et al,
2013b), or as part of a more comprehensive system
(Berant et al, 2011; Lewis and Steedman, 2013).
For example, consider the verb ?take?. While
the inference relation ?have? take? does not gen-
erally hold, it does hold in the case of some light
verbs, such as ?have a look? take a look?, under-
scoring the importance of taking more inclusive
LCs into account. On the other hand, the pred-
icate ?likely to give a green light? is unlikely to
appear often even within a very large corpus, and
could benefit from taking its lexical sub-units (e.g.,
?likely? or ?give a green light?) into account.
We present a novel approach to the task that
models the selection and relative weighting of the
predicate?s LCs using latent variables. This ap-
proach allows the classifier that uses the distri-
butional representations to take into account the
most relevant LCs in order to make the predic-
tion. By doing so, we avoid the notoriously dif-
ficult problem of defining and identifying MWPs
and account for predicates of various sizes and de-
grees of compositionality. To our knowledge, this
is the first work to address lexical relations be-
tween MWPs of varying degrees of composition-
ality within distributional semantics.
We conduct experiments on the dataset of Ze-
ichner et al (2012) and compare our methods with
analogous ones that select a fixed LC, using state-
of-the-art feature sets. Our method obtains sub-
stantial performance gains across all scenarios.
Finally, we note that our approach is cognitively
appealing. Significant cognitive findings support
the claim that a speaker?s lexicon consists of par-
tially overlapping lexical units of various sizes, of
which several can be evoked in the interpretation
of an utterance (Jackendoff, 2002; Wray, 2008).
2 Background and Related Work
Inference Relations. The detection of inference
relations between predicates has become a central
task over the past few years (Sekine, 2005; Zan-
zotto et al, 2006; Schoenmackers et al, 2010;
Berant et al, 2011; Melamud et al, 2013a, in-
ter alia). Inference rules are used in a wide va-
riety of applications including Question Answer-
ing (Ravichandran and Hovy, 2002), Information
Extraction (Shinyama and Sekine, 2006), and as
a main component in Textual Entailment systems
(Dinu and Wang, 2009; Dagan et al, 2013).
Most approaches to the task used distributional
similarity as a major component within their sys-
tem. Lin and Pantel (2001) introduced DIRT, an
unsupervised distributional system for detecting
inference relations. The system is still considered
a state-of-the-art baseline (Melamud et al, 2013a),
and is often used as a component within larger sys-
tems. Schoenmackers et al (2010) presented an
unsupervised system for learning inference rules
directly from open-domain web data. Melamud
et al (2013a) used topic models to combine type-
level predicate inference rules with token-level in-
formation from their arguments in a specific con-
text. Melamud et al (2013b) used lexical expan-
sion to improve the representation of infrequent
predicates. Lewis and Steedman (2013) combined
distributional and symbolic representations, eval-
uating on a Question Answering task, as well as
on a quantification-focused entailment dataset.
Several studies tackled the task using super-
vised systems. Weisman et al (2012) used a set
of linguistically motivated features, but evaluated
their system on a corpus that consists almost en-
tirely of single-word predicates. Mirkin et al
(2006) presented a system for learning inference
rules between nouns, using distributional similar-
ity and pattern-based features. Hagiwara et al
(2009) identified synonyms using a supervised ap-
proach relying on distributional and syntactic fea-
tures. Berant et al (2011) used distributional simi-
larity between predicates to weight the edges of an
entailment graph. By imposing global constraints
on the structure of the graph, they obtained a more
accurate set of inference rules.
Previous work used simple methods to select
the predicate?s LC. Some filtered out frequent
highly ambiguous verbs (Lewis and Steedman,
2013), others selected a single representative word
(Melamud et al, 2013a), while yet others used
multi-word LCs but treated them as fixed expres-
sions (Lin and Pantel, 2001; Berant et al, 2011).
The goals of the above studies are largely com-
645
plementary to ours. While previous work focused
either on improving the quality of the distribu-
tional representations themselves or on their incor-
poration into more elaborate systems, we focus on
the integration of the distributional representation
of multiple LCs to improve the identification of
inference relations between MWPs.
MWP Extraction and Identification. MWPs
have received considerable attention over the years
in both theoretical and applicative contexts. Their
position on the crossroads of syntax and the lexi-
con, their varying degrees of compositionality, as
well as the wealth of linguistic phenomena they
exhibit, made them the object of ongoing linguis-
tic discussion (Alsina et al, 1997; Butt, 2010).
In NLP, the discovery and identification of
MWEs in general and MWPs in particular has
been the focus of much work over the years
(Lin, 1999; Baldwin et al, 2003; Biemann and
Giesbrecht, 2011). Despite wide interest, the
field has yet to converge to a general and widely
agreed-upon method for identifying MWPs. See
(Ramisch et al, 2013) for an overview.
Most work on MWEs emphasized idiosyncratic
or non-compositional expressions. Other lines of
work focused on specific MWP classes such as
light verbs (Tu and Roth, 2011; Vincze et al,
2013) and phrasal verbs (McCarthy et al, 2003;
Pichotta and DeNero, 2013). Our work proposes a
uniform treatment to MWPs of varying degrees of
compositionality, and avoids defining MWPs ex-
plicitly by modelling their LCs as latent variables.
Compositional Distributional Semantics.
Much work in recent years has concentrated on
the relation between the distributional representa-
tions of composite phrases and the representations
of their component sub-parts (Widdows, 2008;
Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010; Coecke et al, 2010). Several works
have used compositional distributional semantics
(CDS) representations to assess the composition-
ality of MWEs, such as noun compounds (Reddy
et al, 2011) or verb-noun combinations (Kiela
and Clark, 2013). Despite significant advances,
previous work has mostly been concerned with
highly compositional cases and does not address
the distributional representation of predicates of
varying degrees of compositionality.
3 Our Proposal: A Latent LC Approach
This section details our approach for distribu-
tionally representing MWPs by leveraging their
component LCs. Section 3.1 describes our gen-
eral approach, Section 3.2 presents our model and
Section 3.3 details the feature set.
3.1 General Approach and Notation
We propose a method for addressing MWPs of
varying degrees of compositionality through the
integration of the distributional representation of
multiple sub-sets of the predicate?s words (LCs).
We use it to tackle a supervised prediction task that
represents predicates distributionally. Our model
assumes a latent distribution over the LCs, and es-
timates its parameters so to best conform to the
goals of the target prediction task.
Formally, given a predicate p, we denote the set
of words comprising it as W (p). The set of al-
lowable LCs for p is denoted with H
p
? 2
W (p)
.
H
p
contains all sub-sets of p that we consider as
apriori possible to represent p. For instance, if p is
?likely to give a green light?, H
p
may include LCs
such as ?likely? or ?give light?. As our method is
aimed at discovering the most relevant LCs, we do
not attempt to analyze the MWPs in advance, but
rather take an inclusive H
p
, allowing the model to
estimate the relative weights of the LCs.
The task we use as a testbed for our approach
is the lexical inference identification task between
predicates. Given a pair of predicates p =
(p
L
, p
R
), the task is to predict whether an infer-
ence relation holds between them. For instance, if
p
L
is ?devour? and p
R
is ?eat greedily?, the clas-
sifier should use the similarity between ?devour?
and ?eat? in order to correctly predict an infer-
ence relation in this case. Selecting the wider LC
?eat greedily? might result in sparser statistics. In
other examples, however, taking a wider LC is po-
tentially beneficial. For instance, the dissimilar-
ity between ?take? and ?make? should not prevent
the classifier from identifying the inference rela-
tion between ?take a step? and ?make a step?.
Our statistical model aims at predicting the cor-
rect label by making use of partially overlapping
LCs of various sizes, both for the premise left-
hand side (LHS) predicate p
L
and the hypothesis
right-hand side (RHS) predicate p
R
. More for-
mally, we take the space of values for our latent
LC variables to be H
p
L
,p
R
= H
p
L
?H
p
R
.
Our evaluation dataset consists of pairs p
(i)
=
(p
(i)
L
, p
(i)
R
) for i ? {1, . . . ,M}, where M is the
number of examples available, coupled with their
gold-standard labels y
(i)
? {1,?1}. For brevity,
we denote H
(i)
= H
p
(i)
= H
p
(i)
L
,p
(i)
R
. We also as-
646
sume the existence of a feature function ?(p, y, h)
which maps a triplet of a predicate pair p, an infer-
ence label y, and a latent state h ? H
p
to R
d
for
some integer d. We denote the training set by D.
3.2 The Model
We address the task with a latent variable log-
linear model, representing the LCs of the predi-
cates. We choose this model for its generality, con-
ceptual simplicity, and because it allows to easily
incorporate various feature sets and sets of latent
variables. We introduce L
2
regularization to avoid
over-fitting. We use maximum likelihood estima-
tion, and arrive at the following objective function:
L(w|D) =
1
M
M
X
i=1
logP (y
(i)
|p
(i)
, w)?
?
2
?w?
2
=
=
1
n
n
X
i=1
0
@
log
X
h?H
(i)
exp
?
w
>
?(p
(i)
, y
(i)
, h)
?
? logZ(w, i)
?
?
?
2
?w?
2
where:
Z(w, i) =
X
y?{?1,1}
X
h?H
i
exp(w
>
?(p
i
, y, h)).
We maximizeL using the BFGS algorithm (No-
cedal and Wright, 1999). The gradient (with re-
spect to w) is the following:
?L = E
h
[?(p
i
, y
i
, h)]? E
h,y
[?(p
i
, y, h)]? ? ? w
H
p
can be defined to be any sub-set of 2
W (p)
given that taking an expectation over H can be
done efficiently. It is therefore possible to use prior
linguistic knowledge to consider only sub-sets of p
that are likely to be non-compositional (e.g., verb-
preposition or verb-noun pairs).
In our experiments we attempt to keep the ap-
proach maximally general, and defineH
p
to be the
set of all subsets of size 1 or 2 of content words in
W
p
1
. We bound the size of h ? H
p
in order to re-
tain computational efficiency and a sufficient fre-
quency of the LCs in H
p
. MWPs of length greater
than 2 are effectively approximated by their set of
subsets of sizes 1 and 2.
Each h can therefore be written as a 4-tuple
(h
A
L
, h
B
L
, h
A
R
, h
B
R
), where h
A
L
(h
A
R
) denotes the first
word of the LHS (RHS) predicate?s LC. h
B
L
(h
B
R
)
denotes the (possibly empty) second word of the
predicate. Inference is carried out by maximizing
P (y|p
(i)
) over y. As |H
p
| = O(k
4
), where k is the
1
We use a POS tagger to identify content words. Preposi-
tions are considered content words under this definition.
number of content words in p, and as the number
of content words is usually small
2
, inference can
be carried out by directly summing over H
(i)
.
Initialization. The introduction of latent vari-
ables into the log-linear model leads to a non-
convex objective function. Consequently, BFGS
is not guaranteed to converge to the global opti-
mum, but rather to a stationary point. The result
may therefore depend on the parameter initializa-
tion. Indeed, preliminary experiments showed that
both initializing w to be zero and using a random
initializer results in lower performance.
Instead, we initialize our model with a simpli-
fied convex model that fixes the LCs to be the
pair of left-most content words comprising each
of the predicates. This is a common method for
selecting the predicate?s LC (e.g., Melamud et al,
2013a). Once h has been fixed, the model col-
lapses to a convex log-linear model. The optimal
w is then taken as an initialization point for the la-
tent variable model. While this method may still
not converge to the global maximum, our experi-
ments show that this initialization technique yields
high quality values for w (see Section 6).
3.3 Feature Set
This section lists the features used for our exper-
iments. We intentionally select a feature set that
relies on either completely unsupervised or shal-
low processing tools that are available for a wide
variety of languages and domains.
Given a predicate pair p
(i)
, a label y ? {1,?1}
and a latent state h ? H
(i)
, we define their feature
vector as ?(p
(i)
, y, h) = y ? ?(p
(i)
, h). The com-
putation of ?(p
(i)
, h) requires a reference corpus
R that contains triplets of the type (p, x, y) where
p is a binary predicate and x and y are its argu-
ments. We use the Reverb corpus as R in our ex-
periments (Fader et al, 2011; see Section 4). We
refrain from encoding features that directly reflect
the vocabulary of the training set. Such features
are not applicable beyond that set?s vocabulary,
and as available datasets contain no more than a
few thousand examples, these features are unlikely
to generalize well.
Table 1 presents the set of features we use in our
experiments. The features can be divided into two
main categories: similarity features between the
LHS and the RHS predicates (table?s top), and fea-
tures that reflect the individual properties of each
2
|H
p
| is about 15 on average in our dataset, where less
than 5% of the H
(i)
are of size greater than 50.
647
C
a
t
e
g
o
r
y
Name Description
S
i
m
i
l
a
r
i
t
y
COSINE DIRT cosine similarity between the vectors of h
L
and h
R
COSINE
A
DIRT cosine similarity between the vectors of h
A
L
and h
A
R
BInc DIRT BInc similarity between the vectors of h
L
and h
R
BInc
A
DIRT BInc similarity between the vectors of h
A
L
and h
A
R
W
o
r
d
A
L
H
S
POS
A
L
The most frequent POS tag for the lemma of h
A
L
POS2
A
L
The second most frequent POS tag for the word lemma of h
A
L
FREQ
A
L
The number of occurrences of h
A
L
in the reference corpus
COMMON
A
L
A binary feature indicating whether h
A
L
appears in both predicates
ORDINAL
A
L
The ordinal number of h
A
L
among the content words of the LHS predicate
P
a
i
r
L
H
S
POS
AB
L
The conjunction of POS
A
L
and POS
B
L
FREQ
AB
L
The frequency of h
A
L
and h
B
L
in the reference corpus
PREFAB
L
P (h
A
L
|h
A
L
) as estimated from the reference corpus
PREFBA
L
P (h
B
L
|h
A
L
) as estimated from the reference corpus
PMIAB
L
The point-wise mutual information of h
A
L
and h
B
L
L
D
A
TOPICS
L
P (topic|h
L
) for each of the induced topics.
TOPICENT
L
The entropy of the topic distribution P (topic|h
L
)
Table 1: The feature set used in our experiments. The top part presents the similarity measures based on the DIRT approach.
The rest of the listed features apply to the LHS predicate (h
L
), and to the first word in it (h
A
L
). Analogous features are
introduced for the second word, h
B
L
, and for the RHS predicate. The upper-middle part presents the word features for h
A
L
. The
lower-middle part presents features that apply where h
L
is of size 2. The bottom part lists the LDA-based features.
of them. Within the LHS feature set, we distin-
guish between two sub-types of features: word
features that encode the individual properties of
h
A
L
and h
B
L
(table?s upper middle part), and pair
features that only apply to LCs of size 2 and re-
flect the relation between h
A
L
and h
B
L
(table?s lower
middle part). We further incorporate LDA-based
features that reflect the selectional preferences of
the predicates (table?s bottom).
Distributional Similarity Features. The distri-
butional similarity features are based on the DIRT
system (Lin and Pantel, 2001). The score defines
for each predicate p and for each argument slot
s ? {L,R} (corresponding to the arguments to the
right and left of that predicate) a vector v
p
s
which
represents the distribution of arguments appearing
in that slot. We take v
p
s
(x) to be the number of
times that the argument x appeared in the slot s of
the predicate p. Given these vectors, the similarity
between the predicates p
1
and p
2
is defined as:
score(p
1
, p
2
) =
q
sim(v
p
1
L
, v
p
2
L
) ? sim(v
p
1
R
, v
p
2
R
)
where sim is some vector similarity measure.
We use two common similarity measures: the
vector cosine metric, and the BInc (Szpektor and
Dagan, 2008) similarity measure. These measures
give complementary perspectives on the similar-
ity between the predicates, as the cosine similar-
ity is symmetric between the LHS and RHS predi-
cates, while BInc takes into account the direction-
ality of the inference relation. Preliminary exper-
iments with other measures, such as those of Lin
(1998) and Weeds and Weir (2003) did not yield
additional improvements.
We encode the similarity of all measures for the
pair h
L
and h
R
as well as the pair h
A
L
and h
A
R
. The
latter feature is an approximation to the similar-
ity between the heads of the predicates, as heads
in English tend to be to the left of the predicates.
These two features coincide for h values of size 1.
Word and Pair Features. These features en-
code the basic properties of the LC. The motiva-
tion behind them is to allow a more accurate lever-
aging of the similarity features, as well as to better
determine the relative weights of h ? H
(i)
.
The feature set is composed of four analogous
sets corresponding to h
A
L
,h
B
L
,h
A
R
and h
B
R
, as well
as two sets of features that capture relations be-
tween h
A
L
, h
B
L
and h
A
R
, h
B
R
(in cases h is of size 2).
The features include the ordinal index of the word
within the predicate, the lemma?s frequency ac-
cording to R, and a feature that indicates whether
that word?s lemma also appears in both predicates
of the pair. For instance, when considering the
predicates ?likely to come? and ?likely to leave?,
?likely? appears in both predicates, while ?come?
and ?leave? appear only in one of them.
In addition, we use POS-based features that
encode the most frequent POS tag for the word
lemma and the second most frequent POS tag (ac-
cording toR). Information about the second most
frequent POS tag can be important in identifying
light verb constructions, such as ?take a swim? or
?give a smile?, where the object is derived from a
verb. It can thus be interpreted as a generalization
648
of the feature that indicates whether the object is
a deverbal noun, which is used by some light verb
identification algorithms (Tu and Roth, 2011).
In cases where h
L
is of size 2, we additionally
encode features that apply to the conjunction of
h
A
L
and h
B
L
. We encode the conjunction of their
POS and the number of times the two lemmas oc-
curred together in R. We also introduce features
that capture the statistical correlation between the
words of h
L
. To do so, we use point-wise mu-
tual information, and the conditional probabili-
ties P (h
A
L
|h
B
L
) and P (h
B
L
|h
A
L
). Similar measures
have often been used for the unsupervised detec-
tion of MWEs (Villavicencio et al, 2007; Fazly
and Stevenson, 2006). We also include the analo-
gous set of features for h
R
.
LDA-based Features. We further incorporate
features based on a Latent Dirichlet Allocation
(LDA) topic model (Blei et al, 2003). Several
recent works have underscored the usefulness of
using topic models to model a predicate?s selec-
tional preferences (Ritter et al, 2010; Dinu and
Lapata, 2010; S?eaghdha, 2010; Lewis and Steed-
man, 2013; Melamud et al, 2013a). We adopt the
approach of Lewis and Steedman (2013), and de-
fine a pseudo-document for each LC in the evalu-
ation corpus. We populate the pseudo-documents
of an LC with its arguments according to R. We
then train an LDA model with 25 topics over these
documents. This yields a probability distribution
P (topic|h) for each LC h, reflecting the types of
arguments h may take.
We further include a feature for the entropy of
the topic distribution of the predicate, which re-
flects its heterogeneity. This feature is motivated
by the assumption that a heterogeneous predicate
is more likely to benefit from selecting a more in-
clusive LC than a homogeneous one.
Technical Issues. All features used, except the
similarity ones and the topic distribution features
are binary. Frequency features are binned into 4
bins of equal frequency. We conjoin some of the
feature sets by multiplying their values. Specifi-
cally, we add the cross product of the features of
the category ?Similarity? (see Table 1) with the
rest of the features. In addition, we conjoin all
LHS (RHS) features with an indicator feature that
indicates whether h
L
(h
R
) is of size two. This re-
sults in 1605 non-constant features.
We further note that some LCs that appear in the
evaluation corpus do not appear at all inR. In our
experiments they amounted to 0.2% of the LCs in
our evaluation dataset. While previous work of-
ten discarded predicates below a certain frequency
from the evaluation, we include them in order to
facilitate comparison to future work. We assign
the similarity features of such examples a 0 value,
and assign their other numerical features the mean
value of those features.
4 Experimental Setup
Corpora and Preprocessing. As a reference
corpus R, we use Reverb (Fader et al, 2011), a
web-based corpus consisting of 15M web extrac-
tions of binary relations. Each relation is a triplet
of a predicate and two arguments, one preceding it
and one following it. Relations were extracted us-
ing regular expressions over the output of a POS
tagger and an NP chunker. Each predicate may
consist of a single verb, a verb and a preposi-
tion or a sequence of words starting in a verb and
ending in a preposition, between which there may
nouns, adjectives, adverbs, pronouns, determiners
and verbs. The verb may also be a copula. Exam-
ples of predicates are ?make the most of?, ?could
be exchanged for? and ?is happy with?.
Reverb is an appealing reference corpus for this
task for several reasons. First, it uses fairly shal-
low preprocessing technology which is available
for many domains and languages. Second, Reverb
applies considerable noise filtering, which results
in extractions of fair quality. Third, our evaluation
dataset is based on Reverb extractions.
We evaluate our algorithm on the dataset of
Zeichner et al (2012). This publicly available
corpus
3
provides pairs of Reverb binary relations
and an indication of whether an inference rela-
tion holds between them within the context of
a specific pair of argument fillers. The corpus
was compiled using distributional methods to de-
tect pairs of relations in Reverb that are likely
to have an inference relation between. Annota-
tors, employed through Amazon Mechanical Turk,
were then asked to determine whether each pair
is meaningful, and if so, to determine whether an
inference relation holds. Further measures were
taken to monitor the accuracy of the annotation.
For example, the pair of predicates ?make the
most of? and ?take advantage of? appears in the
corpus as a pair between which an inference rela-
tion holds. The arguments in this case are ?stu-
dents? and ?their university experience?. An ex-
3
http://tinyurl.com/krx2acd
649
ample of a pair between which an inference rela-
tion does not hold is ?tend to neglect? and ?under-
estimate the importance of?, where the arguments
are ?Robert? and ?his family?.
The dataset contains 6,565 instances in total.
We use 5,411 pairs of them, discarding instances
that were deemed as meaningless by the annota-
tors. We also discard cases where the set of ar-
guments is reversed between the LHS and RHS
predicates. In these examples, p
R
(x, y) is infer-
able from p
L
(y, x), rather than from p
L
(x, y). As
there are less than 150 reversed instances in the
corpus, experimenting on this sub-set is unlikely
to be informative.
The average length of a predicate in the cor-
pus is 2.7 words (including function words). In
87.3% of the predicate pairs, there was more than
one LC (i.e., |H
p
| > 1), underscoring the im-
portance of correctly leveraging the different LCs.
We randomly partition the corpus into a training
set which contains 4,343 instances (?80%), and a
test set that contains 1,068 instances, maintaining
the same positive to negative label ratio in both
datasets
4
. Development was carried out using
cross-validation on the training data (see below).
We use a Maximum Entropy POS Tagger,
trained on the Penn Treebank, and the WordNet
lemmatizer, both implemented within the NLTK
package (Loper and Bird, 2002). To obtain a
coarse-grained set of POS tags, we collapse the
tag set to 7 categories: nouns, verbs, adjectives,
adverbs, prepositions, the word ?to? and a cate-
gory that includes all other words. A Reverb argu-
ment is represented as the conjunction of its con-
tent words that appear more than 10 times in the
corpus. Function words are defined according to
their POS tags and include determiners, possessive
pronouns, existential ?there?, numbers and coordi-
nating conjunctions. Auxiliary verbs and copulas
are also considered function words.
To compute the LDA features, we use the on-
line variational Bayes algorithm of (Hoffman et
al., 2010) as implemented in the Gensim software
package (Rehurek and Sojka, 2010).
Evaluated Algorithms. The only two previous
works on this dataset (Melamud et al, 2013a;
Melamud et al, 2013b) are not directly compara-
ble, as they used unsupervised systems and evalu-
4
A script that replicates our train-test partition of the cor-
pus can be found here: http://homepages.inf.ed.
ac.uk/oabend/mwpreds.html
ated on sub-sets of the evaluation dataset. Instead,
we use several baselines to demonstrate the use-
fulness of integrating multiple LCs, as well as the
relative usefulness of our feature sets.
The simplest baseline is ALLNEG, which pre-
dicts the most frequent label in the dataset (in our
case: ?no inference?). The other evaluated sys-
tems are formed by taking various subsets of our
feature set. We experiment with 4 feature sets. The
smallest set, SIM, includes only the similarity fea-
tures. This feature set is related to the composi-
tional distributional model of Mitchell and Lap-
ata (2010) (see Section 6). We note that despite
recent advances in identifying predicate inference
relations, the DIRT system (Lin and Pantel, 2001)
remains a strong baseline, and is often used as a
component in state-of-the-art systems (Berant et
al., 2011), and specifically in the two aforemen-
tioned works that used the same evaluation corpus.
The next feature set BASIC includes the features
found to be most useful during the development
of the model: the most frequent POS tag, the fre-
quency features and the feature Common. More
inclusive is the feature set NO-LDA, which in-
cludes all features except the LDA features. Ex-
periments with this set were performed in order
to isolate the effect of the LDA features. Finally,
ALL includes our complete set of features.
The more direct comparison is against partial
implementations of our system where the LC h is
deterministically selected. Determining h for each
predicate yields a regular log-linear binary classi-
fication model. We use two variants of this base-
line. The first, LEFTMOST, selects the left-most
content word for each predicate. Similar selec-
tion strategy was carried out by Melamud et al
(2013a). The second, VPREP, selects h to be the
verb along with its following preposition. In cases
the predicate contains multiple verbs, the one pre-
ceding the preposition is selected, and where the
predicate does not contain any non-copula verbs,
it regresses to LEFTMOST. This LC selection
method approximates a baseline that includes sub-
categorized prepositions. Such cases are highly
frequent and account for a large portion of the
MWPs in English. Including a verb?s preposition
in its LC was commonly done in previous work
(e.g., Lewis and Steedman, 2013).
We also attempted to identify verb-preposition
constructions using a dependency parser. Unfor-
tunately, our evaluation dataset is only available in
650
a lemmatized version, which posed a difficulty for
the parser. Due to the low quality of the resulting
parses, we implemented VPREP using POS-based
regular expressions as defined above.
The full model is denoted with LATENTLC. For
each system and feature set, we report results us-
ing 10-fold cross-validation on the training set, as
well as results on the test set. Both cases use
the same set of parameters determined by cross-
validation on the training set. As the task at hand
is a binary classification problem, we use accuracy
scores to rate the performance of our systems.
5 Results
Table 2 presents the results of our experi-
ments. Rows correspond to the evaluated algo-
rithms, while columns correspond to the feature
sets used and the evaluation scenarios (i.e., train-
ing set cross-validation or test set evaluation). Our
experiments make first use of this dataset in its
fullest form for the problem of supervised learning
of inference relations, and may serve as a starting
point for further exploration of this dataset.
For all feature sets and settings, LATENTLC
scored highest, often with a considerable margin
of up to 3.0% in the cross-validation and up to
4.6% on the test set relative to the LEFTMOST
baseline, and 5.1% (cross-validation) and 6.8%
(test) margins relative to VPREP.
The best scoring result of our LATENTLC
model in the cross-validation scenario is 65.72%,
obtained by the feature set All. The best scoring
result by any of the baseline models in this sce-
nario is 62.7%, obtained by the same feature set.
For the test set scenario, LATENTLC obtained its
highest accuracy, 65.73%, when using the feature
set Basic. This is a substantial improvement over
the highest scoring baseline model in this scenario
that obtained 61.6% accuracy, using the feature set
All. This performance gap is substantial when tak-
ing into consideration that the improvements ob-
tained by the highly competitive DIRT similarity
features using the stronger LEFTMOST baseline,
result in an improvement of 3.1% and 5.3% over
the trivial ALLNEG baseline in the test set and
cross-validation scenarios respectively.
Comparing the different feature sets on our pro-
posed model, we find that the Basic feature set
gives a consistent and substantial increase over the
Sim feature set. Improvements are of 2.8% (test)
and 2.2% (cross-validation). Introducing more
elaborate features (i.e., the feature sets NoLDA
and All) yields some improvements in the cross-
validation, but these improvements are not repli-
cated on the test set. This may be due to idiosyn-
crasies in the test set that are averaged out in the
cross-validation scenario.
For a qualitative analysis, we took the best per-
forming model of the data set (i.e., with the Basic
feature set), and extracted the set of instances
where it made a correct prediction while both
baselines made an error. This set contains many
verb-preposition pairs, such as ?list as ? report
as? or ?submit via? deliver by?, underscoring the
utility of leveraging multiple LCs rather than con-
sidering only a head word (as with LEFTMOST)
or the entire phrase (as with VPREP). Other ex-
amples in this set contain more complex patterns.
These include the positive pairs ?talk much about
? have much to say about? and ?increase with
? go up with?, and the negative ?make predic-
tion about ? meet the challenge of? and ?enjoy
watching? love to play?.
6 Discussion
Relation to CDS. Much recent work subsumed
under the title Compositional Distributional Se-
mantics addressed the distributional representa-
tion of multi-word phrases (see Section 2). This
line of work focuses on compositional predicates,
such as ?kick the ball? and not on idiosyncratic
predicates such as ?kick the bucket?.
A variant of the CDS approach can be framed
within ours. Assume we wish to compute the
similarity of the predicates p
L
= (w
1
, ..., w
n
)
and p
R
= (w
?
1
, ..., w
?
m
). Let us denote the vec-
tor space representations of the individual words
as v
1
, ..., v
n
and v
?
1
, ..., v
?
m
respectively. A stan-
dard approach in CDS is to compose distributional
representations by taking their vector sum v
L
=
v
1
+ v
2
...+ v
n
and v
R
= v
?
1
+ ...+ v
?
m
(Mitchell
and Lapata, 2010). One of the most effective sim-
ilarity measures is the cosine similarity, which is a
normalized dot product. The distributional sim-
ilarity between p
L
and p
R
under this model is
sim(p
L
, p
R
) =
?
n
i=1
?
m
j=1
sim(w
i
, w
?
j
), where
sim(w
i
, w
?
j
) is the dot product between v
i
and v
?
j
.
This similarity score is similar in spirit to a
simplified version of our statistical model that
restricts the set of allowable LCs H
p
to be
{({w
i
}, {w
?
j
})|i ? n, j ? m}, i.e., only LCs of
size 1. Indeed, taking H
p
as above, and cosine
similarity as the only feature (i.e., w ? R), yields
the distribution
651
Test Set Cross Validation
Algorithm Sim Basic NoLDA All Sim Basic NoLDA All
LATENTLC 62.9 65.7 64.4 64.6 62.7 ? 1.9 64.9 ? 1.9 65.0 ? 1.7 65.7 ?1.9
LEFTMOST 59.0 61.1 60.0 60.4 61.2 ? 2.1 62.5 ? 2.4 62.4 ?2.2 62.7 ? 2.0
VPREP 56.1 60.9 60.7 61.6
?
58.1 ? 1.7 60.8 ? 2.2 60.4 ? 2.6 60.6 ? 2.2
ALLNEG 55.9 55.9
Table 2: Results for the various evaluated systems. Accuracy results are presented in percents, followed in the cross vali-
dation scenario by the standard deviation over the folds. The rows correspond to the various systems as defined in Section 4.
LATENTLC is our proposed model. The columns correspond to the various feature sets, from the least to the most inclusive.
SIM includes only similarity features. BASIC additionally includes POS-based and frequency features. NOLDA includes all
features except LDA-based features. ALL is the full feature set. ALLNEG is the classifier that invariably predicts the label ?no
inference?. Bold marks best overall accuracy per column, and
?
marks figures that are not significantly worse (McNemar?s test,
p < 0.05). The same positive to negative label ratio was maintained in both the cross validation and test set scenarios. In all
cases, LATENTLC obtains substantial improvements over the baseline systems.
P (y|p) ?
X
(w
i
,w
?
j
)?H
p
exp
`
w ? y ? sim(w
i
, w
?
j
)
?
.
This derivation highlights the relation of a sim-
plified version of our approach to the additive
CDS model, as both approaches effectively aver-
age over the similarities of all pairs of words in p
L
and p
R
. The derivation also highlights a few ad-
vantages of our approach. First, our approach al-
lows to straightforwardly introduce additional fea-
tures and to weight them in a way most consistent
with the task at hand. Second, it allows much more
flexibility in defining the set of allowable LCs,H
p
.
Specifically, H
p
may contain LCs of sizes greater
than 1. Third, our approach uses standard proba-
bilistic modelling, and therefore has a natural sta-
tistical interpretation.
In order to appreciate the effect of these advan-
tages, we perform an experiment that takes H to
be the set of all LCs of size 1, and uses a sin-
gle similarity measure. We run a 10-fold cross-
validation on our training data, obtaining 61.3%
accuracy using COSINE and 62.2% accuracy us-
ing BInc. The performance gap between these re-
sults and the accuracy obtained by our full model
(65.7%) underscores the latter?s effectiveness in
integrating multiple features and LCs.
Effectiveness of Optimization Method. Our
maximization of the log-likelihood function is
not guaranteed to converge to a global optimum.
Therefore, the quality of the learned parameters
may be sensitive to the initialization point. We
hereby describe an experiment that tests the sen-
sitivity of our approach to such variance.
Selecting the highest scoring feature set on our
test set (i.e., BASIC), we ran the model with mul-
tiple initializers, by randomly perturbing our stan-
dard convex initializer (see Section 3). Concretely,
given a convex initializer w, we select the starting
point to be w + ?, where ?
i
? N (0, ?|w
i
|). We
ran this experiment 400 times with ? = 0.8.
To combine the resulting weight vectors into a
single classifier, we apply two types of standard
approaches: a Product of Experts (Hinton, 2002),
as well as a voting approach that selects the most
frequently predicted label. Neither of these exper-
iments yielded any significant performance gain.
This demonstrates the robustness of our optimiza-
tion method to the initialization point.
7 Conclusion
We have presented a novel approach to the
distributional representation of multi-word pred-
icates. Since MWPs demonstrate varying levels
of compositionality, a uniform treatment of MWPs
either as fixed expressions or through head words
is lacking. Instead, our approach integrates mul-
tiple lexical units contained in the predicate. The
approach takes into account both multi-word LCs
that address low compositionality cases, as well as
single-word LCs that address compositional cases
and are more frequent. It assumes a latent distribu-
tion over the LCs of the predicates, and estimates
it relative to a target application task.
We addressed the supervised inference identi-
fication task, obtaining substantial improvement
over state-of-the-art baseline systems. In future
work we intend to assess the benefit of this ap-
proach in MWP classes that are well-known from
the literature. We believe that a permissive ap-
proach that integrates multiple analyses would
perform better than standard single-analysis meth-
ods in a wide range of applications.
Acknowledgements. We would like to thank
Mike Lewis, Reshef Meir, Oren Melamud,
Michael Roth and Nathan Schneider for their help-
ful comments. This work was supported by ERC
Advanced Fellowship 249520 GRAMPLUS.
652
References
Alex Alsina, Joan Wanda Bresnan, and Peter Sells.
1997. Complex predicates. Center for the Study of
Language and Information.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pages 89?96.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP, pages 1183?1193.
Jonathan Berant, Jacob Goldberger, and Ido Dagan.
2011. Global learning of typed entailment rules. In
ACL, pages 610?619.
Chris Biemann and Eugenie Giesbrecht. 2011. Dis-
tributional semantics and compositionality 2011:
Shared task description and results. In Workshop
on Distributional Semantics and Compositionality,
pages 21?28.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet alocation. the Journal of
machine Learning research, 3:993?1022.
Miriam Butt. 2010. The light verb jungle: still hack-
ing away. In Complex predicates: cross-linguistic
perspectives on event structure, pages 48?78. Cam-
bridge University Press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. In J. van
Bentham, M. Moortgat, and W. Buszkowski, editors,
Linguistic Analysis, volume 36, pages 435?384.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies, 6(4):1?220.
Georgiana Dinu and Mirella Lapata. 2010. Topic mod-
els for meaning similarity in context. In COLING:
Posters, pages 250?258.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In EACL, pages 211?219.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP, pages 1535?1545.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In EACL, pages 337?344.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition us-
ing distributional features and syntactic patterns. In-
formation and Media Technologies, 4(2):558?582.
Geoffrey E Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation, 14(8):1771?1800.
Matthew Hoffman, Francis R Bach, and David M Blei.
2010. Online learning for latent Dirichlet alocation.
In NIPS, pages 856?864.
Ray Jackendoff. 2002. Foundations of language:
Brain, meaning, grammar, evolution. Oxford Uni-
versity Press.
Douwe Kiela and Stephen Clark. 2013. Detect-
ing compositionality of multi-word expressions us-
ing nearest neighbours in vector space models. In
EMNLP, pages 1427?1432.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. TACL, 1:179?
192.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In SIGKDD 2001,
pages 323?328.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?774.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL, pages 317?324.
Edward Loper and Steven Bird. 2002. NLTK: The
natural language toolkit. In ACL Workshop on Ef-
fective tools and methodologies for teaching natural
language processing and computational linguistics,
pages 63?70.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In ACL workshop on Multiword
expressions: analysis, acquisition and treatment,
pages 73?80.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013a. A two level
model for context sensitive inference rules. In ACL
2013, pages 1331?1340.
Oren Melamud, Ido Dagan, Jacob Goldberger, and Idan
Szpektor. 2013b. Using lexical expansion to learn
inference rules from sparse data. In ACL: Short Pa-
pers, pages 283?288.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
COLING-ACL: Poster Session, pages 579?586.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Jorge. Nocedal and Stephen J Wright. 1999. Numeri-
cal optimization, volume 2. Springer New York.
653
Karl Pichotta and John DeNero. 2013. Identify-
ing phrasal verbs using many bilingual corpora. In
EMNLP, pages 636?646.
Carlos Ramisch, Aline Villavicencio, and Valia Kor-
doni. 2013. Introduction to the special issue on
multiword expressions: From theory to practice and
use. ACM Transactions on Speech and Language
Processing (TSLP), 10(2):3.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In ACL, pages 41?47.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In IJCNLP, pages 210?218.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of LREC 2010 workshop New Challenges
for NLP Frameworks, pages 46?50.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional pref-
erences. In ACL, pages 424?434.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2001. Multiword
expressions: A pain in the neck for NLP. In CI-
CLing, pages 1?15.
Stefan Schoenmackers, Oren Etzioni, Daniel S Weld,
and Jesse Davis. 2010. Learning first-order Horn
clauses from web text. In EMNLP, pages 1088?
1098.
Diarmuid
?
O. S?eaghdha. 2010. Latent variable models
of selectional preference. In ACL 2010, pages 435?
444.
Maggie Seaton and Alison Macaulay, editors. 2002.
Collins COBUILD Idioms Dictionary. Harper-
Collins Publishers, 2nd edition.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between NE pairs.
In IWP, pages 4?6.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In HLT-NAACL, pages 304?311.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In COLING, pages
849?856.
Yuancheng Tu and Dan Roth. 2011. Learning English
light verb constructions: contextual or statistical. In
ACL HLT 2011, page 31.
Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In EMNLP-
CoNLL, pages 1034?1043.
Veronika Vincze, Istv?an Nagy T., and Rich?ard Farkas.
2013. Identifying English and Hungarian light verb
constructions: A contrastive approach. In ACL:
Short Papers, pages 255?261.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In EMNLP, pages
81?88.
Hila Weisman, Jonathan Berant, Idan Szpektor, and
Ido Dagan. 2012. Learning verb inference rules
from linguistically-motivated evidence. In EMNLP-
CoNLL, pages 194?204.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sym-
posium on Quantum Interaction, volume 26, pages
28?35.
Alison Wray. 2008. Formulaic language: Pushing the
boundaries. Oxford University Press.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In ACL-COLING, pages 849?
856.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
ACL: Short Papers, pages 156?160.
654
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052?1061,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Provably Correct Learning Algorithm for Latent-Variable PCFGs
Shay B. Cohen
School of Informatics
University of Edinburgh
scohen@inf.ed.ac.uk
Michael Collins
Department of Computer Science
Columbia University
mcollins@cs.columbia.edu
Abstract
We introduce a provably correct learning
algorithm for latent-variable PCFGs. The
algorithm relies on two steps: first, the use
of a matrix-decomposition algorithm ap-
plied to a co-occurrence matrix estimated
from the parse trees in a training sample;
second, the use of EM applied to a convex
objective derived from the training sam-
ples in combination with the output from
the matrix decomposition. Experiments on
parsing and a language modeling problem
show that the algorithm is efficient and ef-
fective in practice.
1 Introduction
Latent-variable PCFGs (L-PCFGs) (Matsuzaki et
al., 2005; Petrov et al, 2006) give state-of-the-art
performance on parsing problems. The standard
approach to parameter estimation in L-PCFGs is
the EM algorithm (Dempster et al, 1977), which
has the usual problems with local optima. Re-
cent work (Cohen et al, 2012) has introduced an
alternative algorithm, based on spectral methods,
which has provable guarantees. Unfortunately this
algorithm does not return parameter estimates for
the underlying L-PCFG, instead returning the pa-
rameter values up to an (unknown) linear trans-
form. In practice, this is a limitation.
We describe an algorithm that, like EM, re-
turns estimates of the original parameters of an L-
PCFG, but, unlike EM, does not suffer from prob-
lems of local optima. The algorithm relies on two
key ideas:
1) A matrix decomposition algorithm (sec-
tion 5) which is applicable to matrices Q of the
form Q
f,g
=
?
h
p(h)p(f | h)p(g | h) where
p(h), p(f | h) and p(g | h) are multinomial dis-
tributions. This matrix form has clear relevance
to latent variable models. We apply the matrix
decomposition algorithm to a co-occurrence ma-
trix that can be estimated directly from a training
set consisting of parse trees without latent anno-
tations. The resulting parameter estimates give us
significant leverage over the learning problem.
2) Optimization of a convex objective function
using EM. We show that once the matrix decom-
position step has been applied, parameter estima-
tion of the L-PCFG can be reduced to a convex
optimization problem that is easily solved by EM.
The algorithm provably learns the parameters of
an L-PCFG (theorem 1), under an assumption that
each latent state has at least one ?pivot? feature.
This assumption is similar to the ?pivot word? as-
sumption used by Arora et al (2013) and Arora et
al. (2012) in the context of learning topic models.
We describe experiments on learning of L-
PCFGs, and also on learning of the latent-variable
language model of Saul and Pereira (1997). A hy-
brid method, which uses our algorithm as an ini-
tializer for EM, performs at the same accuracy as
EM, but requires significantly fewer iterations for
convergence: for example in our L-PCFG exper-
iments, it typically requires 2 EM iterations for
convergence, as opposed to 20-40 EM iterations
for initializers used in previous work.
While this paper?s focus is on L-PCFGs, the
techniques we describe are likely to be applicable
to many other latent-variable models used in NLP.
2 Related Work
Recently a number of researchers have developed
provably correct algorithms for parameter esti-
mation in latent variable models such as hidden
Markov models, topic models, directed graphical
models with latent variables, and so on (Hsu et
al., 2009; Bailly et al, 2010; Siddiqi et al, 2010;
Parikh et al, 2011; Balle et al, 2011; Arora et
al., 2013; Dhillon et al, 2012; Anandkumar et
al., 2012; Arora et al, 2012; Arora et al, 2013).
Many of these algorithms have their roots in spec-
tral methods such as canonical correlation analy-
sis (CCA) (Hotelling, 1936), or higher-order ten-
sor decompositions. Previous work (Cohen et al,
2012; Cohen et al, 2013) has developed a spec-
tral method for learning of L-PCFGs; this method
learns parameters of the model up to an unknown
1052
linear transformation, which cancels in the inside-
outside calculations for marginalization over la-
tent states in the L-PCFG. The lack of direct pa-
rameter estimates from this method leads to prob-
lems with negative or unnormalized probablities;
the method does not give parameters that are in-
terpretable, or that can be used in conjunction with
other algorithms, for example as an initializer for
EM steps that refine the model.
Our work is most directly related to the algo-
rithm for parameter estimation in topic models de-
scribed by Arora et al (2013). This algorithm
forms the core of the matrix decomposition algo-
rithm described in section 5.
3 Background
This section gives definitions and notation for L-
PCFGs, taken from (Cohen et al, 2012).
3.1 L-PCFGs: Basic Definitions
An L-PCFG is an 8-tuple (N , I,P,m, n, pi, t, q)
where: N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We as-
sume that N = I ? P , and I ? P = ?. Hence
we have partitioned the set of non-terminals into
two subsets. [m] is the set of possible hidden
states.
1
[n] is the set of possible words. For
all (a, b, c) ? I ? N ? N , and (h
1
, h
2
, h
3
) ?
[m] ? [m] ? [m], we have a context-free rule
a(h
1
) ? b(h
2
) c(h
3
). The rule has an associ-
ated parameter t(a? b c, h
2
, h
3
| a, h
1
). For all
a ? P , h ? [m], x ? [n], we have a context-free
rule a(h)? x. The rule has an associated param-
eter q(a ? x | a, h). For all a ? I, h ? [m],
pi(a, h) is a parameter specifying the probability
of a(h) being at the root of a tree.
A skeletal tree (s-tree) is a sequence of rules
r
1
. . . r
N
where each r
i
is either of the form a ?
b c or a ? x. The rule sequence forms a
top-down, left-most derivation under a CFG with
skeletal rules.
A full tree consists of an s-tree r
1
. . . r
N
, to-
gether with values h
1
. . . h
N
. Each h
i
is the value
for the hidden variable for the left-hand-side of
rule r
i
. Each h
i
can take any value in [m].
For a given skeletal tree r
1
. . . r
N
, define a
i
to
be the non-terminal on the left-hand-side of rule
r
i
. For any i ? [N ] such that r
i
is of the form
a? b c, define h
(2)
i
and h
(3)
i
as the hidden state
1
For any integer n, we use [n] to denote the set
{1, 2, . . . n}.
value of the left and right child respectively. The
model then defines a distribution as
p(r
1
. . . r
N
, h
1
. . . h
N
) =
pi(a
1
, h
1
)
?
i:a
i
?I
t(r
i
, h
(2)
i
, h
(3)
i
| a
i
, h
i
)
?
i:a
i
?P
q(r
i
| a
i
, h
i
)
The distribution over skeletal trees is
p(r
1
. . . r
N
) =
?
h
1
...h
N
p(r
1
. . . r
N
, h
1
. . . h
N
).
3.2 Definition of Random Variables
Throughout this paper we will make reference
to random variables derived from the distribution
over full trees from an L-PCFG. These random
variables are defined as follows. First, we select
a random internal node, from a random tree, as
follows: 1) Sample a full tree r
1
. . . r
N
, h
1
. . . h
N
from the PMF p(r
1
. . . r
N
, h
1
. . . h
N
); 2) Choose
a node i uniformly at random from [N ]. We then
give the following definition:
Definition 1 (Random Variables). If the rule r
i
for
the node i is of the form a? b c, we define ran-
dom variables as follows: R
1
is equal to the rule r
i
(e.g., NP? D N). A,B,C are the labels for node i,
the left child of node i, and the right child of node
i respectively. (E.g., A = NP, B = D, C = N.) T
1
is the inside tree rooted at node i. T
2
is the inside
tree rooted at the left child of node i, and T
3
is the
inside tree rooted at the right child of node i. O is
the outside tree at node i. H
1
, H
2
, H
3
are the hid-
den variables associated with node i, the left child
of node i, and the right child of node i respectively.
E is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
If the rule r
i
for the selected node i is
of the form a ? x, we have random vari-
ables R
1
, T
1
, H
1
, A
1
, O,E as defined above, but
H
2
, H
3
, T
2
, T
3
, B, and C are not defined.
4 The Learning Algorithm for L-PCFGs
Our goal is to design a learning algorithm for L-
PCFGs. The input to the algorithm will be a train-
ing set consisting of skeletal trees, assumed to be
sampled from some underlying L-PCFG. The out-
put of the algorithm will be estimates for the pi,
t, and q parameters. The training set does not
include values for the latent variables; this is the
main challenge in learning.
This section focuses on an algorithm for recov-
ery of the t parameters. A description of the al-
gorithms for recovery of the pi and q parameters
is deferred until section 6.1 of this paper; these
1053
steps are straightforward once we have derived the
method for the t parameters.
We describe an algorithm that correctly recov-
ers the parameters of an L-PCFG as the size of the
training set goes to infinity (this statement is made
more precise in section 4.2). The algorithm relies
on an assumption?the ?pivot? assumption?that
we now describe.
4.1 Features, and the Pivot Assumption
We assume a function ? from inside trees to a fi-
nite set F , and a function ? that maps outside trees
to a finite set G. The function ?(t) (?(o)) can be
thought of as a function that maps an inside tree
t (outside tree o) to an underlying feature. As
one example, the function ?(t) might return the
context-free rule at the root of the inside tree t;
in this case the set F would be equal to the set
of all context-free rules in the grammar. As an-
other example, the function ?(o) might return the
context-free rule at the foot of the outside tree o.
In the more general case, we might have K sep-
arate functions ?
(k)
(t) for k = 1 . . .K mapping
inside trees to K separate features, and similarly
we might have multiple features for outside trees.
Cohen et al (2013) describe one such feature def-
inition, where features track single context-free
rules as well as larger fragments such as two or
three-level sub-trees. For simplicity of presenta-
tion we describe the case of single features ?(t)
and ?(o) for the majority of this paper. The exten-
sion to multiple features is straightforward, and is
discussed in section 6.2; the flexibility allowed by
multiple features is important, and we use multiple
features in our experiments.
Given functions ? and ?, we define additional
random variables: F = ?(T
1
), F
2
= ?(T
2
), F
3
=
?(T
3
), and G = ?(O).
We can now give the following assumption:
Assumption 1 (The Pivot Assumption). Under
the L-PCFG being learned, there exist values ? >
0 and ? > 0 such that for each non-terminal a,
for each hidden state h ? [m], the following state-
ments are true: 1) ?f ? F such that P (F =
f | H
1
= h,A = a) > ? and for all h
?
6= h,
P (F = f | H
1
= h
?
, A = a) = 0; 2) ?g ? G
such that P (G = g | H
1
= h,A = a) > ? and
for all h
?
6= h, P (G = g | H
1
= h
?
, A = a) = 0.
This assumption is very similar to the assump-
tion made by Arora et al (2012) in the con-
text of learning topic models. It implies that for
each (a, h) pair, there are inside and outside tree
features?which following Arora et al (2012) we
refer to as pivot features?that occur only
2
in the
presence of latent-state value h. As in (Arora et
al., 2012), the pivot features will give us consider-
able leverage in learning of the model.
4.2 The Learning Algorithm
Figure 1 shows the learning algorithm for L-
PCFGs. The algorithm consists of the following
steps:
Step 0: Calculate estimates p?(a? b c | a),
p?(g, f
2
, f
3
| a? b c) and p?(f, g | a). These
estimates are easily calculated using counts taken
from the training examples.
Step 1: Calculate values r?(f | h, a) and s?(g |
h, a); these are estimates of p(f | h
1
, a) and
p(g | h
1
, a) respectively. This step is achieved us-
ing a matrix decomposition algorithm, described
in section 5 of this paper, on the matrix
?
Q
a
with
entries [
?
Q
a
]
f,g
= p?(f, g | a).
Step 2: Use the EM algorithm to find
?
t values
that maximize the objective function in Eq. 1 (see
figure 1). Crucially, this is a convex optimization
problem, and the EM algorithm will converge to
the global maximum of this likelihood function.
Step 3: Rule estimates are calculated using an
application of the laws of probability.
Before giving a theorem concerning correctness
of the algorithm we introduce two assumptions:
Assumption 2 (Strict Convexity). If we have the
equalities s?(g | h
1
, a) = P (G = g | H
1
=
h
1
, A = a), r?(f
2
| h
2
, b) = P (F
2
= f
2
| H
2
=
h
2
, B = b) and r?(f
3
| h
3
, c) = P (F
3
= f
3
|
H
2
= h
3
, C = c), then the function in Eq. 1 (fig-
ure 1) is strictly concave.
The function in Eq. 1 is always concave; this
assumption adds the restriction that the function
must be strictly concave?that is, it has a unique
global maximum?in the case that the r? and s? es-
timates are exact estimates.
Assumption 3 (Infinite Data). After running Step
0 of the algorithm we have
p?(a? b c | a) = p(a? b c | a)
p?(g, f
2
, f
3
| a? b c) = p(g, f
2
, f
3
| a? b c)
p?(f, g | a) = p(f, g | a)
where p(. . .) is the probability under the underly-
ing L-PCFG.
2
The requirements P (F = f | H
1
= h
?
, A = a) = 0
and P (G = g | H
1
= h
?
, A = a) = 0 are almost certainly
overly strict; in theory and practice these probabilities should
be able to take small but strictly positive values.
1054
We use the term ?infinite data? because under
standard arguments, p?(. . .) converges to p(. . .) as
M goes to?.
The theorem is then as follows:
Theorem 1. Consider the algorithm in figure 1.
Assume that assumptions 1-3 (the pivot, strong
convexity, and infinite data assumptions) hold for
the underlying L-PCFG. Then there is some per-
mutation ? : [m] ? [m] such that for all
a? b c, h
1
, h
2
, h
3
,
?
t(a? b c, h
2
, h
3
| a? b c, h
1
)
= t(a? b c, ?(h
2
), ?(h
3
) | a? b c, ?(h
1
))
where
?
t are the parameters in the output, and t are
the parameters of the underlying L-PCFG.
This theorem states that under assumptions 1-
3, the algorithm correctly learns the t parameters
of an L-PCFG, up to a permutation over the la-
tent states defined by ?. Given the assumptions we
have made, it is not possible to do better than re-
covering the correct parameter values up to a per-
mutation, due to symmetries in the model. As-
suming that the pi and q parameters are recovered
in addition to the t parameters (see section 6.1),
the resulting model will define exactly the same
distribution over full trees as the underlying L-
PCFG up to this permutation, and will define ex-
actly the same distribution over skeletal trees, so
in this sense the permutation is benign.
Proof of theorem 1: Under the assumptions of
the theorem,
?
Q
a
f,g
= p(f, g | a) =
?
h
p(h |
a)p(f | h, a)p(g | h, a). Under the pivot assump-
tion, and theorem 2 of section 5, step 1 (the matrix
decomposition step) will therefore recover values
r? and s? such that r?(f | h, a) = p(f | ?(h), a) and
s?(g | h, a) = p(g | ?(h), a) for some permuta-
tion ? : [m] ? [m]. For simplicity, assume that
?(j) = j for all j ? [m] (the argument for other
permutations involves a straightforward extension
of the following argument). Under the assump-
tions of the theorem, p?(g, f
2
, f
3
| a? b c) =
p(g, f
2
, f
3
| a? b c), hence the function being
optimized in Eq. 1 is equal to
?
g,f
2
,f
3
p(g, f
2
, f
3
| a? b c) log ?(g, f
2
, f
3
)
where
?(g, f
2
, f
3
) =
?
h
1
,h
2
,h
3
(
?
t(h
1
, h
2
, h
3
| a? b c)
?p(g | h
1
, a)p(f
2
| h
2
, b)p(f
3
| h
3
, c))
Now consider the optimization problem in Eq. 1.
By standard results for cross entropy, the maxi-
mum of the function
?
g,f
2
,f
3
p(g, f
2
, f
3
| a? b c) log q(g, f
2
, f
3
| a? b c)
with respect to the q values is achieved at
q(g, f
2
, f
3
| a? b c) = p(g, f
2
, f
3
| a? b c). In
addition, under the assumptions of the L-PCFG,
p(g, f
2
, f
3
| a? b c)
=
?
h
1
,h
2
,h
3
(p(h
1
, h
2
, h
3
| a? b c)
?p(g | h
1
, a)p(f
2
| h
2
, b)p(f
3
| h
3
, c))
Hence the maximum of Eq. 1 is achieved at
?
t(h
1
, h
2
, h
3
| a? b c) = p(h
1
, h
2
, h
3
| a? b c)
(2)
because this gives ?(g, f
2
, f
3
) = p(g, f
2
, f
3
|
a? b c). Under the strict convexity assump-
tion the maximum of Eq. 1 is unique, hence the
?
t values must satisfy Eq. 2. Finally, it follows
from Eq. 2, and the equality p?(a? b c | a) =
p(a? b c | a), that Step 3 of the algorithm gives
?
t(a? b c, h
2
, h
3
| a, h
1
) = t(a? b c, h
2
, h
3
|
a, h
1
).
We can now see how the strict convexity as-
sumption is needed. Without this assumption,
there may be multiple settings for
?
t that achieve
?(g, f
2
, f
3
) = p(g, f
2
, f
3
| a? b c); the values
?
t(h
1
, h
2
, h
3
| a? b c) = p(h
1
, h
2
, h
3
| a? b c)
will be included in this set of solutions, but other,
inconsistent solutions will also be included.
As an extreme example of the failure of the
strict convexity assumption, consider a feature-
vector definition with |F| = |G| = 1. In
this case the function in Eq. 1 reduces to
log
?
h
1
,h
2
,h
3
?
t(h
1
, h
2
, h
3
| a? b c). This func-
tion has a maximum value of 0, achieved at all val-
ues of
?
t. Intuitively, this definition of inside and
outside tree features loses all information about
the latent states, and does not allow successful
learning of the underlying L-PCFG. More gener-
ally, it is clear that the strict convexity assumption
will depend directly on the choice of feature func-
tions ?(t) and ?(o).
Remark: The infinite data assumption, and
sample complexity. The infinite data assump-
tion deserves more discussion. It is clearly a
strong assumption that there is sufficient data for
1055
Input: A set ofM skeletal trees sampled from some underlying L-PCFG. The count[. . .] function counts the number of times
that event . . . occurs in the training sample. For example, count[A = a] is the number of times random variableA takes value
a in the training sample.
Step 0: Calculate the following estimates from the training samples:
? p?(a? b c | a) = count[R
1
= a? b c]/count[A = a]
? p?(g, f
2
, f
3
| a? b c) = count[G = g, F
2
= f
2
, F
3
= f
3
, R
1
= a? b c]/count[R
1
= a? b c]
? p?(f, g | a) = count[F = f,G = g,A = a]/count[A = a]
? ?a ? I, define a matrix
?
Q
a
? R
d?d
?
where d = |F| and d
?
= |G| as [
?
Q
a
]
f,g
= p?(f, g | a).
Step 1: ?a ? I, use the algorithm in figure 2 with input
?
Q
a
to derive estimates r?(f | h, a) and s?(g | h, a).
Remark: These quantities are estimates of P (F
1
= f | H
1
= h,A = a) and P (G = g | H = h,A = a) respectively. Note
that under the independence assumptions of the L-PCFG,
P (F
1
= f | H
1
= h,A = a) = P (F
2
= f | H
2
= h,A
2
= a) = P (F
3
= f | H
3
= h,A
3
= a).
Step 2: For each rule a? b c, find
?
t(h
1
, h
2
, h
3
| a? b c) values that maximize
?
g,f
2
,f
3
p?(g, f
2
, f
3
| a? b c) log
?
h
1
,h
2
,h
3
?
t(h
1
, h
2
, h
3
| a? b c)s?(g | h
1
, a)r?(f
2
| h
2
, b)r?(f
3
| h
3
, c) (1)
under the constraints
?
t(h
1
, h
2
, h
3
| a? b c) ? 0, and
?
h
1
,h
2
,h
3
?
t(h
1
, h
2
, h
3
| a? b c) = 1.
Remark: the function in Eq. 1 is concave in the values
?
t being optimized over. We use the EM algorithm, which converges to
a global optimum.
Step 3: ?a? b c, h
1
, h
2
, h
3
, calculate rule parameters as follows:
?
t(a? b c, h
2
, h
3
| a, h
1
) =
?
t(a? b c, h
1
, h
2
, h
3
| a)/
?
b,c,h
2
,h
3
?
t(a? b c, h
1
, h
2
, h
3
| a)
where
?
t(a? b c, h
1
, h
2
, h
3
| a) = p?(a? b c | a)?
?
t(h
1
, h
2
, h
3
| a? b c).
Output: Parameter estimates
?
t(a? b c, h
2
, h
3
| a, h
1
) for all rules a? b c, for all (h
1
, h
2
, h
3
) ? [m]? [m]? [m].
Figure 1: The learning algorithm for the t(a? b c, h
1
, h
2
, h
3
| a) parameters of an L-PCFG.
the estimates p? in assumption 3 to have converged
to the correct underlying values. A more detailed
analysis of the algorithm would derive sample
complexity results, giving guarantees on the sam-
ple size M required to reach a level of accuracy
 in the estimates, with probability at least 1 ? ?,
as a function of , ?, and other relevant quantities
such as n, d, d
?
,m, ?, ? and so on.
In spite of the strength of the infinite data as-
sumption, we stress the importance of this result
as a guarantee for the algorithm. First, a guar-
antee of correct parameter values in the limit of
infinite data is typically the starting point for a
sample complexity result (see for example (Hsu
et al, 2009; Anandkumar et al, 2012)). Sec-
ond, our sense is that a sample complexity result
can be derived for our algorithm using standard
methods: specifically, the analysis in (Arora et
al., 2012) gives one set of guarantees; the remain-
ing optimization problems we solve are convex
maximum-likelihood problems, which are also
relatively easy to analyze. Note that several pieces
of previous work on spectral methods for latent-
variable models focus on algorithms that are cor-
rect under the infinite data assumption.
5 The Matrix Decomposition Algorithm
This section describes the matrix decomposition
algorithm used in Step 1 of the learning algorithm.
5.1 Problem Setting
Our goal will be to solve the following matrix de-
composition problem:
Matrix Decomposition Problem (MDP) 1. De-
sign an algorithm with the following inputs, as-
sumptions, and outputs:
1056
Inputs: Integers m, d and d
?
, and a matrix Q ?
R
d?d
?
such that Q
f,g
=
?
m
h=1
pi(h)r(f | h)s(g |
h) for some unknown parameters pi(h), r(f | h)
and s(g | h) satisfying:
1) pi(h) ? 0,
?
m
h=1
pi(h) = 1;
2) r(f | h) ? 0,
?
d
f=1
r(f | h) = 1;
3) s(g | h) ? 0,
?
d
?
g=1
s(g | h) = 1.
Assumptions: There are values ? > 0 and ? >
0 such that the r parameters of the model are ?-
separable, and the s parameters of the model are
?-separable.
Outputs: Estimates p?i(h), r?(f | h) and s?(g | h)
such that there is some permutation ? : [m]? [m]
such that ?h, p?i(h) = pi(?(h)), ?f, h, r?(f |
h) = r(f | ?(h)), and ?g, h, s?(g | h) = s(g |
?(h)).
The definition of ?-separability is as follows (?-
separability for s(g | h) is analogous):
Definition 2 (?-separability). The parameters
r(f | h) are ?-separable if for all h ? [m], there
is some j ? [d] such that: 1) r(j | h) ? ?; and 2)
r(j | h
?
) = 0 for h
?
6= h.
This matrix decomposition problem has clear
relevance to problems in learning of latent-
variable models, and in particular is a core step of
the algorithm in figure 1. When given a matrix
?
Q
a
with entries
?
Q
a
f,g
=
?
h
p(h | a)p(f | h, a)p(g |
h, a), where p(. . .) refers to a distribution derived
from an underlying L-PCFG which satisfies the
pivot assumption, the method will recover the val-
ues for p(h | a), p(f | h, a) and p(g | h, a) up to a
permutation over the latent states.
5.2 The Algorithm of Arora et al (2013)
This section describes a variant of the algorithm of
Arora et al (2013), which is used as a component
of our algorithm for MDP 1. One of the proper-
ties of this algorithm is that it solves the following
problem:
Matrix Decomposition Problem (MDP) 2. De-
sign an algorithm with the following inputs, as-
sumptions, and outputs:
Inputs: Same as matrix decomposition problem 1.
Assumptions: The parameters r(f | h) of the
model are ?-separable for some value ? > 0.
Outputs: Estimates p?i(h) and r?(f | h) such that
?? : [m] ? [m] such that ?h, p?i(h) = pi(?(h)),
?f, h, r?(f | h) = r(f | ?(h)).
This is identical to Matrix Decomposition Prob-
lem 1, but without the requirement that the values
s(g | h) are returned by the algorithm. Thus an
algorithm that solves MDP 2 in some sense solves
?one half? of MDP 1.
For completeness we give a sketch of the algo-
rithm that we use; it is inspired by the algorithm
of Arora et al (2012), but has some important dif-
ferences. The algorithm is as follows:
Step 1: Derive a function ? : [d
?
] ? R
l
that
maps each integer g ? [d
?
] to a representation
?(g) ? R
l
. The integer l is typically much smaller
than d
?
, implying that the representation is of low
dimension. Arora et al (2012) derive ? as a ran-
dom projection with a carefully chosen dimension
l. In our experiments, we use canonical correlation
analysis (CCA) on the matrixQ to give a represen-
tation ?(g) ? R
l
where l = m.
Step 2: For each f ? [d], calculate
v
f
= E[?(g) | f ] =
d
?
?
g=1
p(g | f)?(g)
where p(g | f) = Q
f,g
/
?
g
Q
f,g
. It follows that
v
f
=
d
?
?
g=1
m
?
h=1
p(h | f)p(g | h)?(g) =
m
?
h=1
p(h | f)w
h
where w
h
? R
l
is equal to
?
d
?
g=1
p(g | h)?(g).
Hence the v
f
vectors lie in the convex hull of a
set of vectors w
1
. . . w
m
? R
l
. Crucially, for any
pivot word f for latent state h, we have p(h | f) =
1, hence v
f
= w
h
. Thus by the pivot assump-
tion, the set of points v
1
. . . v
d
includes the ver-
tices of the convex hull. Each point v
j
is a convex
combination of the vertices w
1
. . . w
m
, where the
weights in this combination are equal to p(h | j).
Step 3: Use the FastAnchorWords algo-
rithm of (Arora et al, 2012) to identify m vectors
v
s
1
, v
s
2
, . . . v
s
m
. The FastAnchorWords algo-
rithm has the guarantee that there is a permutation
? : [m]? [m] such that v
s
i
= w
?(i)
for all i. This
algorithm recovers the vertices of the convex hull
described in step 2, using a method that greedily
picks points that are as far as possible from the
subspace spanned by previously picked points.
Step 4: For each f ? [d] solve the problem
arg min
?
1
,?
2
,...,?
m
||
?
h
?
h
v
s
h
? v
f
||
2
subject to ?
h
? 0 and
?
h
?
h
= 1. We use the
algorithm of (Frank and Wolfe, 1956; Clarkson,
2010) for this purpose. Set q(h | f) = ?
h
.
1057
Return the final quantities:
p?i(h) =
?
f
p(f)q(h|f) r?(f |h) =
p(f)q(h|f)
?
f
p(f)q(h|f)
where p(f) =
?
g
Q
f,g
.
5.3 An Algorithm for MDP 1
Figure 2 shows an algorithm that solves MDP 1.
In steps 1 and 2 of the algorithm, the algorithm
of section 5.2 is used to recover estimates r?(f |
h) and s?(g | h). These distributions are equal to
p(f | h) and p(g | h) up to permutations ? and
?
?
of the latent states respectively; unfortunately
there is no guarantee that ? and ?
?
are the same
permutation. Step 3 estimates parameters t(h
?
|
h) that effectively map the permutation implied by
r?(f | h) to the permutation implied by s?(g | h);
the latter distribution is recalculated as
?
h
?
?
t(h
?
|
h)s?(g | h
?
).
We now state the following theorem:
Theorem 2. The algorithm in figure 2 solves Ma-
trix Decomposition Problem 1.
Proof: See the supplementary material.
Remark: A natural alternative to the algorithm
presented would be to run Step 1 of the original
algorithm, but to replace steps 2 and 3 with a step
that finds s?(g | h) values that maximize
?
f,g
Q
f,g
log
?
h
r?(h | f)s?(g | h)
This is again a convex optimization problem. We
may explore this algorithm in future work.
6 Additional Details of the Algorithm
6.1 Recovery of the pi and q Parameters
The recovery of the pi and q parameters relies on
the following additional (but benign) assumptions
on the functions ? and ?:
1) For any inside tree t such that t is a unary
rule of the form a ? x, the function ? is defined
as ?(t) = t.
3
2) The set of outside tree features G contains a
special symbol 2, and g(o) = 2 if and only if the
outside tree o is derived from a non-terminal node
at the root of a skeletal tree.
3
Note that if other features on unary rules are desired,
we can use multiple feature functions ?
1
(t) . . . ?
K
(t), where
?
1
(t) = t for inside trees, and the functions ?
2
(t) . . . ?
K
(t)
define other features.
Inputs: As in Matrix Decomposition Problem 1.
Assumptions: As in Matrix Decomposition Problem 1.
Algorithm:
Step 1. Run the algorithm of section 5.2 on the matrix Q
to derive estimates r?(f | h) and p?i(h). Note that under
the guarantees of the algorithm, there is some permutation
? such that r?(f | h) = r(f | ?(h)). Define
r?(h | f) =
r?(f | h)p?i(h)
?
h
r?(f | h)p?i(h)
Step 2. Run the algorithm of section 5.2 on the matrix Q
>
to derive estimates s?(g | h). Under the guarantees of the
algorithm, there is some permutation ?
?
such that s?(g | h) =
s(g | ?
?
(h)). Note however that it is not necessarily the case
that ? = ?
?
.
Step 3. Find
?
t(h
?
| h) for all h, h
?
? [m] that maximize
?
f,g
Q
f,g
log
?
h,h
?
r?(h | f)
?
t(h
?
| h)s?(g | h
?
) (3)
subject to
?
t(h
?
| h) ? 0, and ?h,
?
h
?
?
t(h
?
| h) = 1.
Remark: the function in Eq. 3 is concave in the
?
t parame-
ters. We use the EM algorithm to find a global optimum.
Step 4. Return the following values:
? p?i(h) for all h, as an estimate of pi(?(h)) for some
permutation ?.
? r?(f | h) for all f, h as an estimate of r(f | ?(h)) for
the same permutation ?.
?
?
h
?
?
t(h
?
| h)s?(g | h
?
) as an estimate of s(f | ?(h))
for the same permutation ?.
Figure 2: The algorithm for Matrix Decomposition Problem 1
Under these assumptions, the algorithm in fig-
ure 1 recovers estimates p?i(a, h) and q?(a ? x |
a, h). Simply set
q?(a? x | a, h) = r?(f | h, a) where f = a? x
and p?i(a, h) = p?(2, h, a)/
?
h,a
p?(2, h, a) where
p?(2, h, a) = g?(2 | h, a)p?(h | a)p?(a). Note that
p?(h | a) can be derived from the matrix decompo-
sition step when applied to
?
Q
a
, and p?(a) is easily
recovered from the training examples.
6.2 Extension to Include Multiple Features
We now describe an extension to allowK separate
functions ?
(k)
(t) for k = 1 . . .K mapping inside
trees to features, and L feature functions ?
(l)
(o)
for l = 1 . . . L over outside trees.
The algorithm in figure 1 can be extended as
follows. First, Step 1 of the algorithm (the matrix
1058
decomposition step) can be extended to provide
estimates r?
(k)
(f
(k)
| h, a) and s?
(l)
(g
(l)
| h, a).
In brief, this involves running CCA on a matrix
E[?(T )(?(O))
>
| A = a] where ? and ? are in-
side and outside binary feature vectors derived di-
rectly from the inside and outside features, using
a one-hot representation. CCA results in a low-
dimensional representation that can be used in the
steps described in section 5.2; the remainder of the
algorithm is the same. In practice, the addition of
multiple features may lead to better CCA repre-
sentations.
Next, we modify the objective function in Eq. 1
to be the following:
?
i,j,k
?
g
i
,f
j
2
,f
k
3
p(g
i
, f
j
2
, f
k
3
| a? b c) log ?
i,j,k
(g
i
, f
j
2
, f
k
3
)
where
?
i,j,k
(g
i
, f
j
2
, f
k
3
)
=
?
h
1
,h
2
,h
3
(
?
t(h
1
, h
2
, h
3
| a? b c)
?s?
i
(g
i
| h
1
, a)r?
j
(f
j
2
| h
2
, b)r?
k
(f
k
3
| h
3
, c)
)
Thus the new objective function consists of a sum
ofL?M
2
terms, each corresponding to a different
combination of inside and outside features. The
function remains concave.
6.3 Use as an Initializer for EM
The learning algorithm for L-PCFGs can be used
as an initializer for the EM algorithm for L-
PCFGs. Two-step estimation methods such as
these are well known in statistics; there are guar-
antees for example that if the first estimator is con-
sistent, and the second step finds the closest local
maxima of the likelihood function, then the result-
ing estimator is both consistent and efficient (in
terms of number of samples required). See for
example page 453 or Theorem 4.3 (page 454) of
(Lehmann and Casella, 1998).
7 Experiments on Parsing
This section describes parsing experiments using
the learning algorithm for L-PCFGs. We use the
Penn WSJ treebank (Marcus et al, 1993) for our
experiments. Sections 2?21 were used as training
data, and sections 0 and 22 were used as develop-
ment data. Section 23 was used as the test set.
The experimental setup is the same as described
by Cohen et al (2013). The trees are bina-
rized (Petrov et al, 2006) and for the EM algo-
rithm we use the initialization method described
sec. 22 sec. 23
m 8 16 24 32
EM
86.69
40
88.32
30
88.35
30
88.56
20
87.76
Spectral 85.60 87.77 88.53 88.82 88.05
Pivot 83.56 86.00 86.87 86.40 85.83
Pivot+EM
86.83
2
88.14
6
88.64
2
88.55
2
88.03
Table 1: Results on the development data (section 22) and
test data (section 23) for various learning algorithms for L-
PCFGs. For EM and pivot+EM experiments, the second line
denotes the number of iterations required to reach the given
optimal performance on development data. Results for sec-
tion 23 are used with the best model for section 22 in the cor-
responding row. The results for EM and spectral are reported
from Cohen et al (2013).
in Matsuzaki et al (2005). For the pivot algo-
rithm we use multiple features ?
1
(t) . . . ?
K
(t) and
?
1
(o) . . . ?
L
(o) over inside and outside trees, us-
ing the features described by Cohen et al (2013).
Table 1 gives the F1 accuracy on the develop-
ment and test sets for the following methods:
EM: The EM algorithm as used by Matsuzaki et
al. (2005) and Petrov et al (2006).
Spectral: The spectral algorithm of Cohen et al
(2012) and Cohen et al (2013).
Pivot: The algorithm described in this paper.
Pivot+EM: The algorithm described in this pa-
per, followed by 1 or more iterations of the
EM algorithm with parameters initialized by the
pivot algorithm. (See section 6.3.)
For the EM and Pivot+EM algorithms, we give
the number of iterations of EM required to reach
optimal performance on the development data.
The results show that the EM, Spectral, and
Pivot+EM algorithms all perform at a very similar
level of accuracy. The Pivot+EM results show that
very few EM iterations?just 2 iterations in most
conditions?are required to reach optimal perfor-
mance when the Pivot model is used as an ini-
tializer for EM. The Pivot results lag behind the
Pivot+EM results by around 2-3%, but they are
close enough to optimality to require very few EM
iterations when used as an initializer.
8 Experiments on the Saul and Pereira
(1997) Model for Language Modeling
We now describe a second set of experiments, on
the Saul and Pereira (1997) model for language
modeling. Define V to be the set of words in the
vocabulary. For any w
1
, w
2
? V , the Saul and
Pereira (1997) model then defines p(w
2
| w
1
) =
?
m
h=1
r(h | w
1
)s(w
2
| h) where r(h | w
1
) and
1059
Brown NYT
m 2 4 8 16 32 128 256 test 2 4 8 16 32 128 256 test
EM
737
14
599
14
488
19
468
12
430
10
388
9
365
8
364
926
36
733
39
562
42
420
33
361
38
284
35
265
32
267
bi-KN +int. 408 415 271 279
tri-KN+int. 386 394 150 158
pivot 852 718 605 559 537 426 597 560 1227 1264 896 717 738 782 886 715
pivot+EM
758
2
582
3
502
2
425
1
374
1
310
1
327
1
357
898
20
754
14
553
13
441
15
394
10
279
19
292
12
281
Table 2: Language model perplexity with the Brown corpus and the Gigaword corpus (New York Times portion) for the second
half of the development set, and the test set. With EM and Pivot+EM, the number of iterations for EM to reach convergence is
given below the perplexity. The best result for each column (for each m value) is in bold. The ?test? column gives perplexity
results on the test set. Each perplexity calculation on the test set is done using the best model on the development set. bi-KN+int
and tri-KN+int are bigram and trigram Kneser-Ney interpolated models (Kneser and Ney, 1995), using the SRILM toolkit.
s(w
2
| h) are parameters of the approach. The
conventional approach to estimation of the param-
eters r(h | w
1
) and s(w
2
| h) from a corpus is
to use the EM algorithm. In this section we com-
pare the EM algorithm to a pivot-based method.
It is straightforward to represent this model as an
L-PCFG, and hence to use our implementation for
estimation.
In this special case, the L-PCFG learning al-
gorithm is equivalent to a simple algorithm, with
the following steps: 1) define the matrix Q
with entries Q
w
1
,w
2
= count(w
1
, w
2
)/N where
count(w
1
, w
2
) is the number of times that bi-
gram (w
1
, w
2
) is seen in the data, and N =
?
w
1
,w
2
count(w
1
, w
2
). Run the algorithm of sec-
tion 5.2 on Q to recover estimates s?(w
2
| h); 2)
estimate r?(h | w
1
) using the EM algorithm to op-
timize the function
?
w
1
,w
2
Q
w
1
,w
2
log
?
h
r?(h |
w
1
)s?(w
2
| h) with respect to the r? parameters;
this function is concave in these parameters.
We performed the language modeling experi-
ments for a number of reasons. First, because in
this case the L-PCFG algorithm reduces to a sim-
ple algorithm, it allows us to evaluate the core
ideas in the method very directly. Second, it al-
lows us to test the pivot method on the very large
datasets that are available for language modeling.
We use two corpora for our experiments. The
first is the Brown corpus, as used by Bengio et
al. (2006) in language modeling experiments. Fol-
lowing Bengio et al (2006), we use the first 800K
words for training (and replace all words that ap-
pear once with an UNK token), the next 200K
words for development, and the remaining data
(165,171 tokens) as a test set. The size of the
vocabulary is 24,488 words. The second corpus
we use is the New York Times portion of the Gi-
gaword corpus. Here, the training set consists of
1.31 billion tokens. We use 159 million tokens for
development set and 156 million tokens for test.
All words that appeared less than 20 times in the
training set were replaced with the UNK token.
The size of the vocabulary is 235,223 words. Un-
known words in test data are ignored when calcu-
lating perplexity (this is the standard set-up in the
SRILM toolkit).
In our experiments we use the first half of each
development set to optimize the number of itera-
tions of the EM or Pivot+EM algorithms. As be-
fore, Pivot+EM uses 1 or more EM steps with pa-
rameter initialization from the Pivot method.
Table 2 gives perplexity results for the differ-
ent algorithms. As in the parsing experiments, the
Pivot method alone performs worse than EM, but
the Pivot+EM method gives results that are com-
petitive with EM. The Pivot+EM method requires
fewer iterations of EM than the EM algorithm.
On the Brown corpus the difference is quite dra-
matic, with only 1 or 2 iterations required, as op-
posed to 10 or more for EM. For the NYT cor-
pus the Pivot+EM method requires more iterations
(around 10 or 20), but still requires significantly
fewer iterations than the EM algorithm.
On the Gigaword corpus, with m = 256, EM
takes 12h57m (32 iterations at 24m18s per itera-
tion) compared to 1h50m for the Pivot method. On
Brown, EM takes 1m47s (8 iterations) compared
to 5m44s for the Pivot method. Both the EM and
pivot algorithm implementations were highly op-
timized, and written in Matlab. Results at other
values of m are similar. From these results the
Pivot method appears to become more competitive
speed-wise as the data size increases (the Giga-
word corpus is more than 1,300 times larger than
the Brown corpus).
9 Conclusion
We have described a new algorithm for parameter
estimation in L-PCFGs. The algorithm is provably
correct, and performs well in practice when used
in conjunction with EM.
1060
References
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and
M. Telgarsky. 2012. Tensor decompositions for
learning latent-variable models. arXiv:1210.7559.
S. Arora, R. Ge, and A. Moitra. 2012. Learning
topic models?going beyond SVD. In Proceedings
of FOCS.
S. Arora, R. Ge, Y. Halpern, D. M. Mimno, A. Moitra,
D. Sontag, Y. Wu, and M. Zhu. 2013. A practical
algorithm for topic modeling with provable guaran-
tees. In Proceedings of ICML.
R. Bailly, A. Habrar, and F. Denis. 2010. A spectral
approach for probabilistic grammatical inference on
trees. In Proceedings of ALT.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
Y. Bengio, H. Schwenk, J.-S. Sen?ecal, F. Morin, and
J.-L. Gauvain. 2006. Neural probabilistic language
models. In Innovations in Machine Learning, pages
137?186. Springer.
K. L. Clarkson. 2010. Coresets, sparse greedy ap-
proximation, and the Frank-Wolfe algorithm. ACM
Transactions on Algorithms (TALG), 6(4):63.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and
L. Ungar. 2013. Experiments with spectral learn-
ing of latent-variable PCFGs. In Proceedings of
NAACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maxi-
mum likelihood estimation from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society B, 39:1?38.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.
Ungar. 2012. Spectral dependency parsing with la-
tent variables. In Proceedings of EMNLP.
M. Frank and P. Wolfe. 1956. An algorithm for
quadratic programming. Naval research logistics
quarterly, 3(1-2):95?110.
H. Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spectral
algorithm for learning hidden Markov models. In
Proceedings of COLT.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
ICASSP.
E. L. Lehmann and G. Casella. 1998. Theory of Point
Estimation (Second edition). Springer.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral
algorithm for latent tree graphical models. In Pro-
ceedings of ICML.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of COLING-ACL.
L. Saul and F. Pereira. 1997. Aggregate and mixed-
order markov models for statistical language pro-
cessing. In Proceedings of EMNLP.
S. Siddiqi, B. Boots, and G. Gordon. 2010. Reduced-
rank hidden markov models. Journal of Machine
Learning Research, 9:741?748.
1061
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062?1072,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Spectral Unsupervised Parsing with Additive Tree Metrics
Ankur P. Parikh
School of Computer Science
Carnegie Mellon University
apparikh@cs.cmu.edu
Shay B. Cohen
School of Informatics
University of Edinburgh
scohen@inf.ed.ac.uk
Eric P. Xing
School of Computer Science
Carnegie Mellon University
epxing@cs.cmu.edu
Abstract
We propose a spectral approach for un-
supervised constituent parsing that comes
with theoretical guarantees on latent struc-
ture recovery. Our approach is grammar-
less ? we directly learn the bracketing
structure of a given sentence without us-
ing a grammar model. The main algorithm
is based on lifting the concept of additive
tree metrics for structure learning of la-
tent trees in the phylogenetic and machine
learning communities to the case where
the tree structure varies across examples.
Although finding the ?minimal? latent tree
is NP-hard in general, for the case of pro-
jective trees we find that it can be found
using bilexical parsing algorithms. Empir-
ically, our algorithm performs favorably
compared to the constituent context model
of Klein and Manning (2002) without the
need for careful initialization.
1 Introduction
Solutions to the problem of grammar induction
have been long sought after since the early days of
computational linguistics and are interesting both
from cognitive and engineering perspectives. Cog-
nitively, it is more plausible to assume that chil-
dren obtain only terminal strings of parse trees and
not the actual parse trees. This means the unsu-
pervised setting is a better model for studying lan-
guage acquisition. From the engineering perspec-
tive, training data for unsupervised parsing exists
in abundance (i.e. sentences and part-of-speech
tags), and is much cheaper than the syntactically
annotated data required for supervised training.
Most existing solutions treat the problem of un-
supervised parsing by assuming a generative pro-
cess over parse trees e.g. probabilistic context
free grammars (Jelinek et al, 1992), and the con-
stituent context model (Klein and Manning, 2002).
Learning then reduces to finding a set of parame-
ters that are estimated by identifying a local max-
imum of an objective function such as the likeli-
hood (Klein and Manning, 2002) or a variant of it
(Smith and Eisner, 2005; Cohen and Smith, 2009;
Headden et al, 2009; Spitkovsky et al, 2010b;
Gillenwater et al, 2010; Golland et al, 2012). Un-
fortunately, finding the global maximum for these
objective functions is usually intractable (Cohen
and Smith, 2012) which often leads to severe lo-
cal optima problems (but see Gormley and Eisner,
2013). Thus, strong experimental results are often
achieved by initialization techniques (Klein and
Manning, 2002; Gimpel and Smith, 2012), incre-
mental dataset use (Spitkovsky et al, 2010a) and
other specialized techniques to avoid local optima
such as count transforms (Spitkovsky et al, 2013).
These approaches, while empirically promising,
generally lack theoretical justification.
On the other hand, recently proposed spectral
methods approach the problem via restriction of
the PCFG model (Hsu et al, 2012) or matrix com-
pletion (Bailly et al, 2013). These novel perspec-
tives offer strong theoretical guarantees but are not
designed to achieve competitive empirical results.
In this paper, we suggest a different approach,
to provide a first step to bridging this theory-
experiment gap. More specifically, we approach
unsupervised constituent parsing from the per-
spective of structure learning as opposed to pa-
rameter learning. We associate each sentence with
an undirected latent tree graphical model, which is
a tree consisting of both observed variables (corre-
sponding to the words in the sentence) and an ad-
ditional set of latent variables that are unobserved
in the data. This undirected latent tree is then di-
rected via a direction mapping to give the final
constituent parse.
In our framework, parsing reduces to finding the
best latent structure for a given sentence. How-
ever, due to the presence of latent variables, struc-
ture learning of latent trees is substantially more
complicated than in observed models. As before,
one solution would be local search heuristics.
Intuitively, however, latent tree models en-
code low rank dependencies among the observed
variables permitting the development of ?spec-
1062
tral? methods that can lead to provably correct
solutions. In particular we leverage the con-
cept of additive tree metrics (Buneman, 1971;
Buneman, 1974) in phylogenetics and machine
learning that can create a special distance met-
ric among the observed variables as a function
of the underlying spectral dependencies (Choi et
al., 2011; Song et al, 2011; Anandkumar et al,
2011; Ishteva et al, 2012). Additive tree met-
rics can be leveraged by ?meta-algorithms? such
as neighbor-joining (Saitou and Nei, 1987) and
recursive grouping (Choi et al, 2011) to provide
consistent learning algorithms for latent trees.
Moreover, we show that it is desirable to learn
the ?minimal? latent tree based on the tree metric
(?minimum evolution? in phylogenetics). While
this criterion is in general NP-hard (Desper and
Gascuel, 2005), for projective trees we find that a
bilexical parsing algorithm can be used to find an
exact solution efficiently (Eisner and Satta, 1999).
Unlike in phylogenetics and graphical models,
where a single latent tree is constructed for all the
data, in our case, each part of speech sequence is
associated with its own parse tree. This leads to a
severe data sparsity problem even for moderately
long sentences. To handle this issue, we present
a strategy that is inspired by ideas from kernel
smoothing in the statistics community (Zhou et al,
2010; Kolar et al, 2010b; Kolar et al, 2010a).
This allows principled sharing of samples from
different but similar underlying distributions.
We provide theoretical guarantees on the re-
covery of the correct underlying latent tree and
characterize the associated sample complexity un-
der our technique. Empirically we evaluate our
method on data in English, German and Chi-
nese. Our algorithm performs favorably to Klein
and Manning?s (2002) constituent-context model
(CCM), without the need for careful initialization.
In addition, we also analyze CCM?s sensitivity to
initialization, and compare our results to Seginer?s
algorithm (Seginer, 2007).
2 Learning Setting and Model
In this section, we detail the learning setting and a
conditional tree model we learn the structure for.
2.1 Learning Setting
Let w = (w
1
, ..., w
`
) be a vector of words corre-
sponding to a sentence of length `. Each w
i
is rep-
resented by a vector in R
p
for p ? N. The vector
is an embedding of the word in some space, cho-
VBD DT NNVBD DT NN
Figure 2: Candidate constituent parses for x = (VBD, DT, NN)
(left-correct, right -incorrect)
sen from a fixed dictionary that maps word types
to R
p
. In addition, let x = (x
1
, ..., x
`
) be the as-
sociated vector of part-of-speech (POS) tags (i.e.
x
i
is the POS tag of w
i
).
In our learning algorithm, we assume that ex-
amples of the form (w
(i)
,x
(i)
) for i ? [N ] =
{1, . . . , N} are given, and the goal is to predict
a bracketing parse tree for each of these examples.
The word embeddings are used during the learn-
ing process, but the final decoder that the learning
algorithm outputs maps a POS tag sequence x to
a parse tree. While ideally we would want to use
the word information in decoding as well, much of
the syntax of a sentence is determined by the POS
tags, and relatively high level of accuracy can be
achieved by learning, for example, a supervised
parser from POS tag sequences.
Just like our decoder, our model assumes that
the bracketing of a given sentence is a function
of its POS tags. The POS tags are generated
from some distribution, followed by a determin-
istic generation of the bracketing parse tree. Then,
latent states are generated for each bracket, and
finally, the latent states at the yield of the bracket-
ing parse tree generate the words of the sentence
(in the form of embeddings). The latent states are
represented by vectors z ? R
m
where m < p.
2.2 Intuition
For intuition, consider the simple tag sequence
x = (VBD, DT, NN). Two candidate constituent
parse structures are shown in Figure 2 and the cor-
rect one is boxed in green (the other in red). Re-
call that our training data contains word phrases
that have the tag sequence x e.g. w
(1)
=
(hit, the, ball), w
(2)
= (ate, an, apple).
Intuitively, the words in the above phrases ex-
hibit dependencies that can reveal the parse struc-
ture. The determiner (w
2
) and the direct object
(w
3
) are correlated in that the choice of deter-
miner depends on the plurality of w
3
. However,
the choice of verb (w
1
) is mostly independent of
the determiner. We could thus conclude that w
2
and w
3
should be closer in the parse tree than w
1
1063
The be ar ate the fish
?1 , ?2 , ?3 , ?4 , ?5 , ?1, ?2, ?3
? = (??,??, ???, ??,??)
?(?)
((DT NN) (VBD (DT NN)))
w 1 w 2 w 3
z 3
z 1
w 4 w 5
z 2
w 1 w 2 w 3
z 3z 1
w 4 w 5
z 2
Figure 1: Example for the tag
sequence (DT, NN, VBD, DT, NN)
showing the overview of our
approach. We first learn a undi-
rected latent tree for the se-
quence (left). We then ap-
ply a direction mapping h
dir
to
direct the latent tree (center).
This can then easily be con-
verted into a bracketing (right).
andw
2
, giving us the correct structure. Informally,
the latent state z corresponding to the (w
2
, w
3
)
bracket would store information about the plural-
ity of z, the key to the dependence betweenw
2
and
w
3
. It would then be reasonable to assume that w
2
and w
3
are independent given z.
2.3 A Conditional Latent Tree Model
Following this intuition, we propose to model the
distribution over the latent bracketing states and
words for each tag sequence x as a latent tree
graphical model, which encodes conditional inde-
pendences among the words given the latent states.
Let V := {w
1
, ..., w
`
, z
1
, ..., z
H
}, with w
i
rep-
resenting the word embeddings, and z
i
represent-
ing the latent states of the bracketings. Then, ac-
cording to our base model it holds that:
p(w, z|x) =
H
?
i=1
p(z
i
|pix(zi), ?(x))
?
`(x)
?
i=1
p(w
i
|pix(wi), ?(x)) (1)
where pix(?) returns the parent node index of the
argument in the latent tree corresponding to tag
sequence x.
1
If z is the root, then pix(z) = ?.
All the w
i
are assumed to be leaves while all the
z
i
are internal (i.e. non-leaf) nodes. The param-
eters ?(x) control the conditional probability ta-
bles. We do not commit to a certain parametric
family, but see more about the assumptions we
make about ? in ?3.2. The parameter space is de-
noted ?. The model assumes a factorization ac-
cording to a latent-variable tree. The latent vari-
ables can incorporate various linguistic properties,
such as head information, valence of dependency
being generated, and so on. This information is
expected to be learned automatically from data.
Our generative model deterministically maps a
POS sequence to a bracketing via an undirected
1
At this point, pi refers to an arbitrary direction of the
undirected tree u(x).
latent-variable tree. The orientation of the tree is
determined by a direction mapping h
dir
(u), which
is fixed during learning and decoding. This means
our decoder first identifies (given a POS sequence)
an undirected tree, and then orients it by applying
h
dir
on the resulting tree (see below).
Define U to be the set of undirected latent trees
where all internal nodes have degree exactly 3 (i.e.
they correspond to binary bracketing), and in addi-
tion h
dir
(u) for any u ? U is projective (explained
in the h
dir
section). In addition, let T be the set
of binary bracketings. The complete generative
model that we follow is then:
? Generate a tag sequence x = (x
1
, . . . , x
`
)
? Decide on u(x) ? U , the undirected latent tree
that x maps to.
? Set t ? T by computing t = h
dir
(u).
? Set ? ? ? by computing ? = ?(x).
? Generate a tuple v = (w
1
, . . . , w
`
, z
1
, ..., z
H
)
where w
i
? R
p
, z
j
? R
m
according to Eq. 1.
See Figure 1 (left) for an example.
The Direction Mapping h
dir
. Generating a
bracketing via an undirected tree enables us to
build on existing methods for structure learning
of latent-tree graphical models (Choi et al, 2011;
Anandkumar et al, 2011). Our learning algorithm
focuses on recovering the undirected tree based
for the generative model that was described above.
This undirected tree is converted into a directed
tree by applying h
dir
. The mapping h
dir
works in
three steps:
? It first chooses a top bracket ([1, R ? 1], [R, `])
where R is the mid-point of the bracket and ` is
the length of the sentence.
? It marks the edge e
i,j
that splits the tree accord-
ing to the top bracket as the ?root edge? (marked
in red in Figure 1(center))
? It then creates t from u by directing the tree out-
ward from e
i,j
as shown in Figure 1(center)
1064
The resulting t is a binary bracketing parse tree.
As implied by the above definition of h
dir
, se-
lecting which edge is the root can be interpreted
as determining the top bracket of the constituent
parse. For example, in Figure 1, the top bracket
is ([1, 2], [3, 5]) = ([DT, NN], [VBD, DT, NN]). Note
that the ?root? edge e
z
1
,z
2
partitions the leaves
into precisely this bracketing. As indicated in the
above section, we restrict the set of undirected
trees to be those such that after applying h
dir
the
resulting t is projective i.e. there are no crossing
brackets. In ?4.1, we discuss an effective heuristic
to find the top bracket without supervision.
3 Spectral Learning Algorithm based on
Additive Tree Metrics
Our goal is to recover t ? T for tag sequence x
using the data D = [(w
(i)
,x
(i)
)]
N
i=1
. To get an in-
tuition about the algorithm, consider a partition of
the set of examplesD intoD(x) = {(w
(i)
,x
(i)
) ?
D|x
(i)
= x}, i.e. each section in the partition has
an identical sequence of part of speech tags. As-
sume for this section |D(x)| is large (we address
the data sparsity issue in ?3.4).
We can then proceed by learning how to map a
POS sequence x to a tree t ? T (through u ? U)
by focusing only on examples in D(x).
Directly attempting to maximize the likelihood
unfortunately results in an intractable optimiza-
tion problem and greedy heuristics are often em-
ployed (Harmeling and Williams, 2011). Instead
we propose a method that is provably consistent
and returns a tree that can be mapped to a bracket-
ing using h
dir
.
If all the variables were observed, then the
Chow-Liu algorithm (Chow and Liu, 1968) could
be used to find the most likely tree structure u ?
U . The Chow-Liu algorithm essentially computes
the distances among all pairs of variables (the neg-
ative of the mutual information) and then finds the
minimum cost tree. However, the fact that the z
i
are latent variables makes this strategy substan-
tially more complicated. In particular, it becomes
challenging to compute the distances among pairs
of latent variables. What is needed is a ?special?
distance function that allows us to reverse engineer
the distances among the latent variables given the
distances among the observed variables. This is
the key idea behind additive tree metrics that are
the basis of our approach.
In the following sections, we describe the key
steps to our method. ?3.1 and ?3.2 largely describe
existing background on additive tree metrics and
latent tree structure learning, while ?3.3 and ?3.4
discuss novel aspects that are unique to our prob-
lem.
3.1 Additive Tree Metrics
Let u(x) be the true undirected tree of sentence x
and assume the nodes V to be indexed by [M ] =
{1, . . . ,M} such that M = |V| = H + `. Fur-
thermore, let v ? V refer to a node in the undi-
rected tree (either observed or latent). We assume
the existence of a distance function that allows us
to compute distances between pairs of nodes. For
example, as we see in ?3.2 we will define the dis-
tance d(i, j) to be a function of the covariance ma-
trix E[v
i
v
>
j
|u(x), ?(x)]. Thus if v
i
and v
j
are both
observed variables, the distance can be directly
computed from the data.
Moreover, the metrics we construct are such
that they are tree additive, defined below:
Definition 1 A function d
u(x) : [M ]?[M ]? R is
an additive tree metric (Erd?os et al, 1999) for the
undirected tree u(x) if it is a distance metric,
2
and
furthermore, ?i, j ? [M ] the following relation
holds:
d
u(x)(i, j) =
?
(a,b)?path
u(x)(i,j)
d
u(x)(a, b) (2)
where path
u(x)(i, j) is the set of all the edges in
the (undirected) path from i to j in the tree u(x).
As we describe below, given the tree structure,
the additive tree metric property allows us to com-
pute ?backwards? the distances among the latent
variables as a function of the distances among the
observed variables.
Define D to be the M ? M distance matrix
among the M variables, i.e. D
ij
= d
u(x)(i, j).
LetD
WW
, D
ZW
(equal toD
>
WZ
), andD
ZZ
indi-
cate the word-word, latent-word and latent-latent
sub-blocks of D respectively. In addition, since
u(x) is assumed to be known from context, we
denote d
u(x)(i, j) just by d(i, j).
Given the fact that the distance between a pair
of nodes is a function of the random variables
they represent (according to the true model), only
D
WW
can be empirically estimated from data.
However, if the underlying tree structure is known,
then Definition 1 can be leveraged to compute
D
ZZ
and D
ZW
as we show below.
2
This means that it satisfies d(i, j) = 0 if and only if
i = j, the triangle inequality and is also symmetric.
1065
v j v ie i , j
(a)
v ie i , jv j
(b)
Figure 3: Two types of edges in general undirected latent
trees. (a) leaf edge, (b) internal edge
We first show how to compute d(i, j) for all i, j
such that i and j are adjacent to each other in u(x),
based only on observed nodes. It then follows that
the other elements of the distance matrix can be
computed based on Definition 1. To show how to
compute distances between adjacent nodes, con-
sider the two cases: (1) (i, j) is a leaf edge; (2)
(i, j) is an internal edge.
Case 1 (leaf edge, figure 3(a)) Assume without
loss of generality that j is the leaf and i is an in-
ternal latent node. Then i must have exactly two
other neighbors a ? [M ] and b ? [M ]. Let A
denote the set of nodes that are closer to a than
i and similarly let B denote the set of nodes that
are closer to b than i. Let A
?
and B
?
denote all
the leaves (word nodes) in A and B respectively.
Then using path additivity (Definition 1), it can be
shown that for any a
?
? A
?
, b
?
? B
?
it holds that:
d(i, j) =
1
2
(d(j, a
?
) + d(j, b
?
)? d(a
?
, b
?
)) (3)
Note that the right-hand side only depends on
distances between observed random variables.
Case 2 (internal edge, figure 3(b)) Both i and
j are internal nodes. In this case, i has exactly
two other neighbors a ? [M ] and b ? [M ], and
similarly, j has exactly other two neighbors g ?
[M ] and h ? [M ]. Let A denote the set of nodes
closer to a than i, and analogously for B, G, and
H . Let A
?
, B
?
, G
?
, and H
?
refer to the leaves in
A,B,G, and H respectively. Then for any a
?
?
A
?
, b
?
? B
?
, g
?
? G
?
, and h
?
? H
?
it can be
shown that:
d(i, j) =
1
4
(
d(a
?
, g
?
) + d(a
?
, h
?
) + d(b
?
, g
?
)
+d(b
?
, h
?
)? 2d(a
?
, b
?
)? 2d(g
?
, h
?
)
)
(4)
Empirically, one can obtain a more robust em-
pirical estimate
?
d(i, j) by averaging over all valid
choices of a
?
, b
?
in Eq. 3 and all valid choices of
a
?
, b
?
, g
?
, h
?
in Eq. 4 (Desper and Gascuel, 2005).
3.2 Constructing a Spectral Additive Metric
In constructing our distance metric, we begin with
the following assumption on the distribution in
Eq. 1 (analogous to the assumptions made in
Anandkumar et al, 2011).
Assumption 1 (Linear, Rank m, Means)
E[z
i
|pix(zi),x] = A
(z
i
|z
pix(z
i
)
,x)pix(zi) ?i ? [H]
where A
(z
i
|pix(z
i
),x) ? R
m?m
has rank m.
E[w
i
|pix(wi),x] = C
(w
i
|pix(w
i
),x)pix(wi) ?i ? [`(x)]
where C
(w
i
|pix(w
i
),x) ? R
p?m
has rank m.
Also assume that E[z
i
z
>
i
|x] has rank m ?i ?
[H].
Note that the matrices A and C are a direct
function of ?(x), but we do not specify a model
family for ?(x). The only restriction is in the form
of the above assumption. If w
i
and z
i
were dis-
crete, represented as binary vectors, the above as-
sumption would correspond to requiring all con-
ditional probability tables in the latent tree to have
rankm. Assumption 1 allows for the w
i
to be high
dimensional features, as long as the expectation
requirement above is satisfied. Similar assump-
tions are made with spectral parameter learning
methods e.g. Hsu et al (2009), Bailly et al (2009),
Parikh et al (2011), and Cohen et al (2012).
Furthermore, Assumption 1 makes it explicit
that regardless of the size of p, the relationships
among the variables in the latent tree are restricted
to be of rank m, and are thus low rank since p >
m. To leverage this low rank structure, we propose
using the following additive metric, a normalized
variant of that in Anandkumar et al (2011):
d
spectral
(i, j) = ? log ?
m
(?x(i, j))
+
1
2
log ?
m
(?x(i, i)) +
1
2
log ?
m
(?x(j, j)) (5)
where ?
m
(A) denotes the product of the top m
singular values of A and ?x(i, j) := E[viv
>
j
|x],
i.e. the uncentered cross-covariance matrix.
We can then show that this metric is additive:
Lemma 1 If Assumption 1 holds then, d
spectral
is
an additive tree metric (Definition 1).
A proof is in the supplementary for completeness.
From here, we use d to denote d
spectral
, since that
is the metric we use for our learning algorithm.
1066
3.3 Recovering the Minimal Projective
Latent Tree
It has been shown (Rzhetsky and Nei, 1993) that
for any additive tree metric, u(x) can be recovered
by solving arg min
u?U
c(u) for c(u):
c(u) =
?
(i,j)?E
u
d(i, j). (6)
where E
u
is the set of pairs of nodes which are
adjacent to each other in u and d(i, j) is computed
using Eq. 3 and Eq. 4.
Note that the metric d we use in defining c(u)
is based on the expectations from the true distri-
bution. In practice, the true distribution is un-
known, and therefore we use an approximation for
the distance metric
?
d. As we discussed in ?3.1
all elements of the distance matrix are functions
of observable quantities if the underlying tree u is
known. However, only the word-word sub-block
D
WW
can be directly estimated from the data
without knowledge of the tree structure.
This subtlety makes solving the minimization
problem in Eq. 6 NP-hard (Desper and Gascuel,
2005) if u is allowed to be an arbitrary undirected
tree. However, if we restrict u to be in U , as we do
in the above, then maximizing c?(u) over U can be
solved using the bilexical parsing algorithm from
Eisner and Satta (1999). This is because the com-
putation of the other sub-blocks of the distance
matrix only depend on the partitions of the nodes
shown in Figure 3 into A, B, G, and H , and not
on the entire tree structure.
Therefore, the procedure to find a bracketing
for a given POS tag x is to first estimate the dis-
tance matrix sub-block
?
D
WW
from raw text data
(see ?3.4), and then solve the optimization prob-
lem arg min
u?U
c?(u) using a variant of the Eisner-
Satta algorithm where c?(u) is identical to c(u) in
Eq. 6, with d replaced with
?
d.
Summary. We first defined a generative model
that describes how a sentence, its sequence of POS
tags, and its bracketing is generated (?2.3). First
an undirected u ? U is generated (only as a func-
tion of the POS tags), and then u is mapped to
a bracketing using a direction mapping h
dir
. We
then showed that we can define a distance met-
ric between nodes in the undirected tree, such that
minimizing it leads to a recovery of u. This dis-
tance metric can be computed based only on the
text, without needing to identify the latent infor-
mation (?3.2). If the true distance metric is known,
Algorithm 1 The learning algorithm for find-
ing the latent structure from a set of examples
(w
(i)
,x
(i)
), i ? [N ].
Inputs: Set of examples (w
(i)
,x
(i)
) for i ? [N ],
a kernel K
?
(j, k, j
?
, k
?
|x,x
?
), an integer m
Data structures: For each i ? [N ], j, k ?
`(x
(i)
) there is a (uncentered) covariance matrix
?
?x(i)(j, k) ? R
p?p
, and a distance
?
d
spectral
(j, k).
Algorithm:
(Covariance estimation) ?i ? [N ], j, k ? `(x
(i)
)
? Let C
j
?
,k
?
|i
? = w
(i
?
)
j
?
(w
(i
?
)
k
?
)
>
, k
j,k,j
?
,k
?
,i,i
?
=
K
?
(j, k, j
?
, k
?
|x
(i)
,x
(i
?
)
) and `
i
?
= `(x
(i
?
)
),
and estimate each p? p covariance matrix as:
?
?x(j, k) =
?
N
i
?
=1
?
`
i
?
j
?
=1
?
`
i
?
k
?
=1
k
j,k,j
?
,k
?
,i,i
?
C
j
?
,k
?
|i
?
?
N
i
?
=1
?
`
i
?
j
?
=1
?
`
i
?
k
?
=1
k
j,k,j
?
,k
?
,i,i
?
? Compute
?
d
spectral
(j, k) ?j, k ? `(x
(i)
) using
Eq. 5.
(Uncover structure) ?i ? [N ]
? Find u?
(i)
= arg min
u?U
c?(u), and for the ith
example, return the structure h
dir
(u?
(i)
).
with respect to the true distribution that generates
the words in a sentence, then u can be fully recov-
ered by optimizing the cost function c(u). How-
ever, in practice the distance metric must be esti-
mated from data, as discussed below.
3.4 Estimation of d from Sparse Data
We now address the data sparsity problem, in par-
ticular that D(x) can be very small, and therefore
estimating d for each POS sequence separately can
be problematic.
3
In order to estimate d from data, we need to es-
timate the covariance matrices ?x(i, j) (for i, j ?
{1, . . . , `(x)}) from Eq. 5.
To give some motivation to our solu-
tion, consider estimating the covariance
matrix ?x(1, 2) for the tag sequence
x = (DT
1
, NN
2
, VBD
3
, DT
4
, NN
5
). D(x) may
be insufficient for an accurate empirical es-
3
This data sparsity problem is quite severe ? for example,
the Penn treebank (Marcus et al, 1993) has a total number
of 43,498 sentences, with 42,246 unique POS tag sequences,
averaging |D(x)| to be 1.04.
1067
timate. However, consider another sequence
x
?
= (RB
1
, DT
2
, NN
3
, VBD
4
, DT
5
, ADJ
6
, NN
7
).
Although x and x
?
are not identical, it is likely
that ?x?(2, 3) is similar to ?x(1, 2) because the
determiner and the noun appear in similar syn-
tactic context. ?x?(5, 7) also may be somewhat
similar, but ?x?(2, 7) should not be very similar
to ?x(1, 2) because the noun and the determiner
appear in a different syntactic context.
The observation that the covariance matrices
depend on local syntactic context is the main driv-
ing force behind our solution. The local syntactic
context acts as an ?anchor,? which enhances or re-
places a word index in a sentence with local syn-
tactic context. More formally, an anchor is a func-
tion G that maps a word index j and a sequence of
POS tags x to a local context G(j,x). The anchor
we use is G(j,x) = (j, x
j
). Then, the covariance
matrices ?x are estimated using kernel smooth-
ing (Hastie et al, 2009), where the smoother tests
similarity between the different anchors G(j,x).
The full learning algorithm is given in Figure 1.
The first step in the algorithm is to estimate the
covariance matrix block
?
?x(i)(j, k) for each train-
ing example x
(i)
and each pair of preterminal po-
sitions (j, k) in x
(i)
. Instead of computing this
block by computing the empirical covariance ma-
trix for positions (j, k) in the data D(x), the al-
gorithm uses all of the pairs (j
?
, k
?
) from all of
N training examples. It averages the empirical
covariance matrices from these contexts using a
kernel weight, which gives a similarity measure
for the position (j, k) in x
(i)
and (j
?
, k
?
) in an-
other example x
(i
?
)
. ? is the kernel ?bandwidth?,
a user-specified parameter that controls how in-
clusive the kernel will be with respect to exam-
ples in D (see ? 4.1 for a concrete example). Note
that the learning algorithm is such that it ensures
that
?
?x(i)(j, k) =
?
?x(i?)(j
?
, k
?
) if G(j,x
(i)
) =
G(j
?
,x
(i
?
)
) and G(k,x
(i)
) = G(k
?
,x
(i
?
)
).
Once the empirical estimates for the covariance
matrices are obtained, a variant of the Eisner-Satta
algorithm is used, as mentioned in ?3.3.
3.5 Theoretical Guarantees
Our main theoretical guarantee is that Algorithm 1
will recover the correct tree u ? U with high prob-
ability, if the given top bracket is correct and if
we obtain enough examples (w
(i)
,x
(i)
) from the
model in ?2. We give the theorem statement be-
low. The constants lurking in the O-notation and
the full proof are in the supplementary.
Denote ?x(j, k)
(r)
as the r
th
singu-
lar value of ?x(j, k). Let ?
?
(x) :=
min
j,k?`(x) min
(
?x(j, k)
(m)
)
.
Theorem 1 Define u? as the estimated tree for tag
sequence x and u(x) as the correct tree. Let
4(x) := min
u
?
?U :u
?
6=u(x)
(c(u(x))? c(u
?
))/(8|`(x)|)
Assume that
N ? O
?
?
m
2
log
(
p
2
`(x)2
?
)
min(?
?
(x)
2
4(x)
2
, ?
?
(x)
2
)?x(?)
2
?
?
Then with probability 1? ?, u? = u(x).
where ?x(?), defined in the supplementary, is a
function of the underlying distribution over the tag
sequences x and the kernel bandwidth ?.
Thus, the sample complexity of our approach
depends on the dimensionality of the latent and
observed states (m and p), the underlying singu-
lar values of the cross-covariance matrices (?
?
(x))
and the difference in the cost of the true tree com-
pared to the cost of the incorrect trees (4(x)).
4 Experiments
We report results on three different languages: En-
glish, German, and Chinese. For English we use
the Penn treebank (Marcus et al, 1993), with sec-
tions 2?21 for training and section 23 for final
testing. For German and Chinese we use the Ne-
gra treebank and the Chinese treebank respectively
and the first 80% of the sentences are used for
training and the last 20% for testing. All punc-
tuation from the data is removed.
4
We primarily compare our method to the
constituent-context model (CCM) of Klein and
Manning (2002). We also compare our method to
the algorithm of Seginer (2007).
4.1 Experimental Settings
Top bracket heuristic Our algorithm requires
the top bracket in order to direct the latent tree.
In practice, we employ the following heuristic to
find the bracket using the following three steps:
? If there exists a comma/semicolon/colon at in-
dex i that has at least a verb before i and both
a noun followed by a verb after i, then return
([0, i ? 1], [i, `(x)]) as the top bracket. (Pick
the rightmost comma/semicolon/colon if multi-
ple satisfy the criterion).
4
We make brief use of punctuation for our top bracket
heuristic detailed below before removing it.
1068
Length CCM CCM-U CCM-OB CCM-UB
? 10 72.5 57.1 58.2 62.9
? 15 54.1 36 24 23.7
? 20 50 34.7 19.3 19.1
? 25 47.2 30.7 16.8 16.6
? 30 44.8 29.6 15.3 15.2
? 40 26.3 13.5 13.9 13.8
Table 1: Comparison of different CCM variants on English
(training). U stands for universal POS tagset, OB stands for
conjoining original POS tags with Brown clusters and UB
stands for conjoining universal POS tags with Brown clusters.
The best setting is just the vanilla setting, CCM.
? Otherwise find the first non-participle verb (say
at index j) and return ([0, j ? 1], [j, `(x)]).
? If no verb exists, return ([0, 1], [1, `(x)]).
Word embeddings As mentioned earlier, each
w
i
can be an arbitrary feature vector. For all lan-
guages we use Brown clustering (Brown et al,
1992) to construct a log(C) + C feature vector
where the first log(C) elements indicate which
mergable cluster the word belongs to, and the last
C elements indicate the cluster identity. For En-
glish, more sophisticated word embeddings are
easily obtainable, and we experiment with neural
word embeddings Turian et al (2010) of length
50. We also explored two types of CCA embed-
dings: OSCCA and TSCCA, given in Dhillon et
al. (2012). The OSCCA embeddings behaved bet-
ter, so we only report its results.
Choice of kernel For our experiments, we use
the kernel
K
?
(j, k, j
?
, k
?
|x,x
?
)
= max
{
0, 1?
?(j, k, j
?
, k
?
|x,x
?
)
?
}
where ? denotes the user-specified bandwidth,
and ?(j, k, j
?
, k
?
|x,x
?
) =
|j ? k| ? |j
?
? k
?
|
|j ? k|+ |j
?
? k
?
|
if
x(j) = x(j
?
) and x(k
?
) = x(k), and sign(j ?
k) = sign(j
?
? k
?
) (and? otherwise).
The kernel is non-zero if and only if the tags at
position j and k in x are identical to the ones in
position j
?
and k
?
in x
?
, and if the direction be-
tween j and k is identical to the one between j
?
and k
?
. Note that the kernel is not binary, as op-
posed to the theoretical kernel in the supplemen-
tary material. Our experiments show that using a
non-zero value different than 1 that is a function
of the distance between j and k compared to the
distance between j
?
and k
?
does better in practice.
Choice of data For CCM, we found that if the
full dataset (all sentence lengths) is used in train-
ing, then performance degrades when evaluating
on sentences of length ? 10. We therefore restrict
the data used with CCM to sentences of length
? `, where ` is the maximal sentence length being
evaluated. This does not happen with our algo-
rithm, which manages to leverage lexical informa-
tion whenever more data is available. We therefore
use the full data for our method for all lengths.
We also experimented with the original POS
tags and the universal POS tags of Petrov et al
(2011). Here, we found out that our method
does better with the universal part of speech tags.
For CCM, we also experimented with the origi-
nal parts of speech, universal tags (CCM-U), the
cross-product of the original parts of speech with
the Brown clusters (CCM-OB), and the cross-
product of the universal tags with the Brown clus-
ters (CCM-UB). The results in Table 1 indicate
that the vanilla setting is the best for CCM.
Thus, for all results, we use universal tags for
our method and the original POS tags for CCM.
We believe that our approach substitutes the need
for fine-grained POS tags with the lexical informa-
tion. CCM, on the other hand, is fully unlexical-
ized.
Parameter Selection Our method requires two
parameters, the latent dimension m and the band-
width ?. CCM also has two parameters, the num-
ber of extra constituent/distituent counts used for
smoothing. For both methods we chose the best
parameters for sentences of length ` ? 10 on the
English Penn Treebank (training) and used this
set for all other experiments. This resulted in
m = 7, ? = 0.4 for our method and 2, 8 for
CCM?s extra constituent/distituent counts respec-
tively. We also tried letting CCM choose differ-
ent hyperparameters for different sentence lengths
based on dev-set likelihood, but this gave worse
results than holding them fixed.
4.2 Results
Test I: Accuracy Table 2 summarizes our re-
sults. CCM is used with the initializer proposed
in Klein and Manning (2002).
5
NN, CC, and BC
indicate the performance of our method for neural
embeddings, CCA embeddings, and Brown clus-
tering respectively, using the heuristic for h
dir
de-
5
We used the implementation available at
http://tinyurl.com/lhwk5n6.
1069
` English German Chinese
NN-O NN CC-O CC BC-O BC CCM BC-O BC CCM BC-O BC CCM
t
r
a
i
n
? 10 70.9 69.2 70.4 68.7 71.1 69.3 72.5 64.6 59.9 62.6 64.9 57.3 46.1
? 20 55.1 53.5 53.2 51.6 53.0 51.5 50 52.7 48.7 47.9 51.4 46 22.4
? 40 46.1 44.5 43.6 41.9 43.3 41.8 26.3 46.7 43.6 19.8 42.6 38.6 15
t
e
s
t
? 10 69.2 66.7 68.3 65.5 68.9 66.1 70.5 66.4 61.6 64.7 58.0 53.2 40.7
? 15 60.3 58.3 58.6 56.4 58.6 56.5 53.8 57.5 53.5 49.6 54.3 49.4 35.9
? 20 54.1 52.3 52.3 50.3 51.9 50.2 50.4 52.8 49.2 48.9 49.7 45.5 20.1
? 25 50.8 49.0 48.6 46.6 48.3 46.6 47.4 50.0 46.8 45.6 46.7 42.7 17.8
? 30 48.1 46.3 45.6 43.7 45.4 43.8 44.9 48.3 45.4 21.9 44.6 40.7 16.1
? 40 45.5 43.8 43.0 41.1 42.7 41.1 26.1 46.9 44.1 20.1 42.2 38.6 14.3
Table 2: F
1
bracketing measure for the test sets and train sets in three languages. NN, CC, and BC indicate the performance of
our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for h
dir
described
in ? 4.1. NN-O, CC-O, and BC-O indicate that the oracle (i.e. true top bracket) was used for h
dir
.
0
5
1 0
1 5
2 0
2 5
3 0
3 5
20- 30 31- 40 41- 50 51- 60 61- 70 71- 80
Fre
que
ncy
 
Bracketing F1 
CCM Random Restarts (Length <= 10) 
Figure 4: Histogram showing performance of CCM across
100 random restarts for sentences of length ? 10.
scribed in ? 4.1. NN-O, CC-O, and BC-O indicate
that the oracle (i.e. true top bracket) was used for
h
dir
. For our method, test set results can be ob-
tained by using Algorithm 1 (except the distances
are computed using the training data).
For English, while CCM behaves better for
short sentences (` ? 10), our algorithm is more
robust with longer sentences. This is especially
noticeable for length ? 40, where CCM breaks
down and our algorithm is more stable. We find
that the neural embeddings modestly outperform
the CCA and Brown cluster embeddings.
The results for German are similar, except CCM
breaks down earlier at sentences of ` ? 30. For
Chinese, our method substantially outperforms
CCM for all lengths. Note that CCM performs
very poorly, obtaining only around 20% accu-
racy even for sentences of ` ? 20. We didn?t
have neural embeddings for German and Chinese
(which worked best for English) and thus only
used Brown cluster embeddings.
For English, the disparity between NN-O (ora-
cle top bracket) and NN (heuristic top bracket) is
rather low suggesting that our top bracket heuris-
tic is rather effective. However, for German and
Chinese note that the ?BC-O? performs substan-
tially better, suggesting that if we had a better top
bracket heuristic our performance would increase.
Test II: Sensitivity to initialization The EM al-
gorithm with the CCM requires very careful ini-
tialization, which is described in Klein and Man-
ning (2002). If, on the other hand, random ini-
tialization is used, the variance of the performance
of the CCM varies greatly. Figure 4 shows a his-
togram of the performance level for sentences of
length ? 10 for different random initializers. As
one can see, for some restarts, CCM obtains ac-
curacies lower than 30% due to local optima. Our
method does not suffer from local optima and thus
does not require careful initialization.
Test III: Comparison to Seginer?s algorithm
Our approach is not directly comparable to
Seginer?s because he uses punctuation, while we
use POS tags. Using Seginer?s parser we were
able to get results on the training sets. On English:
75.2% (` ? 10), 64.2% (` ? 20), 56.7% (` ? 40).
On German: 57.8% (` ? 10), 45.0% (` ? 20), and
39.9% (` ? 40). On Chinese: 56.6% (` ? 10),
45.1% (` ? 20), and 38.9% (` ? 40).
Thus, while Seginer?s method performs better
on English, our approach performs 2-3 points bet-
ter on German, and both methods give similar per-
formance on Chinese.
5 Conclusion
We described a spectral approach for unsu-
pervised constituent parsing that comes with
theoretical guarantees on latent structure recovery.
Empirically, our algorithm performs favorably to
the CCM of Klein and Manning (2002) without
the need for careful initialization.
Acknowledgements: This work is supported
by NSF IIS1218282, NSF IIS1111142, NIH
R01GM093156, and the NSF Graduate Research
Fellowship Program under Grant No. 0946825
(NSF Fellowship to APP).
1070
References
A. Anandkumar, K. Chaudhuri, D. Hsu, S. M. Kakade,
L. Song, and T. Zhang. 2011. Spectral methods
for learning multivariate latent tree structure. arXiv
preprint arXiv:1107.1283.
R. Bailly, F. Denis, and L. Ralaivola. 2009. Gram-
matical inference as a principal component analysis
problem. In Proceedings of ICML.
R. Bailly, X. Carreras, F. M. Luque, and A. Quattoni.
2013. Unsupervised spectral learning of WCFG
as low-rank matrix completion. In Proceedings of
EMNLP.
P. F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra,
and J.C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational linguistics,
18(4):467?479.
O. P. Buneman. 1971. The recovery of trees from mea-
sures of dissimilarity. Mathematics in the archaeo-
logical and historical sciences.
P. Buneman. 1974. A note on the metric properties of
trees. Journal of Combinatorial Theory, Series B,
17(1):48?50.
M.J. Choi, V. YF Tan, A. Anandkumar, and A.S. Will-
sky. 2011. Learning latent tree graphical mod-
els. The Journal of Machine Learning Research,
12:1771?1812.
C. K. Chow and C. N. Liu. 1968. Approximating
Discrete Probability Distributions With Dependence
Trees. IEEE Transactions on Information Theory,
IT-14:462?467.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in un-
supervised grammar induction. In Proceedings of
HLT-NAACL.
S. B. Cohen and N. A. Smith. 2012. Empirical risk
minimization for probabilistic grammars: Sample
complexity and hardness of learning. Computa-
tional Linguistics, 38(3):479?526.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
R. Desper and O. Gascuel. 2005. The minimum evo-
lution distance-based approach to phylogenetic in-
ference. Mathematics of evolution and phylogeny,
pages 1?32.
P. S. Dhillon, J. Rodu, D. P. Foster, and L. H. Ungar.
2012. Two step cca: A new spectral method for es-
timating vector models of words. In Proceedings of
ICML.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proceedings of ACL.
P. Erd?os, M. Steel, L. Sz?ekely, and T. Warnow. 1999.
A few logs suffice to build (almost) all trees: Part ii.
Theoretical Computer Science, 221(1):77?118.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar
induction. In Proceedings of ACL.
K. Gimpel and N.A. Smith. 2012. Concavity and ini-
tialization for unsupervised dependency parsing. In
Proceedings of NAACL.
D. Golland, J. DeNero, and J. Uszkoreit. 2012. A
feature-rich constituent context model for grammar
induction. In Proceedings of ACL.
M. Gormley and J. Eisner. 2013. Nonconvex global
optimization for latent-variable models. In Proceed-
ings of ACL.
S. Harmeling and C. KI Williams. 2011. Greedy
learning of binary latent trees. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
33(6):1087?1097.
T. Hastie, R. Tibshirani, and J. Friedman. 2009. The
Elements of Statistical Learning: Data Mining, In-
ference, and Prediction. Springer Series in Statis-
tics. Springer Verlag.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of
NAACL-HLT.
D. Hsu, S. Kakade, and T. Zhang. 2009. A spectral
algorithm for learning hidden Markov models. In
Proceedings of COLT.
D. Hsu, S. M. Kakade, and P. Liang. 2012. Identi-
fiability and unmixing of latent parse trees. arXiv
preprint arXiv:1206.3137.
M. Ishteva, H. Park, and L. Song. 2012. Unfolding
latent tree structures using 4th order tensors. arXiv
preprint arXiv:1210.1258.
F. Jelinek, J. D. Lafferty, and R. L. Mercer. 1992. Ba-
sic methods of probabilistic context free grammars.
Springer.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of ACL.
M. Kolar, A. P. Parikh, and E. P. Xing. 2010a. On
sparse nonparametric conditional covariance selec-
tion. In Proceedings of ICML.
M. Kolar, L. Song, A. Ahmed, and E. P. Xing. 2010b.
Estimating time-varying networks. The Annals of
Applied Statistics, 4(1):94?123.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
1071
A.P. Parikh, L. Song, and E.P. Xing. 2011. A spectral
algorithm for latent tree graphical models. In Pro-
ceedings of ICML.
S. Petrov, D. Das, and R. McDonald. 2011. A univer-
sal part-of-speech tagset. ArXiv:1104.2086.
A. Rzhetsky and M. Nei. 1993. Theoretical founda-
tion of the minimum-evolution method of phyloge-
netic inference. Molecular Biology and Evolution,
10(5):1073?1095.
N. Saitou and M. Nei. 1987. The neighbor-joining
method: a new method for reconstructing phylo-
genetic trees. Molecular biology and evolution,
4(4):406?425.
Y. Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of ACL.
L. Song, A.P. Parikh, and E.P. Xing. 2011. Kernel
embeddings of latent tree graphical models. In Pro-
ceedings of NIPS.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From baby steps to leapfrog: how less is more in
unsupervised dependency parsing. In Proceedings
of NAACL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010b. Viterbi training improves un-
supervised dependency parsing. In Proceedings of
CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2013.
Breaking out of local optima with count transforms
and model recombination: A study in grammar in-
duction. In Proceedings of EMNLP.
J. P. Turian, L.-A. Ratinov, and Y. Bengio. 2010. Word
representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL.
S. Zhou, J. Lafferty, and L. Wasserman. 2010. Time
varying undirected graphs. Machine Learning,
80(2-3):295?319.
1072
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 19?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Social Links from Latent Topics in Microblogs?
Kriti Puniyani and Jacob Eisenstein and Shay Cohen and Eric P. Xing
School of Computer Science
Carnegie Mellon University
{kpuniyan,jacobeis,scohen,epxing}@cs.cmu.edu
1 Introduction
Language use is overlaid on a network of social con-
nections, which exerts an influence on both the topics
of discussion and the ways that these topics can be ex-
pressed (Halliday, 1978). In the past, efforts to under-
stand this relationship were stymied by a lack of data, but
social media offers exciting new opportunities. By com-
bining large linguistic corpora with explicit representa-
tions of social network structures, social media provides
a new window into the interaction between language and
society. Our long term goal is to develop joint sociolin-
guistic models that explain the social basis of linguistic
variation.
In this paper we focus on microblogs: internet jour-
nals in which each entry is constrained to a few words
in length. While this platform receives high-profile at-
tention when used in connection with major news events
such as natural disasters or political turmoil, less is
known about the themes that characterize microblogging
on a day-to-day basis. We perform an exploratory anal-
ysis of the content of a well-known microblogging plat-
form (Twitter), using topic models to uncover latent se-
mantic themes (Blei et al, 2003). We then show that these
latent topics are predictive of the network structure; with-
out any supervision, they predict which other microblogs
a user is likely to follow, and to whom microbloggers will
address messages. Indeed, our topical link predictor out-
performs a competitive supervised alternative from tra-
ditional social network analysis. Finally, we explore the
application of supervision to our topical link predictor,
using regression to learn weights that emphasize topics
of particular relevance to the social network structure.
2 Data
We acquired data from Twitter?s streaming ?Gardenhose?
API, which returned roughly 15% of all messages sent
over a period of two weeks in January 2010. This com-
?We thank the reviews for their helpful suggestions and Brendan
O?Connor for making the Twitter data available.
prised 15GB of compressed data; we aimed to extract a
representative subset by first sampling 500 people who
posted at least sixteen messages over this period, and
then ?crawled? at most 500 randomly-selected followers
of each of these original authors. The resulting data in-
cludes 21,306 users, 837,879 messages, and 10,578,934
word tokens.
Text Twitter contains highly non-standard orthography
that poses challenges for early-stage text processing.1 We
took a conservative approach to tokenization, splitting
only on whitespaces and apostrophes, and eliminating
only token-initial and token-final punctuation characters.
Two markers are used to indicate special tokens: #, indi-
cating a topic (e.g. #curling); and @, indicating that the
message is addressed to another user. Topic tokens were
included after stripping the leading #, but address tokens
were removed. All terms occurring less than 50 times
were removed, yielding a vocabulary of 11,425 terms.
Out-of-vocabulary items were classified as either words,
URLs, or numbers. To ensure a fair evaluation, we re-
moved ?retweets? ? when a user reposts verbatim the
message of another user ? if the original message author
is also part of the dataset.
Links We experiment with two social graphs extracted
from the data: a follower graph and a communication
graph. The follower graph places directed edges between
users who have chosen to follow each other?s updates;
the message graph places a directed edge between users
who have addressed messages to each other (using the @
symbol). Huberman et al (2009) argue that the commu-
nication graph captures direct interactions and is thus a
more accurate representation of the true underlying social
structure, while the follower graph contains more con-
nections than could possibly be maintained in a realistic
social network.
1For example, some tweets use punctuation for tokenization (You
look like a retired pornstar!lmao) while others
use punctuation inside the token (lOv!n d!s th!ng call3d
l!f3).
19
Figure 1: Mean rank of test links (lower is better), reported over 4-fold cross-validation. Common-neighbors is a network-based
method that ignores text; the LDA (Latent Dirichlet Allocation) methods are grouped by number of latent topics.
3 Method
We constructed a topic model over twitter messages,
identifying the latent themes that characterize the cor-
pus. In standard topic modeling methodology, topics de-
fine distributions over vocabulary items, and each docu-
ment contains a set of latent topic proportions (Blei et al,
2003). However, the average message on Twitter is only
sixteen word tokens, which is too sparse for traditional
topic modeling; instead, we gathered together all of the
messages from a given user into a single document. Thus
our model learns the latent topics that characterize au-
thors, rather than messages.
Authors with similar topic proportions are likely to
share interests or dialect, suggesting potential social con-
nections. Author similarity can be quantified without
supervision by taking the dot product of the topic pro-
portions. If labeled data is available (a partially ob-
served network), then regression can be applied to learn
weights for each topic. Chang and Blei (2009) describe
such a regression-based predictor, which takes the form
exp
(
??T (z?i ? z?j) ? (z?i ? z?j)? ?
)
, denoting the pre-
dicted strength of connection between authors i and j.
Here z?i (z?j) refers to the expected topic proportions for
user i (j), ? is a vector of learned regression weights, and
? is an intercept term which is only necessary if a the link
prediction function must return a probability. We used
the updates from Chang and Blei to learn ? in a post hoc
fashion, after training the topic model.
4 Results
We constructed topic models using an implemen-
tation of variational inference2 for Latent Dirich-
let Allocation (LDA). The results of the run with
the best variational bound on 50 topics can be
found at http://sailing.cs.cmu.edu/
socialmedia/naacl10ws/. While many of
the topics focus on content (for example, electronics
and sports), others capture distinct languages and even
dialect variation. Such dialects are particularly evident in
2http://www.cs.princeton.edu/?blei/lda-c
stopwords (you versus u). Structured topic models that
explicitly handle these two orthogonal axes of linguistic
variation are an intriguing possibility for future work.
We evaluate our topic-based approach for link predic-
tion on both the message and follower graphs, compar-
ing against an approach that only considers the network
structure. Liben-Nowell and Kleinberg (2003) perform
a quantitative comparison of such approaches, finding
that the relatively simple technique of counting the num-
ber of shared neighbors between two nodes is a surpris-
ingly competitive predictor of whether they are linked;
we call this approach common-neighbors. We evaluate
this method and our own supervised LDA+regression ap-
proach by hiding half of the edges in the graph, and pre-
dicting them from the other half.
For each author in the dataset, we apply each method
to rank all possible links; the evaluation computes the av-
erage rank of the true links that were held out (for our
data, a random baseline would score 10653 ? half the
number of authors in the network). As shown in Figure
1, topic-based link prediction outperforms the alternative
that considers only the graph structure. Interestingly, post
hoc regression on the topic proportions did not consis-
tently improve performance, though joint learning may
do better (e.g., Chang and Blei, 2009). The text-based ap-
proach is especially strong on the message graph, while
the link-based approach is more competitive on the fol-
lowers graph; a model that captures both features seems
a useful direction for future work.
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet aloca-
tion. Journal of Machine Learning Research, 3:993?1022.
J. Chang and D. Blei. 2009. Hierarchical relational models for
document networks. Annals of Applied Statistics.
M.A.K. Halliday. 1978. Language as social semiotic: The
social interpretation of language and meaning. University
Park Press.
Bernardo Huberman, Daniel M. Romero, and Fang Wu. 2009.
Social networks that matter: Twitter under the microscope.
First Monday, 14(1?5), January.
D. Liben-Nowell and J. Kleinberg. 2003. The link prediction
problem for social networks. In Proc. of CIKM.
20
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 64?71,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Bilingual POS Tagging with Markov Random Fields
Desai Chen Chris Dyer Shay B. Cohen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
desaic@andrew.cmu.edu, {cdyer,scohen,nasmith}@cs.cmu.edu
Abstract
In this paper, we give a treatment to the prob-
lem of bilingual part-of-speech induction with
parallel data. We demonstrate that na??ve op-
timization of log-likelihood with joint MRFs
suffers from a severe problem of local max-
ima, and suggest an alternative ? using con-
trastive estimation for estimation of the pa-
rameters. Our experiments show that estimat-
ing the parameters this way, using overlapping
features with joint MRFs performs better than
previous work on the 1984 dataset.
1 Introduction
This paper considers unsupervised learning of lin-
guistic structure?specifically, parts of speech?in
parallel text data. This setting, and more gener-
ally the multilingual learning scenario, has been
found advantageous for a variety of unsupervised
NLP tasks (Snyder et al, 2008; Cohen and Smith,
2010; Berg-Kirkpatrick et al, 2010; Das and Petrov,
2011).
We consider globally normalized Markov random
fields (MRFs) as an alternative to directed models
based on multinomial distributions or locally nor-
malized log-linear distributions. This alternate pa-
rameterization allows us to introduce correlated fea-
tures that, at least in principle, depend on any parts
of the hidden structure. Such models, sometimes
called ?undirected,? are widespread in supervised
NLP; the most notable instances are conditional ran-
dom fields (Lafferty et al, 2001), which have en-
abled rich feature engineering to incorporate knowl-
edge and improve performance. We conjecture that
the ?features view? of NLP problems is also more
appropriate in unsupervised settings than the con-
trived, acyclic causal stories required by directed
models. Indeed, as we will discuss below, previous
work on multilingual POS induction has had to re-
sort to objectionable independence assumptions to
avoid introducing cyclic dependencies in the causal
network.
While undirected models are formally attractive,
they are computationally demanding, particularly
when they are used generatively, i.e., as joint dis-
tributions over input and output spaces. Inference
and learning algorithms for these models are usually
intractable on realistic datasets, so we must resort to
approximations. Our emphasis here is primarily on
the machinery required to support overlapping fea-
tures, not on weakening independence assumptions,
although we weaken them slightly. Specifically, our
parameterization permits us to model the relation-
ship between aligned words in any configuration,
rather than just those that conform to an acyclic gen-
erative process, as previous work in this area has
done (?2). We incorporate word prefix and suffix
features (up to four characters) in an undirected ver-
sion of a model designed by Snyder et al (2008).
Our experiments suggest that feature-based MRFs
offer advantages over the previous approach.
2 Related Work
The task of unsupervised bilingual POS induction
was originally suggested and explored by Snyder et
al. (2008). Their work proposes a joint model over
pairs of tag sequences and words that can be under-
stood as a pair of hidden Markov models (HMMs)
64
in which aligned words share states (a fixed and
observable word alignment is assumed). Figure 1
gives an example for a French-English sentence pair.
Following Goldwater and Griffiths (2007), the tran-
sition, emission and coupling parameters are gov-
erned by Dirichlet priors, and a token-level col-
lapsed Gibbs sampler is used for inference. The hy-
perparameters of the prior distributions are inferred
from data in an empirical Bayesian fashion.
  
Why repeat that catastrophe ?
Pourquoi r?p?ter la m?me ?catastrophe
x1/y1 X2/y2 y3 y4 x5/y6x4/y5
x3
Figure 1: Bilingual Directed POS induction model
When word alignments are monotonic (i.e., there
are no crossing links in the alignment graph), the
model of Snyder et al is straightforward to con-
struct. However, crossing alignment links pose a
problem: they induce cycles in the tag sequence
graph, which corresponds to an ill-defined probabil-
ity model. Their solution is to eliminate such align-
ment pairs (their algorithm for doing so is discussed
below). Unfortunately, this is a potentially a seri-
ous loss of information. Crossing alignments often
correspond to systematic word order differences be-
tween languages (e.g., SVO vs. SOV languages). As
such, leaving them out prevents useful information
about entire subsets of POS types from exploiting of
bilingual context.
In the monolingual setting, Smith and Eisner
(2005) showed similarly that a POS induction model
can be improved with spelling features (prefixes and
suffixes of words), and Haghighi and Klein (2006)
describe an MRF-based monolingual POS induction
model that uses features. An example of such a
monolingual model is shown in Figure 2. Both pa-
pers developed different approximations of the com-
putationally expensive partition function. Haghighi
and Klein (2006) approximated by ignoring all sen-
tences of length greater than some maximum, and
the ?contrastive estimation? of Smith and Eisner
(2005) approximates the partition function with a set
Eco
nom
ic
dis
cre
pan
cie
s
A
N
areV
gro
win
g
V
Figure 2: Monolingual MRF tag model (Haghighi
and Klein, 2006)
of automatically distorted training examples which
are compactly represented in WFSTs.
Das and Petrov (2011) also consider the prob-
lem of unsupervised bilingual POS induction. They
make use of independent conventional HMM mono-
lingual tagging models that are parameterized with
feature-rich log-linear models (Berg-Kirkpatrick et
al., 2010). However, training is constrained with tag
dictionaries inferred using bilingual contexts derived
from aligned parallel data. In this way, the complex
inference and modeling challenges associated with a
bilingual tagging model are avoided.
Finally, multilingual POS induction has also been
considered without using parallel data. Cohen et al
(2011) present a multilingual estimation technique
for part-of-speech tagging (and grammar induction),
where the lack of parallel data is compensated by
the use of labeled data for some languages and unla-
beled data for other languages.
3 Model
Our model is a Markov random field whose ran-
dom variables correspond to words in two parallel
sentences and POS tags for those words. Let s =
?s1, . . . , sNs? and t = ?t1, . . . , tNt? denote the two
word sequences; these correspond to Ns + Nt ob-
served random variables.1 Let x and y denote the se-
quences of POS tags for s and t, respectively. These
are the hidden variables whose values we seek to in-
fer. We assume that a word alignment is provided for
the sentences. Let A ? {1, . . . , Ns} ? {1, . . . Nt}
denote the word correspondences specified by the
alignment. The MRF?s unnormalized probability S
1We use ?source? and ?target? but the two are completely
symmetric in our undirected framework.
65
assigns:
S(s, t,x,y | A,w) =
expw>
(
Ns?
i=1
fs-emit(si, xi) +
Ns?
i=2
fs-tran(xi?1, xi)
+
Nt?
i=1
ft-emit(ti, yi) +
Nt?
i=2
ft-tran(yi?1, yi)
+
?
(i,j)?A
falign-POS(xi, yj)
?
?
where w is a numerical vector of feature weights
that parameterizes the model. Each f? corre-
sponds to features on pairs of random variables;
a source POS tag and word, two adjacent source
POS tags, similarly for the target side, and aligned
source/target POS pairs. For simplicity, we let f de-
note the sum of these five feature vectors. (In most
settings, each feature/coordinate will be specific to
one of the five addends.) In this paper, the features
are indicators for each possible value of the pair of
random variables, plus prefix and suffix features for
words (up to four characters). These features encode
information similar to the Bayesian bilingual HMM
discussed in ?2. Future work might explore exten-
sions to this basic feature set.
The marginal probability of the words is given by:
p(s, t | A,w) =
?
x,y S(x,y, s, t | A,w)
?
s?,t?
?
x,y S(s
?, t?,x,y | A,w)
.
Maximum likelihood estimation would choose
weights w to optimize a product of quantities like
the above, across the training data.
A key advantage of this representation is that any
alignments may be present. In directed models,
crossing links create forbidden cycles in the graph-
ical model. For example, Figure 3 shows a cross-
ing link between ?Economic discrepancies? and ?di-
vergences economiques.? Snyder et al (2008) dealt
with this problem by deleting word correspondences
that created cycles. The authors deleted crossing
links by considering each alignment link in the order
of the source sentence, deleting it if it crossed pre-
vious links. Deleting crossing links removes some
information about word correspondence.
divergences
?conomiques
Eco
nom
ic
disc
rep
anc
ies
N
A
A
N
Les ART
vont areV V
croissant gro
win
g
V V
Figure 3: Bilingual tag model.
4 Inference and Parameter Learning
When using traditional generative models, such as
hidden Markov models, the unsupervised setting
lends itself well to maximizing joint log-likelihood,
leading to a model that performs well (Snyder et
al., 2008). However, as we show in the following
analysis, maximizing joint log-likelihood for a joint
Markov random field with arbitrary features suffers
from serious issues which are related to the com-
plexity of the optimized objective surface.
4.1 MLE with Gradient Descent
For notational simplicity, we assume a single pair of
sentences s and t; generalizing to multiple training
instances is straightforward. The marginalized log-
likelihood of the data given w is
L(w) = log p(s, t | w)
= log
?
x,y S(x,y, s, t | w)
?
s?,t?
?
x,y S(x,y, s
?, t? | w)
.
In general, maximizing marginalized log-
likelihood is a non-concave optimization problem.
Iterative hill-climbing methods (e.g., expectation-
maximization and gradient-based optimization) will
lead only to local maxima, and these may be quite
shallow. Our analysis suggests that the problem
is exacerbated when we move from directed to
undirected models. We next describe a simple
experiment that gives insight into the problem.
We created a small synthetic monolingual data set
for sequence labeling. Our synthetic data consists of
the following five sequences of observations: {(0 1 2
3) , (1 2 3 0) , (2 3 0 1) , (3 0 1 2) , (0 1 2 3)}. We then
66
maximized the marginalized log-likelihood for two
models: a hidden Markov model and an MRF. Both
use the same set features, only the MRF is globally
normalized. The number of hidden states in both
models is 4.
The global maximium in both cases would be
achieved when the emission probabilities (or feature
weights, in the case of MRF) map each observation
symbol to a single state. When we tested whether
this happens in practice, we noticed that it indeed
happens for hidden Markov models. The MRF, how-
ever, tended to use fewer than four tags in the emis-
sion feature weights, i.e., for half of the tags, all
emission feature weights were close to 0. This ef-
fect also appeared in our real data experiments.
The reason for this problem with the MRF, we be-
lieve, is that the parameter space of the MRF is un-
derconstrained. HMMs locally normalize the emis-
sion probabilities, which implies that a tag cannot
?disappear??a total probability mass of 1 must al-
ways be allocated to the observation symbols. With
MRFs, however, there is no such constraint. Fur-
ther, effective deletion of a state y requires zeroing
out transition probabilities from all other states to
y, a large number of parameters that are completely
decoupled within the model.
  Wh Wy rh ry yh yy eh ey ph py ah ayhthrheh
ahchhcthcrh
ceh p e y r W
(a) likelihood
  WhyyWhyr WhyeWhypWhyaWhyt WhycWhysWhyo e p a tWrWpWtW
sWyWWyrWypW
ytWysW a p?c p?ae?a
(b) contrastive objective
Figure 4: Histograms of local optima found by opti-
mizing the length neighborhood objective (a) and the
contrastive objective (b) on a synthetic dataset with
8 sentences of length 7. The weights are initialized
uniformly at random in the interval [?1, 1]. We plot
frequency versus negated log-likelihood (lower hor-
izontal values are better). An HMM always finds a
solution that uses all available tags. The numbers at
the top are numbers of tags used by each local opti-
mum.
Our bilingual model is more complex than the
above example, and we found in preliminary exper-
iments that the effect persists there, as well. In the
following section, we propose a remedy to this prob-
lem based on contrastive estimation (Smith and Eis-
ner, 2005).
4.2 Contrastive Estimation
Contrastive estimation maximizes a modified ver-
sion of the log-likelihood. In the modified version,
it is the normalization constant of the log-likelihood
that changes: it is limited to a sum over possible ele-
ments in a neighborhood of the observed instances.
More specifically, in our bilingual tagging model,
we would define a neighborhood function for sen-
tences, N(s, t) which maps a pair of sentences to
a set of pairs of sentences. Using this neighborhood
function, we maximize the following objective func-
tion:
Lce(w)
= log p(S = s,T = t | S ? N1(s),T ? N2(t),w)
= log
?
x,y S(s, t,x,y | w)
?
s?,t??N(s,t)
?
x,y
S(s?, t?,x,y | w).
(1)
We define the neighborhood function using
a cross-product of monolingual neighborhoods:
N(s, t) = N1(s) ? N1(t). N1 is the ?dynasearch?
neighborhood function (Potts and van de Velde,
1995; Congram et al, 2002), used for contrastive
estimation previously by Smith (2006). This neigh-
borhood defines a subset of permutations of a se-
quence s, based on local transpositions. Specifically,
a permutation of s is in N1(s) if it can be derived
from s through swaps of any adjacent pairs of words,
with the constraint that each word only be moved
once. This neighborhood can be compactly repre-
sented with a finite-state machine of size O(Ns) but
encodes a number of sequences equal to the Nsth
Fibonacci number.
Monolingual Analysis To show that contrastive
estimation indeed gives a remedy to the local max-
imum problem, we return to the monolingual syn-
thetic data example from ?4.1 and apply contrastive
estimation on this problem. The neighborhood we
use is the dynasearch neighborhood. In Figure 4b
67
we compare the maxima identified using MLE with
the monolingual MRF model to the maxima identi-
fied by contrastive estimation. The results are con-
clusive: MLE tends to get stuck much more often in
local maxima than contrastive estimation.
Following an analysis of the feature weights
found by contrastive estimation, we found that con-
trastive estimation puts more weight on the transi-
tion features than emission features, i.e., the tran-
sition features weights have larger absolute values
than emission feature weights. We believe that this
could explain why contrastive estimation finds better
local maximum that plain MLE, but we leave explo-
ration of this effect for future work.
It is interesting to note that even though the con-
trastive objective tends to use more tags available in
the dictionary than the likelihood objective does, the
maximum objective that we were able to find does
not correspond to the tagging that uses all available
tags, unlike with HMM, where the maximum that
achieved highest likelihood also uses all available
tags.
4.3 Optimizing the Contrastive Objective
To optimize the objective in Eq. 1 we use a generic
optimization technique based on the gradient. Using
the chain rule for derivatives, we can derive the par-
tial derivative of the log-likelihood with respect to a
weight wi:
?Lce(w)
?wi
= Ep(X,Y|s,t,w)[fi]
? Ep(S,T,X,Y|S?N1(s),T?N1(t),w)[fi]
The second term corresponds to a computationally
expensive inference problem, because of the loops
in the graphical model. This situation is differ-
ent from previous work on linear chain-structured
MRFs (Smith and Eisner, 2005; Haghighi and Klein,
2006), where exact inference is possible. To over-
come this problem, we use Gibbs sampling to obtain
the two expectations needed by the gradient. This
technique is closely related to methods like stochas-
tic expectation-maximization (Andrieu et al, 2003)
and to contrastive divergence (Hinton, 2000).
The training algorithm iterates between sam-
pling part-of-speech tags and sampling permutations
of words to compute the expected value of fea-
tures. To sample permutations, the sampler iterates
through the sentences and decides, for each sen-
tence, whether to swap a pair of adjacent tags and
words or not. The Markov blanket for computing
the probability of swapping a pair of tags and words
is shown in Figure 5. We run the algorithm for a
fixed number (50) of iterations. By testing on a de-
velopment set, we observed that the accuracy may
increase after 50 iterations, but we chose this small
number of iterations for speed.
  
N A
divergences ?conomiques
A N
Economic discrepancies
V
vont 
  
N A
divergences ?conomiques
A N
Economic discrepancies
AVt
?s 
?
von? 
are
?
Figure 5: Markov blanket of a tag (left) and of a pair
of adjacent tags and words (right).
In preliminary experiments we considered
stochastic gradient descent, with online updating.
We found this led to low-accuracy local optima,
and opted for gradient descent with batch updates
in our implementation. The step size was chosen to
limit the maximum absolute value of the update in
any weight to 0.1. Preliminary experiments showed
only harmful effects from regularization, so we did
not use it. These issues deserve further analysis and
experimentation in future research.
5 Experiments
We next describe experiments using our undirected
model to unsupervisedly learn POS tags.
With unsupervised part-of-speech tagging, it is
common practice to use a full or partial dictionary
that maps words to possible part-of-speech tags. The
goal of the learner is then to discern which tag a
word should take among the tags available for that
word. Indeed, in all of our experiments we make
use of a tag dictionary. We consider both a com-
plete tag dictionary, where all of the POS tags for all
words in the data are known,2 and a smaller tag dic-
tionary that only provides possible tags for the 100
2Of course, additional POS tags may be possible for a given
word that were not in evidence in our finite dataset.
68
most frequent words in each language, leaving the
other words completely ambiguous. The former dic-
tionary makes the problem easier by reducing ambi-
guity; it also speeds up inference.
Our experiments focus on the Orwell novel 1984
dataset for our experiments, the same data used by
Snyder et al (2008). It consists of parallel text of
the 1984 novel in English, Bulgarian, Slovene and
Serbian (Erjavec, 2004), totalling 5,969 sentences in
each language. The 1984 datset uses fourteen part-
of-speech tags, two of which denote punctuation.
The tag sets for English and other languages have
minor differences in determiners and particles.
We use the last 25% of sentences in the dataset
as a test set, following previous work. The dataset
is manually annotated with part-of-speech tags. We
use automatically induced word alignments using
Giza++ (Och and Ney, 2003). The data show very
regular patterns of tags that are aligned together:
words with the same tag in two languages tend to
be aligned with each other.
When a complete tag dictionary derived from the
Slavic language data is available, the level of ambi-
guity is very low. The baseline of choosing random
tags for each word gives an accuracy in the low 80s.
For English, we use an extended tag dictionary built
from the Wall Street Journal and the 1984 data. The
English tag dictionary is much more ambiguous be-
cause it is obtained from a much larger dataset. The
random baseline gives an accuracy of around 56%.
(See Table 1.)
In our first set of experiments (?5.1), we perform
a ?sanity check? with a monolingual version of the
MRF that we described in earlier sections. We com-
pare it against plain HMM to assure that the MRFs
behave well in the unsupervised setting.
In our second set of experiments (?5.2), we com-
pare the bilingual HMM model from Snyder et al
(2008) to the joint MRF model. We show that using
an MRF has an advantage over an HMM model in
the partial tag dictionary setting.
5.1 Monolingual Experiments
We turn now to two monolingual experiments that
verify our model?s suitability for the tagging prob-
lem.
Language Random HMM MRF
Bulgarian 82.7 88.9 93.5
English 56.2 90.7 87.0
Serbian 83.4 85.1 89.3
Slovene 84.7 87.4 94.5
Table 1: Unsupervised monolingual tagging accura-
cies with complete tag dictionary on 1984 data.
Supervised Learning As a very primitive com-
parison, we trained a monolingual supervised MRF
model to compare to the results of supervised
HMMs. The training procedure is based on sam-
pling, just like the unsupervised estimation method
described in ?4.3. The only difference is that there is
no need to sample the words because the tags are the
only random variables to be marginalized over. Our
model and HMM give very close performance with
difference in accuracy less than 0.1%. This shows
that the MRF is capable of representing an equiva-
lent model represented by the HMM. It also shows
that gradient descent with MCMC approximate in-
ference is capable of finding a good model with the
weights initialized to all 0s.
Unsupervised Learning We trained our model
under the monolingual setting as a sanity check for
our approximate training algorithm. Our model un-
der monolingual mode is exactly the same as the
models introduced in ?2. We ran our model on the
1984 data with the complete tag dictionary. A com-
parison between our result and monolingual directed
model is shown in Table 1. ?Random? is obtained by
choosing a random tag for each word according to
the tag dictionary. ?HMM? is a Bayesian HMM im-
plemented by (Snyder et al, 2008). We also imple-
mented a basic (non-Bayesian) HMM. We trained
the HMM with EM and obtained rsults similar to the
Bayesian HMM (not shown).
5.2 Billingual Results
Table 2 gives the full results in the bilingual setting
for the 1984 dataset with a partial tag dictionary. In
general, MRFs do better than their directed counter-
parts, the HMMs. Interestingly enough, removing
crossing links from the data has only a slight adverse
effect. It appears like the prefix and suffix features
are more important than having crossing links. Re-
69
Language pair HMM MRF MRF w/o cross. MRF w/o spell.
English 71.3 73.3? 0.6 73.4? 0.6 67.4? 0.9
Bulgarian 62.6 62.3? 0.3 63.8? 0.4 55.2? 0.5
Serbian 54.1 55.7? 0.2 54.6? 0.3 47.7? 0.5
Slovene 59.7 61.4? 0.3 60.4? 0.3 56.7? 0.4
English 66.5 73.3? 0.3 73.4? 0.2 62.3? 0.5
Slovene 53.8 59.7? 2.5 57.6? 2.0 52.1? 1.3
Bulgarian 54.2 58.1? 0.1 56.3? 1.3 58.0? 0.2
Serbian 56.9 58.6? 0.3 59.0? 1.2 55.1? 0.3
English 68.2 72.8? 0.6 72.7? 0.6 65.7? 0.4
Serbian 54.7 58.5? 0.6 57.7? 0.3 54.2? 0.3
Bulgarian 55.9 59.8? 0.1 60.3? 0.5 55.0? 0.4
Slovene 58.5 61.4? 0.3 61.6? 0.4 58.1? 0.6
Average 59.7 62.9 62.5 56.5
Table 2: Unsupervised bilingual tagging accuracies with tag dictionary only for the top 100 frequent words.
?HMM? is the result reported by (Snyder et al, 2008). ?MRF? is our contrastive model averaged over ten
runs. ?MRF w/o cross.? is our model trained without crossing links, like Snyder et al?s HMM. ?MRF
w/o spell.? is our model without prefix and suffix features. Numbers appearing next to results are standard
deviations over the ten runs.
Language w/ cross. w/o cross.
French 73.8 70.3
English 56.0 59.2
Table 3: Effect of removing crossing links when
learning French and English in a bilingual setting.
moving the prefix and suffix features gives substan-
tially lower results on average, results even below
plain HMMs.
The reason that crossing links do not change the
results much could be related to fact that most of
the sentence pairs in the 1984 dataset do not contain
many crossing links (only 5% of links cross another
link). To see whether crossing links do have an ef-
fect when they come in larger number, we tested our
model on French-English data. We aligned 10,000
sentences from the Europarl corpus (Koehn, 2005),
resulting in 87K crossing links out of a total of 673K
links. Using the Penn treebank (Marcus et al, 1993)
and the French treebank (Abeille? et al, 2003) to
evaluate the model, results are given in Table 3. It is
evident that crossing links have a larger effect here,
but it is mixed: crossing links improve performance
for French while harming it for English.
6 Conclusion
In this paper, we explored the capabilities of joint
MRFs for modeling bilingual part-of-speech mod-
els. Exact inference with dynamic programming is
not applicable, forcing us to experiment with ap-
proximate inference techniques. We demonstrated
that using contrastive estimation together with Gibbs
sampling for the calculation of the gradient of the
objective function leads to better results in unsuper-
vised bilingual POS induction.
Our experiments also show that the advantage of
using MRFs does not necessarily come from the fact
that we can use non-monotonic alignments in our
model, but instead from the ability to use overlap-
ping features such as prefix and suffix features for
the vocabulary in the data.
Acknowledgments
We thank the reviewers and members of the ARK
group for helpful comments on this work. This re-
search was supported in part by the NSF through
grant IIS-0915187 and the U. S. Army Research
Laboratory and the U. S. Army Research Office un-
der contract/grant number W911NF-10-1-0533.
70
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for French. In A. Abeille?, editor, Treebanks.
Kluwer, Dordrecht.
C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan.
2003. An introduction to MCMC for machine learn-
ing. Machine Learning, 50:5?43.
T. Berg-Kirkpatrick, A. Bouchard-Cote, J. DeNero, and
D. Klein. 2010. Unsupervised learning with features.
In Proceedings of NAACL.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proceedings of EMNLP.
R. K. Congram, C. N. Potts, and S. L. van de Velde.
2002. An iterated Dynasearch algorithm for the
single-machine total weighted tardiness scheduling
problem. Informs Journal On Computing, 14(1):52?
67.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Procedings of ACL.
T. Erjavec. 2004. MULTEXT-East version 3: Multilin-
gual morphosyntactic specifications, lexicons and cor-
pora. In Proceedings of LREC.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proceedings of HLT-
NAACL.
G. E. Hinton. 2000. Training products of experts by
minimizing contrastive divergence. Technical Report
GCNU TR 2000-004, University College London.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit 2005.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
C. N. Potts and S. L. van de Velde. 1995. Dynasearch?
iterative local improvement by dynamic programming.
Part I: The traveling salesman problem. Technical re-
port.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of EMNLP.
71
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56?64,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Spectral Learning of Refinement HMMs
Karl Stratos1 Alexander M. Rush2 Shay B. Cohen1 Michael Collins1
1Department of Computer Science, Columbia University, New-York, NY 10027, USA
2MIT CSAIL, Cambridge, MA, 02139, USA
{stratos,scohen,mcollins}@cs.columbia.edu, srush@csail.mit.edu
Abstract
We derive a spectral algorithm for learn-
ing the parameters of a refinement HMM.
This method is simple, efficient, and can
be applied to a wide range of supervised
sequence labeling tasks. Like other spec-
tral methods, it avoids the problem of lo-
cal optima and provides a consistent esti-
mate of the parameters. Our experiments
on a phoneme recognition task show that
when equipped with informative feature
functions, it performs significantly better
than a supervised HMM and competitively
with EM.
1 Introduction
Consider the task of supervised sequence label-
ing. We are given a training set where the j?th
training example consists of a sequence of ob-
servations x(j)1 ...x(j)N paired with a sequence of
labels a(j)1 ...a(j)N and asked to predict the cor-
rect labels on a test set of observations. A
common approach is to learn a joint distribu-
tion over sequences p(a1 . . . aN , x1 . . . xN ) as a
hidden Markov model (HMM). The downside of
HMMs is that they assume each label ai is inde-
pendent of labels before the previous label ai?1.
This independence assumption can be limiting,
particularly when the label space is small. To re-
lax this assumption we can refine each label ai
with a hidden state hi, which is not observed in
the training data, and model the joint distribu-
tion p(a1 . . . aN , x1 . . . xN , h1 . . . hN ). This re-
finement HMM (R-HMM), illustrated in figure 1,
is able to propagate information forward through
the hidden state as well as the label.
Unfortunately, estimating the parameters of an
R-HMM is complicated by the unobserved hid-
den variables. A standard approach is to use the
expectation-maximization (EM) algorithm which
a1, h1 a2, h2 aN , hN
x1 x2 xN
(a)
a1 a2 aN
h1 h2 hN
x1 x2 xN
(b)
Figure 1: (a) An R-HMM chain. (b) An equivalent
representation where labels and hidden states are
intertwined.
has no guarantee of finding the global optimum of
its objective function. The problem of local op-
tima prevents EM from yielding statistically con-
sistent parameter estimates: even with very large
amounts of data, EM is not guaranteed to estimate
parameters which are close to the ?correct? model
parameters.
In this paper, we derive a spectral algorithm for
learning the parameters of R-HMMs. Unlike EM,
this technique is guaranteed to find the true param-
eters of the underlying model under mild condi-
tions on the singular values of the model. The al-
gorithm we derive is simple and efficient, relying
on singular value decomposition followed by stan-
dard matrix operations.
We also describe the connection of R-HMMs
to L-PCFGs. Cohen et al (2012) present a spec-
tral algorithm for L-PCFG estimation, but the
na??ve transformation of the L-PCFG model and
its spectral algorithm to R-HMMs is awkward and
opaque. We therefore work through the non-trivial
derivation the spectral algorithm for R-HMMs.
We note that much of the prior work on spec-
tral algorithms for discrete structures in NLP has
shown limited experimental success for this fam-
ily of algorithms (see, for example, Luque et al,
2012). Our experiments demonstrate empirical
56
success for the R-HMM spectral algorithm. The
spectral algorithm performs competitively with
EM on a phoneme recognition task, and is more
stable with respect to the number of hidden states.
Cohen et al (2013) present experiments with a
parsing algorithm and also demonstrate it is com-
petitive with EM. Our set of experiments comes as
an additional piece of evidence that spectral algo-
rithms can function as a viable, efficient and more
principled alternative to the EM algorithm.
2 Related Work
Recently, there has been a surge of interest in spec-
tral methods for learning HMMs (Hsu et al, 2012;
Foster et al, 2012; Jaeger, 2000; Siddiqi et al,
2010; Song et al, 2010). Like these previous
works, our method produces consistent parameter
estimates; however, we estimate parameters for a
supervised learning task. Balle et al (2011) also
consider a supervised problem, but our model is
quite different since we estimate a joint distribu-
tion p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) as opposed
to a conditional distribution and use feature func-
tions over both the labels and observations of the
training data. These feature functions also go be-
yond those previously employed in other spectral
work (Siddiqi et al, 2010; Song et al, 2010). Ex-
periments show that features of this type are cru-
cial for performance.
Spectral learning has been applied to related
models beyond HMMs including: head automata
for dependency parsing (Luque et al, 2012),
tree-structured directed Bayes nets (Parikh et al,
2011), finite-state transducers (Balle et al, 2011),
and mixture models (Anandkumar et al, 2012a;
Anandkumar et al, 2012b).
Of special interest is Cohen et al (2012), who
describe a derivation for a spectral algorithm for
L-PCFGs. This derivation is the main driving
force behind the derivation of our R-HMM spec-
tral algorithm. For work on L-PCFGs estimated
with EM, see Petrov et al (2006), Matsuzaki et al
(2005), and Pereira and Schabes (1992). Petrov
et al (2007) proposes a split-merge EM procedure
for phoneme recognition analogous to that used in
latent-variable parsing.
3 The R-HMM Model
We decribe in this section the notation used
throughout the paper and the formal details of R-
HMMs.
3.1 Notation
We distinguish row vectors from column vectors
when such distinction is necessary. We use a
superscript > to denote the transpose operation.
We write [n] to denote the set {1, 2, . . . , n} for
any integer n ? 1. For any vector v ? Rm,
diag(v) ? Rm?m is a diagonal matrix with en-
tries v1 . . . vm. For any statement S , we use [[S]]
to refer to the indicator function that returns 1 if S
is true and 0 otherwise. For a random variable X ,
we use E[X] to denote its expected value.
A tensor C ? Rm?m?m is a set of m3 val-
ues Ci,j,k for i, j, k ? [m]. Given a vector v ?
Rm, we define C(v) to be the m ? m matrix
with [C(v)]i,j = ?k?[m]Ci,j,kvk. Given vectors
x, y, z ? Rm, C = xy>z> is anm?m?m tensor
with [C]i,j,k = xiyjzk.
3.2 Definition of an R-HMM
An R-HMM is a 7-tuple ?l,m, n, pi, o, t, f? for in-
tegers l,m, n ? 1 and functions pi, o, t, f where
? [l] is a set of labels.
? [m] is a set of hidden states.
? [n] is a set of observations.
? pi(a, h) is the probability of generating a ?
[l] and h ? [m] in the first position in the
labeled sequence.
? o(x|a, h) is the probability of generating x ?
[n], given a ? [l] and h ? [m].
? t(b, h?|a, h) is the probability of generating
b ? [l] and h? ? [m], given a ? [l] and
h ? [m].
? f(?|a, h) is the probability of generating the
stop symbol ?, given a ? [l] and h ? [m].
See figure 1(b) for an illustration. At any time step
of a sequence, a label a is associated with a hidden
state h. By convention, the end of an R-HMM
sequence is signaled by the symbol ?.
For the subsequent illustration, let N be the
length of the sequence we consider. A full se-
quence consists of labels a1 . . . aN , observations
x1 . . . xN , and hidden states h1 . . . hN . The model
assumes
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) = pi(a1, h1)?
N?
i=1
o(xi|ai, hi)?
N?1?
i=1
t(ai+1, hi+1|ai, hi)? f(?|aN , hN )
57
Input: a sequence of observations x1 . . . xN ; operators?
Cb|a, C?|a, c1a, cax
?
Output: ?(a, i) for all a ? [l] and i ? [N ]
[Forward case]
? ?1a ? c1a for all a ? [l].
? For i = 1 . . . N ? 1
?i+1b ?
?
a?[l]
Cb|a(caxi)? ?
i
a for all b ? [l]
[Backward case]
? ?N+1a ? C?|a(caxN ) for all a ? [l]
? For i = N . . . 1
?ia ?
?
b?[l]
?i+1b ? Cb|a(caxi) for all a ? [l]
[Marginals]
? ?(a, i)? ?ia ? ?ia for all a ? [l], i ? [N ]
Figure 2: The forward-backward algorithm
A skeletal sequence consists of labels a1 . . . aN
and observations x1 . . . xN without hidden states.
Under the model, it has probability
p(a1 . . . aN , x1 . . . xN )
=
?
h1...hN
p(a1 . . . aN , x1 . . . xN , h1 . . . hN )
An equivalent definition of an R-HMM is
given by organizing the parameters in matrix
form. Specifically, an R-HMM has parameters?
pia, oax, T b|a, fa
? where pia ? Rm is a column
vector, oax is a row vector, T b|a ? Rm?m is a ma-
trix, and fa ? Rm is a row vector, defined for all
a, b ? [l] and x ? [n]. Their entries are set to
? [pia]h = pi(a, h) for h ? [m]
? [oax]h = o(x|a, h) for h ? [m]
? [T b|a]h?,h = t(b, h?|a, h) for h, h? ? [m]
? [fa]h = f(?|a, h) for h ? [m]
4 The Forward-Backward Algorithm
Given an observation sequence x1 . . . xN , we want
to infer the associated sequence of labels under
an R-HMM. This can be done by computing the
marginals of x1 . . . xN
?(a, i) =
?
a1...aN : ai=a
p(a1 . . . aN , x1 . . . xN )
for all labels a ? [l] and positions i ? [N ]. Then
the most likely label at each position i is given by
a?i = arg maxa?[l] ?(a, i)
The marginals can be computed using a tensor
variant of the forward-backward algorithm, shown
in figure 2. The algorithm takes additional quanti-
ties ?Cb|a, C?|a, c1a, cax
? called the operators:
? Tensors Cb|a ? Rm?m?m for a, b ? [l]
? Tensors C?|a ? R1?m?m for a ? [l]
? Column vectors c1a ? Rm for a ? [l]
? Row vectors cax ? Rm for a ? [l] and x ? [n]
The following proposition states that these opera-
tors can be defined in terms of the R-HMM param-
eters to guarantee the correctness of the algorithm.
Proposition 4.1. Given an R-HMM with param-
eters ?pia, oax, T b|a, fa
?, for any vector v ? Rm
define the operators:
Cb|a(v) = T b|adiag(v) c1a = pia
C?|a(v) = fadiag(v) cax = oax
Then the algorithm in figure 2 correctly computes
marginals ?(a, i) under the R-HMM.
The proof is an algebraic verification and deferred
to the appendix. Note that the running time of the
algorithm as written is O(l2m3N).1
Proposition 4.1 can be generalized to the fol-
lowing theorem. This theorem implies that the op-
erators can be linearly transformed by some invert-
ible matrices as long as the transformation leaves
the embedded R-HMM parameters intact. This
observation is central to the derivation of the spec-
tral algorithm which estimates the linearly trans-
formed operators but not the actual R-HMM pa-
rameters.
Theorem 4.1. Given an R-HMM with parameters?
pia, oax, T b|a, fa
?, assume that for each a ? [l] we
have invertible m ?m matrices Ga and Ha. For
any vector v ? Rm define the operators:
Cb|a(v) = GbT b|adiag(vHa)(Ga)?1 c1a = Gapia
C?|a(v) = fadiag(vHa)(Ga)?1 cax = oax(Ha)?1
Then the algorithm in figure 2 correctly computes
marginals ?(a, i) under the R-HMM.
The proof is similar to that of Cohen et al (2012).
1We can reduce the complexity to O(l2m2N) by pre-
computing the matricesCb|a(cax) for all a, b ? [l] and x ? [n]
after parameter estimation.
58
5 Spectral Estimation of R-HMMs
In this section, we derive a consistent estimator for
the operators ?Cb|a, C?|a, c1a, cax
? in theorem 4.1
through the use of singular-value decomposition
(SVD) followed by the method of moments.
Section 5.1 describes the decomposition of the
R-HMM model into random variables which are
used in the final algorithm. Section 5.2 can be
skimmed through on the first reading, especially
if the reader is familiar with other spectral algo-
rithms. It includes a detailed account of the deriva-
tion of the R-HMM algorithm.
For a first reading, note that an R-HMM se-
quence can be seen as a right-branching L-PCFG
tree. Thus, in principle, one can convert a se-
quence into a tree and run the inside-outside algo-
rithm of Cohen et al (2012) to learn the parame-
ters of an R-HMM. However, projecting this trans-
formation into the spectral algorithm for L-PCFGs
is cumbersome and unintuitive. This is analo-
gous to the case of the Baum-Welch algorithm for
HMMs (Rabiner, 1989), which is a special case of
the inside-outside algorithm for PCFGs (Lari and
Young, 1990).
5.1 Random Variables
We first introduce the random variables un-
derlying the approach then describe the opera-
tors based on these random variables. From
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ), we draw an R-
HMM sequence (a1 . . . aN , x1 . . . xN , h1 . . . hN )
and choose a time step i uniformly at random from
[N ]. The random variables are then defined as
X = xi
A1 = ai and A2 = ai+1 (if i = N , A2 = ?)
H1 = hi and H2 = hi+1
F1 = (ai . . . aN , xi . . . xN ) (future)
F2 = (ai+1 . . . aN , xi+1 . . . xN ) (skip-future)
P = (a1 . . . ai, x1 . . . xi?1) (past)
R = (ai, xi) (present)
D = (a1 . . . aN , x1 . . . xi?1, xi+1 . . . xN ) (destiny)
B = [[i = 1]]
Figure 3 shows the relationship between the ran-
dom variables. They are defined in such a way
that the future is independent of the past and the
present is independent of the destiny conditioning
on the current node?s label and hidden state.
Next, we require a set of feature functions over
the random variables.
? ? maps F1, F2 to ?(F1), ?(F2) ? Rd1 .
a1 ai?1 ai ai+1 aN
x1 xi?1 xi xi+1 xN
P
F1
F2
(a)
a1 ai?1 ai ai+1 aN
x1 xi?1 xi xi+1 xN
D R
(b)
Figure 3: Given an R-HMM sequence, we define
random variables over observed quantities so that
conditioning on the current node, (a) the future F1
is independent of the past P and (b) the present R
is independent of the density D.
? ? maps P to ?(P ) ? Rd2 .
? ? maps R to ?(R) ? Rd3 .
? ? maps D to ?(D) ? Rd4 .
We will see that the feature functions should be
chosen to capture the influence of the hidden
states. For instance, they might track the next la-
bel, the previous observation, or important combi-
nations of labels and observations.
Finally, we require projection matrices
?a ? Rm?d1 ?a ? Rm?d2
?a ? Rm?d3 ?a ? Rm?d4
defined for all labels a ? [l]. These matrices
will project the feature vectors of ?, ?, ?, and ?
from (d1, d2, d3, d4)-dimensional spaces to an m-
dimensional space. We refer to this reduced di-
mensional representation by the following random
variables:
F 1 = ?A1?(F1) (projected future)
F 2 = ?A2?(F2) (projected skip-future: if i = N , F 2 = 1)
P = ?A1?(P ) (projected past)
R = ?A1?(R) (projected present)
D = ?A1?(D) (projected destiny)
Note that they are all vectors in Rm.
59
5.2 Estimation of the Operators
Since F 1, F 2, P , R, and D do not involve hid-
den variables, the following quantities can be di-
rectly estimated from the training data of skeletal
sequences. For this reason, they are called observ-
able blocks:
?a = E[F 1P>|A1 = a] ?a ? [l]
?a = E[R D>|A1 = a] ?a ? [l]
Db|a = E[[[A2 = b]]F 2P>R>|A1 = a] ?a, b ? [l]
dax = E[[[X = x]]D>|A1 = a] ?a ? [l], x ? [n]
The main result of this paper is that under cer-
tain conditions, matrices ?a and ?a are invert-
ible and the operators ?Cb|a, C?|a, c1a, cax
? in the-
orem 4.1 can be expressed in terms of these ob-
servable blocks.
Cb|a(v) = Db|a(v)(?a)?1 (1)
C?|a(v) = D?|a(v)(?a)?1 (2)
cax = dax(?a)?1 (3)
c1a = E[[[A1 = a]]F 1|B = 1] (4)
To derive this result, we use the following defini-
tion to help specify the conditions on the expecta-
tions of the feature functions.
Definition. For each a ? [l], define matrices
Ia ? Rd1?m, Ja ? Rd2?m, Ka ? Rd3?m,W a ?
Rd4?m by
[Ia]k,h = E[[?(F1)]k|A1 = a,H1 = h]
[Ja]k,h = E[[?(P )]k|A1 = a,H1 = h]
[Ka]k,h = E[[?(R)]k|A1 = a,H1 = h]
[W a]k,h = E[[?(D)]k|A1 = a,H1 = h]
In addition, let ?a ? Rm?m be a diagonal matrix
with [?a]h,h = P (H1 = h|A1 = a).
We now state the conditions for the correctness of
Eq. (1-4). For each label a ? [l], we require that
Condition 6.1 Ia, Ja,Ka,W a have rank m.
Condition 6.2 [?a]h,h > 0 for all h ? [m].
The conditions lead to the following proposition.
Proposition 5.1. Assume Condition 6.1 and 6.2
hold. For all a ? [l], define matrices
?a1 = E[?(F1)?(P )>|A1 = a] ? Rd1?d2
?a2 = E[?(R)?(D)>|A1 = a] ? Rd3?d4
Let ua1 . . . uam ? Rd1 and va1 . . . vam ? Rd2 be the
top m left and right singular vectors of ?a. Sim-
ilarly, let la1 . . . lam ? Rd3 and ra1 . . . ram ? Rd4 be
the top m left and right singular vectors of ?a.
Define projection matrices
?a = [ua1 . . . uam]> ?a = [va1 . . . vam]>
?a = [la1 . . . lam]> ?a = [ra1 . . . ram]>
Then the following m?m matrices
Ga = ?aIa Ga = ?aJa
Ha = ?aKa Ha = ?aW a
are invertible.
The proof resembles that of lemma 2 of Hsu et al
(2012). Finally, we state the main result that shows?
Cb|a, C?|a, c1a, cax
? in Eq. (1-4) using the projec-
tions from proposition 5.1 satisfy theorem 4.1. A
sketch of the proof is deferred to the appendix.
Theorem 5.1. Assume conditions 6.1 and 6.2
hold. Let ??a,?a,?a,?a? be the projection ma-
trices from proposition 5.1. Then the operators in
Eq. (1-4) satisfy theorem 4.1.
In summary, these results show that with the
proper selection of feature functions, we can con-
struct projection matrices ??a,?a,?a,?a? to ob-
tain operators ?Cb|a, C?|a, c1a, cax
? which satisfy
the conditions of theorem 4.1.
6 The Spectral Estimation Algorithm
In this section, we give an algorithm to estimate
the operators ?Cb|a, C?|a, c1a, cax
? from samples of
skeletal sequences. Suppose the training set con-
sists of M skeletal sequences (a(j), x(j)) for j ?
[M ]. ThenM samples of the random variables can
be derived from this training set as follows
? At each j ? [M ], choose a position
ij uniformly at random from the positions
in (a(j), x(j)). Sample the random vari-
ables (X,A1, A2, F1, F2, P,R,D,B) using
the procedure defined in section 5.1.
This process yields M samples
(x(j), a(j)1 , a
(j)
2 , f
(j)
1 , f
(j)
2 , p(j), r(j), d(j), b(j)) for j ? [M ]
Assuming (a(j), x(j)) are i.i.d. draws from
the PMF p(a1 . . . aN , x1 . . . xN ) over skeletal se-
quences under an R-HMM, the tuples obtained
through this process are i.i.d. draws from the joint
PMF over (X,A1, A2, F1, F2, P,R,D,B).
60
Input: samples of (X,A1, A2, F1, F2, P,R,D,B); feature
functions ?, ?, ?, and ?; number of hidden states m
Output: estimates
?
C?b|a, C??|a, c?1a, c?ax
?
of the operators
used in algorithm 2
[Singular Value Decomposition]
? For each label a ? [l], compute empirical estimates of
?a1 = E[?(F1)?(P )>|A1 = a]
?a2 = E[?(R)?(D)>|A1 = a]
and obtain their singular vectors via an SVD. Use
the top m singular vectors to construct projections?
??a, ??a, ??a, ??a
?
.
[Sample Projection]
? Project (d1, d2, d3, d4)-dimensional samples of
(?(F1), ?(F2), ?(P ), ?(R), ?(D))
with matrices
?
??a, ??a, ??a, ??a
?
to obtain m-
dimensional samples of
(F 1, F 2, P ,R,D)
[Method of Moments]
? For each a, b ? [l] and x ? [n], compute empirical
estimates
?
??a, ??a, D?b|a, d?ax
?
of the observable blocks
?a = E[F 1P>|A1 = a]
?a = E[R D>|A1 = a]
Db|a = E[[[A2 = b]]F 2P>R>|A1 = a]
dax = E[[[X = x]]D>|A1 = a]
and also c?1a = E[[[A1 = a]]F 1|B = 1]. Finally, set
C?b|a(v)? D?b|a(v)(??a)?1
C??|a(v)? D??|a(v)(??a)?1
c?ax ? d?ax(??a)?1
Figure 4: The spectral estimation algorithm
The algorithm in figure 4 shows how to derive
estimates of the observable representations from
these samples. It first computes the projection
matrices ??a,?a,?a,?a? for each label a ? [l]
by computing empirical estimates of ?a1 and ?a2
in proposition 5.1, calculating their singular vec-
tors via an SVD, and setting the projections in
terms of these singular vectors. These projection
matrices are then used to project (d1, d2, d3, d4)-
0 5 10 15 20 25 30hidden states (m)54.0
54.555.0
55.556.0
56.557.0
57.5
accur
acy
SpectralEM
Figure 5: Accuracy of the spectral algorithm and
EM on TIMIT development data for varying num-
bers of hidden states m. For EM, the highest scor-
ing iteration is shown.
dimensional feature vectors
(
?(f (j)1 ), ?(f
(j)
2 ), ?(p(j)), ?(r(j)), ?(d(j))
)
down to m-dimensional vectors
(
f (j)1 , f
(j)
2 , p
(j), r(j), d(j)
)
for all j ? [M ]. It then computes correlation
between these vectors in this lower dimensional
space to estimate the observable blocks which are
used to obtain the operators as in Eq. (1-4). These
operators can be used in algorithm 2 to compute
marginals.
As in other spectral methods, this estimation al-
gorithm is consistent, i.e., the marginals ??(a, i)
computed with the estimated operators approach
the true marginal values given more data. For
details, see Cohen et al (2012) and Foster et al
(2012).
7 Experiments
We apply the spectral algorithm for learning
R-HMMs to the task of phoneme recognition.
The goal is to predict the correct sequence of
phonemes a1 . . . aN for a given a set of speech
frames x1 . . . xN . Phoneme recognition is often
modeled with a fixed-structure HMM trained with
EM, which makes it a natural application for spec-
tral training.
We train and test on the TIMIT corpus of spoken
language utterances (Garofolo and others, 1988).
The label set consists of l = 39 English phonemes
following a standard phoneme set (Lee and Hon,
1989). For training, we use the sx and si utter-
ances of the TIMIT training section made up of
61
?(F1) ai+1 ? xi, ai+1, xi, np(ai . . . aN )
?(P ) (ai?1, xi?1), ai?1, xi?1, pp(a1 . . . ai)
?(R) xi
?(D) ai?1 ? xi?1, ai?1, xi?1, pp(a1 . . . ai),
pos(a1 . . . aN )
iy r r r r r r ow . . .. . .
pp b m e np
Figure 6: The feature templates for phoneme
recognition. The simplest features look only at the
current label and observation. Other features in-
dicate the previous phoneme type used before ai
(pp), the next phoneme type used after ai (np),
and the relative position (beginning, middle, or
end) of ai within the current phoneme (pos). The
figure gives a typical segment of the phoneme se-
quence a1 . . . aN
M = 3696 utterances. The parameter estimate is
smoothed using the method of Cohen et al (2013).
Each utterance consists of a speech signal
aligned with phoneme labels. As preprocessing,
we divide the signal into a sequence of N over-
lapping frames, 25ms in length with a 10ms step
size. Each frame is converted to a feature repre-
sentation using MFCC with its first and second
derivatives for a total of 39 continuous features.
To discretize the problem, we apply vector quanti-
zation using euclidean k-means to map each frame
into n = 10000 observation classes. After pre-
processing, we have 3696 skeletal sequence with
a1 . . . aN as the frame-aligned phoneme labels and
x1 . . . xN as the observation classes.
For testing, we use the core test portion of
TIMIT, consisting of 192 utterances, and for de-
velopment we use 200 additional utterances. Ac-
curacy is measured by the percentage of frames
labeled with the correct phoneme. During infer-
ence, we calculate marginals ? for each label at
each position i and choose the one with the highest
marginal probability, a?i = arg maxa?[l] ?(a, i).
The spectral method requires defining feature
functions ?, ?, ?, and ?. We use binary-valued
feature vectors which we specify through features
templates, for instance the template ai ? xi corre-
sponds to binary values for each possible label and
output pair (ln binary dimensions).
Figure 6 gives the full set of templates. These
feature functions are specially for the phoneme
labeling task. We note that the HTK baseline
explicitly models the position within the current
Method Accuracy
EM(4) 56.80
EM(24) 56.23
SPECTRAL(24), no np, pp, pos 55.45
SPECTRAL(24), no pos 56.56
SPECTRAL(24) 56.94
Figure 7: Feature ablation experiments on TIMIT
development data for the best spectral model (m =
24) with comparisons to the best EM model (m =
4) and EM with m = 24.
Method Accuracy
UNIGRAM 48.04
HMM 54.08
EM(4) 55.49
SPECTRAL(24) 55.82
HTK 55.70
Figure 8: Performance of baselines and spectral
R-HMM on TIMIT test data. Number of hidden
states m optimized on development data (see fig-
ure 5). The improvement of the spectral method
over the EM baseline is significant at the p ? 0.05
level (and very close to significant at p ? 0.01,
with a precise value of p ? 0.0104).
phoneme as part of the HMM structure. The spec-
tral method is able to encode similar information
naturally through the feature functions.
We implement several baseline for phoneme
recognition: UNIGRAM chooses the most likely
label, arg maxa?[l] p(a|xi), at each position;
HMM is a standard HMM trained with maximum-
likelihood estimation; EM(m) is an R-HMM
with m hidden states estimated using EM; and
SPECTRAL(m) is an R-HMM with m hidden
states estimated with the spectral method de-
scribed in this paper. We also compare to HTK,
a fixed-structure HMM with three segments per
phoneme estimated using EM with the HTK
speech toolkit. See Young et al (2006) for more
details on this method.
An important consideration for both EM and the
spectral method is the number of hidden states m
in the R-HMM. More states allow for greater label
refinement, with the downside of possible overfit-
ting and, in the case of EM, more local optima.
To determine the best number of hidden states, we
optimize both methods on the development set for
a range of m values between 1 to 32. For EM,
62
we run 200 training iterations on each value of m
and choose the iteration that scores best on the de-
velopment set. As the spectral algorithm is non-
iterative, we only need to evaluate the develop-
ment set once per m value. Figure 5 shows the
development accuracy of the two method as we
adjust the value of m. EM accuracy peaks at 4
hidden states and then starts degrading, whereas
the spectral method continues to improve until 24
hidden states.
Another important consideration for the spectral
method is the feature functions. The analysis sug-
gests that the best feature functions are highly in-
formative of the underlying hidden states. To test
this empirically we run spectral estimation with a
reduced set of features by ablating the templates
indicating adjacent phonemes and relative posi-
tion. Figure 7 shows that removing these features
does have a significant effect on development ac-
curacy. Without either type of feature, develop-
ment accuracy drops by 1.5%.
We can interpret the effect of the features in
a more principled manner. Informative features
yield greater singular values for the matrices ?a1
and ?a2, and these singular values directly affect
the sample complexity of the algorithm; see Cohen
et al (2012) for details. In sum, good feature func-
tions lead to well-conditioned ?a1 and ?a2, which in
turn require fewer samples for convergence.
Figure 8 gives the final performance for the
baselines and the spectral method on the TIMIT
test set. For EM and the spectral method, we
use best performing model from the develop-
ment data, 4 hidden states for EM and 24 for
the spectral method. The experiments show that
R-HMM models score significantly better than a
standard HMM and comparatively to the fixed-
structure HMM. In training the R-HMM models,
the spectral method performs competitively with
EM while avoiding the problems of local optima.
8 Conclusion
This paper derives a spectral algorithm for the
task of supervised sequence labeling using an R-
HMM. Unlike EM, the spectral method is guar-
anteed to provide a consistent estimate of the pa-
rameters of the model. In addition, the algorithm
is simple to implement, requiring only an SVD
of the observed counts and other standard ma-
trix operations. We show empirically that when
equipped with informative feature functions, the
spectral method performs competitively with EM
on the task of phoneme recognition.
Appendix
Proof of proposition 4.1. At any time step i ? [N ] in the al-
gorithm in figure 2, for all label a ? [l] we have a column
vector ?ia ? Rm and a row vector ?ia ? Rm. The value of
these vectors at each index h ? [m] can be verified as
[?ia]h =
?
a1...ai,h1...hi:
ai=a,hi=h
p(a1 . . . ai, x1 . . . xi?1, h1 . . . hi)
[?ia]h =?
ai...aN ,hi...hN :
ai=a,hi=h
p(ai+1 . . . aN , xi . . . xN , hi+1 . . . hN |ai, hi)
Thus ?ia?ia is a scalar equal to
?
a1...aN ,h1...hN :ai=a
p(a1 . . . aN , x1 . . . xN , h1 . . . hN )
which is the value of the marginal ?(a, i).
Proof of theorem 5.1. It can be verified that c1a = Gapia. For
the others, under the conditional independence illustrated in
figure 3 we can decompose the observable blocks in terms of
the R-HMM parameters and invertible matrices
?a = Ga?a(Ga)> ?a = Ha?a(Ha)>
Db|a(v) = GbT b|adiag(vHa)?a(Ga)>
D?|a(v) = fadiag(vHa)?a(Ga)> dax = oax?a(Ha)>
using techniques similar to those sketched in Cohen et al
(2012). By proposition 5.1, ?a and ?a are invertible, and
these observable blocks yield the operators that satisfy theo-
rem 4.1 when placed in Eq. (1-3).
References
A. Anandkumar, D. P. Foster, D. Hsu, S.M. Kakade, and Y.K.
Liu. 2012a. Two svds suffice: Spectral decompositions
for probabilistic topic modeling and latent dirichlet alo-
cation. Arxiv preprint arXiv:1204.6703.
A. Anandkumar, D. Hsu, and S.M. Kakade. 2012b. A
method of moments for mixture models and hidden
markov models. Arxiv preprint arXiv:1203.0683.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral
learning algorithm for finite state transducers. Machine
Learning and Knowledge Discovery in Databases, pages
156?171.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-
gar. 2012. Spectral learning of latent-variable PCFGs. In
Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics. Association for Computa-
tional Linguistics.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-
gar. 2013. Experiments with spectral learning of latent-
variable pcfgs. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies.
63
D. P. Foster, J. Rodu, and L.H. Ungar. 2012. Spec-
tral dimensionality reduction for hmms. Arxiv preprint
arXiv:1203.6130.
J. S. Garofolo et al 1988. Getting started with the darpa
timit cd-rom: An acoustic phonetic continuous speech
database. National Institute of Standards and Technology
(NIST), Gaithersburgh, MD, 107.
D. Hsu, S.M. Kakade, and T. Zhang. 2012. A spectral al-
gorithm for learning hidden markov models. Journal of
Computer and System Sciences.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6):1371?
1398.
K. Lari and S. J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer speech & language, 4(1):35?56.
K.F. Lee and H.W. Hon. 1989. Speaker-independent phone
recognition using hidden markov models. Acoustics,
Speech and Signal Processing, IEEE Transactions on,
37(11):1641?1648.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. 2012.
Spectral learning for non-deterministic dependency pars-
ing. In EACL, pages 409?419.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic cfg
with latent annotations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Linguis-
tics, pages 75?82. Association for Computational Linguis-
tics.
A. Parikh, L. Song, and E.P. Xing. 2011. A spectral algo-
rithm for latent tree graphical models. In Proceedings of
the 28th International Conference on Machine Learning.
F. Pereira and Y. Schabes. 1992. Inside-outside reestima-
tion from partially bracketed corpora. In Proceedings
of the 30th annual meeting on Association for Computa-
tional Linguistics, pages 128?135. Association for Com-
putational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learn-
ing accurate, compact, and interpretable tree annotation.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages
433?440. Association for Computational Linguistics.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learn-
ing structured models for phone recognition. In Proc. of
EMNLP-CoNLL.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Proceed-
ings of the IEEE, 77(2):257?286.
S. Siddiqi, B. Boots, and G. J. Gordon. 2010. Reduced-
rank hidden Markov models. In Proceedings of the Thir-
teenth International Conference on Artificial Intelligence
and Statistics (AISTATS-2010).
L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola.
2010. Hilbert space embeddings of hidden markov mod-
els. In Proceedings of the 27th International Conference
on Machine Learning. Citeseer.
S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw,
XA Liu, G. Moore, J. Odell, D. Ollason, D. Povey, et al
2006. The htk book (for htk version 3.4).
64
