Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 807?811,
Dublin, Ireland, August 23-24, 2014.
UTU: Disease Mention Recognition and Normalization with CRFs and
Vector Space Representations
Suwisa Kaewphan
1,2,3?
, Kai Hakaka
1?
, Filip Ginter
1
1
Dept. of Information Technology, University of Turku, Finland
2
Turku Centre for Computer Science (TUCS), Turku, Finland
3
The University of Turku Graduate School (UTUGS), University of Turku, Finland
sukaew@utu.fi, kahaka@utu.fi, ginter@cs.utu.fi
Abstract
In this paper we present our system par-
ticipating in the SemEval-2014 Task 7
in both subtasks A and B, aiming at
recognizing and normalizing disease and
symptom mentions from electronic medi-
cal records respectively. In subtask A, we
used an existing NER system, NERsuite,
with our own feature set tailored for this
task. For subtask B, we combined word
vector representations and supervised ma-
chine learning to map the recognized men-
tions to the corresponding UMLS con-
cepts. Our system was placed 2nd and 5th
out of 21 participants on subtasks A and B
respectively showing competitive perfor-
mance.
1 Introduction
The SemEval 2014 task 7 aims to advance the de-
velopment of tools for analyzing clinical text. The
task is organized by providing the researchers an-
notated clinical records to develop systems that
can detect the mentions of diseases and symptoms
in medical records. In particular, the SemEval task
7 comprises two subtasks, recognizing the men-
tions of diseases and symptoms (task A) and map-
ping the mentions to unique concept identifiers
that belong to the semantic group of disorders in
the Unified Medical Language System (UMLS).
Our team participated in both of these sub-
tasks. In subtask A, we used an existing named
entity recognition (NER) system, NERsuite, sup-
plemented with UMLS dictionary and normaliza-
tion similarity features. In subtask B, we com-
bined compositional word vector representations
?
These authors contributed equally.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
with supervised machine learning to map the rec-
ognized mentions from task A to the UMLS con-
cepts. Our best systems, evaluated on strict match-
ing criteria, achieved F-score of 76.6% for the sub-
task A and accuracy of 60.1% for the subtask B,
showing competitive performance in both tasks.
2 Task A: Named Entity Recognition
with NERSuite
The ML approach based on conditional random
fields (CRFs) has shown to have state-of-the-art
performance in recognizing the biological entities.
We thus performed task A by using NERsuite,
an existing NER toolkit with competitive perfor-
mance on biological entity recognition (Campos
et al., 2013).
NERsuite is a NER system that is built on
top of the CRFsuite (Okazaki, 2007). It con-
sists of three language processing modules: a to-
kenizer, a modified version of the GENIA tagger
and a named entity recognizer. NERsuite allows
user-implemented features in addition to dictio-
nary matching and features shown to benefit the
systems such as raw token, lemma, part-of-speech
(POS) and text chunk.
Prior to detecting the disease mentions by the
recognizer module of NERsuite, the clinical text
is split into sentences by using GENIA Sentence
Splitter, a supervised ML system that is known to
be well optimized for biomedical texts (S?tre et
al., 2007). The sentences are subsequently tok-
enized and POS tagged.
To represent the positive entities, the ?BIO?
model was used in our system. The first tokens
of positive mentions are labeled with ?B? and the
rest with ?I?. Negative examples, non-entities, are
thus labeled with ?O?. This model was used for
both contiguous and discontiguous entities.
The features include the normalization similar-
ity (see Section 3.3), types of medical records (dis-
charge, echo, radiology and ecg), and UMLS dic-
807
Trained Model Precision Recall F-score
train + positive samples 77.3% 72.4% 74.8%
train + development 76.7 % 76.5% 76.6%
Table 1: The results of our different NERsuite
models, announced by the organizers.
tionary matching in addition to NERsuite?s own
feature generation.
The UMLS dictionary is prepared by extract-
ing the UMLS database for the semantic types de-
manded by the task. In addition to those 11 seman-
tic types, ?Finding? was also included in our dic-
tionary since, according to its definition, the con-
cept is also deemed relevant for the task. Due to
the common use of acronyms, which are not ex-
tensively provided by UMLS, we also extended
the coverage of our prepared UMLS dictionary
by extracting medical acronyms from the UMLS
database using regular expression.
We assessed the effect of dictionary matching
by training the models with and without the com-
piled UMLS dictionary and evaluating against the
development set. The model trained with dictio-
nary features outperformed the one without. The
best model was obtained by training the NERsuite
with UMLS dictionary in case-number-symbol
normalization mode. In this mode, all letters,
numbers and symbols are converted to lower case,
zero (0) and underscore ( ) respectively.
The regularization parameter (C2) was selected
by using development set to evaluate the best
model. The default parameter (C2 = 1.0) gave
the best performing system and thus was used
throughout the work.
Finally, for the NER task, we submitted two
models. The first model was trained with the orig-
inal training data and duplicates of sentences with
at least one entity mention. The second model
was trained by using the combination of the first
model?s training data and development set.
2.1 Results and Discussions
Our NER system from both submissions benefited
from the increased number of training examples
while the more diverse training data set gave a bet-
ter performance. The official results are shown in
table 1.
The analysis of our best performing NER sys-
tem is not possible since the gold standard of the
test data is not publicly available. We thus simply
analyze our second NER system based on the eval-
uation on the development data. The F-score of the
system was 75.1% and 88.0% for the strict and re-
laxed evaluation criteria respectively. Among all
the mistakes made by the system, the discontigu-
ous entities were the most challenging ones for the
NERsuite. In development data, the discontiguous
entities contribute about 10% of all entities, how-
ever, only 2% were recognized correctly. On the
contrary, the system did well for the other types as
73% were correctly recognized under strict crite-
ria. This demonstrates that the ?BIO? model has
limitations in representing the discontiguous enti-
ties. Improving the model to better represent the
discontiguous entities can possibly boost the per-
formance of the NER system significantly.
3 Task B: Normalization with
Compositional Vector Representations
Our normalization approach is based on con-
tinuous distributed word vector representations,
namely the state-of-the-art method word2vec
(Mikolov et al., 2013a). Our word2vec model
was trained on a subset of abstracts and full ar-
ticles from the PubMed and PubMed Central re-
sources. This data was used as it was readily
available to us from the EVEX resource (Van Lan-
deghem et al., 2013). Before training, all non-
alphanumeric characters were removed and all to-
kens were lower-cased. Even though a set of unan-
notated clinical reports was provided in the task
to support unsupervised learning methods, our ex-
periments on the development set showed better
performance with the model trained with PubMed
articles. This might be due to the size of the cor-
pora, as the PubMed data included billions of to-
kens whereas the provided clinical reports totaled
in over 200 million tokens.
The dimensionality of the word vectors was set
to 300 and we used the continuous skip-gram ap-
proach. For other word2vec parameters default
values were used.
One interesting feature demonstrated by
Mikolov et al. (2013b; 2013c) is that the vectors
conserve some of the semantic characteristics in
element-wise addition and subtraction. In this task
we used the same approach of simply summing
the word-level vectors to create compositional
vectors for multi-word entities and concepts, i.e.
we looked up the vectors for every token appear-
ing in a concept name or entity and summed them
to form a vector to represent the whole phrase.
808
We then formed a lexicon including all preferred
terms and synonyms of all the concepts in the
subset of UMLS defined in the task guidelines.
This lexicon is a mapping from the compositional
vector representations of the concept names into
the corresponding UMLS identifiers. To select the
best concept for a recognized entity we calculated
cosine similarity between the vector representa-
tion of the given entity and all the concept vectors
in the lexicon and the concept with the highest
similarity was chosen.
Word2vec is generally able to relate different
forms of the same word to each other, but we no-
ticed a small improvement in accuracy when pos-
sessive suffixes were removed and all tokens were
lemmatized.
3.1 Detecting CUI-less Mentions
As some of the mentions in the training data do not
have corresponding concepts in the semantic cat-
egories listed in the task guidelines, they are an-
notated as ?CUI-less?. However, our normaliza-
tion approach will always find the nearest match-
ing concept, thus getting penalized for wrong pre-
dictions in the official evaluation. To overcome
this problem, we implemented three separate steps
for detecting the ?CUI-less? mentions. As the
simplest approach we set a fixed cosine similarity
threshold and if the maximal similarity falls below
it, the mention is normalized to ?CUI-less?. The
threshold value was selected using a grid search to
optimize the performance on the official develop-
ment set. Although this method resulted in decent
performance, it is not capable of coping with cases
where the mention has very high similarity or even
exact match with a concept name. For instance
our system normalized ?aspiration? mentions into
UMLS concept ?Pulmonary aspiration? which has
a synonym ?Aspiration?, thus resulting in an exact
match. To resolve this kind of cases, we used sim-
ilar approach as in the DNorm system (Leaman et
al., 2013b), where the ?CUI-less? mentions occur-
ring several times in the training data were added
to the concept lexicon with concept ID ?CUI-less?.
As the final step we trained a binary SVM classi-
fier to distinguish the ?CUI-less? mentions. The
classifier utilized bag-of-word features as well as
the compositional vectors. The performance im-
provement provided by each of these steps is pre-
sented in table 2. This evaluation shows that each
step increases the performance considerably, but
Method Strict accuracy
B 43.6
T 48.4
T + L 53.5
T + L + C 55.4
O 59.3
Table 2: Evaluation of the different approaches
to detect CUI-less entities on the official develop-
ment set compared to a baseline without CUI-less
detection and an oracle method with perfect de-
tection. This evaluation was done with the entities
recognized by our NER system instead of the gold
standard entities. B = baseline without CUI-less
detection, T = similarity threshold, L = Lexicon-
based method, C = classifier, O = Oracle.
the overall performance is still 3.9pp below per-
fect detection.
3.2 Acronym Resolution
Abbreviations, especially acronyms, form a con-
siderable portion of the entity mentions in clini-
cal reports. One of the problems in normalizing
the acronyms is disambiguation as one acronym
can be associated with multiple diseases. Previ-
ous normalization systems (Leaman et al., 2013b)
handle this by selecting the matching concept with
most occurrences in the training data. However,
this approach does not resolve the problem of
non-standard acronyms, i.e. acronyms that are not
known in the UMLS vocabulary or in other medi-
cal acronym dictionaries. Our goal was to resolve
both of these problems by looking at the other enti-
ties found in the same document instead of match-
ing the acronym against the concept lexicon. With
this approach for instance entity mention ?CP?
was on multiple occasions correctly normalized
into the concept ?Chest Pain?, even though UMLS
is not aware of this acronym for the given concept
and in fact associates it with several other con-
cepts such as ?Chronic Pancreatitis? and ?Cerebral
Palsy?. However, the overall gain in accuracy ob-
tained from this method was only minor.
3.3 Normalization Feedback to Named
Entity Recognition
While basic exact match dictionary features pro-
vide usually a large improvement in NER perfor-
mance, they are prone to bias the system to high
precision and low recall. As both noun and ad-
jective forms of medical concepts, e.g. ?atrium?
and ?atrial?, are commonly used in clinical texts,
809
the entities may not have exact dictionary matches.
Moreover the different forms of medical terms
may not share a common morphological root dis-
covered by simple stemming methods, thus com-
plicating approximate matching. In this task we
tried to boost the recall of our entity recognition by
feeding back the normalization similarity informa-
tion as features. These features included the max-
imum similarity between the token and the UMLS
concepts as a numerical value as well as a boolean
feature describing whether the similarity exceeded
a certain threshold.
In addition we experimented by calculating the
similarities for bigrams and trigrams in a slid-
ing window around the tokens, but these features
did not provide any further performance improve-
ments.
3.4 Other Directions Explored
The DNorm system utilizes TF-IDF vectors to rep-
resent the entities and concepts but instead of cal-
culating cosine similarity, the system trains a rank-
ing algorithm to measure the maximal similarity
(Leaman et al., 2013a). Their evaluation, carried
out on the NCBI disease corpus (Do?gan et al.,
2014), showed a notable improvement in perfor-
mance compared to cosine similarity. In our anal-
ysis we noticed that in 39% of the false predic-
tions made by our normalization system, the cor-
rect concept was in the top 10 most similar con-
cepts. This strongly suggested that a similar rank-
ing method might be beneficial with our system as
well. To test this we trained a linear SVM to rerank
the top 10 concepts with highest cosine similarity,
but we were not able to increase the overall per-
formance of the system. However, due to the strict
time constraints of the task, we cannot conclude
whether this approach is feasible or not.
As our compositional vectors are formed by
summing the word vectors, each word has an equal
weight in the sum. Due to this our system made
various errors where the entity was a single word
matching closely to several concepts with longer
names. For instance entity ?hypertensive? was
falsely normalized to concept ?Hypertensive car-
diopathy? whereas the correct concept was ?Hy-
pertensive disorder?. These mistakes could have
been prevented to some extent if the more impor-
tant words had had a larger weight in the sum, e.g.
word ?disorder? is of low significance when try-
ing to distinguish different disorders. However,
Team Strict accuracy Relaxed accuracy
UTH CCB 74.1 87.3
UWM 66.0 90.9
RelAgent 63.9 91.2
IxaMed 60.4 86.2
UTU 60.1 78.3
Table 3: Official evaluation results for the top 5
teams in the normalization task.
weighting the word vectors with their IDF values,
document in this case being an UMLS concept, did
not improve the performance.
3.5 Results
The official results for the normalization task are
shown in table 3. Our system achieved accuracy
of 60.1% when evaluated with the official strict
evaluation metric. This result suggests that com-
positional vector representations are a competitive
approach for entity normalization. However, the
best performing team surpassed our performance
by 14.0pp, showing that there is plenty of room for
other teams to improve. It is worth noting though
that their recall in the NER task tops ours by 8.2pp
thus drastically influencing the normalization re-
sults as well. To evaluate the normalization sys-
tems in isolation from the NER task, a separate
evaluation set with gold standard entities should
be provided.
4 Conclusions
Overall, our NER system can perform well with
the same default settings of NERsuite for gene
name recognition. The performance improves
when relevant features, such as UMLS dictionary
matching and word2vec similarity are added. We
speculated that representing the nature of the data
with more suitable model can improve the system
performance further. As a part of a combined sys-
tem, the improvement on NER system can result
in the increased performance of normalization sys-
tem.
Our normalization system showed competitive
results as well, indicating that word2vec-based
vector representations are a feasible way of solv-
ing the normalization task. As future work we
would like to explore different methods for cre-
ating the compositional vectors and reassess the
applicability of the reranking approach described
in section 3.4.
810
Acknowledgements
Computational resources were provided by CSC
? IT Center for Science Ltd, Espoo, Finland. This
work was supported by the Academy of Finland.
References
David Campos, S?ergio Matos, and Jos?e Lu??s Oliveira.
2013. Gimli: open source and high-performance
biomedical name recognition. BMC bioinformatics,
14(1):54.
Rezarta Islamaj Do?gan, Robert Leaman, and Zhiyong
Lu. 2014. NCBI disease corpus: a resource for dis-
ease name recognition and concept normalization.
Journal of Biomedical Informatics, 47:1?10, Feb.
Robert Leaman, Rezarta Islamaj Do?gan, and Zhiyong
Lu. 2013a. DNorm: disease name normaliza-
tion with pairwise learning to rank. Bioinformatics,
29(22):2909?2917.
Robert Leaman, Ritu Khare, and Zhiyong Lu.
2013b. NCBI at 2013 ShARe/CLEF eHealth Shared
Task: Disorder normalization in clinical notes with
DNorm. In Proceedings of the Conference and Labs
of the Evaluation Forum.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In ICLR Workshop.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111?3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji,
Yusuke Miyao, Y Matsubyashi, and Tomoko Ohta.
2007. AKANE system: protein-protein interaction
pairs in BioCreAtIvE2 challenge, PPI-IPS subtask.
In Proceedings of the BioCreative II, pages 209?
212.
Sofie Van Landeghem, Jari Bj?orne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, and Filip Ginter. 2013. Large-
scale event extraction from literature with multi-
level gene normalization. PLoS ONE, 8(4):e55814.
811
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 26?34,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
EVEX in ST?13: Application of a large-scale text mining resource
to event extraction and network construction
Kai Hakala1, Sofie Van Landeghem3,4, Tapio Salakoski1,2,
Yves Van de Peer3,4 and Filip Ginter1
1. Dept. of Information Technology, University of Turku, Finland
2. Turku Centre for Computer Science (TUCS), Finland
3. Dept. of Plant Systems Biology, VIB, Belgium
4. Dept. of Plant Biotechnology and Bioinformatics, Ghent University, Belgium
kahaka@utu.fi, solan@psb.ugent.be, yvpee@psb.ugent.be,
ginter@cs.utu.fi, tapio.salakoski@utu.fi
Abstract
During the past few years, several novel
text mining algorithms have been de-
veloped in the context of the BioNLP
Shared Tasks on Event Extraction. These
algorithms typically aim at extracting
biomolecular interactions from text by in-
specting only the context of one sen-
tence. However, when humans inter-
pret biomolecular research articles, they
usually build upon extensive background
knowledge of their favorite genes and
pathways. To make such world knowl-
edge available to a text mining algorithm,
it could first be applied to all available lit-
erature to subsequently make a more in-
formed decision on which predictions are
consistent with the current known data. In
this paper, we introduce our participation
in the latest Shared Task using the large-
scale text mining resource EVEX which
we previously implemented using state-of-
the-art algorithms, and which was applied
to the whole of PubMed and PubMed Cen-
tral. We participated in the Genia Event
Extraction (GE) and Gene Regulation Net-
work (GRN) tasks, ranking first in the for-
mer and fifth in the latter.
1 Introduction
The main objective of our entry was to test the
usability of the large-scale text mining resource
EVEX to provide supporting information to an
existing state-of-the-art event extraction system.
In the GE task, EVEX is used to extract addi-
tional features for event extraction, capturing the
occurrence of relevant events in other documents
across PubMed and PubMed Central. In the GRN
task, EVEX is the sole source of information, i.e.
our entry consists of a modified subset of EVEX,
rather than a new text mining system specifically
trained for the task.
In the 2011 GE task, the majority of partici-
pating systems used features solely extracted from
the immediate textual context of the event candi-
date, typically restricted to a single sentence (Kim
et al, 2012; McClosky et al, 2012; Bjo?rne et al,
2012b; Vlachos and Craven, 2012). Several stud-
ies have subsequently incorporated coreference re-
lations, capturing information also from surround-
ing sentences (Yoshikawa et al, 2011; Miwa et al,
2012). However, no prior work exists on extend-
ing the event context to the information extracted
from other documents on a large scale. The moti-
vation for this entry is thus to test whether a gain
can be obtained by aggregating information across
documents with mutually supporting evidence.
In the following sections, we first introduce
EVEX as the underlying text mining resource, and
then describe the methods developed specifically
for the GRN and GE task entries. Finally, a de-
tailed error analysis of the results offers insight
into the performance of our systems and provides
possible directions of future development.
2 EVEX
EVEX1 is a text mining resource built on top
of events extracted from all PubMed abstracts
and PubMed Central Open-Access full-text doc-
uments (Van Landeghem et al, 2013a). The ex-
traction was carried out using a combination of
the BANNER named entity detector (Leaman and
Gonzalez, 2008) and the TEES event extraction
system as made publicly available subsequent to
the last Shared Task (ST) of 2011 (Bjo?rne et al,
2012a). Specifically, this version of TEES was
trained on the ST?11 GE data.
1http://www.evexdb.org
26
On top of the individual event occurrences,
EVEX provides event generalizations, allowing
the integration and summarization of knowledge
across different articles (Van Landeghem et al,
2011). For instance, the canonicalization algo-
rithm deals with small lexical variations by re-
moving non-alphanumerical characters (e.g. ?Esr-
1? to ?esr1?). The canonical generalization then
groups those events together with the same event
type and the same canonicalized arguments. Addi-
tionally, gene normalization data has recently been
integrated within the EVEX resource, assigning
taxonomic classification and database identifiers
to gene mentions in text using the GenNorm sys-
tem (Wei and Kao, 2011). Finally, the assignment
of genes to homologous families allows a more
coarse-grained generalization of the textual data.
For each generalized event, a confidence score is
automatically calculated based upon the original
TEES classification procedure, with higher values
representing more confident predictions.
Finally, the EVEX resource provides a network
interpretation which transforms events into pair-
wise gene/protein relations to represent a typed,
directed network. The primary advantage of such
a network, as compared to the complex, recursive
event structures, is that a network is more eas-
ily analysed and integrated with other external re-
sources (Kaewphan et al, 2012; Van Landeghem
et al, 2013b).
3 GRN Task
The Gene Regulatory Network subtask of the
ST?13 aims at evaluating the ability of text min-
ing systems to automatically compile a gene regu-
lation network from the literature. The task is fo-
cused specifically on sporulation in Bacillus sub-
tilis, a thoroughly studied process.
3.1 Challenge definition
The primary goal of our participation in this task
was assessing the ability to reconstruct regulatory
networks directly from the EVEX resource. Con-
sequently, we have applied the EVEX data as it
is publicly available. This decision has two major
consequences. First, we have used the predicted
BANNER entities rather than the gold-standard
entity annotation, artificially rendering the chal-
lenge more difficult. Second, we did not adapt the
EVEX events, which follow the ST?11 GE formal-
ism, to the novel annotation scheme of the GRN
EVEX type GRN type
Binding Binding
Regulation* of Transcription Transcription
Regulation* of Gene expression Transcription
Positive regulation of Any* Activation
Negative regulation of Any* Inhibition
Regulation of Any* Regulation
Table 1: Conversion of EVEX event types to the
GRN types. The table is traversed from top to
bottom, and the first rule that matches is applied.
Regulation* refers to any type of regulatory event,
and Any* refers to any other non-regulatory event
type.
challenge, but rather derived the network data di-
rectly from the EVEX interactions. For example,
given these trigger annotations
T1 Protein 37 43 sigmaB
T2 Gene 54 58 katX
T3 Transcription 59 69 expression
a GE Transcription event looks like
E1 Transcription:T3 Theme:T2 Cause:T1
while the GRN annotation is given by
R1 Transcription Target:E1 Agent:T1
E1 Action_Target:T3 Target:T2
However, both formalisms can easily be trans-
lated into the required GRN network format:
sigB Interaction.Transcription katX
where ?sigB? is annotated as the Gene identifier
of ?sigmaB?. These gene identifiers are provided
in the gold-standard entity annotations. Note that
in this context, ?gene identifiers? are standardized
gene symbols rather than numeric identifiers, and
full gene normalization is thus not required.
3.2 From EVEX to GRN data
As a first step towards creating a gene regula-
tory network directly from EVEX, we have down-
loaded all pairwise relations of the canonical gen-
eralization (Section 2). For each such relation,
we also obtain important meta-data, including the
confidence value, the PubMed IDs in which a re-
lation was found, whether or not those articles
describe Bacillus subtilis research, and whether
or not those articles are part of the GRN train-
ing or test set. In the most stringent setting, we
could then limit the EVEX results only to those
relations found in the articles of the GRN dataset
(72 in training, 45 in the development set, 55 in
the test set). Additionally, we could test whether
performance can be improved by also adding all
Bacillus subtilis articles (17,065 articles) or even
27
GRN event type Possible target types Possible agent types
Interaction.Binding Protein Gene
Interaction.Transcription Protein, PolymeraseComplex Gene, Operon
Interaction.Regulation
Protein, PolymeraseComplex Gene, Operon, Protein, ProteinComplexInteraction.Activation
Interaction.Inhibition
Table 2: Entity-type filtering of event predictions. Only those events for which the arguments (the target
as well as the agent) have the correct entity types, are retained in the result set.
all EVEX articles in which at least one event was
found (4,107,953 articles).
To match the canonicalized BANNER entities
from EVEX to the standardized gene symbols
required for the GRN challenge, we have con-
structed a mapping based on the GRN data. First,
we have scanned all gold-standard entities and
removed non-alphanumerical characters from the
gene symbols as tagged in text. Next, these canon-
ical forms were linked to the corresponding stan-
dardized gene symbols in the gold-standard anno-
tations. From the EVEX data, we then only re-
tained those relations that could be linked to two
gene symbols occurring together in a sentence.
Finally, it was necessary to convert the origi-
nal EVEX event types to the GRN relation types.
This mapping is summarized in Table 1. Because
EVEX Binding events are symmetrical and GRN
Bindings are not, we add both possible directions
to the result set. Note that some GRN types could
not be mapped because they have no equivalent
within the EVEX resource, such as the GRN type
?Requirement? or ?Promoter?.
3.3 Filtering the data
After converting the EVEX pairwise relations to
the GRN network format, it is necessary to fur-
ther process the set of predictions to obtain a co-
herent network. One additional filtering step con-
cerns the entity types of the arguments of a specific
event type. From the GRN data, we can retrieve
a symbol-to-type mapping, recording whether a
specific symbol referred to e.g. a gene, protein
or operon in a certain article. After careful in-
spection of the GRN guidelines and the training
data, we enforced the filtering rules as listed in
Table 2. For example, this procedure success-
fully removes protein-protein interactions from
the dataset, which are excluded according to the
GRN guidelines. Even though these rules are oc-
casionally more restrictive than the original GRN
guidelines, their effectiveness to prune the data
was confirmed on the training set.
Further, the GRN guidelines specify that a set
of edges with the same Agent and Target should
be resolved into a single edge, giving preference
to a more specialized type, such as Transcription
in favour of Regulation. Further, contradictory
types between a specific entity pair (e.g. Inhibition
and Activation) may occur simultaneously in the
GRN data. For the EVEX data however, it is more
beneficial to try and pick one single correct event
type from the set of predictions, effectively reduc-
ing the false positive rate. To this end, the EVEX
confidence values are used to determine the single
most plausible candidate. Further analyses on the
training data suggested that the best performance
could be achieved when only retaining the ?Mech-
anism? edges (Transcription and Binding) in cases
when no regulatory edge was found. Finally, we
noted that the EVEX Binding events more often
correspond to the GRN Transcription type, and
they were thus systematically refactored as such
(after entity-type filtering). We believe this shift
in semantics is caused by the fact that a promoter
binding is usually extracted as a binding event by
the TEES classifier, while it can semantically be
seen as a Transcription event, especially in those
cases where the Theme is a protein name, and the
Cause a gene symbol (Table 2).
3.4 Results
Table 3 lists the results of our method on the GRN
training data, which was primarily used for tun-
ing the parameters described in Section 3.3. The
highest recall (42%) could be obtained when using
all EVEX data, without restrictions on entity types
and without restricting to Bacillus subtilis articles.
As a result, this set of predictions may contain re-
lations between homologs in related species which
have the same name. While the relaxed F-score
(41%) is quite high, the Slot Error Rate (SER)
score (1.56) is unsatisfying, as SER scores should
be below 1 for decent predictions.
When applying entity type restrictions to the
prediction set, relaxed precision rises from 39%
28
Dataset ETF SER F Rel. P Rel. R Rel. F Rel. SER
All EVEX data no 1.56 8.86 39.29% 41.98% 40.59% 1.23
All EVEX data yes 1.15 11.53 59.74% 35.11% 44.23% 0.89
B. subtilis PMIDs yes 0.954 20.81 71.43% 22.90% 34.68% 0.86
GRN PMIDs yes 0.939 17.39 80.00% 18.32% 29.81% 0.86
Table 3: Performance measurement of a few different system settings, applied on the training data. The
SER score is the main evaluation criterion of the GRN challenge. The relaxed precision, recall, F and
SER scores are produced by scoring the predictions regardless of the specific event types. ETF refers to
entity type filtering.
to 60%, the relaxed F-score obtains a maximum
score of 44%, and the SER score improves to
1.15. The SER score can further be improved
when restricting the data to Bacillus subtilis arti-
cles (0.954). The optimal SER score is obtained by
further limiting the prediction set to only those re-
lations found in the articles from the GRN dataset
(0.939), maximizing at the same time the relaxed
precision rate (80%).
The final run which obtained the best SER score
on the training data was subsequently applied on
the GRN test data. It is important to note that the
parameter selection of our system was not overfit-
ted on the training data, as the SER score of our
final submission on the test data is 0.92, i.e. higher
than the best run on the training data.
Table 4 summarizes the official results of all
participants to the GRN challenge. Interestingly,
the TEES classifier has been modified to retrain
itself on the GRN data and to produce event
annotations in the GRN formalism (Bjo?rne and
Salakoski, 2013), obtaining a final SER score of
0.86. It is remarkable that this score is only 0.06
points better than our system which needed no re-
training, and which was based upon the original
GE annotation format and predicted gene/protein
symbols rather than gold-standard ones. Addition-
ally, the events in EVEX have been produced by a
version of TEES which was maximized on F-score
rather than SER score, and these measurements
are not mutually interchangeable (Table 3). We
conclude that even though our GRN system ob-
tained last place out of 5 participants, we believe
that its relative close performance to the TEES
submission demonstrates that large-scale text min-
ing resources can be used for gene regulatory net-
work construction without the need for retraining
the text mining component.
3.5 Error analysis
To determine the underlying reasons of our rela-
tively low recall rate, we have analysed the 117
SER Relaxed SER
University of Ljubljana 0.73 0.64
K.U.Leuven 0.83 0.66
TEES-2.1 0.86 0.76
IRISA-TexMex 0.91 0.60
EVEX 0.92 0.81
Table 4: Official GRN performance rates.
false negative predictions of our final run on the
training dataset. We found that 23% could be at-
tributed to a missing or incompatible BANNER
entity, 59% to a false negative TEES prediction,
15% to a wrong GRN event type and 3% to incor-
rectly mapping the gene symbol to the standard-
ized GRN format. Analysing the 16 false positives
in the same dataset, 25% could be attributed to an
incorrectly predicted event structure, and 62.5% to
a wrongly predicted event type. One case was cor-
rectly predicted but from a sentence outside the
GRN data, and in one case a correctly predicted
negation context was not taken into account. In
conclusion, future work on the GRN conversion of
TEES output should mainly focus on refining the
event type prediction, while general performance
could be enhanced by further improving the TEES
classification system.
4 GE Task
Our GE submission builds on top of the TEES 2.1
system2 as available just prior to the ST?13 test pe-
riod. First applying the unmodified TEES system,
we subsequently re-ranked its output and enforced
a cut-off threshold with the objective of removing
false positives from the TEES output (Section 4.1).
In the official evaluation, this step results in a mi-
nor 0.23pp increase of F-score compared to unpro-
cessed TEES output (Table 5). This yields the first
rank in the primary measure of the task with TEES
ranking second.
The main motivation for the re-ranking ap-
2https://github.com/jbjorne/TEES/wiki/
TEES-2.1
29
P R F
EVEX 58.03 45.44 50.97
TEES-2.1 56.32 46.17 50.74
BioSEM 62.83 42.47 50.68
NCBI 61.72 40.53 48.93
DlutNLP 57.00 40.81 47.56
Table 5: Official precision, recall and F-score rates
of the top-5 GE participants, in percentages.
proach was the ability to incorporate external in-
formation from EVEX to compare the TEES event
predictions and identify the most reliable ones.
Further, such a re-ranking approach leads to an in-
dependent component which is in no way bound to
TEES as the underlying event extraction system.
The component can be combined with any system
with sufficient recall to justify output re-ranking.
4.1 Event re-ranking
The output of TEES is re-ranked using SVMrank,
a formulation of Support Vector Machines which
is trained to optimize ranking, rather than classifi-
cation (Joachims, 2006). It differs from the basic
linear SVM classifier in the training phase, when a
query structure is defined as a subset of instances
which can be meaningfully compared among each
other ? in our case all events from a single sen-
tence. During training, only instances within a
single query are compared and the SVM does not
aim to learn a global ranking across sentences and
documents. We also experimented with polyno-
mial and radial basis kernels, feature vector nor-
malization and broadening the ranking query sets
to whole sections or narrowing them to only events
with shared triggers, but none of these settings
were found to further enhance the performance.
The re-ranker assigns a numerical score to each
event produced by TEES, and all events below
a certain threshold score are removed. To set
this threshold, a linear SVM regressor is applied
with the SVMlight package (Joachims, 1999) to
each sentence individually, i.e. we do not apply a
data-wide, pre-set threshold. Unlike the re-ranker
which receives features from a single event at a
time, the regressor receives features describing the
set of events in a single sentence.
Re-ranker features
Each event is described using a number of fea-
tures, including the TEES prediction scores for
triggers and arguments, the event structure, and
the EVEX information about this as well as simi-
lar events. Events can be recursively nested, with
the root event containing other events as its ar-
guments. The root event is of particular impor-
tance as the top-most event. A number of fea-
tures are thus dedicated specifically to this root
event, while other features capture properties of
the nested events.
Features derived from TEES confidence scores:
? TEES trigger detector confidence of the root
event and its difference from the confidence
of the negative class, i.e. the margin by which
the event was predicted by TEES.
? Minimum and maximum argument confi-
dences of the root event.
? Minimum and maximum argument confi-
dences, including recursively nested events
(if any).
? Minimum and maximum trigger confidences,
including recursively nested events (if any).
? Difference between the minimum and max-
imum argument confidences compared to
other events sharing the same trigger word.
Features describing the structure of the event:
? Event type of the root trigger.
? For each path in the event from the root to
a leaf argument, the concatenation of event
types along the path.
? For each path in the event from a leaf argu-
ment to another leaf argument, the concate-
nation of event types along the path.
? The event structure encoded in the bracketed
notation with leaf (T)heme and (C)ause argu-
ments replaced by a placeholder string, e.g.
Regulation(C:_, T:Acetylation(T:_)).
Features describing other events in the same sen-
tence:
? Event counts for each event type.
? Event counts for each unique event structure
given by the bracketed structure notation.
All event counts extracted from EVEX are rep-
resented as their base-10 logarithm to compress
the range and suppress differences in counts of
very common events.
The following features are generated in two ver-
sions, one by grouping the events according to the
EVEX canonical generalization and one for the
Entrez Gene generalization (Section 2)3.
3The generalizations based on gene families were evalu-
ated as well, but did not result in a positive performance gain.
30
? All occurrences of the given event in EVEX.
? For each path from root to a leaf gene/protein,
all occurrences of that exact path in EVEX.
? For each pair of genes/proteins in the event,
all occurrences of that pair in the network in-
terpretation of EVEX.
? For each pair of genes/proteins in the event,
all occurrences of that pair with a different
event type in the network interpretation of
EVEX.
For each event, path, or pair under considera-
tion, features are created for the base-10 logarithm
of the count in EVEX and of the number of unique
articles in which it was identified, as well as for
the minimum, maximum, and average confidence
values, discretized into six unique categories.
Regressor features
While the re-ranker features capture a single event
at a time, the threshold regressor features aggre-
gate information about events extracted within one
sentence. The features include:
? For each event type, the average and mini-
mum re-ranker confidence score, as well as
the count of events of that type.
? For each event type, the count of events shar-
ing the same trigger.
? For each event type, the count of events shar-
ing the same arguments.
? Minimum and maximum confidence values
of triggers and arguments in the TEES out-
put for the sentence.
? The section in the article in which the sen-
tence appears, as given in the ST data.
4.2 Training phase
To train the re-ranker and the regressor, false pos-
itive events are needed in addition to the true pos-
itive events in the training data. We thus apply
TEES to the training data and train the re-ranker
using the correct ranking of the extracted events.
A true positive event is given the rank 1 and a false
positive event gets the rank -1. A query structure
is then defined, grouping all events from a sin-
gle sentence to avoid mutual comparison of events
across sentences and documents during the train-
ing phase.
The trained re-ranker is then again applied to
the training data. For every sentence, the optimal
threshold is set to be the re-ranker score of the last
event which should be retained so as to maximize
# P R F
Simple events 833 -0.08 -0.36 -0.23
Protein mod. 191 +0.09 -2.09 -1.12
Binding 333 +0.43 -1.20 -0.44
Regulation 1944 +2.38 -0.67 +0.36
All 3301 +1.71 -0.73 +0.23
Table 6: Performance difference in percentage
points against the TEES system in the official test
set results, shown for different event types.
the F-score. In case the sentence only contains
false positives, the highest score is used, increased
by an empirically established value of 0.2. A sim-
ilar strategy is applied for sentences only contain-
ing true positives by using the lowest score, de-
creased by 0.2.
In both steps, the SVM regularization parameter
C is set by a grid search on the development set.
Applying TEES and the re-ranker back to the
training set results in a notably smaller propor-
tion of false positives than would be expected on
a novel input. To obtain a fully realistic train-
ing dataset for the re-ranker and threshold regres-
sor would involve re-training TEES in a cross-
validation setting, but this was not feasible due to
the tight schedule constraints of the shared task,
and is thus left as future work.
4.3 Error analysis
Although the re-ranking approach resulted in a
consistent gain over the state-of-the-art TEES sys-
tem on both the development and the test sets,
the overall improvement is only modest. As sum-
marized in Table 6, the gain over the TEES sys-
tem can be largely attributed to regulation events
which exhibit a 2.38pp gain in precision for a
0.67pp loss in recall. Regulation events are at the
same time by far the largest class of events, thus
affecting the overall score the most.
In this section, we analyse the re-ranker and
threshold regressor in isolation to understand their
individual contributions to the overall result and to
identify interesting directions for future research.
To isolate the re-ranker from the threshold re-
gressor and to identify the maximal attainable per-
formance, we set an oracle threshold in every sen-
tence so as to maximize the sentence F-score and
inspect the performance at this threshold, effec-
tively bypassing the threshold regressor. This,
however, provides a very optimistic estimate for
sentences where all predicted events are false pos-
itives, because the oracle then simply obtains the
31
All events P R F
B-C oracle (re-ranked) 81.32 39.61 53.27
W-C oracle (re-ranked) 54.92 39.61 46.02
W-C oracle (random) 51.06 39.19 44.34
Current system 47.15 39.61 43.05
TEES 45.46 40.39 42.77
Single-arg. events
B-C oracle (re-ranked) 81.37 50.58 62.38
W-C oracle (re-ranked) 56.09 50.58 53.19
W-C oracle (random) 52.73 50.00 51.33
Current system 48.66 50.44 49.53
TEES 47.16 51.09 49.04
Multiple-arg. events
B-C oracle (re-ranked) 81.02 16.83 27.87
W-C oracle (re-ranked) 48.61 16.83 25.00
W-C oracle (random) 42.66 16.75 24.05
Current system 39.64 17.12 23.91
TEES 37.57 18.17 24.50
Table 7: Performance comparison of the best case
(B-C) and worst case (W-C) oracles, the current
system with the re-ranker and threshold regressor,
and TEES. As an additional baseline, the worst
case oracle is also calculated for randomly ranked
output. All results are reported also separately for
single and multiple-argument events.
decisions from the gold standard and the rank-
ing itself is irrelevant. This effect is particu-
larly pronounced in sentences where only a sin-
gle, false positive event is predicted (15.9% of all
sentences with at least one event). Therefore, in
addition to this best case oracle score, we also de-
fine a worst case oracle score, where no events
are removed from sentences containing only false-
positives. This error analysis is carried out on the
development set using our own implementation of
the performance measure to obtain per-event cor-
rectness judgments.
The results are shown in Table 7. Even for the
worst case oracle, the re-ranked output has the po-
tential to provide a 9.5pp increase in precision for
a 0.8pp loss in recall over the baseline TEES sys-
tem. How much of this potential gain is realized
depends on the accuracy of the threshold regres-
sor. In the current system, only a 1.7pp precision
increase for a 0.8pp recall loss is attained, demon-
strating that the threshold regressor leaves much
room for improvement.
The best case oracle precision is 26.4pp higher
than the worst case oracle, indicating that substan-
tial performance losses can be attributed to sen-
tences with purely false positive events. Indeed,
sentences only containing one or two incorrect
events account for 26% of all sentences with at
least one predicted event. Due to their large impact
TEES 1-arg N-arg Full
Simple events 64.43 +0.07 ?0.00 +0.07
Protein mod. 40.47 +0.06 ?0.00 +0.06
Binding 82.03 ?0.00 ?0.00 ?0.00
Regulation 30.34 +0.70 -0.14 +0.53
All events 45.04 +0.66 ?0.00 +0.64
Table 8: Performance of the system on the de-
velopment set when applied to single-argument
events only (1-arg), to multiple-argument events
only (N-arg), and to all events (Full).
on the overall system performance, these cases
may justify a focused effort in future research.
To establish the relative merit of the re-ranker,
we compare the worst-case oracle scores of the re-
ranked output against random ranking, averaged
over 10 randomization runs. While the difference
between TEES output and the random ranking re-
flects the effect of using an oracle to optimize per-
sentence score, the difference between the ran-
dom ranking and the re-ranker output shows an
actual added value of the re-ranker, not attained
from the use of oracle thresholds. Here it is of
particular interest to note that this difference is
more pronounced for events with multiple argu-
ments (5.95pp of precision) as opposed to single-
argument events (3.36pp of precision), possibly
due to the fact that such events have a much richer
feature representation and also employ the EVEX
resource. To assess the contribution of EVEX
data, a re-ranker was trained solely on features de-
rived from EVEX. This re-ranker achieved an F-
score of 1.26pp higher than randomized ranking,
thus suggesting that these features have a positive
influence on the overall score.
To verify these results and measure their im-
pact on the official evaluation, Table 8 summa-
rizes the performance on the development set us-
ing the official evaluation service. To study the
effect on single-argument events (column 1-arg),
the re-ranker score for multiple-argument events
is artificially increased to always fall above the
threshold. A similar strategy is used to study
the effect on multiple-argument events (column
N-arg). These results confirm that the overall
performance gain of our system on top of TEES
is obtained on single-argument events. Further,
multiple-argument events have only a negligible
effect on the overall score, demonstrating that, due
to their low frequency, little can be gained or lost
purely on multiple-argument events.
To summarize the error analysis, the results in
32
Table 7 suggest that the re-ranker is more effec-
tive on multiple-argument events where it receives
more features including external information from
EVEX. On the other hand, the results in Table 8
clearly demonstrate that the system is overall more
effective on single-argument events. This would
suggest a ?mismatch? between the re-ranker and
the threshold regressor, each being more effective
on a different class of events. One possible expla-
nation is the fact that the threshold regressor pre-
dicts a single threshold for all events in a sentence,
regardless of their type and number of arguments.
If these cannot be distinguished by one threshold,
it is clear that the threshold regressor will optimize
for the largest event type, i.e. a single-theme regu-
lation. Studying ways to allow the regressor to act
separately on various event types will be important
future work.
4.4 Discussion and future work
One of the main limitations of our approach is
that it can only increase precision, but not recall,
since it removes events from the TEES output, but
is not able to introduce new events. As TEES
utilizes separate processing stages for predicting
event triggers and argument edges, recall can be
adjusted by altering either of these steps. We
have briefly experimented with modifying TEES
to over-generate events by artificially lowering the
prediction threshold for event triggers. However,
this simple strategy of over-generating triggers
leads to a number of clearly incorrect events and
did not provide any performance gain. As future
work, we thus hope to explore effective ways to
over-generate events in a more controlled and ef-
fective fashion. In particular, a more detailed eval-
uation is needed to assess whether the rate of trig-
ger over-generation should be adjusted separately
for each event type. Another direction to explore
is to over-generate argument edges. This will en-
tail a detailed analysis of partially correct events
with a missing argument in TEES output. As in
the case of triggers, it is likely that each event type
will need to be optimized separately.
A notable amount of sentences include only
false positive predictions, severely complicating
the threshold regression. In an attempt to over-
come this issue, we trained a sentence classifier
for excluding sentences that should not contain
any events. This classifier partially utilized the
same features as the threshold regressor, as well
as bag of words and bag of POS tags. This
method showed some promise when used together
with trigger over-generation, but the gain was not
enough to surpass the lost precision caused by the
over-generation. If the event over-generation can
be improved, the feasibility of this method should
be re-evaluated.
5 Conclusions
We have presented our participation in the latest
BioNLP Shared Task by mainly relying on the
large-scale text mining resource EVEX. For the
GRN task, we were able to produce a gene reg-
ulatory network from the EVEX data without re-
training specific text mining algorithms. Using
predicted gene/protein symbols and the GE for-
malism, rather than gold standard entities and the
GRN annotation scheme, our final result on the
test set only performed 0.06 SER points worse
as compared to the corresponding TEES submis-
sion. This encouraging result warrants the use of
generic large-scale text mining data in network bi-
ology settings. As future work, we will extend the
EVEX dataset with information on the entity types
to enable pruning of false-positive events and
more fine-grained classification of event types,
such as the distinction between promoter binding
(Protein-Gene Binding) and protein-protein inter-
actions (Protein-Protein Binding).
In the GE task, we explored a re-ranking ap-
proach to improve the precision of the TEES
event extraction system, also incorporating fea-
tures from the EVEX resource. This approach
led to a modest increase in the overall F-score
of TEES and resulted in the first rank on the GE
task. In the subsequent error analysis, we have
demonstrated that the re-ranker provides an oppor-
tunity for a substantial increase of performance,
only partially realized by the regressor which sets
a per-sentence threshold. The analysis has identi-
fied numerous future research directions.
Acknowledgments
Computational resources were provided by CSC
IT Center for Science Ltd., Espoo, Finland. The
work of KH and FG was supported by the
Academy of Finland, and of SVL by the Research
Foundation Flanders (FWO). YVdP and SVL ac-
knowledge the support from Ghent University
(Multidisciplinary Research Partnership Bioinfor-
matics: from nucleotides to networks).
33
References
Jari Bjo?rne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the BioNLP
2013 Shared Task. In Proceedings of BioNLP
Shared Task 2013 Workshop. In press.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012a.
Generalizing biomedical event extraction. BMC
Bioinformatics, 13(suppl. 8):S4.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012b.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD).
Suwisa Kaewphan, Sanna Kreula, Sofie Van Lan-
deghem, Yves Van de Peer, Patrik Jones, and Filip
Ginter. 2012. Integrating large-scale text mining
and co-expression networks: Targeting NADP(H)
metabolism in E. coli with event extraction. In Pro-
ceedings of the Third Workshop on Building and
Evaluating Resources for Biomedical Text Mining
(BioTxtM 2012).
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The Genia event and protein coreference tasks of the
BioNLP Shared Task 2011. BMC Bioinformatics,
13(Suppl 11):S1.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: an executable survey of advances in biomedi-
cal named entity recognition. Pacific Symposium on
Biocomputing. Pacific Symposium on Biocomputing,
pages 652?663.
David McClosky, Sebastian Riedel, Mihai Surdeanu,
Andrew McCallum, and Christopher Manning.
2012. Combining joint models for biomedical event
extraction. BMC Bioinformatics, 13(Suppl 11):S9.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759?1765.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011. EVEX: a PubMed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of the BioNLP
2011 Workshop, pages 28?37.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, and Filip Ginter. 2013a. Large-
scale event extraction from literature with multi-
level gene normalization. PLoS ONE, 8(4):e55814.
Sofie Van Landeghem, Stefanie De Bodt, Zuzanna J.
Drebert, Dirk Inze?, and Yves Van de Peer. 2013b.
The potential of text mining in data integration and
network biology for plant research: A case study on
Arabidopsis. The Plant Cell, 25(3):794?807.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Chih-Hsuan Wei and Hung-Yu Kao. 2011. Cross-
species gene normalization by species inference.
BMC Bioinformatics, 12(Suppl 8):S5.
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu Hi-
rao, Masayuki Asahara, and Yuji Matsumoto. 2011.
Coreference based event-argument relation extrac-
tion on biomedical text. Journal of Biomedical Se-
mantics, 2(Suppl 5):S6.
34
