Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17?20,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
RDRPOSTagger: A Ripple Down Rules-based Part-Of-Speech Tagger
Dat Quoc Nguyen
1
and Dai Quoc Nguyen
1
and Dang Duc Pham
2
and Son Bao Pham
1
1
Faculty of Information Technology
University of Engineering and Technology
Vietnam National University, Hanoi
{datnq, dainq, sonpb}@vnu.edu.vn
2
L3S Research Center, Germany
pham@L3S.de
Abstract
This paper describes our robust, easy-
to-use and language independent toolkit
namely RDRPOSTagger which employs
an error-driven approach to automatically
construct a Single Classification Ripple
Down Rules tree of transformation rules
for POS tagging task. During the demon-
stration session, we will run the tagger on
data sets in 15 different languages.
1 Introduction
As one of the most important tasks in Natural
Language Processing, Part-of-speech (POS) tag-
ging is to assign a tag representing its lexical
category to each word in a text. Recently, POS
taggers employing machine learning techniques
are still mainstream toolkits obtaining state-of-
the-art performances
1
. However, most of them are
time-consuming in learning process and require a
powerful computer for possibly training machine
learning models.
Turning to rule-based approaches, the most
well-known method is proposed by Brill (1995).
He proposed an approach to automatically learn
transformation rules for the POS tagging problem.
In the Brill?s tagger, a new selected rule is learned
on a context that is generated by all previous rules,
where a following rule will modify the outputs of
all the preceding rules. Hence, this procedure re-
turns a difficulty to control the interactions among
a large number of rules.
Our RDRPOSTagger is presented to overcome
the problems mentioned above. The RDRPOSTag-
ger exploits a failure-driven approach to auto-
matically restructure transformation rules in the
form of a Single Classification Ripple Down Rules
(SCRDR) tree (Richards, 2009). It accepts inter-
actions between rules, but a rule only changes the
1
http://aclweb.org/aclwiki/index.php?title=POS_Tagging_(State_of_the_art)
outputs of some previous rules in a controlled con-
text. All rules are structured in a SCRDR tree
which allows a new exception rule to be added
when the tree returns an incorrect classification.
A specific description of our new RDRPOSTagger
approach is detailed in (Nguyen et al., 2011).
Packaged in a 0.6MB zip file, implementations
in Python and Java can be found at the tagger?s
website http://rdrpostagger.sourceforge.net/. The
following items exhibit properties of the tagger:
? The RDRPOSTagger is easy to configure and
train. There are only two threshold parameters uti-
lized to learn the rule-based model. Besides, the
tagger is very simple to use with standard input
and output, having clear usage and instructions
available on its website.
? The RDRPOSTagger is language independent.
This POS tagging toolkit has been successfully
applied to English and Vietnamese. To train the
toolkit for other languages, users just provide a
lexicon of words and the most frequent associated
tags. Moreover, it can be easily combined with ex-
isting POS taggers to reach an even better result.
? The RDRPOSTagger obtains very competitive
accuracies. On Penn WSJ Treebank corpus (Mar-
cus et al., 1993), taking WSJ sections 0-18 as the
training set, the tagger achieves a competitive per-
formance compared to other state-of-the-art En-
glish POS taggers on the test set of WSJ sections
22-24. For Vietnamese, it outperforms all previ-
ous machine learning-based POS tagging systems
to obtain an up-to-date highest result on the Viet-
namese Treebank corpus (Nguyen et al., 2009).
? The RDRPOSTagger is fast. For instance in
English, the time
2
taken to train the tagger on
the WSJ sections 0-18 is 40 minutes. The tagging
speed on the test set of the WSJ sections 22-24 is
2800 words/second accounted for the latest imple-
mentation in Python whilst it is 92k words/second
2
Training and tagging times are computed on a Windows-
7 OS computer of Core 2Duo 2.4GHz & 3GB of memory.
17
Figure 1: A part of our SCRDR tree for English POS tagging.
for the implementation in Java.
2 SCRDR methodology
A SCRDR tree (Richards, 2009) is a binary tree
with two distinct types of edges. These edges are
typically called except and if-not edges. Associ-
ated with each node in a tree is a rule. A rule has
the form: if ? then ? where ? is called the condi-
tion and ? is referred to as the conclusion.
Cases in SCRDR are evaluated by passing a
case to the root of the tree. At any node in the
tree, if the condition of a node N ?s rule is satis-
fied by the case, the case is passed on to the ex-
ception child ofN using the except link if it exists.
Otherwise, the case is passed on to the N ?s if-not
child. The conclusion given by this process is the
conclusion from the last node in the SCRDR tree
which fired (satisfied by the case). To ensure that
a conclusion is always given, the root node typi-
cally contains a trivial condition which is always
satisfied. This node is called the default node.
A new node containing a new rule (i.e. a new ex-
ception rule) is added to an SCRDR tree when the
evaluation process returns the wrong conclusion.
The new node is attached to the last node in the
evaluation path of the given case with the except
link if the last node is the fired one. Otherwise, it
is attached with the if-not link.
For example with the SCRDR tree in the fig-
ure 1, given a case ?as/IN investors/NNS an-
ticipate/VB a/DT recovery/NN? where ?antici-
pate/VB? is the current word and tag pair, the case
satisfies the conditions of the rules at nodes (0),
(1) and (3), it then is passed to the node (6) (utiliz-
ing except links). As the case does not satisfy the
condition of the rule at node (6), it will be trans-
ferred to node (7) using if-not link. Since the case
does not fulfill the conditions of the rules at nodes
(7) and (8), we have the evaluation path (0)-(1)-
(3)-(6)-(7)-(8) with fired node (3). Therefore, the
tag for ?anticipate? is concluded as ?VBP?.
Rule (1) - the rule at node (1) - is the exception
rule
3
of the default rule (0). As node (2) is the if-
not child node of the node (1), the associated rule
(2) is also an exception rule of the rule (0). Simi-
larly, both rules (3) and (4) are exception rules of
the rule (1) whereas all rules (6), (7) and (8) are
exception rules of the rule (3), and so on. Thus,
the exception structure of the SCRDR tree extends
to 4 levels: rules (1) and (2) at layer 1, rules (3),
(4) and (5) at layer 2, rules (6), (7) and (8) at layer
3, and rule (9) at layer 4.
3 The RDRPOSTagger toolkit
The toolkit consists of four main compo-
nents: Utility, Initial-tagger, SCRDR-learner and
SCRDR-tagger.
3.1 The Utility
The major functions of this component are to eval-
uate tagging performances (displaying accuracy
results), and to create a lexicon of words and the
most frequent associated tags as well as to extract
Raw corpus from an input golden training corpus.
3.2 The Initial-tagger
The initial-tagger developed in the RDRPOSTag-
ger toolkit is based on the lexicon which is gen-
erated in the use of the Utility component to as-
sign a tag for each word. To deal with unknown
words, the initial-tagger utilizes several regular ex-
pressions or heuristics for English and Vietnamese
whereas the most frequent tag in the training cor-
pus is exploited to label unknown-words when
adapting to other languages.
3.3 The SCRDR-learner
The SCRDR-learner component uses a failure-
driven method to automatically build a SCRDR
tree of transformation rules. Figure 3 describes the
learning process of the learner.
3
The default rule is the unique rule which is not an excep-
tion rule of any other rule. Every rule in layer n is an excep-
tion rule of a rule in layer n? 1.
18
#12: if next1
st
Tag == ?object.next1
st
Tag? then tag = ?correctTag?
#14: if prev1
st
Tag == ?object.prev1
st
Tag? then tag = ?correctTag?
#18: if word == ?object.word? && next1
st
Tag == ?object.next1
st
Tag? then tag = ?correctTag?
Figure 2: Rule template examples.
Figure 3: The diagram of the learning process of the learner.
The Initialized corpus is returned by perform-
ing the Initial-tagger on the Raw corpus. By com-
paring the initialized one with the Golden corpus,
an Object-driven dictionary of pairs (Object, cor-
rectTag) is produced in which Object captures the
5-word window context covering the current word
and its tag in following format (previous 2
nd
word
/ previous 2
nd
tag, previous 1
st
word / previous
1
st
tag, word / currentTag, next 1
st
word / next 1
st
tag, next 2
nd
word / next 2
nd
tag) from the initial-
ized corpus, and the correctTag is the correspond-
ing tag of the current word in the golden corpus.
There are 27 Rule templates applied for Rule se-
lector which is to select the most suitable rules
to build the SCRDR tree. Examples of the rule
templates are shown in figure 2 where elements
in bold will be replaced by concrete values from
Objects in the object-driven dictionary to create
concrete rules. The SCRDR tree of rules is initial-
ized by building the default rule and all exception
rules of the default one in form of if currentTag =
?TAG? then tag = ?TAG? at the layer-1 exception
structure, for example rules (1) and (2) in the fig-
ure 1, and the like. The learning approach to con-
struct new exception rules to the tree is as follows:
? At a node-F in the SCRDR tree, let SO be
the set of Objects from the object-driven dictio-
nary, which those Objects are fired at the node-F
but their initialized tags are incorrect (the current-
Tag is not the correctTag associated). It means that
node-F gives wrong conclusions to all Objects in
the SO set.
? In order to select a new exception rule of the
rule at node-F from all concrete rules which are
generated for all Objects in the SO set, the se-
lected rule have to satisfy constraints: (i) The rule
must be unsatisfied by cases for which node-F has
already given correct conclusions. This constraint
does not apply to node-F at layer-1 exception struc-
ture. (ii) The rule must associate to a highest score
value of subtracting B from A in comparison to
other ones, where A and B are the numbers of the
SO?s Objects which are correctly and incorrectly
concluded by the rule respectively. (iii) And the
highest value is not smaller than a given threshold.
The SCRDR-learner applies two threshold pa-
rameters: first threshold is to choose exception
rules at the layer-2 exception structure (e.g rules
(3), (4) and (5) in figure 1), and second threshold
is to select rules for higher exception layers.
? The process to add new exception rules is re-
peated until there is no rule satisfying the con-
straints above. At each iteration, a new rule is
added to the current SCRDR tree to correct error
conclusions made by the tree.
3.4 The SCRDR-tagger
The SCRDR-tagger component is to perform the
POS tagging on a raw text corpus where each line
is a sequence of words separated by white space
characters. The component labels the text corpus
by using the Initial-tagger. It slides due to a left-
to-right direction on a 5-word window context to
generate a corresponding Object for each initially
tagged word. The Object is then classified by the
learned SCRDR tree model to produce final con-
clusion tag of the word as illustrated in the exam-
ple in the section 2.
4 Evaluation
The RDRPOSTagger has already been success-
fully applied to English and Vietnamese corpora.
4.1 Results for English
Experiments for English employed the Penn WSJ
Treebank corpus to exploit the WSJ sections 0-18
(38219 sentences) for training, the WSJ sections
19-21 (5527 sentences) for validation and the WSJ
sections 22-24 (5462 sentences) for test.
Using a lexicon created in the use of the train-
19
ing set, the Initial-tagger obtains an accuracy of
93.51% on the test set. By varying the thresholds
on the validation set, we have found the most suit-
able values
4
of 3 and 2 to be used for evaluating
the RDRPOSTagger on the test set. Those thresh-
olds return a SCRDR tree model of 2319 rules
in a 4-level exception structure. The training time
and tagging speed for those thresholds are men-
tioned in the introduction section. On the same test
set, the RDRPOSTagger achieves a performance at
96.49% against 96.46% accounted for the state-of-
the-art POS tagger TnT (Brants, 2000).
For another experiment, only in training pro-
cess: 1-time occurrence words in training set are
initially tagged as out-of-dictionary words. With
a learned tree model of 2418 rules, the tagger
reaches an accuracy of 96.51% on the test set.
Retraining the tagger utilizing another initial
tagger
5
developed in the Brill?s tagger (Brill,
1995) instead of the lexicon-based initial one,
the RDRPOSTagger gains an accuracy result of
96.57% which is slightly higher than the perfor-
mance at 96.53% of the Brill?s.
4.2 Results for Vietnamese
In the first Evaluation Campaign
6
on Vietnamese
Language Processing, the POS tagging track pro-
vided a golden training corpus of 28k sentences
(631k words) collected from two sources of the
national VLSP project and the Vietnam Lexicog-
raphy Center, and a raw test corpus of 2100 sen-
tences (66k words). The training process returned
a SCRDR tree of 2896 rules
7
. Obtaining a highest
performance on the test set, the RDRPOSTagger
surpassed all other participating systems.
We also carry out POS tagging experiments on
the golden corpus of 28k sentences and on the
Vietnamese Treebank of 10k sentences (Nguyen
et al., 2009) according to 5-fold cross-validation
scheme
8
. The average accuracy results are pre-
sented in the table 1. Achieving an accuracy of
92.59% on the Vietnamese Treebank, the RDR-
4
The thresholds 3 and 2 are reused for all other experi-
ments in English and Vietnamese.
5
The initial tagger gets a result of 93.58% on the test set.
6
http://uet.vnu.edu.vn/rivf2013/campaign.html
7
It took 100 minutes to construct the tree leading to tag-
ging speeds of 1100 words/second and 45k words/second for
the implementations in Python and Java, respectively, on the
computer of Core 2Duo 2.4GHz & 3GB of memory.
8
In each cross-validation run, one fold is selected as test
set, 4 remaining folds are merged as training set. The initial
tagger exploits a lexicon generated from the training set. In
training process, 1-time occurrence words are initially labeled
as out-of-lexicon words.
Table 1: Accuracy results for Vietnamese
Corpus Initial-tagger RDRPOSTagger
28k 91.18% 93.42%
10k 90.59% 92.59%
POSTagger outperforms previous Maximum En-
tropy Model, Conditional Random Field and Sup-
port Vector Machine-based POS tagging systems
(Tran et al., 2009) on the same evaluation scheme.
5 Demonstration and Conclusion
In addition to English and Vietnamese, in the
demonstration session, we will present promising
experimental results and run the RDRPOSTagger
for other languages including Bulgarian, Czech,
Danish, Dutch, French, German, Hindi, Italian,
Lao, Portuguese, Spanish, Swedish and Thai. We
will also let the audiences to contribute their own
data sets for retraining and testing the tagger.
In this paper, we describe the rule-based
POS tagging toolkit RDRPOSTagger to auto-
matically construct transformation rules in form
of the SCRDR exception structure. We be-
lieve that our robust, easy-to-use and language-
independent toolkit RDRPOSTagger can be useful
for NLP/CL-related tasks.
References
Thorsten Brants. 2000. TnT: a statistical part-of-
speech tagger. In Proc. of 6th Applied Natural Lan-
guage Processing Conference, pages 224?231.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. Comput. Linguist.,
21(4):543?565.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the penn treebank. Comput.
Linguist., 19(2):313?330.
Phuong Thai Nguyen, Xuan Luong Vu, Thi
Minh Huyen Nguyen, Van Hiep Nguyen, and
Hong Phuong Le. 2009. Building a Large
Syntactically-Annotated Corpus of Vietnamese. In
Proc. of LAW III workshop, pages 182?185.
Dat Quoc Nguyen, Dai Quoc Nguyen, Son Bao Pham,
and Dang Duc Pham. 2011. Ripple Down Rules for
Part-of-Speech Tagging. In Proc. of 12th CICLing -
Volume Part I, pages 190?201.
Debbie Richards. 2009. Two decades of ripple down
rules research. Knowledge Engineering Review,
24(2):159?184.
Oanh Thi Tran, Cuong Anh Le, Thuy Quang Ha, and
Quynh Hoang Le. 2009. An experimental study
on vietnamese pos tagging. Proc. of the 2009 Inter-
national Conference on Asian Language Processing,
pages 23?27.
20
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 128?135,
Baltimore, Maryland, USA. June 27, 2014.
c
?2014 Association for Computational Linguistics
Sentiment Classification on Polarity Reviews:
An Empirical Study Using Rating-based Features
Dai Quoc Nguyen
?
and Dat Quoc Nguyen
?
and Thanh Vu
?
and
Son Bao Pham
?
?
Faculty of Information Technology
University of Engineering and Technology
Vietnam National University, Hanoi
{dainq, datnq, sonpb}@vnu.edu.vn
?
Computing and Communications Department
The Open University, Milton Keynes, UK
thanh.vu@open.ac.uk
Abstract
We present a new feature type named
rating-based feature and evaluate the
contribution of this feature to the task
of document-level sentiment analy-
sis. We achieve state-of-the-art re-
sults on two publicly available stan-
dard polarity movie datasets: on the
dataset consisting of 2000 reviews pro-
duced by Pang and Lee (2004) we ob-
tain an accuracy of 91.6% while it
is 89.87% evaluated on the dataset of
50000 reviews created by Maas et al.
(2011). We also get a performance
at 93.24% on our own dataset consist-
ing of 233600 movie reviews, and we
aim to share this dataset for further re-
search in sentiment polarity analysis
task.
1 Introduction
This paper focuses on document-level sen-
timent classification on polarity reviews.
Specifically, the document-level sentiment
analysis is to identify either a positive or
negative opinion in a given opinionated re-
view (Pang and Lee, 2008; Liu, 2010). In
early work, Turney (2002) proposed an un-
supervised learning algorithm to classify re-
views by calculating the mutual information
between a given phrase and reference words
?excellent? and ?poor?. Pang et al. (2002)
applied supervised learners of Naive Bayes,
Maximum Entropy, and Support Vector Ma-
chine (SVM) to determine sentiment polarity
over movie reviews. Pang and Lee (2004)
presented a minimum cut-based approach to
detect whether each review? sentence is more
likely subjective or not. Then the sentiment of
the whole document review is determined by
employing a machine learning method on the
document?s most-subjective sentences.
Recently, most sentiment polarity clas-
sification systems (Whitelaw et al., 2005;
Kennedy and Inkpen, 2006; Martineau and
Finin, 2009; Maas et al., 2011; Tu et al., 2012;
Wang and Manning, 2012; Nguyen et al.,
2013) have obtained state-of-the-art results by
employing machine learning techniques using
combination of various features such as N-
grams, syntactic and semantic representations
as well as exploiting lexicon resources (Wil-
son et al., 2005; Ng et al., 2006; Baccianella
et al., 2010; Taboada et al., 2011).
In this paper, we firstly introduce a novel
rating-based feature for the sentiment polarity
classification task. Our rating-based feature
can be seen by that the scores ? which users
employ to rate entities on review websites ?
could bring useful information for improving
the performance of classifying polarity senti-
ment. For a review with no associated score,
we could predict a score for the review in the
use of a regression model learned from an ex-
ternal independent dataset of reviews and their
actual corresponding scores. We refer to the
128
predicted score as the rating-based feature for
learning sentiment categorization.
By combining the rating-based feature with
unigrams, bigrams and trigrams, we then
present the results from sentiment classifica-
tion experiments on the benchmark datasets
published by Pang and Lee (2004) and Maas
et al. (2011).
To sum up, the contributions of our study
are:
? Propose a novel rating-based feature and
describe regression models learned from
the external dataset to predict the feature
value for the reviews in the two experi-
mental datasets.
? Achieve state-of-the-art performances in
the use of the rating-based feature for the
sentiment polarity classification task on
the two datasets.
? Analyze comprehensively the profi-
ciency of the rating-based feature to the
accuracy performance.
? Report additional experimental results on
our own dataset containing 233600 re-
views.
The paper is organized as follows: We pro-
vide some related works and describe our ap-
proach in section 2 and section 3, respectively.
We detail our experiments in section 4. Fi-
nally, section 5 presents concluding remarks.
2 Related Works
Whitelaw et al. (2005) described an approach
using appraisal groups such as ?extremely
boring?, or ?not really very good? for senti-
ment analysis, in which a semi-automatically
constructed lexicon is used to return appraisal
attribute values for related terms. Kennedy
and Inkpen (2006) analyzed the effect of con-
textual valence shifters on sentiment classi-
fication of movie reviews. Martineau and
Finin (2009) weighted bag-of-words in em-
ploying a delta TF-IDF function for train-
ing SVMs to classify the reviews. Maas et
al. (2011) introduced a model to catch sen-
timent information and word meanings. Tu
et al. (2012) proposed an approach utiliz-
ing high-impact parse features for convolution
kernels in document-level sentiment recogni-
tion. Meanwhile, Wang and Manning (2012)
obtained a strong and robust performance
by identifying simple NB and SVM vari-
ants. Dahl et al. (2012) applied the restricted
Boltzmann machine to learn representations
capturing meaningful syntactic and semantic
properties of words. In addition, Nguyen et
al. (2013) constructed a two-stage sentiment
classifier applying reject option, where docu-
ments rejected at the first stage are forwarded
to be classified at the second stage.
3 Our Approach
We apply a supervised machine learning ap-
proach to handle the task of document-level
sentiment polarity classification. For machine
learning experiments, besides the N-gram fea-
tures, we employ a new rating-based feature
for training models.
3.1 Rating-based Feature
Our proposed rating-based feature can be seen
by the fact that, on various review websites,
users? reviews of entities such as products,
services, events and their properties ordinar-
ily associate to scores which the users utilize
to rate the entities: a positive review mostly
corresponds with a high score whereas a neg-
ative one strongly correlates to a low score.
Therefore, the rated score could bring useful
information to enhance the sentiment classifi-
cation performance.
We consider the rated score associated to
each document review as a feature named RbF
for learning classification model, in which
the rating-based feature RbF?s value of each
document review in training and test sets
is estimated based on a regression model
learned from an external independent dataset
of reviews along with their actual associated
scores.
129
3.2 N-gram Features
In most related works, unigrams are consid-
ered as the most basic features, in which each
document is represented as a collection of
unique unigram words where each word is
considered as an individual feature.
In addition, we take into account bigrams
and trigrams since a combination of unigram,
bigram and trigram features (N-grams) could
outperform a baseline performance based on
unigram features as pointed out in (Ng et al.,
2006; Martineau and Finin, 2009; Wang and
Manning, 2012).
We calculate the value of the N-gram fea-
ture i
th
by using term frequency - inverse doc-
ument frequency (tf*idf) weighting scheme for
the document D as follows:
Ngram
iD
= log(1 + tf
iD
) ? log
|{D}|
df
i
where tf
iD
is the occurrence frequency of
the feature i
th
in document D, |{D}| is the
number of documents in the data corpus {D},
and df
i
is the number of documents contain-
ing the feature i
th
. We then normalize N-gram
feature vector of the document D as follows:
????????
?Ngram
D
=
?
??{D}
?
??????
Ngram
?
?
|{D}| ? ?
???????
Ngram
D
?
?
???????
Ngram
D
4 Experimental Results
4.1 Experimental Setup
Benchmark datasets. We conducted exper-
imental evaluations on the polarity dataset
PL04
1
of 2000 movie reviews constructed by
Pang and Lee (2004). The dataset PL04 con-
sists of 1000 positive and 1000 negative doc-
ument reviews in which each review was split
into sentences with lowercase normalization.
In order to compare with other published re-
sults, we evaluate our method according to
10-fold cross-validation scheme on the dataset
PL04.
In addition, we carry out experiments on
a large dataset IMDB11
2
of 50000 movie re-
views produced by Maas et al. (2011). The
large dataset IMDB11 contains a training set
1
http://www.cs.cornell.edu/people/pabo/movie-review-data/
2
http://ai.stanford.edu/?amaas/data/sentiment/
of 25000 labeled reviews and a test set of
25000 labeled reviews, where training and test
sets have 12500 positive reviews and 12500
negative reviews in each.
Machine learning algorithm. We utilize
SVM implementation in LIBSVM
3
(Chang
and Lin, 2011) for learning classification
models in all our experiments on the two
benchmark datasets.
Preprocess. We did not apply stop-word
removal, stemming and lemmatization to the
dataset in any process in our system, because
such stop-words as negation words might in-
dicate sentiment orientation, and as pointed
out by Leopold and Kindermann (2002) stem-
ming and lemmatization processes could be
detrimental to accuracy.
In all experiments on PL04, we kept 30000
most frequent N-grams in the training set for
each cross-validation run over each polarity
class. After removing duplication, on an aver-
age, there are total 39950 N-gram features in-
cluding 10280 unigrams, 20505 bigrams and
9165 trigrams.
On the dataset IMDB11, it was 40000 most
frequent N-grams in each polarity class to be
selected for creating feature set of 53724 N-
grams consisting of 13038 unigrams, 26907
bigrams and 13779 trigrams.
RbF feature extraction procedure. We
aim to create an independent dataset for learn-
ing a regression model to predict the feature
RbF?s value for each document review in ex-
perimental datasets. Since Maas et al. (2011)
also provided 7091 IMDB movie titles
4
, we
used those movie titles to extract all user re-
views that their associated scores
5
are not
equal to either 5 or 6 from the IMDB website.
3
http://www.csie.ntu.edu.tw/?cjlin/libsvm/. Using linear
kernel, default parameter settings.
4
http://www.imdb.com/. It is noted that the 7091 movie
titles are completely different from those that were used to
produce the datasets PL04 and IMDB11.
5
The score scale ranges from 1 to 10. As the reviews cor-
responding to rated scores 5 or 6 are likely to be ambiguous
for expressing positive or negative sentiments, we decide to
ignore those 5-6 score reviews. We also abandon user reviews
having no associated rated scores.
130
Figure 1: The score distribution of SAR14.
Consequently, we created an independent
score-associated review dataset (SAR14)
6
of
233600 movie reviews and their accompany-
ing actual scores. The external dataset SAR14
consists of 167378 user reviews connected to
scores valued from 7 to 10, and 66222 reviews
linked to 1-4 rated ones (as shown in Fig-
ure 1). Using SAR14, we employed Support
Vector Regression algorithm implemented in
SVM
light
package
7
(Joachims, 1999) to learn
the regression model employing unigram fea-
tures. We then applied the learned model
to predict real score values of reviews in the
benchmark datasets, and referred to those val-
ues as the values of the feature RbF.
Although using N-gram features (consist-
ing of unigrams, bigrams and trigrams) may
give better results, we tend to use only uni-
grams for learning the regression model be-
cause of saving the training time on the large
size of SAR14. Furthermore, using unigram
features is good enough as presented in sec-
tion 4.4. To extract the RbF feature?s value
for each PL04?s movie review, the regres-
sion model was trained with 20000 most fre-
6
The SAR14 data set is available to download at
https://sites.google.com/site/nquocdai/resources
7
http://svmlight.joachims.org/. Using with default param-
eter settings.
quent unigrams whilst 35000 most frequent
unigrams were employed to learn regression
model to estimate the RbF feature for each re-
view in the dataset IMDB11.
4.2 Results on PL04
Table 1 shows the accuracy results of our
method in comparison with other state-of-the-
art SVM-based performances on the dataset
PL04. Our method achieves a baseline accu-
racy of 87.6% which is higher than baselines
obtained by all other compared approaches.
The accuracy based on only RbF feature is
88.2% being higher than those published in
(Pang and Lee, 2004; Martineau and Finin,
2009; Nguyen et al., 2013). By exploiting
a combination of unigram and RbF features,
we gain a result at 89.8% which is compara-
ble with the highest performances reached by
(Whitelaw et al., 2005; Ng et al., 2006; Wang
and Manning, 2012). It is evident that rising
from 87.6% to 89.8% proves the effectiveness
of using RbF in sentiment polarity classifica-
tion.
Turning to the use of N-grams, we attain
an accuracy of 89.25% which is 1.65% higher
than the baseline result of 87.6%. This shows
the usefulness of adding bigram and trigram
131
Features PL04 IMDB11
Unigrams (baseline) 87.60 83.69
N-grams 89.25 88.67
RbF 88.20 89.14
Unigrams + RbF 89.80 84.71
N-grams + RbF 91.60 89.87
Pang and Lee (2004) 87.20 ??
Whitelaw et al. (2005) 90.20 ??
Ng et al. (2006) 90.50 ??
Martineau and Finin (2009) 88.10 ??
Maas et al. (2011) 88.90 88.89
Tu et al. (2012) 88.50 ??
Dahl et al. (2012) ?? 89.23
Wang and Manning (2012) 89.45 91.22
Nguyen et al. (2013) 87.95 ??
Table 1: Accuracy results (in %).
features to improve the accuracy. With 91.6%,
we reach a new state-of-the-art performance
by combining N-gram and RbF features. We
also note that our state-of-the-art accuracy is
1.1% impressively higher than the highest ac-
curacy published by Ng et al. (2006).
4.3 Results on IMDB11
Table 1 also shows the performance results
of our approach on the dataset IMDB11. Al-
though our method gets a baseline accuracy of
83.69% which is lower than other baseline re-
sults of 88.23% and 88.29% reported by Maas
et al. (2011) and Wang and Manning (2012)
respectively, we achieve a noticeable accuracy
of 89.14% based on only RbF feature.
Furthermore, starting at the result of
88.67% with N-gram features, we obtain a
significant increase to 89.87% by employing
N-gram and RbF features. Particularly, we do
better than the performance at 89.23% pub-
lished by Dahl et al. (2012) with a 0.64% im-
provement in accuracy on 160 test cases.
From our experimental results in section
4.2 and 4.3, we conclude that there are signif-
icant gains in performance results by adding
bigrams and trigrams as well as RbF fea-
ture for sentiment polarity classification. Our
method combining N-grams and RbF fea-
ture outperforms most other published results
on the two benchmark datasets PL04 and
IMDB11.
4.4 Effects of RbF to Accuracy
This section is to give a detail analysis about
the effects of using RbF feature to accuracy
results of our approach (as shown in Figure
2) using full combination of N-gram and RbF
features in which the RbF feature is predicted
by regression models learned on the dataset
SAR14 in varying number K of most frequent
unigrams from 5000 to 40000.
On the dataset PL04, the highest accuracy
obtained by using only the RbF feature is
88.90% at K?s value of 10000, which it is
equal to that published by Maas et al. (2011).
In most cases of using N-gram and RbF fea-
tures, we obtain state-of-the-art results which
are higher than 91%.
On the IMDB11 dataset, at K?s value of
5000, we achieve the lowest accuracy of
89.29% by using N-gram and RbF features,
which it is slightly higher than the accuracy of
89.23% given by Dahl et al. (2012). In cases
that K?s value is higher than 10000, accura-
cies using only RbF feature are around 89.1%,
while using the full combination returns re-
sults which are higher than 89.74%.
132
Figure 2: Effects of rating-based feature to our method?s performance. The horizontal presents
the number of unigram features selected for learning regression models.
4.5 Results on SAR14
As mentioned in section 4.1, our dataset
SAR14 contains 233600 movie reviews. We
label a review as ?positive? or ?negative? if
the review has a score ? 7 or ? 4 respec-
tively. Therefore, we create a very large
dataset of 167378 positive reviews and 66222
negative reviews. Due to the large size of the
dataset SAR14 and the training and classifi-
cation time, we employed LIBLINEAR
8
(Fan
et al., 2008) for this experiment under 10 fold
cross validation scheme. We kept 50000 N-
8
Using L2-regularized logistic regression and
setting tolerance of termination criterion to 0.01.
http://www.csie.ntu.edu.tw/?cjlin/liblinear/
grams over each polarity class in the training
set for each cross-validation run. Finally, we
obtained an accuracy of 93.24% by using N-
gram features.
5 Conclusion
In this paper, we conducted an experimen-
tal study on sentiment polarity classification.
We firstly described our new rating-based fea-
ture, in which the rating-based feature is es-
timated based on a regression model learned
from our external independent dataset SAR14
of 233600 movie reviews. We then exam-
ined the contribution of the rating-based fea-
ture and N-grams in a machine learning-based
133
approach on two datasets PL04 and IMDB11.
Specifically, we reach state-of-the-art accu-
racies at 91.6% and 89.87% on the dataset
PL04 and IMDB11 respectively. Further-
more, by analyzing the effects of rating-based
feature to accuracy performance, we show
that the rating-based feature is very efficient to
sentiment classification on polarity reviews.
And adding bigram and trigram features also
enhances accuracy performance. Further-
more, we get an accuracy of 93.24% on the
dataset SAR14, and we also share this dataset
for further research in sentiment polarity anal-
ysis task.
Acknowledgment
This work is partially supported by the Re-
search Grant from Vietnam National Univer-
sity, Hanoi No. QG.14.04.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), pages 2200?2204.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
George Dahl, Hugo Larochelle, and Ryan P. Adams.
2012. Training restricted boltzmann machines on
word observations. In Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML-
12), pages 679?686.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Sch?olkopf, Christopher J. C. Burges, and Alexan-
der J. Smola, editors, Advances in Kernel Methods:
Support Vector Machines, pages 169?184.
Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment Classification of Movie Reviews Using Con-
textual Valence Shifters. Computational Intelli-
gence, 22(2):110?125.
Edda Leopold and J?org Kindermann. 2002. Text cat-
egorization with support vector machines. how to
represent texts in input space? Mach. Learn., 46(1-
3):423?444.
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Handbook of Natural Language Processing, Sec-
ond Edition, pages 1?38.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, Vol 1, pages 142?150.
Justin Martineau and Tim Finin. 2009. Delta tfidf: an
improved feature space for sentiment analysis. In
Proceedings of the Third Annual Conference on We-
blogs and Social Media, pages 258?261.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the role of linguistic knowledge
sources in the automatic identification and classi-
fication of reviews. In Proceedings of the COL-
ING/ACL onMain conference poster sessions, pages
611?618.
Dai Quoc Nguyen, Dat Quoc Nguyen, and Son Bao
Pham. 2013. A Two-Stage Classifier for Sentiment
Analysis. In Proceedings of the 6th International
Joint Conference on Natural Language Processing,
pages 897?901.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), pages 271?278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing - Volume 10, pages 79?86.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Comput. Lin-
guist., 37(2):267?307, June.
Zhaopeng Tu, Yifan He, Jennifer Foster, Josef van Gen-
abith, Qun Liu, and Shouxun Lin. 2012. Identify-
ing high-impact sub-structures for convolution ker-
nels in document-level sentiment classification. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), ACL ?12, pages 338?343.
134
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, ACL ?02, pages 417?424.
Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), ACL ?12, pages
90?94.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In Proceedings of the 14th ACM inter-
national conference on Information and knowledge
management, CIKM ?05, pages 625?631.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354.
135
