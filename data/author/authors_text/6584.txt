What?s the Trouble: Automatically Identifying Problematic Dialogues in
DARPA Communicator Dialogue Systems
Helen Wright Hastie, Rashmi Prasad, Marilyn Walker
AT& T Labs - Research
180 Park Ave, Florham Park, N.J. 07932, U.S.A.
hhastie,rjprasad,walker@research.att.com
Abstract
Spoken dialogue systems promise effi-
cient and natural access to information
services from any phone. Recently, spo-
ken dialogue systems for widely used ap-
plications such as email, travel informa-
tion, and customer care have moved from
research labs into commercial use. These
applications can receive millions of calls
a month. This huge amount of spoken
dialogue data has led to a need for fully
automatic methods for selecting a subset
of caller dialogues that are most likely
to be useful for further system improve-
ment, to be stored, transcribed and further
analyzed. This paper reports results on
automatically training a Problematic Di-
alogue Identifier to classify problematic
human-computer dialogues using a corpus
of 1242 DARPA Communicator dialogues
in the travel planning domain. We show
that using fully automatic features we can
identify classes of problematic dialogues
with accuracies from 67% to 89%.
1 Introduction
Spoken dialogue systems promise efficient and nat-
ural access to a large variety of information services
from any phone. Deployed systems and research
prototypes exist for applications such as personal
email and calendars, travel and restaurant informa-
tion, personal banking, and customer care. Within
the last few years, several spoken dialogue systems
for widely used applications have moved from re-
search labs into commercial use (Baggia et al, 1998;
Gorin et al, 1997). These applications can receive
millions of calls a month. There is a strong require-
ment for automatic methods to identify and extract
dialogues that provide training data for further sys-
tem development.
As a spoken dialogue system is developed, it is
first tested as a prototype, then fielded in a limited
setting, possibly running with human supervision
(Gorin et al, 1997), and finally deployed. At each
stage from research prototype to deployed commer-
cial application, the system is constantly undergoing
further development. When a system is prototyped
in house or first tested in the field, human subjects
are often paid to use the system and give detailed
feedback on task completion and user satisfaction
(Baggia et al, 1998; Walker et al, 2001). Even
when a system is deployed, it often keeps evolving,
either because customers want to do different things
with it, or because new tasks arise out of develop-
ments in the underlying application. However, real
customers of a deployed system may not be willing
to give detailed feedback.
Thus, the widespread use of these systems has
created a data management and analysis problem.
System designers need to constantly track system
performance, identify problems, and fix them. Sys-
tem modules such as automatic speech recognition
(ASR), natural language understanding (NLU) and
dialogue management may rely on training data col-
lected at each phase. ASR performance assessment
relies on full transcription of the utterances. Dia-
logue manager assessment relies on a human inter-
face expert reading a full transcription of the dia-
logue or listening to a recording of it, possibly while
examining the logfiles to understand the interaction
between all the components. However, because of
the high volume of calls, spoken dialogue service
providers typically can only afford to store, tran-
scribe, and analyze a small fraction of the dialogues.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 384-391.
                         Proceedings of the 40th Annual Meeting of the Association for
Therefore, there is a great need for methods for
both automatically evaluating system performance,
and for extracting subsets of dialogues that provide
good training data for system improvement. This is
a difficult problem because by the time a system is
deployed, typically over 90% of the dialogue inter-
actions result in completed tasks and satisfied users.
Dialogues such as these do not provide very use-
ful training data for further system development be-
cause there is little to be learned when the dialogue
goes well.
Previous research on spoken dialogue evaluation
proposed the application of automatic classifiers for
identifying and predicting of problematic dialogues
(Litman et al, 1999; Walker et al, 2002) for the
purpose of automatically adapting the dialogue man-
ager. Here we apply similar methods to the dialogue
corpus data-mining problem described above. We
report results on automatically training a Problem-
atic Dialogue Identifier (PDI) to classify problem-
atic human-computer dialogues using the October-
2001 DARPA Communicator corpus.
Section 2 describes our approach and the dialogue
corpus. Section 3 describes how we use the DATE
dialogue act tagging scheme to define input features
for the PDI. Section 4 presents a method and results
for automatically predicting task completion. Sec-
tion 5 presents results for predicting problematic di-
alogues based on the user?s satisfaction. We show
that we identify task failure dialogues with 85% ac-
curacy (baseline 59%) and dialogues with low user
satisfaction with up to 89% accuracy. We discuss the
application of the PDI to data mining in Section 6.
Finally, we summarize the paper and discuss future
work.
2 Corpus, Methods and Data
Our experiments apply CLASSIFICATION and RE-
GRESSION trees (CART) (Brieman et al, 1984) to
train a Problematic Dialogue Identifier (PDI) from
a corpus of human-computer dialogues. CLASSI-
FICATION trees are used for categorical response
variables and REGRESSION trees are used for con-
tinuous response variables. CART trees are binary
decision trees. A CLASSIFICATION tree specifies
what queries to perform on the features to maximize
CLASSIFICATION ACCURACY, while REGRESSION
trees derive a set of queries to maximize the COR-
RELATION of the predicted value and the original
value. Like other machine learners, CART takes as
input the allowed values for the response variables;
the names and ranges of values of a fixed set of input
features; and training data specifying the response
variable value and the input feature values for each
example in a training set. Below, we specify how
the PDI was trained, first describing the corpus, then
the response variables, and finally the input features
derived from the corpus.
Corpus: We train and test the PDI on the DARPA
Communicator October-2001 corpus of 1242 dia-
logues. This corpus represents interactions with
real users, with eight different Communicator travel
planning systems, over a period of six months from
April to October of 2001. The dialogue tasks range
from simple domestic round trips to multileg inter-
national trips requiring both car and hotel arrange-
ments. The corpus includes logfiles with logged
events for each system and user turn; hand transcrip-
tions and automatic speech recognizer (ASR) tran-
scription for each user utterance; information de-
rived from a user profile such as user dialect region;
and a User Satisfaction survey and hand-labelled
Task Completion metric for each dialogue. We ran-
domly divide the corpus into 80% training (894 dia-
logues) and 20% testing (248 dialogues).
Defining the Response Variables: In principle,
either low User Satisfaction or failure to complete
the task could be used to define problematic dia-
logues. Therefore, both of these are candidate re-
sponse variables to be examined. The User Satisfac-
tion measure derived from the user survey ranges be-
tween 5 and 25. Task Completion is a ternary mea-
sure where no Task Completion is indicated by 0,
completion of only the airline itinerary is indicated
by 1, and completion of both the airline itinerary and
ground arrangements, such as car and hotel book-
ings, is indicated by 2. We also defined a binary ver-
sion of Task Completion, where Binary Task Com-
pletion=0 when no task or subtask was complete
(equivalent to Task Completion=0), and Binary Task
Completion=1 where all or some of the task was
complete (equivalent to Task Completion=1 or Task
Completion=2).
Figure 1 shows the frequency of dialogues for
varying User Satisfaction for cases where Task
Completion is 0 (solid line) and Task Completion
is greater than 0 (dotted lines). Note that Task Com-
pletion is 1 or 2 for a number of dialogues for which
User Satisfaction is low. Figure 2 illustrates such a
dialogue (system turns are labelled S, user turns as
U, and ASR hypotheses as REC). Here, low User
Satisfaction may be due to the fact that the user had
to repeat herself many times before the system un-
derstood the departure city. An automatic surrogate
for ASR accuracy (such as ASR confidence) would
Figure 1: User Satisfaction for dialogues where Task
Completion is 0, 1 or 2
not be adequate for identifying this problematic di-
alogue, because here either the dialogue manager or
the SLU component is at fault. Another dialogue
subset of interest in Figure 1 is one for which Task
Completion is 0 but User Satisfaction is high. A
common cause for non-Task Completion in these di-
alogues is database access problems. The fact that
the interaction went well until this point is not cap-
tured by the Task Completion metric. Thus we de-
cided to use both User Satisfaction and Task Com-
pletion as response variables in separate experiments
in order to allow the PDI to capture the relationship
between these two variables.
Input Features: Both User Satisfaction and Task
Completion predictors are trained using two types of
automatically extractable features: (1) logfile fea-
tures; and (2) system dialogue act types. The
logfile features include efficiency metrics such as
the number of system and user turns spent on the
task; the total time of the dialogue in milliseconds;
and situational metrics, such as phone type1 which
affects ASR. The system dialogue act types are
extracted from the logfiles using the DATE (Dia-
logue Act Tagging for Evaluation) automatic tag-
1We assume this is automatically derivable by automatic
number identification (ANI).
S13: Sorry, I must have misunderstood. I just heard you
say that the departure city was Minto, Arkansas.
Was the departure city Minneapolis-St-Paul, Minto,
Arkansas or Milan, Italy?
U14: THE DEPARTURE CITY IS MINNEAPOLIS
SAINT PAUL
REC: the departure city in minneapolis saint paul
S15: Here?s a summary of the second flight segment:
I?ve got you leaving from Minto, Arkansas on Fri-
day, October twelfth in the morning and arriving in
newark flying on continental. Is this correct?
U16: NO I WANT TO LEAVE FROM MINNEAPOLIS
REC: no i want to leave from minneapolis
Repeat dialogue segment 3 times
S25: Sorry, I must have misunderstood. I just heard you
say that the departure city was Minto, Arkansas. Was
the departure city Minto, Arkansas or Minneapolis-
St-Paul ?
U26: THE DEPARTURE CITY IS MINNEAPOLIS
SAINT PAUL
REC: the departure city minneapolis by all
S27: Recapping the second flight segment: I have you
leaving out of minneapolis-st-paul on Friday, Octo-
ber twelfth in the morning and arriving in newark
flying on continental. Is this correct?
U28: YES
REC: yes
Figure 2: Sample low user satisfaction dialogue
ging scheme (Walker et al, 2001). The purpose
of these features is to extract numerical correlates
of system dialogue behaviors. This dialogue act la-
belling procedure is detailed in Section 3.
Figure 3 summarizes the types of features used
to train the User Satisfaction predictor. In addition
to the efficiency metrics and the DATE labels, Task
Success can itself be used as a predictor. This can
either be the hand-labelled feature or an approxima-
tion as predicted by the Task Completion Predictor,
described in Section 4. Figure 4 shows the system
design for automatically predicting User Satisfac-
  Efficiency Measures
? Hand-labelled: WERR, SERR
? Automatic: TimeOnTask, TurnsOnTask, Nu-
mOverlaps, MeanUsrTurnDur, MeanWrdsPerUs-
rTurn, MeanSysTurnDur, MeanWrdsPerSysTurn,
DeadAlive, Phone-type, SessionNumber
  Qualitative Measures
? Automatic: DATE Unigrams, e.g. present-
info:flight, acknowledgement:flight booking etc.
? Automatic: DATE Bigrams, e.g. present-
info:flight+acknowledgement:flight booking etc.
  Task Success Features
? Hand-labelled: HL Task Completion
? Automatic: Auto Task Completion
Figure 3: Features used to train the User Satisfaction
Prediction tree
tion with the three types of input features.
DATE
Output 
of
SLS
Completion
Auto Task Completion
CART
Predictor
UserSatisfaction
Task 
Predictor
TAGGER
Automatic
Logfile 
Features
DATE
Rules
Figure 4: Schema for User Satisfaction prediction
3 Extracting DATE Features
The dialogue act labelling of the corpus follows
the DATE tagging scheme (Walker et al, 2001).
In DATE, utterance classification is done along
three cross-cutting orthogonal dimensions. The
CONVERSATIONAL-DOMAIN dimension specifies
the domain of discourse that an utterance is about.
The SPEECH ACT dimension captures distinctions
between communicative goals such as requesting
information (REQUEST-INFO) or presenting infor-
mation (PRESENT-INFO). The TASK-SUBTASK di-
mension specifies which travel reservation subtask
the utterance contributes to. The SPEECH ACT and
CONVERSATIONAL-DOMAIN dimensions are gen-
eral across domains, while the TASK-SUBTASK di-
mension is domain- and sometimes system-specific.
Within the conversational domain dimension,
DATE distinguishes three domains (see Figure 5).
The ABOUT-TASK domain is necessary for evaluat-
ing a dialogue system?s ability to collaborate with
a speaker on achieving the task goal. The ABOUT-
COMMUNICATION domain reflects the system goal
of managing the verbal channel of communication
and providing evidence of what has been under-
stood. All implicit and explicit confirmations are
about communication. The ABOUT-SITUATION-
FRAME domain pertains to the goal of managing the
user?s expectations about how to interact with the
system.
DATE distinguishes 11 speech acts. Examples of
each speech act are shown in Figure 6.
The TASK-SUBTASK dimension distinguishes
among 28 subtasks, some of which can also be
grouped at a level below the top level task. The
TOP-LEVEL-TRIP task describes the task which con-
tains as its subtasks the ORIGIN, DESTINATION,
Conversational Domain Example
ABOUT-TASK And what time didja wanna
leave?
ABOUT-
COMMUNICATION
Leaving from Miami.
ABOUT-SITUATION-
FRAME
You may say repeat, help me
out, start over, or, that?s wrong
Figure 5: Example utterances distinguished within
the Conversational Domain Dimension
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dol-
lars.
OFFER Would you like me to hold this op-
tion?
ACKNOWLEDGMENT I will book this leg.
BACKCHANNEL Okay.
STATUS-REPORT Accessing the database; this
might take a few seconds.
EXPLICIT-
CONFIRM
You will depart on September 1st.
Is that correct?
IMPLICIT-
CONFIRM
Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENING-
CLOSING
Hello. Welcome to the C M U
Communicator.
Figure 6: Example speech act utterances
DATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVAL
and ITINERARY tasks. The GROUND task includes
both the HOTEL and CAR-RENTAL subtasks. The
HOTEL task includes both the HOTEL-NAME and
HOTEL-LOCATION subtasks.2
For the DATE labelling of the corpus, we imple-
mented an extended version of the pattern matcher
that was used for tagging the Communicator June
2000 corpus (Walker et al, 2001). This method
identified and labelled an utterance or utterance se-
quence automatically by reference to a database of
utterance patterns that were hand-labelled with the
DATE tags. Before applying the pattern matcher,
a named-entity labeler was applied to the system
utterances, matching named-entities relevant in the
travel domain, such as city, airport, car, hotel, airline
names etc.. The named-entity labeler was also ap-
plied to the utterance patterns in the pattern database
to allow for generality in the expression of com-
municative goals specified within DATE. For this
named-entity labelling task, we collected vocabulary
lists from the sites, which maintained such lists for
2ABOUT-SITUATION-FRAME utterances are not specific to
any particular task and can be used for any subtask, for example,
system statements that it misunderstood. Such utterances are
given a ?meta? dialogue act status in the task dimension.
developing their system.3 The extension of the pat-
tern matcher for the 2001 corpus labelling was done
because we found that systems had augmented their
inventory of named entities and utterance patterns
from 2000 to 2001, and these were not accounted
for by the 2000 tagger database. For the extension,
we collected a fresh set of vocabulary lists from the
sites and augmented the pattern database with ad-
ditional 800 labelled utterance patterns. We also
implemented a contextual rule-based postprocessor
that takes any remaining unlabelled utterances and
attempts to label them by looking at their surround-
ing DATE labels. More details about the extended
tagger can be found in (Prasad and Walker, 2002).
On the 2001 corpus, we were able to label 98.4 
of the data. A hand evaluation of 10 randomly se-
lected dialogues from each system shows that we
achieved a classification accuracy of 96  at the ut-
terance level.
For User Satisfaction Prediction, we found that
the distribution of DATE acts were better captured
by using the frequency normalized over the total
number of dialogue acts. In addition to these un-
igram proportions, the bigram frequencies of the
DATE dialogue acts were also calculated. In the fol-
lowing two sections, we discuss which DATE labels
are discriminatory for predicting Task Completion
and User Satisfaction.
4 The Task Completion Predictor
In order to automatically predict Task Comple-
tion, we train a CLASSIFICATION tree to catego-
rize dialogues into Task Completion=0, Task Com-
pletion=1 or Task Completion=2. Recall that a
CLASSIFICATION tree attempts to maximize CLAS-
SIFICATION ACCURACY, results for Task Comple-
tion are thus given in terms of percentage of dia-
logues correctly classified. The majority class base-
line is 59.3% (dialogues where Task Completion=1).
The tree was trained on a number of different in-
put features. The most discriminatory ones, how-
ever, were derived from the DATE tagger. We
use the primitive DATE tags in conjunction with a
feature called GroundCheck (GC), a boolean fea-
ture indicating the existence of DATE tags related
to making ground arrangements, specifically re-
quest info:hotel name, request info:hotel location,
offer:hotel and offer:rental.
Table 1 gives the results for Task Completion pre-
diction accuracy using the various types of features.
3The named entities were preclassified into their respective
semantic classes by the sites.
Baseline Auto ALF + ALF +
Logfile GC GC+ DATE
TC 59% 59% 79% 85%
BTC 86% 86% 86% 92%
Table 1: Task Completion (TC) and Binary Task
Completion (BTC) prediction results, using auto-
matic logfile features (ALF), GroundCheck (GC)
and DATE unigram frequencies
The first row is for predicting ternary Task Comple-
tion, and the second for predicting binary Task Com-
pletion. Using automatic logfile features (ALF) is
not effective for the prediction of either types of Task
Completion. However, the use of GroundCheck re-
sults in an accuracy of 79% for the ternary Task
Completion which is significantly above the base-
line (df = 247, t = -6.264, p  .0001). Adding in the
other DATE features yields an accuracy of 85%. For
Binary Task Completion it is only the use of all the
DATE features that yields an improvement over the
baseline of 92%, which is significant (df = 247, t =
5.83, p  .0001).
A diagram of the trained decision tree for ternary
Task Completion is given in Figure 7. At any junc-
tion in the tree, if the query is true then one takes
the path down the right-hand side of the tree, oth-
erwise one takes the left-hand side. The leaf nodes
contain the predicted value. The GroundCheck fea-
ture is at the top of the tree and divides the data
into Task Completion  2 and Task Completion  2.
If GroundCheck  1, then the tree estimates that
Task Completion is 2, which is the best fit for the
data given the input features. If GroundCheck  0
and there is an acknowledgment of a booking, then
probably a flight has been booked, therefore, Task
Completion is predicted to be 1. Interestingly, if
there is no acknowledgment of a booking then Task
Completion  0, unless the system got to the stage of
asking the user for an airline preference and if re-
quest info:top level trip  2. More than one of these
DATE types indicates that there was a problem in the
dialogue and that the information gathering phase
started over from the beginning.
The binary Task Completion decision tree simply
checks if an acknowledgement:flight booking
has occurred. If it has, then Binary Task Com-
pletion=1, otherwise it looks for the DATE act
about situation frame:instruction:meta situation info,
which captures the fact that the system has told
the user what the system can and cannot do, or
has informed the user about the current state of the
task. This must help with Task Completion, as the
tree tells us that if one or more of these acts are
observed then Task Completion=1, otherwise Task
Completion=0.
TC=1
GroundCheck =0
TC=2
request_info:airline <1
request_info:top_level_trip < 2
acknow.: flight_booking< 1
TC=0TC=1
TC=0 TC=1
Figure 7: Classification Tree for predicting Task
Completion (TC)
5 The User Satisfaction Predictor
Feature Log LF + LF +
used features unigram bigram
HL TC 0.587 0.584 0.592
Auto TC 0.438 0.434 0.472
HL BTC 0.608 0.607 0.614
Auto BTC 0.477 0.47 0.484
Table 2: Correlation results using logfile fea-
tures (LF), adding unigram proportions and bigram
counts, for trees tested on either hand-labelled (HL)
or automatically derived Task Completion (TC) and
Binary Task Completion (BTC)
Quantitative Results: Recall that REGRESSION
trees attempt to maximize the CORRELATION of the
predicted value and the original value. Thus, the re-
sults of the User Satisfaction predictor are given in
terms of the correlation between the predicted User
Satisfaction and actual User Satisfaction as calcu-
lated from the user survey. Here, we also provide R 
for comparison with previous studies. Table 2 gives
the correlation results for User Satisfaction for dif-
ferent feature sets. The User Satisfaction predictor
is trained using the hand-labelled Task Completion
feature for a topline result and using the automati-
cally obtained Task Completion (Auto TC) for the
fully automatic results. We also give results using
Binary Task Completion (BTC) as a substitute for
Task Completion. The first column gives results us-
ing features extracted from the logfile; the second
column indicates results using the DATE unigram
proportions and the third column indicates results
when both the DATE unigram and bigram features
are available.
The first row of Table 2 indicates that perfor-
mance across the three feature sets is indistinguish-
able when hand-labelled Task Completion (HL TC)
is used as the Task Completion input feature. A
comparison of Row 1 and Row 2 shows that the
PDI performs significantly worse using only auto-
matic features (z = 3.18). Row 2 also indicates that
the DATE bigrams help performance, although the
difference between R = .438 and R = .472 is not
significant. The third and fourth rows of Table 1
indicate that for predicting User Satisfaction, Bi-
nary Task Completion is as good as or better than
Ternary Task Completion. The highest correlation of
0.614 (   	

 ) uses hand-labelled Binary Task
Completion and the logfile features and DATE uni-
gram proportions and bigram counts. Again, we see
that the Automatic Binary Task Completion (Auto
BTC) performs significantly worse than the hand-
labelled version (z = -3.18). Row 4 includes the best
totally automatic system: using Automatic Binary
Task Completion and DATE unigrams and bigrams
yields a correlation of 0.484 ( 	 ).
Regression Tree Interpretation: It is interest-
ing to examine the trees to see which features are
used for predicting User Satisfaction. A metric
called Feature Usage Frequency indicates which fea-
tures are the most discriminatory in the CART tree.
Specifically, Feature Usage Frequency counts how
often a feature is queried for each data point, nor-
malized so that the sum of Feature Usage Frequency
values for all the features sums to one. The higher a
feature is in the tree, the more times it is queried. To
calculate the Feature Usage Frequency, we grouped
the features into three types: Task Completion, Log-
file features and DATE frequencies. Feature Us-
age Frequency for the logfile features is 37%. Task
Completion occurs only twice in the tree, however,
it makes up 31because it occurs at the top of the
tree. The Feature Usage Frequency for DATE cat-
egory frequency is 32%. We will discuss each of
these three groups of features in turn.
The most used logfile feature is TurnsOnTask
which is the number of turns which are task-
oriented, for example, initial instructions on how
to use the system are not taken as a TurnOnTask.
Shorter dialogues tend to have a higher User Sat-
isfaction. This is reflected in the User Satisfaction
scores in the tree. However, dialogues which are
long (TurnsOnTask  79 ) can be satisfactory (User
Satisfaction = 15.2) as long as the task that is com-
pleted is long, i.e., if ground arrangements are made
in that dialogue (Task Completion=2). If ground ar-
rangements are not made, the User Satisfaction is
lower (11.6). Phone type is another important fea-
ture queried in the tree, so that dialogues conducted
over corded phones have higher satisfaction. This
is likely to be due to better recognition performance
from corded phones.
As mentioned previously, Task Completion is at
the top of the tree and is therefore the most queried
feature. This captures the relationship between Task
Completion and User Satisfaction as illustrated in
Figure 1.
Finally, it is interesting to examine which DATE
tags the tree uses. If there have been more than
three acknowledgments of bookings, then several
legs of a journey have been successfully booked,
therefore User Satisfaction is high. In particular,
User Satisfaction is high if the system has asked
if the user would like a price for their itinerary
which is one of the final dialogue acts a system
does before the task is completed. The DATE act
about comm:apology:meta slu reject is a measure
of the system?s level of misunderstanding. There-
fore, the more of these dialogue act types the lower
User Satisfaction. This part of the tree uses length
in a similar way described earlier, whereby long di-
alogues are only allocated lower User Satisfaction
if they do not involve ground arrangements. Users
do not seem to mind longer dialogues as long as
the system gives a number of implicit confirma-
tions. The dialogue act request info:top level trip
usually occurs at the start of the dialogue and re-
quests the initial travel plan. If there are more than
one of this dialogue act, it indicates that a START-
OVER occurred due to system failure, and this leads
to lower User Satisfaction. A rule containing the
bigram request info:depart day month date+USER
states that if there is more than one occurrence of this
request then User Satisfaction will be lower. USER
is the single category used for user-turns. No auto-
matic method of predicting user speech act is avail-
able yet for this data. A repetition of this DATE
bigram indicates that a misunderstanding occurred
the first time it was requested, or that the task is
multi-leg in which case User Satisfaction is gener-
ally lower.
The tree that uses Binary Task Completion is
identical to the tree described above, apart from
one binary decision which differentiates dialogues
where Task Completion=1 and Task Completion=2.
Instead of making this distinction, it just uses dia-
logue length to indicate the complexity of the task.
In the original tree, long dialogues are not penalized
if they have achieved a complex task (i.e. if Task
Completion=2). The Binary Task Completion tree
has no way of making this distinction and therefore
just penalizes very long dialogues (where TurnsOn-
Task  110). The Feature Usage Frequency for the
Task Completion features is reduced from 31% to
21%, and the Feature Usage Frequency for the log-
file features increases to 47%. We have shown that
this more general tree produces slightly better re-
sults.
6 Results for Identifying Problematic
Dialogues for Data Mining
So far, we have described a PDI that predicts User
Satisfaction as a continuous variable. For data min-
ing, system developers will want to extract dialogues
with predicted User Satisfaction below a particular
threshold. This threshhold could vary during dif-
ferent stages of system development. As the sys-
tem is fine tuned there will be fewer and fewer dia-
logues with low User Satisfaction, therefore in order
to find the interesting dialogues for system develop-
ment one would have to raise the User Satisfaction
threshold. In order to illustrate the potential value
of our PDI, consider an example threshhold of 12
which divides the data into 73.4% good dialogues
where User Satisfaction  12 which is our baseline
result.
Table 3 gives the recall and precision for the PDIs
described above which use hand-labelled Task Com-
pletion and Auto Task Completion. In the data,
26.6% of the dialogues are problematic (User Sat-
isfaction is under 12), whereas the PDI using hand-
labelled Task Completion predicts that 21.8% are
problematic. Of the problematic dialogues, 54.5%
are classified correctly (Recall). Of the dialogues
that it classes as problematic 66.7% are problematic
(Precision). The results for the automatic system
show an improvement in Recall: it identifies more
problematic dialogues correctly (66.7%) but the pre-
cision is lower.
What do these numbers mean in terms of our orig-
inal goal of reducing the number of dialogues that
need to be transcribed to find good cases to use
Task Completion Dialogue Recall Prec.
Hand-labelled Good 90% 84.5%
Hand-labelled Problematic 54.5% 66.7%
Automatic Good 88.5% 81.3%
Automatic Problematic 66.7% 58.0%
Table 3: Precision and Recall for good and prob-
lematic dialogues (where a good dialogue has User
Satisfaction  12) for the PDI using hand-labelled
Task Completion and Auto Task Completion
for system improvement? If one had a budget to
transcribe 20% of the dataset containing 100 dia-
logues, then by randomly extracting 20 dialogues,
one would transcribe 5 problematic dialogues and 15
good dialogues. Using the fully automatic PDI, one
would obtain 12 problematic dialogues and 8 good
dialogues. To look at it another way, to extract 15
problematic dialogues out of 100, 55% of the data
would need transcribing. To obtain 15 problem-
atic dialogues using the fully automatic PDI, only
26% of the data would need transcribing. This is a
massive improvement over randomly choosing dia-
logues.
7 Discussion and Future Developments
This paper presented a Problematic Dialogue Identi-
fier which system developers can use for evaluation
and to extract problematic dialogues from a large
dataset for system development. We describe PDIs
for predicting both Task Completion and User Satis-
faction in the DARPA Communicator October 2001
corpus.
There has been little previous work on recogniz-
ing problematic dialogues. However, a number of
studies have been done on predicting specific errors
in a dialogue, using a variety of automatic and hand-
labelled features, such as ASR confidence and se-
mantic labels (Aberdeen et al, 2001; Hirschberg et
al., 2000; Levow, 1998; Litman et al, 1999). Pre-
vious work on predicting problematic dialogues be-
fore the end of the dialogue (Walker et al, 2002)
achieved accuracies of 87% using hand-labelled fea-
tures (baseline 67%). Our automatic Task Comple-
tion PDI achieves an accuracy of 85%.
Previous work also predicted User Satisfaction
by applying multi-variate linear regression features
with and without DATE features and showed that
DATE improved the model fit from   	
 to
 (Walker et al, 2001). Our best model
has an Training a Dialogue Act Tagger For Human-Human and Human-Computer
Travel Dialogues
Rashmi Prasad and Marilyn Walker
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ 07932, U.S.A.
rjprasad, walker@research.att.com
Abstract
While dialogue acts provide a useful
schema for characterizing dialogue be-
haviors in human-computer and human-
human dialogues, their utility is limited
by the huge effort involved in hand-
labelling dialogues with a dialogue act la-
belling scheme. In this work, we exam-
ine whether it is possible to fully auto-
mate the tagging task with the goal of en-
abling rapid creation of corpora for eval-
uating spoken dialogue systems and com-
paring them to human-human dialogues.
We report results for training and test-
ing an automatic classifier to label the in-
formation provider?s utterances in spoken
human-computer and human-human dia-
logues with DATE (Dialogue Act Tagging
for Evaluation) dialogue act tags. We
train and test the DATE tagger on var-
ious combinations of the DARPA Com-
municator June-2000 and October-2001
human-computer corpora, and the CMU
human-human corpus in the travel plan-
ning domain. Our results show that we
can achieve high accuracies on the human-
computer data, and surprisingly, that the
human-computer data improves accuracy
on the human-human data, when only
small amounts of human-human training
data are available.
1 Introduction
Recent research on dialogue is based on the as-
sumption that dialogue acts provide a useful way
of characterizing dialogue behaviors in both human-
human (HH) and human-computer (HC) dialogue
(Isard and Carletta, 1995; Shriberg et al, 2000; Di
Eugenio et al, 1998; Cattoni et al, 2001). Previous
research has used dialogue act tagging for tasks such
as improving recognition performance (Shriberg et
al., 2000), identifying important parts of a dialogue
(Finke et al, 1998), evaluating and comparing spo-
ken dialogue systems (Walker et al, 2001c; Cattoni
et al, 2001; Hastie et al, 2002), as a constraint on
nominal expression generation (Jordan, 2000), and
for comparing HH to HC dialogues (Doran et al,
2001).
Our work builds directly on the previous applica-
tion of the DATE (Dialogue Act Tagging for Eval-
uation) tagging scheme to the evaluation and com-
parison of DARPA Communicator dialogues. The
hypothesis underlying the use of dialogue act tag-
ging in spoken dialogue evaluation is that a system?s
dialogue behaviors have a strong effect on its usabil-
ity. Because Communicator systems have unique
dialogue strategies, and a unique way of represent-
ing and achieving particular communicative goals,
DATE was developed to consistently label dialogue
behaviors across systems so that the potential util-
ity of dialogue act tagging could be explored. In
previous work, Walker and Passonneau defined the
DATE scheme, and labelled the system utterances in
the June 2000 data collection of 663 dialogues from
nine participating Communicator systems (Walker
et al, 2001c; Walker et al, 2001a). They then
derived dialogue act metrics from the DATE tags
and showed that when these metrics were used in
the PARADISE evaluation framework (Walker et al,
1997) that they improved models of user satisfaction
by an absolute 5   , and that the new metrics could be
used to understand which system?s dialogue strate-
gies were most effective.
     Philadelphia, July 2002, pp. 162-173.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
A major part of evaluation effort using dialogue
act tagging, however, is to actually label the dia-
logues with the dialogue act tags. In previous work
(Walker et al, 2001c), the DATE labelling of the
June-2000 corpus was done using a semi-automatic
method that involved collection of a large number
of utterance patterns from the different sites par-
ticipating in the collection and subsequent hand la-
belling of these patterns. The 100   coverage and
accuracy achieved by the pattern matcher that was
implemented for labelling the system utterances was
crucially at the cost of maintaining a large labelled
pattern database.1 Furthermore, since the collected
patterns were very specific and often exact dupli-
cates of the system utterances in the dialogues, slight
variations in the system utterances over time led to
a reduction in coverage of the pattern matcher. For
example, on the October-2001 collection, the tagger
was able to label only 60   of the data. Accounting
for the unmatched (and thus unlabeled) utterances
turned out again to be a tedious process of manually
augmenting the pattern database with new utterance
patterns.
We examine whether it is possible to fully au-
tomate the tagging task with the goal being to en-
able rapid creation of new spoken dialogue corpora
that can be submitted to evaluation procedures that
rely on tagging schemes like DATE. Our hypothesis,
arising from rough qualitative studies, is that within
the same domain there are large similarities not only
between different HC dialogues (across system as
well as over a period of time) such as the Commu-
nicator dialogues, but also between HC dialogues
and HH dialogues. Figure 1 shows a HC dialogue
and Figure 2 shows a HH dialogue from the CMU-
corpus (U=user, A=human agent, S=computer sys-
tem) in which the information providers? utterances
for information gathering and information presenta-
tion across both dialogues are very similar. Thus,
based on this and other similar observations, we de-
cided to test the extent to which sufficiently general
tagging rules can be learned from HC data to label
other HC data and HH data in similar domains with
high accuracy.
We focus on labelling only the information
providers? side of the dialogues. The primary mo-
tivation for this is to be able to characterize system
dialogue strategies for cross-site evaluation. System
dialogue behaviors have a large impact on users? ex-
1It should be noted, however, that the task of labelling the
utterances in the pattern database was still a considerable sim-
plification over entire dialogues being labelled by human anno-
tators, which is the usual practice in tagging tasks.
S: Leaving from Phoenix, And, what city are you flying to?
U: SEOUL KOREA
S: Flying from Phoenix to Seoul, What date would you like
to fly?
U: WEDNESDAY OCTOBER ELEVENTH
S: Leaving on the 11th of October, What time woodja like to
depart?
U: ANY TIME
S: From Seoul. Are you returning to Phoenix?
U: YES
S: And what date didja wanna return?
U: SUNDAY OCTOBER FIFTEENTH
S: I may have misheard You saying the 5th of October.
Please be aware that the return date must be later than the
departure date. Please tell me what date you would like too
return.
Figure 1: Excerpt of a HC Dialogue from the June-
2000 Corpus
U: hello
A: people?s travel. what city do you want to fly to
U: chicago
A: on what date
U: on the twenty second of may there will be two people
travelling
A: what time do you need to depart
U: as soon as possible after five p.m.
A: the first flight after five p.m. on that date is at five thiry
five p.m. arriving in chicago at six oh six p.m. on u.s. air
U: is that o?hare
A: yes it is
U: okay. i?ll take that
A: on what date do you want to return
U: the following sunday
A: at what time do you need to depart
U: i want to arrive no later than seven in the evening
Figure 2: Excerpt of a HH Dialogue from the CMU-
Corpus
perience and on users? behavior. Furthermore, users
in the HC dialogues rarely took initiative and their
utterances showed very little variation (Doran et al,
2001). In addition, we believe that once the system
side of the dialogues is labelled, it will be much eas-
ier to label the user side of the dialogues.
We report the results of applying a rule-induction
method to train and test DATE taggers on various
combinations of the DARPA Communicator June-
2000 and October-2001 HC corpora, and the CMU
HH corpus in the travel planning domain. The ac-
curacy of a DATE tagger trained and tested on the
June-2000 corpus is 98.5   . On the October-2001
corpus, this tagger achieves an accuracy of only
71.8   , but adding 2000 utterances from the 2001
corpus to the training data improves accuracy on the
rest of the 2001 corpus to 93.8   . The accuracy of
a tagger trained on the HC corpora and tested on
the CMU-corpus is 36.7   (a significant improve-
ment over the baseline of 28   ). A DATE tagger
trained on 305 examples of the HH data achieves
an accuracy of 48.75   , but the addition of the HC
training data improves accuracy to 55.5   (majority
class baseline=28   ). This pair of results demon-
strates quantitatively that the HC data can be used to
improve performance of a tagger for HH data. How-
ever, a larger training corpus of HH data improves
performance to 76.6   accuracy, as estimated by 20-
fold cross-validation on the CMU-corpus.
Section 2 describes the corpora, the DATE dia-
logue act tagging scheme, methods for tagging the
corpora for the experiments, and the features used
to train a DATE dialogue act tagger for DATE la-
belling of the corpora. Section 3 presents our re-
sults. We postpone discussion and comparison with
related work till Section 4.
2 Corpus, Data, Methods
Our experiments apply the rule learning program
RIPPER (Cohen, 1996) to train a DATE dialogue act
tagger for the utterances of the information provider
in HC and HH travel planning dialogues. Like
other automatic classifiers, RIPPER takes as input the
names of a set of classes to be learned, the names
and ranges of values of a fixed set of features, and
training data specifying the class and feature val-
ues for each example in a training set. Its output is
a classication model for predicting the class of fu-
ture examples. In RIPPER, the classification model is
learned using greedy search guided by an informa-
tion gain metric, and is expressed as an ordered set
of if-then rules. Although any of several automatic
classifiers could be used to train an automatic DATE
tagger, RIPPER supports textual features, which are
important for this problem, and outputs if-then rules
that are easy to understand and which make clear
which features are useful to the DATE tagger when
classifying utterances.
To apply RIPPER, the utterances in the corpus
must be encoded in terms of a set of classes (the out-
put classification) and a set of input features that are
used as predictors for the classes. Below we describe
the corpora, the classes derived from the DATE tag-
ging scheme, the methods used for tagging the cor-
pora using the DATE scheme, and the features that
are extracted from the dialogue in which each utter-
ance occurs.
2.1 Travel Planning Corpora
Our experiments utilize both HC and HH dialogues
in the travel planning domain. The DARPA Com-
municator HC dialogue corpus consists of the June-
2000 corpus and the October-2001 corpus. The
June-2000 corpus contains 663 experimental dia-
logues collected during a three week period in June
of 2000 for conversations between human users and
9 different Communicator travel planning systems.
The October-2001 corpus contains 1252 experimen-
tal dialogues collected between April and October of
2001 for conversations between human users and 8
different COMMUNICATOR travel planning systems.
The dialogues were quite complex, ranging between
simple one way trips requiring no ground arrange-
ments to multileg trips to international or domes-
tic destinations that required car and hotel arrange-
ments. The dialogues typically lasted between 2 and
10 minutes. There was a great deal of variation in
the dialogue strategies implemented by the different
systems, both between the sites during each collec-
tion as well as within a single site across the differ-
ent collections, from 2000 to 2001. There were a to-
tal of 22930 system utterances in the June-2000 cor-
pus and a total of 69766 utterances in the October-
2001 corpus. Each dialogue interaction was logged
by each system using a shared logfile standard. We
were primarily interested in three logged features:
(1) the text of each system utterance; (2) what the
recognizer understood for each user utterance; and
(3) the transcription that each site provided for what
the user actually said. We describe below in Section
2.4 how we used these three logfile features to derive
the features used to train the DATE tagger.
The HH dialogue corpus consists of the CMU-
corpus (Eskenazi et al, 1999). Dialogues in the
travel planning domain were collected by the Com-
municator group at Carnegie Mellon University
(CMU), who arranged with the onsite travel agency
People?s Travel to record calls from a number of vol-
unteer subjects who called the human travel agent
to plan intended trips. These calls were then tran-
scribed and the recordings and the transcriptions
were made available to members of the Communi-
cator community. Labellers at our site subsequently
segmented the travel agent side of the conversation
into utterances where each utterance realized a sin-
gle dialogue act. We used this utterance level seg-
mentation to define the unit for tagging in the exper-
iments described below. The CMU-corpus consists
of 38 dialogues with a total of 1062 travel agent ut-
terances.
2.2 Class Assignment
The classes used to train the DATE tagger are
derived directly from the DATE tagging scheme
(Walker et al, 2001c). DATE classifies each ut-
terance along three cross-cutting orthogonal dimen-
sions of utterance classification: (1) a SPEECH ACT
dimension; (2) a CONVERSATIONAL-DOMAIN di-
mension; and (3) a TASK-SUBTASK dimension. The
SPEECH ACT and CONVERSATIONAL-DOMAIN di-
mensions should be general across domains, while
the TASK-SUBTASK dimension involves a task
model that is not only domain specific, but could
vary from system to system because some systems
might make finer-grained subtask distinctions.
The SPEECH ACT dimension captures distinc-
tions between distinct communicative goals such as
requesting information (REQUEST-INFO), present-
ing information (PRESENT-INFO) and making offers
(OFFER) to act on behalf of the caller. The types of
speech acts are specified and illustrated in Figure 3.
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dol-
lars.
OFFER Would you like me to hold this op-
tion?
ACKNOWLEDGMENT I will book this leg.
BACKCHANNEL Okay.
STATUS-REPORT Accessing the database; this might
take a few seconds.
EXPLICIT-
CONFIRM
You will depart on September 1st. Is
that correct?
IMPLICIT-
CONFIRM
Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENING-
CLOSING
Hello. Welcome to the C M U Com-
municator.
Figure 3: Example Speech Acts in DATE
The CONVERSATIONAL-DOMAIN dimension dis-
tinguishes between talk devoted to the task of book-
ing airline reservations (?about-task?) versus talk
devoted to maintaining the verbal channel of com-
munication (?about-communication?) (Allen and
Core, 1997). DATE adds a third domain called
?about-situation-frame?, to distinguish utterances
that provide information about the interactional con-
text, e.g. Try saying a short sentence, or I know
about 500 international destinations.
The TASK-SUBTASK dimension focusses on spec-
ifying which subtask of the travel reservation task
the utterance contributes to. Some examples are
given in Figure 4. This dimension distinguishes
among 28 subtasks, some of which can also be
grouped at a level below the top level task. The
TOP-LEVEL-TRIP task describes the task which con-
tains as its subtasks the ORIGIN, DESTINATION,
DATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVAL
and ITINERARY tasks. The GROUND task includes
both the HOTEL and CAR subtasks. The HOTEL
task includes both the HOTEL-NAME and HOTEL-
LOCATION subtasks.
Some utterances, especially about-situation-
frame utterances such as instructions and apologies
are not specific to any task. For example, apologies
made by the system about a misunderstanding
can be made within any subtask. We give these
utterances a ?meta? value in the task dimension.
Task Example
TOP-LEVEL-
TRIP
What are your travel plans?
ORIGIN And, what city are you leaving from?
DESTINATION And, where are you flying to?
DATE What day would you like to leave?
TIME Departing at what time?.
AIRLINE Did you have an airline preference?
RETRIEVAL Accessing the database; this might take a
few seconds.
ITINERARY The airfare for this trip is 390 dollars.
GROUND Did you need to make any ground arrange-
ments?.
HOTEL Did you need a hotel?.
HOTEL-
NAME
Do you have a preferred hotel chain?.
HOTEL-
LOCATION
Would you like a hotel near downtown or
near the airport?.
CAR Do you need a car in San Jose?
CAR-TYPE What kind of car did you want?
CAR-RENTAL Do you have a preferred rental agency?
Figure 4: Example Subtasks in DATE
It is possible to achieve very specific labelling
of system utterances by applying all three dimen-
sions simultaneously. For example, one set of out-
put classes for the DATE tagger consists of the
combination of all three classes so that an utter-
ance such as I found three ights that match your
request is classified as ABOUT-TASK:PRESENT-
INFO:FLIGHT.2 However, the DATE scheme also
makes it possible to train and test a DATE tag-
ger for just the SPEECH-ACT dimension or just the
TASK dimension. Figure 5 shows utterances from
a June-2000 dialogue fragment that are classified
along each of the three DATE dimensions.
Tagging utterances along the SPEECH ACT dimen-
sion provides the most general tagging. This level of
categorization is task-independent and possibly sit-
uation independent, ie. from HC to HH dialogues.
One set of experiments simply tests performance of
a DATE tagger for the speech-act dimension on the
HC dialogue data. In addition, we also train a DATE
tagger on the HC dialogues using only the speech
2DATE labels that are specified for all the three dimensions
have the dimension values given in three fields separated by ?:?.
The first field contains the value for the Conversational-Domain
Dimension, the second for the Speech-Act Dimension, and the
third for the Task-Subtask Dimension.
act dimension for the purpose of applying it to a test
set of the CMU-corpus of HH dialogues.3
2.3 Preparation of Training and Test Data via
DATE Tagging
The DATE labelling of the June-2000 data was done
with a semi-automatic tagger: an utterance or ut-
terance sequence is identified and labelled automat-
ically by reference to a database of utterance pat-
terns hand-labelled with DATE tags. The collection
and DATE labelling of the utterance patterns was
done in cooperation with site developers. As dis-
cussed above, these patterns for the 2000 data set
were often quite specific, and often involved whole
utterances. However, since the systems use template
based generation and have only a limited number
of ways of saying the same content, relatively few
utterance patterns needed to be hand-labelled when
compared to the actual number of utterances occur-
ring in the corpus. Further abstraction on the pat-
terns was done with a named-entity labeller which
replaces specific tokens of city names, airports, ho-
tels, airlines, dates, times, cars, and car rental com-
panies with their generic type labels. For example,
what time do you want to leave  AIRPORT  on
 DATE-TIME  ? is the typed utterance for what
time do you want to leave Newark International on
Monday?. For the 2000 tagging, the number of ut-
terances in the pattern database was 1700 whereas
the total number of utterances in the 663 dialogues
was 22930. The named-entity labeller was also ap-
plied to the system utterances in the corpus. We
collected vocabulary lists from all the sites for the
named-entity labelling task. In most cases, systems
had preclassified the individual tokens into generic
types.
The tagger implements a simple pattern match-
ing algorithm to do the dialogue act labelling: for
each utterance pattern in the pattern database, the
tagger attempts to find a match in the dialogues;
if the match succeeds, the DATE label of that pat-
tern is assigned to the matching utterance in the dia-
logue. The matching ignores punctuation since sys-
tems vary in the way they record punctuation.4
Certain utterances have different communicative
functions depending on the context in which they
3Tagging utterances along the TASK dimension may pro-
vide a rough notion of discourse segmentation in that utterances
about the same task may be grouped together. Due to lack of
space, however, we do not present results for task tagging.
4Ignoring punctuation does not, however, create an utterance
segmentation problem for the tagger. We assume that the utter-
ances in the pattern database provide the reference points for
utterance boundaries.
occur. For example, phrases like leaving in the
 DATE-TIME  are implicit confirmations when
they constitute an utterance on their own, but are part
of the flight information presentation when they oc-
cur embedded in utterances such as I have one ight
leaving in the  DATE-TIME  . To prevent incor-
rect labelling for such ambiguous cases, the pattern
database is sorted so that sub-patterns are listed later
than the patterns within which they are embedded,
and the pattern matcher is forced to match patterns
in their order of occurrence in the database.
    Systems 
   Lists from 
Named?Entity
Named?Entity
DATE contextual rules
Dialogues with
       Labels
Named Entity
  from Systems
Dialogue Logfiles
   Classification 
   Labeller
Pattern Database
    with DATE 
  
DATE
Labelled Dialogues
       DATE
Pattern Matcher
Figure 6: The DATE Dialogue Act Tagger
While this tagger achieved 100   accuracy for the
2000 data by using many specific patterns, when ap-
plied to the 2001 corpus it was able to label only
60   of the data. On examination of the unlabelled
utterances, we found that many systems had aug-
mented their inventory of named-entity items as well
as system utterances from the 2000 to the 2001 data
collection. As a result, there were many new pat-
terns unaccounted for in the existing named-entity
lists as well as in the pattern database. In an at-
tempt to cover the remaining 40   of the data, we
therefore augmented the named-entity lists by ob-
taining a new set of preclassified vocabulary items
from the sites, and added 800 hand-labelled pat-
terns to the pattern database. For the labelling of
any additional unaccounted-for patterns, we imple-
mented a contextual rule-based postprocessor that
looks at the surrounding dialogue acts of an un-
matched utterance within a turn and attempts to la-
bel it. The contextual rules are intended to capture
rigid system dialogue behaviors that are reflected
in the DATE sequences within a turn.5 For exam-
ple, one very frequently occurring DATE sequence
within system turns is about task:present info:flight,
about task:present info:price, about task:offer:flight. The
rule using this contextual information can be infor-
mally stated as follows: if in a turn, the first two
utterances are labelled as about task:present info:flight
and about task:present info:price, and the third utterance
is unlabelled, assign the third utterance the label
about task:offer:flight. Not all turn-internal DATE se-
quences are used as contextual rules, however, be-
cause many of them are highly ambiguous. For ex-
ample, about communicaton:apology:meta slu reject can be
followed by a system instruction as well as any kind
of request for information (typically) repeated from
the previous system utterance. Figure 6 shows the
current DATE tagging system, augmented with the
DATE rule-based postprocessor.
With the 2000 tagger augmented with the addi-
tional named-entity items, utterance patterns, and
the postprocessor, we were able to label 98.4   of
the (69766) utterances in the 2001 corpus.
We conducted a hand evaluation of 10 dialogues
which we selected randomly from each system. The
evaluation of the total 80 dialogues shows that we
achieved 96   accuracy on the 2001 tagging.
In order to label the HH corpus of 1062 ut-
terances, we started with 10 dialogues (305 utter-
ances) labelled with the CSTAR dialogue act tag-
ging scheme (Finke et al, 1998; Doran et al, 2001).
We automatically converted the labels to DATE, and
then hand-corrected them. We labelled the rest of
the HH data by training a DATE tagger, applying it
to the remainder of the corpus, and hand-correcting
the results.
2.4 Feature Extraction
The corpus is used to construct the machine learning
features as follows. In RIPPER, feature values are
continuous (numeric), set-valued (textual), or sym-
bolic. We encoded each utterance in terms of a set
of 19 features that were either derived from the log-
files, derived from human transcription of the user
utterances, or represent aspects of the dialogue con-
text in which each utterance occurs.
The complete feature set used by the machine
learner is described in Figure 7. The features fall
into three categories: (1) target utterance features
; (2) context features ; and (3) whole dialogue fea-
tures.
5The logfile standard distinguishes system and user turns
within the dialogues.
 target utterance features: utt-string, contains-word-
FLIGHT-or-AIRLINE, contains-word-HOTEL-or-
ROOM, contains-word-RENTAL-or-CAR, contains-
word-CITY-or-AIRPORT, contains-word-DATE-TIME,
pattern-length.
 context features: left-sys-utt-string, right-sys-utt-
string, da-num, position-in-turn, left?dacontext1,
left-da-context2, usr-orig-string, usr-typed-string,
rec-orig-string, rec-typed-string usr-rec-string-identity.
 whole dialogue features: system-name, turn-number.
Figure 7: Features used by the Machine Learner
The target utterance features include the target
utterance string for which the dialogue act is to
be predicted (utt-string), and a set of features
derived from the named-entity labelling about what
semantic types are instantiated in the target string.
For example the feature contains-word-FLIGHT-
or-AIRLINE is represented by a boolean variable
specifying whether the utterance string contains
the words FLIGHT or AIRLINE. Similar features
are contains-word-HOTEL-or-ROOM, contains-
word-RENTAL-or-CAR, contains-word-CITY-or-
AIRPORT, and contains-word-DATE-TIME. The
pattern-length feature encodes the character length
of the target utterance. The motivation for these
features is to represent basic aspects of the target
utterance, e.g. its length, and the lexical items and
semantic types that appear in the utterance.
The context features encode simple aspects
about the context in which the target utterance oc-
curs. Two of these represent the system utter-
ance strings to the left and right of the target ut-
terance (left-sys-utt-string and right-sys-utt-string).
The left-da-context1 and left-da-context2 features
represent the left unigram and bigram dialogue act
context of the target utterance; this goes beyond
the target turn to only the last dialogue act in the
previous system turn. The da-num feature encodes
the number of dialogue acts in the target turn and
the position-in-turn feature encodes the position of
the target utterance in its turn. In addition, the
user?s previous utterance is represented as part of the
context, both in terms of automatically extractable
features like what the automatic speech recognizer
thought the user said (rec-orig-string), and a version
of this on which the named-entity labeller has been
run (rec-typed-string), as well as in terms of hu-
man generated transcriptions of the user?s utterance.
Features based on the transcriptions include the
original human transcription (usr-orig-string) and
the transcription after named-entity tagging (usr-
typed-string). The usr-rec-string-identity feature is a
Training Data Test Data Dim Maj. Cl. Baseline Acc. (SE)
JUNE-2000 4fold Xval JUNE-2000 All 6.45% 98.5% 0.11%
JUNE-2000 OCTOBER-2001 All 9.52   71.82   0.17  
JUNE-2000 & 2000 ex-
amples of October-2001
October-2001 w/out 2000 All 10.18   93.82   0.09  
Table 1: Results for Identifying Three-Way DATE Tags in the October-2001 Communicator Corpus, (Dim
= Dimension of Date used for output classification (Maj. Cl. = Majority Class, Acc = Accuracy, SE =
Standard Error)
boolean feature based on comparing the user?s tran-
scribed utterance with the recognizer?s hypothesis
of what the user said, using simple string-identity.
Some applications of DATE tagging would not use
features derived from human generated transcrip-
tions so the experiments below report accuracy fig-
ures for DATE taggers which ignore these features.
The motivation for the context features is to repre-
sent aspects of the context in which the utterance
occurs in terms of a window of surrounding lexical
items and dialogue acts.
The whole dialogue features are the name of the
site whose system generated the dialogue (system-
name), and the turn number of the target utterance
within the whole dialogue (turn-number). For HH
dialogues the system-name has the value ?human?.
The motivation for including the system-name fea-
ture is to see whether there are any aspects of the di-
alogue act realizations that are specific to particular
systems. The motivation for the turn-number fea-
ture is that particular types of dialogue acts are more
likely to occur in particular phases of the dialogue.
3 Results
Given the corpora and features described above, we
constructed a set of training and test files for use
with the RIPPER engine. Each spoken dialogue ut-
terance by the system or by the human travel agent
in the corpora are represented in terms of the fea-
tures and class values described above. One of the
primary goals in these experiments is to test the abil-
ity of the trained DATE tagger to learn and apply
general rules for dialogue act tagging. In the HC
data, we examine how a DATE tagger trained on
the June-2000 corpus performs on the October-2001
corpus, with and without 2000 labelled examples of
October-2001 training data. For the HH data, we
examine how a DATE tagger trained on the two HC
corpora (June-2000 and October-2001) performs on
the CMU-corpus, with and without 305 utterances of
HH labelled training data. We first report accuracy
results for a DATE tagger trained and tested on the
HC June-2000 and October-2001 corpora and then
report results for the HH CMU-corpus.
Human-Computer Results: Table 1 shows that
the reported accuracies for the HC experiments are
signifcantly better than the baseline in each case and
the differences between the rows are also statisti-
cally significant. The first row shows that the ac-
curacy of a DATE tagger trained and tested using
four-fold cross-validation on the June-2000 data is
98.5   with a standard error of only 0.11   . This
indicates that after training on 75   of the data,
there are few unexpected utterances in the remain-
ing 25   . However, the second row shows that a
DATE tagger trained on the 9 systems represented
in the June-2000 corpus and tested on the (sub-
set) 8 systems represented in the October-2001 cor-
pus only achieves 71.82   accuracy. This roughly
matches our earlier finding in Section 2.3 that dur-
ing the interval from June-2000 to April-2001 when
the 2001 data collection began, many changes had
been made to the Communicator systems and that
the learned rules from the June-2000 data were not
able to generalize as well to the October-2001 cor-
pus.. The third row shows that the overall variation
in the data is still low: when 2000 labelled examples
of the October-2001 data are added to the June-2000
data for training, the accuracy increases to 93.82   .
This suggests that adding a small amount of new la-
belled training data for successive versions of a sys-
tem would support high accuracy DATE tagging for
the new version of the system.
Some of the rules that RIPPER learned from the
HC corpora for predicting the DATE tag for ut-
terances requesting information about the origin
city, e.g. What city are you departing from?,
and requesting information about the destination
city, e.g. Where are you traveling to?, are shown
in Figure 8. The figure shows that all of the
rules for both about task:request info:orig city and small
about task:request info:dest city utilize the utter-
ance string feature. This suggests that single words
in utterances can be regarded as reliable indicators
Training Data Test Data Dim Maj. Cl. Baseline Acc. (SE)
JUNE-2000 4fold Xval JUNE-2000 SPA 31.28   99.1   .09  
JUNE-2000 OCTOBER-2001 SPA 31.28   82.57   0.14  
JUNE-2000 & 2000 ex-
amples of October-2001
October-2001 w/out 2000 SPA 30.88   95.68   0.08  
Table 2: Results for Identifying Speech-Act DATE tags in the October-2001 Communicator Corpus, (Dim
= Dimension of Date used for output classification (SPA = Speech Act, Maj. Cl. = Majority Class, Acc =
Accuracy, SE = Standard Error)
of DATE tags. More interestingly, the words uti-
lized are intuitively plausible for the travel planning
domain. For example, the learned question words
such as which, where and would are significant for
utterances that have request info as their SPEECH-
ACT dimension. The words city, airport, from, des-
tination and departing are significant predictors of
utterances that have orig city and dest city as their
task dimension.
if utt-string contains city  utt-string contains from 
pattern-length  38
or if utt-string contains airport  pattern-length  38
or if utt-string contains city  pattern-length  17  pattern-
length  15
or if utt-string contains from  pattern-length  66  utt-
string contains Where
or if utt-string contains city  utt-string contains say
or if utt-string contains DEPARTING
or if utt-string contains which  utt-string contains From
or if utt-string contains city  system-name=IBM  utt-
string contains departure
or if utt-string contains fly  utt-string contains which 
left-sys-utt-string contains city
or if utt-string contains fly  utt-string contains O
then about task:request info:orig city
if utt-string contains where  utt-string contains must
or if utt-string contains city  pattern-length  35
or if utt-string contains Where
or if utt-string contains destination
or if utt-string contains DESTINATION
or if utt-string contains which  utt-string contains city
or if utt-string contains where
or if utt-string contains WOULD
then about task:request info:dest city
Figure 8: Rules for DATE tags
about task:request info:orig city and
about task:request info:dest city for Training
on the June-2000 Corpus and 2000 Examples of
October-2001 Corpus.
Human-Computer Speech-Act Results: Be-
cause the DATE scheme describes utterances
in terms of SPEECH-ACT, CONVERSATIONAL-
DOMAIN and TASK dimensions, it is also possible to
extract from the composite labels and examine the
DATE tagger performance for the individual dimen-
sions. Here we focus on the SPEECH-ACT dimension
since, as mentioned above, it is more likely to gen-
eralize to HH travel dialogues and to other task do-
mains. Table 2 shows the results for a DATE tagger
trained and tested on only the SPEECH-ACT dimen-
sion. The reported accuracies are signifcantly better
than the baseline in each case and the differences be-
tween the rows are also statistically significant. The
results support our original hypothesis, showing that
the June-2000 SPEECH-ACT DATE tagger general-
izes more readily to the October-2001 corpus, with
an accuracy of 82.57   (Row 2). Furthermore, as
before, even a small amount of training data from
the 2001 corpus makes a significant improvement in
accuracy to 95.68   (Row 3), which is close to the
99.1   accuracy (Row 1) reported for training and
testing on the June-2000 corpus as estimated by 4-
fold cross-validation.
Human-Human Results: In order to examine
whether there is any generalization from labelled
HC data to HH data for the same task, we apply a
DATE tagger trained on only the SPEECH-ACT di-
mension. The first row of Table 3 shows that when
a DATE tagger is trained on only the HC corpus and
tested on the HH corpus that the accuracy is 36.72  
(a significant improvement over the baseline). This
result demonstrates quantitatively that the HC data
can be used to improve performance of a tagger for
HH data.
Now, let us consider a situation where we only
have 305 HH labelled utterances from 10 of the HH
dialogues to train a DATE tagger. Row 2 shows that
we achieve 48.75   accuracy when testing on the re-
mainder of the HH corpus. However if we add the
HC data to the training set, the accuracy improves
significantly to 55.48   (Row 3). Again this result
demonstrates quantitatively that the HC data can im-
prove performance of a tagger for HH data.
Row 4 shows that the utility of the HC corpus de-
creases if larger amounts of HH labelled data are
available; using 95   of the data to train and test-
Training Data Test Data Maj. Cl. Baseline Acc. (SE)
JUNE-2000 & OCTOBER-
2001
CMU-CORPUS 28.07   36.72   2.76  
305 CMU-CORPUS CMU-CORPUS - 305 43.93   48.75   1.82  
JUNE-2000, OCTOBER-
2001 & 305 CMU-CORPUS
CMU-CORPUS - 305 28.04   55.48   1.81  
CMU-CORPUS 20fold Xval CMU-CORPUS 54.14   76.56   1.03  
Table 3: Results for Identifying DATE Speech-Act Tags in the CMU Human-Human Corpus (Maj. Cl. =
Majority Class, Acc. = Accuracy, SE = Standard Error)
ing on 5   with 20-fold cross-validation achieves an
accuracy of 76.56   .
Examination of the errors that the tagger makes
indicates both similarities and differences between
HH and HC dialogues. For example, information is
presented in small installments in the HH dialogues
whereas information presentation utterances in the
HC dialogues tend to be very long. The information
presentation utterances in HH dialogues then appear
to be syntactically similar to the implicit confirma-
tions in the HC data. Finally, some utterance types
that are very frequent in the HC data such as instruc-
tions rarely occur in the HH dialogues.
The rules that are learned for a DATE tagger
trained on the HC corpora and the HH CMU-corpus
for the offer SPEECH-ACT are in Figures 9 and 10.
There are two main conclusions that can be drawn
from these figures about the generalization from HC
to HH corpora in the SPEECH-ACT dimension. First,
in general, a larger number of rules are learned for
the HH data, suggesting that there is greater varia-
tion for the same speech act in HH dialogues. While
this is not surprising, there is also significant over-
lap in the features and values used in the rules. For
example, the utterance string feature utilizes words
such as select, ight, do, okay, ne, these in both rule
sets.
4 Discussion and Future Work
In summary our results show that: (1) It is possi-
ble to assign DATE dialogue act tags to system ut-
terances in HC dialogues from many different sys-
tems for the same domain with high accuracy; (2) A
DATE tagger trained on data from an earlier version
of the system only achieves moderate accuracy on a
later version of the system without a small amount
labelled training data from that later version; (3) La-
belled training data from HC dialogues can improve
the performance of a DATE tagger for HH dialogue
when only a small amount of HH training data is
available.
Previous work has also reported results for di-
alogue act taggers, using similar features to those
we use, with accuracies ranging from 62   to 75  
(Reithinger and Klesen, 1997; Shriberg et al, 2000;
Samuel et al, 1998). Our best accuracy for the HC
data is 98   . The best performance for the HH cor-
pus is 76   accuracy for the cross-validation study
using only HH data. However, accuracies reported
for previous work are not directly comparable to
ours for several reasons. First, some of our results
concern labelling the system side of utterances in
HC dialogues for the purpose of automatic evalua-
tion of system performance. It is much easier to de-
velop a high accuracy tagger for HC dialogue than it
is for HH dialogue.
We also applied the DATE tagger to HH dialogue,
and focused on the travel agent side of the dialogue.
Here the accuracies that we report are more compa-
rable with that of other researchers, but large differ-
ences should nevertheless be expected due to differ-
ences in the types of corpora, dialogue act tagging
schemes, and features used.
We considered the possibility of generating dia-
logue acts automatically in the logfiles. This idea
was attractive because it is possible to easily im-
plement the generation of dialogue acts tags in the
logfiles. Large amounts of human-computer data
would then be available for the human-human la-
belling task or for evaluation efforts. However, this
turned out to be impractical because we found it dif-
ficult to get dialogue designers across the different
participating sites to agree on a labelling standard.
We therefore believe that machine learning meth-
ods for classification such as the one discussed here
might still be necessary to automate the tagging task
for rapid evaluation and labelling efforts.
As part of the ISLE NSF/EU project, the labelled
corpus that we developed for this work will soon be
released by the LDC, and other researchers will then
be able to utilize it to improve upon our results. In
addition, we believe this corpus could be useful as a
training resource for spoken response generation in
dialogue systems. For example, the dialogue act rep-
resentation can be used to provide a broad range of
text-planning inputs for a stochastic sentence plan-
ner in the travel domain (Walker et al, 2001b), or
to represent the systems? dialogue strategies for re-
inforcement learning (Walker, 2000; Scheffler and
Young, 2002). In future work, we hope to demon-
strate that features derived from the labelling of the
system side of the dialogue can also improve per-
formance of a dialogue act tagger for the human ut-
terances in the dialogue, and to conduct additional
analyses demonstrating the utility of this representa-
tion for cross-site evaluation.
5 Acknowledgments
Thanks to John Aberdeen and Christy Doran for
their contribution of a labelled set of 10 HH dia-
logues from the CMU-corpus. The work reported in
this paper was partially funded by DARPA contract
MDA972-99-3-0003.
References
J. Allen and M. Core. 1997. Draft of DAMSL: Dialog
act markup in several layers. Coding scheme devel-
oped by the MultiParty group, 1st Discourse Tagging
Workshop, Univ. of Penn, March 1996.
R. Cattoni, M. Danieli, A. Panizza, V. Sandrini, and
C. Soria. 2001. Building a corpus of annotated dia-
logues: the ADAM experience. In Proc. of the Con-
ference Corpus-Linguistics-2001, Lancaster, U.K.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In 14th Conference of AAAI.
B. Di Eugenio, P. W. Jordan, J. D. Moore, and
R. H. Thomason. 1998. An empirical investigation
of collaborative dialogues. In ACL-COLING98, Proc.
of the 36th ACL Conference.
C. Doran, J. Aberdeen, L. Damianos, and L. Hirschman.
2001. Comparing several aspects of human-computer
and human-human dialogues. In SIGDIAL Workshop
in conjuction with Eurospeech 2001.
M. Eskenazi, A. Rudnicky, K. Gregory, P. Constantinides,
R. Brennan, K. Bennett, and J. Allen. 1999. Data col-
lection and processing in the carnegie mellon commu-
nicator. In Proc. of Eurospeech-99, pages 2695?2698.
M. Finke, M. Lapata, A. Lavie, L. Levin, L. Mayfield
Tomokiyo, T. Polzin, K. Ries, A. Waibel, and K. Zech-
ner. 1998. Clarity: Inferring discourse structure from
speech. In AAAI Symposium on Applying Machine
Learning to Discourse Processing Proceedings, Stan-
ford, California.
H. Hastie, R. Prasad, and M. A. Walker. 2002. Auto-
matic evaluation: Using a date dialogue act tagger for
user satisfaction and task completion prediction. In
LREC 2002.
A. Isard and J. C. Carletta. 1995. Replicability of trans-
action and action coding in the map task corpus. In
M. A. Walker and J. Moore, eds., AAAI Spring Sympo-
sium: Empirical Methods in Discourse Interpretation
and Generation, pages 60?67.
P. W. Jordan. 2000. Intentional Influences on Object Re-
descriptions in Dialogue: Evidence from an Empirical
Study. Ph.D. thesis, Intelligent Systems Program, Uni-
versity of Pittsburgh.
N. Reithinger and M. Klesen. 1997. Dialogue act clas-
sification using language models. In Proc. of Eu-
rospeech ?97, pages 2235?2238, Rhodes, Greece.
K. Samuel, S. Carberry, and K. Vijay-Shanker. 1998. Di-
alogue act tagging with transformation-based learning.
In Proc. of COLING-ACL, pages 1150?1156.
K. Scheffler and S. Young. 2002. Automatic learning of
dialogue strategy using dialogue simulation and rein-
forcement learning. In HLT Conference.
E. Shriberg, P. Taylor, R. Bates, A. Stolcke, K. Ries,
D. Jurafsky, N. Coccaro, R. Martin, M. Meteer, and
C. Van Ess-Dykema. 2000. Can prosody aid the au-
tomatic classification of dialog acts in conversational
speech. Language and Speech: Special Issue on
Prosody and Conversation.
M. A. Walker, D. Litman, C. A. Kamm, and A. Abella.
1997. PARADISE: A general framework for evaluat-
ing spoken dialogue agents. In Proc. of ACL/EACL 97,
pages 271?280.
M. A. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garo-
folo, L. Hirschman, A. Le, S. Lee, S. Narayanan,
K. Papineni, B. Pellom, J. Polifroni, A. Potamianos,
P. Prabhu, A. Rudnicky, G. Sanders, S. Seneff, D. Stal-
lard, and S. Whittaker. 2001a. Darpa communica-
tor dialog travel planning systems: The june 2000 data
collection. In Proc. of EUROSPEECH 2001.
M. A. Walker, O. Rambow, and M. Rogati. 2001b. Spot:
A trainable sentence planner. In Proc. of NAACL.
M. A. Walker, R. Passonneau, and J. E. Boland. 2001c.
Quantitative and qualitative evaluation of DARPA
communicator spoken dialogue systems. In Proc. of
ACL 2001.
M. A. Walker. 2000. An application of reinforcement
learning to dialogue strategy selection in a spoken di-
alogue system for email. Journal of Artificial Intelli-
gence Research, 12:387?416.
Speaker Utterance C-Domain Speech Act Task
SYS Welcome. about-task opening/closing meta
You are logged in as a guest user. about-frame instruction meta
You may say repeat, help me out, start over, or, that?s wrong, about-frame instruction meta
you can also correct and interrupt the system at any time. about-frame instruction meta
What airport woodja like to fly out of? about-task request-info orig-city
USER MIAMI FLORIDA
SYS Leaving from Miami, about-comm implicit-confirm orig-city
And, what city are you flying to? about-task request-info dest-city
USER MINNEAPOLIS MINNESOTA
SYS Flying from Miami to Minneapolis, about-comm implicit-confirm orig-city, dest-
city
What date would you like to fly? about-task request-info depart-date
USER UH MONDAY OCTOBER TWENTY THIRD
SYS Leaving on the 23rd of October, about-comm implicit-conf date
And what time didja wanna leave? about-task request-info time
USER UH LATE MORNING
SYS From Minneapolis. about-comm implicit-conf origin
Are you returning to Miami? about-task request-info trip-type
USER UM
SYS Are you going back to Miami? about-task request-info trip-type
Please say yes or no. about-frame instruct meta
USER NO
SYS Okay, it will just take a moment while I look. about-task status-report retrieval
SYS I found 3 flights from Miami to Minneapolis on October 23rd
. For option one I have a flight with United departing at 10
A M and arriving at 3 P M with one stopover and
about-task present-info itinerary
Ticket price is 390 dollars. about-task present-info price
Please say next option, or, flight details, or, I?ll take it. about-task offer itinerary
USER I?LL TAKE IT
SYS Great! I am adding this flight to your itinerary. about-task acknowledge booking
Okay, Is Minneapolis your final destination? about-task request-info trip-type
USER YES IT IS
SYS Would you like to make any ground arrangements? about-task offer ground
USER NO
SYS Now, would you like to hear a summary of your itinerary? about-task offer itinerary
USER YES PLEASE
SYS Okay, Here?s your itinerary. On October 23rd Northwest
Flight 123 departs Miami at 2 P M, arrives Pittsburgh at 4 30
P M connecting to Northwest Flight 146 that departs Pitts-
burgh at 5 P M and arrives Minneapolis at 7 P M.
about-task present-info itinerary
Figure 5: Dialogue Illustrating the Conversational-Domain, Speech-Act, and Task-Subtask Dimensions of
DATE
if left-context1=SPA-present-info  pattern-length  25  position-in-turn  2  right-sys-utt-string contains none
or if left-context2=SPA-present-info-SPA-present-info  system-name=ATT
or if right-sys-utt-string contains also  left-sys-utt-string contains at
or if utt-string contains select
or if utt-string contains confirm
or if utt-string contains fine
or if right-sys-utt-string contains locations  utt-string contains If
or if left-context2=SPA-implicit-confirm-SPA-instruction  utt-string contains Which
or if utt-string contains Okay  utt-string contains flight
or if left-context2=SPA-explicit-confirm-SPA-acknowledgement  utt-string contains flight
or if utt-string contains these  utt-string contains Are
or if rec-orig-string contains sixteenth  utt-string contains Do
then offer
Figure 9: Rules learned for DATE SPEECH-ACT offer using June-2000 plus 2000 Examples of October-2001
as Training
if left-sys-utt-string contains ?NUMBER?  pattern-length  25  right-sys-utt-string contains none  utt-string contains OK
or if position-in-turn  2  left-sys-utt-string contains dollars  pattern-length  55  contains-word-CITY-or-AIRPORT=false
or if utt-string contains this  pattern-length  37  contains-word-FLIGHT-or-AIRLINE=true
or if left-sys-utt-string contains per  da-num  2
or if right-sys-utt-string contains rate
or if utt-string contains these
or if utt-string contains itinerary  pattern-length  41
or if utt-string contains reservation
or if utt-string contains select
or if utt-string contains book  utt-string contains it
or if utt-string contains whether
or if utt-string contains OK  utt-string contains Is
or if utt-string contains MAKE
or if utt-string contains one  right-sys-utt-string contains none
or if utt-string contains fine
or if utt-string contains Kay
or if right-sys-utt-string contains locations
or if utt-string contains THE  utt-string contains LIKE
or if left-sys-utt-string contains over  utt-string contains flight  utt-string contains would
or if utt-string contains take  utt-string contains Do
or if left-sys-utt-string contains yes  utt-string contains what  utt-string contains flight
then offer
Figure 10: Rules learned for DATE SPEECH-ACT offer using 305 CMU-Corpus Utterances as Training
Towards an Annotated Corpus of Discourse Relations in Hindi 
Rashmi Prasad*, Samar Husain?, Dipti Mishra Sharma? and Aravind Joshi* 
 
Abstract 
We describe our initial efforts towards 
developing a large-scale corpus of Hindi 
texts annotated with discourse relations. 
Adopting the lexically grounded approach 
of the Penn Discourse Treebank (PDTB), 
we present a preliminary analysis of 
discourse connectives in a small corpus. 
We describe how discourse connectives are 
represented in the sentence-level 
dependency annotation in Hindi, and 
discuss how the discourse annotation can 
enrich this level for research and 
applications. The ultimate goal of our work 
is to build a Hindi Discourse Relation Bank 
along the lines of the PDTB. Our work will 
also contribute to the cross-linguistic 
understanding of discourse connectives. 
1 Introduction 
An increasing interest in human language 
technologies such as textual summarization, 
question answering, natural language generation 
has recently led to the development of several 
discourse annotation projects aimed at creating 
large scale resources for natural language 
processing. One of these projects is the Penn 
Discourse Treebank (PDTB Group, 2006),1whose 
goal is to annotate the discourse relations holding 
between eventualities described in a text, for 
example causal and contrastive relations. The 
PDTB is unique in using a lexically grounded 
approach for annotation: discourse relations are 
anchored in lexical items (called ?explicit 
discourse connectives?) whenever they are 
                                                 
* University of Pennsylvania, Philadelphia, PA, USA, 
{rjprasad,joshi}@seas.upenn.edu 
? Language Technologies Research Centre, IIIT, Hyderabad, 
India, samar@research.iiit.ac.in, dipti@iiit.ac.in 
1 http://www.seas.upenn.edu/?pdtb 
 
explicitly realized in the text. For example, in (1), 
the causal relation between ?the federal 
government suspending US savings bonds sales? 
and ?Congress not lifting the ceiling on 
government debt? is expressed with the explicit 
connective ?because?.2 The two arguments of each 
connective are also annotated, and the annotations 
of both connectives and their arguments are 
recorded in terms of their text span offsets.3  
 
(1) The federal government suspended sales of U.S. 
savings bonds because Congress hasn?t lifted the 
ceiling on government debt. 
 
One of the questions that arises is how the 
PDTB style annotation can be carried over to 
languages other than English. It may prove to be a 
challenge cross-linguistically, as the guidelines and 
methodology appropriate for English may not 
apply as well or directly to other languages, 
especially when they differ greatly in syntax and 
morphology. To date, cross-linguistic 
investigations of connectives in this direction have 
been carried out for Chinese (Xue, 2005) and 
Turkish (Deniz and Webber, 2008). This paper 
explores discourse relation annotation in Hindi, a 
language with rich morphology and free word 
order. We describe our study of ?explicit 
connectives? in a small corpus of Hindi texts, 
discussing them from two perspectives. First, we 
consider the type and distribution of Hindi 
connectives, proposing to annotate a wider range 
                                                 
2 The PDTB also annotates implicit discourse relations, but 
only locally, between adjacent sentences. Annotation here 
consists of providing connectives (called ?implicit discourse 
connectives?) to express the inferred relation. Implicit 
connectives are beyond the scope of this paper, but will be 
taken up in future work. 
3 The PDTB also records the senses of the connectives, and 
each connective and its arguments are also marked for their 
attribution. Sense annotation and attribution annotation are not 
discussed in this paper. We will, of course, pursue these 
aspects in our future work concerning the building of a Hindi 
Discourse Relation Bank. 
 
The 6th Workshop on Asian Languae Resources, 2008
73
of connectives than the PDTB. Second, we 
consider how the connectives are represented in 
the Hindi sentence-level dependency annotation, in 
particular discussing how the discourse annotation 
can enrich the sentence-level structures. We also 
briefly discuss issues involved in aligning the 
discourse and sentence-level annotations.  
Section 2 provides a brief description of Hindi 
word order and morphology. In Section 3, we 
present our study of the explicit connectives 
identified in our texts, discussing them in light of 
the PDTB. Section 4 describes how connectives 
are represented in the sentence-level dependency 
annotation in Hindi. Finally, Section 5 concludes 
with a summary and future work. 
2 Brief Overview of Hindi Syntax and 
Morphology 
Hindi is a free word order language with SOV as 
the default order. This can be seen in (2), where 
(2a) shows the constituents in the default order, 
and the remaining examples show some of the 
word order variants of (2a). 
 
(2)  a. malaya       nao         samaIr         kao     iktaba    dI .  
           malay   ERG  sameer    DAT  book   gave 
           ?Malay gave the book to Sameer? (S-IO-DO-V)4 
       b. malaya nao iktaba samaIr kao dI. (S-DO-IO-V) 
       c. samaIr kao malaya nao iktaba dI. (IO-S-DO-V) 
       d. samaIr kao iktaba malaya nao dI. (IO-DO-S-V) 
       e. iktaba malaya nao samaIr kao dI. (DO-S-IO-V) 
        f. iktaba samaIr kao malaya nao dI.  (DO-IO-S-V) 
 
Hindi also has a rich case marking system, 
although case marking is not obligatory. For 
example, in (2), while the subject and indirect 
object are explicitly for the ergative (ERG) and 
dative (DAT) cases, the direct object is unmarked 
for the accusative. 
3 Discourse Connectives in Hindi 
Given the lexically grounded approach adopted for 
discourse annotation, the first question that arises 
is how to identify discourse connectives in Hindi. 
Unlike the case of the English connectives in the 
PDTB, there are no resources that alone or together 
provide an exhaustive list of connectives in the 
                                                 
4 S=Subject; IO=Indirect Object; DO=Direct Object; 
V=Verb; ERG=Ergative; DAT=Dative 
language. We did try to create a list from our own 
knowledge of the language and grammar, and also 
by translating the list of English connectives in the 
PDTB. However, when we started looking at real 
data, this list proved to be incomplete. For 
example, we discovered that the form of the 
complementizer ?ik? also functions as a temporal 
subordinator, as in (3). 
 
(3) [ vah  baalaTI      ko   gaMdo      panaI      sao      ApnaI    caaOklaoT  
      [he   bucket  of  dirty  water  from  his     chocolates 
      inakalanao          hI     vaalaa    qaa]    ik    {]sakI  mammaI       nao  
      taking-out  just doing was]  that  {his   mother ERG 
      ]sao    raok idyaa } 
      him stop did} 
?He was just going to take out the chocolates from 
the dirty water in the bucket when his mother stopped 
him.? 
 
The method of collecting connectives will 
therefore necessarily involve ?discovery during 
annotation?. However, we wanted to get some 
initial ideas about what kinds of connectives were 
likely to occur in real text, and to this end, we 
looked at 9 short stories with approximately 8000 
words. Our goal here is to develop an initial set of 
guidelines for annotation, which will be done on 
the same corpus on which the sentence-level 
dependency annotation is being carried out (see 
Section 4). Table 1 provides the full set of 
connectives we found in our texts, grouped by 
syntactic type. The first four columns give the 
syntactic grouping, the Hindi connective 
expressions, the English gloss, and the English 
equivalent expressions, respectively. The last 
column gives the number of occurrences we found 
of each expression. In the rest of this section, we 
describe the function and distribution of discourse 
connectives in Hindi based on our texts. In the 
discussion, we have noted our points of departure 
from the PDTB where applicable, both with 
respect to the types of relations being annotated as 
well as with respect to terminology. For argument 
naming, we use the PDTB convention: the clause 
with which the connective is syntactically 
associated is called Arg2 and the other clause is 
called Arg1. Two special conventions are followed 
for paired connectives, which we describe below. 
In all Hindi examples in this paper, Arg1 is 
enclosed in square brackets and Arg2 is in braces. 
The 6th Workshop on Asian Languae Resources, 2008
74
Connective Type Hindi Gloss English Num 
Sub. Conj. @yaaoMik 
(@yaaoM)ik..[salaIe 
(Agar|yadI)..tba|tao 
(jaba).. tba|tao 
jaba tk.. tba tk (ko ilae)  
jaOsao hI..(tao)  
[tnaa|eosaa..kI  
taik  
ik 
why-that 
(why)-that..this-for  
(if)..then 
(when)..then  
when till..then till (of for)  
as just..(then)  
so|such..that  
so-that  
that  
 
because  
because 
if..(then)  
when  
until  
as soon as  
so that  
so that  
when
  
2 
3 
15 
50 
2 
5 
12 
1 
5 
Sentential Relatives ijasasao 
jaao 
ijasako karNa 
which-with 
which 
which-of reason 
because of which 
because of which 
because of which 
5 
1 
1 
Subordinator pr 
(-kr|-ko|krko) 
samaya 
hue 
ko baad 
sao 
ko phlao 
ko ilae 
maoM 
ko karNa 
upon 
(do) 
time 
happening 
of later 
with 
of before 
of for 
in 
of reason
 
upon 
after|while 
while 
while 
after 
due to 
before 
in order to 
while 
because of
 
9 
111 
1 
28 
3 
1 
1 
4 
1 
3 
Coord. Conj. laoikna|pr|prntu 
AaOr|tqaa  
yaa  
yaaoM tao..pr  
naa kovala..balaik  
but  
and  
or  
such TOP..but  
not only..but
 
but  
and  
or  
but  
not only..but
 
51 
117 
2 
2 
1 
Adverbial tba  
baad maoM  
ifr  
[saIilae  
nahIM tao  
tBaI tao  
saao  
vahI|yahI nahIM  
then  
later in  
then  
this-for  
not then  
then-only TOP  
so  
that|this-only not  
then  
later  
then  
that is why  
otherwise  
that is why  
so  
not only that  
2 
5 
4 
7 
5 
1 
10 
1 
TOTAL    472 
     Table 1: A Partial List of Discourse Connectives in Hindi. Parentheses are used for optional  
     elements; ?|? is used for alternating elements; TOP = topic marker.
 
3.1 Types of Discourse Connectives 
3.1.1 Subordinating Conjunctions 
Finite adverbial subordinate clauses are 
introduced by independent lexical items called 
?subordinating conjunctions?, such as @yaaoMik 
(?because?), as in (4), and they typically occur as 
right or left attached to the main clause. 
 
(4) [maOM   [sa  saBaI    Qana        kao       rajya      ko   baadSaah  
      [I  this  all   wealth ACC kingdom of  king 
      kao      do      dota],    @yaaoMik       {vahI           samast 
     
 
 
  DAT give would], why-that {he-EMPH all 
      QartI     kI  sampda      ka  svaamaI   hO} 
      earth  of   wealth  of   lord  is} 
?I would give all this wealth to the king, because he 
alone is the lord of this whole world?s wealth.? 
 
As the first group in Table 1 shows, 
subordinating conjunctions in Hindi often come 
paired, with one element in the main clause and 
the other in the subordinate clause (Ex.5). One 
of these elements can also be implicit (Ex.6),  
The 6th Workshop on Asian Languae Resources, 2008
75
and in our texts, this was most often the 
subordinate clause element. 
 
(5)  @yaaoMik       {yah   tumharI  ja,maIna   pr   imalaa     hO},      [sailae 
       because {this  your  land  on  found  has}, this-for 
       [[sa       Qana      pr  tumhara  AiQakar  hO] 
       [this treasure on  your  right  is] 
?Because this was found on your land, you have the 
right to this treasure.? 
 
(6)  []saka  vaSa       calata]     tao    {vah   ]sao   Gar        sao 
       [her   power  walk] then  {she  it   home  from 
       baahr  inakala  dotI}  
       out  take  would} 
?Had it been in her power, she would have banished 
it from the house.? 
 
When both elements of the paired connective are 
explicit, their text spans must be selected 
discontinuously. The main clause argument is 
called Arg1 and the subordinate clause 
argument, Arg2. 
Subordinating conjunctions, whether single or 
paired, can occur in non-initial positions in their 
clause. However, this word order variability is 
not completely unconstrained. First, not all 
conjunctions display this freedom. For example, 
while ?jaba? (?when?) can be clause-medial (Ex. 
7), ?@yaaoMik? (?because?) cannot. Second, when the 
main clause precedes the subordinate clause, the 
main clause element, if explicit, cannot appear 
clause-initially at all. Consider the causal ?@yaaoMik.. 
[salaIe? (Ex.5), which represents the subordinate-
main clause order. In the reverse order, the 
explicit main clause ?[salaIe? (Ex.8) appears 
clause medially. Placing this element in clause-
initial position is not possible. 
 
(7) {lakD,haro         kI  p%naI      kao}    jaba    {yah 
      {woodcutter of  wife DAT} when {this 
       maalaUma            pD,a  ik     [sa  icaiDyaa  ko   karNa,  
       knowledge put  that this bird   of   reason 
       kama     CaoD,kr    Gar       Aa   gayaa     hO}   tao      [vah 
       work leaving home come went is} then  [she 
       ]sa    pr       barsa        pD,I]. 
       him on  anger-rain put] 
 ?When the woodcutter?s wife found out that he had 
left his work and come home to care for the bird, she 
raged at him.? 
 
(8)  [. . . pr  icaraga   kI  ba%tI    ]sakanaa  yaa   daohrI  
       [. . .but lamp of  light  light    or  another 
       ba%tI    lagaanaa]         Saayad [sailae         []icat nahIM  
       light  putting] perhaps this-for  [appropriate not 
       samaJato          qao]  ik     {tola  ka  Apvyaya   haogaa}. 
       Consider did]  that  {oil  of  waste   be-FUT}. 
?. . . but he did not consider it appropriate to light the 
lamp repeatedly or light another lamp, perhaps 
because it would be a waste of oil.? 
3.1.2 Sentential Relative Pronouns 
Since discourse relations are defined as holding 
between eventualities, we have also identified 
relations that are expressed syntactically as 
relative pronouns in sentential relative clauses, 
which modify the main clause verb denoting an 
eventuality, rather than some entity denoting 
noun phrase. For example, in (9), a 
result/purpose relation is conveyed between ?the 
man?s rushing home? and ?the bird being taken 
care of?, and we believe that this relation 
between the eventualities should be captured 
despite it?s syntactic realization as the relative 
pronoun ?ijasasao? (?because of which/so that?). (10) 
gives an example of a modified relative 
pronoun. 
 
(9) [saara  kama     caaoD,kr     vah  ]sa    baImaar   icaiD,yaa 
      [all  work  leaving  he  that  sick  bird 
      kao       ]zakr       dbaa     Gar     kI    Aaor       Baagaa], 
      ACC picking-up fast home of direction ran], 
      ijasasao             {]saka   sahI       [laaja   ikyaa  jaa    sako} 
      from-which {her    proper  care  do   go  able} 
?Leaving all his work, he picked up the bird and ran 
home very fast, so that the bird could be given proper 
care.? 
 
(10) [}M^TaoM       ko   hr     baar    kdma  rKnao       pr 
        [camels of  every time step keeping upon 
        icaiD,yaao M ko  isar     Aapsa          maoM   tqaa   }M^T      kI 
        birds of  head each-other in and camels of 
        gardna   sao    Tkra            rho     qao]   ijasako karNa 
        neck with hit-against be had] of-which reason 
       {]na     pixayaaoM   kI   drdBarI      caIKoM        inakla 
       {those birds  of   painful  screams come-out  
         rhI   qaIM}. 
         be had} 
?With each step of the camels, the birds heads were 
hitting against each other as well as with the camels? 
necks because of which the birds were screaming 
painfully.? 
3.1.3 Subordinators 
In contrast to the subordinating conjunctions, 
elements introducing non-finite subordinate 
clauses are called ?subordinators?. Unlike 
The 6th Workshop on Asian Languae Resources, 2008
76
English, where certain non-finite subordinate 
clauses, called ?free adjuncts?, appear without 
any overt marking so that their relationship with 
the main clause is unspecified, Hindi non-finite 
subordinate clauses almost always appear with 
overt marking. However, also unlike English, 
where the same elements may introduce both 
finite and non-finite clauses (cf. After leaving, 
she caught the bus vs. After she left, she caught 
the bus), different sets of elements are used in 
Hindi. In fact, as can be seen in the subordinator 
group in Table 1, the non-finite clause markers 
are either postpositions (Ex.11), particles 
following verbal participles (Ex.12), or suffixes 
marking serial verbs (Ex.13). 
 
(11) {mammaI          ko     manaa        krnao}      ko karNa     [ramaU 
        {mummy of  warning  doing} of reason [Ramu 
         qaaoD,I  qaaoD,I    caaOklaoT      baD,o     AnaMd        ko   saaqa 
         little little chocolate big  pleasure  of  with  
         Ka  rha      qaa]. 
         eat being be] 
?Because of his mother?s warning, Ramu was eating 
bits of chocolate with a lot of pleasure.? 
 
(12) . . . AaOr    {Kolato}      hue               [yah    BaUla     jaata hO 
        . . . and  {playing} happening [this forget go is 
         ik    yaid  ]saka  ima~        BaI    Apnao  iKlaaOnao    kao 
         that if    his   friends also their  toys     to 
         ]sao     haqa   nahIM   lagaanao         dota,  tao      ]sao 
         him hand not   touching did,  then  he  
           iktnaa        baura     lagata] 
         how-much bad   feel] 
?. . . and while playing, he forgets that if his friends 
too didn?t let him touch their toys, then how bad he 
would feel.? 
 
(13) {ApnaI  p%naI     sao     yah       sauna}kr      [lakD,hara  
        {self  wife from this   listen}-do  [woodcutter 
         bahut    duKI       huAa] 
         much sad  became] 
?Upon hearing this from his wife, the woodcutter 
became very sad.? 
 
While subordinators constitute a frequently-
used way to mark discourse relations, their 
annotation raises at least two difficult problems, 
both of which have implications for the 
reliability of annotation. The first is that these 
markers are used for marking both argument 
clauses and adjunct clauses, so that annotators 
would be required to make difficult decisions for 
distinguishing them: in the former case, the 
marker would not be regarded as a connective, 
while in the latter case, it would. Second, the 
clauses marked by these connectives often seem 
to be semantically weak. This is especially true 
of verbal participles, which are nonfinite verb 
appearing in a modifying relation with another 
finite verb. Whereas in some cases (Ex.12-13) 
the two verbs are perceived as each projecting 
?two distinct events? between which some 
discourse relation can be said to exist, in other 
cases (Ex.14), the two verbs seem to project two 
distinct actions but as part of a ?single complex 
event? (Verma, 1993). These judgments can be 
very subtle, however, and our final decision on 
whether to annotate such constructions will be 
made after some initial annotation and 
evaluation. 
 
(14) {doKto        hI         doKto      saba  baOla             Baagato } 
        {looking EMPH looking all buffalos running} 
         hue              [gaaoSaalaa   phu^Mca     gae] 
         happening [shed   reach  did] 
?Within seconds all the buffalos came running to the 
shed.? 
 
The naming convention for the arguments of 
subordinators is the same as for the 
subordinating conjunctions: the clause 
associated with the subordinator is called Arg2 
while its matrix clause is called Arg1. 
Unlike subordinating conjunctions, 
subordinators do not come paired and they can 
only appear clause-finally. Clause order, while 
not fixed, is restricted in that the nonfinite 
subordinate clause can appear either before the 
main clause or embedded in it, but never after 
the main clause.  
3.1.4 Coordinating Conjunctions 
Coordinating conjunctions in Hindi are found in 
both inter-sentential (Ex.15) and intra-sentential 
(Ex.16) contexts, they always appear as 
independent elements, and they almost always 
appear clause-initially. 5  For these connectives, 
                                                 
5 While the contrastive connectives  ?pr?, ?prntU? appear only 
clause-initially, it seems possible for the contrastive ?laoikna? 
to appear clause-medially, suggesting that these two types 
may correspond to the English ?but? and ?however?, 
respectively. However, we did not find any examples of 
clause-medial ?laoikna? in our texts, and this behavior will 
have to be verified with further annotation. 
The 6th Workshop on Asian Languae Resources, 2008
77
the first clause is called Arg1 and the second, 
Arg2. 
 
(15) [jaba      vah  laaOTta    tao       gaa-gaakr          ]saka  mana 
         [when he  return then sing-singing   his  mind 
         KuSa      kr    dotI].   laoikna  {]sakI p%naI   kao      vah 
         happy do gave].   But   {his wife DAT  the  
         icaiD,yaa   fUTI    AaM^K  nahIM    sauhatI   qaI}. 
         bird    torn  eye  not   bear  did} 
?Upon his return, she would make him happy by 
singing. But his wife could not tolerate the bird even 
a little bit.? 
 
(16) [ tBaI          drvaaja,a      Kulaa]  AaOr    {maalaikna  Aa 
        [then-only door opened]  and  {wife  come 
         ga[- }. 
        went} 
?Just then the door opened and the wife came in.? 
 
We also recognize paired coordinating 
conjunctions, such as ?naa kovala..balaik? (See Table 
1). The argument naming convention for these is 
the same as for the single conjunctions. 
3.1.5 Discourse Adverbials 
Discourse adverbials in Hindi modify their clau- 
ses as independent elements, and some of these 
are free to appear in non-initial positions in the 
clause. Example (17) gives an example of the 
consequence adverb, ?saao?. The Arg2 of discourse 
adverbials is the clause they modify, whereas 
Arg1 is the other argument. 
 
(17) [icaiD,yaa  jabaana      kT   jaanao     AaOr   maalaikna  ko  eosao 
        [bird    tongue  cut  going  and  wife  of  this  
        vyavahar       sao      Dr   ga[-     qaI]. saao    {vah     iksaI 
        behavior with fear go  had]. So  {she  some 
        trh        ]D,kr    calaI        ga[-}. 
        manner flying  walk    went}. 
?The bird was scared due to her tongue being cut and 
because of the wife?s behavior. So she somehow flew 
away.? 
 
As with the PDTB, one of our goals with the 
Hindi discourse annotation is to explore the 
structural distance of Arg1 from the discourse 
adverbial. If the Arg1 clause is found to be snon-
adjacent to the connective and the Arg2 clause, 
it may suggest that adverbials in Hindi behave 
anaphorically. In the texts we looked at, we did 
not find any instances of non-adjacent Arg1s. 
Addtional annotation will provide further 
evidence in this regard. 
4 Hindi Sentence-level Annotation 
andDiscourse Connectives 
The sentence-level annotation task in Hindi is 
an ongoing effort which aims to come up with a 
dependency annotated treebank for the NLP/CL 
community working on Indian languages. 
Presently a million word Hindi corpus is being 
manually annotated (Begum et al, 2008). The 
dependency annotation is being done on top of 
the corpus which has already been marked for 
POS tag and chunk information. The scheme has 
28 tags which capture various dependency 
relations. These relations are largely inspired by 
the Paninian grammatical framework. Given 
below are some relations, reflecting the 
argument structure of the verb. 
 
a) kta- (agent) (k1) 
b) kma- (theme) (k2) 
c) krNa (instrument) (k3) 
d) samp`dana sampradaan (recipient) (k4) 
e) Apadana (source) (k5) 
f) AiQakrNa (location) (k7) 
 
Figure 1 shows how Examples (2a-f) are 
represented in the framework. Note that agent 
and theme are rough translations for ?kta-? and 
?kma-? respectively. Unlike thematic roles, these 
relations are not purely semantic, and are 
motivated not only through verbal semantics but 
also through vibhaktis (postpositions) and TAM 
(Tense, aspect and modality) markers (Bharati et 
al., 1995). The relations are therefore syntactico-
semantic, and unlike thematic roles there is a 
greater binding between these relations and the 
syntactic cues. 
 
 
k1 k4 k2
 
 
 
Figure 1: Dependency Diagram for Example (2) 
Some discourse relations that we have identified 
are already clearly represented in the sentence-
level annotation. But for those that aren?t, the 
   dI 
   malaya    samaIr    iktaba 
The 6th Workshop on Asian Languae Resources, 2008
78
discourse level annotations will enrich the 
sentence-level. In the rest of this section, we 
discuss the representation of the different types 
of connectives at the sentence level, and discuss 
how the discourse annotation will add to the 
information present in the dependency 
structures. 
 
Subordinating Conjunctions Subordinating 
conjunctions are lexically represented in the 
dependency tree, taking the subordinating clause 
as their dependents while themselves attaching 
to the main verb (the root of the tree). Figure 2 
shows the dependency tree for Example (4) 
containing the subordinating conjunction ?@yaaoMik?. 
Note that the edge between the connective and 
the main verb gives us the causal relation 
between the two clauses, the relation label being 
?rh? (relation hetu ?cause?). Thus, the discourse 
level can be taken to be completely represented 
at the sentence-level. 
 
hE
k1 k2 k4 rh
ccofr6
k1sk1
r6
r6 
 
Figure 2: Dependency Tree for Subordinating 
Conjunction in Example (4) 
 
Paired Subordinating Conjunctions Unlike 
Example (4), however, the analysis for the 
paired connective in Example (5), given in 
Figure 3, is insufficient. Despite the lexical 
representation of the connective in the tree, the 
correct interpretation of the paired conjunction 
and the clauses which it relates is only possible 
at the discourse level. In particular, the 
dependencies don?t show that ?@yaaoMik? and ?[salaIe? 
are two parts of the same connective, expressing 
a single relation and taking the same two 
arguments. Thus, the discourse annotation will 
be able to provide the appropriate argument 
structure and semantics for these paired 
connectives.  
 
 
ccof
k2 k1 rh
ccof
k7p
r6
k1
 
 
Figure 3: Dependency Tree for Paired 
Subordinating Conjunction in Example (5) 
 
Subordinators As mentioned earlier, Hindi 
nonfinite subordinate clauses almost always 
appear with overt marking. But unlike the 
subordinating conjunctions, subordinators are 
not lexically represented in the dependency 
trees. Figure 4 gives the dependency 
representation for Example (11) containing a 
postposition subordinator ?ko karNa?, which relates 
the main and subordinate clauses causally. As 
the figure shows, while the causal relation label 
(?rh?) appears on the edge between the main and 
subordinate verbs, the subordinator itself is not 
lexically represented as the mediator of this 
relation. The lexically grounded annotation at 
the discourse level will thus provide the textual 
anchors of such relations, enriching the 
dependency representation. Furthermore, while 
many of the subordinators in Table 1 are fully 
specified in the dependency trees for the 
semantic relation they denote (e.g., ?pr? and ?maoM? 
marked as the ?k7t? (location in time) relation, 
and ?ko karNa? and ?sao? marked as the ?rh? 
(cause/reason) relation), others, like the particle 
?hue? are underspecified for their semantics, being 
marked only as ?vmod? (verbal modifier). The 
discourse-level annotation will thus be the 
source for the semantics of these subordinators. 
 
Coordinating Conjunctions Coordinating 
conjunctions at the sentence level anchor the 
root of the dependency tree. Figure 5 shows the 
 do dotao oo oo o   
   maOMOM O MO M   Qana   baadSaah    @yaaoMikoM oMoM   
rajya  
   vahI   svaamaI  
  sampda  
   QartI  
   [sailae 
    
AiQakar hOO OO 
    Qana  tuuuumhara  @yaaoMikoMoMo M   
  imalaa hOO OO 
    yah 
  
 ja,maIna, ,,  
 
  tumharIuuu  
The 6th Workshop on Asian Languae Resources, 2008
79
dependency representation of Example (16) 
containing a coordinating conjunction. 
 
 
rh k1 k2
vmod
k1
 
 
Figure 4: Dependency Tree for Subordinator in 
Example (11) 
 
 
ccof ccof
k7t k1 k1
 
 
Figure 5: Dependency Tree for Coordinating 
Conjunction in Example (16) 
 
While the sentence-level dependency analysis 
here is similar to the one we get at the discourse 
level, the semantics of these conjunctions are 
again underspecified, being all marked as ?ccof?, 
and can be obtained from the discourse level. 
 
Discourse Adverbials Like subordinating 
conjunctions, discourse adverbials are 
represented lexically in the dependency tree. 
They are attached to the verb of their clause as 
its child node and their denoted semantic 
relation is specified clearly. This can be seen 
with the temporal adverb ?tBaI? (?then-only?) and 
its semantic label ?k7t? in Figure 5. At the same 
time, since the Arg1 discourse argument of 
adverbials is most often in the prior context, the 
discourse annotation will enrich the semantics of 
these connectives by providing the Arg1 
argument. 
5 Summary and Future Work 
In this paper, we have described our study of 
discourse connectives in a small corpus of Hindi 
texts in an effort towards developing an 
annotated corpus of discourse relations in Hindi. 
Adopting the lexically grounded approach of the 
Penn Discourse Treebank, we have identified a 
wide range of connectives, analyzing their types 
and distributions, and discussing some of the 
issues involved in the annotation. We also 
described the representation of the connectives 
in the sentence-level dependency annotation 
being carried out independently for Hindi, and 
discussed how the discourse annotations can 
enrich the information provided at the sentence 
level. While we focused on explicit connectives 
in this paper, future work will investigate the 
annotation of implicit connectives, the semantic 
classification of connectives, and the attribution 
of connectives and their arguments. 
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. In Proceedings of IJCNLP-2008. 
Hyderabad, India. 
Akshar Bharati, Vineet Chaitanya, and Rajeev 
Sangal. 1995. Natural Language Processing: A 
Paninian Perspective. Prentice Hall of India. 
http://ltrc.iiit.ac.in/downloads/nlpbook/nlppanini.p
df. 
Manindra K. Verma (ed.). 1993. Complex Predicates 
in South Asian Languages. New Delhi: Manohar. 
The PDTB-Group. 2006. The Penn Discourse 
TreeBank 1.0 Annotation Manual. Technical 
Report IRCS-06-01, IRCS, University of 
Pennsylvania. 
Bonnie Webber, Aravind Joshi, Matthew Stone, and 
Alistair Knott. 2003. Anaphora and discourse 
structure. Computational Linguistics, 29(4):545?
587. 
Nianwen Xue. 2005. Annotating Discourse 
Connectives in the Chinese Treebank. In 
Proceedings of the ACL Workshop on Frontiers in 
Corpus Annotation II: Pie in the Sky. Ann Arbor, 
Michigan.  
Deniz Zeyrek and Bonnie Webber. 2008. A 
Discourse Resource for Turkish: Annotating 
Discourse Connectives in the METU Corpus. In 
Proceedings of IJCNLP-2008. Hyderabad, India. 
 
 
  Ka rha qaa 
manaa krnaoo oo rama   caaOO OOklaoTooo    AanaMdMMM  
 mammaI 
  AaOrOOO  
 Kulaauuu   Aa ga[-- -- 
maalaikna  drvaaja,a, ,,   tBaI  
The 6th Workshop on Asian Languae Resources, 2008
80
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Trainable Sentence Planning for Complex Information
Presentation in Spoken Dialog Systems
Amanda Stent
Stony Brook University
Stony Brook, NY 11794
U.S.A.
stent@cs.sunysb.edu
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104
U.S.A.
rjprasad@linc.cis.upenn.edu
Marilyn Walker
University of Sheffield
Sheffield S1 4DP
U.K.
M.A.Walker@sheffield.ac.uk
Abstract
A challenging problem for spoken dialog sys-
tems is the design of utterance generation mod-
ules that are fast, flexible and general, yet pro-
duce high quality output in particular domains.
A promising approach is trainable generation,
which uses general-purpose linguistic knowledge
automatically adapted to the application do-
main. This paper presents a trainable sentence
planner for the MATCH dialog system. We
show that trainable sentence planning can pro-
duce output comparable to that of MATCH?s
template-based generator even for quite com-
plex information presentations.
1 Introduction
One very challenging problem for spoken dialog
systems is the design of the utterance genera-
tion module. This challenge arises partly from
the need for the generator to adapt to many
features of the dialog domain, user population,
and dialog context.
There are three possible approaches to gener-
ating system utterances. The first is template-
based generation, used in most dialog systems
today. Template-based generation enables a
programmer without linguistic training to pro-
gram a generator that can efficiently produce
high quality output specific to different dialog
situations. Its drawbacks include the need to
(1) create templates anew by hand for each ap-
plication; (2) design and maintain a set of tem-
plates that work well together in many dialog
contexts; and (3) repeatedly encode linguistic
constraints such as subject-verb agreement.
The second approach is natural language gen-
eration (NLG), which divides generation into:
(1) text (or content) planning, (2) sentence
planning, and (3) surface realization. NLG
promises portability across domains and dialog
contexts by using general rules for each genera-
tion module. However, the quality of the output
for a particular domain, or a particular dialog
context, may be inferior to that of a template-
based system unless domain-specific rules are
developed or general rules are tuned for the par-
ticular domain. Furthermore, full NLG may be
too slow for use in dialog systems.
A third, more recent, approach is trainable
generation: techniques for automatically train-
ing NLG modules, or hybrid techniques that
adapt NLG modules to particular domains or
user groups, e.g. (Langkilde, 2000; Mellish,
1998; Walker, Rambow and Rogati, 2002).
Open questions about the trainable approach
include (1) whether the output quality is high
enough, and (2) whether the techniques work
well across domains. For example, the training
method used in SPoT (Sentence Planner Train-
able), as described in (Walker, Rambow and Ro-
gati, 2002), was only shown to work in the travel
domain, for the information gathering phase of
the dialog, and with simple content plans in-
volving no rhetorical relations.
This paper describes trainable sentence
planning for information presentation in the
MATCH (Multimodal Access To City Help) di-
alog system (Johnston et al, 2002). We pro-
vide evidence that the trainable approach is
feasible by showing (1) that the training tech-
nique used for SPoT can be extended to a
new domain (restaurant information); (2) that
this technique, previously used for information-
gathering utterances, can be used for infor-
mation presentations, namely recommendations
and comparisons; and (3) that the quality
of the output is comparable to that of a
template-based generator previously developed
and experimentally evaluated with MATCH
users (Walker et al, 2002; Stent et al, 2002).
Section 2 describes SPaRKy (Sentence Plan-
ning with Rhetorical Knowledge), an extension
of SPoT that uses rhetorical relations. SPaRKy
consists of a randomized sentence plan gen-
erator (SPG) and a trainable sentence plan
ranker (SPR); these are described in Sections 3
strategy:recommend
items: Chanpen Thai
relations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); jus-
tify(nuc:1;sat:4)
content: 1. assert(best(Chanpen Thai))
2. assert(has-att(Chanpen Thai, decor(decent)))
3. assert(has-att(Chanpen Thai, service(good))
4. assert(has-att(Chanpen Thai, cuisine(Thai)))
Figure 1: A content plan for a recommendation
for a restaurant in midtown Manhattan
strategy:compare3
items: Above, Carmine?s
relations:elaboration(1;2); elaboration(1;3); elabora-
tion(1,4); elaboration(1,5); elaboration(1,6);
elaboration(1,7); contrast(2;3); contrast(4;5);
contrast(6;7)
content: 1. assert(exceptional(Above, Carmine?s))
2. assert(has-att(Above, decor(good)))
3. assert(has-att(Carmine?s, decor(decent)))
4. assert(has-att(Above, service(good)))
5. assert(has-att(Carmine?s, service(good)))
6. assert(has-att(Above, cuisine(New Ameri-
can)))
7. assert(has-att(Carmine?s, cuisine(italian)))
Figure 2: A content plan for a comparison be-
tween restaurants in midtown Manhattan
and 4. Section 5 presents the results of two
experiments. The first experiment shows that
given a content plan such as that in Figure 1,
SPaRKy can select sentence plans that commu-
nicate the desired rhetorical relations, are sig-
nificantly better than a randomly selected sen-
tence plan, and are on average less than 10%
worse than a sentence plan ranked highest by
human judges. The second experiment shows
that the quality of SPaRKy?s output is compa-
rable to that of MATCH?s template-based gen-
erator. We sum up in Section 6.
2 SPaRKy Architecture
Information presentation in the MATCH sys-
tem focuses on user-tailored recommendations
and comparisons of restaurants (Walker et al,
2002). Following the bottom-up approach to
text-planning described in (Marcu, 1997; Mel-
lish, 1998), each presentation consists of a set of
assertions about a set of restaurants and a spec-
ification of the rhetorical relations that hold be-
tween them. Example content plans are shown
in Figures 1 and 2. The job of the sentence
planner is to choose linguistic resources to real-
ize a content plan and then rank the resulting
alternative realizations. Figures 3 and 4 show
alternative realizations for the content plans in
Figures 1 and 2.
Alt Realization H SPR
2 Chanpen Thai, which is a Thai restau-
rant, has decent decor. It has good
service. It has the best overall quality
among the selected restaurants.
3 .28
5 Since Chanpen Thai is a Thai restau-
rant, with good service, and it has de-
cent decor, it has the best overall qual-
ity among the selected restaurants.
2.5 .14
6 Chanpen Thai, which is a Thai restau-
rant, with decent decor and good ser-
vice, has the best overall quality among
the selected restaurants.
4 .70
Figure 3: Some alternative sentence plan real-
izations for the recommendation in Figure 1. H
= Humans? score. SPR = SPR?s score.
Alt Realization H SPR
11 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above, which is a New American
restaurant, with good decor, has good
service. Carmine?s, which is an Italian
restaurant, with good service, has de-
cent decor.
2 .73
12 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above has good decor, and Carmine?s
has decent decor. Above and Carmine?s
have good service. Above is a New
American restaurant. On the other
hand, Carmine?s is an Italian restau-
rant.
2.5 .50
13 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above is a New American restaurant.
It has good decor. It has good service.
Carmine?s, which is an Italian restau-
rant, has decent decor and good service.
3 .67
20 Above and Carmine?s offer exceptional
value among the selected restaurants.
Carmine?s has decent decor but Above
has good decor, and Carmine?s and
Above have good service. Carmine?s is
an Italian restaurant. Above, however,
is a New American restaurant.
2.5 .49
25 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above has good decor. Carmine?s is
an Italian restaurant. Above has good
service. Carmine?s has decent decor.
Above is a New American restaurant.
Carmine?s has good service.
NR NR
Figure 4: Some of the alternative sentence plan
realizations for the comparison in Figure 2. H
= Humans? score. SPR = SPR?s score. NR =
Not generated or ranked
The architecture of the spoken language gen-
eration module in MATCH is shown in Figure 5.
The dialog manager sends a high-level commu-
nicative goal to the SPUR text planner, which
selects the content to be communicated using a
user model and brevity constraints (see (Walker
Synthesizer
How to Say It
Realizer
Surface
Assigner
Prosody
Speech
  
UTTERANCE
SYSTEM
Sentence
SPUR
Planner
Communicative
DIALOGUE
MANAGER
Goals
Text
Planner
What to Say
Figure 5: A dialog system with a spoken lan-
guage generator
et al, 2002)). The output is a content plan for
a recommendation or comparison such as those
in Figures 1 and 2.
SPaRKy, the sentence planner, gets the con-
tent plan, and then a sentence plan generator
(SPG) generates one or more sentence plans
(Figure 7) and a sentence plan ranker (SPR)
ranks the generated plans. In order for the
SPG to avoid generating sentence plans that are
clearly bad, a content-structuring module first
finds one or more ways to linearly order the in-
put content plan using principles of entity-based
coherence based on rhetorical relations (Knott
et al, 2001). It outputs a set of text plan
trees (tp-trees), consisting of a set of speech
acts to be communicated and the rhetorical re-
lations that hold between them. For example,
the two tp-trees in Figure 6 are generated for
the content plan in Figure 2. Sentence plans
such as alternative 25 in Figure 4 are avoided;
it is clearly worse than alternatives 12, 13 and
20 since it neither combines information based
on a restaurant entity (e.g Babbo) nor on an
attribute (e.g. decor).
The top ranked sentence plan output by the
SPR is input to the RealPro surface realizer
which produces a surface linguistic utterance
(Lavoie and Rambow, 1997). A prosody as-
signment module uses the prior levels of linguis-
tic representation to determine the appropriate
prosody for the utterance, and passes a marked-
up string to the text-to-speech module.
3 Sentence Plan Generation
As in SPoT, the basis of the SPG is a set of
clause-combining operations that operate on tp-
trees and incrementally transform the elemen-
tary predicate-argument lexico-structural rep-
resentations (called DSyntS (Melcuk, 1988))
associated with the speech-acts on the leaves
of the tree. The operations are applied in a
bottom-up left-to-right fashion and the result-
ing representation may contain one or more sen-
tences. The application of the operations yields
two parallel structures: (1) a sentence plan
tree (sp-tree), a binary tree with leaves labeled
by the assertions from the input tp-tree, and in-
terior nodes labeled with clause-combining op-
erations; and (2) one or more DSyntS trees
(d-trees) which reflect the parallel operations
on the predicate-argument representations.
We generate a random sample of possible
sentence plans for each tp-tree, up to a pre-
specified number of sentence plans, by ran-
domly selecting among the operations accord-
ing to a probability distribution that favors pre-
ferred operations1. The choice of operation is
further constrained by the rhetorical relation
that relates the assertions to be combined, as
in other work e.g. (Scott and de Souza, 1990).
In the current work, three RST rhetorical rela-
tions (Mann and Thompson, 1987) are used in
the content planning phase to express the rela-
tions between assertions: the justify relation
for recommendations, and the contrast and
elaboration relations for comparisons. We
added another relation to be used during the
content-structuring phase, called infer, which
holds for combinations of speech acts for which
there is no rhetorical relation expressed in the
content plan, as in (Marcu, 1997). By explicitly
representing the discourse structure of the infor-
mation presentation, we can generate informa-
tion presentations with considerably more inter-
nal complexity than those generated in (Walker,
Rambow and Rogati, 2002) and eliminate those
that violate certain coherence principles, as de-
scribed in Section 2.
The clause-combining operations are general
operations similar to aggregation operations
used in other research (Rambow and Korelsky,
1992; Danlos, 2000). The operations and the
1Although the probability distribution here is hand-
crafted based on assumed preferences for operations such
as merge, relative-clause and with-reduction, it
might also be possible to learn this probability distribu-
tion from the data by training in two phases.
nucleus:<3>assert-com-decor
contrast
nucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisine
nucleus:<7>assert-com-cuisine
contrast
nucleus:<4>assert-com-service
nucleus:<5>assert-com-service
contrast
elaboration
nucleus:<1>assert-com-list_exceptional infer
nucleus:<3>assert-com-decor
nucleus:<5>assert-com-service
nucleus:<7>assert-com-cuisine
inferinfer
nucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisine
nucleus:<4>assert-com-service
elaboration
nucleus:<1>assert-com-list_exceptional contrast
Figure 6: Two tp-trees for alternative 13 in Figure 4.
constraints on their use are described below.
merge applies to two clauses with identical
matrix verbs and all but one identical argu-
ments. The clauses are combined and the non-
identical arguments coordinated. For example,
merge(Above has good service;Carmine?s has
good service) yields Above and Carmine?s have
good service. merge applies only for the rela-
tions infer and contrast.
with-reduction is treated as a kind of
?verbless? participial clause formation in which
the participial clause is interpreted with the
subject of the unreduced clause. For exam-
ple, with-reduction(Above is a New Amer-
ican restaurant;Above has good decor) yields
Above is a New American restaurant, with good
decor. with-reduction uses two syntactic
constraints: (a) the subjects of the clauses must
be identical, and (b) the clause that under-
goes the participial formation must have a have-
possession predicate. In the example above, for
instance, the Above is a New American restau-
rant clause cannot undergo participial forma-
tion since the predicate is not one of have-
possession. with-reduction applies only for
the relations infer and justify.
relative-clause combines two clauses with
identical subjects, using the second clause to
relativize the first clause?s subject. For ex-
ample, relative-clause(Chanpen Thai is a
Thai restaurant, with decent decor and good ser-
vice;Chanpen Thai has the best overall quality
among the selected restaurants) yields Chanpen
Thai, which is a Thai restaurant, with decent
decor and good service, has the best overall qual-
ity among the selected restaurants. relative-
clause also applies only for the relations infer
and justify.
cue-word inserts a discourse connective
(one of since, however, while, and, but, and on
the other hand), between the two clauses to be
combined. cue-word conjunction combines
two distinct clauses into a single sentence with a
coordinating or subordinating conjunction (e.g.
Above has decent decor BUT Carmine?s has
good decor), while cue-word insertion inserts
a cue word at the start of the second clause, pro-
ducing two separate sentences (e.g. Carmine?s
is an Italian restaurant. HOWEVER, Above
is a New American restaurant). The choice of
cue word is dependent on the rhetorical relation
holding between the clauses.
Finally, period applies to two clauses to be
treated as two independent sentences.
Note that a tp-tree can have very different
realizations, depending on the operations of the
SPG. For example, the second tp-tree in Fig-
ure 6 yields both Alt 11 and Alt 13 in Figure 4.
However, Alt 13 is more highly rated than Alt
11. The sp-tree and d-tree produced by the SPG
for Alt 13 are shown in Figures 7 and 8. The
composite labels on the interior nodes of the sp-
PERIOD_elaboration
PERIOD_contrast
RELATIVE_CLAUSE_inferPERIOD_infer
PERIOD_infer <4>assert-com-service <7>assert-com-cuisine MERGE_infer
<3>assert-come-decor <5>assert-com-service<2>assert-com-decor<6>assert-com-cuisine
<1>assert-com-list_exceptional
Figure 7: Sentence plan tree (sp-tree) for alternative 13 in Figure 4
offer
exceptional
among
restaurant
selected
Above_and_Carmine?s
Carmine?s
BE3
restaurantCarmine?s
Italian
decor
decent AND2
service
good
HAVE1
PERIOD
New_American
BE3
Above Above decor
good
HAVE1
restaurant
Above
good
HAVE1
service
PERIOD
PERIOD
value
PERIOD
Figure 8: Dependency tree (d-tree) for alternative 13 in Figure 4
tree indicate the clause-combining relation se-
lected to communicate the specified rhetorical
relation. The d-tree for Alt 13 in Figure 8 shows
that the SPG treats the period operation as
part of the lexico-structural representation for
the d-tree. After sentence planning, the d-tree
is split into multiple d-trees at period nodes;
these are sent to the RealPro surface realizer.
Separately, the SPG also handles referring ex-
pression generation by converting proper names
to pronouns when they appear in the previous
utterance. The rules are applied locally, across
adjacent sequences of utterances (Brennan et
al., 1987). Referring expressions are manipu-
lated in the d-trees, either intrasententially dur-
ing the creation of the sp-tree, or intersenten-
tially, if the full sp-tree contains any period op-
erations. The third and fourth sentences for Alt
13 in Figure 4 show the conversion of a named
restaurant (Carmine?s) to a pronoun.
4 Training the Sentence Plan
Ranker
The SPR takes as input a set of sp-trees gener-
ated by the SPG and ranks them. The SPR?s
rules for ranking sp-trees are learned from a la-
beled set of sentence-plan training examples us-
ing the RankBoost algorithm (Schapire, 1999).
Examples and Feedback: To apply Rank-
Boost, a set of human-rated sp-trees are en-
coded in terms of a set of features. We started
with a set of 30 representative content plans for
each strategy. The SPG produced as many as 20
distinct sp-trees for each content plan. The sen-
tences, realized by RealPro from these sp-trees,
were then rated by two expert judges on a scale
from 1 to 5, and the ratings averaged. Each sp-
tree was an example input for RankBoost, with
each corresponding rating its feedback.
Features used by RankBoost: RankBoost
requires each example to be encoded as a set of
real-valued features (binary features have val-
ues 0 and 1). A strength of RankBoost is that
the set of features can be very large. We used
7024 features for training the SPR. These fea-
tures count the number of occurrences of certain
structural configurations in the sp-trees and the
d-trees, in order to capture declaratively de-
cisions made by the randomized SPG, as in
(Walker, Rambow and Rogati, 2002). The fea-
tures were automatically generated using fea-
ture templates. For this experiment, we use
two classes of feature: (1) Rule-features: These
features are derived from the sp-trees and repre-
sent the ways in which merge, infer and cue-
word operations are applied to the tp-trees.
These feature names start with ?rule?. (2) Sent-
features: These features are derived from the
DSyntSs, and describe the deep-syntactic struc-
ture of the utterance, including the chosen lex-
emes. As a result, some may be domain specific.
These feature names are prefixed with ?sent?.
We now describe the feature templates used
in the discovery process. Three templates were
used for both sp-tree and d-tree features; two
were used only for sp-tree features. Local feature
templates record structural configurations local
to a particular node (its ancestors, daughters
etc.). Global feature templates, which are used
only for sp-tree features, record properties of the
entire sp-tree. We discard features that occur
fewer than 10 times to avoid those specific to
particular text plans.
Strategy System Min Max Mean S.D.
Recommend SPaRKy 2.0 5.0 3.6 .71
HUMAN 2.5 5.0 3.9 .55
RANDOM 1.5 5.0 2.9 .88
Compare2 SPaRKy 2.5 5.0 3.9 .71
HUMAN 2.5 5.0 4.4 .54
RANDOM 1.0 5.0 2.9 1.3
Compare3 SPaRKy 1.5 4.5 3.4 .63
HUMAN 3.0 5.0 4.0 .49
RANDOM 1.0 4.5 2.7 1.0
Table 1: Summary of Recommend, Compare2
and Compare3 results (N = 180)
There are four types of local feature
template: traversal features, sister features,
ancestor features and leaf features. Local
feature templates are applied to all nodes in a
sp-tree or d-tree (except that the leaf feature is
not used for d-trees); the value of the resulting
feature is the number of occurrences of the
described configuration in the tree. For each
node in the tree, traversal features record the
preorder traversal of the subtree rooted at
that node, for all subtrees of all depths. An
example is the feature ?rule traversal assert-
com-list exceptional? (with value 1) of the
tree in Figure 7. Sister features record all
consecutive sister nodes. An example is the fea-
ture ?rule sisters PERIOD infer RELATIVE
CLAUSE infer? (with value 1) of the
tree in Figure 7. For each node in the
tree, ancestor features record all the ini-
tial subpaths of the path from that node
to the root. An example is the feature
?rule ancestor PERIOD contrast*PERIOD
infer? (with value 1) of the tree in Figure 7.
Finally, leaf features record all initial substrings
of the frontier of the sp-tree. For example, the
sp-tree of Figure 7 has value 1 for the feature
?leaf #assert-com-list exceptional#assert-com-
cuisine?.
Global features apply only to the sp-
tree. They record, for each sp-tree and for
each clause-combining operation labeling a non-
frontier node, (1) the minimal number of leaves
dominated by a node labeled with that op-
eration in that tree (MIN); (2) the maximal
number of leaves dominated by a node la-
beled with that operation (MAX); and (3)
the average number of leaves dominated by
a node labeled with that operation (AVG).
For example, the sp-tree in Figure 7 has
value 3 for ?PERIOD infer max?, value 2 for
?PERIOD infer min? and value 2.5 for ?PE-
RIOD infer avg?.
5 Experimental Results
We report two sets of experiments. The first ex-
periment tests the ability of the SPR to select a
high quality sentence plan from a population of
sentence plans randomly generated by the SPG.
Because the discriminatory power of the SPR is
best tested by the largest possible population of
sentence plans, we use 2-fold cross validation for
this experiment. The second experiment com-
pares SPaRKy to template-based generation.
Cross Validation Experiment: We re-
peatedly tested SPaRKy on the half of the cor-
pus of 1756 sp-trees held out as test data for
each fold. The evaluation metric is the human-
assigned score for the variant that was rated
highest by SPaRKy for each text plan for each
task/user combination. We evaluated SPaRKy
on the test sets by comparing three data points
for each text plan: HUMAN (the score of the
top-ranked sentence plan); SPARKY (the score
of the SPR?s selected sentence); and RANDOM
(the score of a sentence plan randomly selected
from the alternate sentence plans).
We report results separately for comparisons
between two entities and among three or more
entities. These two types of comparison are gen-
erated using different strategies in the SPG, and
can produce text that is very different both in
terms of length and structure.
Table 1 summarizes the difference between
SPaRKy, HUMAN and RANDOM for recom-
mendations, comparisons between two entities
and comparisons between three or more enti-
ties. For all three presentation types, a paired
t-test comparing SPaRKy to HUMAN to RAN-
DOM showed that SPaRKy was significantly
better than RANDOM (df = 59, p < .001) and
significantly worse than HUMAN (df = 59, p
< .001). This demonstrates that the use of a
trainable sentence planner can lead to sentence
plans that are significantly better than baseline
(RANDOM), with less human effort than pro-
gramming templates.
Comparison with template generation:
For each content plan input to SPaRKy, the
judges also rated the output of a template-
based generator for MATCH. This template-
based generator performs text planning and sen-
tence planning (the focus of the current pa-
per), including some discourse cue insertion,
clause combining and referring expression gen-
eration; the templates themselves are described
in (Walker et al, 2002). Because the templates
are highly tailored to this domain, this genera-
tor can be expected to perform well. Example
template-based and SPaRKy outputs for a com-
parison between three or more items are shown
in Figure 9.
Strategy System Min Max Mean S.D.
Recommend Template 2.5 5.0 4.22 0.74
SPaRKy 2.5 4.5 3.57 0.59
HUMAN 4.0 5.0 4.37 0.37
Compare2 Template 2.0 5.0 3.62 0.75
SPaRKy 2.5 4.75 3.87 0.52
HUMAN 4.0 5.0 4.62 0.39
Compare3 Template 1.0 5.0 4.08 1.23
SPaRKy 2.5 4.25 3.375 0.38
HUMAN 4.0 5.0 4.63 0.35
Table 2: Summary of template-based genera-
tion results. N = 180
Table 2 shows the mean HUMAN scores for
the template-based sentence planning. A paired
t-test comparing HUMAN and template-based
scores showed that HUMAN was significantly
better than template-based sentence planning
only for compare2 (df = 29, t = 6.2, p < .001).
The judges evidently did not like the template
for comparisons between two items. A paired
t-test comparing SPaRKy and template-based
sentence planning showed that template-based
sentence planning was significantly better than
SPaRKy only for recommendations (df = 29, t
= 3.55, p < .01). These results demonstrate
that trainable sentence planning shows promise
for producing output comparable to that of a
template-based generator, with less program-
ming effort and more flexibility.
The standard deviation for all three template-
based strategies was wider than for HUMAN
or SPaRKy, indicating that there may be
content-specific aspects to the sentence plan-
ning done by SPaRKy that contribute to out-
put variation. The data show this to be cor-
rect; SPaRKy learned content-specific prefer-
ences about clause combining and discourse cue
insertion that a template-based generator can-
System Realization H
Template Among the selected restaurants, the fol-
lowing offer exceptional overall value.
Uguale?s price is 33 dollars. It has good
decor and very good service. It?s a
French, Italian restaurant. Da Andrea?s
price is 28 dollars. It has good decor and
very good service. It?s an Italian restau-
rant. John?s Pizzeria?s price is 20 dollars.
It has mediocre decor and decent service.
It?s an Italian, Pizza restaurant.
4.5
SPaRKy Da Andrea, Uguale, and John?s Pizze-
ria offer exceptional value among the se-
lected restaurants. Da Andrea is an Ital-
ian restaurant, with very good service, it
has good decor, and its price is 28 dol-
lars. John?s Pizzeria is an Italian , Pizza
restaurant. It has decent service. It has
mediocre decor. Its price is 20 dollars.
Uguale is a French, Italian restaurant,
with very good service. It has good decor,
and its price is 33 dollars.
4
Figure 9: Comparisons between 3 or more
items, H = Humans? score
not easily model, but that a trainable sentence
planner can. For example, Table 3 shows the
nine rules generated on the first test fold which
have the largest negative impact on the final
RankBoost score (above the double line) and
the largest positive impact on the final Rank-
Boost score (below the double line), for com-
parisons between three or more entities. The
rule with the largest positive impact shows that
SPaRKy learned to prefer that justifications in-
volving price be merged with other information
using a conjunction.
These rules are also specific to presentation
type. Averaging over both folds of the exper-
iment, the number of unique features appear-
ing in rules is 708, of which 66 appear in the
rule sets for two presentation types and 9 ap-
pear in the rule sets for all three presentation
types. There are on average 214 rule features,
428 sentence features and 26 leaf features. The
majority of the features are ancestor features
(319) followed by traversal features (264) and
sister features (60). The remainder of the fea-
tures (67) are for specific lexemes.
To sum up, this experiment shows that the
ability to model the interactions between do-
main content, task and presentation type is a
strength of the trainable approach to sentence
planning.
6 Conclusions
This paper shows that the training technique
used in SPoT can be easily extended to a new
N Condition ?s
1 sent anc PROPERNOUN RESTAURANT
*HAVE1 ? 16.5
-0.859
2 sent anc II Upper East Side*ATTR IN1*
locate ? 4.5
-0.852
3 sent anc PERIOD infer*PERIOD infer
*PERIOD elaboration ? -?
-0.542
4 rule anc assert-com-service*MERGE infer
? 1.5
-0.356
5 sent tvl depth 0 BE3 ? 4.5 -0.346
6 rule anc PERIOD infer*PERIOD infer
*PERIOD elaboration ? -?
-0.345
7 rule anc assert-com-decor*PERIOD infer
*PERIOD infer*PERIOD contrast *PE-
RIOD elaboration? -?
-0.342
8 rule anc assert-com-food quality*MERGE
infer ? 1.5
0.398
9 rule anc assert-com-price*CW
CONJUNCTION infer*PERIOD justify
? -?
0.527
Table 3: The nine rules generated on the first
test fold which have the largest negative impact
on the final RankBoost score (above the dou-
ble line) and the largest positive impact on the
final RankBoost score (below the double line),
for Compare3. ?s represents the increment or
decrement associated with satisfying the condi-
tion.
domain and used for information presentation
as well as information gathering. Previous work
on SPoT also compared trainable sentence plan-
ning to a template-based generator that had
previously been developed for the same appli-
cation (Rambow et al, 2001). The evalua-
tion results for SPaRKy (1) support the results
for SPoT, by showing that trainable sentence
generation can produce output comparable to
template-based generation, even for complex in-
formation presentations such as extended com-
parisons; (2) show that trainable sentence gen-
eration is sensitive to variations in domain ap-
plication, presentation type, and even human
preferences about the arrangement of particu-
lar types of information.
7 Acknowledgments
We thank AT&T for supporting this research,
and the anonymous reviewers for their helpful
comments on this paper.
References
I. Langkilde. Forest-based statistical sentence gen-
eration. In Proc. NAACL 2000, 2000.
S. E. Brennan, M. Walker Friedman, and C. J. Pol-
lard. A centering approach to pronouns. In Proc.
25th Annual Meeting of the ACL, Stanford, pages
155?162, 1987.
L. Danlos. 2000. G-TAG: A lexicalized formal-
ism for text generation inspired by tree ad-
joining grammar. In Tree Adjoining Grammars:
Formalisms, Linguistic Analysis, and Processing.
CSLI Publications.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Mal-
oor. MATCH: An architecture for multimodal di-
alogue systems. In Annual Meeting of the ACL,
2002.
A. Knott, J. Oberlander, M. O?Donnell and C. Mel-
lish. Beyond Elaboration: the interaction of rela-
tions and focus in coherent text. In Text Repre-
sentation: linguistic and psycholinguistic aspects,
pages 181-196, 2001.
B. Lavoie and O. Rambow. A fast and portable re-
alizer for text generation systems. In Proc. of the
3rd Conference on Applied Natural Language Pro-
cessing, ANLP97, pages 265?268, 1997.
W.C. Mann and S.A. Thompson. Rhetorical struc-
ture theory: A framework for the analysis of texts.
Technical Report RS-87-190, USC/Information
Sciences Institute, 1987.
D. Marcu. From local to global coherence: a
bottom-up approach to text planning. In Proceed-
ings of the National Conference on Artificial In-
telligence (AAAI?97), 1997.
C. Mellish, A. Knott, J. Oberlander, and M.
O?Donnell. Experiments using stochastic search
for text planning. In Proceedings of INLG-98.
1998.
I. A. Melc?uk. Dependency Syntax: Theory and Prac-
tice. SUNY, Albany, New York, 1988.
O. Rambow and T. Korelsky. Applied text genera-
tion. In Proceedings of the Third Conference on
Applied Natural Language Processing, ANLP92,
pages 40?47, 1992.
O. Rambow, M. Rogati and M. A. Walker. Evalu-
ating a Trainable Sentence Planner for a Spoken
Dialogue Travel System In Meeting of the ACL,
2001.
R. E. Schapire. A brief introduction to boosting. In
Proc. of the 16th IJCAI, 1999.
D. R. Scott and C. Sieckenius de Souza. Getting
the message across in RST-based text generation.
In Current Research in Natural Language Gener-
ation, pages 47?73, 1990.
A. Stent, M. Walker, S. Whittaker, and P. Maloor.
User-tailored generation for spoken dialogue: An
experiment. In Proceedings of ICSLP 2002., 2002.
M. A. Walker, S. J. Whittaker, A. Stent, P. Mal-
oor, J. D. Moore, M. Johnston, and G. Vasireddy.
Speech-Plans: Generating evaluative responses
in spoken dialogue. In Proceedings of INLG-02.,
2002.
M. Walker, O. Rambow, and M. Rogati. Training a
sentence planner for spoken dialogue using boost-
ing. Computer Speech and Language: Special Is-
sue on Spoken Language Generation, 2002.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 265?272,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning to Generate Naturalistic Utterances Using Reviews in Spoken
Dialogue Systems
Ryuichiro Higashinaka
NTT Corporation
rh@cslab.kecl.ntt.co.jp
Rashmi Prasad
University of Pennsylvania
rjprasad@linc.cis.upenn.edu
Marilyn A. Walker
University of Sheffield
walker@dcs.shef.ac.uk
Abstract
Spoken language generation for dialogue
systems requires a dictionary of mappings
between semantic representations of con-
cepts the system wants to express and re-
alizations of those concepts. Dictionary
creation is a costly process; it is currently
done by hand for each dialogue domain.
We propose a novel unsupervised method
for learning such mappings from user re-
views in the target domain, and test it on
restaurant reviews. We test the hypothesis
that user reviews that provide individual
ratings for distinguished attributes of the
domain entity make it possible to map re-
view sentences to their semantic represen-
tation with high precision. Experimental
analyses show that the mappings learned
cover most of the domain ontology, and
provide good linguistic variation. A sub-
jective user evaluation shows that the con-
sistency between the semantic representa-
tions and the learned realizations is high
and that the naturalness of the realizations
is higher than a hand-crafted baseline.
1 Introduction
One obstacle to the widespread deployment of
spoken dialogue systems is the cost involved
with hand-crafting the spoken language generation
module. Spoken language generation requires a
dictionary of mappings between semantic repre-
sentations of concepts the system wants to express
and realizations of those concepts. Dictionary cre-
ation is a costly process: an automatic method
for creating them would make dialogue technol-
ogy more scalable. A secondary benefit is that a
learned dictionary may produce more natural and
colloquial utterances.
We propose a novel method for mining user re-
views to automatically acquire a domain specific
generation dictionary for information presentation
in a dialogue system. Our hypothesis is that re-
views that provide individual ratings for various
distinguished attributes of review entities can be
used to map review sentences to a semantic rep-
An example user review (we8there.com)
Ratings Food=5, Service=5, Atmosphere=5,
Value=5, Overall=5
Review
comment
The best Spanish food in New York. I am
from Spain and I had my 28th birthday
there and we all had a great time. Salud!
?
Review comment after named entity recognition
The best {NE=foodtype, string=Spanish} {NE=food,
string=food, rating=5} in {NE=location, string=New
York}. . . .
?
Mapping between a semantic representation (a set of
relations) and a syntactic structure (DSyntS)
? Relations:
RESTAURANT has FOODTYPE
RESTAURANT has foodquality=5
RESTAURANT has LOCATION
([foodtype, food=5, location] for shorthand.)
? DSyntS:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
lexeme : food
class : common noun
number : sg
article : def
ATTR
[
lexeme : best
class : adjective
]
ATTR
?
?
lexeme : FOODTYPE
class : common noun
number : sg
article : no-art
?
?
ATTR
?
?
?
?
?
lexeme : in
class : preposition
II
?
?
lexeme : LOCATION
class : proper noun
number : sg
article : no-art
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Example of procedure for acquiring a
generation dictionary mapping.
resentation. Figure 1 shows a user review in the
restaurant domain, where we hypothesize that the
user rating food=5 indicates that the semantic rep-
resentation for the sentence ?The best Spanish
food in New York? includes the relation ?RESTAU-
RANT has foodquality=5.?
We apply the method to extract 451 mappings
from restaurant reviews. Experimental analyses
show that the mappings learned cover most of the
domain ontology, and provide good linguistic vari-
ation. A subjective user evaluation indicates that
the consistency between the semantic representa-
tions and the learned realizations is high and that
the naturalness of the realizations is significantly
higher than a hand-crafted baseline.
265
Section 2 provides a step-by-step description of
the method. Sections 3 and 4 present the evalua-
tion results. Section 5 covers related work. Sec-
tion 6 summarizes and discusses future work.
2 Learning a Generation Dictionary
Our automatically created generation dictionary
consists of triples (U ,R,S) representing a map-
ping between the original utterance U in the user
review, its semantic representation R(U), and its
syntactic structure S(U). Although templates are
widely used in many practical systems (Seneff and
Polifroni, 2000; Theune, 2003), we derive syn-
tactic structures to represent the potential realiza-
tions, in order to allow aggregation, and other
syntactic transformations of utterances, as well as
context specific prosody assignment (Walker et al,
2003; Moore et al, 2004).
The method is outlined briefly in Fig. 1 and de-
scribed below. It comprises the following steps:
1. Collect user reviews on the web to create a
population of utterances U .
2. To derive semantic representations R(U):
? Identify distinguished attributes and
construct a domain ontology;
? Specify lexicalizations of attributes;
? Scrape webpages? structured data for
named-entities;
? Tag named-entities.
3. Derive syntactic representations S(U).
4. Filter inappropriate mappings.
5. Add mappings (U ,R,S) to dictionary.
2.1 Creating the corpus
We created a corpus of restaurant reviews by
scraping 3,004 user reviews of 1,810 restau-
rants posted at we8there.com (http://www.we8-
there.com/), where each individual review in-
cludes a 1-to-5 Likert-scale rating of different
restaurant attributes. The corpus consists of
18,466 sentences.
2.2 Deriving semantic representations
The distinguished attributes are extracted from the
webpages for each restaurant entity. They in-
clude attributes that the users are asked to rate,
i.e. food, service, atmosphere, value, and over-
all, which have scalar values. In addition, other
attributes are extracted from the webpage, such
as the name, foodtype and location of the restau-
rant, which have categorical values. The name
attribute is assumed to correspond to the restau-
rant entity. Given the distinguished attributes, a
Dist. Attr. Lexicalization
food food, meal
service service, staff, waitstaff, wait staff, server,
waiter, waitress
atmosphere atmosphere, decor, ambience, decoration
value value, price, overprice, pricey, expensive,
inexpensive, cheap, affordable, afford
overall recommend, place, experience, establish-
ment
Table 1: Lexicalizations for distinguished at-
tributes.
simple domain ontology can be automatically de-
rived by assuming that a meronymy relation, rep-
resented by the predicate ?has?, holds between the
entity type (RESTAURANT) and the distinguished
attributes. Thus, the domain ontology consists of
the relations:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
RESTAURANT has foodquality
RESTAURANT has servicequality
RESTAURANT has valuequality
RESTAURANT has atmospherequality
RESTAURANT has overallquality
RESTAURANT has foodtype
RESTAURANT has location
We assume that, although users may discuss
other attributes of the entity, at least some of the
utterances in the reviews realize the relations spec-
ified in the ontology. Our problem then is to iden-
tify these utterances. We test the hypothesis that,
if an utterance U contains named-entities corre-
sponding to the distinguished attributes, thatR for
that utterance includes the relation concerning that
attribute in the domain ontology.
We define named-entities for lexicalizations of
the distinguished attributes, starting with the seed
word for that attribute on the webpage (Table 1).1
For named-entity recognition, we use GATE (Cun-
ningham et al, 2002), augmented with named-
entity lists for locations, food types, restaurant
names, and food subtypes (e.g. pizza), scraped
from the we8there webpages.
We also hypothesize that the rating given for the
distinguished attribute specifies the scalar value
of the relation. For example, a sentence contain-
ing food or meal is assumed to realize the re-
lation ?RESTAURANT has foodquality.?, and the
value of the foodquality attribute is assumed to be
the value specified in the user rating for that at-
tribute, e.g. ?RESTAURANT has foodquality = 5? in
Fig. 1. Similarly, the other relations in Fig. 1 are
assumed to be realized by the utterance ?The best
Spanish food in New York? because it contains
1In future, we will investigate other techniques for boot-
strapping these lexicalizations from the seed word on the
webpage.
266
filter filtered retained
No Relations Filter 7,947 10,519
Other Relations Filter 5,351 5,168
Contextual Filter 2,973 2,195
Unknown Words Filter 1,467 728
Parsing Filter 216 512
Table 2: Filtering statistics: the number of sen-
tences filtered and retained by each filter.
one FOODTYPE named-entity and one LOCATION
named-entity. Values of categorical attributes are
replaced by variables representing their type be-
fore the learned mappings are added to the dictio-
nary, as shown in Fig. 1.
2.3 Parsing and DSyntS conversion
We adopt Deep Syntactic Structures (DSyntSs) as
a format for syntactic structures because they can
be realized by the fast portable realizer RealPro
(Lavoie and Rambow, 1997). Since DSyntSs are a
type of dependency structure, we first process the
sentences with Minipar (Lin, 1998), and then con-
vert Minipar?s representation into DSyntS. Since
user reviews are different from the newspaper ar-
ticles on which Minipar was trained, the output
of Minipar can be inaccurate, leading to failure in
conversion. We check whether conversion is suc-
cessful in the filtering stage.
2.4 Filtering
The goal of filtering is to identify U that realize
the distinguished attributes and to guarantee high
precision for the learned mappings. Recall is less
important since systems need to convey requested
information as accurately as possible. Our proce-
dure for deriving semantic representations is based
on the hypothesis that if U contains named-entities
that realize the distinguished attributes, thatRwill
include the relevant relation in the domain ontol-
ogy. We also assume that if U contains named-
entities that are not covered by the domain ontol-
ogy, or words indicating that the meaning of U de-
pends on the surrounding context, that R will not
completely characterizes the meaning of U , and so
U should be eliminated. We also require an accu-
rate S for U . Therefore, the filters described be-
low eliminate U that (1) realize semantic relations
not in the ontology; (2) contain words indicating
that its meaning depends on the context; (3) con-
tain unknown words; or (4) cannot be parsed ac-
curately.
No Relations Filter: The sentence does not con-
tain any named-entities for the distinguished
attributes.
Other Relations Filter: The sentence contains
named-entities for food subtypes, person
Rating
Dist.Attr.
1 2 3 4 5 Total
food 5 8 6 18 57 94
service 15 3 6 17 56 97
atmosphere 0 3 3 8 31 45
value 0 0 1 8 12 21
overall 3 2 5 15 45 70
Total 23 15 21 64 201 327
Table 3: Domain coverage of single scalar-valued
relation mappings.
names, country names, dates (e.g., today, to-
morrow, Aug. 26th) or prices (e.g., 12 dol-
lars), or POS tag CD for numerals. These in-
dicate relations not in the ontology.
Contextual Filter: The sentence contains index-
icals such as I, you, that or cohesive markers
of rhetorical relations that connect it to some
part of the preceding text, which means that
the sentence cannot be interpreted out of con-
text. These include discourse markers, such
as list item markers with LS as the POS tag,
that signal the organization structure of the
text (Hirschberg and Litman, 1987), as well
as discourse connectives that signal semantic
and pragmatic relations of the sentence with
other parts of the text (Knott, 1996), such as
coordinating conjunctions at the beginning of
the utterance like and and but etc., and con-
junct adverbs such as however, also, then.
Unknown Words Filter: The sentence contains
words not in WordNet (Fellbaum, 1998)
(which includes typographical errors), or
POS tags contain NN (Noun), which may in-
dicate an unknown named-entity, or the sen-
tence has more than a fixed length of words,2
indicating that its meaning may not be esti-
mated solely by named entities.
Parsing Filter: The sentence fails the parsing to
DSyntS conversion. Failures are automati-
cally detected by comparing the original sen-
tence with the one realized by RealPro taking
the converted DSyntS as an input.
We apply the filters, in a cascading manner, to the
18,466 sentences with semantic representations.
As a result, we obtain 512 (2.8%) mappings of
(U ,R,S). After removing 61 duplicates, 451 dis-
tinct (2.4%) mappings remain. Table 2 shows the
number of sentences eliminated by each filter.
3 Objective Evaluation
We evaluate the learned expressions with respect
to domain coverage, linguistic variation and gen-
erativity.
2We used 20 as a threshold.
267
# Combination of Dist. Attrs Count
1 food-service 39
2 food-value 21
3 atmosphere-food 14
4 atmosphere-service 10
5 atmosphere-food-service 7
6 food-foodtype 4
7 atmosphere-food-value 4
8 location-overall 3
9 food-foodtype-value 3
10 food-service-value 2
11 food-foodtype-location 2
12 food-overall 2
13 atmosphere-foodtype 2
14 atmosphere-overall 2
15 service-value 1
16 overall-service 1
17 overall-value 1
18 foodtype-overall 1
19 food-foodtype-location-overall 1
20 atmosphere-food-service-value 1
21 atmosphere-food-overall-
service-value
1
Total 122
Table 4: Counts for multi-relation mappings.
3.1 Domain Coverage
To be usable for a dialogue system, the mappings
must have good domain coverage. Table 3 shows
the distribution of the 327 mappings realizing a
single scalar-valued relation, categorized by the
associated rating score.3 For example, there are 57
mappings with R of ?RESTAURANT has foodqual-
ity=5,? and a large number of mappings for both
the foodquality and servicequality relations. Al-
though we could not obtain mappings for some re-
lations such as price={1,2}, coverage for express-
ing a single relation is fairly complete.
There are also mappings that express several re-
lations. Table 4 shows the counts of mappings
for multi-relation mappings, with those contain-
ing a food or service relation occurring more fre-
quently as in the single scalar-valued relation map-
pings. We found only 21 combinations of rela-
tions, which is surprising given the large poten-
tial number of combinations (There are 50 com-
binations if we treat relations with different scalar
values differently). We also find that most of the
mappings have two or three relations, perhaps sug-
gesting that system utterances should not express
too many relations in a single sentence.
3.2 Linguistic Variation
We also wish to assess whether the linguistic
variation of the learned mappings was greater
than what we could easily have generated with a
hand-crafted dictionary, or a hand-crafted dictio-
nary augmented with aggregation operators, as in
3There are two other single-relation but not scalar-valued
mappings that concern LOCATION in our mappings.
(Walker et al, 2003). Thus, we first categorized
the mappings by the patterns of the DSyntSs. Ta-
ble 5 shows the most common syntactic patterns
(more than 10 occurrences), indicating that 30%
of the learned patterns consist of the simple form
?X is ADJ? where ADJ is an adjective, or ?X is RB
ADJ,? where RB is a degree modifier. Furthermore,
up to 55% of the learned mappings could be gen-
erated from these basic patterns by the application
of a combination operator that coordinates mul-
tiple adjectives, or coordinates predications over
distinct attributes. However, there are 137 syntac-
tic patterns in all, 97 with unique syntactic struc-
tures and 21 with two occurrences, accounting for
45% of the learned mappings. Table 6 shows ex-
amples of learned mappings with distinct syntactic
structures. It would be surprising to see this type
of variety in a hand-crafted generation dictionary.
In addition, the learned mappings contain 275 dis-
tinct lexemes, with a minimum of 2, maximum of
15, and mean of 4.63 lexemes per DSyntS, indi-
cating that the method extracts a wide variety of
expressions of varying lengths.
Another interesting aspect of the learned map-
pings is the wide variety of adjectival phrases
(APs) in the common patterns. Tables 7 and 8
show the APs in single scalar-valued relation map-
pings for food and service categorized by the as-
sociated ratings. Tables for atmosphere, value and
overall can be found in the Appendix. Moreover,
the meanings for some of the learned APs are very
specific to the particular attribute, e.g. cold and
burnt associated with foodquality of 1, attentive
and prompt for servicequality of 5, silly and inat-
tentive for servicequality of 1. and mellow for at-
mosphere of 5. In addition, our method places the
adjectival phrases (APs) in the common patterns
on a more fine-grained scale of 1 to 5, similar to
the strength classifications in (Wilson et al, 2004),
in contrast to other automatic methods that clas-
sify expressions into a binary positive or negative
polarity (e.g. (Turney, 2002)).
3.3 Generativity
Our motivation for deriving syntactic representa-
tions for the learned expressions was the possibil-
ity of using an off-the-shelf sentence planner to
derive new combinations of relations, and apply
aggregation and other syntactic transformations.
We examined how many of the learned DSyntSs
can be combined with each other, by taking ev-
ery pair of DSyntSs in the mappings and apply-
ing the built-in merge operation in the SPaRKy
generator (Walker et al, 2003). We found that
only 306 combinations out of a potential 81,318
268
# syntactic pattern example utterance count ratio accum.
1 NN VB JJ The atmosphere is wonderful. 92 20.4% 20.4%
2 NN VB RB JJ The atmosphere was very nice. 52 11.5% 31.9%
3 JJ NN Bad service. 36 8.0% 39.9%
4 NN VB JJ CC JJ The food was flavorful but cold. 25 5.5% 45.5%
5 RB JJ NN Very trendy ambience. 22 4.9% 50.3%
6 NN VB JJ CC NN VB JJ The food is excellent and the atmosphere is great. 13 2.9% 53.2%
7 NN CC NN VB JJ The food and service were fantastic. 10 2.2% 55.4%
Table 5: Common syntactic patterns of DSyntSs, flattened to a POS sequence for readability. NN, VB,
JJ, RB, CC stand for noun, verb, adjective, adverb, and conjunction, respectively.
[overall=1, value=2] Very disappointing experience for
the money charged.
[food=5, value=5] The food is excellent and plentiful at a
reasonable price.
[food=5, service=5] The food is exquisite as well as the
service and setting.
[food=5, service=5] The food was spectacular and so was
the service.
[food=5, foodtype, value=5] Best FOODTYPE food with
a great value for money.
[food=5, foodtype, value=5] An absolutely outstanding
value with fantastic FOODTYPE food.
[food=5, foodtype, location, overall=5] This is the best
place to eat FOODTYPE food in LOCATION.
[food=5, foodtype] Simply amazing FOODTYPE food.
[food=5, foodtype] RESTAURANTNAME is the best of the
best for FOODTYPE food.
[food=5] The food is to die for.
[food=5] What incredible food.
[food=4] Very pleasantly surprised by the food.
[food=1] The food has gone downhill.
[atmosphere=5, overall=5] This is a quiet little place
with great atmosphere.
[atmosphere=5, food=5, overall=5, service=5, value=5]
The food, service and ambience of the place are all fabu-
lous and the prices are downright cheap.
Table 6: Acquired generation patterns (with short-
hand for relations in square brackets) whose syn-
tactic patterns occurred only once.
combinations (0.37%) were successful. This is
because the merge operation in SPaRKy requires
that the subjects and the verbs of the two DSyntSs
are identical, e.g. the subject is RESTAURANT and
verb is has, whereas the learned DSyntSs often
place the attribute in subject position as a definite
noun phrase. However, the learned DSyntS can
be incorporated into SPaRKy using the semantic
representations to substitute learned DSyntSs into
nodes in the sentence plan tree. Figure 2 shows
some example utterances generated by SPaRKy
with its original dictionary and example utterances
when the learned mappings are incorporated. The
resulting utterances seem more natural and collo-
quial; we examine whether this is true in the next
section.
4 Subjective Evaluation
We evaluate the obtained mappings in two re-
spects: the consistency between the automatically
derived semantic representation and the realiza-
food=1 awful, bad, burnt, cold, very ordinary
food=2 acceptable, bad, flavored, not enough, very
bland, very good
food=3 adequate, bland and mediocre, flavorful but
cold, pretty good, rather bland, very good
food=4 absolutely wonderful, awesome, decent, ex-
cellent, good, good and generous, great, out-
standing, rather good, really good, tradi-
tional, very fresh and tasty, very good, very
very good
food=5 absolutely delicious, absolutely fantastic, ab-
solutely great, absolutely terrific, ample, well
seasoned and hot, awesome, best, delectable
and plentiful, delicious, delicious but simple,
excellent, exquisite, fabulous, fancy but tasty,
fantastic, fresh, good, great, hot, incredible,
just fantastic, large and satisfying, outstand-
ing, plentiful and outstanding, plentiful and
tasty, quick and hot, simply great, so deli-
cious, so very tasty, superb, terrific, tremen-
dous, very good, wonderful
Table 7: Adjectival phrases (APs) in single scalar-
valued relation mappings for foodquality.
tion, and the naturalness of the realization.
For comparison, we used a baseline of hand-
crafted mappings from (Walker et al, 2003) ex-
cept that we changed the word decor to at-
mosphere and added five mappings for overall.
For scalar relations, this consists of the realiza-
tion ?RESTAURANT has ADJ LEX? where ADJ is
mediocre, decent, good, very good, or excellent for
rating values 1-5, and LEX is food quality, service,
atmosphere, value, or overall depending on the re-
lation. RESTAURANT is filled with the name of
a restaurant at runtime. For example, ?RESTAU-
RANT has foodquality=1? is realized as ?RESTAU-
RANT has mediocre food quality.? The location
and food type relations are mapped to ?RESTAU-
RANT is located in LOCATION? and ?RESTAU-
RANT is a FOODTYPE restaurant.?
The learned mappings include 23 distinct se-
mantic representations for a single-relation (22 for
scalar-valued relations and one for location) and
50 for multi-relations. Therefore, using the hand-
crafted mappings, we first created 23 utterances
for the single-relations. We then created three ut-
terances for each of 50multi-relations using differ-
ent clause-combining operations from (Walker et
al., 2003). This gave a total of 173 baseline utter-
ances, which together with 451 learned mappings,
269
service=1 awful, bad, great, horrendous, horrible,
inattentive, forgetful and slow, marginal,
really slow, silly and inattentive, still
marginal, terrible, young
service=2 overly slow, very slow and inattentive
service=3 bad, bland and mediocre, friendly and
knowledgeable, good, pleasant, prompt,
very friendly
service=4 all very warm and welcoming, attentive,
extremely friendly and good, extremely
pleasant, fantastic, friendly, friendly and
helpful, good, great, great and courteous,
prompt and friendly, really friendly, so
nice, swift and friendly, very friendly, very
friendly and accommodating
service=5 all courteous, excellent, excellent and
friendly, extremely friendly, fabulous,
fantastic, friendly, friendly and helpful,
friendly and very attentive, good, great,
great, prompt and courteous, happy and
friendly, impeccable, intrusive, legendary,
outstanding, pleasant, polite, attentive and
prompt, prompt and courteous, prompt
and pleasant, quick and cheerful, stupen-
dous, superb, the most attentive, unbeliev-
able, very attentive, very congenial, very
courteous, very friendly, very friendly and
helpful, very friendly and pleasant, very
friendly and totally personal, very friendly
and welcoming, very good, very helpful,
very timely, warm and friendly, wonderful
Table 8: Adjectival phrases (APs) in single scalar-
valued relation mappings for servicequality.
yielded 624 utterances for evaluation.
Ten subjects, all native English speakers, eval-
uated the mappings by reading them from a web-
page. For each system utterance, the subjects were
asked to express their degree of agreement, on a
scale of 1 (lowest) to 5 (highest), with the state-
ment (a) The meaning of the utterance is consis-
tent with the ratings expressing their semantics,
and with the statement (b) The style of the utter-
ance is very natural and colloquial. They were
asked not to correct their decisions and also to rate
each utterance on its own merit.
4.1 Results
Table 9 shows the means and standard deviations
of the scores for baseline vs. learned utterances for
consistency and naturalness. A t-test shows that
the consistency of the learned expression is signifi-
cantly lower than the baseline (df=4712, p < .001)
but that their naturalness is significantly higher
than the baseline (df=3107, p < .001). However,
consistency is still high. Only 14 of the learned
utterances (shown in Tab. 10) have a mean consis-
tency score lower than 3, which indicates that, by
and large, the human judges felt that the inferred
semantic representations were consistent with the
meaning of the learned expressions. The correla-
tion coefficient between consistency and natural-
ness scores is 0.42, which indicates that consis-
Original SPaRKy utterances
? Babbo has the best overall quality among the selected
restaurants with excellent decor, excellent service and
superb food quality.
? Babbo has excellent decor and superb food quality
with excellent service. It has the best overall quality
among the selected restaurants.
?
Combination of SPaRKy and learned DSyntS
? Because the food is excellent, the wait staff is pro-
fessional and the decor is beautiful and very com-
fortable, Babbo has the best overall quality among the
selected restaurants.
? Babbo has the best overall quality among the selected
restaurants because atmosphere is exceptionally nice,
food is excellent and the service is superb.
? Babbo has superb food quality, the service is excep-
tional and the atmosphere is very creative. It has the
best overall quality among the selected restaurants.
Figure 2: Utterances incorporating learned
DSyntSs (Bold font) in SPaRKy.
baseline learned stat.
mean sd. mean sd. sig.
Consistency 4.714 0.588 4.459 0.890 +
Naturalness 4.227 0.852 4.613 0.844 +
Table 9: Consistency and naturalness scores aver-
aged over 10 subjects.
tency does not greatly relate to naturalness.
We also performed an ANOVA (ANalysis Of
VAriance) of the effect of each relation in R on
naturalness and consistency. There were no sig-
nificant effects except that mappings combining
food, service, and atmosphere were significantly
worse (df=1, F=7.79, p=0.005). However, there
is a trend for mappings to be rated higher for
the food attribute (df=1, F=3.14, p=0.08) and the
value attribute (df=1, F=3.55, p=0.06) for consis-
tency, suggesting that perhaps it is easier to learn
some mappings than others.
5 Related Work
Automatically finding sentences with the same
meaning has been extensively studied in the field
of automatic paraphrasing using parallel corpora
and corpora with multiple descriptions of the same
events (Barzilay and McKeown, 2001; Barzilay
and Lee, 2003). Other work finds predicates of
similar meanings by using the similarity of con-
texts around the predicates (Lin and Pantel, 2001).
However, these studies find a set of sentences with
the same meaning, but do not associate a specific
meaning with the sentences. One exception is
(Barzilay and Lee, 2002), which derives mappings
between semantic representations and realizations
using a parallel (but unaligned) corpus consisting
of both complex semantic input and correspond-
ing natural language verbalizations for mathemat-
270
shorthand for relations and utterance score
[food=4] The food is delicious and beautifully
prepared.
2.9
[overall=4] A wonderful experience. 2.9
[service=3] The service is bland and mediocre. 2.8
[atmosphere=2] The atmosphere here is eclec-
tic.
2.6
[overall=3] Really fancy place. 2.6
[food=3, service=4] Wonderful service and
great food.
2.5
[service=4] The service is fantastic. 2.5
[overall=2] The RESTAURANTNAME is once a
great place to go and socialize.
2.2
[atmosphere=2] The atmosphere is unique and
pleasant.
2.0
[food=5, foodtype] FOODTYPE and FOODTYPE
food.
1.8
[service=3] Waitstaff is friendly and knowl-
edgeable.
1.7
[atmosphere=5, food=5, service=5] The atmo-
sphere, food and service.
1.6
[overall=3] Overall, a great experience. 1.4
[service=1] The waiter is great. 1.4
Table 10: The 14 utterances with consistency
scores below 3.
ical proofs. However, our technique does not re-
quire parallel corpora or previously existing se-
mantic transcripts or labeling, and user reviews are
widely available in many different domains (See
http://www.epinions.com/).
There is also significant previous work on min-
ing user reviews. For example, Hu and Liu (2005)
use reviews to find adjectives to describe products,
and Popescu and Etzioni (2005) automatically find
features of a product together with the polarity of
adjectives used to describe them. They both aim at
summarizing reviews so that users can make deci-
sions easily. Our method is also capable of finding
polarities of modifying expressions including ad-
jectives, but on a more fine-grained scale of 1 to
5. However, it might be possible to use their ap-
proach to create rating information for raw review
texts as in (Pang and Lee, 2005), so that we can
create mappings from reviews without ratings.
6 Summary and Future Work
We proposed automatically obtaining mappings
between semantic representations and realizations
from reviews with individual ratings. The results
show that: (1) the learned mappings provide good
coverage of the domain ontology and exhibit good
linguistic variation; (2) the consistency between
the semantic representations and realizations is
high; and (3) the naturalness of the realizations are
significantly higher than the baseline.
There are also limitations in our method. Even
though consistency is rated highly by human sub-
jects, this may actually be a judgement of whether
the polarity of the learned mapping is correctly
placed on the 1 to 5 rating scale. Thus, alter-
nate ways of expressing, for example foodqual-
ity=5, shown in Table 7, cannot be guaranteed to
be synonymous, which may be required for use in
spoken language generation. Rather, an examina-
tion of the adjectival phrases in Table 7 shows that
different aspects of the food are discussed. For
example ample and plentiful refer to the portion
size, fancy may refer to the presentation, and deli-
cious describes the flavors. This suggests that per-
haps the ontology would benefit from represent-
ing these sub-attributes of the food attribute, and
sub-attributes in general. Another problem with
consistency is that the same AP, e.g. very good
in Table 7 may appear with multiple ratings. For
example, very good is used for every foodquality
rating from 2 to 5. Thus some further automatic
or by-hand analysis is required to refine what is
learned before actual use in spoken language gen-
eration. Still, our method could reduce the amount
of time a system designer spends developing the
spoken language generator, and increase the natu-
ralness of spoken language generation.
Another issue is that the recall appears to be
quite low given that all of the sentences concern
the same domain: only 2.4% of the sentences
could be used to create the mappings. One way
to increase recall might be to automatically aug-
ment the list of distinguished attribute lexicaliza-
tions, using WordNet or work on automatic iden-
tification of synonyms, such as (Lin and Pantel,
2001). However, the method here has high pre-
cision, and automatic techniques may introduce
noise. A related issue is that the filters are in some
cases too strict. For example the contextual fil-
ter is based on POS-tags, so that sentences that do
not require the prior context for their interpreta-
tion are eliminated, such as sentences containing
subordinating conjunctions like because, when, if,
whose arguments are both given in the same sen-
tence (Prasad et al, 2005). In addition, recall is
affected by the domain ontology, and the automat-
ically constructed domain ontology from the re-
view webpages may not cover all of the domain.
In some review domains, the attributes that get
individual ratings are a limited subset of the do-
main ontology. Techniques for automatic feature
identification (Hu and Liu, 2005; Popescu and Et-
zioni, 2005) could possibly help here, although
these techniques currently have the limitation that
they do not automatically identify different lexi-
calizations of the same feature.
A different type of limitation is that dialogue
systems need to generate utterances for informa-
tion gathering whereas the mappings we obtained
271
can only be used for information presentation.
Thus these would have to be constructed by hand,
as in current practice, or perhaps other types of
corpora or resources could be utilized. In addi-
tion, the utility of syntactic structures in the map-
pings should be further examined, especially given
the failures in DSyntS conversion. An alternative
would be to leave some sentences unparsed and
use them as templates with hybrid generation tech-
niques (White and Caldwell, 1998). Finally, while
we believe that this technique will apply across do-
mains, it would be useful to test it on domains such
as movie reviews or product reviews, which have
more complex domain ontologies.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This work was supported by a
Royal Society Wolfson award to Marilyn Walker
and a research collaboration grant from NTT to
the Cognitive Systems Group at the University of
Sheffield.
References
Regina Barzilay and Lillian Lee. 2002. Bootstrapping lex-
ical choice via multiple-sequence alignment. In Proc.
EMNLP, pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. HLT/NAACL, pages 16?23.
Regina Barzilay and Kathleen McKeown. 2001. Extracting
paraphrases from a parallel corpus. In Proc. 39th ACL,
pages 50?57.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva,
and Valentin Tablan. 2002. GATE: A framework and
graphical development environment for robust NLP tools
and applications. In Proc. 40th ACL.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication). The
MIT Press.
Julia Hirschberg and Diane. J. Litman. 1987. Now let?s talk
about NOW: Identifying cue phrases intonationally. In
Proc. 25th ACL, pages 163?171.
Minqing Hu and Bing Liu. 2005. Mining and summarizing
customer reviews. In Proc. KDD, pages 168?177.
Alistair Knott. 1996. A Data-Driven Methodology for Moti-
vating a Set of Coherence Relations. Ph.D. thesis, Univer-
sity of Edinburgh, Edinburgh.
Benoit Lavoie and Owen Rambow. 1997. A fast and portable
realizer for text generation systems. In Proc. 5th Applied
NLP, pages 265?268.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Systems.
Johanna D. Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, comparative
descriptions in spoken dialogue. In Proc. 7th FLAIR.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with re-
spect to rating scales. In Proc. 43st ACL, pages 115?124.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Proc.
HLT/EMNLP, pages 339?346.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, and Bonnie Webber. 2005. The Penn
Discourse TreeBank as a resource for natural language
generation. In Proc. Corpus Linguistics Workshop on Us-
ing Corpora for NLG.
Stephanie Seneff and Joseph Polifroni. 2000. Formal and
natural language generation in the mercury conversational
system. In Proc. ICSLP, volume 2, pages 767?770.
Marie?t Theune. 2003. From monologue to dialogue: natural
language generation in OVIS. In AAAI 2003 Spring Sym-
posium on Natural Language Generation in Written and
Spoken Dialogue, pages 141?150.
Peter D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classification
of reviews. In Proc. 40th ACL, pages 417?424.
Marilyn Walker, Rashmi Prasad, and Amanda Stent. 2003.
A trainable generator for recommendations in multimodal
dialog. In Proc. Eurospeech, pages 1697?1700.
Michael White and Ted Caldwell. 1998. EXEMPLARS: A
practical, extensible framework for dynamic text genera-
tion. In Proc. INLG, pages 266?275.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opinion
clauses. In Proc. AAAI, pages 761?769.
Appendix
Adjectival phrases (APs) in single scalar-valued
relation mappings for atmosphere, value, and
overall.
atmosphere=2 eclectic, unique and pleasant
atmosphere=3 busy, pleasant but extremely hot
atmosphere=4 fantastic, great, quite nice and simple,
typical, very casual, very trendy, wonder-
ful
atmosphere=5 beautiful, comfortable, excellent, great,
interior, lovely, mellow, nice, nice and
comfortable, phenomenal, pleasant, quite
pleasant, unbelievably beautiful, very
comfortable, very cozy, very friendly,
very intimate, very nice, very nice and
relaxing, very pleasant, very relaxing,
warm and contemporary, warm and very
comfortable, wonderful
value=3 very reasonable
value=4 great, pretty good, reasonable, very good
value=5 best, extremely reasonable, good, great,
reasonable, totally reasonable, very good,
very reasonable
overall=1 just bad, nice, thoroughly humiliating
overall=2 great, really bad
overall=3 bad, decent, great, interesting, really
fancy
overall=4 excellent, good, great, just great, never
busy, not very busy, outstanding, recom-
mended, wonderful
overall=5 amazing, awesome, capacious, delight-
ful, extremely pleasant, fantastic, good,
great, local, marvelous, neat, new, over-
all, overwhelmingly pleasant, pampering,
peaceful but idyllic, really cool, really
great, really neat, really nice, special,
tasty, truly great, ultimate, unique and en-
joyable, very enjoyable, very excellent,
very good, very nice, very wonderful,
warm and friendly, wonderful
272
Annotation and Data Mining of the Penn Discourse TreeBank
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104 USA
rjprasad@linc.cis.upenn.edu
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@linc.cis.upenn.edu
Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
joshi@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
The Penn Discourse TreeBank (PDTB) is a new re-
source built on top of the Penn Wall Street Journal
corpus, in which discourse connectives are anno-
tated along with their arguments. Its use of stand-
off annotation allows integration with a stand-off
version of the Penn TreeBank (syntactic structure)
and PropBank (verbs and their arguments), which
adds value for both linguistic discovery and dis-
course modeling. Here we describe the PDTB and
some experiments in linguistic discovery based on
the PDTB alone, as well as on the linked PTB and
PDTB corpora.
1 Introduction
Large scale annotated corpora such as the Penn
TreeBank (Marcus et al, 1993) have played a cen-
tral role in speech and natural language research.
However, with the demand for more powerful NLP
applications comes a need for greater richness in
annotation ? hence, the development of PropBank
(Kingsbury and Palmer, 2002), which adds basic se-
mantics to the PTB in the form of verb predicate-
argument annotation and eventually similar annota-
tion of nominalizations. We have been developing
yet another annotation layer above these both. The
Penn Discourse TreeBank (PDTB) adds low-level
discourse structure and semantics through the anno-
tation of discourse connectives and their arguments,
using connective-specific semantic role labels. With
this added knowledge, the PDTB (together with the
PTB and PropBank) should support more in-depth
NLP research and more powerful applications.
Work on the PDTB is grounded in a lexical-
ized approach to discourse ? DLTAG (Webber and
Joshi, 1998; Webber et al, 1999a; Webber et al,
2000; Webber et al, 2003). Here, low-level dis-
course structure and semantics are taken to re-
sult (in part) from composing elementary predicate-
argument relations whose predicates come mainly
from discourse connectives1 and whose arguments
1Despite this, we have deliberately adopted a policy of hav-
come from units of discourse ? clausal, sentential
or multi-sentential units. The PDTB therefore dif-
fers from the RST-annotated corpus (Carlson et al,
2003) which starts with (abstract) rhetorical rela-
tions (Mann and Thompson, 1988) and annotates a
subset of the Penn WSJ corpus with those relations
that can be taken to hold between (primarily) pairs
of discourse spans identified in the corpus.
The current paper focuses on what can be dis-
covered through analyzing PDTB annotation, both
on its own and together with the Penn TreeBank.
Section 2 of the paper briefly reviews the theo-
retical background of the project, its current state,
the guidelines given to annotators, the annotation
tool they used (WordFreak), and the extent of inter-
annotator agreement. Section 3 shows how we have
used PDTB annotation, along with the PTB, to ex-
tract several features pertaining to discourse con-
nectives and their arguments, and discusses the rel-
evance of these features for NLP research and ap-
plications. Section 4 concludes with the summary.
2 Project overview
2.1 Theoretical background
The PDTB project builds on basic ideas presented
in Webber and Joshi (1998), Webber et al (1999b)
and Webber et al (2003) ? that connectives are
discourse-level predicates which project predicate-
argument structure on a par with verbs at the sen-
tence level. Webber and Joshi (1998) propose a
tree-adjoining grammar for discourse (DLTAG) in
which compositional aspects of discourse meaning
are formally defined, thus teasing apart composi-
tional from non-compositional layers of meaning.
In this framework, connectives are grouped into nat-
ural classes depending on the structure that they
project at the discourse level. Subordinate and coor-
dinating conjunctions, for example, require two ar-
ing the annotations independent of the DLTAG structural de-
scriptions for two reasons: (1) to make the annotated cor-
pus useful to researchers working in different frameworks and
(2) to simplify the annotators? task, thereby increasing inter-
annotator reliability.
guments that can be identified structurally from ad-
jacent units of discourse. What Webber et al (2003)
call anaphoric connectives (discourse adverbials,
such as otherwise, instead, furthermore, etc.) also
require two arguments ? one derived structurally,
and the other derived anaphorically from the pre-
ceding discourse. The crucial contribution of this
framework to the design of PDTB is what can be
seen as a bottom-up approach to discourse structure.
Specifically, instead of appealing to an abstract (and
arbitrary) set of discourse relations whose identifi-
cation may confound multiple sources of discourse
meaning, we start with the annotation of discourse
connectives and their arguments, thus exposing a
clearly defined level of discourse representation.
2.2 Project description
The PTDB project began in November 2002. The
first phase, including pilot annotations and prelim-
inary development of guidelines, was completed in
May 2003, and we expect to release the PDTB by
November 2005. Intermediate versions of the an-
notated corpus will be made available for feedback
from the community.
The PDTB corpus will include annotations of
four types of connectives: subordinating and co-
ordinating conjunctions, adverbial connectives and
implicit connectives. The final number of annota-
tions will amount to approximately 30K: 20K anno-
tations of the 250 types explicit connectives identi-
fied in the corpus and 10K annotations of implicit
connectives. The final version of the corpus will
also characterize the semantic role of each argu-
ment.
To date, we have annotated 10 explicit connec-
tives (therefore, as a result, instead, otherwise, nev-
ertheless, because, although, even though, when, so
that), amounting to a total of 2717 annotations, as
well as 386 tokens of implicit connectives. Anno-
tations have been performed by two to four annota-
tors.
2.3 Annotation guidelines
The annotation guidelines for PDTB have
been revised considerably since the pilot
phase of the project in May 2003. The cur-
rent version of the guidelines is available at
http://www.cis.upenn.edu/   pdtb. Below
we outline basic points from the guidelines.
What counts as a discourse connective? We
count as discourse connectives (1) all subordinat-
ing and coordinating conjunctions, (2) all discourse
adverbials, and (3) all inter-sentential implicit con-
nectives. Discourse adverbials include only those
adverbials which convey relationships between two
abstract objects such as events, states, propositions,
etc. (Asher, 1993). For instance, in Example 1, as
a result conveys a cause-effect relation between the
event of limiting the size of industries and that of
industries operating out of small, expensive, and in-
efficient units. In contrast, the semantic interpreta-
tion of the clausal adverbial strangely in Example 2
only requires a single event/state which it classifies
in the set of strange events/states.2
(1) [In the past, the socialist policies of the govern-
ment strictly limited the size of new steel mills,
petrochemical plants, car factories and other in-
dustrial concerns to conserve resources and re-
strict the profits businessmen could make]. As
a result, [industry operated out of small, expen-
sive, highly inefficient industrial units].
(2) Strangely, conventional wisdom inside the Belt-
way regards these transfer payments as ?uncon-
trollable? or ?nondiscretionary.?
Implicit connectives are taken to occur between
adjacent sentences not related by any explicit con-
nective. They are annotated with whatever explicit
connective the annotator feels could be inserted,
with the original meaning retained. Assessment of
inter-annotator agreement groups these annotations
into five coarse classes (Miltsakaki et al, 2004).
Currently, we are not annotating implicit connec-
tives intra-sententially (such as between a main
clause and a free adjunct) or across paragraphs.
What counts as a legal argument? The sim-
plest argument to a connective is what we take to
be the minimum unit of discourse. Because we
take discourse relations to hold between abstract
objects, we require that an argument contain at least
one clause-level predication (usually a verb ? tensed
or untensed), though it may span as much as a se-
quence of clauses or sentences. The two exceptions
are nominal phrases that express an event or a state,
and discourse deictics that denote an abstract ob-
ject.
What we describe to annotators as arguments to
discourse connectives are actually the textual span
from which the argument is derived (Webber et al,
1999a; Webber et al, 2003). This is especially clear
in the case of the first argument of instead in (3),
which does not actually include the negation, al-
though it is part of the selected text.3
2For a more detailed discussion of how discourse adver-
bials can be distinguished from clausal adverbials, see Forbes
(2003).
3For a corpus-based study of the arguments of instead, see
(Miltsakaki et al, 2003).
(3) [No price for the new shares has been set]. In-
stead, [the companies will leave it up to the mar-
ketplace to decide].
How far does an argument extend? One par-
ticularly significant addition to the guidelines came
as a result of differences among annotators as to
how large a span constituted the argument of a con-
nective. During pilot annotations, annotators used
three annotation tags: CONN for the connective
and ARG1 and ARG2 for the two arguments. To
this set, we have added two optional tags, SUP1
and SUP2 (supplementary), for cases when the an-
notator wants to mark textual spans s/he considers
to be useful, supplementary information for the in-
terpretation of an argument. Examples (4) and (5)
demonstrate its use. The arguments are shown in
square brackets, while spans providing supplemen-
tary information are shown in parentheses.4
(4) Although [started in 1965], [Wedtech didn?t re-
ally get rolling until 1975] (when Mr. Neuberger
discovered the Federal Government?s Section 8
minority business program).
(5) Because [mutual fund trades don?t take effect un-
til the market close] (? in this case, at 4 p.m. ?)
[these shareholders effectively stayed put].
2.4 Inter-Annotation Reliability
An extensive discussion of inter-annotator reliabil-
ity in the PDTB is presented in (Miltsakaki et al,
2004). The three things that are relevant to the dis-
cussion here of using the PDTB for linguistic dis-
covery are (1) the agreement criterion, (2) the level
of inter-annotator agreement, and (3) the types of
inter-annotator variation.
With respect to agreement, we did not use the
kappa statistic (Siegel and Castellan, 1988) because
it requires the data tokens to be classified into dis-
crete categories and PDTB annotation involves se-
lecting a span of text whose length is not prescribed
a priori.5 Instead of kappa, we assessed inter-
annotator agreement using an exact match crite-
rion: for any ARG1 or ARG2 token, agreement was
recorded as 1 when both annotators made identical
4SUP annotations have not been used in the current
experiments.
5Carlson et al (2003) avoid this by using two sets of cat-
egories: one set in which there is a separate category for each
span that could constitute an elementary discourse unit, and one
set in which there is only a separate category for each span that
at least one annotator has selected. Because the arguments of
connectives tend to be longer and hence more variable than the
elementary spans used in the RST-corpus, we do not see any
gain from introducing the first set of categories, and the second
set is equivalent to our exact match criterion.
textual selections for the annotation and 0 when the
annotators made non-identical selections.
Treating ARG1 and ARG2 annotations as inde-
pendent tokens for assessment, the total number of
inter-annotator judgments assessed for explicit con-
nectives was twice the number of connective tokens,
i.e, 5434. In this measure, we achieved a high-level
of agreement on the arguments to subordinate con-
junctions (92.4%), while lower agreement on ad-
verbials (71.8%).6 This difference between the two
types is not surprising, since locating the anaphoric
(ARG1) argument of adverbial connectives is be-
lieved to be a harder task than that of locating the
arguments of subordinating conjunctions. For ex-
ample, the anaphoric argument of the adverbial con-
nectives may be located in some non-adjacent span
of text, even several paragraphs away.
A detailed analysis of inter-annotator variation
shows that most of the disagreements (79%) in-
volved Partial Overlap ? that is, text that is com-
mon to what is selected separately by each annota-
tor. Partial overlap subsumes categories such as (a)
higher verb, where one of the annotators included
some extra clausal material that contained a higher
governing predicate, (b) dependent clause, where
one of the annotators included extra clausal mate-
rial which was syntactically dependent on the clause
selected by both, and (c) parenthetical, where one
of the annotators included text that occurred in the
middle of the other annotator?s selection. Example 6
illustrates a case of higher verb disagreement.
(6) a. [he knew the RDF was neither rapid nor de-
ployable nor a force] ? even though [it cost
$8 billion or $10 billion a year].
b. he knew [the RDF was neither rapid nor de-
ployable nor a force] ? even though [it cost
$8 billion or $10 billion a year].
The partial overlap disagreements are important
with respect to the experiments described in the next
section, because most of this variation turns out to
be irrelevant to the experiments. We will elaborate
on this further in the next section.
3 Data Mining
PDTB annotation indicates two things: the argu-
ments of each explicit discourse connective and the
lexical tokens that actually play a role as discourse
connectives. It should be clear that the former
6In Miltsakaki et al (2004), we have reported on the anno-
tation of implicit connectives as well. We achieved 72% agree-
ment on the use of explicit expressions in place of the implicit
connectives. More details on the implicit connective annotation
can be found in this work.
cannot be derived automatically from existing re-
sources, since determining the size and location of
the arguments is not simply a matter of sentential
syntax or verb predicate argument relations. But
the latter is also a non-trivial feature because every
lexical item that functions as a discourse connective
also has a range of other functions. While some of
these functions correlate with POS-tags other than
those used in annotating connectives, the PTB POS-
tags themselves cannot always be reliably distin-
guished, given inconsistencies in how the lexical
items are analyzed.
We believe that the PDTB annotation can con-
tribute to a range of linguistic discovery and lan-
guage modeling tasks, such as
 providing empirical evidence for the DLTAG
claim that discourse adverbials get one argu-
ment anaphorically, while structural connec-
tives such as conjunctions establish relations
between adjacent units of text (Creswell et al,
2002).
 acquiring common usage patterns of connec-
tives and identifying their dependencies, in or-
der to support ?natural? choices in Natural
Language Generation (di Eugenio et al, 1997;
Moser and Moore, 1995; Williams and Reiter,
2003).
 developing decision procedures for resolving
and interpreting discourse adverbials (Milt-
sakaki et al, 2003) which can be built on top of
discourse parsing systems (Forbes et al, 2003).
 developing ?word sense disambiguation? pro-
cedures for distinguishing among different
senses of a connective and hence interpret-
ing connectives correctly (e.g., distinguishing
between temporal and explanatory since, be-
tween hypothetical and counterfactual if, be-
tween epistemic and semantic because, etc.)
 providing empirical evidence for theories of
anaphoric phenomena such as verb phrase el-
lipsis that see them as sensitive to the type of
discourse relation in which they are expressed
(Hardt and Romero, 2002; Kehler, 2002).
The value of carrying out such studies using a sin-
gle corpus with multiple layers of annotation is that
relationships between phenomena are clearer. (The
downside is focusing on a single genre ? newspa-
per text ? and a particular ?house style? ? that of
the Wall Street Journal. However, developing the
PDTB may help facilitate the production of more
such corpora, through an initial pass of automatic
annotation, followed by manual correction, much
as was done in developing the PTB (Marcus et al,
1993).)
Here we present some preliminary experiments
we have carried out on the current version of the
PDTB. We automatically extracted features asso-
ciated with discourse connectives and their argu-
ments, both from the PDTB annotation alone as well
as from the integrated annotation of the PDTB and
PTB. The findings reveal novel patterns regarding
the location and size of the arguments of discourse
connectives and suggest additional experiments.
The multi-layered annotations for PDTB, PTB
(and soon to be available PropBank) are rendered in
XML within a ?stand-off? annotation architecture
in which multiple (independently conducted) anno-
tations refer to the same primary document. Word-
Freak directly renders the PDTB annotations in the
stand-off XML representation, but for the syntactic
layer, the PTB phrase structure constituent annota-
tions had to first be converted to the XML stand-off
representation.7
For preparing the connective tokens for data min-
ing, we started with the 2717 annotations for the
10 explicit connectives reported in Section 2.2 and
extracted those tokens on which we achieved full
?exact match? agreement as well as ?partial over-
lap? agreement on both the arguments (cf. Sec-
tion 2.4). We felt justified in combining both sets
because ?partial overlap? disagreements, which oc-
curred mostly within sentences, did not make any
overall difference to the features that were extracted.
The total number of tokens we obtained from this
was 2688. 51 tokens on this set had to be thrown out
since the official release of the Penn TreeBank did
not have the corresponding syntactic annotations for
these tokens.8 From the remaining 2637 tokens, we
extracted two sets of features, one for adverbials
(229 tokens) and the other for subordinating con-
junctions (2408 tokens).
For the adverbials, we wanted to determine
whether the results reported in earlier work
(Creswell et al, 2002) held up. Among other
things, this work examined whether (1) anaphoric
arguments could be reliably annotated, to facili-
tate the development of robust anaphora resolu-
tion algorithms, and (2) there were differences be-
7Thanks to Jeremy Lacivita for implementing the represen-
tation of PTB in stand-off XML form. The stand-off represen-
tation of PTB will be released together with the PDTB corpus.
8Researchers who are currently conducting or are planning
to conduct multi-layered annotations or experiments with the
Penn TreeBank should be aware that the official release con-
tains more source and PoS-tagged files than the parsed files.
Future annotations of the PDTB will only be performed on texts
that are parsed.
tween the type, size and location of the arguments
of anaphoric (adverbial) connectives and those of
structural connectives.
The high inter-annotator agreement reported in
this earlier study has now been confirmed by the
PDTB annotation (cf. Section 2.4). As for the other,
we automatically extracted some of the same fea-
tures that were hand-annotated in Creswell et al
(2002) to determine the distribution of these con-
nectives with respect to their position (POS) and
the size and location (LOC) of their anaphoric argu-
ments. These features are further described below:
POS: pertains to the position of the connective in
its host argument, i.e., the argument in which it oc-
curs.9 POS can take three defined values: INIT for
argument-initial position (Examples 7-9), MED for
argument-medial position (Examples 10-11), and
FINAL for argument-final position (Examples 12
and 13). Note that the host argument of the con-
nective is a sentence in Example 8 and 9, a VP con-
junct in Example 7, a free adjunct in Example 10,
the main clause of a sentence in Example 11, a sub-
ordinate clause in Example 12, and finally, the first
of the two coordinated sentences in Example 13.
LOC: pertains to the size and location of the
anaphoric argument of the connective. LOC can
take four defined values: SS for when the anaphoric
argument occurs in the same sentence as the con-
nective (Examples 7, 10 and 11), PS for when the
argument occurs in the immediately previous sen-
tence (Examples 12 and 13), PP for when the argu-
ment occurs in the immediately preceding sequence
of sentences (Example 8), and NC for when the ar-
gument occurs in some non-contiguous sentence(s)
(Example 9). A sentence is defined as minimally
a main clause and all of its attached subordinate
clauses, if any. Coordinated main clauses, by this
definition, are treated as separate sentences. Note
that according to the definition of the LOC feature,
the anaphoric argument may constitute the entire
sentence(s), as in Examples 8, 9 and 13, or it may be
part of the sentence(s), as in Examples 7 and 10-12.
An important aspect of the LOC feature is that
it involved the multi-layering of PDTB and PTB,
since the PDTB itself contains no information about
syntactic constituency or even sentence boundaries.
For deriving the LOC feature values, we needed in-
formation not only about the sentence boundaries
of texts, but also about coordinated clause bound-
aries, which requires accessing sentence-internal
constituents.
9We achieved 94.1% agreement on the host argument
(ARG2) annotations.
(7) INIT-SS: Rep. John LaFalce (D., N.Y.) said Mr.
Johnson refused [to testify jointly with Mr. Mul-
ford] and instead [asked to appear after the Trea-
sury official had completed his testimony].
(8) INIT-PP: [But Mr. Treybig questions whether
that will be enough to stop Tandem?s first main-
frame from taking on some of the functions that
large organizations previously sought from Big
Blue?s machines. ?The answer isn?t price re-
ductions, but new systems,? he said]. Never-
theless, [Tandem faces a variety of challenges,
the biggest being that customers generally view
the company?s computers as complementary to
IBM?s mainframes].
(9) INIT-NC: [For years, costume jewelry makers
fought a losing battle]. Jewelry displays in de-
partment stores were often cluttered and unin-
spired. And the merchandise was, well, fake.
As a result, [marketers of faux gems steadily lost
space in department stores to more fashionable
rivals ? cosmetics makers].
(10) MED-SS: Investors usually don?t want [to take
physical delivery of a contract], [preferring in-
stead to profit from its price swings and then end
any obligation to take delivery or make delivery
as it nears expiration].
(11) MED-SS: Although [bond prices weren?t as
volatile on Tuesday trading as stock prices],
[traders nevertheless said action also was much
slower yesterday in the Treasury market].
(12) FIN-PS: Buyers can look forward to double-
digit annual returns if [they are right]. But they
will have disappointing returns or even losses if
[interest rates rise] instead.
(13) FIN-PS: [Tons of delectably rotting potatoes,
barley and wheat will fill damp barns across the
land as thousands of farmers turn the state?s buy-
ers away]. [Many a piglet won?t be born] as a re-
sult, and many a ham will never hang in a butcher
shop.
The distribution of the POS feature values across
the different connectives, given in Table 1, shows
that the connectives in this set occurred predomi-
nantly in the initial position of their host argument.
The question of whether or not these different po-
sitions correlate with any aspect of the informa-
tion structure of the arguments (Forbes et al, 2003;
Kruijff-Korbayova? and Webber, 2001) is, however,
an open one and will need to be explored further
with the PDTB annotations.
INIT MED FIN TOTAL
201 (87.8%) 13 (5.7%) 15 (6.5%) 229
Table 1: Distribution of the Position (POS) of Dis-
course Adverbials
CONN SS PS PP NC Total
nevertheless 3 (9.7%) 17 (54.8%) 3 (9.7%) 8 (25.8%) 31
otherwise 2 (11.1%) 14 (77.8%) 1 (5.6%) 1 (5.6%) 18
as a result 3 (4.8%) 44 (69.8%) 5 (7.9%) 12 (19%) 63
therefore 11 (55%) 7 (35%) 1 (5%) 1 (5%) 20
instead 22 (22.7%) 62 (63.9%) 2 (2.1%) 11 (11.3%) 97
TOTAL 41 (17.9%) 144 (62.9%) 12 (5.2%) 33 (14.4%) 229
Table 2: Distribution for Location (LOC) of Anaphoric Argument of Adverbial Connectives
The distribution of the LOC values across the dif-
ferent connectives is shown in Table 2. We first look
at all the connectives taken together (i.e., the final
TOTAL row) and focus on differences in LOC and
what such differences suggest.
The first thing that is evident from the TOTAL
row in Table 2 is the significant proportion of ARG1
tokens that occur in a position non-adjacent to the
discourse adverbial (NC = 14.4%). This accords
with the results in (Creswell et al, 2002), in terms
of providing evidence that discourse adverbials (un-
like structural connectives) are not getting both their
arguments from structurally defined positions.
The second point that is evident from the TOTAL
row is the significant proportion of ARG1 tokens
in SS location. This includes instances of ARG1
in complement clauses (Example 7), subordinate
clauses (Example 11), relative clauses (both restric-
tive and non-restrictive, as in Example 14), pre-
ceding VP conjuncts (Example 15), and from main
clauses, where the adverbial is attached to a free ad-
junct, as in Example 16.
(14) [  The British pound], [pressured by last week?s
resignations of key Thatcher administration of-
ficials], nevertheless [  rose Monday to $1.5820
from Friday?s $1.5795].10
(15) Appealing to a young audience, [he scraps an
old reference to Ozzie and Harriet] and instead
[quotes the Grateful Dead].
(16) [The transmogrified brokers never let the C-word
cross their lips], instead [stressing such terms as
?safe,? ?insured? and ?guaranteed?].
While one might want to argue that the latter is
no different from adjacent full clauses and hence
should be treated the same as a location in the pre-
vious sentence (i.e., LOC=PS), the other SS cases
provide additional evidence for an anaphoric anal-
ysis of these discourse adverbials since there al-
ready exists a separate structural relation in each
case. Furthermore, in Example 7, the arguments of
the conjunction and, though not yet addressed by
our annotators, differ from the arguments of instead.
10The subscripts on the bracketed spans in this example indi-
cate discontinuous parts of the host argument of nevertheless.
Any attempt to treat instead as a structural connec-
tive will produce a syntactic analysis with crossing
branches ? a source of both theoretical and practical
(parsing) problems (Forbes et al, 2003).
Turning now to the individual analysis of adver-
bials, Table 2 shows that the 4 connectives other
than therefore pattern rather similarly with respect
to the location of the anaphoric argument (SS,
PS, PP, NC). All of them except therefore have
their antecedent predominantly in the previous sen-
tence (between 54.8% and 77.8%). The question
is whether the difference in how therefore patterns
? i.e., drawing its antecedent 55% of the time from
the same sentence ? is simply a consequence of hav-
ing such few data points (i.e., only 20) or a matter of
?house style? (with all the examples from the Wall
Street Journal) or a difference that is theoretically
motivated. If the answer lies in house style or the-
ory, then it is relevant to work in natural language
generation. Further annotation and analysis of ad-
verbials and their arguments in the PDTB will pro-
vide more information as to this puzzle.
At the start of this section, we indicated five dif-
ferent areas in which PDTB annotation could con-
tribute to linguistic discovery and language model-
ing. This data mining experiment illustrates the first
three, as well as providing information relevant to
further development of discourse parsing systems
and natural language generation systems. For fu-
ture work, we intend to explore further the extrac-
tion and study of other features related to discourse
adverbials. Two features that we are currently work-
ing to extract automatically pertain to (a) the co-
occurrence of discourse adverbials with other con-
nectives in the host argument, and (b) the syntac-
tic type and depth of the anaphoric arguments, such
as whether the argument was a finite or non-finite
complement clause, a relative clause, or a finite or
non-finite subordinate clause etc.
For the subordinating conjunctions (Table 3), we
extracted features pertaining to the relative position
of the two arguments of the conjunction. Subordi-
nating conjunctions often take their arguments in
the same sentence with the subordinate clause as
one argument and the main clause as its other ar-
gument. However, the subordinate clause can either
occur to the right of the main clause, i.e., postposed,
as in Example 17, or it can occur preposed, i.e., be-
fore the main clause, as in Example 18.
(17) ARG1-ARG2: But Sen. McCain says [Mr.
Keating broke off their friendship abruptly in
1987], because [the senator refused to press the
thrift executive?s case as vigorously as Mr. Keat-
ing wanted].
(18) ARG2-ARG1: Because [Swiss and EC insurers
are widely present on each other?s markets], [the
accord isn?t expected to substantially increase
near-term competition].
The distribution of the relative position of the
arguments of these connectives, given in Table 3,
shows significant differences across the connec-
tives.
CONN ARG1-ARG2 ARG2-ARG1 Total
when 545 (54%) 465 (46%) 1010
because 822 (90%) 93 (10%) 915
even though 77 (75%) 26 (25%) 103
although 129 (37%) 218 (63%) 347
so that 33 (100%) 0 (0%) 33
Total 1606 (67%) 802 (33%) 2408
Table 3: Distribution for Argument order for Subor-
dinating Conjunctions
There are a few interesting things to note here.
First, even if one considers only the four subordi-
nating conjunctions with  100 tokens, no two of
them pattern in the same way.
Second, with when, the almost equal distribution
of preposed and postposed tokens suggests either
free variation of the two patterns or different uses
of the two patterns, with each use favoring a differ-
ent pattern. The latter would accord with a theo-
retical distinction that has been made between post-
posed when expressing a purely temporal relation
between the two clauses, and preposed when ex-
pressing a contingent relation between them (Moens
and Steedman, 1988). Integrated evidence from the
PTB and PropBank may help distinguish the two
possibilities.
Third, there is a striking contrast between the pat-
terning of although and even though, especially if
one assumes that even though (like even when, even
after, even if, etc.) involves application of the topi-
calizer even to the subordinate clause, just as it can
apply to other constituents. Further annotation and
analysis of the PDTB will reveal whether all subor-
dinating conjunctions that co-occur with even pat-
tern like even though, or whether this is specific to
the concessive.
Finally, when Williams and Reiter (2003) exam-
ined 342 texts from the RST annotation of the Penn
TreeBank corpus (Carlson et al, 2003), they re-
ported that 77% of the instances of concessive re-
lations that they examined appeared in the order
ARG2-ARG1. (The eleven instances of although
that they examined and the three instances of even
though appeared in concessive relations, along with
instances of but, despite, however, etc.) If we were
to collapse together all instances of although and
even though annotated in the PDTB (totalling 450),
we would find that 46% (206) patterned as ARG1-
ARG2, and 54% of them (244) patterned as ARG2-
ARG1. This might lead us to draw a similar con-
clusion to Williams and Reiter (2003). But it would
also disguise the fact noted above that although and
even though pattern oppositely to one another. This
suggests (1) that making the feature extraction pro-
cedure specific to particular connectives, as in the
PDTB, will reveal distributional patterns that are
lost when more abstract relations are the focus of the
annotation, and (2) that a larger set of annotated to-
kens can show more reliable distributional patterns.
In sum, data mining of PDTB with respect to sub-
ordinating conjunctions has shown radically differ-
ent distribution patterns regarding the relative po-
sition of the arguments. Some of these have con-
firmed and strengthened previous theoretical claims
and some have suggested new and promising re-
search directions. Further work in this area will also
be extremely relevant for NLG sentence planning
components employing discourse relations (Walker
et al (2003), Stent et al (2004), among others),
where the sentence planner needs to make decisions
regarding cue placement. Finally, while our ap-
proach is ?syntactic?, with the distribution of the
connectives and their arguments being explored in
terms of whether they are subordinating conjunc-
tions, coordinating conjunctions, or adverbial con-
nectives, one can also explore the patterning of
connectives in terms of semantic categories, once
their semantic role annotation is complete (cf. Sec-
tion 2.2). The latter could be especially interesting
to cross-linguistic studies of discourse, as well as
to applications such as multilingual generation and
MT are envisaged.11
4 Summary
In this paper we have presented the Penn Dis-
course TreeBank (PDTB), a large-scale discourse-
level annotated corpus that is being developed to-
wards the creation of a multi-layered annotated cor-
pus, integrating the Penn TreeBank, PropBank and
11We thank an anonymous reviewer for pointing this out.
the PDTB. The PDTB encodes low-level discourse
structure information, marking discourse connec-
tives as indicators of discourse relations, and their
arguments. We have reported high inter-annotator
agreement for the PDTB annotation. Our data min-
ing experience and preliminary results show that the
multi-layered corpora is a rich source of information
that can be exploited towards the development of
powerful and efficient natural language understand-
ing and generation systems as well as towards large-
scale corpus-based research.
Acknowledgments
We are very grateful to Tom Morton and Jeremy
Lacivita for the development and modification of
the WordFreak annotation tool. Special thanks to
Jeremy for providing continuous technical support.
Thanks are also due to our annotators, Cassandre
Creswell, Driya Amandita, John Laury, Emily Paw-
ley, Alan Lee, Alex Derenzy and Steve Pettington.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a Discourse-Tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Jan van Kuppevelt and Ronnie Smith, edi-
tors, Current Directions in Discourse and Dialogue.
Kluwer Academic Publishers.
Cassandre Creswell, Katherine Forbes, Eleni Miltsakaki,
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2002. The Discourse Anaphoric Properties of Con-
nectives. In Proceedings of DAARC2002. Edic?o?es
Colibri.
Barbara di Eugenio, Johanna D. Moore, and Massimo
Paolucci. 1997. Learning Features that Predict Cue
Usage. In Proceedings of ACL/EACL 97.
Kate Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop
Sarkar, Aravind Joshi, and Bonnie Webber. 2003. D-
LTAG System: Discourse Parsing with a Lexicalized
Tree-Adjoining Grammar. Journal of Logic, Lan-
guage and Information, 12(3):261?279.
Kate Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics,
University of Pennsylvania.
Dan Hardt and Maribel Romero. 2002. Ellipsis and the
Structure of Discourse. In Proceedings of Sinn und
Bedeutung VI.
Andrew Kehler. 2002. Coherence, Reference and the
Theory of Grammar. CSLI Publications.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of LREC-02.
Ivana Kruijff-Korbayova? and Bonnie Webber. 2001. In-
formation Structure and the Semantics of ?otherwise?.
In Proceedings of ESSLLI 2001: Workshop on Infor-
mation Structure, Discourse Structure and Discourse
Semantics.
William Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory. Toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Eleni Miltsakaki, Cassandre Creswell, Kate Forbes, Ar-
avind Joshi, and Bonnie Webber. 2003. Anaphoric
Arguments of Discourse Connectives: Semantic
Properties of Antecedents versus Non-Antecedents.
In Proceedings of the Computational Treatment of
Anaphora Workshop, EACL 2003.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating Discourse Con-
nectives and their Arguments. In Proceedings of the
NAACL/HLT Workshop: Frontiers in Corpus Annota-
tion.
Marc Moens and Mark Steedman. 1988. Temporal On-
tology and Temporal Reference. Computational Lin-
guistics, 14(2):15?28.
Megan G. Moser and Johanna Moore. 1995. Investi-
gating Cue Selection and Placement in Tutorial Dis-
course. In Proceedings of ACL95.
Sidney Siegel and N. J. Castellan. 1988. Nonparama-
teric Statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of ACL-2004.
Marilyn Walker, Rashmi Prasad, and Amanda Stent.
2003. A Trainable Generator for Recommendations
in Multimodal Dialogue. In Eurospeech, 2003.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
Lexicalized Tree-Adjoining Grammar for Discourse.
In ACL/COLING Workshop on Discourse Relations
and Discourse Markers, Montreal.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999a. Discourse Relations: A Struc-
tural and Presuppositional Account Using Lexicalized
TAG. In Proceedings of ACL-99.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999b. What are Little Texts Made of? A
Structural and Presuppositional Account Using Lex-
icalized TAG. In Proceedings of the International
Workshop on Levels of Representation in Discourse
(LORID ?99).
Bonnie Webber, Alistair Knott, and Aravind Joshi.
2000. Multiple Discourse Connectives in a Lexi-
calized Grammar for Discourse. In Proceedings of
IWCS-00.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and Discourse Struc-
ture. Computational Linguistics, 29:545?587.
Sandra Williams and Ehud Reiter. 2003. A Corpus
Analysis of Discourse Relations for Natural Language
Generation. In Proceedings of Corpus Linguistics.
Annotating Discourse Connectives And Their Arguments
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@linc.cis.upenn.edu
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104 USA
rjprasad@linc.cis.upenn.edu
Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
joshi@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
This paper describes a new, large scale
discourse-level annotation project ? the Penn
Discourse TreeBank (PDTB). We present an
approach to annotating a level of discourse
structure that is based on identifying discourse
connectives and their arguments. The PDTB
is being built directly on top of the Penn Tree-
Bank and Propbank, thus supporting the extrac-
tion of useful syntactic and semantic features
and providing a richer substrate for the devel-
opment and evaluation of practical algorithms.
We provide a detailed preliminary analysis of
inter-annotator agreement ? both the level of
agreement and the types of inter-annotator vari-
ation.
1 Introduction
Large scale annotated corpora have played a critical role
in speech and natural language research. The Penn Tree-
Bank (PTB) is an example of such a resource with world-
wide impact on natural language processing (Marcus et
al., 1993). However, the PTB deals with text only at
the sentence level: with the demand for more power-
ful NLP applications comes a need for greater richness
in annotation. At the sentence level, Penn Propbank
is adding predicate-argument annotation to sentences in
PTB (Kingsbury and Palmer, 2002). At the discourse-
level are efforts to produce corpora annotated with rhetor-
ical relations (Carlson et al, 2003). This paper describes
a more basic discourse-level annotation project ? the
Penn Discourse TreeBank (PDTB) ? that aims to produce
a large-scale corpus in which discourse connectives are
annotated, along with their arguments.
There have been several approaches to describing dis-
course in terms of discourse relations (Mann and Thomp-
son, 1988; Asher and Lascarides, 1998; Polanyi and
van den Berg, 1996). In these approaches, the additional
meaning the discourse contributes beyond the sentence
derives from discourse relations. Specification of the dis-
course relations for a discourse thus constitutes a descrip-
tion of a certain level of discourse structure.
Rather than starting from (abstract) discourse rela-
tions, we describe an approach to annotating a large-
scale corpus in terms of a more basic characterisation
of discourse structure in terms of discourse connectives
and their arguments. The motivation for such an ap-
proach stems from work by Webber and Joshi (1998),
Webber et al (1999a), Webber et al (2000) which inte-
grates sentence level structures with discourse level struc-
ture (using tree-adjoining grammars for both cases, LTAG
and DLTAG, respectively).1 This allows structural com-
position and its associated semantic composition at the
sentence level to be smoothly carried over to the dis-
course level, a goal also shared by Gardent (1997),
Schilder (1997) and Polanyi and van den Berg (1996),
among others.2
Discourse connectives and their arguments can be suc-
cessfully annotated with high reliability (cf. Section
4). This is not surprising, given that the task resem-
bles that of annotating verbs and their arguments at
the sentence level (Kingsbury and Palmer, 2002). In
fact, we use a fine-grained, lexically grounded annota-
tion in which argument labels are specific to the dis-
1In the PDTB annotations, we have deliberately adopted
a policy to make the annotations independent of the DLTAG
framework for two reasons: (1) to make the annotated corpus
widely useful to researchers working in different frameworks
and (2) to make the annotators? task easier, thereby increasing
interannotator reliability.
2However, the approaches in Gardent (1997),
Schilder (1997), and Polanyi and van den Berg (1996) are
different in two ways: a) the process by which discourse
derives compositional aspects of meaning is considered entirely
separate from how clauses do so, and b) only two mechanisms
are used for deriving discourse semantics ? compositional
semantics and inference.
course connectives involved, in much the same way as
in Kingsbury and Palmer (2002). In contrast, a recent
attempt (Carlson et al, 2003) at using RST-type rela-
tions for annotating a much smaller corpus has already
revealed difficulties involved in reliably annotating more
abstract discourse relations. Moreover, this type of anno-
tation does not contain any record of the basis on which
a relation was assigned.
The paper is organized as follows. Section 2 provides
a brief overview of the fundamental ideas that provide
the basis for the design of the PDTB annotation. Section
3 gives a detailed description of the annotation project,
including information about the size of the corpus, com-
pleted annotations as well as annotation instructions as
formulated in the guidelines. Section 4 presents data
analysis based on current annotations as well as results
from inter-annotator agreement. Section 5 wraps up with
a summary of the work.
2 Theoretical background
The annotation project presented in this paper builds
on basic ideas presented in Webber and Joshi (1998),
Webber et al (1999b) and Webber et al (2003) ? that
connectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at the
sentence level. Webber and Joshi (1998) propose a tree-
adjoining grammar for discourse (DLTAG) in which
compositional aspects of discourse meaning are for-
mally defined, thus teasing apart compositional from non-
compositional layers of meaning. In this framework, con-
nectives are grouped into natural classes depending on the
structure that they project at the discourse level. Subordi-
nate and coordinating conjunctions, for example, require
two arguments that can be identified structurally from ad-
jacent units of discourse. What Webber et al (2003) call
anaphoric discourse connectives (some, but not all, dis-
course adverbials, such as ?otherwise?, ?instead?, ?fur-
thermore?, etc.) also require two arguments, but only one
of them derives structurally. For the complete interpreta-
tion of these connectives, their other argument needs to
be recovered. The crucial contribution of this framework
to the design of the current project is what can be seen
as a bottom-up approach to discourse structure. Specifi-
cally, instead of appealing to an abstract (and arbitrary)
set of discourse relations whose identification involves
confounding multiple sources of discourse meaning, we
start with the annotation of discourse connectives and
their arguments, thus exposing a clearly defined level of
discourse representation.
3 Project description
The PTDB project began in November 2002. The first
phase, including pilot annotations and preliminary devel-
opment of guidelines, was completed in May 2003. The
PDTB is expected to be released by November 2005. In-
termediate versions of the annotated corpus will be made
available for receiving feedback.
The PDTB corpus will include annotations of four
types of connectives: subordinating conjunctions, coor-
dinating conjunctions, adverbial connectives and implicit
connectives. We specify each of these types in more de-
tail in Section 3.1. The final number of annotations in
the corpus will amount to approximately 30,000; 10,000
implicit connectives and 20,000 annotations of the 250
explicit connectives identified in the corpus. The final
version of the corpus will also contain characterizations
of the semantic roles associated with the arguments of
each type of connective.
In this paper we present the results of annotating 10
explicit connectives, amounting to a total of 2717 anno-
tations, as well as 386 tokens of implicit connectives. The
set of 10 connectives comprises the adverbial connectives
?therefore?, ?as a result?, ?instead?, ?otherwise?, ?never-
theless?, and the subordinate conjunctions ?because?, ?al-
though?, ?even though?, ?when?, and ?so that?. In all
cases, annotations have been performed by four annota-
tors. While this slows down the annotation process con-
siderably, the nature, significance and magnitude of the
project as well as the well-known complexity of discourse
annotation tasks impels us to strive for maximum relia-
bility, achieved by having the task performed by multiple
annotators.3
Individual annotation proceeds one connective at a
time. The annotation tool WordFreak4 is used to iden-
tify all instances of the given connective in the corpus,
and these are then annotated independently and manu-
ally by four annotators. This way, the annotators quickly
gain experience with that connective and develop a better
understanding of its predicate-argument characteristics.
Similarly, for the annotation of implicit connectives, all
instances (as specified in the guidelines, see Section 3.2)
are identified one file at a time. For this task, the anno-
tators are required to read the entire file so that they can
make well-informed and reliable decisions about the im-
plicit connectives and their arguments. In addition, after
the arguments of each implicit connective have been iden-
tified, the annotators provide, if possible, an explicit con-
nective (or other suitable expression) that best expresses
the inferred relation. As with explicit connectives, anno-
tations of implicit connectives are done by four annota-
3When inter-annotator consistency has stabilized, we intend
to reduce the number of annotators to three, or maybe two at the
minimum.
4WordFreak was developed by Tom Morton at the University
of Pennsylvania. It has been substantially modified by Jeremy
Lacivita to fit the needs of the PDTB project. A snapshot of the
tool can be seen at http://www.cis.upenn.edu/?pdtb.
tors.
Compared with Propbank?s annotation of verb
predicate-argument structures, annotation of arguments
of discourse predicates is different in interesting ways.
Propbank annotators have to determine the number of ar-
guments required by each verb. In contrast, discourse
connectives exhibit a clear predicate-argument structure
requiring only two arguments. The main challenge we
have discovered for annotating discourse connectives is
determining the extent of their arguments. Even subor-
dinate conjunctions whose arguments never cross a sen-
tence boundary may sometimes be the source of disagree-
ment between annotators.
In what follows, we present a brief overview of the
classes of connectives that we annotate, followed by
highlights of the annotation manual and relevant corpus
examples.
3.1 Discourse connectives
We classify discourse connectives into four classes: sub-
ordinate and coordinating conjunctions, adverbials and
implicit connectives. Examples of each type are given be-
low, with their arguments shown in square brackets and
the connectives, in italics.
3.1.1 Subordinate conjunctions
Subordinate conjunctions introduce clauses that are
syntactically dependent on a main clause. The most com-
mon types of relations that they express are temporal
(e.g., ?when?, ?as soon as?), causal e.g., ?because?), con-
cessive (e.g., ?although?, ?even though?), purpose (e.g.,
?so that?, ?in order that?) and conditional (e.g., ?if?, ?un-
less). Clauses introduced with a subordinate conjunction
may be preposed (or, more rarely, interposed) with re-
spect to the main clause, as shown in (1).
(1) Because [the drought reduced U.S. stockpiles], [they
have more than enough storage space for their new
crop], and that permits them to wait for prices to rise.
3.1.2 Coordinating conjunctions
Coordinating conjunctions are ones such as ?and?,
?but?, and ?or?. Example (2) shows the annotation of an
instance of the conjunction ?and?.
(2) [William Gates and Paul Allen in 1975 developed
an early language-housekeeper system for PCs], and
[Gates became an industry billionaire six years after
IBM adapted one of these versions in 1981].
Instances of coordinating conjunctions which coordi-
nate nominal or other non-clausal constituents are ex-
cluded from annotation. We also exclude cases of VP-
coordination because in such cases the arguments of the
connective can be retrieved automatically from the syn-
tactic layer.
3.1.3 Adverbial connectives
Adverbial connectives are sentence-modifying adverbs
which express a discourse relation (Forbes, 2003). The
class of adverbial connectives includes ?however?, ?there-
fore?, ?then?, ?otherwise?, etc. In this class, we have also
included prepositional phrases with a similar sentence
modifying function such as ?as a result?, ?in addition?,
?in fact?, etc. Example (3) shows the annotation of an
instance of the adverbial connective ?as a result?.
(3) ...[many analysts expected energy prices to rise at the
consumer level too]. As a result, [many economists
were expecting the consumer price index to increase
significantly more than it did].
The arguments of adverbial connectives may or may
not be adjacent to the sentence containing the connective.
In a few cases, an argument may be found one or two
paragraphs away from the connective.
3.1.4 Implicit connectives
Implicit connectives are identified between adjacent
sentences with no explicit connectives.5 The annotation
of implicit connectives is intended to capture the connec-
tion between two sentences appearing in adjacent posi-
tions. For example, in (4), the two adjacent sentences
are connected in a way similar to having the explicit con-
nective ?but? contrasting them. Indeed, for implicit con-
nectives, annotators are asked to provide, when possible,
an explicit connective that best describes the inferred re-
lation. The explicit connective provided in (4) was ?in
contrast?.
(4) ...[The $6 billion that some 40 companies are looking to
raise in the year ending March 31 compares with only
$2.7 billion raised on the capital market in the previous
fiscal year]. IMPLICIT-(In contrast) [In fiscal 1984 be-
fore Mr. Gandhi came to power, only $810 million was
raised].
3.2 Annotation guidelines
The annotation guidelines for PDTB have been revised
considerably since the pilot phase of the project in May
2003. The current version of the guidelines is available at
http://www.cis.upenn.edu/?pdtb. Below we out-
line the basic points.
3.2.1 What counts as a discourse connective?
We count as discourse connectives (1) all subordinat-
ing and coordinating conjunctions, (2) certain adverbials,
and (3) implicit connectives. The adverbials include only
those which convey a relation between events or states.
For example, in (5) ?as a result? conveys a cause-effect re-
lation between the event of limiting the size of new steel
5There are, of course, other implicit connectives that we are
not taking into account.
mills and that of the industry operating out of small, ex-
pensive and highly inefficient units. In contrast, the se-
mantic interpretation of ?strangely? in (6) only requires a
single event/state which it classifies in the set of strange
events/states.6
(5) [In the past, the socialist policies of the government
strictly limited the size of new steel mills, petrochem-
ical plants, car factories and other industrial concerns to
conserve resources and restrict the profits businessmen
could make]. As a result, industry operated out of small,
expensive, highly inefficient industrial units.
(6) Strangely, conventional wisdom inside the Beltway re-
gards these transfer payments as ?uncontrollable? or
?nondiscretionary.?
The guidelines also highlight instances of lexical items
with multiple functions, only one of which is as a dis-
course connective. For example, ?when? can either serve
as a subordinate conjunction or introduce a relative clause
modifying a nominal phrase, as in (7), where the when-
clause modifies the nominal ?1985?.7Here we again ben-
efit from building discourse annotation on top of Penn
TreeBank because the syntactic annotation of when-
clauses distinguishes the two functions: When-relatives
are marked as NP-modifiers adjoining to an NP, whereas
adverbial when-clauses adjoin to a sentential node.
(7) Attorneys have argued since 1985, when the law took
effect.
Similarly, some since-clauses function as NP modifiers
as shown in (8). In such cases, ?since? is not annotated as
a connective. As in the case of when-clauses, instances of
NP modifying since-clauses can be identified in the Penn
TreeBank by virtue of their syntactic annotation.
(8) In the decade since the communist nation emerged from
isolation, its burgeoning trade with the West has lifted
Hong Kong?s status as a regional business partner.
Finally, implicit connectives count as connectives.
They are identified between adjacent sentences which do
not contain any other explicit connectives. Currently, we
are not annotating implicit connectives intra-sententially,
such as between the matrix clause and free adjunct in Ex-
ample (9). We plan to incorporate annotations of implicit
intra-sentential connectives at a later stage of the project.
(9) Second, they channel monthly mortgage payments into
semiannual payments, reducing the administrative bur-
den on investors.
6For a more detailed discussion of the basis for distin-
guishing discourse adverbials from clausal adverbials, see
Forbes (2003).
7In cases of when-relatives, a when-clause can be annotated
as SUP (see Section 3.2.3).
3.2.2 What counts as a legal argument?
Because we take discourse relations to hold between
abstract objects, we require that an argument contains at
least one predicate along with its arguments. Of course,
a sequence of clauses or sentences may also form a legal
argument, containing multiple predicates.
Because our annotations are done directly on top of
the Penn TreeBank, annotators may select as an argument
certain textual spans that appear to exclude one or more
arguments of the predicate. These are cases in which
these arguments are directly retrievable from the syntac-
tic annotation. Thus, we are able to select only the pred-
icates that are required for the interpretation of the dis-
course connective and simultaneously access their argu-
ments for the complete interpretation of the clause while
keeping the annotations of single arguments simple and
maximally contiguous. In (10), for example, the relative
clause is marked as one of the two arguments of the con-
nective ?even though?. The subject of the verb in the rela-
tive clause is directly retrievable from the Penn TreeBank
annotation. Similarly, in (11) the subject of the infinitival
clause is also available from the syntactic representation.
(10) Workers described ?clouds of dust? [that hung over
parts of the factory] even though [exhaust fans venti-
lated the air].
(11) The average maturity for funds open only to institutions,
considered by some [to be a stronger indicator] because
[those managers watch the market closely], reached a
high point for the year ? 33 days.
There are two exceptions to the requirement that an
argument include a verb ? these are nominal phrases that
express an event or a state, and discourse deictics that
denote an event or state. In (12), for example, the nominal
phrase ?fainting spells? can be marked as a legal argument
of the connective ?when? because the phrase expresses an
event of fainting.
(12) Its symptoms include a cold sweat at the sound of de-
bate, clammy hands in the face of congressional crit-
icism, and [fainting spells] when [someone writes the
word ?controversy.?]
Discourse deictic expressions are forms such as ?this?
and ?that? that can be used to denote the interpretation
of clausal textual spans from the preceding discourse.
In (13), for example, ?that? denotes the interpretation of
the sentence immediately preceding it. Our annotators
are guided to make argument selections that assume that
anaphoric and deictic expressions have been resolved.
Thus, in (13), they are able to select ?That?s? as one ar-
gument of the connective ?because?.
(13) Airline stocks typically sell at a discount of about one-
third to the stock market?s price-earnings ratio ? which
is currently about 13 times earnings. [That?s] because
[airline earnings, like those of auto makers, have been
subject to the cyclical ups-and-downs of the economy].
The annotators are also informed that in some cases,
an argument of a connective must be derived from the
selected textual span (Webber et al, 1999a; Webber et al,
2003). This is the case for the first argument of ?instead?
in (14), which does not include the negation, although it
is contained in the selected text.8
(14) [No price for the new shares has been set]. Instead, [the
companies will leave it up to the marketplace to decide].
In sum, legal arguments can be groups of sentences,
single sentences (a main clause and its subordinate
clauses), single clauses (tensed or non-tensed), NPs that
specify events or situations, and discourse deictic expres-
sions.
3.2.3 How far does an argument extend?
One particularly significant addition to the guidelines
came as a result of differences among annotators as to
how large a span constituted the argument of a connec-
tive. During pilot annotations, annotators used three an-
notation tags: CONN for the connective and ARG1 and
ARG2 for the two arguments. To this set, we have added
the optional tags SUP1, SUP2 (supplementary) for cases
when the annotator wants to mark textual spans s/he con-
siders to be useful, supplementary information for the
interpretation an argument. Example (15) demonstrates
the use of SUP1. Arguments are shown in square brack-
ets, while spans providing supplementary information are
shown in parentheses.
(15) Although [started in 1965], [Wedtech didn?t really get
rolling until 1975] (when Mr. Neuberger discovered the
Federal Government?s Section 8 minority business pro-
gram).
4 Data analysis
To test the reliability of the annotation, we first con-
sidered the kappa statistic (Siegel and Castellan, 1988)
which is used extensively in empirical studies of dis-
course (Carletta, 1996). The kappa coefficient provides
an inter-annotator agreement figure for any number of an-
notators by measuring pairwise agreement between them
and by correcting for chance expected agreement. How-
ever, the statistic requires the data tokens to be classified
into discrete categories, and as a result, we could not ap-
ply it to our data since the PDTB annotation tokens can-
not be classified as such. Rather, annotation in the PDTB
constitutes either selection of a span of text for the ar-
guments of connectives which can be of indeterminate
length or providing explicit expressions for implicit con-
nectives from an open-ended class of expressions.
8For a preliminary corpus-based analysis of the arguments
of ?instead?, see Miltsakaki et al (2003).
Instead, we have assessed inter-annotator agreement in
terms of agreement/disagreement on span or named ex-
pression identity for each token as a percentage of the
pairs of spans or expressions that actually matched ver-
sus those that should have. For the argument annotations,
we use a most conservative measure - the exact match
criterion. In addition, we also used different diagnostics
for the argument annotations for the explicit connectives,
reporting percentage agreement on different classes of to-
kens, such as those in which the first argument (ARG1)
annotations and second argument (ARG2) annotations
were counted independently, as well as those in which the
ARG1 and ARG2 annotations (for each connective) were
counted together as a single token. For all the argument
annotations, the computation of agreement excluded the
supplementary annotations (cf. Section 3.2.3).
We present here agreement results on ARG1 and
ARG2 annotations by two annotators for the annotation
of ten explicit connectives, amounting to a total of 2717
annotations, and 368 annotations of implicit connectives,
including agreement results on the explicit expression the
annotators used in in place of the implicit connectives as
well as the ARG1 and ARG2 annotations of the implicit
connectives.9 The ten explicit connectives include 5 sub-
ordinating conjunctions (when, because, even though, al-
though, and so that) and 5 adverbials (nevertheless, oth-
erwise, instead, therefore, and as a result).
4.1 Inter-annotator Agreement
4.1.1 Explicit connectives
For the explicit connective annotations, we used two
diagnostics for measuring inter-annotator agreement. In
the first diagnostic , we took the class of tokens as the to-
tal number of argument annotations, treating ARG1 and
ARG2 annotations as independent tokens. The total num-
ber of tokens in this class is therefore twice the number
of connective tokens, i.e, 5434. We recorded agreement
using the exact match criterion. That is, for any ARG1
or ARG2 token, agreement was recorded as 1 when both
annotators made identical textual selections for the an-
notation and 0 when the annotators made non-identical
selections.
We achieved 90.2% agreement (4900/5434 tokens)
on the annotations for this class. Agreement on only
ARG1 tokens was 86.3%, and agreement on only ARG2
tokens was 94.1%. Further distribution of the agree-
ments by connective is given in Table 1. Connectives
are grouped in the table by type (subordinating conjunc-
tion (SUBCONJ) and adverbial (ADV)). The second col-
9Right now SUP1 and SUP2 annotations are for our use only
and are not included in the current evaluations. Additional an-
notations by another 2 annotators are currently underway. The
2 annotators of the explicit connectives are different from the 2
annotators of the implicit connectives.
umn gives the number of agreeing tokens for each con-
nective and the third column gives the total number of
(ARG1+ARG2) tokens available for that connective. The
last column gives the percent agreement for the connec-
tive in that row, i.e., as a percentage of tokens for which
agreement was 1 (column 2) versus the total number of
tokens for that connective (column 3).
CONNECTIVES AGR No. Conn. Total %AGR
when 1877 2032 92.4%
because 1703 1824 93.4%
even though 194 206 94.1%
although 635 704 90.1%
so that 66 74 89.2%
TOTAL SUBCONJ 4469 4834 92.4%
nevertheless 56 94 59.6%
otherwise 44 46 95.7%
instead 172 236 72.9%
as a result 110 168 65.5%
therefore 49 56 87.5%
TOTAL ADV. 431 600 71.8%
OVERALL TOTAL 4900 5434 90.2%
Table 1: Distribution of Agreement by Connective, with
ARG1 and ARG2 Annotations Counted Independently
The table shows that we achieved high agreement
on argument annotations of subordinating conjunctions
(92.4%). Average agreement on the adverbials was lower
(71.8%). This difference between the two types is not sur-
prising, since locating the anaphoric (ARG1) argument of
adverbial connectives is believed to be a harder task than
that of locating the arguments of subordinating conjunc-
tions. For example, the anaphoric argument of the ad-
verbial connectives may be located in some non-adjacent
span of text, even several paragraphs away. Arguments of
subordinating conjunctions, on the other hand, can most
often be found in spans of text adjacent to the connective.
The table also shows that there was uniform agreement
across the different subordinating conjunctions (roughly
90%), whereas the adverbials showed more variation.
In particular, agreement on otherwise and therefore was
high (95.7% and 87.5% respectively), while lower for
the other three adverbials, instead (72.9%), as a result
(65.5%), and nevertheless (59.6%). This suggests either
greater variability in how these adverbials are interpreted
or greater complexity in their interpretation, which results
in more variability when people are forced to associate an
interpretation with a particular text span.
We also computed agreement using a second more
conservative diagnostic in which we took the class of to-
kens as the total number of connective tokens (2717) so
that the ARG1 and ARG2 annotations for each connec-
tive were treated together as part of the same token. Here
again, we recorded agreement using the exact match mea-
sure. That is, for any connective token, agreement was
recorded as 1 when both annotators made identical tex-
tual selections for the annotation of both arguments and
0 when the annotators made non-identical selections for
any one or both arguments.
We achieved 82.8% agreement (2249/2717 tokens) on
the annotations for this class. Table 2 gives the distribu-
tion of the agreements by connective. The table shows
relatively lower agreements when compared with the first
diagnostic, for both subordinating conjunctions (86%) as
well as adverbials (57%). However, this difference is un-
derstandable since the token class as defined for this di-
agnostic yields a stricter measure of agreement.
CONNECTIVES AGR No. Conn. Total %AGR
when 868 1016 86.4%
because 804 912 88.2%
even though 91 103 88.3%
although 288 352 81.8%
so that 27 34 79.4%
TOTAL SUBCONJ 2078 2417 86.0%
nevertheless 18 47 38.3%
otherwise 21 23 91.3%
instead 72 118 61.0%
as a result 38 84 45.2%
therefore 22 28 78.6%
TOTAL ADV. 171 300 57.0%
OVERALL TOTAL 2249 2717 82.8%
Table 2: Distribution of Agreement by Connective, with
ARG1 and ARG2 Annotations Counted Together
We classified disagreements into 4 major types. The
result of classifying the 534 disagreements from Diag-
nostic 1 (Table 1) is given in Table 3. The third column
gives the percent of the total disagreements for each type.
DISAGREEMENT TYPE No. %
Missing Annotations 72 13.5%
No Overlap 30 5.6%
Partial Overlap
Parentheticals 53 9.9%
higher verb 181 33.9%
dependent clause 182 34.1%
Other 6 1.1%
Unresolved 10 1.9%
TOTAL 534 100%
Table 3: Disagreement Classification
The majority of disagreements (79%) were due to
Partial Overlap, which subsumes the categories Higher
Verb, Dependent Clause, Parenthetical and Other. Par-
tial Overlap means that there was partial overlap in the
annotations selected by the two annotators. Higher verb
includes tokens where one of the annotators included the
governing predicate for the clause marked by both anno-
tators. The higher clause occurred on the left or right pe-
riphery of the lower clause. Dependent Clause includes
tokens where one of the annotators included extra clausal
material that is syntactically dependent on the clause that
was selected by both, and that occurs on the left or right
periphery of the common text. Parenthetical means
that one of the annotators included a medial parentheti-
cal, while the other did not. The intervening text could be
the main as well as the dependent clause. An example is
provided below:
(16) Bankers said [warrants for Hong Kong stocks are at-
tractive] because [they give foreign investors], wary of
volatility in the colony?s stock market, [an opportunity
to buy shares without taking too great a risk].
(17) Bankers said [warrants for Hong Kong stocks are at-
tractive] because [they give foreign investors, wary of
volatility in the colony?s stock market, an opportunity
to buy shares without taking too great a risk].
Other included tokens with partial overlap between an-
notations, but in addition included a combination of more
than type, such as higher verb+dependent clause.
Note that disagreements that contain a partial over-
lap could be counted as agreeing tokens if we relaxed
the more conservative exact match measure to a partial
match measure. Our subjective view was that in several
cases, the ?extra? textual material, especially those fit-
ting the dependent clause and parenthetical category did
not make any significant semantic contribution in terms
of their inclusion or exclusion in the argument. With the
partial match measure, excluding these cases reduces the
disagreements to half the given number, giving us 94.5%
agreement overall.
The No Overlap tokens were cases of true disagree-
ment in that there was no overlap in the annotations se-
lected by the annotators. These tokens constituted 5.6%
of the disagreements. Examples (18) and (19) shows the
two annotations for a token in which there was no over-
lap in the ARG1 annotation. Missing Annotations also
constituted a substantial proportion of the disagreements
(13.5%) and was used for tokens where the annotation
was missing for one annotator. Note that these don?t re-
ally count as disagreement, since all connectives are pre-
theoretically assumed to require two arguments. Unre-
solved includes tokens which have introduced new issues
for the annotation guidelines and cannot be resolved at
this time. These include issues such as how to treat com-
paratives, certain types of adjunct clauses, certain types
of nominalizations etc.
(18) [The word ?death? cannot be escaped entirely by the
industry], but salesmen dodge it wherever possible or
cloak it in euphemisms, [preferring to talk about ?sav-
ings? and ?investment?] instead.
(19) The word ?death? cannot be escaped entirely by the
industry, but salesmen dodge it wherever possible or
[cloak it in euphemisms], preferring [to talk about ?sav-
ings? and ?investment?] instead.
4.1.2 Implicit connectives
For the 386 tokens of implicit connectives, we ana-
lyzed inter-annotator agreement between two annotators
for (a) the explicit connectives they provided in place of
an implicit connective, and (b) the argument annotations
of the implicit connectives.
As a preliminary step in analyzing agreement on the
type of explicit connective provided by the annotators in
place of an implicit connective, we considered 5 groups
of connectives conveying : a) additional information
(e.g., ?furthermore?, ?in addition?) b) cause-effect rela-
tions (e.g., ?because?, ?as a result?), c) temporal relations
(e.g., ?then?, ?simultaneously?), d) contrastive relations
(e.g., ?however?, ?although?), and e) restatement or sum-
marization (e.g., ?in other words?, ?in sum?). 10 Agree-
ment was then computed on these basic groups of con-
nectives.11 From the total of 386 tokens of implicit con-
nectives, 9 were excluded from the analysis due to tech-
nical error (missing annotation). For the remaining 307
tokens, we achieved 72% agreement on the type of ex-
plicit connective that best conveyed the interpretation of
the implicit connective.
For the argument annotations of the implicit connec-
tives, we present agreement results from using the first
diagnostic used for the explicit connectives. That is, we
counted ARG1 and ARG2 annotations as independent to-
kens and computed percent agreement using the exact
match criterion. On the 772 ARG1 and ARG2 tokens,
we achieved 85.1% (657/772) agreement between 2 an-
notators. The analysis of the 115 disagreements is given
in Table 4. Note that here again, the number of disagree-
ments reduces to half using the partial match measure for
the parenthetical and dependent clause classes, giving us
92.6% agreement overall.
DISAGREEMENT TYPE No. %
Missing Annotations 6 5.2%
No Overlap 2 1.7%
Partial Overlap
parenthetical 13 11.3%
higher verb 24 20.9%
dependent clause 44 38.3%
sentence 19 16.5%
other 3 2.6%
Unresolved 4 3.5%
TOTAL 115 100%
Table 4: Disagreement Classification for Implicit Con-
nective ARG Annotations
10These groups are based on types of coherence relations de-
rived from corpus-based distributions of connectives presented
in (Knott, 1996). Initially, we also considered a group of con-
nectives expressing hypothetical relations but no such connec-
tives were identified in the annotations.
11Some polysemous connectives such as ?while? and ?in fact?
appeared in more than one group.
5 Summary
In this paper we presented a new and innovative
discourse-level annotation project, the Penn Discourse
TreeBank (PDTB), in which discourse connectives and
their arguments are annotated, thereby defining a clear
level of discourse structure that can be reliably annotated
for a large corpus. Our inter-annotator results confirm our
expectations of high agreement and annotation reliability.
At a later stage of the project, we plan to provide seman-
tic characterizations of the arguments of connectives and
resolve any cases of polysemy that might arise.
Acknowledgments
We are very grateful to Tom Morton and Jeremy Lacivita
for the development and special modification of the
WordFreak annotation tool. Special thanks to Jeremy for
providing continuous technical support. Thanks are also
due to our annotators, Cassie Creswell, Driya Amandita,
John Laury, Emily Pawley, Alan Lee, Alex Derenzy and
Steve Pettington. Also, many thanks to Katherine Forbes
Riley and Jean Carletta for their comments and sugges-
tions. Finally, we would like to thank the reviewers for
their very useful comments. This work was partially sup-
ported by NSF Grant EIA 02-24417.
References
Nicholas Asher and Alex Lascarides. 1998. The seman-
tics and pragmatics of presupposition. Journal of Se-
mantics, 15(3):239?300.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks. Computational Linguistics, 22:249?254.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski,
2003. Current Directions in Discourse and Dialogue,
chapter Building a Discourse-Tagged Corpus in the
Framework of Rhetorical Structure Theory. Kluwer
Academic Publishers.
Kate Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics,
University of Pennsylvania.
Claire Gardent. 1997. Discourse tree adjoining gram-
mars. Claus 89, University of the Saarlandes, Saar-
brucken.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Third International Confer-
ence on Language Resources and Evaluation, LREC-
02, Las Palmas, Canary Islands, Spain.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. thesis,
University of Edinburgh.
William Mann and Sandra Thompson. 1988. Rhetori-
cal structure theory. toward a functional theory of text
organization. Text, 8(3):243?281.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: the Penn Treebank. Computational
Linguistics, 19:313?330.
Eleni Miltsakaki, Cassandre Creswell, Kate Forbes, Ar-
avind Joshi, and Bonnie Webber. 2003. Anaphoric
arguments of discourse connectives: Semantic prop-
erties of antecedents versus non-antecedents. In Pro-
ceedings of the Computational Treatment of Anaphora
Workshop, EACL 2003, Budapest.
Livia Polanyi and Martin van den Berg. 1996. Discourse
structure and discourse interpretation. In Proceedings
of the Tenth Amsterdam Colloquium, University of Am-
sterdam, pages 113?131.
Frank Schilder. 1997. Discourse tree grammar or how
to get attached to a discourse? In Proceedings of the
the second International Workshop on Computational
Semantics (IWCS-II), Tilburg, The Netherlands, pages
261?273.
Sidney Siegel and N. J. Castellan. 1988. Nonparama-
teric Statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
lexicalized tree adjoining grammar for discourse. In
ACL/COLING Workshop on Discourse Relations and
Discourse Markers, Montreal, pages 8?92. Montreal,
Canada.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999a. Discourse relations: A struc-
tural and presuppositional account using lexicalized
TAG. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, Mary-
land, pages 41?48. College Park MD.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999b. What are little texts made of? A
structural and presuppositional account using lexical-
ized TAG. In Proceedings of the International Work-
shop on Levels of Representation in Discourse (LORID
?99), Edinburgh, pages 145?149.
Bonnie Webber, Alistair Knott, and Aravind Joshi. 2000.
Multiple discourse connectives in a lexicalized gram-
mar for discourse. In Proceedings of the Third Interna-
tional Workshop on Computational Semantics, Tilburg,
The Netherlands.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29?36,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Attribution and the (Non-)Alignment of Syntactic and Discourse Arguments
of Connectives
Nikhil Dinesh and Alan Lee and Eleni Miltsakaki and Rashmi Prasad and Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
fnikhild,aleewk,elenimi,rjprasad,joshig@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
The annotations of the Penn Discourse
Treebank (PDTB) include (1) discourse
connectives and their arguments, and (2)
attribution of each argument of each con-
nective and of the relation it denotes. Be-
cause the PDTB covers the same text as
the Penn TreeBank WSJ corpus, syntac-
tic and discourse annotation can be com-
pared. This has revealed significant dif-
ferences between syntactic structure and
discourse structure, in terms of the argu-
ments of connectives, due in large part to
attribution. We describe these differences,
an algorithm for detecting them, and fi-
nally some experimental results. These re-
sults have implications for automating dis-
course annotation based on syntactic an-
notation.
1 Introduction
The overall goal of the Penn Discourse Treebank
(PDTB) is to annotate the million word WSJ cor-
pus in the Penn TreeBank (Marcus et al, 1993) with
a layer of discourse annotations. A preliminary re-
port on this project was presented at the 2004 work-
shop on Frontiers in Corpus Annotation (Miltsakaki
et al, 2004a), where we described our annotation
of discourse connectives (both explicit and implicit)
along with their (clausal) arguments.
Further work done since then includes the an-
notation of attribution: that is, who has expressed
each argument to a discourse connective (the writer
or some other speaker or author) and who has ex-
pressed the discourse relation itself. These ascrip-
tions need not be the same. Of particular interest is
the fact that attribution may or may not play a role
in the relation established by a connective. This may
lead to a lack of congruence between arguments at
the syntactic and the discourse levels. The issue of
congruence is of interest both from the perspective
of annotation (where it means that, even within a
single sentence, one cannot merely transfer the an-
notation of syntactic arguments of a subordinate or
coordinate conjunction to its discourse arguments),
and from the perspective of inferences that these an-
notations will support in future applications of the
PDTB.
The paper is organized as follows. We give a brief
overview of the annotation of connectives and their
arguments in the PDTB in Section 2. In Section 3,
we describe the annotation of the attribution of the
arguments of a connective and the relation it con-
veys. In Sections 4 and 5, we describe mismatches
that arise between the discourse arguments of a con-
nective and the syntactic annotation as provided by
the Penn TreeBank (PTB), in the cases where all the
arguments of the connective are in the same sen-
tence. In Section 6, we will discuss some implica-
tions of these issues for the theory and practice of
discourse annotation and their relevance even at the
level of sentence-bound annotation.
2 Overview of the PDTB
The PDTB builds on the DLTAG approach to dis-
course structure (Webber and Joshi, 1998; Webber
et al, 1999; Webber et al, 2003) in which con-
nectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at
29
the sentence level. Initial work on the PDTB has
been described in Miltsakaki et al (2004a), Milt-
sakaki et al (2004b), Prasad et al (2004).
The key contribution of the PDTB design frame-
work is its bottom-up approach to discourse struc-
ture: Instead of appealing to an abstract (and arbi-
trary) set of discourse relations whose identification
may confound multiple sources of discourse mean-
ing, we start with the annotation of discourse con-
nectives and their arguments, thus exposing a clearly
defined level of discourse representation.
The PDTB annotates as explicit discourse connec-
tives all subordinating conjunctions, coordinating
conjunctions and discourse adverbials. These pred-
icates establish relations between two abstract ob-
jects such as events, states and propositions (Asher,
1993).1
We use Conn to denote the connective, and Arg1
and Arg2 to denote the textual spans from which the
abstract object arguments are computed.2 In (1), the
subordinating conjunction since establishes a tem-
poral relation between the event of the earthquake
hitting and a state where no music is played by a
certain woman. In all the examples in this paper, as
in (1), Arg1 is italicized, Arg2 is in boldface, and
Conn is underlined.
(1) She hasn?t played any music since the earthquake
hit.
What counts as a legal argument? Since we take
discourse relations to hold between abstract objects,
we require that an argument contains at least one
clause-level predication (usually a verb ? tensed or
untensed), though it may span as much as a sequence
of clauses or sentences. The two exceptions are
nominal phrases that express an event or a state, and
discourse deictics that denote an abstract object.
1For example, discourse adverbials like as a result are dis-
tinguished from clausal adverbials like strangely which require
only a single abstract object (Forbes, 2003).
2Each connective has exactly two arguments. The argument
that appears in the clause syntactically associated with the con-
nective, we call Arg2. The other argument is called Arg1. Both
Arg1 and Arg2 can be in the same sentence, as is the case for
subordinating conjunctions (e.g., because). The linear order of
the arguments will be Arg2 Arg1 if the subordinate clause ap-
pears sentence initially; Arg1 Arg2 if the subordinate clause ap-
pears sentence finally; and undefined if it appears sentence me-
dially. For an adverbial connective like however, Arg1 is in the
prior discourse. Hence, the linear order of its arguments will be
Arg1 Arg2.
Because our annotation is on the same corpus as
the PTB, annotators may select as arguments textual
spans that omit content that can be recovered from
syntax. In (2), for example, the relative clause is
selected as Arg1 of even though, and its subject can
be recovered from its syntactic analysis in the PTB.
In (3), the subject of the infinitival clause in Arg1 is
similarly available.
(2) Workers described ?clouds of blue dust? that hung
over parts of the factory even though exhaust fans
ventilated the air.
(3) The average maturity for funds open only to institu-
tions, considered by some to be a stronger indicator
because those managers watch the market closely,
reached a high point for the year ? 33 days.
The PDTB also annotates implicit connectives be-
tween adjacent sentences where no explicit connec-
tive occurs. For example, in (4), the two sentences
are contrasted in a way similar to having an explicit
connective like but occurring between them. Anno-
tators are asked to provide, when possible, an ex-
plicit connective that best describes the relation, and
in this case in contrast was chosen.
(4) The $6 billion that some 40 companies are looking to
raise in the year ending March 21 compares with only
$2.7 billion raise on the capital market in the previous
year. IMPLICIT - in contrast In fiscal 1984, before
Mr. Gandhi came into power, only $810 million
was raised.
When complete, the PDTB will contain approxi-
mately 35K annotations: 15K annotations of the 100
explicit connectives identified in the corpus and 20K
annotations of implicit connectives.3
3 Annotation of attribution
Wiebe and her colleagues have pointed out the
importance of ascribing beliefs and assertions ex-
pressed in text to the agent(s) holding or making
them (Riloff and Wiebe, 2003; Wiebe et al, 2004;
Wiebe et al, 2005). They have also gone a consid-
erable way towards specifying how such subjective
material should be annotated (Wiebe, 2002). Since
we take discourse connectives to convey semantic
predicate-argument relations between abstract ob-
jects, one can distinguish a variety of cases depend-
ing on the attribution of the discourse relation or its
3The annotation guidelines for the PDTB are available at
http://www.cis.upenn.edu/pdtb.
30
arguments; that is, whether the relation or arguments
are ascribed to the author of the text or someone
other than the author.
Case 1: The relation and both arguments are at-
tributed to the same source. In (5), the concessive
relation between Arg1 and Arg2, anchored on the
connective even though is attributed to the speaker
Dick Mayer, because he is quoted as having said
it. Even where a connective and its arguments are
not included in a single quotation, the attribution can
still be marked explicitly as shown in (6), where only
Arg2 is quoted directly but both Arg1 and Arg2 can
be attibuted to Mr. Prideaux. Attribution to some
speaker can also be marked in reported speech as
shown in the annotation of so that in (7).
(5) ?Now, Philip Morris Kraft General Foods? parent
company is committed to the coffee business and to
increased advertising for Maxwell House,? says Dick
Mayer, president of the General Foods USA division.
?Even though brand loyalty is rather strong for cof-
fee, we need advertising to maintain and strengthen
it.?
(6) B.A.T isn?t predicting a postponement because the
units ?are quality businesses and we are en-
couraged by the breadth of inquiries,? said Mr.
Prideaux.
(7) Like other large Valley companies, Intel also noted
that it has factories in several parts of the nation,
so that a breakdown at one location shouldn?t leave
customers in a total pinch.
Wherever there is a clear indication that a relation
is attributed to someone other than the author of the
text, we annotate the relation with the feature value
SA for ?speaker attribution? which is the case for
(5), (6), and (7). The arguments in these examples
are given the feature value IN to indicate that they
?inherit? the attribution of the relation. If the rela-
tion and its arguments are attributed to the writer,
they are given the feature values WA and IN respec-
tively.
Relations are attributed to the writer of the text by
default. Such cases include many instances of re-
lations whose attribution is ambiguous between the
writer or some other speaker. In (8), for example,
we cannot tell if the relation anchored on although
is attributed to the spokeswoman or the author of the
text. As a default, we always take it to be attributed
to the writer.
Case 2: One or both arguments have a different at-
tribution value from the relation. While the default
value for the attribution of an argument is the attribu-
tion of its relation, it can differ as in (8). Here, as in-
dicated above, the relation is attributed to the writer
(annotated WA) by default, but Arg2 is attributed to
Delmed (annotated SA, for some speaker other than
the writer, and other than the one establishing the
relation).
(8) The current distribution arrangement ends in March
1990 , although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
Annotating the corpus with attribution is neces-
sary because in many cases the text containing the
source of attribution is located in a different sen-
tence. Such is the case for (5) where the relation
conveyed by even though, and its arguments are at-
tributed to Dick Mayer.
We are also adding attribution values to the anno-
tation of the implicit connectives. Implicit connec-
tives express relations that are inferred by the reader.
In such cases, the author intends for the reader to
infer a discourse relation. As with explicit connec-
tives, we have found it useful to distinguish implicit
relations intended by the writer of the article from
those intended by some other author or speaker. To
give an example, the implicit relation in (9) is at-
tributed to the writer. However, in (10) both Arg1
and Arg2 have been expressed by the speaker whose
speech is being quoted. In this case, the implicit re-
lation is attributed to the speaker.
(9) Investors in stock funds didn?t panic the week-
end after mid-October?s 190-point market plunge.
IMPLICIT-instead Most of those who left stock
funds simply switched into money market funds.
(10) ?People say they swim, and that may mean they?ve
been to the beach this year,? Fitness and Sports. ?It?s
hard to know if people are responding truthfully.
IMPLICIT-because People are too embarrassed to
say they haven?t done anything.?
The annotation of attribution is currently under-
way. The final version of the PDTB will include an-
notations of attribution for all the annotated connec-
tives and their arguments.
Note that in the Rhetorical Structure Theory
(RST) annotation scheme (Carlson et al, 2003), at-
tribution is treated as a discourse relation. We, on
the other hand, do not treat attribution as a discourse
31
relation. In PDTB, discourse relations (associated
with an explicit or implicit connective) hold between
two abstracts objects, such as events, states, etc. At-
tribution relates a proposition to an entity, not to an-
other proposition, event, etc. This is an important
difference between the two frameworks. One conse-
quence of this difference is briefly discussed in Foot-
note 4 in the next section.
4 Arguments of Subordinating
Conjunctions in the PTB
A natural question that arises with the annotation
of arguments of subordinating conjunctions (SUB-
CONJS) in the PDTB is to what extent they can be
detected directly from the syntactic annotation in the
PTB. In the simplest case, Arg2 of a SUBCONJ is its
complement in the syntactic representation. This is
indeed the case for (11), where since is analyzed as
a preposition in the PTB taking an S complement
which is Arg2 in the PDTB, as shown in Figure 1.
(11) Since the budget measures cash flow, a new $1 di-
rect loan is treated as a $1 expenditure.
Furthermore, in (11), since together with its com-
plement (Arg2) is analyzed as an SBAR which mod-
ifies the clause a new $1 direct loan is treated as a
$1 expenditure, and this clause is Arg1 in the PDTB.
Can the arguments always be detected in this
way? In this section, we present statistics showing
that this is not the case and an analysis that shows
that this lack of congruence between the PDTB and
the PTB is not just a matter of annotator disagree-
ment.
Consider example (12), where the PTB requires
annotators to include the verb of attribution said
and its subject Delmed in the complement of al-
though. But although as a discourse connective de-
nies the expectation that the supply of dialysis prod-
ucts will be discontinued when the distribution ar-
rangement ends. It does not convey the expectation
that Delmed will not say such things. On the other
hand, in (13), the contrast established by while is be-
tween the opinions of two entities i.e., advocates and
their opponents.4
4This distinction is hard to capture in an RST-based pars-
ing framework (Marcu, 2000). According to the RST-based an-
notation scheme (Carlson et al, 2003) ?although Delmed said?
and ?while opponents argued? are elementary discourse units
(12) The current distribution arrangement ends in March
1990, although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
(13) Advocates said the 90-cent-an-hour rise, to $4.25 an
hour by April 1991, is too small for the working poor,
while opponents argued that the increase will still
hurt small business and cost many thousands of
jobs.
In Section 5, we will identify additional cases. What
we will then argue is that it will be insufficient to
train an algorithm for identifying discourse argu-
ments simply on the basis of syntactically analysed
text.
We now present preliminary measurements of
these and other mismatches between the two corpora
for SUBCONJS. To do this we describe a procedural
algorithm which builds on the idea presented at the
start of this section. The statistics are preliminary in
that only the annotations of a single annotator were
considered, and we have not attempted to exclude
cases in which annotators disagree.
We consider only those SUBCONJS for which both
arguments are located in the same sentence as the
connective (which is the case for approximately 99%
of the annotated instances). The syntactic configura-
tion of such relations pattern in a way shown in Fig-
ure 1. Note that it is not necessary for any of Conn,
Arg1, or Arg2 to have a single node in the parse tree
that dominates it exactly. In Figure 1 we do obtain a
single node for Conn, and Arg2 but for Arg1, it is
the set of nodes fNP; V Pg that dominate it exactly.
Connectives like so that, and even if are not domi-
nated by a single node, and cases where the annota-
tor has decided that a (parenthetical) clausal element
is not minimally necessary to the interpretation of
Arg2 will necessitate choosing multiple nodes that
dominate Arg2 exactly.
Given the node(s) in the parse tree that dominate
Conn (fINg in Figure 1), the algorithm we present
tries to find node(s) in the parse tree that dominate
Arg1 and Arg2 exactly using the operation of tree
subtraction (Sections 4.1, and 4.2). We then discuss
its execution on (11) in Section 4.3.
annotated in the same way: as satellites of the relation Attribu-
tion. RST does not recognize that satellite segments, such as
the ones given above, sometimes participate in a higher RST
relation along with their nuclei and sometimes not.
32
S12
SBAR NP
A new $1 direct
loan
VP
is treated as a
$1 expenditure
IN S
2
the budget mea-
sures cash flowsince
Given N
Conn
= fINg, our goal is to find N
Arg1
=
fNP; V Pg, and N
Arg2
= fS
2
g. Steps:
 h
Conn
= IN
 x
Conn+Arg2
= SBAR  parent(h
Conn
)
 x
Conn+Arg1+Arg2
= S
12
 lowest Ancestor
parent(x
Conn+Arg2
)
with la-
bel S or SBAR. Note that x 2 Ancestor
x
 N
Arg2
= x
Conn+Arg2
 N
Conn
= SBAR  fINg
= fS
2
g
 N
Arg1
= x
Conn+Arg1+Arg2
  fx
Conn+Arg2
g
= S
12
  fSBARg
= fNP; V Pg
Figure 1: The syntactic configuration for (11), and the execution of the tree subtraction algorithm on this configuration.
4.1 Tree subtraction
We will now define the operation of tree subtraction
the graphical intuition for which is given in Figure
2. Let T be the set of nodes in the tree.
Definition 4.1. The ancestors of any node t 2 T ,
denoted by Ancestor
t
 T is a set of nodes such
that t 2 Ancestor
t
and parent(u; t) ) ([u 2
Ancestor
t
] ^ [Ancestor
u
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 31?38,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Annotating Attribution in the Penn Discourse TreeBank
Rashmi Prasad and Nikhil Dinesh and Alan Lee and Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
 
rjprasad,nikhild,aleewk,joshi  @linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
An emerging task in text understanding
and generation is to categorize information
as fact or opinion and to further attribute
it to the appropriate source. Corpus an-
notation schemes aim to encode such dis-
tinctions for NLP applications concerned
with such tasks, such as information ex-
traction, question answering, summariza-
tion, and generation. We describe an anno-
tation scheme for marking the attribution
of abstract objects such as propositions,
facts and eventualities associated with dis-
course relations and their arguments an-
notated in the Penn Discourse TreeBank.
The scheme aims to capture the source and
degrees of factuality of the abstract ob-
jects. Key aspects of the scheme are anno-
tation of the text spans signalling the attri-
bution, and annotation of features record-
ing the source, type, scopal polarity, and
determinacy of attribution.
1 Introduction
News articles typically contain a mixture of infor-
mation presented from several different perspec-
tives, and often in complex ways. Writers may
present information as known to them, or from
some other individual?s perspective, while further
distinguishing between, for example, whether that
perspective involves an assertion or a belief. Re-
cent work has shown the importance of recogniz-
ing such perspectivization of information for sev-
eral NLP applications, such as information extrac-
tion, summarization, question answering (Wiebe
et al, 2004; Stoyanov et al, 2005; Riloff et al,
2005) and generation (Prasad et al, 2005). Part of
the goal of such applications is to distinguish be-
tween factual and non-factual information, and to
identify the source of the information. Annotation
schemes (Wiebe et al, 2005; Wilson and Wiebe,
2005; PDTB-Group, 2006) encode such distinc-
tions to facilitate accurate recognition and repre-
sentation of such perspectivization of information.
This paper describes an extended annotation
scheme for marking the attribution of discourse re-
lations and their arguments annotated in the Penn
Discourse TreeBank (PDTB) (Miltsakaki et al,
2004; Prasad et al, 2004; Webber et al, 2005), the
primary goal being to capture the source and de-
grees of factuality of abstract objects. The scheme
captures four salient properties of attribution: (a)
source, distinguishing between different types of
agents to whom AOs are attributed, (b) type, re-
flecting the degree of factuality of the AO, (c) sco-
pal polarity of attribution, indicating polarity re-
versals of attributed AOs due to surface negated
attributions, and (d) determinacy of attribution, in-
dicating the presence of contexts canceling the en-
tailment of attribution. The scheme also describes
annotation of the text spans signaling the attri-
bution. The proposed scheme is an extension of
the core scheme used for annotating attribution
in the first release of the PDTB (Dinesh et al,
2005; PDTB-Group, 2006). Section 2 gives an
overview of the PDTB, Section 3 presents the ex-
tended annotation scheme for attribution, and Sec-
tion 4 presents the summary.
2 The Penn Discourse TreeBank (PDTB)
The PDTB contains annotations of discourse rela-
tions and their arguments on the Wall Street Jour-
nal corpus (Marcus et al, 1993). Following the
approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized ap-
31
proach towards the annotation of discourse rela-
tions, treating discourse connectives as the an-
chors of the relations, and thus as discourse-level
predicates taking two abstract objects (AOs) as
their arguments. For example, in (1), the subordi-
nating conjunction since is a discourse connective
that anchors a TEMPORAL relation between the
event of the earthquake hitting and a state where
no music is played by a certain woman. (The 4-
digit number in parentheses at the end of examples
gives the WSJ file number of the example.)
(1) She hasn?t played any music since the earthquake
hit. (0766)
There are primarily two types of connectives
in the PDTB: ?Explicit? and ?Implicit?. Explicit
connectives are identified form four grammati-
cal classes: subordinating conjunctions (e.g., be-
cause, when, only because, particularly since),
subordinators (e.g., in order that), coordinating
conjunctions (e.g., and, or), and discourse adver-
bials (e.g., however, otherwise). In the examples
in this paper, Explicit connectives are underlined.
For sentences not related by an Explicit connec-
tive, annotators attempt to infer a discourse rela-
tion between them by inserting connectives (called
?Implicit? connectives) that best convey the in-
ferred relations. For example, in (2), the inferred
CAUSAL relation between the two sentences was
annotated with because as the Implicit connective.
Implicit connectives together with their sense clas-
sification are shown here in small caps.
(2) Also unlike Mr. Ruder, Mr. Breeden appears to
be in a position to get somewhere with his agenda.
Implicit=BECAUSE (CAUSE) As a former White
House aide who worked closely with Congress, he
is savvy in the ways of Washington. (0955)
Cases where a suitable Implicit connective
could not be annotated between adjacent sentences
are annotated as either (a) ?EntRel?, where the
second sentence only serves to provide some fur-
ther description of an entity in the first sentence
(Example 3); (b) ?NoRel?, where no discourse re-
lation or entity-based relation can be inferred; and
(c) ?AltLex?, where the insertion of an Implicit
connective leads to redundancy, due to the rela-
tion being alternatively lexicalized by some ?non-
connective? expression (Example 4).
(3) C.B. Rogers Jr. was named chief executive officer of
this business information concern. Implicit=EntRel
Mr. Rogers, 60 years old, succeeds J.V. White, 64,
who will remain chairman and chairman of the ex-
ecutive committee (0929).
(4) One in 1981 raised to $2,000 a year from $1,500
the amount a person could put, tax-deductible,
into the tax-deferred accounts and widened cov-
erage to people under employer retirement plans.
Implicit=AltLex (consequence) [This caused] an ex-
plosion of IRA promotions by brokers, banks, mu-
tual funds and others. (0933)
Arguments of connectives are simply labelled
Arg2, for the argument appearing in the clause
syntactically bound to the connective, and Arg1,
for the other argument. In the examples here, Arg1
appears in italics, while Arg2 appears in bold.
The basic unit for the realization of an AO ar-
gument of a connective is the clause, tensed or un-
tensed, but it can also be associated with multiple
clauses, within or across sentences. Nominaliza-
tions and discourse deictics (this, that), which can
also be interpreted as AOs, can serve as the argu-
ment of a connective too.
The current version of the PDTB also contains
attribution annotations on discourse relations and
their arguments. These annotations, however, used
the earlier core scheme which is subsumed in the
extended scheme described in this paper.
The first release of the Penn Discourse
TreeBank, PDTB-1.0 (reported in PDTB-
Group (2006)), is freely available from
http://www.seas.upenn.edu/?pdtb.
PDTB-1.0 contains 100 distinct types of Explicit
connectives, with a total of 18505 tokens, anno-
tated across the entire WSJ corpus (25 sections).
Implicit relations have been annotated in three
sections (Sections 08, 09, and 10) for the first
release, totalling 2003 tokens (1496 Implicit
connectives, 19 AltLex relations, 435 EntRel
tokens, and 53 NoRel tokens). The corpus also
includes a broadly defined sense classification for
the implicit relations, and attribution annotation
with the earlier core scheme. Subsequent releases
of the PDTB will include Implicit relations
annotated across the entire corpus, attribution
annotation using the extended scheme proposed
here, and fine-grained sense classification for both
Explicit and Implicit connectives.
3 Annotation of Attribution
Recent work (Wiebe et al, 2005; Prasad et al,
2005; Riloff et al, 2005; Stoyanov et al, 2005),
has shown the importance of recognizing and rep-
resenting the source and factuality of information
in certain NLP applications. Information extrac-
tion systems, for example, would perform better
32
by prioritizing the presentation of factual infor-
mation, and multi-perspective question answering
systems would benefit from presenting informa-
tion from different perspectives.
Most of the annotation approaches tackling
these issues, however, are aimed at performing
classifications at either the document level (Pang
et al, 2002; Turney, 2002), or the sentence or word
level (Wiebe et al, 2004; Yu and Hatzivassiloglou,
2003). In addition, these approaches focus primar-
ily on sentiment classification, and use the same
for getting at the classification of facts vs. opin-
ions. In contrast to these approaches, the focus
here is on marking attribution on more analytic se-
mantic units, namely the Abstract Objects (AOs)
associated with predicate-argument discourse re-
lations annotated in the PDTB, with the aim of
providing a compositional classification of the fac-
tuality of AOs. The scheme isolates four key prop-
erties of attribution, to be annotated as features:
(1) source, which distinguishes between different
types of agents (Section 3.1); (2) type, which en-
codes the nature of relationship between agents
and AOs, reflecting the degree of factuality of the
AO (Section 3.2); (3) scopal polarity, which is
marked when surface negated attribution reverses
the polarity of the attributed AO (Section 3.3), and
(4) determinacy, which indicates the presence of
contexts due to which the entailment of attribu-
tion gets cancelled (Section 3.4). In addition, to
further facilitate the task of identifying attribution,
the scheme also aims to annotate the text span
complex signaling attribution (Section 3.5)
Results from annotations using the earlier attri-
bution scheme (PDTB-Group, 2006) show that a
significant proportion (34%) of the annotated dis-
course relations have some non-Writer agent as
the source for either the relation or one or both ar-
guments. This illustrates the simplest case of the
ambiguity inherent for the factuality of AOs, and
shows the potential use of the PDTB annotations
towards the automatic classification of factuality.
The annotations also show that there are a variety
of configurations in which the components of the
relations are attributed to different sources, sug-
gesting that recognition of attributions may be a
complex task for which an annotated corpus may
be useful. For example, in some cases, a rela-
tion together with its arguments is attributed to the
writer or some other agent, whereas in other cases,
while the relation is attributed to the writer, one
or both of its arguments is attributed to different
agent(s). For Explicit connectives. there were 6
unique configurations, for configurations contain-
ing more than 50 tokens, and 5 unique configura-
tions for Implicit connectives.
3.1 Source
The source feature distinguishes between (a) the
writer of the text (?Wr?), (b) some specific agent
introduced in the text (?Ot? for other), and (c)
some generic source, i.e., some arbitrary (?Arb?)
individual(s) indicated via a non-specific reference
in the text. The latter two capture further differ-
ences in the degree of factuality of AOs with non-
writer sources. For example, an ?Arb? source for
some information conveys a higher degree of fac-
tuality than an ?Ot? source, since it can be taken
to be a ?generally accepted? view.
Since arguments can get their attribution
through the relation between them, they can be an-
notated with a fourth value ?Inh?, to indicate that
their source value is inherited from the relation.
Given this scheme for source, there are broadly
two possibilities. In the first case, a relation
and both its arguments are attributed to the same
source, either the writer, as in (5), or some other
agent (here, Bill Biedermann), as in (6). (At-
tribution feature values assigned to examples are
shown below each example; REL stands for the
discourse relation denoted by the connective; At-
tribution text spans are shown boxed.)
(5) Since the British auto maker became a takeover
target last month, its ADRs have jumped about
78%. (0048)
REL Arg1 Arg2
[Source] Wr Inh Inh
(6) ?The public is buying the market when in re-
ality there is plenty of grain to be shipped,?
said Bill Biedermann  (0192)
REL Arg1 Arg2
[Source] Ot Inh Inh
As Example (5) shows, text spans for im-
plicit Writer attributions (corresponding to im-
plicit communicative acts such as I write, or I say),
are not marked and are taken to imply Writer attri-
bution by default (see also Section 3.5).
In the second case, one or both arguments have
a different source from the relation. In (7), for
example, the relation and Arg2 are attributed to
the writer, whereas Arg1 is attributed to another
agent (here, Mr. Green). On the other hand, in (8)
and (9), the relation and Arg1 are attributed to the
writer, whereas Arg2 is attributed to another agent.
33
(7) When Mr. Green won a $240,000 verdict in a land
condemnation case against the state in June 1983,
he says Judge O?Kicki unexpectedly awarded him
an additional $100,000. (0267)
REL Arg1 Arg2
[Source] Wr Ot Inh
(8) Factory orders and construction outlays were largely
flat in December while purchasing agents said
manufacturing shrank further in October. (0178)
REL Arg1 Arg2
[Source] Wr Inh Ot
(9) There, on one of his first shopping trips, Mr.
Paul picked up several paintings at stunning prices.
 Afterward, Mr. Paul is said by Mr. Guterman
to have phoned Mr. Guterman, the New York de-
veloper selling the collection, and gloated. (2113)
REL Arg1 Arg2
[Source] Wr Inh Ot
Example (10) shows an example of a generic
source indicated by an agentless passivized attri-
bution on Arg2 of the relation. Note that pas-
sivized attributions can also be associated with
a specific source when the agent is explicit, as
shown in (9). ?Arb? sources are also identified
by the occurrences of adverbs like reportedly, al-
legedly, etc.
(10) Although index arbitrage is said to add liquidity to
markets, John Bachmann,  says too much liq-
uidity isn?t a good thing. (0742)
REL Arg1 Arg2
[Source] Wr Ot Arb
We conclude this section by noting that ?Ot?
is used to refer to any specific individual as the
source. That is, no further annotation is provided
to indicate who the ?Ot? agent in the text is. Fur-
thermore, as shown in Examples (11-12), multiple
?Ot? sources within the same relation do not indi-
cate whether or not they refer to the same or differ-
ent agents. However, we assume that the text span
annotations for attribution, together with an inde-
pendent mechanism for named entity recognition
and anaphora resolution can be employed to iden-
tify and disambiguate the appropriate references.
(11) Suppression of the book, Judge Oakes observed ,
would operate as a prior restraint and thus involve
the First Amendment. Moreover, and
here Judge Oakes went to the heart of the question ,
?Responsible biographers and historians con-
stantly use primary sources, letters, diaries, and
memoranda. (0944)
REL Arg1 Arg2
[Source] Wr Ot Ot
(12) The judge was considered imperious, abrasive and
ambitious, those who practiced before him say .
Yet, despite the judge?s imperial bearing, no one
ever had reason to suspect possible wrongdoing,
says John Bognato, president of Cambria  .(0267)
REL Arg1 Arg2
[Source] Wr Ot Ot
3.2 Type
The type feature signifies the nature of the rela-
tion between the agent and the AO, leading to dif-
ferent inferences about the degree of factuality of
the AO. In order to capture the factuality of the
AOs, we start by making a three-way distinction
of AOs into propositions, facts and eventualities
(Asher, 1993). This initial distinction allows for
a more semantic, compositional approach to the
annotation and recognition of factuality. We de-
fine the attribution relations for each AO type as
follows: (a) Propositions involve attribution to an
agent of his/her (varying degrees of) commitment
towards the truth of a proposition; (b) Facts in-
volve attribution to an agent of an evaluation to-
wards or knowledge of a proposition whose truth
is taken for granted (i.e., a presupposed proposi-
tion); and (c) Eventualities involve attribution to
an agent of an intention/attitude towards an even-
tuality. In the case of propositions, a further dis-
tinction is made to capture the difference in the de-
gree of the agent?s commitment towards the truth
of the proposition, by distinguishing between ?as-
sertions? and ?beliefs?. Thus, the scheme for the
annotation of type ultimately uses a four-way dis-
tinction for AOs, namely between assertions, be-
liefs, facts, and eventualities. Initial determination
of the degree of factuality involves determination
of the type of the AO.
AO types can be identified by well-defined se-
mantic classes of verbs/phrases anchoring the at-
tribution. We consider each of these in turn.
Assertions are identified by ?assertive predi-
cates? or ?verbs of communication? (Levin, 1993)
such as say, mention, claim, argue, explain etc.
They take the value ?Comm? (for verbs of Com-
munication). In Example (13), the Ot attribution
on Arg1 takes the value ?Comm? for type. Im-
plicit writer attributions, as in the relation of (13),
also take (the default) ?Comm?. Note that when an
argument?s attribution source is not inherited (as
in Arg1 in this example) it also takes its own inde-
pendent value for type. This example thus conveys
that there are two different attributions expressed
within the discourse relation, one for the relation
and the other for one of its arguments, and that
both involve assertion of propositions.
34
(13) When Mr. Green won a $240,000 verdict in a land
condemnation case against the state in June 1983,
he says Judge O?Kicki unexpectedly awarded him
an additional $100,000. (0267)
REL Arg1 Arg2
[Source] Wr Ot Inh
[Type] Comm Comm Null
In the absence of an independent occurrence of
attribution on an argument, as in Arg2 of Exam-
ple (13), the ?Null? value is used for the type on
the argument, meaning that it needs to be derived
by independent (here, undefined) considerations
under the scope of the relation. Note that unlike
the ?Inh? value of the source feature, ?Null? does
not indicate inheritance. In a subordinate clause,
for example, while the relation denoted by the sub-
ordinating conjunction may be asserted, the clause
content itself may be presupposed, as seems to be
the case for the relation and Arg2 of (13). How-
ever, we found these differences difficult to deter-
mine at times, and consequently leave this unde-
fined in the current scheme.
Beliefs are identified by ?propositional attitude
verbs? (Hintikka, 1971) such as believe, think, ex-
pect, suppose, imagine, etc. They take the value
?PAtt? (for Propostional Attitude). An example of
a belief attribution is given in (14).
(14) Mr. Marcus believes spot steel prices will continue
to fall through early 1990 and then reverse them-
selves. (0336)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
Facts are identified by the class of ?factive and
semi-factive verbs? (Kiparsky and Kiparsky, 1971;
Karttunen, 1971) such as regret, forget, remember,
know, see, hear etc. They take the value ?Ftv?
(for Factive) for type (Example 15). In the current
scheme, this class does not distinguish between
the true factives and semi-factives, the former in-
volving an attitute/evaluation towards a fact, and
the latter involving knowledge of a fact.
(15) The other side , he argues knows Giuliani has al-
ways been pro-choice, even though he has personal
reservations. (0041)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ftv Null Null
Lastly, eventualities are identified by a class of
verbs which denote three kinds of relations be-
tween agents and eventualities (Sag and Pollard,
1991). The first kind is anchored by verbs of influ-
ence like persuade, permit, order, and involve one
agent influencing another agent to perform (or not
perform) an action. The second kind is anchored
by verbs of commitment like promise, agree, try,
intend, refuse, decline, and involve an agent com-
mitting to perform (or not perform) an action. Fi-
nally, the third kind is anchored by verbs of ori-
entation like want, expect, wish, yearn, and in-
volve desire, expectation, or some similar mental
orientation towards some state(s) of affairs. These
sub-distinctions are not encoded in the annotation,
but we have used the definitions as a guide for
identifying these predicates. All these three types
are collectively referred to and annotated as verbs
of control. Type for these classes takes the value
?Ctrl? (for Control). Note that the syntactic term
control is used because these verbs denote uni-
form structural control properties, but the primary
basis for their definition is nevertheless semantic.
An example of the control attribution relation an-
chored by a verb of influence is given in (16).
(16) Eward and Whittington had planned to leave the bank
earlier, but Mr. Craven had persuaded them to re-
main until the bank was in a healthy position.
(1949)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ctrl Null Null
Note that while our use of the term source ap-
plies literally to agents responsible for the truth of
a proposition, we continue to use the same term
for the agents for facts and eventualities. Thus,
for facts, the source represents the bearers of atti-
tudes/knowledge, and for considered eventualities,
the source represents intentions/attitudes.
3.3 Scopal Polarity
The scopal polarity feature is annotated on re-
lations and their arguments to primarily identify
cases when verbs of attribution are negated on the
surface - syntactically (e.g., didn?t say, don?t think)
or lexically (e.g., denied), but when the negation in
fact reverses the polarity of the attributed relation
or argument content (Horn, 1978). Example (17)
illustrates such a case. The ?but? clause entails an
interpretation such as ?I think it?s not a main con-
sideration?, for which the negation must take nar-
row scope over the embedded clause rather than
the higher clause. In particular, the interpretation
of the CONTRAST relation denoted by but requires
that Arg2 should be interpreted under the scope
of negation.
35
(17) ?Having the dividend increases is a supportive ele-
ment in the market outlook, but I don?t think it?s a
main consideration,? he says. (0090)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Comm Null PAtt
[Polarity] Null Null Neg
To capture such entailments with surface nega-
tions on attribution verbs, an argument of a con-
nective is marked ?Neg? for scopal polarity when
the interpretation of the connective requires the
surface negation to take semantic scope over the
lower argument. Thus, in Example (17), scopal
polarity is marked as ?Neg? for Arg2.
When the neg-lowered interpretations are not
present, scopal polarity is marked as the default
?Null? (such as for the relation and Arg1 of Ex-
ample 17).
It is also possible for the surface negation of at-
tribution to be interpreted as taking scope over the
relation, rather than an argument. We have not ob-
served this in the corpus yet, so we describe this
case with the constructed example in (18). What
the example shows is that in addition to entailing
(18b) - in which case it would be annotated par-
allel to Example (17) above - (18a) can also en-
tail (18c), such that the negation is intrepreted as
taking semantic scope over the ?relation? (Lasnik,
1975), rather than one of the arguments. As the
scopal polarity annotations for (18c) show, low-
ering of the surface negation to the relation is
marked as ?Neg? for the scopal polarity of the re-
lation.
(18) a. John doesn?t think Mary will get cured because
she took the medication.
b.   John thinks that because Mary took the
medication, she will not get cured.
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Null Neg Null
c.   John thinks that Mary will get cured
not because she took the medication (but be-
cause she has started practising yoga.)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Neg Null Null
We note that scopal polarity does not capture
the appearance of (opaque) internal negation that
may appear on arguments or relations themselves.
For example, a modified connective such as not
because does not take ?Neg? as the value for sco-
pal polarity, but rather ?Null?. This is consistent
with our goal of marking scopal polarity only for
lowered negation, i.e., when surface negation from
the attribution is lowered to either the relation or
argument for interpretation.
3.4 Determinacy
The determinacy feature captures the fact that the
entailment of the attribution relation can be made
indeterminate in context, for example when it ap-
pears syntactically embedded in negated or condi-
tional contexts.. The annotation attempts to cap-
ture such indeterminacy with the value ?Indet?.
Determinate contexts are simply marked as the de-
fault ?Null?. For example, the annotation in (19)
conveys the idea that the belief or opinion about
the effect of higher salaries on teachers? perfor-
mance is not really attributed to anyone, but is
rather only being conjectured as a possibility.
(19) It is silly libel on our teachers to think they would
educate our children better if only they got a few
thousand dollars a year more. (1286)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Null Null Null
[Determinacy] Indet Null Null
3.5 Attribution Spans
In addition to annotating the properties of attribu-
tion in terms of the features discussed above, we
also propose to annotate the text span associated
with the attribution. The text span is annotated as
a single (possibly discontinuous) complex reflect-
ing three of the annotated features, namely source,
type and scopal polarity. The attribution span also
includes all non-clausal modifiers of the elements
contained in the span, for example, adverbs and
appositive NPs. Connectives, however, are ex-
cluded from the span, even though they function
as modifiers. Example (20) shows a discontinu-
ous annotation of the attribution, where the paren-
thetical he argues is excluded from the attribution
phrase the other side knows, corresponding to the
factive attribution.
(20) The other side , he argues knows Giuliani has al-
ways been pro-choice, even though he has personal
reservations. (0041)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ftv Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
Inclusion of the fourth feature, determinacy,
is not ?required? to be included in the current
scheme because the entailment cancelling contexts
36
can	 be very complex. For example, in Exam-
ple (19), the conditional interpretation leading to
the indeterminacy of the relation and its arguments
is due to the syntactic construction type of the en-
tire sentence. It is not clear how to annotate the
indeterminacy induced by such contexts. In the
example, therefore, the attribution span only in-
cludes the anchor for the type of the attribution.
Spans for implicit writer attributions are left un-
marked since there is no corresponding text that
can be selected. The absence of a span annota-
tion is simply taken to reflect writer attribution,
together with the ?Wr? value on the source fea-
ture.
Recognizing attributions is not trivial since they
are often left unexpressed in the sentence in which
the AO is realized, and have to be inferred from the
prior discourse. For example, in (21), the relation
together with its arguments in the third sentence
are attributed to Larry Shapiro, but this attribution
is implicit and must be inferred from the first sen-
tence.
(21) ?There are certain cult wines that can command these
higher prices,? says Larry Shapiro of Marty?s, 
?What?s different is that it is happening with young
wines just coming out. We?re seeing it partly because
older vintages are growing more scarce.? (0071)
REL Arg1 Arg2
[Source] Ot Inh Inh
The spans for such implicit ?Ot? attributions
mark the text that provides the inference of the
implicit attribution, which is just the closest occur-
rence of the explicit attribution phrase in the prior
text.
The final aspect of the span annotation is that
we also annotate non-clausal phrases as the an-
chors attribution, such as prepositional phrases
like according to X, and adverbs like reportedly,
allegedly, supposedly. One such example is shown
in (22).
(22) No foreign companies bid on the Hiroshima project,
according to the bureau . But the Japanese prac-
tice of deep discounting often is cited by Ameri-
cans as a classic barrier to entry in Japan?s mar-
ket. (0501)
REL Arg1 Arg2
[Source] Wr Ot Inh
[Type] Comm Comm Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
Note that adverbials are free to pick their own type
of attribution. For example, supposedly as an at-
tribution adverb picks ?PAtt? as the value for type.
3.6 Attribution of Implicit Relations
Implicit connectives and their arguments in the
PDTB are also marked for attribution. Implicit
connectives express relations that are inferred by
the reader. In such cases, the writer intends for
the reader to infer a discourse relation. As with
Explicit connectives, implicit relations intended
by the writer of the article are distinguished from
those intended by some other agent introduced by
the writer. For example, while the implicit rela-
tion in Example (23) is attributed to the writer, in
Example (24), both Arg1 and Arg2 have been
expressed by someone else whose speech is be-
ing quoted: in this case, the implicit relation is at-
tributed to the other agent.
(23) The gruff financier recently started socializing in
upper-class circles. Implicit = FOR EXAMPLE
(ADD.INFO) Although he says he wasn?t keen on go-
ing, last year he attended a New York gala where
his daughter made her debut. (0800)
REL Arg1 Arg2
[Source] Wr Inh Inh
[Type] Comm Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
(24) ?We asked police to investigate why they are
allowed to distribute the flag in this way.
Implicit=BECAUSE (CAUSE) It should be con-
sidered against the law,?
said Danny Leish, a spokesman for the association .
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Comm Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
For implicit relations, attribution is also anno-
tated for AltLex relations but not for EntRel and
NoRel, since the former but not the latter refer to
the presense of discourse relations.
4 Summary
In this paper, we have proposed and described an
annotation scheme for marking the attribution of
both explicit and implicit discourse connectives
and their arguments in the Penn Discourse Tree-
Bank. We discussed the role of the annotations for
the recognition of factuality in natural language
applications, and defined the notion of attribution.
The scheme was presented in detail with exam-
ples, outlining the ?feature-based annotation? in
terms of the source, type, scopal polarity, and
determinacy associated with attribution, and the
?span annotation? to highlight the text reflecting
the attribution features.
37
Ackno


wledgements
The Penn Discourse TreeBank project is partially
supported by NSF Grant: Research Resources,
EIA 02-24417 to the University of Pennsylva-
nia (PI: A. Joshi). We are grateful to Lukasz
Abramowicz and the anonymous reviewers for
useful comments.
References
Nicholas. Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer, Dordrecht.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005.
Attribution and the (non)-alignment of syntactic and
discourse arguments of connectives. In Proceedings
of the ACL Workshop on Frontiers in Corpus Anno-
tation II: Pie in the Sky, Ann Arbor, Michigan.
Jaakko Hintikka. 1971. Semantics for propositional at-
titudes. In L. Linsky, editor, Reference and Modal-
ity, pages 145?167. Oxford.
Laurence Horn. 1978. Remarks on neg-raising. In
Peter Cole, editor, Syntax and Semantics 9: Prag-
matics. Academic Press, New York.
Lauri Karttunen. 1971. Some observations on factiv-
ity. Papers in Linguistics, 4:55?69.
Carol Kiparsky and Paul Kiparsky. 1971. Fact. In
D. D. Steinberg and L. A. Jakobovits, editors, Se-
mantics: An Interdisciplinary Reader in Philosophy,
Linguistics and Psychology, pages 345?369. Cam-
bridge University Press, Cambridge.
Howard Lasnik. 1975. On the semantics of nega-
tion. In Contemporary Research in Philosophi-
cal Logic and Linguistic Semantics, pages 279?313.
Dordrecht: D. Reidel.
Beth Levin. 1993. English Verb Classes And Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse con-
nectives and their arguments. In Proceedings of the
HLT/NAACL Workshop on Frontiers in Corpus An-
notation, pages 9?16, Boston, MA.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), pages 79?86.
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse Treebank. In Proceedings of
the ACL Workshop on Discourse Annotation, pages
88?95, Barcelona, Spain.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan
Lee, Eleni Miltsakaki, and Bonnie Webber. 2005.
The Penn Discourse TreeBank as a resource for nat-
ural language generation. In Proceedings of the
Corpus Linguistics Workshop on Using Corpora for
NLG.
Ellen Riloff, Janyce Wiebe, and Willian Phillips. 2005.
Exploiting subjectivity classification to improve in-
formation extraction. In Proceedings of the 20th Na-
tional Conference on Artificial Intelligence (AAAI-
2005).
Ivan A. Sag and Carl Pollard. 1991. An integrated
theory of complement control. Language, 67(1):63?
113.
The PDTB-Group. 2006. The Penn Discourse Tree-
Bank 1.0 Annotation Manual. Technical Report
IRCS-06-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania.
Veseli Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using
the OpQA corpus. In Proceedings of HLT-EMNLP.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL 2002,
pages 417?424.
Bonnie Webber, Aravind Joshi, M. Stone, and Alis-
tair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29(4):545?587.
Bonnie Webber, Aravind Joshi, Eleni Miltsakaki,
Rashmi Prasad, Nikhil Dinesh, Alan Lee, and
K. Forbes. 2005. A short introduction to the PDTB.
In Copenhagen Working Papers in Language and
Speech Processing.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308.
Janyce Wiebe, Theresa Wilson, , and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 1(2).
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Proceedings of the
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky, Ann Arbor, Michigan.
Hon Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of EMNLP-2003, pages
129?136, Saporo, Japan.
38
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 92?93,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
A Pilot Annotation to Investigate Discourse Connectivity in Biomedical Text  
 
 
 
Hong Yu, Nadya Frid, Susan McRoy Rashmi Prasad, Alan Lee, Aravind Joshi 
University of Wisconsin-Milwaukee University of Pennsylvania 
P.O.Box 413 3401 Walnut Street 
Milwaukee, WI 53201 Philadelphia, PA 19104, USA 
Hongyu,frid,mcroy@uwm.edu Rjprasad,aleewk,joshi@seas.upenn.edu
 
 
 
 
 
 
Abstract 
The goal of the Penn Discourse Treebank 
(PDTB) project is to develop a large-scale cor-
pus, annotated with coherence relations marked 
by discourse connectives. Currently, the primary 
application of the PDTB annotation has been to 
news articles. In this study, we tested whether 
the PDTB guidelines can be adapted to a differ-
ent genre. We annotated discourse connectives 
and their arguments in one 4,937-token full-text 
biomedical article. Two linguist annotators 
showed an agreement of 85% after simple con-
ventions were added. For the remaining 15% 
cases, we found that biomedical domain-specific 
knowledge is needed to capture the linguistic 
cues that can be used to resolve inter-annotator 
disagreement. We found that the two annotators 
were able to reach an agreement after discussion. 
Thus our experiments suggest that the PDTB an-
notation can be adapted to new domains by mini-
mally adjusting the guidelines and by adding 
some further domain-specific linguistic cues. 
1 Introduction 
Large scale annotated corpora, e.g., the Penn 
TreeBank (PTB) project (Marcus et al 1993), 
have played an important role in text-mining. 
The Penn Discourse Treebank (PDTB) 
(http://www.seas.upenn.edu/~pdtb) (Prasad et al 
2008a) annotates the argument structure, seman-
tics, and attribution of discourse connectives and 
their arguments. The current release of PDTB-
2.0 contains the annotations of 1,808 Wall Street 
Journal articles (~1 million words) from the 
Penn TreeBank (Marcus et al 1993) II distribu-
tion and a total of 40,600 discourse connective  
tokens (Prasad et al 2008b). This work exam-
ines whether the PDTB annotation guidelines 
can be adapted to a different genre, the biomedi-
cal literature.  
2 Notation 
A discourse connective can be defined as a 
word or multiword expression that signals a 
discourse relation. Discourse connectives 
can be subordinating conjunctions (e.g., be-
cause, when, although), coordinating con-
junctions (e.g., but, or, nor) and adverbials 
(e.g., however, as a result, for example). A 
discourse connective takes in two argu-
ments, Arg1 and Arg2. Arg2 is the argument 
that appears in the clause that is syntacti-
cally bound to the connective and Arg1 is 
the other argument. In the sentence ?John 
failed the exam because he was lazy? the dis-
course connective is underlined, Arg1 ap-
pears in italics and Arg2 appears in bold. 
3 A Pilot Annotation 
Following the PDTB annotation manual (Prasad 
et al 2008b), we conducted a pilot annotation of 
discourse connectivity in biomedical text. As an 
initial step, we only annotated the three most 
92
important components of a discourse relation; 
namely, a discourse connective and its two ar-
guments; we did not annotate attribution. Two 
linguist annotators independently annotated one 
full-text biomedical article (Verpy et al 1999) 
that we randomly selected. The article is 4,937 
tokens long. When the annotation work was 
completed, we measured the inter-annotator 
agreement, following the PDTB exact match 
criterion (Miltsakaki et al 2004). According to 
this criterion, a discourse relation is in dis-
agreement if there is disagreement on any text-
span (i.e., the discourse connective or any of its 
two arguments). In addition, we also measured 
the agreement in the components (i.e., discourse 
connectives and the arguments). We discussed 
the annotation results and made suggestions to 
adapt the PDTB guidelines to biomedical text.  
4 Results and Discussion 
The first annotator identified 74 discourse con-
nectives, and the second annotator identified 75, 
68 of which were the same as those identified by 
the first annotator. The combined total number 
of discourse connectives was 81. The overall 
agreement in discourse connective identification 
was 68/81=84%.  
 
Of the 68 discourse connectives that were anno-
tated by both annotators, 31 were an exact 
match, 31 had an exact match for Arg1, and 54 
had an exact match for Arg2. The overall 
agreement for the 68 discourse relations is 
45.6% for exact match, 45.6% for Arg1, and 
79.4% for Arg2. The PDTB also reported a 
higher level of agreement in annotating Arg2 
than in annotating Arg1 (Miltsakaki et al 2004). 
We manually analyzed the cases with disagree-
ment. We found the disagreements are nearly all 
related to the annotation of citation references, 
supplementary clauses, and other conventions. 
When a few conventions for these cases were 
added, the inter-annotator agreement went up to 
85%. We also found that different interpretation 
of a relation and its arguments by annotators 
plays an important role for the remaining 15% 
inconsistency, and domain-specific knowledge 
is necessary to resolve such cases.   
 
5 New Conventions 
After the completion of the pilot annotation and 
the discussion, we decided to add the following 
conventions to the PDTB annotation guidelines 
to address the characteristics of biomedical text: 
 
i. Citation references are to be annotated as 
a part of an argument because the inclu-
sion will benefit many text-mining tasks 
including identifying the semantic rela-
tions among citations. 
ii. Clausal supplements (e.g., relative or 
parenthetical constructions) that modify  
arguments but are not minimally 
necessary for the interpretation of the 
relation,  are annotated as part of the 
arguments. 
iii. We will annotate a wider variety of 
nominalizations as arguments than 
allowed by the PDTB guidelines. 
 
We anticipate that these changes will both de-
crease the amount of effort required for annota-
tion and increase the reliability of the 
annotation. 
6 References 
Marcus M, Santorini B, Marcinkiewicz M (1993) 
Building a Large Annotated Corpus of Eng-
lish: The Penn Treebank. Computational 
Linguistics 19 
Miltsakaki E, Prasad R, Joshi A, Webber B (2004) 
Annotating discourse connectives and their 
arguments. Paper presented at Proceedings 
of the NAACL/HLT Workshop: Frontiers in 
Corpus Annotation 
Prasad R, Dinesh N, Lee A, Miltsakaki E, Robaldo L, 
Joshi A, Webber B (2008a) The Penn Dis-
course Treebank 2.0. Paper presented at The 
6th International Conference on Language 
Resources and Evaluation (LREC). Marra-
kech, Morroco 
Prasad R, Miltsakaki E, Dinesh N, Lee A, Joshi A, 
Robaldo L, Webber B (2008b) The Penn 
Discourse TreeBank 2.0 Annotation Manual. 
Technical Report: IRCS-08-01 
Verpy E, Leibovici M, Petit C (1999) Characteriza-
tion of otoconin-95, the major protein of 
murine otoconia, provides insights into the 
formation of these inner ear biominerals. 
Proc Natl Acad Sci U S A 96:529-534 
 
 
93
  
The Hindi Discourse Relation Bank 
Umangi Oza*, Rashmi Prasad?, Sudheer Kolachina*, Dipti Misra Sharma* and 
Aravind Joshi? 
*Language Technologies Research Centre 
IIIT Hyderabad, Gachibowli, Hyderabad, Andhra Pradesh, India 500032 
oza.umangi,sudheer.kpg08@gmail.com,dipti@iiit.ac.in 
 
?Institute for Research in Cognitive Science/Computer and Information Science 
3401 Walnut Street, Suite 400A 
Philadelphia, PA USA 19104 
rjprasad,joshi@seas.upenn.edu 
 
  
Abstract 
We describe the Hindi Discourse Relation 
Bank project, aimed at developing a large 
corpus annotated with discourse relations. 
We adopt the lexically grounded approach of 
the Penn Discourse Treebank, and describe 
our classification of Hindi discourse connec-
tives, our modifications to the sense classifi-
cation of discourse relations, and some cross-
linguistic comparisons based on some initial 
annotations carried out so far. 
1 Introduction 
To enable NLP research and applications beyond 
the sentence-level, corpora annotated with dis-
course level information have been developed. 
The recently developed Penn Discourse Tree-
bank (PDTB) (Prasad et al, 2008), for example, 
provides annotations of discourse relations (e.g., 
causal, contrastive, temporal, and elaboration 
relations) in the Penn Treebank Corpus. Recent 
interest in cross-linguistic studies of discourse 
relations has led to the initiation of similar dis-
course annotation projects in other languages as 
well, such as Chinese (Xue, 2005), Czech (Mla-
dov? et al, 2008), and Turkish (Deniz and Web-
ber, 2008). In this paper, we describe our ongo-
ing work on the creation of a Hindi Discourse 
Relation Bank (HDRB), broadly following the 
approach of the PDTB.1 The size of the HDRB 
corpus is 200K words and it is drawn from a 
400K word corpus on which Hindi syntactic de-
pendency annotation is being independently con-
ducted (Begum et al, 2008). Source corpus texts 
are taken from the Hindi newspaper Amar Ujala, 
and comprise news articles from several do-
mains, such as politics, sports, films, etc. We 
                                                 
1 An earlier study of Hindi discourse connectives towards 
the creation of HDRB is presented in Prasad et al (2008). 
present our characterization of discourse connec-
tives and their arguments in Hindi (Section 2), 
our proposals for modifying the sense classifica-
tion scheme (Section 3), and present some cross-
linguistics comparisons based on annotations 
done so far (Section 4). Section 5 concludes with 
a summary and future work.  
2 Discourse Relations and Arguments 
Following the PDTB approach, we take dis-
course relations to be realized in one of three 
ways: (a) as explicit connectives, which are 
?closed class? expressions drawn from well-
defined grammatical classes; (b) as alternative 
lexicalizations (AltLex), which are non-
connective expressions that cannot be defined as 
explicit connectives; and (c) as implicit connec-
tives, which are implicit discourse relations ?in-
ferred? between adjacent sentences not related by 
an explicit connective. When no discourse rela-
tion can be inferred between adjacent sentences, 
either an entity-based coherence relation (called 
EntRel) or the absence of a relation (called No-
Rel) is marked between the sentences. The two 
abstract object relata of a discourse relation are 
called the relation?s arguments (named Arg1 and 
Arg2), and argument annotation follows the ?mi-
nimality principle? in that only as much is se-
lected as the argument text span as is minimally 
necessary to interpret the relation. Finally, each 
discourse relation is assigned a sense label based 
on a hierarchical sense classification. 
2.1 Explicit Connectives 
In addition to the three major grammatical 
classes of Explicit connectives in the PDTB ? 
subordinating conjunctions, coordinating con-
junctions, and adverbials ? we recognize three 
other classes, described below. 
 
  
Sentential Relatives: These are relative pro-
nouns that conjoin a relative clause with its ma-
trix clause. As the name suggests, only relatives 
that modify verb phrases are treated as discourse 
connectives, and not those that modify noun 
phrases. Some examples are ????? (so that), 
????? ???? (because of which). 
 
1) [???? ??? ?????? ?? ?? ??????? ?? ????? ??? 
?? ?? ?? ????] ????? {???? ??? ???? ???? 
?? ???} 
?[Dropping all his work, he picked up the bird 
and ran towards the dispensary], so that {it 
could be given proper treatment}.? 
Subordinators: These include postpositions (Ex. 
2), verbal participles, and suffixes that introduce 
non-finite clauses with an abstract object inter-
pretation.2 
2) [?? ?? ????? ???]?? {??-??-?? ???????? ???? 
????? ???}? 
?Upon [hearing Baa?s words], {Gandhiji felt very 
ashamed}.? 
Particles: Particles such as ??, ?? act as dis-
course connectives. ?? is an emphatic inclusive 
particle used to suggest the inclusion of verbs, 
entities, adverbs, and adjectives. Instances of 
such particles which indicate the inclusion of 
verbs are taken as discourse connectives (Ex. 3) 
while others are not. 
3) ??? ?? ? ????? ????? ?? ??? ??? ? ???? ? ?? 
?????? ?? ??? ??? ??? ??? ???]?{?????? ??? 
??? ??? ???????? ???} ?? {?? ??? ???}? 
?[People see this as a consequence of the improv-
ing relation between the two countries]. {The 
Kashmiris are} also {learning an political lesson 
from this}.? 
2.2 Arguments of Discourse Relations 
In the PDTB, the assignment of the Arg1 and 
Arg2 labels to a discourse relation?s arguments is 
syntactically driven, in that the Arg2 label is as-
                                                 
2 Subordinators that denote the manner of an action are not 
discourse connectives, but since such disambiguation is a 
difficult task, we have decided to annotate subordinators in 
a later phase of the project.  
 
signed to the argument with which the connec-
tive was syntactically associated, while the Arg1 
label is assigned to the ?other? argument. In 
HDRB, however, the Arg1/Arg2 label assign-
ment is semantically driven, in that it is based on 
the ?sense? of the relation to which the argu-
ments belong.  Thus, each sense definition for a 
relation specifies the sense-specific semantic role 
of each of its arguments, and stipulates one of the 
two roles to be Arg1, and the other, Arg2.   For 
example, the ?cause? sense definition, which in-
volves a causal relation between two eventuali-
ties, specifies that one of its arguments is the 
cause, while the other is the effect, and further 
stipulates that the cause will be assigned the label 
Arg2, while the effect will be assigned the label 
Arg1.  Apart from giving meaning to the argu-
ment labels, our semantics-based convention has 
the added advantage simplifying the sense classi-
fication scheme. This is discussed further in Sec-
tion 3.  
2.3 Implicit Discourse Relations 
The HDRB annotation of implicit discourse rela-
tions largely follows the PDTB scheme. The only 
difference is that while implicit relations in 
PDTB are annotated only between paragraph-
internal adjacent sentences, we also annotate 
such relations across paragraph boundaries.  
3 Senses of Discourse Relations  
Broadly, we follow the PDTB sense classifica-
tion in that we take it to be a hierarchical classi-
fication, with the four top level sense classes of 
?Temporal?, ?Contingency?, ?Comparison?, and 
?Expansion?. Further refinements to the top class 
level are provided at the second type level and 
the third subtype level. Here, we describe our 
points of departure from the PDTB classification. 
The changes are partly motivated by general 
considerations for capturing additional senses, 
and partly by language-specific considerations. 
Figure 1 reflects the modifications we have made 
to the sense scheme. These are described below. 
 
Eliminating argument-specific labels: In the 
PDTB sense hierarchy, the tags at the type level 
are meant to express further refinements of the 
relations? semantics, while the tags at the subtype 
level are meant to reflect different orderings of 
the arguments (see Section 2.2). In HDRB, we 
eliminate these argument-ordering labels from 
the subtype level, since these labels don?t direct-
ly pertain to the meaning of discourse relations. 
  
All levels in the sense hierarchy thus have the 
purpose of specifying the semantics of the rela-
tion to different degrees of granularity. The rela-
tive ordering of the arguments is instead speci-
fied in the definition of the type-level senses, and 
is inherited by the more refined senses at the sub-
type level.   
 
 
 
     
 
Figure 1: HDRB (Modified) Sense Classification 
 
 
Uniform treatment of pragmatic relations: As 
in PDTB, discourse relations in HDRB are 
pragmatic when their relations have to be in-
ferred from the propositional content of the ar-
guments. However, we replace the PDTB prag-
matic senses with a uniform three-way classifica-
tion. Each pragmatic sense at the type level is 
further distinguished into three subtypes: ?epis-
temic? (Sweetser 1990), ?speech-act? (Sweetser 
1990), and ?propositional?. The propositional 
subtype involves the inference of a complete 
proposition. The relation is then taken to hold 
between this inferred proposition and the propo-
sitional content of one of the arguments. 
 
The ?Goal? sense: Under the ?Contingency? 
class, we have added a new type ?Goal?, which 
applies to relations where the situation described 
in one of the arguments is the goal of the situa- 
tion described in the other argument (which 
enables the achievement of the goal).   
4 Initial Annotation Experiments 
Based on the guidelines as described in this pa-
per, we annotated both explicit and implicit rela-
tions in 35 texts (averaging approx. 250 
words/text) from the HDRB corpus. A total of 
602 relation tokens were annotated. Here we 
present some useful distributions we were able to 
derive from our initial annotation, and discuss 
them in light of cross-linguistic comparisons of 
discourse relations.  
 
Types and Tokens of Discourse Relations: Ta-
ble 1 shows the overall distribution of the differ-
ent relation types, i.e., Explicit, AltLex, Implicit, 
EntRel, and NoRel. The second column reports 
the number of unique expressions used to realize 
the relation ? Explicit, Implicit and AltLex ? 
while the third column reports the total number 
of tokens and relative frequencies.  
 
Relations Types Tokens (%) 
Explicit 49 189 (31.4%) 
Implicit 35 185 (30.7%) 
AltLex 25 37 (6.14%) 
EntRel NA 140 (23.25%) 
NoRel NA 51 (8.5%) 
TOTAL 109 602 
Table 1: Distribution of Discourse Relations 
 
These distributions show some interesting simi-
larities and differences with the PDTB distribu-
tions (cf. Prasad et al, 2008). First, given that 
Hindi has a much richer morphological paradigm 
than English; one would have expected that it 
would have fewer explicit connectives. That is, 
one might expect Hindi to realize discourse rela-
tions morphologically more often than not, just 
as it realizes other syntactic relations.  However, 
even in the small data set of 602 tokens that we 
have annotated so far, we have found 49 unique 
explicit connectives, which is roughly half the 
number reported for the 1 million words anno-
tated in English texts in PDTB. It is expected that 
we will find more unique types as we annotate 
additional data. The relation type distribution 
  
thus seems to suggest that the availability of 
richer morphology in a language doesn?t affect 
connective usage. Second, the percentage of Alt-
Lex relations is higher in HDRB ? 6.14% com-
pared to 1.5% in PDTB, suggesting that Hindi 
makes greater usage of non-connective cohesive 
links with the prior discourse. Further studies are 
needed to characterize the forms and functions of 
AltLex expressions in both English and Hindi. 
 
Senses of Discourse Relations: We also ex-
amined the distributions for each sense class in 
HDRB and computed the relative frequency of 
the relations realized explicitly and implicitly. 
Cross-linguistically, one would expect languages 
to be similar in whether or not a relation with a 
particular sense is realized explicitly or implicit-
ly, since this choice lies in the domain of seman-
tics and inference, rather than syntax. Thus, we 
were interested in comparing the sense distribu-
tions in HDRB and PDTB. Table 2 shows these 
distributions for the top class level senses. (Here 
we counted the AltLex relations together with 
explicit connectives.) 
 
Sense Class Explicit (%) Implicit (%) 
Contingency 57 (58.2%) 41 (41.8%) 
Comparison 68 (76.5%) 21 (23.5%) 
Temporal 43 (65.2%) 23 (34.8%) 
Expansion 64(40%) 94(60%) 
Table 2: Distribution of Class Level Senses 
 
The table shows that sense distributions in 
HDRB are indeed similar to those reported in the 
PDTB (cf. Prasad et al, 2008). That is, the 
chances of ?Expansion? and ?Contingency? rela-
tions being explicit are lower compared to 
?Comparison? and ?Temporal? relations.    
5 Summary and Future Work 
This paper has reported on the Hindi Discourse 
Relation Bank (HDRB) project, in which dis-
course relations, their arguments, and their 
senses are being annotated. A major goal of our 
work was to investigate how well the Penn Dis-
course Treebank (PDTB) and its guidelines could 
be adapted for discourse annotation of Hindi 
texts. To a large extent, we have successfully 
adapted the PDTB scheme. Proposed changes 
have to do with identification of some new syn-
tactic categories for explicit connectives, and 
some general and language-driven modifications 
to the sense classification. From our initial anno-
tations, we found that (a) there doesn?t seem to 
be an inverse correlation between the usage fre-
quency of explicit connectives and the morpho-
logical richness of a language, although there 
does seem to be an increased use of cohesive 
devices in such a language; and (b) sense distri-
butions confirm the lack of expectation of cross-
linguistic ?semantic? differences. Our future goal 
is to complete the discourse annotation of a 200K 
word corpus, which will account for half of the 
400K word corpus being also annotated for syn-
tactic dependencies. We also plan to extend the 
annotation scheme to include attributions.   
 
Acknowledgements 
This work was partially supported by NSF grants 
EIA-02-24417, EIA-05-63063, and IIS-07-
05671.  
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. Proc. of IJCNLP-2008.  
Lucie Mladov?, ??rka Zik?nov? and Eva Haji?ov?. 
2008. From Sentence to Discourse: Building an 
Annotation Scheme for Discourse Based on Prague 
Dependency Treebank. Proc. of LREC-2008. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie 
Webber. 2008. The Penn Discourse TreeBank 2.0. 
Proc. of LREC-2008.  
Rashmi Prasad, Samar Husain, Dipti Mishra Sharma, 
and Aravind Joshi. 2008. Towards an Annotated 
Corpus of Discourse Relations in Hindi. Proc. of 
IJCNLP-2008. 
 Eve Sweetser.1990. From etymology to pragmat-
ics: Metaphorical and cultural aspects of se-
mantic structure .   Cambridge University Press. 
Nianwen Xue. 2005. Annotating Discourse Connec-
tives in the Chinese Treebank. Proc. of the ACL 
Workshop on Frontiers in Corpus Annotation 
II: Pie in the Sky.  
Deniz Zeyrek and Bonnie Webber. 2008. A Discourse 
Resource for Turkish: Annotating Discourse Con-
nectives in the METU Corpus. Proc. of IJCNLP-
2008.  
Coling 2010: Poster Volume, pages 1023?1031,
Beijing, August 2010
Realization of Discourse Relations by Other Means: Alternative
Lexicalizations
Rashmi Prasad and Aravind Joshi
University of Pennsylvania
rjprasad,joshi@seas.upenn.edu
Bonnie Webber
University of Edinburgh
bonnie@inf.ed.ac.uk
Abstract
Studies of discourse relations have not, in
the past, attempted to characterize what
serves as evidence for them, beyond lists
of frozen expressions, or markers, drawn
from a few well-defined syntactic classes.
In this paper, we describe how the lexical-
ized discourse relation annotations of the
Penn Discourse Treebank (PDTB) led to
the discovery of a wide range of additional
expressions, annotated as AltLex (alterna-
tive lexicalizations) in the PDTB 2.0. Fur-
ther analysis of AltLex annotation sug-
gests that the set of markers is open-
ended, and drawn from a wider variety
of syntactic types than currently assumed.
As a first attempt towards automatically
identifying discourse relation markers, we
propose the use of syntactic paraphrase
methods.
1 Introduction
Discourse relations that hold between the content
of clauses and of sentences ? including relations
of cause, contrast, elaboration, and temporal or-
dering ? are important for natural language pro-
cessing tasks that require sensitivity to more than
just a single sentence, such as summarization, in-
formation extraction, and generation. In written
text, discourse relations have usually been con-
sidered to be signaled either explicitly, as lexical-
ized with some word or phrase, or implicitly due
to adjacency. Thus, while the causal relation be-
tween the situations described in the two clauses
in Ex. (1) is signalled explicitly by the connective
As a result, the same relation is conveyed implic-
itly in Ex. (2).
(1) John was tired. As a result he left early.
(2) John was tired. He left early.
This paper focusses on the problem of how to
characterize and identify explicit signals of dis-
course relations, exemplified in Ex. (1). To re-
fer to all such signals, we use the term ?discourse
relation markers? (DRMs). Past research (e.g.,
(Halliday and Hasan, 1976; Martin, 1992; Knott,
1996), among others) has assumed that DRMs
are frozen or fixed expressions from a few well-
defined syntactic classes, such as conjunctions,
adverbs, and prepositional phrases. Thus the lit-
erature presents lists of DRMs, which researchers
try to make as complete as possible for their cho-
sen language. In annotating lexicalized discourse
relations of the Penn Discourse Treebank (Prasad
et al, 2008), this same assumption drove the ini-
tial phase of annotation. A list of ?explicit con-
nectives? was collected from various sources and
provided to annotators, who then searched for
these expressions in the text and annotated them,
along with their arguments and senses. The same
assumption underlies methods for automatically
identifying DRMs (Pitler and Nenkova, 2009).
Since expressions functioning as DRMs can also
have non-DRM functions, the task is framed as
one of classifying given individual tokens as DRM
or not DRM.
In this paper, we argue that placing such syn-
tactic and lexical restrictions on DRMs limits
a proper understanding of discourse relations,
which can be realized in other ways as well. For
example, one should recognize that the instantia-
tion (or exemplification) relation between the two
sentences in Ex. (3) is explicitly signalled in the
second sentence by the phrase Probably the most
egregious example is, which is sufficient to ex-
press the instantiation relation.
(3) Typically, these laws seek to prevent executive
branch officials from inquiring into whether cer-
tain federal programs make any economic sense or
proposing more market-oriented alternatives to reg-
ulations. Probably the most egregious example is a
1023
proviso in the appropriations bill for the executive
office that prevents the president?s Office of Man-
agement and Budget from subjecting agricultural
marketing orders to any cost-benefit scrutiny.
Cases such as Ex. (3) show that identifying
DRMs cannot simply be a matter of preparing a
list of fixed expressions and searching for them in
the text. We describe in Section 2 how we identi-
fied other ways of expressing discourse relations
in the PDTB. In the current version of the cor-
pus (PDTB 2.0.), they are labelled as AltLex (al-
ternative lexicalizations), and are ?discovered? as
a result of our lexically driven annotation of dis-
course relations, including explicit as well as im-
plicit relations. Further analysis of AltLex anno-
tations (Section 3) leads to the thesis that DRMs
are a lexically open-ended class of elements which
may or may not belong to well-defined syntactic
classes. The open-ended nature of DRMs is a
challenge for their automated identification, and
in Section 4, we point to some lessons we have
already learned from this annotation. Finally, we
suggest that methods used for automatically gen-
erating candidate paraphrases may help to expand
the set of recognized DRMs for English and for
other languages as well (Section 5).
2 AltLex in the PDTB
The Penn Discourse Treebank (Prasad et al,
2008) constitutes the largest available resource of
lexically grounded annotations of discourse rela-
tions, including both explicit and implicit rela-
tions.1 Discourse relations are assumed to have
two and only two arguments, called Arg1 and
Arg2. By convention, Arg2 is the argument syn-
tactically associated with the relation, while Arg1
is the other argument. Each discourse relation is
also annotated with one of the several senses in the
PDTB hierarchical sense classification, as well as
the attribution of the relation and its arguments.
In this section, we describe how the annotation
methodology of the PDTB led to the identification
of the AltLex relations.
Since one of the major goals of the annota-
tion was to lexically ground each relation, a first
step in the annotation was to identify the explicit
1http://www.seas.upenn.edu/?pdtb
markers of discourse relations. Following stan-
dard practice, a list of such markers ? called ?ex-
plicit connectives? in the PDTB ? was collected
from various sources (Halliday and Hasan, 1976;
Martin, 1992; Knott, 1996; Forbes-Riley et al,
2006).2 These were provided to annotators, who
then searched for these expressions in the corpus
and marked their arguments, senses, and attribu-
tion.3 In the pilot phase of the annotation, we
also went through several iterations of updating
the list, as and when annotators reported seeing
connectives that were not in the current list. Im-
portantly, however, connectives were constrained
to come from a few well-defined syntactic classes:
? Subordinating conjunctions: e.g., because,
although, when, while, since, if, as.
? Coordinating conjunctions: e.g., and, but,
so, either..or, neither..nor.
? Prepositional phrases: e.g., as a result, on
the one hand..on the other hand, insofar as,
in comparison.
? adverbs: e.g., then, however, instead, yet,
likewise, subsequently
Ex. (4) illustrates the annotation of an explicit
connective. (In all PDTB examples in the paper,
Arg2 is indicated in boldface, Arg1 is in italics,
the DRM is underlined, and the sense is provided
in parentheses at the end of the example.)
(4) U.S. Trust, a 136-year-old institution that is one of
the earliest high-net worth banks in the U.S., has
faced intensifying competition from other firms that
have established, and heavily promoted, private-
banking businesses of their own. As a result,
U.S. Trust?s earnings have been hurt. (Contin-
gency:Cause:Result)
After all explicit connectives in the list were
annotated, the next step was to identify implicit
discourse relations. We assumed that such rela-
tions are triggered by adjacency, and (because of
resource limitations) considered only those that
held between sentences within the same para-
graph. Annotators were thus instructed to supply
a connective ? called ?implicit connective? ? for
2All explicit connectives annotated in the PDTB are listed
in the PDTB manual (PDTB-Group, 2008).
3These guidelines are recorded in the PDTB manual.
1024
each pair of adjacent sentences, as long as the re-
lation was not already expressed with one of the
explicit connectives provided to them. This proce-
dure led to the annotation of implicit connectives
such as because in Ex. (5), where a causal relation
is inferred but no explicit connective is present in
the text to express the relation.
(5) To compare temperatures over the past 10,000
years, researchers analyzed the changes in concen-
trations of two forms of oxygen. (Implicit=because)
These measurements can indicate temperature
changes, . . . (Contingency:Cause:reason)
Annotators soon noticed that in many cases,
they were not able to supply an implicit connec-
tive. Reasons supplied included (a) ?there is a re-
lation between these sentences but I cannot think
of a connective to insert between them?, (b) ?there
is a relation between the sentences for which I
can think of a connective, but it doesn?t sound
good?, and (c) ?there is no relation between the
sentences?. For all such cases, annotators were
instructed to supply ?NONE? as the implicit con-
nective. Later, we sub-divided these ?NONE? im-
plicits into ?EntRel?, for the (a) type above (an
entity-based coherence relation, since the second
sentence seemed to continue the description of
some entity mentioned in the first); ?NoRel? (no
relation) for the (c) type; and ?AltLex?, for the (b)
type, which we turn to next.
Closer investigation of the (b) cases revealed
that the awkwardness perceived by annotators
when inserting an implicit connective was due to
redundancy in the expression of the relation: Al-
though no explicit connective was present to re-
late the two sentences, some other expression ap-
peared to be doing the job. This is indeed what
we found. Subsequently, instances of AltLex were
annotated if:
1. A discourse relation can be inferred between
adjacent sentences.
2. There is no explicit connective present to re-
late them.
3. The annotator is not able to insert an im-
plicit connective to express the inferred rela-
tion (having used ?NONE? instead), because
inserting it leads to an awkward redundancy
in expressing the relation.
Under these conditions, annotators were in-
structed to look for and mark as Altlex, whatever
alternative expression appeared to denote the re-
lation. Thus, for example, Ex. (6) was annotated
as AltLex because although a causal relation is in-
ferred between the sentences, inserting a connec-
tive like because makes expression of the relation
redundant. Here the phrase One reason is is taken
to denote the relation and is marked as AltLex.
(6) Now, GM appears to be stepping up the pace of its
factory consolidation to get in shape for the 1990s.
One reason is mounting competition from new
Japanese car plants in the U.S. that are pour-
ing out more than one million vehicles a year
at costs lower than GM can match. (Contin-
gency:Cause:reason)
The result of this procedure led to the annota-
tion of 624 tokens of AltLex in the PDTB. We
turn to our analysis of these expressions in the
next section.
3 What is found in AltLex?
Several questions arise when considering the Alt-
Lex annotations. What kind of expressions are
they? What can we learn from their syntax?
Do they project discourse relations of a different
sort than connectives? How can they be identi-
fied, both during manual annotation and automat-
ically? To address these questions, we examined
the AltLex annotation for annotated senses, and
for common lexico-syntactic patterns extracted
using alignment with the Penn Treebank (Marcus
et al, 1993).4
3.1 Lexico-syntactic Characterization
We found that we could partition AltLex annota-
tion into three groups by (a) whether or not they
belonged to one of the syntactic classes admit-
ted as explicit connectives in the PDTB, and (b)
whether the expression was frozen (ie, blocking
free substitution, modification or deletion of any
of its parts) or open-ended. The three groups are
shown in Table 1 and discussed below.
4The source texts of the PDTB come from the Penn
Treebank (PTB) portion of the Wall Street Journal corpus.
The PDTB corpus provides PTB tree alignments of all its
text span annotations, including connectives, AltLex?s, argu-
ments of relations, and attribution spans.
1025
AltLex Group No (%) Examples
Syntactically
admitted, lexi-
cally frozen
92 (14.7%) quite the contrary (ADVP), for one thing (PP), as well (ADVP),
too (ADVP), soon (ADVP-TMP), eventually (ADVP-TMP),
thereafter (RB), even (ADVP), especially (ADVP), actually
(ADVP), still (ADVP), only (ADVP), in response (PP)
Syntactically
free, lexically
frozen
54 (8.7%) What?s more (SBAR-ADV), Never mind that (ADVP-
TMP;VB;DT), To begin with (VP), So (ADVP-PRD-TPC),
Another (DT), further (JJ), As in (IN;IN), So what if
(ADVP;IN), Best of all (NP)
Syntactically
and lexically
free
478 (76.6%) That compares with (NP-SBJ;VBD;IN), After these payments
(PP-TMP), That would follow (NP-SBJ;MD;VB), The plunge
followed (NP-SBJ;VBD), Until then (PP-TMP), The increase
was due mainly to (NP-SBJ;VBD;JJ;RB;TO), That is why (NP-
SBJ;VBZ;WHADVP), Once triggered (SBAR-TMP)
TOTAL 624 ?
Table 1: Breakdown of AltLex by Syntactic and Lexical Flexibility. Examples in the third column are
accompanied (in parentheses) with their PTB POS tags and constituent phrase labels obtained from the
PDTB-PTB alignment.
Syntactically admitted and lexically frozen:
The first row shows that 14.7% of the strings an-
notated as AltLex belong to syntactic classes ad-
mitted as connectives and are similarly frozen.
(Syntactic class was obtained from the PDTB-
PTB alignment.) So, despite the effort in prepar-
ing a list of connectives (cf. Section 1), additional
ones were still found in the corpus through AltLex
annotation. This suggests that any pre-defined list
of connectives should only be used to guide anno-
tators in a strategy for ?discovering? connectives.
Syntactically free and lexically frozen: AltLex
expressions that were frozen but belonged to syn-
tactic classes other than those admitted for the
PDTB explicit connectives accounted for 8.7%
(54/624) of the total (Table 1, row 2). For exam-
ple, the AltLex What?s more (Ex. 7) is parsed as
a clause (SBAR) functioning as an adverb (ADV).
It is also frozen, in not undergoing any change (eg,
What?s less, What?s bigger, etc.5
(7) Marketers themselves are partly to blame: They?ve
increased spending for coupons and other short-
term promotions at the expense of image-building
advertising. What?s more, a flood of new prod-
ucts has given consumers a dizzying choice of
5Apparently similar headless relative clauses such as
What?s more exciting differ from What?s more in not func-
tioning as adverbials, just as NPs.
brands, many of which are virtually carbon
copies of one other. (Expansion:Conjunction)
Many of these AltLex annotations do not con-
stitute a single constituent in the PTB, as with
Never mind that. These cases suggest that ei-
ther the restrictions on connectives as frozen ex-
pressions should be relaxed to admit all syntactic
classes, or the syntactic analyses of these multi-
word expressions is irrelevant to their function.
Both syntactically and lexically free: This
third group (Table 1, row 3) constitutes the major-
ity of AltLex annotations ? 76.6% (478/624). Ad-
ditional examples are shown in Table 2. Common
syntactic patterns here include subjects followed
by verbs (Table 2a-c), verb phrases with comple-
ments (d), adverbial clauses (e), and main clauses
with a subordinating conjunction (f).
All these AltLex annotations are freely modifi-
able, with their fixed and modifiable parts shown
in the regular expressions defined for them in Ta-
ble 2. Each has a fixed ?core? phrase shown as
lexical tokens in the regular expression, e.g, con-
sequence of, attributed to, plus obligatory and op-
tional elements shown as syntactic labels. Op-
tional elements are shown in parentheses. <NX>
indicates any noun phrase, <PPX>, any prepo-
sitional phrase, <VX>, any verb phrase, and
1026
AltLex String AltLex Pattern
(a) A consequence of their departure could be ... <DTX> consequence (<PPX>) <VX>
(b) A major reason is ... <DTX> (<JJX>) reason (<PPX>) <VX>
(c) Mayhap this metaphorical connection made ... (<ADVX>) <NX> made
(d) ... attributed the increase to ... attributed <NX> to
(e) Adding to that speculation ... Adding to <NX>
(f) That may be because ... <NX> <VX> because
Table 2: Complex AltLex strings and their patterns
<JJX>, any adjectival phrase
These patterns show, for example, that other
variants of the identified AltLex A major reason
is include The reason is, A possible reason for the
increase is, A reason for why we should consider
DRMs as an open class is, etc. This is robust sup-
port for our claim that DRMs should be regarded
as an open class: The task of identifying them can-
not simply be a matter of checking an a priori list.
Note that the optional modification seen here
is clearly also possible with many explicit con-
nectives such as if (eg, even if just if, only if ),
as shown in Appendix C of the PDTB manual
(PDTB-Group, 2008). This further supports the
thesis that DRMs should be treated as an open
class that includes explicit connectives.
3.2 Semantic Characterization
AltLex strings were annotated as denoting the dis-
course relation that held between otherwise un-
marked adjacent utterances (Section 2). We found
them to convey this relation in much the same
way as anaphoric discourse adverbials. Accord-
ing to (Forbes-Riley et al, 2006), discourse ad-
verbials convey both the discourse relation and an
anaphoric reference to its Arg1. The latter may be
either explicit (e.g., through the use of a demon-
strative like ?this? or ?that?), or implicit. Thus,
both as a result of that and as a result are dis-
course adverbials in the same way: the latter refers
explicitly to Arg1 via the pronoun ?that?, while
former does so via an implicit internal argument.
(A result must be a result of something.)
The examples in Table 2 make this same two?
part semantic contribution, albeit with more com-
plex expressions referring to Arg1 and more com-
plex modification of the expression denoting the
relation. For example, in the AltLex shown in
(Table 2c), Mayhap this metaphorical connection
made (annotated in Ex. (8)), the relation is de-
noted by the causal verb made, while Arg1 is
referenced through the definite description this
metaphorical connection. In addition, the adverb
Mayhap further modifies the relational verb.
(8) Ms. Bartlett?s previous work, which
earned her an international reputation
in the non-horticultural art world, of-
ten took gardens as its nominal subject.
Mayhap this metaphorical connection made
the BPC Fine Arts Committee think she had a
literal green thumb. (Contingency:Cause:Result)
These complex AltLex?s also raise the question
of why we find them at all in language. One part of
the answer is that these complex AltLex?s are used
to convey more than just the meaning of the rela-
tion. In most cases, we found that substituting the
AltLex with an adverbial connective led to some
aspect of the meaning being lost, as in Ex. (9-
10). Substituting For example for the AltLex with
an (necessary) accompanying paraphrase of Arg2
loses the information that the example provided as
Arg2 is possibly the most egregious one. The con-
nective for example does not allow similar modi-
fication. This means that one must use a different
strategy such as an AltLex expression.
(9) Typically, these laws seek to prevent exec-
utive branch officials from inquiring into
whether certain federal programs make
any economic sense or proposing more
market-oriented alternatives to regulations.
Probably the most egregious example is a pro-
viso in the appropriations bill for the executive
office that prevents the president?s Office of
Management and Budget from subjecting agri-
cultural marketing orders to any cost-benefit
scrutiny. (Expansion:Instantiation)
(10) For example, a proviso in the appropriations bill
for the executive office prevents the president?s Of-
1027
fice of Management and Budget from subjecting
agricultural marketing orders to any cost-benefit
scrutiny.
Another part of the answer to Why AltLex? is
that it can serve to convey a relation for which the
lexicon lacks an adverbial connective. For exam-
ple, while English has several adverbial connec-
tives that express a ?Cause:Consequence? relation
(eg, as a result, consequently, etc.), it lacks an
adverbial connective expressing ?Cause:Reason?
(or explanation) albeit having at least two sub-
ordinating conjunctions that do so (because and
since). Thus, we find an AltLex whenever this re-
lation needs to be expressed between sentences, as
shown in Ex. (11).
(11) But a strong level of investor withdrawals is
much more unlikely this time around, fund man-
agers said. A major reason is that investors al-
ready have sharply scaled back their purchases
of stock funds since Black Monday. (Contin-
gency:Cause:reason)
Note, however, that even for such relations such
as Cause:Reason, it is still not the case that a list of
canned expressions will be sufficient to generate
the Altlex or to identify them, since this relation
can itself be further modified. In Ex. (12), for ex-
ample, the writer intends to convey that there are
multiple reasons for the walkout, although only
one of them is eventually specified in detail.
(12) In Chile, workers at two copper mines, Los
Bronces and El Soldado, which belong to the
Exxon-owned Minera Disputada, yesterday voted
to begin a full strike tomorrow, an analyst
said. Reasons for the walkout, the analyst said,
included a number of procedural issues, such as
a right to strike. (Contingency:Cause:reason)
4 Lessons learned from AltLex
Like all lexical phenomena, DRMs appear to
have a power-law distribution, with some very
few high-frequency instances like (and, but), a
block of mid-frequency instances (eg, after, be-
cause, however), and many many low-frequency
instances in the ?long tail? (eg, much as, on the
contrary, in short, etc.). Given the importance
of DRMs for recognizing and classifying dis-
course relations and their arguments, what have
we learned from the annotation of AltLex?
First, the number of expressions found through
AltLex annotation, that belong to syntactic classes
admitted as connectives and also similarly frozen
(Table 1, row 1) shows that even in the PDTB,
there are additional instances of what we have
taken to be explicit connectives. By recognizing
them and unambiguously labelling their senses,
we will start to reduce the number of ?hard cases?
of implicit connectives whose sense has to be rec-
ognized (Marcu and Echihabi, 2002; Sporleder
and Lascarides, 2008; Pitler et al, 2009; Lin et al,
2009). Secondly, the number of tokens of expres-
sions from other syntactic classes that have been
annotated as AltLex (Table 1, rows 2 and 3) may
actually be higher than was caught via our Alt-
Lex annotation, thus making them even more im-
portant for discourse processing. To assess this,
we selected five of them and looked for all their
tokens in the WSJ raw files underlying both the
PTB and the PDTB. After eliminating those to-
kens that had already been annotated, we judged
whether the remaining ones were functioning as
connectives. Table 3 shows the expressions we
used in the first column, with the second and third
columns reporting the number of tokens annotated
in PDTB, and the number of additional tokens in
the WSJ corpus functioning as connectives. (The
asterisk next to the expressions is a wild card to al-
low for variations along the lines discussed for Ta-
ble 2.) These results show that these DRMs occur
two to three times more frequently than already
annotated.
Increased frequencies of AltLex occurrence are
also observed in discourse annotation projects un-
dertaken subsequent to the PDTB, since they were
able to be more sensitive to the presence of Alt-
Lex. The Hindi Discourse Relation Bank (HDRB)
(Oza et al, 2009), for example, reports that 6.5%
of all discourse relations in the HDRB have been
annotated as AltLex, compared to 1.5% in the
PDTB. This also provides cross-linguistic evi-
dence of the importance of recognizing the full
range of DRMs in a language.
5 Identifying DRMs outside the PDTB
As the set of DRMs appears to be both open-ended
and distributed like much else in language, with
a very long tail, it is likely that many are miss-
ing from the one-million word WSJ corpus anno-
tated in the PDTB 2.0. Indeed, in annotating En-
1028
AltLex Annotated Unannotated
The reason* 8 15
That?s because 11 16
The result* 12 18
That/This would* 5 16
That means 11 17
TOTAL 47 82
Table 3: Annotated and Unannotated instances of AltLex
glish biomedical articles with discourse relations,
Yu et al(2008) report finding many DRMs that
don?t appear in the WSJ (e.g., as a consequence).
If one is to fully exploit DRMs in classifying
discourse relations, one must be able to identify
them all, or at least many more of them than we
have to date. One method that seems promising
is Callison-Burch?s paraphrase generation through
back-translation on pairs of word-aligned corpora
(Callison-Birch, 2007). This method exploits the
frequency with which a word or phrase is back
translated (from texts in language A to texts in
language B, and then back from texts in language
B to texts in language A) across a range of pivot
languages, into other words or phrases.
While there are many factors that introduce
low-frequency noise into the process, including
lexical ambiguity and errors in word alignment,
Callison-Burch?s method benefits from being able
to use the many existing word-aligned translation
pairs developed for creating translation models for
SMT. Recently, Callison-Burch showed that para-
phrase errors could be reduced by syntactically
constraining the phrases identified through back-
translation to ones with the same syntactic cat-
egory as assigned to the source (Callison-Birch,
2008), using a large set of syntactic categories
similar to those used in CCG (Steedman, 2000).
For DRMs, the idea is to identify through back-
translation, instances of DRMs that were neither
included in our original set of explicit connec-
tive nor subsequently found through AltLex an-
notation. To allow us to carry out a quick pi-
lot study, Callison-Burch provided us with back-
translations of 147 DRMs (primarily explicit con-
nectives annotated in the PDTB 2.0, but also in-
cluding a few from other syntactic classes found
through AltLex annotation). Preliminary analysis
of the results reveals many DRMs that don?t ap-
pear anywhere in the WSJ Corpus (eg, as a con-
sequence, as an example, by the same token), as
well as additional DRMs that appear in the cor-
pus but were not annotated as AltLex (e.g., above
all, after all, despite that). Many of these latter
instances appear in the initial sentence of a para-
graph, but the annotation of implicit connectives
? which is what led to AltLex annotation in the
first place (Section 2) ? was not carried out on
these sentences.
There are two further things to note before clos-
ing this discussion. First, there is an additional
source of noise in using back-translation para-
phrase to expand the set of identified DRMs. This
arises from the fact that discourse relations can
be conveyed either explicitly or implicitly, and
a translated text may not have made the same
choices vis-a-vis explicitation as its source, caus-
ing additional word alignment errors (some of
which are interesting, but most of which are not).
Secondly, this same method should prove useful
for languages other English, although there will be
an additional problem to overcome for languages
(such as Turkish) in which DRMs are conveyed
through morphology as well as through distinct
words and phrases.
6 Related work
We are not the first to recognize that discourse re-
lations can realized by more than just one or two
syntactic classes. Halliday and Hasan (1976) doc-
ument prepositional phrases like After that being
used to express conjunctive relations. More im-
portantly, they note that any definite description
can be substituted for the demonstrative pronoun.
1029
Similarly, Taboada (2006), in looking at how of-
ten RST-based rhetorical relations are realized by
discourse markers, starts by considering only ad-
verbials, prepositional phrases, and conjunctions,
but then notes the occurrence of a single instance
of a nominal fragment The result in her corpus.
Challenging the RST assumption that the basic
unit of a discourse is a clause, with discourse rela-
tions holding between adjacent clausal units, Kib-
ble (1999) provides evidence that informational
discourse relations (as opposed to intentional dis-
course relations) can hold intra-clausally as well,
with the relation ?verbalized? and its arguments
realized as nominalizations, as in Early treatment
with Brand X can prevent a cold sore developing.
Since his focus is intra-clausal, he does not ob-
serve that verbalized discourse relations can hold
across sentences as well, where a verb and one
of its arguments function similarly to a discourse
adverbial, and in the end, he does not provide a
proposal for how to systematically identify these
alternative realizations. Le Huong et al (2003),
in developing an algorithm for recognizing dis-
course relations, consider non-verbal realizations
(called NP cues) in addition to verbal realizations
(called VP cues). However, they provide only one
example of such a cue (?the result?). Like Kib-
ble (1999), Danlos (2006) and Power (2007) also
focus only on identifying verbalizations of dis-
course relations, although they do consider cases
where such relations hold across sentences.
What has not been investigated in prior work
is the basis for the alternation between connec-
tives and AltLex?s, although there are several ac-
counts of why a language may provide more than
one connective that conveys the same relation.
For example, the alternation in Dutch between
dus (?so?), daardoor (?as a result?), and daarom
(?that?s why?) is explained by Pander Maat and
Sanders (2000) as having its basis in ?subjectiv-
ity?.
7 Conclusion and Future Work
Categorizing and identifying the range of ways in
which discourse relations are realized is impor-
tant for both discourse understanding and gener-
ation. In this paper, we showed that existing prac-
tices of cataloguing these ways as lists of closed
class expressions is problematic. We drew on our
experience in creating the lexically grounded an-
notations of the Penn Discourse Treebank, and
showed that markers of discourse relations should
instead be treated as open-class items, with uncon-
strained syntactic possibilities. Manual annota-
tion and automatic identification practices should
develop methods in line with this finding if they
aim to exhaustively identify all discourse relation
markers.
Acknowledgments
We want to thank Chris Callison-Burch, who
graciously provided us with EuroParl back-
translation paraphrases for the list of connectives
we sent him. This work was partially supported
by NSF grant IIS-07-05671.
References
Callison-Birch, Chris. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, School of Informatics, Univer-
sity of Edinburgh.
Callison-Birch, Chris. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Danlos, Laurence. 2006. Discourse verbs. In Pro-
ceedings of the 2nd Workshop on Constraints in Dis-
course, pages 59?65, Maynooth, Ireland.
Forbes-Riley, Katherine, Bonnie Webber, and Aravind
Joshi. 2006. Computing discourse semantics: The
predicate-argument semantics of discourse connec-
tives in D-LTAG. Journal of Semantics, 23:55?106.
Halliday, M. A. K. and Ruqaiya Hasan. 1976. Cohe-
sion in English. London: Longman.
Huong, LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2003. Using cohesive devices to recog-
nize rhetorical relations in text. In Proceedings of
4th Computational Linguistics UK Research Collo-
quium (CLUK 4), University of Edinburgh, UK.
Kibble, Rodger. 1999. Nominalisation and rhetorical
structure. In Proceedings of ESSLLI Formal Gram-
mar conference, Utrecht.
Knott, Alistair. 1996. A Data-Driven Methodology
for Motivating a Set of Coherence Relations. Ph.D.
thesis, University of Edinburgh, Edinburgh.
1030
Lin, Ziheng, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, Singapore.
Maat, Henk Pander and Ted Sanders. 2000. Do-
mains of use or subjectivity? the distribution of
three dutch causal connectives explained. TOPICS
IN ENGLISH LINGUISTICS, pages 57?82.
Marcu, Daniel and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the Association for Com-
putational Linguistics.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Martin, James R. 1992. English text: System and
structure. Benjamins, Amsterdam.
Oza, Umangi, Rashmi Prasad, Sudheer Kolachina,
Dipti Mishra Sharma, and Aravind Joshi. 2009.
The hindi discourse relation bank. In Proceedings
of the ACL 2009 Linguistic Annotation Workshop III
(LAW-III), Singapore.
Pitler, Emily and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the Joint Conference of the 47th
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference
on Natural Language Processing, Singapore.
Pitler, Emily, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse
relations in text. In Proceedings of the Joint Con-
ference of the 47th Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing.
Power, Richard. 2007. Abstract verbs. In ENLG ?07:
Proceedings of the Eleventh European Workshop on
Natural Language Generation, pages 93?96, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of 6th International Conference on
Language Resources and Evaluation (LREC 2008).
PDTB-Group. 2008. The Penn Discourse TreeBank
2.0 Annotation Manual. Technical Report IRCS-08-
01, Institute for Research in Cognitive Science, Uni-
versity of Pennsylvania.
Sporleder, Caroline and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: an assessment. Natural Language En-
gineering, 14(3):369?416.
Steedman, Mark. 2000. The Syntactic Process. MIT
Press, Cambridge MA.
Taboada, Maite. 2006. Discourse markers as signals
(or not) of rhetorical relations. Journal of Pragmat-
ics, 38(4):567?592.
Yu, Hong, Nadya Frid, Susan McRoy, P Simpson,
Rashmi Prasad, Alan Lee, and Aravind Joshi. 2008.
Exploring discourse connectivity in biomedical text
for text mining. In Proceedings of the 16th Annual
International Conference on Intelligent Systems for
Molecular Biology BioLINK SIG Meeting, Toronto,
Canada.
1031
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 667?674, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UWM-TRIADS: Classifying Drug-Drug Interactions with Two-Stage SVM and Post-Processing   Majid Rastegar-Mojarad Richard D. Boyce Rashmi Prasad University of Wisconsin-Milwaukee University of Pittsburgh University of Wisconsin-Milwaukee Milwaukee, WI, USA Pittsburgh, PA, USA Milwaukee, WI, USA Rastega3@uwm.edu rdb20@pitt.edu prasadr@uwm.edu    Abstract 
We describe our system for the DDIExtraction-2013 shared task of classifying Drug-Drug interactions (DDIs) given labeled drug mentions. The challenge called for a five-way classification of all drug pairs in each sentence: a drug pair is either non-interacting, or interacting as one of four types. Our approach begins with the use of a two-stage weighted SVM classifier to handle the highly unbalanced class distribution: the first stage for a binary classification of drug pairs as interacting or non-interacting, and the second stage for further classification of interacting pairs from the first stage into one of the four interacting types. Our SVM features exploit stemmed words, lemmas, bigrams, part of speech tags, verb lists, and similarity measures, among others. For each stage, we also developed a set of post-processing rules based on observations in the training data. Our best system achieved 0.472 F-measure.  1 Introduction Potential drug-drug interactions (DDIs), defined as the co-prescription of two drugs that are known to interact, are a significant source of pre-ventable drug-related harm (i.e., adverse drug events, or ADEs) (Nebeker et al, 2004). Gurwitz et al in their cohort study of ADEs among older Americans receiving ambulatory care, found that 13.3% of preventable errors leading to an ADE involved the co-prescription of drugs for which a ?...well established, clinically important interac-tion? was known (Gurwitz et al, 2003). Nearly 7% (23/338) of the ADEs experienced by resi-dents of two academic nursing homes over a nine-month period were attributable to DDIs (Gurwitz et al, 2005). Sixteen cohort and case-control studies reported an elevated risk of hospi-
talization in patients who were exposed to DDIs (Hines et al, 2011). Failure to properly manage a DDI is a medical error, and the Institute of Medicine has noted that a lack of drug knowledge is one of the most fre-quent proximal causes of such errors (Committee on Identifying and Preventing Medication Errors, 2007). Indeed, health care providers often have inadequate knowledge of what drug interactions can occur, of patient specific factors that can in-crease the risk of harm from an interaction, and how to properly manage an interaction when pa-tient exposure cannot be avoided (Chen et al, 2005; Hines et al, 2012). Unfortunately, there is no single complete and authoritative source of DDI knowledge (Hines et al, 2012). Rather, there are multiple sources, each tasked with extracting, evaluating, and stay-ing up-to-date with pertinent DDIs reported in the literature, and drug product labeling (Boyce et al, 2012). The dynamic nature of drug knowledge, combined with the enormity of the biomedical literature, makes this task extremely challenging. Hence, natural language processing methods for identifying and extracting DDIs are receiving increased attention.   In 2011, the first shared task challenge for DDI extraction, DDIExtraction-2011 (Segura-Bedmar et al, 2011), invited participants to develop au-tomatic methods to extract DDIs. The task fo-cused on the identification of all possible pairs of interacting drugs, without specifying anything further about the interactions. By contrast, the DDIExtraction-2013 (Segura-Bedmar et al, 2013) shared task emphasized the importance of recognizing what is being asserted about the in-teraction. Accordingly, the challenge called for a 
667
five-way classification of sentences for each drug-pair: ? Advice: the sentence notes a recommendation or advice related to the concomitant use of the two drugs (e.g., ?? UROXATRAL should NOT be used in combination with other alpha-blockers.?); ? Effect: the sentence states the effect of the drug interaction, including pharmacodynamic effect or mechanism of interaction (e.g., ?Quinolones may enhance the effects of the oral anticoagulant, warfarin, ??); ? Mechanism: the sentence describes a phar-macokinetic mechanism (e.g., ?Grepafloxa-cin is a competitive inhibitor of the metabolism of theophylline.?). ? Int: the sentence mentions a drug interaction but doesn?t provide any additional infor-mation (e.g., ?The interaction of omeprazole and ketoconazole has been established.?). ? None: the sentence does not show an interac-tion between the two drugs; To focus on, and separately evaluate, different aspects of the problem, the 2013 shared task was divided into two subtasks. One task focused on the recognition and classification of drug names, while the other focused on the identification and classification of DDIs, with the drug names pro-vided from the gold standard. In this paper, we describe our approach for handling the second task, namely, DDI identification and classifica-tion of all possible pairs of drugs in the provided corpus. Our approach combined machine-learning methods with the use of rules for post-processing. A key feature of our machine-learning approach is that it is specifically de-signed to handle the highly unbalanced class dis-tribution via the use of a two-stage weighted SVM classifier. In addition to a variety of fea-tures exploited for the classifier, we also devel-oped a set of post-processing rules, with a different set of rules applied after each stage of SVM classification. Finally, our approach is also aimed towards exploring the efficacy of methods that do not need to rely on syntactic-parse based features. The paper is organized as follows. In the next section, we describe the training and test data set 
used in the challenge. In section 3, we describe our method, the classifiers used at each stage, their features, and post processing. In section 4, we present the evaluation and results. We con-clude in Section 5 with discussion and future work.  2 Data The DDIExtraction-2013 challenge provided a DDI corpus for development, containing 142 Medline abstracts on the subject of drug-drug interactions, and 572 documents describing drug-drug interactions from the DrugBank database. The corpus includes 6976 sentences that were annotated with four types of pharmacological entities and four types of DDIs. The DDIs types are: advice, effect, mechanism, and int.1 Table 1 shows the number of instances for each type. Ex-amples can be seen in Section 1. The test set in-cludes 33 Medline abstracts and 158 DrugBank documents containing 1299 sentences and 5519 drug pairs. 
Table 1: Number of instances in each class 3 Methods Classification of each drug pair in a sentence in-volved distinguishing between 5 classes, advice, effect, mechanism, int and none. As described in Section 2 (see Table 1), a major challenge in this task is posed by the unbalanced distribution of the classes. First, considering just the positive vs. negative classes, just 16.9% (4037/23772) of drug pairs are in the positive class, which include interacting drug pairs (labeled as advice, effect, mechanism and int). Furthermore, the four types                                                 1 http://www.cs.york.ac.uk/semeval-2013/task9/data/uploads/task-9.2-ddi-extraction.pdf 
Type Number 
Positive Advice 827 
Effect 1700 
Mechanism 1322 
Int 188 
Negative None (non-interacting drugs) 23772 
Total 27809 
668
within the positive class are also unbalanced, with the int type constituting only 4.6% (188/4037) of the instances. A classifier trained on this data will, therefore, be biased towards the majority class(es). We employed a two-stage classification approach to cope with this problem, as described below. 3.1 Two-stage classification Figure 1 shows the architecture of the system. In the first stage, we trained a binary classifier to classify drug pairs into positive and negative classes. Then, in the second stage, we considered only instances that were classified as positive by the first classifier, and classified them into ad-vice, effect, mechanism, and int classes, using a multi-class classifier.  A two-stage classifier of-fers a distinct advantage over a one-stage classi-fier for the DDI data set, which is highly skewed towards one class, but particularly because this majority class is also clearly semantically distinct from the other positive classes (see Table 1). By reframing part of this problem as a binary classification task, we can exploit binary clas-sification techniques and allow the classifier to be particularly attentive to features distin-guishing positive and negative drug pairs, while at the same time avoiding the bias against each of the non-majority classes. Our experiments with the training set confirm this idea. Despite the above advantage of a two-stage SVM, however, the unbalanced class problem still remains, especially for training at the first stage, where we have 20854 negative instances and 4026 positives instances. In the second stage, the data is somewhat unbal-anced as well, with   20.5% as advice, 42.2% as effect, 32.6% as mechanism, and only 4.7% as int. To handle this problem further, we ex-plored different approaches and algorithms, including SMOTE (Chawla et al 2002) and other resampling algorithms. Our best results over the training data were obtained with Support Vector Machine (SVM) with differ-ent class weights. We used LibSVM (Chang and Lin, 2011) and set class weights for each stage using results of cross-validation over the training data (see Table 3 for class 
weights).  As we wanted to pass the positively classified instances from the first stage to the second stage classifier, we favored the positive class in the first stage. This resulted in a relatively high num-ber of false positives for the positive instances, which we attempted to reduce with a set of post-processing rules before sending them to the se-cond stage classifier. A different set of post-processing rules were also developed to apply on the output of the second stage classifier. 3.2 Pre-processing Before classification, all sentence instances in the training and test set were pre-processed for the following:  ? All letters were changed to lower case. ? All drug names were normalized by replacing them with one of two strings; one used for drug mentions that were candidates for clas-
One/more instances 
Pre-Processing POS tagger 
Stop Words list Lemmatizer 
Stemmer 
Sentence with more than two drugs 
Final Classification 
Post-Processing 
Post-Processing 
Instances classi-fied as positive Instances classi-fied as negative 
First Stage Binary Classifier (Weighted-SVM) 
Second Stage Multi-Class Classifier (Weighted-SVM) 
Classified as positive Classified as negative 
Figure 1: The Architecture of the system 
669
sification in the instance, and the other used for all other drug mentions.  ? All numbers were normalized by replacing them with the same string. ? Stop words and punctuation were removed. We used different stop word lists for differ-ent systems that were submitted to the chal-lenge. ? Part of speech (POS) tags were obtained with the Stanford NLP tool (Toutanova et al 2003). ? Words were stemmed with the Porter Stem-mer (Porter, 1980). ? Words were lemmatized with the dragon tool (Zhou et al 2007). ? Synsets for words were obtained using WordNet (Fellbaum, 1998). 3.3 Features Since each sentence can have more than two drug mentions, we generated an instance of the sen-tence for each drug pair. We used different com-binations of various features for the three different systems submitted to the challenge (Section 3.4.3). The following describes all the features separated into two categories: features per sentence and features per drug-pair instances.   Features per sentence: These are sentence-level features that have the same values across all in-stances of a sentence. 1- Words: This is a binary feature for all words that appeared more than once in the corpus, indicating the presence or absence of each such word in the sentence. We considered stemmed words as well as lemmatized words.  2- Word bigrams: This is a binary feature for all word bigrams that appeared more than once in the corpus, indicating the presence or ab-sence of each such bigram in the sentence 3- Number of words: This feature represents the total number of words in the sentence 4- Number of drug mentions: This feature repre-sents the total number of drug mentions in the sentence.  5- Cosine similarity between centroid vector of each class and the instance: Inspired by the vector space Information Retrieval approach, we added new features to represent the co-
sine similarity between a sentence and the centroid of normalized vectors for sentences assigned the class X. Cosine similarity is cal-culated based on modified tf*idf. We com-puted modified tf*idf for a word w in class C, based on the following formula:  (TF * IDF)w,C = log(count(w,C)+1)*log(total # Inst / (# inst _ contains_w+1))   TF is the logarithm of the number of times the word occurs in all sentences assigned to the class. IDF is 1.0 divided by the logarithm of number of instances in the class divided by the number of times the word occurs across all classes. To cal-culate the centroid vector for class C, a vector is created for each sentence in class C by giving each word in the sentence a modified TF*IDF weight. The centroid vector for class C is the mean of all vectors of sentences in class C. The Cosine similarity between a given instance and the centroid vector of each class is then used a feature.  Features per instance (for each drug-pair): In contrast to sentence-level features, these features may have different values across the different drug-pair instances. In each instance, we distin-guished the two main drugs of interest for the instance from all other additional drugs men-tioned in the instance. 1- Number of words between two main drugs: This represents the total number of words be-tween the two main drugs.  2- Number of drugs between two main drugs: This represents the total number of additional drugs appearing between the two main drugs. 3- Number of verbs: We used the number of verbs in the instance as a feature, but relative to their sentential position. In particular, we split each instance into three sections: (i) be-fore the first main drug, (ii) between the two main drugs, and (iii) after the second main drug. Then, we counted the number of verbs in each section, and used them as three dif-ferent features. 4- Number of verbs using class-specific verb lists: For each class, we extracted two lists of verbs. The first list contains verbs that ap-
670
peared in just that class but not in the others. Thus, the set of verbs extracted for each class are unique and different from the verbs asso-ciated with other classes. The second list in-cludes all verbs that appeared in that class and their synonyms, extracted from Word-Net. Then, for each of the three sentence sec-tions, as described above, we created two features to represent the number of verbs from each of these lists that appeared in the section. (An alternative way to represent this feature would be to weight the verbs accord-ing to their relative frequencies in the differ-ent classes.) 5- POS of words between two main drugs: This is a binary feature for word POS tags ob-tained from POS tagging, and indicates the presence or absence of each POS between the two main drugs. 3.4 Post processing As described in Section 3.1, we developed a set of post-processing rules for each stage of the classifier. Here, we describe these rules, devel-oped on the basis of observations in the training data.  3.4.1 Post processing after the first stage Post-processing rules for the first stage were de-signed to reduce the number of false positives for the positive class, since the weight assignment in this stage favors this class. We provide examples for each rule:  ? The instance is classified as negative if both drug mentions have the same name, since a drug cannot interact with itself.   ?In controlled clinical trials of AUGMENTIN XR, 22 patients received concomitant allopurinol and AUGMENTIN XR.?  ? The instance is classified as negative if one of the drugs is a plural form of the other one, since, as above, they refer to the same drug.    ?Oral Anticoagulants: Interaction studies with warfarin failed to identify any clinically im-portant effect on the serum concentrations of the anticoagulant or on its anticoagulant effect.? 
 ? The instance is classified as negative if one of the drug mentions refers to a drug class name of the other, since we don?t expect a drug to interact with its class. Drug class names were obtained from a classification provided by the FDA.2 In the example below, ?MAOI? is the drug class name for ?isocar-boxazid?.   ?You cannot take mazindol if you have taken a monoamine oxidase inhibitor (MAOI) such as isocarboxazid (Marplan), tranylcypromine (Par-nate), or phenelzine (Nardil) in the last 14 days.?  ? The instance is classified as negative if ?,? or ?, and? appears between the two main drug mentions, and is accompanied by an addi-tional drug mention. This rules identifies contexts where drugs are mentioned as a set, in interaction with a different drug. The fol-lowing sentences show ?glyburide?, ?tolbut-amide? and ?glipzide? as part of a set of drugs in interaction with the additional drug ?DIFLUCAN?.   ?DIFLUCAN reduces the metabolism of tolbut-amide, glyburide, and glipizide and increases the plasma concentration of these agents.?   ?DIFLUCAN reduces the metabolism of tolbut-amide, glyburide, and glipizide and increases the plasma concentration of these agents.?  ? The sentence is classified as negative if ?,? and additional drugs appear between the main drug mentions. Like the previous rule, this again recognizes drugs mentioned as a set but identifies non-adjacent mentions. For example, the following sentence doesn?t ex-press any interaction between ?tolbutamide? and ?glipizide?, and the rule recognizes them as part of a set mention even though they are non-adjacent.  ?DIFLUCAN reduces the metabolism of tolbut-amide, glyburide, and glipizide and increases the plasma concentration of these agents.?                                                  2http://www.fda.gov/ForIndustry/DataStandards/StructuredProductLabeling/ucm162549.htm 
671
? The instance is classified as negative if ?or? appears between the two main drug mentions and the sentence contains additional drug mentions. The presence of additional drug mentions in the sentence is required here since such conjoined pairs can interact with each other when they occur alone.   ?Concurrent ingestion of antacid (20 mL of ant-acid containing aluminum hydroxide, magnesium hydroxide, and simethicone) did not significantly affect the exposure of oxybutynin or desethyloxy-butynin.? 3.4.2 Post processing after the second stage Post-processing after the second classifier identi-fies sentences like the following:  ?Coadministration of alosetron and strong CYP3A4 inhibitors, such as clarithromycin, teli thromycin, protease inhibitors, voriconazole, and itraconazole has not been evaluated but should be undertaken with caution because of similar potential drug interac-tions.?  Examples like these illustrate that if drugs are mentioned as a set, then all drugs in the set must have the same interaction type with a drug men-tioned outside the set. Thus, in the example, the interaction of each of ?clarithromycin?, ?teli-thromycin?, ?protease inhibitors?, ?voricona-zole?, and ?itraconazole? with ?alosetron? should be classified in the same way. We used several syntactic and lexical cues to identify set mentions of drugs. Then, since the SVM classi-fier can make different decisions for each such pair (e.g., it may assign one label to the interac-tion of ?clarithromycin? with  ?alosetron? and 
another label to the interaction of ?telithromycin? with ?alosetron?), we applied uniform labeling for the interaction of all such pairs. The majority label was used as the common label. Ties were not encountered in this data, although a solution would have to be devised otherwise. An important consideration for this rule is that it uses both positively and negatively labeled in-stances. The former are taken from the result of the second stage classifier, and the latter from the negative instances of the first stage classifier and the negative instances of the first post-processor. These varied inputs to the rule are illustrated by the three ingoing arrows into the second post-processor in Figure 1.  3.4.3 Submitted Systems   We used the Weka (Hall et al 2009) tool for all experiments and submitted three systems (Sys-tem1, System 2, and System 3 in Table 2) to the challenge.  All systems used the same two-stage approach and SVM classification (LibSVM), but differed in the use of some of the features (Sec-tion 3.3) and in the weights assignment (Table 3). We used linear kernel and the cost (C) was 1.2 and gamma was 0.5.  In System 1, we used stemmed words (instead of lemmatized words) and a stop word list of 165 words. In System 2, we used stemmed words again, but a different 
System Stage Class Weight System 1 First Stage  Positive 6.5 Negative 1.0 Second Stage Advice 800.0 Effect 600.0 Int 3200.0 Mechanism 500.0 System 2 First Stage  Positive 2.5 Negative 1.0 Second Stage Advice 800.0 Effect 600.0 Int 3200.0 Mechanism 500.0 System 3 First Stage  Positive 6.5 Negative 1.0 Second Stage Advice 80.0 Effect 60.0 Int 320.0 Mechanism 50.0 Table 3: Class weight assignments in different systems 
System Metric Drug-Bank Medline All System 1 Prec 0.44 0.21 0.43 Rec 0.49 0.23 0.47 F 0.46 0.22 0.45 System 2 Prec 0.49 0.30 0.47 Rec 0.49 0.41 0.47 F 0.49 0.35 0.47 System 3 Prec 0.42 0.26 0.40 Rec 0.51 0.47 0.50 F 0.46 0.33 0.44 Table 2. Results of each system. The three systems are described in Section 3.4.3. 
672
stop word list of 263 words. Finally, in System 3, we used lemmatized words and the same stop word list of 263 words as in System 2. Weights assignment was different across all systems, as shown in Table 3. 4 Results Table 2 shows the evaluation results of our sys-tem over the test set. Our best results are achieved with System 2, in which we used stemmed words and our 263 stop word list, in addition to the other features described in Section 3.3. Both the stop word list and the use of stemmed vs. lemmatized words can be seen to affect the performance. Clearly, a larger stop word list is more useful, since both System 2 and System 3 show an improvement over System 1. On the other hand, the use of lemmas (used in System 3) seems to be detrimental, compared with stemmed words.  5 Conclusion and future work To the best of our knowledge, this is the first study to explore the value of a two-stage SVM classification process for performing the complex task of identifying sentences describing DDIs, and making the important distinction between statements providing advice, mechanism and ef-fect, or declaring a pharmacokinetic and pharma-codynamic DDI: critical distinctions in the fields of pharmacology and pharmacy. We find that the use of a two-stage classifier to handle the prob-lem of an unbalanced class distribution for the task of identifying and classifying DDIs is feasi-ble but requires further development. It?s valuable to consider these results within the context of previous efforts for extracting DDIs. Ten research papers were presented at the 2011 SemEval Conference (Segura-Bedmar et al 2011) which used a smaller DDI corpus (Medline abstracts were not included) and a simpler classi-fication task (Segura-Bedmar et al 2010). The best performing system in this challenge utilized an ensemble learning approach (Thomas et al 2011) and produced an F-measure of 0.657. The  second best performing method utilized compo-site kernels, a method that combines feature-based and kernel-based methods, and was found 
to perform with an F-measure of 0.64 (Chow-dhury et al 2011). Other NLP research has fo-cused exclusively on extracting pharmacokinetic DDIs from either Medline (e.g., Airola et al 2008) or drug product labeling (e.g., Boyce et al 2012). Due to time constraints, we couldn?t test other classifiers such as Na?ve Bayes, JRip and Ran-domforest in our approach. Future work will test if SVM is the best choice for the first stage bina-ry classifier. It is possible that libShortText (Yu et al 2013) works better than LibSVM because this task is for sentence classification. We also plan to explore if Na?ve Bayes, JRip, or Random-forest could work better than SVM for the second stage multi-class classifier.  Since only three systems were permitted to the challenge, and since the labeled test data was not available until the time of writing, we did not have the opportunity to test the impact of all the features that we considered, or of the post-processing rules. This will be explored in future work.  We also plan to explore some variations to our approach. For example, we will try to incorporate some of the rules, especially those in the first post-processor, as features in our system. Finally, although we did utilize some semantic infor-mation from WordNet for this work, we would like to explore additional rich features, drawing on syntax, semantics and discourse.   References  Airola A., S. Pyysalo, J. Bj?rne, T. Pahikkala, F. Ginter and T. Salakoski. 2008. All-paths graph ker-nel for protein-protein interaction extraction with evaluation of cross-corpus learning. BMC Bioin-formatics 9. Suppl 11 (2008): S2 Boyce R. D., C. Collins, M. Clayton, J. Kloke  and J. R. Horn. 2012. Inhibitory metabolic drug interac-tions with newer psychotropic drugs: inclusion in package inserts and influences of concurrence in drug interaction screening software. The Annals of Pharmacotherapy 46.10 (2012): 1287-1298 Boyce R. D., G. Gardner and H. Harkema. 2012. Us-ing Natural Language Processing to Extract Drug-Drug Interaction Information from Package Inserts. Proceedings of the 2012 Workshop on BioNLP.  
673
Chang C. and C. Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on In-telligent Systems and Technology. 2(3): 27. Chawla N. V., K. W. Boyer, L. O. Hall and W. P. Kegelmeyer. 2002. SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial In-telligence Research 16: 321-357. Chen Y. F., A. J. Avery, K. E. Neil, C. Johnson, M. E. Dewey and I. H. Stockley. 2005. Incidence and possible causes of prescribing potentially hazard-ous/contraindicated drug combinations in general practice. Drug Safety, 28(1): 67-80. Chowdhury F. M., A. B. Abacha, A. Lavelli and P. Zweigenbaum. 2011. Two Different Machine Learning Techniques for Drug-Drug Interaction Extraction. Proceedings of the 1st Challenge Task on Drug-Drug Interaction Extraction (DDIExtrac-tion-2011), 19?26.  Committee on Identifying and Preventing Medication Errors, Aspden P, Wolcott J, Bootman JL, and Cronenwett LR. 2007. Preventing Medication Er-rors: Quality Chasm Series. Washington, D.C. The National Academies Press. Fellbaum C. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press. Gurwitz J. H., T. S. Field, L. R. Harrold, J. Roth-schild, K. Debellis, A. C. Seger, C. Cadoret, L. S. Fish, L. Garber, M. Kelleher and D. W. Bates. 2003. Incidence and preventability of adverse drug events among older persons in the ambulatory set-ting. Journal of the American Medical Association, 289(9): 1107-1116. Gurwitz J. H., T. S. Field, J. Judge, P. Rochon, L. R. Harrold, C. Cadoret, M. Lee, K. White, J. LaPrino, J. Erramuspe-Mainard, M. DeFlorio, L. Gavendo, J. Auger and D. W. Bates. 2005. The incidence of ad-verse drug events in two large academic long-term care facilities. The American Journal of Medicine, 118(3): 251-258. Hall M., E. Frank, G. Holmes, B. Pfahringer, P. Reutemann and I. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explora-tions, Volume 11, Issue 1. Hines L. E., and J. E. Murphy. 2011. Potentially harm-ful drug-drug interactions in the elderly: a review. The American Journal of Geriatric Pharmacother-apy, 9(6): 364-377. Hines L. E., D. C. Malone and J. E. Murphy. 2012. Recommendations for Generating, Evaluating, and Implementing Drug-Drug Interaction Evidence. Pharmacotherapy: The Journal of Human Pharma-cology and Drug Therapy, 32(4): 304-313. Nebeker J. R., P. Barach and M. H. Samore. 2004. Clarifying Adverse Drug Events: A Clinician?s Guide to Terminology, Documentation, and Re-
porting. Annals of Internal Medicine, 140(10): 795-801. Porter M. F. 1980. An algorithm for suffix stripping. Program, 14(3): 130-137. Segura-Bedmar I., P. Martinez and C. Pablo-Sanchez. 2010. Extracting drug-drug interactions from bio-medical texts. BMC Bioinformatics 11, Suppl 5, P9.  Segura-Bedmar I., P. Martinez and D. S?nchez-Cisneros. 2011. The 1st DDIExtraction-2011 chal-lenge task: Extraction of Drug-Drug Interactions from biomedical texts. Proceedings of the 1st Chal-lenge Task on Drug-Drug Interaction Extraction (DDIExtraction-2011). Segura-Bedmar I., P. Mart?nez and M. Herrero-Zazo. 2013. SemEval-2013 Task 9: Extraction of Drug-Drug Interactions from Biomedical Texts. Proceed-ings of the 7th International Workshop on Semantic Evaluation (SemEval 2013). Thomas P., M. Neves, I. Solt, D. Tikk and U. Leser. 2011. Relation Extraction for Drug-Drug Interac-tions using Ensemble Learning. Proceedings of the 1st Challenge Task on Drug-Drug Interaction Ex-traction (DDIExtraction-2011). Toutanova K., D. Klein, C. Manning and Y. Sing-er. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL, 173-180. Yu H., C. Ho, Y. Juan and C. Lin. 2013. LibShortText: A Library for Short-text Classification and Analy-sis. Technical Report. http://www.csie.ntu.edu.tw/~cjlin/ pa-pers/libshorttext.pdf. Zhou X., X. Zhang and X. Hu. 2007. Dragon Toolkit: Incorporating Auto-learned Semantic Knowledge into Large-Scale Text Retrieval and Mining. In Proceedings of the 19th IEEE International Con-ference on Tools with Artificial Intelligence (ICTAI), 197-201. 
674
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59?62,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Using entity features to classify implicit discourse relations
Annie Louis, Aravind Joshi, Rashmi Prasad, Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
{lannie,joshi,rjprasad,nenkova}@seas.upenn.edu
Abstract
We report results on predicting the sense
of implicit discourse relations between ad-
jacent sentences in text. Our investigation
concentrates on the association between
discourse relations and properties of the
referring expressions that appear in the re-
lated sentences. The properties of inter-
est include coreference information, gram-
matical role, information status and syn-
tactic form of referring expressions. Pre-
dicting the sense of implicit discourse re-
lations based on these features is consid-
erably better than a random baseline and
several of the most discriminative features
conform with linguistic intuitions. How-
ever, these features do not perform as well
as lexical features traditionally used for
sense prediction.
1 Introduction
Coherent text is described in terms of discourse re-
lations such as ?cause? and ?contrast? between its
constituent clauses. It is also characterized by en-
tity coherence, where the connectedness of the text
is created by virtue of the mentioned entities and
the properties of referring expressions. We aim to
investigate the association between discourse rela-
tions and the way in which references to entities
are realized. In our work, we employ features re-
lated to entity realization to automatically identify
discourse relations in text.
We focus on implicit relations that hold be-
tween adjacent sentences in the absence of dis-
course connectives such as ?because? or ?but?.
Previous studies on this task have zeroed in on
lexical indicators of relation sense: dependencies
between words (Marcu and Echihabi, 2001; Blair-
Goldensohn et al, 2007) and the semantic orien-
tation of words (Pitler et al, 2009), or on general
syntactic regularities (Lin et al, 2009).
The role of entities has also been hypothesized
as important for this task and entity-related fea-
tures have been used alongside others (Corston-
Oliver, 1998; Sporleder and Lascarides, 2008).
Corpus studies and reading time experiments per-
formed by Wolf and Gibson (2006) have in fact
demonstrated that the type of discourse relation
linking two clauses influences the resolution of
pronouns in them. However, the predictive power
of entity-related features has not been studied in-
dependently of other factors. Further motivation
for studying this type of features comes from new
corpus evidence (Prasad et al, 2008), that about a
quarter of all adjacent sentences are linked purely
by entity coherence, solely because they talk about
the same entity. Entity-related features would be
expected to better separate out such relations.
We present the first comprehensive study of the
connection between entity features and discourse
relations. We show that there are notable differ-
ences in properties of referring expressions across
the different relations. Sense prediction can be
done with results better than random baseline us-
ing only entity realization information. Their per-
formance, however, is lower than a knowledge-
poor approach using only the words in the sen-
tences as features. The addition of entity features
to these basic word features is also not beneficial.
2 Data
We use 590 Wall Street Journal (WSJ) articles
with overlapping annotations for discourse, coref-
erence and syntax from three corpora.
The Penn Discourse Treebank (PDTB) (Prasad
et al, 2008) is the largest available resource of
discourse relation annotations. In the PDTB, im-
plicit relations are annotated between adjacent
sentences in the same paragraph. They are as-
signed senses from a hierarchy containing four top
level categories?Comparison, Contingency, Tem-
poral and Expansion.
59
An example ?Contingency? relation is shown
below. Here, the second sentence provides the
cause for the belief expressed in the first.
Ex 1. These rate indications aren?t directly comparable.
Lending practices vary widely by location.
Adjacent sentences can also become related
solely by talking about a common entity without
any of the above discourse relation links between
their propositions. Such pairs are annotated as En-
tity Relations (EntRels) in the PDTB, for example:
Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S
sales to remain steady at about 1,200 cars in 1990. The luxury
auto maker last year sold 1,214 cars in the U.S.
We use the coreference annotations from the
Ontonotes corpus (version 2.9) (Hovy et al, 2006)
to compute our gold-standard entity features. The
WSJ portion of this corpus contains 590 articles.
Here, nominalizations and temporal expressions
are also annotated for coreference but we use the
links between noun phrases only. We expect these
features computed on the gold-standard annota-
tions to represent an upper bound on the perfor-
mance of entity features.
Finally, the Penn Treebank corpus (Marcus et
al., 1994) is used to obtain gold-standard parse and
grammatical role information.
Only adjacent sentences within the same para-
graph are used in our experiments.
3 Entity-related features
We associate each referring expression in a sen-
tence with a set of attributes as described below.
In Section 3.2, we detail how we combine these
attributes to compute features for a sentence pair.
3.1 Referring expression attributes
Grammatical role. In exploratory analysis of
Comparison relations, we often observed parallel
syntactic realizations for entities in the subject po-
sition of the two sentences:
Ex 3. {Longer maturities}E1 are thought to indicate de-
clining interest rates. {Shorter maturities}E2 are considered
a sign of rising rates because portfolio managers can capture
higher rates sooner.
So, for each noun phrase, we record whether
it is the subject of a main clause (msubj), subject
of other clauses in the sentence (esubj) or a noun
phrase not in subject position (other).
Given vs. New. When an entity is first intro-
duced in the text, it is considered a new entity.
Subsequent mentions of the same entity are given
(Prince, 1992). New-given distinction could help
to identify some of the Expansion and Entity re-
lations. When a sentence elaborates on another, it
might contain a greater number of new entities.
We use the Ontonotes coreference annotations
to mark the information status for entities. For
an entity, if an antecedent is found in the previ-
ous sentences, it is marked as given, otherwise it
is a new entity.
Syntactic realization. In Entity relations, the sec-
ond sentence provides more information about a
specific entity in the first and a definite description
for this second mention seems likely. Also, given
the importance of named entities in news, entities
with proper names might be the ones frequently
described using Entity relations.
We use the part of speech (POS) tag associated
with the head of the noun phrase to assign one of
the following categories: pronoun, nominal, name
or expletive. When the head does not belong to
the above classes, we simply record its POS tag.
We also mark whether the noun phrase is a definite
description using the presence of the article ?the?.
Modification. We expected modification proper-
ties to be most useful for predicting Comparison
relations. Also, named or new entities in Entity
relations are very likely to have post modification.
We record whether there are premodifiers or
postmodifiers in a given referring expression. In
the absence of pre- and postmodifiers, we indicate
bare head realization.
Topicalization. Preposed prepositional or ad-
verbial phrases before the subject of a sentence
indicate the topic under which the sentence is
framed. We observed that this property is frequent
in Comparison and Temporal relations. An exam-
ple Comparison is shown below.
Ex 4. {Under British rules}T1, Blue Arrow was able to
write off at once $1.15 billion in goodwill arising from the
purchase. {As a US-based company}T2, Blue Arrow would
have to amortize the good will over as many as 40 years, cre-
ating a continuing drag on reported earnings.
When the left sibling of a referring expression is
a topicalized phrase, we mark the topic attribute.
Number. Using the POS tag of the head word, we
note whether the entity is singular or plural.
3.2 Features for classification
Next, for each sentence pair, we associate two sets
of features using the attributes described above.
60
Let S1 and S2 denote the two adjacent sentences
in a relation, where S1 occurs first in the text.
Sentence level. These features characterize S1
and S2 individually. For each sentence, we add a
feature for each of the attributes described above.
The value of the feature is the number of times that
attribute is observed in the sentence; i.e., the fea-
ture S1given would have a value of 3 if there are 3
given entities in the first sentence.
Sentence pair. These features capture the interac-
tions between the entities present in S1 and S2.
Firstly, for each pair of entities (a, b), such that
a appears in S1 and b appears in S2, we assign
one of the following classes: (i) SAME: a and b
are coreferent, (ii) RELATED: their head words are
identical, (iii) DIFFERENT: neither coreferent nor
related. The RELATED category was introduced to
capture the parallelism often present in Compari-
son relations. Even though the entities themselves
are not coreferent, they share the same head word
(i.e. longer maturities and shorter maturities).
For features, we use the combination of the
class ((i), (ii) or (iii)) with the cross product of
the attributes for a and b. For example if a has
attributes {msubj, noun, ...} and b has attributes
{esubj, defdesc, ...} and a and b are corefer-
ent, we would increment the count for features?
{sameS1msubjS2esubj, sameS1msubjS2defdesc,
sameS1nounS2esubj, sameS1nounS2defdesc ...}.
Our total set of features observed for instances
in the training data is about 2000.
We experimented with two variants of fea-
tures: one using coreference annotations from
the Ontonotes corpus (gold-standard) and an-
other based on approximate coreference informa-
tion where entities with identical head words are
marked as coreferent.
4 Experimental setup
We define five classification tasks which disam-
biguate if a specific PDTB relation holds between
adjacent sentences. In each task, we classify the
relation of interest (positive) versus a category
with a naturally occurring distribution of all of the
other relations (negative).
Sentence pairs from sections 0 to 22 of WSJ are
used as training data and we test on sections 23
and 24. Given the skewed distribution of positive
and negative examples for each task, we randomly
downsample the negative instances in the training
set to be equal to the positive examples. The sizes
of training sets for the tasks are
Expansion vs other (4716)
Contingency vs other (2466)
Comparison vs other (1138)
Temporal vs other (474)
EntRel vs other (2378)
Half of these examples are positive and the
other negative in each case.
The test set contains 1002 sentence pairs:
Comp. (133), Cont. (230), Temp. (34), Expn.
(369), EntRel (229), NoRel1 (7). We do not down-
sample our test set. Instead, we evaluate our pre-
dictions on the natural distribution present in the
data to get a realistic estimate of performance.
We train a linear SVM classifier (LIBLIN-
EAR2) for each task.3 The optimum regulariza-
tion parameter was chosen using cross validation
on the training data.
5 Results
5.1 Feature analysis
We ranked the features (based on gold-standard
coreference information) in the training sets by
their information gain. We then checked which
attributes are common among the top five features
for different classification tasks.
As we had expected, the topicalization attribute
and RELATED entities frequently appear among
the top features for Comparison.
Features with the name attribute were highly
predictive of Entity relations as hypothesized.
However, while we had expected Entity relations
to have a high rate of coreference, we found coref-
erent mentions to be very indicative of Temporal
relations: all the top features involve the SAME at-
tribute. A post-analysis showed that close to 70%
of Temporal relations involve coreferent entities
compared to around 50% for the other classes.
The number of pronouns in the second sentence
was most characteristic of the Contingency rela-
tion. In the training set for Contingency task,
about 45% of sentences pairs belonging to Contin-
gency relation have a pronoun in the second sen-
tence. This is considerably larger than 32%, which
is the percentage of sentence pairs in the negative
examples with a pronoun in second sentence.
1PDTB relation for sentence pair when both entity and
discourse relations are absent, very rare about 1% of our data.
2http://www.csie.ntu.edu.tw/?cjlin/liblinear/
3SVMs with linear kernel gave the best performance. We
also experimented with SVMs with radial basis kernel, Naive
Bayes and MaxEnt classifiers.
61
5.2 Performance on sense prediction
The classification results (fscores) are shown in
Table 1. The random baseline (Base.) represents
the results if we predicted positive and negative re-
lations according to their proportion in the test set.
Entity features based on both gold-standard
(EntGS) and approximate coreference (EntApp)
outperform the random baseline for all the tasks.
The drop in performance without gold-standard
coreference information is strongly noticable only
for Expansion relations.
The best improvement from the baseline is seen
for predicting Contingency and Entity relations,
with around 15% absolute improvement in fscore
with both EntGS and EntApp features. The im-
provements for Comparisons and Expansions are
around 11% in the approximate case. Temporal
relations benefit least from these features. These
relations are rare, comprising 3% of the test set
and harder to isolate from other relations. Overall,
our results indicate that discourse relations and en-
tity realization have a strong association.
5.3 Comparison with lexical features
In the context of using entity features for sense
prediction, one would also like to test how these
linguistically rich features compare with simpler
knowledge-lean approaches used in prior work.
Specifically, we compare with word pairs, a
simple yet powerful set of features introduced by
Marcu and Echihabi (2001). These features are the
cross product of words in the first sentence with
those in the second.
We trained classifiers on the word pairs from the
sentences in the PDTB training sets. In Table 1,
we report the performance of word pairs (WP) as
well as their combination with gold-standard en-
tity features (WP+EntGS). Word pairs turn out as
stronger predictors for all discourse relations com-
pared to our entity features (except for Expansion
prediction with EntGS features). Further, no ben-
efits over word pair results are obtained by com-
bining entity realization information.
6 Conclusion
In this work, we used a task-based approach to
show that the two components of coherence?
discourse relations and entities?are related and
interact with each other. Coreference, givenness,
syntactic form and grammatical role of entities can
predict the implicit discourse relation between ad-
Task Base. EntGS EntApp WP WP+EntGS
Comp vs Oth. 13.27 24.18 24.14 27.30 26.19
Cont vs Oth. 22.95 37.57 38.16 38.17 38.99
Temp vs Oth. 3.39 7.58 5.61 11.09 10.04
Expn vs Oth. 36.82 52.42 47.82 48.54 49.06
Ent vs Oth. 22.85 38.03 36.73 38.48 38.14
Table 1: Fscore results
jacent sentences with results better than random
baseline. However, with respect to developing au-
tomatic discourse parsers, these entity features are
less likely to be useful. They do not outperform
or complement simpler lexical features. It would
be interesting to explore whether other aspects of
entity reference might be useful for this task, such
as bridging anaphora. But currently, annotations
and tools for these phenomena are not available.
References
S. Blair-Goldensohn, K. McKeown, and O. Rambow.
2007. Building and refining rhetorical-semantic re-
lation models. In HLT-NAACL.
S.H. Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In NAACL-HLT.
Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In EMNLP.
D. Marcu and A. Echihabi. 2001. An unsupervised ap-
proach to recognizing discourse relations. In ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In ACL-IJCNLP.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In LREC.
E. Prince. 1992. The zpg letter: subject, definiteness,
and information status. In Discourse description:
diverse analyses of a fund raising text, pages 295?
325. John Benjamins.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
F. Wolf and E. Gibson. 2006. Coherence in natural
language: data structures and applications. MIT
Press.
62
