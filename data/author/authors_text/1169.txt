Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 996?1004, Prague, June 2007. c?2007 Association for Computational Linguistics
 
Learning to Find English to Chinese Transliterations on the Web 
Jian-Cheng Wu 
 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, Taiwan 
d928322@oz.nthu.edu.tw 
Jason S. Chang 
 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, Taiwan 
jschang@cs.nthu.edu.tw 
Abstract 
We present a method for learning to find 
English to Chinese transliterations on the 
Web. In our approach, proper nouns are 
expanded into new queries aimed at maxi-
mizing the probability of retrieving trans-
literations from existing search engines. 
The method involves learning the sublexi-
cal relationships between names and their 
transliterations. At run-time, a given name 
is automatically extended into queries with 
relevant morphemes, and transliterations in 
the returned search snippets are extracted 
and ranked. We present a new system, 
TermMine, that applies the method to find 
transliterations of a given name. Evaluation 
on a list of 500 proper names shows that 
the method achieves high precision and re-
call, and outperforms commercial machine 
translation systems. 
1 Introduction 
Increasingly, short passages or web pages are be-
ing translated by desktop machine translation soft-
ware or are submitted to machine translation ser-
vices on the Web every day. These texts usually 
contain some proportion of proper names (e.g., 
place and people names in ?The cities of Mesopo-
tamia prospered under Parthian and Sassanian 
rule.?), which may not be handled properly by a 
machine translation system. Online machine trans-
lation services such as Google Translate1 or Yahoo! 
Babelfish2 typically use a bilingual dictionary that 
is either manually compiled or learned from a par-
                                                 
1 Google Translate: translate.google.com/translate_t 
2 Yahoo! Babelfish: babelfish.yahoo.com 
allel corpus. However, such dictionaries often have 
insufficient coverage of proper names and techni-
cal terms, leading to poor translation performance 
due to out of vocabulary (OOV) problem.  
Handling name transliteration is also important 
for cross language information retrieval (CLIR) 
and terminology translation (Quah 2006). There 
are also services on the Web specifically targeting 
transliteration aimed at improving CLIR, including 
CHINET (Kwok et al 2005) and LiveTrans (Lu, 
Chien, and Lee 2004). 
The OOV problems of machine translation (MT) 
or CLIR can be handled more effectively by learn-
ing to find transliteration on the Web. Consider the 
sentence in Example (1), containing three proper 
names. Google Translate produces the sentence in 
Example (2) and leaves ?Parthian? and ?Sas-
sanian? not translated. A good response might be a 
translation like Example (3) with appropriate 
transliterations (underlined). 
(1) The cities of Mesopotamia prospered under 
Parthian and Sassanian rule. 
(2) ????? parthian ????sassanian ?
?? 
(3) ??????3??????4???5???
????? 
These transliterations can be more effectively 
retrieved from mixed-code Web pages by extend-
ing each of the proper names into a query. Intui-
tively, by requiring one of likely transliteration 
morphemes (e.g., ???(Ba) or ???(Pa) for names 
beginning with the prefix ?par-?), we can bias the 
search engine towards retrieving the correct trans-  
                                                 
3 ??????(Meisuobudamiya) is the transliteration of 
?Mesopotamia.? 
4 ???(Badiya) is the transliteration of ?Parthian.? 
5 ??(Sashan) is the transliteration of ?Sassanian.? 
996
 Figure 1. An example of TermMine search for transliterations of the name ?Parthian? 
 
literations (e.g., ???? ?(Badiya) and ???
??(Patiya)) in snippets of many top-ranked docu-
ments.  
This approach to terminology translation by 
searching is a strategy increasingly adopted by 
human translators. Quah (2006) described a mod-
ern day translator would search for the translation 
of a difficult technical term such as ??????
??????? by expanding the query with the 
word ?film? (back transliteration of the component 
?????? of the term in question). This kind of 
query expansion (QE) indeed increases the chance 
of finding the correct translation ?anisotropic con-
ductive film? in top-ranked snippets. However, the 
manual process of expanding query, sending 
search request, and extracting transliteration is te-
dious and time consuming. Furthermore, unless the 
query expansion is done properly, snippets con-
taining answers might not be ranked high enough 
for this strategy to be the most effective.  
We present a new system, TermMine, that auto-
matically learns to extend a given name into a 
query expected to retrieve and extract translitera-
tions of the proper name. An example of machine 
transliteration of ?Parthian? is shown in Figure 1. 
TermMine has determined the best 10 query ex-
pansions (e.g., ?Parthian ? ,? ?Parthian ? ?). 
TermMine learns these effective expansions auto-
matically during training by analyzing a collection 
of place names and their transliterations, and deriv-
ing cross-language relationships of prefix and post-
fix morphemes. For instance, TermMine learns that 
a name that begins with the prefix ?par-? is likely 
to have a transliteration beginning with ??? or 
???). We describe the learning process in Section 
3. 
This prototype demonstrates a novel method for 
learning to find transliterations of proper nouns on 
the Web based on query expansion aimed at 
maximizing the probability of retrieving translit-
erations from existing search engines. Since the 
method involves learning the morphological rela-
tionships between names and their transliterations, 
we refer to this IR-based approach as morphologi-
cal query expansion approach to machine translit-
eration. This novel approach is general in scope 
and can also be applied to back transliteration and 
to translation with slight modifications, even 
though we focus on transliteration in this paper. 
The remainder of the paper is organized as fol-
lows. First, we give a formal statement for the 
problem (Section 2). Then, we present a solution to 
the problem by proposing new transliteration prob-
ability functions, describing the procedure for es-
timating parameters for these functions (Section 3) 
and the run-time procedure for searching and ex-
 
997
 tracting transliteration via a search engine (Section 
4). As part of our evaluation, we carry out two sets 
of experiments, with or without query expansion, 
and compare the results. We also evaluate the re-
sults against two commercial machine translation 
online services (Section 5). 
2 Problem Statement 
Using online machine translation services for name 
transliteration does not work very well. Searching 
in the vicinity of the name in mixed-code Web 
pages is a good strategy. However, query expan-
sion is needed for this strategy to be effective. 
Therefore, to find transliterations of a name, a 
promising approach is to automatically expand the 
given name into a query with the additional re-
quirement of some morpheme expected to be part 
of relevant transliterations that might appear on the 
Web. 
 
Table 1. Sample name-transliteration pairs from the 
training collection. 
Name Transliteration Name Transliteration
Aabenraa ???   Aarberg ??? 
Aabybro ???? Aarburg ??? 
Aachen ?? Aardenburg ??? 
Aalesund ??? Aargau ?? 
Aaley ?? Aars ??? 
Aalten ??? Aba ?? 
Aarau ?? Abacaxis ????? 
 
Now, we formally state the problem we are deal-
ing with: 
 
While a proper name N is given. Our goal 
is to search and extract the transliteration 
T of N from Web pages via a general-
purpose search engine SE. For that, we 
expand N into a set of queries q1, q2, ?, 
qm, such that the top n document snippets 
returned by SE for the queries are likely to 
contain some transliterations T of the 
given name N. 
 
In the next section, we propose using a probabil-
ity function to model the relationships between 
names and transliterations and describe how the 
parameters in this function can be estimated.  
3 Learning Relationships for QE  
We attempt to derive cross-language morpho-
logical relationships between names and translit-
erations and use them to expand a name into an 
effective query for searching and extracting trans-
literations. For the purpose of expanding the given 
name, N, into effective queries to search and ex-
tract transliterations T, we define a probabilistic 
function for mapping prefix syllable from the 
source to the target languages. The prefix translit-
eration function P(TP | NP) is the probability of T 
has a prefix TP under the condition that the name N 
has a prefix NP. 
P (TP | NP) = Count (TP,NP) / Count (NP)       (1) 
where  Count (TP,NP) is the number of TP and NP 
co-occurring in the pairs of training set 
(see Table 1), and Count(NP) is the num-
ber of NP occurring in training set.  
 
Similarly, we define the function P (TS | NS) for 
postfixes TS and NS: 
P (TS | NS) = Count (TS,NS) / Count (NS)        (2) 
The prefixes and postfixes are intended as a syl-
lable in the two languages involved, so the two 
prefixes correspond to each other (See Table 2&3). 
Due to the differences in the sound inventory, the 
Roman prefix corresponding to a syllabic prefix in 
Chinese may vary, ranging from a consonant, a 
vowel, or a consonant followed by a vowel (but not 
a vowel followed by a consonant). So, it is likely 
such a Roman prefix has from one to four letters. 
On the contrary, the prefix syllable for a name 
written in Chinese is readily identifiable. 
 
Table 2. Sample cross-language morphological relation-
ships between prefixes. 
Name 
Prefix (NP)
Transliteration
Prefix (TP) 
NP 
Count 
TP 
Count
Co-occ.
Count
a- ?(A) 1,456 854 854
a- ?(Ya) 1,456 267 264
ab- ?(A) 77 854 45
ab- ?(Ya) 77 267 32
b- ?(Bu) 2,319 574 566
b- ?(Ba) 2,319 539 521
ba- ?(Ba) 650 574 452
bu- ?(Bu) 299 539 182
 
998
 Table 3. Sample cross-language morphological relation-
ships between postfixes. 
Name 
Postfix (Ns) 
Transliteration
Postfix (Ts) 
Ns 
Count 
Ts 
Count
Co-occ.
Count 
-a ?(La) 4,774 1,044 941
-a ?(Ya) 4,774 606 568
-la ?(La) 461 1,044 422
-ra ?(La) 534 1,044 516
-ia ?(Ya) 456 606 391
-nia ?(Ya) 81 606 77
-burg ?(Bao) 183 230 175
 
We also observe that a preferred prefix (e.g., 
???(Ai)) is often used for a Roman prefix (e.g., 
?a-? or ?ir-?), while occasionally other homo-
phonic characters are used (e.g., ???(Ai)). The 
skew distribution creates problems for reliable es-
timation of transliteration functions. To cope with 
this data sparseness problem, we use homophone 
classes and a function CL that maps homophonic 
characters to the same class number. For instance, 
??? and ??? are homophonic, and both are as-
signed the same class identifier(see Table 4 for 
more samples). 
Therefore, we have  
CL (???) = CL (???) = 275. 
Table 4. Some examples of classes of homophonic 
characters. The class ID of each class is assigned arbi-
trarily. 
Class 
ID 
Transl. 
char 
Pronun- 
ciation 
Class 
ID 
Transl.
char  
Pronun-
ciation 
1 ? Ba  2 ? Bo 
1 ? Ba 275 ? Ai 
1 ? Ba 275 ? Ai 
1 ? Ba 275 ? Ai 
1 ? Ba 276 ? Ao 
1 ? Ba 276 ? Ao 
2 ? Bo 276 ? Ao 
2 ? Bo ? ? ? 
 
With homophonic classes of transliteration mor-
phemes, we define class-based transliteration prob-
ability as follows 
PCL(C | NP) = Count(TP,NP) / Count(NP)       (3) 
where CL(TP) = C            
PCL(C | NS) = Count(TS,NS) / Count(NS)       (4) 
where CL(TS) = C              
and then we rewrite P (TP | NP) and P (TS | NS) as  
  
P (TP | NP) = PCL(CL(TP ) | NP)                        (5) 
P (TS | NS) = PCL(CL(TS ) | NS)                        (6) 
With class-based transliteration probabilities, we 
are able to cope with difficulty in estimating pa-
rameters for rare events which are under repre-
sented in the training set. Table 5 shows that ??? 
belongs to a homophonic class co-occurring with 
?a-? for 46 times, even when only one instance of 
(???, ?a-?). 
After cross-language relationships for prefixes 
and postfixes are automatically trained, the prefix 
relationships are stored as prioritized query expan-
sion rules. In addition to that, we also need a trans-
literation probability function to rank candidate 
transliterations at run-time (Section 4). To cope 
with data sparseness, we consider names (or trans-
literations) with the same prefix (or postfix) as a 
class. With that in mind, we use both prefix and 
postfix to formulate an interpolation-based estima-
tor for name transliteration probability:  
P(T | N)=max ?1P(TP | NP)+?2P(TS | NS)        (7) 
               NP, NS  
where ?1 + ?2 = 1 and NP, NS, TP, and TS are the 
prefix and postfix of the given name N 
and transliteration T. 
 
For instance, the probability of ??????
? ?(Meisuobudamiya) as a transliteration of 
?Mesopotamia? is estimated as follows 
 
 P (?????? | ?Mesopotamia?)  
= ?1P (??? | ?me-?)+ ?2 P (??? | ?-a?) 
 
(1) For each entry in the bilingual name list, pair 
up prefixes and postfixes in names and trans-
literations. 
(2) Calculate counts of these affixes and their co-
occurrences. 
(3) Estimate the prefix and postfix transliteration 
functions 
(4) Estimate class-based prefix and postfix trans-
literation functions  
Figure 2. Outline of the process used to train the 
TermMine system. 
 
The system follows the procedure shown in Fig-
ure 2 to estimate these probabilities. In Step (1), 
999
 the system generates all possible prefix pairs for 
each name-transliteration pair. For instance, con-
sider the pair, (?Aabenraa,? ?????), the system 
will generate eight pairs: 
(a-, ?-), (aa-, ?-), (aab-, ?-), (aabe-, ?-), 
(-a, -?), (-aa, -?), (-raa, -?), and (-nraa, -?). 
Finally, the transliteration probabilities are esti-
mated based on the counts of prefixes, postfixes, 
and their co-occurrences. The derived probabilities 
embody a number of relationships:  
(a) Phoneme to syllable relationships (e.g., ?b? vs. 
? ? ? as in ?Brooklyn? and ? ? ? ?
??(Bulukelin)),  
(b) Syllable to syllable relationships (e.g., ?bu? vs. 
???),  
(c) Phonics rules  (e.g., ?br-? vs. ??? and ??? vs. 
?cl-?). The high probability of P(??? | ?cl-?) 
amounts to the phonics rule that stipulates ?c? 
be pronounced with a ?k? sound in the context 
of ?l.? 
4 Transliteration Search and Extraction 
At run-time, the system follows the procedure in 
Figure 3 to process the given name. In Step (1), the 
system looks up in the prefix relationship table to 
find the n best relationships (n = MaxExpQueries) 
for query expansion with preference for relation-
ships with higher probabilistic value. For instance, 
to search for transliterations of ?Acton,? the system 
looks at all possible prefixes and postfixes of ?Ac-
ton,? including a-, ac-, act-, acto-, -n, -on, -ton, 
and -cton, and determines the best query expan-
sions: ?Acton ?,? ?Acton ?,? ?Acton ?,? ?Ac-
ton ?,? ?Acton ?,? etc. These effective expan-
sions are automatically derived during the training 
stage described in Section 3 by analyzing a large 
collection of name-transliteration pairs.  
In Step (2), the system sends off each of these 
queries to a search engine to retrieve up to 
MaxDocRetrieved document snippets. In Step (3), 
the system discards snippets that have too little 
proportion of target-language text. See Example (4) 
for a snippet that has high portion of English text 
and therefore is less likely to contain a translitera-
tion. In Step (4), the system considers the sub-
strings in the remaining snippets. 
 
 
(1) Look up the table for top MaxExpQueries 
prefix and posfix relationships relevant to 
the given name and use the target mor-
phemes in the relationship to form ex-
panded queries  
(2) Search for Web pages with the queries and 
filter out snippets containing at less than 
MinTargetRate portion of target language 
text 
(3) Evaluate candidates based on class-based 
transliteration probability (Equation 5) 
(4) Output top one candidate for evaluation 
Figure 3. Outline of the steps used to search, extract, 
and rank transliterations. 
 
Table 5. Sample data for class-based morphological 
transliteration probability of prefixes, where # of NP 
denotes the number of the name prefix NP; # of C, NP 
denotes the number of all TP belonging to the class C 
co-occurring with the NP; # TP, NP denotes the number 
of transliteration prefix TP co-occurs with the NP; P(C|NP) 
denotes the probability of all TP belonging to C co-
occurring with the NP; P(TP|NP) denotes the probability 
of the Tp co-occurs with the NP. 
NP Class
ID 
TP # of NP # of 
C,NP 
# of 
TP,NP 
P(C|NP) P(TP|NP)
a- 275 ? 1456 46 28 0.032 0.019
a- 275 ? 1456 46 17 0.032 0.012
a- 275 ? 1456 46 1 0.032 0.000
a- 276 ? 1456 103 100 0.071 0.069
a- 276 ? 1456 103 2 0.071 0.001
a- 276 ? 1456 103 1 0.071 0.000
ba- 2 ? 652 5 3 0.008 0.005
ba- 2 ? 652 5 1 0.008 0.002
ba- 2 ? 652 5 1 0.008 0.002
 
Table 6. Sample data for class-based morphological 
transliteration probability of postfixes. Notations are 
similar to those for Table 5. 
Ns Class 
ID 
Ts # of Ns # of  
C,Ns 
# of 
Ts,Ns 
P(C|Ns) P(Ts|Ns)
-li 103 ? 142 140 85 0.986 0.599
-li 103 ? 142 140 52 0.986 0.366
-li 103 ? 142 140 2 0.986 0.014
-li 103 ? 142 140 1 0.986 0.007
-li 103 ? 142 140 0 0.986 0.000
-raa 112 ? 4 1 1 0.250 0.250
-raa 112 ? 4 1 0 0.250 0.000
-raa 112 ? 4 1 0 0.250 0.000
-raa 112 ? 4 1 0 0.250 0.000
 
1000
 For instance, Examples (5-7) shows remaining 
snippets that have high proportion of Chinese text. 
The strings ?????(Akedun) is a transliteration 
found in snippet shown in Example (5), a candi-
date beginning with the prefix ??? and ending 
with the postfix  ??? and is within the distance of 
1 of the instance ?Acton,? separated by a punctua-
tion token. The string ????? (Aikedun) found 
in Example (6) is also a legitimate transliteration 
beginning with a different prefix ??,? while ??
???(Aiketeng) in Example (7) is a transliteration 
beginning with yet another prefix ??.? Translit-
eration ????? appears at a distance of 3 from 
?Acton,? while two instances of ????? appear 
at the distances of 1 and 20 from the nearest in-
stances of ?Acton.?  
 
(4) Acton moive feel pics!! - ?? 
????: ??? > ???? > ?? > Acton 
moive feel pics!! Hop Hero - Acton moive feel 
pics!! 
http://www.hkmassive.com/forum/viewthread.php?
tid=2368&fpage=1 Watch the slide show! ... 
(5) New Home Alert - Sing Tao New Homes 
Please select, Acton ???, Ajax ???, Allis-
ton ????, Ancaster ????, Arthur ??, 
Aurora ???, Ayr ??, Barrie ??, Beamsville, 
Belleville ... 
(6) STS-51-F ? Wikipedia 
????????????????????
? ... ?????? (Karl Henize ???? STS-51-
F ??)?????; ?????? (Loren Acton?
??? STS-51-F??)???????; ??-?
???? (John-David F. ... 
(7) ?????-00-Acton-Australia.htm 
Acton Systems is a world leading manufacturer 
supplying stuctured cabling systems suited to the 
Australian and New Zealand marketplace. ???
????????????????, ????
???????? Custom made leads are now 
available ... 
 
The occurrence counts and average distance 
from instances of the given name are tallied for 
each of these candidates. Candidates with a low 
occurrence count and long average distance are 
excluded from further consideration.  Finally, all 
candidates are evaluated and ranked using Equa-
tion (7) given in Section 3. 
5 Evaluation 
In the experiment carried out to assess the feasibil-
ity to the proposed method, a data set of 23,615 
names and transliterations was used. This set of 
place name data is available from NICT, Taiwan 
for training and testing. There are 967 distinct Chi-
nese characters presented in the data, and more 
details of training data are available in Table 7. 
The English part consists of Romanized versions 
of names originated from many languages, includ-
ing Western and Asian languages. Most of the time, 
the names come with a Chinese counterpart based 
solely on transliteration. But occasionally, the Chi-
nese counterpart is part translation and part trans-
literation. For instance, the city of ?Southampton? 
has a Chinese counterpart consisting of ?? ? 
(translation of ?south?) and ????? (translitera-
tion of ?ampton?). 
 
Table 7. Training data and statistics  
Type of Data Used in Experiment Number
Name-transliteration pairs 23,615
Training data 23,115
Test data 500
Distinct transliteration morphemes 967
Distinct transliteration morphemes  
(80% coverage) 100
Names with part translation and 
 part transliteration (estimated) 300 
Cross-language prefix relationships 21,016 
Cross-language postfix relationships 26,564 
 
We used the set of parameters shown in Table 8 
to train and run System TermMine. A set of 500 
randomly selected were set aside for testing. We 
paired up the prefixes and postfixes in the remain-
ing 23,116 pairs, by taking one to four leading or 
trailing letters of each Romanized place names and 
the first and last Chinese transliteration character 
to estimate P (TP | NP) and P (TS | NS). 
 
Table 8. Parameters for training and testing 
Parameter Value Description 
MaxPrefixLetters 4 Max number of let-ters in a prefix 
MaxPostfixLetters 4 Max number of let-ters in a postfix 
MaxExpQueries 10 Max number of ex-panded queries 
MaxDocRetrieved 1000 Max number of document retrieved
1001
 MinTargetRate 0.5 Min rate of target text in a snippet 
MinOccCount 1 
Min number of co-
occurrence of query 
and transliteration 
candidate in snippets
MaxAvgDistance 4 Max distance be-tween N and T 
WeightPrefixProb 0.5 
Weight of Prefix 
probability (?1) 
WeightPostfixProb 0.5 
Weight of Postfix 
probability (?2) 
 
We carried out two kinds of evaluation on Sys-
tem TermMine, with and without query expansion. 
With QE option off, the name itself was sent off as 
a query to the search engine, while with QE option 
turned on, up to 10 expanded queries were sent for 
each name. We also evaluated the system against 
Google Translate and Yahoo! Babelfish. We dis-
carded the results when the names are returned un-
translated. After that, we checked the correctness 
of all remaining results by hand. Table 9 shows a 
sample of the results produced by the three systems. 
In Table 10, we show performance differences 
of system TermMine in query expansion option. 
Without QE, the system returns transliterations 
(applicability) less than 50% of the time. Neverthe-
less, there are enough snippets for extracting and 
ranking of transliterations. The precision rate of the 
top-ranking transliterations is 88%.  With QE 
turned on, the applicability rate increases signifi-
cantly to 60%. The precision rate also improved 
slightly to 0.89. 
The performance evaluation of three systems is 
shown in Table 11. For the test set of 500 place 
names, Google Translate returned 146 translitera-
tions and Yahoo! Babelfish returned only 44, while 
TermMine returned 300. Of the returned translit-
erations, Google Translate and Yahoo! Babelfish 
achieved a precision rate around 50%, while 
TermMine achieved a precision rate almost as high 
as 90%. The results show that System TermMine 
outperforms both commercial MT systems by a 
wide margin, in the area of machine transliteration 
of proper names.  
 
Table 9. Sample output by three systems evaluated. The 
stared transliterations are incorrect. 
Name TermMine Google Translate 
Yahoo! 
Babelfish
Arlington  ???  ???  ??? 
Toledo  ???  ??? - 
Palmerston  ????  ???? - 
Cootamundra  ?????  ????? - 
Bangui  ??  ?? - 
Australasia  ????? *???  ????? 
Wilson  ???  ???  ??? 
Mao *???  ?  ? 
Inverness  ???? *??  ???? 
Cyprus  ????  ????  ???? 
Rostock  ????  ????  ???? 
Bethel  ???  ??? *?? 
Arcade  ??? *?? *?? 
Lomonosov  ?????  ????? - 
Oskaloosa  ?????  ????? - 
 
Table 10. Performance evaluation of TermMine 
 Method
Evaluation 
TermMine 
QE- 
TermMine 
QE+ 
# of cases performed 238  300
Applicability  0.48  0.60
# Correct Answers    209    263
Precision  0.88  0.89
Recall  0.42  0.53
F-measure 0.57 0.66
 
Table 11. Performance evaluation of three systems 
Method
Evaluation  
TermMine 
QE+ 
Google 
Translate 
Yahoo! 
Babelfish
# of cases done  300  146  44
# of correct  
answers 
  263  67  23
Applicability  0.60  0.29  0.09
Precision  0.89  0.46  0.52
Recall  0.53  0.13 0.05
F-measure 0.66    0.21 0.08
6 Comparison with Previous Work 
Machine transliteration has been an area of active 
research. Most of the machine transliteration 
method attempts to model the transliteration proc-
ess of mapping between graphemes and phonemes. 
Knight and Graehl (1998) proposed a multilayer 
model and a generate-and-test approach to perform 
back transliteration from Japanese to English based 
on the model. In our work we address an issue of 
producing transliteration by way of search.  
Goto et al (2003), and Li et al (2004) proposed 
a grapheme-based transliteration model. Hybrid 
transliteration models were described by Al-
Onaizan and Knight (2002), and Oh et al (2005).  
1002
 Recently, some of the machine transliteration study 
has begun to consider the problem of extracting 
names and their transliterations from parallel cor-
pora (Qu and Grefenstette 2004, Lin, Wu and 
Chang 2004; Lee and Chang 2003, Li and Grefen-
stette 2005).  
Cao and Li (2002) described a new method for 
base noun phrase translation by using Web data. 
Kwok, et al (2001) described a system called 
CHINET for cross language name search. Nagata 
et al (2001) described how to exploit proximity 
and redundancy to extract translation for a given 
term. Lu, Chien, and Lee (2002) describe a method 
for name translation based on mining of anchor 
texts. More recently, Zhang, Huang, and Vogel 
(2005) proposed to use occurring words to expand 
queries for searching and extracting transliterations. 
Oh and Isahara (2006) use phonetic-similarity to 
recognize transliteration pairs on the Web. 
In contrast to previous work, we propose a sim-
ple method for extracting transliterations based on 
a statistical model trained automatically on a bilin-
gual name list via unsupervised learning. We also 
carried out experiments and evaluation of training 
and applying the proposed model to extract trans-
literations by using web as corpus.  
7 Conclusion and Future Work 
Morphological query expansion represents an in-
novative way to capture cross-language relations in 
name transliteration. The method is independent of 
the bilingual lexicon content making it easy to 
adopt to other proper names such person, product, 
or organization names. This approach is useful in a 
number of machine translation subtasks, including 
name transliteration, back transliteration, named 
entity translation, and terminology translation.  
Many opportunities exist for future research and 
improvement of the proposed approach. First, the 
method explored here can be extended as an alter-
ative way to support such MT subtasks as back 
transliteration (Knight and Graehl 1998) and noun 
phrase translation (Koehn and Knight 2003). Fi-
nally, for more challenging MT tasks, such as han-
dling sentences, the improvement of translation 
quality probably will also be achieved by combin-
ing this IR-based approach and statistical machine 
translation. For example, a pre-processing unit may 
replace the proper names in a sentence with trans-
literations (e.g., mixed code text ?The cities of ?
????? prospered under ??? and ?? 
rule.? before sending it off to MT for final transla-
tion. 
References 
GW Bian, HH Chen. Cross-language information access 
to multilingual collections on the internet. 2000. 
Journal of American Society for Information Science 
& Technology (JASIST), Special Issue on Digital Li-
braries, 51(3), pp.281-296, 2000. 
Y. Cao and H. Li. Base Noun Phrase Translation Using 
Web Data and the EM Algorithm. 2002. In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics (COLING?02), pp.127-133, 2002. 
PJ. Cheng, JW. Teng, RC. Chen, JH. Wang, WH. Lu, 
and LF. Chien. Translating unknown queries with 
web corpora for cross-language information retrieval. 
2004. In Proceedings of the 27th ACM International 
Conference on Research and Development in Infor-
mation Retrieval (SIGIR04), pp. 146-153, 2004. 
I. Goto, N. Kato, N. Uratani, and T. Ehara. Translitera-
tion considering context information based on the 
maximum entropy method. In Proceedings of Ninth 
Machine Translation Summit, pp.125-132, 2003. 
F. Huang, S. Vogel, and A. Waibel. Automatic extrac-
tion of named entity translingual equivalence based 
on multi-feature cost minimization. In Proceeding of 
the 41st ACL, Workshop on Multilingual and Mixed-
Language Named Entity Recognition, Sapporo, 2003. 
A. Kilgarriff and Grefenstette, G. 2003. Introduction to 
the Special Issue on the Web as Corpus.  Computa-
tional Linguistics 29(3), pp. 333-348, 2003. 
K. Knight, J. Graehl. Machine Transliteration. 1998.  
Computational Linguistics 24(4), pp.599-612, 1998. 
P. Koehn, K. Knight. 2003. Feature-Rich Statistical 
Translation of Noun Phrases. In Proceedings of the 
41st Annual Meeting on Association for Computa-
tional Linguistics, pp. 311-318, 2003. 
J. Kupiec. 1993. An Algorithm for Finding Noun Phrase 
Correspondences in Bilingual Corpora. In Proceed-
ings of the 31st Annual Meeting of the Association 
for Computational Linguistics, pp. 17-22, 1993. 
KL Kwok. 2001. NTCIR-2 Chinese, Cross Language 
Retrieval Experiments Using PIRCS. In Proceedings 
of NTCIR Workshop Meeting, pp.111-118, 2001. 
KL Kwok, P Deng, N Dinstl, HL Sun, W Xu, P Peng, 
and Doyon, J. 2005. CHINET: a Chinese name finder 
system for document triage. In Proceedings of 2005 
1003
 International Conference on Intelligence Analysis, 
2005. 
C.J. Lee, and Jason S. Chang. 2003. Acquisition of Eng-
lish-Chinese Transliterated Word Pairs from Parallel- 
Aligned Texts using a Statistical Machine Translit-
eration Model, In Proceedings of HLT-NAACL 2003 
Workshop, pp. 96-103, 2003. 
H. Li, M. Zhang, and J. Su. 2004. A joint source-
channel model for machine transliteration. In Pro-
ceedings of the 42nd Annual Meeting on Association 
for Computational Linguistics, pp.159-166, 2004. 
Y. Li, G. Grefenstette. 2005. Translating Chinese Ro-
manized name into Chinese idiographic characters 
via corpus and web validation. In Proceedings of  
CORIA 2005, pp. 323-338, 2005. 
T. Lin, J.C. Wu, and J. S. Chang. 2004. Extraction of 
Name and Transliteration in Monolingual and Paral-
lel Corpora. In Proceedings of AMTA 2004, pp.177-
186, 2004. 
WH. Lu, LF. Chien, and HJ. Lee. 2002. Translation of 
web queries using anchor text mining. ACM Transac-
tions on Asian Language Information Processing, 
1(2):159?172, 2002. 
WH Lu, LF Chien, HJ Lee. Anchor text mining for 
translation of Web queries: A transitive translation 
approach. ACM Transactions on Information Systems 
22(2), pp. 242-269, 2004. 
M. Nagata, T. Saito, and K. Suzuki. Using the Web as a 
bilingual dictionary. 2001. In Proceedings of 39th. 
ACL Workshop on Data-Driven Methods in Machine 
Translation, pp. 95-102, 2001. 
J.-H Oh, and H. Isahara. 2006. Mining the Web for 
Transliteration Lexicons: Joint-Validation Approach, 
In IEEE/WIC/ACM International Conference on Web 
Intelligence, pp. 254-261, 2006.  
J.-H. Oh and K.-S. Choi. 2005. An ensemble of graph-
eme and phoneme for machine transliteration. In Pro-
ceedings of IJCNLP05, pp.450?461, 2005. 
Y. Qu, and G. Grefenstette. 2004. Finding Ideographic 
Representations of Japanese Names Written in Latin 
Script via Language Identification and Corpus Vali-
dation. In Proceedings of the 42nd Annual Meeting of 
the Association for Computational Linguistics, 
pp.183-190, 2004. 
CK Quah. 2006. Translation and Technology, Palgrave 
Textbooks in Translation and Interpretation, Pal-
grave MacMillan. 
Y Zhang, F Huang, S Vogel. 2005. Mining translations 
of OOV terms from the web through cross-lingual 
query expansion. In Proceedings of the 28th Annual 
International ACM SIGIR, pp.669-670, 2005.  
Y. Zhang and P. Vines. 2004. Detection and translation 
of oov terms prior to query time. In Proceedings of 
the 27th annual international ACM SIGIR conference 
on Research and development in information re-
trieval, pp.524-525, 2004. 
1004
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 478?486,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Acquiring Translation Equivalences of Multiword Expressions by 
Normalized Correlation Frequencies 
Ming-Hong Bai1,2 Jia-Ming You1 Keh-Jiann Chen1 Jason S. Chang2 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
mhbai@sinica.edu.tw, swimming@hp.iis.sinica.edu.tw, 
kchen@iis.sinica.edu.tw, jschang@cs.nthu.edu.tw 
 
Abstract 
In this paper, we present an algorithm for ex-
tracting translations of any given multiword 
expression from parallel corpora. Given a 
multiword expression to be translated, the 
method involves extracting a short list of tar-
get candidate words from parallel corpora 
based on scores of normalized frequency, 
generating possible translations and filtering 
out common subsequences, and selecting the 
top-n possible translations using the Dice 
coefficient. Experiments show that our ap-
proach outperforms the word alignment-
based and other naive association-based me-
thods. We also demonstrate that adopting the 
extracted translations can significantly im-
prove the performance of the Moses machine 
translation system. 
1 Introduction 
Translation of multiword expressions (MWEs), 
such as compound words, phrases, collocations 
and idioms, is important for many NLP tasks, 
including the techniques are helpful for dictio-
nary compilation, cross language information 
retrieval, second language learning, and machine 
translation. (Smadja et al, 1996; Gao et al, 2002; 
Wu and Zhou, 2003). However, extracting exact 
translations of MWEs is still an open problem, 
possibly because the senses of many MWEs are 
not compositional (Yamamoto and Matsumoto, 
2000), i.e., their translations are not composi-
tions of the translations of individual words. For 
example, the Chinese idiom ???? should be 
translated as ?turn a blind eye,? which has no 
direct relation with respect to the translation of 
each constituent (i.e., ?to sit?, ?to see? and ?to 
ignore?) at the word level.  
Previous SMT systems (e.g., Brown et al, 
1993) used a word-based translation model 
which assumes that a sentence can be translated 
into other languages by translating each word 
into one or more words in the target language. 
Since many concepts are expressed by idiomatic 
multiword expressions instead of single words, 
and different languages may realize the same 
concept using different numbers of words (Ma et 
al., 2007; Wu, 1997), word alignment based me-
thods, which are highly dependent on the proba-
bility information at the lexical level, are not 
well suited for this type of translation.  
To address the above problem, some methods 
have been proposed for extending word align-
ments to phrase alignments. For example, Och et 
al. (1999) proposed the so-called grow-diag-
final heuristic method for extending word 
alignments to phrase alignments. The method is 
widely used and has achieved good results for 
phrase-based statistical machine translation. 
(Och et al, 1999; Koehn et al, 2003; Liang et al, 
2006). Instead of using heuristic rules, Ma et al 
(2008) showed that syntactic information, e.g., 
phrase or dependency structures, is useful in ex-
tending the word-level alignment. However, the 
above methods still depend on word-based 
alignment models, so they are not well suited to 
extracting the translation equivalences of seman-
tically opaque MWEs due to the lack of word 
level relations between the translational corres-
pondences. Moreover, the aligned phrases are 
not precise enough to be used in many NLP ap-
plications like dictionary compilation, which 
require high quality translations. 
Association-based methods, e.g., the Dice 
coefficient, are widely used to extract transla-
tions of MWEs. (Kupiec, 1993; Smadja et al, 
1996; Kitamura and Matsumoto, 1996; Yama-
moto and Matsumoto, 2000; Melamed, 2001). 
The advantage of such methods is that associa-
tion relations are established at the phrase level 
instead of the lexical level, so they have the po-
tential to resolve the above-mentioned transla-
tion problem. However, when applying associa-
tion-based methods, we have to consider the fol-
lowing complications. The first complication, 
which we call the contextual effect, causes the 
extracted translation to contain noisy words. For 
478
example, translations of the Chinese idiom ??
?? (best of both worlds) extracted by a naive 
association-based method may contain noisy 
collocation words like difficult, try and cannot, 
which are not part of the translation of the idiom. 
They are actually translations of its collocation 
context, such as ??(difficult), ??(try), and 
??(cannot). This problem arises because naive 
association methods do not deal with the effect 
of strongly collocated contexts carefully. If we 
can incorporate lexical-level information to dis-
count the noisy collocation words, the contextual 
effect could be resolved. 
 
English (y) fy fx,y Dice(x,y)
quote out of context 22 19 0.56 
take out of context 17 11 0.35 
interpret out of context 2 2 0.08 
out of context 53 32 0.65 
Table 1. The Dice coefficient tends to select a com-
mon subsequence of translations. (The frequency of
???? ,fx, is 46.) 
 
The second complication, which we call the 
common subsequence problem, is that the Dice 
coefficient tends to select the common subse-
quences of a set of similar translations instead of 
the full translations. Consider the translations of 
???? (quote out of context) shown in the 
first three rows of Table 1. The Dice coefficient 
of each translation is smaller than that of the 
common subsequence ?out of context? in the last 
row. If we can tell common subsequence apart 
from correct translations, the common subse-
quence problem could be resolved. 
In this paper, we propose an improved preci-
sion method for extracting MWE translations 
from parallel corpora. Our method is similar to 
that of Smadja et al (1996), except that we in-
corporate lexical-level information into the asso-
ciation-based method. The algorithm works ef-
fectively for various types of MWEs, such as 
phrases, single words, rigid word sequences (i.e., 
no gaps) and gapped word sequences. Our expe-
riment results show that the proposed translation 
extraction method outperforms word alignment-
based methods and association-based methods. 
We also demonstrate that precise translations 
derived by our method significantly improve the 
performance of the Moses machine translation 
system. 
The remainder of this paper is organized as 
follows. Section 2 describes the methodology for   
extracting translation equivalences of MWEs. 
Section 3 describes the experiment and presents 
the results. In Section 4, we consider the appli-
cation of our results to machine translation. Sec-
tion 5 contains some concluding remarks. 
2 Extracting Translation Equivalences   
Our MWE translation extraction method is simi-
lar to the two-phase approach proposed by 
Smadja et al (1996). The two phases can be 
briefly described as follows:  
Phase 1: Extract candidate words correlated to 
the given MWE from parallel text. 
Phase 2:  
1. Generate possible translations for the 
MWE by combining the candidate words. 
2. Select possible translations by the Dice 
coefficient. 
We propose an association function, called the 
normalized correlation frequency, to extract 
candidate words in the phase 1. This method 
incorporates lexical-level information with asso-
ciation measure to overcome the contextual ef-
fect. In phase 2, we also propose a weighted fre-
quency function to filter out false common sub-
sequences from possible translations. The filter-
ing step is applied before the translation select-
ing step of phase 2.   
Before describing our extraction method, we 
define the following important terms used 
throughout the paper. 
Focused corpus (FC): This is the corpus 
created for each targeted MWE. It is a subset of 
the original parallel corpora, and is comprised of 
the selected aligned sentence pairs that contain 
the source MWE and its translations. 
Candidate word list (CW): A list of extracted 
candidate words for the translations of the 
source MWE. 
2.1 Selecting Candidate Words 
For a source MWE, we try to extract from the 
FC a set of k candidate words CW that are high-
ly correlated to the source MWE. We then as-
sume that the target translation is a combination 
of some words in CW. As noted by Smadja et al 
(1996), this two-step approach drastically reduc-
es the search space. 
However, translations of collocated context 
words in the source word sequence create noisy 
candidate words, which might cause incorrect 
extraction of target translations by naive statis-
tical correlation measures, such as the Dice coef-
479
ficient used by Smadja et al (1996). The need to 
avoid this context effect motivates us to propose 
a candidate word selection method that uses the 
normalized correlation frequency as an associa-
tion measure. 
The rationale behind the proposed method is 
as follows. When counting the word frequency, 
each word in the target corpus normally contri-
butes a frequency count of one. However, we are 
only interested in the word counts correlated to a 
MWE. Therefore, intuitively, we define the 
normalized count of a target word e as the trans-
lation probability of e given the MWE.  
We explain the concept of normalizing the 
correlation count in Section 2.1.1 and the com-
putation of the normalized correlation frequency 
in Section 2.1.2. 
2.1.1 Normalizing Correlation Count 
We propose an association measure called the 
normalized correlation frequency, which ranks 
the association strength of target words with the 
source MWE. For ease of explanation, we use 
the following notations: let f=f1,f2,?,fm and 
e=e1,e2,?,en be a pair of parallel Chinese and 
English sentences; and let w=t1,t2,?,tr be the 
Chinese source MWE. Hence, w is a subse-
quence of f.  
When counting the word frequency, each 
word in the target corpus normally contributes a 
frequency count of one. However, since we are 
interested in the word counts that correlate to w, 
we adopt the concept of the translation model 
proposed by Brown et al(1993). Each word e in 
a sentence e might be generated by some words, 
denoted as r, in the source sentence f. If r is 
non-empty the relation between r and w should 
fit one of the following cases: 
 
1) All words in r belong to w, i.e., wr ? , so 
we say that e is only generated by w. 
2) No words in r belong to w, i.e., wfr ?? , 
so we say that e is only generated by context 
words.  
3) Some words in r belong to w, while others 
are context words. 
 
Intuitively, In Cases 1 and 2, the correlation 
count of an instance e should be 1 and 0 respec-
tively. In Case 3, the normalized count of e is 
the expected frequency generated by w divided 
by the expected frequency generated by f. With 
that in mind, we define the weighted correlation 
count, wcc, as follows:  
 
?
?
??
??
?+
?+=
f
w
f
w
wfe
j
i
f j
f i
fep
fep
ewcc
||)|(
||)|(
),,;( , 
 
where ? is a very small smoothing factor in case 
e is not generated by any word in f. The proba-
bility p(e | f) is the word translation probability 
trained by IBM Model 1 on the whole parallel 
corpus. 
The rationale behind the weighted correlation 
count, wcc, is that if e is part of the translation of 
w, then its association with w should be stronger 
than other words in the context. Hence its wcc 
should be closer to 1. Otherwise, the association 
is weaker and the wcc should be closer to 0. 
2.1.2 Normalized Correlation 
Once the weighted correlation counts wcc is 
computed for each word in FC, we compute the 
normalized correlation frequency for each word 
e as the total sum of the  of all w 
in bilingual sentences (e, f)  in FC. The norma-
lized correlation frequency (ncf) is defined as 
follows: 
),,;( wfeewcc
 
?
=
=
n
i
iiewccencf
1
)()( ),,;();( wfew . 
 
We choose the top-n English words ranked by 
ncf as our candidate words and filter out those 
whose ncf is less than a pre-defined threshold. 
Table 2 shows the candidate words for the Chi-
nese term ???? (quote/take/interpret out of 
context) sorted by their ncf values. To illustrate 
the effectiveness ncf, we also display candidate 
words of the term with their Dice values in 
Tables 3. As shown in the tables, noise words 
such as justify, meaning and unfair are ranked 
lower using ncf than using Dice, while correct 
candidates, such as out, take and remark are 
ranked higher.  We present more experimental 
results in Section 3. 
 
2.2 Generation and Ranking of Candi-
date Translations 
After determining the candidate words, candi-
date translations of w can be generated by mark-
ing the candidate words in each sentence of FC. 
The word sequences marked in each sentence 
are deemed possible translations. At the same 
time, the weakly associated function words,  
480
Candidate words e freq ncf(e,w) 
context 54 31.55 
out 58 24.58 
quote 26 5.84 
take 23 4.81 
remark 8 1.84 
interpret 3 1.38 
piecemeal 1 0.98 
deliberate 3 0.98 
Table 2. Candidate words for the Chinese term 
???? sorted by their global normalized correla-
tion frequencies. 
 
Candidate words e freq dice(e,w) 
context 54 0.0399 
quote 26 0.0159 
deliberate 3 0.0063 
justify 3 0.0034 
interpretation 7 0.0032 
meaning 3 0.0029 
cite 3 0.0025 
unfair 4 0.0023 
Table 3. Candidate words for the Chinese term ??
??  sorted by their Dice coefficient values. 
 
which we fail to select in the candidate word 
selection stage, should be recovered. The rule is 
quite simple: if a function word is adjacent to 
any candidate word, it should be recovered. For 
example, in the following sentence, the function 
word of would be recovered and added to the 
marked sequence: 
 
?The financial secretary has 
been quoted out of context. 
??? ?? ? ?? ? ????.?  
 
 The marked words are shown in boldface.  
2.2.1 Generating Possible Translations 
Although we have selected a reliable candidate 
word list, it may still contain some noisy words 
due to the MWE?s collocation context. Consider 
the following example: 
 
...as quoted in the audit 
report, if taken out of con-
text...  
 
In this instance, quoted is a false positive; there-
fore, the marked word sequence m ?quoted tak-
en out of context? is not the correct translation. 
To avoid such false positives, we include m and 
all its subsequences as possible translations.  
quoted taken out of context 
quoted taken out of 
quoted taken out context 
quoted taken of context 
quoted out of context 
taken out of context 
? 
quoted out 
taken out 
quoted 
taken 
out 
context
Table 4. Example subsequences generated of w and 
add them to the candidate translation list.  
 
Table 4 shows the subsequences of m in the 
above example. The generation process is used 
to increase the coverage of correct translations in 
the candidate list; otherwise, many correct trans-
lations will be lost. However, the process may 
also trigger the side effect of the common sub-
sequence problem described in Section 1.  Since 
all candidates compete for the best translations 
by comparing their association strength with w, 
the common subsequences will have an advan-
tage. 
 
2.2.2 Filtering Common Subsequences 
To resolve the common subsequence effect prob-
lem, we evaluate each candidate translation, in-
cluding its subsequences, by a concept similar to 
the normalized correlation frequency. As men-
tioned in Section 1, the Dice coefficient tends to 
select the common subsequences of some candi-
dates because they have higher frequencies. To 
avoid this problem, we use the normalized corre-
lation frequency to filter out false common sub-
sequences from the candidate translation list. 
Here, we also use the weighted correlation count 
wcc to weight the frequency count of a candidate 
translation. Suppose we have a marked sequence 
in a sentence, m, whose subsequences are gener-
ated in the way described in the previous section. 
If the weighted count of m is assigned the score 
1, the weighted count (wc) of a subsequence t is 
then defined as follows: 
 
?
???
?=
tm
wfewmfet
e
ewccwc )),,;(1(),,,;( . 
 
The underlying concept of wc is that the original 
marked sequence m is supposed to be the most 
481
likely translation of w and the weighted count is 
set to 1. Then, if a subsequence t is generated by 
removing a word e from m, the weighted count 
of the subsequence is reduced by multiplying the 
complement probability of e generated by w. 
Note that the weighted correlation count wcc is 
the probability of the word e generated by w. 
After all  in each sentence of 
the FC have been computed, the weighted fre-
quency for a sequence t can be determined by 
summing the weighted frequencies over FC as 
follows:  
),,,;( wmfetwc
 
?
??
=
FC
wcwf
),(
),,,;();(
fe
wmfetwt . 
 
We compute the wf for each candidate transla-
tion and then sort the candidate translations by 
their wf values. 
Next, we filter out common subsequences 
based on the following rule: for a sequence t, if 
there is a super-sequence t' on the sorted candi-
date translation list and the wf value of t is less 
than that of t', then t is assumed be a common 
subsequence of real translations and removed 
from the list. 
 
candidate translation list freq wf 
quote out of context 19 17.55 
of context 35 15.45 
out of context 32 14.82 
quote of context 19 13.32 
out 35 11.92 
quote 23 11.63 
quote out 19 9.42 
Table 5. Part of the candidate translation list for the 
Chinese idiom, ????, sorted by the wf values. 
 
Table 5 shows an example of the rule?s appli-
cation. The candidate translation list is sorted by 
the translations? wf values. Then, candidates 2-7 
are removed because they are subsequences of 
the first candidate and their wf values are smaller 
than that of the first candidate. 
2.3 Selection of Candidate Translations 
Having removed the common subsequences of 
real translations from the candidate translation 
list of w, we can select the best translations by 
comparing their association strength with w for 
the remaining candidates.  The Dice coefficient 
is a good measure for assessing the association 
strength and selecting translations from the can-
didate list. For a candidate translation t, the Dice 
coefficient is defined as follows: 
 
)()(
),(2
),(
wt
wt
wt
pp
p
Dice += . 
 
Where p(t,w), p(t), p(w) are probabilities of  
(t,w), t, w derived from the training corpus.  
After obtaining the Dice coefficients of the 
candidate translations, we select the top-n candi-
date translations as possible translations of w. 
 
3 Experiments 
In our experiments, we use the Hong Kong Han-
sard and the Hong Kong News parallel corpora 
as training data. The training data was prepro-
cessed by Chinese word segmentation to identify 
words and parsed by Chinese parser to extract 
MWEs. To evaluate the proposed approach, we 
randomly extract 309 Chinese MWEs from 
training data, including dependent word pairs 
and rigid idioms. We then randomly select 103 
of those MWEs as the development set and use 
the other 206 as the test set. The reference trans-
lations of each Chinese MWE are manually ex-
tracted from the parallel corpora. 
 
3.1 Evaluation of Word Candidates 
To evaluate the method for selecting candidate 
words, we use the coverage rate, which is de-
fined as follows: 
 
?
?
?=
w w
ww
||
||1
A
CA
n
coverage , 
 
where n is the number of MWEs in the test set, 
Aw denotes the word set of the reference transla-
tions of w, and Cw denotes a candidate word list 
extracted by the system.  
Table 6 shows the coverage of our method, 
NCF, compared with the coverage of the IBM 
model 1 and the association-based methods MI, 
Chi-square, and Dice. As we can see, the top-10 
candidate words of NCF cover almost 90% of 
the words in the reference translations. Whereas, 
the coverage of the association-based methods 
and IBM model 1 is much lower than 90%. The 
result implies that the candidate extraction me-
thod can extract a more precise candidate set 
than other methods. 
 
482
Method Top10 Top20 Top30 
MI 0.514 0.684 0.760 
Chi-square 0.638 0.765 0.828 
Dice 0.572 0.735 0.803 
IBM 1 0.822 0.900 0.948
NCF 0.899 0.962 0.973 
Table 6. The coverage rates of the candidate words 
extracted by the compared methods 
 
Figure 1 shows the curve diagram of the cov-
erage rate of each method. As the figure shows, 
when the size of the candidate list is increased, 
the coverage rate of using NCF rises rapidly as n 
increases but levels off after n=20. Whereas, the 
coverage rates of other measures grow much 
slowly.  
 
 
Figure 1. The curve diagram of the coverage of 
the candidate word list compiled by each method. 
 
From the evaluation of candidate word selec-
tion, we find that the ncf method, which incorpo-
rates lexical-level information into association-
based measure, can effectively filter out noisy 
words and generates a highly reliable list of can-
didate words for a given MWE. 
 
3.2 Evaluating Extracted Translations 
To evaluate the quality of MWE translations 
extracted automatically, we use the following 
three criteria: 
 
1) Translation accuracy: 
This criterion is used to evaluate the top-n 
translations of the system. It treats each 
translation produced as a string and com-
pares the whole string with the given ref-
erence translations. If any one of the top-n 
hypothesis translations is included in the 
reference translations, it is deemed correct.   
2) WER (word error rate): 
This criterion compares the top-1 hypo-
thesis translation with the reference trans-
lations by computing the edit distance (i.e., 
the minimum number of substitutions, in-
sertions, and deletions) between the hypo-
thesis translation and the given reference 
translations. 
3) PER (position-independent word error 
rate): 
This criterion ignores the word order and 
computes the edit distance between the 
top-1 hypothesis translation and the given 
reference translations. 
 
We also use the MT task to evaluated our me-
thod with other systems. For that, we use the 
GIZA++ toolkit (Och et al, 2000 ) to align the 
Hong Kong Hansard and Hong Kong News pa-
rallel corpora. Then, we extract the translations 
of the given source sequences from the aligned 
corpus as the baseline. We use the following two 
methods to extract translations from the aligned 
results. 
 
1) Uni-directional alignment  
We mark all English words that were 
linked to any constituent of w in the pa-
rallel Chinese-English aligned corpora. 
Then, we extract the marked sequences 
from the corpora and compute the fre-
quency of each sequence. The top-n high 
frequency sequences are returned as the 
possible translations of w. 
2) Bi-directional alignments 
We use the grow-diag-final heuristic (Och 
et al, 1999) to combine the Chinese-
English and English-Chinese alignments, 
and then extract the top-n high frequency 
sequences as described in method 1. 
 
To determine the effect of the common subse-
quence filtering method, FCS, we divide the 
evaluation of our system into two phases: 
 
1) NCF+Dice: 
This system uses the normalized correla-
tion frequency, NCF, to select candidate 
words as described in Section 2.1. It then 
extracts candidate translations (described 
in Section 2.2), but FCS is not used. 
2) NCF+FCS+Dice: 
This is similar to system 1, but it uses 
FCS to filter out common subsequences 
(described in subsection 2.2.2). 
483
Method WER(%) PER(%) 
Uni-directional 4.84 4.02 
Bi-directional 5.84 5.12 
NCF+Dice 3.55 3.24 
NCF+FCS+Dice 2.45 2.23 
Table 7. Translation error rates of the systems. 
 
 
Method Top1 Top2 Top3 
Uni-directional 67.5 79.6 83.0 
Bi-directional 65.5 77.7 81.1 
NCF+Dice 72.8 85.9 88.3 
NCF+FCS+Dice 78.2 89.3 91.7 
Table 8. Translation accuracy rates of the systems. 
(%) 
 
Table 7 shows the word error rates for the 
above systems. As shown in the first and second 
rows, the translations extracted from uni-
directional alignments are better than those ex-
tracted from bi-directional alignments. This 
means that the grow-diag-final heuristic reduces 
the accuracy rate when extracting MWE transla-
tions.  
The results in the third row show that the 
NCF+Dice system outperforms the methods 
based on GIZA++. In other words, the NCF me-
thod can effectively resolve the difficulties of 
extracting MWE translations discussed in Sec-
tion 1. 
In addition, the fourth row shows that the 
NCF+FCS+Dice system also outperforms the 
NCF+Dice system.  Thus, the FCS method can 
resolve the common subsequence problem effec-
tively. 
Table 8 shows the translation accuracy rates 
of each system. The NCF+FCS+Dice system 
achieves the best translation accuracy. Moreover, 
it significantly improves the performance of 
finding MWE translation equivalences. 
 
4 Applying  MWE Translations to MT 
To demonstrate the usefulness of extracted 
MWE translations to existing statistical machine 
translation systems, we use the XML markup 
scheme provided by the Moses decoder, which 
allows the specification of translations for parts 
of a sentence. The procedure for this experiment 
consists of three steps: (1) the extracted MWE 
translations are added to the test set with the 
XML markup scheme, (2) after which the data is 
input to the Moses decoder to complete the 
translation task, (3) the results are evaluated 
 Moses  MWE +Moses
NIST06-sub 23.12 23.49 
NIST06 21.57 21.79 
 Table 9. BLEU scores of the translation results. 
 
using the BLEU metric (Papineni et al, 2002). 
4.1 Experimental Settings 
To train a translation model for Moses, we use 
the Hong Kong Hansard and the Hong Kong 
News parallel corpora as training data 
(2,222,570 sentence pairs). We also use the 
same parallel corpora to extract translations of 
MWEs. The NIST 2008 evaluation data (1,357 
sentences, 4 references) is used as development 
set and NIST 2006 evaluation data (1,664 sen-
tences, 4 references) is used as test set. 
4.2 Selection of MWEs 
Due to the limitation of the XML markup 
scheme, we only consider two types of MWEs: 
continuous bigrams and idioms. Since the goal 
of this experiment is not focus on extraction of 
MWEs, simple methods are applied to extract 
MWEs from the training data: (1) we collect all 
continuous bigrams from Chinese sentences in 
the training data and then simply filter out the 
bigrams by mutual information (MI) with a thre-
shold1, (2) we also extract all idioms from Chi-
nese sentences of the training data by collecting 
all 4-syllables words from the training data and 
filtering out obvious non-idioms, such as deter-
minative-measure words and temporal words by 
their part-of-speeches, because most Chinese 
idioms are 4-syllables words.  
In total, 33,767 Chinese bigram types and 
20,997 Chinese idiom types were extracted from 
training data; and the top-5 translations of each 
MWE were extracted by the method described in 
Section 2. Meanwhile 1,171 Chinese MWEs 
were added to the translations in the test set. The 
Chinese words covered by the MWEs in test 
data set were 2,081 (5.3%). 
 
4.3 Extra Information 
When adding the translations to the test data, 
two extra types of information are required by 
the Moses decoder. The first type comprises the 
function words between the translation and its 
context. For example, if ??  ??/economic 
cooperation is added to the test data, possible  
                                                 
1 We set the threshold at 5. 
484
source sentence ... ????<MWE>????</MWE>????? ... 
Moses ... entered blinded by the colourful community ... 
MWE+Moses ... entered the colourful community ... 
reference ... entered the colorful society ... 
source sentence ... ?????  <MWE>???  ??</MWE> ??? ... 
Moses ... do not want to see an escalation of crisis ... 
MWE+Moses ... do not want to see a further escalation of crisis ... 
reference ... don 't want to see the further escalation of the crisis ... 
source sentence ... ????????<MWE>?????</MWE> ... 
Moses ... the people 's interests ... 
MWE+Moses ... the people of the fundamental interests ... 
reference ... the fundamental interests of the masses ... 
Table 10. Examples of improved translation quality with the MWE translation equivalences. 
 
function words, such as ?in? or ?with?, should be 
provided for the translation. Because the Moses 
decoder does not generate function words that 
are context dependent, it treats a function word 
as a part of the translation. Therefore, we collect 
possible function words for each translation 
from the corpora when the conditional probabili-
ty is larger than a threshold2. 
The second type of information is the phrase 
translation probability and lexical weighting. 
Computing the phrase translation probability is 
trivial in the training corpora, but lexical weight-
ing (Koehn et al, 2003) needs lexical-level 
alignment. For convenience, we assume that 
each word in an MWE links to each word in the 
translations. Under this assumption, the lexical 
weighting is simplified as follows:   
 
??
??= ?= aji ji
n
i
w efpajij
ap
),(1
)|(
|}),(|{|
1
),|( ef
        ? ?
= ??
?
n
i e
ji
j
efp
1
)|(
||
1
ee
. 
 
Then, it is trivial to compute the simplified lexi-
cal weighting of each MWE correspondence 
when the word translation probability table is 
provided. Here, we use the IBM model 1 to learn 
the table from the training data. 
4.4 Evaluation Results 
We trained a model using Moses toolkit (Koehn 
et al, 2007) on the training data as our baseline 
system.  
Table 9 shows the influence of adding the 
MWE translations to the test data. In the first 
row (NIST06-sub), we only consider sentences 
containing MWE translations for BLEU score 
evaluation (726 sentences). In the second row, 
we took the whole NIST 2006 evaluation set 
into consideration (1,664 sentences). The Chi-
nese words covered by the MWEs in NIST06-
sub and NIST06 were 9.9% and 5.3% respec-
tively. 
Adding MWE translations to the test data sta-
tistically significantly lead to better results than 
those of the baseline. Significance was tested 
using a paired bootstrap (Koehn, 2004) with 
1000 samples (p<0.02). Although the improve-
ment in BLEU score seems small, it is actually 
reasonably good given that the MWEs account 
for only 5% of the NIST06 test set. Examples of 
improved translations are shown in Table 10. 
There is still room for improvement of the pro-
posed MWE extraction method in order to pro-
vide more MWE translation pairs or design a 
feasible way to incorporate discontinuous bilin-
gual MWEs to the decoder. 
5 Conclusions and Future Work 
We have proposed a high precision algorithm for 
extracting translations of multiword expressions 
from parallel corpora. The algorithm can be used 
to translate any language pair and any type of 
word sequence, including rigid sequences and 
discontinuous sequences. Our evaluation results 
show that the algorithm can cope with the diffi-
culties caused by indirect association and the 
common subsequence effects, leading to signifi-
cant improvement over the word alignment-
based extraction methods used by the state of the 
art systems and other association-based extrac-
tion methods. We also demonstrate that ex-
tracted translations significantly improve the                                                  
2 We set the threshold at 0.1. 
485
performance of the Moses machine translation 
system. 
In future work, it would be interesting to de-
velop a machine translation model that can be 
integrated with the translation acquisition algo-
rithm in a more effective way. Using the norma-
lized-frequency score to help phrase alignment 
tasks, as the grow-diag-final heuristic, would 
also be interesting direction to explore. 
 
Acknowledgement 
This research was supported in part by the Na-
tional Science Council of Taiwan under the NSC 
Grants: NSC 96-2221-E-001-023-MY3. 
References  
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parame-
ter Estimation. Computational Linguistics, 
19(2):263-311.  
Gao, Jianfeng, Jian-Yun Nie, Hongzhao He, Weijun 
Chen, Ming Zhou. 2002. Resolving Query Trans-
lation Ambiguity using a Decaying Co-occurrence 
Model and Syntactic Dependence Relations. In 
Proc. of SIGIR?02. pp. 183 -190. 
Kitamura, Mihoko and Yuji Matsumoto. 1996. Au-
tomatic Extraction of Word Sequence Correspon-
dences in Parallel Corpora. In Proc. of the 4th An-
nual Workshop on Very Large Corpora. pp. 79-87. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Proc. 
of HLT/NAACL?03. pp. 127-133. 
Koehn, Philipp. 2004. Statistical significance tests for 
machine translation evaluation. In Proc. 
EMNLP?04. pp. 388-395. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses: 
Open source toolkit for statistical machine transla-
tion. In ACL?07, demonstration session. 
Kupiec, Julian. 1993. An Algorithm for Finding 
Noun Phrase Correspondences in Bilingual Corpo-
ra. In Proc. of ACL?93 . pp. 17-22. 
Liang, Percy, Ben Taskar, Dan Klein. 2006. Align-
ment by Agreement. In Proc. of HLT/NAACL?06. 
pp. 104-111. 
Ma, Yanjun, Nicolas Stroppa, Andy Way. 2007. 
Bootstrapping Word Alignment via Word Packing. 
In Proc. of ACL?07. pp. 304-311. 
Ma, Yanjun, Sylwia Ozdowska, Yanli Sun, and Andy 
Way. 2008. Improving Word Alignment Using 
Syntactic Dependencies. In Proc. of ACL/HLT?08 
Second Workshop on Syntax and Structure in Sta-
tistical Translation. pp. 69-77. 
Melamed, Ilya Dan. 2001. Empirical Methods for 
Exploiting parallel Texts. MIT press. 
Och, Franz Josef and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of ACL?00. 
pp. 440-447. 
Och, Franz Josef, Christoph Tillmann, and Hermann 
Ney. 1999. Improved Alignment Models for Sta-
tistical Machine Translation. In Proc. of 
EMNLP/VLC?99. pp. 20-28. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Proc. 
of ACL?02. pp. 311-318. 
Smadja, Frank, Kathleen R. McKeown, and Vasileios 
Hatzivassiloglou. 1996. Translating Collocations 
for Bilingual Lexicons: A Statistical Approach. 
Computational Linguistics, 22(1):1-38. 
Wu, Dekai. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403.  
Wu, Hua, Ming Zhou. 2003. Synonymous Colloca-
tion Extraction Using Translation Information. In 
Proc. of ACL?03. pp. 120-127. 
Yamamoto, Kaoru, Yuji Matsumoto. 2000. Acquisi-
tion of Phrase-level Bilingual Correspondence us-
ing Dependency Structure. In Proc. of COL-
ING?00. pp. 933-939. 
 
486
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 519 ? 529, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Web-Based Unsupervised Learning for Query 
Formulation in Question Answering 
Yi-Chia Wang1, Jian-Cheng Wu2, Tyne Liang1, and Jason S. Chang2 
1
 Dep. of Computer and Information Science, National Chiao Tung University,  
1001 Ta Hsueh Rd., Hsinchu, Taiwan 300, R.O.C. 
rhyme.cis92g@nctu.edu.tw, tliang@cis.nctu.edu.tw 
2
 Dep. of Computer Science, National Tsing Hua University,  
101, Section 2 Kuang Fu Road, Hsinchu, Taiwan 300, R.O.C. 
d928322@oz.nthu.edu.tw, jschang@cs.nthu.edu.tw 
Abstract. Converting questions to effective queries is crucial to open-domain 
question answering systems. In this paper, we present a web-based 
unsupervised learning approach for transforming a given natural-language 
question to an effective query. The method involves querying a search engine 
for Web passages that contain the answer to the question, extracting patterns 
that characterize fine-grained classification for answers, and linking these 
patterns with n-grams in answer passages. Independent evaluation on a set of 
questions shows that the proposed approach outperforms a naive keyword-
based approach in terms of mean reciprocal rank and human effort. 
1   Introduction 
An automated question answering (QA) system receives a user?s natural-language 
question and returns exact answers by analyzing the question and consulting a large 
text collection [1, 2]. As Moldovan et al [3] pointed out, over 60% of the QA errors 
can be attributed to ineffective question processing, including query formulation and 
query expansion.  
A naive solution to query formulation is using the keywords in an input question as 
the query to a search engine. However, it is possible that the keywords may not appear 
in those answer passages which contain answers to the given question. For example, 
submitting the keywords in ?Who invented washing machine?? to a search engine like 
Google may not lead to retrieval of answer passages like ?The inventor of the automatic 
washer was John Chamberlain.? In fact, by expanding the keyword set (?invented?, 
?washing?, ?machine?) with ?inventor of,? the query to a search engine is effective in 
retrieving such answer passages as the top-ranking pages. Hence, if we can learn how to 
associate a set of questions (e.g. (?who invented ???) with effective keywords or 
phrases (e.g. ?inventor of?) which are likely to appear in answer passages, the search 
engine will have a better chance of retrieving pages containing the answer. 
In this paper, we present a novel Web-based unsupervised learning approach to 
handling question analysis for QA systems. In our approach, training-data questions 
are first analyzed and classified into a set of fine-grained categories of question 
520 Y.-C. Wang et al 
patterns. Then, the relationships between the question patterns and n-grams in answer 
passages are discovered by employing a word alignment technique. Finally, the best 
query transforms are derived by ranking the n-grams which are associated with a 
specific question pattern. At runtime, the keywords in a given question are extracted 
and the question is categorized. Then the keywords are expanded according the 
category of the question. The expanded query is the submitted to a search engine in 
order to bias the search engine to return passages that are more likely to contain 
answers to the question. Experimental results indicate the expanded query indeed 
outperforms the approach of directly using the keywords in the question. 
2   Related Work 
Recent work in Question Answering has attempted to convert the original input 
question into a query that is more likely to retrieve the answers. Hovy et al [2] utilized 
WordNet hypernyms and synonyms to expand queries to increase recall. Hildebrandt et 
al. [4] looked up in a pre-compiled knowledge base and a dictionary to expand a 
definition question. However, blindly expanding a word using its synonyms or 
dictionary gloss may cause undesirable effects. Furthermore, it is difficult to determine 
which of many related word senses should be considered when expanding the query.  
Radev et al [5] proposed a probabilistic algorithm called QASM that learns the best 
query expansion from a natural language question. The query expansion takes the 
form of a series of operators, including INSERT, DELETE, REPLACE, etc., to 
paraphrase a factual question into the best search engine query by applying 
Expectation Maximization algorithm. On the other hand, Hermjakob et al [6] 
described an experiment to observe and learn from human subjects who were given a 
question and asked to write queries which are most effective in retrieving the answer 
to the question. First, several randomly selected questions are given to users to 
?manually? generate effective queries that can bias Web search engines to return 
answers. The questions, queries, and search results are then examined to derive seven 
query reformulation techniques that can be used to produce queries similar to the ones 
issued by human subjects. 
In a study closely related to our work, Agichtein et al [7] presented Tritus system 
that automatically learns transforms of wh-phrases (e.g. expanding ?what is? to 
?refers to?) by using FAQ data. The wh-phrases are restricted to sequences of 
function word beginning with an interrogative, (i.e. who, what, when, where, why, 
and how).  These wh-phrases tend to coarsely classify questions into a few types. 
Tritus uses heuristic rules and thresholds of term frequencies to learn transforms. 
In contrast to previous work, we rely on a mathematical model trained on a set of 
questions and answers to learn how to transform the question into an effective query. 
Transformations are learned based on a more fine-grained question classification 
involving the interrogative and one or more content words. 
3   Transforming Question to Query 
The method is aimed at automatically learning of the best transforms that turn a given 
natural language question into an effective query by using the Web as corpus. To that 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 521 
end, we first automatically obtain a collection of answer passages (APs) as the 
training corpus from the Web by using a set of (Q, A) pairs. Then we identify the 
question pattern for each Q by using statistical and linguistic information. Here, a 
question pattern Qp is defined as a question word plus one or two keywords that are 
related to the question word. Qp represents the question intention and it can be treated 
as a preference indicative for fine-grained type of named entities. Finally, we decide 
the transforms Ts for each Qp by choosing those phrases in the APs that are 
statistically associated with Qp and adjacent to the answer A. 
Table 1. An example of converting a question (Q) with its answer (A) to a SE query and 
retrieving answer passages (AP) 
(Q, A) AP 
Bungalow For Rent in Islamabad, Capital 
Pakistan. Beautiful Big House For ? What is the capital of Pakistan?  Answer:( Islamabad) 
(k1, k2,?, kn, A) 
Islamabad is the capital of Pakistan. Current 
time, ? 
capital, Pakistan, Islamabad ?the airport which serves Pakistan's capital Islamabad, ? 
3.1   Search the Web for Relevant Answer Passages 
For training purpose, a large amount of question/answer passage pairs are mined from 
the Web by using a set of question/answer pairs as seeds.  
More formally, we attempt to retrieve a set of (Q, AP) pairs on the Web for training 
purpose, where Q stands for a natural language question, and AP is a passage 
containing at least one keyword in Q and A (the answer to Q). The seed data (Q, A) 
pairs can be acquired from many sources, including trivia game Websites, TREC QA 
Track benchmarks, and files of Frequently Asked Questions (FAQ). The output of 
this training-data gathering process is a large collection of (Q, AP) pairs. We describe 
the procedure in details as follows: 
1. For each (Q, A) pair, the keywords k1, k2,?, kn are extracted from Q by removing 
stopwords. 
2. Submit (k1, k2,?, kn, A) as a query to a search engine SE. 
3. Download the top n summaries returned by SE. 
4. Separate sentences in the summaries, and remove HTML tags, URL, special 
character references (e.g., ?&lt;?). 
5. Retain only those sentences which contain A and some ki. 
Consider the example of gathering answer passages from the Web for the (Q, A) 
pair where Q = ?What is the capital of Pakistan?? and A = ?Islamabad.? See Table 1 
for the query submitted to a search engine and potential answer passages returned. 
3.2   Question Analysis 
This subsection describes the presented identification of the so-called ?question 
pattern? which is critical in categorizing a given question and transforming the 
question into a query. 
522 Y.-C. Wang et al 
Formally, a ?question pattern? for any question is defined as following form: 
question-word  head-word+ 
where ?question-word? is one of the interrogatives (Who/What/Where/When/How) 
and the ?head-word? represents the headwords in the subsequent chunks that tend to 
reflect the intended answer more precisely. If the first headword is a light verb, an 
additional headword is needed. For instance, ?who had hit? is a reasonable question 
pattern for ?Who had a number one hit in 1984 with ?Hello???, while ?who had? 
seems to be too coarse. 
In order to determine the appropriate question pattern for each question, we 
examined and analyzed a set of questions which are part-of-speech (POS) tagged and 
phrase-chunked. With the help of a set of simple heuristic rules based on POS and 
chunk information, fine-grained classification of questions can be carried out 
effectively. 
Question Pattern Extraction 
After analyzing recurring patterns and regularity in quizzes on the Web, we designed 
a simple procedure to recognize question patterns. The procedure is based on a small 
set of prioritized rules. 
The question word which is one of the wh-words (?who,? ?what,? ?when,? 
?where,? ?how,? or ?why?) tagged as determiner or adverbial question word. 
According to the result of POS tagging and phrase chunking, we further decide the 
main verb and the voice of the question. Then, we apply the following expanded rules 
to extract words to form question patterns: 
Rule 1: Question word in a chunk of length more than one (see Example (1) in Table 2). 
Qp = question word + headword in the same chunk 
Rule 2: Question word followed by a light verb and Noun Phrase(NP) or 
Prepositional Phrase(PP) chunk (Example (2)). 
Qp = question word + light verb +headword in the following NP or PP chunk 
Rule 3: Question word followed immediately by a verb (Example (3)).  
Qp = question word + headword in the following Verb Phrase(VP) or NP chunk 
Rule 4: Question word followed by a passive VP (Example (4)).  
Qp = Question word + ?to be? + headword in the passive VP chunk 
Rule 5: Question word followed by the copulate ?to be? and an NP (Example (5)).  
Qp = Question word + ?to be? + headword in the next NP chunk 
Rule 6: If none of the above rules are applicable, the question pattern is the question 
word. 
By exploiting linguistic information of POS and chunks, we can easily form the 
question pattern. These heuristic rules are intuitive and easy to understand. Moreover, 
the fact that these patterns which tend to recur imply that they are general and it is 
easy to gather training data accordingly. These question patterns also indicate a 
preference for the answer to be classified with a fine-grained type of proper nouns. In 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 523 
the next section, we describe how we exploit these patterns to learn the best question-
to-query transforms. 
Table 2. Example questions and question patterns (of words shown in bold) 
(1) Which female singer performed the first song on Top of the Pops? 
(2) Who in 1961 made the first space flight? 
(3) Who painted ?The Laughing Cavalier?? 
(4) What is a group of geese called? 
(5) What is the second longest river in the world? 
3.3   Learning Best Transforms 
This section describes the procedure for learning transforms Ts which convert the 
question pattern Qp into bigrams in relevant APs. 
Word Alignment Across Q and AP 
We use word alignment techniques developed for statistical machine translation to 
find out the association between question patterns in Q and bigrams in AP. The reason 
why we use bigrams in APs instead of unigrams is that bigrams tend to have more 
unique meaning than single words and are more effective in retrieving relevant 
passages. 
We use Competitive Linking Algorithm [8] to align a set of (Q, AP) pairs. The 
method involves preprocessing steps for each (Q, AP) pair so as to filter useless 
information: 
1. Perform part-of-speech tagging on Q and AP. 
2. Replace all instances of A with the tag <ANS> in APs to indicate the location of 
the answers. 
3. Identify the question pattern, Qp and keywords which are not a named entity. We 
denote the question pattern and keywords as q1, q2, ..., qn. 
4. Convert AP into bigrams and eliminate bigrams with low term frequency (tf) or 
high document frequency (df). Bigrams composed of two function words are also 
removed, resulting in bigrams a1, a2, ..., am. 
We then align q?s and a?s via Competitive Linking Algorithm (CLA) procedure as 
follows: 
Input: A collection C of (Q; A) pairs, where (Q; A) = (q1 = Qp , q2, q3, ..., qn ; a1, a2, 
..., am) 
Output: Best alignment counterpart a?s for all q?s in C 
1. For each pair of (Q; A) in C and for all qi and aj in each pair of C, calculate LLR(qi, 
aj), logarithmic likelihood ratio (LLR) between qi and aj, which reflects their 
statistical association. 
2. Discard (q, a) pairs with a LLR value lower than a threshold. 
524 Y.-C. Wang et al 
3. For each pair of (Q; A) in C and for all qi and aj therein, carry out Steps 4-7: 
4. Sort list of (qi, aj) in each pair of (Q ; A) by decreasing LLR value. 
5. Go down the list and select a pair if it does not conflict with previous selection. 
6. Stop when running out of pairs in the list. 
7. Produce the list of aligned pairs for all Qs and APs. 
8. Tally the counts of aligning (q, a). 
9. Select top k bigrams, t1, t2, ..., tk, for every question pattern or keyword q. 
The LLR statistics is generally effective in distinguishing related terms from 
unrelated ones. However, if two terms occur frequently in questions, their alignment 
counterparts will also occur frequently, leading to erroneous alignment due to indirect 
association. CLA is designed to tackle the problem caused by indirect association. 
Therefore, if we only make use of the alignment counterpart of the question pattern, 
we can keep the question keywords in Q so as to reduce the errors caused by indirect 
association. For instance, the question ?How old was Bruce Lee when he died?? Our 
goal is to learn the best transforms for the question pattern ?how old.? In other words, 
we want to find out what terms are associated with ?how old? in the answer passages. 
However, if we consider the alignment counterparts of ?how old? without considering 
those keyword like ?died,? we run the risk of getting ?died in? or ?is dead? rather than 
?years old? and ?age of.? If we have sufficient data for a specific question pattern like 
?how long,? we will have more chances to obtain alignment counterparts that are 
effective terms for query expansion. 
Distance Constraint and Proximity Ranks 
In addition to the association strength implied with alignment counts and co-
occurrence, the distance of the bigrams to the answer should also be considered. We 
observe that terms in the answer passages close to the answers intuitively tend to be 
useful in retrieving answers. Thus, we calculate the bigrams appearing in a window of 
three words appearing on both sides of the answers to provide additional constraints 
for query expansion. 
Combing Alignment and Proximity Ranks 
The selection of the best bigrams as the transforms for a specific question pattern is 
based on a combined rank of alignment count and proximity count. It takes the 
average of these two counts to re-rank bigrams. The average rank of a bigram b is  
Rankavg (b) = (Rankalign (b)+ Rankprox (b))/2, 
where Rankalign (b) is the rank of b?s alignment count and Rankprox (b) is the rank of 
b?s proximity count. The n top-ranking bigrams  for a specific type of question will be 
chosen to transform the question pattern into query terms. For the question pattern 
?how old,? the candidate bigrams with alignment ranks, co-occurring ranks, and 
average ranks are shown in Table 3. 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 525 
Table 3. Average rank calculated from for the bigram counterparts of ?how old? 
Bigrams Alignment Rank Proximity Rank Avg. Rank Final Rank 
age of 1 1 1 1 
years old 2 2 2 2 
ascend the 3 - - - 
throne in 4 3 3.5 3 
the youngest 3 - - - 
? ? ? ? ? 
3.4   Runtime Transformation of Questions 
At runtime, a given question Q submitted by a user is converted into one or more 
keywords and a question pattern, which is subsequently expanded in to a sequence of 
query terms based on the transforms obtained at training. 
We follow the common practice of keyword selection in formulating Q into a 
query: 
? Function words are identified and discarded. 
? Proper nouns that are capitalized or quoted are treated as a single search term with 
quotes. 
Additionally, we expand the question patterns based on alignment and proximity 
considerations: 
? The question pattern Qp is identified according to the rules (in Section 3.2) and is 
expanded to be a disjunction (sequence of ORs) of Qp?s headword and n top-
ranking bigrams (in section 3.3) 
? The query will be a conjunction (sequence of ANDs) of expanded Qp, proper 
names, and remaining keywords. Except for the expanded Qp, all other proper 
names and keywords will be in the original order in the given question for the best 
results. 
Table 4. An example of transformation from question into query 
Question 
How old was Bruce Lee when he died? 
Question pattern Proper noun Keyword 
how old 
Transformation 
age of, years old 
?Bruce Lee? died 
Expanded query 
Boolean query: ( ?old? OR ?age of? OR ?years old? ) AND ?Bruce Lee? AND ?died? 
Equivalent Google query: (old || ?age of? || ?years old?) ?Bruce Lee? died 
526 Y.-C. Wang et al 
For example, formulating a query for the question ?How old was Bruce Lee when 
he died?? will result in a question pattern ?how old.? Because there is a proper noun 
?Bruce Lee? in the question and a remaining keyword ?died,? the query becomes  
?( ?old? OR ?age of? OR ?years old? ) AND ?Bruce Lee? AND ?died.?? Table 4 lists the 
query formulating for the example question.  
4   Experiments and Evaluation 
The proposed method is implemented by using the Web search engine, Google, as the 
underlying information retrieval system. The experimental results are also justified 
with assessing the effectiveness of question classification and query expansion. 
We used a POS tagger and chunker to perform shallow parsing of the questions 
and answer passages. The tagger was developed using the Brown corpus and 
WordNet. The chunker is built from the shared CoNLL-2000 data provided by 
CoNLL-2000. The shared task CoNLL-2000 provides a set of training and test data 
for chunks. The chunker we used produces chunks with an average precision rate of 
about 94%. 
4.1   Evaluation of Question Patterns 
The 200 questions from TREC-8 QA Track provide an independent evaluation of how 
well the proposed method works for question pattern extraction works. We will also 
give an error analysis. 
Table 5. Evaluation results of question pattern extraction 
 Two ?good? labels At least one ?good? label 
Precision (%) 86 96 
Table 6. The first five questions with question patterns and judgment 
Question Question pattern Judgment 
Who is the author of the book, "The Iron 
Lady: A Biography of Margaret Thatcher"? Who-author good 
What was the monetary value of the Nobel 
Peace Prize in 1989? What value good  
What does the Peugeot company manufacture? What do 
manufacture good 
How much did Mercury spend on advertising 
in 1993?      How much good 
What is the name of the managing director of 
Apricot Computer? What name bad 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 527 
Two human judges both majoring in Foreign Languages were asked to assess the 
results of question pattern extraction and give a label to each extracted question 
pattern. A pattern will be judged as ?good? if it clearly expresses the answer 
preference of the question; otherwise, it is tagged as ?bad.? The precision rate of 
extraction for these 200 questions is shown in Table 5. The second column indicates 
the precision rate when both of two judges agree that an extracted question pattern is 
?good.? In addition, the third column indicates the rate of those question patterns that 
are found to be ?good? by either judge. The results imply that the proposed pattern 
extraction rules are general, since they are effective even for questions independent of 
the training and development data. Table 6 shows evaluation results for ?two ?good? 
labels? of the first five questions. 
We summarize the reasons behind these bad patterns: 
? Incorrect part-of-speech tagging and chunking 
? Imperative questions such as ?Name the first private citizen to fly in space.? 
? Question patterns that are not specific enough 
For instance, the system produces ?what name? for ?What is the name of the 
chronic neurological autoimmune disease which ? ??, while the judges suggested 
that ?what disease.?. Indeed, some of the patterns extracted can be modified to meet 
the goal of being more fine-grained and indicative of a preference to a specific type of 
proper nouns or terminology. 
4.2   Evaluation of Query Expansion 
We implemented a prototype of the proposed method called Atlas (Automatic 
Transform Learning by Aligning Sentences of question and answer). To develop the 
system of Atlas, we gathered seed training data of questions and answers from a trivia 
game website, called QuizZone1. We collected the questions posted in June, 2004 on 
QuizZone and obtained 3,851 distinct question-answer pairs. We set aside the first 45 
questions for testing and used the rest for training. For each question, we form a query 
with question keywords and the answer and submitted the query to Google to retrieve 
top 100 summaries as the answer passages. In all, we collected 95,926 answer passages.  
At training time, we extracted a total of 338 distinct question patterns from 3,806 
questions. We aligned these patterns and keywords with bigrams in the 95,926 answer 
passages, identified the locations of the answers, and obtained the bigrams appearing 
within a distance of 3 of the answers. At runtime, we use the top-ranking bigram to 
expand each question pattern. If no such bigrams are found, we use only the keyword 
in the question patterns. The expanded terms for question pattern are placed at the 
beginning of the query.  
We submitted forty-five keyword queries and the same number of expanded 
queries generated by Atlas for the test questions to Google and obtained ten returned 
summaries for evaluation. For the evaluation, we use three indicators to measure the 
performance. The first indicator is the mean reciprocal rank (MRR) of the first 
relevant document (or summary) returned. If the r-th document (summary) returned is 
the one with the answer, then the reciprocal rank of the document (summary) is 1/r. 
                                                          
1
 QuizZone (http://www.quiz-zone.co.uk) 
528 Y.-C. Wang et al 
The mean reciprocal rank is the average reciprocal rank of all test questions. The 
second indicator of effective query is the recall at R document retrieved (Recall at R). 
The last indicator measures the human effort (HE) in finding the answer. HE is 
defined as the least number of passages needed to be viewed for covering all the 
answers to be returned from the system. 
The average length of these test questions is short. We believe the proposed 
question expansion scheme helps those short sentences, which tend to be less 
effective in retrieving answers. We evaluated the expanded queries against the same 
measures for summaries returned by simple keyword queries. Both batches of 
returned summaries for the forty-five questions were verified by two human judges. 
As shown in Table 7, the MRR produced by keyword-based scheme is slightly lower 
than the one yielded by the presented query expansion scheme. Nevertheless, such 
improvement is encouraging by indicating the effectiveness of the proposed method. 
Table 8 lists the comparisons in more details. It is found that our method is 
effective in bringing the answers to the top 1 and top 2 summaries as indicated by the 
high Recall of 0.8 at R = 2. In addition, Table 8 also shows that less user?s efforts are 
needed by using our approach. That is, for each question, the average of summaries 
required to be viewed by human beings goes down from 2.7 to 2.3. 
In the end, we found that those bigrams containing a content word and a function 
word  turn out to be very effective. For instance, our method tends to favor transforms 
Table 7. Evaluation results of MRR 
Performances MRR 
GO (Direct keyword query for Google) 0.64 
AT+GO (Atlas expanded query for Google) 0.69 
Table 8. Evaluation Result of Recall at R and Human Effort 
Rank count Recall at R Rank GO AT+GO GO AT+GO 
1 25 26 0.56 0.58 
2 6 10 0.69 0.80 
3 5 3 0.80 0.87 
4 0 1 0.80 0.89 
5 1 1 0.82 0.91 
6 2 0 0.87 0.91 
7 1 0 0.89 0.91 
8 2 0 0.93 0.91 
9 0 1 0.93 0.93 
10 0 0 0.93 0.93 
No answers 3 3 
Human Effort 122 105 
 
# of questions 45 45 
HE per question 2.7 2.3 
 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 529 
such as ?who invented? to bigrams such as ?invented by,? ?invent the,? and ?inventor 
of.? This contrasts to conventional wisdom of using a stoplist of mostly function 
words and excluding them from consideration in a query. Our experiment also shows 
a function word as part of a phrasal term seems to be very effective, for it indicate an 
implied relation with the answer.  
5   Conclusion and Future Work 
In this paper, we introduce a method for learning query transformations that improves 
the ability to retrieve passages with answers using the Web as corpus. The method 
involves question classification and query transformations using a learning-based 
approach. We also describe the experiment with over 3,000 questions indicates that 
satisfactory results were achieved. The experimental results show that the proposed 
method provides effective query expansion that potentially can lead to performance 
improvement for a question answering system. 
A number of future directions present themselves. First, the patterns learned from 
answer passages acquired on the Web can be refined and clustered to derive a 
hierarchical classification of questions for more effective question classification. Second, 
different question patterns, like ?who wrote? and ?which author?, should be treated as the 
same in order to cope with data sparseness and improve system performance. On the 
other hand, an interesting direction is the generating pattern transformations that contain 
the answer extraction patterns for different types of questions.  
References 
1. Ittycheriah, A., Franz, M., Zhu, W.-J., and Rathaparkhi, A. 2000. IBM?s statistical 
question answering system. In Proceedings of the TREC-9 Question Answering Track, 
Gaithersburg, Maryland. 
2. Hovy, E., Gerber, L., Hermjakob, U., Junk, M., and Lin, C.-Y. 2000. Question answering 
in Webclopedia. In Proceedings of the TREC-9 Question Answering Track, Gaithersburg, 
Maryland. 
3. Moldovan D., Pasca M., Harabagiu S., & Surdeanu M. 2002. Performance Issues and error 
Analysis in an Open-Domain Question Answering System. In Proceedings of the 40th 
Annual Meeting of ACL, Philadelphia, Pennsylvania. 
4. Hildebrandt, W., Katz, B., & Lin, J. 2004. Answering definition questions with multiple 
knowledge sources. In Proceedings of the 2004 Human Language Technology Conference 
and the North American Chapter of the Association for Computational. 
5. Radev, D. R., Qi, H., Zheng, Z., Blair-Goldensohn, S., Fan, Z. Z. W., and Prager, J. M. 
2001. Mining the web for answers to natural language questions. In Proceedings of the 
International Conference on Knowledge Management (CIKM-2001), Atlanta, Georgia. 
6. Hermjakob, U., Echihabi, A., and Marcu, D. 2002. Natural Language Based 
Reformulation Resource and Web Exploitation for Question Answering. In Proceeding of 
TREC-2002, Gaithersburg, Maryland. 
7. Agichtein, E., Lawrence, S., and Gravano, L. Learning to find answers to questions on the 
Web. 2003. In ACM Transactions on Internet Technology (TOIT), 4(2):129-162. 
8. Melamed, I. D. 1997. A Word-to-Word Model of Translational Equivalence. In 
Proceedings of the 35st Annual Meeting of ACL, Madrid, Spain. 
9. Yi-Chia Wang, Jian-Cheng Wu, Tyne Liang, and Jason S. Chang. 2004. Using the Web as 
Corpus for Un-supervised Learning in Question Answering, Proceedings of Rocling 2004, 
Taiwan. 
NAACL HLT Demonstration Program, pages 21?22,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Learning to find transliteration on the Web 
Chien-Cheng Wu Department of Computer Science National Tsing Hua University  101 Kuang Fu Road, Hsin chu, Taiwan d9283228@cs.nthu.edu.tw 
Jason S. Chang Department of Computer Science National Tsing Hua University  101 Kuang Fu Road, Hsin chu, Taiwan jschang@cs.nthu.edu.tw 
  This prototype demonstrate a novel method for learning to find transliterations of proper nouns on the Web based on query expansion aimed at maximizing the probability of retrieving translit-erations from existing search engines. Since the method we used involves learning the morphologi-cal relationships between names and their translit-erations, we refer to this IR-based approach as morphological query expansion for machine trans-literation. The morphological query expansion approach is general in scope and can be applied to translation and transliteration, but we focus on transliteration in this paper. Many texts containing proper names (e.g., ?The cities of Mesopotamia prospered under Parthian and Sassanian rule.?) are submitted to machine translation services on the Web every day, and there are also service on the Web specifically tar-get transliteration of proper names, including CHINET (Kwok et al 2005) ad Livetrans (Lu, Chien, and Lee 2004). Machine translation systems on the Web such as Yahoo Translate (babelfish.yahoo.com) and Goo-gle Translate (translate.google.com/translate_t.g) typically use a bilingual dictionary that is either manually compiled or learned from a parallel cor-pus. However, such dictionaries often have insuffi-cient coverage of proper names and technical terms, leading to poor translation due to out of vo-cabulary problem. The OOV problems of machine translation or cross language information retrieval can be handled more effectively by learning to find transliteration on the Web.  Consider Sentence 1 containing three place names. 1. The cities of Mesopotamia prospered under Parthian and Sassanian rule. 
2. ?????parthian ????sassanian??? 3. ??????????????????????? Google Translate produce Sentence 2, leaving ?Parthian? and ?Sassanian? not translated. A good response might be a translation like Sentence 3 where all place names have appropriate translitera-tions (underlined). These transliterations can be more effectively retrieved from mixed code Web pages by extend each of the place names into a query (e.g., ?Parthian NEAR ??). Intuitively by requiring one of likely prefix transliteration mor-phemes (e.g., ??? or ??? for ?par-? names), we can bias the search engine towards retrieving the correct transliterations (e.g., ????? and ?????) in snippets of many top-ranked documents.  The method involves pairing up the prefixing morphemes between name and transliteration in a set of train data, calculating the statistical associa-tion for these pair, and selecting pairs with a high degree of statistical association. The results of this training stage are morphological relationships be-tween prefixes and postfixes of names and translit-erations. At run time, a given name is automati-cally extended into a query with relevant prefixing morphemes, then the query is submit to some search engine. After retrieving snippets from a search engine, the system extract transliterations from the snippets based on redundancy, proximity between name and transliteration, and cross lan-guage morphological relationships of prefix and postfix. We present a new machine transliteration sys-tem based on information retrieval and morpho-logical query expansion. The system automatically 
21
learns to extend the proper names into a query ex-pected to retrieve and extract transliterations of the proper names. Consider the case of transliteration of ?Parthian.? The system looks at possible pre-fixes of the given name, including p-, pa-, par-, and part-, and determine determines the best n query expansions (e.g., ?Parthian ?,? ?Parthian ??). These effective expansions automatically dur-ing training by analyzing a collection of 23,615 place names and transliterations pairs.  We evaluated the prototype system using a list of 500 proper names. The results show that 60% of the time there are sufficient relevant data on the Web to carry out effective machine transliteration based on IR and morphological query expansion. Of many results returned by the system, the top 1, two and three results are 0.88, 0.93, and 0.94. By performing query expansion, the system improves the recall rate from 0.48 to 0.60. The results indicate that most names and trans-literation counterparts can often be found on the Web and the proposed method are very effective in  retrieving and extracting transliterations based on a statistical machine transliteration model trained on a bilingual name list. Our demonstration prototype shows alternative transliterations in use on the Web and snippets of such usage, so that the user can easily validate these transliterations.  The prototype supports: ? Searching and extracting transliterations of a given term ? Listing alternative transliterations on the Web ? Listing alternative transliteration in a local dictionary  ? Browsing of snippets containing for each al-terative transliteration ? Saving transliterations in a local dictionary ? Selecting and saving transliteration in snip-pets to a local dictionary The method explored here can be extended as an alterative way to support such MT subtasks as back transliteration (Knight and Graehl 1998) and noun phrase translation (Koehn and Knight 2003). Fi-nally, for more challenging tasks, such as handling sentences, the improvement of translation quality 
probably will also be achieved by combining this IR-based approach and statistical machine transla-tion. For example, a preprocessing unit may re-place the proper names in a sentence with translit-erations (e.g., mixed code text such as Sentence 4) on the fly or by looking up a local dictionary before sending it off to MT for finally translation. 4. The cities of ?????? prospered under ??? and ?? rule. Morphological query expansion represents an innovative way to capture cross-linguistic relations in name transliteration. The method is independent of the bilingual lexicon content making it easy to adopt to other proper names such person, product, or organization names. This approach is useful in a number of machine translation subtasks, including name transliteration, back transliteration, named entity translation, and terminology translation.  References Y. Cao and H. Li. (2002). Base Noun Phrase Transla-tion Using Web Data and the EM Algorithm, In Proc. of COLING 2002, pp.127-133. K. Knight, J. Graehl. (1998). Machine Transliteration. In Journal of Computational Linguistics 24(4), pp.599-612. P. Koehn, K. Knight. (2003). Feature-Rich Statistical Translation of Noun Phrases. In Proc. of ACL 2003, pp.311-318. KL Kwok, P Deng, N Dinstl, HL Sun, W Xu, P Peng, CHINET: a Chinese name finder system for docu-ment triage. Proceedings of 2005 International Con-ference on Intelligence, 2005. T. Lin, J.C. Wu, and J. S. Chang. (2004). Extraction of Name and Transliteration in Monolingual and Paral-lel Corpora. In Proc. of AMTA 2004, pp.177-186. WH Lu, LF Chien, HJ Lee. Anchor text mining for translation of Web queries: A transitive translation approach. ACM Transactions on Information Sys-tems (TOIS), 2004. M. Nagata, T. Saito, and K. Suzuki. (2001). Using the Web as a bilingual dictionary. In Proc. of ACL 2001 DD-MT Workshop, pp.95-102.  
22
Demonstration Script  
 
The system runs under Microsoft Windows as a local application pro-gram. It opens up a form and accepts a name for translation. The user can choose to search on the Web or to look up in the local dictionary for re-sults previously obtained and edited. 
 
 
The system now ex-panded the query and sending them to a search engine to retrieve snip-pets containing translit-erations. 
  
23
 The system extracted transliterations from the snippets and showed the list of most likely an-swers. 
 
 
The user can click each answer and view a list of snippets where the trans-literation appears. 
  
24
The user finds and selects ?????? in the snippets for ????? 
 The transliteration ?????? is added to the list 
 
The user can do a num-ber of things with the transliteration list: 1. View the snip-pets associated with each transliteration 2. Delete a transliteration 3. Save the list along with the snippet 4. Select a string in the snippets and add it to the an-swer list by clicking the right bottom 
   
25
Ngram Char Map_no Ngramno Prob alla ? 2 11 0.1818 alla ? 9 11 0.8182 anti ? 1 11 0.0909 anti ? 10 11 0.9091 bart ? 11 11 1.0000 bata ? 1 11 0.0909 bata ? 10 11 0.9091 belo ? 2 11 0.1818 belo ? 9 11 0.8182 beth ? 1 11 0.0909 beth ? 10 11 0.9091 buch ? 2 11 0.1818  
This slide shows the relationships between the prefixes of names and translations. 
  Ngram Char Map_no Ngramno Prob ague ? 4 11 0.3636 ague ? 6 11 0.5455 ague ? 1 11 0.0909 alen ? 1 11 0.0909 alen ? 2 11 0.1818 alen ? 8 11 0.7273 andi ? 1 11 0.0909 andi ? 7 11 0.6364 andi ? 2 11 0.1818 andi ? 1 11 0.0909  
This slide shows the relationships between the postfixes of names and translations. 
  
26
Cluster Morpheme Phonetic  symbol ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ??  
The performance can improved by clustering transliteration user can click each answer and view a list of snippets where the transliteration appears. 
    
27
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254?262,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Bilingual Linguistic Reordering Model for Statistical  
Machine Translation 
 
 
Han-Bin Chen, Jian-Cheng Wu and Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Guangfu Road, Hsinchu, Taiwan 
{hanbin,d928322,jschang}@cs.nthu.edu.tw 
 
 
 
Abstract 
In this paper, we propose a method for learn-
ing reordering model for BTG-based statisti-
cal machine translation (SMT). The model 
focuses on linguistic features from bilingual 
phrases. Our method involves extracting reor-
dering examples as well as features such as 
part-of-speech and word class from aligned 
parallel sentences. The features are classified 
with special considerations of phrase lengths. 
We then use these features to train the maxi-
mum entropy (ME) reordering model. With 
the model, we performed Chinese-to-English 
translation tasks. Experimental results show 
that our bilingual linguistic model outper-
forms the state-of-the-art phrase-based and 
BTG-based SMT systems by improvements of 
2.41 and 1.31 BLEU points respectively. 
1 Introduction 
Bracketing Transduction Grammar (BTG) is a spe-
cial case of Synchronous Context Free Grammar 
(SCFG), with binary branching rules that are either 
straight or inverted. BTG is widely adopted in 
SMT systems, because of its good trade-off be-
tween efficiency and expressiveness (Wu, 1996). 
In BTG, the ratio of legal alignments and all possi-
ble alignment in a translation pair drops drastically 
especially for long sentences, yet it still covers 
most of the syntactic diversities between two lan-
guages. 
It is common to utilize phrase translation in 
BTG systems. For example in (Xiong et al, 2006), 
source sentences are segmented into phrases. Each 
sequences of consecutive phrases, mapping to cells 
in a CKY matrix, are then translated through a bi-
lingual phrase table and scored as implemented in 
(Koehn et al, 2005; Chiang, 2005). In other words, 
their system shares the same phrase table with 
standard phrase-based SMT systems.  
 
 3 ? ?   3 ? ?  
three   after  2A  
years 1A   three   
ago  2A  years 1A    
(a) (b) 
Figure 1: Two reordering examples, with straight 
rule applied in (a), and inverted rule in (b). 
 
 
On the other hand, there are various proposed 
BTG reordering models to predict correct orienta-
tions between neighboring blocks (bilingual 
phrases). In Figure 1, for example, the role of reor-
dering model is to predict correct orientations of 
neighboring blocks A1 and A2. In flat model (Wu, 
1996; Zens et al, 2004; Kumar and Byrne, 2005), 
reordering probabilities are assigned uniformly 
during decoding, and can be tuned depending on 
different language pairs. It is clear, however, that 
this kind of model would suffer when the dominant 
rule is wrongly applied. 
Predicting orientations in BTG depending on 
context information can be achieved with lexical 
features. For example, Xiong et al (2006) pro-
posed MEBTG, based on maximum entropy (ME) 
classification with words as features. In MEBTG, 
first words of blocks are considered as the fea-
tures, which are then used to train a ME model 
254
for predicting orientations of neighboring blocks. 
Xiong et al (2008b) proposed a linguistically an-
notated BTG (LABTG), in which linguistic fea-
tures such as POS and syntactic labels from 
source-side parse trees are used. Both MEBTG 
and LABTG achieved significant improvements 
over phrase-based Pharaoh (Koehn, 2004) and 
Moses (Koehn et al, 2007) respectively, on Chi-
nese-to-English translation tasks. 
 
 ?    ?   ?? 
Nes  Nf    Nv 
?   ?? 
DE   Na 
the  details  of 
   14      49     50 
 
2A  
the  plan 
   14    18 1A  
 
Figure 2: An inversion reordering example, with 
POS below source words, and class numbers below 
target words. 
 
 
However, current BTG-based reordering meth-
ods have been limited by the features used.  Infor-
mation might not be sufficient or representative, if 
only the first (or tail) words are used as features. 
For example, in Figure 2, consider target first-word 
features extracted from an inverted reordering ex-
ample (Xiong et al, 2006) in MEBTG, in which 
first words on two blocks are both "the". This kind 
of feature set is too common and not representative 
enough to predict the correct orientation. Intui-
tively, one solution is to extend the feature set by 
considering both boundary words, forming a more 
complete boundary description. However, this 
method is still based on lexicalized features, which 
causes data sparseness problem and fails to gener-
alize. In Figure 2, for example, the orientation 
should basically be the same, when the 
source/target words "??/plan" from block A1 is 
replaced by other similar nouns and translations 
(e.g. "plans", "events" or "meetings"). However, 
such features would be treated as unseen by the 
current ME model, since the training data can not 
possibly cover all such similar cases. 
In this paper we present an improved reorder-
ing model based on BTG, with bilingual linguistic 
features from neighboring blocks. To avoid data 
sparseness problem, both source and target words 
are classified; we perform part-of-speech (POS) 
tagging on source language, and word classifica-
tion on target one, as shown in Figure 2. Addition-
ally, features are extracted and classified 
depending on lengths of blocks in order to obtain a 
more informed model. 
The rest of this paper is organized as follows. 
Section 2 reviews the related work. Section 3 de-
scribes the model used in our BTG-based SMT 
systems. Section 4 formally describes our bilingual 
linguistic reordering model. Section 5 and Section 
6 explain the implementation of our systems. We 
show the experimental results in Section 7 and 
make the conclusion in Section 8. 
2 Related Work 
In statistical machine translation, reordering model 
is concerned with predicting correct orders of tar-
get language sentence given a source language one 
and translation pairs. For example, in phrase-based 
SMT systems (Koehn et al, 2003; Koehn, 2004), 
distortion model is used, in which reordering prob-
abilities depend on relative positions of target side 
phrases between adjacent blocks. However, distor-
tion model can not model long-distance reordering, 
due to the lack of context information, thus is diffi-
cult to predict correct orders under different cir-
cumstances. Therefore, while phrase-based SMT 
moves from words to phrases as the basic unit of 
translation, implying effective local reordering 
within phrases, it suffers when determining phrase 
reordering, especially when phrases are longer than 
three words (Koehn et al, 2003). 
There have been much effort made to improve 
reordering model in SMT. For example, research-
ers have been studying CKY parsing over the last 
decade, which considers translations and orienta-
tions of two neighboring block according to 
grammar rules or context information. In hierar-
chical phrase-based systems (Chiang, 2005), for 
example, SCFG rules are automatically learned 
from aligned bilingual corpus, and are applied in 
CKY style decoding. 
As an another application of CKY parsing tech-
nique is BTG-based SMT. Xiong et al (2006) and 
Xiong et al (2008a) developed MEBTG systems, 
in which first or tail words from reordering exam-
ples are used as features to train ME-based reorder-
ing models. 
Similarly, Zhang et al (2007) proposed a model 
similar to BTG, which uses first/tail words of 
phrases, and syntactic labels (e.g. NP and VP) 
255
from source parse trees as features. In their work, 
however, inverted rules are allowed to apply only 
when source phrases are syntactic; for non-
syntactic ones, blocks are combined straight with a 
constant score.  
More recently, Xiong et al (2008b) proposed 
LABTG, which incorporates linguistic knowledge 
by adding features such as syntactic labels and 
POS from source trees to improve their MEBTG. 
Different from Zhang's work, their model do not 
restrict non-syntactic phrases, and applies inverted 
rules on any pair of neighboring blocks. 
Although POS information is used in LABTG 
and Zhang's work, their models are syntax-oriented, 
since they focus on syntactic labels. Boundary POS 
is considered in LABTG only when source phrases 
are not syntactic phrases. 
In contrast to the previous works, we present a 
reordering model for BTG that uses bilingual in-
formation including class-level features of POS 
and word classes. Moreover, our model is dedi-
cated to boundary features and considers different 
combinations of phrase lengths, rather than only 
first/tail words. In addition, current state-of-the-art 
Chinese parsers, including the one used in LABTG 
(Xiong et al, 2005), lag beyond in inaccuracy, 
compared with English parsers (Klein and Man-
ning, 2003; Petrov and Klein 2007). In our work, 
we only use more reliable information such as 
Chinese word segmentation and POS tagging (Ma 
and Chen, 2003). 
3 The Model 
Following Wu (1996) and Xiong et al (2006), we 
implement BTG-based SMT as our system, in 
which three rules are applied during decoding: 
 ? ?21 AAA ?   (1) 
21 AAA ?    (2) 
yxA /?    (3) 
 
where A1 and A2 are blocks in source order. Straight 
rule (1) and inverted rule (2) are reordering rules. 
They are applied for predicting target-side order 
when combining two blocks, and form the reorder-
ing model with the distributions 
 
reoorderAA ?)(P ,,reo 21  
 
where order ?{straight, inverted}. 
In MEBTG, a ME reordering model is trained 
using features extracted from reordering examples 
of aligned parallel corpus. First words on neighbor-
ing blocks are used as features. In reordering ex-
ample (a), for example, the feature set is 
 
{"S1L=three", "S2L=ago", "T1L=3", "T2L=?"} 
 
where "S1" and "T1" denote source and target 
phrases from the block A1. 
Rule (3) is lexical translation rule, which trans-
lates source phrase x into target phrase y. We use 
the same feature functions as typical phrase-based 
SMT systems (Koehn et al, 2005): 
 
654
321
ee)|(
)|()|()|()|(Ptrans
???
???
y
lw
lw
xyp
yxpxypyxpyx
???
???  
 
where 43 )|()|( ?? xypyxp lwlw ? , 5e? and 6e ?y  
are lexical translation probabilities in both direc-
tions, phrase penalty and word penalty. 
During decoding, the blocks are produced by 
applying either one of two reordering rules on two 
smaller blocks, or applying lexical rule (3) on 
some source phrase. Therefore, the score of a block 
A is defined as 
 
reolm orderAAAA
AAA
?? ),,(P),(P
)P()P()P(
reo21lm
21
21???
??  
 
or 
 
)|(P)(P)P( translm yxAA lm ?? ?  
 
where lmA ?)(Plm  and lmAA ?),(P 21lm?  are respec-
tively the usual and incremental score of language 
model. 
To tune all lambda weights above, we perform 
minimum error rate training (Och, 2003) on the 
development set described in Section 7. 
Let B be the set of all blocks with source side 
sentence C. Then the best translation of C is the 
target side of the block A , where 
 
256
)P(argmaxA A
BA?
?  
4 Bilingual Linguistic Model 
In this section, we formally describe the problem 
we want to address and the proposed method. 
4.1 Problem Statement 
We focus on extracting features representative of 
the two neighboring blocks being considered for 
reordering by the decoder, as described in Section 
3. We define S(A) and T(A) as the information on 
source and target side of a block A. For two 
neighboring blocks A1 and A2, the set of features 
extracted from information of them is denoted as 
feature set function F(S(A1), S(A2), T(A1), S(A2)). In 
Figure 1 (b), for example, S(A1) and T(A1) are sim-
ply the both sides sentences "3 ? " and "three 
years", and F(S(A1), S(A2), T(A1), S(A2)) is 
 
{"S1L=three", "S2L=after", "T1L=3", "T2L=?"} 
 
where "S1L" denotes the first source word on the 
block A1, and "T2L" denotes the first target word 
on the block A2. 
Given the adjacent blocks A1 and A2, our goal 
includes (1) adding more linguistic and representa-
tive information to A1 and A2 and (2) finding a fea-
ture set function F' based on added linguistic 
information in order to train a more linguistically 
motivated and effective model. 
4.2 Word Classification 
As described in Section 1, designing a more com-
plete feature set causes data sparseness problem, if 
we use lexical features. One natural solution is us-
ing POS and word class features.  
In our model, we perform Chinese POS tagging 
on source language. In Xiong et al (2008b) and 
Zhang et al (2007), Chinese parsers with Penn 
Chinese Treebank (Xue et al, 2005) style are used 
to derive source parse trees, from which source-
side features such as POS are extracted. However, 
due to the relatively low accuracy of current Chi-
nese parsers compared with English ones, we in-
stead use CKIP Chinese word segmentation system 
(Ma and Chen, 2003) in order to derive Chinese 
tags with high accuracy. Moreover, compared with 
the Treebank Chinese tagset, the CKIP tagset pro-
vides more fine-grained tags, including many tags 
with semantic information (e.g., Nc for place 
nouns, Nd for time nouns), and verb transitivity 
and subcategorization (e.g., VA for intransitive 
verbs, VC for transitive verbs, VK for verbs that 
take a clause as object). 
On the other hand, using the POS features in 
combination with the lexical features in target lan-
guage will cause another sparseness problem in the 
phrase table, since one source phrase would map to 
multiple target ones with different POS sequences. 
As an alternative, we use mkcls toolkit (Och, 
1999), which uses maximum-likelihood principle 
to perform classification on target side. After clas-
sification, the toolkit produces a many-to-one 
mapping between English tokens and class num-
bers. Therefore, there is no ambiguity of word 
class in target phrases and word class features can 
be used independently to avoid data sparseness 
problem and the phrase table remains unchanged. 
As mentioned in Section 1, features based on 
words are not representative enough in some cases, 
and tend to cause sparseness problem. By classify-
ing words we are able to linguistically generalize 
the features, and hence predict the rules more 
robustly. In Figure 2, for example, the target words 
are converted to corresponding classes, and form 
the more complete boundary feature set 
 
{"T1L=14", "T1R=18", "T2L=14", "T2R=50"}  (4) 
 
In the feature set (4), #14 is the class containing 
"the", #18 is the class containing "plans", and #50 
is the class containing "of." Note that we add last-
word features "T1R=18" and "T2R=50". As men-
tioned in Section 1, the word "plan" from block A1 
is replaceable with similar nouns. This extends to 
other nominal word classes to realize the general 
rule of inverting "the ... NOUN" and "the ... of". 
It is hard to achieve this kind of generality using 
only lexicalized feature. With word classification, 
we gather feature sets with similar concepts from 
the training data. Table 1 shows the word classes 
can be used effectively to cope with data sparse-
ness. For example, the feature set (4) occurs 309 
times in our training data, and only 2 of them are 
straight, with the remaining 307 inverted examples, 
implying that similar features based on word 
classes lead to similar orientation. Additional ex-
amples of similar feature sets with different word 
classes are shown in Table 1. 
257
class X T1R = X    straight/inverted
9 graph, government 2/488 
18 plans, events 2/307 
20 bikes, motors 0/694 
48 day, month, year 4/510 
Table 1: List of feature sets in the form of 
{"T1L=14", "T1R=X", "T2L=14", "T2R=50"}. 
 
4.3 Feature with Length Consideration 
Boundary features using both the first and last 
words provide more detailed descriptions of 
neighboring blocks. However, we should take the 
special case blocks with length 1 into consideration. 
For example, consider two features sets from 
straight and inverted reordering examples (a) and 
(b) in Figure 3. There are two identical source fea-
tures in both feature set, since first words on block 
A1 and last words on block A2 are the same: 
 
{"S1L=P","S2R=Na"}?F(S(A1),S(A2),T(A1), S(A2)) 
 
Therefore, without distinguishing the special case, 
the features would represent quite different cases 
with the same feature, possibly leading to failure to 
predict orientations of two blocks.  
We propose a method to alleviate the problem of 
features with considerations of lengths of two ad-
jacent phrases by classifying both the both source 
and target phrase pairs into one of four classes: M, 
L, R and B, corresponding to different combina-
tions of phrase lengths. 
Suppose we are given two neighboring blocks 
A1 and A2, with source phrases P1 and P2 respec-
tively. Then the feature set from source side is 
classified into one of the classes as follows. We 
give examples of feature set for each class accord-
ing to Figure 4. 
 
 ?? P 
?? ?? 
Neqa  Na   
? ?? 
P    Nc 
?? ??
VC    Na
 hold meeting  2A  for 1A    
these 
reasons  2A   
in 
jordan 1A   
(a) (b) 
Figure 3: Two reordering examples with ambigu-
ous features on source side. 
 
A1 A2  A1  A2 
? 
Nh 
?? 
VE 
 ?? 
P 
 ??  ?? 
    Neqa    Na 
I think  for  these  reasons
              (a)                                     (b) 
M class                             L class 
 
A1 A2  A1  A2 
??  ? 
Na  Caa 
?? 
Na 
 ?  ?? 
P     Nc 
 ??  ??
      VC      Na
technology and equipment in  Jordan  hold  meeting
                (c)                                        (d) 
              R class                                 B class 
Figure 4:   Examples of different length combina-
tions, mapping to four classes. 
 
 
1. M class. The lengths of P1 and P2 are both 1. In 
Figure 4 (a), for example, the feature set is 
 
{"M1=Nh", "M2=VE"} 
 
2. L class. The length of P1 is 1, and the length of 
P2 is greater than 1. In Figure 4 (b), for exam-
ple, the feature set is 
 
{"L1=P", "L2=Neqa", "L3=Na"} 
 
3. R class. The length of P1 is greater than 1, and 
the length of P2 is 1. In Figure 4 (c), for exam-
ple, the feature set is 
 
{"R1=Na", "R2=Caa", "R3=Na"} 
 
4. B class. The lengths of P1 and P2 are both 
greater than 1. In Figure 4 (d), for example, the 
feature set is 
 
{"B1=P", "B2=Nc", "B3=VC", "B4=Na"} 
 
We use the same scheme to classify the two tar-
get phrases. Since both source and target words are 
classified as described in Section 4.2, the feature 
sets are more representative and tend to lead to 
consistent prediction of orientation. Additionally, 
the length-based features are easy to fit into mem-
ory, in contrast to lexical features in MEBTG. 
To summarize, we extract features based on 
word lengths, target-language word classes, and 
fine-grained, semantic oriented parts of speech. To 
illustrate, we use the neighboring blocks from Fig-
258
ure 2 to show an example of complete bilingual 
linguistic feature set: 
 
{"S.B1=Nes", "S.B2=Nv", "S.B3=DE", 
"S.B4=Na", "T.B1=14", "T.B2=18", "T.B3=14", 
"T.B4=50"} 
 
where "S." and "T." denote source and target sides. 
In the next section, we describe the process of 
preparing the feature data and training an ME 
model. In Section 7, we perform evaluations of this 
ME-based reordering model against standard 
phrase-based SMT and previous work based on 
ME and BTG. 
5 Training 
In order to train the translation and reordering 
model, we first set up Moses SMT system (Koehn 
et al, 2007). We obtain aligned parallel sentences 
and the phrase table after the training of Moses, 
which includes running GIZA++ (Och and Ney, 
2003), grow-diagonal-final symmetrization and 
phrase extraction (Koehn et al, 2005). Our system 
shares the same translation model with Moses, 
since we directly use the phrase table to apply 
translation rules (3). 
On the other side, we use the aligned parallel 
sentences to train our reordering model, which in-
cludes classifying words, extracting bilingual 
phrase samples with orientation information, and 
training an ME model for predicting orientation. 
To perform word classification, the source sen-
tences are tagged and segmented before the Moses 
training. As for target side, we ran the Moses 
scripts to classify target language words using the 
mkcls toolkit before running GIZA++. Therefore, 
we directly use its classification result, which gen-
erate 50 classes with 2 optimization runs on the 
target sentences. 
To extract the reordering examples, we choose 
sentence pairs with top 50% alignment scores pro-
vided by GIZA++, in order to fit into memory. 
Then the extraction is performed on these aligned 
sentence pairs, together with POS tags and word 
classes, using basically the algorithm presented in 
Xiong et al (2006). However, we enumerate all 
reordering examples, rather than only extract the 
smallest straight and largest inverted examples. 
Finally, we use the toolkit by Zhang (2004) to train 
the ME model with extracted reordering examples. 
6 Decoding 
We develop a bottom-up CKY style decoder in our 
system, similar to Chiang (2005). For a Chinese 
sentence C, the decoder finds its best translation on 
the block with entire C on source side. The decoder 
first applies translation rules (3) on cells in a CKY 
matrix. Each cell denotes a sequence of source 
phrases, and contains all of the blocks with possi-
ble translations. The longest length of source 
phrase to be applied translations rules is restricted 
to 7 words, in accordance with the default settings 
of Moses training scripts. 
To reduce the search space, we apply threshold 
pruning and histogram pruning, in which the block 
scoring worse than 10-2 times the best block in the 
same cell or scoring worse than top 40 highest 
scores would be pruned. These pruning techniques 
are common in SMT systems. We also apply re-
combination, which distinguish blocks in a cell 
only by 3 leftmost and rightmost target words, as 
suggested in (Xiong et al, 2006). 
7 Experiments and Results 
We perform Chinese-to-English translation task 
on NIST MT-06 test set, and use Moses and 
MEBTG as our competitors.  
The bilingual training data containing 2.2M sen-
tences pairs from Hong Kong Parallel Text 
(LDC2004T08) and Xinhua News Agency 
(LDC2007T09), with length shorter than 60, is 
used to train the translation and reordering model. 
The source sentences are tagged and segmented 
with CKIP Chinese word segmentation system (Ma 
and Chen, 2003). 
About 35M reordering examples are extracted 
from top 1.1M sentence pairs with higher align-
ment scores. We generate 171K features for lexi-
calized model used in MEBTG system, and 1.41K 
features for our proposed reordering model. 
For our language model, we use Xinhua news 
from English Gigaword Third Edition 
(LDC2007T07) to build a trigram model with 
SRILM toolkit (Stolcke, 2002). 
Our development set for running minimum error 
rate training is NIST MT-08 test set, with sentence 
lengths no more than 20. We report the experimen-
tal results on NIST MT-06 test set. Our evaluation 
metric is BLEU (Papineni et al, 2002) with case-
insensitive matching from unigram to four-gram. 
259
System BLEU-4 
Moses(distortion) 22.55 
Moses(lexicalized) 23.42 
MEBTG 23.65 
WC+LC 24.96 
Table 2: Performances of various systems. 
 
 
The overall result of our experiment is shown in 
Table 2. The lexicalized MEBTG system proposed 
by Xiong et al (2006) uses first words on adjacent 
blocks as lexical features, and outperforms phrase-
based Moses with default distortion model and en-
hanced lexicalized model, by 1.1 and 0.23 BLEU 
points respectively. This suggests lexicalized 
Moses and MEBTG with context information out-
performs distance-based distortion model. Besides, 
MEBTG with structure constraints has better 
global reordering estimation than unstructured 
Moses, while incorporating their local reordering 
ability by using phrase tables.  
The proposed reordering model trained with 
word classification (WC) and length consideration 
(LC) described in Section 4 outperforms MEBTG 
by 1.31 point. This suggests our proposed model 
not only reduces the model size by using 1% fewer 
features than MEBTG, but also improves the trans-
lation quality. 
We also evaluate the impacts of WC and LC 
separately and show the results in Table 3-5. Table 
3 shows the result of MEBTG with word classified 
features. While classified MEBTG only improves 
0.14 points over original lexicalized one, it drasti-
cally reduces the feature size. This implies WC 
alleviates data sparseness by generalizing the ob-
served features. 
Table 4 compares different length considerations, 
including boundary model demonstrated in Section 
4.2, and the proposed LC in Section 4.3. Although 
boundary model describes features better than us-
ing only first words, which we will show later, it 
suffers from data sparseness with twice feature size 
of MEBTG. The LC model has the largest feature 
size but performs best among three systems, sug-
gesting the effectiveness of our LC. 
In Table 5 we show the impacts of WC and LC 
together. Note that all the systems with WC sig-
nificantly reduce the size of features compared to 
lexicalized ones. 
 
System Feature size BLEU-4
MEBTG 171K 23.65 
WC+MEBTG 0.24K 23.79 
Table 3: Performances of lexicalized and word 
classified MEBTG. 
 
 
System Feature size BLEU-4
MEBTG 171K 23.65 
Boundary 349K 23.42 
LC 780K 23.86 
Table 4: Performances of BTG systems with dif-
ferent representativeness. 
 
 
System Feature size BLEU-4
MEBTG 171K 23.65 
WC+MEBTG 0.24K 23.79 
WC+Bounary 0.48K 24.29 
WC+LC 1.41K 24.96 
Table 5: Different representativeness with word 
classification. 
 
 
While boundary model is worse than first-word 
MEBTG in Table 4, it outperforms the latter when 
both are performed WC. We obtain the best result 
that outperforms the baseline MEBTG by more 
than 1 point when we apply WC and LC together.  
Our experimental results show that we are able 
to ameliorate the sparseness problem by classifying 
words, and produce more representative features 
by considering phrase length. Moreover, they are 
both important, in that we are unable to outperform 
our competitors by a large margin unless we com-
bine both WC and LC. In conclusion, while de-
signing more representative features of reordering 
model in SMT, we have to find solutions to gener-
alize them. 
8 Conclusion and Future Works 
We have proposed a bilingual linguistic reordering 
model to improve current BTG-based SMT sys-
tems, based on two drawbacks of previously pro-
posed reordering model, which are sparseness and 
representative problem. 
First, to solve the sparseness problem in previ-
ously proposed lexicalized model, we perform 
word classification on both sides. 
260
Secondly, we present a more representative fea-
ture extraction method. This involves considering 
length combinations of adjacent phrases. 
The experimental results of Chinese-to-English 
task show that our model outperforms baseline 
phrase-based and BTG systems. 
We will investigate more linguistic ways to clas-
sify words in future work, especially on target lan-
guage. For example, using word hierarchical 
structures in WordNet (Fellbaum, 1998) system 
provides more linguistic and semantic information 
than statistically-motivated classification tools. 
Acknowledgements 
This work was supported by National Science 
Council of Taiwan grant NSC 95-2221-E-007-182-
MY3. 
References   
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL 2005, pp. 263-270. 
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, 
Massachusetts. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT/NAACL 2003. 
Philipp Koehn. 2004. Pharaoh: a Beam Search  Decoder 
for Phrased-Based Statistical Machine Translation 
Models. In Proceedings of AMTA 2004. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In International Workshop on Spoken Language 
Translation. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan,Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source 
toolkit for statistical machine translation. In Proceed-
ings of ACL 2007, Demonstration Session. 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. In Proceedings of ACL 2003. 
Shankar Kumar and William Byrne. 2005. Local phrase 
reordering models for statistical machine translation. 
In Proceedings of HLT-EMNLP 2005. 
Wei-Yun Ma and Keh-Jiann Chen. 2003. Introduction 
to CKIP Chinese Word Segmentation System for the 
First International Chinese Word Segmentation 
Bakeoff. In Proceedings of ACL, Second SIGHAN 
Workshop on Chinese Language Processing, pp168-
171.  
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL ?99: Ninth 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 71?76, 
Bergen, Norway, June. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29:19-51. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
ACL 2003, pages 160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the 40th Annual Meeting of the ACL, pages 311?318. 
Slav Petrov and Dan Klein. 2007. Improved Inference-
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL 2007. 
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing, 
volume 2, pages 901?904. 
Dekai Wu. 1996. A Polynomial-Time Algorithm for 
Statistical Machine Translation. In Proceedings of 
ACL 1996. 
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and 
Yueliang Qian. 2005. Parsing the Penn Chinese tree-
bank with semantic knowledge. In Proceedings of 
IJCNLP 2005, pages 70-81. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
ACL-COLING 2006. 
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu, 
and Shouxun Liu. 2008a. Refinements in BTG-based 
statistical machine translation. In Proceedings of 
IJCNLP 2008, pp. 505-512. 
Deyi Xiong, Min Zhang, Ai Ti Aw, and Haizhou Li. 
2008b. Linguistically Annotated BTG for Statistical 
Machine Translation. In Proceedings of COLING 
2008. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
261
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.  
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. 
Reordering Constraints for Phrase-Based Statistical 
Machine Translation. In Proceedings of CoLing 2004, 
Geneva, Switzerland, pp. 205-211.  
Le Zhang. 2004. Maximum Entropy Modeling Toolkit 
for Python and C++. Available at http://homepa 
ges.inf.ed.ac.uk/s0450736/maxent_toolkit.html. 
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 
2007. Phrase Reordering Model Integrating Syntactic 
Knowledge for SMT. In Proceedings of EMNLP-
CoNLL 2007. 
 
262
TotalRecall: A Bilingual Concordance for Computer Assisted Translation and 
Language Learning
 
Jian-Cheng Wu , Kevin C. Yeh 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 
300, Taiwan, ROC 
g904307@cs.nthu.edu.tw 
Thomas C. Chuang 
Department of Computer Science 
Van Nung Institute of Technology 
No. 1 Van-Nung Road 
Chung-Li Tao-Yuan, Taiwan, ROC 
tomchuang@cc.vit.edu.tw 
Wen-Chi Shei , Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 300, 
Taiwan, ROC 
jschang@cs.nthu.edu.tw 
Abstract 
This paper describes a Web-based Eng-
lish-Chinese concordance system, Total-
Recall, developed to promote translation 
reuse and encourage authentic and idio-
matic use in second language writing. We 
exploited and structured existing high-
quality translations from the bilingual Si-
norama Magazine to build the concor-
dance of authentic text and translation. 
Novel approaches were taken to provide 
high-precision bilingual alignment on the 
sentence, phrase and word levels. A 
browser-based user interface (UI) is also 
developed for ease of access over the 
Internet. Users can search for word, 
phrase or expression in English or Chi-
nese. The Web-based user interface facili-
tates the recording of the user actions to 
provide data for further research. 
1 Introduction 
A concordance tool is particularly useful for study-
ing a piece of literature when thinking in terms of a 
particular word, phrase or theme. It will show ex-
actly how often and where a word occurs, so can 
be helpful in building up some idea of how differ-
ent themes recur within an article or a collection of 
articles. Concordances have been indispensable for 
lexicographers and increasingly considered useful 
for language instructor and learners. A bilingual 
concordance tool is like a monolingual concor-
dance, except that each sentence is followed by its 
translation counterpart in a second language. It 
could be extremely useful for bilingual lexicogra-
phers, human translators and second language 
learners. Pierre Isabelle, in 1993, pointed out: ?ex-
isting translations contain more solutions to more 
translation problems than any other existing re-
source.? It is particularly useful and convenient 
when the resource of existing translations is made 
available on the Internet. A web based bilingual 
system has proved to be very useful and popular. 
For example, the English-French concordance sys-
tem, TransSearch (Macklovitch et al 2000). Pro-
vides a familiar interface for the users who only 
need to type in the expression in question, a list of 
citations will come up and it is easy to scroll down 
until one finds one that is useful. TotalRecall 
comes with an additional feature making the solu-
tion more easily recognized. The user not only get 
all the citations related to the expression in ques-
tion, but also gets to see the translation counterpart 
highlighted. 
 
TotalRecall extends the translation memory 
technology and provide an interactive tool intended 
for translators and non-native speakers trying to 
find ideas to properly express themselves. Total-
Recall empower the user by allow her to take the 
initiative in submitting queries for searching au-
thentic, contemporary use of English. These que-
ries may be single words, phrases, expressions or 
even full sentence, the system will search a sub-
stantial and relevant corpus and return bilingual 
citations that are helpful to human translators and 
second language learners. 
2 Aligning the corpus 
Central to TotalRecall is a bilingual corpus and a 
set of programs that provide the bilingual analyses 
to yield a translation memory database out of the 
bilingual corpus. Currently, we are working with a 
collection of Chinese-English articles from the Si-
norama magazine. A large bilingual collection of 
Studio Classroom English lessons will be provided 
in the near future.  That would allow us to offer 
bilingual texts in both translation directions and 
with different levels of difficulty. Currently, the 
articles from Sinaroma seems to be quite usefully 
by its own, covering a wide range of topics, re-
flecting the personalities, places, and events in 
Taiwan for the past three decade.  
 
The concordance database is composed of bi-
lingual sentence pairs, which are mutual transla-
tion. In addition, there are also tables to record 
additional information, including the source of 
each sentence pairs, metadata, and the information 
on phrase and word level alignment. With that ad-
ditional information, TotalRecall provides various 
functions, including 1. viewing of the full text of 
the source with a simple click. 2. highlighted 
translation counterpart of the query word or phrase. 
3. ranking that is pedagogically useful for transla-
tion and language learning. 
 
We are currently running an experimental pro-
totype with Sinorama articles, dated mainly from 
1995 to 2002. There are approximately 50,000 bi-
lingual sentences and over 2 million words in total. 
We also plan to continuously updating the database 
with newer information from Sinorama magazine 
so that the concordance is kept current and relevant 
to the . To make these up to date and relevant.  
 
The bilingual texts that go into TotalRecall 
must be rearranged and structured. We describe the 
main steps below: 
2.1 Sentence Alignment 
After parsing each article from files and put them 
into the database, we need to segment articles into 
sentences and align them into pairs of mutual 
translation. While the length-based approach 
(Church and Gale 1991) to sentence alignment 
produces surprisingly good results for the close 
language pair of French and English at success 
rates well over 96%, it does not fair as well for 
distant language pairs such as English and Chinese. 
Work on sentence alignment of English and Chi-
nese texts (Wu 1994), indicates that the lengths of 
English and Chinese texts are not as highly corre-
lated as in French-English task, leading to lower 
success rate (85-94%) for length-based aligners.  
 
Table 1  The result of Chinese collocation candi-
dates extracted. The shaded collocation pairs are 
selected based on competition of whole phrase log 
likelihood ratio and word-based translation prob-
ability. Un-shaded items 7 and 8 are not selected 
because of conflict with previously chosen bilin-
gual collocations, items 2 and 3. 
 
Simard, Foster, and Isabelle (1992) pointed out 
cognates in two close languages such as English 
and French can be used to measure the likelihood 
of mutual translation. However, for the English-
Chinese pair, there  are no  orthographic,  phonetic 
or semantic cognates readily recognizable by the 
computer. Therefore, the cognate-based approach 
is not applicable to the Chinese-English tasks.  
 
At first, we used the length-based method for 
sentence alignment. The average precision of 
aligned sentence pairs is about 95%. We are now 
switching to a new alignment method based on 
punctuation statistics. Although the average ratio 
of the punctuation counts in a text is low (less than 
15%), punctuations provide valid additional evi-
dence, helping to achieve high degree of alignment 
precision. It turns out that punctuations are telling 
evidences for sentence alignment, if we do more 
than hard matching of punctuations and take into 
consideration of intrinsic sequencing of punctua-
tion in ordered comparison. Experiment results 
show that the punctuation-based approach outper-
forms the length-based approach with precision 
rates approaching 98%.  
2.2 Phrase and Word Alignment 
After sentences and their translation counterparts 
are identified, we proceeded to carry out finer-
grained alignment on the phrase and word levels. 
We employ  part of speech patterns  and  statistical  
Figure 1. The results of searching for ?hard+? with default ranking. 
 
analyses to extract bilingual phrases/collocations 
from a parallel corpus. The preferred syntactic pat-
terns are obtained from idioms and collocations in 
the machine readable English-Chinese version of 
Longman Dictionary of Contemporary of English. 
 
Phrases matching the patterns are extract from 
aligned sentences in a parallel corpus. Those 
phrases are subsequently matched up via cross lin-
guistic statistical association. Statistical association 
between the whole phrase as well as words in 
phrases are used jointly to link a collocation and its 
counterpart collocation in the other language. See 
Table 1 for an example of extracting bilingual col-
locations. The word and phrase level information is 
kept in relational database for use in processing 
queries, hightlighting translation counterparts, and 
ranking citations. Sections 3 and 4 will give more 
details about that. 
3 The Queries 
The goal of the TotalRecall System is to allow a 
user to look for instances of specific words or ex-
pressions. For this purpose, the system opens up 
two text boxes for the user to enter queries in any 
one of the languages involved or both. We offer 
some special expressions for users to specify the 
following queries: 
 
? Exact single word query - W. For instance, 
enter ?work? to find citations that contain 
?work,? but not ?worked?, ?working?, 
?works.? 
? Exact single lemma query ? W+. For in-
stance, enter ?work+? to find citations that 
contain ?work?, ?worked?, ?working?, 
?works.? 
? Exact string query. For instance, enter ?in 
the work? to find citations that contain the 
three words, ?in,? ?the,? ?work? in a row, 
but not citations that contain the three words 
in any other way. 
? Conjunctive and disjunctive query. For in-
stance, enter ?give+ advice+? to find cita-
tions that contain ?give? and ?advice.? It is 
also possible to specify the distance between 
?give? and ?advice,? so they are from a VO 
construction. Similarly, enter ?hard | diffi-
cult | tough? to find citations that involve 
difficulty to do, understand or bear some-
thing, using any of the three words. 
Once a query is submitted, TotalRecall dis-
plays the results on Web pages. Each result ap-
pears as a pair of segments, usually one sentence 
each in English and Chinese, in side-by-side for-
mat. The words matching the query are high-
lighted, and a ?context? hypertext link is included 
in each row. If this link is selected, a new page ap-
pears displaying the original document of the pair. 
If the user so wishes, she can scroll through the 
following or preceding pages of context in the 
original document. 
4 Ranking 
It is well known that the typical user usual has no 
patient to go beyond the first or second pages re-
turned by a search engine. Therefore, ranking and 
putting the most useful information in the first one 
or two is of paramount importance for search en-
gines. This is also true for a concordance.  
 
Experiments with a focus group indicate that 
the following ranking strategies are important: 
 
? Citations with a translation counterpart 
should be ranked first. 
? Citations with a frequent translation coun-
terpart appear before ones with less frequent 
translation 
? Citations with same translation counterpart 
should be shown in clusters by default. The 
cluster can be called out entirely on demand. 
? Ranking by nonlinguistic features should 
also be provided, including date, sentence 
length, query position in citations, etc. 
 
With various ranking options available, the users 
can choose one that is most convenient and 
productive for the work at hand. 
5 Conclusion 
In this paper, we describe a bilingual concordance 
designed as a computer assisted translation and 
language learning tool. Currently, TotalRecall 
uses Sinorama Magazine corpus as the translation 
memory and will be continuously updated as new 
issues of the magazine becomes available. We 
have already put a beta version on line and ex-
perimented with a focus group of second language 
learners. Novel features of TotalRecall include 
highlighting of query and corresponding transla-
tions, clustering and ranking of search results ac-
cording translation and frequency. 
 
TotalRecall enable the non-native speaker who 
is looking for a way to express an idea in English 
or Chinese. We are also adding on the basic func-
tions to include a log of user activities, which will 
record the users? query behavior and their back-
ground. We could then analyze the data and find 
useful information for future research. 
Acknowledgement  
We acknowledge the support for this study through 
grants from National Science Council and Ministry 
of Education, Taiwan (NSC 90-2411-H-007-033-
MC and MOE EX-91-E-FA06-4-4) and a special 
grant for preparing the Sinorama Corpus for distri-
bution by the Association for Computational Lin-
guistics and Chinese Language Processing. 
References 
Chuang, T.C. and J.S. Chang (2002), Adaptive Sentence 
Alignment Based on Length and Lexical Information, ACL 
2002, Companion Vol. P. 91-2. 
Gale, W. & K. W. Church, "A Program for Aligning Sen-
tences in Bilingual Corpora" Proceedings of the 29th An-
nual Meeting of the Association for Computational 
Linguistics, Berkeley, CA, 1991. 
Macklovitch, E., Simard, M., Langlais, P.: TransSearch: A 
Free Translation Memory on the World Wide Web. Proc. 
LREC 2000 III, 1201--1208 (2000). 
Nie, J.-Y., Simard, M., Isabelle, P. and Durand, R.(1999) 
Cross-Language Information Retrieval based on Parallel 
Texts and Automatic Mining of Parallel Texts in the Web. 
Proceedings of SIGIR ?99, Berkeley, CA. 
Simard, M., G. Foster & P. Isabelle (1992), Using cognates to 
align sentences in bilingual corpora. In Proceedings of 
TMI92, Montreal, Canada, pp. 67-81. 
Wu, Dekai (1994), Aligning a parallel English-Chinese corpus 
statistically with lexical criteria. In The Proceedings of the 
32nd Annual Meeting of the Association for Computational 
Linguistics, New Mexico, USA, pp. 80-87. 
Wu, J.C. and J.S. Chang (2003), Bilingual Collocation Extrac-
tion Based on Syntactic and Statistical Analyses, ms. 
Yeh, K.C., T.C. Chuang, J.S. Chang (2003), Using Punctua-
tions for Bilingual Sentence Alignment- Preparing Parallel 
Corpus for Distribution by the ACLCLP, ms. 
Subsentential Translation Memory for  
Computer Assisted Writing and Translation 
Jian-Cheng Wu 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 300, 
Taiwan, ROC 
D928322@oz.nthu.edu.tw 
Thomas C. Chuang 
Department of Computer Science 
Van Nung Institute of Technology
No. 1 Van-Nung Road 
Chung-Li Tao-Yuan, Taiwan, ROC
tomchuang@cc.vit.edu.tw 
Wen-Chi Shei , Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 300, 
Taiwan, ROC 
jschang@cs.nthu.edu.tw 
 
Abstract 
This paper describes a database of translation 
memory, TotalRecall, developed to encourage 
authentic and idiomatic use in second 
language writing. TotalRecall is a bilingual 
concordancer that support search query in 
English or Chinese for relevant sentences and 
translations. Although initially intended for 
learners of English as Foreign Language (EFL) 
in Taiwan, it is a gold mine of texts in English 
or Mandarin Chinese. TotalRecall is 
particularly useful for those who write in or 
translate into a foreign language. We exploited 
and structured existing high-quality 
translations from bilingual corpora from a 
Taiwan-based Sinorama Magazine and 
Official Records of Hong Kong Legislative 
Council to build a bilingual concordance. 
Novel approaches were taken to provide high-
precision bilingual alignment on the 
subsentential and lexical levels. A browser-
based user interface was developed for ease of 
access over the Internet. Users can search for 
word, phrase or expression in English or 
Mandarin. The Web-based user interface 
facilitates the recording of the user actions to 
provide data for further research. 
1 Introduction 
Translation memory has been found to be more 
effective alternative to machine translation for 
translators, especially when working with batches 
of similar texts. That is particularly true with so-
called delta translation of the next versions for 
publications that need continuous revision such as 
an encyclopaedia or user?s manual. On another 
area of language study, researchers on English 
Language Teaching (ELT) have increasingly 
looked to concordancer of very large corpora as a 
new re-source for translation and language learning. 
Concordancers have been indispensable for 
lexicographers. But now language teachers and 
students also embrace the concordancer to foster 
data-driven, student-centered learning.  
A bilingual concordance, in a way, meets the 
needs of both communities, the computer assisted 
translation (CAT) and computer assisted language 
learning (CALL). A bilingual concordancer is like 
a monolingual concordance, except that each 
sentence is followed by its translation counterpart 
in a second language. ?Existing translations 
contain more solutions to more translation 
problems than any other existing resource.? 
(Isabelle 1993). The same can be argued for 
language learning; existing texts offer more 
answers for the learner than any teacher or 
reference work do.   
However, it is important to provide easy access 
for translators and learning writers alike to find the 
relevant and informative citations quickly. For in-
stance, the English-French concordance system, 
TransSearch provides a familiar interface for the 
users (Macklovitch et al 2000). The user type in 
the expression in question, a list of citations will 
come up and it is easy to scroll down until one 
finds translation that is useful much like using a 
search engine. TransSearch exploits sentence 
alignment techniques (Brown et al1990; Gale and 
Church 1990) to facilitate bilingual search at the 
granularity level of sentences.     
In this paper, we describe a bilingual 
concordancer which facilitate search and 
visualization with fine granularity. TotalRecall 
exploits subsentential and word alignment to 
provide a new kind of bilingual concordancer. 
Through the interactive interface and clustering of 
short subsentential bi-lingual citations, it helps 
translators and non-native speakers find ways to 
translate or express them-selves in a foreign 
language. 
2 Aligning the corpus 
Central to TotalRecall is a bilingual corpus and a 
set of programs that provide the bilingual analyses 
to yield a translation memory database out of the 
bilingual corpus. Currently, we are working with 
  
A: Database selection B: English query C: Chinese query D: Number of items per page 
E: Normal view F: Clustered summary according to translation G: Order by counts or lengths 
H: Submit bottom I: Help file J: Page index K: English citation L: Chinese citation M: Date and title 
N: All citations in the cluster O: Full text context P: Side-by-side sentence alignment 
Figure 2. The results of searching for ?hard? 
 
bilingual corpora from a Taiwan-based Sinorama 
Magazine and Official Records of Hong Kong 
Legislative Council. A large bilingual collection of 
Studio Classroom English lessons will be provided 
in the near future.  That would allow us to offer 
bilingual texts in both translation directions and 
with different levels of difficulty. Currently, the 
articles from Sinorama seems to be quite usefully 
by its own, covering a wide range of topics, 
reflecting the personalities, places, and events in 
Taiwan for the past three decades.  
The concordance database is composed of bi-
lingual sentence pairs, which are mutual translation. 
In addition, there are also tables to record 
additional information, including the source of 
each sentence pairs, metadata, and the information 
on phrase and word level alignment. With that 
additional information, TotalRecall provides 
various functions, including 1. viewing of the full 
text of the source with a simple click. 2. 
highlighted translation counterpart of the query 
word or phrase. 3. ranking that is pedagogically 
useful for translation and language learning. 
We are currently running an operational system 
with Sinorama Magazine articles and HK LEGCO 
records. These bilingual texts that go into 
TotalRecall must be rearranged and structured. We 
describe the main steps below: 
2.1 Subsentential alignment 
While the length-based approach (Church and 
Gale 1991) to sentence alignment produces very 
good results for close language pairs such as 
French and English at success rates well over 96%, 
it does not fair as well for disparate language pairs 
such as English and Mandarin Chinese. Also 
sentence alignment tends to produce pairs of a long 
Chinese sentence and several English sentences. 
Such pairs of mutual translation make it difficult 
for the user to read and grasp the answers 
embedded in the retrieved citations.  
We develop a new approach to aligning English 
and Mandarin texts at sub-sentential level in 
parallel corpora based on length and punctuation 
marks. 
The subsentential alignment starts with parsing 
each article from corpora and putting them into the 
database. Subsequently articles are segmented into 
subsentential segments. Finally, segments in the 
two languages which are mutual translation are 
aligned. 
Sentences and subsentenial phrases and clauses 
are broken up by various types of punctuation in 
the two languages. For fragments much shorter 
than sentences, the variances of length ratio are 
larger leading to unacceptably low precision rate 
for alignment. We combine length-based and 
punctuation-based approach to cope with the 
difficulties in subsentential alignment. 
Punctuations in one language translate more or less 
consistently into punctuations in the other language. 
Therefore the information is useful in 
compensating for the weakness of length-based 
approach. In addition, we seek to further improve 
the accuracy rates by employing cognates and 
lexical information. We experimented with an 
implementation of the pro-posed method on a very 
large Mandarin-English parallel corpus of records 
of Hong Kong Legislative Council with 
satisfactory results. Experiment results show that 
the punctuation-based approach outperforms the 
length-based approach with precision rates 
approaching 98%. 
 
 
Figure 1  The result of subsentential alignment 
and collocation alignment. 
 
2.2 Word and Collocation Alignment 
After sentences and their translation counterparts 
are identified, we proceeded to carry out finer-
grained alignment on the word level. We employed 
the Competitive Linking Algorithm (Melamed 
2000) produce high precision word alignment. We 
also extract English collocations and their transla-
tion equivalent based on the result of word align-
ment. These alignment results were subsequently 
used to cluster citations and highlight translation 
equivalents of the query. 
 
3 Aligning the corpus 
TotalRecall allows a user to look for instances of 
specific words or expressions and its translation 
counterpart. For this purpose, the system opens up 
two text boxes for the user to enter queries in any 
or both of the two languages involved. We offer 
some special expressions for users to specify the 
following queries: 
 
? Single or multi-word query ? spaces be-
tween words in a query are considered as ?and.?  
For disjunctive query, use ?||? to de-note ?or.? 
? Every word in the query will be expanded 
to all surface forms for search. That includes 
singular and plural forms, and various tense of the 
verbs.  
? TotalRecall automatically ignore high fre-
quency words in a stoplist such as ?the,? ?to,? and 
?of.? 
? It is also possible to ask for exact match by 
submitting query in quotes. Any word within the 
quotes will not be ignored. It is useful for 
searching named entities. 
Once a query is submitted, TotalRecall displays 
the results on Web pages. Each result appears as a 
pair of segments in English and Chinese, in side-
by-side format. A ?context? hypertext link is in-
cluded for each citation. If this link is selected, a 
new page appears displaying the original document 
of the pair. If the user so wishes, she can scroll 
through the following or preceding pages of con-
text in the original document. TotalRecall present 
the results in a way that makes it easy for the user 
to grasp the information returned to her: 
? When operating in the monolingual mode, 
TotalRecall presents the citation according to 
lengths. 
? When operating in the bilingual mode, To-
talRecall clusters the citations according to the 
translation counterparts and presents the user with 
a summary page of one example each for different 
translations. The query words and translation 
counterparts are high-lighted. 
4 Conclusion 
In this paper, we describe a bilingual 
concordance designed as a computer assisted 
translation and language learning tool. Currently, 
TotalRecll uses Sinorama Magazine and 
HKLEGCO corpora as the databases of translation 
memory. We have already put a beta version on 
line and experimented with a focus group of 
second language learners. Novel features of 
TotalRecall include highlighting of query and 
corresponding translations, clustering and ranking 
of search results according translation and 
frequency. 
TotalRecall enable the non-native speaker who is 
looking for a way to express an idea in English or 
Mandarin. We are also adding on the basic func-
tions to include a log of user activities, which will 
record the users? query behavior and their back-
ground. We could then analyze the data and find 
useful information for future research. 
Subsentential alignment results 
 
From 1983 to 1991, the average rate of wage growth for all trades 
and industries was only 1.6%.  
???????????????????? 1.6%? 
This was far lower than the growth in labour productivity, which 
averaged 5.3%. 
????????????? 5.3%??? 
But, it must also be noted that the average inflation rate was as 
high as 7.7% during the same period.  
???????????? 7.7%? 
As I have said before, even when the economy is booming, the 
workers are unable to share the fruit of economic success. 
??????????????????????????
??? 
Acknowledgement  
We acknowledge the support for this study 
through grants from National Science Council and 
Ministry of Education, Taiwan (NSC 91-2213-E-
007-061 and MOE EX-92-E-FA06-4-4) and a 
special grant for preparing the Sinorama Corpus 
for distri-bution by the Association for 
Computational Lin-guistics and Chinese Language 
Processing. 
References  
Brown P., Cocke J., Della Pietra S., Jelinek F., 
Lafferty J., Mercer R., & Roossin P. (1990). A 
statistical approach to machine translation. 
Computational Linguistics, vol. 16. 
Gale, W. & K. W. Church, "A Program for 
Aligning Sen-tences in Bilingual Corpora" 
Proceedings of the 29th An-nual Meeting of the 
Association for Computational Linguistics, 
Berkeley, CA, 1991. 
Isabelle, Pierre, M. Dymetman, G. Foster, J-M. 
Jutras, E. Macklovitch, F. Perrault, X. Ren and 
M. Simard. 1993. Translation Analysis and 
Translation Automation. In Pro-ceedings of the 
Fifth International Conference on Theoreti-cal 
and Methodological Issues in Machine 
Translation, Kyoto, Japan, pp. 12-20. 
I. Dan Melamed. 2000. Models of translational 
equivalence among words. Computational 
Linguistics, 26(2):221?249, June. 
TANGO: Bilingual Collocational Concordancer 
Jia-Yan Jian  
Department of Computer 
Science 
National Tsing Hua 
University 
101, Kuangfu Road, 
Hsinchu, Taiwan 
g914339@oz.nthu.edu.tw 
Yu-Chia Chang  
Inst. of Information 
System and Applictaion 
National Tsing Hua 
University 
101, Kuangfu Road, 
Hsinchu, Taiwan 
u881222@alumni.nthu.e
du.tw 
Jason S. Chang 
Department of Computer 
Science 
National Tsing Hua 
University 
101, Kuangfu Road, 
Hsinchu, Taiwan 
jschang@cs.nthu.edu.tw 
 
Abstract 
In this paper, we describe TANGO as a 
collocational concordancer for looking up 
collocations. The system was designed to 
answer user?s query of bilingual collocational 
usage for nouns, verbs and adjectives. We first 
obtained collocations from the large 
monolingual British National Corpus (BNC). 
Subsequently, we identified collocation 
instances and translation counterparts in the 
bilingual corpus such as Sinorama Parallel 
Corpus (SPC) by exploiting the word-
alignment technique. The main goal of the 
concordancer is to provide the user with a 
reference tools for correct collocation use so 
as to assist second language learners to acquire 
the most eminent characteristic of native-like 
writing. 
1 Introduction 
Collocations are a phenomenon of word 
combination occurring together relatively often. 
Collocations also reflect the speaker?s fluency of a 
language, and serve as a hallmark of near native-
like language capability. 
Collocation extraction is critical to a range of 
studies and applications, including natural 
language generation, computer assisted language 
learning, machine translation, lexicography, word 
sense disambiguation, cross language information 
retrieval, and so on.  
Hanks and Church (1990) proposed using point-
wise mutual information to identify collocations in 
lexicography; however, the method may result in 
unacceptable collocations for low-count pairs. The 
best methods for extracting collocations usually 
take into consideration both linguistic and 
statistical constraints. Smadja (1993) also detailed 
techniques for collocation extraction and 
developed a program called XTRACT, which is 
capable of computing flexible collocations based 
on elaborated statistical calculation. Moreover, log 
likelihood ratios are regarded as a more effective 
method to identify collocations especially when the 
occurrence count is very low (Dunning, 1993).    
Smadja?s XTRACT is the pioneering work on 
extracting collocation types. XTRACT employed 
three different statistical measures related to how 
associated a pair to be collocation type. It is 
complicated to set different thresholds for each 
statistical measure. We decided to research and 
develop a new and simple method to extract 
monolingual collocations. 
We also provide a web-based user interface 
capable of searching those collocations and its 
usage. The concordancer supports language 
learners to acquire the usage of collocation. In the 
following section, we give a brief overview of the 
TANGO concordancer. 
2 TANGO 
TANGO is a concordancer capable of answering 
users? queries on collocation use. Currently, 
TANGO supports two text collections: a 
monolingual corpus (BNC) and a bilingual corpus 
(SPC). The system consists of four main parts: 
2.1 Chunk and Clause Information 
Integrated 
For CoNLL-2000 shared task, chunking is 
considered as a process that divides a sentence into 
syntactically correlated parts of words. With the 
benefits of CoNLL training data, we built a 
chunker that turn sentences into smaller syntactic 
structure of non-recursive basic phrases to 
facilitate precise collocation extraction. It becomes 
easier to identify the argument-predicate 
relationship by looking at adjacent chunks. By 
doing so, we save time as opposed to n-gram 
statistics or full parsing. Take a text in CoNLL-
2000 for example: 
The words correlated with the same chunk tag 
can be further grouped together (see Table 1). For 
instance, with chunk information, we can extract 
 Confidence/B-NP in/B-PP the/B-NP 
pound/I-NP is/B-VP widely/I-VP ex-
pected/I-VP to/I-VP take/I-VP an-
other/B-NP sharp/I-NP dive/I-NP if/B-
SBAR trade/B-NP figures/I-NP for/B-PP 
September/B-NP  
 
 
(Note: Every chunk type is associated with two 
different chunk tags: B-CHUNK for the first word 
of the chunk and I-CHUNK for the other words in 
the same chunk) 
 
the target VN collocation ?take dive? from the 
example by considering the last word of two 
adjacent VP and NP chunks. We build a robust and 
efficient chunking model from training data of the 
CoNLL shared task, with up to 93.7% precision 
and recall. 
Sentence chunking Features
Confidence NP 
in PP 
the pound NP 
is expected to take VP 
another sharp dive NP 
if SBAR 
trade figures NP 
for PP 
September NP 
Table 1: Chunked Sentence 
In some cases, only considering the chunk 
information is not enough. For example, the 
sentence ??the attitude he had towards the 
country is positive?? may cause problem. With 
the chunk information, the system extracts out the 
type ?have towards the country? as a VPN 
collocation, yet that obviously cuts across two 
clauses and is not a valid collocation. To avoid that 
kind of errors, we further take the clause 
information into account. 
With the training and test data from CoNLL-
2001, we built an efficient HMM model to identify 
clause relation between words. The language 
model provides sufficient information to avoid 
extracting wrong collocations. Examples show as 
follows (additional clause tags will be attached): 
 
(1) ?.the attitude (S* he has *S) toward the country 
(2) (S* I think (S* that the people are most 
concerned with the question of (S* when 
conditions may become ripe. *S)S)S) 
As a result, we can avoid combining a verb with 
an irrelevant noun as its collocate as ?have toward 
country? in (1) or ?think ? people? in (2). When 
the sentences in the corpus are annotated with the 
chunk and clause information, we can 
consequently extract collocations more precisely. 
2.2 Collocation Type Extraction 
A large set of collocation candidates can be 
obtained from BNC, via the process of integrating 
chunk and clause information. We here consider 
three prevalent Verb-Noun collocation structures 
in corpus: VP+NP, VP+PP+NP, and VP+NP+PP. 
Exploiting Logarithmic Likelihood Ratio (LLR) 
statistics, we can calculate the strength of 
association between two collocates. The 
collocational type with threshold higher than 7.88 
(confidence level 99.5%) will be kept as one entry 
in our collocation type list. 
2.3 Collocation Instance Identification 
We subsequently identify collocation instances 
in the bilingual corpus (SPC) with the collocation 
types extracted from BNC in the previous step. 
Making use of the sequence of chunk types, we 
again single out the adjacent structures of VN, 
VPN, and VNP. With the help of chunk and clause 
information, we thus find the valid instances where 
the expected collocation types are located, so as to 
build a collocational concordance. Moreover, the 
quantity and quality of BNC also facilitate the 
collocation identification in another smaller 
bilingual corpus with better statistic measure. 
English sentence Chinese sentence 
If in this time no one 
shows concern for them, 
and directs them to 
correct thinking, and 
teaches them how to 
express and release 
emotions, this could very 
easily leave them with a 
terrible personality 
complex they can never 
resolve. 
???????
???????
???????
???????
???????
???????
???????
?? 
Occasionally some 
kungfu movies may 
appeal to foreign 
audiences, but these too 
are exceptions to the 
rule. 
??????
???????
???????
?????? 
Table 2: Examples of collocational translation 
memory 
 
Type Collocation types in BNC  
VN 631,638 
VPN 15,394 
VNP 14,008 
Table 3: The result of collocation types extracted 
from BNC and collocation instances identified in 
SPC 
2.4 Extracting Collocational Translation 
Equivalents in Bilingual Corpus 
When accurate instances are obtained from 
bilingual corpus, we continue to integrate the 
statistical word-alignment techniques (Melamed, 
1997) and dictionaries to find the translation 
candidates for each of the two collocates. We first 
locate the translation of the noun. Subsequently, 
we locate the verb nearest to the noun translation 
to find the translation for the verb. We can think of 
collocation with corresponding translations as a 
kind of translation memory (shows in Table 2).The 
implementation result of BNC and SPC shows in 
the Table 3, 4, and 5. 
3 Collocation Concordance 
With the collocation types and instances 
extracted from the corpus, we built an online 
collocational concordancer called TANGO for 
looking up translation memory. A user can type in 
any English query and select the intended part of 
speech of query and collocate. For example in 
Figure 1, after query for the verb collocates of the 
noun ?influence? is submitted, the results are 
displayed on the return page. The user can then 
browse through different collocates types and also 
click to get to see all the instances of a certain 
collocation type. 
Noun VN types 
Language 320 
Influence 319 
Threat 222 
Doubt 199 
Crime 183 
Phone 137 
Cigarette 121 
Throat 86 
Living 79 
Suicide 47 
Table 4: Examples of collocation types including 
a given noun  in BNC 
 
 
VN type Example 
Exert 
influence 
That means they would 
already be exerting their 
influence by the time the 
microwave background was 
born. 
Exercise 
influence 
The Davies brothers, Adrian 
(who scored 14 points) and 
Graham (four), exercised an 
important creative influence 
on Cambridge fortunes while 
their flankers Holmes and 
Pool-Jones were full of fire 
and tenacity in the loose. 
Wield 
influence 
Fortunately, George V had 
worked well with his father 
and knew the nature of the 
current political trends, but he 
did not wield the same 
influence internationally as his 
esteemed father. 
Table 5: Examples of collocation instances 
extracted from SPC 
Moreover, using the technique of bilingual 
collocation alignment and sentence alignment, the 
system will display the target collocation with 
highlight to show translation equivalents in con-
text. Translators or learners, through this web-
based interface, can easily acquire the usage of 
each collocation with relevant instances. This 
collocational concordancer is a very useful tool for 
self-inductive learning tailored to intermedi-ate or 
advanced English learners. 
Users can obtain the result of the VN or AN 
collocations related to their query. TANGO shows 
the collocation types and instances with 
collocations and translation counterparts high-
lighted. 
The evaluation (shows in Table 6) indicates an 
average precision of 89.3 % with regard to 
satisfactory. 
4 Conclusion and Future Work 
In this paper, we describe an algorithm that 
employs linguistic and statistical analyses to 
extract instance of VN collocations from a very 
large corpus; we also identify the corresponding 
translations in a parallel corpus. The algorithm is 
applicable to other types of collocations without 
being limited by collocation?s span. The main 
difference between our algorithm and previous 
work lies in that we extract valid instances instead 
of types, based on linguistic information of chunks 
and clauses. Moreover, in our research we observe 
Type The number of 
selected 
sentences 
Translation 
Memory 
Translation 
Memory (*) 
Precision of 
Translation 
Memory 
Precision of 
Translation 
Memory (*) 
VN 100 73 90 73 90 
VPN 100 66 89 66 89 
VNP 100 78 89 78 89 
Table 6: Experiment result of collocational translation memory from Sinorama parallel Corpus 
 
 
Figure 1: The caption of the table 
other types related to VN such as VPN (ie. verb + 
preposition + noun) and VNP (ie. verb + noun + 
preposition), which will also be crucial for 
machine translation and computer assisted 
language learning. In the future, we will apply our 
method to more types of collocations, to pave the 
way for more comprehensive applications. 
Acknowledgements 
This work is carried out under the project 
?CANDLE? funded by National Science Council 
in Taiwan (NSC92-2524-S007-002). Further 
information about CANDLE is available at 
http://candle.cs.nthu.edu.tw/. 
References  
Dunning, T (1993) Accurate methods for the statistics 
of surprise and coincidence, Computational 
Linguistics 19:1, 61-75. 
Hanks, P. and Church, K. W. Word association norms, 
mutual information, and lexicography. 
Computational Linguistics, 1990, 16(1), pp. 22-29. 
Melamed, I. Dan. "A Word-to-Word Model of 
Translational Equivalence". In Procs. of the ACL97. 
pp 490-497. Madrid Spain, 1997.  
Smadja, F. 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1):143-177. 
 
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 37?40, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Learning Source-Target Surface Patterns for  
Web-based Terminology Translation 
 
Jian-Cheng Wu 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road,  
Hsinchu, 300, Taiwan 
D928322@oz.nthu.edu.tw 
Tracy Lin 
Dep. of Communication Eng. 
National Chiao Tung University 
1001, Ta Hsueh Road,  
Hsinchu, 300, Taiwan 
tracylin@cm.nctu.edu.tw 
Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road,  
Hsinchu, 300, Taiwan 
jschang@cs.nthu.edu.tw 
 
Abstract 
This paper introduces a method for learn-
ing to find translation of a given source 
term on the Web. In the approach, the 
source term is used as a query and part of 
patterns to retrieve and extract transla-
tions in Web pages. The method involves 
using a bilingual term list to learn source-
target surface patterns. At runtime, the 
given term is submitted to a search engine 
then the candidate translations are ex-
tracted from the returned summaries and 
subsequently ranked based on the surface 
patterns, occurrence counts, and translit-
eration knowledge. We present a proto-
type called TermMine that applies the 
method to translate terms. Evaluation on a 
set of encyclopedia terms shows that the 
method significantly outperforms the 
state-of-the-art online machine translation 
systems. 
1 Introduction 
Translation of terms has long been recognized as 
the bottleneck of translation by translators. By re-
using prior translations a significant time spent in 
translating terms can be saved. For many years 
now, Computer-Aided Translation (CAT) tools 
have been touted as very useful for productivity 
and quality gains for translators. CAT tools such as 
Trados typically require up-front investment to 
populate multilingual terminology and translation 
memory. However, such investment has proven 
prohibitive for many in-house translation depart-
ments and freelancer translators and the actual 
productivity gains realized have been insignificant 
except for a few, very repetitive types of content. 
Much more productivity gain could be achieved by 
providing translation service of terminology. 
Consider the job of translating a textbook such 
as ?Artificial Intelligence ? A Modern Approach.? 
The best practice is probably to start by translating 
the indexes (Figure 1). It is not uncommon for 
these repetitive terms to be translated once and 
applied consistently throughout the book. For ex-
ample, A good translation F = "????" for the 
given term E = "acoustic model," might be avail-
able on the Web due to the common practice of 
including the source terms (often in brackets,  see 
Figure 2)  when   using  a   translated term (e.g. 
"???????????Acoustic Model? ?
????  ?"). The surface patterns of co-
occurring source and target terms (e.g., "F?E") 
can be learned by using the Web as corpus. Intui-
tively, we can submit E and F to a search engine  
 
Figure 1. Some index entries in ?Artificial intelli-
gence ? A Modern Approach? page 1045. 
academy award, 458 
accessible, 41 
accusative case, 806 
Acero, A., 580, 1010 
Acharya, A., 131, 994 
achieves, 389 
Ackley, D. H., 133, 987 
acoustic model, 568 
 
Figure 2. Examples of web page summaries with 
relevant translations returned by Google for some 
source terms in Figure 1. 
1. ... ???? Academy Awards. ???? Berlin International 
Film Festival. ... 
2. ... ?????????(inherent Case)???????
(accusative Case)???? ... 
3. ... ??????????(Alfred H. Ackley) ??????
??????????????.. 
4. ..?????? ??????????????????
?????????Acoustic Model??????... 
37
and then extract the strings beginning with F and 
ending with E (or vice versa) to obtain recurring 
source-target patterns. At runtime, we can submit 
E as query, request specifically for target-language 
web-pages. With these surface patterns, we can 
then extract translation candidates Fs from the 
summaries returned by the search engine. Addi-
tional information of occurrence counts and trans-
literation patterns can be taken into consideration 
to rank Fs. 
 
Table 1. Translations by the machine translation 
system Google Translate and TermMine. 
Terms Google Translate TermMine
academy award *???? ???? 
accusative case *???? ?? 
Ackley - ??? 
acoustic model *???? ???? 
 
For instance, among many candidate translations, 
we will pick the translations "????" for "acous-
tic model" and "???" for "Ackley, " because 
they fit certain surface-target surface patterns and 
appears most often in the relevant webpage sum-
maries. Furthermore, the first morpheme "?" in "
???" is consistent with prior transliterations of 
"A-" in "Ackley" (See Table 1). 
We present a prototype system called TermMine, 
that automatically extracts translation on the Web 
(Section 3.3) based on surface patterns of target 
translation and source term in Web pages auto-
matically learned on bilingual terms (Section 3.1). 
Furthermore, we also draw on our previous work 
on machine transliteration (Section 3.2) to provide 
additional evidence. We evaluate TermMine on a 
set of encyclopedia terms and compare the quality 
of translation of TermMine (Section 4) with a 
online translation system. The results seem to indi-
cate the method produce significantly better results 
than previous work. 
2 Related Work 
There is a resurgent of interested in data-intensive 
approach to machine translation, a research area 
started from 1950s. Most work in the large body of 
research on machine translation (Hutchins and 
Somers, 1992), involves production of sentence-
by-sentence translation for a given source text. In 
our work, we consider a more restricted case where 
the given text is a short phrase of terminology or 
proper names (e.g., ?acoustic model? or ?George 
Bush?).  
A number of systems aim to translate words and 
phrases out of the sentence context. For example, 
Knight and Graehl (1998) describe and evaluate a 
multi-stage method for performing backwards 
transliteration of Japanese names and technical 
terms into English by the machine using a genera-
tive model. In addition, Koehn and Knight (2003) 
show that it is reasonable to define noun phrase 
translation without context as an independent MT 
subtask and build a noun phrase translation subsys-
tem that improves statistical machine translation 
methods. 
Nagata, Saito, and Suzuki (2001) present a sys-
tem for finding English translations for a given 
Japanese technical term by searching for mixed 
Japanese-English texts on the Web. The method 
involves locating English phrases near the given 
Japanese term and scoring them based on occur-
rence counts and geometric probabilistic function 
of byte distance between the source and target 
terms. Kwok also implemented a term translation 
system for CLIR along the same line. 
Cao and Li (2002) propose a new method to 
translate base noun phrases. The method involves 
first using Web-based method by Nagata et al, and 
if no translations are found on the Web, backing 
off to a hybrid method based on dictionary and 
Web-based statistics on words and context vectors. 
They experimented with noun-noun NP report that 
910 out of 1,000 NPs can be translated with an av-
erage precision rate of 63%.  
In contrast to the previous research, we present a 
system that automatically learns surface patterns 
for finding translations of a given term on the Web 
without using a dictionary. We exploit the conven-
tion of including the source term with the transla-
tion in the form of recurring patterns to extract 
translations. Additional evident of data redundancy 
and transliteration patterns is utilized to validate 
translations found on the Web. 
3 The TermMine System 
In this section we describe a strategy for searching 
the Web pages containing translations of a given 
term (e.g., ?Bill Clinton? or ?aircraft carrier?) and 
extracting translations therein. The proposed 
method involves learning the surface pattern 
38
knowledge (Section 3.1) necessary for locating 
translations. A transliteration model automatically 
trained on a list of proper name and transliterations 
(Section 3.2) is also utilized to evaluate and select 
transliterations for proper-name terms. These 
knowledge sources are used in concert to search, 
rank, and extract translations (Section 3.3). 
3.1 Source and Target Surface patterns 
With a set of terms and translations, we can learn 
the co-occurring patterns of a source term E and its 
translation F following the procedure below: 
(1) Submit a conjunctive query (i.e. E AND F) for 
each pair (E, F) in a bilingual term list to a 
search engine. 
(2) Tokenize the retrieved summaries into three 
types of tokens: I. A punctuation II. A source 
word, designated with the letter "w" III. A 
maximal block of target words (or characters in 
the case of language without word delimiters 
such as Mandarin or Japanese). 
(3) Replace the tokens for E?s instances with the 
symbol ?E? and the type-III token containing 
the translation F with the symbol ?F?. Note the 
token denoted as ?F? is a maximal string cover-
ing the given translation but containing no 
punctuations or words in the source language. 
(4) Calculate the distance between E and F by 
counting the number of tokens in between.  
(5) Extract the strings of tokens from E to F (or the 
other way around) within a maximum distance 
of d (d is set to 3) to produce ranked surface 
patterns P. 
 
For instance, with the source-target pair ("Califor-
nia," "??") and a retrieved summary of "...??
??. ??? Northern California. ...," the surface 
pattern "FwE" of distance 1 will be derived. 
3.2 Transliteration Model  
TermMine also relies on a machine transliteration 
model (Lin, Wu and Chang 2004) to confirm the 
transliteration of proper names. We use a list of 
names and transliterations to estimate the translit-
eration probability function P(? | ?), for any given 
transliteration unit (TU) ? and transliteration char-
acter (TC) ?. Based on the Expectation Maximiza-
tion (EM) algorithm. A TU for an English name 
can be a syllable or consonants which corresponds 
to a character in the target transliteration. Table 2 
shows some examples of sub-lexical alignment 
between proper names and transliterations. 
Table 2. Examples of aligned transliteration units. 
Name transliteration Viterbi alignment 
Spagna ???? s-? pag-? n-? a-?
Kohn ?? Koh-? n-? 
Nayyar ?? Nay-? yar-? 
Rivard ??? ri-? var-? d-? 
Hall ?? ha-? ll-? 
Kalam ?? ka-? lam-? 
Figure 3. Transliteration probability trained on 
1,800 bilingual names (? denotes an empty string). 
? ? P(?|?) ? ? P(?|?) ? ? P(?|?)
? .458 b ? .700 ye ? .667 
? .271  ? .133  ? .333 
? .059  ? .033 z ? .476 
a 
? .051  ? .033  ? .286 
? .923 an ? .923  ? .095 an 
? .077  ? .077  ? .048 
3.3 Finding and locating translations 
At runtime, TermMine follows the following steps 
to translate a given term E: 
(1) Webpage retrieval. The term E is submitted to 
a Web search engine with the language option 
set to the target language to obtain a set of 
summaries.  
(2) Matching patterns against summaries. The 
surface patterns P learned in the training phase 
are applied to match E in the tokenized summa-
ries, to extract a token that matches the F sym-
bol in the pattern. 
(3) Generating candidates. We take the distinct 
substrings C of all matched Fs as the candidates. 
(4) Ranking candidates. We evaluate and select 
translation candidates by using both data re-
dundancy and the transliteration model. Candi-
dates with a count or transliteration probability 
lower than empirically determined thresholds 
are discarded. 
I. Data redundancy. We rank translation candi-
dates by numbers of instances it appeared in the 
retrieved summaries. 
II. Transliteration Model. For upper-case E, we 
assume E is a proper name and evaluate each 
candidate translation C by the likelihood of C as 
the transliteration of E using the transliteration 
model described in (Lin, Wu and Chang 2004).  
39
Figure 4. The distribution of distances between 
source and target terms in Web pages. 
0
1000
2000
3000
4000
5000
Distance
Co
un
t
Count 63 111 369 2182 4961 2252 718 91 34
-4 -3 -2 -1 0 1 2 3 4
Figure 5. The distribution of distances between 
source and target terms in Web pages. 
Pattern Count Acc. Percent Example distance
FE 3036 28.1% ???? ATLAS 0 
EF 1925 45.9% Elton John???? 0 
E(F 1485 59.7% Austria(??? -1 
F?E 1251 71.2% ?????Atlas 1 
F(E 361 74.6% ????(Atlas 1 
F.E 203 76.5% Peter Pan. ??? 1 
EwF 197 78.3% ?? Northern California -1 
E,F 153 79.7% Mexico, ??? -1 
F??E 137 81.0% ??????Titanic 2 
F??E 119 82.1% ??????Atlas 2 
 
 
(5) Expanding the tentative translation. Based 
on a heuristics proposed by Smadja (1991) to 
expand bigrams to full collocations, we extend 
the top-ranking candidate with count n on both 
sides, while keeping the count greater than n/2 
(empirically determined). Note that the con-
stant n is set to 10 in the experiment described 
in Section 4. 
(6) Final ranking. Rank the expanded versions of 
candidates by occurrence count and output the 
ranked list. 
4 Experimental results 
We took the answers of the first 215 questions on a 
quiz Website (www.quiz-zone.co.uk) and hand-
translations as the training data to obtain a of sur-
face patterns. For all but 17 source terms, we are 
able to find at least 3 instances of co-occurring of 
source term and translation. Figure 4 shows distri-
bution of the distances between co-occurring 
source and target terms. The distances tend to con-
centrate between - 3 and + 3 (10,680 out of 12,398 
instances, or 86%). The 212 surface patterns ob-
tained from these 10,860 instances, have a very 
skew distribution with the ten most frequent sur-
face patterns accounting for 82% of the cases (see 
Figure 5). In addition to source-target surface pat-
terns, we also trained a transliteration model (see 
Figure 3) on 1,800 bilingual proper names appear-
ing in Taiwanese editions of Scientific American 
magazine.  
Test results on a set of 300 randomly selected 
proper names and technical terms from Encyclope-
dia Britannica indicate that TermMine produces 
300 top-ranking answers, of which 263 is the exact 
translations (86%) and 293 contain the answer key 
(98%). In comparison, the online machine transla-
tion service, Google translate produces only 156 
translations in full, with 103 (34%) matching the 
answer key exactly, and 145 (48%) containing the 
answer key. 
5 Conclusion 
We present a novel Web-based, data-intensive ap-
proach to terminology translation from English to 
Mandarin Chinese. Experimental results and con-
trastive evaluation indicate significant improve-
ment over previous work and a state-of-sate 
commercial MT system.  
References 
Y. Cao and H. Li. (2002). Base Noun Phrase Translation Us-
ing Web Data and the EM Algorithm, In Proc. of COLING 
2002, pp.127-133. 
W. Hutchins and H. Somers. (1992). An Introduction to Ma-
chine Translation. Academic Press. 
K. Knight, J. Graehl. (1998). Machine Transliteration. In 
Journal of Computational Linguistics 24(4), pp.599-612. 
P. Koehn, K. Knight. (2003). Feature-Rich Statistical Transla-
tion of Noun Phrases. In Proc. of ACL 2003, pp.311-318. 
K. L. Kwok, The Chinet system. (2004). (personal 
communication). 
T. Lin, J.C. Wu, J. S. Chang. (2004). Extraction of Name and 
Transliteration in Monolingual and Parallel Corpora. In 
Proc. of AMTA 2004, pp.177-186. 
M. Nagata, T. Saito, and K. Suzuki. (2001). Using the Web as 
a bilingual dictionary. In Proc. of ACL 2001 DD-MT 
Workshop, pp.95-102. 
F. A. Smadja. (1991). From N-Grams to Collocations: An 
Evaluation of Xtract. In Proc. of ACL 1991,  pp.279-284. 
40
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 1?4,
Sydney, July 2006. c?2006 Association for Computational Linguistics
FAST ? An Automatic Generation System for Grammar Tests 
 
 
Chia-Yin Chen 
Inst. of Info. Systems & Applications 
National Tsing Hua University 
101, Kuangfu Road, 
Hsinchu, 300, Taiwan 
G936727@oz.nthu.edu.tw 
Hsien-Chin Liou 
Dep. of Foreign Lang. & Lit. 
National Tsing Hua University 
101, Kuangfu Road, 
Hsinchu, 300, Taiwan 
hcliu@mx.nthu.edu.tw 
Jason S. Chang 
Dep. of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, 
Hsinchu, 300, Taiwan 
jschang@cs.nthu.edu.tw
 
  
 
Abstract 
This paper introduces a method for the 
semi-automatic generation of grammar 
test items by applying Natural Language 
Processing (NLP) techniques. Based on 
manually-designed patterns, sentences 
gathered from the Web are transformed 
into tests on grammaticality. The method 
involves representing test writing 
knowledge as test patterns, acquiring 
authentic sentences on the Web, and 
applying generation strategies to 
transform sentences into items. At 
runtime, sentences are converted into two 
types of TOEFL-style question: multiple-
choice and error detection. We also 
describe a prototype system FAST (Free 
Assessment of Structural Tests). 
Evaluation on a set of generated 
questions indicates that the proposed 
method performs satisfactory quality. 
Our methodology provides a promising 
approach and offers significant potential 
for computer assisted language learning 
and assessment. 
1 Introduction 
Language testing, aimed to assess learners? 
language ability, is an essential part of language 
teaching and learning. Among all kinds of tests, 
grammar test is commonly used in every 
educational assessment and is included in well-
established standardized tests like TOEFL (Test 
of English as Foreign Language). 
Larsen-Freeman (1997) defines grammar is 
made of three dimensions: form, meaning, and 
use (See Figure 1). Hence, the goal of a grammar  
test is to test learners to use grammar accurately, 
meaningfully, and appropriately.  Consider the 
possessive case of the personal noun in English. 
The possessive form comprises an apostrophe 
and the letter ?s?. For example, the possessive 
form of the personal noun ?Mary? is ?Mary?s?. 
The grammatical meaning of the possessive case 
can be (1) showing the ownership: ?Mary?s book 
is on the table.? (= a book that belongs to Mary); 
(2) indicating the relationship: ?Mary?s sister is 
a student.? (=the sister that Mary has).Therefore, 
a comprehensive grammar question needs to 
examine learners? grammatical knowledge from 
all three aspects (morphosyntax, semantics and 
pragmatics).  
 
 
 
 
 
 
 
 
Figure 1: Three Dimensions of Grammar 
(Larsen-Freeman, 1997) 
The most common way of testing grammar is 
the multiple-choice test (Kathleen and Kenji, 
1996). Multiple-choice test format on 
grammaticality consists of two kinds: one is the 
traditional multiple-choice test and the other is 
the error detection test. Figure 2 shows a typical 
example of traditional multiple-choice item. As 
for Figure 3, it shows a sample of error detection 
question. 
Traditional multiple-choice is composed of 
three components, where we define the sentence 
with a gap as the stem, the correct choice to the 
gap as the key and the other incorrect choices as 
the distractors. For instance, in Figure 2, the 
Form Meaning 
(appropriateness) 
(accuracy) 
ness) 
(meaningful- 
Use 
1
In the Great Smoky Mountains, one can see _____ 150 
different kinds of tress. 
(A) more than 
(B) as much as 
(C) up as 
(D) as many to 
Although maple trees are among the most colorful  
                                             (A)   
varieties in the fall, they lose its leaves 
                    (B)                     (C)   
sooner than oak trees.  
       (D) 
partially blanked sentence acts as the stem and 
the key ?more than? is accompanied by three 
distractors of ?as much as?, ?up as?, and ?as 
many to?. On the other hand, error detection item 
consists of a partially underlined sentence (stem) 
where one choice of the underlined part 
represents the error (key) and the other 
underlined parts act as distractors to distract test 
takers. In Figure 3, the stem is ?Although maple 
trees are among the most colorful varieties in the 
fall, they lose its leaves sooner than oak trees.? 
and ?its? is the key with distractors ?among?, ?in 
the fall?, and ?sooner than.? 
 
Grammar tests are widely used to assess 
learners? grammatical competence, however, it is 
costly to manually design these questions. In 
recent years, some attempts (Coniam, 1997; 
Mitkov and Ha, 2003; Liu et al, 2005) have been 
made on the automatic generation of language 
testing. Nevertheless, no attempt has been made 
to generate English grammar tests. Additionally, 
previous research merely focuses on generating 
questions of traditional multiple-choice task, no 
attempt has been made for the generation of error 
detection test types. 
In this paper, we present a novel approach to 
generate grammar tests of traditional multiple-
choice and error detection types. First, by 
analyzing syntactic structure of English 
sentences, we constitute a number of patterns for 
the development of structural tests. For example, 
a verb-related pattern requiring an infinitive as 
the complement (e.g., the verb ?tend?) can be 
formed from the sentence ?The weather tends to 
improve in May.? For each pattern, distractors 
are created for the completion of each grammar 
question. As in the case of foregoing sentence, 
wrong alternatives are constructed by changing 
the verb ?improve? into different forms: ?to 
improving?, ?improve?, and ?improving.? Then, 
we collect authentic sentences from the Web as 
the source of the tests. Finally, by applying 
different generation strategies, grammar tests in 
two test formats are produced. A complete 
grammar question is generated as shown in 
Figure 4. Intuitively, based on certain surface 
pattern (See Figure 5), computer is able to 
compose a grammar question presented in Figure 
4. We have implemented a prototype system 
FAST and the experiment results have shown that 
about 70 test patterns can be successfully written 
to convert authentic Web-based texts into 
grammar tests. 
 
 
 
  
 
 
 
* X/INFINITIVE * CLAUSE. 
 
* _______* CLAUSE. 
(A) X/INFINITIVE 
(B) X/to VBG 
(C) X/VBG 
(D) X/VB 
 
2 Related Work 
Since the mid 1980s, item generation for test 
development has been an area of active research. 
In our work, we address an aspect of CAIG 
(computer-assisted item generation) centering on 
the semi-automatic construction of grammar tests. 
Recently, NLP (Natural Language Processing) 
has been applied in CAIG to generate tests in 
multiple-choice format. Mitkov and Ha (2003) 
established a system which generates reading 
comprehension tests in a semi-automatic way by 
using an NLP-based approach to extract key 
concepts of sentences and obtain semantically 
alternative terms from WordNet. 
Coniam (1997) described a process to 
compose vocabulary test items relying on corpus 
word frequency data. Recently, Gao (2000) 
presented a system named AWETS that semi-
automatically constructs vocabulary tests based 
on word frequency and part-of-speech 
information. Most recently, Hoshino and 
Nakagawa (2005) established a real-time system 
which automatically generates vocabulary 
questions by utilizing machine learning 
techniques. Brown, Frishkoff, and Eskenazi 
(2005) also introduced a method on the 
automatic generation of 6 types of vocabulary 
questions by employing data from WordNet. 
I intend _______ you that we cannot approve your 
application. 
(A) to inform 
(B) to informing 
(C) informing 
(D) inform
Figure 4: An example of generated question.
Figure 5: An example of surface pattern. Figure 3: An example of error detection. 
Figure 2: An example of multiple-choice. 
2
Liu, Wang, Gao, and Huang (2005) proposed 
ways of the automatic composing of English 
cloze items by applying word sense 
disambiguation method to choose target words of 
certain sense and collocation-based approach to 
select distractors.  
Previous work emphasizes the automatic 
generation of reading comprehension, 
vocabulary, and cloze questions. In contrast, we 
present a system that allows grammar test writers 
to represent common patterns of test items and 
distractors. With these patterns, the system 
automatically gathers authentic sentences and 
generates grammar test items. 
3 The FAST System 
The question generation process of the FAST 
system includes manual design of test patterns 
(including construct pattern and distractor 
generation pattern), extracting sentences from the 
Web, and semi-automatic generation of test 
items by matching sentences against patterns. In 
the rest of this section, we will thoroughly 
describe the generation procedure.  
3.1 Question Generation Algorithm 
Input: P = common patterns for grammar test 
items, URL = a Web site for gathering sentences 
Output: T, a set of grammar test items g 
 
1. Crawl the site URL for webpages 
2. Clean up HTML tags. Get sentences S 
therein that are self-contained. 
3. Tag each word in S with part of speech (POS) 
and base phrase (or chunk). (See Figure 6 for 
the example of the tagging sentence ?A 
nuclear weapon is a weapon that derives its 
and or fusion.?) 
 
 
 
 
 
 
 
 
 
 
 
4. Match P against S to get a set of candidate 
sentences D. 
5. Convert each sentence d in D into a grammar 
test item g. 
3.2 Writing Test Patterns 
Grammar tests usually include a set of patterns 
covering different grammatical categories. These 
patterns are easily to conceptualize and to write 
down. In the first step of the creation process, we 
design test patterns. 
A construct pattern can be observed through 
analyzing sentences of similar structural features. 
Sentences ?My friends enjoy traveling by plane.? 
and ?I enjoy surfing on the Internet.? are 
analyzed as an illustration. Two sentences share 
identical syntactic structure {* enjoy X/Gerund 
*}, indicating the grammatical rule for the verb 
?enjoy? needing a gerund as the complement. 
Similar surface patterns can be found when 
replacing ?enjoy? by verbs such as ?admit? and 
?finish? (e.g., {* admit X/Gerund *} and {* 
finish X/Gerund *} ). These two generalize these 
surface patterns, we write a construct pattern {* 
VB VBG *} in terms of POS tags produced by a 
POS tagger. Thus, a construct pattern 
characterizing that some verbs require a gerund 
in the complement is contrived. 
Distractor generation pattern is dependent on 
each designed construct pattern and therefore 
needs to design separately. Distractors are 
usually composed of words in the construct 
pattern with some modifications: changing part 
of speech, adding, deleting, replacing, or 
reordering of words. By way of example, in the 
sentence ?Strauss finished writing two of his 
published compositions before his tenth 
birthday.?, ?writing? is the pivot word according 
to the construct pattern {* VBD VBG *}. 
Distractors for this question are: ?write?, 
?written?, and ?wrote?. Similar to the way for the 
construct pattern devise, we use POS tags to 
represent distractor generation pattern: {VB}, 
{VBN}, and {VBD}. We define a notation 
scheme for the distractor designing. The symbol 
$0 designates the changing of the pivot word in 
the construct pattern while $9 and $1 are the 
words proceeding and following the pivot word, 
respectively. Hence, distractors for the 
abovementioned question are {$0 VB}, {$0 
VBN}, and {$0 VBD}  
3.3   Web Crawl for Candidate Sentences 
As the second step, we extract authentic 
materials from the Web for the use of question 
stems. We collect a large number of sentences 
from websites containing texts of learned genres 
(e.g., textbook, encyclopedia).  
Lemmatization:  a nuclear weapon be a weapon that derive its 
energy from the nuclear reaction of fission 
and or fusion. 
POS:  a/at nuclear/jj weapon/nn be/bez a/at weapon/nn that/wps
          derive/vbz its/pp$ energy/nn from/in the/at nuclear/jj 
         reaction/nns of/in fission/nn  and/cc or/cc fusion/nn ./.  
Chunk:   a/B-NP nuclear/I-NP weapon/I-NP be/B-VP a/B-NP 
               weapon/I-NP that/B-NP derive/B-VP its/B-NP 
               energy/I-NP from/B-PP the/B-NP nuclear/I-NP 
reaction/I-NP of/B-PP fission/B-NP and/O or/B-UCP 
fusion/B-NP ./O  
Figure 6: Lemmatization, POS tagging and      
           chunking of a sentence. 
3
3.4 Test Strategy   
The generation strategies of multiple-choice and 
error detection questions are different. The 
generation strategy of traditional multiple-choice 
questions involves three steps. The first step is to 
empty words involved in the construct pattern. 
Then, according to the distractor generation 
pattern, three erroneous statements are produced. 
Finally, option identifiers (e.g., A, B, C, D) are 
randomly assigned to each alternative.  
The test strategy for error detection questions 
is involved with: (1) locating the target point, (2) 
replacing the construct by selecting wrong 
statements produced based on distractor 
generation pattern, (3) grouping words of same 
chunk type to phrase chunk (e.g., ?the/B-NP 
nickname/I-NP? becomes ?the nickname/NP?) 
and randomly choosing three phrase chunks to 
act as distractors, and (4) assigning options based 
on position order information.  
4 Experiment and Evaluation Results  
In the experiment, we first constructed test 
patterns by adapting a number of grammatical 
rules organized and classified in ?How to 
Prepare for the TOEFL?, a book written by 
Sharpe (2004). We designed 69 test patterns 
covering nine grammatical categories. Then, the 
system extracted articles from two websites, 
Wikipedia (an online encyclopedia) and VOA 
(Voice of American). Concerning about the 
readability issue (Dale-Chall, 1995) and the self-
contained characteristic of grammar question 
stems, we extracted the first sentence of each 
article and selected sentences based on the 
readability distribution of simulated TOEFL tests. 
Finally, the system matched the tagged sentences 
against the test patterns. With the assistance of 
the computer, 3,872 sentences are transformed 
into 25,906 traditional multiple-choice questions 
while 2,780 sentences are converted into 24,221 
error detection questions. 
A large amount of verb-related grammar 
questions were blindly evaluated by seven 
professor/students from the TESOL program. 
From a total of 1,359 multiple-choice questions, 
77% were regarded as ?worthy? (i.e., can be 
direct use or only needed minor revision) while 
80% among 1,908 error detection tasks were 
deemed to be ?worthy?. The evaluation results 
indicate a satisfactory performance of the   
proposed method.   
5 Conclusion 
We present a method for the semi-automatic 
generation of grammar tests in two test formats 
by using authentic materials from the Web. At 
runtime, a given sentence sharing classified 
construct patterns is generated into tests on 
grammaticality. Experimental results assess the 
facility and appropriateness of the introduced 
method and indicate that this novel approach 
does pave a new way of CAIG.  
References 
Coniam, D. (1997). A Preliminary Inquiry into 
Using Corpus Word Frequency Data in the 
Automatic Generation of English Cloze Tests. 
CALICO Journal, No 2-4, pp. 15- 33. 
Gao, Z.M. (2000). AWETS: An Automatic Web-
Based English Testing System. In Proceedings 
of the 8th Conference on Computers in 
Education/International Conference on 
Computer-Assisted Instruction ICCE/ICCAI, 
2000, Vol. 1, pp. 28-634. 
Hoshino, A. & Nakagawa, H. (2005). A Real-
Time Multiple-Choice Question Generation for 
Language Testing-A Preliminary Study-. In 
Proceedings of the Second Workshop on 
Building Educational Applications Using NLP, 
pp. 1-8, Ann Arbor, Michigan, 2005. 
Larsen-Freeman, D. (1997). Grammar and its 
teaching: Challenging the myths (ERIC Digest). 
Washington, DC: ERIC Clearinghouse on 
languages and Linguistics, Center for Applied 
Linguistics. Retrieved July 13, 2005, from 
http://www.vtaide.com/png/ERIC/Grammar.htm 
Liu, C.L., Wang, C.H., Gao, Z.M., & Huang, 
S.M. (2005). Applications of Lexical 
Information for Algorithmically Composing 
Multiple-Choice Cloze Items, In Proceedings of 
the Second Workshop on Building Educational 
Applications Using NLP, pp. 1-8, Ann Arbor, 
Michigan, 2005. 
Mitkov, R. & Ha, L.A. (2003). Computer-Aided 
Generation of Multiple-Choice Tests. In 
Proceedings of the HLT-NAACL 2003 
Workshop on Building Educational 
Applications Using Natural Language 
Processing, Edmonton, Canada, May, pp. 17 ? 
22. 
Sharpe, P.J. (2004). How to Prepare for the 
TOEFL. Barrons? Educational Series, Inc. 
Chall, J.S. & Dale, E. (1995). Readability 
Revisited: The New Dale-Chall Readability 
Formula. Cambridge, MA:Brookline Books. 
 
4
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 41?44,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Computational Analysis of Move Structures in Academic Abstracts 
Jien-Chen Wu1   Yu-Chia Chang1   Hsien-Chin Liou2   Jason S. Chang1 
CS1 and FLL2, National Tsing Hua Univ. 
{d928322,d948353}@oz.nthu.edu.tw, hcliu@mx.nthu.edu.tw, 
jason.jschang@gmail.com 
Abstract 
This paper introduces a method for 
computational analysis of move 
structures in abstracts of research articles. 
In our approach, sentences in a given 
abstract are analyzed and labeled with a 
specific move in light of various 
rhetorical functions. The method involves 
automatically gathering a large number 
of abstracts from the Web and building a 
language model of abstract moves. We 
also present a prototype concordancer, 
CARE, which exploits the move-tagged 
abstracts for digital learning. This system 
provides a promising approach to Web-
based computer-assisted academic 
writing. 
1 Introduction 
In recent years, with the rapid development of 
globalization, English for Academic Purposes 
has drawn researchers' attention and become the 
mainstream of English for Specific Purposes, 
particularly in the field of English of Academic 
Writing (EAW). EAW deals mainly with genres, 
including research articles (RAs), reviews, 
experimental reports, and other types of 
academic writing. RAs play the most important 
role of offering researchers the access to actively 
participating in the academic and discourse 
community and sharing academic research 
information with one another. 
Abstracts are constantly regarded as the first 
part of RAs and few scholarly RAs go without an 
abstract. ?A well-prepared abstract enables 
readers to identify the basic content of a 
document quickly and accurately.? (American 
National Standards Institute, 1979) Therefore, 
RAs' abstracts are equally important to writers 
and readers. 
Recent research on abstract requires manually 
analysis, which is time-consuming and labor-
intensive. Moreover, with the rapid development 
of science and technology, learners are 
increasingly engaged in self-paced learning in a 
digital environment. Our study, therefore, 
attempts to investigate ways of automatically 
analyzing the move structure of English RAs? 
abstracts and develops an online learning system, 
CARE (Concordancer for Academic wRiting in 
English). It is expected that the automatic 
analytical tool for move structures will facilitate 
non-native speakers (NNS) or novice writers to 
be aware of appropriate move structures and 
internalize relevant knowledge to improve their 
writing. 
2 Macrostructure of Information in 
RAs 
Swales (1990) presented a simple and succinct 
picture of the organizational pattern for a RA?
the IMRD structure (Introduction, Methods, 
Results, and Discussion). Additionally Swales 
(1981, 1990) introduced the theory of genre 
analysis of a RA and a four-move scheme, which 
was later refined as the "Create a Research 
Space" (CARS) model for analyzing a RA?s 
introduction section.  
Even though Swales seemed to have 
overlooked the abstract section, in which he did 
not propose any move analysis, he himself 
plainly realized ?abstracts continue to remain a 
neglected field among discourse analysts? 
(Swales, 1990, p. 181). Salager-Meyer (1992) 
also stated, ?Abstracts play such a pivotal role in 
any professional reading? (p. 94). Seemingly 
researchers have perceived this view, so research 
has been expanded to concentrate on the abstract 
in recent years. 
Anthony (2003) further pointed out, ?research 
has shown that the study of rhetorical 
organization or structure of texts is particularly 
useful in the technical reading and writing 
classroom? (p. 185). Therefore, he utilized 
computational means to create a system, Mover, 
which could offer move analysis to assist 
abstract writing and reading. 
3 CARE 
Our system focuses on automatically 
computational analysis of move structures (i.e. 
41
Background, Purpose, Method, Result, and 
Conclusion) in RA abstracts. In particular, we 
investigate the feasibility of using a few 
manually labeled data as seeds to train a Markov 
model and to automatically acquire move-
collocation relationships based on a large number 
of unlabeled data. These relationships are then 
used to analyze the rhetorical structure of 
abstracts. It is important that only a small 
number of manually labeled data are required 
while much of move tagging knowledge is 
learned from unlabeled data. We attempt to 
identify which rhetorical move is correspondent 
to a sentence in a given abstract by using features 
(e.g. collocations in the sentence). Our learning 
process is shown as follows: 
 
(1)Automatically collect abstracts from the Web for 
     training 
(2)Manually label each sentence in a small set of given  
     abstracts 
(3)Automatically extract collocations from all abstracts 
(4)Manually label one move for each distinct collocation 
(5)Automatically expand collocations indicative of each 
    move 
(6)Develop a hidden Markov model for move tagging 
Figure 1: Processes used to learn collocation 
classifiers 
3.1 Collecting Training Data 
In the first four processes, we collected data 
through a search engine to build the abstract 
corpus A. Three specialists in computer science 
tagged a small set of the qualified abstracts based 
on our coding scheme of moves. Meanwhile, we 
extracted the collocations (Jian et al, 2004) from 
the abstract corpus, and labeled these extracted 
collocations with the same coding scheme.  
3.2 Automatically Expanding Collocations 
for Moves 
To balance the distribution in the move-tagged 
collocation (MTC), we expand the collocation for 
certain moves in this stage. We use the one-
move-per-collocation constraint to bootstrap, 
which mainly hinges on the feature redundancy 
of the given data, a situation where there is often 
evidence to indicate that a given should be 
annotated with a certain move. That is, given one 
collocation ci is tagged with move mi, all 
sentences S containing collocation ci will be 
tagged with mi as well; meanwhile, the other 
collocations in S are thus all tagged with mi. For 
example: 
 
Step 1. The collocation ?paper address? 
extracted from corpus A is labeled with the ?P? 
move. Then we use it to label other untagged 
sentences US (e.g. Examples (1) through (2)) 
containing ?paper address? as ?P? in A. As a 
result, these US become tagged sentences TS 
with ?P? move. 
 
  (1)This paper addresses the state explosion problem in  
       automata based ltl model checking. //P// 
  (2)This paper addresses the problem of fitting mixture  
       densities to multivariate binned and truncated data. //P// 
 
Step 2. We then look for other features (e.g. the 
collocation, ?address problem?) that occur in TS 
of A to discover new evidences of a ?P? move 
(e.g. Examples (3) through (4)). 
 
  (3)This paper addresses the state explosion problem in  
       automata based ltl model checking. 
  (4)This paper addresses the problem of fitting mixture   
       densities to multivariate binned and truncated data. 
 
Step 3. Subsequently, the feature ?address 
problem? can be further exploited to tag 
sentences which realize the ?P? move but do not 
contain the collocation ?paper address?, thus 
gradually expanding the scope of the annotations 
to A. For example, in the second iteration, 
Example (5) and (6) can be automatically tagged 
as indicating the ?P? move. 
 
   (5)In this paper we address the problem of query  
       answering using views for non-recursive data log    
       queries embedded in a Description Logics  
       knowledge base. //P// 
  (6)We address the problem of learning robust  
       plans for robot navigation by observing  
       particular robot behaviors. //P// 
 
From these examples ((5) and (6)), we can 
extend to another feature ?we address?, which 
can be tagged as ?P? move as well. The 
bootstrapping processes can be repeated until no 
new feature with high enough frequency is found 
(a sample of collocation expanded list is shown 
in Table1).  
 
Type Collocation Move Count of 
Collocation 
with mj 
Total of 
Collocation 
Occurrences
NV we present P 3,441 3,668 
NV we show R 1,985 2,069 
NV we propose P 1,722 1,787 
NV we describe P 1,505 1,583 
? ? ? ? ? 
Table 1: The sample of the expanded collocation 
list 
42
3.3 Building a HMM for Move Tagging 
The move sequence probability P(ti+1? ti) is 
given as the following description: 
We are given a corpus of unlabeled abstracts A 
= {A1,?, AN}. We are also given a small labeled 
subset S = {L1,?, Lk} of A, where each abstract 
Li consists of a sequence of sentence and move 
{t1, t2,?, tk}. The moves ti take out of a value 
from a set of possible move M = {m1,m2,?,mn}. 
Then 11
( | )( | )
( )
i i
i i
i
N t tP t t
N t
+
+
? ?= ? ?? ?
 
According to the bi-gram move sequence 
score (shown in Table 2), we can see move 
sequences follow a certain schematic pattern. For 
instance, the ?B? move is usually directly 
followed by the ?P? move or ?B? move, but not 
by the ?M? move. Also rarely will a ?P? move 
occur before a ?B? move. Furthermore, an 
abstract seldom have a move sequence wherein 
?P? move directly followed by the ?R? move, 
which tends to be a bad move structure. In sum, 
the move progression generally follows the 
sequence of "B-P-M-R-C". 
 
Table 2: The score of bi-gram move sequence 
(Note that ?$? denotes the beginning or the 
ending of a given abstract.) 
 
Finally, we synchronize move sequence and 
one-move-per-collocation probabilities to train a 
language model to automatically learn the 
relationship between those extracted linguistic 
features based on a large number of unlabeled 
data. Meanwhile, we set some parameters of the 
proposed model, such as, the threshold of the 
number of collocation occurring in a given 
abstract, the weight of move sequence and 
collocation and smoothing. Based on these 
parameters, we implement the Hidden Markov 
Model (HMM). The algorithm is described as the 
following: 
1 1 1 1 1( ,...., ) ( ) ( | ) ( | ) ( | )n i i i ip s s p t p s t p t t p s t?= ?  
The moves ti take out of a value from a set of 
possible moves M={m1, m2, ?., mk} (The 
following parameters ?1 and ?2 will be 
determined based on some heuristics). 
( | )i i ip S t m=  
= ?1 if Si contains a collocation in MTCj 
 i j=  
= ?2 if Si contains a collocation in MTCj 
 but i j?  
= 
1
k
 if Si does not contain a collocation MTCj 
The optimal move sequence t* is 
1 2
1 2 1
, ,...,
( *, *,..., *) ( ,..., | ,..., )arg max
n
n n i n
t t t
t t t p s s t t=  
In summary, at the beginning of training time, 
we use a few human move-tagged sentences as 
seed data. Then, collocation-to-move and move-
to-move probabilities are employed to build the 
HMM. This probabilistic model derived at the 
training stage will be applied at run time. 
4 Evaluation 
In terms of the training data, we retrieved 
abstracts from the search engine, Citeseer; a 
corpus of 20,306 abstracts (95,960 sentences) 
was generated. Also 106 abstracts composed of 
709 sentences were manually move-tagged by 
four informants. Meanwhile, we extracted 72,708 
collocation types and manually tagged 317 
collocations with moves.  
At run time, 115 abstracts containing 684 
sentences were prepared to be the training data. 
We then used our proposed HMM to perform 
some experimentation with the different values 
of parameters: the frequency of collocation types, 
the number of sentences with collocation in each 
abstract, move sequence score and collocation 
score.  
4.1 Performance of CARE 
We investigated how well the HMM model 
performed the task of automatic move tagging 
under different values of parameters. The 
parameters involved included the weight of 
transitional probability function, the number of 
sentences in an abstract, the minimal number of 
instance for the applicable collocations. Figure 2 
indicates the best precision of 80.54% when 627 
sentences were qualified with the set of various 
Move ti Move ti+1 - log P (ti+1|ti) 
$ B 0.7802 
$ P 0.6131 
B B 0.9029 
B M 3.6109 
B P 0.5664 
C $ 0.0000 
M $ 4.4998 
M C 1.9349 
M M 0.7386 
M R 1.0033 
P M 0.4055 
P P 1.1431 
P R 4.2341 
R $ 0.9410 
R C 0.8232 
R R 1.7677 
43
parameters, including 0.7 as the weight of 
transitional probability function and a frequency 
threshold of 18 for a collocation to be applicable, 
and the minimally two sentences containing an 
applicable collocation. Although it is important 
to have many collocations, it is crucial that we 
set an appropriate frequency threshold of 
collocation so as not to include unreliable 
collocation and lower the precision rate. 
 
Figure2: The results of tagging performance with 
different setting of weight and threshold for 
applicable collocations (Note that C_T denotes 
the frequency threshold of collocation) 
5 System Interface 
The goal of the CARE System is to allow a 
learner to look for instances of sentences labeled 
with moves. For this purpose, the system is 
developed with three text boxes for learners to 
enter queries in English (as shown in Figure3.): 
? Single word query (i.e. directly input one 
word to query)  
? Multi-word query (i.e. enter the result 
show to find citations that contain the 
three words, ?the?, ?paper? and ?show? 
and all the derivatives) 
? Corpus selection (i.e. learners can focus on 
a corpus in a specific domain)  
Once a query is submitted, CARE displays the 
results in returned Web pages. Each result 
consists of a sentence with its move annotation. 
The words matching the query are highlighted. 
Figure 3: The sample of searching result with the 
phrase ?the result show? 
6 Conclusion 
In this paper, we have presented a method for 
computational analysis of move structures in 
RAs' abstracts and addressed its pedagogical 
applications. The method involves learning the 
inter-move relationships, and some labeling rules 
we proposed. We used a large number of 
abstracts automatically acquired from the Web 
for training, and exploited the HMM to tag 
sentences with the move of a given abstract. 
Evaluation shows that the proposed method 
outperforms previous work with higher precision. 
Using the processed result, we built a prototype 
concordance, CARE, enriched with words, 
phrases and moves. It is expected that NNS can 
benefit from such a system in learning how to 
write an abstract for a research article. 
References 
Anthony, L. and Lashkia, G. V. 2003. Mover: A 
machine learning tool to assist in the reading and 
writing of technical papers. IEEE Trans. Prof. 
Communication, 46:185-193.  
American National Standards Institute. 1979. 
American national standard for writing abstracts. 
ANSI Z39, 14-1979. New York: Author.  
Jian, J. Y., Chang, Y. C., and Chang, J. S. 2004. 
TANGO: Bilingual Collocational Concordancer, 
Post & demo in ACL 2004, Barcelona. 
Salager-Meyer, F. S. 1992. A text-type and move 
analysis study of verb tense and modality 
distribution in medical English abstracts. English 
for Specific Purposes, 11:93-113.  
Swales, J.M. 1981. Aspects of article introductions. 
Birmingham, UK: The University of Aston, 
Language Studies Unit.   
Swales, J.M. 1990. Genre analysis: English in 
Academic and Research Settings. Cambridge 
University Press.  
44
Acquisition of English-Chinese Transliterated Word Pairs from Parallel-
Aligned Texts using a Statistical Machine Transliteration Model 
Chun-Jen Lee1, 2 
1 Telecommunication Labs. 
Chunghwa Telecom Co., Ltd. 
Chungli, Taiwan, R.O.C. 
cjlee@cht.com.tw 
Jason S. Chang2 
2 Department of Computer Science 
National Tsing Hua University 
Hsinchu, Taiwan, R.O.C. 
jschang@cs.nthu.edu.tw 
  
Abstract 
This paper presents a framework for extract-
ing English and Chinese transliterated word 
pairs from parallel texts. The approach is 
based on the statistical machine transliteration 
model to exploit the phonetic similarities be-
tween English words and corresponding Chi-
nese transliterations. For a given proper noun 
in English, the proposed method extracts the 
corresponding transliterated word from the 
aligned text in Chinese. Under the proposed 
approach, the parameters of the model are 
automatically learned from a bilingual proper 
name list. Experimental results show that the 
average rates of word and character precision 
are 86.0% and 94.4%, respectively. The rates 
can be further improved with the addition of 
simple linguistic processing. 
1 Introduction 
Automatic bilingual lexicon construction based on bi-
lingual corpora has become an important first step for 
many studies and applications of natural language proc-
essing (NLP), such as machine translation (MT), cross-
language information retrieval (CLIR), and bilingual 
text alignment. As noted in Tsuji (2002), many previous 
methods (Dagan et al, 1993; Kupiec, 1993; Wu and Xia, 
1994; Melamed, 1996; Smadja et al, 1996) deal with 
this problem based on frequency of words appearing in 
the corpora, which can not be effectively applied to low-
frequency words, such as transliterated words. These 
transliterated words are often domain-specific and cre-
ated frequently. Many of them are not found in existing 
bilingual dictionaries. Thus, it is difficult to handle 
transliteration only via simple dictionary lookup. For 
CLIR, the accuracy of transliteration highly affects the 
performance of retrieval. 
In this paper, we present a framework of acquisition 
for English and Chinese transliterated word pairs based 
on the proposed statistical machine transliteration model. 
Recently, much research has been done on machine 
transliteration for many language pairs, such as Eng-
lish/Arabic (Al-Onaizan and Knight, 2002), Eng-
lish/Chinese (Chen et al, 1998; Lin and Chen, 2002; 
Wan and Verspoor, 1998), English/Japanese (Knight 
and Graehl, 1998), and English/Korean (Lee and Choi, 
1997; Oh and Choi, 2002). Most previous approaches to 
machine transliteration have focused on the use of a 
pronunciation dictionary for converting source words 
into phonetic symbols, a manually assigned scoring 
matrix for measuring phonetic similarities between 
source and target words, or a method based on heuristic 
rules for source-to-target word transliteration. However, 
words with unknown pronunciations may cause prob-
lems for transliteration. In addition, using either a lan-
guage-dependent penalty function to measure the 
similarity between bilingual word pairs, or handcrafted 
heuristic mapping rules for transliteration may lead to 
problems when porting to other language pairs.  
The proposed method in this paper requires no con-
version of source words into phonetic symbols. The 
model is trained automatically on a bilingual proper 
name list via unsupervised learning. 
The remainder of the paper is organized as follows: 
Section 2 gives an overview of machine transliteration 
and describes the proposed model. Section 3 describes 
how to apply the model for extraction of transliterated 
target words from parallel texts. Experimental setup and 
quantitative assessment of performance are presented in 
Section 4. Concluding remarks are made in Section 5. 
2 Statistical Machine Transliteration 
Model 
2.1 Overview of the Noisy Channel Model  
Machine transliteration can be regarded as a noisy 
channel, as illustrated in Figure 1. Briefly, the language 
model generates a source word E and the transliteration 
model converts the word E to a target transliteration C. 
Then, the channel decoder is used to find the word ? 
that is the most likely to the word E that gives rise to the 
transliteration C. 
Language
Model
P(E)
Transli-
Teration
Model
P(C|E)
Channel
Decoder
argmax
E
E C
P(E|C)
E?
 
Figure 1. The noisy channel model in ma-
chine transliteration. 
 
Under the noisy channel model, the back-
transliteration problem is to find out the most probable 
word E, given transliteration C. Letting P(E) be the 
probability of a word E, then for a given transliteration 
C, the back-transliteration probability of a word E can 
be written as P(E|C). By Bayes? rule, the transliteration 
problem can be written as follows: 
.
)(
)|()(maxarg)|(maxarg?
CP
ECPEPCEPE
EE
==    (1) 
Since P(C) is constant for the given C, we can rewrite 
Eq. (1) as follows: 
),|()(maxarg? ECPEPE
E
=      
(2) 
The first term, P(E), in Eq. (2) is the language model, 
the probability of E. The second term, P(C|E), in Eq. (2) 
is the transliteration model, the probability of the trans-
literation C conditioned on E. 
Below, we assume that E is written in English, while 
C is written in Chinese. Since Chinese and English are 
not in the same language family, there is no simple or 
direct way of mapping and comparison. One feasible 
solution is to adopt a Chinese romanization system1 to 
represent the pronunciation of each Chinese character. 
Among the many romanization systems for Chinese, 
Wade-Giles and Pinyin are the most widely used. The 
Wade-Giles system is commonly used in Taiwan today 
and has traditionally been popular among Western 
scholars. For this reason, we use the Wade-Giles system 
to romanize Chinese characters. However, the proposed 
approach is equally applicable to other romanization 
systems.  
The language model gives the prior probability P(E) 
which can be modeled using maximum likelihood esti-
mation. As for the transliteration model P(C|E), we can 
approximate it using the transliteration unit (TU), which 
is a decomposition of E and C. TU is defined as a se-
                                                          
1  Ref. sites: ?http://www.romanization.com/index.html? and 
?http://www.edepot.com/taoroman.html?. 
quence of characters transliterated as a base unit. For 
English, a TU can be a monograph, a digraph, or a tri-
graph (Wells, 2001). For Chinese, a TU can be a sylla-
ble initial, a syllable final, or a syllable (Chao, 1968) 
represented by corresponding romanized characters. To 
illustrate how this approach works, take the example of 
an English name, ?Smith?, which can be segmented into 
four TUs and aligned with the romanized transliteration. 
Assuming that the word is segmented into ?S-m-i-th?, 
then a possible alignment with the Chinese translitera-
tion ???? (Shihmissu)? is depicted in Figure 2.  
S             m            i       th
Shih      m          i            ssu
? ? ?
 
Figure 2. TU alignment between English and 
Chinese romanized character sequences. 
 
2.2 Formal Description: Statistical Translitera-
tion Model (STM) 
A word E with l characters and a romanized word C 
with n characters are denoted by le1  and 
nc1 , respec-
tively. Assume that the number of aligned TUs for (E, 
C) is N, and let },...,,{ 21 NmmmM =  be an alignment 
candidate, where mj is the match type of the j-th TU. 
The match type is defined as a pair of TU lengths for the 
two languages. For instance, in the case of (Smith, 
Shihmissu), N is 4, and M is {1-4, 1-1, 1-1, 2-3}. We 
write E and C as follows:  
,
,...,,
,...,,
2111
2111
??
??? ===
===
N
Nn
N
Nl
vvvvcC
uuuueE      
(3) 
where ui and vj are the i-th TU of E and the j-th TU of 
C, respectively. 
Then the probability of C given E, P(C|E), is formulated 
as follows: 
).|(),|()|,()|( EMPEMCPEMCPECP
MM
?? ==  (4) 
To reduce computational complexity, one alternative 
approach is to modify the summation criterion in Eq. (4) 
into maximization. Therefore, we can approximate 
P(C|E) as follows: 
)|(),|(max)|( EMPEMCPECP
M
?  
).(),|(max MPEMCP
M
?      (5) 
We approximate )(),|( MPEMCP  as follows: 
),...,,()|()(),|( 2111 N
NN mmmPuvPMPEMCP =  
).()|(
1
iii
N
i
mPuvP
=
??     (6) 
Therefore, we have 
( ).)(log)|(logmax)|(log
1
?
=
+?
N
i
iiiM
mPuvPECP    
(7) 
Let ),( jiS  be the maximum accumulated log prob-
ability between the first i characters of E and the first j 
characters of C. Then, ),()|(log nlSECP = , the maxi-
mum accumulated log probability among all possible 
alignment paths of E with length l and C with length n, 
can be computed using a dynamic programming (DP) 
strategy, as shown in the following: 
 
Step 1 (Initialization): 
0)0,0( =S        
(8) 
Step 2 (Recursion): 
    .0   ,0                                       
),(log)|(log                  
),(max),(
,
njli
khPecP
kjhiSjiS
i
hi
j
kj
kh
????
++
??=
??
   
(9) 
Step 3 (Termination): 
 ),(log)|(log                  
),(max),(
,
khPecP
knhlSnlS
l
hl
n
kn
kh
++
??=
??
 (10) 
 
where ),( khP  is defined as the probability of the match 
type ?h-k?. 
 
2.3 Estimation of Model Parameters 
To describe the iterative procedure for re-estimation of 
probabilities of )|( ij uvP  and )( imP , we first define 
the following functions: 
 
),( ji vucount  = the number of occurrences of 
aligned pair ui and vi in the training 
set. 
 
)( iucount  = the number of occurrences of ui in 
the training set. 
 
),( khcount  = the total number of occurrences 
of ui with length h aligned with vj 
with length k in the training set. 
 
Therefore, the translation probability )|( ij uvP  can be 
approximated as follows: 
.
)(
),(
)|(
i
ji
ij ucount
vucount
uvP =   
 (11) 
The probability of the match type, ),( khP , can be es-
timated as follows: 
.
),(
),(),( ??=
i j
jicount
khcountkhP   
 (12) 
For the reason that ),( ji vucount  is unknown in the 
beginning , a reasonable initial estimate of the parame-
ters of the translation model is to constrain the TU 
alignments of a word pair (E, C) within a position dis-
tance ?  (Lee and Choi, 1997). Assume that 1?+= hppi eu  
and 1?+= kqqj cv , and ),( ji vud?  is the allowable posi-
tion distance within ?  for the aligned pair (ui, vi). 
),( ji vud?  is defined as follows: 
, 
 )1()1(
             ,
 ),(
???
???
?
<??+??+
<??
=
?
?
?
n
lkqhp
and
n
lqp
vud ji
 (13) 
where l and n are the length of the source word E and 
the target word C, respectively. 
To accelerate the convergence of EM training and 
reduce the noisy TU aligned pairs (ui, vj), we restrict the 
combination of TU pairs to limited patterns. Consonant 
TU pairs only with same or similar phonemes are al-
lowed to be matched together. An English consonant is 
also allowed to matching with a Chinese syllable begin-
ning with same or similar phonemes. An English semi-
vowel TU can either be matched with a Chinese 
consonant or a vowel with same or similar phonemes, or 
be matched with a Chinese syllable beginning with 
same or similar phonemes. 
As for the probability of match type, ),( khP , it is 
set to uniform distribution in the initialization phase, 
shown as follows: 
,1),(
T
khP =      (14) 
where T is the total number of match types allowed. 
Based on the Expectation Maximization (EM) algo-
rithm (Dempster et al, 1977) with Viterbi decoding 
(Forney, 1973), the iterative parameter estimation pro-
cedure is described as follows:  
 
Step 1 (Initialization):  
Use Eq. (13) to generate likely TU alignment 
pairs. Calculate the initial model parameters, 
)|( ij uvP  and ),( khP , using Eq. (11) and Eq. 
(12). 
Step 2 (Expection):  
Based on current model parameters, find the 
best Viterbi path for each E and C word pair in 
the training set. 
Step 3 (Maximization):  
Based on all the TU alignment pairs obtained 
from Step 2, calculate the new model parame-
ters using Eqs. (11) and (12). Replace the 
model parameters with the new model parame-
ters. If it reaches a stopping criterion or a pre-
defined iteration numbers, then stop the 
training procedure. Otherwise, go back to Step 
2. 
3 Extraction of Transliteration from Par-
allel Text 
The task of machine transliteration is useful for many 
NLP applications, and one interesting related problem is 
how to find the corresponding transliteration for a given 
source word in a parallel corpus. We will describe how 
to apply the proposed model for such a task. 
For that purpose, a sentence alignment procedure is 
applied first to align parallel texts at the sentence level. 
Then, we use a tagger to identify proper nouns in the 
source text. After that, the model is applied to isolate the 
transliteration in the target text.  In general, the pro-
posed transliteration model could be further augmented 
with linguistic processing, which will be described in 
more details in the next subsection. The overall process 
is summarized in Figure 3. 
Bilingual corpus
Sentence alignment
Source 
sentence
Target 
sentence
Proper names: 
Word extraction
Source word
Proper names: 
Source & Target words
Linguistic
processing
Transli-
terator 
Prepro-
cessing
 
Figure 3. The overall process for the extrac-
tion of transliteration from parallel text. 
 
An excerpt from the magazine Scientific American 
(Cibelli et al, 2002) is illustrated as follows: 
 
Source sentence:  
?Rudolf Jaenisch, a cloning expert at the 
Whitehead Institute for Biomedical Re-
search at the Massachusetts Institute of 
Technology, concurred:? 
Target sentence: 
????????????????????
???????? 
 
In the above excerpt, three English proper nouns ?Jae-
nisch?, ?Whitehead?, and ?Massachusetts? are identi-
fied by a tagger. Utilizing Eqs. (7) and the DP approach 
formulated by Eqs. (8)-(10), we found the target word 
?huaihaite (??? )? most likely corresponding to 
?Whitehead?. In order to retrieve the transliteration for a 
given proper noun, we need to keep track of the optimal 
TU decoding sequence associated with the given Chi-
nese term for each word pair under the proposed 
method. The aligned TUs can be easily obtained via 
backtracking the best Viterbi path (Manning and 
Schutze, 1999). For the example mentioned above, the 
alignments of the TU matching pairs via the Viterbi 
backtracking path are illustrated in Figure 4. 
Match Type            TU Pair
:
0 - 1 ,    -- y
0 - 1 ,    -- u
0 - 1 ,    -- a
0 - 1 ,    -- n
2 - 2 , Wh -- hu
1 - 1 , i     -- a
1 - 0 , t     --
1 - 1 , e    -- i
1 - 1 , h    -- h
0 - 1 , -- a
2 - 1 , ea  -- i
1 - 2 , d   -- te
0 - 1 , -- s
0 - 1 , -- h
0 - 1 , -- e
0 - 1 , -- n
0 - 1 , -- g
:
?
?
?
?
?
 
Figure 4. The alignments of the TU matching pairs 
via the Viterbi backtracking path. 
 
3.1 Linguistic Processing 
Some language-dependent knowledge can be integrated 
to further improve the performance, especially when we 
focus on specific language pairs. 
 
Linguistic Processing Rule 1 (R1): 
Some source words have both transliteration and trans-
lation, which are equally acceptable and can be used 
interchangeably. For example, the source word ?Eng-
land? is translated into ??? (Yingkou)? and transliter-
ated into ???? (Yingkolan)?, respectively, as shown 
in Figure 5. Since the proposed model is designed spe-
cifically for transliteration, such cases may cause prob-
lems. One way to overcome this limitation is to handle 
those cases by using a list of commonly used proper 
names and translations. 
England vs. ??
The Spanish Armada sailed to England in 
1588.
??????????????????
England vs.???
England is the only country coterminous 
with Wales.
????????????????
 
Figure 5. Examples of mixed usages of 
translation and transliteration. 
 
Linguistic Processing Rule 2 (R2): 
From error analysis of the aligned results of the training 
set, the proposed approach suffers from the fluid TUs, 
such as ?t?, ?d?, ?tt?, ?dd?, ?te?, and ?de?. Sometimes 
they are omitted in transliteration, and sometimes they 
are transliterated as a Chinese character. For instance, 
?d? is usually transliterated into ???, ???, or ??? 
corresponding to Chinese TU of ?te?. The English TU 
?d? is transliterated as ??? in (Clifford, ????), but 
left out in (Radford, ???). In the example shown in 
Figure 6, ?David (??)? is mistakenly matched up with 
?????.  
(A boy by the name of David.)
?? ? ? ? ?????
?? Ta Wei Te ???
??? David .
 
Figure 6. Example of the transliterated 
word extraction for ?David?. 
 
However, that problem caused by fluid TUs 
can be partly overcome by adding more linguistic 
constraints in the post-processing phase. We calcu-
late the Chinese character distributions of proper 
nouns from a bilingual proper name list. A small 
set of Chinese characters is often used for translit-
eration. Therefore, it is possible to improve the 
performance by pruning extra tailing characters, 
which do not belong to the transliterated character 
set, from the transliteration candidates. For in-
stance, the probability of ??, ?, ?, ?, ?? being 
used in transliteration is very low. So correct trans-
literation ??? ? for the source word ?David? 
could be extracted by removing the character ???.  
3.2 Working Flow by Integrating Linguistic and 
Statistical Information 
Combining the linguistic processing and transliteration 
model, we present the algorithm for transliteration ex-
traction as follows: 
 
Step 1: Look up the translation list as stated in 
R1. If the translation of a source word 
appears in both the entry of the transla-
tion list and the aligned target sentence 
(or paragraph), then pick the translation 
as the target word. Otherwise, go to Step 
2. 
Step 2: Pass the source word and its aligned tar-
get sentence (or paragraph) through the 
proposed model to extract the target 
word.  
Step 3: Apply linguistic processing R2 to re-
move superfluous tailing characters in 
the target word.  
 
After the above processing, the performance of 
source-target word extraction is significantly improved 
over the previous experiment.  
4 Experiments 
In this section, we focus on the setup of experiments 
and performance evaluation for the proposed model. 
4.1 Experimental Setup 
The corpus T0 for training consists of 2,430 pairs of 
English names together with their Chinese translitera-
tions. Two experiments are conducted. In the first ex-
periment, we analyze the convergence characteristics of 
this model training based on a similarity-based frame-
work (Chen et al, 1998; Lin and Chen, 2002). A valida-
tion set T1, consisting of 150 unseen person name pairs, 
was collected from Sinorama Magazine (Sinorama, 
2002). For each transliterated word in T1, a set of 1,557 
proper names is used as potential answers. In the second 
experiment, a parallel corpus T2 was prepared to evalu-
ate the performance of proposed methods. T2 consists of 
500 bilingual examples from the English-Chinese ver-
sion of the Longman Dictionary of Contempory English 
(LDOCE) (Proctor, 1988). 
4.2 Evaluation Metric 
In the first experiment, a set of source words was com-
pared with a given target word, and then was ranked by 
similarity scores. The source word with the highest 
similarity score is chosen as the answer to the back-
transliteration problem. The performance is evaluated 
by rates of the Average Rank (AR) and the Average 
Reciprocal Rank (ARR) following Voorhees and Tice 
(2000). 
?
=
=
N
i
iR
N
AR
1
)(1    
 (15) 
?
=
=
N
i
iRN
ARR
1
)(
11   
 (16) 
where N is the number of testing data, and R(i) is the 
rank of the i-th testing data. Higher values of ARR indi-
cate better performance.  
 
0
0.5
1
1.5
2
2.5
3
3.5
1 2 3 4 5
Iteration number
Ra
te 
of 
AR
0.72
0.74
0.76
0.78
0.8
0.82
0.84
Ra
te 
of 
AR
R
AR
ARR
 
Figure 7. Performance at each iteration on 
the validation set T1. 
 
In Figure 7, we show the rates of AR and ARR for the 
validation set T1 by varying the number of iterations of 
the EM training algorithm from 1 to 6. We note that the 
rates become saturated at the 2nd iteration, which indi-
cates the efficiency of the proposed training approach.  
As for the second experiment, performance on the 
extraction of transliterations is evaluated based on pre-
cision and recall rates on the word and character level. 
Since we consider exact one proper name in the source 
language and one transliteration in the target language at 
a time. The word recall rates are same as word precision 
rates: 
=)(WP Precision Word  
.
 wordscorrect of number
 wordsextractedcorrectly  of number  (17) 
 
The character level recall and precision are as follows: 
=)( CPprecision Character  
,
characters correct of number
characters extractedcorrectly  of number  (18) 
=)(CR Recall Character  
.
characters correct of number
characters extractedcorrectly  of number  (19) 
 
For the purpose of easier evaluation, T2 was de-
signed to contain exact one proper name in the source 
language and one transliteration in the target language 
for each bilingual example. Therefore, if more than one 
proper name occurs in a bilingual example, we separate 
them into several testing examples. We also separate a 
compound proper name in one example into individual 
names to form multiple examples. For example, in the 
first case, two proper names ?Tchaikovsky? and ?Stra-
vinsky? were found in the testing sample ?Tchaikovsky 
and Stravinsky each wrote several famous ballets?. In 
the second case, a compound proper name ?Cyril 
Tourneur? was found in ?No one knows who wrote that 
play, but it is usually ascribed to Cyril Tourneur?. How-
ever, in the third case, ?New York? is transliterated as a 
whole Chinese word ????, so it can not be separated 
into two words. Therefore, the testing data for the above 
examples will be semi-automatically constructed. For 
simplicity, we considered each proper name in the 
source sentence in turn and determined its correspond-
ing transliteration independently. Table 1 shows some 
examples of the testing set T2. 
 
Table 1. Part of bilingual examples of the test-
ing set T2. 
 
In the experiment of transliterated word extraction, 
the proposed method achieves on average 86.0% word 
accuracy rate, 94.4% character precision rate, and 
96.3% character recall rate, as shown in row 1 of Table 
2. The performance can be further improved with a sim-
ple statistical and linguistic processing, as shown in 
Table 2. 
 
Methods WP CP CR 
Baseline 86.0% 94.4% 96.3% 
Baseline+R1 88.6% 95.4% 97.7% 
Baseline+R2 90.8% 97.4% 95.9% 
Baseline+R1+R2 94.2% 98.3% 97.7% 
Table 2. The experimental results of transliter-
ated word extraction for T2. 
 
In the baseline model, we find that there are some 
errors caused by translations which are not strictly trans-
literated; and there are some source words transferred 
into target words by means of transliteration and transla-
tion mutually. Therefore, R1 can be viewed as the pre-
processing to extract transliterated words. Some errors 
are further eliminated by R2 which considers the usage 
of the transliterated characters in the target language. In 
this experiment, we use a transliterated character set of 
735 Chinese characters. 
5 Conclusion 
In this paper, we describe a framework to deal with the 
problem of acquiring English-Chinese bilingual translit-
erated word pairs from parallel-aligned texts. An unsu-
pervised learning approach to the proposed machine 
transliteration model is also presented. The proposed 
approach automatically learned the parameters of the 
model from a bilingual proper name list. It is not re-
stricted to the availability of pronunciation dictionary in 
the source language. From the experimental results, it 
indicates that our methods achieve excellent perform-
ance. With the statistical-based characteristic of the pro-
posed model, we plan to extend the experiments to bi-
directional transliteration and other different corpora.  
References 
Yaser Al-Onaizan and Kevin Knight. 2002. Translating 
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting 
of the Association for Computational Linguistics 
(ACL), pages 400-408. 
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, and 
Shih-Chung Tsai. 1998. Proper name translation in 
cross-language information retrieval. In Proceedings 
of 17th COLING and 36th ACL, pages 232-236. 
Yuen Ren Chao. 1968. A Grammar of spoken Chinese. 
Berkeley, University of California Press.  
Dagan, I., Church, K. W., and Gale, W. A. 1993. Robust 
bilingual word alignment for machine aided transla-
tion. In Proceedings of the Workshop on Very Large 
Corpora: Academic and Industrial Perspectives, 
pages 1-8, Columbus Ohio. 
Jose B. Cibelli, Robert P. Lanza, Michael D. West, and 
Carol Ezzell. 2002. What Clones? Scientific Ameri-
can, Inc., New York, January. 
http://www.sciam.com. 
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1-38. 
G. D. Forney. 1973. The Viterbi algorithm. Proceedings 
of IEEE, 61:268-278, March. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
transliteration. Computational Linguistics, 
24(4):599-612. 
Julian Kupiec. 1993. An algorithm for finding noun 
phrase correspondences in bilingual corpora. In Pro-
ceedings of the 40th Annual Conference of the 
He is a (second) Caesar in speech and leader-
ship. 
??????????????????. 
Can you adduce any reason at all for his 
strange behaviour, Holmes? 
????, ??????????????
????  
They appointed him to catch all the rats in 
Hamelin. 
???????????????. 
Drink Rossignol, the aristocrat of table wines! 
??????! ????????! 
Cleopatra was bitten by an asp. 
????????????????. 
Schoenberg used atonality in the music of his 
middle period. 
??????????????. 
Now that this painting has been authenticated 
as a Rembrandt, it's worth 10 times as much 
as I paid for it! 
??????????????, ????
???????????! 
Association for Computational Linguistics (ACL), 
pages 17-22, Columbus, Ohio. 
Jae Sung Lee and Key-Sun Choi. 1997. A statistical 
method to generate various foreign word translitera-
tions in multilingual information retrieval system. In 
Proceedings of the 2nd International Workshop on 
Information Retrieval with Asian Languages 
(IRAL'97), pages 123-128, Tsukuba, Japan. 
Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward 
transliteration by learning phonetic similarity. In 
CoNLL-2002, Sixth Conference on Natural Lan-
guage Learning, Taipei, Taiwan.  
Christopher D. Manning and Hinrich Schutze. 1999. 
Foundations of Statistical Natural Language Proc-
essing, MIT Press; 1st edition. 
I Dan Melamed. 1996. Automatic construction of clean 
broad coverage translation lexicons. In Proceedings 
of the 2nd Conference of the Association for Ma-
chine Translation in the Americas (AMTA'96), 
Montreal, Canada. 
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation 
and contextual rules. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics (COLING), Taipei, Taiwan. 
P. Proctor, 1988. Longman English-Chinese Dictionary 
of Contemporary English, Longman Group (Far 
East) Ltd., Hong Kong. 
Sinorama. 2002. Sinorama Magazine. 
http://www.greatman.com.tw/sinorama.htm. 
Bonnie Glover Stalls and Kevin Knight. 1998. Translat-
ing names and technical terms in Arabic text. In 
Proceedings of the COLING/ACL Workshop on 
Computational Approaches to Semitic Languages. 
Frank Z. Smadja, Kathleen McKeown, and Vasileios 
Hatzivassiloglou. 1996. Translating collocations for 
bilingual lexicons: a statistical approach. Computa-
tional Linguistics, 22(1):1-38. 
Keita Tsuji. 2002. Automatic extraction of translational 
Japanese-KATAKANA and English word pairs 
from bilingual corpora. International Journal of 
Computer Processing of Oriental Languages, 
15(3):261-279. 
Ellen M. Voorhees and Dawn M. Tice. 2000. The trec-8 
question answering track report. In English Text Re-
trieval Conference (TREC-8). 
Stephen Wan and Cornelia Maria Verspoor. 1998. 
Automatic English-Chinese name transliteration for 
development of multilingual resources. In Proceed-
ings of 17th COLING and 36th ACL, pages 1352-
1356. 
J. C. Wells. 2001. Longman Pronunciation Dictionary 
(New Edition), Addison Wesley Longman, Inc. 
Dekai Wu and Xuanyin Xia. 1994. Learning an English-
Chinese lexicon from a parallel corpus. In Proceed-
ings of the First Conference of the Association for 
Machine Translation in the Americas, pages 206?
213. 
 
Class Based Sense Definition Model for Word Sense Tagging and Disambiguation 
Tracy Lin 
Department of Communication Engineering 
National Chiao Tung University, 
1001, Ta Hsueh Road, Hsinchu, 300, Taiwan, ROC 
tracylin@cm.nctu.edu.tw  
 
Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 300, Taiwan, ROC 
jschang@cs.nthu.edu.tw 
 
Abstract 
We present an unsupervised learning 
strategy for word sense disambiguation 
(WSD) that exploits multiple linguistic 
resources including a parallel corpus, a bi-
lingual machine readable dictionary, and a 
thesaurus. The approach is based on Class 
Based Sense Definition Model (CBSDM) 
that generates the glosses and translations 
for a class of word senses. The model can 
be applied to resolve sense ambiguity for 
words in a parallel corpus. That sense 
tagging procedure, in effect, produces a 
semantic bilingual concordance, which 
can be used to train WSD systems for the 
two languages involved. Experimental re-
sults show that CBSDM trained on 
Longman Dictionary of Contemporary 
English, English-Chinese Edition 
(LDOCE E-C) and Longman Lexicon of 
Contemporary English (LLOCE) is very 
effectively in turning a Chinese-English 
parallel corpus into sense tagged data for 
development of WSD systems. 
1. Introduction 
Word sense disambiguation has been an important 
research area for over 50 years. WSD is crucial for 
many applications, including machine translation, 
information retrieval, part of speech tagging, etc. 
Ide and Veronis (1998) pointed out the two major 
problems of WSD: sense tagging and data sparse-
ness. On one hand, tagged data are very difficult to 
come by, since sense tagging is considerably more 
difficult than other forms of linguistic annotation. 
On the other hand, although the data sparseness is 
a common problem, it is especially severe for 
WSD. The problems were attacked in various ways. 
Yarowsky (1992) showed a class-based approach 
under which a very large untagged corpus and the-
saurus can be used effectively for unsupervised 
training for noun homograph disambiguation. 
However, the method does not offer a method that 
explicitly produces sense tagged data for any given 
sense inventory. Li and Huang (1999) described a 
similar unsupervised approach for Chinese text 
based on a Chinese thesaurus. As noted in Meri-
aldo (1994), even minimal hand tagging improved 
on the results of unsupervised methods. Yarowsky 
(1995) showed that the learning strategy of boot-
strapping from small tagged data led to results ri-
valing supervised training methods. Li and Li 
(2002) extended the approach by using corpora in 
two languages to bootstrap the learning process. 
They showed bilingual bootstrapping is even more 
effective. The bootstrapping approach is limited by 
lack of a systematic procedure of preparing seed 
data for any word in a given sense inventory. The 
approach also suffers from errors propagating from 
one iteration into the next. Li and Huang  
 
Another alternative involves using a parallel 
corpus as a surrogate for tagged data. Gale, Church 
and Yarowsky (1992) exploited the so-called one 
sense per translation constraint for WSD. They 
reported high precision rates of a WSD system for 
two-way disambiguation of six English nouns 
based on their translations in an English-French 
Parallel corpus. However, when working with a 
particular sense inventory, there is no obvious way 
to know whether the one sense per translation con-
straint holds or how to determine the relevant 
translations automatically. 
 
Diab and Resnik (2002) extended the transla-
tion-based learning strategy with a weakened con-
straint that many instances of a word in a parallel 
corpus often correspond to lexically varied but se-
mantically consistent translations. They proposed 
to group those translations into a target set, which 
can be automatically tagged with correct senses 
based on the hypernym hierarchy of WordNet. 
Diab and Resnik?s work represents a departure 
from previous unsupervised approaches in that no 
seed data is needed and explicit tagged data are 
produced for a given sense inventory (WordNet in 
their case). The system trained on the tagged data 
was shown to be on a par with the best ?supervised 
training? systems in SENSEVAL-2 competition. 
However, Diab and Resnik?s method is only appli-
cable to nominal WordNet senses. Moreover, the 
method is seriously hampered by noise and seman-
tic inconsistency in a target set. Worse still, it is 
not always possible to rely on the hypernym hier-
archy for tagging a target set. For instance, the 
relevant senses of the target set of {serve, tee off} 
for the Chinese counterpart  [faqiu] do not 
have a common hypernym: 
 
Sense 15 
serve ? (put the ball into play; as in games like tennis) 
? move ? (have a turn; make one?s move in a game) 
Sense 1 
Tee off ? (strike a golf ball from a tee at the start of a game) 
? play ? (participating in game or sports) 
? compete ? (compete for something) 
 
This paper describes a new WSD approach to 
simultaneously attack the problems of tagging and 
data sparseness. The approach assumes the avail-
ability of a parallel corpus of text written in E (the 
first language, L1+) and C (the second language, 
L2), an L1 to L2 bilingual machine readable dic-
tionary M, and a L1 thesaurus T. A so-called Mu-
tually Assured Resolution of Sense Algorithm 
(MARS) and Class Based Sense Definition Model 
(CBSDM) are proposed to identify the word senses 
in I for each word in a semantic class of words L in 
T. Unlike Diab and Resnik, we do not apply the 
MARS algorithm directly to target sets to avoid 
the noisy words therein. The derived classes senses 
and their relevant glosses in L1 and L2 make it 
possible to build Class Based Sense Definition and 
Translation Models (CBSDM and CBSTM), which 
subsequently can be applied to assign sense tags to 
words in a parallel corpus. 
The main idea is to exploit the defining L1 and 
L2 words in the glosses to resolve the sense ambi-
                                                           
+
 This has nothing to do with the direction of translation and is 
not to be confused with the native and second language dis-
tinction made in the literature of Teaching English As a Sec-
ond Language (TESL) and Computer Assisted Language 
Learning. 
guity. For instance, for the class containing ?serve? 
and ?tee off,? the approach exploits common defin-
ing words, including ?ball? and ?game? in two 
relevant serve-15 and tee off-1 to assign the cor-
rect senses to ?serve? and ?tee off.? The character 
bigram  [faqiu] in an English-Chinese 
MRD: 
 
serve v 10 [I?; T1] to begin play by striking  (the 
ball) to the opponent  (LDOCE E-C p. 
1300), 
 
would make it possible to align and sense tag 
?serve? or ?tee off? in a parallel corpus such as the 
bilingual citations in Example 1:  
 
(1C)  
(1E) drink a capful before teeing off at each hole. 
(Source: Sinorama, 1999, Nov. Issue, p.15, Who 
Played the First Stroke?). 
 
That effectively attaches semantic information to 
bilingual citations and turns a parallel corpus into a 
Bilingual Semantic Concordance (BSC). The BSC 
enables us to simultaneously attack two critical 
WSD problems of sense tagging difficulties and 
data sparseness, thus provides an effective ap-
proach to WSD. BSC also embodies a projection 
of the sense inventory from L1 onto L2, thus cre-
ates a new sense inventory and semantic concor-
dance for L2. If I is based on WordNet for English, 
it is then possible to obtain an L2 WordNet. There 
are many additional applications of BSC, including 
bilingual lexicography, cross language information 
retrieval, and computer assisted language learning. 
 
The remainder of the paper is organized as fol-
lows: Sections 2 and 3 lay out the approach and 
describe the MARS and SWAT algorithms. Sec-
tion 4 describes experiments and evaluation. Sec-
tion 5 contains discussion and we conclude in 
Section 6. 
2. Class Based Sense Definition Model 
We will first illustrate our approach with an exam-
ple. A formal treatment of the approach will follow 
in Section 2.2. 
2.1 An example 
To make full use of existing machine readable dic-
tionaries and thesauri, some kind of linkage and 
integration is necessary (Knight and Luk, 1994). 
Therefore, we are interested in linking thesaurus 
classes and MRD senses: Given a thesaurus class S, 
it is important that the relevant senses for each 
word w in S is determined in a MRD-based sense 
inventory I. We will show such linkage is useful 
for WSD and is feasible, based solely on the words 
of the glosses in I. For instance, given the follow-
ing set of word (N060) in Longman Lexicon of 
Contemporary English  (McArthur 1992): 
L = {difficult, hard, stiff, tough, arduous, awkward}. 
Although those words are highly ambiguous, 
the juxtaposition immediately brings to mind the 
relevant senses. Specifically for the sense inven-
tory of LDOCE E-C, the relevant senses for L are 
as follows: 
 
Therefore, we have the intended senses, S 
S = {difficult-1, hard-2, stiff-6, tough-4, arduous-1, awk-
ward-2}.  
It is reasonable to assume each sense in I is ac-
companied by a sense definition written in the 
same language (L1). We use D(S) to denote the 
glosses of S. Therefore we have 
D(S) = ?not easy; hard to do, make, understand, etc.;  diffi-
cult to do or understand; difficult to do; difficult to do; not 
easy; demanding effort; needing much effort; difficult; not 
well made for use; difficult to use; causing difficulty;?  
The intuition of bringing out the intended 
senses of semantically related words can be for-
malized by Class Based Sense Definition Model 
(CBSDM), which is a micro language model gen-
erating D(S), the glosses of S in I. For simplicity, 
we assume an unigram language model P(d) that 
generates the content words d in the glosses of S. 
Therefore, we have 
D(S) = ?easy hard do make understand difficult do under-
stand difficult do difficult do easy demanding effort need-
ing much effort difficult well made use difficult use causing 
difficulty?  
 
If we have the relevant senses, it is a simple 
matter of counting to estimate P(d). Conversely, 
with P(d) available to us, we can pick the relevant 
sense of S in I which is most likely generated by 
P(d). The problem of learning the model P(d) lend 
itself nicely to an iterative relaxation method such 
as the Expectation and Maximization Algorithm 
(Dempster, Laird, Rubin, 1977). 
 
Initially, we assume all senses of S word in I is 
equally likely and use all the defining words 
therein to estimate P(d) regardless of whether they 
are relevant. For LDOCE senses, initial estimate of 
the relevant glosses is as follows: 
 
D(S) = ?easy hard do make understand people unfriendly 
quarrelling pleased ? firm stiff broken pressed bent diffi-
cult do understand forceful needing using force body 
mind ?bent painful moving moved ? strong weakened 
suffer uncomfortable conditions cut worn bro-
ken ?needing effort difficult lacking skill moving body 
parts body CLUMSY made use difficult use causing diffi-
culty?  
 
Table 1. The initial CBSDM for n-word list {difficult, 
hard, stiff, tough, arduous, awkward} based on the rele-
vant and irrelevant LDOCE senses, n = 6. 
Defining word d Count, k P(d) = k/n 
Difficult 5 0.83 
Effort 3 0.50 
Understand 2 0.33 
Bad 2 0.33 
Bent 2 0.33 
Body 2 0.33 
Broken 2 0.33 
Difficulty 2 0.33 
Easy 2 0.33 
Firm 2 0.33 
Hard 2 0.33 
Moving 2 0.33 
Needing 2 0.33 
Water 2 0.33 
 
As evident from Table 1, the initial estimates of 
P(d) are quite close to the true probability distribu-
tion (based on the relevant senses only). The three 
top ranking defining words ?difficult,? ?effort,? and 
?understand? appear in glosses of relevant senses, 
and not in irrelevant senses. Admittedly, there are 
still some noisy, irrelevant words such as ?bent? 
and ?broken.? But they do not figure prominently 
in the model from the start and will fade out gradu-
ately with successive iterations of re-estimation. 
We estimate the probability of a particular sense s 
being in S by P(D(s)), the probability of its gloss 
under P(d). For intance, we have 
 
P(hard-1) = P(D(hard-1)) = P(?firm and stiff; which ??), 
P(hard-2) = P(D(hard-2)) = P(?difficult to do or understand?).  
 
On the other hand, we re-estimate the probabil-
ity P(d) of a defining word d under CBSDM by 
how often d appears in a sense s and P(s). P(d) is 
positively prepositional to the frequency of d in 
D(s) and to the value of P(s). Under that re-
estimation scheme, the defining words in relevant 
senses will figure more prominently in CBSDM, 
leading to more accurate estimation for probability 
of s being in S. For instance, in the first round, 
?difficult? in the gloss of hard-2 will weigh twice 
more than ?firm? in the gloss of irrelevant hard-1, 
leading to relatively higher unigram probability for 
?difficult.? That in turn makes hard-2 even more 
probable than hard-1. See Table 2. 
 
Table 2. First round estimates for P(s), the probability of 
sense s in S. 
Sense* Definition P(s) 
hard-1 firm and stiff; which can-
not easily be broken 
0.2857 
hard-2 difficult to do or under-
stand 
0.7143 
stiff-1 not easily bent 0.2857 
stiff-6 difficult to do 0.7143 
* in LDOCE. 
** Assuming )(max)(
)(
dPsP
sDd?
?  
 
Often the senses in I are accompanied with 
glosses written in a second language (L2); exclu-
sively (as in a simple bilingual word list) or addi-
tionally (as in LDOCE E-C). Either way, the words 
in L2 glosses can be incorporated into D(s) and 
P(d). For instance, the character unigrams and/or 
overlapping bigrams in the Mandarin glosses of S 
in LDOCE E-C and their appearance counts and 
probability are shown in Table 3. 
 
Table 3. Classes Based Sense Translation Model for 
{difficult-1, hard-2, stiff-6, tough-4, arduous-1, awk-
ward-2} in LDOCE*. 
 
 
We call the part of CBSDM that are involved 
with words written in L2, Class Based Sense 
Translation Model.  CBSTM trained on a thesaurus 
and a bilingual MRD can be exploited to align 
words and translation counter part as well as to 
assign word sense in a parallel corpus. For instance, 
given a pair of aligned sentences in a parallel cor-
pus: 
 
 
(2E) A scholar close to Needham analyses the reasons 
that he was able to achieve this huge work as 
being due to a combination of factors that 
would be hard to find in any other person. 
(Source: 1990, Dec Issue Page 24, Giving Jus-
tice Back to China --Dr. Joseph Needham and 
the History of Science and Civilisation in China) 
It is possible to apply CBSTM to obtain the fol-
lowing pair of translation equivalent, (  [nan], 
?hard?) and, at the same time, determine the in-
tended sense. For instance, we can label the cita-
tion with hard-2LDOCE, leading to the following 
quadruple: 
(3) (hard,  [nan], hard-2
 LDOCE , (2C, 2E)) 
After we have done this for all pairs of word and 
translation counterpart, we would in effect estab-
lish a Bilingual Semantic Concordance (BSC).  
 
2.2 The Model 
We assume that there is a Class Based Sense Defi-
nition Model, which can be viewed as a language 
model that generates the glosses for a class of 
senses S. Assume that we are given L, the words of 
S but not explicitly the intended senses S. In addi-
tion, we are given a sense inventory I in the form 
of an MRD with the regular glosses, which are 
written in L1 and/or L2. We are concerned with 
two problems: (1) Unsupervised training of M, 
CBSDM for S; (2) Determining S by identifying a 
relevant sense in I, if existing, for each word in L. 
Those two problems can be solved based on 
Maximum Likelihood Principle: Finding M and S 
such that M generates the glosses of S with maxi-
mum probability. For that, we utilize the Expecta-
tion and Maximization Algorithm to derive M and 
S through Mutually Assured Resolution of Sense 
Algorithm (MARS) given below: 
Mutual Assured Resolution of Sense Algorithm  
Determine the intended sense for each of a set of seman-
tic related words. 
Input: (1) Class of words L = {w1 w2 ?wn}; 
(2) Sense inventory I. 
Output: (1) Senses S from I for words in L; 
(2) CBSTM M from L1 to L2. 
1. Initially, we assume that each of the senses wi,j, j = 
1, mi in I is equally probable to be in S with prob-
ability 
i
ji,
1),|(
m
LiwP = , j = 1, mi; where mi is 
the number of senses in I for the word wi. 
2. Estimate CBSDM P(d | L) for L , 
,
),(),|(
)|(
kj,i,ji,kj,max
n
ddEQLiwP
LdP i
?
=
where d is a unigram or overlapping bigram in L1 
or L2, di,j,k = the kth word in D(wi,j), and EQ(x, y) 
= 1, if x = y and 0 otherwise; 
3. Re-estimate P(wi,j | i,L) according to di,j,k , k = 1,n i,j : 
,)|P(15.0)|P(5.0),|(P kj,i,
ji,
kj,i,ji,1 max ?+=
k
k Ld
n
LdLiw
?
=
=
i,1
ji,1
ji,1
ji, ),|(P
),|(P),|P(
mj
Liw
Liw
Liw ; 
4. Repeat Steps 2 and 3 until the values of P(d | L) and 
P(wi,j | i, L) converge; 
5. For each i, find the most probable sense wi,j* ,  
j*=argmax
 j P(wi,j | i, L) ; 
6. Output S = { wi,j* | j*=argmax j P(wi,j | i, L)} ; 
7. Estimate and output CBSTM for L, 
n
tcI
LcP ni
?
=
?
=
,1
j*,i )(
)|( , 
where c is a unigram or overlapping bigram in L2 
and ti,j is the L2 gloss of wi,j. 
 
Note that the purpose of Step 2 is to estimate how likely 
a word will appear in the definition of S based on the 
definining word for the senses, wi,j  and relevant prob-
ability P(wi,j | i,L). This likelihood of the word d being 
used to define senses in questions is subsequently used 
to re-estimate P(wi,j | i,L), the likelihood of the jth sense, 
wi,j of wi being in the intended senses of L. 
3. Application to Word Sense Tagging 
Armed with the Class Based Sense Translation 
Model, we can attack the word alignment and 
sense tagging problems simultaneously. Each word 
in a pair of aligned sentences in a parallel corpus 
will be considered and assigned a counterpart 
translation and intended sense in the given context 
through the proposed algorithm below: 
 
Simutaneous Word Alignment and Tagging Algorithm (SWAT) 
Align and sense tag words in a give sentence and trans-
lation. 
Input: (1) Pair of sentences (E, C); 
(2) Word w, POS p in question; 
(3) Sense Inventory I; 
(4) CBSTM, P(c|L). 
Output:  (1) Translation c of w in C; 
(2) Intended sense s for w. 
1. Perform part of speech tagging on E; 
2. Proceed if w with part of speech p is found in the 
results of tagging E; 
3. For all classes L to which (w, p) belongs and all 
words c in C: 
,)|(maxmaxarg*
),(
??
???
?
= LcPL
cwLINKL
( )*)|(maxarg* LcPc
c
= , 
where LINK(x, y) means x and y are two word 
aligned based on Competitive Linking Align-
ment 
4. Output c* as the translation; 
5. Output the sense of w in L* as the intended sense. 
 
To make sense tagging more precise, it is advisable 
to place constraint on the translation counterpart c 
of w. SWAT considers only those translations c 
that has been linked with w based the Competitive 
Linking Algorithm (Melamed 1997) and logarith-
mic likelihood ratio (Dunning 1993). 
 
Table 4. The experimental results of assigning LDOCE 
senses to classes of LLOCE.  
 
 
4. Experiments and evaluation 
In order to assess the feasibility of the proposed 
approach, we carried out experiments and evalua-
tion on an implementation of MARS and SWAT 
based on LDOCE E-C, LLOCE, and Sinorama. 
 
First experiment was involved with the train-
ability of CBSDM and CBSTM via MARS. The 
second experiment was involved with the effec-
tiveness of using SWAT and CBSTM to annotate a 
parallel corpus with sense information. Evaluation 
was done on a set of 14 nouns, verbs, adjectives, 
and adverbs studies in previous work. The set in-
cludes the nouns ?bass,? ?bow,? ?cone,? ?duty,? 
?gallery,? ?mole,? ?sentence,? ?slug,? ?taste,? 
?star,? ?interest,? ?issue,? the adjective ?hard,? 
and the verb ?serve.? 
 
Table 5. Evaluation of the MARS Algorithm based on 
12 nouns, 1 verb, 1 adjective in LDOCE. 
Word Pos #Senses #Done #Correct Prec 
(LB*) 
Prec. 
Bass N 4 1 1 0.25 1.00 
Bow N 5 2 2 0.25 1.00 
Cone N 3 3 2 0.33 0.67 
Duty N 2 2 2 0.13 1.00 
Galley N 3 3 2 0.33 0.67 
Mole N 3 2 2 0.33 1.00 
Sentence N 2 2 2 1.00 1.00 
Slug N 2 2 2 0.20 1.00 
Taste N 6 1 1 0.17 1.00 
Star N 8 2 2 0.13 1.00 
Interest N 6 4 4 0.17 1.00 
Issue N 7 4 3 0.14 0.75 
Serve V 13 4 2 0.08 0.50 
Hard A 12 2 2 0.08 1.00 
Avg.  4.14 1.36 1.29 0.26 0.90 
* The lower bound of precision of picking one sense in random. 
 
Table 6. Experimental results of sense tagging the Sinorama 
parallel Corpus.
 
Word Instance #done #correct Precision 
Star 173 86 82 0.95 
Hard 325 37 33 0.89 
4.1 Experiment 1: Training CBSDM 
We applied MARS to assign LDOCE senses to 
word classes in LLOCE. Some results related to 
the test set are shown in Tables 4. The evaluation 
in Tables indicates that MARS assigns LDOCE 
senses to an LLOCE class with a high average pre-
cision rate of 90%.  
4.2 Experiment 2: Sense Tagging 
We applied SWAT to sense tag English words in 
some 50,000 reliably aligned sentence pairs in Si-
norama parallel Corpus based on LDOCE sense 
inventory. The results are shown in Tables 6. 
Evaluation indicates an average precision rate of 
around 90%.  
 
5. Discussion 
The proposed approach offers a new method for 
automatic learning for the task of word sense dis-
ambiguation. The class based approach attacks the 
problem of tagging and data sparseness in a way 
similar to the Yarowsky approach (1992) based on 
thesaurus categories. We differ from the 
Yarowsky?s approach, in the following ways: 
i. The WSD problem is solved for two languages in-
stead of one within a single sense inventory. Fur-
thermore, an explicit sense tagged corpus is 
produced in the process. 
ii. It is possible to work with any number of sense in-
ventories. 
iii. The method is applicable not only to nouns but 
also to adjectives and verbs, since it does not rely 
on topical context, which is effective only for 
nouns as pointed out by Towell and Voorhees 
(1998). 
The approach is very general and modular and 
can work in conjunction with a number of learning 
strategies for word sense disambiguation 
(Yarowsky, 1995; Li and Li, 2002). 
 
6. Conclusion 
In this paper, we present the Mutual Assured Reso-
lution of Sense (MARS) Algorithm for assigning 
relevant senses to word classes in a given sense 
inventory (i.e. LDOCE or WordNet). We also de-
scribe the SWAT Algorithm for automatic sense 
tagging of a parallel corpus. 
We carried out experiments on an implementa-
tion of the MARS and SWAT Algorithms for all 
the senses in LDOCE and LLOCE. Evaluation on a 
set of 14 highly ambiguous words showed that 
very high precision CBSDM and CBSTM can be 
constructed. High applicability and precision rates 
were achieved, when applying CBSTM to sense 
tagging of a Chinese-English parallel corpus. 
 
A number of interesting future directions pre-
sent themselves. First, it would be interesting to 
see how effectively we can broaden the coverage 
of CBSTM via backing off smoothing. Second, a 
CBSTM trained directly on a parallel corpus would 
be more effective in word alignment and sense 
tagging. The approach of training CBSTM on the 
L2 glosses in a bilingual MRD may lead to occa-
sional mismatch between MRD translations and in-
context translations. Third, there is a lack of re-
search for a more abstractive and modular repre-
sentation of sense differences and commonality. 
There is potential of developing Sense Definition 
Model to identify and represent semantic and sty-
listic differentiation reflected in the MRD glosses 
pointed out in DiMarco, Hirst and Stede (1993). 
Last but not the least, it would be interesting to 
apply MARS to both LDOCE E-C and WordNet 
and project WordNet?s sense inventory to a sen-
cond language via CBSDM and a parallel corpus, 
thus creating a Chinese WordNet and semantic 
concordance. 
Acknowledgement  
We acknowledge the support for this study through 
grants from National Science Council and Ministry 
of Education, Taiwan (NSC 90-2411-H-007-033-
MC and MOE EX-91-E-FA06-4-4). 
References 
Dagan, Ido; A. Itai, and U. Schwall (1991). Two lan-
guages are more informative than one. Proceedings 
of the 29th Annual Meeting of the Association for 
Computational Linguistics, 18-21 June 1991, Berke-
ley, California, 130-137. 
Dempster, A., N. Laird, and D. Rubin (1977). Maxi-
mum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society, 
Series B, 39(1):1?38. 
Diab, M. and  P. Resnik, (2002). An Unsupervised 
Method for Word Sense Tagging using Parallel Cor-
pora, Proceedings of ACL, 255-262. 
DiMarco, C., G. Hirst, M. Stede, (1993). "The semantic 
and stylistic differentiation of synonyms and near-
synonyms." In: Working notes of the AAAI Spring 
Symposium on Building Lexicons for Machine Trans-
lation. Stanford University. 
Dunning, T (1993) Accurate methods for the statistics 
of surprise and coincidence, Computational Linguis-
tics 19:1, 61-75. 
Gale, W., K. Church, and D. Yarowsky, (1992). Using 
Bilingual Materials to Develop Word Sense Disam-
biguation Methods. In Proceedings, Fourth Interna-
tional Conference on Theoretical and 
Methodological Issues in Machine Translation. 
Montreal, 101-112, 1992. 
Ide, N. and J. V?ronis (1998). Word sense disambigua-
tion: The state of the art. Computational Linguistics, 
24:1, 1-40. 
Knight, K, and A. Luk, (1994). Building a Large-Scale 
Knowledge Base for Machine Translation, Proc. of 
the National Conference on Artificial Intelligence 
(AAAI). 
Knight, K., I. Chander, M. Haines, V. Hatzivassiloglou, 
E. Hovy, M. Iida, S. Luk, A. Okumura, R. Whitney, 
K. Yamada, (1994). "Integrating Knowledge Bases 
and Statistics in MT, Proc. of the Conference of the 
Association for Machine Translation in the Americas 
(AMTA). 
Leacock, C., G. Towell, and E. Voorhees (1993). Cor-
pus-based statistical sense resolution. Proceedings of 
the ARPA Human Language Technology Worskshop, 
San Francisco, Morgan Kaufman. 
Li, C, and H. Li (2002). Word Translation Disambigua-
tion Using Bilingual Bootstrapping, Proceedings of 
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Philadelphia, July 2002, 
343-351. 
Li, Juanzi and C. Huang (1999). A Model for Word 
Sense Disambiguation. In Computational Linguistics 
and Chinese Language Processing,4(2), August 1999, 
pp.1-20 
McArthur, T. (1992) Longman Lexicon of Contempo-
rary English, Longman Group (Far East) Ltd., Hong 
Kong. 
Mei, J. J., et al (1984) Tongyici Cilin, Shanghai, Com-
mercial Press. (in Chinese) 
Melamed, I.D. (1997). "A Word-to-Word Model of 
Translational Equivalence". In Procs. of the ACL97. 
pp 490-497. Madrid Spain.  
Merialdo, B, (1994). Tagging English Text with a 
Probabilistic Model, Computational Linguistics, 
20(2):155-171. 
Miller, G., A, R.. Beckwith, C. Fellbaum, D. Gross and 
K.J. Miller. (1990). WordNet: An on-line lexical da-
tabase. International Journal of Lexicography, 3(4), 
235- 244. 
Proctor, P. (1988) Longman English-Chinese Dictionary 
of Contemporary English, Longman Group (Far East) 
Ltd., Hong Kong. 
Towell, G. and E. Voorhees. (1998) Disambiguating 
Highly Ambiguous Words. Computational Linguis-
tics, vol. 24, no. 1, 125-146. 
Yarowsky, D. (1992). Word sense disambiguation using 
statistical models of Roget's categories trained on 
large corpora. Proceedings of the 14th International 
Conference on Computational Linguistics, 
COLING'92, 23-28 August, Nantes, France, 454-460. 
Yarowsky, D. (1995). Unsupervised word sense disam-
biguation rivaling supervised methods. Proceedings 
of the 33rd Annual Meeting of the Association for 
Computational Linguistics, 189-196 
 
Improving Word Alignment by Adjusting Chinese Word Segmentation
Ming-Hong Bai1,2 Keh-Jiann Chen1 Jason S. Chang2
1 Institute of Information Science, Academia Sinica 
2 Department of Computer Science, National Tsing-Hua University 
mhbai@sinica.edu.tw kchen@iis.sinica.edu.tw jschang@cs.nthu.edu.tw
 
Abstract  
Most of the current Chinese word 
alignment tasks often adopt word 
segmentation systems firstly to identify 
words. However, word-mismatching 
problems exist between languages and will 
degrade the performance of word 
alignment. In this paper, we propose two 
unsupervised methods to adjust word 
segmentation to make the tokens 1-to-1 
mapping as many as possible between the 
corresponding sentences. The first method 
is learning affix rules from a bilingual 
terminology bank. The second method is 
using the concept of impurity measure 
motivated by the decision tree. Our 
experiments showed that both of the 
adjusting methods improve the 
performance of word alignment 
significantly. 
1 Introduction 
Word alignment is an important preprocessing task 
for statistical machine translation. There have been 
many statistical word alignment methods proposed 
since the IBM models have been introduced. Most 
existing methods treat word tokens as basic 
alignment units (Brown et al, 1993; Vogel et al, 
1996; Deng and Byrne, 2005), however, many 
languages have no explicit word boundary markers, 
such as Chinese and Japanese. In these languages, 
word segmentation (Chen and Liu, 1992; Chen and 
Bai, 1998; Chen and Ma, 2002; Ma and Chen, 
2003; Gao et al, 2005) is often carried out firstly 
to identify words before word alignment (Wu and 
Xia, 1994). However, the differences in 
lexicalization may degrade word alignment 
performance, for different languages may realize 
the same concept using different numbers of words 
(Ma et al, 2007; Wu, 1997). For instance, Chinese 
multi-syllabic words composed of more than one 
meaningful morpheme which may be translated to 
several English words. For example, the Chinese 
word ??? is composed of two meaning units, 
?? and ?, and is translated to Department of 
Education in English. The morphemes ?? and ? 
have their own meanings and are translated to 
Education and Department respectively. The 
phenomenon of lexicalization mismatch will 
degrade the performance of word alignment for 
several reasons. The first reason is that it will 
reduce the cooccurrence counts of Chinese and 
English tokens. Consider the previous example. 
Since ??? is treated as a single unit, it does not 
contribute to the occurrence counts of Education/
?? and Department/? token pairs. Secondly, the 
rarely occurring compound word may cause the 
garbage collectors effect (Moore, 2004; Liang et 
al., 2006), aligning a rare word in source language 
to too many words in the target language, due to 
the frequency imbalance with the corresponding 
translation words in English (Lee, 2004). Finally, 
the IBM models (Moore, 2004) impose the 
limitation that each word in the target sentence can 
be generated by at most one word in the source 
sentence. In this case, a many-to-one alignment, 
links a phrase in the source sentence to a single 
token in the target sentence, is not allowed, forcing 
most links of a phrase in the source sentence to be 
abolished. As in the previous example, when 
aligning from English to Chinese, ??? can only 
be linked to one of the English words, say 
Education, because of the limitation of the IBM 
model. However for remedy, many of the current 
word alignment methods combine the results of 
both alignment directions, via intersection or 
249
grow-diag-final heuristic, to improve the alignment 
reliability (Koehn et al, 2003; Liang et al, 2006; 
Ayan et al, 2006; DeNero et al, 2007). However 
the many-to-one link limitation will undermine the 
reliability due to the fact that some links are not 
allowed in one of the directions. 
In this paper, we propose two novel methods to 
adjust word segmentation so as to decrease the 
effect of lexicalization differences to improve word 
alignment performance. The main idea of our 
methods is to adjust Chinese word segmentation 
according to their translation derived from parallel 
sentences in order to make the tokens compatible 
to 1-to-1 mapping between the corresponding 
sentences. The first method is based on learning a 
set of affix rules from bilingual terminology bank, 
and adjusting the segmentation according to these 
affix rules when preprocessing the Chinese part of 
the parallel corpus. The second method is based on 
the so-called impurity measure, which was 
motivated by the decision tree (Duda et al, 2001). 
 
2 Related Works 
Our methods are motivated by the translation-
driven segmentation method proposed by Wu 
(1997) to segment words in a way to improve word 
alignment. However, Wu's method needs a 
translation lexicon to filter out the links which 
were not in the lexicon and the result was only 
evaluated on the sentence pairs which were 
covered by the lexicon.  
A word packing method has been proposed by 
Ma et al (2007) to improve the word alignment 
task. Before carrying out word alignment, this 
method packs several consecutive words together 
when those words believed to correspond to a 
single word in the other language. Our basic idea is 
similar to this, but on the contrary, we try to 
unpack words which are translations of several 
words in the other language. Since the word 
packing method treats the packed consecutive 
words as a single token, as we mentioned in the 
previous section, it weakens the association 
strength of translation pairs of their morphemes 
while applying the IBM word alignment model. 
A lot of morphological analysis methods have 
been proposed to improve the performance of word 
alignment for inflectional language (Lee et al, 
2003; Lee, 2004; Goldwater, 2005). They proposed 
to split a word into a morpheme sequence of the 
pattern prefix*-stem-suffix* (* denotes zero or 
more occurrences of a morpheme). Their 
experiments showed that morphological analysis 
can improve the quality of machine translation by 
reducing data sparseness and by making the tokens 
in two languages correspond more 1-to-1. 
However, these segmentation methods were 
developed from the monolingual perspective. 
3 Adjusting Word Segmentation 
The goal of word segmentation adjustment is to 
adjust the segmentation of Chinese words such that 
we have as many 1-to-1 links to the English words 
as possible. In this task, we will face the problem 
of finding the proper morpheme boundaries for 
Chinese words. The challenge is that almost all 
characters of Chinese are morphemes and therefore 
almost every character boundary in a word could 
be the boundary of a morpheme, there is no simple 
rules to find the suitable boundaries of morphemes. 
Furthermore, not all meaningful morphemes need 
to be segmented to meet the requirement of 1-to-1 
mapping. For example, washing machine/???
can be segmented into ?? and ? corresponding 
to washing and machine while heater/??? does 
not need, it depends on their translations.  
In this paper, we have proposed two different 
methods to solve this problem: 1. learning affix 
rules from terminology bank to segment 
morphemes and 2. using impurity measure to 
finding the morpheme boundaries. The detail of 
these methods will be described in the following 
sections. 
4 Affix Rule Method 
The main idea of this method is to segment a 
Chinese word according to some properly designed 
conditional dependent affix rules. As shown in 
Figure 1, each rule is composed of three 
conditional constraints, a) affix condition, b) 
English word condition and c) exception condition. 
In the affix condition, we place a underscore on the 
left of a morpheme, such as _?, to denote a suffix 
and on the right, such as ?_, to denote a prefix. 
The affix rules are applied to each word by 
checking the following three conditions:  
1. The target word has the affix. 
250
2. The English word which is the target of 
translation exists in the parallel sentence. 
3. The target word does not contain the 
morphemes in the exception list (The 
morpheme in the exception list shows an 
alternative segmentation.). 
 
If the target word satisfies all of the above 
conditions of any rule, then the morpheme should 
be separated from the word. The remaining 
problem will be how to derive the set of affix rules. 
 
affix English word exception
_? machine  
_? engine  
?_ vice  
?_ deputy ?? 
_? industry ?? 
Figure 1.  Samples of affix rules. 
 
4.1 Training Data 
We use an unsupervised method to extract affix 
rules from a Chinese-English terminology bank1. 
The bilingual terminology bank a total of 
1,046,058 English terms with Chinese transla-
tions in 63 categories. Among them, 60% or 
629,352 terms are compounds. We take the 
advantage of the terminology bank, that all 
terminologies are 1-to-1 well translated, to find the 
best morpheme segmentation from ambiguous 
segmentations of a Chinese word according to its 
English counterpart. Then we extracted affix rules 
from the word-to-morpheme alignment results of 
terms and translation.  
 
4.2 Word-to-Morpheme Alignment 
The training phase of word-to-morpheme 
alignment is based loosely on word-to-word 
alignment of the IBM model 1. Instead of using 
Chinese words, we considered all the possible 
morphemes. For example, consider the task of 
aligning Department of Education and ??? as 
                                                 
1 The bilingual terminology bank was compiled by the Na-
tional Institute for Compilation and Translation. It is freely 
download at http://terms.nict.gov.tw by registering your in-
formation. 
shown as Figure 2. We use the EM algorithm to 
train the translation probabilities of word-
morpheme pairs based on IBM model 1.  
 
 
Figure 2. Example of word-to-morpheme 
alignment. 
 
In the aligning phase, the original IBM model 1 
does not work properly as we expected. Because 
the English words prefer to link to single character 
and it results that some correct Chinese translations 
will not be linked. The reason is that the 
probability of a morpheme, say p(??|education), 
is always less than its substring, p(?|education), 
since whatever ?? occurs ? and ? always 
occur but not vice versa. So the aligning result will 
be ? /Education and ? /Department, ?  is 
abandoned. To overcome this problem, a constraint 
of alignment is imposed to the model to ensure that 
the aligning result covers every Chinese characters 
of a target word and no overlapped characters in 
the result morpheme sequence. For instances, both
? /Education    ? /Department and ? ?
/Education    ??/Department are not allowed 
alignment sequences. The constraint is applied to 
each possible aligning result. If the alignment 
violates the constraint, it will be rejected.  
Since the new alignment algorithm must 
enumerate all of the possible alignments, the 
process is very time consuming. Therefore, it is 
advantageous to use a bilingual terminology bank 
rather than a parallel corpus. The average length of 
terminologies is short and much shorter than a 
typical sentence in a parallel corpus. This makes 
words to morphemes alignment computationally 
feasible and the results highly accurate (Chang et 
al., 2001; Bai et al, 2006). This makes it possible 
to use the result as pseudo gold standards to 
evaluate affix rules as described in section 4.3. 
 
 
 
 
251
 
air|?? refrigeration|?? machine|? 
building|?? industry|? 
compound|?? steam|?? engine|? 
electronics|?? industry|? 
vice|? chancellor|?? 
Figure 3. Sample of word-to-morpheme alignment. 
4.3 Rule Extraction 
After the alignment task, we will get a word-to-
morpheme aligned terminology bank as shown in 
Figure 3. We can subsequently extract affix rules 
from the aligned terminology bank by the 
following steps: 
 
1) Generate candidates of affix rule: 
For each alignment, we produce all alignment 
links as affix rules. For instance, with 
(electronics|??  industry|? ), we would 
produce two rules: 
 
     (a) ??_, electronics 
     (b) _?, industry 
 
2) Evaluate the rules: 
The precision of each candidate rule is 
estimated by applying the rule to segment the 
Chinese terms. If a Chinese term contains the 
affix shown in the rule, the affix will be 
segmented. The results of segmentation are 
then to compare with the segmentation results 
of the alignments done by the algorithm of the 
section 4.2 as pseudo gold standards. Some 
example results of rule evaluations are shown 
in Figure 4.    
 
affix English word 
Rule 
Applied  
Correct 
segments precision
?_ master 458 378 0.825 
??_ periodic 130 100 0.769 
??_ video 46 40 0.870 
_? chain 147 107 0.728 
_? box 716 545 0.761 
Figure 4. Sample evaluations of candidate rules. 
3) Adding exception condition: 
In the third step, we sort the rules according to 
their precision rates in descending order, 
resulting in rules R1..Rn . And then for each Ri , 
we scan R1 to Ri-1, if there is a rule, Rj, have 
the same English word condition and the affix 
condition of Ri subsume that of Rj, then we 
add affix condition of Rj as exception 
condition of Ri. For example, _? , industry 
and _??, industry are rule candidates in the 
sorted table and have the same English word 
condition. Furthermore, the condition _? 
subsumes that of ??, we add ?? to the 
exception condition of the rule with a shorter 
affix. 
 
4) Reevaluate the rules with exception 
condition: 
After adding the exception conditions, the 
rules are reevaluated with considering the ex-
ception condition to get new evaluation scores. 
 
5) Select rules by scores: 
Finally, filter out the rules with scores lower 
than a threshold2. 
 
The reason of using exception condition is that 
an affix is usually an abbreviation of a word, such 
as _? is an abbreviation of ??. In general, a full 
morpheme is preferred to be segmented than its 
abbreviation while both occurred in a target word. 
For example, when applying rules to ????
/electronic industry, _?? ,industry is preferred 
than _?,industry. However, in the evaluation step, 
precision rate of _?,industry will be reduced when 
applying to full morphemes, such as ????
/electronic industry, and then could be filtered out 
if the precision is lower than the threshold.  
5 Impurity Measure Method 
The impurity measure was used by decision tree 
(Duda et al, 2001) to split the training examples 
into smaller and smaller subsets progressively 
according to features and hope that all the samples 
in each subset is as pure as possible. For 
convenient, they define the impurity function 
rather than the purity function of a subset as 
follows:  
 ??=
j
jj wPwPSimpurity )(log)()( 2  
                                                 
2 We set the threshold as 0.7.  
252
                     
 
(a) impurity value of ????.                   (b) impurity values of ?? and ??. 
Figure 5. Examples of impurity values. 
 
Where P(wj) is the fraction of examples at set S 
that are in category wj. By the well-known 
properties of entropy if all the examples are of the 
same category the impurity is 0; otherwise it is 
positive, with the greatest value occurring when 
the different classes are equal likely.  
5.1 Impurity Measure of Translation 
In our experiment, the impurity measure is used 
to split a Chinese word into two substrings and 
hope that all the characters in a substring are 
generated by the parallel English words as pure as 
possible. Here, we treat a Chinese word as a set of 
characters, the parallel English words as categories 
and the fraction of examples is redefined by the 
expected fraction number of characters that are 
generated by each English word. So we redefine 
the entropy impurity as follows: 
 
);|(log);|();( 2 fe,fe,fe,
e
efcefcfI
e
E ?
??
?=
In which f denotes the target Chinese word, e and f 
denote the parallel English and Chinese sentence 
that f belongs to and   is the expected 
fraction number of characters in f that are 
generated by word e. The expected fraction 
number can be defined as follows: 
);|( fe,efc
??
?
?? ??
??=
e
fe,
e fc
fc
ecp
ecp
efc
)|(
)|(
);|(  
Where p(c | e) denotes the translation probability 
of Chinese character c given English word e. 
 
For example, as shown in Figure 5, the impurity 
value of ????, Figure 5.(a), is much higher 
than values of ?? and ??, Figure 5.(b). Which 
means that the generating relations from English to 
Chinese tokens are purified by breaking ???? 
into ?? and ??.   
The translation probabilities between Chinese 
characters and English word can be trained using 
IBM model 1 by treating Chinese characters as 
tokens. 
5.2 Target Word Selection 
In this experiment, we treat the Chinese words 
which can be segmented into morphemes and 
linked to different English words as target words. 
In order to speedup our impurity method only tar-
get words will be segmented during the process. 
Therefore we investigate the actual distribution of 
target words first, we have tagged 1,573 Chinese 
words manually with target and non-target. It turns 
out that only 6.87% of the Chinese words are 
tagged as target and 94.4% of target words are 
nouns. The results show that most of the Chinese 
words do not need to be re-segmented and their 
POS distribution is very unbalanced. The results 
show that we can filter out the non-target words by 
simple clues. In our experiment, we use three fea-
tures to filter out non-target words: 
 
1) POS: Since 94.4% of the target words are 
nouns, we focus our experiment on nouns 
and filter out words with other POS.  
2) One-to-many alignment in GIZA++:  Only 
Chinese words which are linked to multiple 
English words in the result of GIZA++ are 
considered to be target words. 
3) Impurity measure: the target words are ex-
pected to have high impurity values. So the 
words with a impurity values larger than a 
threshold are selected as target words3. 
                                                 
3 In our experiment, we use 0.3 as our threshold. 
253
5.3 Best Breaking Point and we used these annotated data as our gold 
standard in testing.  The goal of segmentation adjustment using 
impurity is to find the best breaking point of a 
Chinese word according to parallel English words. 
When a word is broken into two substrings, the 
new substrings can be compared to original word 
by the information gain which is defined in terms 
of impurity as follows: 
Because of the modification of Chinese tokens 
caused by the word segmentation adjustment, a 
problem has been created when we wanted to 
compare the results to the copy which did not 
undergo adjustment. Therefore, after the alignment 
was done, we merged the alignment links related to 
tokens that were split up during adjustment. For 
example, the two links of foreign/?? minister/?
? were merged as foreign minister/????. 
),;(
2
1
),;(
2
1
),;(    
),,(
11
11
fefefe niE
i
EE
n
i
i
fIfIfI
fffIG
+
+
??
=
  
The evaluation of word alignment results are 
shown in Table 1, including precision-recall and 
AER evaluation methods. In which the baseline is 
alignment result of the unadjusted data. The table 
shows that after the adjustment of word 
segmentation, both methods obtain significant 
improvement over the baseline, especially for the 
English-Chinese direction and the intersection 
results of both directions. The impurity method in 
particular improves alignment in both English-
Chinese and Chinese-English directions.  
Where i denotes a break point in f,  denotes 
first i characters of f, and  denotes last n-i 
characters of f. If the information gain of a 
breaking point is positive, the result substrings are 
considered to be better, i.e. more pure than original 
word.  
if1
n
if 1+
The goal of finding the best breaking point can 
be achieved by finding the point which maximizes 
the information gain as the following formula: 
The improvement of intersection of both 
directions is important for machine translation. 
Because the intersection result has higher precision, 
a lot of machine translation method relies on 
intersecting the alignment results. The phrase-
based machine translation (Koehn et al, 2003) 
uses the grow-diag-final heuristic to extend the 
word alignment to phrase alignment by using the 
intersection result. Liang (Liang et al, 2006) has 
proposed a symmetric word alignment model that 
merges two simple asymmetric models into a 
symmetric model by maximizing a combination of 
likelihood and agreement between the models. 
This method uses the intersection as the agreement 
of both models in the training time. The method 
has reduced the alignment error significantly over 
the traditional asymmetric models.  
),,(maxarg 111
n
i
i
ni
fffIG +<?  
Note that a word can be separated into two 
substrings each time. If we want to segment a 
complex word composed of many morphemes, just 
split the word again and again like the construction 
of decision tree, until the information gain is 
negative or less than a threshold4. 
6 Experiments 
In order to evaluate the effect of our methods on 
the word alignment task, we preprocessed parallel 
corpus in three ways: First we use a state-of-the-art 
word segmenter to tokenize the Chinese part of the 
corpus. Then, we used the affix rules to adjust 
word segmentation. Finally, we do the same but by 
using the impurity measure method.  We used the 
GIZA++ package (Och and Ney, 2003) as the word 
alignment tool to align tokens on the three copies 
of preprocessed parallel corpora.  
In order to analyze the adjustment results, we 
also manually segment and link the words of 
Chinese sentences to make the alignments 1-to-1 
mapping as many as possible according to their 
translations for the 112 gold standard sentences.  
Table 2 shows the results of our analysis, the 
performance of impurity measure method is also 
slightly better than the affix rules in both recall and 
precision measure. 
We used the first 100,000 sentences of Hong 
Kong News parallel corpus from LDC as our 
training data. And 112 randomly selected parallel 
sentences were aligned manually with sure and 
possible tags, as described in (Och and Ney, 2000), 
                                                 
4 In our experiment, we set 0 as the threshold. 
254
 
 direction Recall precision F-score AER 
English-Chinese 68.3 61.2 64.6 35.7 
Chinese-English 79.6 67.0 72.8 27.8 baseline 
intersection 59.9 92.0 72.6 26.6 
English-Chinese 78.2 64.6 70.8 29.8 
Chinese-English 80.2 68.0 73.6 27.0 affix rules 
intersection 69.1 92.3 79.0 20.2 
English-Chinese 78.1 64.9 70.9 29.7 
Chinese-English 81.4 70.4 75.5 25.0 impurity 
intersection 70.2 91.9 79.6 19.8 
Table 1. Alignment results based on the standard word segmentation data. 
 
 recall precision 
affix rules 82.35 66.66 
impurity 84.31 67.72 
Table 2. Alignment results based on the manual 
word segmentation data. 
 
7 Conclusion 
In this paper, we have proposed two Chinese word 
segmentation adjustment methods to improve word 
alignment. The first method uses the affix rules 
learned from a bilingual terminology bank and 
then applies the rules to the parallel corpus to split 
the compound Chinese words into morphemes ac-
cording to its counterpart parallel sentence. The 
second method uses the impurity method, which 
was motivated by the method of decision tree. The 
experimental results show that both methods lead 
to significant improvement in word alignment per-
formance. 
 
Acknowledgements: This research was supported 
in part by the National Science Council of Taiwan 
under NSC Grants: NSC95-2422-H-001-031. 
References  
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going 
Beyond AER: An Extensive Analysis of Word 
Alignments and Their Impact on MT. In Proceedings 
of ACL 2006, pages 9-16, Sydney, Australia. 
Ming-Hong Bai, Keh-Jiann Chen and Jason S. Chang. 
2006. Sense Extraction and Disambiguation for 
Chinese Words from Bilingual Terminology Bank. 
Computational Linguistics and Chinese Language 
Processing, 11(3):223-244. 
Petter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The 
Mathematics of Machine Translation: Parameter 
Estimation. Computational Linguistics, 19(2):263-
311.  
Jason S Chang, David Yu, Chun-Jun Lee. 2001. Statisti-
cal Translation Model for Phrases(in Chinese). Com-
putational Linguistics and Chinese Language Proc-
essing, 6(2):43-64. 
Keh-Jiann Chen, Ming-Hong Bai. 1998. Unknown 
Word Detection for Chinese by a Corpus-based 
Learning Method. International Journal of 
Computational linguistics and Chinese Language 
Processing, 1998, Vol.3, #1, pages 27-44.  
Keh-Jiann Chen, Wei-Yun Ma. 2002. Unknown Word 
Extraction for Chinese Documents. In Proceedings of 
COLING 2002, pages 169-175, Taipei, Taiwan.  
Keh-Jiann Chen, Shing-Huan Liu. 1992. Word 
Identification for Mandarin Chinese Sentences. In 
Proceedings of 14th COLING, pages 101-107.  
John DeNero, Dan Klein. 2007. Tailoring Word 
Alignments to Syntactic Machine Translation. In 
Proceedings of ACL 2007, pages 17-24, Prague, 
Czech Republic. 
Yonggang Deng, William Byrne. 2005. HMM word and 
phrase alignment for statistical machine translation. 
In Proceedings of HLT-EMNLP 2005, pages 169-176, 
Vancouver, Canada.  
Richard O. Duda, Peter E. Hart, David G. Stork. 2001. 
Pattern Classification. John Wiley & Sons, Inc.  
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning 
Huang.  2005. Chinese word segmentation and 
named entity recognition: a pragmatic approach. 
Computational Linguistics, 31(4) 
Sharon Goldwater, David McClosky. 2005. Improving 
Statistical MT through Morphological Analysis. In 
255
Proceedings of HLT/EMNLP 2005, pages 676-683, 
Vancouver, Canada.  
Philipp Koehn, Franz J. Och, Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings of 
HLT/NAACL 2003, pages 48-54, Edmonton, Canada. 
Young-Suk Lee. 2004. Morphological Analysis for 
Statistical Machine Translation. In Proceedings of 
HLT-NAACL 2004, pages 57-60, Boston, USA.  
Young-Suk Lee, Kishore Papineni, Salim Roukos. 2003. 
Language Model Based Arabic Word Segmentation. 
In Proceedings of ACL 2003, pages 399-406, 
Sapporo, Japan.  
Percy Liang, Ben Taskar, Dan Klein. 2006. Alignment 
by Agreement. In Proceedings of HLT-NAACL 2006, 
pages 104-111, New York, USA.  
Wei-Yun Ma, Keh-Jiann Chen. 2003. A Bottom-up 
Merging Algorithm for Chinese Unknown Word 
Extraction. In Proceedings of ACL 2003, Second 
SIGHAN Workshop on Chinese Language Processing, 
pp31-38, Sapporo, Japan.  
Yanjun Ma, Nicolas Stroppa, Andy Way. 2007. 
Bootstrapping Word Alignment via Word Packing. In 
Proceedings of ACL 2007, pages 304-311, Prague, 
Czech Republic.  
Robert C. Moore. 2004. Improving IBM Word-
Alignment Model 1. In Proceedings of ACL 2004, 
pages 519-526, Barcelona, Spain.  
Franz Josef Och, Hermann Ney. A Systematic 
Comparison of Various Statistical Alignment Models, 
Computational Linguistics, volume 29, number 1, pp. 
19-51 March 2003. 
Franz J. Och, Hermann Ney., Improved Statistical 
Alignment Models, In Proceedings of the 38th An-
nual Meeting of the Association for Computational 
Linguistics, 2000, Hong Kong, pp. 440-447. 
Stefan Vogel, Hermann Ney, Christoph Tillmann. 1996. 
HMM-based word alignment in statistical translation. 
In Proceedings of COLING 1996, pages 836-841, 
Copenhagen, Denmark.  
Dekai Wu, Xuanyin Xia. 1994. Learning an English-
Chinese Lexicon from a Parallel Corpus. In 
Proceedings of AMTA 1994, pages 206-213, 
Columbia, MD.  
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3):377-403.  
256
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 928?937,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Ambiguity Resolution for Vt-N Structures in Chinese 
 
 
Yu-Ming Hsieh1,2       Jason S. Chang2       Keh-Jiann Chen1 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
morris@iis.sinica.edu.tw, jason.jschang@gmail.com 
kchen@iis.sinica.edu.tw 
 
  
 
Abstract 
The syntactic ambiguity of a transitive 
verb (Vt) followed by a noun (N) has 
long been a problem in Chinese parsing. 
In this paper, we propose a classifier to 
resolve the ambiguity of Vt-N structures. 
The design of the classifier is based on 
three important guidelines, namely, 
adopting linguistically motivated features, 
using all available resources, and easy in-
tegration into a parsing model. The lin-
guistically motivated features include 
semantic relations, context, and morpho-
logical structures; and the available re-
sources are treebank, thesaurus, affix da-
tabase, and large corpora. We also pro-
pose two learning approaches that resolve 
the problem of data sparseness by auto-
parsing and extracting relative 
knowledge from large-scale unlabeled 
data. Our experiment results show that 
the Vt-N classifier outperforms the cur-
rent PCFG parser. Furthermore, it can be 
easily and effectively integrated into the 
PCFG parser and general statistical pars-
ing models. Evaluation of the learning 
approaches indicates that world 
knowledge facilitates Vt-N disambigua-
tion through data selection and error cor-
rection. 
1 Introduction 
In Chinese, the structure of a transitive verb (Vt) 
followed by a noun (N) may be a verb phrase 
(VP), a noun phrase (NP), or there may not be a 
dependent relation, as shown in (1) below. In 
general, parsers may prefer VP reading because a 
transitive verb followed by a noun object is nor-
mally a VP structure. However, Chinese verbs 
can also modify nouns without morphological 
inflection, e.g., ?? /farming ? /pond. Conse-
quently, parsing Vt-N structures is difficult be-
cause it is hard to resolve such ambiguities with-
out prior knowledge. The following are some 
typical examples of various Vt-N structures:  
1) 
??/solve ??/problem ? VP 
??/solving ??/method ? NP 
??/solve ??/mankind (??/problem)?None 
To find the most effective disambiguation fea-
tures, we need more information about the Vt-N 
? NP construction and the semantic relations 
between Vt and N. Statistical data from the Sini-
ca Treebank (Chen et al., 2003) indicates that 
58% of Vt-N structures are verb phrases, 16% 
are noun phrases, and 26% do not have any de-
pendent relations. It is obvious that the semantic 
relations between a Vt-N structure and its con-
text information are very important for differen-
tiating between dependent relations. Although 
the verb-argument relation of VP structures is 
well understood, it is not clear what kind of se-
mantic relations result in NP structures. In the 
next sub-section, we consider three questions: 
What sets of nouns accept verbs as their modifi-
ers? Is it possible to identify the semantic types 
of such pairs of verbs and nouns? What are their 
semantic relations? 
1.1 Problem Analysis 
Analysis of the instances of NP(Vt-N) structures 
in the Sinica Treebank reveals the following four 
types of semantic structures, which are used in 
the design of our classifier. 
 
Type 1. Telic(Vt) + Host(N): Vt denotes the 
telic function (purpose) of the head noun N, e.g., 
928
?? /research ?? /tool; ?? /explore ?
/machine; ?/gamble ?/house; ??/search ?
?/program. The telic function must be a salient 
property of head nouns, such as tools, buildings, 
artifacts, organizations and people. To identify 
such cases, we need to know the types of nouns 
which take telic function as their salient property. 
Furthermore, many of the nouns are monosyl-
labic words, such as ?/people, ?/instruments, 
?/machines. 
Type 2. Host-Event(Vt) + Attribute(N): 
Head nouns are attribute nouns that denote the 
attributes of the verb, e.g., ??/research ??
/method (method of research); ??/attack ??
/strategy (attacking strategy); ??/write ??
/context (context of writing); ?/gamble ?/rule 
(gambling rules). An attribute noun is a special 
type of noun. Semantically, attribute nouns de-
note the attribute types of objects or events, such 
as weight, color, method, and rule. Syntactically, 
attribute nouns do not play adjectival roles (Liu, 
2008). By contrast, object nouns may modify 
nouns. The number of attributes for events is 
limited. If we could discover all event-attribute 
relations, then we can solve this type of construc-
tion. 
Type 3. Agentive + Host: There is only a lim-
ited number of such constructions and the results 
of the constructions are usually ambiguous, e.g., 
??/fried rice (NP), ??/shouting sound. The 
first example also has the VP reading. 
Type 4. Apposition + Affair: Head nouns are 
event nouns and modifiers are verbs of apposi-
tion events, e.g. ??/collide ??/accident, ?
? /destruct ?? /movement, ?? /hate ??
/behavior. There is finite number of event nouns.  
 
Furthermore, when we consider verbal modi-
fiers, we find that verbs can play adjectival roles 
in Chinese without inflection, but not all verbs 
play adjectival roles. According to Chang et al. 
(2000) and our observations, adjectival verbs are 
verbs that denote event types rather than event 
instances; that is, they denote a class of events 
which that are concepts in an upper-level ontolo-
gy. One important characteristic of adjectival 
verbs is that they have conjunctive morphologi-
cal structures, i.e., the words are conjunct with 
two nearly synonymous verbs, e.g., ?/study ?
/search (research), ? /explore ? /detect (ex-
plore), and ?/search ?/find (search). Therefore, 
we need a morphological classifier that can de-
tect the conjunctive morphological structure of a 
verb by checking the semantic parity of two 
morphemes of the verb. 
Based on our analysis, we designed a Vt-N 
classifier that incorporates the above features to 
solve the problem. However, there is a data 
sparseness problem because of the limited size of 
the current Treebank. In other words, Treebank 
cannot provide enough training data to train a 
classifier properly. To resolve the problem, we 
should mine useful information from all availa-
ble resources. 
The remainder of this paper is organized as 
follows. Section 2 provides a review of related 
works. In Section 3, we describe the disambigua-
tion model with our selected features, and intro-
duce a strategy for handling unknown words. We 
also propose a learning approach for a large-
scale unlabeled corpus. In Section 4, we report 
the results of experiments conducted to evaluate 
the proposed Vt-N classifier on different feature 
combinations and learning approaches. Section 5 
contains our concluding remarks. 
2 Related Work 
Most works on V-N structure identification focus 
on two types of relation classification: modifier-
head relations and predicate-object relations (Wu, 
2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; 
Yu et al., 2008). They exclude the independent 
structure and conjunctive head-head relation, but 
the cross-bracket relation does exist between two 
adjacent words in real language. For example, if 
???/all over  ??/world ? was included in the 
short sentence ???/all over  ??/world ??
/countries?, it would be an independent structure. 
A conjunctive head-head relation between a verb 
and a noun is rare. However, in the sentence ??
? ?? ? ? ??? (Both service and equip-
ment are very thoughtful.), there is a conjunctive 
head-head relation between the verb ? ?
/service and the noun ??/equipment. Therefore, 
we use four types of relations to describe the V-
N structures in our experiments. The symbol 
?H/X? denotes a predicate-object relation; ?X/H? 
denotes a modifier-head relation; ?H/H? denotes 
a conjunctive head-head relation; and ?X/X? de-
notes an independent relation. 
Feature selection is an important task in V-N 
disambiguation. Hence, a number of studies have 
suggested features that may help resolve the am-
biguity of V-N structures (Zhao and Huang, 1999; 
Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 
2005; Chen, 2008). Zhao and Huang used lexi-
cons, semantic knowledge, and word length in-
929
formation to increase the accuracy of identifica-
tion. Although they used the Chinese thesaurus 
CiLin (Mei et al., 1983) to derive lexical seman-
tic knowledge, the word coverage of CiLin is 
insufficient. Moreover, none of the above papers 
tackle the problem of unknown words. Sun and 
Jurafsky exploit the probabilistic rhythm feature 
(i.e., the number of syllables in a word or the 
number of words in a phrase) in their shallow 
parser. Their results show that the feature im-
proves the parsing performance, which coincides 
with our analysis in Section 1.1. Chiu et al.?s 
study shows that the morphological structure of 
verbs influences their syntactic behavior. We 
follow this finding and utilize the morphological 
structure of verbs as a feature in the proposed Vt-
N classifier. Qiu?s approach uses an electronic 
syntactic dictionary and a semantic dictionary to 
analyze the relations of V-N phrases. However, 
the approach suffers from two problems: (1) low 
word coverage of the semantic dictionary and (2) 
the semantic type classifier is inadequate. Finally, 
Chen proposed an automatic VN combination 
method with features of verbs, nouns, context, 
and the syllables of words. The experiment re-
sults show that the method performs reasonably 
well without using any other resources. 
Based on the above feature selection methods, 
we extract relevant knowledge from Treebank to 
design a Vt-N classifier. However we have to 
resolve the common problem of data sparseness. 
Learning knowledge by analyzing large-scale 
unlabeled data is necessary and proved useful in 
previous works (Wu, 2003; Chen et al., 2008; Yu 
et al., 2008). Wu developed a machine learning 
method that acquires verb-object and modifier-
head relations automatically. The mutual infor-
mation scores are then used to prune verb-noun 
whose scores are below a certain threshold. The 
author found that accurate identification of the 
verb-noun relation improved the parsing perfor-
mance by 4%. Yu et al. learned head-modifier 
pairs from parsed data and proposed a head-
modifier classifier to filter the data. The filtering 
model uses the following features: a PoS-tag pair 
of the head and the modifier; the distance be-
tween the head and the modifier; and the pres-
ence or absence of punctuation marks (e.g., 
commas, colons, and semi-colons) between the 
head and the modifier. Although the method im-
proves the parsing performance by 2%, the filter-
ing model obtains limited data; the recall rate is 
only 46.35%. The authors also fail to solve the 
problem of Vt-N ambiguity. 
Our review of previous works and the obser-
vations in Section 1.1 show that lexical words, 
semantic information, the syllabic length of 
words, neighboring PoSs and the knowledge 
learned from large-scale data are important for 
Vt-N disambiguation. We consider more features 
for disambiguating Vt-N structures than previous 
studies. For example, we utilize (1) four relation 
classification in a real environment, including 
?X/H?, ?H/X?, ?X/X? and ?H/H? relations; (2) un-
known word processing of Vt-N words (includ-
ing semantic type predication and morph-
structure predication); (3) unsupervised data se-
lection (a simple and effective way to extend 
knowledge); and (4) supervised knowledge cor-
rection, which makes the extracted knowledge 
more useful. 
3 Design of the Disambiguation Model 
The disambiguation model is a Vt-N relation 
classifier that classifies Vt-N relations into ?H/X? 
(predicate-object relations), ?X/H? (modifier-
head relations), ?H/H? (conjunctive head-head 
relations), or ?X/X? (independent relations). We 
use the Maximum Entropy toolkit (Zhang, 2004) 
to construct the classifier. The advantage of us-
ing the Maximum Entropy model is twofold: (1) 
it has the flexibility to adjust features; and (2) it 
provides the probability values of the classifica-
tion, which can be easily integrated into our 
PCFG parsing model. 
In the following sections, we discuss the de-
sign of our model for feature selection and ex-
traction, unknown word processing, and world 
knowledge learning. 
3.1 Feature Selection and Extraction 
We divide the selected features into five groups: 
PoS tags of Vt and N, PoS tags of the context, 
words, semantics, and additional information. 
Table 1 shows the feature types and symbol nota-
tions. We use symbols of t1 and t2 to denote the 
PoS of Vt and N respectively. The context fea-
ture is neighboring PoSs of Vt and N: the sym-
bols of t-2 and t-1 represent its left PoSs, and the 
symbol t3 and t4 represent its right PoSs. The se-
mantic feature is the lexicon?s semantic type ex-
tracted from E-HowNet sense expressions 
(Huang et al., 2008). For example, the E-
HowNet expression of ? ? ? /vehicles? is 
{LandVehicle|? :quantity={mass|? }}, so its 
semantic type is {LandVehicle|?}. We discuss 
the model?s performance with different feature 
combinations in Section 4. 
930
Feature  Feature Description 
PoS PoS of Vt and N 
t1; t2 
Context Neighboring PoSs 
t-2; t-1; t3; t4 
Word Lexical word 
w1; w2 
Semantic Semantic type of word 
st1; st2 
Additional 
Information 
Morphological structure of verb 
Vmorph 
 Syllabic length of noun 
Nlen 
 
Table 1. The features used in the Vt-N classifier 
 
The example in Figure 1 illustrates feature la-
beling of a Vt-N structure. First, an instance of a 
Vt-N structure is identified from Treebank. Then, 
we assign the semantic type of each word with-
out considering the problem of sense ambiguity 
for the moment. This is because sense ambigui-
ties are partially resolved by PoS tagging, and 
the general problem of sense disambiguation is 
beyond the scope of this paper. Furthermore, 
Zhao and Huang (1999) demonstrated that the 
retained ambiguity does not have an adverse im-
pact on identification. Therefore, we keep the 
ambiguous semantic type for future processing. 
 
zhe        zaochen     xuexi  zhongwen    DE    fongchao 
this         cause        learn     Chinese                    trend 
?This causes the trend of learning Chinese.? 
 
Figure 1. An example of a tree with a Vt-N struc-
ture 
 
Table 2 shows the labeled features for ???
/learn  ??/Chinese? in Figure 1. The column x  
and y describe relevant features in ???/learn? 
and ???/Chinese? respectively. Some features 
are not explicitly annotated in the Treebank, e.g., 
the semantic types of words and the morphologi-
cal structure of verbs. We propose labeling 
methods for them in the next sub-section. 
Feature Type x y 
Word w1=?? w2=?? 
PoS t1=VC t2=Na 
Semantic st1=study|?? st2=language|?? 
Context t-2=Nep; t-1=VK; t3=DE; t4=Na 
Additional 
Information 
Vmorph=VV Nlen=2 
Relation Type  rt = H/X 
 
Table 2. The feature labels of Vt-N pair in Figure 
1 
3.2 Unknown Word Processing 
In Chinese documents, 3% to 7% of the words 
are usually unknown (Sproat and Emerson, 
2003). By ?unknown words?, we mean words not 
listed in the dictionary. More specifically, in this 
paper, unknown words means words without se-
mantic type information (i.e., E-HowNet expres-
sions) and verbs without morphological structure 
information. Therefore, we propose a method for 
predicting the semantic types of unknown words, 
and use an affix database to train a morph-
structure classifier to derive the morphological 
structure of verbs. 
 
Morph-Structure Predication of Verbs: We 
use data analyzed by Chiu et al. (2004) to devel-
op a classifier for predicating the morphological 
structure of verbs. There are four types of mor-
phological structures for verbs: the coordinating 
structure (VV), the modifier-head structure (AV), 
the verb-complement structure (VR), and the 
verb-object structure (VO). To classify verbs 
automatically, we incorporate three features in 
the proposed classifier, namely, the lexeme itself, 
the prefix and the suffix, and the semantic types 
of the prefix and the suffix. Then, we use train-
ing data from the affix database to train the clas-
sifier. Table 3 shows an example of the unknown 
verb ???? /disseminate? and the morph-
structure classifier shows that it is a ?VR? type. 
 
Feature Feature Description 
Word=??? Lexicon 
PW=?? Prefix word 
PWST={disseminate|??} Semantic Type of 
Prefix Word ?? 
SW=? Suffix Word 
SWST={Vachieve|??} Semantic Type of 
Suffix Word ? 
 
Table 3. An example of an unknown verb and 
feature templates for morph-structure predication 
931
 
Semantic Type Provider: The system ex-
ploits WORD, PoS, affix and E-HowNet infor-
mation to obtain the semantic types of words (see 
Figure 2). If a word is known and its PoS is giv-
en, we can usually find its semantic type by 
searching the E-HowNet database. For an un-
known word, the semantic type of its head mor-
pheme is its semantic type; and the semantic type 
of the head morpheme is obtained from E-
HowNet1. For example, the unknown word ??
?? /disseminate?, its prefix word is ???
/disseminate? and we learn that its semantic type 
is {disseminate|??} from E-HowNet. There-
fore, we assign {disseminate|??} as the se-
mantic type of ???? /disseminate?. If the 
word or head morpheme does not exist in the 
affix database, we assign a general semantic type 
based on its PoS, e.g., nouns are {thing|??} 
and verbs are {act|??}. In this matching pro-
cedure, we may encounter multiple matching 
data of words and affixes. Our strategy is to keep 
the ambiguous semantic type for future pro-
cessing. 
 
Input: WORD, PoS 
Output: Semantic Type (ST) 
procedure STP(WORD, PoS) 
 (* Initial Step *) 
 ST := null; 
 (* Step 1: Known word *) 
 if WORD already in E-HowNet then 
  ST := EHowNet(WORD, PoS); 
 else if WORD in Affix database then 
  ST := EHowNet(affix of WORD, PoS); 
 (* Step 2 : Unknown word *) 
 if ST is null and PoS is ?Vt? then 
  ST := EHowNet(prefix of WORD, PoS);  
 else if ST is null and PoS is ?N? then 
  ST := EHowNet(suffix of WORD, PoS);  
 (* Step 3 : default *) 
 if ST is null and PoS is ?Vt? then 
  ST := ?act|???; 
 else if ST is null and PoS is ?N? then 
  ST := ?thing|??? 
 (* Finally *) 
 STP := ST; 
end; 
 
Figure 2. The Pseudo-code of the Semantic Type 
Predication Algorithm. 
 
1 The E-HowNet function in Figure 2 will return a null ST 
value where words do not exist in E-HowNet or Affix data-
base. 
3.3 Learning World Knowledge 
Based on the features discussed in the previous 
sub-section, we extract prior knowledge from 
Treebank to design the Vt-N classifier. However, 
the training suffers from the data sparseness 
problem. Furthermore most ambiguous Vt-N 
relations are resolved by common sense 
knowledge that makes it even harder to construct 
a well-trained system. An alternative way to ex-
tend world knowledge is to learn from large-
scale unlabeled data (Wu, 2003; Chen et al., 
2008; Yu et al., 2008). However, the unsuper-
vised approach accumulates errors caused by 
automatic annotation processes, such as word 
segmentation, PoS tagging, syntactic parsing, 
and semantic role assignment. Therefore, how to 
extract useful knowledge accurately is an im-
portant issue. 
To resolve the error accumulation problem, we 
propose two methods: unsupervised NP selection 
and supervised error correction. The NP selec-
tion method exploits the fact that an intransitive 
verb followed by a noun can only be interpreted 
as an NP structure, not a VP structure. It is easy 
to find such instances with high precision by 
parsing a large corpus. Based on the selection 
method, we can extend contextual knowledge 
about NP(V+N) and extract nouns that take ad-
jectival verbs as modifiers. The error correction 
method involves a small amount of manual edit-
ing in order to make the data more useful and 
reduce the number of errors in auto-extracted 
knowledge. The rationale is that, in general, high 
frequency Vt-N word-bigram is either VP or NP 
without ambiguity. Therefore, to obtain more 
accurate training data, we simply classify each 
high frequency Vt-N word bigram into a unique 
correct type without checking all of its instances. 
We provide more detailed information about the 
method in Section 4.3. 
4 Experiments and Results 
4.1 Experimental Setting 
We classify Vt-N structures into four types of 
syntactic structures by using the bracketed in-
formation (tree structure) and dependency rela-
tion (head-modifier) to extract the Vt-N relations 
from treebank automatically. The resources used 
in the experiments as follows. 
Treebank: The Sinica Treebank contains 
61,087 syntactic tree structures with 361,834 
words. We extracted 9,017 instances of Vt-N 
structures from the corpus. Then, we randomly 
                                                 
932
selected 1,000 of the instances as test data and 
used the remainder (8,017 instances) as training 
data. Labeled information of word segmentation 
and PoS-tagging were retained and utilized in the 
experiments. 
E-HowNet: E-HowNet contains 99,525 lexi-
cal semantic definitions that provide information 
about the semantic type of words. We also im-
plement the semantic type predication algorithm 
in Figure 2 to generate the semantic types of all 
Vt and N words, including unknown words. 
Affix Data: The database includes 13,287 ex-
amples of verbs and 27,267 examples of nouns, 
each example relates to an affix. The detailed 
statistics of the verb morph-structure categoriza-
tion are shown in Table 4. The data is used to 
train a classifier to predicate the morph-structure 
of verbs. We found that verbs with a conjunctive 
structure (VV) are more likely to play adjectival 
roles than the other three types of verbs. The 
classifier achieved 87.88% accuracy on 10-fold 
cross validation of the above 13,287 verbs. 
 
 VV VR AV VO 
Prefix 920 2,892 904 662 
Suffix 439 7,388 51 31 
 
Table 4. The statistics of verb morph-structure 
categorization 
 
Large Corpus: We used a Chinese parser to 
analyze sentence structures automatically. The 
auto-parsed tree structures are used in Experi-
ment 2 (described in the Sub-section 4.3). We 
obtained 1,262,420 parsed sentences and derived 
237,843 instances of Vt-N structure as our da-
taset (called as ASBC). 
4.2 Experiment 1: Evaluation of the Vt-N 
Classifier 
In this experiment, we used the Maximum En-
tropy Toolkit (Zhang, 2004) to develop the Vt-N 
classifier. Based on the features discussed in Sec-
tion 3.1, we designed five models to evaluate the 
classifier?s performance on different feature 
combinations.  
The features and used in each model are de-
scribed below. The feature values shown in 
brackets refer to the example in Figure 1. 
? M1 is the baseline model. It uses PoS-tag 
pairs as features, such as (t1=VC, t2=Na). 
? M2 extends the M1 model by adding con-
text features of (t-1=VK, t1=VC), (t2=Na, 
t3=DE), (t-2=Nep, t-1=VK, t1=VC), (t2=Na, 
t3=DE, t4=Na) and (t-1=VK, t3=DE). 
? M3 extends the M2 model by adding lexi-
con features of (w1=??, t1=VK, w2=?
?, t2=Na), (w1???, w2=??), (w1=?
?) and (w2=??). 
? M4 extends the M3 model by adding se-
mantic features of (st1=study|??, t1=VK , 
st2=language|?? , t2=Na), (st1=study|?
? , t1=VK) and (st2=language| ? ? , 
t2=Na). 
? M5 extends the M4 model by adding two 
features: the morph-structure of verbs; and 
the syllabic length of nouns 
(Vmorph=?VV?) and (Nlen=2). 
Table 5 shows the results of using different 
feature combinations in the models. The symbol 
P1(%) is the 10-fold cross validation accuracy of 
the training data, and the symbol P2(%) is the 
accuracy of the test data. By adding contextual 
features, the accuracy rate of M2 increases from 
59.10% to 72.30%. The result shows that contex-
tual information is the most important feature 
used to disambiguate VP, NP and independent 
structures. The accuracy of M2 is approximately 
the same as the result of our PCFG parser be-
cause both systems use contextual information. 
By adding lexical features (M3), the accuracy 
rate increases from 72.30% to 80.20%. For se-
mantic type features (M4), the accuracy rate in-
creases from 80.20% to 81.90%. The 1.7% in-
crease in the accuracy rate indicates that seman-
tic generalization is useful. Finally, in M5, the 
accuracy rate increases from 81.90% to 83.00%. 
The improvement demonstrates the benefits of 
using the verb morph-structure and noun length 
features. 
 
Models Feature for Vt-N P1(%) P2(%) 
M1 (t1,t2) 61.94 59.10 
M2 + (t-1,t1) (t2,t3) (t-2,t-
1,t1) (t2,t3,t4) (t-1,t3) 
76.59 72.30 
M3 + (w1,t1,w2,t2) (w1,w2) 
(w2) (w1) 
83.55 80.20 
M4 + (st1,t1,st2,t2) (st1,t1) 
(st2, t2) 
84.63 81.90 
M5 + (Vmorph) (Nlen) 85.01 83.00 
 
Table 5. The results of using different feature 
combinations 
 
933
Next, we consider the influence of unknown 
words on the Vt-N classifier. The statistics shows 
that 17% of the words in Treebank lack semantic 
type information, e.g., ??/StayIn, ??/fill, ?
?/posted, and ??/tied. The accuracy of the 
Vt-N classifier declines by 0.7% without seman-
tic type information for unknown words. In other 
words, lexical semantic information improves the 
accuracy of the Vt-N classifier. Regarding the 
problem of unknown morph-structure of words, 
we observe that over 85% of verbs with more 
than 2 characters are not found in the affix data-
base. If we exclude unknown words, the accura-
cy of the Vt-N prediction decreases by 1%. 
Therefore, morph-structure information has a 
positive effect on the classifier. 
4.3 Experiment 2: Using Knowledge Ob-
tained from Large-scale Unlabeled Data 
by the Selection and Correction Meth-
ods. 
In this experiment, we evaluated the two 
methods discussed in Section 3, i.e., unsuper-
vised NP selection and supervised error correc-
tion. We applied the data selection method (i.e., 
distance=1, with an intransitive verb (Vi) fol-
lowed by an object noun (Na)) to select 46,258 
instances from the ASBC corpus and compile a 
dataset called Treebank+ASBC-Vi-N. Table 6 
shows the performance of model 5 (M5) on the 
training data derived from Treebank and Tree-
bank+ASBC-Vi-N. The results demonstrate that 
learning more nouns that accept verbal modifiers 
improves the accuracy. 
 
 
Treebank+ 
ASBC-Vi-N 
Treebank 
size of training 
instances 
46,258 8,017 
M5 - P2(%) 83.90 83.00 
 
Table 6. Experiment results on the test data for 
various knowledge sources 
 
We had also try to use the auto-parsed results 
of the Vt-N structures from the ASBC corpus as 
supplementary training data for train M5. It de-
grades the model?s performance by too much 
error when using the supplementary training data. 
To resolve the problem, we utilize the supervised 
error correction method, which manually correct 
errors rapidly because high frequency instances 
(w1, w2) rarely have ambiguous classifications in 
different contexts. So we designed an editing tool 
to correct errors made by the parser in the classi-
fication of high frequency Vt-N word pairs. After 
the manual correction operation, which takes 40 
man-hours, we assign the correct classifications 
(w1, t1, w2, t2, rt) for 2,674 Vt-N structure types 
which contains 10,263 instances to creates the 
ASBC+Correction dataset. Adding the corrected 
data to the original training data increases the 
precision rate to 88.40% and reduces the number 
of errors by approximately 31.76%, as shown in 
the Treebank+ASBC+Correction column of Ta-
ble 7. 
 
 
Treebank+ 
ASBC+Correction 
Treebank+ 
ASBC-Vi-N 
Treebank 
size of train-
ing instances 
56,521 46,258 8,017 
M5 - P2(%) 88.40 83.90 83.00 
 
Table 7. Experiment results of classifiers with 
different training data 
 
We also used the precision and recall rates to 
evaluate the performance of the models on each 
type of relation. The results are shown in Table 8. 
Overall, the Treebank+ASBC+Correction meth-
od achieves the best performance in terms of the 
precision rate. The results for Treebank+ASBC-
Vi-N show that the unsupervised data selection 
method can find some knowledge to help identi-
fy NP structures. In addition, the proposed mod-
els achieve better precision rates than the PCFG 
parser. The results demonstrate that using our 
guidelines to design a disambiguation model to 
resolve the Vt-N problem is successful. 
 
 H/X X/H X/X 
Treebank 
R(%) 91.11 67.90 74.62 
P(%) 84.43 78.57 81.86 
Treebank+ 
ASBC-Vi-N 
R(%) 91.00 72.22 71.54 
P(%) 84.57 72.67 85.71 
Treebank+ 
ASBC+Correction 
R(%) 98.62 60.49 83.08 
P(%) 86.63 88.29 93.51 
PCFG 
R(%) 90.54 23.63 80.21 
P(%) 78.24 73.58 75.00 
 
Table 8. Performance comparison of different 
classification models. 
 
4.4 Experiment 3: Integrating the Vt-N 
classifier with the PCFG Parser 
Identifying Vt-N structures correctly facilitates 
statistical parsing, machine translation, infor-
934
mation retrieval, and text classification. In this 
experiment, we develop a baseline PCFG parser 
based on feature-based grammar representation 
by Hsieh et al. (2012) to find the best tree struc-
tures (T) of a given sentence (S). The parser then 
selects the best tree according to the evaluation 
score Score(T,S) of all possible trees. If there are 
n PCFG rules in the tree T, the Score(T,S) is the 
accumulation of the logarithmic probabilities of 
the i-th grammar rule (RPi). Formula 1 shows the 
baseline PCFG parser. 
 
?
=
=
n
i
iRPSTScore
1
)(),(  (1)
 
 
The Vt-N models can be easily integrated into 
the PCFG parser. Formula 2 represents the inte-
grated structural evaluation model. We combine 
RPi and VtNPi with the weights w1 and w2 re-
spectively, and set the value of w2 higher than 
that of w1. VtNPi is the probability produced by 
the Vt-N classifier for the type of the relation 
between Vt-N bigram determined by the PCFG 
parsing. The classifier is triggered when a [Vt, N] 
structure is encountered; otherwise, the Vt-N 
model is not processed. 
 
?
=
?+?=
n
i
ii VtNPwRPwSTScore
1
21 )(),(  (2)
 
 
The results of evaluating the parsing model in-
corporated with the Vt-N classifier (see Formula 
2) are shown in Table 9 and Table 10. The P2 is 
the accuracy of Vt-N classification on the test 
data. The bracketed f-score (BF2) is the parsing 
performance metric. Based on these results, the 
integrated model outperforms the PCFG parser in 
terms of Vt-N classification. Because the Vt-N 
classifier only considers sentences that contain 
Vt-N structures, it does not affect the parsing 
accuracies of other sentences.  
 
 
PCFG +  
M5 (Treebank) PCFG 
P2(%) 80.68 77.09 
BF(%) 83.64 82.80 
 
Table 9. The performance of the PCFG parser 
with and without model M5 from Treebank. 
 
2 The evaluation formula is (BP*BR*2) / (BP+BR), where 
BP is the precision and BR is the recall. 
 
PCFG +  
M5 (Treebank+ASBC+Correction) PCFG 
P2(%) 87.88 77.09 
BF(%) 84.68 82.80 
 
Table 10. The performance of the PCFG parser 
with and without model M5 from Tree-
bank+ASBC+Correction data set. 
 
4.5 Experiment 4: Comparison of Various 
Chinese Parsers 
In this experiment, we give some comparison 
results in various parser: ?PCFG Parser? (base-
line), ?CDM Parser? (Hsieh et al., 2012), and 
?Berkeley Parser? (Petrov et al., 2006). The CDM 
parser achieves the best score in Traditional Chi-
nese Parsing task of SIGHAN Bake-offs 2012 
(Tseng et al., 2012). Petrov?s parser (as Berkeley, 
version is 2009 1.1) is the best PCFG parser for 
non-English language and it is an open source. In 
our comparison, we use the same training data 
for training models and parse the same test da-
taset based on the gold standard word segmenta-
tion and PoS tags. We have already discussed the 
PCFG parser in Section 4.4. As for CDM parser, 
we retrain relevant model in our experiments. 
And since Berkeley parser take different tree 
structure (Penn Treebank format), we transform 
the experimental data to Berkeley CoNLL format 
and re-train a new model with parameters ?-
treebank CHINESE -SMcycles 4? 3 from training 
data. Moreover we use ?-useGoldPOS? parame-
ters to parse test data and further transform them 
to Sinica Treebank style from the Berkeley par-
ser?s results. The different tree structure formats 
of Sinica Treebank and Penn Treebank are as 
follow: 
 
Sinica Treebank:  
S(NP(Head:Nh:??)|Head:VC:??
|NP(Head:Na:??)) 
 
Penn Treebank:  
( (S (NP (Head:Nh (Nh ??))) (Head:VC 
(VC ??)) (NP (Head:Na (Na ??))))) 
 
The evaluation results on the testing data, i.e. 
in P2 metric, are as follows. The accuracy of 
PCFG parser is 77.09%; CDM parser reaches 
78.45% of accuracy; and Berkeley parser is 
70.68%. The results show that the problem of Vt-
3 The ?-treebank CHINESE -SMcycles 4? is the best train-
ing parameter in Traditional Chinese Parsing task of 
SIGHAN Bake-offs 2012. 
                                                 
                                                 
935
N cannot be well solved by any general parser 
including CDM parser and Berkeley?s parser. It 
is necessary to have a different approach aside 
from the general model. So we set the target for a 
better model for Vt-N classification which can be 
easily integrated into the existing parsing model. 
So far our best model achieved the P2 accuracy 
of 87.88%.  
5 Concluding Remarks 
We have proposed a classifier to resolve the am-
biguity of Vt-N structures. The design of the 
classifier is based on three important guidelines, 
namely, adopting linguistically motivated fea-
tures, using all available resources, and easy in-
tegration into parsing model. After analyzing the 
Vt-N structures, we identify linguistically moti-
vated features, such as lexical words, semantic 
knowledge, the morphological structure of verbs, 
neighboring parts-of-speech, and the syllabic 
length of words. Then, we design a classifier to 
verify the usefulness of each feature. We also 
resolve the technical problems that affect the 
prediction of the semantic types and morph-
structures of unknown words. In addition, we 
propose a framework for unsupervised data se-
lection and supervised error correction for learn-
ing more useful knowledge. Our experiment re-
sults show that the proposed Vt-N classifier sig-
nificantly outperforms the PCFG Chinese parser 
in terms of Vt-N structure identification. Moreo-
ver, integrating the Vt-N classifier with a parsing 
model improves the overall parsing performance 
without side effects. 
In our future research, we will exploit the pro-
posed framework to resolve other parsing diffi-
culties in Chinese, e.g., N-N combination. We 
will also extend the Semantic Type Predication 
Algorithm (Figure 2) to deal with all Chinese 
words. Finally, for real world knowledge learn-
ing, we will continue to learn more useful 
knowledge by auto-parsing to improve the pars-
ing performance. 
Acknowledgments 
We thank the anonymous reviewers for their val-
uable comments. This work was supported by 
National Science Council under Grant NSC99-
2221-E-001-014-MY3. 
Reference 
Li-li Chang, Keh-Jiann Chen, and Chu-Ren Huang. 
2000. Alternation Across Semantic Fields: A Study 
on Mandarin Verbs of Emotion. Internal Journal of 
Computational Linguistics and Chinese Language 
Processing (IJCLCLP), 5(1):61-80. 
Keh-Jiann Chen, Chu-Ren Huang, Chi-Ching Luo, 
Feng-Yi Chen, Ming-Chung Chang, Chao-Jan 
Chen, , and Zhao-Ming Gao. 2003. Sinica Tree-
bank: Design Criteria, Representational Issues and 
Implementation. In (Abeille 2003) Treebanks: 
Building and Using Parsed Corpora, pages 231-
248. Dordrecht, the Netherlands: Kluwer. 
Li-jiang Chen. 2008. Autolabeling of VN Combina-
tion Based on Multi-classifier. Journal of Comput-
er Engineering, 34(5):79-81. 
Wenliang Chen, Daisuke Kawahara, Kiyotaka 
Uchimoto, Yujjie Zhang, and Hitoshi Isahara. 2008. 
Dependency Parsig with Short Dependency Rela-
tions in Unlabeled Data.  In Proceedings of the 
third International Joint Conference on Natural 
Language Processing (IJCNLP). pages 88-94.. 
Chih-ming Chiu, Ji-Chin Lo, and Keh-Jiann Chen. 
2004. Compositional Semantics of Mandarin Affix 
Verbs. In Proceedings of the Research on Compu-
tational Linguistics Conference (ROCLING), pages 
131-139. 
Yu-Ming Hsieh, Ming-Hong Bai, Jason S. Chang, and 
Keh-Jiann Chen. 2012. Improving PCFG Chinese 
Parsing with Context-Dependent Probability Re-
estimation, In Proceedings of the Second CIPS-
SIGHAN Joint Conference on Chinese Language 
Processing, pages 216?221. 
Shu-Ling Huang, You-Shan Chung, Keh-Jiann Chen. 
2008. E-HowNet: the Expansion of HowNet. In 
Proceedings of the First National HowNet work-
shop, pages 10-22, Beijing, China. 
Chunhi Liu, Xiandai Hanyu Shuxing Fanchou Yianjiu 
(??????????). Chengdu: Bashu Books, 
2008. 
Jiaju Mei, Yiming Lan, Yunqi Gao, and Yongxian 
Ying. 1983. A Dictionary of Synonyms. Shanghai 
Cishu Chubanshe.  
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceesings of 
COLING/ACL, pages 433-400. 
Likun Qiu. 2005. Constitutive Relation Analysis for 
V-N Phrases. Journal of Chinese Language and 
Computing, 15(3):173-183. 
Richard Sproat and Thomas Emerson, 2003. The first 
International Chinese Word Segmentation Bakeoff. 
In Proceedings of the Second SIGHAN Workshop 
on Chinese Language Processing, pages 133-143. 
Honglin Sun and Dan Jurafsky. 2003. The Effect of 
Rhythm on Structural Disambiguation in Chinese. 
In Proceedings of the Second SIGHAN Workshop 
on Chinese Language Processing, pages 39-46. 
936
Yuen-Hsieh Tseng, Lung-Hao Lee, and Liang-Chih 
Yu. 2012. Tranditional Chinese Parsing Evaluation 
at SIGHAN Bake-offs 2012. In Proceedings of the 
Second CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pages 199-205. 
Andi Wu. 2003. Learning Verb-Noun Relations to 
Improve Parsing. In Proceedings of the Second 
SIGHAN workshop on Chinese Language Pro-
cessing, pages 119-124. 
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi. 
2008. Chinese Dependency Parsing with Large 
Scale Automatically Constructed Case Structures, 
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING2008), 
pages 1049-1056. 
Jun Zhao and Chang-ning Huang. 1999. The Com-
plex-feature-based Model for Acquisition of VN-
construction Structure Templates. Journal of Soft-
ware, 10(1):92-99. 
Le Zhang. 2004. Maximum Entropy Modeling 
Toolkit for Python and C++. Reference Manual. 
937
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 16?19,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
TransAhead: A Writing Assistant for CAT and CALL 
 
*Chung-chi Huang  ++Ping-che Yang  *Mei-hua Chen *Hung-ting Hsieh  +Ting-hui Kao 
 
   
+Jason S. Chang 
*ISA, NTHU, HsinChu, Taiwan, R.O.C.  
++III, Taipei, Taiwan, R.O.C. +CS, NTHU, HsinChu, Taiwan, R.O.C. 
{u901571,maciaclark,chen.meihua,vincent732,maxis1718,jason.jschang}gmail.com 
 
Abstract 
We introduce a method for learning to 
predict the following grammar and text 
of the ongoing translation given a source 
text. In our approach, predictions are 
offered aimed at reducing users? burden 
on lexical and grammar choices, and 
improving productivity. The method 
involves learning syntactic phraseology 
and translation equivalents. At run-time, 
the source and its translation prefix are 
sliced into ngrams to generate subsequent 
grammar and translation predictions. We 
present a prototype writing assistant, 
TransAhead1, that applies the method to 
where computer-assisted translation and 
language learning meet. The preliminary 
results show that the method has great 
potentials in CAT and CALL (significant 
boost in translation quality is observed). 
1.  Introduction 
More and more language learners use the MT 
systems on the Web for language understanding 
or learning. However, web translation systems 
typically suggest a, usually far from perfect, one-
best translation and hardly interact with the user. 
Language learning/sentence translation could 
be achieved more interactively and appropriately 
if a system recognized translation as a 
collaborative sequence of the user?s learning and 
choosing from the machine-generated predictions 
of the next-in-line grammar and text and the 
machine?s adapting to the user?s accepting 
/overriding the suggestions. 
Consider the source sentence ????????
?????????? (We play an important role 
in closing this deal). The best learning 
environment is probably not the one solely 
                                                          
1Available at http://140.114.214.80/theSite/TransAhead/ 
which, for the time being, only supports Chrome browsers. 
providing the automated translation. A good 
learning environment might comprise a writing 
assistant that gives the user direct control over 
the target text and offers text and grammar 
predictions following the ongoing translations. 
We present a new system, TransAhead, that 
automatically learns to predict/suggest the 
grammatical constructs and lexical translations 
expected to immediately follow the current 
translation given a source text, and adapts to the 
user?s choices. Example TransAhead responses 
to the source ?????????????????? 
and the ongoing translation ?we? and ?we play 
an important role? are shown in Figure 12(a) and 
(b) respectively. TransAhead has determined the 
probable subsequent grammatical constructions 
with constituents lexically translated, shown in 
pop-up menus (e.g., Figure 1(b) shows a 
prediction ?IN[in] VBG[close, end, ?]? due to 
the history ?play role? where lexical items in 
square brackets are lemmas of potential 
translations). TransAhead learns these constructs 
and translations during training. 
At run-time, TransAhead starts with a source 
sentence, and iteratively collaborates with the 
user: by making predictions on the successive 
grammar patterns and lexical translations, and by 
adapting to the user?s translation choices to 
reduce source ambiguities (e.g., word 
segmentation and senses). In our prototype, 
TransAhead mediates between users and 
automatic modules to boost users? writing/ 
translation performance (e.g., productivity). 
2.  Related Work 
CAT has been an area of active research. Our 
work addresses an aspect of CAT focusing on 
language learning. Specifically, our goal is to 
build a human-computer collaborative writing 
assistant: helping the language learner with in- 
text  grammar  and  translation  and  at  the  same 
                                                          
2
 Note that grammatical constituents (in all-capitalized 
words) are represented using Penn parts-of-speech and the 
history based on the user input is shown in shades. 
16
 Figure 1. Example TransAhead responses to a source text under the translation (a) ?we? and (b) ?we play an important role?. Note 
that the grammar/text predictions of (a) and (b) are not placed directly under the current input focus for space limit. (c) and (d) 
depict predominant grammar constructs which follow and (e) summarizes the translations for the source?s character-based ngrams. 
 
time updating the system?s segmentation 
/translation options through the user?s word 
choices. Our intended users are different from 
those of the previous research focusing on what 
professional translator can bring for MT systems 
(e.g., Brown and Nirenburg, 1990). 
More recently, interactive MT (IMT) systems 
have begun to shift the user?s role from analyses 
of the source text to the formation of the target 
translation. TransType project (Foster et al 2002) 
describes such pioneering system that supports 
next word predictions. Koehn (2009) develops 
caitra which displays one phrase translation at a 
time and offers alternative translation options. 
Both systems are similar in spirit to our work. 
The main difference is that we do not expect the 
user to be a professional translator and we 
provide translation hints along with grammar 
predictions to avoid the generalization issue 
facing phrase-based system. 
Recent work has been done on using fully-
fledged statistical MT systems to produce target 
hypotheses completing user-validated translation 
prefix in IMT paradigm. Barrachina et al(2008) 
investigate the applicability of different MT 
kernels within IMT framework. Nepveu et al
(2004) and Ortiz-Martinez et al(2011) further 
exploit user feedbacks for better IMT systems 
and user experience. Instead of trigged by user 
correction, our method is triggered by word 
delimiter and assists in target language learning. 
In contrast to the previous CAT research, we 
present a writing assistant that suggests 
subsequent grammar constructs with translations 
and interactively collaborates with learners, in 
view of reducing users? burden on grammar and 
word choice and enhancing their writing quality. 
3.  The TransAhead System 
3.1 Problem Statement 
For CAT and CALL, we focus on predicting a 
set of grammar patterns with lexical translations 
likely to follow the current target translation 
given a source text. The predictions will be 
examined by a human user directly. Not to 
overwhelm the user, our goal is to return a 
reasonable-sized set of predictions that contain 
suitable word choices and correct grammar to 
choose and learn from. Formally speaking, 
Problem Statement: We are given a target-
language reference corpus Ct, a parallel corpus 
Cst, a source-language text S, and its target 
translation prefix Tp. Our goal is to provide a set 
of predictions based on Ct and Cst likely to 
further translate S in terms of grammar and text. 
For this, we transform S and Tp into sets of 
ngrams such that the predominant grammar 
constructs with suitable translation options 
following Tp are likely to be acquired. 
3.2  Learning to Find Pattern and Translation 
We attempt to find syntax-based phraseology and 
translation equivalents beforehand (four-staged) 
so that a real-time system is achievable. 
Firstly, we syntactically analyze the corpus Ct. 
In light of the phrases in grammar book (e.g., 
one?s in ?make up one?s mind?), we resort to 
parts-of-speech for syntactic generalization. 
Secondly, we build up inverted files of the words 
in Ct for the next stage (i.e., pattern grammar 
generation). Apart from sentence and position 
information, a word?s lemma and part-of-speech 
(POS) are also recorded. 
(b) 
Source text: 
???????????????? 
(a) 
Pop-up predictions/suggestions: 
we MD VB[play, act, ..] , ? 
we VBP[play, act, ..] DT , ? 
we VBD[play, act, ..] DT , ? 
Pop-up predictions/suggestions: 
play role IN[in] VBG[close, end, ..] , ? 
important role IN[in] VBG[close, end, ..] , ? 
role IN[in] VBG[close, end, ..] , ? 
(c) 
(d) 
(e) 
Patterns for ?we?: 
we MD VB , ?, 
we VBP DT , ?, 
we VBD DT , ? 
Patterns for ?we play an important role?: 
play role IN[in] DT , 
play role IN[in] VBG , ?, 
important role IN[in] VBG , ?, 
role IN[in] VBG , ? 
Translations for the source text: 
????: we, ?; ????: close, end, ?;  ?; ????: 
play, ?; ????: critical, ?; ?; ???: act, ?; ?; 
???: heavy, ?; ???: will, wish, ?; ???: cents, ?; 
???: outstanding, ? 
Input your source text and start to interact with TransAhead! 
17
We then leverage the procedure in Figure 2 to 
generate grammar patterns for any given 
sequence of words (e.g., contiguous or not). 
 
Figure 2. Automatically generating pattern grammar. 
 
The algorithm first identifies the sentences 
containing the given sequence of words, query. 
Iteratively, Step (3) performs an AND operation 
on the inverted file, InvList, of the current word 
wi and interInvList, a previous intersected results. 
Afterwards, we analyze query?s syntax-based 
phraseology (Step (5)). For each element of the 
form ([wordPosi(w1),?,wordPosi(wn)], sentence 
number) denoting the positions of query?s words 
in the sentence, we generate grammar pattern 
involving replacing words with POS tags and 
words in wordPosi(wi) with lemmas, and 
extracting fixed-window3  segments surrounding 
query from the transformed sentence. The result 
is a set of grammatical, contextual patterns. 
The procedure finally returns top N 
predominant syntactic patterns associated with 
the query. Such patterns characterizing the 
query?s word usages follow the notion of pattern 
grammar in (Hunston and Francis, 2000) and are 
collected across the target language. 
In the fourth and final stage, we exploit Cst for 
bilingual phrase acquisition, rather than a manual 
dictionary, to achieve better translation coverage 
and variety. We obtain phrase pairs through 
leveraging IBM models to word-align the bitexts, 
?smoothing? the directional word alignments via 
grow-diagonal-final, and extracting translation 
equivalents using (Koehn et al 2003). 
3.3  Run-Time Grammar and Text Prediction 
Once translation equivalents and phraseological 
tendencies are learned, TransAhead then 
predicts/suggests the following grammar and text 
of a translation prefix given the source text using 
the procedure in Figure 3. 
We first slice the source text S and its 
translation prefix Tp into character-level and 
                                                          
3
 Inspired by (Gamon and Leacock, 2010). 
word-level ngrams respectively. Step (3) and (4) 
retrieve the translations and patterns learned 
from Section 3.2. Step (3) acquires the active 
target-language vocabulary that may be used to 
translate the source text. To alleviate the word 
boundary issue in MT raised by Ma et al(2007), 
TransAhead non-deterministically segments the 
source text using character ngrams and proceeds 
with collaborations with the user to obtain the 
segmentation for MT and to complete the 
translation. Note that a user vocabulary of 
preference (due to users? domain of knowledge 
or errors of the system) may be exploited for 
better system performance. On the other hand, 
Step (4) extracts patterns preceding with the 
history ngrams of {tj}. 
 
Figure 3. Predicting pattern grammar and translations. 
 
In Step (5), we first evaluate and rank the 
translation candidates using linear combination: 
( ) ( )( ) ( )1 1 1 2 2   i i pP t s P s t P t T? ?? + + ?  
where ?i is combination weight, P1 and P2 are 
translation and language model respectively, and 
t is one of the translation candidates under S and 
Tp. Subsequently, we incorporate the lemmatized 
translation candidates into grammar constituents 
in GramOptions. For example, we would include 
?close? in pattern ?play role IN[in] VBG? as 
?play role IN[in] VBG[close]?. 
At last, the algorithm returns the 
representative grammar patterns with confident 
translations expected to follow the ongoing 
translation and further translate the source. This 
algorithm will be triggered by word delimiter to 
provide an interactive environment where CAT 
and CALL meet. 
4.  Preliminary Results 
To train TransAhead, we used British National 
Corpus and Hong Kong Parallel Text and 
deployed GENIA tagger for POS analyses. 
To evaluate TransAhead in CAT and CALL, 
we introduced it to a class of 34 (Chinese) first-
year college students learning English as foreign 
language. Designed to be intuitive to the general 
public, esp. language learners, presentational 
tutorial lasted only for a minute. After the tutorial, 
the participants were asked to translate 15 
procedure PatternFinding(query,N,Ct) (1)  interInvList=findInvertedFile(w1 of query) 
for each word wi in query except for w1 (2)     InvList=findInvertedFile(wi) (3a)   newInterInvList= ? ; i=1; j=1 
(3b)   while i<=length(interInvList) and j<=lengh(InvList) 
(3c)      if interInvList[i].SentNo==InvList[j].SentNo 
(3d)         Insert(newInterInvList, interInvList[i],InvList[j]) 
else 
(3e)         Move i,j accordingly 
(3f)    interInvList=newInterInvList 
(4) Usage= ?  
for each element in interInvList 
(5)     Usage+={PatternGrammarGeneration(element,Ct)} (6) Sort patterns in Usage in descending order of frequency (7) return the N patterns in Usage with highest frequency 
procedure MakePrediction(S,Tp) 
(1) Assign sliceNgram(S) to {si} (2) Assign sliceNgram(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptions 
           and incorporate them into GramOptions (6) Return GramOptions 
18
Chinese texts from (Huang et al 2011a) one by 
one (half with TransAhead assistance, and the 
other without). Encouragingly, the experimental 
group (i.e., with the help of our system) achieved 
much better translation quality than the control 
group in BLEU (Papineni et al 2002) (i.e., 
35.49 vs. 26.46) and significantly reduced the 
performance gap between language learners and 
automatic decoder of Google Translate (44.82).  
We noticed that, for the source ?????????
?????????, 90% of the participants in the 
experimental group finished with more 
grammatical and fluent translations (see Figure 4) 
than (less interactive) Google Translate (?We 
conclude this transaction plays an important 
role?). In comparison, 50% of the translations of 
the source from the control group were erroneous. 
 
Figure 4. Example translations with TransAhead assistance. 
 
Post-experiment surveys indicate that a) the 
participants found TransAhead intuitive enough 
to collaborate with in writing/translation; b) the 
participants found TransAhead suggestions 
satisfying, accepted, and learned from them; c) 
interactivity made translation and language 
learning more fun and the participants found 
TransAhead very recommendable and would like 
to use the system again in future translation tasks. 
5.  Future Work and Summary 
Many avenues exist for future research and 
improvement. For example, in the linear 
combination, the patterns? frequencies could be 
considered and the feature weight could be better 
tuned. Furthermore, interesting directions to 
explore include leveraging user input such as 
(Nepveu et al 2004) and (Ortiz-Martinez et al 
2010) and serially combining a grammar checker 
(Huang et al 2011b). Yet another direction 
would be to investigate the possibility of using 
human-computer collaborated translation pairs to 
re-train word boundaries suitable for MT. 
In summary, we have introduced a method for 
learning to offer grammar and text predictions 
expected to assist the user in translation and 
writing (or even language learning). We have 
implemented and evaluated the method. The 
preliminary results are encouragingly promising, 
prompting us to further qualitatively and 
quantitatively evaluate our system in the near 
future (i.e., learners? productivity, typing speed 
and keystroke ratios of ?del? and ?backspace? 
(possibly hesitating on the grammar and lexical 
choices), and human-computer interaction, 
among others). 
Acknowledgement 
This study is conducted under the ?Project 
Digital Convergence Service Open Platform? of 
the Institute for Information Industry which is 
subsidized by the Ministry of Economy Affairs 
of the Republic of China. 
References  
S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. 
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomas, E. 
Vidal, and J.-M. Vilar. 2008. Statistical approaches to 
computer-assisted translation. Computer Linguistics, 
35(1): 3-28. 
R. D. Brown and S. Nirenburg. 1990. Human-computer 
interaction for semantic disambiguation. In Proceedings 
of COLING, pages 42-47. 
G. Foster, P. Langlais, E. Macklovitch, and G. Lapalme. 
2002. TransType: text prediction for translators. In 
Proceedings of ACL Demonstrations, pages 93-94. 
M. Gamon and C. Leacock. 2010. Search right and thou 
shalt find ? using web queries for learner error 
detection. In Proceedings of the NAACL Workshop on 
Innovative Use of NLP for Building Educational 
Applications, pages 37-44. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, H.-C. Liou, and J. 
S. Chang. 2011a. GRASP: grammar- and syntax-based 
pattern-finder in CALL. In Proceedings of ACL. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, and J. S. Chang. 
2011b. EdIt: a broad-coverage grammar checker using 
pattern grammar. In Proceedings of ACL. 
S. Hunston and G. Francis. 2000. Pattern Grammar: A 
Corpus-Driven Approach to the Lexical Grammar of 
English. Amsterdam: John Benjamins. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of NAACL. 
P. Koehn. 2009. A web-based interactive computer aided 
translation tool. In Proceedings of ACL. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrapping word 
alignment via word packing. In Proceedings of ACL. 
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004. 
Adaptive language and translation models for interactive 
machine translation. In Proceedings of EMNLP. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19-51. 
D. Ortiz-Martinez, L. A. Leiva, V. Alabau, I. Garcia-Varea, 
and F. Casacuberta. 2011. An interactive machine 
translation system with online learning. In Proceedings 
of ACL System Demonstrations, pages 68-73. 
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. Bleu: a 
method for automatic evaluation of machine translation. 
In Proceedings of ACL, pages 311-318. 
1. we play(ed) a critical role in closing/sealing this/the deal. 
2. we play(ed) an important role in ending/closing this/the deal. 
19
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 352?356,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
TransAhead: A Computer-Assisted Translation and Writing Tool 
G
G
*Chung-chi Huang      +Ping-che Yang **Keh-jiann Chen       ++Jason S. Chang 
  
*ISA, NTHU, HsinChu, Taiwan, R.O.C. **IIS, Academia Sinica, Taipei, Taiwan, R.O.C. 
+III, Taipei, Taiwan, R.O.C. ++CS, NTHU, HsinChu, Taiwan, R.O.C. 
{*u901571,+maciaclark,++jason.jschang}@gmail.com; **kchen@iis.sinica.edu.tw 
  
 
Abstract 
We introduce a method for learning to predict 
text completion given a source text and partial 
translation. In our approach, predictions are 
offered aimed at alleviating users? burden on 
lexical and grammar choices, and improving 
productivity. The method involves learning 
syntax-based phraseology and translation 
equivalents. At run-time, the source and its 
translation prefix are sliced into ngrams to 
generate and rank completion candidates, 
which are then displayed to users. We present 
a prototype writing assistant, TransAhead, that 
applies the method to computer-assisted 
translation and language learning. The 
preliminary results show that the method has 
great potentials in CAT and CALL with 
significant improvement in translation quality 
across users. 
1 Introduction 
More and more language workers and learners use 
the MT systems on the Web for information 
gathering and language learning. However, web 
translation systems typically offer top-1 
translations (which are usually far from perfect) 
and hardly interact with the user. 
Text translation could be achieved more 
interactively and effectively if a system considered 
translation as a collaborative between the machine 
generating suggestions and the user accepting or 
overriding on those suggestions, with the system 
adapting to the user?s action. 
Consider the source sentence ?????????
????????? (We play an important role in 
closing this deal). The best man-machine 
interaction is probably not the one used by typical 
existing MT systems. A good working 
environment might be a translation assistant that 
offers suggestions and gives the user direct control 
over the target text. 
We present a system, TransAhead1, that learns 
to predict and suggest lexical translations (and 
their grammatical patterns) likely to follow the 
ongoing translation of a source text, and adapts to 
the user?s choices. Example responses of 
TransAhead to the source sentence ????????
?????????? and two partial translations  
are shown in Figure 1. The responses include text 
and grammatical patterns (in all-cap labels 
representing parts-of-speech). TransAhead 
determines and displays the probable subsequent 
grammatical constructions and partial translations 
in the form of parts-of-speech and words (e.g., 
?IN[in] VBG[close,?]? for keywords ?play role? 
where lexical items in square brackets are lemmas 
of potential translations) in a pop-up. TransAhead 
learns these constructs and translations during 
training. 
At run-time, TransAhead starts with a source 
sentence, and iterates with the user, making 
predictions on the grammar patterns and lexical 
translations, while adapting to the user?s 
translation choices to resolve ambiguities in the 
source sentence related to word segmentation and 
word sense. In our prototype, TransAhead 
mediates between users and suggestion modules to 
translation quality and  productivity. 
2 Related Work 
Computer Assisted Translation (CAT) has been an 
area of active research. We focus on offering 
suggestions during the  translation process with  an 
                                                           
1
 http://140.114.214.80/theSite/TransAhead/ (Chrome only) 
352
 Figure 1. Example TransAhead responses to a source text under the translation (a) ?we? and (b) ?we play an 
important role?. Note that the grammar/text predictions of (a) and (b) are not placed directly under the caret (current 
input focus) for space limit. (c) and (d) depict predominant grammar constructs which follow and (e) summarizes 
the confident translations of the source?s character-based ngrams. The frequency of grammar pattern is shown in 
round brackets while the history (i.e., keyword) based on the user input is shown in shades. 
 
emphasis on language learning. Specifically, our 
goal is to build a translation assistant to help 
translator (or learner-translator) with inline 
grammar help and translation. Unlike recent 
research focusing on professional (e.g., Brown and 
Nirenburg, 1990), we target on both professional 
and student translators. 
More recently, interactive MT (IMT) systems 
have begun to shift the user?s role from post-
editing machine output to collaborating with the 
machine to produce the target text. Foster et al
(2000) describe TransType, a pioneering system 
that supports next word predictions. Along the 
similar line, Koehn (2009) develops caitra which 
predicts and displays phrasal translation 
suggestions one phrase at a time. The main 
difference between their systems and TransAhead 
is that we also display grammar patterns to provide 
the general patterns of predicted translations so a 
student translator can learn and become more 
proficient. 
Recent work has been done on using fully-
fledged statistical MT systems to produce target 
hypotheses completing user-validated translation 
prefix in IMT paradigm. Barrachina et al (2008) 
investigate the applicability of different MT 
kernels within IMT framework. Nepveu et al 
(2004) and Ortiz-Martinez et al (2011) further 
exploit user feedbacks for better IMT systems and 
user experience. Instead of triggered by user 
correction, our method is triggered by word 
delimiter and assists both translation and learning 
the target language. 
In contrast to the previous CAT research, we 
present a writing assistant that suggests grammar 
constructs as well as lexical translations following 
users? partial translation, aiming to provide users 
with choice to ease mental burden and enhance 
performance. 
3 The TransAhead System 
3.1 Problem Statement 
We focus on predicting a set of grammar patterns 
with lexical translations likely to follow the current 
partial target translation of a source text. The 
predictions will be examined by a human user 
directly. Not to overwhelm the user, our goal is to 
return a reasonable-sized set of predictions that 
contain suitable word choices and grammatical 
patterns to choose and learn from. Formally, 
Problem Statement: We are given a target-
language reference corpus Ct, a parallel corpus Cst, 
a source-language text S, and its translation prefix 
Tp. Our goal is to provide a set of predictions based 
on Ct and Cst likely to further translate S in terms of 
grammar and text. For this, we transform S and Tp 
into sets of ngrams such that the predominant 
grammar constructs with suitable translation 
options following Tp are likely to be acquired. 
(b) 
Source text: ???????????????? 
(a) 
Pop-up predictions/suggestions: 
we MD VB[play, act, ..]  (41369), ? 
we VBP[play, act, ..] DT  (13138), ? 
we VBD[play, act, ..] DT  (8139), ? 
Pop-up predictions/suggestions: 
play role IN[in] VBG[close, end, ..] (397), ? 
important role IN[in] VBG[close, end, ..]  (110), ? 
role IN[in] VBG[close, end, ..] (854), ? 
(c) 
(d) 
(e) 
Patterns for ?we?: 
we MD VB (41369), ?, 
we VBP DT (13138), ?, 
we VBD DT (8139), ? 
Patterns for ?we play an important role?: 
play role IN[in] DT (599), 
play role IN[in] VBG (397), ?, 
important role IN[in] VBG (110), ?, 
role IN[in] VBG (854), ? 
Translations for the source text: 
????: we, ?; ????: close, end, ?;  ?; ????: 
play, ?; ????: critical, ?; ?; ???: act, ?; ?; 
???: heavy, ?; ???: will, wish, ?; ???: cents, ?; 
???: outstanding, ? 
Input your source text and start to interact with TransAhead! 
353
3.2 Learning to Find Pattern and Translation 
In the training stage, we find and store syntax-
based phraseological tendencies and translation 
pairs. These patterns and translations are intended 
to be used in a real-time system to respond to user 
input speedily. 
First, we part of speech tag sentences in Ct. 
Using common phrase patterns (e.g., the 
possessive noun one?s in ?make up one?s mind?) 
seen in grammar books, we resort to parts-of-
speech (POS) for syntactic generalization. Then, 
we build up inverted files of the words in Ct for the 
next stage (i.e., pattern grammar generation). Apart 
from sentence and position information, a word?s 
lemma and POS are also recorded. 
Subsequently, we use the procedure in Figure 2 
to generate grammar patterns following any given 
sequence of words, either contiguous or skipped. 
 
 
Figure 2. Automatically generating pattern grammar. 
 
The algorithm first identifies the sentences 
containing the given sequence of words, query. 
Iteratively, Step (3) performs an AND operation on 
the inverted file, InvList, of the current word wi and 
interInvList, a previous intersected results. 
After that, we analyze query?s syntax-based 
phraseology (Step (5)). For each element of the 
form ([wordPosi(w1),?, wordPosi(wn)], sentence 
number) denoting the positions of query?s words in 
the sentence, we generate grammar pattern 
involving replacing words in the sentence with 
POS tags and words in wordPosi(wi) with lemmas, 
and extracting fixed-window 2  segments 
surrounding query from the transformed sentence. 
The result is a set of grammatical patterns (i.e., 
syntax-based phraseology) for the query. The 
procedure finally returns top N predominant 
                                                           
2
 Inspired by (Gamon and Leacock, 2010). 
syntactic patterns of the query. Such patterns 
characterizing the query?s word usages in the spirit 
of pattern grammar in (Hunston and Francis, 2000) 
and are collected across the target language. 
In the fourth and final stage, we exploit Cst for 
bilingual phrase acquisition, rather than a manual 
dictionary, to achieve better translation coverage 
and variety. We obtain phrase pairs through a 
number of steps, namely, leveraging IBM models 
for bidirectional word alignments, grow-diagonal-
final heuristics to extract phrasal equivalences 
(Koehn et al, 2003). 
3.3 Run-Time Grammar and Text Prediction 
Once translation equivalents and phraseological 
tendencies are learned, they are stored for run-time 
reference. TransAhead then predicts/suggests the 
following grammar and text of a translation prefix 
given the source text using the procedure in Figure 
3. 
 
 
Figure 3. Predicting pattern grammar and 
translations at run-time. 
 
We first slice the source text S into character-
level ngrams, represented by {si}. We also find the 
word-level ngrams of the translation prefix Tp. But 
this time we concentrate on the ngrams, may 
skipped, ending with the last word of Tp (i.e., 
pivoted on the last word) since these ngrams are 
most related to the subsequent grammar patterns. 
Step (3) and (4) retrieve translations and patterns 
learned from Section 3.2. Step (3) acquires the 
target-language active vocabulary that may be used 
to translate the source. To alleviate the word 
boundary issue in MT (Ma et al (2007)), the word 
boundary in our system is loosely decided. Initially, 
TransAhead non-deterministically segments the 
source text using character ngrams for translations 
and proceeds with collaborations with the user to 
obtain the segmentation for MT and to complete 
the translation. Note that Tp may reflect some 
translated segments, reducing the size of the active 
vocabulary, and that a user vocabulary of 
preference (due to users? domain knowledge or 
procedure PatternFinding(query,N,Ct) (1)  interInvList=findInvertedFile(w1 of query) 
for each word wi in query except for w1 (2)     InvList=findInvertedFile(wi) (3a)   newInterInvList= ? ; i=1; j=1 
(3b)   while i<=length(interInvList) and j<=lengh(InvList) 
(3c)      if interInvList[i].SentNo==InvList[j].SentNo 
(3d)         Insert(newInterInvList, interInvList[i],InvList[j]) 
else 
(3e)         Move i,j accordingly 
(3f)    interInvList=newInterInvList 
(4) Usage= ?  
for each element in interInvList 
(5)     Usage+={PatternGrammarGeneration(element,Ct)} (6) Sort patterns in Usage in descending order of frequency 
(7) return the N patterns in Usage with highest frequency 
procedure MakePrediction(S,Tp) 
(1) Assign sliceNgram(S) to {si} (2) Assign sliceNgramWithPivot(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptions 
           and incorporate them into GramOptions (6) Return GramOptions 
354
errors of the system) may be exploited for better 
system performance. In addition, Step (4) extracts 
patterns preceding with the history ngrams of {tj}. 
In Step (5), we first evaluate and rank the 
translation candidates using linear combination: 
( ) ( )( ) ( )1 1 1 2 2   i i pP t s P s t P t T? ?? + + ?  
where ?i is combination weight, P1 and P2 are 
translation and language model respectively, and t 
is one of the translation candidates under S and Tp. 
Subsequently, we incorporate the lemmatized 
translation candidates according to their ranks into 
suitable grammar constituents in GramOptions. 
For example, we would include ?close? in pattern 
?play role IN[in] VBG? as ?play role IN[in] 
VBG[close]?. 
At last, the algorithm returns the representative 
grammar patterns with confident translations 
expected to follow the ongoing translation and 
further translate the source. This algorithm will be 
triggered by word delimiter to provide an 
interactive CAT and CALL environment. Figure 1 
shows example responses of our working prototype. 
4 Preliminary Results 
In developing TransAhead, we used British 
National Corpus and Hong Kong Parallel Text as 
target-language reference corpus and parallel 
training corpus respectively, and deployed GENIA 
tagger for lemma and POS analyses. 
To evaluate TransAhead in CAT and CALL, we 
introduced it to a class of 34 (Chinese) college 
freshmen learning English as foreign language. We 
designed TransAhead to be accessible and intuitive, 
so the user training tutorial took only one minute. 
After the tutorial, the participants were asked to 
translate 15 Chinese texts from (Huang et al, 2011) 
(half with TransAhead assistance called experi-
mental group, and the other without any system 
help whatsoever called control group). The 
evaluation results show that the experimental 
group achieved much better translation quality than 
the control group with an average BLEU score 
(Papineni et al, 2002) of 35.49 vs. 26.46. 
Admittedly, the MT system Google Translate 
produced translations with a higher BLEU score of 
44.82. 
Google Translate obviously has much more 
parallel training data and bilingual translation 
knowledge. No previous work in CAT uses Google 
Translate for comparison. Although there is a 
difference in average translation quality between 
the experimental TransAhead group and the 
Google Translate, it is not hard for us to notice the 
source sentences were better translated by 
language learners with the help of TransAhead. 
Take the sentence  ????????????????
?? for example. A total of 90% of the participants 
in the experimental group produced more 
grammatical and fluent translations (see Figure 4) 
than that (?We conclude this transaction plays an 
important role?) by Google Translate. 
 
 
Figure 4. Example translations with 
TransAhead assistance. 
 
Post-experiment surveys indicate that (a) the 
participants found Google Translate lack human-
computer interaction while TransAhead is intuitive 
to collaborate with in translation/writing; (b) the 
participants found TransAhead grammar and 
translation predictions useful for their immediate 
task and for learning; (c) interactivity made the 
translation and language learning a fun process 
(like image tagging game of (von Ahn and Dabbish, 
2004)) and the participants found TransAhead very 
recommendable and would like to use it again in 
future translation tasks. 
5 Summary 
We have introduced a method for learning to offer 
grammar and text predictions expected to assist the 
user in translation and writing. We have 
implemented and evaluated the method. The 
preliminary results are encouragingly promising. 
As for the further work, we intend to evaluate and 
improve our system further in learner productivity 
in terms of output quality, typing speed, and the 
amount of using certain keys such as delete and 
backspace. 
Acknowledgement 
This study is conducted under the ?Project Digital 
Convergence Service Open Platform? of the 
Institute for Information Industry which is 
subsidized by the Ministry of Economy Affairs of 
the Republic of China. 
1. we play(ed) a critical role in closing this/the deal. 
2. we play(ed) a critical role in sealing this/the deal. 
3. we play(ed) an important role in ending this/the deal. 
4. we play(ed) an important role in closing this/the deal. 
355
References 
S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. 
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomas, E. 
Vidal, and J.-M. Vilar. 2008. Statistical approaches 
to computer-assisted translation. Computational 
Linguistics, 35(1): 3-28. 
R. D. Brown and S. Nirenburg. 1990. Human-computer 
interaction for semantic disambiguation. In 
Proceedings of COLING, pages 42-47. 
G. Foster, P. Langlais, E. Macklovitch, and G. Lapalme. 
2002. TransType: text prediction for translators. In 
Proceedings of ACL Demonstrations, pages 93-94. 
M. Gamon and C. Leacock. 2010. Search right and thou 
shalt find ? using web queries for learner error 
detection. In Proceedings of the NAACL Workshop. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, H.-C. Liou, and 
J. S. Chang. 2011. GRASP: grammar- and syntax-
based pattern-finder in CALL. In Proceedings of 
ACL Workshop. 
S. Hunston and G. Francis. 2000. Pattern Grammar: A 
Corpus-Driven Approach to the Lexical Grammar of 
English. Amsterdam: John Benjamins. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical 
phrase-based translation. In Proceedings of NAACL. 
P. Koehn. 2009. A web-based interactive computer 
aided translation tool. In Proceedings of ACL. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrapping 
word alignment via word packing. In Proceedings of 
ACL. 
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 
2004. Adaptive language and translation models for 
interactive machine translation. In Proceedings of 
EMNLP. 
D. Ortiz-Martinez, L. A. Leiva, V. Alabau, I. Garcia-
Varea, and F. Casacuberta. 2011. An interactive 
machine translation system with online learning. In 
Proceedings of ACL System Demonstrations, pages 
68-73. 
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. Bleu: 
a method for automatic evaluation of machine 
translation. In Proceedings of ACL, pages 311-318. 
L. von Ahn and L. Dabbish. 2004. Labeling images with 
a computer game. In Proceedings of CHI. 
356
Proceedings of the ACL 2010 Conference Short Papers, pages 115?119,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Collocation Suggestion in Academic Writing 
 Jian-Cheng Wu1 Yu-Chia Chang1,* Teruko Mitamura2 Jason S. Chang1 1 National Tsing Hua University Hsinchu, Taiwan {wujc86, richtrf, jason.jschang} @gmail.com 
2 Carnegie Mellon University  Pittsburgh, United States            teruko@cs.cmu.edu 
 Abstract In recent years, collocation has been widely acknowledged as an essential characteristic to distinguish native speak-ers from non-native speakers. Research on academic writing has also shown that collocations are not only common but serve a particularly important discourse function within the academic community. In our study, we propose a machine learning approach to implementing an online collocation writing assistant. We use a data-driven classifier to provide collocation suggestions to improve word choices, based on the result of classifica-tion. The system generates and ranks suggestions to assist learners? collocation usages in their academic writing with sat-isfactory results. * 1 Introduction The notion of collocation has been widely dis-cussed in the field of language teaching for dec-ades. It has been shown that collocation, a suc-cessive common usage of words in a chain, is important in helping language learners achieve native-like fluency. In the field of English for Academic Purpose, more and more researchers are also recognizing this important feature in academic writing. It is often argued that colloca-tion can influence the effectiveness of a piece of writing and the lack of such knowledge might cause cumulative loss of precision (Howarth, 1998). Many researchers have discussed the function of collocations in the highly conventionalized and specialized writing used within academia. Research also identified noticeable increases in the quantity and quality of collocational usage by                                                            * Corresponding author: Yu-chia Chang (Email address: richtrf@gmail.com) 
native speakers (Howarth, 1998). Granger (1998) reported that learners underuse native-like collo-cations and overuse atypical word combinations. This disparity in collocation usage between na-tive and non-native speakers is clear and should receive more attention from the language tech-nology community. To tackle such word usage problems, tradi-tional language technology often employs a da-tabase of the learners' common errors that are manually tagged by teachers or specialists (e.g. Shei and Pain, 2000; Liu, 2002). Such system then identifies errors via string or pattern match-ing and offer only pre-stored suggestions. Com-piling the database is time-consuming and not easily maintainable, and the usefulness is limited by the manual collection of pre-stored sugges-tions. Therefore, it is beneficial if a system can mainly use untagged data from a corpus contain-ing correct language usages rather than the error-tagged data from a learner corpus. A large corpus of correct language usages is more readily avail-able and useful than a small labeled corpus of incorrect language usages. For this suggestion task, the large corpus not only provides us with a rich set of common col-locations but also provides the context within which these collocations appear. Intuitively, we can take account of such context of collocation to generate more suitable suggestions. Contextual information in this sense often entails more lin-guistic clues to provide suggestions within sen-tences or paragraph. However, the contextual information is messy and complex and thus has long been overlooked or ignored. To date, most fashionable suggestion methods still rely upon the linguistic components within collocations as well as the linguistic relationship between mis-used words and their correct counterparts (Chang et al, 2008; Liu, 2009).  In contrast to other research, we employ con-textual information to automate suggestions for verb-noun lexical collocation. Verb-noun collo-cations are recognized as presenting the most 
115
challenge to students (Howarth, 1996; Liu, 2002). More specifically, in this preliminary study we start by focusing on the word choice of verbs in collocations which are considered as the most difficult ones for learners to master (Liu, 2002; Chang, 2008). The experiment confirms that our collocation writing assistant proves the feasibility of using machine learning methods to automatically prompt learners with collocation suggestions in academic writing.  2 Collocation Checking and Suggestion This study aims to develop a web service, Collo-cation Inspector (shown in Figure 1) that accepts sentences as input and generates the related can-didates for learners. In this paper, we focus on automatically pro-viding academic collocation suggestions when users are writing up their abstracts. After an ab-stract is submitted, the system extracts linguistic features from the user?s text for machine learning model. By using a corpus of published academic texts, we hope to match contextual linguistic clues from users? text to help elicit the most rele-vant suggestions. We now formally state the problem that we are addressing: Problem Statement: Given a sentence S writ-ten by a learner and a reference corpus RC, our goal is to output a set of most probable sugges-tion candidates c1, c2, ... , cm. For this, we train a classifier MC to map the context (represented as feature set f1, f2, ..., fn) of each sentence in RC to the collocations. At run-time, we predict these collocations for S as suggestions. 2.1 Academic Collocation Checker Train-ing Procedures Sentence Parsing and Collocation Extraction: We start by collecting a large number of ab-stracts from the Web to develop a reference cor-pus for collocation suggestion. And we continue to identify collocations in each sentence for the subsequent processing. Collocation extraction is an essential step in preprocessing data. We only expect to extract the collocation which comprises components having a syntactic relationship with one another. How-ever, this extraction task can be complicated. Take the following scholarly sentence from the reference corpus as an example (example (1)): 
(1) We introduce a novel method for learning to find documents on the web. 
 Figure 1. The interface for the collocation suggestion  
nsubj (introduce-2, We-1) det (method-5, a-3) amod (method-5, novel-4) dobj (introduce-2, method-5) prepc_for (introduce-2, learning-7) aux (find-9, to-8) ? ? Figure 2. Dependency parsing of Example (1)  Traditionally, through part-of-speech tagging, we can obtain a tagged sentence as follows (ex-ample (2)). We can observe that the desired col-location ?introduce method?, conforming to ?VERB+NOUN? relationship, exists within the sentence. However, the distance between these two words is often flexible, not necessarily rigid. Heuristically writing patterns to extract such verb and noun might not be effective. The patterns between them can be tremendously varied. In addition, some verbs and nouns are adjacent, but they might be intervened by clause and thus have no syntactic relation with one another (e.g. ?pro-pose model? in example (3)). 
(2) We/PRP  introduce/VB  a/DT  novel/JJ  method/NN  for/IN  learning/VBG  to/TO  find/VB  documents/NNS  on/IN  the/DT  web/NN  ./.  
(3) We proposed that the web-based model would be more ef-fective than corpus-based one. A natural language parser can facilitate the ex-traction of the target type of collocations. Such parser is a program that works out the grammati-cal structure of sentences, for instance, by identi-fying which group of words go together or which 
116
word is the subject or object of a verb. In our study, we take advantage of a dependency parser, Stanford Parser, which extracts typed dependen-cies for certain grammatical relations (shown in Figure 2). Within the parsed sentence of example (1), we can notice that the extracted dependency ?dobj (introduce-2, method-4)? meets the crite-rion.  Using a Classifier for the Suggestion task: A classifier is a function generally to take a set of attributes as an input and to provide a tagged class as an output. The basic way to build a clas-sifier is to derive a regression formula from a set of tagged examples. And this trained classifier can thus make predication and assign a tag to any input data. The suggestion task in this study will be seen as a classification problem. We treat the colloca-tion extracted from each sentence as the class tag (see examples in Table 1). Hopefully, the system can learn the rules between tagged classes (i.e. collocations) and example sentences (i.e. schol-arly sentences) and can predict which collocation is the most appropriate one given attributes ex-tracted from the sentences. Another advantage of using a classifier to automate suggestion is to provide alternatives with regard to the similar attributes shared by sentences. In Table 1, we can observe that these collocations exhibit a similar discourse function and can thus become interchangeable in these sentences. Therefore, based on the outputs along with the probability from the classifier, we can provide more than one adequate suggestions.  Feature Selection for Machine Learning: In the final stage of training, we build a statistical machine-learning model. For our task, we can use a supervised method to automatically learn the relationship between collocations and exam-ple sentences. We choose Maximum Entropy (ME) as our train-ing algorithm to build a collocation suggestion classifier. One advantage of an ME classifier is that in addition to assigning a classification it can provide the probability of each assignment. The ME framework estimates probabilities based on the principle of making as few assumptions as possible. Such constraints are derived from the training data, expressing relationships between features and outcomes.  Moreover, an effective feature selection can increase the precision of machine learning. In our study, we employ the contextual features which  
Table 1. Example sentences and class tags (colloca-tions) Example Sentence  Class tag   We introduce a novel method for learning to find documents on the web.  introduce  We presented a method of improving Japa-nese dependency parsing by using large-scale statistical information.  present  In this paper, we will describe a method of identifying the syntactic role of antece-dents, which consists of two phases  describe  In this paper, we suggest a method that automatically constructs an NE tagged cor-pus from the web to be used for learning of NER systems.  suggest   consist of two elements, the head and the ngram of context words:  Head: Each collocation comprises two parts, collocate and head. For example, in a given verb-noun collocation, the verb is the collocate as well as the target for which we provide suggestions; the noun serves as the head of collocation and convey the essential meaning of the collocation. We use the head as a feature to condition the classifier to generate candidates relevant to a given head.  Ngram: We use the context words around the target collocation by considering the correspond-ing unigrams and bigrams words within the sen-tence. Moreover, to ensure the relevance, those context words, before and after the punctuation marks enclosing the collocation in question, will be excluded. We use the parsed sentence from previous step (example (2)) to show the extracted context features1 (example (4)): 
(4) CN=method UniV_L=we UniV_R=a UniV_R=novel UniN_L=a UniN_L=novel UniN_R=for UniN_R=learn BiV_R=a_novel BiN_L=a_novel BiN_R=for_learn BiV_I=we_a BiN_I=novel_for  
                                                           1 CN refers to the head within collocation. Uni and Bi indi-cate the unigram and bigram context words of window size two respectively. V and N differentiate the contexts related to verb or noun. The ending alphabets L, R, I show the posi-tion of the words in context, L = left, R = right, and I = in between. 
117
2.2 Automatic Collocation Suggestion at Run-time After the ME classifier is automatically trained, the model is used to find out the best collocation suggestion. Figure 3 shows the algorithm of pro-ducing suggestions for a given sentence. The input is a learner?s sentence in an abstract, along with an ME model trained from the reference corpus.  In Step (1) of the algorithm, we parse the sen-tence for data preprocessing. Based on the parser output, we extract the collocation from a given sentence as well as generate features sets in Step (2) and (3). After that in Step (4), with the trained machine-learning model, we obtain a set of likely collocates with probability as predicted by the ME model. In Step (5), SuggestionFilter singles out the valid collocation and returns the best collocation suggestion as output in Step (6). For example, if a learner inputs the sentence like Example (5), the features and output candidates are shown in Table 2. 
(5) There are many investiga-tions about wireless network communication, especially it is important to add Internet transfer calculation speeds.  3 Experiment From an online research database, CiteSeer, we have collected a corpus of 20,306 unique ab-stracts, which contained 95,650 sentences. To train a Maximum Entropy classifier, 46,255 col-locations are extracted and 790 verbal collocates are identified as tagged classes for collocation suggestions. We tested the classifier on scholarly sentences in place of authentic student writings which were not available at the time of this pilot study. We extracted 364 collocations among 600 randomly selected sentences as the held out test data not overlapping with the training set. To automate the evaluation, we blank out the verb collocates within these sentences and treat these verbs directly as the only correct suggestions in question, although two or more suggestions may be interchangeable or at least appropriate. In this sense, our evaluation is an underestimate of the performance of the proposed method.    While evaluating the quality of the suggestions provided by our system, we used the mean recip-rocal rank (MRR) of the first relevant sugges-tions returned so as to assess whether the sugges-tion list contains an answer and how far up the answer is in the list as a quality metric of the sys-  
Procedure CollocationSuggestion(sent, MEmodel)   (1)   parsedSen = Parsing(sent) (2)   extractedColl = CollocationExtraction(parsedSent) (3)   features = AssignFeature(ParsedSent)   (4)   probCollection = MEprob(features, MEmodel)    (5)   candidate = SuggestionFilter(probCollection) (6)   Return candidate  Figure 3. Collocation Suggestion at Run-time  Table 2. An example from learner?s sentence Extracted Collocation Features Ranked Candidates 
add speed 
CN=speed UniV_L=important UniV_L=to UniV_R=internet UniV_R=transfer UniN_L=transfer UniN_L=calculation BiV_L=important_to BiV_R=internet_transfer BiN_L=transfer_calcula-tion BiV_I=to_intenet 
improve increase determine maintain ? ? 
 Table 3. MRR for different feature sets Feature Sets Included In Classifier MRR  Features of HEAD 0.407 Features of CONTEXT 0.469 Features of HEAD+CONTEXT 0.518  tem output. Table 3 shows that the best MRR of our prototype system is 0.518. The results indi-cate that on average users could easily find an-swers (exactly reproduction of the blanked out collocates) in the first two to three ranking of suggestions. It is very likely that we get a much higher MMR value if we would go through the lists and evaluate each suggestion by hand. Moreover, in Table 3, we can further notice that contextual features are quite informative in com-parison with the baseline feature set containing merely the feature of HEAD. Also the integrated feature set of HEAD and CONTEXT together achieves a more satisfactory suggestion result. 4 Conclusion Many avenues exist for future research that are important for improving the proposed method. For example, we need to carry out the experi-ment on authentic learners? texts. We will con-duct a user study to investigate whether our sys-tem would improve a learner?s writing in a real setting. Additionally, adding classifier features based on the translation of misused words in learners? text could be beneficial  (Chang et al, 
118
2008). The translation can help to resolve preva-lent collocation misuses influenced by a learner's native language. Yet another direction of this research is to investigate if our methodology is applicable to other types of collocations, such as AN and PN in addition to VN dealt with in this paper. In summary, we have presented an unsuper-vised method for suggesting collocations based on a corpus of abstracts collected from the Web. The method involves selecting features from the reference corpus of the scholarly texts. Then a classifier is automatically trained to determine the most probable collocation candidates with regard to the given context. The preliminary re-sults show that it is beneficial to use classifiers for identifying and ranking collocation sugges-tions based on the context features.  Reference Y. Chang, J. Chang, H. Chen, and H. Liou. 2008. An automatic collocation writing assistant for Taiwan-ese EFL learners: A case of corpus-based NLP technology. Computer Assisted Language Learn-ing, 21(3), pages 283-299. S. Granger. 1998. Prefabricated patterns in advanced EFL writing: collocations and formulae. In Cowie, A. (ed.) Phraseology: theory, analysis and applica-tions. Oxford University Press, Oxford, pages 145-160. P. Howarth. 1996. Phraseology in English Academic Writing. T?bingen: Max Niemeyer Verlag. P. Howarth. 1998. The phraseology of learner?s aca-demic writing. In Cowie, A. (ed.) Phraseology: theory, analysis and applications. Oxford Univer-sity Press, Oxford, pages 161-186. D. Hawking and N. Craswell. 2002. Overview of the TREC-2001 Web track. In Proceedings of the 10th Text Retrieval Conference (TREC 2001), pages 25-31. L. E. Liu. 2002. A corpus-based lexical semantic in-vestigation of verb-noun miscollocations in Taiwan learners? English. Unpublished master?s thesis, Tamkang University, Taipei, January. A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated suggestions for miscollocations. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 47-50. C. C. Shei and H. Pain. 2000. An ESL writer?s collo-cational aid. Computer Assisted Language Learn-ing, 13, pages 167-182. 
119
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 26?31,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar
Chung-Chi Huang Mei-Hua Chen Shih-Ting Huang Jason S. Chang
Institute of Information Systems and Department of Computer Science,
Applications, National Tsing Hua University, National Tsing Hua University,
HsinChu, Taiwan, R.O.C. 300 HsinChu, Taiwan, R.O.C. 300
{u901571,chen.meihua,koromiko1104,Jason.jschang}@gmail.com
Abstract
We introduce a new method for learning to 
detect grammatical errors in learner?s writ-
ing and provide suggestions. The method 
involves parsing a reference corpus and 
inferring grammar patterns in the form of a 
sequence of content  words, function words, 
and parts-of-speech (e.g., ?play ~ role in 
Ving? and ?look forward to  Ving?). At run-
time, the given passage submitted by the 
learner is matched using an extended 
Levenshtein algorithm against  the set  of 
pattern rules in order to detect  errors and 
provide suggestions. We present a proto-
type implementation of the proposed 
method, EdIt, that  can handle a broad range 
of errors. Promising results are illustrated 
with three common types of errors in non-
native writing.
1 Introduction
Recently, an increasing number of research has 
targeted language learners? need in editorial assis-
tance including detecting and correcting grammar 
and usage errors in texts written in a second lan-
guage. For example, Microsoft  Research has de-
veloped the ESL Assistant, which provides such a 
service to ESL and EFL learners.
Much of the research in this area depends on 
hand-crafted rules and focuses on certain error 
types. Very little research provides a general 
framework for detecting and correcting all types of 
errors. However, in the sentences of ESL writing, 
there may be more than one errors and one error 
may affect the performance of handling other er-
rors. Erroneous sentences could be more efficiently 
identified and corrected if a grammar checker han-
dles all errors at  once, using a set of pattern rules 
that reflect the predominant usage of the English 
language.
Consider the sentences, ?He play an important 
roles to close this deals.? and ?He looks forward to 
hear you.? The first  sentence contains inaccurate 
word forms (i.e., play, roles, and deals), and rare 
usage (i.e., ?role to close?), while the second sen-
tence use the incorrect verb form of ?hear?. Good 
responses to these writing errors might  be (a) Use 
?played? instead of ?play.? (b) Use ?role? instead 
of ?roles?,  (c) Use ?in closing? instead of ?to 
close? (d) Use ?to hearing? instead of ?to hear?, 
and (e) insert  ?from? between ?hear? and ?you.? 
These suggestions can be offered by learning the 
patterns rules related to ?play ~ role? and ?look 
forward? based on analysis of ngrams and collo-
cations in a very large-scale reference corpus. With 
corpus statistics, we could learn the needed phra-
seological tendency in the form of pattern rules 
such as ?play ~ role in  V-ing) and ?look forward 
to V-ing.? The use of such pattern rules is in line 
with the recent  theory of Pattern Grammar put 
forward by Hunston and Francis (2000).
We present  a system, EdIt, that automatically 
learns to provide suggestions for rare/wrong usages 
in non-native writing. Example EdIt  responses to a 
26
text are shown in Figure 1. EdIt has retrieved the 
related pattern grammar of some ngram and collo-
cation sequences given the input  (e.g., ?play ~ role 
in V-ing
1
?, and ?look forward to V-ing?). EdIt 
learns these patterns during pattern extraction 
process by syntactically analyzing a collection of 
well-formed, published texts.
At run-time, EdIt first processes the input  pas-
sages in the article (e.g., ?He play an important 
roles to close ?) submitted by the L2 learner. And 
EdIt  tag the passage with part  of speech informa-
tion, and compares the tagged sentence against  the 
pattern rules anchored at  certain collocations (e.g., 
?play ~ role? and ?look forward?). Finally, EdIt 
finds the minimum-edit-cost  patterns matching the 
passages using an extended Levenshtein?s algo-
rithm (Levenshtein, 1966). The system then high-
lights the edits and displays the pattern rules as 
suggestions for correction. In our prototype, EdIt 
returns the preferred word form and preposition 
usages to the user directly (see Figure 1); alterna-
tively, the actual surface words (e.g., ?closing? and 
?deal?) could be provided.
Input:
Related pattern rules
play ~ role in Noun
play ~ role in V-ing
he plays DET
he played DET
look forward to V-ing
hear from PRON ...
Suggestion:
He played an important role in closing this deal. He looks 
forward to hearing from you.
He play an important roles to close this 
deals. He looks forward to hear you.
Figure 1. Example responses to the non-native writing.
2 Related Work
Grammar checking has been an area of active re-
search. Many methods, rule-oriented or data-
driven, have been proposed to tackle the problem 
of detecting and correcting incorrect grammatical 
and usage errors in learner texts. It is at  times no 
easy to distinguish these errors. But Fraser and 
Hodson (1978) shows the distinction between these 
two kinds of errors.
For some specific error types (e.g., article and 
preposition error), a number of interesting rule-
based systems have been proposed. For example, 
Uria et al (2009) and Lee et  al. (2009) leverage 
heuristic rules for detecting Basque determiner and 
Korean particle errors, respectively. Gamon et  al. 
(2009) bases some of the modules in ESL Assistant 
on rules derived from manually inspecting learner 
data. Our pattern rules, however, are automatically 
derived from readily available well-formed data, 
but nevertheless very helpful for correcting errors 
in non-native writing.
More recently, statistical approaches to develop-
ing grammar checkers have prevailed. Among un-
supervised checkers, Chodorow and Leacock 
(2000) exploits negative evidence from edited tex-
tual corpora achieving high precision but low re-
call, while Tsao and Wible (2009) uses general 
corpus only. Additionally, Hermet et al (2008) and 
Gamon and Leacock (2010) both use Web as a 
corpus to detect  errors in non-native writing. On 
the other hand, supervised models, typically treat-
ing error detection/correction as a classification 
problem, may train on well-formed texts as in the 
methods by De Felice and Pulman (2008) and Te-
treault et  al. (2010), or with additional learner texts 
as in the method proposed by Brockett et al 
(2006). Sun et  al. (2007) describes a method for 
constructing a supervised detection system trained 
on raw well-formed and learner texts without error 
annotation. 
Recent work has been done on incorporating 
word class information into grammar checkers. For 
example, Chodorow and Leacock (2000) exploit 
bigrams and trigrams of function words and part-
of-speech (PoS) tags, while Sun et al (2007) use 
labeled sequential patterns of function, time ex-
pression, and part-of-speech tags. In an approach 
similar to our work, Tsao and Wible (2009) use a 
combined ngrams of words forms, lemmas, and 
part-of-speech tags for research into constructional 
phenomena. The main differences are that  we an-
chored each pattern rule in lexical collocation so 
as to avoid deriving rules that  is may have two 
1
 In the pattern rules, we translate the part-of-speech tag to labels that are commonly used in learner dictionaries. For 
instance, we use V-ing for the tag VBG denoting the progressive verb form, and Pron and Pron$ denotes a pronoun 
and a possessive pronoun respectively.
27
consecutive part-of-speech tags (e.g, ?V Pron$ 
socks off?). The pattern rules we have derived are 
more specific and can be effectively used in detect-
ing and correcting errors.
In contrast  to the previous research, we intro-
duce a broad-coverage grammar checker that ac-
commodates edits such as substitution, insertion 
and deletion, as well as replacing word forms or 
prepositions using pattern rules automatically de-
rived from very large-scale corpora of well-formed 
texts.
3 The EdIt System
Using supervised training on a learner corpus is not 
very feasible due to the limited availability of 
large-scale annotated non-native writing. Existing 
systems trained on learner data tend to offer high 
precision but low recall. Broad coverage grammar 
checkers may be developed using readily available 
large-scale corpora. To detect  and correct errors in 
non-native writing, a promising approach is to 
automatically extract  lexico-syntactical pattern 
rules that  are expected to distinguish correct and in 
correct sentences.
3.1 Problem Statement
We focus on correcting grammatical and usage 
errors by exploiting pattern rules of specific collo-
cation (elastic or rigid such as ?play ~ rule? or 
?look forward?). For simplification, we assume 
that there is no spelling errors. EdIt provides sug-
gestions to common writing errors
2
 of the follow-
ing correlated with essay scores
3
.
(1)  wrong word form
(A) singular determiner preceding plural noun
(B) wrong verb form: concerning modal verbs (e.g., 
?would said?), subject-verb agreement, auxiliary 
(e.g., ?should have tell the truth?), gerund and in-
finitive usage (e.g., ?look forward to see you? and 
?in an attempt to helping you?)
(2) wrong preposition (or infinitive-to)
(A) wrong preposition (e.g., ?to depends of it?)
(B) wrong preposition and verb form (e.g., ?to play 
an important role to close this deal?)
(3) transitivity errors
(A) transitive verb (e.g., ?to discuss about the mat-
ter? and ?to affect to his decision?)
(B) intransitive verb (e.g., ?to listens the music?)
The system is designed to find pattern rules related 
to the errors and return suggestionst. We now for-
mally state the problem that we are addressing.
Problem  Statement: We are given a reference 
corpus C and a non-native passage T. Our goal is 
to detect  grammatical and usage errors in T and 
provide suggestions for correction. For this, we 
extract a set of pattern rules, u
1
,?, u
m
 from C 
such that the rules reflect the predominant usage 
and are likely to distinguish most errors in non-
native writing. 
In the rest  of this section, we describe our solu-
tion to this problem. First, we define a strategy for 
identifying predominant  phraseology of frequent 
ngrams and collocations in Section 3.2. Afer that, 
we show how EdIt proposes grammar correc-
tionsedits to non-native writing at  run-time in Sec-
tion 3.3.
3.2 Deriving Pattern Rules
We attempt  to derive patterns (e.g., ?play ~ role in 
V-ing?) from C expected to represent the immedi-
ate context  of collocations (e.g., ?play ~ role? or 
?look forward?). Our derivation process consists of 
the following four-stage:
Stage 1. Lemmatizing, POS Tagging and Phrase 
chunking. In the first  stage, we lemmatize and tag 
sentences in C. Lemmatization and POS tagging 
both help to produce more general pattern rules 
from ngrams or collocations. The based phrases are 
used to extract collocations.
Stage 2. Ngrams and Collocations. In the second 
stage of the training process, we calculate ngrams 
and collocations in C, and pass the frequent 
ngrams and collocations to Stage 4.
We employ a number of steps to acquire statisti-
cally significant collocations--determining the pair 
of head words in adjacent base phrases, calculating 
their pair-wise mutual information values, and fil-
tering out candidates with low MI values. 
Stage 3. onstructing Inverted Files. In the third 
stage in the training procedure, we build up in-
verted files for the lemmas in C for quick access in 
Stage 4. For each word lemma we store surface 
words, POS tags, pointers to sentences with base 
phrases marked.
2
 See (Nicholls, 1999) for common errors.
3
 See (Leacock and Chodorow, 2003) and (Burstein et al, 2004) for correlation.
28
procedure GrammarChecking(T,PatternGrammarBank)
(1) Suggestions=??//candidate suggestions
(2) sentences=sentenceSplitting(T)
for each sentence in sentences
(3)   userProposedUsages=extractUsage(sentence)
for each userUsage in userProposedUsages
(4)     patGram=findPatternGrammar(userUsage.lexemes, 
PatternGrammarBank)
(5)     minEditedCost=SystemMax; minEditedSug=??
for each pattern in patGram
(6)        cost=extendedLevenshtein(userUsage,pattern)
if cost<minEditedCost
(7)            minEditedCost=cost; minEditedSug=pattern
if minEditedCost>0
(8)       append (userUsage,minEditedSug) to Suggestions
(9) Return Suggestions
Figure 2. Grammar suggestion/correction at run-time
Stage 4. Deriving pattern rules. In the fourth and 
final stage, we use the method described in a pre-
vious work (Chen et al, 2011) and use the inverted 
files to find all sentences containing a give word 
and collocation. Words surrounding a collocation 
are identified and generalized based on their corre-
sponding POS tags. These sentences are then trans-
formed into a set  of n-gram of words and POS 
tags, which are subsequently counted and ranked to 
produce  pattern rules with high frequencies. 
3.3 Run-Time Error Correction
Once the patterns rules are derived from a corpus 
of well-formed texts, EdIt utilizes them to check 
grammaticality and provide suggestions for a given 
text via the procedure in Figure 2.
In Step (1) of the procedure, we initiate a set 
Suggestions to collect grammar suggestions to the 
user text T according to the bank of pattern gram-
mar PatternGrammarBank. Since EdIt  system fo-
cuses on grammar checking at  sentence level, T is 
heuristically split (Step (2)).
For each sentence, we extract ngram and POS 
tag sequences userUsage in T. For the example of 
?He play an important  roles. He looks forword to 
hear you?,  we extract ngram such as he V DET, 
play an JJ NNS, play ~ roles to V, this NNS, look 
forward to VB, and hear Pron. 
For each userUsage, we first access the pattern 
rules related to the word and collocation within 
(e.g., play-role  patterns for ?play ~ role to close?) 
Step (4). And then we compare userUsage against 
these rules (from Step (5) to (7)). We use the ex-
tended Levenshtein?s algorithm shown in Figure 3 
to compare userUsage and pattern rules.
Figure 3. Algorithm for identifying errors
If only partial matches are found for userUsage, 
that could mean we have found a potential errors. 
We use minEditedCost and minEditedSug to con-
train the patterns rules found for error suggestions 
(Step (5)). In the following, we describe how to 
find minimal-distance edits.
In Step (1) of the algorithm in Figure 3 we allo-
cate and initialize costArray to gather the dynamic 
programming based cost  to transform userUsage 
into a specific contextual rule pattern. Afterwards, 
the algorithm defines the cost of performing substi-
tution (Step (2)), deletion (Step (3)) and insertion 
(Step (4)) at  i-indexed userUsage and j-indexed 
pattern. If the entries userUsage[i] and pattern[j] 
are equal literally (e.g., ?VB? and ?VB?) or gram-
matically (e.g., ?DT? and ?Pron$?), no edit  is 
needed, hence, no cost  (Step (2a)). On the other 
hand, since learners tend to select wrong word 
form and preposition, we set a lower cost  for  sub-
stitution among different word forms of the same 
lemma or lemmas with the same POS tag (e.g., 
replacing V with V-ing or replacing to with in?. In 
addition to the conventional deletion and insertion 
(Step (3b) and (4b) respectively), we look ahead to 
the elements userUsage[i+1] and pattern[j+1] con-
sidering the fact that  ?with or without preposition? 
and ?transitive or intransitive verb? often puzzles 
EFL learners (Step (3a) and (4a)). Only a small 
edit cost is counted if the next  elements in use-
rUsage and Pattern are ?equal?. In Step (6) the 
extended Levenshtein?s algorithm returns the 
minimum edit  cost of revising userUsage using 
pattern.
Once we obtain the costs to transform the use-
rUsage into a similar, frequent pattern rules, we 
propose the minimum-cost rules as suggestions  for 
procedure extendedLevenshtein(userUsage,pattern)
(1) allocate and initialize costArray
for i in range(len(userUsage))
for j in range(len(pattern))
if equal(userUsage[i],pattern[j]) //substitution
(2a)       substiCost=costArray[i-1,j-1]+0
elseif sameWordGroup(userUsage[i],pattern[j])
(2b)       substiCost=costArray[i-1,j-1]+0.5
(2c)     else substiCost=costArray[i-1,j-1]+1
if equal(userUsage[i+1],pattern[j+1]) //deletion
(3a)       delCost=costArray[i-1,j]+smallCost
(3b)     else delCost=costArray[i-1,j]+1
if equal(userUsage[i+1],pattern[j+1]) //insertion
(4a)        insCost=costArray[i,j-1]+smallCost
(4b)      else insCost=costArray[i,j-1]+1
(5)        costArray[i,j]=min(substiCost,delCost,insCost)
(6) Return costArray[len(userUsage),len(pattern)]
29
correction (e.g., ?play ~ role in V-ing? for revising 
?play ~ role to V?) (Step (8) in Figure 2), if its 
minimum edit  cost  is greater than zero. Otherwise, 
the usage is considered valid. Finally, the Sugges-
tions accumulated for T are returned to users (Step 
(9)). Example input  and editorial suggestions re-
turned to the user are shown in Figure 1. Note that 
pattern rules involved flexible collocations are de-
signed to take care of long distance dependencies 
that might be always possible to cover with limited 
ngram (for n less than 6). In addition, the long pat-
ter rules can be useful even when it  is not  clear 
whether there is an error when looking at a very 
narrow context. For example, ?hear? can be either 
be transitive or intransitive depending on context. 
In the context of ?look forward to? and person 
noun object, it is should be intransitive and  require 
the preposition ?from? as suggested in the results 
provided by EdIt (see Figure 1).
In existing grammar checkers, there are typically 
many modules examining different types of errors 
and different module may have different  priority 
and conflict  with one another. Let  us note that this 
general framework for error detection and correc-
tion is an original contribution of our work. In ad-
dition, we incorporate probabilities conditioned on 
word positions in order to weigh edit  costs. For 
example, the conditional probability of V to imme-
diately follow ?look forward to? is virtually  0, 
while the probability of V-ing  to do so is approxi-
mates 0.3. Those probabilistic values are used to 
weigh different edits. 
4 Experimental Results
In this section, we first present the experimental 
setting in EdIt (Section 4.1). Since our goal is to 
provide to learners a means to efficient  broad-
coverage grammar checking, EdIt  is web-based 
and the acquisition of the pattern grammar in use is 
offline. Then, we illustrate three common types of 
errors, scores correlated, EdIt
4
 capable of handling.
4.1 Experimental Setting
We used British National Corpus (BNC) as our 
underlying general corpus C. It is a 100 million 
British English word collection from a wide range 
of sources. We exploited GENIA tagger to obtain 
the lemmas, PoS tags and shallow parsing results 
of C?s sentences, which were all used in construct-
ing inverted files and used as examples for GRASP 
to infer lexicalized pattern grammar.
Inspired by (Chen et al, 2011) indicating EFL 
learners tend to choose incorrect  prepositions and 
following word forms following a VN collocation, 
and (Gamon and Leacock, 2010) showing fixed-
length and fixed-window lexical items are the best 
evidence for correction, we equipped EdIt with 
pattern grammar rules consisting of fixed-length 
(from one- to five-gram) lexical sequences or VN 
collocations and their fixed-window usages (e.g., 
?IN(in) VBG? after ?play ~ role?, for window 2).
4.2 Results
We examined three types of errors and the mixture 
of them for our correction system (see Table 1). In 
this table, results of ESL Assistant  are shown for 
comparison, and grammatical suggestions are un-
derscored. As suggested, lexical and PoS informa-
tion in learner texts is useful for a grammar 
checker, pattern grammar EdIt  uses is easily acces-
sible and effective in both grammaticality and us-
age check, and a weighted extension to Leven-
shtein?s algorithm in EdIt  accommodates substitu-
tion, deletion and insertion edits to learners? fre-
quent mistakes in writing.
5 Future Work and Summary 
Many avenues exist  for future research and im-
provement. For example, we could augment  pat-
tern grammar with lexemes? PoS information in 
that the contexts of a word of different PoS tags 
vary. Take discuss for instance. The present tense 
verb discuss is often followed by determiners and 
nouns while the passive is by the preposition in  as 
in ?? is discussed in Chapter one.? Additionally, 
an interesting direction to explore is enriching pat-
tern grammar with semantic role labels (Chen et 
al., 2011) for simple semantic check.
In summary, we have introduced a method for 
correcting errors in learner text based on its lexical 
and PoS evidence. We have implemented the 
method and shown that the pattern grammar and 
extended Levenshtein algorithm in this method are 
promising in grammar checking. Concerning EdIt?s 
broad coverage over different  error types, simplic-
ity in design, and short  response time, we plan to 
evaluate it more fully: with or without  conditional 
probability using majority voting or not.
4
 At http://140.114.214.80/theSite/EdIt_demo2/
30
Erroneous sentence EdIt suggestion ESL Assistant suggestion
Incorrect word form
? a sunny days ? a sunny N a sunny day
every days, I ? every N every day
I would said to ? would V would say
he play a ? he V-ed none
? should have tell the truth should have V-en should have to tell
? look forward to see you look forward to V-ing none
? in an attempt to seeing you an attempt to V none
? be able to solved this problem able to V none
Incorrect preposition
he plays an important role to close ? play ~ role in none
he has a vital effect at her. have ~ effect on effect on her
it has an effect on reducing ? have ~ effect of V-ing none
? depend of the scholarship depend on depend on
Confusion between intransitive and transitive verb
he listens the music. missing ?to? after ?listens? missing ?to? after ?listens?
it affects to his decision. unnecessary ?to? unnecessary ?to?
I understand about the situation. unnecessary ?about? unnecessary ?about?
we would like to discuss about this matter. unnecessary ?about? unnecessary ?about?
Mixed
she play an important roles to close this deals. she V-ed; an Adj N;
play ~ role in V-ing; this N
play an important role;
close this deal
I look forward to hear you. look forward to V-ing;
missing ?from? after ?hear?
none
Table 1. Three common score-related error types and their examples with suggestions from EdIt and ESL Assistant.
References
C. Brockett, W. Dolan, and M. Gamon. 2006. Correcting ESL 
errors using phrasal SMT techniques. In Proceedings of the 
ACL.
J. Burstein, M. Chodorow, and C. Leacock. 2004. Automated 
essay evaluation: the criterion online writing  service. AI 
Magazine, 25(3):27-36.
M. H. Chen, C. C. Huang, S. T. Huang, H. C. Liou, and J. S. 
Chang. 2011. A cross-lingual pattern retrieval framework. 
In Proceedings of the CICLing.
M. Chodorow and C. Leacock. 2000. An unsupervised method 
for detecting grammatical  errors. In Proceedings of the 
NAACL, pages 140-147.
R. De Felice and S. Pulman. 2008. A classifer-based approach 
to  preposition and determiner error correction in  L2 Eng-
lish. In COLING.
I. S. Fraser and L. M. Hodson. 1978. Twenty-one kicks at the 
grammar horse. English Journal.
M. Gamon, C. Leacock, C. Brockett, W. B. Bolan, J. F. Gao, 
D. Belenko, and A. Klementiev.  Using statistical tech-
niques and web search to correct ESL errors. CALICO, 
26(3): 491-511.
M. Gamon and C. Leacock. 2010. Search right and thou shalt 
find ? using web queries for learner error detection. In 
Proceedings of the NAACL.
M. Hermet, A. Desilets, S. Szpakowicz. 2008. Using the web 
as a linguistic resource to automatically correct lexico-
syntatic errors. In LREC, pages 874-878.
S. Hunston and G. Francis. 2000. Pattern grammar: a corpus-
driven approach to the lexical grammar of English.
C. M. Lee, S. J. Eom, and M. Dickinson. 2009. Toward ana-
lyzing Korean learner particles. In CALICO.
V. I. Levenshtein. 1966. Binary codes capable of correcting 
deletions, insertions and reversals. Soviet Physics Doklady, 
10:707-710.
C. Leacock and M. Chodorow. 2003. Automated grammatical 
error detection.
D. Nicholls. 1999. The Cambridge Learner Corpus ? error 
coding and analysis for writing dictionaries and other 
books for English Learners.
G. H. Sun, X. H. Liu, G. Cong, M. Zhou, Z. Y. Xiong, J. Lee, 
and C. Y. Lin. 2007. Detecting erroneous sentences using 
automatically mined sequential patterns. In ACL.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse 
features for prepositions selection and error detection. In 
Proceedings of the ACL, pages 353-358.
N. L. Tsao and D. Wible. 2009. A method for unsupervised 
broad-coverage lexical error detection and correction. In 
NAACL Workshop, pages 51-54.
31
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 130?134,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
 
 
Learning to Find Translations and Transliterations on the Web 
 
 
Joseph Z. Chang Jason S. Chang Jyh-Shing Roger Jang 
Department of Computer Science, Department of Computer Science, Department of Computer Science, 
National Tsing Hua University National Tsing Hua University National Tsing Hua University 
101, Kuangfu Road,  
Hsinchu, 300, Taiwan 
101, Kuangfu Road,  
Hsinchu, 300, Taiwan 
101, Kuangfu Road,  
Hsinchu, 300, Taiwan 
joseph.nthu.tw@gmail.com jschang@cs.nthu.edu.tw jang@cs.nthu.edu.tw 
 
 
 
 
Abstract 
In this paper, we present a new method 
for learning to finding translations and 
transliterations on the Web for a given 
term. The approach involves using a small 
set of terms and translations to obtain 
mixed-code snippets from a search engine, 
and automatically annotating the snippets 
with tags and features for training a 
conditional random field model. At run-
time, the model is used to extracting 
translation candidates for a given term. 
Preliminary experiments and evaluation 
show our method cleanly combining 
various features, resulting in a system that 
outperforms previous work.  
1 Introduction 
The phrase translation problem is critical to 
machine translation, cross-lingual information 
retrieval, and multilingual terminology (Bian and 
Chen 2000, Kupiec 1993). Such systems typically 
use a parallel corpus. However, the out of 
vocabulary problem (OOV) is hard to overcome 
even with a very large training corpus due to the 
Zipf nature of word distribution, and ever growing 
new terminology and named entities. Luckily, 
there are an abundant of webpages consisting 
mixed-code text, typically written in one language 
but interspersed with some sentential or phrasal 
translations in another language. By retrieving and 
identifying such translation counterparts on the 
Web, we can cope with the OOV problem. 
Consider the technical term named-entity 
recognition. The best places to find the Chinese 
translations for named-entity recognition are 
probably not some parallel corpus or dictionary, 
but rather mixed-code webpages. The following 
example is a snippet returned by the Bing search 
engine for the query, named entity recognition: 
 
... ?????????????? (Natural Language 
Parsing)????? (Question Classification)????? 
(Named Entity Recognition)?? ... 
 
This snippet contains three technical terms in 
Chinese (i.e., ?????? zhiran yuyan poxi, 
???? wenti fenlei, ???? zhuanming 
bianshi), followed by source terms in brackets 
(respectively, Natural Language Parsing, Question 
Classification, and Named Entity Recognition). 
Quoh (2006) points out that submitting the source 
term and partial translation to a search engine is a 
good strategy used by many translators. 
Unfortunately, the user still has to sift through 
snippets to find the translations. For a given 
English term, such translations can be extracted by 
casting the problem as a sequence labeling task for 
classifying the Chinese characters in the snippets 
as either translation or non-translation. Previous 
work has pointed out that such translations usually 
exhibit characteristics related to word translation, 
word transliteration, surface patterns, and 
proximity to the occurrences of the original phrase 
(Nagata et. al 2001 and Wu et. al 2005). 
130
  
Thus, we also associate features to each Chinese 
token (characters or words) to reflect the likelihood 
of the token being part of the translation. We 
describe how to train a CRF model for identifying 
translations in more details in Section 3. 
At run-time, the system accepts a given phrase 
(e.g., named-entity recognition), and then query a 
search engine for webpages in the target  language 
(e.g., Chinese) using the advance search function. 
Subsequently, we retrieve mixed-code snippets and 
identify the translations of the given term. The 
system can potentially be used to assist translators  
to find the most common translation for a given 
term, or to supplement a bilingual terminology 
bank (e.g., adding multilingual titles to existing 
Wikipedia); alternatively, they can be used as 
additional training data for a machine translation 
system, as described in Lin et al (2008). 
2 Related Work 
Phrase translation and transliteration is important 
for cross-language tasks. For example, Knight and 
Graehl (1998) describe and evaluate a multi-stage 
machine translation method for back transliterating 
English names into Japanese, while Bian and Chen 
(2000) describe cross-language information access 
to multilingual collections on the Internet.  
Recently, researchers have begun to exploit 
mixed code webpages for word and phrase 
translation. Nagata et al (2001) present a system 
for finding English translations for a given 
Japanese technical term using Japanese-English 
snippets returned by a search engine. Kwok et al 
(2005) focus on named entity transliteration and 
implemented a cross-language name finder. Wu et 
al. (2005) proposed a method to learn surface 
patterns to find translations in mixed code snippets.  
Some researchers exploited the hyperlinks in 
Webpage to find translations. Lu, et al (2004) 
propose a method for mining translations of web 
queries from anchor texts. Cheng, et al(2004) 
propose a similar method for translating unknown 
queries with web corpora for cross-language 
information retrieval. Gravano (2006) also propose 
similar methods using anchor texts. 
In a study more closely related to our work, Lin 
et al (2008) proposed a method that performs 
word alignment between translations and phrases 
within parentheses in crawled webpages. They use 
heuristics to align words and translations, while we  
Token TR TL Distance Label
?         0 0 14 O 
62 0 0 13 O 62th ?         0 0 12 O 
?         3 0 11 B 
Emmy ?         3 0 10 I 
Award ?         0 5 9 I 
?         0 0 8 O 
awarding ?         0 0 7 O 
?        0 0 6 O 
ceremony ?         0 0 5 O 
?         0 0 4 O 
(           0 0 3 O the         0 0 2 O 
62th       0 0 1 O 
Emmy 0 0 0 E 
Award 0 0 0 E 
)        0 0 -1 O 
 
Figure 1. Example training data. 
 
use a learning based approach to find translations.  
In contrast to previous work described above, 
we exploit surface patterns differently as a soft 
constraint, while requiring minimal human 
intervention to prepare the training data.   
3 Method 
To find translations for a given term on the Web, a 
promising approach is automatically learning to 
extract phrasal translations or transliterations of 
phrase based on machine learning, or more 
specifically the conditional random fields (CRF) 
model. 
We focus on the issue of finding translations in 
mixed code snippets returned by a search engine. 
The translations are identified, tallied, ranked, and 
returned as the output of the system. 
3.1 Preparing Data for CRF Classifier 
We make use a small set of term and translation 
pairs as seed data to retrieve and annotate mixed-
code snippets from a search engine. Features are 
generated based on other external knowledge 
sources as will be described in Section 3.1.2 and 
3.1.3. An example data generated with given term 
Emmy Award with features and translation/non-
translation labels is shown in Figure 1 using the 
common BIO notation. 
3.1.1 Retrieving and tagging snippets. We use a 
list of randomly selected source and target terms as 
seed data (e.g., Wikipedia English titles and their 
131
  
Chinese counterpart using the language links). We 
use the English terms (e.g., Emmy Awards) to 
query a search engine with the target webpage 
language set to the target language (e.g., Chinese), 
biasing the search engine to return Chinese 
webpages interspersed with some English phrases. 
We then automatically label each Chinese 
character of the returned snippets, with B, I, O 
indicating respectively beginning, inside, and 
outside of translations. In Figure 1, the translation 
??? (ai mei jiang) are labeled as B I I, while all 
other Chinese characters are labeled as O.   An 
additional tag of E is used to indicate the 
occurrences of the given term (e.g., Emmy Awards 
in Figure 1).   
3.1.2 Generating translation feature. We 
generate translation features using external 
bilingual resources. The ?2 score proposed by Gale 
and Church (1991) is used to measure the 
correlations between English and Chinese tokens: 
 where e is an English word and f is a Chinese 
character. The scores are calculated by counting 
co-occurrence of Chinese characters and English 
words in bilingual dictionaries or termbanks, 
where P(e, f) represents the probability of the co-
occurrence of English word e and Chinese 
character f, and P(e, ?f) represents the probability 
the co-occurrence of e and any Chinese characters 
excluding  f.   
We used the publicly available English-Chinese 
Bilingual WordNet and NICT terminology bank to 
generate translation features in our 
implementation. The bilingual WordNet has 
99,642 synset entries, with a total of some 270,000 
translation pairs, mainly common nouns. The 
NICT database has over 1.1 million bilingual terms 
in 72 categories, covering a wide variety of 
different fields. 
3.1.3 Generating transliteration feature. Since 
many terms are transliterated, it is important to 
include transliteration feature. We first use a list of 
name transliterated pairs, then use Expectation-
Maximization (EM) algorithm to align English 
syllables Romanized Chinese characters. Finally, 
we use the alignment information to generate 
transliteration feature for a Chinese token with 
respect to English words in the query. 
We extract person or location entries in 
Wikipedia as name transliterated pairs to generate 
transliteration features in our implementation. This 
can be achieved by examining the Wikipedia 
categories for each entry. A total of some 15,000 
bilingual names of persons and 24,000 bilingual 
place names were obtained and forced aligned to 
obtain transliteration relationships. 
3.1.4 Generating distance feature. In the final 
stage of preparing training data, we add the 
distance, i.e. number of words, between a Chinese 
token feature and the English term in question, 
aimed at exploiting the fact that translations tend to 
occur near the source term, as noted in Nagata et 
al. (2001) and Wu et al (2005).    
Finally, we use the data labeled with translation 
tags and three kinds feature values to train a CRF 
model. 
3.2 Run-Time Translation Extraction 
With the trained CRF model, we then attempt to 
find translations for a given phrase. The system  
begins by submitting the given phrase as query to a 
search engine to retrieve snippets, and generate 
features for each tokens in the same way as done in 
the training phase. We then use the trained model 
to tag the snippets, and extract translation 
candidates by identifying consecutive Chinese 
tokens labeled as B and I. 
Finally, we compute the frequency of all the 
candidates identified in all snippets, and output the 
one with the highest frequency. 
4 Experiments and Evaluation 
We extracted the Wikipedia titles of English and 
Chinese articles connected through language links 
for training and testing. We obtained a total of 
155,310 article pairs, from which we then 
randomly selected 13,150 and 2,181 titles as seeds 
to obtain the training and test data. Since we are 
using Wikipedia bilingual titles as the gold 
standard, we exclude any snippets from the 
wikipedia.org domain, so that we are not using 
Wikipedia article content in both training and 
testing stage. The test set contains 745,734 
snippets or 9,158,141 tokens (Chinese character or 
English word). The reference answer appeared a 
total of 48,938 times or 180,932 tokens (2%), and 
an average of 22.4 redundant answer instances per 
input. 
132
  
System Coverage Exact match Top5 exact match
Full (En-Ch) 80.4% 43.0% 56.4%
-TL 83.9% 27.5% 40.2%
-TR 81.2% 37.4% 50.3%
-TL-TR 83.2% 21.1% 32.8%
LIN En-Ch 59.6% 27.9% not reported
LIN Ch-En 70.8% 36.4% not reported
LCD (En-Ch) 10.8% 4.8% N/A
NICT (En-Ch) 24.2% 32.1% N/A
Table 1. Automatic evaluation results of  8 experiments:  
(1) Full system (2-4)  -TL,  -TR, -TL-TR : Full system 
deprecating TL, TR, and TL+TL features (5,6) LIN En-
Ch and En-Ch : the results in Lin et al (2008) (6) LDC: 
LDC E-C dictionary (7) NICT : NICT term bank. 
 
English Wiki Chinese Wiki Extracted Ev. 
Pope Celestine IV  ??????  ?????? A 
Fujian  ???  ?? A 
Waste  ??  ?? A 
Collateral  ???? ?? B 
Ludwig Erhard  ????????  ??? P 
Osman I  ?????  ??? P 
Bubble sort  ???? ?? P 
The Love Suicides 
at Sonezaki  
?????  ???? E 
Ammonium  ? ???? E 
Table 2. Cases failing the exact match test.  
 
Result Count Percentage 
A+B: correct 53 55.8% 
P: partially corr. 30 31.6% 
E: incorrect 8 8.4% 
N: no results 4 4.2% 
total 95 100% 
Table 3. Manual evaluation of unlink titles. 
 
To compare our method with previous work, we 
used a similar evaluation procedure as described in 
Lin et al (2008). We ran the system and produced 
the translations for these 2,181 test data, and 
automatically evaluate the results using the metrics 
of coverage, i.e. when system was able to produce 
translation candidates, and exact match precision. 
This precision rate is an under-estimations, since 
a term may have many alternative translations that 
does not match exactly with one single reference 
translation. To give a more accurate estimate of 
real precision, we resorted to manual evaluation on 
a small part of the 2,181 English phrases and a 
small set of English Wikipedia titles without a 
Chinese language link.  
4.1 Automatic Evaluation 
In this section, we describe the evaluation based on 
English-Chinese titles extracted from Wikipedia as 
the gold standard. Our system produce the top-1 
translations by ranking candidates by frequency 
and output the most frequent translations. Table 1 
shows the results we have obtained as compared to 
the results of Lin et al (2008).  
Table 1 shows the evaluation results of 8 
experiments. The results indicate that using 
external knowledge to generate feature improves 
system performance significantly. By adding 
translation feature (TL) or transliteration feature 
(TR) to the system with no external knowledge 
features (-TL-TR) improves exact match precision 
by about 6% and 16% respectively. Because many 
Wikipedia titles are named entities, transliteration 
feature is the most important. Overall, the system 
with full features perform the best, finding 
reasonably correct translations for 8 out of 10 
phrases. 
4.2 Manual Evaluation 
Evaluation based on exact match against a single 
reference answer leads to under-estimation, 
because an English phrase is often translated into 
several Chinese counterparts. Therefore, we asked 
a human judge to examine and mark the outputs of 
our full system. The judge was instructed to mark 
each output as A: correct translation alternative, B: 
correct translation but with a difference sense from 
the reference, P: partially correct translation, and 
E: incorrect translation. 
Table 2 shows some translations generated by 
the full system that does not match the single 
reference translation. Half of the translations are 
correct translations (A and B), while a third are 
partially correct translation (P). Notice that it is a 
common practice to translate only the surname of a 
foreign person. Therefore, some partial translations 
may still be considered as correct (B). 
To Evaluate titles without a language link, we 
sampled a list of 95 terms from the unlinked 
portion of Wikipedia using the criteria: (1) with a 
frequency count of over 2,000 in Google Web 1T. 
(2) containing at least three English words. (3) not 
a proper name. Table 3 shows the evaluation 
133
  
results. Interestingly, our system provides correct 
translations for over 50% of the cases, and at least 
partially correct almost 90% of the cases. 
5 Conclusion and Future work 
We have presented a new method for finding 
translations on the Web for a given term. In our 
approach, we use a small set of terms and 
translations as seeds to obtain and to tag mixed-
code snippets returned by a search engine, in order 
to train a CRF model for sequence labels. This 
CRF model is then used to tag the returned 
snippets for a given query term to extraction 
translation candidates, which are then ranked and 
returned as output. Preliminary experiments and 
evaluations show our learning-based method 
cleanly combining various features, producing 
quality translations and transliterations.  
Many avenues exist for future research and 
improvement. For example, existing query 
expansion methods could be implemented to 
retrieve more webpages containing translations. 
Additionally, an interesting direction to explore is 
to identify phrase types and train type-specific 
CRF model. In addition, natural language 
processing techniques such as word stemming and 
word lemmatization could be attempted. 
References  
G. W. Bian, H. H. Chen. Cross-language information 
access to multilingual collections on the internet. 
2000. Journal of American Society for Information 
Science  & Technology (JASIST), Special Issue on 
Digital Libraries, 51(3), pp.281-296, 2000. 
Y. Cao and H. Li. Base Noun Phrase Translation Using 
Web Data and the EM Algorithm. 2002. In 
Proceedings of the 19th International Conference on 
Computational Linguistics (COLING?02), pp.127-
133, 2002. 
P. J. Cheng, J. W. Teng, R. C. Chen, J. H. Wang, W. H. 
Lu, and L. F. Chien. Translating unknown queries 
with web corpora for cross-language information 
retrieval. In Proceedings of the 27th ACM 
International Conference on Research and 
Development in Information Retrieval, pp.146-153, 
2004. 
F. Huang, S. Vogel, and A. Waibel. Automatic 
extraction of named entity translingual equivalence 
based on multi-feature cost minimization. In 
Proceeding of the 41st ACL, Workshop on 
Multilingual and Mixed-Language Named Entity 
Recognition, Sapporo, 2003. 
K. Knight, J. Graehl. Machine Transliteration. 1998. 
Computational Linguistics 24(4), pp.599-612, 1998. 
P. Koehn, K. Knight. 2003. Feature-Rich Statistical 
Translation of Noun Phrases. In Proceedings of the 
41st Annual Meeting on Association for 
Computational Linguistics, pp. 311-318, 2003. 
J. Kupiec. 1993. An Algorithm for Finding Noun Phrase 
Correspondences in Bilingual Corpora. In 
Proceedings of the 31st Annual Meeting of the 
Association for Computational Linguistics, pp. 17-
22, 1993.  
KL Kwok, P Deng, N Dinstl, HL Sun, W Xu, P Peng, 
and Doyon, J. 2005. CHINET: a Chinese name finder 
system for document triage. In Proceedings of 2005  
D. Lin, S. Zhao, B.V. Durme, and M. Pa?ca. 2008. 
Mining Parenthetical Translation from the Web by 
Word Alignment, In Proceedings of ACL 2008, pp. 
994-1002, 2008. 
Y. Li, G. Grefenstette. 2005. Translating Chinese 
Romanized name into Chinese idiographic characters 
via corpus and web validation. In Proceedings of 
CORIA 2005, pp. 323-338, 2005. 
M. Nagata, T. Saito, and K. Suzuki. Using the Web as a 
bilingual dictionary. 2001. In Proceedings of 39th. 
ACL Workshop on Data-Driven Methods in Machine 
Translation, pp. 95-102, 2001. 
Y. Qu, and G. Grefenstette. 2004. Finding Ideographic 
Representations of Japanese Names Written in Latin 
Script via Language Identification and Corpus 
Validation. In Proceedings of the 42nd Annual 
Meeting of the Association for Computational 
Linguistics, pp.183-190, 2004. 
CK Quah. 2006. Translation and Technology, Palgrave 
Textbooks in Translation and Interpretation, Palgrave 
MacMillan. 
R Sproat and C Shih. Statistical Method for Finding 
Word Boundaries in Chinese Text, Computer 
Processing of Chinese and Oriental languages. 1990. 
J. C. Wu, T. Lin and J. S. Chang. Learning Source-
Target Surface Patterns for Web-based Terminology 
Translation. In Proceeding of the ACL 2005 on 
Interactive poster and demonstration sessions 
(ACLdemo '05). 2005. 
Y Zhang, F Huang, S Vogel. 2005. Mining translations 
of OOV terms from the web through cross-lingual 
query expansion. In Proceedings of the 28th Annual 
International ACM SIGIR, pp.669-670, 2005.  
134
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 55?60,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
DOMCAT: A Bilingual Concordancer for Domain-Specific Computer 
Assisted Translation 
Ming-Hong Bai1,2 Yu-Ming Hsieh1,2 Keh-Jiann Chen1 Jason S. Chang2 
1 Institute of Information Science, Academia Sinica, Taiwan 
2 Department of Computer Science, National Tsing-Hua University, Taiwan 
mhbai@sinica.edu.tw, morris@iis.sinica.edu.tw, 
kchen@iis.sinica.edu.tw, jason.jschang@gmail.com 
 
Abstract 
In this paper, we propose a web-based 
bilingual concordancer, DOMCAT 1 , for 
domain-specific computer assisted 
translation. Given a multi-word expression 
as a query, the system involves retrieving 
sentence pairs from a bilingual corpus, 
identifying translation equivalents of the 
query in the sentence pairs (translation 
spotting) and ranking the retrieved sentence 
pairs according to the relevance between 
the query and the translation equivalents. 
To provide high-precision translation 
spotting for domain-specific translation 
tasks, we exploited a normalized 
correlation method to spot the translation 
equivalents. To ranking the retrieved 
sentence pairs, we propose a correlation 
function modified from the Dice coefficient 
for assessing the correlation between the 
query and the translation equivalents. The 
performances of the translation spotting 
module and the ranking module are 
evaluated in terms of precision-recall 
measures and coverage rate respectively. 
1 Introduction 
A bilingual concordancer is a tool that can retrieve 
aligned sentence pairs in a parallel corpus whose 
source sentences contain the query and the 
translation equivalents of the query are identified 
in the target sentences. It helps not only on finding 
translation equivalents of the query but also 
presenting various contexts of occurrence. As a 
result, it is extremely useful for bilingual 
                                                          
1 http://ckip.iis.sinica.edu.tw/DOMCAT/ 
lexicographers, human translators and second 
language learners (Bowker and Barlow 2004; 
Bourdaillet et al, 2010; Gao 2011).  
Identifying the translation equivalents, 
translation spotting, is the most challenging part of 
a bilingual concordancer. Recently, most of the 
existing bilingual concordancers spot translation 
equivalents in terms of word alignment-based 
method. (Jian et al, 2004; Callison-Burch et al, 
2005; Bourdaillet et al, 2010). However, word 
alignment-based translation spotting has some 
drawbacks. First, aligning a rare (low frequency) 
term may encounter the garbage collection effect 
(Moore, 2004; Liang et al, 2006) that cause the 
term to align to many unrelated words. Second, the 
statistical word alignment model is not good at 
many-to-many alignment due to the fact that 
translation equivalents are not always correlated in 
lexical level. Unfortunately, the above effects will 
be intensified in a domain-specific concordancer 
because the queries are usually domain-specific 
terms, which are mostly multi-word low-frequency 
terms and semantically non-compositional terms. 
Wu et al (2003) employed a statistical 
association criterion to spot translation equivalents 
in their bilingual concordancer. The association-
based criterion can avoid the above mentioned 
effects. However, it has other drawbacks in 
translation spotting task. First, it will encounter the 
contextual effect that causes the system incorrectly 
spot the translations of the strongly collocated 
context. Second, the association-based translation 
spotting tends to spot the common subsequence of 
a set of similar translations instead of the full 
translations. Figure 1 illustrates an example of 
contextual effect, in which ?Fan K'uan? is 
incorrectly spotted as part of the translation of the 
query term ?????? ? (Travelers Among 
Mountains and Streams), which is the name of the 
55
painting painted by ?Fan K'uan/?? ? since the 
painter?s name is strongly collocated with the 
name of the painting. 
 
Sung , Travelers Among Mountains and Streams , Fan 
K'uan 
???????? 
Figure 1. ?Fan K'uan? may be incorrectly spotted as 
part of the translation of ???????, if pure 
association method is applied. 
 
Figure 2 illustrates an example of common 
subsequence effect, in which ??????? (the 
River During the Qingming Festival/ Up the River 
During Qingming) has two similar translations as 
quoted, but the Dice coefficient tends to spot the 
common subsequences of the translations. 
(Function words are ignored in our translation 
spotting.) 
 
Expo 2010 Shanghai-Treasures of Chinese Art Along 
the River During the Qingming Festival 
2010?????????????????? 
Oversized Hanging Scrolls and Handscrolls Up the 
River During Qingming 
???????????? 
Figure 2. The Dice coefficient tends to spot the common 
subsequences ?River During Qingming?. 
Bai et al (2009) proposed a normalized 
frequency criterion to extract translation 
equivalents form sentence aligned parallel corpus. 
This criterion takes lexical-level contexture effect 
into account, so it can effectively resolve the above 
mentioned effect. But the goal of their method is to 
find most common translations instead of spotting 
translations, so the normalized frequency criterion 
tends to ignore rare translations. 
In this paper, we propose a bilingual 
concordancer, DOMCAT, for computer assisted 
domain-specific term translation. To remedy the 
above mentioned effects, we extended the 
normalized frequency of Bai et al (2009) to a 
normalized correlation criterion to spot translation 
equivalents. The normalized correlation inherits 
the characteristics of normalized frequency and is 
adjusted for spotting rare translations. These 
characteristics are especially important for a 
domain-specific bilingual concordancer to spot 
translation pairs of low-frequency and semantically 
non-compositional terms.  
The remainder of this paper is organized as 
follows.  Section 2 describes the DOMCAT system. 
In Section 3, we describe the evaluation of the 
DOMCAT system. Section 4 contains some 
concluding remarks. 
2 The DOMCAT System 
Given a query, the DOMCAT bilingual 
concordancer retrieves sentence pairs and spots 
translation equivalents by the following steps: 
 
1. Retrieve the sentence pairs whose source 
sentences contain the query term. 
2. Extract translation candidate words from the 
retrieved sentence pairs by the normalized 
correlation criterion. 
3. Spot the candidate words for each target 
sentence and rank the sentences by 
normalized the Dice coefficient criterion. 
 
In step 1, the query term can be a single word, a 
phrase, a gapped sequence and even a regular 
expression. The parallel corpus is indexed by the 
suffix array to efficiently retrieve the sentences.  
The step 2 and step 3 are more complicated and 
will be described from Section 2.1 to Section 2.3. 
2.1 Extract Translation Candidate Words 
After the queried sentence pairs retrieved from the 
parallel corpus, we can extract translation 
candidate words from the sentence pairs. We 
compute the local normalized correlation with 
respect to the query term for each word e in each 
target sentence. The local normalized correlation 
is defined as follows: 
 
?
?
??
??
??
???
f
q
f
qfeq
j
i
f j
f i
fep
fepelnc ||)|(
||)|(),,;(      (1) 
 
where q denotes the query term, f denotes the 
source sentence and e denotes the target sentence,  
? is a small smoothing factor. The probability p(e|f) 
is the word translation probability derived from the 
entire parallel corpus by IBM Model 1 (Brown et 
al., 1993). The sense of local normalized 
correlation of e can be interpreted as the 
probability of word e being part of translation of 
the query term q under the condition of sentence 
pair (e, f). 
56
Once the local normalized correlation is 
computed for each word in retrieved sentences, we 
compute the normalized correlation on the 
retrieved sentences. The normalized correlation is 
the average of all lnc values and defined as follows:  
 
?
?
?
n
i
iielncnenc 1
)()( ),,;(1);( feqq            (2) 
 
where n is the number of retrieved sentence pairs.  
After the nc values for the words of the retrieved 
target sentences are computed, we can obtain a 
translation candidate list by filtering out the words 
with lower nc values. 
To compare with the association-based method, 
we also sorted the word list by the Dice coefficient 
defined as follows: 
 
)()(
),(2),( q
qq freqefreq
efreqedice ??           (3) 
 
where freq is frequency function which  computes 
frequencies from the parallel corpus. 
 
Candidate words NC 
mountain 0.676 
stream 0.442 
traveler 0.374 
among 0.363 
sung 0.095 
k'uan 0.090 
Figure 3(a). Candidate words sorted by nc values. 
 
Candidate words Dice 
traveler 0.385 
reduced 0.176 
stream 0.128 
k'uan 0.121 
fan 0.082 
among 0.049 
mountain 0.035 
Figure 3(b). Candidate words sorted by Dice coefficient 
values. 
 
Figure 3(a) and (b) illustrate examples of 
translation candidate words of the query term ??
???? ? (Travelers Among Mountains and 
Streams) sorted by the nc values, NC, and the Dice 
coefficients respectively. The result shows that the 
normalized correlation separated the related words 
from unrelated words much better than the Dice 
coefficient. 
The rationale behind the normalized correlation 
is that the nc value is the strength of word e 
generated by the query compared to that of 
generated by the whole sentence. As a result, the 
normalized correlation can easily separate the 
words generated by the query term from the words 
generated by the context. On the contrary, the Dice 
coefficient counts the frequency of a co-occurred 
word without considering the fact that it could be 
generated by the strongly collocated context.  
 
2.2 Translation Spotting 
Once we have a translation candidate list and 
respective nc values, we can spot the translation 
equivalents by the following spotting algorithm. 
For each target sentence, first, spot the word with 
highest nc value. Then extend the spotted sequence 
to the neighbors of the word by checking their nc 
values of neighbor words but skipping function 
words. If the nc value is greater than a threshold ?, 
add the word into spotted sequence. Repeat the 
extending process until no word can be added to 
the spotted sequence. 
The following is the pseudo-code for the 
algorithm: 
 
S is the target sentence 
H is the spotted word sequence 
?is the threshold of translation candidate words 
 
Initialize: 
H? ?
emax?S[0] Foreach ei in S: If nc(ei) > nc(emax):  
emax ??ei 
If nc(emax )??:?
add?emax?to?H 
Repeat until no word add to H 
ej?left?neighbor?of?H?
If?nc(ej?)??:?
? ???add?ej?to?H?
ek?right?neighbor?of?H?
If nc(?ek?)???:?
? ???add?ek?to?H?
Figure 4: Pseudo-code of translation spotting process. 
 
57
2.3 Ranking 
The ranking mechanism of a bilingual 
concordancer is used to provide the most related 
translation of the query on the top of the outputs 
for the user. So, an association metric is needed to 
evaluate the relations between the query and the 
spotted translations. The Dice coefficient is a 
widely used measure for assessing the association 
strength between a multi-word expression and its 
translation candidates. (Kupiec, 1993; Smadja et 
al., 1996; Kitamura and Matsumoto, 1996; 
Yamamoto and Matsumoto, 2000; Melamed, 2001)  
The following is the definition of the Dice 
coefficient: 
 
)()(
),(2),( qt
qtqt freqfreq
freqdice ??            (4) 
 
where q denotes a multi-word expression to be 
translated, t denotes a translation candidate of q. 
However, the Dice coefficient has the common 
subsequence effect (as mentioned in Section 1) due 
to the fact that the co-occurrence frequency of the 
common subsequence is usually larger than that of 
the full translation; hence, the Dice coefficient 
tends to choose the common subsequence. 
To remedy the common subsequence effect, we 
introduce a normalized frequency for a spotted 
sequence defined as follows: 
 
?
?
?
n
i
iilnfnf
1
)()( ),,;(),( feqtqt            (5) 
 
where lnf is a function which compute normalized 
frequency locally in each sentence. The following 
is the definition of lnf: 
 
?
???
??
tH
feqfeqt
e
elnclnf )),,;(1(),,;(      (6) 
 
where H is the spotted sequence of the sentence 
pair (e,f), H-t are the words in H but not in t. The 
rationale behind lnf function is that: when counting 
the local frequency of t in a sentence pair, if t is a 
subsequence of H, then the count of t should be 
reasonably reduced by considering the strength of 
the correlation between the words in H-t and the 
query. 
Then, we modify the Dice coefficient by 
replacing the co-occurrence frequency with 
normalized frequency as follows: 
 
)()(
),(2),( qt
qtqt freqfreq
nfnf_dice ??        (7) 
 
The new scoring function, nf_dice(t,q), is 
exploited as our criterion for assessing the 
association strength between the query and the 
spotted sequences. 
3 Experimental Results 
3.1 Experimental Setting 
We use the Chinese/English web pages of the 
National Palace Museum 2  as our underlying 
parallel corpus. It contains about 30,000 sentences 
in each language. We exploited the Champollion 
Toolkit (Ma et al, 2006) to align the sentence pairs. 
The English sentences are tokenized and 
lemmatized by using the NLTK (Bird and Loper, 
2004) and the Chinese sentences are segmented by 
the CKIP Chinese segmenter (Ma and Chen, 2003). 
To evaluate the performance of the translation 
spotting, we selected 12 domain-specific terms to 
query the concordancer. Then, the returned spotted 
translation equivalents are evaluated against a 
manually annotated gold standard in terms of recall 
and precision metrics. We also build two different 
translation spotting modules by using the GIZA++ 
toolkit (Och and Ney, 2000) with the 
intersection/union of the bidirectional word 
alignment as baseline systems. 
To evaluate the performance of the ranking 
criterion, we compiled a reference translation set 
for each query by collecting the manually 
annotated translation spotting set and selecting 1 to 
3 frequently used translations. Then, the outputs of 
each query are ranked by the nf_dice function and 
evaluated against the reference translation set. We 
also compared the ranking performance with the 
Dice coefficient. 
3.2 Evaluation of Translation Spotting 
We evaluate the translation spotting in terms of the 
Recall and Precision metrics defined as follows: 
 
                                                          
2 http://www.npm.gov.tw 
58
||
||
1
)(
1
)()(
?
?
?
? ?? n
i
i
g
n
i
ii
g
H
HHRecall                     (8) 
||
||
1
)(
1
)()(
?
?
?
? ?? n
i
i
n
i
ii
g
H
HHPrecision                     (9) 
 
where i denotes the index of the retrieved 
sentence, )(iH  is the spotted sequences of the ith 
sentence returned by the concordancer,  and )(igH is 
the gold standard spotted sequences of the ith 
sentence. Table 1 shows the evaluation of 
translation spotting for normalized correlation, NC, 
compared with the intersection and union of 
GIZA++ word alignment. The F-score of the 
normalized correlation is much higher than that of 
the word alignment methods. It is noteworthy that 
the normalized correlation increased the recall rate 
without losing the precision rate. This may indicate 
that the normalized correlation can effectively 
conquer the drawbacks of the word alignment-
based translation spotting and the association-
based translation spotting mentioned in Section 1. 
 
 Recall Precision F-score 
Intersection 0.4026 0.9498 0.5656 
Union 0.7061 0.9217 0.7996 
NC 0.8579 0.9318 0.8933 
Table 1. Evaluation of the translation spotting 
queried by 12 domain-specific terms. 
 
We also evaluate the queried results of each 
term individually (as shown in Table 2). As it 
shows, the normalized correlation is quite stable 
for translation spotting. 
 
Query terms GIZA Intersection GIZA Union NC R P F R P F R P F 
??? (Maogong cauldron) 0.27 0.86 0.41 0.87 0.74 0.80  0.92 0.97 0.94 
????(Jadeite cabbage) 0.48 1.00 0.65 1.00 0.88 0.94  0.98 0.98 0.98 
?????(Travelers Among Mountains and Streams) 0.28 0.75 0.41 1.00 0.68 0.81 0.94 0.91 0.92
?????(Up the River During Qingming) 0.22 0.93 0.35 0.97 0.83 0.89  0.99 0.91 0.95
???(Ching-te-chen) 0.50 0.87 0.63 0.73 0.31 0.44 1.00 0.69 0.82
??(porcelain) 0.53 0.99 0.69 0.93 0.64 0.76 0.78 0.96 0.86
??(cobalt blue glaze) 0.12 1.00 0.21 0.85 0.58 0.69 0.94 0.86 0.90
??(inscription) 0.20 0.89 0.32 0.71 0.34 0.46  0.88 0.95 0.91
????(Three Friends and a Hundred Birds) 0.58 0.99 0.73 1.00 0.97 0.99 1.00 0.72 0.84
??(wild cursive script) 0.42 1.00 0.59 0.63 0.80 0.71 0.84 1.00 0.91
???(Preface to the Orchid Pavilion Gathering) 0.33 0.75 0.46 0.56 0.50 0.53 0.78 1.00 0.88
????(Latter Odes to the Red Cliff) 0.19 0.50 0.27 0.75 0.46 0.57 0.94 0.88 0.91
Table 2. Evaluation of the translation spotting for each term
3.3 Evaluation of Ranking 
To evaluate the performance of a ranking function, 
we ranked the retrieved sentences of the queries by 
the function. Then, the top-n sentences of the 
output are evaluated in terms of the coverage rate 
defined as follows: 
?coverage  
queries of #
top-nin on  translatia findcan  queries of #   (10) 
 
The meaning of the coverage rate can be 
interpreted as: how many percent of the query can 
find an acceptable translation in the top-n results.  
We use the reference translations, as described in 
Section 3.1, as acceptable translation set for each 
query of our experiment. Table 3 shows the 
coverage rate of the nf_dice function compared 
with the Dice coefficient. As it shows, in the 
outputs ranked by the Dice coefficient, uses 
usually have to look up more than 3 sentences to 
find an acceptable translation; while in the outputs 
ranked by the nf_dice function, users can find an 
acceptable translation in top-2 sentences. 
 
59
 
 dice nf_dice 
top-1 0.42  0.92 
top-2 0.75  1.00 
top-3 0.92  1.00 
Table 3. Evaluation of the ranking criteria. 
4 Conclusion and Future Works 
In this paper, we proposed a bilingual 
concordancer, DOMCAT, designed as a domain-
specific computer assisted translation tool. We 
exploited a normalized correlation which 
incorporate lexical level information into 
association-based method that effectively avoid the 
drawbacks of the word alignment-based translation 
spotting as well as the association-based translation 
spotting. 
In the future, it would be interesting to extend 
the parallel corpus to the internet to retrieve more 
rich data for the computer assisted translation. 
References  
Bai, Ming-Hong, Jia-Ming You, Keh-Jiann Chen, Jason 
S. Chang. 2009. Acquiring Translation Equivalences 
of Multiword Expressions by Normalized Correlation 
Frequencies. In Proceedings of EMNLP, pages 478-
486. 
Bird, Steven and Edward Loper. 2004. NLTK: The 
Natural Language Toolkit. In Proceedings of ACL, 
pages 214-217. 
Bourdaillet, Julien, St?phane Huet, Philippe Langlais 
and Guy Lapalme. 2010. TRANSSEARCH: from a 
bilingual concordancer to a translation finder. 
Machine Translation, 24(3-4): 241?271. 
Bowker, Lynne, Michael Barlow. 2004. Bilingual 
concordancers and translation memories: A 
comparative evaluation. In Proceedings of the 
Second International Workshop on Language 
Resources for Translation Work, Research and 
Training , pages. 52-61. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311. 
Callison-Burch, Chris, Colin Bannard and Josh 
Schroeder. 2005. A Compact Data Structure for 
Searchable Translation Memories. In Proceedings of 
EAMT. 
Gao, Zhao-Ming. 2011. Exploring the effects and use of 
a Chinese?English parallel concordancer. Computer-
Assisted Language Learning 24.3 (July 2011): 255-
275. 
Jian, Jia-Yan, Yu-Chia Chang and Jason S. Chang. 2004. 
TANGO: Bilingual Collocational Concordancer. In 
Proceedings of ACL, pages 166-169. 
Kitamura, Mihoko and Yuji Matsumoto. 1996. 
Automatic Extraction of Word Sequence 
Correspondences in Parallel Corpora. In Proceedings 
of WVLC-4 pages 79-87. 
Kupiec, Julian. 1993. An Algorithm for Finding Noun 
Phrase Correspondences in Bilingual Corpora. In 
Proceedings of ACL, pages 17-22. 
Liang, Percy, Ben Taskar, Dan Klein. 2006. Alignment 
by Agreement. In Proceedings of HLT-NAACL 2006, 
pages 104-111, New York, USA. 
Ma, Wei-Yun and Keh-Jiann Chen. 2003. Introduction 
to CKIP Chinese word segmentation system for the 
first international Chinese word segmentation 
bakeoff. In Proceedings of the second SIGHAN 
workshop on Chinese language processing, pages 
168-171. 
Ma, Xiaoyi. 2006. Champollion: A Robust Parallel Text 
Sentence Aligner. In Proceedings of the Fifth 
International Conference on Language Resources 
and Evaluation. 
Melamed, Ilya Dan. 2001. Empirical Methods for 
Exploiting parallel Texts. MIT press. 
Moore, Robert C. 2004. Improving IBM Word-
Alignment Model 1. In Proceedings of ACL, pages 
519-526, Barcelona, Spain. 
Och, Franz J., Hermann Ney., 2000, Improved 
Statistical Alignment Models, In Proceedings of ACL, 
pages 440-447. Hong Kong. 
Smadja, Frank, Kathleen R. McKeown, and Vasileios 
Hatzivassiloglou. 1996. Translating Collocations for 
Bilingual Lexicons: A Statistical Approach. 
Computational Linguistics, 22(1):1-38. 
Wu, Jian-Cheng, Kevin C. Yeh, Thomas C. Chuang, 
Wen-Chi Shei, Jason S. Chang.  2003. TotalRecall: A 
Bilingual Concordance for Computer Assisted 
Translation and Language Learning. In Proceedings 
of ACL, pages 201-204. 
Yamamoto, Kaoru, Yuji Matsumoto. 2000. Acquisition 
of Phrase-level Bilingual Correspondence using 
Dependency Structure. In Proceedings of COLING, 
pages 933-939. 
60
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 157?162,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
FLOW: A First-Language-Oriented Writing Assistant System 
 
 
Mei-Hua Chen*, Shih-Ting Huang+, Hung-Ting Hsieh*, Ting-Hui Kao+, Jason S. Chang+ 
  * Institute of Information Systems and Applications 
+ Department of Computer Science 
National Tsing Hua University  
HsinChu, Taiwan, R.O.C. 30013 
{chen.meihua,koromiko1104,vincent732,maxis1718,jason.jschang}@gmail.com 
 
 
Abstract 
Writing in English might be one of the most 
difficult tasks for EFL (English as a Foreign 
Language) learners. This paper presents 
FLOW, a writing assistance system. It is built 
based on first-language-oriented input function 
and context sensitive approach, aiming at 
providing immediate and appropriate 
suggestions including translations, paraphrases, 
and n-grams during composing and revising 
processes. FLOW is expected to help EFL 
writers achieve their writing flow without being 
interrupted by their insufficient lexical 
knowledge.  
 
1. Introduction 
Writing in a second language (L2) is a challenging 
and complex process for foreign language learners. 
Insufficient lexical knowledge and limited 
exposure to English might interrupt their writing 
flow (Silva, 1993). Numerous writing instructions 
have been proposed (Kroll, 1990) as well as 
writing handbooks have been available for 
learners. Studies have revealed that during the 
writing process, EFL learners show the inclination 
to rely on their native languages (Wolfersberger, 
2003) to prevent a breakdown in the writing 
process (Arndt, 1987; Cumming, 1989). However, 
existing writing courses and instruction materials, 
almost second-language-oriented, seem unable to 
directly assist EFL writers while writing. 
This paper presents FLOW1 (Figure 1), an 
interactive system for assisting EFL writers in 
                                                          
1 FLOW: http:// flowacldemo.appspot.com 
composing and revising writing. Different from 
existing tools, its context-sensitive and first-
language-oriented features enable EFL writers to 
concentrate on their ideas and thoughts without 
being hampered by the limited lexical resources. 
Based on the studies that first language use can 
positively affect second language composing, 
FLOW attempts to meet such needs. Given any L1 
input, FLOW displays appropriate suggestions 
including translation, paraphrases, and n-grams 
during composing and revising processes. We use 
the following example sentences to illustrate these 
two functionalities.  
Consider the sentence ?We propose a method 
to?. During the composing stage, suppose a writer 
is unsure of the phrase ?solve the problem?, he 
could write ??????, a corresponding word in 
his native language, like ?We propose a method to 
?????. The writer?s input in the writing area 
of FLOW actively triggers a set of translation 
suggestions such as ?solve the problem? and 
?tackle the problem? for him/her to complete the 
sentence.  
In the revising stage, the writer intends to 
improve or correct the content. He/She is likely to 
change the sentence illustrated above into ?We try 
all means to solve the problem.? He would select 
the phrase ?propose a method? in the original 
sentence and input a L1 phrase ????, which 
specifies the meaning he prefers. The L1 input 
triggers a set of context-aware suggestions 
corresponding to the translations such as ?try our 
best? and ?do our best? rather than ?try your best? 
and ?do your best?. The system is able to do that 
mainly by taking a context-sensitive approach. 
FLOW then inserts the phrase the writer selects 
into the sentence. 
157
   
Figure 1. Screenshot of FLOW 
 
In this paper, we propose a context-sensitive 
disambiguation model which aims to automatically 
choose the appropriate phrases in different contexts 
when performing n-gram prediction, paraphrase 
suggestion and translation tasks. As described in 
(Carpuat and Wu, 2007), the disambiguation model 
plays an important role in the machine translation 
task. Similar to their work, we further integrate the 
multi-word phrasal lexical disambiguation model 
to the n-gram prediction model, paraphrase model 
and translation model of our system. With the 
phrasal disambiguation model, the output of the 
system is sensitive to the context the writer is 
working on. The context-sensitive feature helps 
writers find the appropriate phrase while 
composing and revising. 
This paper is organized as follows. We review 
the related work in the next section. In Section 3, 
we brief our system and method. Section 4 reports 
the evaluation results. We conclude this paper and 
point out future directions to research in Section 5. 
 
2. Related Work 
2.1 Sub-sentential paraphrases  
A variety of data-driven paraphrase extraction 
techniques have been proposed in the literature.  
One of the most popular methods leveraging 
bilingual parallel corpora is proposed by Bannard 
and Callison-Burch (2005). They identify 
paraphrases using a phrase in another language as a 
pivot. Using bilingual parallel corpora for 
paraphrasing demonstrates the strength of semantic 
equivalence. Another line of research further 
considers context information to improve the 
performance. Instead of addressing the issue of 
local paraphrase acquisition, Max (2009) utilizes 
the source and target contexts to extract sub-
sentential paraphrases by using pivot SMT 
systems. 
 
2.2 N-gram suggestions  
After a survey of several existing writing tools, we 
focus on reviewing two systems closely related to 
our study.  
PENS (Liu et al 2000), a machine-aided English 
writing system, provides translations of the 
corresponding English words or phrases for 
writers? reference. Different from PENS, FLOW 
further suggests paraphrases to help writers revise 
their writing tasks. While revising, writers would 
alter the use of language to express their thoughts. 
The suggestions of paraphrases could meet their 
need, and they can reproduce their thoughts more 
fluently.  
Another tool, TransType (Foster, 2002), a text 
editor, provides translators with appropriate 
translation suggestions utilizing trigram language 
model. The differences between our system and 
TransType lie in the purpose and the input. FLOW 
aims to assist EFL writers whereas TransType is a 
tool for skilled translators. On the other hand, in 
TransType, the human translator types translation 
of a given source text, whereas in FLOW the input, 
158
either a word or a phrase, could be source or target 
languages.  
 
2.3 Multi-word phrasal lexical disambiguation 
In the study more closely related to our work, 
Carpuat and Wu (2007) propose a novel method to 
train a phrasal lexical disambiguation model to 
benefit translation candidates selection in machine 
translation. They find a way to integrate the state-
of-the-art Word Sense Disambiguation (WSD) 
model into phrase-based statistical machine 
translation. Instead of using predefined senses 
drawn from manually constructed sense 
inventories, their model directly disambiguates 
between all phrasal translation candidates seen 
during SMT training. In this paper, we also use the 
phrasal lexical disambiguation model; however, 
apart from using disambiguation model to help 
machine translation, we extend the disambiguation 
model. With the help of the phrasal lexical 
disambiguation model, we build three models: a 
context-sensitive n-gram prediction model, a 
paraphrase suggestion model, and a translation 
model which are introduced in the following 
sections. 
 
3. Overview of FLOW  
The FLOW system helps language learners in two 
ways: predicting n-grams in the composing stage 
and suggesting paraphrases in the revising stage 
(Figure 2). 
3.1  System architecture 
Composing Stage 
During the composing process, a user inputs S.  
FLOW first determines if the last few words of S is 
a L1 input. If not, FLOW takes the last k words to 
predict the best matching following n-grams. 
Otherwise, the system uses the last k words as the 
query to predict the corresponding n-gram 
translation. With a set of prediction (either 
translations or n-grams), the user could choose an 
appropriate suggestion to complete the sentence in 
the writing area. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
NO  
Writing process  
Input K 
K consists of  first language 
First-Language-Oriented N-gram 
Prediction 
User interface 
Context-Sensitive N-gram Prediction 
YES  
Revising process 
Get word sequence L and R 
surrounding user selected text K 
Foreign Language F 
is input 
Ontext-Sensitive Paraphrase 
Suggestion  
First-Language-Oriented Paraphrase 
Suggestion 
User interface 
Input S 
NO  YES  
159
Figure 2. Overall Architecture of FLOW in writing and 
revising processes 
 
 
Revising Stage 
In the revising stage, given an input I and the user 
selected words K, FLOW obtains the word 
sequences L and R surrounding K as reference for 
prediction. Next, the system suggests sub-
sentential paraphrases for K based on the 
information of L and R. The system then searches 
and ranks the translations. 
 
3.2  N-gram prediction 
In the n-gram prediction task, our model takes the 
last k words with m 2 English words and n foreign 
language words, {e1, e2, ?em, f1, f2 ?fn}, of the 
source sentences S as the input. The output would 
be a set of n-gram predictions. These n-grams can 
be concatenated to the end of the user-composed 
sentence fluently. 
 
Context-Sensitive N-gram Prediction (CS-NP) 
The CS-NP model is triggered to predict a 
following n-gram when a user composes sentences 
consisted of only English words with no foreign 
language words, namely, n is equal to 0.  The goal 
of the CS-NP model is to find the English phrase e 
that maximizes the language model probability of 
the word sequence, {e1, e2, ?em, e}: 
 
? ? argmax
?,????
???|??, ??, ? ??? 
???|??, ??, ? ??? ? ?
???, ??, ? ??, ??
????, ??, ? ???   
 
Translation-based N-gram Prediction (TB-NP) 
When a user types a set of L1 expression f = { f1, f2 
?fn }, following the English sentences S, the 
FLOW system will predict the possible translations 
of f. A simple way to predict the translations is to 
find the bilingual phrase alignments T(f) using the 
method proposed by (Och and Ney, 2003). 
However, the T(f) is ambiguous in different 
contexts. Thus, we use the context {e1, e2, ?em} 
proceeding f to fix the prediction of the translation. 
Predicting the translation e can be treated as a sub-
sentential translation task: 
                                                          
2 In this paper, m = 5. 
 
? ? argmax?????? ???|???, ??, ? ???,  
 
where we use the user-composed context {e1, e2, 
?em} to disambiguate the translation of f. 
Although there exist more sophisticated models 
which could make a better prediction, a simple 
na?ve-Bayes model is shown to be accurate and 
efficient in the lexical disambiguation task 
according to (Yarowsky and Florian, 2002).  
Therefore, in this paper, a na?ve-Bayes model is 
used to disambiguate the translation of f. In 
addition to the context-word feature, we also use 
the context-syntax feature, namely surrounding 
POS tag Pos, to constrain the syntactic structure of 
the prediction. The TB-NP model could be 
represented in the following equation: 
 
?? ? argmax
?
?????1, ?2,? ??, 1?, ???,2??,? 
 
??? ? ???, ??, ? ???  
According to the Bayes theorem, 
 
?????1, ?2, ???, 1?, ???,2??
??????|?? ? ?????????
????????
 
The probabilities can be estimated using a parallel 
corpus, which is also used to obtain bilingual 
phrase alignment. 
 
3.3  Paraphrase Suggestion 
Unlike the N-gram prediction, in the paraphrase 
suggestion task, the user selects k words, {e1, e2, 
?ek}, which he/she wants to paraphrase. The 
model takes the m words {r1, r2, ?rm} and n words 
{l1, l2, ?ln} in the right and left side of the user- 
selected k words respectively. The system also 
accepts an additional foreign language input, {f1,f2, 
?fl}, which helps limit the meaning of suggested 
paraphrases to what the user really wants. The 
output would be a set of paraphrase suggestions 
that the user-selected phrases can be replaced by 
those paraphrases precisely. 
 
Context-Sensitive Paraphrase Suggestion (CS-
PS) 
The CS-PS model first finds a set of local 
paraphrases P of the input phrase K using the 
160
pivot-based method proposed by Bannard and 
Callison-Burch (2005). Although the pivot-based 
method has been proved efficient and effective in 
finding local paraphrases, the local paraphrase 
suggestions may not fit different contexts. Similar 
to the previous n-gram prediction task, we use the 
na?ve-Bayes approach to disambiguate these local 
paraphrases. The task is to find the best e such that 
e with the highest probability for the given context 
R and L. We further require paraphrases to have 
similar syntactic structures to the user-selected 
phrase in terms of POS tags, Pos. 
 
?? ? argmax
???
???|?1, ?2,? ??, 1?, 2?,? ???,??? 
 
Translation-based Paraphrase Suggestion (TB-
PS) 
After the user selects a phrase for paraphrasing, 
with a L1 phrase F as an additional input, the 
suggestion problem will be: 
 
?? ? argmax
????????????????
???|??, ??, ? ??, ??, ??, ? ??, ???? 
 
The TB-PS model disambiguates paraphrases from 
the translations of F instead of paraphrases P. 
 
4. Experimental Results 
In this section, we describe the experimental 
setting and the preliminary results. Instead of 
training a whole machine translation using toolkits 
such as Moses (Koehn et. al, 2007), we used only 
bilingual phrase alignment as translations to 
prevent from the noise produced by the machine 
translation decoder. Word alignments were 
produced using Giza++ toolkit (Och and Ney, 
2003), over a set of 2,220,570 Chinese-English 
sentence pairs in Hong Kong Parallel Text 
(LDC2004T08) with sentences segmented using 
the CKIP Chinese word segmentation system (Ma 
and Chen, 2003). In training the phrasal lexical 
disambiguation model, we used the English part of 
Hong Kong Parallel Text as our training data.  
To assess the effectiveness of FLOW, we selected 
10 Chinese sentences and asked two students to 
translate the Chinese sentences to English 
sentences using FLOW. We kept track of the 
sentences the two students entered. Table 1 shows 
the selected results. 
 
 
 
 
Model Results 
TB-PS ????, the price of rice... 
 in short 
 all in all 
 in a nutshell 
 in a word 
 to sum up 
CS-PS She looks forward to coming 
 look forward to 
 looked forward to 
 is looking forward to 
forward to
 expect 
CS-PS there is no doubt that ? 
 there is no question 
it is beyond doubt 
 I have no doubt 
beyond doubt
 it is true 
CS-NP We put forward ? 
 the proposal 
additional
 our opinion 
the motion
 the bill 
TB-NP ...on ways to identify tackle ?? 
 money laundering 
 money 
 his 
 forum entitled 
 money laundry 
Table 1. The preliminary results of FLOW 
 
Both of the paraphrase models CS-PS and TB-PS 
perform quite well in assisting the user in the 
writing task. However, there are still some 
problems such as the redundancy suggestions, e.g., 
?look forward to? and ?looked forward to?. 
Besides, although we used the POS tags as 
features, the syntactic structures of the suggestions 
are still not consistent to an input or selected 
phrases. The CS-NP and the TB-NP model also 
perform a good task. However, the suggested 
phrases are usually too short to be a semantic unit. 
The disambiguation model tends to produce shorter 
phrases because they have more common context 
features.  
 
161
5. Conclusion and Future Work  
In this paper, we presented FLOW, an interactive 
writing assistance system, aimed at helping EFL 
writers compose and revise without interrupting 
their writing flow. First-language-oriented and 
context-sensitive features are two main 
contributions in this work. Based on the studies on 
second language writing that EFL writers tend to 
use their native language to produce texts and then 
translate into English, the first-language-oriented 
function provides writers with appropriate 
translation suggestions. On the other hand, due to 
the fact that selection of words or phrases is 
sensitive to syntax and context, our system 
provides suggestions depending on the contexts. 
Both functions are expected to improve EFL 
writers? writing performance. 
In future work, we will conduct experiments to 
gain a deeper understanding of EFL writers? 
writing improvement with the help of FLOW, such 
as integrating FLOW into the writing courses to 
observe the quality and quantity of students? 
writing performance. Many other avenues exist for 
future research and improvement of our system. 
For example, we are interested in integrating the 
error detection and correction functions into 
FLOW to actively help EFL writers achieve better 
writing success and further motivate EFL writers 
to write with confidence. 
 
References  
Valerie Arndt. 1987. Six writers in search of texts: A 
protocol based study of L1 and L2 writing. ELT 
Journal, 41, 257-267. 
Colin Bannard and Chris Callison-Burch. 2005. 
Paraphrasing with bilingual parallel corpora. In 
Proceedings of ACL, pp. 597-604. 
Marine Carpuat and Dekai Wu. 2007. Improving 
Statistical Machine Translation using Word Sense 
Disambiguation. In Proceedings of EMNLP-CoNLL, 
pp 61?72. 
Alister Cumming. 1989. Writing expertise and second 
language proficiency. Language Learning, 39, 81-
141. 
George Foster, Philippe Langlais, and Guy Lapalme. 
2002. Transtype: Text prediction for translators. In 
Proceedings of ACL Demonstrations, pp. 93-94. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan,Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constrantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of ACL Demonstration Session, pp. 
177?180.  
Barbara Kroll. 1990. Second Language Writing: 
Research Insights for the Classroom. Cambridge 
University Press, Cambridge. 
Aur?elien Max. 2009. Sub-sentential Paraphrasing by 
Contextual Pivot Translation. In Proceedings of the 
2009 Workshop on Applied Textual Inference, ACL-
IJCNLP, pp 18-26. 
Tony Silva.  1993. Toward an Understanding of the 
Distinct Nature of L2 Writing: The ESL Research 
and Its Implications. TESOL Quarterly 27(4): 657?
77. 
Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, and 
Changning Huan. 2000. PENS: A Machine-Aided 
English Writing System for Chinese Users. In 
Proceedings of ACL, pp 529-536. 
Mark Wolfersberger. 2003. L1 to L2 writing process 
and strategy transfer: a look at lower proficiency 
writers. TESL-EJ: Teaching English as a Second or 
Foreign Language, 7(2), A6 1-15. 
 
162
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 139?144,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linggle: a Web-scale Linguistic Search Engine for Words in Context    Joanne Boisson+, Ting-Hui Kao*, Jian-Cheng Wu*, Tzu-His Yen*, Jason S. Chang* +Institute of Information Systems and Applications *Department of Computer Science National Tsing Hua University HsinChu, Taiwan, R.O.C. 30013 {joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang} @gmail.com     Abstract 
In this paper, we introduce a Web-scale lin-guistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and ad-ditional regular expression (RE) operators. In our approach, we incorporate inverted file in-dexing, PoS information from BNC, and se-mantic indexing based on Latent Dirichlet Al-location with Google Web 1T. The method in-volves parsing the query to transforming it in-to several keyword retrieval commands. Word chunks are retrieved with counts, further filter-ing the chunks with the query as a RE, and fi-nally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are also provided.  In addition, Linggle provides example sentences from The New York Times on demand. The current implementation of Linggle is the most functionally comprehen-sive, and is in principle language and dataset independent. We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major lan-guages in the world. 1 Introduction As a non-native speaker writing in English, one encounters many problems. Doubts concerning the usage of a preposition, the mandatory presen-ce of a determiner, the correctness of the associa-tion of a verb with an object, or the need for syn-onyms of a term in a given context are issues that arise frequently. Printed collocation dictionaries and reference tools based on compiled corpora offer limited coverage of word usage while knowledge of collocations is vital to acquire a 
good level of linguistic competency. We propose to address these limitations with a comprehen-sive system aimed at helping the learners ?know a word by the company it keeps? (Firth, 1957). Linggle (linggle.com). The system based on Web-scaled datasets is designed to be a broad coverage language reference tool for English Second Language learners (ESL). It is conceived to search information related to word usage in context under various conditions. First, we build an inverted file index for the Google Web 1T n-grams to support queries with RE-like patterns including PoS and synonym matches. For example, for the query ?$V $D +important role?, Linggle retrieves 4-grams that start with a verb and a determiner followed by a synonym of important and the keyword role (e.g., play a significant role 202,800). A natural lan-guage interface is also available for users who are less familiar with pattern-based searches. For example, the question ?How can I describe a beach?? would retrieve two word chunks such as ?sandy beach 413,300? and ?rocky beach 16,800?. The n-gram search implementation is achieved through filtering, re-indexing, populat-ing an HBase database with the Web 1T n-grams and augmenting them with the most frequent PoS for words (without disambiguation) derived from the British National Corpus (BNC).   The n-grams returned for a query can then be linked to examples extracted from the New York Times Corpus (Sandhaus, 2008) in order to provide full sentential context for more effective learning.  In some situations, the user might need to search for words in a specific syntactic relation (e.g., Verb-Object collocation). The query absorb $N in n-grams display mode returns all the nouns that follow the verb ordered by decreasing n-gram counts. Some of these nouns might not be objects of the verb absorb. In contrast, the same 
139
query in cluster display mode will control that two words have been labeled verb-object by a parser. Moreover, n-grams grouped by object topic/domain give the learner an overview of the usage of the verb. For example the verb absorb takes clusters of objects related to the topics liq-uid, energy, money, knowledge, and population.  
  Figure 1. An example Linggle search for the que-ry ?absorb $N.?  This tendency of predicates to prefer certain classes of arguments is defined by Wilks (1978) as selectional preferences and widely reported in the literature. Erk and Pad? (2010) extend exper-iments on selectional preference induction to in-verse selectional preference, considering the re-striction imposed on predicates. Inverse sectional preference is also implemented in linggle (e.g. ?$V apple?). Linggle presents clusters of synonymous col-locates (adjectives, nouns and verbs) of a query keyword. We obtained the clusters by building on Lin and Pantel?s (2002) large-scale repository of dependencies and word similarity scores. Us-ing the method proposed by Ritter and Etzioni (2010) we induce selectional preference with a Latent Dirichlet Allocation (LDA) model to seed the clusters. The rest of the paper is organized as follows. We review the related work in the next section. Then we present the syntax of the queries and the functionalities of the system (Section 3). We de-scribe the details of implementation including the indexing of the n-grams and the clustering algo-rithm (Section 4) and draw perspective of devel-opment of Web scale search engines (Section 5). 2 Related work Web-scale Linguistic Search Engine (LSE) has been an area of active research. Recently, the state-of-the-art in LSE research has been re-
viewed in Fletcher (2012). We present in this paper a linguistic search engine that provides a more comprehensive and powerful set of query features.  Kilgarriff et al (2001) describe the implemen-tation of the linguistic search engine Word Sketch (2001) that displays collocations and de-pendencies acquired from a large corpus such as the BNC. Word Sketch is not as flexible as typi-cal search engines, only supporting a fixed set of queries.  Recently, researchers have been attempting to go one step further and work with Web scale da-tasets, but it is difficult for an academic institute to crawl a dataset that is on par with the datasets built by search engine companies. In 2006, Google released the Web 1T for several major languages of the world (trillion-word n-gram da-tasets for English, Japanese, Chinese, and ten European languages), to stimulate NLP research in many areas.  In 2008, Chang described a pro-totype that enhances Google Web 1T bigrams with PoS tags and supports search in the dataset by wildcards (wild-PoS), to identify recurring collocations. Wu, Witten and Franken (2010) describe a more comprehensive system (FLAX) that combines filtered Google data with text ex-amples from the BNC for several learning activi-ties.  In a way similar to Chang (2008) and Wu, Witten and Franken (2010), Stein, Potthast, and Trenkmann (2010) describe the implementation and application of NetSpeak, a system that pro-vides quick access to the Google Web 1T n-gram with RE-like queries (alternator ?|?, one arbitrary word ?*?, arbitrary number of words between two specified words ???). In contrast to Linggle, NetSpeak does not support PoS wildcard or con-ceptual clustering. An important function in both Linggle and NetSpeak is synonym query. NetSpeak uses WordNet (Fellbaum 2010) synsets to support synonym match. But WordNet synsets tend to contain very little synonyms, leading to poor coverage. Alternatively, one can use the distribu-tional approach to similarity based on a very large corpus. Lin and Pantel (2002) report efforts to build a large repository of dependencies ex-tracted from large corpora such as Wikipedia, and provide similarity between words (demo.patrickpantel.com). We use these results both for handling synonym queries and to or-ganize the n-grams into semantic classes.  More recently, Ritter and Etzioni (2010) pro-pose to apply an LDA model (Blei et al 2003) to 
140
the problem of inducing selectional preference. The idea is to consider the verbs in a corpus as the documents of a traditional LDA model. The arguments of the verb that are encountered in the corpus are treated as the words composing a document in the traditional model. The model seems to successfully infer the semantic classes that correspond to the preferred arguments of a verb. The topics are semi-automatically labeled with WordNet classes to produce a repository of human interpretable class-based selectional pref-erence. This choice might be due to the fact that if most LDA topic heads are usually reasonable upon human inspection, some topics are also in-coherent (Newman 2010) and lower frequency words are not handled as successfully. We con-trol the coherence of the topics and rearrange them into human interpretable clusters using a distributional similarity measure.  Microsoft Sempute Project (Sempute Team 2013) also explores core technologies and appli-cations of semantic computing. As part of Sempute project, NeedleSeek is aimed at auto-matically extracting data to support general se-mantic Web searches. While Linggle focuses on n-gram information for language learning, NeedleSeek also uses LDA to support question answering (e.g., What were the Capitals of an-cient China?) . In contrast to the previous research in Web scale linguistic search engines, we present a sys-tem that supports queries with keywords, wild-card words, POS, synonyms, and additional regular expression (RE) operators and displays the results according the count, similarity, and topic with clusters of synonyms or conceptually related words. We exploit and combine the power of both LDA analysis and distributional similarity to provide meaningful semantic classes that are constrained with members of high simi-larity. Distributional similarity (Lin 1998) and LDA topics become two angles of attack to view language usage and corpus patterns. 3 Linggle Functionalities The syntax of Linggle queries involves basic regular expression of keywords enriched with wildcard PoS and synonyms. Linggle queries can be either pattern-based commands or natural lan-guage questions. The natural language queries are currently handled by simple string matching based on a limited set of questions and command pairs provided by a native speaker informant.  
3.1 Natural language queries The handling of queries formulated in natural language has been implemented with handcrafted patterns refined from a corpus of questions found on various websites. Additionally, we asked both native and non-native speakers to use the system for text edition and to write down all the ques-tions that arise during the exercise.  Linggle transforms a question into commands for further processing based on a set of canned texts (e.g., ?How to describe a beach?? will be converted to ?$A beach?). We are in the process of gathering more examples of language-related question and answer pairs from Answers.com to improve the precision, versatility, and coverage. 3.2 Syntax of queries The syntax of the patterns for n-grams is shown in Table 1. The syntax supports two types of que-ry functions: basic keyword search with regular expression capability and semantic search.  Basic search operators enable the users to que-ry zero, one or more arbitrary words up to five words. For example, the query ?set off ? $N? is intended to search for all nouns in the right con-text of set off, within a maximum distance of three words.  In addition, the ??? operator in front of a word represents a search for n-grams with or without the word. For example, a user wanting to deter-mine whether to use the word to between listen and music can formulate the query ?listen ?to music.? Yet another operation ?|? is provided to search for information related to word choice. For ex-ample the query ?build | construct ... dream? can be used to reveal that people build a dream much more often than they construct a dream. A set of PoS symbols (shown in Table 2) is defined to support queries that need more preci-sion than the symbol *. More work might be needed to resolve PoS ambiguity for n-grams. Currently, any word that has been labeled with the requested PoS in the BNC more than 5% of the time is displayed.  The ?+? operator is provided to support se-mantic queries. Placed in front of a word, it is intended to search for synonyms in the context. For example the query ?+sandy beach? would generate rocky beach, stony beach, barren beach in the top three results. The query ?+abandoned beach? generates deserted, destroyed and empty beach at the top of the list. To support conceptual clustering of collocational n-grams, we need to 
141
identify synonyms related to different senses of a given word. Table 3 shows an example of the result obtained for the ambiguous word bank as a unigram query. We can see the two main senses of the word (river bank and institution) as clus-ters.  Operators  Description * Any Word ? With/without the word ? Zero or more words | Alternator $ Part of speech + Synonyms Table 1: Operators in the Linggle queries   Part of speech  Description N Noun V Verb A Adjective R Adverb PP Preposition NP Proper Noun PR Pronoun D Determiner Table 2: Part-of-speech in the Linggle queries   A cluster button on the interface activates or cancels conceptual clustering. When Linggle is switched into a cluster display mode, adjective-nouns, verb-objects and subject-verb relations can be browsed based on the induced conceptual clusters (see Figure 1). The New York Times Example Base In order to display complete sentence examples for users, the New York Times Corpus sentences are indexed by word. When the user searches for words in a specific syntactic relation, morpho-logical query expansion is performed and pat-terns are used to increase both the coverage and the precision of the provided examples. For ex-ample, the bi-gram kill bacteria will be associat-ed with the example sentence ?The bacteria are killed by high temperatures.?. 3.3 Semantic Clusters Two types of semantic clusters are provided in Linggle: selectional preference and clusters of synonyms. Selectional preference expresses for example that an apple is more likely to be eaten or cooked than to be killed or hanged. Different classes of arguments for a predicate (or of predi-cates for an argument) can be found automatical-ly. The favorite class of objects for the verb drink 
is LIQUID with the noun water ranked at the top. Less frequent objects belonging to the same class include liquor in the tail of the list. We aim at grouping arguments and predicates into semantic clusters for better readability.  valley mountain river lake hill bay plain north ridge coast city district town area community municipality country village land region route highway road railway bridge crossing canal railroad junction stream creek tributary  organization business institution company industry organisation agency school department university government court board channel network affiliate outlet supplier manufacturer distributor vendor retailer in-vestor broker provider lender owner creditor share-holder customer employer Table 3: First two level-one clusters of synonyms for the word ?bank? We produce clusters with a two-layer structure. Level one represents loose topical relatedness roughly corresponding to broad domains, while level two is aimed at grouping together closely similar words. For example, among the objects of the verb cultivate, the nouns tie and contact belong to the same level-two cluster. Attitude and spirit belong to another level-two cluster but both pairs are in the same level-one cluster. The nouns fruit and vegetable are clustered together in another level-one cluster. This double-layer representation is a solution to express at once close synonymy and topic relatedness. The clus-ters of symonyms displayed in Table 3 follow the same representation. 4 Implementation of the system In this section, we describe the implementation of Linggle, including how to index and store n-grams for a fast access (Section 4.1) and construction of the LDA models (Section 4.2). We will describe the clustering method in more details in section 5.  4.1 N-grams preprocessing The n-grams are first filtered keeping only the words that are in WordNet and in the British Na-tional Corpus, and then indexed by word and position in the n-gram, in a way similar to the rotated n-gram approach proposed by Lin et. al. (2010). The files are then stored in an Apache 
142
HBase NoSQL base. The major advantages of using a NoSQL database is the excellent perfor-mance in querying the ability of storing large amounts of data across several servers and the capability to scale up when we have additional entries in the dataset, or additional datasets to add to the system. 4.2 LDA models computations Two types of LDA models are calculated for Linggle. The first type is a selectional preference model between heads and modifiers. Six models are calculated in total for the subject-verb, the verb-object and the adjective-noun relations done in a similar way to Ritter and Etzioni?s (2010) model with binary relations instead of triples. The second is a word/synonyms model in which a word is considered as a document in LDA and its synonyms as the words of the document. This second model has the effect of splitting the syno-nyms of a word into different topics, as shown in Table 3.  Seeds                                             parameter: s1 1. Consider the m first topics for a verb v ac-cording to the LDA per document-topic dis-tribution (?) 2. Consider S = o1,?,on, a set of n objects of v.  3. Split S into m classes C1,..,Cm according to their LDA per topic-word probability: oi  is assigned to the topic in which it has the highest probability. 4. For each class Ci, move every object oj that is not similar to any other ok of Ci , according to a similarity threshold s1 into a new created class. Level 2                                           parameter: s2  While (Argmaxci ,cj Sim( ci , cj ) > s2):            Merge Argmaxci ,cj Sim( ci , cj ) into one class. Level 1                                           parameter: s3  While (Argmaxci ,cj Sim(ci , cj ) > s3):            Group Argmaxci ,cj Sim( ci ,cj ) under the             same level 1 cluster. Table 4:  Clustering Algorithm for the object of a giv-en verb  The hyperparameters alpha, eta, that affect the sparsity of the document-topic (theta) and the topic-word (lambda) distributions are both set to 0.5 and the number of topics is set to 300. More research would be necessary to optimize the val-ue for the parameters in the perspective of the clustering algorithm, as quickly discussed in the next section.  
 Sim (ci, cj): 1. Build the Cartesian product C = ci ? cj 2. Get P the set of the similarity between all word pairs in C 3. Return Sim(ci,cj) the mean of the scores in P  Table 5:  Similarity between two classes ti and tj 5 Clustering algorithm The clustering algorithm combines topic model-ing results and a semantic similarity measure. We use Pantel?s dependencies repository to compute LDA models for subject-verbs, verbs-objects and adjective-nouns relations in both di-rections. Currently, we also use Pantel?s similari-ty measure. It has a reasonable precision partly because it relies on parser information instead of bag of words windows. However the coverage of the available scores is lower than what would be needed for Linggle. We will address this issue in the near future by extending it with similarity scores computed from the n-grams. We combine the two distributional semantics approaches in a simple manner inspired by clus-tering by committee algorithm  (CBC). The simi-larity measure is used to refine the LDA topics and to generate finer grain clusters. Conversely, LDA topics can also be seen as the seeds of our clustering algorithm. This algorithm intends to constrain the words that belong to a final cluster more strictly than LDA does in order to obtain clearly interpretable clusters. The exact same algorithm is applied to synonym models, for synonyms of nouns, adjec-tives and verbs (shown in Table 3). Table 4 shows the algorithm for constructing double layer clusters for a set S of objects of a verb v. The objects are first roughly split into classes, attributing a single topic to every object oi. The topic of a word oi is determined accord-ing to its per topic-word probability. More exper-iments could be done using the product of the per document-topic and the per topic-word LDA probabilities instead, in order to take into account the specific verb when assigning a topic to the object. Such a way of assigning topics should also be more sensitive to the LDA hyperparame-ters.  At this stage, some classes are incoherent and that low frequency words that do not appear in the head of any topic are often misclassified. Words are rearranged between the classes and create new classes if necessary using the simi-larity measure. If any word of a class is not simi-
143
lar to any other word in this class (the threshold is set to s1 = 0.09), a new class is created for it. Any two classes are then merged if their simi-larity (computer accordingly to Table 5) is above s2=0.06, forming the level 2 clusters. Classes are then grouped together if the similarity between them is above s3 = 0.02 forming the level 1 clus-ters. Finally, the classes that contain less than three words are not displayed in Linggle and the predi-cate-arguments counts in the Web 1T are re-trieved using a few hand crafted RE and morpho-logical expansion of the nouns and the verbs.  This algorithm appears to generate interpreta-ble semantic classes and to be quite robust re-garding the threshold parameters. More tests and rigorous evaluation are left to future work.   6 Conclusion There are many different directions in which Linggle will be improved. The first one is to al-low users to work with word forms and with multiword expressions. The second one concerns the extension of the coverage of the example base with several large corpora such as Wikipe-dia and the extension of the coverage of the simi-larity measure. The third direction concerns the development of automatic suggestions for text edition, such as suggesting a better adjective or a different preposition in the context of a sentence. Finally, Linggle is currently being extended to Chinese. We presented a prototype that gives access to Web Scale collocations. Linggle displays both word usage and word similarity information. Depending on the type of the input query, the results are displayed under the form of lists or clusters of n-grams. The system is designed to become a multilingual platform for text edition and can also become a valuable resource for natural language processing research. References  David Blei, A. Ng, and M. Jordan. 2003. Latent Di-richlet alocation. Journal of Machine Learning Research, 3:993?1022, January 2003. Jason S. Chang, 2008. Linggle: a web-scale language reference search engine. Unpublished manuscript. Katrin Erk and Sebastian Pad?. 2010. A Flexible, Corpus-Driven Model of Regular and Inverse Se-lectional Preferences. In Proceedings of ACL 2010.  Christiane Fellbaum. 2010. WordNet. MIT Press, Cambridge, MA. 
John Rupert Firth. 1957. The Semantics of Linguistics Science. Papers in linguistics 1934-1951. London: Oxford University Press. William H Fletcher. 2012. Corpus analysis of the world wide web." In The Encyclopedia of Applied Linguistics. Adam Kilgarriff , and David Tugwell. 2001. Word sketch: Extraction and display of significant collo-cations for lexicography. In Proceedings of COL-LOCTION: Computational Extraction, Analysis and Exploitation workshop, 39th ACL and 10th EACL, pp. 32-38. Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th inter-national conference on Computational linguistics, volume 2. Association for Computational Linguis-tics, pp. 768-774.  Dekang Lin, and Patrick Pantel. 2002. Concept Dis-covery from Text. In Proceedings of Conference on Computational Linguistics (COLING-02). pp. 577-583. Taipei, Taiwan. Dekang Lin, Kenneth Ward Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, Sushant Narsale. 2010. New tools for web-scale n-grams. In Proceedings of LREC. David Newman, Jey Han Lau, Karl Grieser and Timo-thy Baldwin (2010). Automatic Evaluation of Topic Coherence. In Proceedings of Human Lan-guage Technologies, 11th NAACL HLT, Los Ange-les, USA, pp. 100?108. Evan Sandhaus. 2008. "New york times corpus: Cor-pus overview." LDC catalogue LDC2008T19.  Sempute Team. 2013. What is NeedleSeek? http://needleseek.msra.cn/readme.htm Benno Stein, Martin Potthast, and Martin Trenkmann. 2010. Retrieving customary Web language to assist writers. Advances in Information Retrieval. Springer Berlin Heidelberg, pp. 631-635.  Martin Potthast, Martin Trenkmann, and Benno Stein. Using Web N-Grams to Help Second-Language Speakers .2010. SIGIR 10 Web N-Gram Workshop, pages 49-49. Alan Ritter, Mausam, and Oren Etzioni. 2010. A Latent Dirichlet Allocation method for Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Lin-guistics (July 2010), pp. 424-434. Yorick Wilks. 1978. Making preferences more active. Artificial Intelligence 11(3), pp. 197-223.  Shaoqun Wu, Ian H. Witten and Margaret Franken (2010). Utilizing lexical data from a web-derived corpus to expand productive collocation knowledge. ReCALL, 22(1), 83?102. 
144
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 96?104,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
GRASP: Grammar- and Syntax-based Pattern-Finder in CALL 
 
 
Chung-Chi Huang*  Mei-Hua Chen* Shih-Ting Huang+  Hsien-Chin Liou**  Jason S. Chang+ 
  
* Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C. 300 
+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C. 300 
**  Department of Foreign Languages and Literature, NTHU, HsinChu, Taiwan, R.O.C. 300 
{u901571,chen.meihua,koromiko1104,hsienchin,jason.jschang}gmail.com 
 
 
 
 
 
 
Abstract 
We introduce a method for learning to 
describe the attendant contexts of a given 
query for language learning. In our 
approach, we display phraseological 
information in the form of a summary of 
general patterns as well as lexical bundles 
anchored at the query. The method 
involves syntactical analyses and inverted 
file construction. At run-time, grammatical 
constructions and their lexical 
instantiations characterizing the usage of 
the given query are generated and 
displayed, aimed at improving learners? 
deep vocabulary knowledge. We present a 
prototype system, GRASP, that applies the 
proposed method for enhanced collocation 
learning. Preliminary experiments show 
that language learners benefit more from 
GRASP than conventional dictionary 
lookup. In addition, the information 
produced by GRASP is potentially useful 
information for automatic or manual 
editing process. 
1 Introduction 
Many learners submit word or phrase queries (e.g., 
?role?) to language learning sites on the Web to 
get usage information every day, and an increasing 
number of services on the Web specifically target 
such queries. Language learning tools such as 
concordancers typically accept single-word queries 
and respond with example sentences containing the 
words. There are also collocation reference tools 
such as Sketch Engine and TANGO that provide 
co-occurring words for the query word. Another 
collocation tool, JustTheWord further organizes 
and displays collocation clusters. 
Learners may want to submit phrase queries 
(fixed or rigid collocaions) to learn further how to 
use the phrase in context, or in other words, to 
acquire the knowledge on the attendant 
phraseology of the query. These queries could be 
answered more appropriately if the tool accepted 
long queries and returned a concise summary of 
their surrounding contexts. 
Consider the query ?play role?. The best 
responses for this query are probably not just 
example sentences, but rather the phraseological 
tendencies described grammatically or lexically. A 
good response of such a summary might contain  
patterns such as ?play Det Adj role? (as in ?play an 
important role?) and ?play ~ role in V-ing? (as in 
?play ~ role in shaping ??). Intuitively, by 
exploiting simple part-of-speech analysis, we can 
derive such patterns, inspired by the grammatical 
theory of Pattern Grammar 1  in order to provide 
more information on demand beyond what is given 
in a grammar book. 
We present a system, GRASP, that provide a 
usage summary of the contexts of the query in the 
form of patterns and frequent lexical bundles. Such 
rich information is expected to help learners and 
lexicographers grasp the essence of word usages. 
An example GRASP response for the query ?play 
                                                           
1
 Please refer to (Hunston and Francis, 2000). 
96
role? is shown in Figure 1. GRASP has retrieved 
the sentences containing the query in a reference 
corpus. GRASP constructs these query-to-sentence 
index in the preparation stage (Section 3). 
 
Figure 1. An example GRASP search for ?play role?. 
 
At run-time, GRASP starts with a search query 
(e.g., ?play role?) submitted by the user. GRASP 
then retrieves example sentences and generates a 
summary of representative contexts, using patterns 
(e.g., ?play ~ role in V-ing?) and lexical bundles 
(e.g., ?play ~ role in shaping. In our 
implementation, GRASP also returns the 
translations and the example sentences of the 
lexical instances, so the learner can use their 
knowledge of native language to enhance the 
learning process. 
2 Related Work 
Computer-assisted language learning (CALL) has 
been an area of active research. Recently, more and 
more research based on natural language 
processing techniques has been done to help 
language learners. In our work, we introduce a 
language learning environment, where summarized 
usage information are provided, including how 
function words and verb forms are used in 
combination with the query. These usage notes 
often help contrast the common sources of error in 
learners? writing (Nicholls, 1999). In our pilot 
teaching experiment, we found learners have 
problems using articles and prepositions correctly 
in sentence composition (as high as 80% of the 
articles and 60% of the prepositions were used 
incorrectly), and GRASP is exactly aimed at 
helping ESL or EFL learners in that area. 
Until recently, collocations and usage 
information are compiled mostly manually 
(Benson et al, 1986). With the accessibility to 
large-scale corpora and powerful computers, it has 
become common place to compile a list of 
collocations automatically (Smadja, 1993). In 
addition, there are many collocation checkers 
developed to help non-native language learners 
(Chang et al, 2008), or learners of English for 
academic purposes (Durrant, 2009). 
Recently, automatic generation of collocations 
for computational lexicography and online 
language learning has drawn much attention. 
Sketch Engine (Kilgarriff et al, 2004) summarizes 
a word?s grammatical and collocation behavior, 
while JustTheWord clusters the co-occurring 
words of single-word queries and TANGO (Jian et 
al., 2004) accommodates cross-lingual collocation 
searches. Moreover, Cheng et al (2006) describe 
how to retrieve mutually expected words using 
concgrams. In contrast, GRASP, going one step 
further, automatically computes and displays the 
information that reveals the regularities of the 
contexts of user queries in terms of grammar 
patterns. 
Recent work has been done on incorporating 
word class information into the analyses of 
phraseological tendencies. Stubbs (2004) 
introduces phrase-frames, which are based on 
lexical ngrams with variable slots, while Wible et 
al. (2010) describe a database called StringNet, 
with lexico-syntactic patterns. Their methods of 
using word class information are similar in spirit to 
our work. The main differences are that our 
patterns is anchored with query words directly and 
generalizes query?s contexts via parts-of-speech, 
and that we present the query?s usage summary in 
Search query: 
Mapping query words to (position, sentence) pairs: 
?play? occurs in (10,77), (4,90), (6,102), ?, and so on. 
?role? occurs in (7,90), (12,122), (6,167), ?, and so on. 
A. In-between pattern grammar: 
   Distance 3 (1624): 
play DT JJ role (1364): 
e.g., ?play an important role? (259), ?play a major role? (168), ? 
play DT VBG role (123): 
e.g., ?play a leading role? (75), ?play a supporting role? (5), ? 
play DT JJR role (40): 
e.g., ?play a greater role? (17), ?play a larger role? (8), ? 
   Distance 2 (480): 
play DT role (63): 
e.g., ?play a role? (197), ?play the role? (123), ? 
play JJ role (63): 
e.g., ?play important role? (15), ?play different role? (6), ? 
   Distance 1 (6): 
play role (6) 
B. Subsequent pattern grammar: 
play ~ role IN(in) DT (707): 
e.g., ?play ~ role in the? (520), ?play ~ role in this? (24), ? 
play ~ role IN(in) VBG (407): 
e.g., ?play ~ role in shaping? (22), ? 
play ~ role IN(in) NN (166): 
e.g., ?play ~ role in society? (7), ?play ~ role in relation? (5), ? 
C. Precedent pattern grammar: 
NN MD play ~ role (83): 
e.g., ?communication will play ~ role ? (2), ? 
JJ NNS play ~ role (69): 
e.g., ?voluntary groups play ~ role? (2), ? 
Type your search query, and push GRASP! 
97
terms of function words as well as content word 
form (e.g., ?play ~ role in V-ing?), as well as 
elastic lexical bundles (e.g., ?play ~ role in 
shaping?). Additionally, we also use semantic 
codes (e.g., PERSON) to provide more information 
in a way similar what is provided in learner 
dictionaries. 
3 The GRASP System 
3.1 Problem Statement 
We focus on constructing a usage summary likely 
to explain the contexts of a given linguistic search. 
The usage summary, consisting of the query?s 
predominant attendant phraseology ranging from 
pattern grammar to lexical phrases, is then returned 
as the output of the system. The returned summary, 
or a set of patterns pivoted with both content and 
function words, can be used for learners? benefits 
directly, or passed on to an error detection and 
correction system (e.g., (Tsao and Wible, 2009) 
and some modules in (Gamon et al, 2009) as rules. 
Therefore, our goal is to return a reasonable-sized 
set of lexical and grammatical patterns 
characterizing the contexts of the query. We now 
formally state the problem that we are addressing. 
Problem Statement: We are given a reference 
corpus C from a wide range of sources and a 
learner search query Q. Our goal is to construct a 
summary of word usages based on C that is likely 
to represent the lexical or grammatical preferences 
on Q?s contexts. For this, we transform the words 
in Q into sets of (word position, sentence record) 
pairs such that the context information, whether 
lexically- or grammatical-oriented, of the querying 
words is likely to be acquired efficiently. 
In the rest of this section, we describe our 
solution to this problem. First, we define a strategy 
for preprocessing our reference corpus (Section 
3.2). Then, we show how GRASP generates 
contextual patterns, comprising the usage summary, 
at run-time (Section 3.3). 
3.2 Corpus Preprocessing 
We attempt to find the word-to-sentence mappings 
and the syntactic counterparts of the L1 sentences 
expected to speed up run-time pattern generation. 
Our preprocessing procedure has two stages. 
Lemmatizing and PoS Tagging. In the first stage, 
we lemmatize each sentence in the reference 
corpus C and generate its most probable POS tag 
sequence. The goal of lemmatization is to reduce 
the impact of morphology on statistical analyses 
while that of POS tagging is to provide a way to 
grammatically describe and generalize the 
contexts/usages of a linguistic query. Actually, 
using POS tags is quite natural: they are often used 
for general description in grammar books, such as 
one?s (i.e., possessive pronoun) in the phrase 
?make up one?s mind?, oneself (i.e., reflexive 
pronoun) in ?enjoy oneself very much?, 
superlative_adjective in ?the most 
superlative_adjective?, NN (i.e., noun) and VB (i.e., 
base form of a verb) in ?insist/suggest/demand that 
NN VB? and so on. 
Constructing Inverted Files. In the second stage, 
we build up inverted files of the lemmas in C for 
quick run-time search. For each lemma, we record 
the sentences and positions in which it occurs. 
Additionally, its corresponding surface word and 
POS tag are kept for run-time pattern grammar 
generation. 
 
Figure 2. Generating pattern grammar and usage 
summary at run-time. 
procedure GRASPusageSummaryBuilding(query,proximity,N,C) 
(1)  queries=queryReformulation(query) 
(2)  GRASPresponses= ?  
for each query in queries 
(3)    interInvList=findInvertedFile(w1 in query) 
for each lemma wi in query except for w1 
(4)      InvList=findInvertedFile(wi) 
//AND operation on interInvList and InvList 
(5a)    newInterInvList= ? ; i=1; j=1 
(5b)    while i<=length(interInvList) and j<=lengh(InvList) 
(5c)       if interInvList[i].SentNo==InvList[j].SentNo 
(5d)         if withinProximity(interInvList[i]. 
wordPosi,InvList[j].wordPosi,proximity) 
(5e)   Insert(newInterInvList, interInvList[i],InvList[j]) 
else if interInvList[i].wordPosi<InvList[j].wordPosi 
(5f)   i++ 
else //interInvList[i].wordPosi>InvList[j].wordPosi 
(5g)   j++ 
else if interInvList[i].SentNo<InvList[j].SentNo 
(5h)          i++ 
else //interInvList[i].SentNo>InvList[j].SentNo 
(5i)           j++ 
(5j)     interInvList=newInterInvList 
//construction of GRASP usage summary for this query 
(6)    Usage= ?  
for each element in interInvList 
(7)       Usage+={PatternGrammarGeneration(query,element,C)} 
(8a)  Sort patterns and their instances in Usage in descending order 
of frequency 
(8b)  GRASPresponse=the N patterns and instances in Usage with 
highest frequency 
(9)    append GRASPresponse to GRASPresponses 
(10) return GRASPresponses 
98
3.3 Run-Time Usage Summary Construction 
Once the word-to-sentence mappings and syntactic 
analyses are obtained, GRASP generates the usage 
summary of a query using the procedure in Figure 
2. 
In Step (1) we reformulate the user query into 
new ones, queries, if necessary. The first type of 
query reformulation concerns the language used in 
query. If it is not in the same language as C, we 
translate query and append the translations to 
queries as if they were submitted by the user. The 
second concerns the length of the query. Since 
single words may be ambiguous in senses and 
contexts or grammar patterns are closely associated 
with words? meanings (Hunston and Francis, 2000), 
we transform single-word queries into their 
collocations, particularly focusing on one word 
sense (Yarowsky, 1995), as stepping stones to 
GRASP patterns. Notice that, in implementation, 
users may be allowed to choose their own 
interested translation or collocation of the query 
for usage learning. The prototypes for first-
language (i.e., Chinese) queries and English 
queries of any length are at A2 and B3 respectively. 
The goal of cross-lingual GRASP is to assist EFL 
users even when they do not know the words of 
their searches and to avoid incorrect queries 
largely because of miscollocation, misapplication, 
and misgeneralization. 
Afterwards, we initialize GRASPresponses to 
collect usage summaries for queries (Step (2)) and 
leverage inverted files to extract and generate each 
query?s syntax-based contexts. In Step (3) we prep 
interInvList for the intersected inverted files of the 
lemmas in query. For each lemma wi within, we 
first obtain its inverted file, InvList (Step (4)) and 
perform an AND operation on interInvList 
(intersected results from previous iteration) and 
InvList (Step (5a) to (5j)4), defined as follows. 
First, we enumerate the inverted lists (Step (5b)) 
after the initialization of their indices i and j and 
temporary resulting intersection newInterInvList 
(Step (5a)). Second, we incorporate a new instance 
of (position, sentence), based on interInvList[i] and 
InvList[j], into newInterInvList (Step (5e)) if the 
sentence records of the indexed list elements are 
the same (Step (5c)) and the distance between their 
                                                           
2
 http://140.114.214.80/theSite/bGRASP_v552/ 
3
 http://140.114.214.80/theSite/GRASP_v552/ 
4
 These steps only hold for sorted inverted files. 
words are within proximity (Step (5d)). Otherwise, 
i and j are moved accordingly. To accommodate 
the contexts of queries? positional variants (e.g., 
?role to play? and ?role ~ play by? for the query 
?play role?), Step (5d) considers the absolute 
distance. Finally, interInvList is set for the next 
AND iteration (Step (5j)). 
Once we obtain the sentences containing query, 
we construct its context summary as below. For 
each element, taking the form ([wordPosi(w1), ?, 
wordPosi(wn)], sentence record) denoting the 
positions of query?s lemmas in the sentence, we 
generate pattern grammar involving replacing 
words in the sentence with POS tags and words in 
wordPosi(wi) with lemmas, and extracting fixed-
window 5  segments surrounding query from the 
transformed sentence. The result is a set of 
grammatical patterns with counts. Their lexical 
realizations also retrieved and displayed. 
The procedure finally generates top N 
predominant syntactic patterns and their N most 
frequent lexical phrases as output (Step (8)). The 
usage summaries GRASP returns are aimed to 
accelerate EFL learners? language understanding 
and learning and lexicographers? word usage 
navigation. To acquire more semantic-oriented 
patterns, we further exploit WordNet and majority 
voting to categorize words, deriving the patterns 
like ?provide PERSON with.? 
4 Experimental Results 
GRASP was designed to generate usage 
summarization of a query for language learning. 
As such, GRASP will be evaluated over CALL. In 
this section, we first present the setting of GRASP 
(Section 4.1) and report the results of different 
consulting systems on language learning in Section 
4.2. 
4.1 Experimental Setting 
We used British National Corpus (BNC) as our 
underlying reference corpus C. It is a British 
English text collection. We exploited GENIA 
tagger to obtain the lemmas and POS tags of C?s 
sentences. After lemmatizing and syntactic 
analyses, all sentences in BNC were used to build 
up inverted files and used as examples for 
grammar pattern extraction. 
                                                           
5
 Inspired by (Gamon and Leacock, 2010). 
99
English (E) sentence with corresponding Chinese (C) translation answer to 1st blank  answer to 2nd blank 
C: ????????????? 
E: Environmental protection has ___ impact ___. 
a profound on the Earth 
C: ????????????? 
E: The real estate agent ___ record profit ___. 
made a on house selling 
C: ?????????????? 
E: They plan to release their new album in ___ future 
the near none 
C: ???????????? 
E: He waited for her for a long time in ___ attempt ___ again. 
an to see her 
 
4.2 Results of Constrained Experiments 
In our experiments, we showed GRASP6  to two 
classes of Chinese EFL (first-year) college students. 
32 and 86 students participated, and were trained 
to use GRASP and instructed to perform a sentence 
translation/composition task, made up of pretest 
and posttest. In (30-minute) pretest, participants 
were to complete 15 English sentences with 
Chinese translations as hints, while, in (20-minute) 
posttest, after spending 20 minutes familiarizing 
word usages of the test candidates from us by 
consulting traditional tools or GRASP, participants 
were also asked to complete the same English 
sentences. We refer to the experiments as 
constrained ones since the test items in pre- and 
post-test are the same except for their order. A 
more sophisticated testing environment, however, 
are to be designed. 
Each test item contains one to two blanks as 
shown in the above table. In the table, the first item 
is supposed to test learners? knowledge on the 
adjective and prepositional collocate of ?have 
impact? while the second test the verb collocate 
make, subsequent preposition on, and preceding 
article a of ?record profit?. On the other hand, the 
third tests the ability to produce the adjective 
enrichment of ?in future?, and the fourth the in-
between article a or possessive his and the 
following infinitive of ?in attempt?. Note that as 
existing collocation reference tools retrieve and 
display collocates, they typically ignore function 
words like articles and determiners, which happen 
to be closely related to frequent errors made by the 
learners (Nicholls, 1999), and fail to provide an 
overall picture of word usages. In contrast, GRASP 
attempts to show the overall picture with 
appropriate function words and word forms. 
We selected 20 collocations and phrases 7 
manually from 100 most frequent collocations in 
                                                           
6
 http://koromiko.cs.nthu.edu.tw/grasp/ 
7
 Include the 15 test items. 
BNC whose MI values exceed 2.2 and used them 
as the target for learning between the pretest and 
posttest. To evaluate GRASP, half of the 
participants were instructed to use GRASP for 
learning and the other half used traditional tools 
such as online dictionaries or machine translation 
systems (i.e., Google Translate and Yahoo! Babel 
Fish). We summarize the performance of our 
participants on pre- and post-test in Table 1 where 
GRASP denotes the experimental group and TRAD 
the control group. 
 
 class 1 class 2 combined 
 pretest posttest  pretest  posttest  pretest posttest 
GRASP 26.4 41.9 43.6 58.4 38.9 53.9 
TRAD 27.1 32.7 43.8 53.4 39.9 48.6 
Table 1. The performance (%) on pre- and post-test. 
 
We observe in Table 1 that (1) the partition of 
the classes was quite random (the difference 
between GRASP and TRAD was insignificant 
under pretest); (2) GRASP summaries of words? 
contexts were more helpful in language learning 
(across class 1, class 2 and combined). Specifically, 
under the column of the 1st class, GRASP helped to 
boost students? achievements by 15.5%, almost 
tripled (15.5 vs. 5.6) compared to the gain using 
TRAD; (3) the effectiveness of GRASP in language 
learning do not confine to students at a certain 
level. Encouragingly, both high- and low-
achieving students benefited from GRASP if we 
think of students in class 2 and those in class 1 as 
the high and the low respectively (due to the 
performance difference on pretests). 
We have analyzed some participants? answers 
and found that GRASP helped to reduce learners? 
article and preposition errors by 28% and 8%, 
comparing to much smaller error reduction rate 7% 
and 2% observed in TRAD group. Additionally, an 
experiment where Chinese EFL students were 
asked to perform the same task but using GRASP 
as well as GRASP with translation information8 
                                                           
8
 http://koromiko.cs.nthu.edu.tw/grasp/ch 
100
was conducted. We observed that with Chinese 
translation there was an additional 5% increase in 
students? test performance. This suggests to some 
extent learners still depend on their first languages 
in learning and first-language information may 
serve as another quick navigation index even when 
English GRASP is presented. 
Overall, we are modest to say that (in the 
constrained experiments) GRASP summarized 
general-to-specific usages, contexts, or phrase-
ologies of words are quite effective in assisting 
learners in collocation and phrase learning. 
5 Applying GRASP to Error Correction 
To demonstrate the viability of GRASP-retrieved 
lexicalized grammar patterns (e.g., ?play ~ role In 
V-ING? and ?look forward to V-ING?) in error 
detection and correction, we incorporate them into 
an extended Levenshtein algorithm (1966) to 
provide broad-coverage sentence-level grammat-
ical edits (involving substitution, deletion, and 
insertion) to inappropriate word usages in learner 
text. 
Previously, a number of interesting rule-based 
error detection/correction systems have been 
proposed for some specific error types such as 
article and preposition error (e.g., (Uria et al, 
2009), (Lee et al, 2009), and some modules in 
(Gamon et al, 2009)). Statistical approaches, 
supervised or unsupervised, to grammar checking 
have become the recent trend. For example, 
unsupervised systems of (Chodorow and Leacock, 
2000) and (Tsao and Wible, 2009) leverage word 
distributions in general and/or word-specific 
corpus for detecting erroneous usages while 
(Hermet et al, 2008) and (Gamon and Leacock, 
2010) use Web as a corpus. On the other hand, 
supervised models, typically treating error 
detection/correction as a classification problem, 
utilize the training of well-formed texts ((De Felice 
and Pulman, 2008) and (Tetreault et al, 2010)), 
learner texts, or both pairwisely (Brockett et al, 
2006). Moreover, (Sun et al, 2007) describes a 
way to construct a supervised error detection 
system trained on well-formed and learner texts 
neither pairwise nor error tagged. 
In contrast to the previous work in grammar 
checking, our pattern grammar rules are 
automatically inferred from a general corpus (as 
described in Section 3) and helpful for correcting 
errors resulting from the others (e.g., ?to close? in 
?play ~ role to close?), our pattern grammar 
lexicalizes on both content and function words and 
lexical items within may be contiguous (e.g., ?look 
forward to V-ING PRP?) or non-contiguous (e.g., 
?play ~ role In V-ING?), and, with word class 
(POS) information, error correction or grammatical 
suggestion is provided at sentence level. 
5.1 Error Correcting Process 
Figure 3 shows how we check grammaticality and 
provide suggestions for a given text with accurate 
spelling. 
 
 
Figure 3. Procedure of grammar suggestion/correction. 
 
In Step (1), we initiate a set Suggestions to 
collect grammar suggestions to the user text T 
according to a bank of patterns 
PatternGrammarBank, i.e., a collection of 
summaries of grammatical usages (e.g., ?play ~ 
role In V-ING?) of queries (e.g., ?play role?) 
submitted to GRASP. Since we focus on grammar 
checking at sentence level, T is heuristically split 
(Step (2)). 
For each sentence, we extract user-proposed 
word usages (Step (3)), that is, the user 
grammatical contexts of ngram and collocation 
sequences. Take for example the (ungrammatical) 
sentences and their corresponding POS sequences 
?he/PRP play/VBP an/DT important/JJ roles/NNS 
to/TO close/VB this/DT deals/NNS? and ?he/PRP 
looks/VBZ forward/RB to/TO hear/VB you/PRP?. 
Ngram contexts include ?he VBP DT?, ?play an JJ 
NNS?, ?this NNS? for the first sentence and ?look 
forward to VB PRP? and ?look forward to hear 
PRP? for the second. And collocation contexts for 
procedure GrammarChecking(T,PatternGrammarBank) 
(1) Suggestions=??//candidate suggestions 
(2) sentences=sentenceSplitting(T) 
for each sentence in sentences 
(3)   userProposedUsages=extractUsage(sentence) 
for each userUsage in userProposedUsages 
(4)     patGram=findPatternGrammar(userUsage.lexemes, 
PatternGrammarBank) 
(5)     minEditedCost=SystemMax; minEditedSug=?? 
for each pattern in patGram 
(6)        cost=extendedLevenshtein(userUsage,pattern) 
if cost<minEditedCost 
(7)            minEditedCost=cost; minEditedSug=pattern 
if minEditedCost>0 
(8)       append (userUsage,minEditedSug) to Suggestions 
(9) Return Suggestions 
101
the first sentence are ?play ~ role to VERB? and 
?close ~ deal .? 
For each userUsage in the sentence (e.g., ?play 
~ role TO VB? and ?look forward to hear PRP?), 
we first acquire the pattern grammar of its lexemes 
(e.g., ?play role? and ?look forward to hear?) such 
as ?play ~ role in V-ing? and ?look forward to 
hear from? in Step (4), and we compare the user-
proposed usage against the corresponding 
predominant, most likely more proper, ones (from 
Step (5) to (7)). We leverage an extended 
Levenshtein?s algorithm in Figure 4 for usage 
comparison, i.e. error detection and correction, 
after setting up minEditedCost and minEditedSug 
for the minimum-cost edit from alleged error usage 
into appropriate one (Step (5)). 
 
 
Figure 4. Extended Levenshtein algorithm for correction. 
 
In Step (1) of the algorithm in Figure 4 we 
allocate and initialize costArray to gather the 
dynamic programming based cost to transform 
userUsage into a specific pattern. Afterwards, the 
algorithm defines the cost of performing 
substitution (Step (2)), deletion (Step (3)) and 
insertion (Step (4)) at i-indexed userUsage and j-
indexed pattern. If the entries userUsage[i] and 
pattern[j] are equal literally (e.g., ?VB? and ?VB?) 
or grammatically (e.g., ?DT? and ?PRP$?9), no edit 
                                                           
9
 ONE?S denotes possessives. 
is needed, hence, no cost (Step (2a)). On the other 
hand, since learners tend to select wrong word 
form and preposition, we make less the cost of the 
substitution of the same word group, say from 
?VERB? to ?V-ing?, ?TO? to ?In? and ?In? to 
?IN(on)? (Step (2b)) compared to a total edit (Step 
(2c)). In addition to the conventional deletion and 
insertion (Step (3b) and (4b) respectively), we look 
ahead to the elements userUsage[i+1] and 
pattern[j+1] considering the fact that ?with or 
without preposition? and ?transitive or intransitive 
verb? often puzzles EFL learners (Step (3a) and 
(4a)). Only a small edit cost is applied if the next 
elements in userUsage and Pattern are ?equal?. In 
Step (6) the extended Levenshtein?s algorithm 
returns the minimum cost to edit userUsage based 
on pattern. 
Once we obtain the costs to transform the 
userUsage into its related frequent patterns, we 
propose the minimum-cost one as its grammatical 
suggestion (Step (8) in Figure 3), if its minimum 
edit cost is greater than zero. Otherwise, the usage 
is considered valid. At last, the gathered 
suggestions Suggestions to T are returned to users 
(Step (9)). Example edits to the user text ?he play 
an important roles to close this deals. he looks 
forward to hear you.? from our working prototype, 
EdIt10, is shown in Figure 5. Note that we exploit 
context checking of collocations to cover longer 
span than ngrams?, and longer ngrams like 
fourgrams and fivegrams to (more or less) help 
semantic checking (or word sense disambiguation). 
For example, ?hear? may be transitive or 
intransitive, but, in the context of ?look forward 
to?, there is strong tendency it is used intransitively 
and follows by ?from?, as EdIt would suggest (see 
Figure 5). 
There are two issues worth mentioning on the 
development of EdIt. First, grammar checkers 
typically have different modules examining 
different types of errors with different priority. In 
our unified framework, we set the priority of 
checking collocations? usages higher than that of 
ngrams?, set the priority of checking longer 
ngrams? usages higher than that of shorter, and we 
do not double check. Alternatively, one may first 
check usages of all sorts and employ majority 
voting to determine the grammaticality of a 
sentence. Second, we further incorporate
                                                           
10
 http://140.114.214.80/theSite/EdIt_demo2/ 
procedure extendedLevenshtein(userUsage,pattern) 
(1) allocate and initialize costArray 
for i in range(len(userUsage)) 
for j in range(len(pattern)) 
//substitution 
if equal(userUsage[i],pattern[j]) 
(2a)       substiCost=costArray[i-1,j-1]+0 
elseif sameWordGroup(userUsage[i],pattern[j]) 
(2b)       substiCost=costArray[i-1,j-1]+0.5 
else 
(2c)       substiCost=costArray[i-1,j-1]+1 
//deletion 
if equal(userUsage[i+1],pattern[j+1]) 
(3a)       delCost=costArray[i-1,j]+smallCost 
else 
(3b)       delCost=costArray[i-1,j]+1 
//insertion 
if equal(userUsage[i+1],pattern[j+1])  
(4a)        insCost=costArray[i,j-1]+smallCost 
else 
(4b)       insCost=costArray[i,j-1]+1 
(5)       costArray[i,j]=min(substiCost,delCost,insCost) 
(6) Return costArray[len(userUsage),len(pattern)] 
102
Erroneous sentence EdIt suggestion ESL Assistant suggestion 
Wrong word form 
? a sunny days ? a sunny NN a sunny day 
every days, I ? every NN every day 
I would said to ? would VB would say 
he play a ? he VBD none 
? should have tell the truth should have VBN should have to tell 
? look forward to see you look forward to VBG none 
? in an attempt to seeing you an attempt to VB none 
? be able to solved this problem able to VB none 
Wrong preposition 
he plays an important role to close ? play ~ role IN(in) none 
he has a vital effect at her. have ~ effect IN(on) effect on her 
it has an effect on reducing ? have ~ effect IN(of) VBG none 
? depend of the scholarship depend IN(on) depend on 
Confusion between intransitive and transitive verb 
he listens the music. missing ?to? after ?listens? missing ?to? after ?listens? 
it affects to his decision. unnecessary ?to? unnecessary ?to? 
I understand about the situation. unnecessary ?about? unnecessary ?about? 
we would like to discuss about this matter. unnecessary ?about? unnecessary ?about? 
Mixture 
she play an important roles to close this deals. she VBD; an JJ NN; 
play ~ role IN(in) VBG; this NN 
play an important role; 
close this deal 
I look forward to hear you. look forward to VBG; 
missing ?from? after ?hear? 
none 
Table 2. Three common score-related error types and their examples with suggestions from EdIt and ESL Assistant. 
 
 
Figure 5. Example EdIt responses to the ungrammatical. 
 
probabilities conditioned on word positions to 
weigh edit costs. For example, the conditional 
probability of ?VERB? being the immediate 
follower of ?look forward to? is virtually zero, but 
the probability of ?V-ing? is around 0.3. 
5.2 Preliminary Results in Error Correction 
We examined three common error types in learner 
text that are highly correlated with essay scores 
(Leacock and Chodorow, 2003; Burstein et al, 
2004), to evaluate EdIt, (see Table 2). In Table 2, 
the results of a state-of-the-art checker, ESL 
Assistant (www.eslassistant.com/), are shown for 
comparison, and information produced by both 
systems are underscored. As indicated, GRASP 
retrieves patterns which are potential useful if 
incorporated into an extension of Levenshtein?s 
algorithm to correct substitution, deletion, and 
insertion errors in learner. 
6 Summary 
We have introduced a new method for producing a 
general-to-specific usage summary of the contexts 
of a linguistic search query aimed at accelerating 
learners? grasp on word usages. We have 
implemented and evaluated the method as applied 
to collocation and phrase learning and grammar 
checking. In the preliminary evaluations we show 
that GRASP is more helpful than traditional 
language learning tools, and that the patterns and 
lexical bundles provided are promising in detecting 
and correcting common types of errors in learner 
writing. 
References 
Morton Benson, Evellyn Benson, and Robert Ilson. 
1986. The BBI Combinatory Dictionary of English: A 
Article: 
Related pattern grammar 
(a) of collocation sequences includes ?play ~ role 
IN(in) NN?, ?play ~ role IN(in) DT?, ?play ~ role 
IN(in) VBG? and so on. 
(b) of ngram sequences includes ?he VBD DT?, ?play 
an JJ NN?, ?this NN?, ?look forward to VBG PRP? 
and ?look forward to hear IN(from) PRP? and so on. 
Grammatical/Usage suggestion: 
For sentence 1: 
(a) use the VBD of ?play?, (b) use the NN of ?roles?, 
(c) use the preposition ?in? and VBG of ?close?, 
instead of ?to close?. (d) use the NN of ?deals? 
For sentence 2: 
(a) insert the preposition ?from? after ?hear?, (b) use 
the ?VBG? of ?hear? 
he play an important roles to close this deals. 
he looks forward to hear you. 
Type your article and push the buttom ?EdIt? ! 
103
guide to word combinations. Philadelphia: John 
Benjamins. 
Chris Brockett, William B. Dolan, and Michael Gamon. 
2006. Correcting ESL errors using phrasal SMT 
techniques. In Proceedings of the ACL, pages 249-
256. 
Jill Burstein, Martin Chodorow, and Claudia Leacock. 
2004. Automated essay evaluation: the criterion 
online writing service. AI Magazine, 25(3): 27-36. 
Yu-Chia Chang, Jason S. Chang, Hao-Jan Chen, and 
Hsien-Chin Liou. 2008. An automatic collocation 
writing assistant for Taiwanese EFL learners: a case 
of corpus-based NLP technology. CALL, 21(3): 283-
299. 
Winnie Cheng, Chris Greaves, and Martin Warren. 2006. 
From n-gram to skipgram to concgram. Corpus 
Linguistics, 11(4): 411-433. 
Martin Chodorow and Claudia Leacock. 2000. An 
unsupervised method for detecting grammatical 
errors. In Proceedings of the NAACL, pages 140-147. 
Rachele De Felice and Stephen G. Pulman. 2008. A 
classifer-based approach to preposition and 
determiner error correction in L2 English. In 
Proceedings of the COLING, pages 169-176. 
Philip Durrant. 2009. Investigating the viability of a 
collocation list for students of English for academic 
purposes. ESP, 28(3): 157-169. 
John R. Firth. 1957. Modes of meaning. In Papers in 
Linguistics. London: Oxford University Press, pages 
190-215. 
Michael Gamon, Claudia Leacock, Chris Brockett, 
William B. Dolan., Jianfeng Gao, Dmitriy Belenko, 
and Alexandre Klementiev. 2009. Using statistical 
techniques and web search to correct ESL errors. 
CALICO, 26(3): 491-511. 
Michael Gamon and Claudia Leacock. 2010. Search 
right and thou shalt find ? using web queries for 
learner error detection. In Proceedings of the NAACL. 
Matthieu Hermet, Alain Desilets, and Stan Szpakowicz. 
2008. Using the web as a linguistic resource to 
automatically correct lexico-syntatic errors. In 
Proceedings of the LREC, pages 874-878. 
Susan Hunston and Gill Francis. 2000. Pattern 
Grammar: A Corpus-Driven Approach to the Lexical 
Grammar of English. Amsterdam: John Benjamins. 
Jia-Yan Jian, Yu-Chia Chang, and Jason S. Chang. 2004. 
TANGO: Bilingual collocational concordancer. In 
ACL Poster. 
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David 
Tugwell. 2004. The sketch engine. In Proceedings of 
the EURALEX, pages 105-116. 
Chong Min Lee, Soojeong Eom, and Markus Dickinson. 
2009. Toward analyzing Korean learner particles. In 
CALICO Workshop. 
Claudia Leacock and Martin Chodorow. 2003. 
Automated grammatical error detection. In M.D. 
Shermis and J.C. Burstein, editors, Automated Essay 
Scoring: A Cross-Disciplinary Perspective, pages 
195-207. 
Vladimir I. Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. Soviet 
Physics Doklady, 10, page 707. 
Diane Nicholls. 1999. The Cambridge Learner Corpus ? 
error coding and analysis for writing dictionaries and 
other books for English Learners. 
John M. Sinclair. 1987. The nature of the evidence. In J. 
Sinclair (ed.) Looking Up. Collins: 150-159. 
Frank Smadja. 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1): 143-177. 
Michael Stubbs. 2004. At 
http://web.archive.org/web/20070828004603/http://www.u
ni-trier.de/uni/fb2/anglistik/Projekte/stubbs/icame-2004.htm. 
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, 
Zhongyang Xiong, John Lee, and Chin-Yew Lin. 
2007. Detecting erroneous sentences using 
automatically mined sequential patterns. In 
Proceedings of the ACL, pages 81-88. 
Joel Tetreault, Jennifer Foster, and Martin Chodorow. 
2010. Using parse features for prepositions selection 
and error detection. In Proceedings of the ACL, pages 
353-358. 
Nai-Lung Tsao and David Wible. 2009. A method for 
unsupervised broad-coverage lexical error detection 
and correction. In NAACL Workshop, pages 51-54. 
Larraitz Uria, Bertol Arrieta, Arantza D. De Ilarraza, 
Montse Maritxalar, and Maite Oronoz. 2009. 
Determiner errors in Basque: analysis and automatic 
detection. Procesamiento del Lenguaje Natural, 
pages 41-48. 
David Wible and Nai-Lung Tsao. 2010. StringNet as a 
computational resource for discovering and 
investigating linguistic constructions. In NAACL 
Workshop, pages 25-31. 
David Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the ACL, pages 189-196. 
104
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 80?85,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
 
 
PREFER: Using a Graph-Based Approach to Generate Paraphrases for 
Language Learning 
  
Mei-Hua Chen*, Shih-Ting Huang+, Chung-Chi Huang*, Hsien-Chin Liou**, Jason S. Chang+ 
  * Institute of Information Systems and Applications 
+ Department of Computer Science 
**  Department of Foreign Languages and Literature 
National Tsing Hua University  
HsinChu, Taiwan, R.O.C. 30013 
{chen.meihua,koromiko1104,u901571,hsienchin,jason.jschang}@gmail.com 
  
  
Abstract 
Paraphrasing is an important aspect of language 
competence; however, EFL learners have long 
had difficulty paraphrasing in their writing 
owing to their limited language proficiency. 
Therefore, automatic paraphrase suggestion 
systems can be useful for writers. In this paper, 
we present PREFER1, a paraphrase reference 
tool for helping language learners improve their 
writing skills. In this paper, we attempt to 
transform the paraphrase generation problem 
into a graphical problem in which the phrases 
are treated as nodes and translation similarities 
as edges. We adopt the PageRank algorithm to 
rank and filter the paraphrases generated by the 
pivot-based paraphrase generation method. We 
manually evaluate the performance of our 
method and assess the effectiveness of 
PREFER in language learning. The results 
show that our method successfully preserves 
both the semantic meaning and syntactic 
structure of the query phrase. Moreover, the 
students? writing performance improve most 
with the assistance of PREFER.  
1. Introduction 
Paraphrasing, or restating information using 
different words, is an essential part of productive 
language competence (Fuchs, 1980; Mel??uk, 1992; 
Martinot, 2003). However, EFL learners have 
difficulty paraphrasing in their writing partly 
                                                 
1 http://140.114.89.231/PREFER 
because of their insufficient lexical knowledge 
(Abasi et al 2006; Chandrasoma et al 2004). If 
they are provided with direct and substantial 
support while writing, they may be able to express 
their thoughts more fluently. Unfortunately, few 
paraphrase reference tools have been developed to 
provide instant assistance to learners in their 
writing process. In the light of the pressing need 
for paraphrase reference tools, we develop 
PREFER, a paraphrasing assistant system to help 
EFL learners vary their expression during writing.  
Over the past decade, paraphrasing techniques 
have played an important role in many areas of 
Natural Language Processing, such as machine 
translation, and question answering. However, very 
few studies have been conducted concerning the 
application of automatic paraphrase generation 
techniques in language learning and teaching.  
In this paper, we treat the paraphrase generation 
problem as a graph-related problem. We adopt the 
PageRank algorithm (Page et al, 1999) to generate 
paraphrases based on the assumption that a page 
with more incoming links is likely to receive a 
higher rank. Meanwhile, a page which is linked by 
a higher ranked page should transitively be ranked 
higher. We take advantage of transitivity of 
relevance to rank and filter the paraphrases 
generated by the pivot-based method (i.e., phrase 
are treated as paraphrases if they share the same 
translations) of Bannard and Callison-Burch 
(2005).  
The advantage of the pivot approach is that the 
generated paraphrases are exactly semantically 
equivalent to the query phrase. However, its 
80
  
quality of the paraphrases highly correlates with 
that of the techniques of bilingual alignment. To 
overcome such limitation, we use the PageRank 
algorithm to refine the generated paraphrases. In 
other words, we leverage the PageRank algorithm 
to find more relevant paraphrases that preserve 
both meaning and grammaticality for language 
learners. The results of a manual evaluation and a 
system assessment show that our approach and 
system perform well. 
2. Related Work 
A number of studies have investigated EFL leaners? 
paraphrase competence. For example, Campbell 
(1987) reveals that language proficiency 
significantly affects paraphrasing competence. 
McInnis (2009) reports that paraphrasing task is 
more difficult for L2 students than that for L1 
students. According to Milicevic (2011), L2 
learners propose less valid paraphrases than native 
speakers. These findings indicate that EFL students 
have problems in paraphrasing. In view of this, we 
develop PREFER, a paraphrase reference tool, for 
helping English learners with their writing. 
Paraphrase generation, on the other hand, has 
been an area of active research and the related 
work has been thoroughly surveyed in 
Androutsopoulos and Malakasiotis (2010) as well 
as in Madnani and Dorr (2010). In the rest of this 
section, we focus on reviewing the methods related 
to our work.  
One prominent approach to paraphrase 
generation is based on bilingual parallel corpora. 
For example, Bannard and Callison-Burch (2005) 
propose the pivot approach to generate phrasal 
paraphrases from an English-German parallel 
corpus. With the advantage of its parallel and 
bilingual natures of such a corpus, the output 
paraphrases do preserve semantic similarity. 
Callison-Burch (2008) further places syntactic 
constraints on generated paraphrases to improve 
the quality of the paraphrases. In this paper, we 
generate paraphrases adopting the pivot-based 
method proposed by Bannard and Callison-Burch 
(2005) in the first round. Then we use a 
graph-based approach to further ensure paraphrase 
candidates preserve both meaning and 
grammaticality. 
In a study more closely related to our work, 
Kok and Brockett (2010) take a graphical view of 
the pivot-based approach. They propose the Hitting 
Time Paraphrase algorithm (HTP) to measure 
similarities between phrases. The smaller the 
number of steps a random walker goes from one 
node to the other, the more likely these two nodes 
are paraphrases. The main difference between their 
work and ours lies in the definition of the graph. 
While they treat multilingual phrases as nodes, we 
treat only English phrases as nodes. Besides, we 
define the edges between nodes as semantic 
relation instead of bilingual alignment. 
In contrast to the previous work, we present a 
graph-based method for refining the paraphrases 
generated by the pivoting approach. Our goal is to 
consolidate the relation between paraphrases to 
provide learners with more and better paraphrases 
which are helpful in expanding their lexical 
knowledge. 
3. Graph-Based Paraphrase Generation 
In this section, we describe how we use the 
PageRank algorithm to rank and filter the 
paraphrases generated by the pivot-based method. 
3.1 Graph Construction 
We first exploit the pivot-based method proposed 
by Bannard and Callison-Burch (2005) to populate 
our graph G using of candidate paraphrases 
cP={             } from a bilingual parallel 
corpus B for a query phrase q. Each phrase in cP is 
also represented as a node in G. Note that the 
query phrase q is excluded from cP.  
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  A simple graph G. Note that the cp1 and 
   
  
 will be linked iff    
  
 is the paraphrase of q 
and is also the paraphrase of      
 
q 
cP 
        
 
   
    
      
81
  
Graph G only contains the paraphrases cpi 
whose probabilities are higher than a certain 
threshold ?2  as nodes. In addition, each cpi is 
linked to the query phrase q with edge e which is 
weighted by the probability  (   | ). Furthermore, 
we establish the edges among the phrases in cP. 
An example graph is shown in Figure 1. By 
repeating the previous steps, for each phrase cp1, 
cp2,... in cP, we find their corresponding 
paraphrases,         
 
  
        and 
         
 
  
       ?., and discard the 
paraphrases that are not in cP. Once the phrases are 
linked with their paraphrases, the graph G is 
created.  
In this paper, we also place a constraint that a 
paraphrase of a phrase q must neither be a 
substring nor a superstring of q. These strings are 
usually aligned with the same foreign language 
phrase while they are not paraphrases at all. For 
example, ?play an important? and ?play an 
important role in? are excluded for ?play an 
important role?. This has the effect of reducing 
some of the noise generated by the pivot-based 
method.   
3.2 Graph-Based Paraphrase Generation 
We then refine the generated paraphrases adopting 
the PageRank algorithm proposed by Page et al 
(1999). Consider a graph consisting of a set of 
webpages on the Web V and a set of hyperlinks E. 
The PageRank algorithm assigns a value PR to 
each webpage as their importance measurement. 
The PR value of a certain page u is defined 
iteratively as the following equation: 
  ( )   ?
  ( )
 ( )
                   ( )
    
 
where Bu is a set of pages linked to u and L(.) 
denotes the number of outbound links from a page 
v.  
Intuitively, by using formula (1) iteratively, we 
are able to calculate the PR values for all nodes 
and thus extract relatively important paraphrases. 
However, the original PageRank algorithm does 
not take the weight of each edge into consideration. 
That is, the PageRank algorithm treats all links 
equally when distributing rank scores. Treating all 
links equally in paraphrase generation task might 
                                                 
2 We set ? to be 0.01. 
lose some linguistic properties. For this, we 
consider the importance of edges of the nodes and 
weight the edges based on the paraphrase 
probability in the pivot-based approach using 
 (      )  ? ( |  ) ( |  )
 
      ( )  
Formula (2) represents the probability that the 
phrase u is the paraphrase of the phrase v. f refers 
to shared translations of v and u. Then for each 
iteration of the PageRank calculation, we reassign 
the PR value for all u in V to be PR?(u) as:  
   ( )   ?
 (   )  ( )
 ( )
           ( )
    
 
Instead of treating all edges equally, formula (3) 
integrates the weights of inbound link and 
outbound link edges (see Section 4 for the 
performance differences with and without 
weighting edges). 
4. Results 
In this section, we first present our experimental 
setting. Then evaluation results are reported. 
4.1 Experimental Setting 
In this paper, word alignments were produced by 
Giza++ toolkit (Och and Ney, 2003) over a set of 
Danish-English section (containing 1,236,427 
sentences) of the Europarl corpus, version 2 
(Koehn, 2002).  
We compared our graph-based approach with a 
strong baseline, the pivot-based method with 
syntactic constraint (SBP) (Callison-Burch, 2008) 
utilizing the same Danish-English corpus. We also 
investigate the contribution of adding the edge 
weights to the PageRank algorithm by building 
two models, PR representing the method of the 
PageRank algorithm without weights and PRw 
representing the method of the weighted PageRank 
algorithm, for comparison.   
To assess the performance of our method, we 
conducted a manual evaluation. We asked an 
experienced English lecturer to randomly select 
100 most commonly used and meaningful phrases 
from 30 research articles in the discipline of 
Computer-Assisted Language Learning (CALL). A 
total of 88 unique phrases were used as our test set 
for evaluation excluding 12 phrases not existing in 
the Europarl corpus. For each phrase, we extracted 
82
  
the corresponding candidate paraphrases and chose 
top 5 for evaluation. Two raters, provided with a 
simplified scoring standard used by Callison-Burch 
(2008), manually evaluate the accuracy of the top 
ranked paraphrases of each phrase by score 0, 1 
and 2. It is worth noting that the raters were asked 
to score each paraphrase candidate by considering 
its appropriateness in various contexts. In this 
evaluation, we strictly deemed a paraphrase to be 
correct if and only if both raters scored 2.  The 
inter-annotator agreement was 0.63.   
The coverage was measured by the number of 
correct answers within top 5 candidates. The 
precision was measured by the number of correct 
answers within the returned answers. 
On the other hand, to assess the effectiveness of 
PREFER in language learning, we carried out an 
experiment with 55 Chinese-speaking EFL college 
freshmen, who had at least six years of formal 
instruction from junior to senior high schools and 
were estimated to be at the intermediate level 
regarding their overall English competence. The 
students were randomly divided into three groups. 
They were asked to paraphrase seven short 
paragraphs in the pre-test with no system support, 
and then paraphrase another seven short 
paragraphs in the post-test using three different 
tools: PREFER (P), LONGMAN Dictionary of 
Contemporary English Online (L), and 
Thesaurus.com (T). A total of 22 default phrases 
(http://140.114.75.22/share/examples.htm) were 
embedded in the paragraphs in the pre- and 
post-tests, targeted at comparing the quality and 
quantity of students? paraphrasing performance. 
Students were not restricted to paraphrase these 
embedded phrases. Instead, they were encouraged 
to replace any possible phrases or even restructure 
sentences. We had two experienced native-speaker 
TESL (Teaching English as a Second Language) 
lecturers to score the students? paraphrasing 
performance. 
4.2 Experimental Results 
4.2.1 Manual Evaluation 
As shown in Table 1, PRw achieved both good 
precision and coverage. Moreover, PR and PRw 
performed better than SBP in both coverage and 
precision. Also, the result that the performance of 
PRw is better than that of PR implies that PRw is 
able to generate more semantically and 
syntactically correct paraphrases. However, the 
precision of 0.19 indicates that there is still room to 
improve the paraphrase generation model.  
 
 
 
  PR PRw SBP 
Coverage 0.17 0.18 0.07 
Precision 0.17 0.19 0.10 
Table 1: The measurement of paraphrases. 
 
Additionally, Mean Reciprocal Rank (MRR) is 
also reported. Here, MRR is defined as a measure 
of how much effort needed to locate the first 
appropriate paraphrase for the given phrase in the 
ranked list of paraphrases. The MRR score of PRw 
(0.53) outperformed PR (0.51) and SBP (0.47). It 
demonstrated that the PRw model facilitates the 
high ranking of good paraphrases (i.e., paraphrases 
with meaning and grammaticality preserved would 
be ranked high).  
4.2.2. Evaluation on Language Learning  
The second evaluation is to assess the effectiveness 
of PREFER applied to CALL. We used a 
comparison method to measure the extent to which 
EFL learners achieved good performance in 
paraphrasing.  
 
Table 2. Comparison of paraphrasing performance 
among students using three different reference tools.  
 
As seen in the first row of Table 2, the students? 
writing performance improved most with the 
assistance of PREFER (i.e., group P), compared 
with group L and group T. We further analyzed 
and compared the number of the rephrased phrases 
and the correct paraphrases, and the rate of 
    P L T 
improvement of paraphrasing 
task 
38.2% -31.6% -6.2% 
all 
paraphrasable 
phrases 
rephrased 38.4% -23.2% 9.5% 
correct 53.3% -17.5% 4.6% 
correctness rate  7.9% 4.9% -3.1% 
22 default 
phrases 
rephrased 68% -16% 28% 
correct 100% -5% 31% 
correctness rate 13.6% 7.9% 1.5% 
83
  
correctness students achieved using different 
reference tools among our testing paraphrase 
candidates (see the middle and bottom panels of 
Table 2). Obviously, the students consulting 
PREFER achieved substantial paraphrasing 
improvement in all three aspects of both all and 
default phrases. But the other two groups seemed 
unable to manage well the paraphrasing task with 
traditional way of phrase information. This limited 
information seems insufficient to enable students 
to familiarize themselves with proper usages of 
phrases which might lead to improper 
paraphrasing. 
In short, PREFER outperformed the other two 
reference tools in assisting EFL learners in their 
paraphrasing task. 
5. Conclusion and Future Work   
In this paper, we treat the paraphrase generation 
problem as a graphical problem. We utilize the 
PageRank algorithm to rank and filter the 
paraphrases generated using the pivot-based 
method. The results show that our method 
significantly produces better paraphrases in both 
precision and coverage compared with the 
syntactically-constrained pivot method of 
Callison-Burch (2008). Additionally, PREFER 
does benefit learners? writing performance. 
 In order to conduct a more comprehensive 
evaluation, we plan to adapt the in-context 
evaluation metric introduced by Callison-Burch et. 
al (2008). A larger test set would be generated 
manually to evaluate the performance of our 
paraphrase system. In addition, we will implement 
various kinds of baseline systems such as Kok and 
Brockett (2010) and Chan et al (2011) to provide a 
more competitive comparison. 
Many avenues exist for future research and 
improvement. For example, we would like to 
extend paraphrasing consecutive n-gram phrases to 
inconsecutive ones such as ones with incomplete 
transitive verbs (e.g., ?provide someone with 
something?). Besides, we are interested in 
weighting edges using syntactic and semantic 
relation in our graph-based method to further 
improve the quality of generated paraphrases. 
 
 
 
References  
Ali R. Abasi, Nahal Akbari, and Barbara Graves. 2006. 
Discourse appropriation, construction of identities, 
and the complex issue of plagiarism: ESL students 
writing in graduate school. Journal of Second 
Language Writing, 15, 102?117. 
Ion Androutsopoulos and Prodromos Malakasiotis. 2010. 
A Survey of Paraphrasing and Textual Entailment 
Methods. Journal of Artificial Intelligence Research 
38, 135?187. 
Colin Bannard and Chris Callison-Burch. 2005. 
Paraphrasing with bilingual parallel corpora. In 
Proceedings of ACL, pp. 597-604. 
Chris Callison-Burch. 2008. Syntactic constraints on 
paraphrases extracted from parallel corpora. In 
Proceedings of EMNLP, pp. 196?205. 
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata. 
2008. ParaMetric: an automatic evaluation metric for 
paraphrasing. In Proceedings of the 22nd 
International Conference on Computational 
Linguistics, pp. 97-104. 
Cherry Campbell. 1990. Writing with others? words: 
Using background reading text in academic 
compositions. In B. Kroll (Ed.), Second language 
writing: Research insights for the classroom (pp. 
211-30). Cambridge, UK: Cambridge University 
Press. 
Tsz Ping Chan, Chris Callison-Burch and Benjamin Van 
Durme. 2011. Reranking Bilingually Extracted 
Paraphrases Using Monolingual Distributional 
Similarity. In Proceedings of the GEMS 2011 
Workshop on GEometrical Models of Natural 
Language Semantics, pp. 33-42.  
Ranamukalage Chandrasoma, Celia Thompson, and 
Alastair Pennycook. 2004. Beyond plagiarism: 
Transgressive and nontransgressive intertextuality. 
Journal of Language, Identity, and Education, 3(3), 
171?194. 
Catherine Fuchs. 1980. Paraphrase et th?orie du 
langage. Contribition ? une histoire des theories 
linguistiques contemporaines et ? la construction 
d?une th?orie ?nonciative de la paraphraase. Th?se 
de doctorat. Paris: Universit? Paris VII. 
Philipp Koehn. 2002. Europarl: A multilingual corpus 
for evaluation of machine translation. Unpublished, 
http://www.isi.edu/~koehn/europarl/. 
Stanley Kok and Chris Brockett. 2010. Hitting the right 
paraphrases in good time. In Proceedings of 
NAACL/HLT, pp. 145-153. 
84
  
Nitin Madnani and Bonnie J. Dorr. 2010. Generating 
phrasal and sentential paraphrases: A survey of 
data-driven methods. Computational Linguistics, 
36(3):341?388. 
Claire Martinot. 2003. Pour une linguistique de 
l?acquisition. La reformulation: du concept descriptif 
au concept explicatif. Langage et Soci?t?, 2(104); 
147-151. 
Lara McInnis. 2009. Analyzing English L1 and L2 
Paraphrasing Strategies through Concurrent Verbal 
Report and Stimulated Recall Protocols. MA Thesis. 
Toronto: University of Toronto. 
Igor Mel??uk. 1992. Paraphrase et lexique: la th?orie 
Sens-Texte et le Dictionnaire explicatif et 
combinatoire. In: Mel??uk, I. et al, Dictionnaire 
explicatif et combinatoire du fran?ais contemporain. 
Recherches lexico-s?mantiques III. Montr?al: Presses 
de l?Universit? de Montr?al; 9-59. 
Jasmina Mili?evi? and Alexandra Tsedryk. 2011. 
Assessing and Improving Paraphrasing Competence 
in FSL. In Proceedings of the 5th International 
Conference on Meaning- Text Theory, pp. 175-184. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): 19-51. 
Lawrence Page, Sergey Brin, Rajeev Motwani, Terry 
Winograd. 1999. The PageRank Citation Ranking: 
Bringing Order to the Web. Technical Report. pp. 
1999-66, Stanford University InfoLab. 
 
85
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 295?301,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
 
 
Helping Our Own: NTHU NLPLAB System Description 
  
Jian-Cheng Wu+, Joseph Z. Chang*, Yi-Chun Chen+, Shih-Ting Huang+, Mei-Hua Chen*, 
Jason S. Chang+ 
  * Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C. 30013 
+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C. 30013 
{wujc86, bizkit.tw, pieyaaa, koromiko1104, chen.meihua, 
jason.jschang}@gmail.com 
  
  
Abstract 
Grammatical error correction has been an active 
research area in the field of Natural Language 
Processing. In this paper, we integrated four 
distinct learning-based modules to correct 
determiner and preposition errors in leaners? 
writing. Each module focuses on a particular 
type of error. Our modules were tested in 
well-formed data and learners? writing. The 
results show that our system achieves high 
recall while preserves satisfactory precision. 
1. Introduction 
Researchers have demonstrated that prepositions 
and determiners are the two most frequent error 
types for language learners (Leacock et al 2010). 
According to Swan and Smith (2001), preposition 
errors might result from L1 interference. Chen and 
Lin (2011) also reveal that prepositions are the 
most perplexing problem for Chinese-speaking 
EFL learners mainly because there are no clear 
preposition counterparts in Chinese for learners to 
refer to. On the other hand, Swan and Smith (2001) 
predict that the possibility of determiner errors 
depends on learners? native language. The 
Cambridge Learners Corpus illustrates that 
learners of Chinese, Japanese, Korean, and Russian 
might have a poor command of determiners.  
In view of the fact that a large number of 
grammatical errors appear in non-native speakers? 
writing, more and more research has been directed 
towards the automated detection and correction of 
such errors to help improve the quality of that 
writing (Dale and Kilgarriff, 2010). In recent years, 
preposition error detection and correction has 
especially been an area of increasingly active 
research (Leacock et al 2010). The HOO 2012 
shared task also focuses on error detection and 
correction in the use of prepositions and 
determiners (Dale et al, 2012).  
Many studies have been done at correcting 
errors using hybrid modules: implementing distinct 
modules to correct errors of different types. In 
other word, instead of using a general module to 
correct any kind of errors, using different modules 
to deal with different error types seems to be more 
effective and promising. In this paper, we propose 
four distinct modules to deal with four kinds of 
determiner and preposition errors (inserting 
missing determiner, replacing erroneous 
determiner, inserting missing preposition, and 
replacing erroneous prepositions). Four 
learning-based approaches are used to detect and 
correct the errors of prepositions and determiners.   
In this paper, we describe our methods in the 
next section. Section 3 reports the evaluation 
results. Then we conclude this paper in Section 4.  
2. System Description 
2.1 Overview 
In this sub-section, we give a general view of our 
system. Figure 1 shows the architecture of the 
integrated error detection and error correction 
system. The input of the system is a sentence in a 
learner?s writing. First, the data is pre-processed 
using the GeniaTagger tool (Tsuruoka et al, 2005), 
which provides the base forms, part-of-speech tags, 
chunk tags and named entity tags. The tag result of 
295
  
the sample sentence ?This virus affects the defense 
system.? is shown in Table 1. The determiner error 
detection module then directly inserts the missing 
determiners and deletes the unnecessary 
determiners. Meanwhile, the error determiners are 
replaced with predicted answers by the determiner 
error correction module. After finishing the 
determiner error correction, the preposition error 
detection and correction module detects and 
corrects the preposition errors of the modified 
input sentence.  
In the following subsections, we first introduce 
the training and testing of the determiner error 
detection and correction modules (Section 3.2). 
Then in section 3.3 we focus on the training and 
testing of the preposition error detection and 
correction modules. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. System Architecture (Run-Time) 
 
 
Word Base form POS Chunk NE 
This This DT B-NP O 
virus virus NN I-NP O 
affects affect VBZ B-VP O 
the the DT B-NP O 
defence defence NN I-NP O 
system system NN I-NP O 
. . . O O 
Table 1. The tag result of sample sentence. 
2.2 Determiners 
In this section, we investigate the performance of 
two maximum entropy classifiers (Ratnaparkhi, 
1997), one for determining whether a noun phrase 
has a determiner or not and the other for selecting 
the appropriate determiner if one is needed.  
 From the British National Corpus (BNC), we 
extract 22,552,979 noun phrases (NPs). For 
determining which features are useful for this task, 
all NPs are divided into two sets, 20 million cases 
as a training set and the others as a validation set.  
For the classifier (named the DetClassifier 
hereafter) trained for predicting whether a NP has a 
determiner or not, the label set contains two labels: 
?Zero? and ?DET.? On the other hand, for the 
classifier (named the SelClassifier hereafter) which 
predicts appropriate determiners, the label set 
contains 9 labels: the, a, an, my, your, our, one, 
this, their. (In the training data, there are 7,249,218 
cases with those labels.) 
Both of the classifiers use contextual and 
syntactic information as features to predict the 
labels. The features include single features such as 
the headword of the NP, the part of speech (PoS) 
of the headword, the words and  PoSs in the 
chunks before or after the NP (pre-NP, post-NP), 
and all words and PoSs in the NP (excluding the 
determiner if there was one), etc. We also combine 
the single features to form more specific features 
for better performance. 
At run time, the given data are also tagged and 
all features for each NP in the data are extracted 
for classification. For testing, all determiners at the 
beginning of the NPs are ignored if they exist. At 
first, the DetClassifier is used to determine 
whether a NP needs a determiner or not. If the 
classifier predicts that the NP should not have a 
determiner but it does, there is an ?UD? 
(Unnecessary determiner) type mistake. In contrast, 
Preposition Error 
Choice 
Determiner Error 
Detection 
Determiner 
Choice 
Preposition Error 
Detection 
Input 
sentence 
Tagger & Parser 
Determiner 
Preposition 
Output 
296
  
if the classifier predicts that the NP should have a 
determiner but it does not, there is a ?MD? type 
mistake. For both ?MD? (Missing determiner) and 
?RD? (Replace determiner) mistake types, we 
would use the SelClassifier to predict which 
determiner is more appropriate for the given NP.  
2.3 Prepositions 
2.3.1 Preposition Error Detection 
In solving other problems in natural language 
processing, supervised training methods suffers 
from the difficulty of acquiring manually labeled 
data. This may not be the case with grammatical 
language error correction. Although high quality 
error learner?s corpora are not currently available 
to the public to provide negative cases, any 
ordinary corpus can used as positive cases at 
training time. 
In our method, we use an ordinary corpus to 
train a Conditional Random Field (CRF) tagger to 
identify the presence of a targeted lexical category. 
The input of the tagger is a sentence with all words 
in the targeting lexical category removed. The 
tagger will tag every word with a positive or 
negative tag, predicting the presence of a word in 
the targeted lexical category. In this paper, we 
choose the top 13 most frequent prepositions: of, to, 
in, for, on, with, as, at, by, from, about, like, since. 
Conditional Random Field 
The sequence labeling is the task of assigning 
labels from a finite set of categories sequentially to 
a set of observation sequences. This problem is 
encountered not only in the field of computational 
linguistics, but also many others, including 
bioinformatics, speech recognition, and pattern 
recognition. 
Traditionally sequence labeling problems are 
solved using the Hidden Markov Model (HMM). 
HMM is a directed graph model in which every 
outcome is conditioned on the corresponding 
observation node and only the previous outcomes. 
Conditional Random Field (CRF) is considered 
the state-of-the-art sequence labeling algorithm. 
One of the major differences of CRF is that it is 
modeled as a undirected graph. CRF also obeys the 
Markov property, with respect to the undirected 
graph, every outcome is conditioned on its 
neighboring outcomes and potentially the entire 
observation sequence. 
 
 
Figure 2. Simplified view of HMM and CRF 
 
Supervised Training 
Obtaining labeled training data is relatively easy 
for this task, that is, it requires no human labeler. 
For this task, we will use this method to target the 
lexical category preposition. To produce training 
data, we simply use an ordinary English corpus 
and use the presence of prepositions as the 
outcome, and remove all prepositions. For example, 
the sentence  
 
?Miss Hardbroom ?s eyes bored into Mildred 
like    a    laser-beam    the    moment    
they    came into view .? 
 
will produce  
 
?Miss _Hardbroom _?s _eyes _bored +Mildred 
_like _a _laser-beam _the _moment _they 
_came  +view .?  
 
where the underscores indicate no preposition 
presence and the plus signs indicate otherwise. 
Combined with additional features described in 
following sections, we use the CRF model to train 
a preposition presence detection tagger. Features 
additional to the words in the sentence are their 
corresponding lemmas, part-of-speech tags, upper 
or lower case, and word suffix. 
At runtime, we first remove all prepositional 
words in the user input sentence, generate 
additional features, and use the trained tagger to 
predict the presence of prepositions in the altered 
sentence. By comparing the tagged result with the 
original sentence, the system can output insertion 
and/or deletion of preposition suggestions. 
The process of generating features is identical to 
producing the training set. To generate 
297
  
part-of-speech tag features at runtime, one simple 
approach is to use an ordinary POS tagger to 
generate POS tags to the tokens in the altered 
sentences, i.e. English sentences without any 
prepositions. A more sophisticated approach is to 
train a specialized POS tagger to tag English 
sentences with their prepositions removed. A 
state-of-the-art part-of-speech tagger can achieve 
around 95% precision. In our implementation, we 
find that using an ordinary POS tagger to tag 
altered sentences yield near 94% precision, 
whereas a specialized POS tagger performed 
around 1% higher precision. 
We used a small portion of the British National 
Corpus (BNC) to train and evaluate our tagger (1M 
and 10M tokens, i.e. words and punctuation marks). 
The British National Corpus contains over 100 
million words of both written (90%) and spoken 
(10%) British English. The written part of the BNC 
is sampled from a wide variety of sources, 
including newspapers, journals, academic books, 
fictions, letter, school and university essays. A 
separate portion of the BNC is selected to evaluate 
the performance of the taggers. The test set 
contains 322,997 tokens (31,916 sentences). 
 
2.3.2 Preposition Error Correction 
Recently, the problem of preposition error 
correction has been viewed as a word sense 
disambiguation problem and all prepositions are 
considered as candidates of the intended senses. In 
previous studies, well-formed corpora and learner 
corpora are both used in training the classifiers. 
However, due to the limited size of learner corpora, 
it is difficult to use the learner corpora to train a 
classifier. A more feasible approach is to use a 
large well-formed corpus to train a model in 
choosing prepositions. Similar to the determiner 
error correction, we choose the maximum entropy 
model as our classifier to choose appropriate 
prepositions underlying certain contexts. In order 
to cover a large variety of genres in learners? 
writing, we use a balanced well-formed corpus, the 
BNC, to train a maximum entropy model.  
Our context features include four feature 
categories which are introduced as follows.  
? Word feature (f1): Word features include a 
window of five content words to the left and 
right with their positions. 
? Head feature (f2): We select two head words 
in the left and right of prepositions with their 
relative orders as head features. For example, 
in Table 2, we select the first head word, face, 
with its relative order, Rh1, as one of the 
head features of preposition, to. More 
specifically, ?Rh1=face? denotes first head 
word, face, right of the preposition, to. 
? Head combine feature (f3): Combine any 
two head features described above to get six 
features. For example, L1R2 denotes two 
head words surrounding the preposition. 
? Phrase combine feature (f4): Combine the 
head words of noun phrase and verb phrase 
where the preposition is between the phrases. 
For example, V_N feature denotes the head 
words of verb phrase and noun phrase where 
the preposition is followed by noun phrase 
and is preceded by verb phrase. 
   
 
Word Feature 
(f1) 
Lw1=leaving, Rw1=face,  
Rw2= chronic, Rw3= condition 
Head Feature 
(f2) 
Lh1=them, Lh2=leaving, 
Rh1=face, Rh2=condition 
Head Combine 
Feature (f3) 
L1L2= them_leaving,  
L1R1= them_face,  
L1R2= them_condition, ? 
Phrase Combine 
Feature (f4) 
N_N= them_condition,  
V_N= leaving_condition,  
N_V= them_face,  
V_V= leaving_face 
Table 2. Features example for leaving them to face this 
chronic condition 
At run time, we extract the features of each 
preposition in learners? writings and ask the model 
to predict the preposition. The preposition error 
detection model described in section 2.3.1 first 
removes all prepositions from test sentences and 
then marks the ?presence? and ?absence? labels in 
every blank of a sentence. For each blank labeled 
?presence?, the correction model predicts the 
preposition which best fits the blank underlying the 
contexts. The correction model does not predict 
when the blanks are labeled ?absence?. Although 
some blanks labeled ?absence? may still 
correspond to prepositions, we decide to reduce 
some recall score to ensure the accuracy of the 
results. 
298
  
3. Experimental Results 
In this section, we present the experimental results 
of the determiner and preposition modules 
respectively.   
3.1 Determiners 
Table 3 shows the performance of the 
DetClassifier of individual feature and Table 4 
shows the performance of the SelClassifier. We 
also wonder how the size of training data 
influences the performance of the models. Table 5 
and 6 show the precision of modes of different 
sizes of training data with the best feature ?whole 
words in NP and last word of pre-NP.? Because the 
performance converges while using more than 5 
million training cases, we use only 1 million 
training cases to investigate the performance of 
using multiple features.  When using all features, 
the precision increases from 84.8% to 85.8% for 
DetClassifier, and from 39.8% to 56.0% for 
SelClassifier. 
We also implement another data-driven model 
for determiner selection (including zero) by using 
the 5gram of Web 1T corpus. The basic concept of 
the model is to use the frequency of determiners 
which fit the context of the given test data to 
choose the determiner candidates. If the frequency 
of the determiner using in the given NP is lower 
than other candidate determiners, we would use the 
most frequent one as the suggestion. However, 
according to our observation during testing, we 
find that the model tends to cause false alarms. To 
reduce the probability of false alarm, we set a high 
threshold for the ratio f1/f2 where f1 is the frequency 
of the used determiner and f2 is the frequency of 
the most frequent determiner. The suggestion is 
accepted only when the ratio exceeds the threshold.  
The major limitation of the proposed method is 
that some errors are ignored due to parsing errors. 
For example, the given data ?the them? should be 
considered as one NP with the ?UD? type error. 
However, the parser would give the chunk result 
?the [B-NP] them [B-NP]? and the error would not 
be recognized. It might need some rules to handle 
these exceptions. Another weakness of the 
proposed methods is that the less frequently used 
determiners are usually considered as errors and 
suggested to be replaced with more frequently used 
ones. For example, possessives such as ?my? 
and ?your?, are usually replaced with ?the.? We 
need to integrate more informative features to 
improve performance. 
 
Features Precision 
head/PoS 79.1% 
word/PoS of pre-NP 70.0% 
word/PoS of all words in NP 85.9% 
PoS of all words in NP 77.8% 
word/PoS of post-NP 71.8% 
whole words in NP 87.2% 
last word/PoS of pre-NP and head/PoS 92.3% 
whole words in NP and last word of 
pre-NP 
96.8% 
Table 3. Precision of features used in the DetClassifier 
 
Features Precision 
head/PoS 55.2% 
word/PoS of pre-NP 49.5% 
word/PoS of all words in NP 53.9% 
PoS of all words in NP 45.3% 
word/PoS of post-NP 46.1% 
whole words in NP 60.4% 
last word/PoS of pre-NP and head/PoS 65.3% 
whole words in NP and last word of 
pre-NP 
70.8% 
Table 4. Precision of features used in the SelClassifier 
 
Size Precision 
1,000,000 84.8% 
5,000,000 96.8% 
10,000,000 96.8% 
15,000,000 96.8% 
20,000,000 96.8% 
Table 5. Precision of different training size for the 
DetClassifier 
 
Size Precision 
1,000,000 39.8% 
3,000,000 43.2% 
5,000,000 44.5% 
7,000,000 61.6% 
7,249,218 70.8% 
Table 6. Precision of different training size for the  
 SelClassifier 
 
 
 
299
  
3.2 Prepositions 
Two sets of evaluation were carried out for 
detection. First, we use a randomly-selected 
portion of the BNC containing 1 million tokens to 
train our tokenizer targeting the 34 highest 
frequency prepositions. Second, we use a larger 
training corpus containing 10 million tokens, also 
randomly selected from the BNC, and target a 
smaller set of the 13 highest frequency 
prepositions, due to the fact that these 13 
prepositions can cover over 90% of the preposition 
errors found in the development set. 
We evaluate the trained taggers using two 
different metrics. First we evaluate the overall 
tagging precision, which is defined as 
 
Poverall   =  # of correctly tagged words  / # of 
all words  
Ppresence =  # correctly tagged PRESENCE / #  
all words labeled with PRESENCE 
 
Since most answer tags are Non-presence, 
Poverall is not informative, we therefore focus on 
Ppresense, and further evaluate the recall of presence, 
defined as: 
 
Rpresence = # correctly tagged PRESENCE  / # 
word should be tagged with PRESENCE  
 
We then evaluate on Precision and Recall of the 
PRESENCE tag using different probabilities to 
threshold the CRF tagging results. Then we show 
the result of two evaluation sets. On the left is the 
tagger train with 1 million tokens, targeting 34 
prepositions. On the right is the tagger trained with 
10 million tokens, targeting 13 prepositions. Only 
the latter tagger is used for producing the 
submitted runs. 
We used the development data released as part 
of HOO 2012 Shared Task as the gold standard for 
the evaluation of our preposition correction module. 
In order to observe the effect of different feature 
sets in training, we first extracted the MT and RT 
instances marked by the gold standard and then ask 
the correction module to correct these prepositions 
directly. Table 7 shows the precision of the models 
trained on different feature sets. The definition of 
precision is the same as the definition in the HOO 
2012 Shared Task. The results shows that the 
model trained using four feature sets achieved 
higher precision.   
Features Precision 
MT RT MT+RT 
f1 43.62% 39.15% 40.48% 
f1+f2 52.58% 43.47% 46.18% 
f1+f2+f3 55.20% 46.77% 49.27% 
f1+f2+f3+f4 55.11% 47% 49.41% 
Table 7. The feature selection and accuracy of the 
preposition correction module. 
 
In addition to the evaluation on the effect of 
different feature sets, we also conducted an 
evaluation done on the development data of HOO 
2012 Shared Task to observe the performance of 
the correction model when combined with the 
detection model. The correction model corrected 
three different types of preposition errors, MT, RT 
and MT+RT simultaneously (Table 8). 
 
 
  MT RT MT+RT 
Precision 1.16% 3.80% 4.96% 
Recall 29.86% 41.14% 37.79% 
  
Table 8. Precision and recall scores of the correction 
modules when combined with the detection module.  
 
Note that when we only corrected the 
preposition errors marked MT by preposition error 
detection module, the precision and recall are both 
lower than that of RT. The amount of false alarm 
instances of detection module in MT seems to be 
too high, thus in this paper, we won?t correct the 
instance marked MT to insure the higher precision 
of overall preposition correction. 
 
4. Conclusion 
In this paper, we integrate four learning-based 
methods in determiner and preposition error 
detection and correction. The integrated system 
simply parses and tags the test sentences and then 
corrects determiners and prepositions step by step. 
The training of our system relies on well-formed 
corpora and thus seems to be easier to 
re-implement it. The large well-formed corpus 
might also insure higher recall.  
In the future, we plan to integrate the system in 
a more flexible way. The detection modules could 
300
  
pass probabilities to the correction modules. The 
correction modules thus could decide whether to 
correct the instances or not. In addition, we plan to 
reduce the false alarm rate of the detection module. 
Besides, a more considerable evaluation would be 
conducted in the near future. 
Acknowledgements 
We would acknowledge the funding support 
from the Project (NSC 100-2627-E-007-001) 
and the help of the participants. Thanks also go to 
the comments of anonymous reviewers on this 
paper. 
References  
Mei-Hua Chen and Maosung Lin, 2011. Factors and 
Analyses of Common Miscollocations of College 
Students in Taiwan. Studies in English Language and 
Literature, 28, pp. 57-72. 
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving 
prepositions. In Proceedings of the Fourth 
ACL-SIGSEM Workshop on Prepositions, pp.  
25-30. 
Robert Dale and Adam Kilgarriff. 2010. Helping Our 
Own: Text massaging for computational linguistics 
as a new shared task. In Proceedings of the 6th 
International Natural Language Generation 
Conference, pp. 261?266. 
Robert Dale, Ilya Anisimoff and George Narroway 
(2012) HOO 2012: A Report on the Preposition and 
Determiner Error Correction Shared Task. In 
Proceedings of the Seventh Workshop on Innovative 
Use of NLP for Building Educational Applications. 
Rachele De Felice and Stephen G. Pulman. 2007. 
Automatically acquiring models of preposition use. 
In Proceedings of the Fourth ACL-SIGSEM 
Workshop on Prepositions, pp. 45-50. 
Claudia Leacock, Martin Chodorow, Michael Gamon, 
and Joel Tetreault. 2010. Automated Grammatical 
Error Detection for Language Learners. Synthesis 
Lectures on Human Language Technologies. Morgan 
and Claypool. 
Adwait Ratnaparkhi. 1997. A linear observed time 
statistical parser based on maximum entropy models. 
In Proceedings of the Second Conference on 
Empirical Methods in Natural Language Processing, 
Brown University, Providence, Rhode Island. 
Michael Swan and Bernard Smith, editors. Learner 
English: A teacher?s guide to interference and other 
problems. Cambridge University Press, 2 edition, 
2001. DOI: 10.1017/CBO9780511667121 19, 23, 91 
Tsuruoka Y, Tateishi Y, Kim JD, Ohta T, McNaught J, 
Ananiadou S, Tsujii J. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In 
Advances in Informatics, 10th Panhellenic 
Conference on Informatics; 11-13 November 2005 
Volos, Greece. Springer; pp. 382-392. 
 
301
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 20?25,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
CoNLL-2013 Shared Task: Grammatical Error Correction NTHU System Description  Ting-Hui Kao+, Yu-Wei Chang*, Hsun-Wen Chiu*, Tzu-Hsi Yen+,  Joanne Boisson*, Jian-Cheng Wu+, Jason S. Chang+
* Institute of Information Systems and Applications + Department of Computer Science National Tsing Hua University  HsinChu, Taiwan, R.O.C. 30013 { maxis1718, teer1990, chiuhsunwen, joseph.yen,  Joanne.boisson, wujc86, jason.jschang} @gmail.com    Abstract 
Grammatical error correction has been an active research area in the field of Natural Language Processing. This paper describes the grammatical error correction system developed at NTHU in participation of the CoNLL-2013 Shared Task. The system consists of four modules in a pipeline to correct errors related to determiners, prepositions, verb forms and noun number. Although more types of errors are involved that than last year?s Shared Task, leading to more complicated problem this year, our system still obtain higher F-score as compared to last year. We received an overall F-measure score of 0.325, which put our system in second place among 17 systems evaluated. 1 Introduction Grammatical error correction is a task involving automatically detecting and correcting grammatical errors and improper choices. Grammatical error correction in writing of English as a second language (L2) or foreign language (EFL) is an important issue, for there are 375 million L2 speakers and 750 million EFL speakers around the world (Graddol, 2006). Most of these non-native speakers tend to make many kinds of error in their writing. An error correction system has the short-term benefit of helping writers improve the quality of writing. In the long run, non-native writers might learn from the corrections and thus gradually gain better command of grammar and word choice. The HOO shared task of 2012 is aimed at detecting and correcting misuse of determiners and prepositions, two types of errors accounting 
for only 38% of all errors. Therefore, there are a lot more errors related to other parts of speech that we have to address in this year?s shared task. In this paper, we describe the system submission from NTHU. The system reads and processes a given sentence through a pipeline of four distinct modules dealing with determiners, prepositions, verb forms and noun plurality. The output of one module feeds into the next module as input. The system finally produces possibly corrected sentences. The rest of the article is organized as follows. Section 2 describes detection and correction approach of each module in detail. Section 3 describes experiment setting and results. Then in Section 4, we discuss strengths and limitations of the proposed system and directions of future work. We conclude in Section 5.  2 System Description The system is designed to read a sentence and process each type of errors in terms and finally produce a corrected sentence. In Section 2.1, we give an overview of the system. Then, in Sections 2.2-2.5, we describe how to correct errors related to noun number, determiner, verb tense, and preposition.   
  Figure 1. System Architecture 
20
Table 1. Moving windows of ?location? Moving Window n-grams MW5 track based on the location based on the location of  on the location of cell the location of cell phone location of cell phone by MW4 based on the location on the location of the location of cell location of cell phone MW3 on the location the location of location of cell MW2 the location location of 2.1 Overview In this section, we give an overview of our system. Figure 1 shows the architecture of the error correction system. In this study, we focus on five different grammatical error types, including the improper usage of Determiner (ArtOrDet), Noun Number (Nn), Verb-Tenses (Vform), Subject-Verb Agreement (SVA), and Preposition (Prep).  In order to deal with these different types of errors systematically, we propose a back-off model based on the moving window approach.  Moving Window  A moving window MW of certain word wi is defined as below. (Leacock et al, 2010; Rozovskaya et al, 2010)   ???,?(?) = {???? ,? ,????? ???  ?, ? = 0, ? ? 1 ?}  (1)  where i denotes the position of word, k the window size, and w the original or replacement word at position i. In our approach, the window size is set to 2 to 5 words.  For example, consider the target word ?location? in the sentence, ?Children can easily be track based on the location of cell phone by parents.? The n-grams in moving windows of related to ?location? of sizes 2 to 5 are shown in Table 1.  Back-off Model To determine whether the target word needs to be changed to a different form (e.g, from ?location? to ?locations?), we first replace the target word with its variant forms (e.g., ?locations? for ?location?) in all MW n-grams and 
Table 2. Trigram information of ?location? and ?locations? in back-off model MW3 n-gram Freq. S3 location on the location the location of location of cell 304,400 3,794,400 1,400 4 M locations on the locations the locations of locations of cell 18,200 374,000 200 0.04 M  then measure the ratio of the counts of the original and replaced n-grams in a corpus. The frequency counts are obtained by querying a linguistic search engine Linggle (Joanne Boisson et al 2013), a web-scale linguistic search engine based on Google Web1T (Brants and Franz, 2006). The sum of n-gram counts, Sk with  the word w (original or replacement) in the ith position is defined as        ??,?(?)  ?=  ? ?????(?????)????  ?? ? ? ?(?)      (2)  With MW and S, we design a Replace function to determine whether is necessary to replace wi with its variant form, w' :  
Figure 2. The function Replace for determining whether to replace a word in location i using moving windows of size k.  The parameters ? and ? in Replace are set empirically.  For instance, in the given sentence ?Children can easily be track based on the location of cell phone by parents?, the target word wi is ?location? and the candidate is ?locations? for the Nn type error. According to Equation 2, the sums S9,3(?location?) of the original trigrams is about 4 million, whereas S9,3(?locations?) of the replaced trigrams is only 0.4 million (see Table 2 for more details). The value of r is 0.096, and depending on the threshold, Replace either returns False or back off to consider again the ratio r of S9,2(?location?) of the original bigrams and S9,2(?locations?) of the replacement bigrams for confidence in replacing the word ?location.?  
function Replace(i, k, w?) r = Si,k(w?)/Si,k(wi) if r > ? return True else if k > 2 and r > ?: return Replace (i, k-1, w?) else: return False 
21
2.2 The number module The number module is designed to correct error related to noun number (i.e., Nn). Two types of error are included, errors of singular noun and plural noun. To correct errors, we identify heads of base noun phrase (i.e., NP consisting of maximal contiguous sequence of tokens without containing another noun phrase or clause) in the given sentence by using part-of-speech tags and GeniaTagger (Tsuruoka et al, 2006), then use the Replace function to replace the original nouns (either singular or plural) to a different form (i.e., singular to plural, or plural to singular). We use two methods in the number module: combining voting with back-off, and using dependency relations.   Combining voting with back-off   Each n-gram in a moving window of various sizes described in Section 2.1 gets to cast a vote. When the sum of frequency counts related to the original noun is higher than that related to the replacement noun, the original noun gets one vote and vise versa. Voting method determines whether to replace the noun based on majority of the votes. For example, all of the 14 replacement n-grams (MWi, k , k = 2, 5) in Table 1 get a vote, because the n-gram with ?location? has higher frequency count that the same n-gram replaced with ?locations?. Intuitively, we should be confident enough to decide to stay with the original noun, i.e., ?location.? Back-off model described in Section 2.1 make a decision to permit the Replace module to change the original noun depend on threshold ?. Both of voting and back-off model need to show that alternative noun number is better. For the scheme of voting and back-off model, we also require the top count ratio and absolute count of 0.95 and 60,000 based on empirical evidence.  Using dependency relations  In some cases, the noun number depends on subject-verb agreement. We use part-of-speech information of subject and governing verb obtained from a tagger to handle such cases. For that, we use 3rd person singular present (i.e., VBZ) and other verb forms (e.g., VBP) to detect noun number mistakes.  Consider the sentence, ?In the society today, there are many ideas or concept that are 
currently in the stages of research and development.?, where ?concept? is a singular noun, but should be plural according to syntactic dependency information. The dependency parser typically produces nsubj(are-7, concept-11) among other relations and the word ?are? is tagged as VBP. Accordingly, we can replace the original noun, ?concept? to its plural form, ?concepts.?  2.3 Determiners module  The determiner is aimed at correcting determiner errors (i.e., errors annotated as ArtOrDet ). Given a sentence, we first identify the base noun phrases and their determiners (or lack of determiner) and using the moving window approach to decide whether there is an error and which alternative form to use. For determiner errors, the variant form of a base NP with a determiner is simple the same NP with determiner removed, while the variant form of a base NP without a determiner is simple the same NP with a determiner added. In addition to the moving window and back-off model, we also use dependency relations to check if a determiner is required for a base noun phrase.  Frequency of n-grams  We adopt the moving window approach and combine it with the back-off model mentioned in Section 2.1 with slight modification for the cases specific to determiner errors. When the head of given Base-NP is the last word of the n-gram, (as in ?Prepare meals for the elderly is my duty.?), the head can often be used as an modifier (as in ?for elderly people? leading to higher counts unrelated to the our case of the word being used as the head.   Therefore, while we adopting the moving window approach, the count of such n-gram is not counted. We set the threshold in the Replace function empirically: ?=5 and ?=0.35.   Dependency  In some cases, the frequency information of n-grams provides limited evidence for identifying mistakes. Therefore, we use more effective rules based on dependency relations to recognize the determiner errors in a way similar to the number module. 
22
Table 3. Verb form n-grams with PMIs. 
Verb Form n-grams PMI Sum happening crash happening happening at 21.5 38.2 59.7 happen crash happen happen at 24.0 35.7 59.7 happened crash happened happened at air crash happened happened at Miami crash happened at 
30.5 43.0 36.2 31.8 43.2 
184.7 
happens crash happens happens at crash happens at 
27.9 42.4 37.0 
107.3 
 We remove a determiner from a noun phrase with a plural head and an existing determiner. Otherwise, this module adds an appropriate determiner before the current noun phrase. For a conjunction (i.e., X and/or Y) of two base NPs, the rules favor adding a determiner such that both NPs have the same kind of determiner. 2.4 The verb-tense module In this section, we mainly concentrate on providing more proper verb tenses. Besides moving window, we introduce accumulated point-wise mutual information (PMI) (Church and Hanks, 1990) to improve the performance of this module. Applying PMI to this topic is based on the hypothesis that an appropriate verb form has a higher PMI measure with the context. To achieve more flexibility than the standard PMI, we use the modified PMI, which is an extension of standard PMI allowing an n-gram s of arbitrary length as input   ???(?) = log ?(?|?)?(??)????                               (3) where wi denotes the i-th word in s, k = | s |, and P(wi) the probability of wi estimated using a very large corpus. P(s|k) is the probability based on maximal likelihood estimation:   ?(?|?) = ???? (?)???? (?)???                               (4)  where S denotes all n-grams of length k. The PMI value of n-grams related to the original and alternative tense forms of a give verb are then calculated to attempt to correct the verb in question with a decision in favor of highest PMI. 
Table 4. Sample search results of ?being ?$PP a dangerous situation? * 
N-gram Count being in a dangerous situation 161 being a dangerous situation 0 being at a dangerous situation 0 being on a dangerous situation 0 ? 0 being about a dangerous situation 0 * Note:? denotes option word and $PP denotes wildcard prepositions  With this extended notion of PMI, we proceed as follows. First, we select each verb in a sentence and extract n-grams in moving window method as described in Section 2.2. Next, we generate more alternative n-grams by substituting all the related verb forms for the selected verb. After that, for all these n-grams, we calculate PMIs and accumulate the measures for each group of verb forms. Finally, if the accumulated PMI of the original verb is lower than the mean value of PMI of all verb forms, the verb in question will be replaced with the verb form associated the highest PMI value. Consider the sentence, ?In late nineteenth century, there was a severe air crash happening at Miami international airport.? We attempt to correct the verbs ?was? and ?happening? in the sentence. Table 3 shows n-grams and corresponding PMIs of each verb form. The accumulated PMI of ?happened? has the maximum value. So, the module changes ?happening? to ?happened.?  2.5 The prepositions module For preposition, we attempt to handle the two types of error: DELETE and REPLACE, and leave the INSERT errors for future work. For DELETE errors, the preposition in question should be deleted from the given sentence, whereas for REPLACE errors the preposition should be replaced with a more appropriate alternative. The third error type of preposition, INSERT, is left for future study. The proposed solution is based on the hypothesis that the usage of preposition often depends on the collocation relation of verb or noun. Therefore, we propose a back-off model, which utilizes the dependency relations to identify the related words of the preposition in question. We proceed as follows: For a target preposition in a given sentence, we extract the n-gram containing the preposition, its prepositional object, and the content word before the 
23
preposition. For example, the n-gram ?being in a dangerous situation? is extracted from the sentence ?This can protect the students from being in a dangerous situation in particularly for the small children who are studying in nursery.? The n-gram ?being in a dangerous situation? is then transformed into a query for a linguistic search engine (e.g., Linggle as described in Joanne et al 2013) to obtain the counts of all preposition variant forms, including NULL (for DELETE) or other prepositions (for REPLACE). The transformation process is very simple involving changing the proposition to a wild part of speech symbol. For example, ?being in a dangerous situation? is transformed to ?being ?$PP a dangerous situation.? The sample search results are shown in Table 4. From the results, we could confirm that the preposition ?in? is used correctly.  Although we use the web-scale n-gram for validation of usage of preposition, however, data sparseness still poses a problem. Furthermore, we cannot obtain information for n-grams with length more than 5, since the Web 1T we used only contains 1 to 5-grams. In order to cope with the data sparseness problem, we transform a query into a more general form, if no result could be obtained in the first round of search. To generalize the query, we remove the modifiers of the prepositional object one after another. Additionally, we also attempt to change the modifiers with the most frequent modifier of the object. Consider the n-gram ?in modern digit world.? The generalized n-grams ?in digit world? and ?in new world? will then be transformed into queries in turns until the results are sufficient for the model to make a decision. To avoid false alarm, empirically determined thresholds are used to measure the ratio of count of a preposition variant form to the original preposition. 3 Experiment To assess the effectiveness of the proposed method, we used the official training and testing data of CoNLL-2013 Shared Task. We also exploited several tools including Linggle, Stanford Parser and Geniatagger in the proposed system. Linggle supports flexible linguistic queries with wild part of speech and returns matching n-grams counts in Google Web 1T 5gram. Stanford Parser and Geniatagger produce syntactical information including dependency relations, 
part-of-speech tags, and phrase boundary. The evaluation scorer, which computes precision, recall, and F-score, is provided by National University of Singapore, the organizer of CoNLL-2013 Shared Task. On the test data, our system obtained the precision, recall and F-score of .3057, 0.346, and .3246, which put us in first place in term of recall and second place in term of F-score. 4 Discussion In this section, we discuss the strengths and limitations of our system and propose approaches to overcome current limitations. The module of noun numbers, moving window and syntactic dependency for correcting errors cannot handle well some ambiguous cases. For example, in this case "In conclusion, what I have mentioned above, we have to agree, tracking system has many benefits?.", according to the gold-standard annotations, ?system has? is corrected to ?systems have?.  However, this module keep the original word because of the 3rd person singular present verb, ?has?. Before ?has? being corrected to ?have?, there was no sufficient evidence to support that ?systems? is a good replacement. In cases like this, it is often difficult to suggest a correction using only the sentential context and n-gram frequency and dependency relations. To correct such an error, we may need to consider the context of the discourse or combine the module of different error types such as noun numbers and verb tense, which is beyond the scope of the current system. We handle the determiner errors with threshold ?  and ?  empirically derived, but it would be more effective if we could use some form of minimal error rate tuning (MERT) to set the parameters. Besides, we found that applying the dependency criteria and moving window method in parallel leads to high recall but low precision. However, the moving window method often fails because of insufficient evidence. In such case, the system can perform better in both precision and recall by favoring the dependency model output. For our system, the performance of correcting verb form errors is severely limited by the lengths of n-gram. The failure related to verb forms correction are mostly caused by the limitation of n-gram length of Web 1T. There is a large portion of sentences where the subject (or the adverbs) and the verb are so far apart, that 
24
they are not within windows of five words. So, it is difficult to use the noun number of the subject to select the correct verb form. Another major area of limitations of handling verb form errors has to do with rare words which lead to unseen n-grams even in a very large dataset like Web 1T. These rare words are mostly name entities that have insufficient coverage when combined other words in n-grams. Intuitively, we can generalize the n-gram matching process as in the case of handling preposition errors. In this study, we use the preposition and object relation (POBJ) to determine whether the use of the preposition is correct. The relation is useful for generalizing the queries and in correcting preposition errors. However, many preposition errors are unrelated to POBJ. For example, in the sentence ?Surveillance technology will help to prevent the family to loss their member...?, the two words ?to loss? should be replace with ?from losing.? Unfortunately, the current system cannot correct such an error in the absence of POBJ relation. In order to correct this kind of error, we have to consider composed relations such as noun-preposition-verb, which is crucial to the capability of correcting such multiple consecutive errors (i.e., preposition plus verb). 5 Conclusion In this paper, we build four modules in determiner, noun number, verb form, and preposition for error detection and correction. For different types of errors, we have developed modules independently in accordance with their features. The constructed modules rely on both moving windows and back-off model to improve grammatical error correction. Additionally, for verb form errors, we introduce point-wise mutual information for higher precision and recall.  We plan to integrate all the modules in a more flexible way than the current pipeline scheme. Yet another direction for future research is to consider the discourse context. 6 Acknowledgements We would like to acknowledge the funding supports from Delta Electronic Corp and National Science Council, Taiwan (contract no: NSC 100-2511-S-007-005-MY3). We are also thankful for helpful comments from the anonymous reviewers.  
References  Joanne Boisson, Ting-Hui Kao, Jian-Cheng Wu, Tzu-Hsi Yen and Jason S. Chang. 2013. Linggle: a Web-scale Linguistic Search Engine for Words in Context. In proceedings of Association for Computational Linguistics demonstrations. (ACL 2013) Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram corpus version 1.1.LDC2006T13 Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics 16(1) (1990) 22?29 Leacock Claudia et al 2010. Automated grammatical error detection for language learners. Synthesis Lectures on Human Language Technologies, 3(1) 1?134. Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013. Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013). Daniel Dahlmeier and Hwee Tou Ng. 2012. Better Evaluation for Grammatical Error Correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2012). pp. 568 ? 572 David Graddol. 2006. English next: Why global English may mean the end of ?English as a Foreign Language.? UK: British Council. John Lee and Stephanie Seneff. 2006. Automatic Grammar Correction for Second-Language Learners. In INTERSPEECH ICSLP. Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto and Joel Tetreault. 2013. The CoNLL-2013 Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Alla Rozovskaya and Dan Roth. 2010. Generating confusion sets for context-sensitive error correction. In Proceedings of EMNLP, pp. 961?970. Yoshimasa Tsuruoka et al Developing a Robust Part-of-Speech Tagger for Biomedical Text. In Advances in Informatics - 10th Panhellenic Conference on Informatics, pp 382?392. 
25
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 91?95,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
NTHU at the CoNLL-2014 Shared Task
Jian-Cheng Wu*, Tzu-Hsi Yen*, Jim Chang*, Guan-Cheng Huang*,
Jimmy Chang*, Hsiang-Ling Hsu+, Yu-Wei Chang+, Jason S. Chang*
* Department of Computer Science
+ Institute of Information Systems and Applications
National Tsing Hua University
HsinChu, Taiwan, R.O.C. 30013
{wujc86, joseph.yen, cwebb.tw, a2961353,
rocmewtwo, ilovecat6717, teer1990, jason.jschang}@gmail.com
Abstract
In this paper, we describe a system for cor-
recting grammatical errors in texts written
by non-native learners. In our approach, a
given sentence with syntactic features are
sent to a number of modules, each focuses
on a specific error type. A main program
integrates corrections from these modules
and outputs the corrected sentence. We
evaluated our system on the official test
data of the CoNLL-2014 shared task and
obtained 0.30 in F-measure.
1 Introduction
Millions of non-native learners are using English
as their second language (ESL) or foreign lan-
guage (EFL). These learners often make different
kinds of grammatical errors and are not aware of
it. With a grammatical error corrector applies rules
or statistical learning methods, learners can use the
system to improve the quality of writing, and be-
come more aware of the common errors. It may
also help learners improve their writing skills.
The CoNLL-2014 shared task is aimed at pro-
moting research on correcting grammatical errors.
Types of errors handled in the shared task are ex-
tended from the five types in the previous shared
task to include all common errors present in an es-
say.
In this paper, we focus on the following errors
made by ESL writers:
? Spelling and comma
? Article and determiner
? Preposition
? Preposition + verb (interactive)
? Noun number
? Word form
? Subject-verb-agreement
For each error type, we developed and tuned a
module based on the official development data. A
main program combines the correction hypotheses
from these modules and produces the final correc-
tion. If multiple modules propose different cor-
rections to the same word/phrase, the correction
proposed by the module with the highest precision
will be chosen.
2 Method
2.1 Spelling and Comma module
In this section, we correct comma errors and
spelling errors, including missing/extraneous hy-
phens. For simplicity, we adopt Aspell
1
and
GingerIt
2
to detect spelling errors and generate
possible replacements, considered as confusable
words, which might contain the word with cor-
rect spelling. Then, we replace the word being
checked with confusable words to generate sen-
tences. Language models trained on well-formed
texts are used to measure the probability of these
1
http://aspell.net/
2
https://pypi.python.org/pypi/gingerit
We use GingerIt only for correcting missing/extraneous
hyphens
91
candidates. Candidate with the highest probability
is chosen as correction.
Omitted commas form a large proportion of
punctuation errors. We apply the CRF model pro-
posed by Israel, et. al. (2012) with some mod-
ification. We replace distance features with syn-
tactic features. More specifically, we do not use
features such as distances to the start of sentence
or last comma. And we add two features, one in-
dicates whether a word is at the end of a clause,
and the other indicates whether the current clause
starts with a prepositional phrase.
2.2 Subject-verb-agreement module
This module corrects subject-verb-agreement er-
rors in a given sentence. Consider the sentence
?The boy in blue pants are my brother?. The cor-
rect sentence should be ?The boy in blue pants is
my brother?. Since a verb could be far from it?s
subject, using ngram counts may fail to detect and
correct such an error.
We use a rule-based method in this module.
In the first stage, we identify the subject of each
clause by using information from the parser. Both
the dependency relation and syntactic structure are
used in this stage. Dependency relations such as
nsubj and rcmod indicate subjects of subject-
verb relation. If there is a verb that has not been
assigned a subject via dependency relations, head
of noun phrase in the same clause will be used in-
stead. And in the second stage, we check whether
subject and verbs agree, for each clause in the sen-
tence.
Here we explain our correction process in more
detail. For each clause, the singular and plural
forms of verbs in the clause must be consistent
with the subject of the clause unless the subject
is a quantifier. Consider the following sentences:
The number of cats is ten.
A number of cats are playing.
Since our judgment only depends on the subject
number, it?s hard to tell whether should we use
a plural verb or not in this case. The quantifiers
we do not handle are listed as follow: number, lot,
quantity, variety, type, amount, neither.
2.3 Number module and Forms module
We correct noun number error in two stages. In
the first stage, we generate a confusion set for each
word. While constructing confusion set for noun
number, both of the singular form and plural form
are included in the set. While constructing con-
fusion set for word forms, we use the word fam-
ilies in Academic Word List (AWL)
3
and British
National Corpus (BNC4000)
4
. Given a content
word, all the words in the same family except
antonyms are entered into the confusion set. How-
ever, comparative form and superlative form of an
adjective are eliminated from the confusion set,
since replacing an adjective with these forms is a
semantic rather than syntactic decision. The fol-
lowing examples illustrate what kinds of alterna-
tives are eliminated:
antonyms: misleading for the word lead
semantics: higher for the word high
Additionally, in the forms module, a correc-
tion is ignored, if it is actually correcting a verb
tense, subject-verb-agreement, or noun number er-
ror. We use part-of-speech (POS) tag to check this.
More specifically, any corrections that changes a
word with a VBZ tag to a word with a VBP or
VBD tag is ignored, and vice versa. And any cor-
rections that switches a noun between it?s singular
form and plural form is also ignored.
With the confusion sets, we proceed to the
second stage. In this stage, we use words in
the confusion set to attempt to replace potential
errors. Language models trained on well-formed
text are used to validate the replacement decisions.
Given a word w, If there is an alternative w? that
fits in the context better, w is flagged as an error
and w? is returned as a correction.
Here is our formula for correcting errors
P (O) =
P
ngram
(O) + P
rnn
(O)
2
P (R) =
P
ngram
(R) + P
rnn
(R)
2
Promotion =
P (R)? P (O)
|O|
While checking a content word w, we replace
w in the original sentence O with alternatives and
generate candidates C. We then generate the can-
didate R with the highest probability among all
3
http://www.victoria.ac.nz/lals/
resources/academicwordlist/sublists
4
http://simple.wiktionary.org/wiki/
Wiktionary:BNC_spoken_freq
92
candidates. We use an interpolated probability
5
of
ngram language model P
ngram
and recurrent neu-
ral network language model P
rnn
. Promotion in-
dicates the increase in probability per word after
we replace sentence O with the candidate R. We
use word number to normalize Promotion follow-
ing Dahlmeier, et al. (2012). Corrections are made
only if Promotion is higher than a empirically de-
termined threshold.
6
2.4 Article and Determiner module
In this subsection, we describe how we correct er-
rors of omitting a determiner or adding a spuri-
ous determiner. The language models mentioned
in the last subsection are also used in this module.
We tune our thresholds for making corrections on
development data
7
, and found that deleting a de-
terminer should have a lower threshold while in-
serting one should have a higher one, so we set
different thresholds accordingly.
8
To cope with the situation where a deter-
miner/article is far ahead of the head of the noun
phrase, we apply another constraint while making
correction decision.
First, we calculate statistics on the head of noun
phrases. We extract the most frequent 100,000
terms in Web-1T 5-gram corpus. These terms are
used to search their definitions in Wikipedia (usu-
ally at the first paragraph). The characteristic of a
definition is that it has no prior context and most
of the noun phrases with a determiner are unique
or known to the general public. Heads of these
nouns phrases are likely to always appear with a
determiner.
Heads that tend to appear with a determiner
the help us to decide whether a determiner should
be added to a noun phrase. We add a determiner
using two constraints. We only insert a determiner
or an article, if the statistics indicate that head of
a noun phrase tends to have a determiner, or the
promotion of log probability is much higher than
the threshold. A similar constraint is also applied,
for deleting a determiner or an article.
5
the probabilities present in the formula are log probabil-
ities
6
the threshold for noun number module is 0.035 and 0.050
for word form module. These threshold were set empirically
after testing on development data
7
test data of the CoNLL-2013 shared task
8
the threshold for insertion is 0.035 and 0.040 for deletion
2.5 Preposition module
For preposition errors, we focus on handling two
types of errors: REPLACE and DELETE. A
preposition, which should be deleted from the
given sentence, is regarded as a DELETE error,
whereas for a preposition, which should be re-
placed with a more appropriate alternative, is re-
garded as a REPLACE error. In this work, we
correct the two types of errors based on the as-
sumption that the usage of preposition often de-
pends on the collocation relations with a verb or
noun. Therefore, we use the dependency relations
such as dobj and pobj, and prep to identify
the related words, and then we validate the usage
of prepositions, and correct the preposition errors.
A dependency-based model is proposed in this
paper to handle the preposition errors. The model
consists of two stages: detecting the possible
preposition errors and correcting the errors.
In the first stage, we use the Stanford depen-
dency parser (Klein and Manning, 2003) to extract
the dependency relations for each preposition. The
relation tuples, which contain the preposition, verb
or noun, and prepositional object. For example,
the tuple of verb-prep-object (listen, to, music),
or the tuple of noun-prep-object (point, of, view)
are extracted for validation. We identify a preposi-
tion containing as an error, if the tuple containing
the preposition does not occur in a reference list
built using a reference corpus. In order to resolve
the data sparseness and false alarm problems, we
need a sufficiently large list of validated tuples.
In this study, the reference tuple lists are gener-
ated from the Google Books Syntactic N-grams
(Goldberg and Orwant, 2013)
9
. For example, we
can find (come, to, end, 236864) and (lead, to, re-
sult, 57632) in the verb-preposition-object refer-
ence list. We have generated 21,773,752 different
dependency tuples for our purpose.
In the second stage, we attempt to correct all
potential preposition errors. At first, a list of can-
didate tuples is generated by substituting the orig-
inal preposition in the error tuple with alterna-
tive prepositions. For example, the generated can-
didate tuples for the error tuple (join, at, party)
will include (join, in, party), (join, on, party), etc.
On the other hand, the tuple, (join, party), which
9
Data sets available from http://
commondatastorage.googleapis.com/books/
syntactic-ngrams/index.html
93
deletes the preposition, is also taken into consid-
eration. All candidates are ranked according to
the frequency provided by the reference lists. The
preposition in the tuple with the highest frequency
is returned as the correction suggestion.
Figure 1: Sample annotated trigrams
Figure 2: Sample trigram group
Figure 3: Sample phrase translation for a trigram
group
2.6 Interactive errors module
This module uses a new method for correcting
serial grammatical errors in a given sentence in
learners writing. A statistical translation model is
generated to attempt to translate the input with se-
rial and interactive errors into a grammatical sen-
tence. The method involves automatically learn-
ing translation models based on Web-scale n-
gram. The model corrects trigrams containing se-
rial preposition-verb errors via translation. Eval-
uation on a set of sentences in a learner corpus
shows that the method corrects serial errors rea-
sonably well.
Consider an error sentence ?I have difficulty to
understand English.? The correct sentence for this
should be ?I have difficulty in understanding En-
glish.? It is hard to correct these two errors one by
one, since the errors are dependent on each other.
Intuitively, by identifying difficulty to understand
as containing serial errors and correct it to diffi-
culty in understanding, we can handle this kind of
problem more effectively.
First, we generate translation phrase table as
follows. We begin by selecting trigrams related to
serial errors and correction from Web 1T 5-gram.
Figure 1 shows some sample annotation trigrams.
Then, we group the selected trigrams by the first
and last word in the trigrams. See Figure 2 for a
sample VPV group of trigrams. Finally, we gener-
ate translation phrase table for each group. Figure
3 shows a sample translation phrase table.
At run time, we tag the input sentence with part
of speech information in order to find trigrams
that fit the type of serial errors. Then, we search
phrase table and generate translations for the
input phrases in a machine translation decoder to
produce a corrected sentence.
3 Experiment
Two types of trigram language models, ngram
model and recurrent neural network (RNN) model,
are used in correcting spelling, noun number, word
form, and determiner errors. We trained the ngram
language model on English Gigaword and BNC
corpus, using the SRILM tool (Stolcke, 2002).
We train the RNN model with RNNLM toolkit
(Mikolov et al., 2011). Complexity of training the
RNN language model is much higher, so we train
it on a smaller corpus, the British National Corpus
(BNC).
We used the Stanford Parser (Klein and Christo-
pher D. Manning, 2003) to obtain dependency re-
lations in the preposition module, and to obtain
POS tags for the word form module. The subject-
verb-agreement module also uses dependency re-
lations contained in test data. Dependency rela-
tions in Google Books Syntactic N-grams (Gold-
94
berg and Orwant, 2013) were also used to develop
our dendepency-based model in the preposition
module.
To assess the effectiveness of the proposed
method, we used the official training, develop-
ment, and test data of the CoNLL-2014 shared
task. On the test data, our system obtained the pre-
cision, recall and F
0.5
score of 0.351, 0.189, and
0.299. The following table shows the performance
breakdown by module.
Figure 4: The performance breakdown by module.
(Displayed in %)
In the spelling and hyphen module, candidates
from Aspell include words that only differ from
the original word in one character, s. Language
models are then used to choose the candidate with
highest probability as our correction. The module
therefore gives some corrections about noun num-
bers or subject-verb-agreement. As a result, some
corrections made by this module overlap with cor-
rections made by the noun numbers module and
the subject-verb-agreement module, which makes
the recall of correcting spelling and hyphen errors,
4.11%, overestimated.
4 Conclusion
In this work, we have built several modules for er-
ror detection and correction. For different types
of errors, we developed modules independently
using different features and thresholds. If mul-
tiple modules propose different corrections to a
word/phrase, the one proposed by the module with
higher precision will be chosen. Many avenues
for future work present themselves. We plan to
integrate modules in a more flexible way. When
faced with different corrections made by different
modules, the decision would better be based on
the confidence of each correction with a uniform
standard, but not on the confidence of modules.
Acknowledgments
We would like to acknowledge the funding sup-
ports from Delta Electronic Corp and National
Science Council (contract no: NSC 100-2511-S-
007-005-MY3), Taiwan. We are also thankful to
anonymous reviewers and the organizers of the
shared task.
References
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng
2012. NUS at the HOO 2012 Shared Task. In Pro-
ceedings of the Seventh Workshop on Building Ed-
ucational Applications Using NLP, Association for
Computational Linguistics, June 7.
Daniel Dahlmeier and Hwee Tou Ng, 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2012).,568-672
Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013.
Building a Large Annotated Corpus of Learner En-
glish: The NUS Corpus of Learner English. In
Proceedings of the 8th Workshop on Innovative Use
of NLP for Building Educational Applications(BEA
2013)
Yoav Goldberg and Jon Orwant 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Proceedings of the Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, GA, 2013.
Ross Israel, Joel Tetreault, and Martin Chodorow
2012. Correcting Comma Errors in Learner Essays,
and Restoring Commas in Newswire Text. In Pro-
ceeding of the 2012 Conference of the North Amer-
ica Chapter of the Association for Computational
Linguistics: Human Language Technologies,284-
294, Montreal Canada, June. Association for Com-
putational Linguistics
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, 423-430.
Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar
Burget, and Jan Honza Cernocky 2011. Strategies
for Training Large Scale Neural Network Language
Models Proceedings of ASRU 2011
Andreas Stolcke 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, vol 2, 901-904
95
