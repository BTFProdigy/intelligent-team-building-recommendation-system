Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 363?371,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Studying the History of Ideas Using Topic Models
David Hall
Symbolic Systems
Stanford University
Stanford, CA 94305, USA
dlwh@stanford.edu
Daniel Jurafsky
Linguistics
Stanford University
Stanford, CA 94305, USA
jurafsky@stanford.edu
Christopher D. Manning
Computer Science
Stanford University
Stanford, CA 94305, USA
manning@stanford.edu
Abstract
How can the development of ideas in a sci-
entific field be studied over time? We ap-
ply unsupervised topic modeling to the ACL
Anthology to analyze historical trends in the
field of Computational Linguistics from 1978
to 2006. We induce topic clusters using Latent
Dirichlet Allocation, and examine the strength
of each topic over time. Our methods find
trends in the field including the rise of prob-
abilistic methods starting in 1988, a steady in-
crease in applications, and a sharp decline of
research in semantics and understanding be-
tween 1978 and 2001, possibly rising again
after 2001. We also introduce a model of the
diversity of ideas, topic entropy, using it to
show that COLING is a more diverse confer-
ence than ACL, but that both conferences as
well as EMNLP are becoming broader over
time. Finally, we apply Jensen-Shannon di-
vergence of topic distributions to show that all
three conferences are converging in the topics
they cover.
1 Introduction
How can we identify and study the exploration of
ideas in a scientific field over time, noting periods of
gradual development, major ruptures, and the wax-
ing and waning of both topic areas and connections
with applied topics and nearby fields? One im-
portant method is to make use of citation graphs
(Garfield, 1955). This enables the use of graph-
based algorithms like PageRank for determining re-
searcher or paper centrality, and examining whether
their influence grows or diminishes over time.
However, because we are particularly interested
in the change of ideas in a field over time, we have
chosen a different method, following Kuhn (1962).
In Kuhn?s model of scientific change, science pro-
ceeds by shifting from one paradigm to another.
Because researchers? ideas and vocabulary are con-
strained by their paradigm, successive incommensu-
rate paradigms will naturally have different vocabu-
lary and framing.
Kuhn?s model is intended to apply only to very
large shifts in scientific thought rather than at the
micro level of trends in research foci. Nonetheless,
we propose to apply Kuhn?s insight that vocabulary
and vocabulary shift is a crucial indicator of ideas
and shifts in ideas. Our operationalization of this in-
sight is based on the unsupervised topic model La-
tent Dirichlet Allocation (LDA; Blei et al (2003)).
For many fields, doing this kind of historical study
would be very difficult. Computational linguistics
has an advantage, however: the ACL Anthology, a
public repository of all papers in the Computational
Linguistics journal and the conferences and work-
shops associated with the ACL, COLING, EMNLP,
and so on. The ACL Anthology (Bird, 2008), and
comprises over 14,000 documents from conferences
and the journal, beginning as early as 1965 through
2008, indexed by conference and year. This re-
source has already been the basis of citation anal-
ysis work, for example, in the ACL Anthology Net-
work of Joseph and Radev (2007). We apply LDA
to the text of the papers in the ACL Anthology to
induce topics, and use the trends in these topics over
time and over conference venues to address ques-
tions about the development of the field.
363
Venue # Papers Years Frequency
Journal 1291 1974?Present Quarterly
ACL 2037 1979-Present Yearly
EACL 596 1983?Present ?2 Years
NAACL 293 2000?Present ?Yearly
Applied NLP 346 1983?2000 ?3 Years
COLING 2092 1965-Present 2 Years
HLT 957 1986?Present ?2 Years
Workshops 2756 1990-Present Yearly
TINLAP 128 1975?1987 Rarely
MUC 160 1991?1998 ?2 Years
IJCNLP 143 2005 ??
Other 120 ?? ??
Table 1: Data in the ACL Anthology
Despite the relative youth of our field, computa-
tional linguistics has witnessed a number of research
trends and shifts in focus. While some trends are
obvious (such as the rise in machine learning meth-
ods), others may be more subtle. Has the field got-
ten more theoretical over the years or has there been
an increase in applications? What topics have de-
clined over the years, and which ones have remained
roughly constant? How have fields like Dialogue or
Machine Translation changed over the years? Are
there differences among the conferences, for exam-
ple between COLING and ACL, in their interests
and breadth of focus? As our field matures, it is im-
portant to go beyond anecdotal description to give
grounded answers to these questions. Such answers
could also help give formal metrics to model the dif-
ferences between the many conferences and venues
in our field, which could influence how we think
about reviewing, about choosing conference topics,
and about long range planning in our field.
2 Methodology
2.1 Data
The analyses in this paper are based on a text-
only version of the Anthology that comprises some
12,500 papers. The distribution of the Anthology
data is shown in Table 1.
2.2 Topic Modeling
Our experiments employ Latent Dirichlet Allocation
(LDA; Blei et al (2003)), a generative latent variable
model that treats documents as bags of words gener-
ated by one or more topics. Each document is char-
acterized by a multinomial distribution over topics,
and each topic is in turn characterized by a multino-
mial distribution over words. We perform parame-
ter estimation using collapsed Gibbs sampling (Grif-
fiths and Steyvers, 2004).
Possible extensions to this model would be to in-
tegrate topic modelling with citations (e.g., Dietz et
al. (2007), Mann et al (2006), and Jo et al (2007)).
Another option is the use of more fine-grained or hi-
erarchical model (e.g., Blei et al (2004), and Li and
McCallum (2006)).
All our studies measure change in various as-
pects of the ACL Anthology over time. LDA, how-
ever, does not explicitly model temporal relation-
ships. One way to model temporal relationships is
to employ an extension to LDA. The Dynamic Topic
Model (Blei and Lafferty, 2006), for example, rep-
resents each year?s documents as generated from a
normal distribution centroid over topics, with the
following year?s centroid generated from the pre-
ceding year?s. The Topics over Time Model (Wang
and McCallum, 2006) assumes that each document
chooses its own time stamp based on a topic-specific
beta distribution.
Both of these models, however, impose con-
straints on the time periods. The Dynamic Topic
Model penalizes large changes from year to year
while the beta distributions in Topics over Time are
relatively inflexible. We chose instead to perform
post hoc calculations based on the observed proba-
bility of each topic given the current year. We define
p?(z|y) as the empirical probability that an arbitrary
paper d written in year y was about topic z:
p?(z|y) =
?
d:td=y
p?(z|d)p?(d|y)
=
1
C
?
d:td=y
p?(z|d)
=
1
C
?
d:td=y
?
z?i?d
I(z?i = z)
(1)
where I is the indicator function, td is the date docu-
ment d was written, p?(d|y) is set to a constant 1/C.
3 Summary of Topics
We first ran LDA with 100 topics, and took 36 that
we found to be relevant. We then hand-selected seed
364
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Class
ificatio
n
Proba
bilistic
 Mode
ls
Stat. P
arsing Stat. M
T
Lex. S
em
Figure 1: Topics in the ACL Anthology that show a
strong recent increase in strength.
words for 10 more topics to improve coverage of the
field. These 46 topics were then used as priors to a
new 100-topic run. The top ten most frequent words
for 43 of the topics along with hand-assigned labels
are listed in Table 2. Topics deriving from manual
seeds are marked with an asterisk.
4 Historical Trends in Computational
Linguistics
Given the space of possible topics defined in the pre-
vious section, we now examine the history of these
in the entire ACL Anthology from 1978 until 2006.
To visualize some trends, we show the probability
mass associated with various topics over time, plot-
ted as (a smoothed version of) p?(z|y).
4.1 Topics Becoming More Prominent
Figure 1 shows topics that have becomemore promi-
nent more recently.
Of these new topics, the rise in probabilistic mod-
els and classification/tagging is unsurprising. In or-
der to distinguish these two topics, we show 20 of
the strongly weighted words:
Probabilistic Models: model word probability set data
number algorithm language corpus method figure proba-
bilities table test statistical distribution function al values
performance
Classification/Tagging: features data corpus set feature
table word tag al test accuracy pos classification perfor-
mance tags tagging text task information class
Some of the papers with the highest weights for
the probabilistic models class include:
N04-1039 Goodman, Joshua. Exponential Priors For Maximum
Entropy Models (HLT-NAACL, 2004)
W97-0309 Saul, Lawrence, Pereira, Fernando C. N. Aggregate And
Mixed-Order Markov Models For Statistical Language
Processing (EMNLP, 1997)
P96-1041 Chen, Stanley F., Goodman, Joshua. An Empirical
Study Of Smoothing Techniques For Language Model-
ing (ACL, 1996)
H89-2013 Church, Kenneth Ward, Gale, William A. Enhanced
Good-Turing And CatCal: Two New Methods For Esti-
mating Probabilities Of English Bigrams (Workshop On
Speech And Natural Language, 1989)
P02-1023 Gao, Jianfeng, Zhang, Min Improving Language Model
Size Reduction Using Better Pruning Criteria (ACL,
2002)
P94-1038 Dagan, Ido, Pereira, Fernando C. N. Similarity-Based
Estimation Of Word Cooccurrence Probabilities (ACL,
1994)
Some of the papers with the highest weights for
the classification/tagging class include:
W00-0713 Van Den Bosch, Antal Using Induced Rules As Com-
plex Features In Memory-Based Language Learning
(CoNLL, 2000)
W01-0709 Estabrooks, Andrew, Japkowicz, Nathalie AMixture-Of-
Experts Framework For Text Classification (Workshop
On Computational Natural Language Learning CoNLL,
2001)
A00-2035 Mikheev, Andrei. Tagging Sentence Boundaries (ANLP-
NAACL, 2000)
H92-1022 Brill, Eric. A Simple Rule-Based Part Of Speech Tagger
(Workshop On Speech And Natural Language, 1992)
As Figure 1 shows, probabilistic models seem to
have arrived significantly before classifiers. The
probabilistic model topic increases around 1988,
which seems to have been an important year for
probabilistic models, including high-impact papers
like A88-1019 and C88-1016 below. The ten papers
from 1988 with the highest weights for the proba-
bilistic model and classifier topics were the follow-
ing:
C88-1071 Kuhn, Roland. Speech Recognition and the Frequency
of Recently Used Words (COLING)
J88-1003 DeRose, Steven. Grammatical Category Disambiguation
by Statistical Optimization. (CL Journal)
C88-2133 Su, Keh-Yi, and Chang, Jing-Shin. Semantic and Syn-
tactic Aspects of Score Function. (COLING)
A88-1019 Church, Kenneth Ward. A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text. (ANLP)
C88-2134 Sukhotin, B.V. Optimization Algorithms of Deciphering
as the Elements of a Linguistic Theory. (COLING)
P88-1013 Haigh, Robin, Sampson, Geoffrey, and Atwell, Eric.
Project APRIL: a progress report. (ACL)
A88-1005 Boggess, Lois. Two Simple Prediction Algorithms to Fa-
cilitate Text Production. (ANLP)
C88-1016 Peter F. Brown, et al A Statistical Approach to Machine
Translation. (COLING)
A88-1028 Oshika, Beatrice, et al. Computational Techniques for
Improved Name Search. (ANLP)
C88-1020 Campbell, W.N. Speech-rate Variation and the Prediction
of Duration. (COLING)
What do these early papers tell us about how
365
Anaphora Resolution resolution anaphora pronoun discourse antecedent pronouns coreference reference definite algorithm
Automata string state set finite context rule algorithm strings language symbol
Biomedical medical protein gene biomedical wkh abstracts medline patient clinical biological
Call Routing call caller routing calls destination vietnamese routed router destinations gorin
Categorial Grammar proof formula graph logic calculus axioms axiom theorem proofs lambek
Centering* centering cb discourse cf utterance center utterances theory coherence entities local
Classical MT japanese method case sentence analysis english dictionary figure japan word
Classification/Tagging features data corpus set feature table word tag al test
Comp. Phonology vowel phonological syllable phoneme stress phonetic phonology pronunciation vowels phonemes
Comp. Semantics* semantic logical semantics john sentence interpretation scope logic form set
Dialogue Systems user dialogue system speech information task spoken human utterance language
Discourse Relations discourse text structure relations rhetorical relation units coherence texts rst
Discourse Segment. segment segmentation segments chain chains boundaries boundary seg cohesion lexical
Events/Temporal event temporal time events tense state aspect reference relations relation
French Function de le des les en une est du par pour
Generation generation text system language information knowledge natural figure domain input
Genre Detection genre stylistic style genres fiction humor register biber authorship registers
Info. Extraction system text information muc extraction template names patterns pattern domain
Information Retrieval document documents query retrieval question information answer term text web
Lexical Semantics semantic relations domain noun corpus relation nouns lexical ontology patterns
MUC Terrorism slot incident tgt target id hum phys type fills perp
Metaphor metaphor literal metonymy metaphors metaphorical essay metonymic essays qualia analogy
Morphology word morphological lexicon form dictionary analysis morphology lexical stem arabic
Named Entities* entity named entities ne names ner recognition ace nes mentions mention
Paraphrase/RTE paraphrases paraphrase entailment paraphrasing textual para rte pascal entailed dagan
Parsing parsing grammar parser parse rule sentence input left grammars np
Plan-Based Dialogue plan discourse speaker action model goal act utterance user information
Probabilistic Models model word probability set data number algorithm language corpus method
Prosody prosodic speech pitch boundary prosody phrase boundaries accent repairs intonation
Semantic Roles* semantic verb frame argument verbs role roles predicate arguments
Yale School Semantics knowledge system semantic language concept representation information network concepts base
Sentiment subjective opinion sentiment negative polarity positive wiebe reviews sentence opinions
Speech Recognition speech recognition word system language data speaker error test spoken
Spell Correction errors error correction spelling ocr correct corrections checker basque corrected detection
Statistical MT english word alignment language source target sentence machine bilingual mt
Statistical Parsing dependency parsing treebank parser tree parse head model al np
Summarization sentence text evaluation document topic summary summarization human summaries score
Syntactic Structure verb noun syntactic sentence phrase np subject structure case clause
TAG Grammars* tree node trees nodes derivation tag root figure adjoining grammar
Unification feature structure grammar lexical constraints unification constraint type structures rule
WSD* word senses wordnet disambiguation lexical semantic context similarity dictionary
Word Segmentation chinese word character segmentation corpus dictionary korean language table system
WordNet* synset wordnet synsets hypernym ili wordnets hypernyms eurowordnet hyponym ewn wn
Table 2: Top 10 words for 43 of the topics. Starred topics are hand-seeded.
366
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Comp
utatio
nal Se
manti
cs
Conce
ptual 
Sema
ntics
Plan-
Base
d Dia
logue
 and D
iscour
se
Figure 2: Topics in the ACL Anthology that show a
strong decline from 1978 to 2006.
probabilistic models and classifiers entered the
field? First, not surprisingly, we note that the vast
majority (9 of 10) of the papers appeared in con-
ference proceedings rather than the journal, con-
firming that in general new ideas appear in confer-
ences. Second, of the 9 conference papers, most
of them appeared in the COLING conference (5) or
the ANLP workshop (3) compared to only 1 in the
ACL conference. This suggests that COLING may
have been more receptive than ACL to new ideas
at the time, a point we return to in Section 6. Fi-
nally, we examined the background of the authors of
these papers. Six of the 10 papers either focus on
speech (C88-1010, A88-1028, C88-1071) or were
written by authors who had previously published on
speech recognition topics, including the influential
IBM (Brown et al) and AT&T (Church) labs (C88-
1016, A88-1005, A88-1019). Speech recognition
is historically an electrical engineering field which
made quite early use of probabilistic and statistical
methodologies. This suggests that researchers work-
ing on spoken language processing were an impor-
tant conduit for the borrowing of statistical method-
ologies into computational linguistics.
4.2 Topics That Have Declined
Figure 2 shows several topics that were more promi-
nent at the beginning of the ACL but which have
shown the most precipitous decline. Papers strongly
associated with the plan-based dialogue topic in-
clude:
J99-1001 Carberry, Sandra, Lambert, Lynn. A Process Model For
Recognizing Communicative Acts And Modeling Nego-
tiation Subdialogues (CL, 1999)
J95-4001 McRoy, Susan W., Hirst, Graeme. The Repair Of Speech
Act Misunderstandings By Abductive Inference (CL,
1995)
P93-1039 Chu, Jennifer. Responding To User Queries In A Collab-
orative Environment (ACL, 1993)
P86-1032 Pollack, Martha E. A Model Of Plan Inference That
Distinguishes Between The Beliefs Of Actors And Ob-
servers (ACL, 1986)
T78-1017 Perrault, Raymond C., Allen, James F. Speech Acts As
A Basis For Understanding Dialogue Coherence (Theo-
retical Issues In Natural Language Processing, 1978)
P84-1063 Litman, Diane J., Allen, James F. A Plan Recognition
Model For Clarification Subdialogues (COLING-ACL,
1984)
Papers strongly associated with the computational
semantics topic include:
J90-4002 Haas, Andrew R. Sentential Semantics For Propositional
Attitudes (CL, 1990)
P83-1009 Hobbs, Jerry R. An Improper Treatment Of Quantifica-
tion In Ordinary English (ACL, 1983)
J87-1005 Hobbs, Jerry R., Shieber, Stuart M. An Algorithm For
Generating Quantifier Scopings (CL, 1987)
C90-1003 Johnson, Mark, Kay, Martin. Semantic Abstraction And
Anaphora (COLING, 1990)
P89-1004 Alshawi, Hiyan, Van Eijck, Jan. Logical Forms In The
Core Language Engine (ACL, 1989)
Papers strongly associated with the conceptual se-
mantics/story understanding topic include:
C80-1022 Ogawa, Hitoshi, Nishi, Junichiro, Tanaka, Kokichi. The
Knowledge Representation For A Story Understanding
And Simulation System (COLING, 1980)
A83-1012 Pazzani, Michael J., Engelman, Carl. Knowledge Based
Question Answering (ANLP, 1983)
P82-1029 McCoy, Kathleen F. Augmenting A Database Knowl-
edge Representation For Natural Language Generation
(ACL, 1982)
H86-1010 Ksiezyk, Tomasz, Grishman, Ralph An Equipment
Model And Its Role In The Interpretation Of Nominal
Compounds (Workshop On Strategic Computing - Natu-
ral Language, 1986)
P80-1030 Wilensky, Robert, Arens, Yigal. PHRAN - A
Knowledge-Based Natural Language Understander
(ACL, 1980)
A83-1013 Boguraev, Branimir K., Sparck Jones, Karen. How To
Drive A Database Front End Using General Semantic In-
formation (ANLP, 1983)
P79-1003 Small, Steven L. Word Expert Parsing (ACL, 1979)
The declines in both computational semantics and
conceptual semantics/story understanding suggests
that it is possible that the entire field of natural lan-
guage understanding and computational semantics
broadly construed has fallen out of favor. To see
if this was in fact the case we created a metatopic
called semantics in which we combined various se-
mantics topics (not including pragmatic topics like
anaphora resolution or discourse coherence) includ-
ing: lexical semantics, conceptual semantics/story
367
 
0
 
0.05 0.1
 
0.15 0.2
 
0.25
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Sema
ntics
Figure 3: Semantics over time
understanding, computational semantics, WordNet,
word sense disambiguation, semantic role labeling,
RTE and paraphrase, MUC information extraction,
and events/temporal. We then plotted p?(z ? S|y),
the sum of the proportions per year for these top-
ics, as shown in Figure 3. The steep decrease in se-
mantics is readily apparent. The last few years has
shown a levelling off of the decline, and possibly a
revival of this topic; this possibility will need to be
confirmed as we add data from 2007 and 2008.
We next chose two fields, Dialogue and Machine
Translation, in which it seemed to us that the topics
discovered by LDA suggested a shift in paradigms in
these fields. Figure 4 shows the shift in translation,
while Figure 5 shows the change in dialogue.
The shift toward statistical machine translation is
well known, at least anecdotally. The shift in di-
alogue seems to be a move toward more applied,
speech-oriented, or commercial dialogue systems
and away from more theoretical models.
Finally, Figure 6 shows the history of several top-
ics that peaked at intermediate points throughout the
history of the field. We can see the peak of unifica-
tion around 1990, of syntactic structure around 1985
of automata in 1985 and again in 1997, and of word
sense disambiguation around 1998.
5 Is Computational Linguistics Becoming
More Applied?
We don?t know whether our field is becoming more
applied, or whether perhaps there is a trend to-
wards new but unapplied theories. We therefore
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Statis
tical M
T
Class
ical M
T
Figure 4: Translation over time
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Dialo
gue S
ystem
s
Plan-
Base
d Dia
logue
 and D
iscour
se
Figure 5: Dialogue over time
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
TAG
Gene
ration Autom
ata
Unific
ation
Synta
ctic S
tructu
re Event
s WSD
Figure 6: Peaked topics
368
 
0
 
0.05 0.1
 
0.15 0.2
 
0.25
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Applic
ations
Figure 7: Applications over time
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Statis
tical M
T
Dialo
gue S
ystem
s
Spelli
ng Co
rrectio
n
Call R
outing
Speec
h Rec
ogniti
on
Biom
edica
l
Figure 8: Six applied topics over time
looked at trends over time for the following appli-
cations: Machine Translation, Spelling Correction,
Dialogue Systems, Information Retrieval, Call Rout-
ing, Speech Recognition, and Biomedical applica-
tions.
Figure 7 shows a clear trend toward an increase
in applications over time. The figure also shows an
interesting bump near 1990. Why was there such
a sharp temporary increase in applications at that
time? Figure 8 shows details for each application,
making it clear that the bump is caused by a tempo-
rary spike in the Speech Recognition topic.
In order to understand why we see this temporary
spike, Figure 9 shows the unsmoothed values of the
Speech Recognition topic prominence over time.
Figure 9 clearly shows a huge spike for the years
1989?1994. These years correspond exactly to the
DARPA Speech and Natural Language Workshop,
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Figure 9: Speech recognition over time
held at different locations from 1989?1994. That
workshop contained a significant amount of speech
until its last year (1994), and then it was revived
in 2001 as the Human Language Technology work-
shop with a much smaller emphasis on speech pro-
cessing. It is clear from Figure 9 that there is still
some speech research appearing in the Anthology
after 1995, certainly more than the period before
1989, but it?s equally clear that speech recognition
is not an application that the ACL community has
been successful at attracting.
6 Differences and Similarities Among
COLING, ACL, and EMNLP
The computational linguistics community has two
distinct conferences, COLING and ACL, with dif-
ferent histories, organizing bodies, and philoso-
phies. Traditionally, COLING was larger, with par-
allel sessions and presumably a wide variety of top-
ics, while ACL had single sessions and a more nar-
row scope. In recent years, however, ACL has
moved to parallel sessions, and the conferences are
of similar size. Has the distinction in breadth of top-
ics also been blurred? What are the differences and
similarities in topics and trends between these two
conferences?
More recently, the EMNLP conference grew out
of the Workshop on Very Large Corpora, sponsored
by the Special Interest Group on Linguistic Data
and corpus-based approaches to NLP (SIGDAT).
EMNLP started as a much smaller and narrower
369
conference but more recently, while still smaller
than both COLING and ACL, it has grown large
enough to be considered with them. How does the
breadth of its topics compare with the others?
Our hypothesis, based on our intuitions as con-
ference attendees, is that ACL is still more narrow
in scope than COLING, but has broadened consid-
erably. Similarly, our hypothesis is that EMNLP has
begun to broaden considerably as well, although not
to the extent of the other two.
In addition, we?re interested in whether the topics
of these conferences are converging or not. Are the
probabilistic and machine learning trends that are
dominant in ACL becoming dominant in COLING
as well? Is EMNLP adopting some of the topics that
are popular at COLING?
To investigate both of these questions, we need a
model of the topic distribution for each conference.
We define the empirical distribution of a topic z at a
conference c, denoted by p?(z|c) by:
p?(z|c) =
?
d:cd=c
p?(z|d)p?(d|c)
=
1
C
?
d:cd=c
p?(z|d)
=
1
C
?
d:cd=c
?
z?i?d
I(z?i = z)
(2)
We also condition on the year for each conference,
giving us p?(z|y, c).
We propose to measure the breadth of a confer-
ence by using what we call topic entropy: the condi-
tional entropy of this conference topic distribution.
Entropy measures the average amount of informa-
tion expressed by each assignment to a random vari-
able. If a conference has higher topic entropy, then it
more evenly divides its probability mass across the
generated topics. If it has lower, it has a far more
narrow focus on just a couple of topics. We there-
fore measured topic entropy:
H(z|c, y) = ?
K?
i=1
p?(zi|c, y) log p?(zi|c, y) (3)
Figure 10 shows the conditional topic entropy
of each conference over time. We removed from
the ACL and COLING lines the years when ACL
 
3.6
 
3.8 4
 
4.2
 
4.4
 
4.6
 
4.8 5
 
5.2
 
5.4
 
5.6  1
980
 
1985
 
1990
 
1995
 
2000
 
2005
ACL C
onfere
nce COLIN
G
EMNL
P
Joint 
COLIN
G/AC
L
Figure 10: Entropy of the three major conferences per
year
and COLING are colocated (1984, 1998, 2006),
and marked those colocated years as points separate
from either plot. As expected, COLING has been
historically the broadest of the three conferences,
though perhaps slightly less so in recent years. ACL
started with a fairly narrow focus, but became nearly
as broad as COLING during the 1990?s. However, in
the past 8 years it has become more narrow again,
with a steeper decline in breadth than COLING.
EMNLP, true to its status as a ?Special Interest? con-
ference, began as a very narrowly focused confer-
ence, but now it seems to be catching up to at least
ACL in terms of the breadth of its focus.
Since the three major conferences seem to be con-
verging in terms of breadth, we investigated whether
or not the topic distributions of the conferences were
also converging. To do this, we plotted the Jensen-
Shannon (JS) divergence between each pair of con-
ferences. The Jensen-Shannon divergence is a sym-
metric measure of the similarity of two pairs of dis-
tributions. The measure is 0 only for identical dis-
tributions and approaches infinity as the two differ
more and more. Formally, it is defined as the aver-
age of the KL divergence of each distribution to the
average of the two distributions:
DJS(P ||Q) =
1
2
DKL(P ||R) +
1
2
DKL(Q||R)
R =
1
2
(P + Q)
(4)
Figure 11 shows the JS divergence between each
pair of conferences over time. Note that EMNLP
370
 
0
 
0.05 0.1
 
0.15 0.2
 
0.25 0.3
 
0.35 0.4
 
0.45 0.5  
1980
 
1985
 
1990
 
1995
 
2000
 
2005
ACL/C
OLING
ACL/E
MNLP
EMNL
P/COL
ING
Figure 11: JS Divergence between the three major con-
ferences
and COLING have historically met very infre-
quently in the same year, so those similarity scores
are plotted as points and not smoothed. The trend
across all three conferences is clear: each confer-
ence is not only increasing in breadth, but also in
similarity. In particular, EMNLP and ACL?s differ-
ences, once significant, are nearly erased.
7 Conclusion
Our method discovers a number of trends in the
field, such as the general increase in applications,
the steady decline in semantics, and its possible re-
versal. We also showed a convergence over time in
topic coverage of ACL, COLING, and EMNLP as
well an expansion of topic diversity. This growth
and convergence of the three conferences, perhaps
influenced by the need to increase recall (Church,
2005) seems to be leading toward a tripartite real-
ization of a single new ?latent? conference.
Acknowledgments
Many thanks to Bryan Gibson and Dragomir Radev
for providing us with the data behind the ACL An-
thology Network. Also to Sharon Goldwater and the
other members of the Stanford NLP Group as well
as project Mimir for helpful advice. Finally, many
thanks to the Office of the President, Stanford Uni-
versity, for partial funding.
References
Steven Bird. 2008. Association of Computational Lin-
guists Anthology. http://www.aclweb.org/anthology-
index/.
David Blei and John D. Lafferty. 2006. Dynamic topic
models. ICML.
David Blei, Andrew Ng, , and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
D. Blei, T. Gri, M. Jordan, and J. Tenenbaum. 2004. Hi-
erarchical topic models and the nested Chinese restau-
rant process.
Kenneth Church. 2005. Reviewing the reviewers. Com-
put. Linguist., 31(4):575?578.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In
ICML, pages 233?240. ACM.
Eugene Garfield. 1955. Citation indexes to science: A
new dimension in documentation through association
of ideas. Science, 122:108?111.
Tom L. Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. PNAS, 101 Suppl 1:5228?5235, April.
Yookyung Jo, Carl Lagoze, and C. Lee Giles. 2007.
Detecting research topics via the correlation between
graphs and texts. In KDD, pages 370?379, New York,
NY, USA. ACM.
Mark T. Joseph and Dragomir R. Radev. 2007. Citation
analysis, centrality, and the ACL anthology. Techni-
cal Report CSE-TR-535-07, University of Michigan.
Department of Electrical Engineering and Computer
Science.
Thomas S. Kuhn. 1962. The Structure of Scientific Rev-
olutions. University Of Chicago Press.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In ICML, pages 577?584, New York, NY, USA.
ACM.
Gideon S. Mann, David Mimno, and Andrew McCal-
lum. 2006. Bibliometric impact measures leveraging
topic analysis. In JCDL ?06: Proceedings of the 6th
ACM/IEEE-CS joint conference on Digital libraries,
pages 65?74, New York, NY, USA. ACM.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topical
trends. In KDD, pages 424?433, New York, NY, USA.
ACM.
371
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 248?256,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Labeled LDA: A supervised topic model for credit attribution in
multi-labeled corpora
Daniel Ramage, David Hall, Ramesh Nallapati and Christopher D. Manning
Computer Science Department
Stanford University
{dramage,dlwh,nmramesh,manning}@cs.stanford.edu
Abstract
A significant portion of the world?s text
is tagged by readers on social bookmark-
ing websites. Credit attribution is an in-
herent problem in these corpora because
most pages have multiple tags, but the tags
do not always apply with equal specificity
across the whole document. Solving the
credit attribution problem requires associ-
ating each word in a document with the
most appropriate tags and vice versa. This
paper introduces Labeled LDA, a topic
model that constrains Latent Dirichlet Al-
location by defining a one-to-one corre-
spondence between LDA?s latent topics
and user tags. This allows Labeled LDA to
directly learn word-tag correspondences.
We demonstrate Labeled LDA?s improved
expressiveness over traditional LDA with
visualizations of a corpus of tagged web
pages from del.icio.us. Labeled LDA out-
performs SVMs by more than 3 to 1 when
extracting tag-specific document snippets.
As a multi-label text classifier, our model
is competitive with a discriminative base-
line on a variety of datasets.
1 Introduction
From news sources such as Reuters to modern
community web portals like del.icio.us, a signif-
icant proportion of the world?s textual data is la-
beled with multiple human-provided tags. These
collections reflect the fact that documents are often
about more than one thing?for example, a news
story about a highway transportation bill might
naturally be filed under both transportation and
politics, with neither category acting as a clear
subset of the other. Similarly, a single web page
in del.icio.us might well be annotated with tags as
diverse as arts, physics, alaska, and beauty.
However, not all tags apply with equal speci-
ficity across the whole document, opening up new
opportunities for information retrieval and cor-
pus analysis on tagged corpora. For instance,
users who browse for documents with a particu-
lar tag might prefer to see summaries that focus
on the portion of the document most relevant to
the tag, a task we call tag-specific snippet extrac-
tion. And when a user browses to a particular
document, a tag-augmented user interface might
provide overview visualization cues highlighting
which portions of the document are more or less
relevant to the tag, helping the user quickly access
the information they seek.
One simple approach to these challenges can
be found in models that explicitly address the
credit attribution problem by associating individ-
ual words in a document with their most appropri-
ate labels. For instance, in our news story about
the transportation bill, if the model knew that the
word ?highway? went with transportation and that
the word ?politicians? went with politics, more
relevant passages could be extracted for either la-
bel. We seek an approach that can automatically
learn the posterior distribution of each word in a
document conditioned on the document?s label set.
One promising approach to the credit attribution
problem lies in the machinery of Latent Dirich-
let Allocation (LDA) (Blei et al, 2003), a recent
model that has gained popularity among theoreti-
cians and practitioners alike as a tool for automatic
corpus summarization and visualization. LDA is
a completely unsupervised algorithm that models
each document as a mixture of topics. The model
generates automatic summaries of topics in terms
of a discrete probability distribution over words
for each topic, and further infers per-document
discrete distributions over topics. Most impor-
tantly, LDA makes the explicit assumption that
each word is generated from one underlying topic.
Although LDA is expressive enough to model
248
multiple topics per document, it is not appropriate
for multi-labeled corpora because, as an unsuper-
vised model, it offers no obvious way of incorpo-
rating a supervised label set into its learning proce-
dure. In particular, LDA often learns some topics
that are hard to interpret, and the model provides
no tools for tuning the generated topics to suit an
end-use application, even when time and resources
exist to provide some document labels.
Several modifications of LDA to incorporate
supervision have been proposed in the literature.
Two such models, Supervised LDA (Blei and
McAuliffe, 2007) and DiscLDA (Lacoste-Julien
et al, 2008) are inappropriate for multiply labeled
corpora because they limit a document to being as-
sociated with only a single label. Supervised LDA
posits that a label is generated from each docu-
ment?s empirical topic mixture distribution. Dis-
cLDA associates a single categorical label variable
with each document and associates a topic mixture
with each label. A third model, MM-LDA (Ram-
age et al, 2009), is not constrained to one label
per document because it models each document as
a bag of words with a bag of labels, with topics for
each observation drawn from a shared topic dis-
tribution. But, like the other models, MM-LDA?s
learned topics do not correspond directly with the
label set. Consequently, these models fall short as
a solution to the credit attribution problem. Be-
cause labels have meaning to the people that as-
signed them, a simple solution to the credit attri-
bution problem is to assign a document?s words to
its labels rather than to a latent and possibly less
interpretable semantic space.
This paper presents Labeled LDA (L-LDA), a
generative model for multiply labeled corpora that
marries the multi-label supervision common to
modern text datasets with the word-assignment
ambiguity resolution of the LDA family of mod-
els. In contrast to standard LDA and its existing
supervised variants, our model associates each la-
bel with one topic in direct correspondence. In the
following section, L-LDA is shown to be a natu-
ral extension of both LDA (by incorporating su-
pervision) and Multinomial Naive Bayes (by in-
corporating a mixture model). We demonstrate
that L-LDA can go a long way toward solving the
credit attribution problem in multiply labeled doc-
uments with improved interpretability over LDA
(Section 4). We show that L-LDA?s credit attribu-
tion ability enables it to greatly outperform sup-
D
?
?
?
?
N
z
w
w
K
?
?
Figure 1: Graphical model of Labeled LDA: un-
like standard LDA, both the label set ? as well as
the topic prior ? influence the topic mixture ?.
port vector machines on a tag-driven snippet ex-
traction task on web pages from del.icio.us (Sec-
tion 6). And despite its generative semantics,
we show that Labeled LDA is competitive with
a strong baseline discriminative classifier on two
multi-label text classification tasks (Section 7).
2 Labeled LDA
Labeled LDA is a probabilistic graphical model
that describes a process for generating a labeled
document collection. Like Latent Dirichlet Allo-
cation, Labeled LDA models each document as a
mixture of underlying topics and generates each
word from one topic. Unlike LDA, L-LDA in-
corporates supervision by simply constraining the
topic model to use only those topics that corre-
spond to a document?s (observed) label set. The
model description that follows assumes the reader
is familiar with the basic LDA model (Blei et al,
2003).
Let each document d be represented by a tu-
ple consisting of a list of word indices w
(d)
=
(w
1
, . . . , w
N
d
) and a list of binary topic pres-
ence/absence indicators ?
(d)
= (l
1
, . . . , l
K
)
where eachw
i
? {1, . . . , V } and each l
k
? {0, 1}.
Here N
d
is the document length, V is the vocabu-
lary size and K the total number of unique labels
in the corpus.
We set the number of topics in Labeled LDA to
be the number of unique labels K in the corpus.
The generative process for the algorithm is found
in Table 1. Steps 1 and 2?drawing the multi-
nomial topic distributions over vocabulary ?
k
for
each topic k, from a Dirichlet prior ??remain
the same as for traditional LDA (see (Blei et al,
2003), page 4). The traditional LDA model then
draws a multinomial mixture distribution ?
(d)
over
allK topics, for each document d, from a Dirichlet
prior ?. However, we would like to restrict ?
(d)
to
be defined only over the topics that correspond to
249
1 For each topic k ? {1, . . . ,K}:
2 Generate ?
k
= (?
k,1
, . . . , ?
k,V
)
T
? Dir(?|?)
3 For each document d:
4 For each topic k ? {1, . . . ,K}
5 Generate ?
(d)
k
? {0, 1} ? Bernoulli(?|?
k
)
6 Generate ?
(d)
= L
(d)
??
7 Generate ?
(d)
= (?
l
1
, . . . , ?
l
M
d
)
T
? Dir(?|?
(d)
)
8 For each i in {1, . . . , N
d
}:
9 Generate z
i
? {?
(d)
1
, . . . , ?
(d)
M
d
} ? Mult(?|?
(d)
)
10 Generate w
i
? {1, . . . , V } ? Mult(?|?
z
i
)
Table 1: Generative process for Labeled LDA:
?
k
is a vector consisting of the parameters of the
multinomial distribution corresponding to the k
th
topic, ? are the parameters of the Dirichlet topic
prior and ? are the parameters of the word prior,
while ?
k
is the label prior for topic k. For the
meaning of the projection matrix L
(d)
, please re-
fer to Eq 1.
its labels ?
(d)
. Since the word-topic assignments
z
i
(see step 9 in Table 1) are drawn from this dis-
tribution, this restriction ensures that all the topic
assignments are limited to the document?s labels.
Towards this objective, we first generate the
document?s labels ?
(d)
using a Bernoulli coin toss
for each topic k, with a labeling prior probability
?
k
, as shown in step 5. Next, we define the vector
of document?s labels to be ?
(d)
= {k|?
(d)
k
= 1}.
This allows us to define a document-specific la-
bel projection matrix L
(d)
of size M
d
? K for
each document d, where M
d
= |?
(d)
|, as fol-
lows: For each row i ? {1, . . . ,M
d
} and column
j ? {1, . . . ,K} :
L
(d)
ij
=
{
1 if ?
(d)
i
= j
0 otherwise.
(1)
In other words, the i
th
row of L
(d)
has an entry of
1 in column j if and only if the i
th
document label
?
(d)
i
is equal to the topic j, and zero otherwise.
As the name indicates, we use the L
(d)
matrix to
project the parameter vector of the Dirichlet topic
prior ? = (?
1
, . . . , ?
K
)
T
to a lower dimensional
vector ?
(d)
as follows:
?
(d)
= L
(d)
?? = (?
?
(d)
1
, . . . , ?
?
(d)
M
d
)
T
(2)
Clearly, the dimensions of the projected vector
correspond to the topics represented by the labels
of the document. For example, suppose K = 4
and that a document d has labels given by ?
(d)
=
{0, 1, 1, 0}which implies ?
(d)
= {2, 3}, then L
(d)
would be:
(
0 1 0 0
0 0 1 0
)
.
Then, ?
(d)
is drawn from a Dirichlet distribution
with parameters ?
(d)
= L
(d)
? ? = (?
2
, ?
3
)
T
(i.e., with the Dirichlet restricted to the topics 2
and 3).
This fulfills our requirement that the docu-
ment?s topics are restricted to its own labels. The
projection step constitutes the deterministic step
6 in Table 1. The remaining part of the model
from steps 7 through 10 are the same as for reg-
ular LDA.
The dependency of ? on both ? and ? is in-
dicated by directed edges from ? and ? to ? in
the plate notation in Figure 1. This is the only ad-
ditional dependency we introduce in LDA?s repre-
sentation (please compare with Figure 1 in (Blei et
al., 2003)).
2.1 Learning and inference
In most applications discussed in this paper, we
will assume that the documents are multiply
tagged with human labels, both at learning and in-
ference time.
When the labels ?
(d)
of the document are ob-
served, the labeling prior ? is d-separated from
the rest of the model given ?
(d)
. Hence the model
is same as traditional LDA, except the constraint
that the topic prior ?
(d)
is now restricted to the
set of labeled topics ?
(d)
. Therefore, we can use
collapsed Gibbs sampling (Griffiths and Steyvers,
2004) for training where the sampling probability
for a topic for position i in a document d in La-
beled LDA is given by:
P (z
i
= j|z
?i
) ?
n
w
i
?i,j
+ ?
w
i
n
(?)
?i,j
+ ?
T
1
?
n
(d)
?i,j
+ ?
j
n
(d)
?i,?
+ ?
T
1
(3)
where n
w
i
?i,j
is the count of word w
i
in topic j, that
does not include the current assignment z
i
, a miss-
ing subscript or superscript (e.g. n
(?)
?i,j
)) indicates
a summation over that dimension, and 1 is a vector
of 1?s of appropriate dimension.
Although the equation above looks exactly the
same as that of LDA, we have an important dis-
tinction in that, the target topic j is restricted to
belong to the set of labels, i.e., j ? ?
(d)
.
Once the topic multinomials ? are learned from
the training set, one can perform inference on any
new labeled test document using Gibbs sampling
250
restricted to its tags, to determine its per-word la-
bel assignments z. In addition, one can also com-
pute its posterior distribution ? over topics by ap-
propriately normalizing the topic assignments z.
It should now be apparent to the reader how
the new model addresses some of the problems in
multi-labeled corpora that we highlighted in Sec-
tion 1. For example, since there is a one-to-one
correspondence between the labels and topics, the
model can display automatic topical summaries
for each label k in terms of the topic-specific dis-
tribution ?
k
. Similarly, since the model assigns a
label z
i
to each word w
i
in the document d au-
tomatically, we can now extract portions of the
document relevant to each label k (it would be all
words w
i
? w
(d)
such that z
i
= k). In addition,
we can use the topic distribution ?
(d)
to rank the
user specified labels in the order of their relevance
to the document, thereby also eliminating spurious
ones if necessary.
Finally, we note that other less restrictive vari-
ants of the proposed L-LDA model are possible.
For example, one could consider a version that
allows topics that do not correspond to the label
set of a given document with a small probability,
or one that allows a common background topic in
all documents. We did implement these variants
in our preliminary experiments, but they did not
yield better performance than L-LDA in the tasks
we considered. Hence we do not report them in
this paper.
2.2 Relationship to Naive Bayes
The derivation of the algorithm so far has fo-
cused on its relationship to LDA. However, La-
beled LDA can also be seen as an extension of
the event model of a traditional Multinomial Naive
Bayes classifier (McCallum and Nigam, 1998) by
the introduction of a mixture model. In this sec-
tion, we develop the analogy as another way to
understand L-LDA from a supervised perspective.
Consider the case where no document in the
collection is assigned two or more labels. Now
for a particular document d with label l
d
, Labeled
LDA draws each word?s topic variable z
i
from a
multinomial constrained to the document?s label
set, i.e. z
i
= l
d
for each word position i in the doc-
ument. During learning, the Gibbs sampler will
assign each z
i
to l
d
while incrementing ?
l
d
(w
i
),
effectively counting the occurences of each word
type in documents labeled with l
d
. Thus in the
singly labeled document case, the probability of
each document under Labeled LDA is equal to
the probability of the document under the Multi-
nomial Naive Bayes event model trained on those
same document instances. Unlike the Multino-
mial Naive Bayes classifier, Labeled LDA does
not encode a decision boundary for unlabeled doc-
uments by comparing P (w
(d)
|l
d
) to P (w
(d)
|?l
d
),
although we discuss using Labeled LDA for multi-
label classification in Section 7.
Labeled LDA?s similarity to Naive Bayes ends
with the introduction of a second label to any doc-
ument. In a traditional one-versus-rest Multino-
mial Naive Bayes model, a separate classifier for
each label would be trained on all documents with
that label, so each word can contribute a count
of 1 to every observed label?s word distribution.
By contrast, Labeled LDA assumes that each doc-
ument is a mixture of underlying topics, so the
count mass of single word instance must instead be
distributed over the document?s observed labels.
3 Credit attribution within tagged
documents
Social bookmarking websites contain millions of
tags describing many of the web?s most popu-
lar and useful pages. However, not all tags are
uniformly appropriate at all places within a doc-
ument. In the sections that follow, we examine
mechanisms by which Labeled LDA?s credit as-
signment mechanism can be utilized to help sup-
port browsing and summarizing tagged document
collections.
To create a consistent dataset for experimenting
with our model, we selected 20 tags of medium
to high frequency from a collection of documents
dataset crawled from del.icio.us, a popular so-
cial bookmarking website (Heymann et al, 2008).
From that larger dataset, we selected uniformly at
random four thousand documents that contained
at least one of the 20 tags, and then filtered each
document?s tag set by removing tags not present
in our tag set. After filtering, the resulting cor-
pus averaged 781 non-stop words per document,
with each document having 4 distinct tags on aver-
age. In contrast to many existing text datasets, our
tagged corpus is highly multiply labeled: almost
90% of of the documents have more than one tag.
(For comparison, less than one third of the news
documents in the popular RCV1-v2 collection of
newswire are multiply labeled). We will refer to
251
this collection of data as the del.icio.us tag dataset.
4 Topic Visualization
A first question we ask of Labeled LDA is how its
topics compare with those learned by traditional
LDA on the same collection of documents. We ran
our implementations of Labeled LDA and LDA
on the del.icio.us corpus described above. Both
are based on the standard collapsed Gibbs sam-
pler, with the constraints for Labeled LDA imple-
mented as in Section 2.
web search site blog css content google list page posted great work comments read nice post great april blog march june wordpress
book image pdf review library posted read copyright books title
web
boo
ks
scie
nce
com
p
ute
r
reli
gion
java
cult
ure
works water map human life work science time world years sleep
windows file version linux comp-uter free system software mac
comment god jesus people gospel bible reply lord religion written
applications spring open web java pattern eclipse development ajax
people day link posted time com-ments back music jane permalink
news information service web on-line project site free search home
web images design content java css website articles page learning
jun quote pro views added check anonymous card core power ghz
life written jesus words made man called mark john person fact name
8
house light radio media photo-graphy news music travel cover
game review street public art health food city history science
13
19
4
3
2
12
Tag (Labeled LDA) (LDA) Topic ID
Figure 2: Comparison of some of the 20 topics
learned on del.icio.us by Labeled LDA (left) and
traditional LDA (right), with representative words
for each topic shown in the boxes. Labeled LDA?s
topics are named by their associated tag. Arrows
from right-to-left show the mapping of LDA topics
to the closest Labeled LDA topic by cosine simi-
larity. Tags not shown are: design, education, en-
glish, grammar, history, internet, language, phi-
losophy, politics, programming, reference, style,
writing.
Figure 2 shows the top words associated with
20 topics learned by Labeled LDA and 20 topics
learned by unsupervised LDA on the del.icio.us
document collection. Labeled LDA?s topics are
directly named with the tag that corresponds to
each topic, an improvement over standard prac-
tice of inferring the topic name by inspection (Mei
et al, 2007). The topics learned by the unsu-
pervised variant were matched to a Labeled LDA
topic highest cosine similarity.
The topics selected are representative: com-
pared to Labeled LDA, unmodified LDA allocates
many topics for describing the largest parts of the
The Elements of Style , William Strunk , Jr.
Asserting that one must first know the rules to break them, this 
classic reference book is a must-have for any student and 
conscientious writer.  Intended for use in which the practice of
composition is combined with the study of literature, it gives in
brief space the principal requirements of plain English style and
concentratesattention on the rules of usage and principles of
composition most commonly violated.
Figure 3: Example document with important
words annotated with four of the page?s tags as
learned by Labeled LDA. Red (single underline)
is style, green (dashed underline) grammar, blue
(double underline) reference, and black (jagged
underline) education.
corpus and under-represents tags that are less un-
common: of the 20 topics learned, LDA learned
multiple topics mapping to each of five tags (web,
culture, and computer, reference, and politics, all
of which were common in the dataset) and learned
no topics that aligned with six tags (books, english,
science, history, grammar, java, and philosophy,
which were rarer).
5 Tagged document visualization
In addition to providing automatic summaries of
the words best associated with each tag in the cor-
pus, Labeled LDA?s credit attribution mechanism
can be used to augment the view of a single doc-
ument with rich contextual information about the
document?s tags.
Figure 3 shows one web document from the col-
lection, a page describing a guide to writing En-
glish prose. The 10 most common tags for that
document are writing, reference, english, gram-
mar, style, language, books, book, strunk, and ed-
ucation, the first eight of which were included in
our set of 20 tags. In the figure, each word that has
high posterior probability from one tag has been
annotated with that tag. The red words come from
the style tag, green from the grammar tag, blue
from the reference tag, and black from the educa-
tion tag. In this case, the model does very well at
assigning individual words to the tags that, subjec-
tively, seem to strongly imply the presence of that
tag on this page. A more polished rendering could
add subtle visual cues about which parts of a page
are most appropriate for a particular set of tags.
252
books
L-LDA this classic reference book is a must-have for any
student and conscientious writer. Intended for
SVM the rules of usage and principles of composition
most commonly violated. Search: CONTENTS Bibli-
ographic
language
L-LDA the beginning of a sentence must refer to the gram-
matical subject 8. Divide words at
SVM combined with the study of literature, it gives in brief
space the principal requirements of
grammar
L-LDA requirements of plain English style and concen-
trates attention on the rules of usage and principles of
SVM them, this classic reference book is a must-have for
any student and conscientious writer.
Figure 4: Representative snippets extracted by
L-LDA and tag-specific SVMs for the web page
shown in Figure 3.
6 Snippet Extraction
Another natural application of Labeled LDA?s
credit assignment mechanism is as a means of se-
lecting snippets of a document that best describe
its contents from the perspective of a particular
tag. Consider again the document in Figure 3. In-
tuitively, if this document were shown to a user
interested in the tag grammar, the most appropri-
ate snippet of words might prefer to contain the
phrase ?rules of usage,? whereas a user interested
in the term style might prefer the title ?Elements
of Style.?
To quantitatively evaluate Labeled LDA?s per-
formance at this task, we constructed a set of 29
recently tagged documents from del.icio.us that
were labeled with two or more tags from the 20 tag
subset, resulting in a total of 149 (document,tag)
pairs. For each pair, we extracted a 15-word win-
dow with the highest tag-specific score from the
document. Two systems were used to score each
window: Labeled LDA and a collection of one-
vs-rest SVMs trained for each tag in the system.
L-LDA scored each window as the expected prob-
ability that the tag had generated each word. For
SVMs, each window was taken as its own doc-
ument and scored using the tag-specific SVM?s
un-thresholded scoring function, taking the win-
dow with the most positive score. While a com-
plete solution to the tag-specific snippet extraction
Model Best Snippet Unanimous
L-LDA 72 / 149 24 / 51
SVM 21 / 149 2 / 51
Table 2: Human judgments of tag-specific snippet
quality as extracted by L-LDA and SVM. The cen-
ter column is the number of document-tag pairs for
which a system?s snippet was judged superior. The
right column is the number of snippets for which
all three annotators were in complete agreement
(numerator) in the subset of document scored by
all three annotators (denominator).
problem might be more informed by better lin-
guistic features (such as phrase boundaries), this
experimental setup suffices to evaluate both kinds
of models for their ability to appropriately assign
words to underlying labels.
Figure 3 shows some example snippets output
by our system for this document. Note that while
SVMs did manage to select snippets that were
vaguely on topic, Labeled LDA?s outputs are gen-
erally of superior subjective quality. To quantify
this intuition, three human annotators rated each
pair of snippets. The outputs were randomly la-
beled as ?System A? or ?System B,? and the anno-
tators were asked to judge which system generated
a better tag-specific document subset. The judges
were also allowed to select neither system if there
was no clear winner. The results are summarized
in Table 2.
L-LDA was judged superior by a wide margin:
of the 149 judgments, L-LDA?s output was se-
lected as preferable in 72 cases, whereas SVM?s
was selected in only 21. The difference between
these scores was highly significant (p < .001) by
the sign test. To quantify the reliability of the judg-
ments, 51 of the 149 document-tag pairs were la-
beled by all three annotators. In this group, the
judgments were in substantial agreement,
1
with
Fleiss? Kappa at .63.
Further analysis of the triply-annotated sub-
set yields further evidence of L-LDA?s advantage
over SVM?s: 33 of the 51 were tag-page pairs
where L-LDA?s output was picked by at least one
annotator as a better snippet (although L-LDA
might not have been picked by the other annota-
tors). And of those, 24 were unanimous in that
1
Of the 15 judgments that were in contention, only two
conflicted on which system was superior (L-LDA versus
SVM); the remaining disagreements were about whether or
not one of the systems was a clear winner.
253
all three judges selected L-LDA?s output. By con-
trast, only 10 of the 51 were tag-page pairs where
SVMs? output was picked by at least one anno-
tator, and of those, only 2 were selected unani-
mously.
7 Multilabeled Text Classification
In the preceding section we demonstrated how La-
beled LDA?s credit attribution mechanism enabled
effective modeling within documents. In this sec-
tion, we consider whether L-LDA can be adapted
as an effective multi-label classifier for documents
as a whole. To answer that question, we applied
a modified variant of L-LDA to a multi-label doc-
ument classification problem: given a training set
consisting of documents with multiple labels, pre-
dict the set of labels appropriate for each docu-
ment in a test set.
Multi-label classification is a well researched
problem. Many modern approaches incorporate
label correlations (e.g., Kazawa et al (2004), Ji
et al (2008)). Others, like our algorithm are
based on mixture models (such as Ueda and Saito
(2003)). However, we are aware of no methods
that trade off label-specific word distributions with
document-specific label distributions in quite the
same way.
In Section 2, we discussed learning and infer-
ence when labels are observed. In the task of mul-
tilabel classification, labels are available at train-
ing time, so the learning part remains the same as
discussed before. However, inferring the best set
of labels for an unlabeled document at test time is
more complex: it involves assessing all label as-
signments and returning the assignment that has
the highest posterior probability. However, this
is not straight-forward, since there are 2
K
possi-
ble label assignments. To make matters worse, the
support of ?(?
(d)
) is different for different label
assignments. Although we are in the process of
developing an efficient sampling algorithm for this
inference, for the purposes of this paper we make
the simplifying assumption that the model reduces
to standard LDA at inference, where the document
is free to sample from any of the K topics. This
is a reasonable assumption because allowing the
model to explore the whole topic space for each
document is similar to exploring all possible label
assignments. The document?s most likely labels
can then be inferred by suitably thresholding its
posterior probability over topics.
As a baseline, we use a set of multiple one-vs-
rest SVM classifiers which is a popular and ex-
tremely competitive baseline used by most previ-
ous papers (see (Kazawa et al, 2004; Ueda and
Saito, 2003) for instance). We scored each model
based on Micro-F1 and Macro-F1 as our evalua-
tion measures (Lewis et al, 2004). While the for-
mer allows larger classes to dominate its results,
the latter assigns an equal weight to all classes,
providing us complementary information.
7.1 Yahoo
We ran experiments on a corpus from the Yahoo
directory, modeling our experimental conditions
on the ones described in (Ji et al, 2008).
2
We
considered documents drawn from 8 top level cat-
egories in the Yahoo directory, where each doc-
ument can be placed in any number of subcate-
gories. The results were mixed, with SVMs ahead
on one measure: Labeled LDA beat SVMs on five
out of eight datasets on MacroF1, but didn?t win
on any datasets on MicroF1. Results are presented
in Table 3.
Because only a processed form of the docu-
ments was released, the Yahoo dataset does not
lend itself well to error analysis. However, only
33% of the documents in each top-level category
were applied to more than one sub-category, so the
credit assignment machinery of L-LDA was un-
used for the majority of documents. We there-
fore ran an artificial second set of experiments
considering only those documents that had been
given more than one label in the training data. On
these documents, the results were again mixed, but
Labeled LDA comes out ahead. For MacroF1,
L-LDA beat SVMs on four datasets, SVMs beat
L-LDA on one dataset, and three were a statistical
tie.
3
On MicroF1, L-LDA did much better than on
the larger subset, outperforming on four datasets
with the other four a statistical tie.
It is worth noting that the Yahoo datasets are
skewed by construction to contain many docu-
ments with highly overlapping content: because
each collection is within the same super-class such
as ?Arts?, ?Business?, etc., each sub-categories?
2
We did not carefully tune per-class thresholds of each of
the one vs. rest classifiers in each model, but instead tuned
only one threshold for all classifiers in each model via cross-
validation on the Arts subsets. As such, our numbers were on
an average 3-4% less than those reported in (Ji et al, 2008),
but the methods were comparably tuned.
3
The difference between means of multiple runs were not
significantly different by two-tailed paired t-test.
254
Dataset %MacroF1 %MicroF1
L-LDA SVM L-LDA SVM
Arts 30.70(1.62) 23.23 (0.67) 39.81(1.85) 48.42 (0.45)
Business 30.81(0.75) 22.82 (1.60) 67.00(1.29) 72.15 (0.62)
Computers 27.55(1.98) 18.29 (1.53) 48.95(0.76) 61.97 (0.54)
Education 33.78(1.70) 36.03 (1.30) 41.19(1.48) 59.45 (0.56)
Entertainment 39.42(1.38) 43.22 (0.49) 47.71(0.61) 62.89 (0.50)
Health 45.36(2.00) 47.86 (1.72) 58.13(0.43) 72.21 (0.26)
Recreation 37.63(1.00) 33.77 (1.17) 43.71(0.31) 59.15 (0.71)
Society 27.32(1.24) 23.89 (0.74) 42.98(0.28) 52.29 (0.67)
Table 3: Averaged performance across ten runs of multi-label text classification for predicting subsets
of the named Yahoo directory categories. Numbers in parentheses are standard deviations across runs.
L-LDA outperforms SVMs on 5 subsets with MacroF1, but on no subsets with MicroF1.
vocabularies will naturally overlap a great deal.
L-LDA?s credit attribution mechanism is most ef-
fective at partitioning semantically distinct words
into their respective label vocabularies, so we ex-
pect that Labeled-LDA?s performance as a text
classifier would improve on collections with more
semantically diverse labels.
7.2 Tagged Web Pages
We also applied our method to text classification
on the del.icio.us dataset, where the documents are
naturally multiply labeled (more than 89%) and
where the tags are less inherently similar than in
the Yahoo subcategories. Therefore we expect La-
beled LDA to do better credit assignment on this
subset and consequently to show improved perfor-
mance as a classifier, and indeed this is the case.
We evaluated L-LDA and multiple one-vs-rest
SVMs on 4000 documents with the 20 tag sub-
set described in Section 3. L-LDA and multiple
one-vs-rest SVMs were trained on the first 80% of
documents and evaluated on the remaining 20%,
with results averaged across 10 random permuta-
tions of the dataset. The results are shown in Ta-
ble 4. We tuned the SVMs? shared cost parameter
C(= 10.0) and selected raw term frequency over
tf-idf weighting based on 4-fold cross-validation
on 3,000 documents drawn from an independent
permutation of the data. For L-LDA, we tuned the
shared parameters of threshold and proportional-
ity constants in word and topic priors. L-LDA and
SVM have very similar performance on MacroF1,
while L-LDA substantially outperforms on Mi-
croF1. In both cases, L-LDA?s improvement is
statistically significantly by a 2-tailed paired t-test
at 95% confidence.
Model %MacroF1 %MicroF1
L-LDA 39.85 (.989) 52.12 (.434)
SVM 39.00 (.423) 39.33 (.574)
Table 4: Mean performance across ten runs of
multi-label text classification for predicting 20
tags on del.icio.us data. L-LDA outperforms
SVMs significantly on both metrics by a 2-tailed,
paired t-test at 95% confidence.
8 Discussion
One of the main advantages of L-LDA on mul-
tiply labeled documents comes from the model?s
document-specific topic mixture ?. By explicitly
modeling the importance of each label in the doc-
ument, Labeled LDA can effective perform some
contextual word sense disambiguation, which sug-
gests why L-LDA can outperform SVMs on the
del.icio.us dataset.
As a concrete example, consider the excerpt
of text from the del.icio.us dataset in Figure 5.
The document itself has several tags, including
design and programming. Initially, many of the
likelihood probabilities p(w|label) for the (con-
tent) words in this excerpt are higher for the label
programming than design, including ?content?,
?client?, ?CMS? and even ?designed?, while de-
sign has higher likelihoods for just ?website? and
?happy?. However, after performing inference on
this document using L-LDA, the inferred docu-
ment probability for design (p(design)) is much
higher than it is for programming. In fact, the
higher probability for the tag more than makes up
the difference in the likelihood for all the words
except ?CMS? (Content Management System), so
255
The website is designed, CMS works, content has been added and the client is happy.
The website is designed, CMS works, content has been added and the client is happy.
Before Inference
After Inference
Figure 5: The effect of tag mixture proportions for credit assignment in a web document. Blue (single
underline) words are generated from the design tag; red (dashed underline) from the programming tag.
By themselves, most words used here have a higher probability in programming than in design. But
because the document as a whole is more about design than programming(incorporating words not shown
here), inferring the document?s topic-mixture ? enables L-LDA to correctly re-assign most words.
that L-LDA correctly infers that most of the words
in this passage have more to do with design than
programming.
9 Conclusion
This paper has introduced Labeled LDA, a novel
model of multi-labeled corpora that directly ad-
dresses the credit assignment problem. The new
model improves upon LDA for labeled corpora
by gracefully incorporating user supervision in the
form of a one-to-one mapping between topics and
labels. We demonstrate the model?s effectiveness
on tasks related to credit attribution within docu-
ments, including document visualizations and tag-
specific snippet extraction. An approximation to
Labeled LDA is also shown to be competitive with
a strong baseline (multiple one vs-rest SVMs) for
multi-label classification.
Because Labeled LDA is a graphical model
in the LDA family, it enables a range of natu-
ral extensions for future investigation. For exam-
ple, the current model does not capture correla-
tions between labels, but such correlations might
be introduced by composing Labeled LDA with
newer state of the art topic models like the Cor-
related Topic Model (Blei and Lafferty, 2006) or
the Pachinko Allocation Model (Li and McCal-
lum, 2006). And with improved inference for un-
supervised ?, Labeled LDA lends itself naturally
to modeling semi-supervised corpora where labels
are observed for only some documents.
Acknowledgments
This project was supported in part by the Presi-
dent of Stanford University through the IRiSS Ini-
tiatives Assessment project.
References
D. M. Blei and J. Lafferty. 2006. Correlated Topic
Models. NIPS, 18:147.
D. Blei and J McAuliffe. 2007. Supervised Topic
Models. In NIPS, volume 21.
D. M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. JMLR.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. PNAS, 1:5228?35.
P. Heymann, G. Koutrika, and H. Garcia-Molina. 2008.
Can social bookmarking improve web search. In
WSDM.
S. Ji, L. Tang, S. Yu, and J. Ye. 2008. Extracting
shared subspace for multi-label classification. In
KDD, pages 381?389, New York, NY, USA. ACM.
H. Kazawa, H. Taira T. Izumitani, and E. Maeda. 2004.
Maximal margin labeling for multi-topic text catego-
rization. In NIPS.
S. Lacoste-Julien, F. Sha, and M. I. Jordan. 2008. Dis-
cLDA: Discriminative learning for dimensionality
reduction and classification. In NIPS, volume 22.
D. D. Lewis, Y. Yang, T. G. Rose, G. Dietterich, F. Li,
and F. Li. 2004. RCV1: A new benchmark collec-
tion for text categorization research. JMLR, 5:361?
397.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In International conference on Machine
learning, pages 577?584.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
AAAI-98 workshop on learning for text categoriza-
tion, volume 7.
Q. Mei, X. Shen, and C Zhai. 2007. Automatic label-
ing of multinomial topic models. In KDD.
D. Ramage, P. Heymann, C. D. Manning, and
H. Garcia-Molina. 2009. Clustering the tagged web.
In WSDM.
N. Ueda and K. Saito. 2003. Parametric mixture mod-
els for multi-labeled text includes models that can be
seen to fit within a dimensionality reduction frame-
work. In NIPS.
256
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 344?354,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Large-Scale Cognate Recovery
David Hall and Dan Klein
Computer Science Division
University of California at Berkeley
{dlwh,klein}@cs.berkeley.edu
Abstract
We present a system for the large scale in-
duction of cognate groups. Our model ex-
plains the evolution of cognates as a sequence
of mutations and innovations along a phy-
logeny. On the task of identifying cognates
from over 21,000 words in 218 different lan-
guages from the Oceanic language family, our
model achieves a cluster purity score over
91%, while maintaining pairwise recall over
62%.
1 Introduction
The critical first step in the reconstruction of an
ancient language is the recovery of related cog-
nate words in its descendants. Unfortunately, this
process has largely been a manual, linguistically-
intensive undertaking for any sizable number of de-
scendant languages. The traditional approach used
by linguists?the comparative method?iterates be-
tween positing putative cognates and then identify-
ing regular sound laws that explain correspondences
between those words (Bloomfield, 1938).
Successful computational approaches have been
developed for large-scale reconstruction of phyloge-
nies (Ringe et al, 2002; Daume? III and Campbell,
2007; Daume? III, 2009; Nerbonne, 2010) and an-
cestral word forms of known cognate sets (Oakes,
2000; Bouchard-Co?te? et al, 2007; Bouchard-Co?te?
et al, 2009), enabling linguists to explore deep his-
torical relationships in an automated fashion. How-
ever, computational approaches thus far have not
been able to offer the same kind of scale for iden-
tifying cognates. Previous work in cognate identi-
fication has largely focused on identifying cognates
in pairs of languages (Mann and Yarowsky, 2001;
Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak,
2001; Mulloni, 2007), with a few recent exceptions
that can find sets in a handful of languages (Bergsma
and Kondrak, 2007; Hall and Klein, 2010).
While it may seem surprising that cognate de-
tection has not successfully scaled to large num-
bers of languages, the task poses challenges not
seen in reconstruction and phylogeny inference. For
instance, morphological innovations and irregular
sound changes can completely obscure relationships
between words in different languages. However, in
the case of reconstruction, an unexplainable word is
simply that: one can still correctly reconstruct its an-
cestor using words from related languages.
In this paper, we present a system that uses two
generative models for large-scale cognate identi-
fication. Both models describe the evolution of
words along a phylogeny according to automatically
learned sound laws in the form of parametric edit
distances. The first is an adaptation of the genera-
tive model of Hall and Klein (2010), and the other
is a new generative model called PARSIM with con-
nections to parsimony methods in computational bi-
ology (Cavalli-Sforza and Edwards, 1965; Fitch,
1971). Our model supports simple, tractable infer-
ence via message passing, at the expense of being
unable to model some cognacy relationships. To
help correct this deficiency, we also describe an ag-
glomerative inference procedure for the model of
Hall and Klein (2010). By using the output of our
system as input to this system, we can find cognate
groups that PARSIM alone cannot recover.
We apply these models to identifying cognate
groups from two language families using the Aus-
tronesian Basic Vocabulary Database (Greenhill et
al., 2008), a catalog of words from about 40% of
the Austronesian languages. We focus on data from
two subfamilies of Austronesian: Formosan and
344
Oceanic. The datasets are by far the largest on
which automated cognate recovery has ever been at-
tempted, with 18 and 271 languages respectively.
On the larger Oceanic data, our model can achieve
cluster purity scores of 91.8%, while maintaining
pairwise recall of 62.1%. We also analyze the mis-
takes of our system, where we find that some of the
erroneous cognate groups our system finds may not
be errors at all. Instead, they may be previously
unknown cognacy relationships that were not anno-
tated in the data.
2 Background
Before we present our model, we first describe ba-
sic facts of the Austronesian language family, along
with a description of the Austronesian Basic Vocab-
ulary Database, which forms the dataset that we use
for our experiments. For far more detailed coverage
of the Austronesian languages, we direct the inter-
ested reader to Blust (2009)?s comprehensive mono-
graph.
2.1 The Austronesian Language Family
The Austronesian language family is one of the
largest in the world, comprising about one-fifth of
the world?s languages. Geographically, it stretches
from its homeland on Formosa (Taiwan) to Mada-
gascar in the west, and as far as Hawai?i and (at one
point) the Easter Islands to the east. Until the ad-
vent of European colonialism spread Indo-European
languages to every continent, Austronesian was the
most widespread of all language families.
Linguistically, the language family is as diverse
as it is large, but a few regularities hold. From
a phonological perspective, two features stand out.
First, the phoneme inventories of these languages
are typically small. For example, it is well-known
that Hawaiian has only 13 phonemes. Moreover, the
phonotactics of these languages are often restrictive.
Sticking with the same example, Hawaiian only al-
lows (C)V syllables: consonants clusters are forbid-
den, and no syllable may end with a consonant.
2.2 The Austronesian Basic Vocabulary
Database
The Austronesian Basic Vocabulary Database
(ABVD) (Greenhill et al, 2008) is an ambitious, on-
going effort to catalog the lexicons and basic facts
about all of the languages in the Austronesian lan-
guage family. It also contains manual reconstruc-
tions for select ancestor languages produced by lin-
guists.
The sample we use?from Bouchard-Co?te? et al
(2009)?contains about 50,000 words across 471
languages spanning all the major divisions of Aus-
tronesian. These words are grouped into cognate
groups and arranged by gloss. For instance, there are
37 distinct cognate groups for the gloss ?tail.? One
of these groups includes the words /ekor/, /ingko/,
/iNkot/, /kiikiPu/, and /PiPina/, among others. Most
of these words have been transcribed into the Inter-
national Phonetic Alphabet, though it appears that
some words are transcribed using the Roman alpha-
bet. For instance, the second word in the example is
likely /iNko/, which is a much more likely sequence
than what is transcribed.
In this sample, there are 6307 such cognate
groups and 210 distinct glosses. The data is
somewhat sparse: fewer than 50% of the possible
gloss/language pairs are present. Moreover, there is
some amount of homoplasy?that is, languages with
a word from more than one cognate group for a given
gloss.
Finally, it is important to note that the ABVD is
still a work in progress: they have data from only
50% of extant Austronesian languages.
2.3 Subfamilies of Austronesian
In this paper we focus on two branches of the Aus-
tronesian language family, one as a development set
and one as a test set. For our development set, we
use the Formosan branch. The languages in this
group are exclusively found on the Austronesian
homeland of Formosa. The family encompasses a
substantial portion of the linguistic diversity of Aus-
tronesian: Blust (2009) argues that Formosan con-
tains 9 of the 10 first-order splits of the Austrone-
sian family. Formosan?s diversity is surprising since
it contains a mere 18 languages. Thus, Formosan is
a smaller development set that nevertheless is repre-
sentative of larger families.
For our final test set, we use the Oceanic sub-
family, which includes almost 50% of the languages
in the Austronesian family, meaning that it repre-
sents around 10% of all languages in the world.
Oceanic also represents a large fraction of the ge-
345
ographic diversity of Austronesian, stretching from
New Zealand in the south to Hawai?i in the north.
Our sample includes 21863 words from 218 lan-
guages in the Oceanic family.
3 Models
In this section we describe two models, one based on
Hall and Klein (2010)?which we call HK10?and
another new model that shares some connection to
parsimony methods in computational biology, which
we call PARSIM. Both are generative models that
describe the evolution of words w` from a set of lan-
guages {`} in a cognate group g along a fixed phy-
logeny T .1 Each cognate group and word is also
associated with a gloss or meaning m, which we as-
sume to be fixed.2 In both models, words evolve
according to regular sound laws ?`, which are spe-
cific to each language. Also, both models will make
use of a language model ?, which is used for gen-
erating words that are not dependent on the word in
the parent language. (We leave ?` and ? as abstract
parameters for now. We will describe them in sub-
sequent sections.)
3.1 HK10
The first model we describe is a small modification
of the phylogenetic model of Hall and Klein (2010).
In HK10, there is an unknown number of cognate
groups G where each cognate group g consists of a
set of words {wg,`}. In each cognate group, words
evolve along a phylogeny, where each word in a lan-
guage is the result of that word evolving from its
parent according to regular sound laws. To model
the fact that not all languages have a cognate in
each group, each language in the tree has an asso-
ciated ?survival? variable Sg,`, where a word may
be lost on that branch (and its descendants) instead
of evolving. Once the words are generated, they are
then ?permuted? so that the cognacy relationships
1Both of these models therefore are insensitive to geo-
graphic and historical factors that cannot be easily approxi-
mated by this tree. See Nichols (1992) for an excellent dis-
cussion of these factors.
2One could easily envision allowing the meaning of a word
to change as well. Modeling this semantic drift has been consid-
ered by Kondrak (2001). In the ABVD, however, any semantic
drift has already been elided, since the database has coarsened
glosses to the extent that there is no meaningful way to model
semantic drift given our data.
M
G
W
For
W
Ata
? ? ?
S
For
S
Ata
S
Pai
S
Squ
S
Ciu
w
pt
w
es
L 
?
w
Pai
w
Pai
w
Pai
w
Pai
w
Pai
w
Pai
W
Pai
W
Pai
S
u
r
v
i
v
a
l
E
v
o
l
u
t
i
o
n
P
e
r
m
u
t
a
t
i
o
n
(a)
I
Ata
I
Pai
I
Squ
I
Ciu
W
Ata
W
Squ
W
Ciu
W
Pai
(b)
W
For
?
?
?
?
Figure 1: Plate diagrams for (a) HK10 (Hall and Klein,
2010) and (b) PARSIM, our new parsimony model, for
a small set of languages. In HK10, words are generated
following a phylogenetic tree according to sound laws ?,
and then ?scrambled? with a permutation pi so that the
original cognate groups are lost. In PARSIM, all words
for each of the M glosses are generated in a single tree,
with innovations I starting new cognate groups. The
languages depicted are Formosan (For), Paiwan (Pai),
Atayalic (Ata), Ciuli Atayalic (Ciu), and Squliq Atayalic
(Squ).
346
are obscured. The task of inference then is to re-
cover the original cognate groups.
The generative process for their model is as fol-
lows:
? For each cognate group g, choose a root word
Wroot ? p(W |?), a language model over
words.
? For each language ` in a pre-order traversal of
the phylogeny:
1. Choose S` ? Bernoulli(?`), indicating
whether or not the word survives.
2. If the word survives, choose W` ?
p(W |?`,Wpar(`)).
3. Otherwise, stop generating words in that
language and its descendants.
? For each language, choose a random permuta-
tion pi of the observed data, and rearrange the
cognates according to this permutation.
We reproduce the graphical model for HK10 for a
small phylogeny in Figure 1a.
Inference in this model is intractable; to perform
inference exactly, one has to reason over all parti-
tions of the data into cognate groups. To address
this problem, Hall and Klein (2010) propose an it-
erative bipartite matching scheme where one lan-
guage is held out from the others, and then words
are assigned to the remaining groups to maximize
the probability of the attachment. That is, for some
language ` and fixed assignments pi?` for the other
languages, they seek an assignment pi` that maxi-
mizes:
pi? = argmax
pi
?
g
log p(w(`,pi`(g))|?, pi,w?`)
Unfortunately, while this approach was effective
with only a few languages (they tested on three), this
algorithm cannot scale to the eighteen languages in
Formosan, let alne the hundreds of languages in
Oceanic. Therefore, we make two simple modifi-
cations. First, we restrict the cognate assignments
to stay within a gloss. Thus, there are many fewer
potential matchings to consider. Second, we use an
agglomerative inference procedure, which greedily
merges cognate groups that result in the greatest gain
in likelihood. That is, for all pairs of cognate groups
ga with words wa and gb with words wb, we com-
pute the score:
log p(wa?b|?)? log p(wa|?)? log p(wb|?)
This score is the difference between the log proba-
bility of generating two cognate groups jointly and
generating them separately. We then merge the two
that generate the highest gain in likelihood. Like
the iterative bipartite matching algorithm described
above, this algorithm is not exact. However, it is
O(n2 log n) (where n is the size of the largest gloss,
which for Oceanic is 153), while the bipartite match-
ing algorithm is O(n3) (Kuhn, 1955).
Actually, the original HK10 is doubly intractable.
They use weighted automata to represent distribu-
tions over strings, but these automata?particularly
if they are non-deterministic?make inference in
any non-trivial graphical model intractable. We dis-
cuss this issue in more detail in Section 6.
3.2 A Parsimony-Inspired Model
We now describe a new model called PARSIM that
supports exact inference tractably, though it sacri-
fices some of the expressive power of HK10. In
our model, each language has at most one word for
each gloss, and this one word changes from one
language to its children according to some edge-
specific Markov process. These changes may either
be mutations, which merely change the surface form
of the word, or innovations, which start a new word
in a new cognate group that is unrelated to the previ-
ous word. Mutations take the form of a conditional
edit operation that models insertions, substitutions,
and deletions that correspond to regular (and, with
lower probability, irregular) sound changes that are
likely to occur between a language and its parent.
Innovations, on the other hand, are generated from a
language model independent of the parent?s word.
Specifically, our generative process takes the fol-
lowing form:
? For each gloss m, choose a root word Wroot ?
?, a language model over words.
? For each language ` in a pre-order traversal of
the phylogeny:
347
malapzo mappo
Ciuli Squliq
purrok
Paiwan
Rukai
pouroukou
Figure 2: A small example of how PARSIM works.
Listed here are the words for ?ten? in four languages
from the Formosan family, along with the tree that ex-
plains them. The dashed line indicates an innovation on
the branch.
1. Choose I` ? Bernoulli(?`), indicating
whether or not the word is an innovation
or a mutation.
2. If it is a mutation, choose W` ?
p(W |?`,Wpar(`)).
3. Otherwise, choose W` ? ?.
We also depict our model as a plate diagram for a
small phylogeny in Figure 1b.
Because there is only one tree per gloss, there
is no assignment problem to consider, which is the
main source of the intractability of HK10. Instead,
pieces of the phylogeny are simply ?cut? into sub-
trees whenever an innovation occurs. Thus, message
passing can be used to perform inference.
As an example of how our process works, con-
sider Figure 2. The Formosan word for ?ten?
probably resembled either /purrok/ or /pouroukou/.
There was an innovation in Ciuli and Squliq?s an-
cestor Atayalic that produced a new word for ten.
This word then mutated separately into the words
/malapzo/ and /mappo/, respectively.
4 Relation to Parsimony
PARSIM is related to the parsimony principle
from computational biology (Cavalli-Sforza and Ed-
wards, 1965; Fitch, 1971), where it is used to search
for phylogenies. When using parsimony, a phy-
logeny is scored according to the derivation that re-
quires the fewest number of changes of state, where
a state is typically thought of as a gene or some other
trait in a species. These genes are typically called
?characters? in the computational biology literature,
and two species would have the same value for a
character if they share the same property that that
state represents.
When inducing phylogenies of languages, a natu-
ral choice for characters are glosses from a restricted
vocabulary like a Swadesh list, and two words are
represented as the same value for a character if they
are cognate (Ringe et al, 2002). Other features can
be used (Daume? III and Campbell, 2007; Daume? III,
2009), but they are not relevant to our discussion.
Consider the small example in Figure 3a with just
four languages. Here, cognacy is encoded using
characters. In this example, at least two changes of
state are required to explain the data: both C and B
must have evolved from A. Therefore, the parsimony
score for this tree is two.
Of course, there is no reason why all changes
should be equally likely. For instance, it might be
extremely likely that B changes into both A and
C, but that A never changes into B or C, and so
weighted variants of parsimony might be neces-
sary (Sankoff and Cedergren, 1983).
With this in mind, PARSIM can be thought of a
weighted variant of parsimony, with two differences.
First, the characters do not indicate ahead of time
which words are related. Instead, the characters are
the words themselves. Second, the transitions be-
tween different states (words) are not uniform. In-
stead, they are weighted by the log probability of
one word changing into another, including both mu-
tations and innovations.
Thus, the task of inference in PARSIM is to find
the most ?parsimonious? explanation for the words
we have observed, which is the same as finding the
most likely derivation. Because the distances be-
tween words (that is, the transition probabilities)
are not known ahead of time, they must instead be
learned, which we discuss in Section 7.3
5 Limitations of the Parsimony Model
Potentially, our parsimony model sacrifices a cer-
tain amount of power to make inference tractable.
Specifically, it cannot model homoplasy, the pres-
ence of more than one word in a language for a given
3It is worth noting that we are not the first to point out a
connection between parsimony and likelihood. Indeed, many
authors in the computational biology literature have formally
demonstrated a connection (Farris, 1973; Felsenstein, 1973).
348
A B C A
A
A
A
(a)
A B B A
{A,B}
{A,B}
(b)
{A,B}
A B B A
A
(c)
A
A
A A
A
(d)
A
A
B B
B
B
B
Figure 3: Trees illustrating parsimony and its limitations. In these trees, there are four languages, with words A, B, and
C in various configurations. (a) The most parsimonious derivation for this tree has all intermediate states as A. There
are thus two changes. (b) An example of homoplasy. Here, given this tree, it seems likely that the ancestral languages
contained both A and B. (c) PARSIM cannot recover the example from (b), and so it encodes two innovations (shown
as dashed lines). (d) The HK10 model can recover this relationship, but this power makes the model intractable.
gloss. Homoplasy can arise for a variety of reasons
in phylogenetic models of cognates, and we describe
some in this section.
Consider the example illustrated in Figure 3b,
where the two central languages share a cognate, as
do the two outer languages. This is the canonical ex-
ample of homoplasy, and PARSIM cannot correctly
recover this grouping. Instead, it can at best only se-
lect group A or group B as the value for the parent,
and leave the other group fragmented as two innova-
tions, as in Figure 3c. On the other hand, HK10 can
recover this relationship (Figure 3d), but this power
is precisely what makes it intractable.
There are two reasons this kind of homoplasy
could arise. The first is that there were indeed two
words in the parent language for this gloss, or that
there were two words with similar meanings and
the two meanings drifted together. Second, the tree
could be an inadequate model of the evolution in
this case. For instance, there could have been a cer-
tain amount of borrowing between two of these lan-
guages, or there was not a single coherent parent lan-
guage, but rather a language continuum that cannot
be explained by any tree.
However, homoplasy seems to be relatively un-
common (though not unheard of) in the Oceanic and
Formosan families. Where it does appear, our model
should simply fail to get one of the cognate groups,
instead explaining all of them via innovation. To
repair this shortcoming, we can simply run the ag-
glomerative clustering procedure for the model of
Hall and Klein (2010), starting from the groups that
PARSIM has recovered. Using this procedure, we
can hopefully recover many of the under-groupings
caused by homoplasy.
6 Inference and Scale
6.1 Inference
In this section we describe the basics of infer-
ence in the PARSIM model. We have a nearly
tree-structured graphical model (Figure 1); it is
not a tree only because of the innovation param-
eters. Therefore, we apply the common trick of
grouping variables to form a tree. Specifically, we
group each word variable W` with its innovation
parameter I`. The distribution of interest is then
p(W`, I`|Wpar(`), ?`, ?`), and the primary operation
is summing out messages ? from the children of a
language and sending a new message to its parent:
?`(wpar(`)) =
?
w`
p(w`|?)
?
`? ? child(`)
?`?(w`)
p(w`|?) = p(w`|I` = 0, wpar(`), ?`)p(I` = 0|?`)
+ p(w`|I` = 1, ?`)P (I` = 1|?`) (1)
The first term involves computing the probability of
the word mutating from its parent, and the second
involves the probability of the child word from a lan-
guage model. We describe the parameters and pro-
cedures for these operations in 7.1.
6.2 Scale
Even though inference by message-passing in our
model is tractable, we needed to make certain con-
cessions to make inference acceptably fast. These
choices mainly affect how we represent distributions
over strings.
349
First, we need to model distributions and mes-
sages over words on the internal nodes of a phy-
logeny. The natural choice in this scenario is to use
weighted finite automata (Mohri et al, 1996). Au-
tomata have been used to successfully model distri-
butions of strings for inferring morphology (Dreyer
and Eisner, 2009) as well as cognate detection (Hall
and Klein, 2010). Even in models that would be
tractable with ?ordinary? messages, inference with
automata quickly becomes intractable, because the
size of the automata grow exponentially with the
number of messages passed. Therefore, approxima-
tions must be used. Dreyer and Eisner (2009) used
a mixture of a k-best list and a unigram language
model, while Hall and Klein (2010) used an approx-
imation procedure that projected complex automata
to simple, tractable automata using a modified KL
divergence.
While either approach could be used here in prin-
ciple, we found that automata machinery was simply
too slow for our application. Instead, we exploit the
intuition that we do not need to accurately recon-
struct the word for any ancestral language. More-
over, it is inefficient to keep track of probabilities for
all strings. Therefore, we only track scores for words
that actually exist in a given gloss, which means that
internal nodes only have mass on those words. That
is, if a gloss has 10 distinct words across all the lan-
guages in our dataset, we pass messages that only
contain information about those 10 words.
Now, this representation?while more efficient
than the automata representations?results in infer-
ence that is still quadratic in the number of words
in a gloss, since we have distributions of the form
p(w`|wpar(`), ?`). Intuitively, it is unlikely that a
word from one distant branch of tree resembles a
word in another branch. Therefore, rather than score
all of these unlikely words, we use a beam where we
only factor in words whose score is at most a fac-
tor of e?10 less than the maximum score. Our initial
experiments found that using a beam provides large
savings in time with little impact on prediction qual-
ity.
7 Learning
PARSIM has three kinds of parameters that we need
to learn: the mutation parameters ?`, the innovation
probabilities ?`, and the global language model ?
for generating new words. We learn these parame-
ters via Expectation Maximization (Dempster et al,
1977), iterating between computing expected counts
and adjusting parameters to maximize the posterior
probability of the parameters. In this section, we de-
scribe those parameters.
7.1 Sound Laws
The core piece of our system is learning the sound
laws associated with each edge. Since the founda-
tion of historical linguists with the neogrammari-
ans, linguists have argued for the regularity of sound
change at the phonemic level (Schleicher, 1861;
Bloomfield, 1938). That is to say, if in some lan-
guage a /t/ changes to a /d/ in some word, it is al-
most certain that it will change in every other place
that has the same surrounding context.
In practice, of course, sound change is not entirely
regular, and complex extralinguistic events can lead
to sound changes that are irregular. For example,
in some cultures in which Oceanic languages are
spoken, the name of the chief is taboo: one cannot
speak his name, nor say any word that sounds too
much like his name. Speakers of these languages
do find ways around this prohibition, often resulting
in sound changes that cannot be explained by sound
laws alone (Keesing and Fifi?i, 1969).
Nevertheless, we find it useful to model sound
change as a largely regular if stochastic process.
We employ a sound change model whose expressive
power is equivalent to that of Hall and Klein (2010),
though with a different parameterization. We model
the evolution of a word w` to its child w`? as a
sequence of unigram edits that include insertions,
deletions, and substitutions. Specifically, we use a
standard three-state pair hidden Markov model that
is closely related to the classic alignment algorithm
of Needleman and Wunsch (1970) (Durbin et al,
2006).
The three states in this HMM correspond to
matches/substitutions, insertions, and deletions. The
transitions are set up such that insertions and dele-
tions cannot be interleaved. This prevents spurious
equivalent alignments, which would cause the model
to assign unnecessarily higher probability to transi-
tions with many insertions and deletions.
Actually learning these parameters involves learn-
350
ing the transition probabilities of this HMM (which
model the overall probability of insertion and dele-
tion) as well as the emission probabilities (which
model the particular edits). Because there are rel-
atively few words for each language (96 on average
in Oceanic), we found it important to tie together
the parameters for the various languages, in contrast
to Hall and Klein (2010) who did not. In our maxi-
mization step, we fit a joint log-linear model for each
language, using features that are both specific to a
language and shared across languages. Our features
included indicators on each substitution, insertion,
and deletion operation, along with an indicator for
the outcome of each edit operation. This last fea-
ture reflects the propensity of a particular phoneme
to appear in a given language at all, no matter what
its ancestral phoneme was. This parameterization
is similar to the one used in the reconstruction sys-
tem of Bouchard-Co?te? et al (2009), except that they
used edit operations that conditioned on the context
of the surrounding word, which is crucial when try-
ing to accurately reconstruct ancestral word forms.
To encourage parameter sharing, we used an `2 reg-
ularization penalty.
7.2 Innovation Parameters
The innovation parameters ?` are parameters for
simple Bernoulli distribution that govern the propen-
sity for a language to start a new word. These pa-
rameters can be learned separately, though due to
data sparsity, we found it better to use a tied param-
eterization as with the sound laws. Specifically, we
fit a log linear model whose features are indicators
on the specific language, as well as a global inno-
vation parameter that is shared across all languages.
As with the sound laws, we used an `2 regularization
penalty to encourage the use of the global innovation
parameter.
7.3 Language Model
Finally, we have a single language model ? that is
also shared across all languages. ? is a simple bi-
gram language model over characters in the Interna-
tional Phonetic Alphabet. ? is used when generating
new words either via innovation or from the root of
the tree.
In principle, we could of course have language
models specific to each language, but because there
Formosan
System Prec Recall F1 Purity
Agg. HK10 77.6 83.2 80.0 84.7
PARSIM 87.8 71.0 78.5 94.6
Combination 85.2 81.3 83.2 92.3
Oceanic
System Prec Recall F1 Purity
PARSIM 84.4 62.1 71.5 91.8
Combination 76.0 73.8 74.9 85.5
Table 1: Results on the Formosan and Oceanic fami-
lies. PARSIM is the new parsimony model in this pa-
per, Agg. HK10 is our agglomerative variant of Hall and
Klein (2010) and Combination uses PARSIM?s output to
seed the agglomerative matcher. For the agglomerative
systems, we report the point with maximal F1 score, but
we also show precision/recall curves. (See Figure 4.)
are so few words per language, we found that
branch-specific language models caused the model
to prefer to innovate at almost every node since the
language models could essentially memorize the rel-
atively small vocabularies of these languages.
8 Experiments
8.1 Cognate Recovery
We ran both PARSIM and our agglomerative ver-
sion of HK10 on the Formosan datasets. For PAR-
SIM, we initialized the mutation parameters ? to a
model that preferred matches to insertions, substi-
tutions and deletions by a factor of e3, innovation
parameters to 0.5, and the language model to a uni-
form distribution over characters. For the agglomer-
ative HK10, we initialized its parameters to the val-
ues found by our model.4
Based on our observations about homoplasy, we
also considered a combined system where we ran
PARSIM, and then seeded the agglomerative cluster-
ing algorithm with the clusters found by PARSIM.
For evaluation, we report a few metrics. First,
we report cluster purity, which is a kind of pre-
cision measure for clusterings. Specifically, each
cluster is assigned to the cognate group that is the
most common cognate word in that group, and then
purity is computed as the fraction of words that
4Attempts to learn parameters directly with the agglomera-
tive clustering algorithm were not effective.
351
0.6 
0.7 
0.8 
0.9 
1 
0.4 0.5 0.6 0.7 0.8 0.9 1 
Pre
cisi
on 
Recall 
Combined System 
PARSIM 
Agg. HK10 
Figure 4: Precision/Recall curves for our systems. The
Combined System starts from PARSIM?s output, so it
has fewer points to plot, and starts from a point with
lower precision. As PARSIM outputs only one result, it
is starred.
are in a cluster whose gold cognate group matches
the cognate group of the cluster. For gold parti-
tions G = {G1, G2, . . . , Gg} and found partitions
F = {F1, F2, . . . , Ff}, we have: purity(G,F ) =
1
N
?
f maxg |Gg?Ff |. We also report pairwise pre-
cision and recall computed over pairs of words.5 Fi-
nally, because agglomerative clustering does not de-
fine a natural ?stopping point? other than when the
likelihood gain decreases to 0?which did not per-
form well in our initial tests?we will report both
a precision/recall curve, as well the maximum pair-
wise F1 obtained by the agglomerative HK10 and
the combined system.
The results are in Table 1. On Formosan, PAR-
SIM has much higher precision and purity than our
agglomerative version of HK10 at its highest point,
though its recall and F1 suffer somewhat. Of course,
the comparison is not quite fair, since we have se-
lected the best possible point for HK10.
However, our combination of the two systems
does even better. By feeding our high-precision re-
sults into the agglomerative system and sacrificing
just a little precision, our combined system achieves
much higher F1 scores than either of the systems
alone.
Next, we also examined precision and recall
curves for the two agglomerative systems on For-
5The main difference between precision and purity is that
pairwise precision is inherently quadratic, meaning that it pe-
nalizes mistakes in large groups much more heavily than mis-
takes in small groups.
mosan, which we have plotted in Figure 4, along
with the one point output by PARSIM.
We then ran PARSIM and the combined system
on the much larger Oceanic dataset. Performance
on all metrics decreased somewhat, but this is to be
expected since there is so much more data. As with
Formosan, PARSIM has higher precision than the
combined system, but it has much lower recall.
8.2 Reconstruction
We also wanted to see how well our cognates could
be used to actually reconstruct the ancestral forms of
words. To do so, we ran a version of Bouchard-Co?te?
et al (2009)?s reconstruction system using both the
cognate groups PARSIM found in the Oceanic lan-
guage family and the gold cognate groups provided
by the ABVD. We then evaluated the average Leven-
shtein distance of the reconstruction for each word
to the reconstruction of that word?s Proto-Oceanic
ancestor provided by linguists. Our evaluation dif-
fers from Bouchard-Co?te? et al (2009) in that they
averaged over cognate groups, which does not make
sense for our task because there are different cognate
groups. Instead, we average over per-modern-word
reconstruction error.
Using this metric, reconstructions using our sys-
tem?s cognates are an average of 2.47 edit opera-
tions from the gold reconstruction, while with gold
cognates the error is 2.19 on average. This repre-
sents an error increase of 12.8%. To see if there
was some pattern to these errors, we also plotted the
fraction of words with each Levenshtein distance for
these reconstructions in Figure 5. While the plots are
similar, the automatic cognates exhibit a longer tail.
Thus, even with automatic cognates, the reconstruc-
tion system can reconstruct words faithfully in many
cases, but in a few instances our system fails.
9 Analysis
We now consider some of the errors made by our
system. Broadly, there are two kinds of mistakes
in a model like ours: those affecting precision and
those affecting recall.
9.1 Precision
Many of our precision errors seem to be due to
our somewhat limited model of sound change. For
instance, the language Pazeh has two words for
352
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
0.4	 ?
0	 ? 1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ? 9	 ?
Fr
act
ion
 of
 W
ord
s 
Levenshtein Distance 
Automatic Cognates 
Gold Cognates 
Figure 5: Percentage of words with varying levels of
Levenshtein distance from the gold reconstruction. Gold
Cognates were hand-annotated by linguists, while Auto-
matic Cognates were found by our system.
?to sleep:? /mudamai/ and /mid@m/. Somewhat
surprisingly the former word is cognate with Pai-
wan /qmereN/ and Saisiat /maPr@m/ while the lat-
ter is not. Our system, however, makes the mistake
of grouping /mid@m/ with the Paiwan and Saisiat
words. Our system has inferred that the insertions of
/u/ and /ai/ (which are required to bring /mudamai/
into alignment with the Saisiat and Paiwan words)
are less likely than substituting a few vowels and the
consonant /r/ for /d/ (which are required to align
/mid@m/). Perhaps a more sophisticated model of
sound change could correctly learn this relationship.
However, a preliminary inspection of the data
seems to indicate that not all of our precision errors
are actually errors, but rather places where the data
is insufficiently annotated (and indeed, the ABVD is
still a work in progress). For instance, consider the
words for ?meat/flesh? in the Formosan languages:
Squliq /hiP/, Bunun /titiP/, Paiwan /seti/, Kavalan
/PisiP/, CentralAmi /titi/, Our system groups all of
these words except for Squliq /hiP/. However, de-
spite these words? similarity, there are actually three
cognate groups here. One includes Squliq /hiP/ and
Kavalan /PisiP/, another includes just Paiwan /seti/,
and the third includes Bunun /titiP/ and CentralAmi
/titi/. Crucially, these cognate groups do not fol-
low the phylogeny closely. Thus, either there was a
significant amount of borrowing between these lan-
guages, or there was a striking amount of homoplasy
in Proto-Formosan, or these words are in fact mostly
cognate. While a more thorough, linguistically-
informed analysis is needed to ensure that these are
actually cognates, we believe that our system, in
conjunction with a trained Austronesian specialist,
could potentially find many more cognate groups,
speeding up the process of completing the ABVD.
9.2 Recall
Our system can also fail to group words that should
be grouped. One recurring problem seems to
be reduplication, which is a fairly common phe-
nomenon in Austronesian languages. For instance,
there is a cognate group for ?to eat? that includes
Bunun /maun/, Thao /kman/, Favorlang /man/, and
Sediq /manakamakan/, among others. Our system
correctly finds this group, with the exception of
/manakamakan/, which is clearly the result of redu-
plication. Reduplication cannot be modeled using
mere sound laws, and so a more complex transition
model is needed to correctly identify these kinds of
changes.
10 Conclusion
We have presented a new system for automatically
finding cognates across many languages. Our sys-
tem is comprised of two parts. The first, PAR-
SIM, is a new high-precision generative model with
tractable inference. The second, HK10, is a mod-
ification of Hall and Klein (2010) that makes their
approximate inference more efficient. We discuss
certain trade-offs needed to make both models scale,
and demonstrated its performance on the Formosan
and Oceanic language families.
References
Shane Bergsma and Greg Kondrak. 2007. Multilingual
cognate identification using integer linear program-
ming. In RANLP Workshop on Acquisition and Man-
agement of Multilingual Lexicons, Borovets, Bulgaria,
September.
Leonard Bloomfield. 1938. Language. Holt, New York.
R. A. Blust. 2009. The Austronesian languages. Aus-
tralian National University.
Alexandre Bouchard-Co?te?, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic approach
to diachronic phonology. In EMNLP.
Alexandre Bouchard-Co?te?, Thomas L. Griffiths, and Dan
Klein. 2009. Improved reconstruction of protolan-
guage word forms. In NAACL, pages 65?73.
L. L. Cavalli-Sforza and A. W. F. Edwards. 1965. Analy-
sis of human evolution. In S. J. Geerts Genetics Today,
353
editor, Proceedings of XIth International Congress of
Genetics, 1963, Vol, page 923?933. 3, 3.
Hal Daume? III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Conference of the Association for Computational Lin-
guistics (ACL).
Hal Daume? III. 2009. Non-parametric Bayesian areal
linguistics. In NAACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38.
Markus Dreyer and Jason Eisner. 2009. Graphical mod-
els over multiple strings. In EMNLP, Singapore, Au-
gust.
R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 2006.
Biological sequence analysis. eleventh edition.
James S. Farris. 1973. On Comparing the Shapes of
Taxonomic Trees. Systematic Zoology, 22(1):50?54,
March.
J. Felsenstein. 1973. Maximum likelihood and mini-
mum steps methods for estimating evolutionnary trees
from data on discrete characters. Systematic Zoology,
23:240?249.
W. M. Fitch. 1971. Toward defining the course of evo-
lution: minimal change for a specific tree topology.
Systematic Zoology, 20:406?416.
S.J. Greenhill, R. Blust, and R.D. Gray. 2008. The
Austronesian basic vocabulary database: from bioin-
formatics to lexomics. Evolutionary Bioinformatics,
4:271?283.
David Hall and Dan Klein. 2010. Finding cognates using
phylogenies. In Association for Computational Lin-
guistics (ACL).
Robert M. Keesing and Jonathan Fifi?i. 1969. Kwaio
word tabooing in its cultural context. Journal of the
Polynesian Society, 78(2):154?177.
Grzegorz Kondrak. 2001. Identifying cognates by pho-
netic and semantic similarity. In NAACL.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:83?97.
John B. Lowe and Martine Mazaudon. 1994. The re-
construction engine: a computer implementation of
the comparative method. Computational Linguistics,
20(3):381?417.
Gideon S. Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages. In
NAACL.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop. John Wiley and Sons.
Andrea Mulloni. 2007. Automatic prediction of cognate
orthography using support vector machines. In ACL,
pages 25?30.
Saul B. Needleman and Christian D. Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
Molecular Biology, 48(3):443 ? 453.
John Nerbonne. 2010. Measuring the diffusion of lin-
guistic change. Philosophical Transactions of the
Royal Society B: Biological Sciences.
J. Nichols. 1992. Linguistic diversity in space and time.
University of Chicago Press.
Michael P. Oakes. 2000. Computer estimation of vocab-
ulary in a protolanguage from word lists in four daugh-
ter languages. Quantitative Linguistics, 7(3):233?243.
Don Ringe, Tandy Warnow, and Ann Taylor. 2002. Indo-
european and computational cladistics. Transactions
of the Philological Society, 100(1):59?129.
D. Sankoff and R. J. Cedergren, 1983. Simultaneuous
comparison of three or more sequences related by a
tree, page 253?263. Addison-Wesley, Reading, MA.
August Schleicher. 1861. A Compendium of the Com-
parative Grammar of the Indo-European, Sanskrit,
Greek and Latin Languages.
354
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1048?1059, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Parser Showdown at the Wall Street Corral:
An Empirical Investigation of Error Types in Parser Output
Jonathan K. Kummerfeld? David Hall? James R. Curran? Dan Klein?
?Computer Science Division ? e-lab, School of IT
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
{jkk,dlwh,klein}@cs.berkeley.edu james@it.usyd.edu.au
Abstract
Constituency parser performance is primarily
interpreted through a single metric, F-score
on WSJ section 23, that conveys no linguis-
tic information regarding the remaining errors.
We classify errors within a set of linguisti-
cally meaningful types using tree transforma-
tions that repair groups of errors together. We
use this analysis to answer a range of ques-
tions about parser behaviour, including what
linguistic constructions are difficult for state-
of-the-art parsers, what types of errors are be-
ing resolved by rerankers, and what types are
introduced when parsing out-of-domain text.
1 Introduction
Parsing has been a major area of research within
computational linguistics for decades, and con-
stituent parser F-scores on WSJ section 23 have ex-
ceeded 90% (Petrov and Klein, 2007), and 92%
when using self-training and reranking (McClosky
et al 2006; Charniak and Johnson, 2005). While
these results give a useful measure of overall per-
formance, they provide no information about the na-
ture, or relative importance, of the remaining errors.
Broad investigations of parser errors beyond the
PARSEVAL metric (Abney et al 1991) have either
focused on specific parsers, e.g. Collins (2003), or
have involved conversion to dependencies (Carroll
et al 1998; King et al 2003). In all of these cases,
the analysis has not taken into consideration how a
set of errors can have a common cause, e.g. a single
mis-attachment can create multiple node errors.
We propose a new method of error classifica-
tion using tree transformations. Errors in the parse
tree are repaired using subtree movement, node cre-
ation, and node deletion. Each step in the process is
then associated with a linguistically meaningful er-
ror type, based on factors such as the node that is
moved, its siblings, and parents.
Using our method we analyse the output of thir-
teen constituency parsers on newswire. Some of
the frequent error types that we identify are widely
recognised as challenging, such as prepositional
phrase (PP) attachment. However, other significant
types have not received as much attention, such as
clause attachment and modifier attachment.
Our method also enables us to investigate where
reranking and self-training improve parsing. Pre-
viously, these developments were analysed only in
terms of their impact on F-score. Similarly, the chal-
lenge of out-of-domain parsing has only been ex-
pressed in terms of this single objective. We are able
to decompose the drop in performance and show that
a disproportionate number of the extra errors are due
to coordination and clause attachment.
This work presents a comprehensive investigation
of parser behaviour in terms of linguistically mean-
ingful errors. By applying our method to multiple
parsers and domains we are able to answer questions
about parser behaviour that were previously only ap-
proachable through approximate measures, such as
counts of node errors. We show which errors have
been reduced over the past fifteen years of parsing
research; where rerankers are making their gains and
where they are not exploiting the full potential of k-
best lists; and what types of errors arise when mov-
ing out-of-domain. We have released our system1 to
enable future work to apply our methodology.
1http://code.google.com/p/berkeley-parser-analyser/
1048
2 Background
Most attempts to understand the behaviour of con-
stituency parsers have focused on overall evaluation
metrics. The three main methods are intrinsic eval-
uation with PARSEVAL, evaluation on dependencies
extracted from the constituency parse, and evalua-
tion on downstream tasks that rely on parsing.
Intrinsic evaluation with PARSEVAL, which calcu-
lates precision and recall over labeled tree nodes, is
a useful indicator of overall performance, but does
not pinpoint which structures the parser has most
difficulty with. Even when the breakdown for par-
ticular node types is presented (e.g. Collins, 2003),
the interaction between node errors is not taken into
account. For example, a VP node could be missing
because of incorrect PP attachment, a coordination
error, or a unary production mistake. There has been
some work that addresses these issues by analysing
the output of constituency parsers on linguistically
motivated error types, but only by hand on sets of
around 100 sentences (Hara et al 2007; Yu et al
2011). By automatically classifying parse errors we
are able to consider the output of multiple parsers on
thousands of sentences.
The second major parser evaluation method in-
volves extraction of grammatical relations (King et
al., 2003; Briscoe and Carroll, 2006) or dependen-
cies (Lin, 1998; Briscoe et al 2002). These met-
rics have been argued to be more informative and
generally applicable (Carroll et al 1998), and have
the advantage that the breakdown over dependency
types is more informative than over node types.
There have been comparisons of multiple parsers
(Foster and van Genabith, 2008; Nivre et al 2010;
Cer et al 2010), as well as work on finding rela-
tions between errors (Hara et al 2009), and break-
ing down errors by a range of factors (McDonald and
Nivre, 2007). However, one challenge is that results
for constituency parsers are strongly influenced by
the dependency scheme being used and how easy it
is to extract the dependencies from a given parser?s
output (Clark and Hockenmaier, 2002). Our ap-
proach does not have this disadvantage, as we anal-
yse parser output directly.
The third major approach involves extrinsic eval-
uation, where the parser?s output is used in a down-
stream task, such as machine translation (Quirk
and Corston-Oliver, 2006), information extraction
(Miyao et al 2008), textual entailment (Yuret et
al., 2010), or semantic dependencies (Dridan and
Oepen, 2011). While some of these approaches give
a better sense of the impact of parse errors, they re-
quire integration into a larger system, making it less
clear where a given error originates.
The work we present here differs from existing
approaches by directly and automatically classifying
errors into meaningful types. This enables the first
very broad, yet detailed, study of parser behaviour,
evaluating the output of thirteen parsers over thou-
sands of sentences.
3 Parsers
Our evaluation is over a wide range of PTB con-
stituency parsers and their variants from the past fif-
teen years. For all parsers we used the publicly avail-
able version, with the standard parameter settings.
Berkeley (Petrov et al 2006; Petrov and Klein,
2007). An unlexicalised parser with a grammar
constructed with automatic state splitting.
Bikel (2004) implementation of Collins (1997).
BUBS (Dunlop et al 2011; Bodenstab et al
2011). A ?grammar-agnostic constituent
parser,? which uses a Berkeley Parser grammar,
but parses with various pruning techniques to
improve speed, at the cost of accuracy.
Charniak (2000). A generative parser with a max-
imum entropy-inspired model. We also use the
reranker (Charniak and Johnson, 2005), and the
self-trained model (McClosky et al 2006).
Collins (1997). A generative lexicalised parser,
with three models, a base model, a model that
uses subcategorisation frames for head words,
and a model that takes into account traces.
SSN (Henderson, 2003; Henderson, 2004). A sta-
tistical left-corner parser, with probabilities es-
timated by a neural network.
Stanford (Klein and Manning, 2003a; Klein and
Manning, 2003b). We consider both the un-
lexicalised PCFG parser (-U) and the factored
parser (-F), which combines the PCFG parser
with a lexicalised dependency parser.
1049
System F P R Exact Speed
ENHANCED TRAINING / SYSTEMS
Charniak-SR 92.07 92.44 91.70 44.87 1.8
Charniak-R 91.41 91.78 91.04 44.04 1.8
Charniak-S 91.02 91.16 90.89 40.77 1.8
STANDARD PARSERS
Berkeley 90.06 90.30 89.81 36.59 4.2
Charniak 89.71 89.88 89.55 37.25 1.8
SSN 89.42 89.96 88.89 32.74 1.8
BUBS 88.50 88.57 88.43 31.62 27.6
Bikel 88.16 88.23 88.10 32.33 0.8
Collins-3 87.66 87.82 87.50 32.22 2.0
Collins-2 87.62 87.77 87.48 32.51 2.2
Collins-1 87.09 87.29 86.90 30.35 3.3
Stanford-L 86.42 86.35 86.49 27.65 0.7
Stanford-U 85.78 86.48 85.09 28.35 2.7
Table 1: PARSEVAL results on WSJ section 23 for the
parsers we consider. The columns are F-score, precision,
recall, exact sentence match, and speed (sents/sec). Cov-
erage was left out as it was above 99.8% for all parsers.
In the ENHANCED TRAINING / SYSTEMS section we in-
clude the Charniak parser with reranking (R), with a self-
trained model (S), and both (SR).
Table 1 shows the standard performance metrics,
measured on section 23 of the WSJ, using all sen-
tences. Speeds were measured using a Quad-Core
Xeon CPU (2.33GHz 4MB L2 cache) with 16GB
of RAM. These results clearly show the variation in
parsing performance, but they do not show which
constructions are the source of those variations.
4 Error Classification
While the statistics in Table 1 give a sense of over-
all parser performance they do not provide linguisti-
cally meaningful intuition for the source of remain-
ing errors. Breaking down the remaining errors by
node type is not particularly informative, as a sin-
gle attachment error can cause multiple node errors,
many of which are for unrelated node types. For
example, in Figure 1 there is a PP attachment error
that causes seven bracket errors (extra S, NP, PP, and
NP, missing S, NP, and PP). Determining that these
correspond to a PP attachment error from just the la-
bels of the missing and extra nodes is difficult. In
contrast, the approach we describe below takes into
consideration the relations between errors, grouping
them into linguistically meaningful sets.
We classify node errors in two phases. First, we
S
VP
VP
S
NP
PP
NP
PP
in 1986
NP
NNP
Applied
IN
of
NP
chief executive officer
VBN
named
VBD
was
NP
PRP
He
(a) Parser output
S
VP
VP
PP
in 1986
S
NP
PP
NP
NNP
Applied
IN
of
NP
chief executive officer
VBN
named
VBD
was
NP
PRP
He
(b) Gold tree
Figure 1: Grouping errors by node type is of limited use-
fulness. In this figure and those that follow the top tree
is the incorrect parse and the bottom tree is the correct
parse. Bold, boxed nodes are either extra (marked in the
incorrect tree) or missing (marked in the correct tree).
This is an example of PP Attachment (in 1986 is too
low), but that is not at all clear from the set of incorrect
nodes (extra S, NP, PP, and NP, missing S, NP, and PP).
find a set of tree transformations that convert the out-
put tree into the gold tree. Second, the transforma-
tion are classified into error types such as PP attach-
ment and coordination. Pseudocode for our method
is shown in Algorithm 1. The tree transformation
stage corresponds to the main loop, while the sec-
ond stage corresponds to the final loop.
4.1 Tree Transformation
The core of our transformation process is a set of op-
erations that move subtrees, create nodes, and delete
nodes. Searching for the shortest path to transform
one tree into another is prohibitively slow.2 We find
2We implemented various search procedures and found sim-
ilar results on the sentences that could be processed in a reason-
1050
Algorithm 1 Tree transformation error classification
U = initial set of node errors
Sort U by the depth of the error in the tree, deepest first
G = ?
repeat
for all errors e ? U do
if e fits an environment template t then
g = new error group
Correct e as specified by t
for all errors f that t corrects do
Remove f from U
Insert f into g
end for
Add g to G
end if
end for
until unable to correct any further errors
for all remaining errors e ? U do
Insert a group into G containing e
end for
for all groups g ? G do
Classify g based on properties of the group
end for
a path by applying a greedy bottom?up approach,
iterating through the errors in order of tree depth.
We match each error with a template based on
nearby tree structure and errors. For example, in
Figure 1 there are four extra nodes that all cover
spans ending at Applied in 1986: S, NP, PP, NP.
There are also three missing nodes with spans end-
ing between Applied and in: PP, NP, and S. Figure 2
depicts these errors as spans, showing that this case
fits three criteria: (1) there are a set of extra spans all
ending at the same point, (2) there are a set of miss-
ing spans all ending at the same point, and (3) the ex-
tra spans cross the missing spans, extending beyond
their end-point. This indicates that the node start-
ing after Applied is attaching too low and should be
moved up, outside all of the extra nodes. Together,
the criteria and transformation form a template.
Once a suitable template is identified we correct
the error by moving subtrees, adding nodes and re-
moving nodes. In the example this is done by mov-
ing the node spanning in 1986 up in the tree until it
is outside of all the extra spans. Since moving the PP
leaves a unary production from an NP to an NP, we
also collapse that level. In total this corrects seven
able amount of time.
named chief executive officer of Applied in 1986
Figure 2: Templates are defined in terms of extra and
missing spans, shown here with unbroken lines above and
dashed lines below, respectively. This is an example of a
set of extra spans that cross a set of missing spans (which
in both cases all end at the same position). If the last two
words are moved, two of the extra spans will match the
two missing spans. The other extra span is deleted during
the move as it creates an NP?NP unary production.
errors, as there are three cases in which an extra node
is present that matches a missing node once the PP
is moved. All of these errors are placed in a single
group and information about the nearby tree struc-
ture before and after the transformation is recorded.
We continue to make passes through the list until
no errors are corrected on a pass. For each remaining
node error an individual error group is created.
The templates were constructed by hand based on
manual analysis of parser output. They cover a range
of combinations of extra and missing spans, with
further variation for whether crossing is occurring
and if so whether the crossing bracket starts or ends
in the middle of the correct bracket. Errors that do
not match any of our templates are left uncorrected.
4.2 Transformation Classification
We began with a large set of node errors, in the first
stage they were placed into groups, one group per
tree transformation used to get from the test tree to
the gold tree. Next we classify each group as one of
the error types below.
PP Attachment Any case in which the transforma-
tion involved moving a Prepositional Phrase, or
the incorrect bracket is over a PP, e.g.
He was (VP named chief executive officer of
fill(NP Applied (PP in 1986)))
where (PP in 1986) should modify the entire
VP, rather than just Applied.
NP Attachment Several cases in which NPs had to
be moved, particularly for mistakes in appos-
itive constructions and incorrect attachments
within a verb phrase, e.g.
The bonds (VP go (PP on sale (NP Oct. 19)))
where Oct. 19 should be an argument of go.
1051
VP
NP
NN
today
NP
VBG
appearing
NP
NN
ad
JJ
new
DT
another
VBD
wrote
(a) Parser output
VP
NP
VP
NP
NN
today
VBG
appearing
NP
NN
ad
JJ
new
DT
another
VBD
wrote
(b) Gold tree
Figure 3: NP Attachment: today is too high, it should
be the argument of appearing, rather than wrote. This
causes three node errors (extra NP, missing NP and VP).
VP
ADVP
ahead of time
S
VP
VP
PP
about it
VB
think
TO
to
VBD
had
(a) Parser output
VP
S
VP
VP
ADVP
ahead of time
PP
about it
VB
think
TO
to
VBD
had
(b) Gold tree
Figure 4: Modifier Attachment: ahead of time is too
high, it should modify think, not had. This causes six
node errors (extra S, VP, and VP, missing S, VP, and VP).
Modifier Attachment Cases involving incorrectly
placed adjectives and adverbs, including errors
corrected by subtree movement and errors re-
quiring only creation of a node, e.g.
(NP (ADVP even more) severe setbacks)
where there should be an extra ADVP node
over even more severe.
Clause Attachment Any group that involves move-
ment of some form of S node.
VP
S
VP
VP
SBAR
unless the agency . . .
NP
the RTC to . . .
VB
restrict
TO
to
VBZ
intends
(a) Parser output
VP
SBAR
unless the agency . . .
S
VP
VP
NP
the RTC to . . .
VB
restrict
TO
to
VBZ
intends
(b) Gold tree
Figure 5: Clause Attachment: unless the agency re-
ceives specific congressional authorization is attaching
too low. This causes six node errors (extra S, VP, and
VP, missing S, VP and VP).
SINV
NP
PP
of major market activity
NP
a breakdown
VBZ
is
VP
VBG
Following
(a) Parser output
SINV
NP
NP
PP
of major market activity
NP
a breakdown
VBZ
is
S
VP
VBG
Following
(b) Gold tree
SINV
:
:
NP-SBJ-1
NP
PP
of major market activity
NP
a breakdown
VBZ
is
S-ADV
VP
VBG
Following
NP-SBJ
-NONE-
*-1
(c) Gold tree with traces and function tags
Figure 6: Two Unary errors, a missing S and a missing
NP. The third tree is the PTB tree before traces and func-
tion tags are removed. Note that the missing NP is over
another NP, a production that does occur widely in the
treebank, particularly over the word it.
1052
NP
PP
NP
NP
Dresdner AG?s 10% decline
CC
and
NP
Mannesmann AG
IN
for
NP
A 16% drop
(a) Parser output
NP
NP
Dresdner AG?s 10% decline
CC
and
NP
PP
NP
Mannesmann AG
IN
for
NP
A 16% drop
(b) Gold tree
Figure 7: Coordination: and Dresdner AG?s 10% de-
cline is too low. This causes four node errors (extra PP
and NP, missing NP and PP).
Unary Mistakes involving unary productions that
are not linked to a nearby error such as a match-
ing extra or missing node. We do not include a
breakdown by unary type, though we did find
that clause labeling (S, SINV, etc) accounted
for a large proportion of the errors.
Coordination Cases in which a conjunction is an
immediate sibling of the nodes being moved, or
is the leftmost or rightmost node being moved.
NP Internal Structure While most NP structure is
not annotated in the PTB, there is some use of
ADJP, NX, NAC and QP nodes. We form a
single group for each NP that has one or more
errors involving these types of nodes.
Different label In many cases a node is present in
the tree that spans the correct set of words, but
has the wrong label, in which case we group the
two node errors, (one extra, one missing), as a
single error.
Single word phrase A range of node errors that
span a single word, with checks to ensure this
is not linked to another error (e.g. one part of a
set of internal noun phrase errors).
Other There is a long tail of other errors. Some
could be placed within the categories above,
but would require far more specific rules.
For many of these error types it would be diffi-
cult to extract a meaningful understanding from only
NP
PP
NP
NNP
Baker
NNP
State
IN
of
NNP
Secretary
(a) Parser output
NP
NNP
Baker
PP
NP
NNP
State
IN
of
NNP
Secretary
(b) Gold tree
Figure 8: NP Internal Structure: Baker is too low, caus-
ing four errors (extra PP and NP, missing PP and NP).
the list of node errors involved. Even for error types
that can be measured by counting node errors or rule
production errors, our approach has the advantage
that we identify groups of errors with a single cause.
For example, a missing unary production may corre-
spond to an extra bracket that contains a subtree that
attached incorrectly.
4.3 Methodology
We used sections 00 and 24 as development data
while constructing the tree transformation and error
group classification methods. All of our examples
in text come from these sections as well, but for all
tables of results we ran our system on section 23.
We chose to run our analysis on section 23 as it is
the only section we are sure was not used in the de-
velopment of any of the parsers, either for tuning or
feature development. Our evaluation is entirely fo-
cused on the errors of the parsers, so unless there is
a particular construction that is unusually prevalent
in section 23, we are not revealing any information
about the test set that could bias future work.
5 Results
Our system enables us to answer questions about
parser behaviour that could previously only be
probed indirectly. We demonstrate its usefulness by
applying it to a range of parsers (here), to reranked
K-best lists of various lengths, and to output for out-
of-domain parsing (following sections).
In Table 2 we consider the breakdown of parser
1053
PP Clause Diff Mod NP 1-Word NP
Parser F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.60 0.38 0.31 0.25 0.25 0.23 0.20 0.14 0.14 0.50
Charniak-RS 92.07
Charniak-R 91.41
Charniak-S 91.02
Berkeley 90.06
Charniak 89.71
SSN 89.42
BUBS 88.63
Bikel 88.16
Collins-3 87.66
Collins-2 87.62
Collins-1 87.09
Stanford-F 86.42
Stanford-U 85.78
Worst 1.12 0.61 0.51 0.39 0.45 0.40 0.42 0.27 0.27 1.13
Table 2: Average number of bracket errors per sentence due to the top ten error types. For instance, Stanford-U
produces output that has, on average, 1.12 bracket errors per sentence that are due to PP attachment. The scale for
each column is indicated by the Best and Worst values.
Nodes
Error Type Occurrences Involved Ratio
PP Attachment 846 1455 1.7
Single word phrase 490 490 1.0
Clause Attachment 385 913 2.4
Modifier Attachment 383 599 1.6
Different Label 377 754 2.0
Unary 347 349 1.0
NP Attachment 321 597 1.9
NP Internal Structure 299 352 1.2
Coordination 209 557 2.7
Unary Clause Label 185 200 1.1
VP Attachment 64 159 2.5
Parenthetical Attachment 31 74 2.4
Missing Parenthetical 12 17 1.4
Unclassified 655 734 1.1
Table 3: Breakdown of errors on section 23 for the Char-
niak parser with self-trained model and reranker. Errors
are sorted by the number of times they occur. Ratio is the
average number of node errors caused by each error we
identify (i.e. Nodes Involved / Occurrences).
errors on WSJ section 23. The shaded area of
each bar indicates the frequency of parse errors (i.e.
empty means fewest errors). The area filled in is
determined by the expected number of node errors
per sentence that are attributed to that type of error.
The average number of node errors per sentence for
a completely full bar is indicated by the Worst row,
and the value for a completely empty bar is indicated
by the Best row. Exact error counts are available at
http://code.google.com/p/berkeley-parser-analyser/.
We use counts of node errors to make the con-
tributions of each type of error more interpretable.
As Table 3 shows, some errors typically cause only
a single node error, where as others, such as co-
ordination, generally cause several. This means
that considering counts of error groups would over-
emphasise some error types, e.g. single word phrase
errors are second most important by number of
groups (in Table 3), but seventh by total number of
node errors (in Table 2).
As expected, PP attachment is the largest contrib-
utor to errors, across all parsers. Interestingly, coor-
dination is sixth on the list, though that is partly due
to the fact that there are fewer coordination decisions
to be made in the treebank.3
By looking at the performance of the Collins
parser we can see the development over the past
fifteen years. There has been improvement across
the board, but in some cases, e.g. clause attach-
ment errors and different label errors, the change has
been more limited (24% and 29% reductions respec-
tively). We investigated the breakdown of the differ-
ent label errors by label, but no particular cases of la-
3This is indicated by the frequency of CCs and PPs in sec-
tions 02?21 of the treebank, 16,844 and 95,581 respectively.
These counts are only an indicator of the number of decisions
as the nodes can be used in ways that do not involve a decision,
such as sentences that start with a conjunction.
1054
PP Clause Diff Mod NP 1-Word NP
System K F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.08 0.04 0.08 0.05 0.06 0.04 0.08 0.04 0.04 0.11
1000 98.30
100 97.54
50 97.18
Oracle 20 96.40
10 95.66
5 94.61
2 92.59
1000 92.07
100 92.08
50 92.07
Charniak 20 92.05
10 92.16
5 91.94
2 91.56
1 91.02
Worst 0.66 0.43 0.33 0.26 0.28 0.26 0.23 0.16 0.19 0.60
Table 4: Average number of bracket errors per sentence for a range of K-best list lengths using the Charniak parser
with reranking and the self-trained model. The oracle results are determined by taking the parse in each K-best list
with the highest F-score.
bel confusion stand out, and we found that the most
common cases remained the same between Collins
and the top results.
It is also interesting to compare pairs of parsers
that share aspects of their architecture. One such
pair is the Stanford parser, where the factored parser
combines the unlexicalised parser with a lexicalised
dependency parser. The main sources of the 0.64
gain in F-score are PP attachment and coordination.
Another interesting pair is the Berkeley parser and
the BUBS parser, which uses a Berkeley grammar,
but improves speed by pruning. The pruning meth-
ods used in BUBS are particularly damaging for PP
attachment errors and unary errors.
Various comparisons can be made between Char-
niak parser variants. We discuss the reranker be-
low. For the self-trained model McClosky et al
(2006) performed some error analysis, considering
variations in F-score depending on the frequency of
tags such as PP, IN and CC in sentences. Here we
see gains on all error types, though particularly for
clause attachment, modifier attachment and coordi-
nation, which fits with their observations.
5.1 Reranking
The standard dynamic programming approach to
parsing limits the range of features that can be em-
ployed. One way to deal with this issue is to mod-
ify the parser to produce the top K parses, rather
than just the 1-best, then use a model with more so-
phisticated features to choose the best parse from
this list (Collins, 2000). While re-ranking has led to
gains in performance (Charniak and Johnson, 2005),
there has been limited analysis of how effectively
rerankers are using the set of available options. Re-
cent work has explored this question in more depth,
but focusing on how variation in the parameters
impacts performance on standard metrics (Huang,
2008; Ng et al 2010; Auli and Lopez, 2011; Ng
and Curran, 2012).
In Table 4 we present a breakdown over error
types for the Charniak parser, using the self-trained
model and reranker. The oracle results use the parse
in each K-best list with the highest F-score. While
this may not give the true oracle result, as F-score
does not factor over sentences, it gives a close ap-
proximation. The table has the same columns as Ta-
ble 2, but the ranges on the bars now reflect the min
and max for these sets.
While there is improvement on all errors when us-
ing the reranker, there is very little additional gain
beyond the first 5-10 parses. Even for the oracle
results, most of the improvement occurs within the
first 5-10 parses. The limited utility of extra parses
1055
PP Clause Diff Mod NP 1-Word NP
Corpus F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.022 0.016 0.013 0.011 0.011 0.010 0.009 0.006 0.005 0.021
WSJ 23 92.07
Brown-F 85.91
Brown-G 84.56
Brown-K 84.09
Brown-L 83.95
Brown-M 84.65
Brown-N 85.20
Brown-P 84.09
Brown-R 83.60
G-Web Blogs 84.15
G-Web Email 81.18
Worst 0.040 0.035 0.053 0.020 0.034 0.023 0.046 0.009 0.029 0.073
Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking and
the self-trained model. We use per word error rates here rather than per sentence as there is great variation in average
sentence length across the domains, skewing the per sentence results.
for the reranker may be due to the importance of
the base parser output probability feature (which, by
definition, decreases within the K-best list).
Interestingly, the oracle performance improves
across all error types, even at the 2-best level. This
indicates that the base parser model is not particu-
larly biased against a single error. Focusing on the
rows for K = 2 we can also see two interesting out-
liers. The PP attachment improvement of the ora-
cle is considerably higher than that of the reranker,
particularly compared to the differences for other er-
rors, suggesting that the reranker lacks the features
necessary to make the decision better than the parser.
The other interesting outlier is NP internal structure,
which continues to make improvements for longer
lists, unlike the other error types.
5.2 Out-of-Domain
Parsing performance drops considerably when shift-
ing outside of the domain a parser was trained on
(Gildea, 2001). Clegg and Shepherd (2005) evalu-
ated parsers qualitatively on node types and rule pro-
ductions. Bender et al(2011) designed a Wikipedia
test set to evaluate parsers on dependencies repre-
senting ten specific linguistic phenomena.
To provide a deeper understanding of the er-
rors arising when parsing outside of the newswire
domain, we analyse performance of the Charniak
parser with reranker and self-trained model on the
eight parts of the Brown corpus (Marcus et al
Corpus Description Sentences Av. Length
WSJ 23 Newswire 2416 23.5
Brown F Popular 3164 23.4
Brown G Biographies 3279 25.5
Brown K General 3881 17.2
Brown L Mystery 3714 15.7
Brown M Science 881 16.6
Brown N Adventure 4415 16.0
Brown P Romance 3942 17.4
Brown R Humour 967 22.7
G-Web Blogs Blogs 1016 23.6
G-Web Email E-mail 2450 11.9
Table 6: Variation in size and contents of the domains we
consider. The variation in average sentence lengths skews
the results for errors per sentences, and so in Table 5 we
consider errors per word.
1993), and two parts of the Google Web corpus
(Petrov and McDonald, 2012). Table 6 shows statis-
tics for the corpora. The variation in average sen-
tence lengths skew the results for errors per sen-
tence. To handle this we divide by the number of
words to determine the results in Table 5, rather than
by the number of sentences, as in previous figures.
There are several interesting features in the table.
First, on the Brown datasets, while the general trend
is towards worse performance on all errors, NP in-
ternal structure is a notable exception and in some
cases PP attachment and unaries are as well.
In the other errors we see similar patterns across
the corpora, except humour (Brown R), on which the
parser is particularly bad at coordination and clause
1056
attachment. This makes sense, as the colloquial na-
ture of the text includes more unusual uses of con-
junctions, for example:
She was a living doll and no mistake ? the ...
Comparing the Brown corpora and the Google
Web corpora, there are much larger divergences. We
see a particularly large decrease in NP internal struc-
ture. Looking at some of the instances of this error, it
appears to be largely caused by incorrect handling of
structures such as URLs and phone numbers, which
do not appear in the PTB. There are also some more
difficult cases, for example:
... going up for sale in the next month or do .
where or do is a QP. This typographical error is ex-
tremely difficult to handle for a parser trained only
on well-formed text.
For e-mail there is a substantial drop on single
word phrases. Breaking the errors down by label we
found that the majority of the new errors are miss-
ing or extra NPs over single words. Here the main
problem appears to be temporal expressions, though
there also appear to be a substantial number of errors
that are also at the POS level, such as when NNP is
assigned to ta in this case:
... let you know that I ?m out ta here !
Some of these issues, such as URL handling,
could be resolved with suitable training data. Other
issues, such as ungrammatical language and uncon-
ventional use of words, pose a greater challenge.
6 Conclusion
The single F-score objective over brackets or depen-
dencies obscures important differences between sta-
tistical parsers. For instance, a single attachment er-
ror can lead to one or many mismatched brackets.
We have created a novel tree-transformation
methodology for evaluating parsers that categorises
errors into linguistically meaningful types. Using
this approach, we presented the first detailed exam-
ination of the errors produced by a wide range of
constituency parsers for English. We found that PP
attachment and clause attachment are the most chal-
lenging constructions, while coordination turns out
to be less problematic than previously thought. We
also noted interesting variations in error types for
parsers variants.
We investigated the errors resolved in reranking,
and introduced by changing domains. We found that
the Charniak rerankers improved most error types,
but made little headway on improving PP attach-
ment. Changing domain has an impact on all error
types, except NP internal structure.
We have released our system so that future con-
stituent parsers can be evaluated using our method-
ology. Our analysis provides new insight into the
development of parsers over the past fifteen years,
and the challenges that remain.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research was par-
tially supported by a General Sir John Monash Fel-
lowship to the first author, the Office of Naval Re-
search under MURI Grant No. N000140911081, an
NSF Fellowship to the second author, ARC Discov-
ery grant DP1097291, the Capital Markets CRC, and
the NSF under grant 0643742.
References
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306?311, Pacific Grove,
California, USA, February.
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated ccg supertagging and parsing. In Proceed-
ings of ACL, pages 470?480, Portland, Oregon, USA,
June.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local deep dependencies in a large corpus. In Proceed-
ings of EMNLP, pages 397?408, Edinburgh, United
Kingdom, July.
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Beam-width prediction for efficient
context-free parsing. In Proceedings of ACL, pages
440?449, Portland, Oregon, USA, June.
1057
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of ACL, pages 41?
48, Sydney, Australia, July.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake, 2002. Relational Evaluation Schemes,
pages 4?8. Las Palmas, Canary Islands, Spain, May.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal.
In Proceedings of LREC, pages 447?454, Granada,
Spain, May.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC, Valletta, Malta,
May.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Ann Ar-
bor, Michigan, USA, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139,
Seattle, Washington, USA, April.
Stephen Clark and Julia Hockenmaier. 2002. Evaluat-
ing a wide-coverage ccg parser. In Proceedings of the
LREC Beyond Parseval Workshop, Las Palmas, Ca-
nary Islands, Spain, May.
Andrew B. Clegg and Adrian J. Shepherd. 2005. Evalu-
ating and integrating treebank parsers on a biomedical
corpus. In Proceedings of the ACL Workshop on Soft-
ware, pages 14?33, Ann Arbor, Michigan, USA, June.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL,
pages 16?23, Madrid, Spain, July.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML, pages
175?182, Palo Alto, California, USA, June.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Rebecca Dridan and Stephan Oepen. 2011. Parser evalu-
ation using elementary dependency matching. In Pro-
ceedings of IWPT, pages 225?230, Dublin, Ireland,
October.
Aaron Dunlop, Nathan Bodenstab, and Brian Roark.
2011. Efficient matrix-encoded grammars and low la-
tency parallelization strategies for cyk. In Proceedings
of IWPT, pages 163?174, Dublin, Ireland, October.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the bnc: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings of LREC, Mar-
rakech, Morocco, May.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of EMNLP, pages 167?202,
Pittsburgh, Pennsylvania, USA, June.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of IWPT, pages 11?22, Prague,
Czech Republic, June.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2009. Descriptive and empirical approaches to captur-
ing underlying dependencies among parsing errors. In
Proceedings of EMNLP, pages 1162?1171, Singapore,
August.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of NAACL, pages 24?31, Edmonton, Canada,
May.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
ACL, pages 95?102, Barcelona, Spain, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586?594, Columbus, Ohio, USA, June.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the 4th Inter-
national Workshop on Linguistically Interpreted Cor-
pora at EACL, Budapest, Hungary, April.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of ACL,
pages 423?430, Sapporo, Japan, July.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. In Proceedings of NIPS, pages 3?10,
Vancouver, British Columbia, Canada, December.
Dekang Lin. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 4(2):97?114.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of NAACL, pages 152?159, New York, New York,
USA, June.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of EMNLP, pages 122?131,
Prague, Czech Republic, June.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
1058
Proceedings of ACL, pages 46?54, Columbus, Ohio,
USA, June.
Dominick Ng and James R. Curran. 2012. N-best CCG
parsing and reranking. In Proceedings of ACL, Jeju,
South Korea, July.
Dominick Ng, Matthew Honnibal, and James R. Curran.
2010. Reranking a wide-coverage ccg parser. In Pro-
ceedings of ALTA, pages 90?98, Melbourne, Australia,
December.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
Go?mez-Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of Coling, pages 833?841, Beijing, China, August.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411, Rochester, New York, USA, April.
Slav Petrov and Ryan McDonald. 2012. SANCL Shared
Task. LDC2012E43. Linguistic Data Consortium.
Philadelphia, Philadelphia, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL, pages
433?440, Sydney, Australia, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP,
pages 62?69, Sydney, Australia, July.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
IWPT, pages 48?57, Dublin, Ireland, October.
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using textual
entailments. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 51?56, Up-
psala, Sweden, July.
1059
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1146?1156, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Training Factored PCFGs with Expectation Propagation
David Hall and Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,klein}@cs.berkeley.edu
Abstract
PCFGs can grow exponentially as additional
annotations are added to an initially simple
base grammar. We present an approach where
multiple annotations coexist, but in a factored
manner that avoids this combinatorial explo-
sion. Our method works with linguistically-
motivated annotations, induced latent struc-
ture, lexicalization, or any mix of the three.
We use a structured expectation propagation
algorithm that makes use of the factored struc-
ture in two ways. First, by partitioning the fac-
tors, it speeds up parsing exponentially over
the unfactored approach. Second, it minimizes
the redundancy of the factors during training,
improving accuracy over an independent ap-
proach. Using purely latent variable annota-
tions, we can efficiently train and parse with
up to 8 latent bits per symbol, achieving F1
scores up to 88.4 on the Penn Treebank while
using two orders of magnitudes fewer parame-
ters compared to the na??ve approach. Combin-
ing latent, lexicalized, and unlexicalized anno-
tations, our best parser gets 89.4 F1 on all sen-
tences from section 23 of the Penn Treebank.
1 Introduction
Many high-performance PCFG parsers take an ini-
tially simple base grammar over treebank labels like
NP and enrich it with deeper syntactic features to
improve accuracy. This broad characterization in-
cludes lexicalized parsers (Collins, 1997), unlexical-
ized parsers (Klein and Manning, 2003), and latent
variable parsers (Matsuzaki et al 2005). Figures
1(a), 1(b), and 1(c) show small examples of context-
free trees that have been annotated in these ways.
When multi-part annotations are used in the same
grammar, systems have generally multiplied these
annotations together, in the sense that an NP that
was definite, possessive, and VP-dominated would
have a single unstructured PCFG symbol that en-
coded all three facts. In addition, modulo backoff
or smoothing, that unstructured symbol would of-
ten have rewrite parameters entirely distinct from,
say, the indefinite but otherwise similar variant of
the symbol (Klein and Manning, 2003). Therefore,
when designing a grammar, one would have to care-
fully weigh new contextual annotations. Should a
definiteness annotation be included, doubling the
number of NPs in the grammar and perhaps overly
fragmenting statistics? Or should it be excluded,
thereby losing important distinctions? Klein and
Manning (2003) discuss exactly such trade-offs and
omit annotations that were helpful on their own be-
cause they were not worth the combinatorial or sta-
tistical cost when combined with other annotations.
In this paper, we argue for grammars with fac-
tored annotations, that is, grammars with annota-
tions that have structured component parts that are
partially decoupled. Our annotated grammars can
include both latent and explicit annotations, as illus-
trated in Figure 1(d), and we demonstrate that these
factored grammars outperform parsers with unstruc-
tured annotations.
After discussing the factored representation, we
describe a method for parsing with factored anno-
tations, using an approximate inference technique
called expectation propagation (Minka, 2001). Our
algorithm has runtime linear in the number of an-
notation factors in the grammar, improving on the
na??ve algorithm, which has runtime exponential in
the number of annotations. Our method, the Ex-
pectation Propagation for Inferring Constituency
(EPIC) parser, jointly trains a model over factored
annotations, where each factor naturally leverages
information from other annotation factors and im-
proves on their mistakes.
1146
(a) NP[agenda]
NN[agenda]
agenda
NP[?s]
The president?s
(b) NP[?S]
NN[?NP]
agenda
NP[?NP-Poss-Det]
The president?s
(c) NP[1]
NN[0]
agenda
NP[1]
The president?s
(d) NP[agenda,?S,1]
NN[agenda,?NP,0]
agenda
NP[?s,?NP-Poss-Det,1]
The president?s
Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997);
(b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al
(2005); and (d) the factored, mixed annotations we argue for in our paper.
We demonstrate the empirical effectiveness of our
approach in two ways. First, we efficiently train
a latent-variable grammar with 8 disjoint one-bit
latent annotation factors, with scores as high as
89.7 F1 on length ?40 sentences from the Penn
Treebank (Marcus et al 1993). This latent vari-
able parser outscores the best of Petrov and Klein
(2008a)?s comparable parsers while using two or-
ders of magnitude fewer parameters. Second, we
combine our latent variable factors with lexicalized
and unlexicalized annotations, resulting in our best
F1 score of 89.4 on all sentences.
2 Intuitions
Modern theories of grammar such as HPSG (Pollard
and Sag, 1994) and Minimalism (Chomsky, 1992)
do not ascribe unstructured conjunctions of anno-
tations to phrasal categories. Rather, phrasal cat-
egories are associated with sequences of metadata
that control their function. For instance, an NP
might have annotations to the effect that it is sin-
gular, masculine, and nominative, with perhaps fur-
ther information about its animacy or other aspects
of the head noun. Thus, it is appealing for a gram-
mar to be able to model these (somewhat) orthog-
onal notions, but most models have no mechanism
to encourage this. As a notable exception, Dreyer
and Eisner (2006) tried to capture this kind of insight
by allowing factored annotations to pass unchanged
from parent label to child label, though they were not
able to demonstrate substantial gains in accuracy.
Moreover, there has been to our knowledge no at-
tempt to employ both latent and non-latent annota-
tions at the same time. There is good reason for this:
lexicalized or highly annotated grammars like those
of Collins (1997) or Klein and Manning (2003) have
a very large number of states and an even larger
number of rules. Further annotating these rules with
latent annotations would produce an infeasibly large
grammar. Nevertheless, it is a shame to sacrifice ex-
pert annotation just to get latent annotations. Thus,
it makes sense to combine these annotation methods
in a way that does not lead to an explosion of the
state space or a fragmentation of statistics.
3 Parsing with Annotations
Suppose we have a raw (binarized) treebank gram-
mar, with productions of the form A ? B C.
The typical process is to then annotate these rules
with additional information, giving rules of the form
A[x] ? B[y] C[z]. In the case of explicit annota-
tions, an x might include information about the par-
ent category, or a head word, or a combination of
things. In the case of latent annotations, x will be
an integer that may or may not correspond to some
linguistic notion. We are interested in the specific
case where each x is actually factored into M dis-
joint parts: A[x1, x2, . . . , xM ]. (See Figure 1(d).)
We call each component of x an annotation factor
or an annotation component.
1147
3.1 Annotation Classes
In this paper, we consider three kinds of annotation
models, representing three of the major traditions in
constituency parsing. Individually, none of our mod-
els are state-of-the-art, instead achieving F1 scores
in the mid-80?s on the Penn Treebank.
The first model is a relatively simple lexicalized
parser. We are not aware of a prior discriminative
lexicalized constituency parser, and it is quite dif-
ferent from the generative models of Collins (1997).
Broadly, it considers features over a binary rule an-
notated with head words: A[h] ? B[h] C[d] and
A[h] ? B[d] C[h], focusing on monolexical rule
features and bilexical dependency features. It is our
best individual model, scoring 87.3 F1 on the devel-
opment set.
The second is similar to the unlexicalized model
of Klein and Manning (2003). This parser starts
from a grammar with labels annotated with sibling
and parent information, and then adds specific an-
notations, such as whether an NP is possessive or
whether a symbol rewrites as a unary. This parser
gets 86.3, tying the original generative version of
Klein and Manning (2003).
Finally, we use a straightforward discriminative
latent variable model much like that of Petrov and
Klein (2008a). Here, each symbol is given a la-
tent annotation, referred to as a substate. Typically,
these substates correlate at least loosely with linguis-
tic phenomena. For instance, NP-1 might be associ-
ated with possessive NPs, while NP-3 might be for
adjuncts. Often, these latent integers are considered
as bit strings, with each bit indicating one latent an-
notation. Prior work in this area has considered the
effect of splitting and merging these states (Petrov et
al., 2006; Petrov and Klein, 2007), as well as ?mul-
tiscale? grammars (Petrov and Klein, 2008b). With
two states (or one bit of annotation), our version of
this parser gets 81.7 F1, edging out the compara-
ble parser of Petrov and Klein (2008a). On the other
hand, our parser gets 83.2 with four states (two bits),
short of the performance of prior work.1
1Much of the difference stems from the different binariza-
tion scheme we employ. We use head-outward binarization,
rather than the left-branching binarization they employed. This
change was to enable integrating lexicalization with our other
models.
3.2 Model Representation
We employ a general exponential family representa-
tion of our grammar. This representation is fairly
general, and?in its generic form?by no means
new, save for the focus on annotation components.
Formally, we begin with a parse tree T over base
symbols for some sentence w, and we decorate the
tree with annotations X , giving a parse tree T [X].
We focus on the case whenX partitions into disjoint
components X = [X1, X2, . . . , XM ]. These com-
ponents are decoupled in the sense that, conditioned
on the coarse tree T , each column of the annota-
tion is independent of every other column. How-
ever, they are crucially not independent conditioned
only on the sentence w. This model is represented
schematically in Figure 2(a).
The conditional probability P(T [X]|w, ?) of an
annotated tree given words is:
P(T [X]|w, ?)
=
?
m fm(T [Xm];w, ?m)?
T ?,X?
?
m fm(T ?[X ?m];w, ?m)
= 1Z(w, ?)
?
m
fm(T [Xm];w, ?m)
(1)
where the factors fm for each model take the form:
fm(T [Xm];w, ?m) = exp
(
?Tm?m(T,Xm,w)
)
Here, Xm is the annotation associated with a partic-
ular model m. ? is a feature function that projects
the raw tree, annotations, and words into a feature
vector. The features ? need to decompose into fea-
tures for each factor fm; we do not allow features
that take into account the annotation from two dif-
ferent components.
We further add a pruning filter that assigns zero
weight to any tree with a constituent that a baseline
unannotated grammar finds sufficiently unlikely, and
a weight of one to any other tree. This filter is similar
to that used in Petrov and Klein (2008a) and allows
for much more efficient training and inference.
Because our model is discriminative, training
takes the form of maximizing the probability of the
training trees given the words. This objective is con-
vex for deterministic annotations, but non-convex
for latent annotations. We (locally) optimize the
1148
flex funlflat f?lat f?unl
(a) (b) (c) 
Full product model Approximate model 
P (T [X]|w; ?) q(T |w)
qlex qlat
qunl
Figure 2: Schematic representation of our model, its approximation, and expectation propagation. (a) The full joint
distribution consists of a product of three grammars with different annotations, here lexicalized, latent, and unlexi-
calized. This model is described in Section 3.2. (b) The core approximation is an anchored PCFG with one factor
corresponding to each annotation component, described in Section 5.1. (c) Fitting the approximation with expectation
propagation, as described in Section 5.3. At the center is the core approximation. During each step, an ?augmented?
distribution qm is created by taking one annotation factor from the full grammar and the rest from the approximate
grammar. For instance, in upper left hand corner the full fLEX is substituted for f?LEX. This new augmented distribution
is projected back to the core approximation. This process is repeated for each factor until convergence.
(non-convex) log conditional likelihood of the ob-
served training data (T (d),w(d)):
`(?) =
?
d
log P(T (d)|w(d), ?)
=
?
d
log
?
X
P(T (d)[X]|w(d), ?)
(2)
Using standard results, the derivative takes the form:
?`(?) =
?
d
E[?(T,X,w)|T (d),w(d)]
?
?
d
E[?(T,X,w)|w(d)]
(3)
The first half of this derivative can be obtained by the
forward/backward-like computation defined by Mat-
suzaki et al(2005), while the second half requires
an inside/outside computation (Petrov and Klein,
2008a). The partition function Z(w, ?) is computed
as a byproduct of the latter computation. Finally,
this objective is regularized, using the L2 norm of ?
as a penalty.
We note that we omit from our parser one major
feature class found in other discriminative parsers,
namely those that use features over the words in the
span (Finkel et al 2008; Petrov and Klein, 2008b).
These features might condition on words on either
side of the split point of a binary rule or take into
account the length of the span. While such features
have proven useful in previous work, they are not the
focus of our current work and so we omit them.
4 The Complexity of Annotated
Grammars
Note that the first term of Equation 3?which is
conditioned on the coarse tree T?factors into M
pieces, one for each of the annotation components.
However, the second term does not factor because it
is conditioned on just the words w. Indeed, na??vely
computing this term requires parsing with the fully
articulated grammar, meaning that inference would
be no more efficient than parsing with non-factored
annotations.
Standard algorithms for parsing run in time
O(G|w|3), where |w| is the length of the sentence,
and G is the size of the grammar, measured in the
number of (binary) rules. Let G0 be the number
of binary rules in the unannotated ?base? grammar.
1149
Suppose that we have M annotation components.
Each annotation component can have up to A primi-
tive annotations per rule. For instance, a latent vari-
able grammar will have A = 8b where b is the num-
ber of bits of annotation. If we compile all annota-
tion components into unstructured annotations, we
can end up with a total grammar size of O(AMG0),
and so in general parsing time scales exponentially
with the number of annotation components. Thus, if
we use latent annotations and the hierarchical split-
ting approach of Petrov et al(2006), then the gram-
mar has size O(8SG0), where S is the number of
times the grammar was split in two. Therefore, the
size of annotated grammars can reach intractable
levels very quickly, particularly in the case of latent
annotations, where all combinations of annotations
are possible.
Petrov (2010) considered an approach to slowing
this growth down by using a set of M independently
trained parsers Pm, and parsed using the product
of the scores from each parser as the score for the
tree. This approach worked largely because train-
ing was intractable: if the training algorithm could
reach the global optimum, then this approach might
have yielded no gain. However, because the opti-
mization technique is local, the same algorithm pro-
duced multiple grammars.
In what follows, we propose another solution that
exploits the factored structure of our grammar with
expectation propagation. Crucially, we are able to
jointly train and parse with all annotation factors,
minimizing redundancy across the models. While
not exact, we will see that expectation propagation
is indeed effective.
5 Factored Inference
The key insight behind the approximate inference
methods we consider here is that the full model is
a product of complex factors that interact in compli-
cated ways, and we will approximate it with a prod-
uct of corresponding simple factors that interact in
simple ways. Since each annotation factor is a rea-
sonable model in both power and complexity on its
own, we can consider them one at a time, replac-
ing all others with their approximations, as shown in
Figure 2(c).
The way we will build these approximations is
with expectation propagation (Minka, 2001). Ex-
pectation propagation (EP) is a general method for
approximate inference that generalizes belief propa-
gation. We describe it here, but we first try to pro-
vide an intuition for how it functions in our system.
We also describe a simplified version of EP, called
assumed density filtering (Boyen and Koller, 1998),
which is somewhat easier to understand and rhetori-
cally convenient. For a more detailed introduction to
EP in general, we direct the reader to either Minka
(2001) or Wainwright and Jordan (2008). Our treat-
ment most resembles the former.
5.1 Factored Approximations
Our goal is to build an approximation that takes in-
formation from all components into account. To be-
gin, we note that each of these components captures
different phenomena: an unlexicalized grammar is
good at capturing structural relationships in a parse
tree (e.g. subject noun phrases have different dis-
tributions than object noun phrases), while a lexi-
calized grammar captures preferred attachments for
different verbs. At the same time, each of these com-
ponent grammars can be thought of as a refinement
of the raw unannotated treebank grammar. By itself,
each of these grammars induces a different poste-
rior distribution over unannotated trees for each sen-
tence. If we can approximate each model?s contri-
bution by using only unannotated symbols, we can
define an algorithm that avoids the exponential over-
head of parsing with the full grammar, and instead
works with each factor in turn.
To do so, we define a sentence specific core
approximation over unannotated trees q(T |w) =
?
m f?m(T,w). Figure 2(b) illustrates this approx-
imation. Here, q(T ) is a product of M structurally
identical factors, one for each of the annotated com-
ponents. We will approximate each model fm by
its corresponding f?m. Thus, there is one color-
coordinated approximate factor for each component
of the model in Figure 2(a).
There are multiple choices for the structure of
these factors, but we focus on anchored PCFGs. An-
chored PCFGs have productions of the form iAj ?
iBk kCj , where i, k, and j are indexes into the sen-
tence. Here, iAj is a symbol representing building
the base symbol A over the span [i, j].
Billott and Lang (1989) introduced anchored
1150
CFGs as ?shared forests,? and Matsuzaki et al
(2005) have previously used these grammars for
finding an approximate one-best tree in a latent vari-
able parser. Note that, even though an anchored
grammar is unannotated, because it is sentence spe-
cific it can represent many complex properties of the
full grammar?s posterior distribution for a given sen-
tence. For example, it might express a preference
for whether a PP token attaches to a particular verb
or to that verb?s object noun phrase in a particular
sentence.
Before continuing, note that a pointwise product
of anchored grammars is still an anchored gram-
mar. The complexity of parsing with a product of
these grammars is therefore no more expensive than
parsing with just one. Indeed, anchoring adds no
inferential cost at all over parsing with an unanno-
tated grammar: the anchored indices i, j, k have to
be computed just to parse the sentence at all. This
property is crucial to EP?s efficiency in our setting.
5.2 Assumed Density Filtering
We now describe a simplified version of EP: parsing
with assumed density filtering (Boyen and Koller,
1998). We would like to train a sequence ofM mod-
els, where each model is trained with knowledge
of the posterior distribution induced by the previous
models. Much as boosting algorithms (Freund and
Schapire, 1995) work by focusing learning on as-
yet-unexplained data points, this approach will en-
courage each model to improve on earlier models,
albeit in a different formal way.
At a high level, assumed density filtering (ADF)
proceeds as follows. First, we have an initially un-
informative q: it assigns the same probability to all
unpruned trees for a given sentence. Then, we fac-
tor in one of the annotated grammars and parse with
this new augmented grammar. This gives us a new
posterior distribution for this sentence over trees an-
notated with just that annotation component. Then,
we can marginalize out the annotations, giving us a
new q that approximates the annotated grammar as
closely as possible without using any annotations.
Once we have incorporated the current model?s com-
ponent, we move on to the next annotated grammar,
augmenting it with the new q, and repeating. In
this way, information from all grammars is incor-
porated into a final posterior distribution over trees
using only unannotated symbols. The algorithm is
then as follows:
? Initialize q(T ) uniformly.
? For each m in sequence:
1. Create the augmented distribution
qm(T[Xm]) ? q(T) ? fm(T[Xm]) and
compute inside and outside scores.
2. Minimize DKL
(
qm(T )||f?m(T )q(T )
)
by
fitting an anchored grammar f?m.
3. Set q(T ) =
?m
m?=1 f?m?(T ).
Step 1 of the inner loop forms an approximate pos-
terior distribution using fm, which is the parsing
model associated with component m, and q, which
is the anchored core approximation to the poste-
rior induced by the first m ? 1 models. Then, the
marginals are computed, and the new posterior dis-
tribution is projected to an anchored grammar, cre-
ating f?m. More intuitively, we create an anchored
PCFG that makes the approximation ?as close as
possible? to the augmented grammar. (We describe
this procedure more precisely in Section 5.4.) Thus,
each term fm is approximated in the context of the
terms that come before it. This contextual approx-
imation is essential: without it, ADF would ap-
proximate the terms independently, meaning that no
information would be shared between the models.
This method would be, in effect, a simple method
for parser combination, not all that dissimilar to the
method proposed by Petrov (2010). Finally, note
that the same inside and outside scores computed in
the loop can be used to compute the expected counts
needed in Equation 3.
Now we consider the runtime complexity of this
algorithm. If the maximum number of annotations
per rule for any factor is A, ADF has complex-
ity O
(
MAG0|w|3
)
when using M factors. In
contrast, parsing with the fully annotated grammar
would have complexityO
(
AMG0|w|3
)
. Critically,
for a latent variable parser with M annotation bits,
the exact algorithm takes time exponential in M ,
while this approximate algorithm takes time linear
in M .
It is worth pausing to consider what this algo-
rithm does during training. At each step, we have
1151
in q an approximation to what the posterior distribu-
tion looks like with the first m? 1 models. In some
places, q will assign high probabilities to spans in the
gold tree, and in some places it will not be so accu-
rate. ?m will be particularly motivated to correct the
latter, because they are less like the gold tree. On the
other hand, ?m will ignore the other ?correct? seg-
ments, because q has already sufficiently captured
them.
5.3 Expectation Propagation
While this sequential algorithm gives us a way to ef-
ficiently combine many kinds of annotations, it is
not a fully joint algorithm: there is no backward
propagation of information from later models to ear-
lier models. Ideally, no model should be privileged
over any other. To correct that, we use EP, which is
essentially the iterative generalization of ADF.
Intuitively, EP cycles among the models, updat-
ing the approximation for that model in turn so that
it closely resembles the predictions made by fm in
the context of all other approximations, as in Fig-
ure 2(c). Thus, each approximate term f?m is cre-
ated using information from all other f?m? , meaning
that the different annotation factors can still ?talk?
to each other. The product of these approximations
q will therefore come to act as an approximation to
the true posterior: it takes into account joint infor-
mation about all annotation components, all within
one tractable anchored grammar.
With that intuition in mind, EP is defined as fol-
lows:
? Initialize contributions f?m to the approximate
posterior q.
? At each step, choose m.
1. Include approximations to all factors other
than m: q\m(T ) =
?
m? 6=m f?m?(T ).
2. Create the augmented distribution by in-
cluding the actual factor for component m
qm(T [Xm]) ? fm(T [Xm])q\m(T )
and compute inside and outside scores.
3. Create a new f?m(T ) that minimizes
DKL
(
qm(T )||f?m(T )q\m(T )
)
.
? Finally, set q(T ) ??m f?m(T ).
Step 2 creates the augmented distribution qm, which
includes fm along with the approximate factors for
all models except the current model. Step 3 creates
a new anchored f?m that has the same marginal dis-
tribution as the true model fm in the context of the
other approximations, just as we did in ADF.
In practice, it is usually better to not recompute
the product of all f?m each time, but instead to main-
tain the full product q(T ) ? ?m f?m and to remove
the appropriate f?m by division. This optimization is
analogous to belief propagation, where messages are
removed from beliefs by division, instead of recom-
puting beliefs on the fly by multiplying all messages.
Schematically, the whole process is illustrated in
Figure 2(c). At each step, one piece of the core
approximation is replaced with the corresponding
component from the full model. This augmented
model is then reapproximated by a new core approx-
imation q after updating the corresponding f?m. This
process repeats until convergence.
5.4 EPIC Parsing
In our parser, EP is implemented as follows. q
and each of the f?m are anchored grammars that as-
sign weights to unannotated rules. The product of
anchored grammars with the annotated factor fm
need not be carried out explicitly. Instead, note
that an anchored grammar is just a function q(A ?
B C, i, k, j) ? R+ that returns a score for every an-
chored binary rule. This function can be easily in-
tegrated into the CKY algorithm for a single anno-
tated grammar by simply multiplying in the value
of q whenever computing the score of the respective
production over some span. The modified inside re-
currence takes the form:
INSIDE(A[x], i, j)
=
?
B,y,C,z
?T?(A[x]? B[y] C[z],w)
?
?
i<k<j
INSIDE(B[y], i, k) ? INSIDE(C[z], k, j)
? q(A? B C, i, k, j)
(4)
Thus, parsing with a pointwise product of an an-
chored grammar and an annotated grammar has no
increased combinatorial cost over parsing with just
the annotated grammar.
1152
To actually perform the projection in step 3 of EP,
we create an anchored grammar from inside and out-
side probabilities. First, we compute the expected
number of times the rule iAj ? iBk kCj occurs,
and then then we locally normalize for each sym-
bol iAj . This actually creates the new q distribution,
and so we have to divide out q\m This process mini-
mizes KL divergence subject to the local normaliza-
tion constraints.
All in all, this gives an algorithm that takes time
O
(
IMAG0|w|3
)
, where I is the maximum num-
ber of iterations, M is the number of models, and
A is the maximum number of annotations for any
given rule.
5.5 Other Inference Algorithms
To our knowledge, expectation propagation has been
used only once in the NLP community; Daume? III
and Marcu (2006) employed an unstructured ver-
sion in a Bayesian model of extractive summariza-
tion. Therefore, it is worth describing how EP dif-
fers from more familiar techniques.
EP can be thought of as a more flexible gen-
eralization of belief propagation, which has been
used several times in NLP (Smith and Eisner, 2008;
Niehues and Vogel, 2008; Cromie`res and Kurohashi,
2009; Burkett and Klein, 2012). In particular, EP al-
lows for the arbitrary choice of messages (the f?m),
meaning that we can use structured messages like
anchored PCFGs.
Mean field (Saul and Jordan, 1996) is another ap-
proximate inference technique that allows for struc-
tured approximations (Xing et al 2003; Burkett et
al., 2010), but here the natural version of mean field
for our model would still be intractable. However,
it is possible to adapt mean field into allowing for
tractable updates that are similar to the ones we pro-
posed. We do not pursue that approach here.
Dual decomposition (Dantzig and Wolfe, 1960;
Komodakis et al 2007) has recently become pop-
ular in the community (Rush et al 2010; Koo et
al., 2010). In fact, EP can be seen as a particular
kind of dual decomposition of the log normalization
constant logZ(w, ?) that is optimized with message
passing rather than (sub-)gradient descent or LP re-
laxations. Indeed, Minka (2001) argues that the EP
objective is more efficiently optimized with message
passing than with gradient updates. This assertion
should be examined for the structured models com-
mon in NLP, but that is beyond the scope of this pa-
per.
Finally, note that EP, like belief propagation but
unlike mean field, is not guaranteed to converge,
though in practice it usually seems to. In our exper-
iments, typically three or four iterations are enough
for almost all sentences to reach convergence, and
we found no loss in cutting off the number of itera-
tions to four.
6 Experiments
In what follows, we describe three experiments.
First, in a small experiment, we examine how effec-
tive the different inference algorithms are for both
training and testing. Second, we scale up our latent
variable model into successively larger products. Fi-
nally, we present a selection of the many possible
model combinations, showing that combining latent
and expert annotation can be quite effective.
6.1 Experimental Setup
For our experiments, we trained and tested on the
Penn Treebank using the standard splits: sections 2-
21 were training, 22 development, and 23 testing.
In preliminary experiments, we report development
set F1 on sentences up to length 40. For our final
test set experiment, we report F1 on sentences from
section 23 up to length 40, as well as all sentences
from that section. Scores reported are computed us-
ing EVALB (Sekine and Collins, 1997). We binarize
trees using Collins? head rules (Collins, 1997).
Each discriminative parser was trained using the
Adaptive Gradient variant of Stochastic Gradient
Descent (Duchi et al 2010). Smaller models were
seeded from larger models. That is, before training
a grammar of 5 models with 1 latent bit each, we
started with weights from a parser with 4 factored
bits. Initial experiments suggested this step did not
affect final performance, but greatly decreased to-
tal training time, especially for the latent variable
parsers. For extracting a one-best tree, we use a
version of the Max-Recall algorithm of Goodman
(1996). When using EP or ADF, we initialized
the core approximation q to the uniform distribution
over unpruned trees.
1153
Parsing
Training ADF EP Exact Petrov
ADF 84.3 84.5 84.5 82.5
EP 84.1 84.6 84.5 78.7
Exact 83.8 84.5 84.9 81.5
Indep. 82.3 82.1 82.2 82.6
Table 1: The effect of algorithm choice for training and
parsing on a product of two 2-state parsers on F1. Petrov
is the product parser of Petrov (2010), and Indep. refers
to independently trained models. For comparison, a four-
state parser achieves a score of 83.2.
When counting parameters, we consider the num-
ber of parameters per binary rule. Hence, a single
four-state latent model would have 64 (= 43) param-
eters per rule, while a product of 5 two-state models
would have just 40 (= 5 ? 23).
6.2 Comparison of Inference Algorithms
In our first experiment, we test the relative perfor-
mance of the various approximate inference meth-
ods at both train and test time. In order to include
exact inference, we necessarily need to look at a
smaller scale example for which exact inference is
still feasible. We examined development perfor-
mance for training and inference on a small product
of two parsers, each with two latent states per sym-
bol.
During training, we have several options. We can
use exact training by parsing with the fully articu-
lated product of both grammars, or, we can instead
use EP, ADF, or independent training. At test time,
we can parse using the full product of both gram-
mars, or, we can instead use EP, ADF, or we can use
the method of Petrov (2010) wherein we multiply
the parsers together in an ad hoc fashion.
The results are in Table 1. The best reported score,
unsurprisingly, is for using exact training and pars-
ing, but using EP for training and parsing results in
a relatively small loss of 0.3 F1. ADF, however, suf-
fers a loss of 0.6 F1 over Exact when used for train-
ing and parsing. Otherwise, Exact and EP seem to
perform fairly similarly at parse time for all training
conditions.
In general, there seems to be a gain for using the
same method for training and testing. Each test-
ing method performs at its best when using models
trained with the same method. Moreover, except for
ADF, the converse holds true: the grammars trained
80 
82 
84 
86 
88 
90 
1 2 3 4 5 6 7 8 
F1
 
Number of Models 
Figure 3: Development F1 plotted against the number M
of one-bit latent annotation components. The best gram-
mar has 6 one-bit annotations, with 89.7 F1.
with a given parsing method are best decoded using
the same method.
Oddly, using Petrov (2010)?s method does not
seem to work well at all for jointly trained models,
except for ADF. Similarly, joint parsing underper-
forms Petrov (2010)?s method when using indepen-
dently trained models. Likely, the joint parsing al-
gorithms are miscalibrating the redundant informa-
tion present in the two independently-trained mod-
els, while the two jointly-trained components come
to depend on each other. In fact, the F1 scores for
the two separate models of the EP parser are in the
60?s.
As expected, ADF does not perform as well as
EP. Therefore, we exclude it from our subsequent
experiments, focusing exclusively on EP.
6.3 Latent Variable Experiments
Most of the previous work in latent variable parsing
has focused on splitting smaller unstructured anno-
tations into larger unstructured annotations. Here,
we consider training a joint model consisting of a
large number of disjoint one-bit (i.e. two-state) la-
tent variable annotations. Specifically, we consider
the performance of products of up to 8 one-bit anno-
tations.
In Figure 3, we show development F1 as a func-
tion of the number of latent bits. Improvement is
roughly linear up to 3 components. Performance
levels off afterwards, with the top performing sys-
tem scoring 89.7 F1. Nevertheless, these parsers
outperform the comparable parsers of Petrov and
Klein (2008a) (89.3), even though our six-bit parser
has many fewer effective parameters per binary rule:
1154
Models F1, ? 40 F1, All
Lexicalized 87.3 86.5
Unlexicalized 86.3 85.4
3xLatent 88.6 87.6
Lex+Unlex 90.2 89.5
Lex+Lat 90.0 89.4
Unlex+Lat 90.0 89.4
Lex+Unlex+Lat 90.2 89.7
Table 2: Development F1 score for various model com-
binations for sentences less than length 40 and all sen-
tences. 3xLatent refers to a latent annotation model with
3 factored latent bits.
48 instead of the 4096 in their best parser. We also
ran our best system on Section 23, where it gets 89.1
and 88.4 on sentences less than length 40 and on all
sentences, respectively. This result compares favor-
ably to the 88.8/88.3 of Petrov and Klein (2008a).
6.4 Heterogeneous Models
We now consider factored models with different
kinds of annotations. Specifically, we tested gram-
mars comprising all subsets of {Lexicalized, Unlex-
icalized, Latent}. We used a model with 3 factored
bits as our representative of the latent variable class,
because it was closest in performance to the other
models. Of course, other smaller and larger combi-
nations are possible, but we found this selection to
be representative.
The development results are in Table 2. Unsur-
prisingly, adding more kinds of annotations helps for
the most part, though the combination of all three
components is not much better than a combination
of just the lexicalized and unlexicalized models. In-
deed, our best systems involved combining the lexi-
calized model with some other model. This is proba-
bly because the lexicalized model can represent very
different syntactic relationships than the latent and
unlexicalized models, meaning there is more diver-
sity in the joint model?s capacity when using combi-
nations involving the lexicalized annotations.
Finally, we ran our best system (the fully com-
bined one) on Section 23 of the Penn Treebank. It
scored 90.1/89.4 F1 on length 40 and all sentences
respectively, slightly edging out the 90.0/89.3 F1
of Petrov and Klein (2008a). However, it is not
quite as good at exact match: 37.7/35.3 vs 40.1/37.7.
Note, though, that their parser makes use of span
features, which deliver a gain of +0.3/0.2F1 respec-
tively, while ours does not. We suspect that similar
gains could be had by incorporating these features,
but we leave that for future work.
7 Conclusion
Factored representations capture a fundamental lin-
guistic insight: grammatical categories are not
monolithic, unanalyzable entities. Instead, they are
composed of numerous facets that together govern
how categories combine into parse trees.
We have developed a new model for grammars
with factored annotations and presented two meth-
ods for parsing with these grammars. Our ex-
periments have demonstrated that our approach
produces higher performance parsers with many
fewer parameters. Moreover, our model works
with both latent and explicit annotations, allowing
us to combine linguistic knowledge with machine
learning. Finally, our source code is available at
http://nlp.cs.berkeley.edu/Software.shtml.
Acknowledgments
We would like to thank Slav Petrov, David Burkett,
Adam Pauls, Greg Durrett and the anonymous re-
viewers for helpful comments. We would also like
to thank Daphne Koller for originally suggesting the
assumed density filtering approach. This work was
partially supported by BBN under DARPA contract
HR0011-12-C-0014, and by an NSF fellowship to
the first author.
References
Sylvie Billott and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association for
Computational Linguistics, pages 143?151, Vancou-
ver, British Columbia, Canada, June.
Xavier Boyen and Daphne Koller. 1998. Tractable in-
ference for complex stochastic processes. In Proceed-
ings of the 14th Conference on Uncertainty in Artificial
Intelligence?UAI 1998, pages 33?42. San Francisco:
Morgan Kaufmann.
David Burkett and Dan Klein. 2012. Fast inference in
phrase extraction models with belief propagation. In
NAACL.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In NAACL.
1155
Noam Chomsky. 1992. A minimalist program for lin-
guistic theory, volume 1. MIT Working Papers in Lin-
guistics, MIT, Cambridge Massachusetts.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In ACL, pages 16?23.
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL.
G. B. Dantzig and P. Wolfe. 1960. Decomposition
principle for linear programs. Operations Research,
8:101?111.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL), Sydney, Australia.
Markus Dreyer and Jason Eisner. 2006. Better informed
training of latent syntactic features. In EMNLP, pages
317?326, July.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. COLT.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In ACL 2008, pages 959?967.
Yoav Freund and Robert E. Schapire. 1995. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In ACL, pages 177?183.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In ICCV, pages 1?8.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82, Morristown, NJ, USA.
Thomas P. Minka. 2001. Expectation propagation for
approximate Bayesian inference. In UAI, pages 362?
369.
Jan Niehues and Stephan Vogel. 2008. Discriminative
word alignment via alignment matrix modeling. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 18?25, June.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT, April.
Slav Petrov and Dan Klein. 2008a. Discriminative log-
linear grammars with latent variables. In NIPS, pages
1153?1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing. In
EMNLP, pages 867?876, Honolulu, Hawaii, October.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July.
Slav Petrov. 2010. Products of random latent variable
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Los Angeles, California, June.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In EMNLP, pages 1?11, Cambridge, MA,
October.
Lawrence Saul and Michael Jordan. 1996. Exploit-
ing tractable substructures in intractable networks. In
NIPS 1995.
Satoshi Sekine and Michael J. Collins. 1997. Evalb ?
bracket scoring program.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145?
156, Honolulu, October.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Varia-
tional Inference. Now Publishers Inc., Hanover, MA,
USA.
Eric P. Xing, Michael I. Jordan, and Stuart J. Russell.
2003. A generalized mean field algorithm for varia-
tional inference in exponential families. In UAI, pages
583?591.
1156
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1898?1907,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A multi-Teraflop Constituency Parser using GPUs
John Canny David Hall Dan Klein
UC Berkeley
Berkeley, CA, 94720
canny@berkeley.edu, dlwh,klein@cs.berkeley.edu
Abstract
Constituency parsing with rich grammars re-
mains a computational challenge. Graph-
ics Processing Units (GPUs) have previously
been used to accelerate CKY chart evalua-
tion, but gains over CPU parsers were mod-
est. In this paper, we describe a collection of
new techniques that enable chart evaluation at
close to the GPU?s practical maximum speed
(a Teraflop), or around a half-trillion rule eval-
uations per second. Net parser performance
on a 4-GPU system is over 1 thousand length-
30 sentences/second (1 trillion rules/sec), and
400 general sentences/second for the Berkeley
Parser Grammar. The techniques we introduce
include grammar compilation, recursive sym-
bol blocking, and cache-sharing.
1 Introduction
Constituency parsing with high accuracy (e.g. latent
variable) grammars remains a computational chal-
lenge. The O(Gs3) complexity of full CKY pars-
ing for a grammar with G rules and sentence length
s, is daunting. Even with a host of pruning heuris-
tics, the high cost of constituency parsing limits its
uses. The most recent Berkeley latent variable gram-
mar for instance, has 1.7 million rules and requires
about a billion rule evaluations for inside scoring of
a single length-30 sentence. GPUs have previously
been used to accelerate CKY evaluation, but gains
over CPU parsers were modest. e.g. in Yi et al
(2011) a GPU parser is described for the Berkeley
Parser grammar which achieves 5 sentences per sec-
ond on the first 1000 sentences of Penn Treebank
section 22 Marcus et al (1993), which is compa-
rable with the best CPU parsers Petrov and Klein
(2007). Our parser achieves 120 sentences/second
per GPU for this sentence set, and over 250 sen-
tences/sec on length ? 30 sentences. These results
use a Berkeley Grammar approximately twice as big
as Yi et al (2011), an apparent 50x improvement.
On a 4-GPU system, we achieve 1000 sentences/sec
for length ? 30 sentences. This is 2 orders of mag-
nitude faster than CPU implementations that rely
heavily on pruning, and 5 orders of magnitude faster
than full CKY evaluation on a CPU.
Key to these results is a collection of new tech-
niques that enable GPU parsing at close to the
GPU?s practical maximum speed (a Teraflop for re-
cent GPUs), or around a half-trillion rule evaluations
per second. The techniques are:
1. Grammar compilation, which allows register-
to-register code for application of grammar
rules. This gives an order of magnitude (10x)
speedup over alternative approaches that use
shared memory.
2. Symbol/rule blocking of the grammar to re-
spect register, constant and instruction cache
limits. This is precondition for 1 above, and
the details of the partitioning have a big (> 4x)
effect on performance.
3. Sub-block partitioning to distribute rules across
the stream processors of the GPU and allow L2
cache acceleration. A factor of 2 improvement.
The code generated by our parser comes close to the
theoretical limits of the GPU. 80% of grammar rules
1898
are evaluated using a single-cycle register-to-register
instruction.
2 GPU Design Principles
In this paper, we focus on the architecture of recent
NVIDIA R? GPUs, though many of the principles we
describe here can be applied to other GPUs (e.g.
those made by AMD R?.) The current NVIDIA R?
KeplerTM series GPU contains between 2 and 16
?stream processors? or SMX?s which share an L2
cache interfacing to the GPUs main memory Anony-
mous (2013). The SMXs in turn comprise 192
cores which share a memory which is partitioned
into ?shared memory? and L1 cache. Shared mem-
ory supports relatively fast communication between
threads in an SMX. Communication between SMXs
has to pass through slower main memory.
The execution of instructions within SMXs is vir-
tualized and pipelined - i.e. it is not a simple task to
count processors, although there are nominally 192
in the KeplerTM series. Register storage is not at-
tached to cores, instead registers are associated in
blocks of 63 or 255 (depending on KeplerTM sub-
architecture) with running threads. Because of this,
it is usually easier for the programmer to think of
the SMXes as 1024 thread processors. These 1024
threads are grouped into 32 groups of 32 threads
called warps. Each warp of threads shares a program
counter and executes code in lock-step. However,
execution is not SIMD - all threads do not execute all
instructions. When the warp encounters a branching
instruction, all branches that are satisfied by some
thread will be executed in sequence. Each thread
only executes the instructions for its own branch,
and idles for the others. NVIDIA R? calls this model
SIMT (Single Instruction, Multiple Threads). Ex-
ecution of diverging branches by a warp is called
warp divergence. While it simplifies programming,
warp divergence understandably hurts performance
and our first goal is to avoid it.
GPUs are generally optimized for single-
precision floating point arithmetic in support of
rendering and simulation. Table 1 shows instruction
throughput (number of instructions that are executed
per cycle on each SMX). The KeplerTM series has
two architectural sub-generations (3.0 and 3.5) with
significant differences in double-precision support.
Data from Anonymous (2012) and NVIDIA (2012).
Instruction type
Architecture
3.0 3.5
Shared memory word access 32 32
FP arithmetic +,-,*,FMA 192 192
DP arithmetic +,-,*,FMA 8 64
Integer +,- 160 160
Integer *,FMA 32 32
float sin, exp, log,... 32 32
Table 1: Instructions per cycle per SMX in generation 3.0
and 3.5 KeplerTM devices
In the table, FP is floating point, DP is double
precision, and FMA is a single-cycle floating-piont
fused multiply-add used in most matrix and vector
operations (A? A+B ?C). Note next that floating
point (single precision) operations are extremely fast
and there is an FPU for each of the 192 processors.
Double precision floating point is 3x slower on high-
end 3.5 GPUS, and much slower (24x) on the com-
modity 3.0 machines. While integer addition is fast,
integer multiply is much slower. Perhaps most sur-
prising is the speed of single-precision transcenden-
tal function evaluation, log, exp, sin, cos, tan, etc.,
which are as fast as shared memory accesses or in-
teger multiplication, and which amount to a quarter-
trillion transcendental evaluations per second on a
GTX-680/K10.
PCFG grammar evaluation nominally requires
two multiplications and an addition per rule (section
4) which can be written:
Sij,m =
?
k=1...j; n,p?Q
Sik,nS(k+1)j,pcmnp (1)
i.e. the CKY node scores are sums of products of
pairs of scores and a weight. This suggests that at
least in principle, it?s possible to achieve a trillion
rule evaluations per second on a 680 or K10 device,
using a * and an FMA operation for each rule. That
assumes we are doing register-to-register operations
however. If we worked through shared memory (first
line of the table), we would be limited to about 80
billion evaluations/sec, 20 times slower. The anal-
ysis underscores that high performance for parsing
on a GPU is really a challenge of data movement.
We next review the different storage types and their
1899
bandwidths, since prudent use of, and movement be-
tween storage types is the key to performance.
2.1 Memory Types and Speeds
There are six types of storage on the GPU which
matter for us. For each type, we give the capac-
ity and aggregate bandwidth on a typical device (a
GTX-680 or K10 running at 1GHz).
Register files These are virtualized and associated
with threads rather than processors. 256kB per
SMX. Each thread in architecture 3.0 devices can
access 63 32-bit registers, or 255 registers for 3.5
devices. Aggregate bandwidth 40 TB/s.
Shared memory/L1 cache is shared by all threads
in an SMX. 64kB per SMX partitioned into shared
memory and cache functions. Aggregate bandwidth
about 1 TB/s.
Constant memory Each SMX has a 48kB read/only
cache separate from the L1 cache. It can store gram-
mar constants and has much higher bandwidth than
shared memory. Broadast bandwidth 13 TB/s.
Instruction cache is 8 KB per SMX. Aggregate
bandwidth 13 TB/s.
L2 cache is 0.5-1.5 MB, shared between all SMXs.
Aggregate bandwidth 500 GB/s.
Global memory is 2-6GB typically, and is shared
by all SMXs. GPUs use a particularly fast form of
SDRAM (compared to CPUs) but it is still much
slower than the other memory types above. Ag-
gregate bandwidth about 160 GB/s.
There is one more very important principle: Coa-
lesced main memory access. From the above it can
be seen that main memory access is much slower
than other memories and can easily bottleneck the
calculations. The figure above (160 GB/s) for main
memory access assumes such access is coalesced.
Each thread in the GPU has a thread and a block
number which determines where it runs on the hard-
ware. Consecutively-numbered threads should ac-
cess consecutive main memory locations for fast
memory access.
These parameters suggest a set of design princi-
ples for peak performance:
1. Maximize use of registers for symbol scores,
and minimize use of shared memory (in fact we
will not use it at all).
2. Maximize use of constant memory for rule
weights, and minimize use of shared memory.
3. Partition the rule set into blocks that respect the
limits on number of registers, constant memory
(needed for grammar rules probabilities) and
instruction cache limits.
4. Minimize main memory access and use L2
cache to speed it up.
Lets look in more detail at how to achieve this.
3 Anatomy of an Efficient GPU Parser
High performance on the GPU requires us to mini-
mize code divergence. This suggests that we do not
use a lexicalized grammar or a grammar that is sen-
sitive to the position of a span within the sentence.
These kinds of grammars?while highly accurate?
have irregular memory access patterns that conflict
with SIMD execution. Instead, an unlexicalized ap-
proach like that of Johnson (2011) or Klein and
Manning (2003), or a latent variable approach like
that of Matsuzaki et al (2005) or Petrov et al (2006)
are more appropriate. We opt for the latter kind: la-
tent variable grammars are fairly small, and their ac-
curacies rival lexicalized approaches.
Our GPU-ized inside algorithm maintains two
data structures: parse charts that store scores for
each labeled span, as usual, and a ?workspace? that
is used to actually perform the updates of the in-
side algorithm. Schematically, this memory lay-
out is represented in Figure 1. A queue is main-
tained CPU-side that enqueues work items of the
form (s, p, l, r), where s is a sentence, and p, l, and
r specify the index in the parse chart for parent, left
child, and right child, respectively. The outer loop
proceeds in increasing span length (or height of par-
ent node scores to be computed). Next the algorithm
iterates over the available sentences. Then it iterates
over the parent nodes at the current length in that
sentences, and finally over all split points for the cur-
rent parent node. In each case, work items are sent to
the queue with that span for all possible split points.
When the queue is full?or when there are no
more work items of that length?the queue is flushed
1900
Figure 1: The architecture of the system. Parse charts are
stored in triangular arrays laid out consecutively in mem-
ory. Scores for left and right children are transposed and
copied into the ?workspace? array, and the inside updates
are calculated for the parent. Scores are then pushed back
to the appropriate cell in the parse charts, maxing them
with scores that are already there. Transposition ensures
that reads and writes are coalesced.
to the GPU, which executes three steps. First, the
scores for each left and right child are copied into
the corresponding column in the workspace. Then
inside updates are applied in parallel for all cells to
get parent scores. Then parents are entered back to
their appropriate cells in the parse charts. This is
typically a many-to one atomic reduction (either a
sum for probability scores, or a max for max-sum
log probability scores). This process repeats until
all span lengths have been processed.
3.1 The Inside Updates
The high-level goal of our parser is to use SIMD
parallelism to evaluate the same rule across many
spans (1024 threads are currently used to process
8192 spans in each kernel). This approaches allows
us to satisfy the GPU performance desiderata from
the previous section. As discussed in section 5 each
GPU kernel actually processes a small subset of the
symbols and rules for the grammar, and kernels are
executed in sequence until the entire grammar has
been processed. Each thread iterates over the rules
in the same order, reading in symbols from the left
child and right child arrays in main memory as nec-
essary.
The two-dimensional work arrays must be stored
in ?symbol-major? order for this to work. That is,
the parent VP for one work item is stored next to the
parent VP for the next work item, while the VP sym-
bol for the first work item is stored on the next ?row?
of the work array. The reason the workspace cells
are stored in ?symbol-major? order is to maximize
coalesced access: each thread in the SMX accesses
the same symbol for a different work item in paral-
lel, and those work items are in consecutive memory
locations.
3.2 The Copy-transpose Operations
Unlike the workspace arrays, the arrays for the parse
charts are stored in ?span-major? order, transposed
from how they are stored in the workspace arrays.
That is, for a given span, the NP symbol is next
to the same span?s VP symbol (for example). This
order accelerates both symbol loading and Viterbi
search later on. It requires a transpose-copy in-
stead of ?non-transposed? copy to move from chart
to workspace arrays and back again, but note that a
non-transposed copy (or direct access to the chart
by the GPU compute kernel) would probably be
slower. The reason is that any linear ordering of
cells in the triangle table will produce short seg-
ments (less than 32 words and often less than 16)
of consecutive memory locations. This will lead to
many non-coalesced memory accesses. By contrast
the span-major representation always uses vectors
whose lengths equals the number of symbols (500-
1000), and these can be accessed almost entirely
with coalesced operations. The copy-transpose op-
erations are quite efficient (the transpose itself is
much faster than the I/O), and come close to the 160
GB/s GPU main memory limit.
The reverse copy-transpose (from parent
workspace cells to chart) is typically many-to-one,
since parent scores derive from multiple splits. They
are implemented using atomic reduce operations
(either atomic sum or atomic max) to ensure data
consistency.
At the heart of our approach is the use of grammar
compilation and symbol/rule blocking, described
next.
1901
4 Grammar Compilation
Each rule in a probabilistic context-free grammar
can be evaluated with an update of the form:
Sij,m =
?
k=1...j; n,p?Q
Sik,nS(k+1)j,pcmnp (2)
where Sij,m is the score for symbol m as a generator
of the span of words from position i to j in the in-
put sentence, cmnp is the probability that symbol m
generates the binary symbol pair n, p, and Q is the
set of symbols. The scores will be stored in a CKY
chart indexed by the span ij and the symbol m.
To evaluate (2) as fast as possible, we want to
use register variables which are limited in number.
The location indices i, j, k can be moved outside the
GPU kernel to reduce the variable count. We use
symbols P , L and R for respectively the score of the
parent, left child and right child in the CKY chart.
Then the core relation in (2) can be written as:
Pm =
?
n,p?Q
LnRpcmnp (3)
In the KeplerTM architecture, register arguments are
non-indexed, i.e. one cannot access register 3 as an
array variable R[i] with i=31. So in order to use
register storage for maximum speed, we must open-
code the grammar. Symbols like L3, R17 are en-
coded as variables L003 and R017, and each rule
must appear as a line of C code:
P043 += L003*R017*0.023123f;
P019 += L012*R123*6.21354e-7f;
: : : :
Open-coding the grammar likely has a host of per-
formance advantages. It allows both compiler and
hardware to ?see? what arguments are coming and
schedule the operations earlier than a ?grammar
as data? approach. Note that we show here the
sum-product code for computing inner/outer symbol
probabilities. For Viterbi parse extraction we replace
+,* with max,+ and work on log scores.
L and R variables must be loaded from main
memory, while P-values are initialized to zero and
then atomically combined (sum or max) with P-
values in memory. Loads are performed as late as
1Even if indexing were possible, it is extremely unlikely that
such accesses could complete in a single cycle
possible, that is, a load instruction will immediately
precede the first use of a symbol:
float R031 = right[tid+65*stride];
P001 += L001*R031*1.338202e-001f;
where tid is the thread ID plus an offset, and stride
is the row dimension of the workspace (typically
8192), and right is the main memory array of right
symbol scores. Similarly, atomic updates to P-
values occur as early as possible, right after the last
update to a value:
G020 += L041*R008*6.202160e-001f;
atomicAdd(&par[tid+6*stride],G020);
These load/store strategies minimize the active life
of each variable and allow reuse of register variables
for symbols whose lifetimes do not overlap. This
will be critical to successful blocking, described in
the next section.
4.1 Common subexpressions
One interesting discovery made by the compiler was
that the same L,R pair is repeated in several rules. In
hindsight, this is obvious because the symbols in this
grammar are splits of base symbols, and so splits of
the parent symbol will be involved in rules with each
pair of L,R splits. The compiler recognized this by
turning the L,R pair into a common subexpression
in a register. i.e. the compiler converts
P008 += L041*R008*6.200769e-001f;
P009 += L041*R008*6.201930e-001f;
P010 += L041*R008*6.202160e-001f;
into
float LRtmp = L041*R008;
P008 += LRtmp*6.200769e-001f;
P009 += LRtmp*6.201930e-001f;
P010 += LRtmp*6.202160e-001f;
and inspection of the resulting assembly code shows
that each rule is compiled into a single fused
multiply-add of LRtmp and a value from con-
stant memory into the P symbol register. This al-
lows grammar evaluation to approach the theoretical
Gflop limit of the GPU. For this to occur, the rules
need to be sorted with matching L,R pairs consecu-
tive. The compiler does not discover this constraint
otherwise or reorder instructions to make it possible.
1902
4.2 Exploiting L2 cache
Finally, we have to generate code to evaluate distinct
minor cube rulesets on each of the 8 SMXes con-
currently in order to benefit from the L2 cache, as
described in the next section. CUDATM (NVIDIA?s
GPU programming Platform) does not allow direct
control of SMX target, but we can achieve this by
running the kernel as 8 thread blocks and then test-
ing the block ID within the kernel and dispatching to
one of 8 blocks of rules. The CUDATM scheduler
will execute each thread block on a different SMX
which gives the desired distribution of code.
5 Symbol and Rule Blocking
The grammar formula (3) is very sparse. i.e. most
productions are impossible and most cmnp are zero.
For the Berkeley grammar used here, only 0.2% of
potential rules occur. Normally this would be bad
news for performance because it suggests low vari-
able re-use. However, the update relation is a tensor
rather than a matrix product. The re-use rate is deter-
mined by the number of rules in which a particular
symbol occurs, which is actually very high (more
than 1000 on average).
The number of symbols is about 1100 in this
grammar, and only a fraction can be stored in a
thread?s register set at one time (which is either 63 or
255 registers). To compute all productions we will
need to break the calculation into smaller groups of
variables that can fit in the available register space.
We can visualize this geometrically in figure 2.
The vectors of symbols P , L and R form the lead-
ing edges of this cube. The cube will be partitioned
into smaller subcubes indexed by subsets of those
symbols, and containing all the rules that apply be-
tween those symbols. The partitioning is chosen so
that the symbols in that subset can fit into available
register storage. In addition, the partitioning is cho-
sen to induce the same number of rules in each cube
- otherwise different code paths in the kernel will run
longer than others, and reduce overall performance.
This figure is a simplification - in order to balance
the number of rules in each subcube, the partition-
ing is not uniform in number of symbols as the figure
suggests.
As can be seen in figure 2, cube partitioning has
two levels. The original P-L-R cube is first par-
 
P R 
L 
Figure 2: Partition of the cube of symbol combinations
into major subcubes (left) and minor subcubes (right).
titioned into ?major? cubes, which are then parti-
tioned into ?minor? cubes (2x2x2 in the figure). A
major cube holds the symbols and rules that are ex-
ecuted in a single GPU kernel. The minor cubes in a
major cube hold the symbols and rules that are exe-
cuted in a particular SMX. For the GTX-680 or K10
with 8 SMXs, this allows different SMXs to concur-
rently work on different 2x2x2 subcubes in a major
cube. This arrangement substantially reduces main
memory bandwidth through the L2 cache (which is
shared between SMXes). Each symbol in a major
cube will be loaded just once from main memory,
but loaded into (up to) 4 different SMXes through
the L2 cache. Subcube division for caching in our
experiments roughly doubled the kernel?s speed.
However, simple partitioning will not work. e.g.
if we blocked into groups of 20 P, L, R symbols
(in order to fit into 60 registers), we would need
1100/20 = 55 blocks along each edge, and a total of
553 ? 160, 000 cells. Each symbol would need to
be loaded 552 = 3025 times, there would be almost
no symbol re-use. Throughput would be limited by
main memory speed to about 100 Gflops, an order
of magnitude slower than our target. Instead, we
use a rule partitioning scheme that creates as small a
symbol footprint as possible in each cube. We use a
spectral method to do this.
Before describing the spectral method we men-
tion an optimization that drops the symbol count by
2. Symbols are either terminal or non-terminal, and
in the Berkeley latent variable grammar there are
roughly equal numbers of them (503 non-terminals
and 631 terminals). All binary rules involve a non-
terminal parent. L and R symbols may be either ter-
minal or non-terminal, so there are 4 distinct types
1903
of rules depending on the L, R, types. We handle
each of these cases with a different kernel, which
rougly halves the number of rules along each edge
(it is either 503 or 631 along each edge). Further-
more, these kernels are called in different contexts,
and a different number of times. e.g. XX (L, R, both
non-terminal) kernels are called O(s3) times for sen-
tences of length s because both L, R children can oc-
cur at every position in the chart. XT and TX kernels
(with one terminal and one non-terminal symbol) are
called only O(s2) times since one of L or R must be
at the base of the chart. Finally TT kernels (both L
and R are terminals) will be called O(s) times. Per-
formance is therefore dominated by the XX kernel.
5.1 Spectral Partitioning
We explored a number of partitioning schemes for
both symbol and rule partitioning. In the end we
settled on a spectral symbol partitioning scheme.
Each symbol is a node in the graph to be parti-
tioned. Each node is assigned a feature vector de-
signed to match it to other nodes with similar sym-
bols occuring in many rules. There was considerable
evolution of this feature set to improve partitioning.
In the end the vector for a particular P symbol is
a = (a1, 0.1 ? a2, 0.1 ? a3) where a1 is a vector
whose elements are indexed by L, R pairs and whose
values represent the number of rules involving both
those symbols (and the parent symbol P), a2 encodes
L symbols and counts the number of rules contain-
ing that L symbol and P, and a3 encodes the R sym-
bols and counts rules containing that R symbol and
P. This feature vector produces a high similarity be-
tween P symbols that exactly share many L,R pairs
and lower similarity for shared L and R.
A spectral clustering/partitioning algorithm ap-
proximately minimizes the total edge weight of
graph cuts. In our case, the total weight of a cut is
to first order the product of the number of L,R pairs
that occur on each side of the cut, and to second or-
der the count of individual L and R pairs that span
the cut. Let S and T be the counts for a particular
LR pair or feature, then we are trying to minimize
the product S*T while keeping the sum S+T, which
is the total occurences of the feature on both sides of
the partition, constant. Such a product is minimized
when one of S or T is zero. Since many symbols
are involved, this typically does not happen to an in-
dividual symbol, but this heuristic is successful at
making the individual symbol or LR pair distribu-
tions across the cuts as unbalanced as possible. i.e.
one side of the cut has very few instances of a given
symbol. The number of instances of a symbol is an
upper bound on the number of subcells in which than
symbol occurs, and therefore on the number of times
it needs to be loaded from memory. Repeating this
operation recursively to produce a 3d cell decom-
position also concentrates each symbol in relatively
few cells, and so tends to reduce the total register
count per cell.
In a bit more detail, from the vectors a above we
construct a matrix A whose columns are the fea-
ture vectors for each P symbol. Next we construct
the symmetric normalized Laplacian L for the adja-
cency matrix ATA. We then compute the eigende-
composition of L, and extract the eigenvector cor-
responding to the second-smallest eigenvalue. Each
node in the graph is assign a real weight from the
corresponding element of this eigenvector. We sort
by these weights, and partition the symbols using
this sort order. We tried both recursive binary par-
titioning, and partitioning into k intervals using the
original sort order, and obtained better results with
the latter.
Partitioning is applied in order P, L, R to gener-
ate the major cubes of the rule/symbol partition, and
then again to generate minor cubes. This partition-
ing is far more efficient than a naive partitioning.
The XX ruleset for our Berkeley grammar has about
343,000 rules over a 5033 cube of non-terminal sym-
bols. The optimal PxLxR cube decomposition (op-
timal in net kernel throughput) for this ruleset was
6x2x2 for major cubes, and then 2x2x2 for minor
cubes. This requires 6x2x2=24 GPU kernels, each
of which encodes 2x2x2=8 code blocks (recall that
each of the 8 SMXs executes a different code block
from the same kernel)2. Most importantly the reload
rate (the mean number of major cells containing a
given symbol, or the mean number of times a sym-
bol needs to be reloaded from main memory) drops
to about 6 (vs. 3000 for naive partitioning). This
is very significant. Each symbol is used on average
343, 000/501 ? 6000 times overall by the XX ker-
2This cube decomposition also respects the constant cache
and instruction cache limits
1904
nel. Dropping the reload factor to 6 means that for
every 1 main memory load of a symbol, there are
approximately 1000 register or L2 cache reuses. A
little further calculation shows that L2 cache items
are used a little more than twice, so the register reuse
rate within kernel code blocks is close to 500 on av-
erage. This is what allows teraflop range speeds.
Note that while the maximum number of registers
per thread in the GTX-680 or K10 is 63, the aver-
age number of variables per minor cube is over 80
for our best-performing kernel, showing a number
of variables have non-overlapping lifetimes. Sorting
rules lexicographically by (L,R,P) does a good job
of minimizing variable lifetime overlap. However
the CUDATM compiler reorders variables anyway
with slightly worse performance on average (there
seems to be no way around this, other than generat-
ing assembly code directly).
6 GPU Viterbi Parse Extraction
In sequential programs for chart generation, it is pos-
sible to compute and save a pointer to the best split
point and score at each node in the chart. However,
here the scores at each node are computed with fine-
grained parallelism. The best split point and score
cannot be computed until all scores are available.
Thus there is a separate Viterbi step after chart scor-
ing.
The gap between GPU and CPU performance
is large enough that CPU Viterbi search was a
bottleneck, even though it requires asymptotically
less work (O(Gs2) worst case, O(Gs) typical) vs
O(Gs3) to compute the CKY scores. Therefore
we wrote a non-recusive GPU-based Viterbi search.
Current GPUs support ?high-level? recursion, but
there is no stack in the SMX. A recursive pro-
gram must create software stack space in either
shared memory or main memory which serious per-
formance impact on small function calls. Instead,
we use an iterative version of Viterbi parse extrac-
tion which uses pre-allocated array storage to store
its output, and such that the partially-complete out-
put array encodes all the information the algorithm
needs to proceed - i.e. the output array is also the
algorithm?s work queue.
Ignoring unaries for the moment, a binary parse
tree for a sentence of length n has 2n ? 1 nodes,
including preterminals, internal nodes, and the root.
We can uniquely represent a tree as an array with
2n ? 1 elements. In this representation, each index
corresponds to a node in prefix (depth-first) order.
For example, the root is always at position 0, and the
second node will correspond to the root?s left child.
If this second node has a left child, it will be the third
node, otherwise the third node will be the second?s
right sibling.
We can uniquely identify the topology of the tree
by storing the ?width? of each node in this array,
where the width is the number of words governed
by that constituent. For a node at position p, its left
child will always be at p + 1, and its right child will
always be at p+ 2 ?w`, where w` is the width of the
left child. The symbol for each node can obviously
be stored with the height. For unaries, we require
exactly one unary rule per node, with the possibil-
ity that it is the identity rule, and so we store two
nodes: one for the ?pre-unary? symbol, and one for
the ?post-unary.? (Identity unary transitions are re-
moved in post-processing.)
Algorithm 1 Non-recursive Viterbi implementation.
The algorithm proceeds left-to-right in depth-first
order along the array representing the tree.
Input: Sentence length n, parse chart V[i,j]
Output: Array tree of size 2?n?2
tree[0].preunary? ROOT
tree[0].width? n
i? 0 . Current leftmost position for span
for p? 0 to 2?n?2 do
j? i + tree[p].width . Rightmost position
postu? BestUnary(V, tree[p].preunary, i, j)
tree[p].postunary? parent
if tree[p].width = 1 then
i? i + 1
else
lc, rc, k? BestBinary(V, parent, i, j)
tree[p + 1].preunary? lc
tree[p + 1].width? k ? i
tree[p + 2?(k?i)].width? j - k
tree[p + 2?(k?i)].preunary? rc
end if
end for
Armed with this representation, we are ready to
describe algorithm 1. The algorithm proceeds in
1905
left-to-right order along the array. First, the sym-
bol of the root is recorded. Then, for each node in
the tree, we search for the best unary rule continuing
it. If the node is a terminal, then no more nodes can
contain the current word, and so we advance the po-
sition of the left most child. Otherwise, if the node
is a non-terminal, we then find its left and right chil-
dren, entering their respective symbols and widths
into the array representing the tree.
The GPU implementation follows the algorithm
outline above although is somewhat technical. Each
parse tree is handled by a separate thread block
(thread blocks are groups of threads that can com-
municate through shared memory, and run on a sin-
gle SMX). Each thread block includes a number of
threads which are used to rapidly (in partly parallel
fashion) iterate through rulesets and symbol vectors
for the BestBinary and BestUnary operations using
coalesced memory accesses. Each thread block first
loads the complete set of L and R scores for the
current split being explored. Recall that these are
in consecutive memory locations using the ?span-
major? ordering, so these loads are coalesced. Then
the thread block parallel-iterates through the rules
for the current parent symbol, which will be in a con-
tiguous block of memory since the rules are sorted
by parent symbol, and again are coalesced. The
thread block therefore needs storage for all the L, R
symbol scores and in addition working storage pro-
portional to the number of threads (to hold the best
child symbol and its score from each thread). The
number of threads is chosen to maximize speed: too
few will cause each thread to do more work and to
run more slowly. Too many will limit the number of
thread blocks (since the total threads concurrently
running on an SMX is 1024) that can run concur-
rently. We found 128 to be optimum.
With these techniques, Viterbi search consumes
approximately 1% of the parser?s running time. Its
throughput is around 10 Gflops, and it is 50-100x
faster than a CPU reference implementation.
7 Experiments
The parser was tested in an desktop computer with
one Intel E5-2650 processor, 64 GB ram, and
2 GTX-690 dual GPUs (effectively 4 GTX-680
GPUs). The high-level parser code is written in a
matrix library in the Scala language, which access
GPU code through JNI and using the JCUDA wrap-
per library for CUDATM.
XX-kernel throughput was 900 Gflops per GPU
for sum-product calculation (which uses a single
FMA for most rules) and 700 Gflops per GPU for
max-sum calculations (which requires two instruc-
tions for most rules). Net parser throughput in-
cluding max-sum CKY evaluation, Viterbi scoring
traspose-copy etc was between 500 and 600 gi-
gaflops per GPU, or about 2 teraflops total. Parsing
max-length-30 sentences from the Penn Treebank
test set ran at 250 sentences/sec per GPU, or 1000
sentences/sec total. General sentences were parsed
at about half this rate, 120 sentences/sec per GPU,
or 480 sentences/sec for the system.
8 Conclusions and Future Work
We described a new approach to GPU constituency
parsing with surprisingly fast performance, close
to the theoretical limits of the GPU and similar to
dense matrix multiplication which achieves the de-
vices highest practical throughput. The resulting
parser parses 1000 length-30 sentences per second
in a 4-GPU computer. The parser has immediate ap-
plication to parsing and eventually to parser training.
The two highest-priority extensions are:
Addition of pruning: coarse-to-fine score pruning
should be applicable to our GPU design as it is to
CPU parsers. GPU pruning will not be as granu-
lar as CPU pruning and is unlikely to yield as large
speedups (4-5 orders of magnitude are common for
CPU parser pruning). But on the other hand, we
hardly need speedups that large, and 1-2 orders of
magnitude would be very useful.
Direct generation of assembly code. Currently our
code generator produces (> 1.7 million lines, about
same as the number of rules) C source code which
must be compiled into GPU binary code. While it
takes only 8 seconds to generate the source code, it
takes more than an hour to compile it. The com-
piler evidently applies a number of optimizations
that we cannot disable, and this takes time. This
is an obstacle to e.g. using this framework to train
a parser where there would be frequent updates to
the grammar. However, since symbol variables cor-
respond almost one-to-one with registers (modulo
1906
lifetime overlap and reuse, which our code gener-
ator is slightly better at than the compiler), there is
no reason for our code generator not to generate as-
sembly code directly. Presumably assembly code is
much faster to translate into kernel modules than C
source, and hopefully this will lead to much faster
kernel generation.
8.1 Code Release
The code will be released under a BSD-style open
source license once its dependencies are fully in-
tegrated. Pre- and Final releases will be here
https://github.com/jcanny/BIDParse
References
Anonymous. 2012. CUDA C PROGRAMMING
GUIDE. Technical Report PG-02829-001-v5.0. In-
cluded with CUDATM Toolkit.
Anonymous. 2013. NVIDIA?s next generation CUDA
compute architecture: KeplerTM GK110. Technical
report. Included with CUDATM Tootkit.
Mark Johnson. 2011. Parsing in parallel on mul-
tiple cores and gpus. In Proceedings of the Aus-
tralasian Language Technology Association Workshop
2011, pages 29?37, Canberra, Australia, December.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. COMPUTA-
TIONAL LINGUISTICS, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
75?82, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
NVIDIA. 2012. private communication.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efficient parallel cky parsing on gpus.
In Proceedings of the 2011 Conference on Parsing
Technologies, Dublin, Ireland, October.
1907
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030?1039,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Finding Cognate Groups using Phylogenies
David Hall and Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,klein}@cs.berkeley.edu
Abstract
A central problem in historical linguistics
is the identification of historically related
cognate words. We present a generative
phylogenetic model for automatically in-
ducing cognate group structure from un-
aligned word lists. Our model represents
the process of transformation and trans-
mission from ancestor word to daughter
word, as well as the alignment between
the words lists of the observed languages.
We also present a novel method for sim-
plifying complex weighted automata cre-
ated during inference to counteract the
otherwise exponential growth of message
sizes. On the task of identifying cognates
in a dataset of Romance words, our model
significantly outperforms a baseline ap-
proach, increasing accuracy by as much as
80%. Finally, we demonstrate that our au-
tomatically induced groups can be used to
successfully reconstruct ancestral words.
1 Introduction
A crowning achievement of historical linguistics
is the comparative method (Ohala, 1993), wherein
linguists use word similarity to elucidate the hid-
den phonological and morphological processes
which govern historical descent. The comparative
method requires reasoning about three important
hidden variables: the overall phylogenetic guide
tree among languages, the evolutionary parame-
ters of the ambient changes at each branch, and
the cognate group structure that specifies which
words share common ancestors.
All three of these variables interact and inform
each other, and so historical linguists often con-
sider them jointly. However, linguists are cur-
rently required to make qualitative judgments re-
garding the relative likelihood of certain sound
changes, cognate groups, and so on. Several re-
cent statistical methods have been introduced to
provide increased quantitative backing to the com-
parative method (Oakes, 2000; Bouchard-Co?te? et
al., 2007; Bouchard-Co?te? et al, 2009); others have
modeled the spread of language changes and spe-
ciation (Ringe et al, 2002; Daume? III and Camp-
bell, 2007; Daume? III, 2009; Nerbonne, 2010).
These automated methods, while providing ro-
bustness and scale in the induction of ancestral
word forms and evolutionary parameters, assume
that cognate groups are already known. In this
work, we address this limitation, presenting a
model in which cognate groups can be discovered
automatically.
Finding cognate groups is not an easy task,
because underlying morphological and phonolog-
ical changes can obscure relationships between
words, especially for distant cognates, where sim-
ple string overlap is an inadequate measure of sim-
ilarity. Indeed, a standard string similarity met-
ric like Levenshtein distance can lead to false
positives. Consider the often cited example of
Greek /ma:ti/ and Malay /mata/, both meaning
?eye? (Bloomfield, 1938). If we were to rely on
Levenshtein distance, these words would seem to
be a highly attractive match as cognates: they are
nearly identical, essentially differing in only a sin-
gle character. However, no linguist would posit
that these two words are related. To correctly learn
that they are not related, linguists typically rely
on two kinds of evidence. First, because sound
change is largely regular, we would need to com-
monly see /i/ in Greek wherever we see /a/ in
Malay (Ross, 1950). Second, we should look at
languages closely related to Greek and Malay, to
see if similar patterns hold there, too.
Some authors have attempted to automatically
detect cognate words (Mann and Yarowsky, 2001;
Lowe and Mazaudon, 1994; Oakes, 2000; Kon-
drak, 2001; Mulloni, 2007), but these methods
1030
typically work on language pairs rather than on
larger language families. To fully automate the
comparative method, it is necessary to consider
multiple languages, and to do so in a model which
couples cognate detection with similarity learning.
In this paper, we present a new generative model
for the automatic induction of cognate groups
given only (1) a known family tree of languages
and (2) word lists from those languages. A prior
on word survival generates a number of cognate
groups and decides which groups are attested in
each modern language. An evolutionary model
captures how each word is generated from its par-
ent word. Finally, an alignment model maps the
flat word lists to cognate groups. Inference re-
quires a combination of message-passing in the
evolutionary model and iterative bipartite graph
matching in the alignment model.
In the message-passing phase, our model en-
codes distributions over strings as weighted finite
state automata (Mohri, 2009). Weighted automata
have been successfully applied to speech process-
ing (Mohri et al, 1996) and more recently to mor-
phology (Dreyer and Eisner, 2009). Here, we
present a new method for automatically compress-
ing our message automata in a way that can take
into account prior information about the expected
outcome of inference.
In this paper, we focus on a transcribed word
list of 583 cognate sets from three Romance lan-
guages (Portuguese, Italian and Spanish), as well
as their common ancestor Latin (Bouchard-Co?te?
et al, 2007). We consider both the case where
we know that all cognate groups have a surface
form in all languages, and where we do not know
that. On the former, easier task we achieve iden-
tification accuracies of 90.6%. On the latter task,
we achieve F1 scores of 73.6%. Both substantially
beat baseline performance.
2 Model
In this section, we describe a new generative
model for vocabulary lists in multiple related lan-
guages given the phylogenetic relationship be-
tween the languages (their family tree). The gener-
ative process factors into three subprocesses: sur-
vival, evolution, and alignment, as shown in Fig-
ure 1(a). Survival dictates, for each cognate group,
which languages have words in that group. Evo-
lution describes the process by which daughter
words are transformed from their parent word. Fi-
nally, alignment describes the ?scrambling? of the
word lists into a flat order that hides their lineage.
We present each subprocess in detail in the follow-
ing subsections.
2.1 Survival
First, we choose a number G of ancestral cognate
groups from a geometric distribution. For each
cognate group g, our generative process walks
down the tree. At each branch, the word may ei-
ther survive or die. This process is modeled in a
?death tree? with a Bernoulli random variable S`g
for each language ` and cognate group g specify-
ing whether or not the word died before reaching
that language. Death at any node in the tree causes
all of that node?s descendants to also be dead. This
process captures the intuition that cognate words
are more likely to be found clustered in sibling lan-
guages than scattered across unrelated languages.
2.2 Evolution
Once we know which languages will have an at-
tested word and which will not, we generate the
actual word forms. The evolution component of
the model generates words according to a branch-
specific transformation from a node?s immediate
ancestor. Figure 1(a) graphically describes our
generative model for three Romance languages:
Italian, Portuguese, and Spanish.1 In each cog-
nate group, each word W` is generated from its
parent according to a conditional distribution with
parameter ?`, which is specific to that edge in the
tree, but shared between all cognate groups.
In this paper, each ?` takes the form of a pa-
rameterized edit distance similar to the standard
Levenshtein distance. Richer models ? such as the
ones in Bouchard-Co?te? et al (2007) ? could in-
stead be used, although with an increased infer-
ential cost. The edit transducers are represented
schematically in Figure 1(b). Characters x and
y are arbitrary phonemes, and ?(x, y) represents
the cost of substituting x with y. ? represents the
empty phoneme and is used as shorthand for inser-
tion and deletion, which have parameters ? and ?,
respectively.
As an example, see the illustration in Fig-
ure 1(c). Here, the Italian word /fwOko/ (?fire?) is
generated from its parent form /fokus/ (?hearth?)
1Though we have data for Latin, we treat it as unobserved
to represent the more common case where the ancestral lan-
guage is unattested; we also evaluate our system using the
Latin data.
1031
GW
VL
W
PI
?
?
? ?
?
W
LA
?
S
LA
S
VL
S
PI
S
IT
S
ES
S
PT
L 
L 
w
pt
w
es
L 
?
w
IT
w
IT
w
IT
w
IT
w
IT
w
IT
W
IT
W
IT
S
u
r
v
i
v
a
l
E
v
o
l
u
t
i
o
n
f u sk
f w
? 
ok
A
l
i
g
n
m
e
n
t
(a)
(b)
(c)
x
:
y
 
/
 
?
(
x
,
y
)
x
:
?
/
?
x
 
 
 
 
?
:
y
/
?
y
o
Figure 1: (a) The process by which cognate words are generated. Here, we show the derivation of Romance language words
W` from their respective Latin ancestor, parameterized by transformations ?` and survival variables S`. Languages shown
are Latin (LA), Vulgar Latin (VL), Proto-Iberian (PI), Italian (IT), Portuguese (PT), and Spanish (ES). Note that only modern
language words are observed (shaded). (b) The class of parameterized edit distances used in this paper. Each pair of phonemes
has a weight ? for deletion, and each phoneme has weights ? and ? for insertion and deletion respectively. (c) A possible
alignment produced by an edit distance between the Latin word focus (?hearth?) and the Italian word fuoco (?fire?).
by a series of edits: two matches, two substitu-
tions (/u/? /o/, and /o/?/O/), one insertion (w)
and one deletion (/s/). The probability of each
individual edit is determined by ?. Note that the
marginal probability of a specific Italian word con-
ditioned on its Vulgar Latin parent is the sum over
all possible derivations that generate it.
2.3 Alignment
Finally, at the leaves of the trees are the observed
words. (We take non-leaf nodes to be unobserved.)
Here, we make the simplifying assumption that in
any language there is at most one word per lan-
guage per cognate group. Because the assign-
ments of words to cognates is unknown, we spec-
ify an unknown alignment parameter pi` for each
modern language which is an alignment of cognate
groups to entries in the word list. In the case that
every cognate group has a word in each language,
each pi` is a permutation. In the more general case
that some cognate groups do not have words from
all languages, this mapping is injective from words
to cognate groups. From a generative perspective,
pi` generates observed positions of the words in
some vocabulary list.
In this paper, our task is primarily to learn the
alignment variables pi`. All other hidden variables
are auxiliary and are to be marginalized to the
greatest extent possible.
3 Inference of Cognate Assignments
In this section, we discuss the inference method
for determining cognate assignments under fixed
parameters ?. We are given a set of languages and
a list of words in each language, and our objec-
tive is to determine which words are cognate with
each other. Because the parameters pi` are either
permutations or injections, the inference task is re-
duced to finding an alignment pi of the respective
word lists to maximize the log probability of the
observed words.
pi? = arg max
pi
?
g
log p(w(`,pi`(g))|?, pi,w?`)
w(`,pi`(g)) is the word in language ` that pi` has
assigned to cognate group g. Maximizing this
quantity directly is intractable, and so instead we
use a coordinate ascent algorithm to iteratively
1032
maximize the alignment corresponding to a
single language ` while holding the others fixed:
pi?` = arg max
pi`
?
g
log p(w(`,pi`(g))|?, pi?`, pi`,w?`)
Each iteration is then actually an instance of
bipartite graph matching, with the words in one
language one set of nodes, and the current cognate
groups in the other languages the other set of
nodes. The edge affinities aff between these
nodes are the conditional probabilities of each
word w` belonging to each cognate group g:
aff(w`, g) = p(w`|w?`,pi?`(g), ?, pi?`)
To compute these affinities, we perform in-
ference in each tree to calculate the marginal
distribution of the words from the language `.
For the marginals, we use an analog of the for-
ward/backward algorithm. In the upward pass, we
send messages from the leaves of the tree toward
the root. For observed leaf nodes Wd, we have:
?d?a(wa) = p(Wd = wd|wa, ?d)
and for interior nodes Wi:
?i?a(wa) =
?
wi
p(wi|wa, ?i)
?
d?child(wi)
?d?i(wi)
(1)
In the downward pass (toward the lan-
guage `), we sum over ancestral words Wa:
?a?d(wd)
=
?
wa
p(wd|wa, ?d)?a??a(wa)
?
d??child(wa)
d? 6=d
?d??a(wa)
where a? is the ancestor of a. Computing these
messages gives a posterior marginal distribution
?`(w`) = p(w`|w?`,pi?`(g), ?, pi?`), which is pre-
cisely the affinity score we need for the bipartite
matching. We then use the Hungarian algorithm
(Kuhn, 1955) to find the optimal assignment for
the bipartite matching problem.
One important final note is initialization. In our
early experiments we found that choosing a ran-
dom starting configuration unsurprisingly led to
rather poor local optima. Instead, we started with
empty trees, and added in one language per itera-
tion until all languages were added, and then con-
tinued iterations on the full tree.
4 Learning
So far we have only addressed searching for
Viterbi alignments pi under fixed parameters. In
practice, it is important to estimate better para-
metric edit distances ?` and survival variables
S`. To motivate the need for good transducers,
consider the example of English ?day? /deI/ and
Latin ?die?s? /dIe:s/, both with the same mean-
ing. Surprisingly, these words are in no way re-
lated, with English ?day? probably coming from a
verb meaning ?to burn? (OED, 1989). However,
a naively constructed edit distance, which for ex-
ample might penalize vowel substitutions lightly,
would fail to learn that Latin words that are bor-
rowed into English would not undergo the sound
change /I/?/eI/. Therefore, our model must learn
not only which sound changes are plausible (e.g.
vowels turning into other vowels is more common
than vowels turning into consonants), but which
changes are appropriate for a given language.2
At a high level, our learning algorithm is much
like Expectation Maximization with hard assign-
ments: after we update the alignment variables pi
and thus form new potential cognate sets, we re-
estimate our model?s parameters to maximize the
likelihood of those assignments.3 The parameters
can be learned through standard maximum likeli-
hood estimation, which we detail in this section.
Because we enforce that a word in language d
must be dead if its parent word in language a is
dead, we just need to learn the conditional prob-
abilities p(Sd = dead|Sa = alive). Given fixed
assignments pi, the maximum likelihood estimate
can be found by counting the number of ?deaths?
that occurred between a child and a live parent,
applying smoothing ? we found adding 0.5 to be
reasonable ? and dividing by the total number of
live parents.
For the transducers ?, we learn parameterized
edit distances that model the probabilities of dif-
ferent sound changes. For each ?` we fit a non-
uniform substitution, insertion, and deletion ma-
trix ?(x, y). These edit distances define a condi-
2We note two further difficulties: our model does not han-
dle ?borrowings,? which would be necessary to capture a
significant portion of English vocabulary; nor can it seam-
lessly handle words that are inherited later in the evolution of
language than others. For instance, French borrowed words
from its parent language Latin during the Renaissance and
the Enlightenment that have not undergone the same changes
as words that evolved ?naturally? from Latin. See Bloom-
field (1938). Handling these cases is a direction for future
research.
3Strictly, we can cast this problem in a variational frame-
work similar to mean field where we iteratively maximize pa-
rameters to minimize a KL-divergence. We omit details for
clarity.
1033
tional exponential family distribution when condi-
tioned on an ancestral word. That is, for any fixed
wa:
?
wd
p(wd|wa, ?) =
?
wd
?
z?
align(wa,wd)
score(z;?)
=
?
wd
?
z?
align(wa,wd)
?
(x,y)?z
?(x, y) = 1
where align(wa, wd) is the set of possible align-
ments between the phonemes in words wa and wd.
We are seeking the maximum likelihood esti-
mate of each ?, given fixed alignments pi:
??` = arg max
?`
p(w|?, pi)
To find this maximizer for any given pi`, we
need to find a marginal distribution over the
edges connecting any two languages a and
d. With this distribution, we calculate the
expected ?alignment unigrams.? That is, for
each pair of phonemes x and y (or empty
phoneme ?), we need to find the quantity:
Ep(wa,wd)[#(x, y; z)] =
?
wa,wd
?
z?
align(wa,wd)
#(x,y; z)p(z|wa, wd)p(wa, wd)
where we denote #(x, y; z) to be the num-
ber of times the pair of phonemes (x, y) are
aligned in alignment z. The exact method for
computing these counts is to use an expectation
semiring (Eisner, 2001).
Given the expected counts, we now need to nor-
malize them to ensure that the transducer repre-
sents a conditional probability distribution (Eis-
ner, 2002; Oncina and Sebban, 2006). We have
that, for each phoneme x in the ancestor language:
?y =
E[#(?, y; z)]
E[#(?, ?; z)]
?(x, y) = (1?
?
y?
?y?)
E[#(x, y; z)]
E[#(x, ?; z)]
?x = (1?
?
y?
?y?)
E[#(x, ?; z)]
E[#(x, ?; z)]
Here, we have #(?, ?; z) =
?
x,y #(x, y; z) and
#(x, ?; z) =
?
y #(x, y; z). The (1 ?
?
y? ?y?)
term ensure that for any ancestral phoneme x,
?
y ?y+
?
y ?(x, y)+?x = 1. These equations en-
sure that the three transition types (insertion, sub-
stitution/match, deletion) are normalized for each
ancestral phoneme.
5 Transducers and Automata
In our model, it is not just the edit distances
that are finite state machines. Indeed, the words
themselves are string-valued random variables that
have, in principle, an infinite domain. To represent
distributions and messages over these variables,
we chose weighted finite state automata, which
can compactly represent functions over strings.
Unfortunately, while initially compact, these au-
tomata become unwieldy during inference, and so
approximations must be used (Dreyer and Eisner,
2009). In this section, we summarize the standard
algorithms and representations used for weighted
finite state transducers. For more detailed treat-
ment of the general transducer operations, we di-
rect readers to Mohri (2009).
A weighted automaton (resp. transducer) en-
codes a function over strings (resp. pairs of
strings) as weighted paths through a directed
graph. Each edge in the graph has a real-valued
weight4 and a label, which is a single phoneme
in some alphabet ? or the empty phoneme ? (resp.
pair of labels in some alphabet ???). The weight
of a string is then the sum of all paths through the
graph that accept that string.
For our purposes, we are concerned with three
fundamental operations on weighted transducers.
The first is computing the sum of all paths through
a transducer, which corresponds to computing the
partition function of a distribution over strings.
This operation can be performed in worst-case
cubic time (using a generalization of the Floyd-
Warshall algorithm). For acyclic or feed-forward
transducers, this time can be improved dramati-
cally by using a generalization of Djisktra?s algo-
rithm or other related algorithms (Mohri, 2009).
The second operation is the composition of two
transducers. Intuitively, composition creates a new
transducer that takes the output from the first trans-
ducer, processes it through the second transducer,
and then returns the output of the second trans-
ducer. That is, consider two transducers T1 and
T2. T1 has input alphabet ? and output alpha-
bet ?, while T2 has input alphabet ? and out-
put alphabet ?. The composition T1 ? T2 returns
a new transducer over ? and ? such that (T1 ?
T2)(x, y) =
?
u T1(x, u) ? T2(u, y). In this paper,
we use composition for marginalization and fac-
tor products. Given a factor f1(x, u;T1) and an-
4The weights can be anything that form a semiring, but for
the sake of exposition we specialize to real-valued weights.
1034
other factor f2(u, y;T2), composition corresponds
to the operation ?(x, y) =
?
u f1(x, u)f2(u, y).
For two messages ?1(w) and ?2(w), the same al-
gorithm can be used to find the product ?(w) =
?1(w)?2(w).
The third operation is transducer minimization.
Transducer composition produces O(nm) states,
where n and m are the number of states in each
transducer. Repeated compositions compound the
problem: iterated composition of k transducers
produces O(nk) states. Minimization alleviates
this problem by collapsing indistinguishable states
into a single state. Unfortunately, minimization
does not always collapse enough states. In the next
section we discuss approaches to ?lossy? mini-
mization that produce automata that are not ex-
actly the same but are much smaller.
6 Message Approximation
Recall that in inference, when summing out in-
terior nodes wi we calculated the product over
incoming messages ?d?i(wi) (Equation 1), and
that these products are calculated using transducer
composition. Unfortunately, the maximal number
of states in a message is exponential in the num-
ber of words in the cognate group. Minimization
can only help so much: in order for two states to
be collapsed, the distribution over transitions from
those states must be indistinguishable. In practice,
for the automata generated in our model, mini-
mization removes at most half the states, which is
not sufficient to counteract the exponential growth.
Thus, we need to find a way to approximate a mes-
sage ?(w) using a simpler automata ??(w; ?) taken
from a restricted class parameterized by ?.
In the context of transducers, previous authors
have focused on a combination of n-best lists
and unigram back-off models (Dreyer and Eis-
ner, 2009), a schematic diagram of which is in
Figure 2(d). For their problem, n-best lists are
sensible: their nodes? local potentials already fo-
cus messages on a small number of hypotheses.
In our setting, however, n-best lists are problem-
atic; early experiments showed that a 10,000-best
list for a typical message only accounts for 50%
of message log perplexity. That is, the posterior
marginals in our model are (at least initially) fairly
flat.
An alternative approach might be to simply
treat messages as unnormalized probability distri-
butions, and to minimize the KL divergence be-
e
 
g
 
u
 
f
 
e
 
o
 
 
 
f
u
u
  u
e
u
g
u
o
u
  
    f
f
f
f
e
e
e
e
e
g
g
g
    g
g
o
o
o
o
o
 f
2 3
e
u
g
o
f
0 1
f
e
o
4
g
o
e
u
f
u
e
o
f
g
5
o
g
u
f
f
u e g o
f
eu g o
f
e u g
f
e e
f
u
e
g
  g
(a)
(b)
(c)
(d)
u
g
o
  e
  u
  f
  o
Figure 2: Various topologies for approximating topologies:
(a) a unigram model, (b) a bigram model, (c) the anchored
unigram model, and (d) the n-best plus backoff model used in
Dreyer and Eisner (2009). In (c) and (d), the relative height
of arcs is meant to convey approximate probabilities.
tween some approximating message ??(w) and the
true message ?(w). However, messages are not
always probability distributions and ? because the
number of possible strings is in principle infinite ?
they need not sum to a finite number.5 Instead, we
propose to minimize the KL divergence between
the ?expected? marginal distribution and the ap-
proximated ?expected? marginal distribution:
?? = arg min
?
DKL(?(w)?(w)||?(w)??(w; ?))
= arg min
?
?
w
?(w)?(w) log
?(w)?(w)
?(w)??(w; ?)
= arg min
?
?
w
?(w)?(w) log
?(w)
??(w; ?)
(2)
where ? is a term acting as a surrogate for the pos-
terior distribution over w without the information
from ?. That is, we seek to approximate ? not on
its own, but as it functions in an environment rep-
resenting its final context. For example, if ?(w) is
a backward message, ? could be a stand-in for a
forward probability.6
In this paper, ?(w) is a complex automaton with
potentially many states, ??(w; ?) is a simple para-
metric automaton with forms that we discuss be-
low, and ?(w) is an arbitrary (but hopefully fairly
simple) automaton. The actual method we use is
5As an extreme example, suppose we have observed that
Wd = wd and that p(Wd = wd|wa) = 1 for all ancestral
words wa. Then, clearly
P
wd
?(wd) =
P
wd
P
p(Wd =
wd|wa) = ? whenever there are an infinite number of pos-
sible ancestral strings wa.
6This approach is reminiscent of Expectation Propaga-
tion (Minka, 2001).
1035
as follows. Given a deterministic prior automa-
ton ? , and a deterministic automaton topology ???,
we create the composed unweighted automaton
? ????, and calculate arc transitions weights to min-
imize the KL divergence between that composed
transducer and ? ? ?. The procedure for calcu-
lating these statistics is described in Li and Eis-
ner (2009), which amounts to using an expectation
semiring (Eisner, 2001) to compute expected tran-
sitions in ? ? ??? under the probability distribution
? ? ?.
From there, we need to create the automaton
??1 ? ? ? ??. That is, we need to divide out the
influence of ?(w). Since we know the topology
and arc weights for ? ahead of time, this is often
as simple as dividing arc weights in ? ? ?? by the
corresponding arc weight in ?(w). For example,
if ? encodes a geometric distribution over word
lengths and a uniform distribution over phonemes
(that is, ?(w) ? p|w|), then computing ?? is as sim-
ple as dividing each arc in ? ? ?? by p.7
There are a number of choices for ? . One is a
hard maximum on the length of words. Another is
to choose ?(w) to be a unigram language model
over the language in question with a geometric
probability over lengths. In our experiments, we
find that ?(w) can be a geometric distribution over
lengths with a uniform distribution over phonemes
and still give reasonable results. This distribution
captures the importance of shorter strings while
still maintaining a relatively weak prior.
What remains is the selection of the topologies
for the approximating message ??. We consider
three possible approximations, illustrated in Fig-
ure 2. The first is a plain unigram model, the
second is a bigram model, and the third is an an-
chored unigram topology: a position-specific un-
igram model for each position up to some maxi-
mum length.
The first we consider is a standard unigram
model, which is illustrated in Figure 2(a). It
has |?| + 2 parameters: one weight ?a for each
phoneme a ? ?, a starting weight ?, and a stop-
ping probability ?. ?? then has the form:
??(w) = ??
?
i?|w|
?wi
Estimating this model involves only computing
the expected count of each phoneme, along with
7Also, we must be sure to divide each final weight in the
transducer by (1 ? |?|p), which is the stopping probability
for a geometric transducer.
the expected length of a word, E[|w|]. We then
normalize the counts according to the maximum
likelihood estimate, with arc weights set as:
?a ? E[#(a)]
Recall that these expectations can be computed us-
ing an expectation semiring.
Finally, ? can be computed by ensuring that the
approximate and exact expected marginals have
the same partition function. That is, with the other
parameters fixed, solve:
?
w
?(w)??(w) =
?
w
?(w)?(w)
which amounts to rescaling ?? by some constant.
The second topology we consider is the bigram
topology, illustrated in Figure 2(b). It is similar
to the unigram topology except that, instead of
a single state, we have a state for each phoneme
in ?, along with a special start state. Each state
a has transitions with weights ?b|a = p(b|a) ?
E[#(b|a)]. Normalization is similar to the un-
igram case, except that we normalize the transi-
tions from each state.
The final topology we consider is the positional
unigram model in Figure 2(c). This topology takes
positional information into account. Namely, for
each position (up to some maximum position), we
have a unigram model over phonemes emitted at
that position, along with the probability of stop-
ping at that position (i.e. a ?sausage lattice?). Es-
timating the parameters of this model is similar,
except that the expected counts for the phonemes
in the alphabet are conditioned on their position in
the string. With the expected counts for each posi-
tion, we normalize each state?s final and outgoing
weights. In our experiments, we set the maximum
length to seven more than the length of the longest
observed string.
7 Experiments
We conduct three experiments. The first is a ?com-
plete data? experiment, in which we reconstitute
the cognate groups from the Romance data set,
where all cognate groups have words in all three
languages. This task highlights the evolution and
alignment models. The second is a much harder
?partial data? experiment, in which we randomly
prune 20% of the branches from the dataset ac-
cording to the survival process described in Sec-
tion 2.1. Here, only a fraction of words appear
1036
in any cognate group, so this task crucially in-
volves the survival model. The ultimate purpose
of the induced cognate groups is to feed richer
evolutionary models, such as full reconstruction
models. Therefore, we also consider a proto-word
reconstruction experiment. For this experiment,
using the system of Bouchard-Co?te? et al (2009),
we compare the reconstructions produced from
our automatic groups to those produced from gold
cognate groups.
7.1 Baseline
As a novel but heuristic baseline for cognate group
detection, we use an iterative bipartite matching
algorithm where instead of conditional likelihoods
for affinities we use Dice?s coefficient, defined for
sets X and Y as:
Dice(X,Y ) =
2|X ? Y |
|X|+ |Y |
(3)
Dice?s coefficients are commonly used in bilingual
detection of cognates (Kondrak, 2001; Kondrak et
al., 2003). We follow prior work and use sets of
bigrams within words. In our case, during bipar-
tite matching the set X is the set of bigrams in the
language being re-permuted, and Y is the union of
bigrams in the other languages.
7.2 Experiment 1: Complete Data
In this experiment, we know precisely how many
cognate groups there are and that every cognate
group has a word in each language. While this
scenario does not include all of the features of the
real-world task, it represents a good test case of
how well these models can perform without the
non-parametric task of deciding how many clus-
ters to use.
We scrambled the 583 cognate groups in the
Romance dataset and ran each method to conver-
gence. Besides the heuristic baseline, we tried our
model-based approach using Unigrams, Bigrams
and Anchored Unigrams, with and without learn-
ing the parametric edit distances. When we did not
use learning, we set the parameters of the edit dis-
tance to (0, -3, -4) for matches, substitutions, and
deletions/insertions, respectively. With learning
enabled, transducers were initialized with those
parameters.
For evaluation, we report two metrics. The first
is pairwise accuracy for each pair of languages,
averaged across pairs of words. The other is accu-
Pairwise Exact
Acc. Match
Heuristic
Baseline 48.1 35.4
Model
Transducers Messages
Levenshtein Unigrams 37.2 26.2
Levenshtein Bigrams 43.0 26.5
Levenshtein Anch. Unigrams 68.6 56.8
Learned Unigrams 0.1 0.0
Learned Bigrams 38.7 11.3
Learned Anch. Unigrams 90.3 86.6
Table 1: Accuracies for reconstructing cognate groups. Lev-
enshtein refers to fixed parameter edit distance transducer.
Learned refers to automatically learned edit distances. Pair-
wise Accuracy means averaged on each word pair; Exact
Match refers to percentage of completely and accurately re-
constructed groups. For a description of the baseline, see Sec-
tion 7.1.
Prec. Recall F1
Heuristic
Baseline 49.0 43.5 46.1
Model
Transducers Messages
Levenshtein Anch. Unigrams 86.5 36.1 50.9
Learned Anch. Unigrams 66.9 82.0 73.6
Table 2: Accuracies for reconstructing incomplete groups.
Scores reported are precision, recall, and F1, averaged over
all word pairs.
racy measured in terms of the number of correctly,
completely reconstructed cognate groups.
Table 1 shows the results under various config-
urations. As can be seen, the kind of approxima-
tion used matters immensely. In this application,
positional information is important, more so than
the context of the previous phoneme. Both Un-
igrams and Bigrams significantly under-perform
the baseline, while Anchored Unigrams easily out-
performs it both with and without learning.
An initially surprising result is that learning ac-
tually harms performance under the unanchored
approximations. The explanation is that these
topologies are not sensitive enough to context, and
that the learning procedure ends up flattening the
distributions. In the case of unigrams ? which have
the least context ? learning degrades performance
to chance. However, in the case of positional uni-
grams, learning reduces the error rate by more than
two-thirds.
7.3 Experiment 2: Incomplete Data
As a more realistic scenario, we consider the case
where we do not know that all cognate groups have
words in all languages. To test our model, we ran-
1037
domly pruned 20% of the branches according the
survival process of our model.8
Because only Anchored Unigrams performed
well in Experiment 1, we consider only it and the
Dice?s coefficient baseline. The baseline needs to
be augmented to support the fact that some words
may not appear in all cognate groups. To do this,
we thresholded the bipartite matching process so
that if the coefficient fell below some value, we
started a new group for that word. We experi-
mented on 10 values in the range (0,1) for the
baseline?s threshold and report on the one (0.2)
that gives the best pairwise F1.
The results are in Table 2. Here again, we see
that the positional unigrams perform much better
than the baseline system. The learned transduc-
ers seem to sacrifice precision for the sake of in-
creased recall. This makes sense because the de-
fault edit distance parameter settings strongly fa-
vor exact matches, while the learned transducers
learn more realistic substitution and deletion ma-
trices, at the expense of making more mistakes.
For example, the learned transducers enable
our model to correctly infer that Portuguese
/d1femdu/, Spanish /defiendo/, and Italian
/difEndo/ are all derived from Latin /de:fendo:/
?defend.? Using the simple Levenshtein transduc-
ers, on the other hand, our model keeps all three
separated, because the transducers cannot know ?
among other things ? that Portuguese /1/, Span-
ish /e/, and Italian /i/ are commonly substituted
for one another. Unfortunately, because the trans-
ducers used cannot learn contextual rules, cer-
tain transformations can be over-applied. For in-
stance, Spanish /nombRar/ ?name? is grouped to-
gether with Portuguese /num1RaR/ ?number? and
Italian /numerare/ ?number,? largely because the
rule Portuguese /u/? Spanish /o/ is applied out-
side of its normal context. This sound change oc-
curs primarily with final vowels, and does not usu-
ally occur word medially. Thus, more sophisti-
cated transducers could learn better sound laws,
which could translate into improved accuracy.
7.4 Experiment 3: Reconstructions
As a final trial, we wanted to see how each au-
tomatically found cognate group faired as com-
pared to the ?true groups? for actual reconstruc-
tion of proto-words. Our model is not optimized
8This dataset will be made available at
http://nlp.cs.berkeley.edu/Main.html#Historical
for faithful reconstruction, and so we used the An-
cestry Resampling system of Bouchard-Co?te? et al
(2009). To evaluate, we matched each Latin word
with the best possible cognate group for that word.
The process for the matching was as follows. If
two or three of the words in an constructed cognate
group agreed, we assigned the Latin word associ-
ated with the true group to it. With the remainder,
we executed a bipartite matching based on bigram
overlap.
For evaluation, we examined the Levenshtein
distance between the reconstructed word and the
chosen Latin word. As a kind of ?skyline,?
we compare to the edit distances reported in
Bouchard-Co?te? et al (2009), which was based on
complete knowledge of the cognate groups. On
this task, our reconstructed cognate groups had
an average edit distance of 3.8 from the assigned
Latin word. This compares favorably to the edit
distances reported in Bouchard-Co?te? et al (2009),
who using oracle cognate assignments achieved an
average Levenshtein distance of 3.0.9
8 Conclusion
We presented a new generative model of word
lists that automatically finds cognate groups from
scrambled vocabulary lists. This model jointly
models the origin, propagation, and evolution of
cognate groups from a common root word. We
also introduced a novel technique for approximat-
ing automata. Using these approximations, our
model can reduce the error rate by 80% over a
baseline approach. Finally, we demonstrate that
these automatically generated cognate groups can
be used to automatically reconstruct proto-words
faithfully, with a small increase in error.
Acknowledgments
Thanks to Alexandre Bouchard-Co?te? for the many
insights. This project is funded in part by the NSF
under grant 0915265 and an NSF graduate fellow-
ship to the first author.
References
Leonard Bloomfield. 1938. Language. Holt, New
York.
9Morphological noise and transcription errors contribute
to the absolute error rate for this data set.
1038
Alexandre Bouchard-Co?te?, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic ap-
proach to diachronic phonology. In EMNLP.
Alexandre Bouchard-Co?te?, Thomas L. Griffiths, and
Dan Klein. 2009. Improved reconstruction of pro-
tolanguage word forms. In NAACL, pages 65?73.
Hal Daume? III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Conference of the Association for Computational
Linguistics (ACL).
Hal Daume? III. 2009. Non-parametric Bayesian model
areal linguistics. In NAACL.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In EMNLP, Singa-
pore, August.
Jason Eisner. 2001. Expectation semirings: Flexible
EM for finite-state transducers. In Gertjan van No-
ord, editor, FSMNLP.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Grzegorz Kondrak, Daniel Marcu, and Keven Knight.
2003. Cognates can improve statistical translation
models. In NAACL.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In NAACL.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83?97.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP.
John B. Lowe and Martine Mazaudon. 1994. The re-
construction engine: a computer implementation of
the comparative method. Computational Linguis-
tics, 20(3):381?417.
Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In NAACL, pages 1?8. Association for
Computational Linguistics.
Thomas P. Minka. 2001. Expectation propagation for
approximate bayesian inference. In UAI, pages 362?
369.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech pro-
cessing. In ECAI-96 Workshop. John Wiley and
Sons.
Mehryar Mohri, 2009. Handbook of Weighted Au-
tomata, chapter Weighted Automata Algorithms.
Springer.
Andrea Mulloni. 2007. Automatic prediction of cog-
nate orthography using support vector machines. In
ACL, pages 25?30.
John Nerbonne. 2010. Measuring the diffusion of lin-
guistic change. Philosophical Transactions of the
Royal Society B: Biological Sciences.
Michael P. Oakes. 2000. Computer estimation of
vocabulary in a protolanguage from word lists in
four daughter languages. Quantitative Linguistics,
7(3):233?243.
OED. 1989. ?day, n.?. In The Oxford English Dictio-
nary online. Oxford University Press.
John Ohala, 1993. Historical linguistics: Problems
and perspectives, chapter The phonetics of sound
change, pages 237?238. Longman.
Jose Oncina and Marc Sebban. 2006. Learning
stochastic edit distance: Application in handwritten
character recognition. Pattern Recognition, 39(9).
Don Ringe, Tandy Warnow, and Ann Taylor. 2002.
Indo-european and computational cladistics. Trans-
actions of the Philological Society, 100(1):59?129.
Alan S.C. Ross. 1950. Philological probability prob-
lems. Journal of the Royal Statistical Society Series
B.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2000. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In NAACL.
1039
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 114?124,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Decentralized Entity-Level Modeling for Coreference Resolution
Greg Durrett, David Hall, and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,dlwh,klein}@cs.berkeley.edu
Abstract
Efficiently incorporating entity-level in-
formation is a challenge for coreference
resolution systems due to the difficulty of
exact inference over partitions. We de-
scribe an end-to-end discriminative prob-
abilistic model for coreference that, along
with standard pairwise features, enforces
structural agreement constraints between
specified properties of coreferent men-
tions. This model can be represented as
a factor graph for each document that ad-
mits efficient inference via belief propaga-
tion. We show that our method can use
entity-level information to outperform a
basic pairwise system.
1 Introduction
The inclusion of entity-level features has been a
driving force behind the development of many
coreference resolution systems (Luo et al, 2004;
Rahman and Ng, 2009; Haghighi and Klein, 2010;
Lee et al, 2011). There is no polynomial-time dy-
namic program for inference in a model with ar-
bitrary entity-level features, so systems that use
such features typically rely on making decisions
in a pipelined manner and sticking with them, op-
erating greedily in a left-to-right fashion (Rahman
and Ng, 2009) or in a multi-pass, sieve-like man-
ner (Raghunathan et al, 2010). However, such
systems may be locked into bad coreference deci-
sions and are difficult to directly optimize for stan-
dard evaluation metrics.
In this work, we present a new structured model
of entity-level information designed to allow effi-
cient inference. We use a log-linear model that can
be expressed as a factor graph. Pairwise features
appear in the model as unary factors, adjacent
to nodes representing a choice of antecedent (or
none) for each mention. Additional nodes model
entity-level properties on a per-mention basis, and
structural agreement factors softly drive properties
of coreferent mentions to agree with one another.
This is a key feature of our model: mentions man-
age their partial membership in various corefer-
ence chains, so that information about entity-level
properties is decentralized and propagated across
individual mentions, and we never need to explic-
itly instantiate entities.
Exact inference in this factor graph is in-
tractable, but efficient approximate inference can
be carried out with belief propagation. Our model
is the first discriminatively-trained model that both
makes joint decisions over an entire document and
models specific entity-level properties, rather than
simply enforcing transitivity of pairwise decisions
(Finkel and Manning, 2008; Song et al, 2012).
We evaluate our system on the dataset from
the CoNLL 2011 shared task using three differ-
ent types of properties: synthetic oracle proper-
ties, entity phi features (number, gender, animacy,
and NER type), and properties derived from un-
supervised clusters targeting semantic type infor-
mation. In all cases, our transitive model of en-
tity properties equals or outperforms our pairwise
system and our reimplementation of a previous
entity-level system (Rahman and Ng, 2009). Our
final system is competitive with the winner of the
CoNLL 2011 shared task (Lee et al, 2011).
2 Example
We begin with an example motivating our use of
entity-level features. Consider the following ex-
cerpt concerning two famous auction houses:
When looking for [art items], [people] go
to [Sotheby?s and Christie?s] because [they]A
believe [they]B can get the best price for
[them].
The first three mentions are all distinct entities,
theyA and theyB refer to people, and them refers to
art items. The three pronouns are tricky to resolve
114
automatically because they could at first glance re-
solve to any of the preceding mentions. We focus
in particular on the resolution of theyA and them.
In order to correctly resolve theyA to people rather
than Sotheby?s and Christie?s, we must take ad-
vantage of the fact that theyA appears as the sub-
ject of the verb believe, which is much more likely
to be attributed to people than to auction houses.
Binding principles prevent them from attaching
to theyB. But how do we prevent it from choos-
ing as its antecedent the next closest agreeing pro-
noun, theyA? One way is to exploit the correct
coreference decision we have already made, theyA
referring to people, since people are not as likely
to have a price as art items are. This observa-
tion argues for enforcing agreement of entity-level
semantic properties during inference, specifically
properties relating to permitted semantic roles.
Because even these six mentions have hundreds
of potential partitions into coreference chains, we
cannot search over partitions exhaustively, and
therefore we must design our model to be able to
use this information while still admitting an effi-
cient inference scheme.
3 Models
We will first present our BASIC model (Sec-
tion 3.1) and describe the features it incorporates
(Section 3.2), then explain how to extend it to use
transitive features (Sections 3.3 and 3.4).
Throughout this section, let x be a variable con-
taining the words in a document along with any
relevant precomputed annotation (such as parse in-
formation, semantic roles, etc.), and let n denote
the number of mentions in a given document.
3.1 BASIC Model
Our BASIC model is depicted in Figure 1 in stan-
dard factor graph notation. Each mention i has
an associated random variable ai taking values in
the set {1, . . . , i?1, <new>}; this variable spec-
ifies mention i?s selected antecedent or indicates
that it begins a new coreference chain. Let a =
(a1, ..., an) be the vector of the ai. Note that a set
of coreference chains C (the final desired output)
can be uniquely determined from a, but a is not
uniquely determined by C.
We use a log linear model of the conditional dis-
tribution P (a|x) as follows:
P (a|x) ? exp
( n?
i=1
wT fA(i, ai, x)
)
When looking for [art items], [people] go to [Sotheby's 
and Christie's] because [they]
A
 believe [they]
B
 can get 
the best price for [them].
art items 0.15
people 0.4
Sotheby?s and 
Christie?s
0.4
<new> 0.05
a
2
a
3
a
4
a
1
A
1
A
2
A
3
A
4
art items 0.05
<new> 0.95
antecedent 
choices
antecedent 
factors
}
}
Figure 1: Our BASIC coreference model. A de-
cision ai is made independently for each men-
tion about what its antecedent mention should
be or whether it should start a new coreference
chain. Each unary factor Ai has a log-linear form
with features examining mention i, its selected an-
tecedent ai, and the document context x.
where fA(i, ai, x) is a feature function that exam-
ines the coreference decision ai for mention i with
document context x; note that this feature function
can include pairwise features based on mention i
and the chosen antecedent ai, since information
about each mention is contained in x.
Because the model factors completely over the
individual ai, these feature functions fA can be ex-
pressed as unary factors Ai (see Figure 1), with
Ai(j) ? exp
(
wT fA(i, j, x)
). Given a setting of
w, we can determine a? = argmaxa P (a|x) and
then deterministically compute C(a), the final set
of coreference chains.
While the features of this model factor over
coreference links, this approach differs from clas-
sical pairwise systems such as Bengtson and Roth
(2008) or Stoyanov et al (2010). Because poten-
tial antecedents compete with each other and with
the non-anaphoric hypothesis, the choice of ai ac-
tually represents a joint decision about i?1 pair-
wise links, as opposed to systems that use a pair-
wise binary classifier and a separate agglomera-
tion step, which consider one link at a time during
learning. This approach is similar to the mention-
ranking model of Rahman and Ng (2009).
3.2 Pairwise Features
We now present the set of features fA used by our
unary factors Ai. Each feature examines the an-
115
tecedent choice ai of the current mention as well
as the observed information x in the document.
For each of the features we present, two conjoined
versions are included: one with an indicator of the
type of the current mention being resolved, and
one with an indicator of the types of the current
and antecedent mentions. Mention types are either
NOMINAL, PROPER, or, if the mention is pronom-
inal, a canonicalized version of the pronoun ab-
stracting away case.1
Several features, especially those based on the
precise constructs (apposition, etc.) and those in-
corporating phi feature information, are computed
using the machinery in Lee et al (2011). Other
features were inspired by Song et al (2012) and
Rahman and Ng (2009).
Anaphoricity features: Indicator of anaphoric-
ity, indicator on definiteness.
Configurational features: Indicator on distance
in mentions (capped at 10), indicator on dis-
tance in sentences (capped at 10), does the an-
tecedent c-command the current mention, are the
two mentions in a subject/object construction, are
the mentions nested, are the mentions in determin-
istic appositive/role appositive/predicate nomina-
tive/relative pronoun constructions.
Match features: Is one mention an acronym of
the other, head match, head contained (each way),
string match, string contained (each way), relaxed
head match features from Lee et al (2011).
Agreement features: Gender, number, ani-
macy, and NER type of the current mention and
the antecedent (separately and conjoined).
Discourse features: Speaker match conjoined
with an indicator of whether the document is an
article or conversation.
Because we use conjunctions of these base fea-
tures together with the antecedent and mention
type, our system can capture many relationships
that previous systems hand-coded, especially re-
garding pronouns. For example, our system has
access to features such as ?it is non-anaphoric?,
?it has as its antecedent a geopolitical entity?, or
?I has as its antecedent I with the same speaker.?
1While this canonicalization could theoretically impair
our ability to resolve, for example, reflexive pronouns, con-
joining features with raw pronoun strings does not improve
performance.
We experimented with synonymy and hyper-
nymy features from WordNet (Miller, 1995), but
these did not empirically improve performance.
3.3 TRANSITIVE Model
The BASIC model can capture many relationships
between pairs of mentions, but cannot necessarily
capture entity-level properties like those discussed
in Section 2. We could of course model entities
directly (Luo et al, 2004; Rahman and Ng, 2009),
saying that each mention refers to some prior en-
tity rather than to some prior mention. However,
inference in this model would require reasoning
about all possible partitions of mentions, which is
computationally infeasible without resorting to se-
vere approximations like a left-to-right inference
method (Rahman and Ng, 2009).
Instead, we would like to try to preserve the
tractability of the BASIC model while still being
able to exploit entity-level information. To do so,
we will allow each mention to maintain its own
distributions over values for a number of proper-
ties; these properties could include gender, named-
entity type, or semantic class. Then, we will re-
quire each anaphoric mention to agree with its an-
tecedent on the value of each of these properties.
Our TRANSITIVE model which implements this
scheme is shown in Figure 2. Each mention i
has been augmented with a single property node
pi ? {1, ..., k}. The unary Pi factors encode prior
knowledge about the setting of each pi; these fac-
tors may be hard (I will not refer to a plural entity),
soft (such as a distribution over named entity types
output by an NER tagger), or practically uniform
(e.g. the last name Smith does not specify a partic-
ular gender).
To enforce agreement of a particular property,
we require a mention to have the same property
value as its antecedent. That is, for mentions i and
j, if ai = j, we want to ensure that pi and pj
agree. We can achieve this with the following set
of structural equality factors:
Ei?j(ai, pi, pj) = 1? I[ai = j ? pi 6= pj ]
In words, this factor is zero if both ai = j and
pi disagrees with pj . These equality factors es-
sentially provide a mechanism by which these pri-
ors Pi can influence the coreference decisions: if,
for example, the factors Pi and Pj disagree very
strongly, choosing ai 6= j will be preferred in or-
der to avoid forcing one of pi or pj to take an un-
desirable value. Moreover, note that although ai
116
E4-3
a
2
a
4
p
4
p
3
p
2
E
4-2
A
2
A
3
A
4
P
2
P
3
P
4
antecedent 
choices
antecedent 
factors
property 
factors
properties
equality 
factors
a
3
}
}
}
}
}
people
Sotheby's
and Christie's
they
Figure 2: The factor graph for our TRANSI-
TIVE coreference model. Each node ai now has
a property pi, which is informed by its own unary
factor Pi. In our example, a4 strongly indicates
that mentions 2 and 4 are coreferent; the factor
E4?2 then enforces equality between p2 and p4,
while the factor E4?3 has no effect.
only indicates a single antecedent, the transitive
nature of the E factors forces pi to agree with the
p nodes of all other mentions likely to be in the
same entity.
3.4 Property Projection
So far, our model as specified ensures agreement
of our entity-level properties, but strictly enforc-
ing agreement may not always be correct. Suppose
that we are using named entity type as an entity-
level property. Organizations and geo-political en-
tities are two frequently confused and ambiguous
tags, and in the gold-standard coreference chains
it may be the case that a single chain contains in-
stances of both. We might wish to learn that or-
ganizations and geo-political entities are ?compat-
ible? in the sense that we should forgive entities
for containing both, but without losing the ability
to reject a chain containing both organizations and
people, for example.
To address these effects, we expand our model
as indicated in Figure 3. As before, we have a
set of properties pi and agreement factors Eij . On
top of that, we introduce the notion of raw prop-
erty values ri ? {1, ..., k} together with priors in
the form of the Ri factors. The ri and pi could in
principle have different domains, but for this work
we take them to have the same domain. The Pi
factors now have a new structure: they now rep-
resent a featurized projection of the ri onto the
pi, which can now be thought of as ?coreference-
p
4p
3
p
2
r
4
r
3
r
2
P
2
P
3
P
4
R
2
R
3
R
4
raw property 
factors
raw properties
projection 
factors
projected 
properties
}
}
}
}
a
2
a
4
A
2
A
3
A
4
a
3
E
3-1
E
4-1
Figure 3: The complete factor graph for our
TRANSITIVE coreference model. Compared to
Figure 2, the Ri contain the raw cluster posteriors,
and the Pi factors now project raw cluster values ri
into a set of ?coreference-adapted? clusters pi that
are used as before. This projection allows men-
tions with different but compatible raw property
values to coexist in the same coreference chain.
adapted? properties. The Pi factors are defined by
Pi(pi, ri) ? exp(wT fP (pi, ri)), where fP is a fea-
ture vector over the projection of ri onto pi. While
there are many possible choices of fP , we choose
it to be an indicator of the values of pi and ri, so
that we learn a fully-parameterized projection ma-
trix.2 The Ri are constant factors, and may come
from an upstream model or some other source de-
pending on the property being modeled.
Our description thus far has assumed that we
are modeling only one type of property. In fact,
we can use multiple properties for each mention
by duplicating the r and p nodes and the R, P ,
and E factors across each desired property. We
index each of these by l ? {1, . . . ,m} for each of
m properties.
The final log-linear model is given by the fol-
lowing formula:
P (a|x) ?
?
p,r
?
?
?
??
i,j,l
El,i?j(ai, pli, plj)
?
?
?
??
i,l
Rli(rli)
?
?
exp
(
wT
?
i
(
fA(i, ai, x) +
?
l
fP (pli, rli)
))]
where i and j range over mentions, l ranges over
2Initialized to zero (or small values), this matrix actually
causes the transitive machinery to have no effect, since all
posteriors over the pi are flat and completely uninformative.
Therefore, we regularize the weights of the indicators of pi =
ri towards 1 and all other features towards 0 to give each raw
cluster a preference for a distinct projected cluster.
117
each of m properties, and the outer sum indicates
marginalization over all p and r variables.
4 Learning
Now that we have defined our model, we must
decide how to train its weights w. The first
issue to address is one of the supervision pro-
vided. Our model traffics in sets of labels a
which are more specified than gold coreference
chains C, which give cluster membership for each
mention but not antecedence. Let A(C) be the
set of labelings a that are consistent with a set
of coreference chains C. For example, if C =
{{1, 2, 3}, {4}}, then (<new>, 1, 2, <new>) ?
A(C) and (<new>, 1, 1, <new>) ? A(C) but
(<new>, 1, <new>, 3) /? A(C), since this im-
plies the chains C = {{1, 2}, {3, 4}}
The most natural objective is a variant of
standard conditional log-likelihood that treats the
choice of a for the specified C as a latent variable
to be marginalized out:
`(w) =
t?
i=1
log
?
? ?
a?A(Ci)
P (a|xi)
?
? (1)
where (xi, Ci) is the ith labeled training example.
This optimizes for the 0-1 loss; however, we are
much more interested in optimizing with respect
to a coreference-specific loss function.
To this end, we will use softmax-margin (Gim-
pel and Smith, 2010), which augments the proba-
bility of each example with a term proportional to
its loss, pushing the model to assign less mass to
highly incorrect examples. We modify Equation 1
to use a new probability distribution P ? instead
of P , where P ?(a|xi) ? P (a|xi) exp (l(a,C))
and l(a,C) is a loss function. In order to
perform inference efficiently, l(a,C) must de-
compose linearly across mentions: l(a,C) =?n
i=1 l(ai, C). Commonly-used coreference met-
rics such as MUC (Vilain et al, 1995) and B3
(Bagga and Baldwin, 1998) do not have this prop-
erty, so we instead make use of a parameterized
loss function that does and fit the parameters to
give good performance. Specifically, we take
l(a,C) =
n?
i=1
[c1I(K1(ai, C)) + c2I(K2(ai, C))
+ c3I(K3(ai, C))]
where c1, c2, and c3 are real-valued weights, K1
denotes the event that ai is falsely anaphoric when
it should be non-anaphoric, K2 denotes the event
that ai is falsely non-anaphoric when it should be
anaphoric, and K3 denotes the event that ai is cor-
rectly determined to be anaphoric but . These can
be computed based on only ai and C. By setting
c1 low and c2 high relative to c3, we can force
the system to be less conservative about making
anaphoricity decisions and achieve a better bal-
ance with the final coreference metrics.
Finally, we incorporate L1 regularization, giv-
ing us our final objective:
`(w) =
t?
i=1
log
?
? ?
a?A(Ci)
P ?(a|xi)
?
?+ ??w?1
We optimize this objective using AdaGrad
(Duchi et al, 2011); we found this to be faster and
give higher performance than L-BFGS using L2
regularization (Liu and Nocedal, 1989). Note that
because of the marginalization over A(Ci), even
the objective for the BASIC model is not convex.
5 Inference
Inference in the BASIC model is straightforward.
Given a set of weights w, we can predict
a? = argmax
a
P (a|x)
We then report the corresponding chains C(a)
as the system output.3 For learning, the gradi-
ent takes the standard form of the gradient of a
log-linear model, a difference of expected feature
counts under the gold annotation and under no
annotation. This requires computing marginals
P ?(ai|x) for each mention i, but because the
model already factors this way, this step is easy.
The TRANSITIVE model is more complex. Ex-
act inference is intractable due to theE factors that
couple all of the ai by way of the pi nodes. How-
ever, we can compute approximate marginals for
the ai, pi, and ri using belief propagation. BP has
been effectively used on other NLP tasks (Smith
and Eisner, 2008; Burkett and Klein, 2012), and is
effective in cases such as this where the model is
largely driven by non-loopy factors (here, the Ai).
From marginals over each node, we can com-
pute the necessary gradient and decode as before:
a? = argmax
a
P? (a|x)
3One could use ILP-based decoding in the style of Finkel
and Manning (2008) and Song et al (2012) to attempt to ex-
plicitly find the optimal C with choice of a marginalized out,
but we did not explore this option.
118
This corresponds to minimum-risk decoding with
respect to the Hamming loss over antecedence pre-
dictions.
Pruning. The TRANSITIVE model requires in-
stantiating a factor for each potential setting of
each ai. This factor graph grows quadratically in
the size of the document, and even approximate in-
ference becomes slow when a document contains
over 200 mentions. Therefore, we use our BA-
SIC model to prune antecedent choices for each
ai in order to reduce the size of the factor graph
that we must instantiate. Specifically, we prune
links between pairs of mentions that are of men-
tion distance more than 100, as well as values for
ai that fall below a particular odds ratio threshold
with respect to the best setting of that ai in the
BASIC model; that is, those for which
log
( PBASIC (ai|x)
maxj PBASIC (ai = j|x)
)
is below a cutoff ?.
6 Related Work
Our BASIC model is a mention-ranking approach
resembling models used by Denis and Baldridge
(2008) and Rahman and Ng (2009), though it is
trained using a novel parameterized loss function.
It is also similar to the MLN-JOINT(BF) model
of Song et al (2012), but we enforce the single-
parent constraint at a deeper structural level, al-
lowing us to treat non-anaphoricity symmetrically
with coreference as in Denis and Baldridge (2007)
and Stoyanov and Eisner (2012). The model of
Fernandes et al (2012) also uses the single-parent
constraint structurally, but with learning via la-
tent perceptron and ILP-based one-best decod-
ing rather than logistic regression and BP-based
marginal computation.
Our TRANSITIVE model is novel; while Mc-
Callum and Wellner (2004) proposed the idea of
using attributes for mentions, they do not actu-
ally implement a model that does so. Other sys-
tems include entity-level information via hand-
written rules (Raghunathan et al, 2010), induced
rules (Yang et al, 2008), or features with learned
weights (Luo et al, 2004; Rahman and Ng, 2011),
but all of these systems freeze past coreference de-
cisions in order to compute their entities.
Most similar to our entity-level approach is
the system of Haghighi and Klein (2010), which
also uses approximate global inference; however,
theirs is an unsupervised, generative system and
they attempt to directly model multinomials over
words in each mention. Their system could be ex-
tended to handle property information like we do,
but our system has many other advantages, such as
freedom from a pre-specified list of entity types,
the ability to use multiple input clusterings, and
discriminative projection of clusters.
7 Experiments
We use the datasets, experimental setup, and scor-
ing program from the CoNLL 2011 shared task
(Pradhan et al, 2011), based on the OntoNotes
corpus (Hovy et al, 2006). We use the standard
automatic parses and NER tags for each docu-
ment. Our mentions are those output by the sys-
tem of Lee et al (2011); we also use their postpro-
cessing to remove appositives, predicate nomina-
tives, and singletons before evaluation. For each
experiment, we report MUC (Vilain et al, 1995),
B3 (Bagga and Baldwin, 1998), and CEAFe (Luo,
2005), as well as their average.
Parameter settings. We take the regularization
constant ? = 0.001 and the parameters of our
surrogate loss (c1, c2, c3) = (0.15, 2.5, 1) for all
models.4 All models are trained for 20 iterations.
We take the pruning threshold ? = ?2.
7.1 Systems
Besides our BASIC and TRANSITIVE systems, we
evaluate a strictly pairwise system that incorpo-
rates property information by way of indicator fea-
tures on the current mention?s most likely property
value and the proposed antecedent?s most likely
property value. We call this system PAIRPROP-
ERTY; it is simply the BASIC system with an ex-
panded feature set.
Furthermore, we compare against a LEFT-
TORIGHT entity-level system like that of Rahman
and Ng (2009).5 Decoding now operates in a se-
quential fashion, with BASIC features computed
as before and entity features computed for each
mention based on the coreference decisions made
thus far. Following Rahman and Ng (2009), fea-
tures for each property indicate whether the cur-
4Additional tuning of these hyper parameters did not sig-
nificantly improve any of the models under any of the exper-
imental conditions.
5Unfortunately, their publicly-available system is closed-
source and performs poorly on the CoNLL shared task
dataset, so direct comparison is difficult.
119
rent mention agrees with no mentions in the an-
tecedent cluster, at least one mention, over half of
the mentions, or all of the mentions; antecedent
clusters of size 1 or 2 fire special-cased features.
These additional features beyond those in Rah-
man and Ng (2009) were helpful, but more in-
volved conjunction schemes and fine-grained fea-
tures were not. During training, entity features of
both the gold and the prediction are computed us-
ing the Viterbi clustering of preceding mentions
under the current model parameters.6
All systems are run in a two-pass manner:
first, the BASIC model is run, then antecedent
choices are pruned, then our second-round model
is trained from scratch on the pruned data.7
7.2 Noisy Oracle Features
We first evaluate our model?s ability to exploit syn-
thetic entity-level properties. For this experiment,
mention properties are derived from corrupted or-
acle information about the true underlying corefer-
ence cluster. Each coreference cluster is assumed
to have one underlying value for each of m coref-
erence properties, each taking values over a do-
main D. Mentions then sample distributions over
D from a Dirichlet distribution peaked around the
true underlying value.8 These posteriors are taken
as the Ri for the TRANSITIVE model.
We choose this setup to reflect two important
properties of entity-level information: first, that it
may come from a variety of disparate sources, and
second, that it may be based on the determinations
of upstream models which produce posteriors nat-
urally. A strength of our model is that it can accept
such posteriors as input, naturally making use of
this information in a model-based way.
Table 1 shows development results averaged
across ten train-test splits with m = 3 proper-
ties, each taking one of |D| = 5 values. We em-
phasize that these parameter settings give fairly
weak oracle information: a document may have
hundreds of clusters, so even in the absence of
noise these oracle properties do not have high dis-
6Using gold entities for training as in Rahman and Ng
(2009) resulted in a lower-performing system.
7We even do this for the BASIC model, since we found
that performance of the pruned and retrained model was gen-
erally higher.
8Specifically, the distribution used is a Dirichlet with
? = 3.5 for the true underlying cluster and ? = 1 for other
values, chosen so that 25% of samples from the distribution
did not have the correct mode. Though these parameters af-
fect the quality of the oracle information, varying them did
not change the relative performance of the different models.
NOISY ORACLE
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
PAIRPROPERTY 66.31 72.68 49.08 62.69
LEFTTORIGHT 66.49 73.14 49.46 63.03
TRANSITIVE 67.37 74.05 49.68 63.70
Table 1: CoNLL metric scores for our four dif-
ferent systems incorporating noisy oracle data.
This information helps substantially in all cases.
Both entity-level models outperform the PAIR-
PROPERTY model, but we observe that the TRAN-
SITIVE model is more effective than the LEFT-
TORIGHT model at using this information.
criminating power. Still, we see that all mod-
els are able to benefit from incorporating this in-
formation; however, our TRANSITIVE model out-
performs both the PAIRPROPERTY model and the
LEFTTORIGHT model. There are a few reasons
for this: first, our model is able to directly use soft
posteriors, so it is able to exploit the fact that more
peaked samples from the Dirichlet are more likely
to be correct. Moreover, our model can propagate
information backwards in a document as well as
forwards, so the effects of noise can be more eas-
ily mitigated. By contrast, in the LEFTTORIGHT
model, if the first or second mention in a cluster
has the wrong property value, features indicating
high levels of property agreement will not fire on
the next few mentions in those clusters.
7.3 Phi Features
As we have seen, our TRANSITIVE model can ex-
ploit high-quality entity-level features. How does
it perform using real features that have been pro-
posed for entity-level coreference?
Here, we use hard phi feature determinations
extracted from the system of Lee et al (2011).
Named-entity type and animacy are both com-
puted based on the output of a named-entity tag-
ger, while number and gender use the dataset of
Bergsma and Lin (2006). Once this informa-
tion is determined, the PAIRPROPERTY and LEFT-
TORIGHT systems can compute features over it di-
rectly. In the TRANSITIVE model, each of the Ri
factors places 34 of its mass on the determined la-bel and distributes the remainder uniformly among
the possible options.
Table 2 shows results when adding entity-level
phi features on top of our BASIC pairwise system
(which already contains pairwise features) and on
top of an ablated BASIC system without pairwise
120
PHI FEATURES
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
LEFTTORIGHT 61.34 70.41 47.64 59.80
TRANSITIVE 62.66 70.92 46.88 60.16
PHI FEATURES (ABLATED BASIC)
BASIC-PHI 59.45 69.21 46.02 58.23
PAIRPROPERTY 61.88 70.66 47.14 59.90
LEFTTORIGHT 61.42 70.53 47.49 59.81
TRANSITIVE 62.23 70.78 46.74 59.92
Table 2: CoNLL metric scores for our systems in-
corporating phi features. Our standard BASIC sys-
tem already includes phi features, so no results are
reported for PAIRPROPERTY. Here, our TRAN-
SITIVE system does not give substantial improve-
ment on the averaged metric. Over a baseline
which does not include phi features, all systems
are able to incorporate them comparably.
phi features. Our entity-level systems successfully
captures phi features when they are not present in
the baseline, but there is only slight benefit over
pairwise incorporation, a result which has been
noted previously (Luo et al, 2004).
7.4 Clustering Features
Finally, we consider mention properties derived
from unsupervised clusterings; these properties
are designed to target semantic properties of nom-
inals that should behave more like the oracle fea-
tures than the phi features do.
We consider clusterings that take as input pairs
(n, r) of a noun head n and a string r which con-
tains the semantic role of n (or some approxima-
tion thereof) conjoined with its governor. Two dif-
ferent algorithms are used to cluster these pairs: a
NAIVEBAYES model, where c generates n and r,
and a CONDITIONAL model, where c is generated
conditioned on r and then n is generated from c.
Parameters for each can be learned with the ex-
pectation maximization (EM) algorithm (Demp-
ster et al, 1977), with symmetry broken by a small
amount of random noise at initialization.
Similar models have been used to learn sub-
categorization information (Rooth et al, 1999)
or properties of verb argument slots (Yao et al,
2011). We choose this kind of clustering for its rel-
ative simplicity and because it allows pronouns to
have more informed properties (from their verbal
context) than would be possible using a model that
makes type-level decisions about nominals only.
Though these specific cluster features are novel
to coreference, previous work has used similar
CLUSTERS
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
PAIRPROPERTY 62.88 70.71 47.45 60.35
LEFTTORIGHT 61.98 70.19 45.77 59.31
TRANSITIVE 63.34 70.89 46.88 60.37
Table 3: CoNLL metric scores for our systems
incorporating clustering features. These features
are equally effectively incorporated by our PAIR-
PROPERTY system and our TRANSITIVE system.
government
officials
court
authorities
ARG0:said
ARG0:say
ARG0:found
ARG0:announced
prices
shares
index
rates
ARG1:rose
ARG1:fell
ARG1:cut
ARG1:closed
way
law
agreement
plan
ARG1:signed
ARG1:announced
ARG1:set
ARG1:approved
attack
problems
attacks
charges
ARG1:cause
ARG2:following
ARG1:reported
ARG1:filed
... ...
... ...
... ...
... ...
...
Figure 4: Examples of clusters produced by the
NAIVEBAYES model on SRL-tagged data with
pronouns discarded.
types of fine-grained semantic class information
(Hendrickx and Daelemans, 2007; Ng, 2007; Rah-
man and Ng, 2010). Other approaches incorpo-
rate information from other sources (Ponzetto and
Strube, 2006) or compute heuristic scores for real-
valued features based on a large corpus or the web
(Dagan and Itai, 1990; Yang et al, 2005; Bansal
and Klein, 2012).
We use four different clusterings in our
experiments, each with twenty clusters:
dependency-parse-derived NAIVEBAYES clusters,
semantic-role-derived CONDITIONAL clusters,
SRL-derived NAIVEBAYES clusters generating
a NOVERB token when r cannot be determined,
and SRL-derived NAIVEBAYES clusters with all
pronoun tuples discarded. Examples of the latter
clusters are shown in Figure 4. Each clustering
is learned for 30 iterations of EM over English
Gigaword (Graff et al, 2007), parsed with the
Berkeley Parser (Petrov et al, 2006) and with
SRL determined by Senna (Collobert et al, 2011).
Table 3 shows results of modeling these cluster
properties. As in the case of oracle features, the
PAIRPROPERTY and LEFTTORIGHT systems use
the modes of the cluster posteriors, and the TRAN-
SITIVE system uses the posteriors directly as the
Ri. We see comparable performance from incor-
porating features in both an entity-level framework
and a pairwise framework, though the TRANSI-
121
MUC B3 CEAFe Avg.
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 F1
BASIC 69.99 55.59 61.96 80.96 62.69 70.66 41.37 55.21 47.30 59.97
STANFORD 61.49 59.59 60.49 74.60 68.25 71.28 47.57 49.45 48.49 60.10
NOISY ORACLE
PAIRPROPERTY 76.49 58.53 66.31 84.98 63.48 72.68 41.84 59.36 49.08 62.69
LEFTTORIGHT 76.92 58.55 66.49 85.68 63.81 73.14 42.07 60.01 49.46 63.03
TRANSITIVE 76.48 60.20 *67.37 84.84 65.69 *74.05 42.89 59.01 *49.68 63.70
PHI FEATURES
LEFTTORIGHT 69.77 54.73 61.34 81.40 62.04 70.41 41.49 55.92 47.64 59.80
TRANSITIVE 70.27 56.54 *62.66 79.81 63.82 *70.92 41.17 54.44 46.88 60.16
PHI FEATURES (ABLATED BASIC)
BASIC-PHI 67.04 53.41 59.45 78.93 61.63 69.21 40.40 53.46 46.02 58.23
PAIRPROPERTY 70.24 55.31 61.88 81.10 62.60 70.66 41.04 55.38 47.14 59.90
LEFTTORIGHT 69.94 54.75 61.42 81.38 62.23 70.53 41.29 55.87 47.49 59.81
TRANSITIVE 70.06 55.98 *62.23 79.92 63.52 70.78 40.90 54.52 46.74 59.92
CLUSTERS
PAIRPROPERTY 71.77 55.95 62.88 81.76 62.30 70.71 40.98 56.35 47.45 60.35
LEFTTORIGHT 69.75 54.82 61.39 81.48 62.29 70.60 41.62 55.89 47.71 59.90
TRANSITIVE 71.54 56.83 *63.34 80.55 63.31 *70.89 40.77 55.14 46.88 60.37
Table 4: CoNLL metric scores averaged across ten different splits of the training set for each experiment.
We include precision, recall, and F1 for each metric for completeness. Starred F1 values on the individual
metrics for the TRANSITIVE system are significantly better than all other results in the same block at the
p = 0.01 level according to a bootstrap resampling test.
MUC B3 CEAFe Avg.
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 F1
BASIC 68.84 56.08 61.81 77.60 61.40 68.56 38.25 50.57 43.55 57.97
PAIRPROPERTY 70.90 56.26 62.73 78.95 60.79 68.69 37.69 51.92 43.67 58.37
LEFTTORIGHT 68.84 55.56 61.49 78.64 61.03 68.72 38.97 51.74 44.46 58.22
TRANSITIVE 70.62 58.06 *63.73 76.93 62.24 68.81 38.00 50.40 43.33 58.62
STANFORD 60.91 62.13 61.51 70.61 67.75 69.15 45.79 44.55 45.16 58.61
Table 5: CoNLL metric scores for our best systems (including clustering features) on the CoNLL blind
test set, reported in the same manner as Table 4.
TIVE system appears to be more effective than the
LEFTTORIGHT system.
7.5 Final Results
Table 4 shows expanded results on our develop-
ment sets for the different types of entity-level
information we considered. We also show in in
Table 5 the results of our system on the CoNLL
test set, and see that it performs comparably to
the Stanford coreference system (Lee et al, 2011).
Here, our TRANSITIVE system provides modest
improvements over all our other systems.
Based on Table 4, our TRANSITIVE system ap-
pears to do better on MUC andB3 than on CEAFe.
However, we found no simple way to change the
relative performance characteristics of our various
systems; notably, modifying the parameters of the
loss function mentioned in Section 4 or changing
it entirely did not trade off these three metrics but
merely increased or decreased them in lockstep.
Therefore, the TRANSITIVE system actually sub-
stantially improves over our baselines and is not
merely trading off metrics in a way that could be
easily reproduced through other means.
8 Conclusion
In this work, we presented a novel coreference ar-
chitecture that can both take advantage of standard
pairwise features as well as use transitivity to en-
force coherence of decentralized entity-level prop-
erties within coreference clusters. Our transitive
system is more effective at using properties than
a pairwise system and a previous entity-level sys-
tem, and it achieves performance comparable to
that of the Stanford coreference resolution system,
the winner of the CoNLL 2011 shared task.
Acknowledgments
This work was partially supported by BBN under
DARPA contract HR0011-12-C-0014, by an NSF
fellowship for the first author, and by a Google fel-
lowship for the second. Thanks to the anonymous
reviewers for their insightful comments.
122
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference Se-
mantics from Web Features. In Proceedings of the
Association for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping Path-Based Pronoun Resolution. In Proceed-
ings of the Conference on Computational Linguistics
and the Association for Computational Linguistics.
David Burkett and Dan Klein. 2012. Fast Inference in
Phrase Extraction Models with Belief Propagation.
In Proceedings of the North American Chapter of
the Association for Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493?2537, November.
Ido Dagan and Alon Itai. 1990. Automatic Process-
ing of Large Corpora for the Resolution of Anaphora
References. In Proceedings of the Conference on
Computational Linguistics - Volume 3.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the Royal
Statistical Society, Series B, 39(1):1?38.
Pascal Denis and Jason Baldridge. 2007. Joint Deter-
mination of Anaphoricity and Coreference Resolu-
tion using Integer Programming. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
Models and Ranking for Coreference Resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Eraldo Rezende Fernandes, C??cero Nogueira dos San-
tos, and Ruy Luiz Milidiu?. 2012. Latent Structure
Perceptron with Feature Induction for Unrestricted
Coreference Resolution. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Proceedings and Conference on Computa-
tional Natural Language Learning - Shared Task.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing Transitivity in Coreference Resolution.
In Proceedings of the Association for Computational
Linguistics: Short Papers.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
Margin CRFs: Training Log-Linear Models with
Cost Functions. In Proceedings of the North Amer-
ican Chapter for the Association for Computational
Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Aria Haghighi and Dan Klein. 2010. Coreference Res-
olution in a Modular, Entity-Centered Model. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics.
Iris Hendrickx and Walter Daelemans, 2007. Adding
Semantic Information: Unsupervised Clusters for
Coreference Resolution.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics: Short Papers.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s Multi-Pass Sieve Corefer-
ence Resolution System at the CoNLL-2011 Shared
Task. In Proceedings of the Conference on Compu-
tational Natural Language Learning: Shared Task.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Mathematical Programming, 45(3):503?528,
December.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Proceedings of
the Association for Computational Linguistics.
Xiaoqiang Luo. 2005. On Coreference Resolution
Performance Metrics. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Andrew McCallum and Ben Wellner. 2004. Condi-
tional Models of Identity Uncertainty with Applica-
tion to Noun Coreference. In Proceedings of Ad-
vances in Neural Information Processing Systems.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38:39?41.
Vincent Ng. 2007. Semantic class induction and coref-
erence resolution. In Proceedings of the Association
for Computational Linguistics.
123
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of the
Conference on Computational Linguistics and the
Association for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution. In Proceed-
ings of the North American Chapter of the Associa-
tion of Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In Proceed-
ings of the Conference on Computational Natural
Language Learning: Shared Task.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A Multi-
Pass Sieve for Coreference Resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing.
Altaf Rahman and Vincent Ng. 2009. Supervised
Models for Coreference Resolution. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Altaf Rahman and Vincent Ng. 2010. Inducing Fine-
Grained Semantic Classes via Hierarchical and Col-
lective Classification. In Proceedings of the Interna-
tional Conference on Computational Linguistics.
Altaf Rahman and Vincent Ng. 2011. Narrowing
the Modeling Gap: A Cluster-Ranking Approach to
Coreference Resolution. Journal of Artificial Intel-
ligence Research, 40(1):469?521, January.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a Semanti-
cally Annotated Lexicon via EM-Based Clustering.
In Proceedings of the Association for Computational
Linguistics.
David A. Smith and Jason Eisner. 2008. Dependency
Parsing by Belief Propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and
Houfeng Wang. 2012. Joint Learning for Corefer-
ence Resolution with Markov Logic. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
Coreference Resolution. In Proceedings of the In-
ternational Conference on Computational Linguis-
tics.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference Resolution with Reconcile. In Pro-
ceedings of the Association for Computational Lin-
guistics: Short Papers.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
Theoretic Coreference Scoring Scheme. In Pro-
ceedings of the Conference on Message Understand-
ing.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving Pronoun Resolution Using Statistics-Based
Semantic Compatibility Information. In Proceed-
ings of the Association for Computational Linguis-
tics.
Xiaofeng Yang, Jian Su, Jun Lang, Chew L. Tan, Ting
Liu, and Sheng Li. 2008. An Entity-Mention Model
for Coreference Resolution with Inductive Logic
Programming. In Proceedings of the Association for
Computational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured Relation Discov-
ery Using Generative Models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
124
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 208?217,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Sparser, Better, Faster GPU Parsing
David Hall Taylor Berg-Kirkpatrick John Canny Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,tberg,jfc,klein}@cs.berkeley.edu
Abstract
Due to their origin in computer graph-
ics, graphics processing units (GPUs)
are highly optimized for dense problems,
where the exact same operation is applied
repeatedly to all data points. Natural lan-
guage processing algorithms, on the other
hand, are traditionally constructed in ways
that exploit structural sparsity. Recently,
Canny et al (2013) presented an approach
to GPU parsing that sacrifices traditional
sparsity in exchange for raw computa-
tional power, obtaining a system that can
compute Viterbi parses for a high-quality
grammar at about 164 sentences per sec-
ond on a mid-range GPU. In this work,
we reintroduce sparsity to GPU parsing
by adapting a coarse-to-fine pruning ap-
proach to the constraints of a GPU. The
resulting system is capable of computing
over 404 Viterbi parses per second?more
than a 2x speedup?on the same hard-
ware. Moreover, our approach allows us
to efficiently implement less GPU-friendly
minimum Bayes risk inference, improv-
ing throughput for this more accurate algo-
rithm from only 32 sentences per second
unpruned to over 190 sentences per second
using pruning?nearly a 6x speedup.
1 Introduction
Because NLP models typically treat sentences in-
dependently, NLP problems have long been seen
as ?embarrassingly parallel? ? large corpora can
be processed arbitrarily fast by simply sending dif-
ferent sentences to different machines. However,
recent trends in computer architecture, particularly
the development of powerful ?general purpose?
GPUs, have changed the landscape even for prob-
lems that parallelize at the sentence level. First,
classic single-core processors and main memory
architectures are no longer getting substantially
faster over time, so speed gains must now come
from parallelism within a single machine. Second,
compared to CPUs, GPUs devote a much larger
fraction of their computational power to actual
arithmetic. Since tasks like parsing boil down to
repeated read-multiply-write loops, GPUs should
be many times more efficient in time, power, or
cost. The challenge is that GPUs are not a good
fit for the kinds of sparse computations that most
current CPU-based NLP algorithms rely on.
Recently, Canny et al (2013) proposed a GPU
implementation of a constituency parser that sac-
rifices all sparsity in exchange for the sheer horse-
power that GPUs can provide. Their system uses a
grammar based on the Berkeley parser (Petrov and
Klein, 2007) (which is particularly amenable to
GPU processing), ?compiling? the grammar into a
sequence of GPU kernels that are applied densely
to every item in the parse chart. Together these
kernels implement the Viterbi inside algorithm.
On a mid-range GPU, their system can compute
Viterbi derivations at 164 sentences per second on
sentences of length 40 or less (see timing details
below).
In this paper, we develop algorithms that can
exploit sparsity on a GPU by adapting coarse-to-
fine pruning to a GPU setting. On a CPU, pruning
methods can give speedups of up to 100x. Such
extreme speedups over a dense GPU baseline cur-
rently seem unlikely because fine-grained sparsity
appears to be directly at odds with dense paral-
lelism. However, in this paper, we present a sys-
tem that finds a middle ground, where some level
of sparsity can be maintained without losing the
parallelism of the GPU. We use a coarse-to-fine
approach as in Petrov and Klein (2007), but with
only one coarse pass. Figure 1 shows an overview
of the approach: we first parse densely with a
coarse grammar and then parse sparsely with the
208
fine grammar, skipping symbols that the coarse
pass deemed sufficiently unlikely. Using this ap-
proach, we see a gain of more than 2x over the
dense GPU implementation, resulting in overall
speeds of up to 404 sentences per second. For
comparison, the publicly available CPU imple-
mentation of Petrov and Klein (2007) parses ap-
proximately 7 sentences per second per core on a
modern CPU.
A further drawback of the dense approach in
Canny et al (2013) is that it only computes
Viterbi parses. As with other grammars with
a parse/derivation distinction, the grammars of
Petrov and Klein (2007) only achieve their full
accuracy using minimum-Bayes-risk parsing, with
improvements of over 1.5 F1 over best-derivation
Viterbi parsing on the Penn Treebank (Marcus et
al., 1993). To that end, we extend our coarse-to-
fine GPU approach to computing marginals, along
the way proposing a new way to exploit the coarse
pass to avoid expensive log-domain computations
in the fine pass. We then implement minimum-
Bayes-risk parsing via the max recall algorithm of
Goodman (1996). Without the coarse pass, the
dense marginal computation is not efficient on a
GPU, processing only 32 sentences per second.
However, our approach allows us to process over
190 sentences per second, almost a 6x speedup.
2 A Note on Experiments
We build up our approach incrementally, with ex-
periments interspersed throughout the paper, and
summarized in Tables 1 and 2. In this paper, we
focus our attention on current-generation NVIDIA
GPUs. Many of the ideas described here apply to
other GPUs (such as those from AMD), but some
specifics will differ. All experiments are run with
an NVIDIA GeForce GTX 680, a mid-range GPU
that costs around $500 at time of writing. Unless
otherwise noted, all experiments are conducted on
sentences of length ? 40 words, and we estimate
times based on batches of 20K sentences.
1
We
should note that our experimental condition dif-
fers from that of Canny et al (2013): they evaluate
on sentences of length ? 30. Furthermore, they
1
The implementation of Canny et al (2013) cannot han-
dle batches so large, and so we tested it on batches of 1200
sentences. Our reimplementation is approximately the same
speed for the same batch sizes. For batches of 20K sentences,
we used sentences from the training set. We verified that there
was no significant difference in speed for sentences from the
training set and from the test set.
use two NVIDIA GeForce GTX 690s?each of
which is essentially a repackaging of two 680s?
meaning that our system and experiments would
run approximately four times faster on their hard-
ware. (This expected 4x factor is empirically con-
sistent with the result of running their system on
our hardware.)
3 Sparsity and CPUs
One successful approach for speeding up con-
stituency parsers has been to use coarse-to-fine
inference (Charniak et al, 2006). In coarse-to-
fine inference, we have a sequence of increasingly
complex grammars G
`
. Typically, each succes-
sive grammar G
`
is a refinement of the preceding
grammar G
`?1
. That is, for each symbol A
x
in
the fine grammar, there is some symbol A in the
coarse grammar. For instance, in a latent variable
parser, the coarse grammar would have symbols
like NP , V P , etc., and the fine pass would have
refined symbols NP
0
, NP
1
, V P
4
, and so on.
In coarse-to-fine inference, one applies the
grammars in sequence, computing inside and out-
side scores. Next, one computes (max) marginals
for every labeled span (A, i, j) in a sentence.
These max marginals are used to compute a prun-
ing mask for every span (i, j). This mask is the set
of symbols allowed for that span. Then, in the next
pass, one only processes rules that are licensed by
the pruning mask computed at the previous level.
This approach works because a low quality
coarse grammar can still reliably be used to prune
many symbols from the fine chart without loss of
accuracy. Petrov and Klein (2007) found that over
98% of symbols can be pruned from typical charts
using a simple X-bar grammar without any loss
of accuracy. Thus, the vast majority of rules can
be skipped, and therefore most computation can
be avoided. It is worth pointing out that although
98% of labeled spans can be skipped due to X-bar
pruning, we found that only about 79% of binary
rule applications can be skipped, because the un-
pruned symbols tend to be the ones with a larger
grammar footprint.
4 GPU Architectures
Unfortunately, the standard coarse-to-fine ap-
proach does not na??vely translate to GPU archi-
tectures. GPUs work by executing thousands of
threads at once, but impose the constraint that
large blocks of threads must be executing the same
209
RAM
CPU
GPU
RAM
Instruction Cache
Parse Charts
Work Array
Grammar
Queue
Sentences
Queue
Masks
Masks
Queue
Trees
Figure 1: Overview of the architecture of our system, which is an extension of Canny et al (2013)?s
system. The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to
the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask
that is used by the CPU when deciding which items to queue during the fine pass. The original system
of Canny et al (2013) only used the fine pass, with no pruning.
instructions in lockstep, differing only in their in-
put data. Thus sparsely skipping rules and sym-
bols will not save any work. Indeed, it may ac-
tually slow the system down. In this section, we
provide an overview of GPU architectures, focus-
ing on the details that are relevant to building an
efficient parser.
The large number of threads that a GPU exe-
cutes are packaged into blocks of 32 threads called
warps. All threads in a warp must execute the
same instruction at every clock cycle: if one thread
takes a branch the others do not, then all threads in
the warp must follow both code paths. This situa-
tion is called warp divergence. Because all threads
execute all code paths that any thread takes, time
can only be saved if an entire warp agrees to skip
any particular branch.
NVIDIA GPUs have 8-15 processors called
streaming multi-processors or SMs.
2
Each SM
can process up to 48 different warps at a time:
it interleaves the execution of each warp, so that
when one warp is stalled another warp can exe-
cute. Unlike threads within a single warp, the 48
warps do not have to execute the same instruc-
tions. However, the memory architecture is such
that they will be faster if they access related mem-
ory locations.
2
Older hardware (600 series or older) has 8 SMs. Newer
hardware has more.
A further consideration is that the number of
registers available to a thread in a warp is rather
limited compared to a CPU. On the 600 series,
maximum occupancy can only be achieved if each
thread uses at most 63 registers (Nvidia, 2008).
3
Registers are many times faster than variables lo-
cated in thread-local memory, which is actually
the same speed as global memory.
5 Anatomy of a Dense GPU Parser
This architecture environment puts very different
constraints on parsing algorithms from a CPU en-
vironment. Canny et al (2013) proposed an imple-
mentation of a PCFG parser that sacrifices stan-
dard sparse methods like coarse-to-fine pruning,
focusing instead on maximizing the instruction
and memory throughput of the parser. They as-
sume that they are parsing many sentences at once,
with throughput being more important than la-
tency. In this section, we describe their dense algo-
rithm, which we take as the baseline for our work;
we present it in a way that sets up the changes to
follow.
At the top level, the CPU and GPU communi-
cate via a work queue of parse items of the form
(s, i, k, j), where s is an identifier of a sentence,
i is the start of a span, k is the split point, and j
3
A thread can use more registers than this, but the full
complement of 48 warps cannot execute if too many are used.
210
Clustering Pruning Sent/Sec Speedup
Canny et al ? 164.0 ?
Reimpl ? 192.9 1.0x
Reimpl Empty, Coarse 185.5 0.96x
Reimpl Labeled, Coarse 187.5 0.97x
Parent ? 158.6 0.82x
Parent Labeled, Coarse 278.9 1.4x
Parent Labeled, 1-split 404.7 2.1x
Parent Labeled, 2-split 343.6 1.8x
Table 1: Performance numbers for computing
Viterbi inside charts on 20,000 sentences of length
?40 from the Penn Treebank. All times are
measured on an NVIDIA GeForce GTX 680.
?Reimpl? is our reimplementation of their ap-
proach. Speedups are measured in reference to this
reimplementation. See Section 7 for discussion of
the clustering algorithms and Section 6 for a de-
scription of the pruning methods. The Canny et al
(2013) system is benchmarked on a batch size of
1200 sentences, the others on 20,000.
is the end point. The GPU takes large numbers of
parse items and applies the entire grammar to them
in parallel. These parse items are enqueued in or-
der of increasing span size, blocking until all items
of a given length are complete. This approach is
diagrammed in Figure 2.
Because all rules are applied to all parse items,
all threads are executing the same sequence of in-
structions. Thus, there is no concern of warp di-
vergence.
5.1 Grammar Compilation
One important feature of Canny et al (2013)?s sys-
tem is grammar compilation. Because registers
are so much faster than thread-local memory, it
is critical to keep as many variables in registers
as possible. One way to accomplish this is to un-
roll loops at compilation time. Therefore, they in-
lined the iteration over the grammar directly into
the GPU kernels (i.e. the code itself), which al-
lows the compiler to more effectively use all of its
registers.
However, register space is limited on GPUs.
Because the Berkeley grammar is so large, the
compiler is not able to efficiently schedule all of
the operations in the grammar, resulting in regis-
ter spills. Canny et al (2013) found they had to
partition the grammar into multiple different ker-
nels. We discuss this partitioning in more detail in
Section 7. However, in short, the entire grammar
G is broken into multiple clusters G
i
where each
rule belongs to exactly one cluster.
NP
DT NN
VB
VP
NP
NP
PP
IN
NP
S
VP
(0, 1, 3)
(0, 2, 3)
(1, 2, 4)
(1, 3, 4)
(2, 3, 5)
(2, 4, 5)
Grammar
Queue
(i, k, j)
Figure 2: Schematic representation of the work
queue used in Canny et al (2013). The Viterbi
inside loop for the grammar is inlined into a ker-
nel. The kernel is applied to all items in the queue
in a blockwise manner.
NP
DT NN
NP
DT NN
NP
DT NN
NP
NP
PP
IN
NP
PP
IN
NP
PP
IN
PP
VB
VP
NP
VB
VP
NP
VB
VP
NP
VP
(0, 1, 3)
(1, 2, 4)
(3, 5, 6)
(1, 3, 4)
(1, 2, 4)
(0, 2, 3)
(2, 4, 5)
(3, 4, 6)
Queues
(i, k, j)
Grammar Clusters
Figure 3: Schematic representation of the work
queue and grammar clusters used in the fine pass
of our work. Here, the rules of the grammar are
clustered by their coarse parent symbol. We then
have multiple work queues, with parse items only
being enqueued if the span (i, j) allows that sym-
bol in its pruning mask.
All in all, Canny et al (2013)?s system is able
to compute Viterbi charts at 164 sentences per sec-
ond, for sentences up to length 40. On larger batch
sizes, our reimplementation of their approach is
able to achieve 193 sentences per second on the
same hardware. (See Table 1.)
6 Pruning on a GPU
Now we turn to the algorithmic and architectural
changes in our approach. First, consider trying to
211
directly apply the coarse-to-fine method sketched
in Section 3 to the dense baseline described above.
The natural implementation would be for each
thread to check if each rule is licensed before
applying it. However, we would only avoid the
work of applying the rule if all threads in the warp
agreed to skip it. Since each thread in the warp is
processing a different span (perhaps even from a
different sentence), consensus from all 32 threads
on any skip would be unlikely.
Another approach would be to skip enqueu-
ing any parse item (s, i, k, j) where the pruning
mask for any of (i, j), (i, k), or (k, j) is entirely
empty (i.e. all symbols are pruned in this cell by
the coarse grammar). However, our experiments
showed that only 40% of parse items are pruned in
this manner. Because of the overhead associated
with creating pruning masks and the further over-
head of GPU communication, we found that this
method did not actually produce any time savings
at all. The result is a parsing speed of 185.5 sen-
tences per second, as shown in Table 1 on the row
labeled ?Reimpl? with ?Empty, Coarse? pruning.
Instead, we take advantage of the partitioned
structure of the grammar and organize our com-
putation around the coarse symbol set. Recall that
the baseline already partitions the grammar G into
rule clusters G
i
to improve register sharing. (See
Section 7 for more on the baseline clustering.) We
create a separate work queue for each partition.
We call each such queue a labeled work queue, and
each one only queues items to which some rule in
the corresponding partition applies. We call the set
of coarse symbols for a partition (and therefore the
corresponding labeled work queue) a signature.
During parsing, we only enqueue items
(s, i, k, j) to a labeled queue if two conditions are
met. First, the span (i, j)?s pruning mask must
have a non-empty intersection with the signature
of the queue. Second, the pruning mask for the
children (i, k) and (k, j) must be non-empty.
Once on the GPU, parse items are processed us-
ing the same style of compiled kernel as in Canny
et al (2013). Because the entire partition (though
not necessarily the entire grammar) is applied to
each item in the queue, we still do not need to
worry about warp divergence.
At the top level, our system first computes prun-
ing masks with a coarse grammar. Then it pro-
cesses the same sentences with the fine gram-
mar. However, to the extent that the signatures
are small, items can be selectively queued only to
certain queues. This approach is diagrammed in
Figure 3.
We tested our new pruning approach using an
X-bar grammar as the coarse pass. The result-
ing speed is 187.5 sentences per second, labeled
in Table 1 as row labeled ?Reimpl? with ?Labeled,
Coarse? pruning. Unfortunately, this approach
again does not produce a speedup relative to our
reimplemented baseline. To improve upon this re-
sult, we need to consider how the grammar clus-
tering interacts with the coarse pruning phase.
7 Grammar Clustering
Recall that the rules in the grammar are partitioned
into a set of clusters, and that these clusters are
further divided into subclusters. How can we best
cluster and subcluster the grammar so as to maxi-
mize performance? A good clustering will group
rules together that use the same symbols, since
this means fewer memory accesses to read and
write scores for symbols. Moreover, we would
like the time spent processing each of the subclus-
ters within a cluster to be about the same. We can-
not move on to the next cluster until all threads
from a cluster are finished, which means that the
time a cluster takes is the amount of time taken
by the longest-running subcluster. Finally, when
pruning, it is best if symbols that have the same
coarse projection are clustered together. That way,
we are more likely to be able to skip a subcluster,
since fewer distinct symbols need to be ?off? for a
parse item to be skipped in a given subcluster.
Canny et al (2013) clustered symbols of the
grammar using a sophisticated spectral clustering
algorithm to obtain a permutation of the symbols.
Then the rules of the grammar were laid out in
a (sparse) three-dimensional tensor, with one di-
mension representing the parent of the rule, one
representing the left child, and one representing
the right child. They then split the cube into 6x2x2
contiguous ?major cubes,? giving a partition of the
rules into 24 clusters. They then further subdi-
vided these cubes into 2x2x2 minor cubes, giv-
ing 8 subclusters that executed in parallel. Note
that the clusters induced by these major and minor
cubes need not be of similar sizes; indeed, they of-
ten are not. Clustering using this method is labeled
?Reimplementation? in Table 1.
The addition of pruning introduces further con-
siderations. First, we have a coarse grammar, with
212
many fewer rules and symbols. Second, we are
able to skip a parse item for an entire cluster if that
item?s pruning mask does not intersect the clus-
ter?s signature. Spreading symbols across clusters
may be inefficient: if a parse item licenses a given
symbol, we will have to enqueue that item to any
queue that has the symbol in its signature, no mat-
ter how many other symbols are in that cluster.
Thus, it makes sense to choose a clustering al-
gorithm that exploits the structure introduced by
the pruning masks. We use a very simple method:
we cluster the rules in the grammar by coarse par-
ent symbol. When coarse symbols are extremely
unlikely (and therefore have few corresponding
rules), we merge their clusters to avoid the over-
head of beginning work on clusters where little
work has to be done.
4
In order to subcluster, we
divide up rules among subclusters so that each
subcluster has the same number of active parent
symbols. We found this approach to subclustering
worked well in practice.
Clustering using this method is labeled ?Parent?
in Table 1. Now, when we use a coarse pruning
pass, we are able to parse nearly 280 sentences
per second, a 70% increase in parsing performance
relative to Canny et al (2013)?s system, and nearly
50% over our reimplemented baseline.
It turns out that this simple clustering algorithm
produces relatively efficient kernels even in the un-
pruned case. The unpruned Viterbi computations
in a fine grammar using the clustering method of
Canny et al (2013) yields a speed of 193 sen-
tences per second, whereas the same computation
using coarse parent clustering has a speed of 159
sentences per second. (See Table 1.) This is not
as efficient as Canny et al (2013)?s highly tuned
method, but it is still fairly fast, and much simpler
to implement.
8 Pruning with Finer Grammars
The coarse to fine pruning approach of Petrov and
Klein (2007) employs an X-bar grammar as its
first pruning phase, but there is no reason why
we cannot begin with a more complex grammar
for our initial pass. As Petrov and Klein (2007)
have shown, intermediate-sized Berkeley gram-
mars prune many more symbols than the X-bar
system. However, they are slower to parse with
4
Specifically, after clustering based on the coarse parent
symbol, we merge all clusters with less than 300 rules in them
into one large cluster.
in a CPU context, and so they begin with an X-bar
grammar.
Because of the overhead associated with trans-
ferring work items to GPU, using a very small
grammar may not be an efficient use of the GPU?s
computational resources. To that end, we tried
computing pruning masks with one-split and two-
split Berkeley grammars. The X-bar grammar can
compute pruning masks at just over 1000 sen-
tences per second, the 1-split grammar parses 858
sentences per second, and the 2-split grammar
parses 526 sentences per second.
Because parsing with these grammars is still
quite fast, we tried using them as the coarse pass
instead. As shown in Table 1, using a 1-split gram-
mar as a coarse pass allows us to produce over 400
sentences per second, a full 2x improvement over
our original system. Conducting a coarse pass
with a 2-split grammar is somewhat slower, at a
?mere? 343 sentences per second.
9 Minimum Bayes risk parsing
The Viterbi algorithm is a reasonably effective
method for parsing. However, many authors
have noted that parsers benefit substantially from
minimum Bayes risk decoding (Goodman, 1996;
Simaan, 2003; Matsuzaki et al, 2005; Titov and
Henderson, 2006; Petrov and Klein, 2007). MBR
algorithms for parsing do not compute the best
derivation, as in Viterbi parsing, but instead the
parse tree that maximizes the expected count of
some figure of merit. For instance, one might want
to maximize the expected number of correct con-
stituents (Goodman, 1996), or the expected rule
counts (Simaan, 2003; Petrov and Klein, 2007).
MBR parsing has proven especially useful in la-
tent variable grammars. Petrov and Klein (2007)
showed that MBR trees substantially improved
performance over Viterbi parses for latent variable
grammars, earning up to 1.5F1.
Here, we implement the Max Recall algorithm
of Goodman (1996). This algorithm maximizes
the expected number of correct coarse symbols
(A, i, j) with respect to the posterior distribution
over parses for a sentence.
This particular MBR algorithm has the advan-
tage that it is relatively straightforward to imple-
ment. In essence, we must compute the marginal
probability of each fine-labeled span ?(A
x
, i, j),
and then marginalize to obtain ?(A, i, j). Then,
for each span (i, j), we find the best possible split
213
point k that maximizes C(i, j) = ?(A, i, j) +
max
k
(C(i, k) + C(k, j)). Parse extraction is
then just a matter of following back pointers from
the root, as in the Viterbi algorithm.
9.1 Computing marginal probabilities
The easiest way to compute marginal probabilities
is to use the log space semiring rather than the
Viterbi semiring, and then to run the inside and
outside algorithms as before. We should expect
this algorithm to be at least a factor of two slower:
the outside pass performs at least as much work as
the inside pass. Moreover, it typically has worse
memory access patterns, leading to slower perfor-
mance.
Without pruning, our approach does not han-
dle these log domain computations well at all:
we are only able to compute marginals for 32.1
sentences/second, more than a factor of 5 slower
than our coarse pass. To begin, log space addition
requires significantly more operations than max,
which is a primitive operation on GPUs. Beyond
the obvious consequence that executing more op-
erations means more time taken, the sheer number
of operations becomes too much for the compiler
to handle. Because the grammars are compiled
into code, the additional operations are all inlined
into the kernels, producing much larger kernels.
Indeed, in practice the compiler will often hang if
we use the same size grammar clusters as we did
for Viterbi. In practice, we found there is an effec-
tive maximum of 2000 rules per kernel using log
sums, while we can use more than 10,000 rules
rules in a single kernel with Viterbi.
With coarse pruning, however, we can avoid
much of the increased cost associated with log
domain computations. Because so many labeled
spans are pruned, we are able to skip many of the
grammar clusters and thus avoid many of the ex-
pensive operations. Using coarse pruning and log
domain calculations, our system produces MBR
trees at a rate of 130.4 sentences per second, a
four-fold increase.
9.2 Scaling with the Coarse Pass
One way to avoid the expense of log domain com-
putations is to use scaled probabilities rather than
log probabilities. Scaling is one of the folk tech-
niques that are commonly used in the NLP com-
munity, but not generally written about. Recall
that floating point numbers are composed of a
mantissa m and an exponent e, giving a number
System Sent/Sec Speedup
Unpruned Log Sum MBR 32.1 ?
Pruned Log Sum MBR 130.4 4.1x
Pruned Scaling MBR 190.6 5.9x
Pruned Viterbi 404.7 12.6x
Table 2: Performance numbers for computing max
constituent (Goodman, 1996) trees on 20,000 sen-
tences of length 40 or less from the Penn Tree-
bank. For convenience, we have copied our pruned
Viterbi system?s result.
f = m ? 2
e
. When a float underflows, the ex-
ponent becomes too low to represent the available
number of bits. In scaling, floating point numbers
are paired with an additional number that extends
the exponent. That is, the number is represented
as f
?
= f ? exp(s). Whenever f becomes either
too big or too small, the number is rescaled back
to a less ?dangerous? range by shifting mass from
the exponent e to the scaling factor s.
In practice, one scale s is used for an entire span
(i, j), and all scores for that span are rescaled in
concert. In our GPU system, multiple scores in
any given span are being updated at the same time,
which makes this dynamic rescaling tricky and ex-
pensive, especially since inter-warp communica-
tion is fairly limited.
We propose a much simpler static solution that
exploits the coarse pass. In the coarse pass, we
compute Viterbi inside and outside scores for ev-
ery span. Because the grammar used in the coarse
pass is a projection of the grammar used in the
fine pass, these coarse scores correlate reasonably
closely with the probabilities computed in the fine
pass: If a span has a very high or very low score
in the coarse pass, it typically has a similar score
in the fine pass. Thus, we can use the coarse
pass?s inside and outside scores as the scaling val-
ues for the fine pass?s scores. That is, in addition
to computing a pruning mask, in the coarse pass
we store the maximum inside and outside score in
each span, giving two arrays of scores s
I
i,j
and s
O
i,j
.
Then, when applying rules in the fine pass, each
fine inside score over a split span (i, k, j) is scaled
to the appropriate s
I
i,j
by multiplying the score by
exp
(
s
I
i,k
+ s
I
k,j
? s
I
i,j
)
, where s
I
i,k
, s
I
k,j
, s
I
i,j
are
the scaling factors for the left child, right child,
and parent, respectively. The outside scores are
scaled analogously.
By itself, this approach works on nearly ev-
ery sentence. However, scores for approximately
214
0.5% of sentences overflow (sic). Because we are
summing instead of maxing scores in the fine pass,
the scaling factors computed using max scores are
not quite large enough, and so the rescaled inside
probabilities grow too large when multiplied to-
gether. Most of this difference arises at the leaves,
where the lexicon typically has more uncertainty
than higher up in the tree. Therefore, in the fine
pass, we normalize the inside scores at the leaves
to sum to 1.0.
5
Using this slight modification, no
sentences from the Treebank under- or overflow.
We know of no reason why this same trick can-
not be employed in more traditional parsers, but
it is especially useful here: with this static scal-
ing, we can avoid the costly log sums without in-
troducing any additional inter-thread communica-
tion, making the kernels much smaller and much
faster. Using scaling, we are able to push our
parser to 190.6 sentences/second for MBR extrac-
tion, just under half the speed of the Viterbi sys-
tem.
9.3 Parsing Accuracies
It is of course important verify the correctness of
our system; one easy way to do so is to exam-
ine parsing accuracy, as compared to the original
Berkeley parser. We measured parsing accuracy
on sentences of length? 40 from section 22 of the
Penn Treebank. Our Viterbi parser achieves 89.7
F1, while our MBR parser scores 91.0. These re-
sults are nearly identical to the Berkeley parsers
most comparable numbers: 89.8 for Viterbi, and
90.9 for their ?Max-Rule-Sum? MBR algorithm.
These slight differences arise from the usual mi-
nor variation in implementation details. In partic-
ular, we use one coarse pass instead of several, and
a different MBR algorithm. In addition, there are
some differences in unary processing.
10 Analyzing System Performance
In this section we attempt to break down how ex-
actly our system is spending its time. We do this in
an effort to give a sense of how time is spent dur-
ing computation on GPUs. These timing numbers
are computed using the built-in profiling capabil-
ities of the programming environment. As usual,
profiles exhibit an observer effect, where the act of
measuring the system changes the execution. Nev-
5
One can instead interpret this approach as changing the
scaling factors to s
I
?
i,j
= s
I
i,j
?
?
i?k<j
?
A
inside(A, k, k +
1), where inside is the array of scores for the fine pass.
System Coarse Pass Fine Pass
Unpruned Viterbi ? 6.4
Pruned Viterbi 1.2 1.5
Unpruned Logsum MBR ? 28.6
Pruned Scaling MBR 1.2 4.3
Table 3: Time spent in the passes of our differ-
ent systems, in seconds per 1000 sentences. Prun-
ing refers to using a 1-split grammar for the coarse
pass.
ertheless, the general trends should more or less be
preserved as compared to the unprofiled code.
To begin, we can compute the number of sec-
onds needed to parse 1000 sentences. (We use sec-
onds per sentence rather than sentences per second
because the former measure is additive.) The re-
sults are in Table 3. In the case of pruned Viterbi,
pruning reduces the amount of time spent in the
fine pass by more than 4x, though half of those
gains are lost to computing the pruning masks.
In Table 4, we break down the time taken by
our system into individual components. As ex-
pected, binary rules account for the vast majority
of the time in the unpruned Viterbi case, but much
less time in the pruned case, with the total time
taken for binary rules in the coarse and fine passes
taking about 1/5 of the time taken by binaries in
the unpruned version. Queueing, which involves
copying memory around within the GPU to pro-
cess the individual parse items, takes a fairly con-
sistent amount of time in all systems. Overhead,
which includes transport time between the CPU
and GPU and other processing on the CPU, is rela-
tively small for most system configurations. There
is greater overhead in the scaling system, because
scaling factors are copied to the CPU between the
coarse and fine passes.
A final question is: how many sentences per
second do we need to process to saturate the
GPU?s processing power? We computed Viterbi
parses of successive powers of 10, from 1 to
100,000 sentences.
6
In Figure 4, we then plotted
the throughput, in terms of number of sentences
per second. Throughput increases through parsing
10,000 sentences, and then levels off by the time it
reaches 100,000 sentences.
6
We replicated the Treebank for the 100,000 sentences
pass.
215
System Coarse Pass Fine Pass
Binary Unary Queueing Masks Overhead Binary Unary Queueing Overhead
Unpruned Viterbi ? ? ? ? ? 5.42 0.14 0.33 0.40
Pruned Viterbi 0.59 0.02 0.19 0.04 0.22 0.56 0.10 0.34 0.22
Pruned Scaling 0.59 0.02 0.19 0.04 0.20 1.74 0.24 0.46 0.84
Table 4: Breakdown of time spent in our different systems, in seconds per 1000 sentences. Binary and
Unary refer to spent processing binary rules. Queueing refers to the amount of time used to move memory
around within the GPU for processing. Overhead includes all other time, which includes communication
between the GPU and the CPU.
Sentenc
es/Seco
nd
0
100
200
300
400
Number of Sentences1 10 100 1K 10K 100K
Figure 4: Plot of speeds (sentences / second) for
various sizes of input corpora. The full power of
the GPU parser is only reached when run on large
numbers of sentences.
11 Related Work
Apart from the model of Canny et al (2013), there
have been a few attempts at using GPUs in NLP
contexts before. Johnson (2011) and Yi et al
(2011) both had early attempts at porting pars-
ing algorithms to the GPU. However, they did
not demonstrate significantly increased speed over
a CPU implementation. In machine translation,
He et al (2013) adapted algorithms designed for
GPUs in the computational biology literature to
speed up on-demand phrase table extraction.
12 Conclusion
GPUs represent a challenging opportunity for nat-
ural language processing. By carefully design-
ing within the constraints imposed by the architec-
ture, we have created a parser that can exploit the
same kinds of sparsity that have been developed
for more traditional architectures.
One of the key remaining challenges going
forward is confronting the kind of lexicalized
sparsity common in other NLP models. The
Berkeley parser?s grammars?by virtue of being
unlexicalized?can be applied uniformly to all
parse items. The bilexical features needed by
dependency models and lexicalized constituency
models are not directly amenable to acceleration
using the techniques we described here. Deter-
mining how to efficiently implement these kinds
of models is a promising area for new research.
Our system is available as open-source at
https://www.github.com/dlwh/puck.
Acknowledgments
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014, by a
Google PhD fellowship to the first author, and
an NSF fellowship to the second. We further
gratefully acknowledge a hardware donation by
NVIDIA Corporation.
References
John Canny, David Hall, and Dan Klein. 2013. A
multi-teraflop constituency parser using GPUs. In
Proceedings of EMNLP, pages 1898?1907, October.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R Shrivaths, Jeremy Moore, Michael Pozar,
et al 2006. Multilevel coarse-to-fine pcfg pars-
ing. In Proceedings of the main conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 168?175. Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In ACL, pages 177?183.
Hua He, Jimmy Lin, and Adam Lopez. 2013. Mas-
sively parallel suffix array queries and on-demand
phrase extraction for statistical machine translation
using gpus. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 325?334, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.
Mark Johnson. 2011. Parsing in parallel on multiple
cores and gpus. In Proceedings of the Australasian
Language Technology Association Workshop.
216
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82, Morristown, NJ, USA.
CUDA Nvidia. 2008. Programming guide.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT.
Khalil Simaan. 2003. On maximizing metrics for syn-
tactic disambiguation. In Proceedings of IWPT.
Ivan Titov and James Henderson. 2006. Loss min-
imization in parse reranking. In Proceedings of
EMNLP, pages 560?567. Association for Computa-
tional Linguistics.
Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efficient parallel cky parsing on
gpus. In Proceedings of the 2011 Conference on
Parsing Technologies, Dublin, Ireland, October.
217
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 228?237,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Less Grammar, More Features
David Hall Greg Durrett Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,gdurrett,klein}@cs.berkeley.edu
Abstract
We present a parser that relies primar-
ily on extracting information directly from
surface spans rather than on propagat-
ing information through enriched gram-
mar structure. For example, instead of cre-
ating separate grammar symbols to mark
the definiteness of an NP, our parser might
instead capture the same information from
the first word of the NP. Moving context
out of the grammar and onto surface fea-
tures can greatly simplify the structural
component of the parser: because so many
deep syntactic cues have surface reflexes,
our system can still parse accurately with
context-free backbones as minimal as X-
bar grammars. Keeping the structural
backbone simple and moving features to
the surface also allows easy adaptation
to new languages and even to new tasks.
On the SPMRL 2013 multilingual con-
stituency parsing shared task (Seddah et
al., 2013), our system outperforms the top
single parser system of Bj?orkelund et al
(2013) on a range of languages. In addi-
tion, despite being designed for syntactic
analysis, our system also achieves state-
of-the-art numbers on the structural senti-
ment task of Socher et al (2013). Finally,
we show that, in both syntactic parsing and
sentiment analysis, many broad linguistic
trends can be captured via surface features.
1 Introduction
Na??ve context-free grammars, such as those em-
bodied by standard treebank annotations, do not
parse well because their symbols have too little
context to constrain their syntactic behavior. For
example, to PPs usually attach to verbs and of
PPs usually attach to nouns, but a context-free PP
symbol can equally well attach to either. Much
of the last few decades of parsing research has
therefore focused on propagating contextual in-
formation from the leaves of the tree to inter-
nal nodes. For example, head lexicalization (Eis-
ner, 1996; Collins, 1997; Charniak, 1997), struc-
tural annotation (Johnson, 1998; Klein and Man-
ning, 2003), and state-splitting (Matsuzaki et al,
2005; Petrov et al, 2006) are all designed to take
coarse symbols like PP and decorate them with
additional context. The underlying reason that
such propagation is even needed is that PCFG
parsers score trees based on local configurations
only, and any information that is not threaded
through the tree becomes inaccessible to the scor-
ing function. There have been non-local ap-
proaches as well, such as tree-substitution parsers
(Bod, 1993; Sima?an, 2000), neural net parsers
(Henderson, 2003), and rerankers (Collins and
Koo, 2005; Charniak and Johnson, 2005; Huang,
2008). These non-local approaches can actually
go even further in enriching the grammar?s struc-
tural complexity by coupling larger domains in
various ways, though their non-locality generally
complicates inference.
In this work, we instead try to minimize the
structural complexity of the grammar by moving
as much context as possible onto local surface fea-
tures. We examine the position that grammars
should not propagate any information that is avail-
able from surface strings, since a discriminative
parser can access that information directly. We
therefore begin with a minimal grammar and it-
eratively augment it with rich input features that
do not enrich the context-free backbone. Previ-
ous work has also used surface features in their
parsers, but the focus has been on machine learn-
ing methods (Taskar et al, 2004), latent annota-
tions (Petrov and Klein, 2008a; Petrov and Klein,
2008b), or implementation (Finkel et al, 2008).
By contrast, we investigate the extent to which
228
we need a grammar at all. As a thought experi-
ment, consider a parser with no grammar, which
functions by independently classifying each span
(i, j) of a sentence as an NP, VP, and so on, or
null if that span is a non-constituent. For exam-
ple, spans that begin with the might tend to be
NPs, while spans that end with of might tend to
be non-constituents. An independent classification
approach is actually very viable for part-of-speech
tagging (Toutanova et al, 2003), but is problem-
atic for parsing ? if nothing else, parsing comes
with a structural requirement that the output be a
well-formed, nested tree. Our parser uses a min-
imal PCFG backbone grammar to ensure a ba-
sic level of structural well-formedness, but relies
mostly on features of surface spans to drive accu-
racy. Formally, our model is a CRF where the fea-
tures factor over anchored rules of a small back-
bone grammar, as shown in Figure 1.
Some aspects of the parsing problem, such as
the tree constraint, are clearly best captured by a
PCFG. Others, such as heaviness effects, are nat-
urally captured using surface information. The
open question is whether surface features are ade-
quate for key effects like subcategorization, which
have deep definitions but regular surface reflexes
(e.g. the preposition selected by a verb will often
linearly follow it). Empirically, the answer seems
to be yes, and our system produces strong results,
e.g. up to 90.5 F1 on English parsing. Our parser
is also able to generalize well across languages
with little tuning: it achieves state-of-the-art re-
sults on multilingual parsing, scoring higher than
the best single-parser system from the SPMRL
2013 Shared Task on a range of languages, as well
as on the competition?s average F1 metric.
One advantage of a system that relies on surface
features and a simple grammar is that it is portable
not only across languages but also across tasks
to an extent. For example, Socher et al (2013)
demonstrates that sentiment analysis, which is
usually approached as a flat classification task,
can be viewed as tree-structured. In their work,
they propagate real-valued vectors up a tree using
neural tensor nets and see gains from their recur-
sive approach. Our parser can be easily adapted
to this task by replacing the X-bar grammar over
treebank symbols with a grammar over the sen-
timent values to encode the output variables and
then adding n-gram indicators to our feature set
to capture the bulk of the lexical effects. When
applied to this task, our system generally matches
their accuracy overall and is able to outperform it
on the overall sentence-level subtask.
2 Parsing Model
In order to exploit non-independent surface fea-
tures of the input, we use a discriminative formula-
tion. Our model is a conditional random field (Laf-
ferty et al, 2001) over trees, in the same vein as
Finkel et al (2008) and Petrov and Klein (2008a).
Formally, we define the probability of a tree T
conditioned on a sentence w as
p(T |w) ? exp
(
?
?
?
r?T
f(r,w)
)
(1)
where the feature domains r range over the (an-
chored) rules used in the tree. An anchored rule
r is the conjunction of an unanchored grammar
rule rule(r) and the start, stop, and split indexes
where that rule is anchored, which we refer to as
span(r). It is important to note that the richness of
the backbone grammar is reflected in the structure
of the trees T , while the features that condition di-
rectly on the input enter the equation through the
anchoring span(r). To optimize model parame-
ters, we use the Adagrad algorithm of Duchi et al
(2010) with L2 regularization.
We start with a simple X-bar grammar whose
only symbols are NP, NP-bar, VP, and so on. Our
base model has no surface features: formally, on
each anchored rule r we have only an indicator of
the (unanchored) rule identity, rule(r). Because
the X-bar grammar is so minimal, this grammar
does not parse very accurately, scoring just 73 F1
on the standard English Penn Treebank task.
In past work that has used tree-structured CRFs
in this way, increased accuracy partially came
from decorating trees T with additional annota-
tions, giving a tree T
?
over a more complex symbol
set. These annotations introduce additional con-
text into the model, usually capturing linguistic in-
tuition about the factors that influence grammati-
cality. For instance, we might annotate every con-
stituent X in the tree with its parent Y , giving a
tree with symbolsX[?Y ]. Finkel et al (2008) used
parent annotation, head tag annotation, and hori-
zontal sibling annotation together in a single large
grammar. In Petrov and Klein (2008a) and Petrov
and Klein (2008b), these annotations were latent;
they were inferred automatically during training.
229
Hall and Klein (2012) employed both kinds of an-
notations, along with lexicalized head word anno-
tation. All of these past CRF parsers do also ex-
ploit span features, as did the structured margin
parser of Taskar et al (2004); the current work pri-
marily differs in shifting the work from the gram-
mar to the surface features.
The problem with rich annotations is that they
increase the state space of the grammar substan-
tially. For example, adding parent annotation can
square the number of symbols, and each subse-
quent annotation causes a multiplicative increase
in the size of the state space. Hall and Klein
(2012) attempted to reduce this state space by fac-
toring these annotations into individual compo-
nents. Their approach changed the multiplicative
penalty of annotation into an additive penalty, but
even so their individual grammar projections are
much larger than the base X-bar grammar.
In this work, we want to see how much of the
expressive capability of annotations can be cap-
tured using surface evidence, with little or no an-
notation of the underlying grammar. To that end,
we avoid annotating our trees at all, opting instead
to see how far simple surface features will go in
achieving a high-performance parser. We will re-
turn to the question of annotation in Section 5.
3 Surface Feature Framework
To improve the performance of our X-bar gram-
mar, we will add a number of surface feature tem-
plates derived only from the words in the sentence.
We say that an indicator is a surface property if
it can be extracted without reference to the parse
tree. These features can be implemented with-
out reference to structured linguistic notions like
headedness; however, we will argue that they still
capture a wide range of linguistic phenomena in a
data-driven way.
Throughout this and the following section, we
will draw on motivating examples from the En-
glish Penn Treebank, though similar examples
could be equally argued for other languages. For
performance on other languages, see Section 6.
Recall that our CRF factors over anchored rules
r, where each r has identity rule(r) and anchor-
ing span(r). The X-bar grammar has only indi-
cators of rule(r), ignoring the anchoring. Let a
surface property of r be an indicator function of
span(r) and the sentence itself. For example, the
first word in a constituent is a surface property, as
averted    financial    disaster
VP
NP
VBD
JJ NN
PARENT = VP
FIRSTWORD = averted
LENGTH = 3
RULE = VP ? VBD NP
PARENT = VP
Span properties
Rule backoffs
Features
...
5 6
7
8
...
LASTWORD = disaster
?
FIRSTWORD = averted
LASTWORD = disaster PARENT = VP
?
?
FIRSTWORD = averted
RULE = VP ? VBD NP
Figure 1: Features computed over the application
of the rule VP ? VBD NP over the anchored
span averted financial disaster with the shown in-
dices. Span properties are generated as described
throughout Section 4; they are then conjoined with
the rule and just the parent nonterminal to give the
features fired over the anchored production.
is the word directly preceding the constituent. As
illustrated in Figure 1, the actual features of the
model are obtained by conjoining surface proper-
ties with various abstractions of the rule identity.
For rule abstractions, we use two templates: the
parent of the rule and the identity of the rule. The
surface features are somewhat more involved, and
so we introduce them incrementally.
One immediate computational and statistical is-
sue arises from the sheer number of possible sur-
face features. There are a great number of spans
in a typical treebank; extracting features for ev-
ery possible combination of span and rule is pro-
hibitive. One simple solution is to only extract
features for rule/span pairs that are actually ob-
served in gold annotated examples during train-
ing. Because these ?positive? features correspond
to observed constituents, they are far less numer-
ous than the set of all possible features extracted
from all spans. As far as we can tell, all past CRF
parsers have used ?positive? features only.
However, negative features?features that are
not observed in any tree?are still powerful indica-
tors of (un)grammaticality: if we have never seen
a PRN that starts with ?has,? or a span that be-
gins with a quotation mark and ends with a close
bracket, then we would like the model to be able to
place negative weights on these features. Thus, we
use a simple feature hashing scheme where posi-
tive features are indexed individually, while nega-
230
Features Section F1
RULE 4 73.0
+ SPAN FIRST WORD + SPAN LAST WORD + LENGTH 4.1 85.0
+ WORD BEFORE SPAN + WORD AFTER SPAN 4.2 89.0
+ WORD BEFORE SPLIT + WORD AFTER SPLIT 4.3 89.7
+ SPAN SHAPE 4.4 89.9
Table 1: Results for the Penn Treebank development set, reported in F1 on sentences of length ? 40
on Section 22, for a number of incrementally growing feature sets. We show that each feature type
presented in Section 4 adds benefit over the previous, and in combination they produce a reasonably
good yet simple parser.
tive features are bucketed together. During train-
ing there are no collisions between positive fea-
tures, which generally receive positive weight, and
negative features, which generally receive nega-
tive weight; only negative features can collide.
Early experiments indicated that using a number
of negative buckets equal to the number of posi-
tive features was effective.
4 Features
Our goal is to use surface features to replicate
the functionality of other annotations, without in-
creasing the state space of our grammar, meaning
that the rules rule(r) remain simple, as does the
state space used during inference.
Before we present our main features, we briefly
discuss the issue of feature sparsity. While lexical
features are a powerful driver of our parser, firing
features on rare words would allow it to overfit the
training data quite heavily. To that end, for the
purposes of computing our features, a word is rep-
resented by its longest suffix that occurs 100 or
more times in the training data (which will be the
entire word, for common words).
1
Table 1 shows the results of incrementally
building up our feature set on the Penn Treebank
development set. RULE specifies that we use only
indicators on rule identity for binary production
and nonterminal unaries. For this experiment and
all others, we include a basic set of lexicon fea-
tures, i.e. features on preterminal part-of-speech
tags. A given preterminal unary at position i in
the sentence includes features on the words (suf-
fixes) at position i ? 1, i, and i + 1. Because the
lexicon is especially sensitive to morphological ef-
fects, we also fire features on all prefixes and suf-
1
Experiments with the Brown clusters (Brown et al,
1992) provided by Turian et al (2010) in lieu of suffixes were
not promising. Moreover, lowering this threshold did not im-
prove performance.
fixes of the current word up to length 5, regardless
of frequency.
Subsequent lines in Table 1 indicate additional
surface feature templates computed over the span,
which are then conjoined with the rule identity as
shown in Figure 1 to give additional features. In
the rest of the section, we describe the features of
this type that we use. Note that many of these fea-
tures have been used before (Taskar et al, 2004;
Finkel et al, 2008; Petrov and Klein, 2008b); our
goal here is not to amass as many feature tem-
plates as possible, but rather to examine the ex-
tent to which a simple set of features can replace a
complicated state space.
4.1 Basic Span Features
We start with some of the most obvious proper-
ties available to us, namely, the identity of the first
and last words of a span. Because heads of con-
stituents are often at the beginning or the end of
a span, these feature templates can (noisily) cap-
ture monolexical properties of heads without hav-
ing to incur the inferential cost of lexicalized an-
notations. For example, in English, the syntactic
head of a verb phrase is typically at the beginning
of the span, while the head of a simple noun phrase
is the last word. Other languages, like Korean or
Japanese, are more consistently head final.
Structural contexts like those captured by par-
ent annotation (Johnson, 1998) are more subtle.
Parent annotation can capture, for instance, the
difference in distribution in NPs that have S as a
parent (that is, subjects) and NPs under VPs (ob-
jects). We try to capture some of this same intu-
ition by introducing a feature on the length of a
span. For instance, VPs embedded in NPs tend
to be short, usually as embedded gerund phrases.
Because constituents in the treebank can be quite
long, we bin our length features into 8 buckets, of
231
no  read  messages  in  his  inbox
VP
VBP NNS
VP ? no VBP NNS
Figure 2: An example showing the utility of span
context. The ambiguity about whether read is an
adjective or a verb is resolved when we construct
a VP and notice that the word proceeding it is un-
likely.
has  an  impact  on  the  market
PPNP
NP
NP ? (NP ... impact) PP)
Figure 3: An example showing split point features
disambiguating a PP attachment. Because impact
is likely to take a PP, the monolexical indicator
feature that conjoins impact with the appropriate
rule will help us parse this example correctly.
lengths 1, 2, 3, 4, 5, 10, 20, and ?21 words.
Adding these simple features (first word, last
word, and lengths) as span features of the X-
bar grammar already gives us a substantial im-
provement over our baseline system, improving
the parser?s performance from 73.0 F1 to 85.0 F1
(see Table 1).
4.2 Span Context Features
Of course, there is no reason why we should con-
fine ourselves to just the words within the span:
words outside the span also provide a rich source
of context. As an example, consider disambiguat-
ing the POS tag of the word read in Figure 2. A
VP is most frequently preceded by a subject NP,
whose rightmost word is often its head. Therefore,
we fire features that (separately) look at the words
immediately preceding and immediately follow-
ing the span.
4.3 Split Point Features
Another important source of features are the words
at and around the split point of a binary rule ap-
plication. Figure 3 shows an example of one in-
(  CEO  of  Enron  )
PRN
(XxX)
     said  ,  ?  Too  bad  ,  ?
VP
x,?Xx,?
Figure 4: Computation of span shape features on
two examples. Parentheticals, quotes, and other
punctuation-heavy, short constituents benefit from
being explicitly modeled by a descriptor like this.
stance of this feature template. impact is a noun
that is more likely to take a PP than other nouns,
and so we expect this feature to have high weight
and encourage the attachment; this feature proves
generally useful in resolving such cases of right-
attachments to noun phrases, since the last word
of the noun phrase is often the head. As another
example, coordination can be represented by an
indicator of the conjunction, which comes imme-
diately after the split point. Finally, control struc-
tures with infinitival complements can be captured
with a rule S? NP VP with the word ?to? at the
split point.
4.4 Span Shape Features
We add one final feature characterizing the span,
which we call span shape. Figure 4 shows how this
feature is computed. For each word in the span,
2
we indicate whether that word begins with a cap-
ital letter, lowercase letter, digit, or punctuation
mark. If it begins with punctuation, we indicate
the punctuation mark explicitly. Figure 4 shows
that this is especially useful in characterizing con-
structions such as parentheticals and quoted ex-
pressions. Because this feature indicates capital-
ization, it can also capture properties of NP in-
ternal structure relevant to named entities, and its
sensitivity to capitalization and punctuation makes
it useful for recognizing appositive constructions.
5 Annotations
We have built up a strong set of features by this
point, but have not yet answered the question of
whether or not grammar annotation is useful on
top of them. In this section, we examine two of the
most commonly used types of additional annota-
tion, structural annotation, and lexical annotation.
2
For longer spans, we only use words sufficiently close to
the span?s beginning and end.
232
Annotation Dev, len ? 40
v = 0, h = 0 90.1
v = 1, h = 0 90.5
v = 0, h = 1 90.2
v = 1, h = 1 90.9
Lexicalized 90.3
Table 2: Results for the Penn Treebank develop-
ment set, sentences of length ? 40, for different
annotation schemes implemented on top of the X-
bar grammar.
Recall from Section 3 that every span feature is
conjoined with indicators over rules and rule par-
ents to produce features over anchored rule pro-
ductions; when we consider adding an annotation
layer to the grammar, what that does is refine the
rule indicators that are conjoined with every span
feature. While this is a powerful way of refining
features, we show that common successful anno-
tation schemes provide at best modest benefit on
top of the base parser.
5.1 Structural Annotation
The most basic, well-understood kind of annota-
tion on top of an X-bar grammar is structural an-
notation, which annotates each nonterminal with
properties of its environment (Johnson, 1998;
Klein and Manning, 2003). This includes vertical
annotation (parent, grandparent, etc.) as well as
horizontal annotation (only partially Markovizing
rules as opposed to using an X-bar grammar).
Table 2 shows the performance of our feature
set in grammars with several different levels of
structural annotation.
3
Klein and Manning (2003)
find large gains (6% absolute improvement, 20%
relative improvement) going from v = 0, h = 0 to
v = 1, h = 1; however, we do not find the same
level of benefit. To the extent that our parser needs
to make use of extra information in order to ap-
ply a rule correctly, simply inspecting the input to
determine this information appears to be almost
as effective as relying on information threaded
through the parser.
In Section 6 and Section 7, we use v = 1 and
h = 0; we find that v = 1 provides a small, reli-
able improvement across a range of languages and
tasks, whereas other annotations are less clearly
beneficial.
3
We use v = 0 to indicate no annotation, diverging from
the notation in Klein and Manning (2003).
Test ? 40 Test all
Berkeley 90.6 90.1
This work 89.9 89.2
Table 3: Final Parseval results for the v = 1, h = 0
parser on Section 23 of the Penn Treebank.
5.2 Lexical Annotation
Another commonly-used kind of structural an-
notation is lexicalization (Eisner, 1996; Collins,
1997; Charniak, 1997). By annotating grammar
nonterminals with their headwords, the idea is to
better model phenomena that depend heavily on
the semantics of the words involved, such as coor-
dination and PP attachment.
Table 2 shows results from lexicalizing the X-
bar grammar; it provides meager improvements.
One probable reason for this is that our parser al-
ready includes monolexical features that inspect
the first and last words of each span, which cap-
tures the syntactic or the semantic head in many
cases or can otherwise provide information about
what the constituent?s type may be and how it is
likely to combine. Lexicalization allows us to cap-
ture bilexical relationships along dependency arcs,
but it has been previously shown that these add
only marginal benefit to Collins?s model anyway
(Gildea, 2001).
5.3 English Evaluation
Finally, Table 3 shows our final evaluation on Sec-
tion 23 of the Penn Treebank. We use the v =
1, h = 0 grammar. While we do not do as well as
the Berkeley parser, we will see in Section 6 that
our parser does a substantially better job of gener-
alizing to other languages.
6 Other Languages
Historically, many annotation schemes for parsers
have required language-specific engineering: for
example, lexicalized parsers require a set of head
rules and manually-annotated grammars require
detailed analysis of the treebank itself (Klein and
Manning, 2003). A key strength of a parser that
does not rely heavily on an annotated grammar is
that it may be more portable to other languages.
We show that this is indeed the case: on nine lan-
guages, our system is competitive with or better
than the Berkeley parser, which is the best single
233
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Avg
Dev, all lengths
Berkeley 78.24 69.17 79.74 81.74 87.83 83.90 70.97 84.11 74.50 78.91
Berkeley-Rep 78.70 84.33 79.68 82.74 89.55 89.08 82.84 87.12 75.52 83.28
Our work 78.89 83.74 79.40 83.28 88.06 87.44 81.85 91.10 75.95 83.30
Test, all lengths
Berkeley 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
Berkeley-Tags 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
Our work 78.75 83.39 79.70 78.43 87.18 88.25 80.18 90.66 82.00 83.17
Table 4: Results for the nine treebanks in the SPMRL 2013 Shared Task; all values are F-scores for
sentences of all lengths using the version of evalb distributed with the shared task. Berkeley-Rep is
the best single parser from (Bj?orkelund et al, 2013); we only compare to this parser on the development
set because neither the system nor test set values are publicly available. Berkeley-Tags is a version of
the Berkeley parser run by the task organizers where tags are provided to the model, and is the best
single parser submitted to the official task. In both cases, we match or outperform the baseline parsers in
aggregate and on the majority of individual languages.
parser
4
for the majority of cases we consider.
We evaluate on the constituency treebanks from
the Statistical Parsing of Morphologically Rich
Languages Shared Task (Seddah et al, 2013).
We compare to the Berkeley parser (Petrov and
Klein, 2007) as well as two variants. First,
we use the ?Replaced? system of Bj?orkelund et
al. (2013) (Berkeley-Rep), which is their best
single parser.
5
The ?Replaced? system modi-
fies the Berkeley parser by replacing rare words
with morphological descriptors of those words
computed using language-specific modules, which
have been hand-crafted for individual languages
or are trained with additional annotation layers
in the treebanks that we do not exploit. Unfor-
tunately, Bj?orkelund et al (2013) only report re-
sults on the development set for the Berkeley-Rep
model; however, the task organizers also use a ver-
sion of the Berkeley parser provided with parts
of speech from high-quality POS taggers for each
language (Berkeley-Tags). These part-of-speech
taggers often incorporate substantial knowledge
of each language?s morphology. Both Berkeley-
Rep and Berkeley-Tags make up for some short-
comings of the Berkeley parser?s unknown word
model, which is tuned to English.
In Table 4, we see that our performance is over-
all substantially higher than that of the Berkeley
parser. On the development set, we outperform the
Berkeley parser and match the performance of the
Berkeley-Rep parser. On the test set, we outper-
4
I.e. it does not use a reranking step or post-hoc combina-
tion of parser results.
5
Their best parser, and the best overall parser from the
shared task, is a reranked product of ?Replaced? Berkeley
parsers.
form both the Berkeley parser and the Berkeley-
Tags parser on seven of nine languages, losing
only on Arabic and French.
These results suggest that the Berkeley parser
may be heavily fit to English, particularly in its
lexicon. However, even when language-specific
unknown word handling is added to the parser, our
model still outperforms the Berkeley parser over-
all, showing that our model generalizes even bet-
ter across languages than a parser for which this
is touted as a strength (Petrov and Klein, 2007).
Our span features appear to work well on both
head-initial and head-final languages (see Basque
and Korean in the table), and the fact that our
parser performs well on such morphologically-
rich languages as Hungarian indicates that our suf-
fix model is sufficient to capture most of the mor-
phological effects relevant to parsing. Of course,
a language that was heavily prefixing would likely
require this feature to be modified. Likewise, our
parser does not perform as well on Arabic and He-
brew. These closely related languages use tem-
platic morphology, for which suffixing is not ap-
propriate; however, using additional surface fea-
tures based on the output of a morphological ana-
lyzer did not lead to increased performance.
Finally, our high performance on languages
such as Polish and Swedish, whose training tree-
banks consist of 6578 and 5000 sentences, respec-
tively, show that our feature-rich model performs
robustly even on treebanks much smaller than the
Penn Treebank.
6
6
The especially strong performance on Polish relative to
other systems is partially a result of our model being able to
produce unary chains of length two, which occur frequently
in the Polish treebank (Bj?orkelund et al, 2013).
234
While ? Gangs ? is never lethargic    , it is hindered by its plot .
4
1
2
2 ? (4 While...) 1
Figure 5: An example of a sentence from the Stan-
ford Sentiment Treebank which shows the utility
of our span features for this task. The presence
of ?While? under this kind of rule tells us that the
sentiment of the constituent to the right dominates
the sentiment to the left.
7 Sentiment Analysis
Finally, because the system is, at its core, a classi-
fier of spans, it can be used equally well for tasks
that do not normally use parsing algorithms. One
example is sentiment analysis. While approaches
to sentiment analysis often simply classify the sen-
tence monolithically, treating it as a bag of n-
grams (Pang et al, 2002; Pang and Lee, 2005;
Wang and Manning, 2012), the recent dataset of
Socher et al (2013) imposes a layer of structure
on the problem that we can exploit. They annotate
every constituent in a number of training trees with
an integer sentiment value from 1 (very negative)
to 5 (very positive), opening the door for models
such as ours to learn how syntax can structurally
affect sentiment.
7
Figure 5 shows an example that requires some
analysis of sentence structure to correctly under-
stand. The first constituent conveys positive senti-
ment with never lethargic and the second conveys
negative sentiment with hindered, but to determine
the overall sentiment of the sentence, we need to
exploit the fact that while signals a discounting of
the information that follows it. The grammar rule
2 ? 4 1 already encodes the notion of the senti-
ment of the right child being dominant, so when
this is conjoined with our span feature on the first
word (While), we end up with a feature that cap-
tures this effect. Our features can also lexicalize
on other discourse connectives such as but or how-
ever, which often occur at the split point between
two spans.
7
Note that the tree structure is assumed to be given; the
problem is one of labeling a fixed parse backbone.
7.1 Adapting to Sentiment
Our parser is almost entirely unchanged from the
parser that we used for syntactic analysis. Though
the treebank grammar is substantially different,
with the nonterminals consisting of five integers
with very different semantics from syntactic non-
terminals, we still find that parent annotation is ef-
fective and otherwise additional annotation layers
are not useful.
One structural difference between sentiment
analysis and syntactic parsing lies in where the rel-
evant information is present in a span. Syntax is
often driven by heads of constituents, which tend
to be located at the beginning or the end, whereas
sentiment is more likely to depend on modifiers
such as adjectives, which are typically present
in the middle of spans. Therefore, we augment
our existing model with standard sentiment anal-
ysis features that look at unigrams and bigrams
in the span (Wang and Manning, 2012). More-
over, the Stanford Sentiment Treebank is unique
in that each constituent was annotated in isolation,
meaning that context never affects sentiment and
that every word always has the same tag. We ex-
ploit this by adding an additional feature template
similar to our span shape feature from Section 4.4
which uses the (deterministic) tag for each word
as its descriptor.
7.2 Results
We evaluated our model on the fine-grained sen-
timent analysis task presented in Socher et al
(2013) and compare to their released system. The
task is to predict the root sentiment label of each
parse tree; however, because the data is annotated
with sentiment at each span of each parse tree, we
can also evaluate how well our model does at these
intermediate computations. Following their exper-
imental conditions, we filter the test set so that it
only contains trees with non-neutral sentiment la-
bels at the root.
Table 5 shows that our model outperforms the
model of Socher et al (2013)?both the published
numbers and latest released version?on the task
of root classification, even though the system was
not explicitly designed for this task. Their model
has high capacity to model complex interactions
of words through a combinatory tensor, but it ap-
pears that our simpler, feature-driven model is just
as effective at capturing the key effects of compo-
sitionality for sentiment analysis.
235
Root All Spans
Non-neutral Dev (872 trees)
Stanford CoreNLP current 50.7 80.8
This work 53.1 80.5
Non-neutral Test (1821 trees)
Stanford CoreNLP current 49.1 80.2
Stanford EMNLP 2013 45.7 80.7
This work 49.6 80.4
Table 5: Fine-grained sentiment analysis results
on the Stanford Sentiment Treebank of Socher et
al. (2013). We compare against the printed num-
bers in Socher et al (2013) as well as the per-
formance of the corresponding release, namely
the sentiment component in the latest version of
the Stanford CoreNLP at the time of this writ-
ing. Our model handily outperforms the results
from Socher et al (2013) at root classification and
edges out the performance of the latest version of
the Stanford system. On all spans of the tree, our
model has comparable accuracy to the others.
8 Conclusion
To date, the most successful constituency parsers
have largely been generative, and operate by refin-
ing the grammar either manually or automatically
so that relevant information is available locally to
each parsing decision. Our main contribution is
to show that there is an alternative to such anno-
tation schemes: namely, conditioning on the input
and firing features based on anchored spans. We
build up a small set of feature templates as part of a
discriminative constituency parser and outperform
the Berkeley parser on a wide range of languages.
Moreover, we show that our parser is adaptable to
other tree-structured tasks such as sentiment anal-
ysis; we outperform the recent system of Socher et
al. (2013) and obtain state of the art performance
on their dataset.
Our system is available as open-source at
https://www.github.com/dlwh/epic.
Acknowledgments
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014, by a
Google PhD fellowship to the first author, and
an NSF fellowship to the second. We further
gratefully acknowledge a hardware donation by
NVIDIA Corporation.
References
Anders Bj?orkelund, Ozlem Cetinoglu, Rich?ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking Meets Morphosyntax: State-of-the-art
Results from the SPMRL 2013 Shared Task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages.
Rens Bod. 1993. Using an Annotated Corpus As a
Stochastic Grammar. In Proceedings of the Sixth
Conference on European Chapter of the Association
for Computational Linguistics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
Eugene Charniak. 1997. Statistical Techniques for
Natural Language Parsing. AI Magazine, 18:33?44.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Computa-
tional Linguistics, 31(1):25?70, March.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In ACL, pages 16?23.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. COLT.
Jason Eisner. 1996. Three New Probabilistic Mod-
els for Dependency Parsing: An Exploration. In
Proceedings of the 16th International Conference on
Computational Linguistics (COLING-96).
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In ACL 2008, pages
959?967.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of Empirical Methods in
Natural Language Processing.
David Hall and Dan Klein. 2012. Training factored
PCFGs with expectation propagation. In EMNLP.
James Henderson. 2003. Inducing History Represen-
tations for Broad Coverage Statistical Parsing. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June. Association for Computational Linguistics.
236
Mark Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613?632, December.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL, pages 423?430.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82, Morristown, NJ, USA.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Ex-
ploiting Class Relationships for Sentiment Catego-
rization with Respect to Rating Scales. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs Up?: Sentiment Classification Us-
ing Machine Learning Techniques. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT.
Slav Petrov and Dan Klein. 2008a. Discriminative
log-linear grammars with latent variables. In NIPS,
pages 1153?1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
867?876, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D. Choi, Rich?ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi?orkowski, Ryan Roth, Wolf-
gang Seeker, Yannick Versley, Veronika Vincze,
Marcin Woli?nski, and Alina Wr?oblewska. 2013.
Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morpho-
logically Rich Languages. In Proceedings of
the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages.
Khalil Sima?an. 2000. Tree-gram Parsing Lexical De-
pendencies and Structural Relations. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In Proceedings of Empirical Methods in
Natural Language Processing.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
Margin Parsing. In In Proceedings of Empirical
Methods in Natural Language Processing.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich Part-
of-speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology - Volume 1.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Sida Wang and Christopher Manning. 2012. Baselines
and Bigrams: Simple, Good Sentiment and Topic
Classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers).
237
