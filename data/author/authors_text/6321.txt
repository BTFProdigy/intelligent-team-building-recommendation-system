Proceedings of the Second Workshop on Statistical Machine Translation, pages 197?202,
Prague, June 2007. c?2007 Association for Computational Linguistics
The ISL Phrase-Based MT System for the 2007 ACL Workshop on
Statistical Machine Translation
M. Paulik1,2, K. Rottmann2, J. Niehues2, S. Hildebrand 1,2 and S. Vogel1
1Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA
2Institut fu?r Theoretische Informatik, Universita?t Karlsruhe (TH), Karlsruhe, Germany
{paulik|silja|vogel}@cs.cmu.edu ; {jniehues|rottmann}@ira.uka.de
Abstract
In this paper we describe the Interactive Sys-
tems Laboratories (ISL) phrase-based ma-
chine translation system used in the shared
task ?Machine Translation for European
Languages? of the ACL 2007 Workshop on
Statistical Machine Translation. We present
results for a system combination of the
ISL syntax-augmented MT system and the
ISL phrase-based system by combining and
rescoring the n-best lists of the two systems.
We also investigate the combination of two
of our phrase-based systems translating from
different source languages, namely Spanish
and German, into their common target lan-
guage, English.
1 Introduction
The shared task of the ACL 2007 Workshop on Sta-
tistical Machine Translation focuses on the auto-
matic translation of European language pairs. The
workshop provides common training sets for trans-
lation model training and language model training
to allow for easy comparison of results between the
participants.
Interactive Systems Laboratories participated in the
English ? Spanish Europarl and News Commen-
tary task as well as in the English ? German Eu-
roparl task. This paper describes the phrase-based
machine translation (MT) system that was applied
to these tasks. We also investigate the feasibility
of combining the ISL syntax-augmented MT system
(Zollmann et al, 2007) with our phrase-based sys-
tem by combining and rescoring the n-best lists pro-
duced by both systems for the Spanish ? English
Europarl task. Furthermore, we apply the same com-
bination technique to combine two of our phrase-
based systems that operate on different source lan-
guages (Spanish and German), but share the same
target language (English).
The paper is organized as follows. In section 2 we
give a general description of our phrase-based sta-
tistical machine translation system. Section 3 gives
an overview of the data and of the final systems
used for the English ? Spanish Europarl and News
Commentary tasks, along with corresponding per-
formance numbers. Section 4 shows the data, final
systems and results for the English ? German Eu-
roparl task. In Section 5, we present our experiments
involving a combination of the syntax-augmented
MT system with the phrase-based MT system and a
combination of the Spanish ? English and German
? English phrase-based systems.
2 The ISL Phrase-Based MT System
2.1 Word and Phrase Alignment
Phrase-to-phrase translation pairs are extracted by
training IBM Model-4 word alignments in both di-
rections, using the GIZA++ toolkit (Och and Ney,
2000), and then extracting phrase pair candidates
which are consistent with these alignments, start-
ing from the intersection of both alignments. This
is done with the help of phrase model training
code provided by University of Edinburgh during
the NAACL 2006 Workshop on Statistical Machine
Translation (Koehn and Monz, 2006). The raw rel-
197
ative frequency estimates found in the phrase trans-
lation tables are then smoothed by applying modi-
fied Kneser-Ney discounting as explained in (Foster
et al, 2006). The resulting phrase translation tables
are pruned by using the combined translation model
score as determined by Minimum Error Rate (MER)
optimization on the development set.
2.2 Word Reordering
We apply a part-of-speech (POS) based reordering
scheme (J. M. Crego et al, 2006) to the POS-tagged
source sentences before decoding. For this, we use
the GIZA++ alignments and the POS-tagged source
side of the training corpus to learn reordering rules
that achieve a (locally) monotone alignment. Fig-
ure 1 shows an example in which three reordering
rules are extracted from the POS tags of an En-
glish source sentence and its corresponding Span-
ish GIZA++ alignment. Before translation, we con-
struct lattices for every source sentence. The lattices
include the original source sentence along with all
the reorderings that are consistent with the learned
rules. All incoming edges of the lattice are anno-
tated with distortion model scores. Figure 2 gives an
example of such a lattice. In the subsequent lattice
decoding step, we apply either monotone decoding
or decoding with a reduced local reordering window,
typically of size 2.
2.3 Decoder and MER Training
The ISL beam search decoder (Vogel, 2003) com-
bines all the different model scores to find the best
translation. Here, the following models were used:
? The translation model, i.e. the phrase-to-
phrase translations extracted from the bilingual
corpus, annoted with four translation model
scores. These four scores are the smoothed for-
ward and backward phrase translation proba-
bilities and the forward and backward lexical
weights.
? A 4-gram language model. The SRI language
model toolkit was used to train the language
model and we applied modified Kneser-Ney
smoothing.
? An internal word reordering model in addition
to the already described POS-based reordering.
  
We all agree on thatPRP DT VB IN DTEn {4} esto {5} estamos {1} todos {2} de {} acuerdo {3}
? PRP DT VB IN DT :   4 ? 5 ? 1 ? 2 ? 3? PRP DT VB:   2 ? 3 ? 1 ? PRP DT VB IN:   3 ? 4 ? 1 ? 2
Figure 1: Rule extraction for the POS-based reorder-
ing scheme.
This internal reordering model assigns higher
costs to longer distance reordering.
? Simple word and phrase count models. The
former is essentially used to compensate for
the tendency of the language model to prefer
shorter translations, while the latter can give
preference to longer phrases, potentially im-
proving fluency.
The ISL SMT decoder is capable of loading
several language models (LMs) at the same time,
namely n-gram SRI language models with n up to
4 and suffix array language models (Zhang and Vo-
gel, 2006) of arbitrary length. While we typically
see gains in performance for using suffix array LMs
with longer histories, we restricted ourselves here to
one 4-gram SRI LM only, due to a limited amount
of available LM training data. The decoding process
itself is organized in two stages. First, all available
word and phrase translations are found and inserted
into a so-called translation lattice. Then the best
combination of these partial translations is found
by doing a best path search through the translation
lattice, where we also allow for word reorderings
within a predefined local reordering window.
To optimize the system towards a maximal BLEU
or NIST score, we use Minimum Error Rate (MER)
Training as described in (Och, 2003). For each
model weight, MER applies a multi-linear search
on the development set n-best list produced by the
system. Due to the limited numbers of translations
in the n-best list, these new model weights are sub-
optimal. To compensate for this, a new full trans-
lation is done. The resulting new n-best list is then
merged with the old n-best list and the optimization
process is repeated. Typically, the translation quality
converges after three iterations.
198
1
20 3
honourable1.0000
Members1.0000 honourable0.3299
Members0.6701 6
75 8
we1.0000
have1.0000
have0.9175
a0.08254,1.0000 a1.0000
?
?Honourable Members, we have a challenging agenda?
Figure 2: Example for a source sentence lattice from
the POS-based reordering scheme.
English Spanish
sentence pairs 1259914
unique sent. pairs 1240151
sentence length 25.3 26.3
words 31.84 M 33.16 M
vocabulary 266.9 K 346.3 K
Table 1: Corpus statistics for the English/Spanish
Europarl corpus.
3 Spanish? English Europarl and News
Commentary Task
3.1 Data and Translation Tasks
The systems for the English ? Spanish translation
tasks were trained on the sentence-aligned Europarl
corpus (Koehn, 2005). Detailed corpus statistics can
be found in Table 1. The available parallel News
Commentary training data of approximately 1 mil-
lion running words for both languages was only
used as additional language model training data, to
adapt our in-domain (Europarl) system to the out-of-
domain (News Commentary) task.
The development sets consist of 2000 Europarl
sentences (dev-EU) and 1057 News Commentary
sentences (dev-NC). The available development-
test data consists of 2 x 2000 Europarl sentences
(devtest-EU and test06-EU) and 1064 News Com-
mentary sentences (test06-NC). All development
and development-test sets have only one reference
translation per sentence.
3.2 Data Normalization
The ACL shared task is very close in form and con-
tent to the Final Text Editions (FTE) task of the TC-
STAR (TC-STAR, 2004) evaluation. For this rea-
son, we decided to apply a similar normalization
scheme to the training data as was applied in our TC-
STAR verbatim SMT system. Although trained on
?verbatimized? data that did not contain any num-
bers, but rather had all numbers and dates spelled
out, it yielded consistently better results than our
TC-STAR FTE SMT system. When translating FTE
content, the verbatim system treated all numbers as
unknown words, i.e. they were left unchanged dur-
ing translation. To compensate for this, we applied
extended postprocessing to the translations that con-
ducts the necessary conversions between Spanish
and English numbers, e.g. the conversion of deci-
mal comma in Spanish to decimal point in English.
Other key points which we adopted from this nor-
malization scheme were the tokenization of punc-
tuation marks, the true-casing of the first word of
each sentence, as well as extended cleaning of the
training data. The latter mainly consisted of the re-
moval of sections with a highly unbalanced source
to target words ratio and the removal of unusual
string combinations and document references, like
for example ?B5-0918/2000?, ?(COM(2000) 335 -
C5-0386/2000 - 2000/0143(CNS))?, etc.
Based on this normalization scheme, we trained and
optimized a baseline in-domain system on accord-
ingly normalized source and reference sentences.
For optimization, we combined the available de-
velopment sets for the Europarl task and the News
Commentary task. In order to further improve
the applied normalization scheme, we experimented
with replacing all numbers with the string ?NMBR?,
rather than spelling them out and by replacing all
document identifiers with the string ?DCMNT?,
rather than deleting them. This was first done for
the language model training data only, and then for
all data, i.e. for the bilingual training data and for
the development set source and reference sentences.
In the latter case, the respective tags were again re-
placed by the correct numbers and document identi-
fiers during postprocessing. Table 2 shows the case
sensitive BLEU scores for the three normalization
approaches on the English ? Spanish Europarl and
News Commentary development sets. These scores
were computed with the official NIST scoring script
against the original (not normalized) references.
3.3 In-domain System
As mentioned above, we combined the Europarl and
News Commentary development sets when optimiz-
ing the in-domain system. This resulted in only one
199
Task baseline LM only all data
Europarl 30.94 31.20 31.26
News Com. 31.28 31.39 31.73
Table 2: Case sensitive BLEU scores on the in-
domain and out-of-domain development sets for the
three different normalization schemes.
Task Eng ? Spa Spa ? Eng
dev-EU 31.29 31.77
dev-NC 31.81 31.12
devtest-EU 31.01 31.40
test06-EU 31.87 31.76
test06-NC 30.23 29.22
Table 3: Case sensitive BLEU scores for the final
English ? Spanish in-domain systems.
set of scaling factors, i.e. the in-domain system
applies the same scaling factors for translating in-
domain data as for translating out-of-domain data.
Our baseline system applied only monotone lattice
decoding. For our final in-domain system, we used a
local reordering window of length 2, which accounts
for the slightly higher scores when compared to the
baseline system. The BLEU scores for both trans-
lation directions on the different development and
development-test sets can be found in Table 3.
3.4 Out-of-domain System
In order to adapt our in-domain system towards the
out-of-domain News Commentary task, we consid-
ered two approaches based on language model adap-
tation. First, we interpolated the in-domain LM
with an out-of-domain LM computed on the avail-
able News Commentary training data. The inter-
polation weights were chosen such as to achieve a
minimal LM perplexity on the out-of-domain de-
velopment set. For both languages, the interpo-
lation weights were approximately 0.5. Our sec-
ond approach was to simply load the out-of-domain
LM as an additional LM into our decoder. In both
cases, we optimized the translation system on the
out-of-domain development data only. For the sec-
ond approach, MER optimization assigned three to
four times higher scaling factors to the consider-
ably smaller out-domain LM than to the original in-
domain LM. Table 4 shows the results in BLEU on
the out-of-domain development and development-
test sets for both translation directions. While load-
Eng ? Spa Spa ? Eng
Task interp 2 LMs interp 2 LMs
dev-NC 33.31 33.28 32.61 32.70
test06-NC 32.55 32.15 30.73 30.55
Table 4: Case sensitive BLEU scores for the final
English ? Spanish out-of-domain systems.
ing a second LM gives similar or slightly better re-
sults on the development set during MER optimiza-
tion, we see consistently worse results on the unseen
development-test set. This, in the context of the rela-
tively small amount of development data, can be ex-
plained by stronger overfitting during optimization.
4 English? German Europarl Task
The systems for the English ? German translation
tasks were trained on the sentence-aligned Europarl
corpus only. The complete corpus consists of ap-
proximately 32 million English and 30 million Ger-
man words.
We applied a similar normalization scheme to the
training data as for the English ? Spanish system.
The main difference was that we did not replace
numbers and that we removed all document refer-
ences. In the translation process, the document ref-
erences were treated as unknown words and there-
fore left unchanged. As above, we trained and op-
timized a first baseline system on the normalized
source and reference sentences. However, we used
only the Europarl task development set during opti-
mization. To achieve further improvements on the
German ? English task, we applied a compound
splitting technique. The compound splitting was
based on (Koehn and Knight, 2003) and was applied
on the lowercased source sentences. The words gen-
erated by the compound splitting were afterwards
true-cased. Instead of replacing a compound by
its separate parts, we added a parallel path into the
source sentence lattices used for translation. The
source sentence lattices were augmented with scores
on their edges indicating whether each edge repre-
sents a word of the original text or if it was gener-
ated during compound splitting.
Table 5 shows the case-sensitive BLEU scores for
the final German ? English systems. In contrast
to the English ? Spanish systems, we used only
monotonous decoding on the lattices containing the
200
task Eng ? Ger Ger ? Eng
dev-EU 18.58 23.85
devtest-EU 18.50 23.87
test06-EU 18.39 23.88
Table 5: Case sensitive BLEU scores for the final
English ? German in-domain systems.
syntactical reorderings.
5 System Combination via n-best List
Combination and Rescoring
5.1 N-best List Rescoring
For n-best list rescoring we used unique 500-best
lists, which may have less than 500 entries for
some sentences. In this evaluation, we used sev-
eral features computed from different information
sources such as features from the translation sys-
tem, additional language models, IBM-1 word lex-
ica and the n-best list itself. We calculated 4 fea-
tures from the IBM-1 word lexica: the word proba-
bility sum as well as the maximum word probabil-
ity in both language directions. From the n-best list
itself, we calculated three different sets of scores.
A position-dependent word agreement score as de-
scribed in (Ueffing and Ney, 2005) with a position
window instead of the Levenshtein alignment, the
n-best list n-gram probability as described in (Zens
and Ney, 2006) and a position-independent n-gram
agreement, which is a variation on the first two. To
tune the feature combination weights, we used MER
optimization.
Rescoring the n-best lists from our individual sys-
tems did not give significant improvements on the
available unseen development-test data. For this rea-
son, we did not apply n-best list rescoring to the indi-
vidual systems. However, we investigated the feasi-
bility of combining two different systems by rescor-
ing the joint n-best lists of both systems. The corre-
sponding results are described in the following sec-
tions.
5.2 Combining Syntax-Augmented MT and
Phrase-Based MT
On the Spanish ? English in-domain task, we par-
ticipated not only with the ISL phrase-based SMT
system as described in this paper, but also with
the ISL syntax-augmented system. The syntax-
task PHRA SYNT COMB
dev-EU 31.77 32.48 32.77
test06-EU 31.76 32.15 32.27
Table 6: Results for combining the syntax-
augmented system (SYNT) with the phrase-based
system (PHRA).
augmented system was trained on the same normal-
ized data as the phrase-based system. However, it
was optimized on the in-domain development set
only. More details on the syntax-augmented system
can be found in (Zollmann et al, 2007). Table 6
lists the respective BLEU scores of both systems as
well as the BLEU score achieved by combining and
rescoring the individual 500-best lists.
5.3 Combining MT Systems with Different
Source Languages
(Och and Ney, 2001) describes methods for trans-
lating text given in multiple source languages into a
single target language. The ultimate goal is to im-
prove the translation quality when translating from
one source language, for example English into mul-
tiple target languages, such as Spanish and German.
This can be done by first translating the English doc-
ument into German and then using the translation as
an additional source, when translating to Spanish.
Another scenario where a multi-source translation
becomes desirable was described in (Paulik et al,
2005). The goal was to improve the quality of au-
tomatic speech recognition (ASR) systems by em-
ploying human-provided simultaneous translations.
By using automatic speech translation systems to
translate the speech of the human interpreters back
into the source language, it is possible to bias the
source language ASR system with the additional
knowledge. Having these two frameworks in mind,
we investigated the possibility of combining our in-
domain German ? English and Spanish ? English
translation systems using n-best list rescoring. Ta-
ble 7 shows the corresponding results. Even though
the German ? English translation performance was
approximately 8 BLEU below the translation perfor-
mance of the Spanish ? English system, we were
able to improve the final translation performance by
up to 1 BLEU.
201
task Spa ? Eng Ger ? Eng Comb.
dev-EU 31.77 23.85 32.76
devtest-EU 31.40 23.87 32.41
test06-EU 31.76 23.88 32.51
Table 7: Results for combining the Spanish ? En-
glish and German ? English phrase-based systems
on the in-domain tasks.
6 Conclusion
We described the ISL phrase-based statistical ma-
chine translation systems that were used for the 2007
ACL Workshop on Statistical Machine Translation.
Using the available out-of-domain News Commen-
tary task training data for language model adapta-
tion, we were able to significantly increase the per-
formance on the out-of-domain task by 2.3 BLEU
for English ? Spanish and by 1.3 BLEU for Span-
ish ? English. We also showed the feasibility of
combining different MT systems by combining and
rescoring their resprective n-best lists. In particular,
we focused on the combination of our phrase-based
and syntax-augmented systems and the combination
of two phrase-based systems operating on different
source languages. While we saw only a minimal im-
provement of 0.1 BLEU for the phrase-based and
syntax-augmented combination, we gained up to 1
BLEU, in case of the multi-source translation.
References
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
Smoothing for Statistical Machine Translation. In
Proc. of Empirical Methods in Natural Language Pro-
cessing, Sydney, Australia.
J. M. Crego et al 2006. N-gram-based SMT System
Enhanced with Reordering Patterns. In Proc. of the
Workshop on Statistical Machine Translation, pages
162?165, New York, USA.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proc. of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 187?193, Budapest, Hungary.
P. Koehn and C. Monz. 2006. Manual and Automatic
Evaluation of Machine Translation between European
Langauges. In Proc. of the Workshop on Statisti-
cal Machine Translation, pages 102?121, New York,
USA.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In Proc. of Machine Trans-
lation Summit.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proc. of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
Hongkong, China.
F. J. Och and H. Ney. 2001. Statistical Multi-Source
Translation. In Proc. of Machine Translation Summit,
pages 253?258, Santiago de Compostela, Spain.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In Proc. of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160 ? 167, Sapporo, Japan.
M. Paulik, S. Stueker, C. Fuegen, T. Schultz, T. Schaaf,
and A. Waibel. 2005. Speech Translation Enhanced
Automatic Speech Recognition. In Proc. of the Work-
shop on Automatic Speech Recognition and Under-
standing, San Juan, Puerto Rico.
TC-STAR. 2004. Technology and Corpora for Speech to
Speech Translation. http://www.tc-star.org.
N. Ueffing and H. Ney. 2005. Word-Level Con-
fidence Estimation for Machine Translation using
Phrase-Based Translation Models. In Proc. of HLT
and EMNLP, pages 763?770, Vancouver, British
Columbia, Canada.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Proc. of Int. Conf. on Natural Lan-
guage Processing and Knowledge Engineering, Bei-
jing, China.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 72?77, New York, USA.
Y. Zhang and S. Vogel. 2006. Suffix Array and its Ap-
plications in Empirical Natural Language Processing.
In the Technical Report CMU-LTI-06-010, Pittsburgh,
USA.
A. Zollmann, A. Venugopal, M. Paulik, and S. Vogel.
2007. The Syntax Augmented MT (SAMT) system
at the Shared Task for the 2007 ACL Workshop on
Statistical Machine Translation. In Proc. of ACL 2007
Workshop on Statistical MachineTranslation, Prague,
Czech Republic.
202
Proceedings of the Second Workshop on Statistical Machine Translation, pages 216?219,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Syntax Augmented MT (SAMT) System for the Shared Task in the 2007
ACL Workshop on Statistical Machine Translation
Andreas Zollmann and Ashish Venugopal and Matthias Paulik and Stephan Vogel
School of Computer Science, Carnegie Mellon University, Pittsburgh
interACT Lab, University of Karlsruhe
{ashishv,zollmann,paulik,vogel+}@cs.cmu.edu
Abstract
We describe the CMU-UKA Syntax Augmented
Machine Translation system ?SAMT? used for the
shared task ?Machine Translation for European Lan-
guages? at the ACL 2007 Workshop on Statistical
Machine Translation. Following an overview of syn-
tax augmented machine translation, we describe pa-
rameters for components in our open-source SAMT
toolkit that were used to generate translation results
for the Spanish to English in-domain track of the
shared task and discuss relative performance against
our phrase-based submission.
1 Introduction
As Chiang (2005) and Koehn et al (2003) note,
purely lexical ?phrase-based? translation models
suffer from sparse data effects when translating con-
ceptual elements that span or skip across several
source language words. Phrase-based models also
rely on distance and lexical distortion models to rep-
resent the reordering effects across language pairs.
However, such models are typically applied over
limited source sentence ranges to prevent errors in-
troduced by these models and to maintain efficient
decoding (Och and Ney, 2004).
To address these concerns, hierarchically struc-
tured models as in Chiang (2005) define weighted
transduction rules, interpretable as components of
a probabilistic synchronous grammar (Aho and Ull-
man, 1969) that represent translation and reordering
operations. In this work, we describe results from
the open-source Syntax Augmented Machine Trans-
lation (SAMT) toolkit (Zollmann and Venugopal,
2006) applied to the Spanish-to-English in-domain
translation task of the ACL?07 workshop on statisti-
cal machine translation.
We begin by describing the probabilistic model of
translation applied by the SAMT toolkit. We then
present settings for the pipeline of SAMT tools that
we used in our shared task submission. Finally, we
compare our translation results to the CMU-UKA
phrase-based SMT system and discuss relative per-
formance.
2 Synchronous Grammars for SMT
Probabilistic synchronous context-free grammars
(PSCFGs) are defined by a source terminal set
(source vocabulary) TS , a target terminal set (target
vocabulary) TT , a shared nonterminal setN and pro-
duction rules of the form
X ? ??, ?,?, w?
where following (Chiang, 2005)
? X ? N is a nonterminal
? ? ? (N ?TS)? : sequence of source nonterminals
and terminals
? ? ? (N ? TT )? : sequence of target nonterminals
and terminals
? the count #NT(?) of nonterminal tokens in ? is
equal to the count #NT(?) of nonterminal tokens
in ?,
? ?: {1, . . . ,#NT(?)} ? {1, . . . ,#NT(?)} one-
to-one mapping from nonterminal tokens in ? to
nonterminal tokens in ?
? w ? [0,?) : nonnegative real-valued weight
Chiang (2005) uses a single nonterminal cate-
gory, Galley et al (2004) use syntactic constituents
for the PSCFG nonterminal set, and Zollmann and
Venugopal (2006) take advantage of CCG (Combi-
natorial Categorical Grammar) (Steedman, 1999) in-
spired ?slash? and ?plus? categories, focusing on tar-
get (rather than source side) categories to generate
well formed translations.
We now describe the identification and estima-
tion of PSCFG rules from parallel sentence aligned
corpora under the framework proposed by Zollmann
and Venugopal (2006).
216
2.1 Grammar Induction
Zollmann and Venugopal (2006) describe a process
to generate a PSCFG given parallel sentence pairs
?f, e?, a parse tree pi for each e, the maximum a
posteriori word alignment a over ?f, e?, and phrase
pairs Phrases(a) identified by any alignment-driven
phrase induction technique such as e.g. (Och and
Ney, 2004).
Each phrase in Phrases(a) (phrases identifiable
from a) is first annotated with a syntactic category
to produce initial rules. If the target span of the
phrase does not match a constituent in pi, heuristics
are used to assign categories that correspond to par-
tial rewriting of the tree. These heuristics first con-
sider concatenation operations, forming categories
like ?NP+VP?, and then resort to CCG style ?slash?
categories like ?NP/NN? giving preference to cate-
gories found closer to the leaves of the tree.
To illustrate this process, consider the following
French-English sentence pair and selected phrase
pairs obtained by phrase induction on an automat-
ically produced alignment a, and matching target
spans with pi.
f = il ne va pas
e = he does not go
PRP ? il, he
VB ? va, go
RB+VB ? ne va pas, not go
S ? il ne va pas, he does not go
The alignment a with the associated target side
parse tree is shown in Fig. 1 in the alignment visual-
ization style defined by Galley et al (2004).
Following the Data-Oriented Parsing inspired
rule generalization technique proposed by Chiang
(2005), one can now generalize each identified
rule (initial or already partially generalized) N ?
f1 . . . fm/e1 . . . en for which there is an initial rule
M ? fi . . . fu/ej . . . ev where 1 ? i < u ? m and
1 ? j < v ? n, to obtain a new rule
N ? f1 . . . fi?1Mkfu+1 . . . fm/e1 . . . ej?1Mkev+1 . . . en
where k is an index for the nonterminal M that in-
dicates the one-to-one correspondence between the
new M tokens on the two sides (it is not in the space
of word indices like i, j, u, v,m, n). The initial rules
listed above can be generalized to additionally ex-
tract the following rules from f, e.
S ? PRP1 ne va pas , PRP1 does not go
S ? il ne VB1 pas , he does not VB1
S ? il RB+VB1, he does RB+VB1
S ? PRP1 RB+VB2, PRP1 does RB+VB2
RB+VB ? ne VB1 pas , not VB1
Fig. 2 uses regions to identify the labeled, source
and target side span for all initial rules extracted on
our example sentence pair and parse. Under this rep-
resentation, generalization can be viewed as a pro-
cess that selects a region, and proceeds to subtract
out any sub-region to form a generalized rule.
S
q
q
q
q
q
q
q
M
M
M
M
M
M
M
NP VP
q
q
q
q
q
q
q
M
M
M
M
M
M
M
PRN AUX RB VB
he does not
q
q
q
q
q
q
q
M
M
M
M
M
M
M
go
q
q
q
q
q
q
q
il ne va pas
Figure 1: Alignment graph (word alignment and target parse
tree) for a French-English sentence pair.
il 1 ne 2 va 3 pas 4
he 1
does 2
not 3
go 4

ffi
ff
ff
9
S
RB+VB
VB
VP
NP+AUX
NP
Figure 2: Spans of initial lexical phrases w.r.t. f, e. Each phrase
is labeled with a category derived from the tree in Fig. 1.
2.2 Decoding
Given a source sentence f , the translation task under
a PSCFG grammar can be expressed analogously to
monolingual parsing with a CFG. We find the most
likely derivation D with source-side f and read off
the English translation from this derivation:
e? = tgt
(
argmax
D:src(D)=f
p(D)
)
(1)
where tgt(D) refers to the target terminals and
src(D) to the source terminals generated by deriva-
tion D.
Our distribution p over derivations is defined by a
log-linear model. The probability of a derivation D
217
is defined in terms of the rules r that are used in D:
p(D) =
pLM (tgt(D))?LM
?
r?D
?
i ?i(r)
?i
Z(?)
(2)
where ?i refers to features defined on each rule,
pLM is a language model (LM) probability applied to
the target terminal symbols generated by the deriva-
tion D, and Z(?) is a normalization constant cho-
sen such that the probabilities sum up to one. The
computational challenges of this search task (com-
pounded by the integration of the LM) are addressed
in (Chiang, 2007; Venugopal et al, 2007). The
feature weights ?i are trained in concert with the
LM weight via minimum error rate (MER) training
(Och, 2003).
We now describe the parameters for the SAMT
implementation of the model described above.
3 SAMT Components
SAMT provides tools to perform grammar induc-
tion ( ?extractrules?, ?filterrules?), from bilingual
phrase pairs and target language parse trees, as well
as translation (?FastTranslateChart?) of source sen-
tences given an induced grammar.
3.1 extractrules
extractrules is the first step of the grammar induc-
tion pipeline, where rules are identified based on the
process described in section 2.1. This tool works on
a per sentence basis, considering phrases extracted
for the training sentence pair ?si, ti? and the corre-
sponding target parse tree pii. extractrules outputs
identified rules for each input sentence pair, along
with associated statistics that play a role in the esti-
mation of the rule features ?. These statistics take
the form of real-valued feature vectors for each rule
as well as summary information collected over the
corpus, such as the frequency of each nonterminal
symbol, or unique rule source sides encountered.
For the shared task evaluation, we ran extrac-
trules with the following extraction parameter
settings to limit the scope and number of rules
extracted. These settings produce the same initial
phrase table as the CMU-UKA phrase based sys-
tem. We limit the source-side length of the phrase
pairs considered as initial rules to 8 (parameter
MaxSourceLength). Further we set the max-
imum number of source and target terminals per
rule (MaxSource/MaxTargetWordCount)
to 5 and 8 respectively with 2 of nonter-
minal pairs (i.e., substitution sites) per rule
(MaxSubstititionCount). We limit the
total number of symbols in each rule to 8
(MaxSource/TargetSymbolCount) and
require all rules to contain at least one source-side
terminal symbol (noAllowAbstractRules,
noAllowRulesWithOnlyTargetTerminals)
since this reduces decoding time considerably. Ad-
ditionally, we discard all rules that contain source
word sequences that do not exist in the development
and test sets provided for the shared task (parameter
-r).
3.2 filterrules
This tool takes as input the rules identified by ex-
tractrules, and associates each rule with a feature
vector ?, representing multiple criteria by which the
decoding process can judge the quality of each rule
and, by extension, each derivation. filterrules is also
in charge of pruning the resulting PSCFG to ensure
tractable decoding.
? contains both real and Boolean valued features
for each rule. The following probabilistic features
are generated by filterrules:
? p?(r| lhs(X)) : Probability of a rule given its left-
hand-side (?result?) nonterminal
? p?(r| src(r)) : Prob. of a rule given its source side
? p?(ul(src(r)),ul(tgt(r))|ul(src(r)) : Probability
of the unlabeled source and target side of the rule
given its unlabeled source side.
Here, the function ul removes all syntactic la-
bels from its arguments, but retains ordering nota-
tion, producing relative frequencies similar to those
used in purely hierarchical systems. As in phrase-
based translation model estimation, ? also contains
two lexical weights (Koehn et al, 2003), counters
for number of target terminals generated. ? also
boolean features that describe rule types (i.e. purely
terminal vs purely nonterminal).
For the shared task submission, we pruned away
rules that share the same source side based on
p?(r| src(r)) (the source conditioned relative fre-
quency). We prune away a rule if this value is
less that 0.5 times the one of the best performing
rule (parameters BeamFactorLexicalRules,
BeamFactorNonlexicalRules).
3.3 FastTranslateChart
The FastTranslateChart decoder is a chart parser
based on the CYK+(Chappelier and Rajman, 1998)
algorithm. Translation experiments in this paper
are performed with a 4-gram SRI language model
trained on the target side of the corpus. Fast-
TranslateChart implements both methods of han-
dling the LM intersection described in (Venugopal
et al, 2007). For this submission, we use the Cube-
Pruning (Chiang, 2007) approach (the default set-
ting). LM and rule feature parameters ? are trained
with the included MER training tool. Our prun-
ing settings allow up to 200 chart items per cell
218
with left-hand side nonterminal ? S? (the reserved
sentence spanning nonterminal), and 100 items per
cell for each other nonterminal. Beam pruning
based on an (LM-scaled) additive beam of neg-
lob probability 5 is used to prune the search fur-
ther. These pruning settings correspond to setting
?PruningMap=0-100-5-@_S-200-5?.
4 Empirical Results
We trained our system on the Spanish-English in-
domain training data provided for the workshop. Ini-
tial data processing and normalizing is described
in the workshop paper for the CMU-UKA ISL
phrase-based system. NIST-BLEU scores are re-
ported on the 2K sentence development ?dev06? and
test ?test06? corpora as per the workshop guide-
lines (case sensitive, de-tokenized). We compare
our scores against the CMU-UKA ISL phrase-based
submission, a state-of-the art phrase-based SMT
system with part-of-speech (POS) based word re-
ordering (Paulik et al, 2007).
4.1 Translation Results
The SAMT system achieves a BLEU score of
32.48% on the ?dev06? development corpus and
32.15% on the unseen ?test06? corpus. This is
slightly better than the score of the CMU-UKA
phrase-based system, which achieves 32.20% and
31.85% when trained and tuned under the same in-
domain conditions. 1
To understand why the syntax augmented ap-
proach has limited additional impact on the Spanish-
to-English task, we consider the impact of reorder-
ing within our phrase-based system. Table 1 shows
the impact of increasing reordering window length
(Koehn et al, 2003) on translation quality for the
?dev06? data.2 Increasing the reordering window
past 2 has minimal impact on translation quality,
implying that most of the reordering effects across
Spanish and English are well modeled at the local or
phrase level. The benefit of syntax-based systems to
capture long-distance reordering phenomena based
on syntactic structure seems to be of limited value
for the Spanish to English translation task.
5 Conclusions
In this work, we briefly summarized the Syntax-
augmented MT model, described how we trained
and ran our implementation of that model on
1The CMU-UKA phrase-based workshop submission was
tuned on out-of-domain data as well.
2Variant of the CMU-UKA ISL phrase-based system with-
out POS based reordering. With POS-based reordering turned
on, additional window-based reordering even for window length
1 had no improvement in NIST-BLEU.
ReOrder 1 2 3 4 POS SAMT
BLEU 31.98 32.24 32.30 32.26 32.20 32.48
Table 1: Impact of phrase based reordering model settings com-
pared to SAMT on the ?dev06? corpus measured by NIST-
BLEU
the MT?07 Spanish-to-English translation task.
We compared SAMT translation results to
a strong phrase-based system trained under
the same conditions. Our system is available
open-source under the GNU General Pub-
lic License (GPL) and can be downloaded at
www.cs.cmu.edu/?zollmann/samt
References
Alfred Aho and Jeffrey Ullman. 1969. Syntax directed
translations and the pushdown assembler. Journal of
Computer and System Sciences.
Jean-Cedric. Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proc. of Tabulation in Parsing and Deduction
(TAPD?98), Paris, France.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics. To appear.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT/NAACL, Boston, Massachusetts.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL, Edmonton,Canada.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
put. Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, Sap-
poro, Japan, July 6-7.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja
Hildebrand, and Stephan Vogel. 2007. The ISL
phrase-based MT system for the 2007 ACL work-
shop on statistical MT. In Proc. of the Association
of Computational Linguistics Workshop on Statistical
Machine Translation.
Mark Steedman. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL, College Park, Maryland.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to syn-
chronous CFG driven MT. In Proc. of HLT/NAACL,
Rochester, NY.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of the Workshop on Statistical Machine Transla-
tion, HLT/NAACL, New York, June.
219
