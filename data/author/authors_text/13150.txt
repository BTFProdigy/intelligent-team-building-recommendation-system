Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 48?57,
Beijing, August 2010
Intrinsic Property-based Taxonomic Relation Extraction from Category
Structure
DongHyun Choi and Eun-Kyung Kim and Sang-Ah Shim and Key-Sun Choi
Semantic Web Research Center
KAIST
cdh4696, kekeeo, sashim, kschoi@world.kaist.ac.kr
Abstract
We propose a novel algorithm to ex-
tract taxonomic (or isa/instanceOf ) rela-
tions from category structure by classi-
fying each category link. Previous algo-
rithms mainly focus on lexical patterns of
category names to classify whether or not
a given category link is an isa/instanceOf.
In contrast, our algorithm extracts intrin-
sic properties that represent the definition
of given category name, and uses those
properties to classify each category link.
Experimental result shows about 5 to 18 %
increase in F-Measure, compared to other
existing systems.
1 Introduction
1.1 Problem Description
Taxonomies are a crucial component of many ap-
plications, including document clustering (Hotho
et al, 2003) and database search (Byron et al,
1997). Due to their importance, many studies
have examined methods of extracting taxonomic
relations automatically - either from unstructured
text (Cimiano et al, 2005; Cimiano(2) et al,
2005), or from structured data such as Wikipedia
category structures (Ponzetto and Strube, 2007;
Nastase and Strube, 2008; Suchanek et al, 2007).
Many researchers have attempted to obtain tax-
onomic relations from unstructured text to con-
stuct a taxonomy, but in most cases such a system
shows poor precision and low recall. Approaches
to extracting taxonomic relations from structured
data show relatively high performance, but to ob-
tain a taxonomy these require huge amounts of
structured data. Recently, as large amounts of
structured data such as the infoboxes and category
structures of Wikipedia or DBpedia (Auer et al,
2007) have become available, an obstacle to this
approach has been removed.
Although a category structure does contain
some kind of hierarchical structure, in many cases
it cannot be considered as an isa/instanceOf hier-
archy. For example, the article ?Pioneer 111? on
Wikipedia is categorized under ?Radio frequency
propagation?, which is related to the ?Pioneer 11?
but is obviously not a taxonomical parent of ?Pio-
neer 11?.
In this paper, we propose a method for extract-
ing taxonomic relations from a given category
structure. More precisely, for a category link in
the given category structure, the algorithm deter-
mines whether the link could be considered an
isa/instanceOf relation, or if the link simply rep-
resents a broader term/narrower term/related term
relation. For a given category link <A, B>, in
which A is the upper category name and B is the
lower category/article name, we attempt to get the
definition of B to classify the link. More precisely,
we analyze the upper categories of B from the
given category structure, to get tokens that rep-
resents the definition of B. Once we get the to-
kens, we compare the tokens with the name of A,
to classify the given category link. We call the
tokens that represent the definition of B ?intrin-
sic tokens? of B; a more precise definition will be
presented in section 3.1.
To show the validity of this approach, the algo-
rithm is applied to Wikipedia?s category structure,
1Pioneer 11 was the probe for second mission of the Pio-
neer program (after its sister probe Pioneer 10) to investigate
Jupiter and the outer solar system.
48
to obtain taxonomic relations there. Wikipedia?s
category structure consists of categories, article
titles and links between them. A Wikipedia arti-
cle represents one document, and a category is the
grouping of those articles by non-categorization-
expert users. Each category has its own name,
which is assigned by these users.
Although Wikipedia?s category structure is
built by non-experts, it can be thought of as reli-
able since it is refined by many people, and it con-
tains 35,904,116 category links between 764,581
categories and 6,301,594 articles, making it a per-
fect target for an experimental taxonomic relation
extraction algorithm.
After describing related works in section 2, our
detailed algorithm is proposed in section 3, and its
experimental results are discussed in section 4. In
section 5, we make some conclusions and propos-
als for future work.
2 Related Works
Methods of taxonomic relation extraction can be
divided into two broad categories depending on
the input: unstructured or structured data. The ex-
traction of taxonomic relations from unstructured
text is mainly carried out using lexical patterns on
the text. The Hearst pattern (Hearst, 1992) is used
in many pattern-based approaches, such as Cimi-
ano (2005).
In addition, there has been research that at-
tempted to use existing structured data, like the
Wikipedia category structure or the contents of a
thesaurus. The system of Ponzetto (2007) deter-
mines whether or not the given Wikipedia cate-
gory link is an isa/instanceOf relation by applying
a set of rules to the category names, while Nas-
tase (2008) defined lexical patterns on category
names, in addition to Ponzetto (2007). The YAGO
system (Suchanek et al, 2007) attempts to classify
whether the given article-category link represents
an instanceOf relation by checking the plurality
of the upper category name.
The algorithm proposed in this paper focuses
on the structured data, mainly the category struc-
ture, to gather isa/instanceOf relations. The
system gets a category structure as input, and
classifies each category link inside the category
structure according to whether it represents an
isa/instanceOf relation or not.
3 Algorithm Description
In section 3.1, we introduce the necessary defini-
tions for isa/instanceOf relations and the required
terms to describe the algorithm. In section 3.2,
we will discuss the hypotheses based on the defi-
nitions described in section 3.1. Next, two binary
classification algorithms will be proposed based
on the hypotheses, which will determine whether
the given category link is an isa/instanceOf rela-
tion or not.
3.1 Definitions
To define isa and instanceOf relations, Mi-
zoguchi (2004) introduces the concept of intrin-
sic property and other related concepts, which are
shown in the following definitions 1, 2 and 3:
Definition 1: Intrinsic property. The intrinsic
property of a thing is a property which is essen-
tial to the thing and it loses its identity when the
property changes.
Definition 2: The ontological definition of a
class. A thing which is a conceptualization of a set
X can be a class if and only if each element x of X
belongs to the class X if and only if the intrinsic
property of x satisfies the intensional condition of
X. And, then and only then, <x instanceOf X>
holds.
Definition 3: isa relation. isa relation holds
only between classes. <class A isa class B>
holds iff the instance set of A is a subset of the
instance set of B.
In addition, we define the following terms for
algorithm description:
Definition 4: intrinsic token. Token 2 T is an
intrinsic token of B iff T represents the intrinsic
property of B.
For example, when B is ?Pioneer 11?, the in-
trinsic tokens of B are ?spacecraft?, ?escape3?,
?Jupiter?, etc.
2For example, token is a segmented term in category
names of Wikipedia category structure.
3Since the main purpose of Pioneer 11 is to escape from
the solar system and fly into the deep space, we thought ?es-
cape? is the intrinsic token of ?Pioneer 11?. In the same con-
text, ?spacecraft escaping the solar system? is a taxonomical
parent of ?Pioneer 11?.
49
Definition 5: category link. <A, B> is called
category link iff A is a category of B, and that fact
is explicitly stated in the given category structure.
Consider the example of Wikipedia. If B is
an article, <A, B> is called an article-category
link, and if B is a category, <A, B> is called a
category-category link. The article is a catego-
rized terminal object.
Definition 6: category structure. Category
structure is the collection of category links, its
component categories, and categorized terminal
objects.
Definition 7: upper category set. The upper
category set of B is defined as the set of upper
categories of B up to n step in the given category
structure, and it is expressed as U(B, n).
For example, if the two category links <Jupiter
spacecraft, Pioneer 11> and <Jupiter, Jupiter
spacecraft> exist inside the given category struc-
ture, then Jupiter spacecraft is the element of
U(Pioneer 11, 1), while Jupiter is not.
Figure 1 shows the category structure of
U(Pioneer 11, 3),which we refer to throughout
this paper to explain our algorithm.
3.2 Hypotheses
According to the classical Aristotelian view, cat-
egories are discrete entities characterized by a set
of properties shared by their members. Thus, we
make the following lemmas:
Lemma 1: If some objects are grouped into the
same category, then they share at least more than
one property.
According to definition 2, if x is an instanceOf
X, then the intrinsic property of x satisfies the def-
inition of X. Since the intrinsic property is the
property related to the definition of the object, we
can assume that in most categorization systems,
the intrinsic property is the most frequently shared
property among those objects categorized in the
same category.
Lemma 2. Intrinsic properties are shared most
frequently among objects in a category.
Lemma 2 means that, for example, the intrin-
sic token T of B will show up frequently among
the names of upper categories of B. But lemma
2 does NOT mean that non-intrinsic tokens will
not frequently appear among the upper category
names. For example, the elements of U(Pioneer
11, 3) from the Wikipedia category structure con-
tain the token ?spacecraft? 4 times, but it also
contain token ?technology? 3 times. Therefore,
we cannot directly use the token frequency to de-
termine which one is the intrinsic token: rather,
we make another assumption to get the ?intrinsic
score? for each token.
Lemma 3. Intrinsic tokens co-occur frequently
with other intrinsic tokens.
Lemma 3 means that, if T1 is an intrinsic to-
ken of B, and T2 co-occurs with T1 inside the
upper category names of B, then there is a high
probability that T2 is also an intrinsic token of B.
For example, for the category link<Jupiter space-
craft, Pioneer 11>, if the token ?spacecraft? is an
intrinsic token of ?Pioneer 11?, we can assume
that the token ?Jupiter? is also an intrinsic token
of ?Pioneer 11?. Since some intrinsic tokens that
are appropriate as modifiers are not appropriate as
head words ? for example, if the token ?Jupiter?
is used as a modifier, it will be a good intrinsic
token of ?Pioneer 11?, but if it is used as a head
word, choosing it as the intrinsic token of ?Pio-
neer 11? would be bad choice ? thus, we distin-
guish between intrinsic score as head word, and
intrinsic score as modifier. If the intrinsic score
of token T is high for article/category name B,
then it means the probability is high that T is an
intrinsic token of B. We assumed that only the
co-occurrences as head word and its modifier are
meaningful. Corollary 3-1. If a modifier co-
occurs with a head word, and the head word is
frequently an intrinsic token of an object, then the
modifier is an intrinsic token of the object.
Corollary 3-2. If a head word co-occurs with a
modifier, and the modifier is frequently an intrin-
sic token of an object, then the head word is an
intrinsic token of the object.
3.3 Proposed Algorithm
Based on the hypotheses proposed in section 3.2,
we propose two algorithms to get the intrinsic
score of each token in the following sections. The
first algorithm, a counting-based approach, uses
only lemmas 1 and 2, and it will be shown why
this algorithm will not work. The second algo-
rithm, a graph-based approach, uses all of the hy-
50
Figure 1: category structure of U(Pioneer 11, 3) from Wikipedia.
potheses to solve the problem.
For the given category link <A, B>, the intrin-
sic score of each token will be calculated based
on its frequency inside U(B, n) while separately
counting the token?s intrinsic score as modifiers
and the intrinsic score as head word. We here
propose a scoring mechanism based on the HITS
page ranking algorithm (Kleinberg, 1999): For the
given category link <A, B>, we first construct a
?modifier graph? using U(B, n), and then calcu-
late the intrinsic score for each token in U(B, n)
using the HITS algorithm. After that, the intrinsic
score of each token will be used to calculate the
score of <A, B>. If the score is higher than some
predefined threshold, then<A, B> is classified as
an isa/instanceOf link, and otherwise it is not.
3.3.1 Counting-based Approach
This method utilizes lemmas 1 and 2 to get the
intrinsic score for each token, and then uses the
score to determine whether the given category link
is an isa/instanceOf link or not.
To utilize this approach, we first score each to-
ken from U(B, n) by counting the frequency of
each token from the words of U(B, n). Table 1
shows the score of each token from U(Pioneer, 3)
for figure 1.
For the ?Pioneer 11? article, there are seven
category links in Wikipedia?s category struc-
ture: <1973 in space exploration, Pioneer 11>,
<Inactive extraterrestrial probes, Pioneer 11>,
<Jupiter spacecraft, Pioneer 11>, <Pioneer pro-
Token Score
space 6
exploration 5
spacecraft, probe 4
1973, technology, year, radio, solar,
system, nasa
3
vehicle, radio, program, 1970s,
extraterrestrial, transport, Saturn,
Jupiter
2
escape, inactive, frequency, propa-
gation, pioneer, ...
1
Table 1: Score for each token from U(Pioneer 11,
3)
gram, Pioneer 11>, <Radio frequency propaga-
tion, Pioneer 11>, <Saturn spacecraft, Pioneer
11>, and <Spacecraft escaping the Solar System,
Pioneer 11>, as shown in figure 1. The scores
of each link using a counting-based approach are
acquired by adding the scores for each token in ta-
ble 1 that is matched with single term occurrence
in category names. Table 2 shows the result of
counting-based approach.
Although the link <1973 in space exploration,
Pioneer 11> receives the highest score among
those seven links, obviously the link does not rep-
resent isa/instanceOf relation. This shows that
the counting approach does not guarantee accu-
racy. Table 1 shows that non-intrinsic tokens oc-
cur frequently (such as ?technology? in this exam-
51
Article-Category Links Score
<1973 in space exploration,
Pioneer 11>
3+6+5=14
<Spacecraft escaping the So-
lar System, Pioneer 11>
4+1+3+3=11
<Inactive extraterrestrial
probes, Pioneer 11>,
1+2+4=7
<Saturn spacecraft, Pioneer
11>
2+4=6
<Jupiter spacecraft, Pioneer
11>
2+4=6
<Radio frequency propaga-
tion, Pioneer 11>
2+1+1=4
<Pioneer program, Pioneer
11>
1+2=3
Table 2: Scoring each category links using count-
ing approach
ple). We call this an ?overloaded existence? error.
To solve the problems described above, we apply
Lemma 3, Corollary 3-1 and 3-2 to our calcula-
tion, and propose a second algorithm based on a
graph-based approach, which will be explained in
the next section.
3.3.2 Graph-based Approach
In this section, we propose a graph-based ap-
proach to get the intrinsic score of each token. To
do this, we first construct a modifier graph from
the words of U(B, n) for a given category link<A,
B>, with each node representing a token from the
elements of U(B, n), and each edge representing
the co-occurrence of tokens inside each element
of U(B, n). Next, we apply a well-known graph
analysis algorithm to that graph, and get the in-
trinsic scores for each node. Finally, we use the
score of each node to get the score of the given
category link.
Constructing modifier graph Modifier graph
constructed here is defined as a directed graph,
in which each node represents each token inside
U(B, n), and each edge represents a co-occurrence
as modifier-head relation inside each category
name of U(B, n). Using the subset of U(Pioneer
11, 3), we get the modifier graph of figure 2.4
Figure 2: Modifier graph of the subset of
U(Pioneer 11, 3): {Spacecraft escaping the Solar
System, Jupiter spacecraft, 1973 in space explo-
ration, NASA probes, Saturn}
Calculating Intrinsic score After constructing
the modifier graph, we apply the HITS algorithm
to the modifier graph. Since the HITS algorithm
cannot reflect the weight of edges, a modified ver-
sion of the HITS algorithm (Mihalcea and Tarau,
2005) is adopted:
Authority(Vi) =
?
Vj?In(Vi)
eji ? Hub(Vj) (1)
Hub(Vi) =
?
Vj?Out(Vi)
eij ? Authority(Vj) (2)
In(Vi) represents the set of vertices which has
the outgoing edge to Vi, Out(Vi) represents the
set of vertices which has the incoming edge from
Vi, and eij represents the weight of the edge from
Vi to Vj . The algorithm for calculating the scores
is as follows:
1. Initialize the authority and hub score of each
node to one.
2. Calculate hub score of each node using the
formula 2.
3. Calculate authority score of each node using
the formula 1.
4. Normalize authority & hub score so that the
sum of authority score of every node and the sum
of hub score of every node are one.
4We used the full set of U(B, n) to create the modifier
graph for the full scale of experimentation in section 4.
52
5. Iterate from step 2 until the score of every
node converges.
In the modifier graph, Authority score can be
mapped to the intrinsic score of a node(token) as
a head word, and Hub score can be mapped to the
intrinsic score of a node(token) as a modifier.
Scoring Category Link Now, we can score the
input category link. The score of category link
<A, B> is given as follows:
Score(< A,B >)
= Authority(h) +
?
a in mod(A)
Hub(a) (3)
Here, Score(< A,B >) represents the final
score of category link <A, B>, h represents the
head word of A, and mod(A) represents the set
of modifiers of A. Since the score of head word
and modifiers are calculated based on the upper
categories of B, this formula can integrate both
meaning of A and B to classify whether the link is
isa/instanceOf. Table 3 shows the scores of seven
article-category links from table 2, calculated us-
ing the graph-based approach.
Article-Category Links Score
<Spacecraft escaping the Solar
System, Pioneer 11>
0.5972
<1973 in space exploration, Pio-
neer 11>
0.4018
<Jupiter spacecraft, Pioneer 11> 0.2105
<Saturn spacecraft, Pioneer 11> 0.2105
<Inactive extraterrestrial probes,
Pioneer 11>,
0.0440
<Radio frequency propagation, Pi-
oneer 11>
0.0440
<Pioneer program, Pioneer 11> 0.0132
Table 3: Scoring each category links using graph-
based approach
The link <Spacecraft escaping the Solar Sys-
tem, Pioneer 11> gets the highest score, while
the link <1973 in space exploration, Pioneer
11>, which got the highest score using counting-
based approach, gets the second place. That
proves the algorithm?s effectiveness for distin-
guishing isa/instanceOf link from other non-
isa/instanceOf links. But there is still a problem -
although the first-ranked link is a isa/instanceOf
link, the second-ranked is not, while the third
and fourth-ranked links (<Jupiter spacecraft, Pi-
oneer 11>, <Saturn spacecraft, Pioneer 11> are
isa/instanceOf links. To get a better result, we
propose four additional modifications in the next
secton.
3.4 Additional Modifications to the
Graph-based Approach
To better reflect the category structure and the
property of category names to the scoring mech-
anism, the following four modifications can be
made. Each of these modification could be ap-
plied independently to the original algorithm de-
scribed in section 3.3.2.
Authority Impact Factor (I). In most cases,
a category name contains only one head word,
while it contains 2 or more modifiers. As Formula
(3) is just the linear sum of the hub scores of each
modifier and the authority score of the head word,
the resultant score is more affected by hub score,
because the number of modifiers is normally big-
ger than the number of head words. To balance
the effect of hub score and authority score, we in-
troduce authority impact factor I:
Score(< A,B >)
= I ? Authority(h) +
?
a in mod(A)
Hub(a) (4)
The authority impact factor is defined as the aver-
age number of modifiers in the elements of U(B,
n), since normally each category name contains
only one head word.
Dummy Node (D). There are some category
names that contain only one head word and no
modifier, thus making it impossible to create the
modifier graph.5 Thus, for such category names
we introduce dummy nodes to include their infor-
mation into the modifier graph. In figure 3, you
can observe the introduction of the dummy node
?dummy0?.
5For example, in figure 2, we cannot find node ?Saturn?
while U(Pioneer 11, 3) contains category name ?Saturn?
53
Figure 3: Modifier graph of the subset of
U(Pioneer 11, 3), with dummy node.
Category Distance Factor (C). We define the
category distance between category/article A and
B as the minimum number of category links re-
quired to reach B from A by following the cate-
gory links. Category distance factor C of a cat-
egory name A from U(B, n) is the reverse of the
category distance between A and B. We assumed
that, if the distance between A and B is higher,
then it is less probable for A to have the intrinsic
property of B. Based on this assumption, category
distance factor C of category name A is multiplied
by the edge score of an edge generated by cate-
gory name A.
Figure 4 shows the modifier graph of figure 2
that applies the category distance factor. Since
the category distance between ?Pioneer 11? and
?NASA probe? is two, the score of edge (NASA,
probe) is 1/2 = 0.5.
Figure 4: Modifier graph of the subset of
U(Pioneer 11, 3), with category distance factor.
Modifier Number Normalization Factor (W).
In the algorithm of building a modifier graph, the
head word of a category name with many mod-
ifiers has the advantage over the head word of a
category name with few modifiers, as if a cate-
gory name contains n modifiers it will generate
n edges incoming to its head word. To overcome
this problem, we defined the modifier number nor-
malization factor W for each category name: it is
defined as the reverse of the number of modifiers
in the category name, and it is multiplied by the
edge score of an edge, generated by the category
name, of the modifier graph. Figure 5 shows the
modifier graph of figure 2 with the modifier num-
ber normalization factor. Since the category name
?Spacecraft escaping the Solar System? has three
modifiers, the scores of edge (escape, Pioneer 11),
(solar, Pioneer 11) and (system, Pioneer 11) are
1/3 = 0.33.
Figure 5: Modifier graph of the subset of
U(Pioneer 11, 3), with modifier number normal-
ization factor.
Removing roleOf Relation (E). To distinguish
the roleOf relation from taxonomic relation,we in-
troduce a new E. This feature simply classify the
link <A, B> as non-instanceOf if category name
A has endings like -er, -ers, -or, -ors, -ian, -ians.
Since only the terminal node can represent the
name of person in category structure, we applied
this feature to classify only article-category links.
One of the example from Wikipedia which should
be judged as roleOf relation is <La Liga foot-
baller, Cristiano Ronaldo>.
After applying above four modifications, we get
the result in table 4. Now, top 3 links all represent
instanceOf links.
54
Article-Category Links Score
<Spacecraft escaping the Solar
System, Pioneer 11>
2.1416
<Jupiter spacecraft, Pioneer 11> 2.1286
<Saturn spacecraft, Pioneer 11> 2.1286
<1973 in space exploration, Pio-
neer 11>
0.0241
<Pioneer program, Pioneer 11> 0.0062
<Inactive extraterrestrial probes,
Pioneer 11>,
0.0026
<Radio frequency propagation, Pi-
oneer 11>
0.0021
Table 4: Scoring each category links using graph-
based approach with four modifications.
4 Implementation
We implemented a combinatory system that com-
bines the algorithm suggested by this paper with
existing lexical pattern-based algorithms. More
precisely, we set two parameters? and ?, in which
? has a consistently higher value than ?. If score
of the given category link, which is retrieved by
the proposed system, is higher than ?, it is classi-
fied as isa/instanceOf. If the score is higher than ?
but lower or equal to ?, the system uses an exist-
ing lexical pattern-based algorithm to classify the
link. If the score is lower than or equal to ?, it is
classified as not isa/instanceOf.
To test the system, we used Wikipedia?s
category structure, which contains 1,160,248
category-category links and 15,778,801 article-
category links between 505,277 categories and
6,808,543 articles. We extract category links from
the Wikipedia category structure and annotate
them to construct the test corpus. During the pro-
cess of choosing category links, we intentionally
removed category links with names containing
any of the following words: ?stub?, ?wikiproject?,
?wikipedia?, ?template?, ?article?, ?start-class?,
?category?, ?redirect?, ?mediawiki?, ?user?, ?por-
tal?, ?page?, and ?list?. These words are normally
used to represent Wikipedia maintenance pages.
After we remove the links described before, we
randomly choose 3,951 category-category links
and 1,688 article-category links. Two annotators
worked separately to annotate whether or not the
given link is an isa/instanceOf link, and in the
event of conflict they would discuss the case and
make a final decision.
We carried out experiments on category-
category link set and article-category link set sep-
arately, since their characteristics are different.
We assumed that the taxonomic relation in a
category-category link is an isa link, while the tax-
onomic relation in an article-category link is an in-
stanceOf link. To acquire the upper category set,
we set n=3 throughout the experiment. For head
word extraction, the method of Collins (1999) is
used, and for lemmatization we used the Lingpipe
toolkit (Alias-i, 2008).
4.1 Experiments on category-category link
We divided the 3,951 category-category links into
two equally-sized sets, and used one set as a train-
ing set and the other one as a test set. The training
set was used to identify the ? and ? values for
isa link classification: in other words, the ? and
? values that showed the best performance when
applied to training set were selected as the actual
parameters used by the system. As Wikipedia?s
category structure contains a huge number of cat-
egory links, precision is more important than re-
call. As recall cannot be ignored, we chose the
parameters that gave the highest precision on the
training set, while giving a recall of at least 0.7.
Also, we carried out experiments on three base-
line systems.The first one determined every link
as an isa link. The second one applied the head
word matching rule (M) only, which says that for
category-category link<A, B>, if the head words
of A and B are the same, then <A, B> should
be classified as an isa link. The third one applies
the method of Ponzetto (P) (Ponzetto and Strube,
2007). The ruleset of Ponzetto includes Head
word matching rule, Modifier-head word match-
ing rule(Ex. <Crime, Crime Comics>: Head
word of ?Crime? and modifier of ?Crime Comics?
matches: Not isa), and the plurality rule used by
YAGO system(Explained at the next chapter)).
Table 5 shows the baseline results, the results
of existing systems, and our best results on the
test set. Usage of authority score is represented
as A, and usage of hub score is represented as H.
Also,we did experiments on all possible combina-
55
tion of features A, H, I, D, C, W, M, P. For exam-
ple, Comb(AHICDM) means that we used feature
A, H, I, C, D to construct the modifier graph and
score the category link, and for those whose score
is between ? and ? we used head word matching
rule to classify them. At the table, P stands for
Precision, R stands for Recall, and F stands for
F-measure.
Setting P R F
Baseline1 0.7277 1.0 0.8424
Baseline2(M) 0.9480 0.6335 0.7595
Baseline3(P) 0.9232 0.6516 0.7640
Comb1(AHM) 0.9223 0.7350 0.8181
Comb2(AHP) 0.8606 0.7211 0.7847
Comb3(AHICM) 0.9325 0.7302 0.8190
Table 5: Experimental result on test set of
category-category links: Baseline vs. System best
result
As you can observe, the precision of head-word
matching (M) is high, meaning that in many cases
the head word represents the intrinsic property.
Also, its recall shows that for category-category
links, at least more than half of the categories are
categorized using the intrinsic property of the ob-
jects grouped within them, which strongly sup-
ports lemma 2 in section 3.2. The comparison of
setting M and AHM, P and AHP shows that the
intrinsic-property based approach increases recall
of the existing system about 7-10 %, at the cost of
of 2-6 % precision loss. This shows that, rather
than looking only at the given category link and
analyzing patterns on its name, by gathering in-
formation from the upper category set, we were
able to significantly increase recall. However, it
also shows that some ?garbage? information is in-
troduced through the upper category set, resulting
in a 2-6 % precision loss. The best system shows
about a 8-10 % increase in recall, with compara-
bly good precision compared to the two baseline
systems.
4.2 Experiments on article-category link
In a similar manner to the experiments on
category-category links, we divided the 1,688
article-category links into two equally-sized sets,
and used one set as a training set and the other one
as a test set. The training set is used to determine
the parameters for instanceOf link classification.
The parameter setting procedure was the same as
in the experiments on category-category links, ex-
cept that we used the article-category links for the
procedure. In this experiment, we also adapted
three baseline systems. The first system classi-
fies every link as an instanceOf link, the second
system adapts the head word matching rule (M),
and the third system applies the rule from Yago
(Y) (Suchanek et al, 2007), which states that for
article-category link <A, B>, if A is plural then
the link could be classified as an instanceOf rela-
tion.
Setting P R F
Baseline1 0.5261 1.0 0.6894
Baseline2(M) 0.7451 0.0856 0.1535
Baseline3(Y) 0.6036 0.5315 0.5653
Comb1(AHY) 0.6082 0.6718 0.6381
Comb2(ADWEY) 0.7581 0.7410 0.7494
Table 6: Experimental result on test set of article-
category links on some settings
Table 6 shows the baseline results and the best
results of the combinatory system. As you can ob-
serve from the above table, M (head word match-
ing rule) does not work well in article-category
links, although its precision is still high or compa-
rable to that of other methods. Since in most cases
an article represents one instance, in many cases
they have their own name, making the recall of
the head word matching rule extremely low. Also,
the combination system 1 (AHY) shows compa-
rable precision with Y but 14 % higher in reall,
resulting 7 % increse in F-Measure.The best sys-
tem shows about 18 % increase in F-measure, es-
pecially 15 % precision increase and 21 % recall
increase compared to YAGO system.
5 Conclusion and Future work
In this paper, we explored a intrinsic token-based
approach to the problem of classifying whether a
category link is a taxonomic relation or not. Un-
like previous works that classify category links,
we acquired the definition of a lower category
56
name by extracting intrinsic tokens and using
them to score the given category link, rather than
by applying predefined lexical rules to the cat-
egory link. Our intrinsic token-based approach
leads to a significant improvement in F-measure
compared to previous state-of-the-art systems.
One possible future direction for research is au-
tomatic instance population, by using those ex-
tracted intrinsic tokens and gathering taxonomic
relations from the category structure.
Acknowledgments
This work was supported by the Industrial
Strategic Technology Development Program
(10035348, Development of a Cognitive Planning
and Learning Model for Mobile Platforms) funded
by the Ministry of Knowledge Economy(MKE,
Korea).
References
Soumen C. Byron, Byron Dom, Rakesh Agrawal, and
Prabhakar Raghavan. 1997. Using taxonomy, dis-
criminants, and signatures for navigating in text
databases. Proceedings of the international confer-
ence on very large data bases, 446?455.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning Concept Hierarchies from Text Cor-
pora using Formal Concept Analysis. Journal of Ar-
tificial Intelligence Research, 24:305?339.
Philipp Cimiano, Aleksander Pivk, Lars Schmidt-
Thieme, and Steffen Staab. 2005. Learning Tax-
onomic Relations from Heterogeneous Sources of
Evidence. Ontology Learning from Text: Methods,
Evaluation and Applications, 59?73.
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. Proceedings of
the 14th conference on Computational linguistics,
2:539?545.
Andreas Hotho, Steffen Staab, and Gerd Stumme.
2003. Ontologies improve text document cluster-
ing. Proceedings of the IEEE International Confer-
ence on Data Mining, 541?544.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632
Simone P. Ponzetto, and Michael Strube. 2007. Deriv-
ing a Large Scale Taxonomy from Wikipedia. Pro-
ceedings of the AAAI07.
Vivi Nastase, and Michael Strube. 2008. Decoding
Wikipedia category names for knowledge acquisi-
tion . Proceedings of the AAAI08.
Riichiro Mizoguchi. 2004. Part 3: Advanced course
of ontological engineering. New Generation Com-
puting, 22(2): 193?220
Rada Mihalcea, and Paul Tarau. 2005. A Language In-
dependent Algorithm for Single and Multiple Doc-
ument Summarization. Proceedings of IJCNLP
2005.
Ian Niles, and Adam Pease. 2003. Linking Lexi-
cons and Ontologies: Mapping WordNet to the Sug-
gested Upper Merged Ontology. Proceedings of the
IEEE International Conference on Information and
Knowledge Engineering.
Soeren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBpedia: A Nucleus for a Web of
Open Data. Lecture Notes in Computer Science,
4825/2007:722?735.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A Core of Semantic
Knowledge Unifying WordNet and Wikipedia Pro-
ceedings of the 16th international conference on
World Wide Web, 697?706.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. University of
Pennsylvania PhD Thesis.
Alias-i. 2008. LingPipe 3.9.1. http://alias-
i.com/lingpipe.
57
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 78?88,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Korean Treebank Transformation for Parser Training
DongHyun Choi
Dept. of Computer Science
KAIST
Korea
cdh4696@world.kaist.ac.kr
Jungyeul Park
Les Editions
an Amzer Vak
France
park@amzer-vak.fr
Key-Sun Choi
Dept. of Computer Science
KAIST
Korea
kschoi@cs.kaist.ac.kr
Abstract
Korean is a morphologically rich language in
which grammatical functions are marked by
inflections and affixes, and they can indicate
grammatical relations such as subject, object,
predicate, etc. A Korean sentence could be
thought as a sequence of eojeols. An eo-
jeol is a word or its variant word form ag-
glutinated with grammatical affixes, and eo-
jeols are separated by white space as in En-
glish written texts. Korean treebanks (Choi
et al, 1994; Han et al, 2002; Korean Lan-
guage Institute, 2012) use eojeol as their fun-
damental unit of analysis, thus representing
an eojeol as a prepreterminal phrase inside
the constituent tree. This eojeol-based an-
notating schema introduces various complex-
ity to train the parser, for example an en-
tity represented by a sequence of nouns will
be annotated as two or more different noun
phrases, depending on the number of spaces
used. In this paper, we propose methods to
transform eojeol-based Korean treebanks into
entity-based Korean treebanks. The methods
are applied to Sejong treebank, which is the
largest constituent treebank in Korean, and the
transformed treebank is used to train and test
various probabilistic CFG parsers. The experi-
mental result shows that the proposed transfor-
mation methods reduce ambiguity in the train-
ing corpus, increasing the overall F1 score up
to about 9 %.
1 Introduction
The result of syntactic parsing is useful for many
NLP applications, such as named entity recogni-
tion (Finkel and Manning, 2009), semantic role la-
beling (Gildea and Jurafsky, 2002), or sentimental
analysis (Nasukawa and Yi, 2003). Currently most
of the state-of-the-art constituent parsers take statis-
tical parsing approach (Klein and Manning, 2003;
Bikel, 2004; Petrov and Klein, 2007), which use
manually annotated syntactic trees to train the prob-
abilistic models of each consituents.
Even though there exist manually annotated Ko-
rean treebank corpora such as Sejong Treebank (Ko-
rean Language Institute, 2012), very few research
projects about the Korean parser, especially using
phrase structure grammars have been conducted. In
this paper, we aim to transform the treebank so that it
could be better used as training data for the already-
existing English constituent parsers.
Most of Korean treebank corpora use eojeols as
their fundamental unit of analysis. An eojeol is
a word or its variant word form agglutinated with
grammatical affixes, and eojeols are separated by
white space as in English written texts (Choi et al,
2011). Figure 1 is one of the example constituent
tree from the Sejong Treebank. As can be observed,
an eojeol is always determined as a prepretermi-
nal phrase 1. But this kind of bracketing guideline
could cause ambiguities to the existing algorithms
for parsing English, because: (1) English does not
have the concept of ?eojeol?, and (2) an eojeol
can contain two or more morphemes with different
grammatical roles. For example, Korean case par-
1A node is a prepreterminal if all the children of this node
are preterminals (Part-Of-Speech tags such as NNP and JKG).
Preterminal is defined to be a node with one child which is itself
a leaf (Damljanovic et al, 2010).
78
Figure 1: An example constituent tree and morphological analysis result from the Sejong treebank
ticles (?josa?) are normally written inside the same
eojeol with their argument nouns, but the whole eo-
jeol is always considered as a prepreterminal noun
phrase in the Korean treebank, as can be seen in the
eojeol Ungaro-GA. Considering that the case parti-
cles in Korean play important role in determining
the syntactic structure of a sentence, this could cause
loss of information during the training phase. More-
over, Emanuel Ungaro is considered as two different
noun phrases, because they simply belong to the two
different eojeols (that is, a space exists between eo-
jeols Emanuel and Ungaro-GA).
In this paper, we propose methods to refine the
Sejong treebank which is currently the largest Ko-
rean treebank corpus. The methods are aimed at de-
creasing the ambiguities during the training phase
of parsers, by separating phrases which are inte-
grated into the same prepreterminal phrase due to
the reason that they happen to be in the same eojeol,
and integrating phrases into the same prepretermi-
nal phrase which are separated because they hap-
pen to be in different eojeols. The refined datasets
are trained and tested against three state-of-the-art
parsers, and the evaluation results for each dataset
are reported.
In section 2, the work about Korean parsers are
briefly introduced. Sejong treebank is described
with more detailed explanation in section 3, while
the methods to transform the treebank are introduced
in section 4. In section 5 the evaluation results of the
transformed treebank using the three existing state-
of-the-art parsers are introduced with an error report,
and we discuss conclusions in section 6.
2 Related Work
There were some trials to build Korean constituent
parsers, but due to the lack of appropriate corpus
those trials were not able to acheive a good re-
sult. (Smith and Smith, 2004) tried to build a Ko-
rean parser by bilingual approach with English, and
achieved labeled precision/recall around 40 % for
Korean. More recently, (Park, 2006) tried to extract
tree adjoining grammars from the Sejong treebank,
and (Oh et al, 2011) build a system to predict a
phrase tag for each eojeol.
Due to the partial free word order and case pari-
cles which can decide the grammatical roles of noun
phrases, there exist some works to build statistical
dependency parsers for Korean. (Chung, 2004) pre-
sented a dependency parsing model using surface
contextual model. (Choi and Palmer, 2011) con-
verted the Sejong treebank into the dependency tree-
bank, and applied the SVM algorithm to learn the
dependency model.
79
NNG General noun IC Interjection JKQ Quotational CP XSV Verb DS
NNP Proper noun MM Adnoun JX Auxiliary PR XSA Adjective DS
NNB Bound noun MAG General adverb JC Conjunctive PR XR Base morpheme
NP Pronoun MAJ Conjunctive adverbEP Prefinal EM SN Number
NR Numeral JKS Subjective CP EF Final EM SL Foreign word
VV Verb JKC Complemental CP EC Conjunctive EM SH Chinese word
VA Adjective JKG Adnomial CP ETN Nominalizing EM NF Noun-like word
VX Auxiliary predicateJKO Objective CP ETM Adnominalizing EMNV Verb-like word
VCP Copula JKB Adverbial CP XPN Noun prefix NA Unknown word
VCN Negation adjective JKV Vocative CP XSN Noun DS SF,SP,SS,SE,SO,SW
Table 1: POS tags used in Sejong treebank (CP: case particle, EM: ending marker, DS: derivational suffix, PR: particle,
SF SP SS SE SO: different types of punctuations, SW: currency symbols and mathematical symbols. Table borrowed
from (Choi and Palmer, 2011))
Apart from the Sejong Treebank, there are few
other Korean treebanks available. The KAIST tree-
bank (Choi et al, 1994) contains constituent trees
about approximately 30K sentences from newspa-
pers, novels and textbooks. Also, the Penn Ko-
rean Treebank (Han et al, 2002) contains 15K
constituent trees constructed from the sentences of
newswire and military domains. The proposed
methods are evaluated using the Sejong treebank be-
cause it is the most recent and the largest Korean
treebank among those which is currently available.
3 Sejong Treebank
The Sejong treebank is the largest constituent
treebank in Korean. It contains approximately
45K manually-annotated constituent trees, and their
sources cover various domains including newspa-
pers, novels and cartoon texts. Figure 1 shows an
example of the Sejong constituent tree.
The tree consists of phrasal nodes and their func-
tional tags as described in table 2. Each eojeol
could contain one or more morphemes with different
POS tags (Table 1 shows the POS tagset). In most
cases, eojeols are determined by white spaces. As
stated in its bracketing guidelines, the Sejong tree-
bank uses eojeols as its fundamental unit of analy-
sis 2. This means that an eojeol is always treated as
one prepreterminal phrase. This could cause confu-
sions to the training system, because an eojeol could
contain many morphemes which have very different
2The bracketing guidelines could be requested from the Se-
jong project, but available only in Korean
grammatical roles, as can be seen in the example
of Ungaro-GA - word Ungaro is a noun, where the
nominative case particle GA suggests that this eojeol
is used as a subject.
Table 2 shows phrase tags and functional tags
used to construct the Sejong treebank. Some phrases
are annotated with functional tags to clarify their
grammatical role inside the sentence. There are
three special phrase tags beside those in table 2:
X indicates phrases containing only case particles
or ending markers, L and R indicate left and right
parenthesis.
Phrase-level tags Functional tags
S Sentence SBJ Subject
Q Quotative clause OBJ Object
NP Noun phrase CMP Complement
VP Verb phrase MOD Modifier
VNP Copula phrase AJT Adjunct
AP Adverb phrase CNJ Conjunctive
DP Adnoun phrase INT Vocative
IP Interjection phrasePRN parenthetical
Table 2: Phrase tags used in Sejong treebank.
4 Transforming Methods: from
Eojeol-based to Entity-based
In this section, we describe the methods to transform
the annotation schema of the Korean treebank from
eojeol-based to entity-based using the examples of
the Sejong treebank.
4.1 Method 1: POS Level Preprocessing
Before starting the actual transforming process, the
system first detects emails, phone numbers and dates
80
based on their unique POS patterns. If the system
detects a sequence of morphemes matching with one
of predefined POS patterns inside an eojeol, then it
groups those morphemes into one entity and tags it
as a noun. This procedure aims to reduce the ambi-
guity of the corpus by reducing many miscellaneous
mrophemes which in fact forms one phone num-
ber, email address or date information into one en-
tity. Figure 2 shows an example of an eojeol whose
five morphemes toghether represent one date, and its
transformation result.
Figure 2: Example of an eojeol containing date: five mor-
phemes are merged into one morpheme representing date.
Also, the morphemes representing chinese char-
acters (POS: SH) and other foreign characters (POS:
SL) are considered as nouns, since they are normally
used to rewrite Korean nouns that have their foreign
origin such as Sino-Korean nouns.
4.2 Method 2: Detecting NPs inside an Eojeol
Although an eojeol is considered to be one prepreter-
minal phrase as a whole, many eojeols contain sep-
arated noun components inside them. For exam-
ple, a noun phrase Ungaro-GA in Figure 3 con-
sists of a separated noun component Ungaro in it,
plus josa GA. The system separates noun compo-
nents from other endings and case particles, creates
a new phrase containing those words and tags it as
an NP. By doing so, the boundaries of the NP are
more clarified - before transforming prepreterminal
NPs could contain case particles and endings, but
after the transformation it is not possible. Also the
internal syntactic structures of phrases are revealed,
providing more information to the parser.
4.3 Method 3: Finding Arguments of Josa
In this step, the system tries to find out the actual ar-
gument of each josa. For example, in figure 4 the
Figure 3: Detecting NP inside an eojeol: Case of a verb
phrase
actual argument of the nominative josa GA is the
whole person name Emanuel Ungaro, not only Un-
garo. The system tries to find out the actual argu-
ment of each josa by using a rather simple heuristic:
1. Traverse the constituent parse tree in bottom-up, right-to-
left manner.
2. If a phrase node is NP, its parent is also NP, and it directly
dominates josa(s), then:
(a) Create a new NP.
(b) Attach the node to that NP, except the josa(s).
(c) Attach all the other children of the parent node to the
newly-created NP.
(d) Remove all the children of the parent, and attach the
new NP and remaining josa part to the parent node.
3. After the procedure ends, find and remove redundant NPs,
if exist.
Figure 4: Example of applying the transformation heuris-
tic
Method 3 is dependent on method 2, since method
2 first determines boundary of NPs which do not in-
clude any case particles.
4.4 Method 4: Integrating a Sequence of
Nouns into One NP
Some of entities represented as sequences of nouns
are considered as two or more separated noun
81
phrases since their components belong to the dif-
ferent eojeols. This could be problematic because
an entity could sometimes be written without any
whitespace between its component nouns. Figure 5
shows one of the case: person name Emanuel Un-
garo is considered as two separated NPs since there
exists a whitespace between a noun Emanual and a
noun Ungaro. In this step, we aim to solve this prob-
lem.
Figure 5: Integrating sequence of nouns representing one
entity into one prepreterminal noun phrase
The system finds out an NP which has two NP
children which dominates only the noun pretermi-
nal children. If the system finds such an NP, then it
removes NP children and attaches their children di-
rectly to the found NP. Figure 5 shows an application
example of the method.
This method is dependent on method 3, since this
method assumes that an NP with its parent also NP
does not have any case particles - which cannot be
hold if method 3 is not applied.
4.5 Method 5: Dealing with Noun
Conjunctions
The system tries to enumerate the noun conjunc-
tions, rather than expressing those conjunctions in
binary format. Current Sejong treebank expresses
noun conjunctions in binary format - that is, to ex-
press the constituent tree for noun conjunctions, the
nonterminal node has one NP child on its left which
contains information about the first item of the con-
junction, and the rest of conjunctions are expressed
on the right child. Figure 63 shows an example of
the Sejong constituent tree expressing the noun con-
junctions, and its transformed version.
3Mike-WA (CNJ) Speaker-GA (NOM) Jangchak-DOI-UH
IT-DA. (?Microphone and speaker are installed.?)
Figure 6: Enumerating Noun Conjunctions
By converting noun conjunctions into rather the
?enumerated? forms, two benefits could be gained:
first, the resultant constituent tree will resemble
more to the Penn-treebank constituent trees. Since
most of the existing English parsers are trained on
the Penn Treebank, we can expect that the enumer-
ated form of conjunctions will more ?fit? to those
parsers. Second, the conjunctions are expressed in
much more explicit format, so the human users can
more easily understand the conjunctive structures in-
side the constituent trees.
4.6 Method 6: Re-tagging Phrase Tags
In this step, the system re-tags some of phrase tags
to clarify their types and to decrease training ambi-
guities. For example, a noun phrase with and with-
out case particles should be distinguished. The sys-
tem re-tags those noun phrases with case particles to
JSP 4 to distinguish them from the pure noun phrases
which consist of only nouns. Also, VP-MOD and
VNP-MOD are re-tagged to DP, since they have very
similar lexical formats with existing DPs. NP-MOD
is converted into JSP-MOD - most of them consist
of a NP with josa JKG, forming possesive cases. S-
MOD remains as S-MOD if its head is JSP-MOD:
4It stands for a ?Josa Phrase?.
82
otherwise, it is also re-tagged to a DP. Figure 75
shows a re-tagging example.
Figure 7: Example of retagging phrase tags: VP-MOD to
DP, NP-MOD to JSP-MOD, and NP-SBJ to JSP-SBJ.
5 Evaluations
In this section, several experiment results using the
standard F1 metric (2PR/(P + R)) are introduced
to show the effect of each transforming method, and
the most frequently shown error cases are explained.
5.1 Experiments using the Sejong Treebank
The proposed transformation methods are applied to
the Sejong treebank, and the converted treebanks are
used to train and test three different well-known sta-
tistical parsers, namely Stanford parser (Klein and
Manning, 2003), Bikel-Collins parser (Bikel, 2012)
and Berkeley parser (Petrov et al, 2006). To figure
out the effect of each method, all six methods are
sequentially applied one by one, and each version of
the treebank is used to train and test each parser. The
baseline treebank is the original Sejong treebank
without any transformations. For the Korean head
word extraction which will be used during parsing,
the head percolation rule of (Choi and Palmer, 2011)
is adapted. According to that paper, particles and
endings were the most useful morphemes to deter-
mine dependencies between eojeols. Based on the
observation, their rules are changed so that they give
the best priorities on those morphemes. We use
the preprocessing method described in (Park, 2006)
for training trees. It replaces symboles with Penn-
Treebank-like tags and corrects wrong morpheme
5See Figure 1 for its transcription and translation.
boundary marks within the eojeol. Methods are ap-
plied cumulatively; for example, symbol ?M 1-6?
means the version of a treebank to which method
1, 2, 3, 4, 5 and 6 are applied cumulatively.6
System Corpus P R F1
Stan.
Baseline 67.88% 61.77% 64.69%
M 1 68.34% 61.93% 64.98%
M 1-2 71.78% 67.50% 69.58%
M 1-3 71.28% 67.91% 69.56%
M 1-4 71.06% 67.08% 69.01%
M 1-5 71.35% 67.27% 69.26%
M 1-6 75.85% 72.07% 73.92%
Bikel.
Baseline 74.81% 70.39% 72.53%
M 1 74.87% 70.45% 72.59%
M 1-2 77.05% 73.84% 75.41%
M 1-3 75.87% 72.88% 74.34%
M 1-4 75.33% 72.10% 73.68%
M 1-5 75.29% 72.18% 73.70%
M 1-6 73.70% 71.05% 72.35%
Berk.
Baseline 75.25% 72.72% 73.96%
M 1 74.54% 71.97% 73.23%
M 1-2 77.27% 75.05% 76.14%
M 1-3 75.60% 73.19% 74.38%
M 1-4 75.69% 73.32% 74.49%
M 1-5 76.53% 74.30% 75.40%
M 1-6 78.60% 76.03% 77.29%
Table 3: Evaluation results of parsers, with various trans-
formed versions of the Sejong treebank.
Table 3 shows the experimental results on each
version of the treebanks using each parser. Since
the corpus covers various domains (i.e. the style of
sentences is not homogeneous.), we perform 10-fold
cross-validation for our experiments. Stan. rep-
resents Stanford parser, Bikel. represents Bikel-
Collins parser, and Berk. means Berkeley parser.
For the Berkeley parser, we set the number of itera-
tion as two for latent annotations. In this set of ex-
periments, only phrase tags are the target of training
and testing, not including functional tags.
As can be observed from the evaluation result, the
performance is improved due to methods 2 and 6
are quite big compared to the effect of other four
6As pointed out by reviewers, we are planning the reversibil-
ity of transformations to be evaluated on the same trees for
meaning comparison.
83
System Corpus P R F1
Stan.
Baseline 71.48% 69.40% 70.43%
M 1 71.89% 69.75% 70.81%
M 1-2 75.90% 73.44% 74.65%
M 1-3 72.32% 69.76% 71.02%
M 1-4 72.37% 69.97% 71.16%
M 1-5 72.80% 70.28% 71.52%
M 1-6 72.32% 69.81% 71.05%
Bikel.
Baseline 69.65% 66.80% 68.19%
M 1 69.73% 66.97% 68.32%
M 1-2 74.33% 71.90% 73.09%
M 1-3 63.94% 64.57% 64.25%
M 1-4 63.95% 65.04% 64.49%
M 1-5 64.09% 65.05% 64.57%
M 1-6 62.94% 64.16% 63.54%
Berk.
Baseline 76.82% 75.28% 76.04%
M 1 76.73% 75.06% 75.89%
M 1-2 79.59% 77.91% 78.74%
M 1-3 75.24% 72.16% 73.67%
M 1-4 75.02% 73.01% 74.00%
M 1-5 75.58% 73.61% 74.58%
M 1-6 74.37% 71.93% 73.13%
Table 4: Evaluation results of parsers, with phrase tags
and functional tags together as learning target.
methods. Especially, the performance increase due
to the method 6 strongly suggests that Sejong phrase
tagsets are not enough to distinguish the types of
phrases effectively. Except those two methods,
only the method 5 increases the overall performance
slightly, and methods 1, 3 and 4 do not have any
significant effect on the performance or even some-
times decrease the overall performance.
Although the usage of functional tags is different
from that of phrase tags, the Sejong treebank has
a very rich functional tag set. Considering the re-
sults of the previous experiments, it is highly likely
that some of phrasal information is encoded into the
functional tags. To prove that, another set of experi-
ments is carried out. In this time, parsers are trained
not only on phrase tags but also on functional tags.
Table 4 shows the evaluation results.
As can be observed, by keeping functional tags
to train and test parsers, the baseline performance
increases 3 to 6 % for the Stanford and Berkeley
parsers. Only the performance of the Bikel parser
is decreased - it is highly possible that the parser
fails to find out the appropriate head word for each
possible tag, because the number of possible tags is
increased greatly by using the functional tags along
with the phrase tags.
In both set of experiments, the method 3 decreases
the overall performance. This strongly suggests that
finding the actual argument of josa directly is quite a
challenging work. The performance drop is consid-
ered mainly because the branching problem at the
higher level of the constituent tree is counted twice
due to the josa.
5.2 Experiments using the Penn Korean
Treebank
To show the effect of the transformation methods
more clearly, the Penn Korean Treebank (Han et al,
2002) is used as another treebank for experimen-
tation: (Chung et al, 2010) describes about major
difficulties of parsing Penn Korean Treebank. The
same three parsers are trained and tested using the
treebank. Due to the different annotation guidelines
and different tagsets, transformation methods 1, 5
and 6 cannot be applied on the treebank. Thus, only
method 2, 3 and 4 are used to transform the treebank.
Table 5 shows the evaluation results.
System Corpus P R F1
Stan.
Baseline 82.84% 80.28% 81.54%
M 2 85.29% 83.25% 84.26%
M 2-3 84.52% 82.71% 83.61%
M 2-4 84.52% 82.92% 83.72%
Bikel.
Baseline 81.49% 78.20% 79.81%
M 2 75.82% 74.47% 75.13%
M 2-3 73.50% 69.66% 71.53%
M 2-4 73.45% 69.66% 71.51%
Berk.
Baseline 85.11% 81.90% 83.47%
M 2 83.40% 81.04% 82.20%
M 2-3 82.36% 80.52% 81.43%
M 2-4 82.97% 81.28% 82.12%
Table 5: Evaluation on Penn Korean Treebank.
The overall performance of training the Penn Ko-
rean treebank is higher than that of the Sejong tree-
bank. There could be two possible explanations.
First one is, since the Penn Korean treebank tries
to follow English Penn treebank guidelines as much
84
as possible, thus annotation guidelines of the Ko-
rean Penn treebank could be much ?familiar? to the
parsers than that of the Sejong treebank. The second
explanation is, since the domain of the Penn Korean
treebank is much more restricted than that of the Se-
jong treebank, the system could be trained for the
specific domain. The best performance was gained
with the Stanford parser, with the treebank trans-
formed by method 2. Actually, (Chung et al, 2010)
also investigated parsing accuracy on the Penn Ko-
rean treebank; the direct comparison could be very
difficult because parsing criteria is different.
5.3 Error Analysis
In this section, some of the parsing error cases are
reported. Berkeley parser trained with the Sejong
treebank is used for error analysis. Both phrase tags
and functional tags are used to train and test the sys-
tem.
5.3.1 Locating Approximate Positions of
Errors
As the first step to analyze the errors, we tried to
figure out at which points of the constituent tree er-
rors frequently occur ? do the errors mainly occur at
the bottom of the trees? Or at the top of the trees?
If we can figure out approximate locations of errors,
then the types of errors could be predicted.
Figure 8: Example of assigning levels to each phrasal
node.
To define the level of each nonterminal node of
the constituent tree, the following rules are used:
? The level of prepreterminal node is 0.
? The levels of other phrasal nodes are defined
as: the maximal level of their children + 1.
? Once the levels of all the phrasal nodes are cal-
culated, normalize the levels so that they have
the values between 0 and 1.
Figure 8 shows an example of constituent tree
with levels assigned to its phrasal nodes. All the
prepreterminal nodes have level value 0, and the top-
most node has level 1.
Figure 9: Performance of the system on each level of the
parse tree
Once the levels are assigned to each constituent
tree, only those constituents with levels larger than
or equal to the predefined threshold ? are used to
evaluate the system. ? are increased from 0 to 1 with
value 0.01. Higher ? value means that the system is
evaluated only for those constituents positioned at
the top level of the constituent tree.
Figure 9 shows the evaluation results. X-axis rep-
resents the value of ?, and Y-axis represents the F1-
score. As can be observed, most of the errors oc-
cur at the mid-level of the constituent trees. Also,
the effects of some methods are explicitly shown
on the graph. For example, method 2 greatly in-
creases the performance at low level of the con-
stituent tree, suggesting improved consistency in de-
temining prepreterminal NP nodes. Also, it is shown
that the proposed methods does not affect the perfor-
mance of mid-level and top-level constituent deci-
sions - this suggests that the future works should be
more focused on providing more information about
those mid-level decision to the treebank annotation.
85
Figure 10: Example of NP boundary detection error. Part
of parse tree as well as name of the enumerated products
are omitted to more clearly show the example itself.
5.3.2 Frequent Error Cases
In this section, four major parsing error cases are
described.
Detecting Boundaries of NP. Although the
method 4 tries to find and gather the sequence of
nouns which actually belong to one NP, it misses
some of the cases. Figure 10 shows such example.
Some parts of the tree are omitted using the notation
?...? to show the example more simply. Although it
is counted as the parser error, the result of the parser
is more likely to be an answer - the number of those
products is 8, not their action. The Sejong treebank
tree is annotated in that way because the number ?8?
and bound noun Gae (?unit?), representing as units,
are separated by a space. To detect such kind of sep-
arated NPs and transform them into one NP will be
our next task.
Finding an AppropriateModifee. Some phrases
modifying other phrases were failed to find their ap-
propriate modifees. Figure 11 shows an example of
such kind of error case.
Detecting an Appropriate Subject of the Sen-
tence. This case frequently occurs when a sentence
is quotated inside the other sentence. In this case,
the subject of quotated sentence is often considered
as the subject of the whole sentence, because the
quotated sentences in Korean are usually first stated
Figure 11: Example of a phrase (JSP-AJT) which failed
to find its right modifee.
and then the subject of the whole sentence shows up.
Figure 12 shows an example of the erroneously de-
tected subject.
The Wrongly-tagged Topmost Node. Some of
Sejong treebank trees have phrases which are not
tagged as S as their topmost nodes. This could cause
confusion during the training. Figure 13 shows such
example.
6 Conclusion and Future Work
Although there exist some manually-annotated
large-enough constituent treebanks such as Sejong
treebank, it was hard to apply the algorithms for En-
glish parsers to Korean treebanks, because they were
annotated in eojeol-based scheme, which concept
does not exist in English. In this paper, we showed
the possibility of acquiring good training and testing
results with the existing parsers trained using the ex-
isting Korean treebanks, if it undergoes some simple
transforming procedures. The error analysis result
shows that, indeed the proposed method improves
the performance of parser at the lower level of con-
stituent tree.
86
Figure 12: Example of a wrongly-detected subject.
Although there exists a performance gain due to
the transforming methods, there are still many gaps
for improvement. The evaluation results and er-
ror analysis results suggests the need to define the
phrase tagset of Sejong treebank in more detail.
Also, the transforming methods themselves are not
perfect yet - we believe still they could be improved
more to increase consistency of the resultant tree-
banks.
We will continuously develop our transforming
methods to improve the parsing result. Furthermore,
we are planning to investigate methods to determine
the appropriate ?detailedness? of phrase tag set, so
that there are no missing information due to too
small number of tags as well as no confusion due
to too many tags.
Acknowledgement
This research was supported by Basic Science
Research Program through the National Research
Foundation of Korea (NRF) funded by the Ministry
of Education, Science and Technology (No. 2011-
Figure 13: Example of the wrongly-tagged topmost node.
Some trees in the treebank have Non-S topmost phrase
nodes.
0026718)
References
Dan Bikel. 2004. On the Parameter Space of Generative
Lexicalized Statistical Parsing Models. Ph.D. thesis,
University of Pennsylvania.
Dan Bikel. 2012. Bikel parser. http://www.cis.
upenn.edu/?dbikel/software.html.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In The Second Workshop on Sta-
tistical Parsing of Morphologically Rich Languages,
pages 1?11.
Key-Sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST tree bank project for Korean:
Present and future development. In Proceedings of
the International Workshop on Sharable Natural Lan-
guage Resources, pages 7?14.
Key-Sun Choi, Isahara Hitoshi, and Maosong Sun. 2011.
Language resource management ? word segmentation
of written texts ? part 2: Word segmentation for Chi-
nese, Japanese and Korean. In ISO 24614-2. ISO.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT 2010 First Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages, pages 49?57, Los Angeles, CA, USA, June.
Association for Computational Linguistics.
87
Hoojung Chung. 2004. Statistical Korean Dependency
Parsing Model based on the Surface Contextual Infor-
mation. Ph.D. thesis, Korea University.
Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Identification of the question fo-
cus: Combining syntactic analysis and ontology-based
lookup through the user interaction. In Proceedings of
7th Language Resources and Evaluation Conference
(LREC), pages 361?368.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In NAACL
?09 Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 326?334.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Chung-Hye Han, Na-Rae Han, Eon-Suk Ko, Heejong Yi,
and Martha Palmer. 2002. Penn Korean treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Informa-
tion and Computation.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics, pages 423?430.
Korean Language Institute. 2012. Sejong treebank.
http://www.sejong.or.kr.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In Proceedings of the 2nd international
conference on Knowledge capture, pages 70?77.
Jin Young Oh, Yo-Sub Han, Jungyeul Park, and Jeong-
Won Cha. 2011. Predicting phrase-level tags using
entropy inspired discriminative models. In 2011 Inter-
national Conference on Information Science and Ap-
plications (ICISA), pages 1?5.
Jungyeul Park. 2006. Extraction of tree adjoining gram-
mars from a treebank for Korean. In Proceedings of
the 21st International Conference on computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics: Student Research
Workshop, pages 73?78.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
COLING-ACL 2006, pages 433?440.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of the EMNLP, pages
49?56.
88
