Proceedings of ACL-08: HLT, pages 156?164,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Searching Questions by Identifying Question Topic and Question Focus 
 
Huizhong Duan1, Yunbo Cao1,2, Chin-Yew Lin2 and Yong Yu1 
1Shanghai Jiao Tong University,  
Shanghai, China, 200240 
{summer, yyu}@apex.sjtu.edu.cn 
2Microsoft Research Asia,  
Beijing, China, 100080 
{yunbo.cao, cyl}@microsoft.com 
 
 
Abstract 
This paper is concerned with the problem of 
question search. In question search, given a 
question as query, we are to return questions 
semantically equivalent or close to the queried 
question. In this paper, we propose to conduct 
question search by identifying question topic 
and question focus. More specifically, we first 
summarize questions in a data structure con-
sisting of question topic and question focus. 
Then we model question topic and question 
focus in a language modeling framework for 
search. We also propose to use the MDL-
based tree cut model for identifying question 
topic and question focus automatically. Expe-
rimental results indicate that our approach of 
identifying question topic and question focus 
for search significantly outperforms the base-
line methods such as Vector Space Model 
(VSM) and Language Model for Information 
Retrieval (LMIR).  
1 Introduction 
Over the past few years, online services have been 
building up very large archives of questions and 
their answers, for example, traditional FAQ servic-
es and emerging community-based Q&A services 
(e.g., Yahoo! Answers1 , Live QnA2, and Baidu 
Zhidao3).   
To make use of the large archives of questions 
and their answers, it is critical to have functionality 
facilitating users to search previous answers. Typi-
cally, such functionality is achieved by first re-
trieving questions expected to have the same 
answers as a queried question and then returning 
the related answers to users. For example, given 
question Q1 in Table 1, question Q2 can be re-
                                                          
1 http://answers.yahoo.com 
2 http://qna.live.com 
3 http://zhidao.baidu.com 
turned and its answer will then be used to answer 
Q1 because the answer of Q2 is expected to par-
tially satisfy the queried question Q1. This is what 
we called question search. In question search, re-
turned questions are semantically equivalent or 
close to the queried question.  
 
Query: 
Q1: Any cool clubs in Berlin or Hamburg? 
Expected: 
Q2: What are the best/most fun clubs in Berlin? 
Not Expected: 
Q3: Any nice hotels in Berlin or Hamburg? 
Q4: How long does it take to Hamburg from Berlin? 
Q5: Cheap hotels in Berlin? 
Table 1. An Example on Question Search 
Many methods have been investigated for tack-
ling the problem of question search. For example, 
Jeon et al have compared the uses of four different 
retrieval methods, i.e. vector space model, Okapi, 
language model, and translation-based model, 
within the setting of question search (Jeon et al, 
2005b).  However, all the existing methods treat 
questions just as plain texts (without considering 
question structure). For example, obviously, Q2 
can be considered semantically closer to Q1 than 
Q3-Q5 although all questions (Q2-Q5) are related 
to Q1. The existing methods are not able to tell the 
difference between question Q2 and questions Q3, 
Q4, and Q5 in terms of their relevance to question 
Q1. We will clarify this in the following. 
In this paper, we propose to conduct question 
search by identifying question topic and question 
focus.  
The question topic usually represents the major 
context/constraint of a question (e.g., Berlin, Ham-
burg) which characterizes users? interests. In con-
trast, question focus (e.g., cool club, cheap hotel) 
presents certain aspect (or descriptive features) of 
the question topic. For the aim of retrieving seman-
tically equivalent (or close) questions, we need to 
156
assure that returned questions are related to the 
queried question with respect to both question top-
ic and question focus. For example, in Table 1, Q2 
preserves certain useful information of Q1 in the 
aspects of both question topic (Berlin) and ques-
tion focus (fun club) although it loses some useful 
information in question topic (Hamburg). In con-
trast, questions Q3-Q5 are not related to Q1 in 
question focus (although being related in question 
topic, e.g. Hamburg, Berlin), which makes them 
unsuitable as the results of question search.  
We also propose to use the MDL-based (Mini-
mum Description Length) tree cut model for auto-
matically identifying question topic and question 
focus. Given a question as query, a structure called 
question tree is constructed over the question col-
lection including the queried question and all the 
related questions, and then the MDL principle is 
applied to find a cut of the question tree specifying 
the question topic and the question focus of each 
question. 
In a summary, we summarize questions in a data 
structure consisting of question topic and question 
focus. On the basis of this, we then propose to 
model question topic and question focus in a lan-
guage modeling framework for search. To the best 
of our knowledge, none of the existing studies ad-
dressed question search by modeling both question 
topic and question focus. 
We empirically conduct the question search with 
questions about ?travel? and ?computers & internet?. 
Both kinds of questions are from Yahoo! Answers. 
Experimental results show that our approach can 
significantly improve traditional methods (e.g. 
VSM, LMIR) in retrieving relevant questions.  
The rest of the paper is organized as follow. In 
Section 2, we present our approach to question 
search which is based on identifying question topic 
and question focus. In Section 3, we empirically 
verify the effectiveness of our approach to question 
search. In Section 4, we employ a translation-based 
retrieval framework for extending our approach to 
fix the issue called ?lexical chasm?. Section 5 sur-
veys the related work. Section 6 concludes the pa-
per by summarizing our work and discussing the 
future directions.  
2 Our Approach to Question Search 
Our approach to question search consists of two 
steps: (a) summarize questions in a data structure 
consisting of question topic and question focus; (b) 
model question topic and question focus in a lan-
guage modeling framework for search. 
In the step (a), we employ the MDL-based (Min-
imum Description Length) tree cut model for au-
tomatically identifying question topic and question 
focus. Thus, this section will begin with a brief 
review of the MDL-based tree cut model and then 
follow that by an explanation of steps (a) and (b). 
2.1 The MDL-based tree cut model 
Formally, a tree cut model ? (Li and Abe, 1998) 
can be represented by a pair consisting of a tree cut 
?, and a probability parameter vector ? of the same 
length, that is, 
? ? ??, ??  (1) 
where ? and ? are 
? ? ???, ??, . . ???,  
? ? ??????, ?????, ? , ??????  
(2) 
where ??, ??, ????are classes determined by a cut 
in the tree and ? ????? ? 1
?
??? . A ?cut? in a tree is 
any set of nodes in the tree that defines a partition 
of all the nodes, viewing each node as representing 
the set of child nodes as well as itself. For example, 
the cut indicated by the dash line in Figure 1 cor-
responds to three classes:???, ????,????, ????, and 
????, ???, ???, ????. 
Figure 1. An Example on the Tree Cut Model 
A straightforward way for determining a cut of a 
tree is to collapse the nodes of less frequency into 
their parent nodes. However, the method is too 
heuristic for it relies much on manually tuned fre-
quency threshold. In our practice, we turn to use a 
theoretically well-motivated method based on the 
MDL principle. MDL is a principle of data com-
pression and statistical estimation from informa-
tion theory (Rissanen, 1978). 
Given a sample ? and a tree cut ?, we employ 
MLE to estimate the parameters of the correspond-
ing tree cut model ?? ? ??, ??? , where ??  denotes 
the estimated parameters.  
According to the MDL principle, the description 
length (Li and Abe, 1998)  ????, ?? of the tree cut 
model ??  and the sample ?? is the sum of the model 
?? 
??? ??? ??? 
??? ??? ??? ??? 
157
description length ????, the parameter description 
length ????|?? , and the data description length 
???|?, ???, i.e. 
????, ?? ? ???? ? ??????? ? ???|?, ???  (3) 
The model description length ???? is a subjec-
tive quantity which depends on the coding scheme 
employed. Here, we simply assume that each tree 
cut model is equally likely a priori. 
The parameter description length ????|?? is cal-
culated as  
??????? ? ?
?
? log |?|  (4) 
where |?|  denotes the sample size and ?  denotes 
the number of free parameters in the tree cut model, 
i.e. ? equals the number of nodes in ? minus one. 
The data description length ???|?, ???  is calcu-
lated as 
?????, ??? ? ?? ???????????   (5) 
where 
 ????? ? ?
|?|
? ????
|?|
 (6) 
where ?  is the class that ?  belongs to and ???? 
denotes the total frequency of instances in class ? 
in the sample ?. 
With the description length defined as (3), we 
wish to select a tree cut model with the minimum 
description length and output it as the result. Note 
that the model description length ???? can be ig-
nored because it is the same for all tree cut models. 
The MDL-based tree cut model was originally 
introduced for handling the problem of generaliz-
ing case frames using a thesaurus (Li and Abe, 
1998). To the best of our knowledge, no existing 
work utilizes it for question search. This may be 
partially because of the unavailability of the re-
sources (e.g., thesaurus) which can be used for 
embodying the questions in a tree structure. In Sec-
tion 2.2, we will introduce a tree structure called 
question tree for representing questions. 
2.2 Identifying question topic and question 
focus 
In principle, it is possible to identify question topic 
and question focus of a question by only parsing 
the question itself (for example, utilizing a syntac-
tic parser). However, such a method requires accu-
rate parsing results which cannot be obtained from 
the noisy data from online services. 
Instead, we propose using the MDL-based tree 
cut model which identifies question topics and 
question foci for a set of questions together. More 
specifically, the method consists of two phases: 
1) Constructing a question tree: represent the 
queried question and all the related questions 
in a tree structure called question tree; 
2) Determining a tree cut: apply the MDL prin-
ciple to the question tree, which yields the cut 
specifying question topic and question focus.  
2.2.1 Constructing a question tree 
In the following, with a series of definitions, we 
will describe how a question tree is constructed 
from a collection of questions. 
Let?s begin with explaining the representation of 
a question. A straightforward method is to 
represent a question as a bag-of-words (possibly 
ignoring stop words). However, this method cannot 
discern ?the hotels in Paris? from ?the Paris hotel?. 
Thus, we turn to use the linguistic units carrying on 
more semantic information. Specifically, we make 
use of two kinds of units: BaseNP (Base Noun 
Phrase) and WH-ngram. A BaseNP is defined as a 
simple and non-recursive noun phrase (Cao and Li, 
2002). A WH-ngram is an ngram beginning with 
WH-words. The WH-words that we consider in-
clude ?when?, ?what?, ?where?, ?which?, and ?how?.  
We refer to these two kinds of units as ?topic 
terms?. With ?topic terms?, we represent a question 
as a topic chain and a set of questions as a question 
tree.  
Definition 1 (Topic Profile) The topic profile 
?? of a topic term ? in a categorized question col-
lection is a probability distribution of categories 
????|??????  where ? is a set of categories.  
???|?? ? ???????,??
? ???????,?????
  (7) 
where ???????, ??  is the frequency of the topic 
term ?  within category ? . Clearly, we 
have?? ???|????? ? 1.  
By ?categorized questions?, we refer to the ques-
tions that are organized in a tree of taxonomy. For 
example, at Yahoo! Answers, the question ?How 
do I install my wireless router? is categorized as 
?Computers & Internet ? Computer Networking?. 
Actually, we can find categorized questions at oth-
er online services such as FAQ sites, too. 
Definition 2 (Specificity) The specificity ?????of 
a topic term ?? is the inverse of the entropy of the 
topic profile???. More specifically, 
???? ? 1 ??? ???|?? log ???|????? ? ???
  (8) 
158
where ?  is a smoothing parameter used to cope 
with the topic terms whose entropy is 0. In our ex-
periments, the value of ? was set 0.001. 
We use the term specificity to denote how spe-
cific a topic term is in characterizing information 
needs of users who post questions. A topic term of 
high specificity (e.g., Hamburg, Berlin) usually 
specifies the question topic corresponding to the 
main context of a question because it tends to oc-
cur only in a few categories. A topic term of low 
specificity is usually used to represent the question 
focus (e.g., cool club, where to see) which is rela-
tively volatile and might occur in many categories. 
Definition 3 (Topic Chain) A topic chain ?? of 
a question ? is a sequence of ordered topic terms 
?? ? ?? ? ? ? ?? such that  
1) ?? is included in 1  ,? ? ? ? ?;  
2) ????? ? ?????,  1 ? ? ? ? ? ?.  
For example, the topic chain of ?any cool clubs 
in Berlin or Hamburg?? is ?Hamburg ? Berlin ?
cool?club? because the specificities for ?Hamburg?, 
?Berlin?, and ?cool club? are 0.99, 0.62, and 0.36. 
Definition 4 (Question Tree) A question tree of 
a question set ? ? ???????
?  is a prefix tree built 
over the topic chains ?? ? ???
?????
?  of the question 
set ?. Clearly, if a question set contains only one 
question, its question tree will be exactly same as 
the topic chain of the question. 
Note that the root node of a question tree is as-
sociated with empty string as the definition of pre-
fix tree requires (Fredkin, 1960). 
 
Figure 2. An Example of a Question Tree 
 
Given the topic chains with respect to the ques-
tions in Table 1 as follow, 
? Q1: Hamburg ? Berlin ? cool?club?
? Q2: Berlin ? fun?club?
? Q3: Hamburg ? Berlin ? nice?hotel?
? Q4: Hamburg ? Berlin ? how?long?does?it?take?
? Q5: Berlin ? cheap?hotel?
we can have the question tree presented in Figure 2.  
2.2.2 Determining the tree cut 
According to the definition of a topic chain, the 
topic terms in a topic chain of a question are or-
dered by their specificity values. Thus, a cut of a 
topic chain naturally separates the topic terms of 
low specificity (representing question focus) from 
the topic terms of high specificity (representing 
question topic). Given a topic chain of a question 
consisting of ?  topic terms, there exist (? ? 1? 
possible cuts. The question is: which cut is the best?  
We propose using the MDL-based tree cut mod-
el for the search of the best cut in a topic chain. 
Instead of dealing with each topic chain individual-
ly, the proposed method handles a set of questions 
together. Specifically, given a queried question, we 
construct a question tree consisting of both the 
queried question and the related questions, and 
then apply the MDL principle to select the best cut 
of the question tree. For example, in Figure 2, we 
hope to get the cut indicated by the dashed line. 
The topic terms on the left of the dashed line 
represent the question topic and those on the right 
of the dashed line represent the question focus. 
Note that the tree cut yields a cut for each individ-
ual topic chain (each path) within the question tree 
accordingly.  
A cut of a topic chain ??? of a question q sepa-
rates the topic chain in two parts: HEAD and TAIL. 
HEAD (denoted as ?????) is the subsequence of 
the original topic chain ???  before the cut. TAIL 
(denoted as ?????) is the subsequence of ??? after 
the cut. Thus,??? ? ????? ? ?????. For instance, 
given the tree cut specified in Figure 2, for the top-
ic chain of Q1 ?Hamburg ? Berlin ? cool?club?, 
the HEAD and TAIL are ?Hamburg ? Berlin? 
and ?cool?club? respectively. 
2.3 Modeling question topic and question fo-
cus for search 
We employ the framework of language modeling 
(for information retrieval) to develop our approach 
to question search. 
In the language modeling approach to informa-
tion retrieval, the relevance of a targeted question 
?? to a queried question ? is given by the probabili-
ty ???|???  of generating the queried question ? 
Q1: Any cool clubs in Berlin or Hamburg? 
Q2: What are the most/best fun clubs in Berlin? 
Q3: Any nice hotels in Berlin or Hamburg? 
Q4: How long does it take to Hamburg from Berlin? 
Q5: Cheap hotels in Berlin? 
ROOT 
Hamburg 
Berlin 
Berlin 
cheap hotel 
fun club 
cool club
nice hotel
how long does it take
159
from the language model formed by the targeted 
question ??.  The targeted question ?? is from a col-
lection ? of questions. 
Following the framework, we propose a mixture 
model for modeling question structure (namely, 
question topic and question focus) within the 
process of searching questions: 
???|??? ? ? ? ??????|??????
????????1 ? ??? ? ??????|??????
 (9) 
In the mixture model, it is assumed that the 
process of generating question topics and the 
process of generating question foci are independent 
from each other.  
In traditional language modeling, a single multi-
nomial model ???|??? over terms is estimated for 
each targeted question ?? . In our case, two multi-
nomial models ??????????  and ??????????  need to 
be estimated for each targeted question ??. 
If unigram document language models are used, 
the equation (9) can then be re-written as, 
???|??? ? ? ? ? ??????????
???????,??
?????? ?
?1 ? ??? ? ? ??????????
???????,??
??????   
(10)
where ???????, ?? is the frequency of ? within ?. 
To avoid zero probabilities and estimate more 
accurate language models, the HEAD and TAIL of 
questions are smoothed using background collec-
tion, 
?????????? ? ? ? ???????????  
?????????????????????????1 ? ?? ? ????|??  
 
(11)
?????????? ? ? ? ???????????  
??????????????????????????1 ? ?? ? ????|??  
 
(12)
where ????|?????? , ????|?????? , and ????|??  are the 
MLE  estimators with respect to the HEAD of ??, 
the TAIL of ??, and the collection ?.  
3 Experimental Results  
We have conducted experiments to verify the ef-
fectiveness of our approach to question search. 
Particularly, we have investigated the use of identi-
fying question topic and question focus for search. 
3.1 Dataset and evaluation measures 
We made use of the questions obtained from Ya-
hoo! Answers for the evaluation. More specifically, 
we utilized the resolved questions under two of the 
top-level categories at Yahoo! Answers, namely 
?travel? and ?computers & internet?. The questions 
include 314,616 items from the ?travel? category 
and 210,785 items from the ?computers & internet? 
category. Each resolved question consists of three 
fields: ?title?, ?description?, and ?answers?. For 
search we use only the ?title? field. It is assumed 
that the titles of the questions already provide 
enough semantic information for understanding 
users? information needs. 
We developed two test sets, one for the category 
?travel? denoted as ?TRL-TST?, and the other for 
?computers & internet? denoted as ?CI-TST?. In 
order to create the test sets, we randomly selected 
200 questions for each category.  
To obtain the ground-truth of question search, 
we employed the Vector Space Model (VSM) (Sal-
ton et al, 1975) to retrieve the top 20 results and 
obtained manual judgments. The top 20 results 
don?t include the queried question itself. Given a 
returned result by VSM, an assessor is asked to 
label it with ?relevant? or ?irrelevant?. If a returned 
result is considered semantically equivalent (or 
close) to the queried question, the assessor will 
label it as ?relevant?; otherwise, the assessor will 
label it as ?irrelevant?. Two assessors were in-
volved in the manual judgments. Each of them was 
asked to label 100 questions from ?TRL-TST? and 
100 from ?CI-TST?. In the process of manually 
judging questions, the assessors were presented 
only the titles of the questions (for both the queried 
questions and the returned questions). Table 2 pro-
vides the statistics on the final test set. 
 
 # Queries # Returned # Relevant
TRL-TST 200 4,000 256 
CI-TST 200 4,000 510 
Table 2. Statistics on the Test Data 
 
We utilized two baseline methods for demon-
strating the effectiveness of our approach, the 
VSM and the LMIR (language modeling method 
for information retrieval) (Ponte and Croft, 1998).  
We made use of three measures for evaluating 
the results of question search methods. They are 
MAP, R-precision, and MRR.  
3.2 Searching questions about ?travel? 
In the experiments, we made use of the questions 
about ?travel? to test the performance of our ap-
proach to question search. More specifically, we 
used the 200 queries in the test set ?TRL-TST? to 
search for ?relevant? questions from the 314,616 
160
questions categorized as ?travel?. Note that only the 
questions occurring in the test set can be evaluated. 
We made use of the taxonomy of questions pro-
vided at Yahoo! Answers for the calculation of 
specificity of topic terms. The taxonomy is orga-
nized in a tree structure. In the following experi-
ments, we only utilized as the categories of 
questions the leaf nodes of the taxonomy tree (re-
garding ?travel?), which includes 355 categories. 
We randomly divided the test queries into five 
even subsets and conducted 5-fold cross-validation 
experiments. In each trial, we tuned the parameters 
?, ?, and ? in the equation (10)-(12) with four of 
the five subsets and then applied it to one remain-
ing subset. The experimental results reported be-
low are those averaged over the five trials. 
 
Methods MAP R-Precision  MRR 
VSM 0.198 0.138 0.228 
LMIR 0.203 0.154 0.248 
LMIR-CUT 0.236 0.192 0.279 
Table 3. Searching Questions about ?Travel? 
 
In Table 3, our approach denoted by LMIR-
CUT is implemented exactly as equation (10).  
Neither VSM nor LMIR uses the data structure 
composed of question topic and question focus.  
From Table 3, we see that our approach outper-
forms the baseline approaches VSM and LMIR in 
terms of all the measures. We conducted a signi-
ficance test (t-test) on the improvements of our 
approach over VSM and LMIR. The result indi-
cates that the improvements are statistically signif-
icant (p-value < 0.05) in terms of all the evaluation 
measures.  
 
 
Figure 3. Balancing between Question Topic and Ques-
tion Focus 
 
In equation (9), we use the parameter ? to bal-
ance the contribution of question topic and the con-
tribution of question focus. Figure 3 illustrates how 
influential the value of ? is on the performance of 
question search in terms of MRR. The result was 
obtained with the 200 queries directly, instead of 
5-fold cross-validation. From Figure 3, we see that 
our approach performs best when ? is around 0.7. 
That is, our approach tends to emphasize question 
topic more than question focus.  
We also examined the correctness of question 
topics and question foci of the 200 queried ques-
tions. The question topics and question foci were 
obtained with the MDL-based tree cut model au-
tomatically. In the result, 69 questions have incor-
rect question topics or question foci. Further 
analysis shows that the errors came from two cate-
gories: (a) 59 questions have only the HEAD parts 
(that is, none of the topic terms fall within the 
TAIL part), and (b) 10 have incorrect orders of 
topic terms because the specificities of topic terms 
were estimated inaccurately. For questions only 
having the HEAD parts, our approach (equation (9)) 
reduces to traditional language modeling approach.  
Thus, even when the errors of category (a) occur, 
our approach can still work not worse than the tra-
ditional language modeling approach. This also 
explains why our approach performs best when ? is 
around 0.7. The error category (a) pushes our mod-
el to emphasize more in question topic. 
 
Methods Results 
VSM 
1. How cold does it usually get in Charlotte, 
NC during winters? 
2. How long and cold are the winters in 
Rochester, NY? 
3. How cold is it in Alaska? 
LMIR 
1. How cold is it in Alaska? 
2. How cold does it get really in Toronto in 
the winter? 
3. How cold does the Mojave Desert get in 
the winter? 
LMIR-
CUT 
1. How cold is it in Alaska? 
2. How cold is Alaska in March and out-
door activities? 
3. How cold does it get in Nova Scotia in the 
winter? 
Table 4. Search Results for 
?How cold does it get in winters in Alaska?? 
 
Table 4 provides the TOP-3 search results which 
are given by VSM, LMIR, and LMIR-CUT (our 
approach) respectively. The questions in bold are 
labeled as ?relevant? in the evaluation set. The que-
ried question seeks for the ?weather? information 
about ?Alaska?. Both VSM and LMIR rank certain 
0.05
0.1
0.15
0.2
0.25
0.3
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
RR
?
161
?irrelevant? questions higher than ?relevant? ques-
tions. The ?irrelevant? questions are not about 
?Alaska? although they are about ?weather?. The 
reason is that neither VSM nor PVSM is aware that 
the query consists of the two aspects ?weather? 
(how cold, winter) and ?Alaska?.  In contrast, our 
approach assures that both aspects are matched. 
Note that the HEAD part of the topic chain of the 
queried question given by our approach is ?Alaska? 
and the TAIL part is ?winter ? how?cold?. 
3.3 Searching questions about ?computers & 
internet? 
In the experiments, we made use of the questions 
about ?computers & internet? to test the perfor-
mance of our proposed approach to question search. 
More specifically, we used the 200 queries in the 
test set ?CI-TST?? to search for ?relevant? questions 
from the 210,785 questions categorized as ?com-
puters & internet?. For the calculation of specificity 
of topic terms, we utilized as the categories of 
questions the leaf nodes of the taxonomy tree re-
garding ?computers & Internet?, which include 23 
categories.  
We conducted 5-fold cross-validation for the pa-
rameter tuning. The experimental results reported 
in Table 5 are averaged over the five trials. 
 
Methods MAP R-Precision  MRR 
VSM 0.236 0.175 0.289 
LMIR 0.248 0.191 0.304 
LMIR-CUT 0.279 0.230 0.341 
Table 5. Searching Questions about ?Computers & In-
ternet? 
 
Again, we see that our approach outperforms the 
baseline approaches VSM and LMIR in terms of 
all the measures. We conducted a significance test 
(t-test) on the improvements of our approach over 
VSM and LMIR. The result indicates that the im-
provements are statistically significant (p-value < 
0.05) in terms of all the evaluation measures.  
We also conducted the experiment similar to 
that in Figure 3. Figure 4 provides the result. The 
trend is consistent with that in Figure 3.  
We examined the correctness of (automatically 
identified) question topics and question foci of the 
200 queried questions, too. In the result, 65 ques-
tions have incorrect question topics or question 
foci. Among them, 47 fall in the error category (a) 
and 18 in the error category (b). The distribution of 
errors is also similar to that in Section 3.2, which 
also justifies the trend presented in Figure 4. 
 
 
Figure 4. Balancing between Question Topic and Ques-
tion Focus 
4 Using Translation Probability 
In the setting of question search, besides the topic 
what we address in the previous sections, another 
research topic is to fix lexical chasm between ques-
tions.  
Sometimes, two questions that have the same 
meaning use very different wording. For example, 
the questions ?where to stay in Hamburg?? and 
?the best hotel in Hamburg?? have almost the same 
meaning but are lexically different in question fo-
cus (where to stay vs. best hotel). This is the so-
called ?lexical chasm?. 
Jeon and Bruce (2007) proposed a mixture mod-
el for fixing the lexical chasm between questions. 
The model is a combination of the language mod-
eling approach (for information retrieval) and 
translation-based approach (for information re-
trieval). Our idea of modeling question structure 
for search can naturally extend to Jeon et al?s 
model. More specifically, by using translation 
probabilities, we can rewrite equation (11) and (12) 
as follow: 
?????????? ? ?? ? ???????????  
??? ? ? ????|??? ? ????????????????????  
??1 ? ?? ? ??? ? ????|??  
(13)
?????????? ? ?? ? ???????????  
??? ? ? ????|??? ? ????????????????????   
??1 ? ?? ? ??? ? ????|??  
 
(14)
where ????|???  denotes the probability that topic 
term ? is the translation of ??. In our experiments, 
to estimate the probability ????|???, we used the 
collections of question titles and question descrip-
tions as the parallel corpus and the IBM model 1 
(Brown et al, 1993) as the alignment model. 
0.15
0.2
0.25
0.3
0.35
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
RR
?
162
Usually, users reiterate or paraphrase their ques-
tions (already described in question titles) in ques-
tion descriptions. 
We utilized the new model elaborated by equa-
tion (13) and (14) for searching questions about 
?travel? and ?computers & internet?. The new mod-
el is denoted as ?SMT-CUT?. Table 6 provides the 
evaluation results. The evaluation was conducted 
with exactly the same setting as in Section 3. From 
Table 6, we see that the performance of our ap-
proach can be further boosted by using translation 
probability.  
 
Data Methods MAP R-Precision MRR
TRL-
TST 
LMIR-CUT 0.236 0.192 0.279
SMT-CUT 0.266 0.225 0.308
CI-
TST 
LMIR-CUT 0.279 0.230 0.341
SMT-CUT 0.282 0.236 0.337
Table 6. Using Translation Probability 
5 Related Work 
The major focus of previous research efforts on 
question search is to tackle the lexical chasm prob-
lem between questions.  
The research of question search is first con-
ducted using FAQ data. FAQ Finder (Burke et al, 
1997) heuristically combines statistical similarities 
and semantic similarities between questions to rank 
FAQs. Conventional vector space models are used 
to calculate the statistical similarity and WordNet 
(Fellbaum, 1998) is used to estimate the semantic 
similarity. Sneiders (2002) proposed template 
based FAQ retrieval systems. Lai et al (2002) pro-
posed an approach to automatically mine FAQs 
from the Web. Jijkoun and Rijke (2005) used su-
pervised learning methods to extend heuristic ex-
traction of Q/A pairs from FAQ pages, and treated 
Q/A pair retrieval as a fielded search task.  
Harabagiu et al (2005) used a Question Answer 
Database (known as QUAB) to support interactive 
question answering. They compared seven differ-
ent similarity metrics for selecting related ques-
tions from QUAB and found that the concept-
based metric performed best. 
Recently, the research of question search has 
been further extended to the community-based 
Q&A data. For example, Jeon et al (Jeon et al, 
2005a; Jeon et al, 2005b) compared four different 
retrieval methods, i.e. vector space model, Okapi, 
language model (LM), and translation-based model, 
for automatically fixing the lexical chasm between 
questions of question search. They found that the 
translation-based model performed best. 
However, all the existing methods treat ques-
tions just as plain texts (without considering ques-
tion structure). In this paper, we proposed to 
conduct question search by identifying question 
topic and question focus. To the best of our know-
ledge, none of the existing studies addressed ques-
tion search by modeling both question topic and 
question focus. 
Question answering (e.g., Pasca and Harabagiu, 
2001; Echihabi and Marcu, 2003; Voorhees, 2004; 
Metzler and Croft, 2005) relates to question search. 
Question answering automatically extracts short 
answers for a relatively limited class of question 
types from document collections. In contrast to that, 
question search retrieves answers for an unlimited 
range of questions by focusing on finding semanti-
cally similar questions in an archive. 
6 Conclusions and Future Work 
In this paper, we have proposed an approach to 
question search which models question topic and 
question focus in a language modeling framework. 
The contribution of this paper can be summa-
rized in 4-fold: (1) A data structure consisting of 
question topic and question focus was proposed for 
summarizing questions; (2) The MDL-based tree 
cut model was employed to identify question topic 
and question focus automatically; (3) A new form 
of language modeling using question topic and 
question focus was developed for question search; 
(4) Extensive experiments have been conducted to 
evaluate the proposed approach using a large col-
lection of real questions obtained from Yahoo! An-
swers.  
Though we only utilize data from community-
based question answering service in our experi-
ments, we could also use categorized questions 
from forum sites and FAQ sites. Thus, as future 
work, we will try to investigate the use of the pro-
posed approach for other kinds of web services.  
Acknowledgement 
We would like to thank Xinying Song, Shasha Li, 
and Shilin Ding for their efforts on developing the 
evaluation data. We would also like to thank Ste-
phan H. Stiller for his proof-reading of the paper. 
 
163
References  
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. In Proc. of ACL?03. 
C. Fellbaum. 1998. WordNet: An electronic lexical da-
tabase. MIT Press. 
D. Metzler and W. B. Croft. 2005. Analysis of statistical 
question classification for fact-based questions. In-
formation Retrieval, 8(3), pages 481-504. 
E. Fredkin. 1960. Trie memory. Communications of the 
ACM, D. 3(9):490-499. 
E. M. Voorhees. 2004. Overview of the TREC 2004 
question answering track. In Proc. of TREC?04. 
E. Sneiders. 2002. Automated question answering using 
question templates that cover the conceptual model 
of the database. In Proc. of the 6th International 
Conference on Applications of Natural Language to 
Information Systems, pages 235-239. 
G. Salton, A. Wong, and C. S. Yang 1975. A vector 
space model for automatic indexing. Communica-
tions of the ACM, vol. 18, nr. 11, pages 613-620.  
H.  Li and N. Abe. 1998. Generalizing case frames us-
ing a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2), pages 217-244. 
J. Jeon and W.B. Croft. 2007. Learning translation-
based language models using Q&A archives. Tech-
nical report, University of Massachusetts. 
J. Jeon, W. B. Croft, and J. Lee. 2005a. Finding seman-
tically similar questions based on their answers. In 
Proc. of SIGIR?05. 
J. Jeon, W. B. Croft, and J. Lee. 2005b. Finding similar 
questions in large question and answer archives. In 
Proc. of CIKM ?05, pages 84-90. 
J. Rissanen. 1978. Modeling by shortest data description. 
Automatica, vol. 14,  pages. 465-471 
J.M. Ponte, W.B. Croft. 1998. A language modeling 
approach to information retrieval. In Proc. of 
SIGIR?98. 
M. A. Pasca and S. M. Harabagiu. 2001. High perfor-
mance question/answering. In Proc. of SIGIR?01, 
pages 366-374. 
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. 
Mercer. 1993. The mathematics of statistical machine 
translation: parameter estimation. Computational 
Linguistics, 19(2):263-311. 
R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L. 
Lytinen, N. Tomuro, and S. Schoenberg. 1997. Ques-
tion answering from frequently asked question files: 
Experiences with the FAQ finder system. Technical 
report, University of Chicago. 
S. Harabagiu, A. Hickl, J. Lehmann and D. Moldovan. 
2005. Experiments with Interactive Question-
Answering. In Proc. of ACL?05. 
V. Jijkoun, M. D. Rijke. 2005. Retrieving Answers from 
Frequently Asked Questions Pages on the Web. In 
Proc. of CIKM?05. 
Y. Cao and H. Li. 2002. Base noun phrase translation 
using web data and the EM algorithm. In Proc. of 
COLING?02. 
Y.-S. Lai, K.-A. Fung, and C.-H. Wu. 2002. Faq mining 
via list detection. In Proc. of the Workshop on Multi-
lingual Summarization and Question Answering, 
2002. 
 
164
Proceedings of ACL-08: HLT, pages 914?922,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Probabilistic Model for Fine-Grained Expert Search   Shenghua Bao1, Huizhong Duan1, Qi Zhou1, Miao Xiong1, Yunbo Cao1,2, Yong Yu1 1Shanghai Jiao Tong University,  2Microsoft Research Asia Shanghai, China, 200240 Beijing, China, 100080 {shhbao,summer,jackson,xiongmiao,yyu} @apex.sjtu.edu.cn yunbo.cao@microsoft.com    Abstract 
Expert search, in which given a query a ranked list of experts instead of documents is returned, has been intensively studied recently due to its importance in facilitating the needs of both information access and knowledge discovery. Many approaches have been pro-posed, including metadata extraction, expert profile building, and formal model generation. However, all of them conduct expert search with a coarse-grained approach. With these, further improvements on expert search are hard to achieve. In this paper, we propose conducting expert search with a fine-grained approach. Specifically, we utilize more spe-cific evidences existing in the documents. An evidence-oriented probabilistic model for ex-pert search and a method for the implementa-tion are proposed. Experimental results show that the proposed model and the implementa-tion are highly effective. 
1 Introduction Nowadays, team work plays a more important role than ever in problem solving. For instance, within an enterprise, people handle new problems usually by leveraging the knowledge of experienced col-leagues. Similarly, within research communities, novices step into a new research area often by learning from well-established researchers in the research area. All these scenarios involve asking the questions like ?who is an expert on X?? or ?who knows about X?? Such questions, which cannot be answered easily through traditional document search, raise a new requirement of searching people with certain expertise.  To meet that requirement, a new task, called ex-pert search, has been proposed and studied inten-sively. For example, TREC 2005, 2006, and 2007 
provide the task of expert search within the enter-prise track. In the TREC setting, expert search is defined as: given a query, a ranked list of experts is returned. In this paper, we engage our study in the same setting.  Many approaches to expert search have been proposed by the participants of TREC and other researchers. These approaches include metadata extraction (Cao et al, 2005), expert profile build-ing (Craswell, 2001, Fu et al, 2007), data fusion (Maconald and Ounis, 2006), query expansion (Macdonald and Ounis, 2007), hierarchical lan-guage model (Petkova and Croft, 2006), and for-mal model generation (Balog et al, 2006; Fang et al, 2006). However, all of them conduct expert search with what we call a coarse-grained ap-proach. The discovering and use of evidence for expert locating is carried out under a grain of document. With it, further improvements on expert search are hard to achieve. This is because differ-ent blocks (or segments) of electronic documents usually present different functions and qualities and thus different impacts for expert locating.  In contrast, this paper is concerned with propos-ing a probabilistic model for fine-grained expert search. In fine-grained expert search, we are to extract and use evidence of expert search (usually blocks of documents) directly. Thus, the proposed probabilistic model incorporates evidence of expert search explicitly as a part of it. A piece of fine-grained evidence is formally defined as a quadru-ple, <topic, person, relation, document>, which denotes the fact that a topic and a person, with a certain relation between them, are found in a spe-cific document. The intuition behind the quadruple is that a query may be matched with phrases in various forms (denoted as topic here) and an expert candidate may appear with various name masks (denoted as person here), e.g., full name, email, or abbreviated names. Given a topic and person, rela-tion type is used to measure their closeness and 
914
document serves as a context indicating whether it is good evidence. Our proposed model for fine-grained expert search results in an implementation of two stages.  1) Evidence Extraction: document segments in various granularities are identified and evidences are extracted from them. For example, we can have segments in which an expert candidate and a que-ried topic co-occur within a same section of docu-ment-001:  ??later, Berners-Lee describes a semantic web search engine experience?? As the result, we can extract an evidence by using same-section relation, i.e., <semantic web search engine, Berners-Lee, same-section, document-001>.   2) Evidence Quality Evaluation: the quality (or reliability) of evidence is evaluated. The quality of a quadruple of evidence consists of four aspects, namely topic-matching quality, person-name-matching quality, relation quality, and document quality. If we regard evidence as link of expert candidate and queried topic, the four aspects will correspond to the strength of the link to query, the strength of the link to expert candidate, the type of the link, and the document context of the link re-spectively. All the evidences with their scores of quality are merged together to generate a single score for each expert candidate with regard to a given query. We empirically evaluate our proposed model and im-plementation on the W3C corpus which is used in the expert search task at TREC 2005 and 2006. Experimental results show that both explored evi-dences and evaluation of evidence quality can im-prove the expert search significantly. Compared with existing state-of-the-art expert search methods, the probabilistic model for fine-grained expert search shows promising improvement.  The rest of the paper is organized as follows. Section 2 surveys existing studies on expert search. Section 3 and Section 4 present the proposed prob-abilistic model and its implementation, respec-tively. Section 5 gives the empirical evaluation. Finally, Section 6 concludes the work. 2 Related Work  
2.1 Expert Search Systems One setting for automatic expert search is to as-sume that data from specific resources are avail-able. For example, Expertise Recommender (Kautz 
et al, 1996), Expertise Browser (Mockus and Herbsleb, 2002) and the system in (McDonald and Ackerman, 1998) make use of log data in software development systems to find experts. Yet another approach is to mine expert and expertise from email communications (Campbell et al, 2003; Dom et al 2003; Sihn and Heeren, 2001). Searching expert from general documents has also been studied (Davenport and Prusak, 1998; Mattox et al, 1999; Hertzum and Pejtersen, 2000). P@NOPTIC employs what is referred to as the ?profile-based? approach in searching for experts (Craswell et al, 2001). Expert/Expert-Locating (EEL) system (Steer and Lochbaum, 1988) uses the same approach in searching for expert groups. DEMOIR (Yimam, 1996) enhances the profile-based approach by separating co-occurrences into different types. In essence, the profile-based ap-proach utilizes the co-occurrences between query words and people within documents. 2.2 Expert Search at TREC A task on expert search was organized within the enterprise track at TREC 2005, 2006 and 2007 (Craswell et al, 2005; Soboroff  et al, 2006; Bai-ley et al, 2007).  Many approaches have been proposed for tack-ling the expert search task within the TREC track. Cao et al (2005) propose a two-stage model with a set of extracted metadata. Balog et al (2006) com-pare two generative models for expert search. Fang et al (2006) further extend their generative model by introducing the prior of expert distribution and relevance feedback. Petkova and Croft (2006) fur-ther extend the profile based method by using a hierarchical language model. Macdonald and Ounis (2006) investigate the effectiveness of the voting approach and the associated data fusion techniques. However, such models are conducted in a coarse-grain scope of document as discussed before. In contrast, our study focuses on proposing a model for conducting expert search in a fine-grain scope of evidence (local context). 3 Fine-grained Expert Search Our research is to investigate a direct use of the local contexts for expert search. We call each local context of such kind as fine-grained evidence.  In this work, a fine-grained evidence is formally defined as a quadruple, <topic, person, relation, 
915
document>. Such a quadruple denotes that a topic and a person occurrence, with a certain relation between them, are found in a specific document.  Recall that topic is different from query. For ex-ample, given a query ?semantic web coordination?, the corresponding topic may be either ?semantic web? or ?web coordination?. Similarly, person here is different from expert candidate. E.g, given an expert candidate ?Ritu Raj Tiwari?, the matched person may be ?Ritu Raj Tiwari?, ?Tiwari?, or ?RRT? etc. Although both the topics and persons may not match the query and expert candidate ex-actly, they do have certain indication on the con-nection of query ?semantic web coordination? and expert ?Ritu Raj Tiwari?. 3.1 Evidence-Oriented Expert Search Model We conduct fine-grained expert search by incorpo-rating evidence of local context explicitly in a probabilistic model which we call an evidence-oriented expert search model. Given a query q, the probability of a candidate c being an expert (or knowing something about q) is estimated as 
( | ) ( , | )
( | , ) ( | )
e
e
P c q P c e q
P c e q P e q
=
=
?
?
, (1) 
where e denotes a quadruple of evidence.  Using the relaxation that the probability of c is independent of a query q given an evidence e, we can reduce Equation (1) as, 
( | ) ( | ) ( | )
e
P c q P c e P e q=
?
. (2) Compared to previous work, our model conducts expert search with a new way in which local con-texts of evidence are used to bridge a query q and an expert candidate c. The new way enables the expert search system to explore various local con-texts in a precise manner.  In the following sub-sections, we will detail two sub-models: the expert matching model P(c|e) and the evidence matching model P(e|q).  3.2 Expert Matching Model We expand the evidence e as quadruple <topic, people, relation, document> (<t, p, r, d> for short) for expert matching. Given a set of related evi-dences, we assume that the generation of an expert candidate c is independent with topic t and omit it 
in expert matching. Therefore, we simplify the ex-pert matching formula as below:  
),|()|(),,|()|( drpPpcPdrpcPecP ==
, (3) where P(c|p) depends on how an expert candidate c matches to a person occurrence p (e.g. full name or email of a person). The different ways of matching an expert candidate c with a person occurrence p results in varied qualities. P(c|p) represents the quality. P(p|r,d) expresses the probability of an occurrence p given a relation r and a document d. P(p|r,d) is estimated in MLE as, 
),(
),,(
),|(
drL
drpfreq
drpP = , (4) 
where freq(p,r,d) is the frequency of person p matched by relation r in document d, and L(r, d) is the frequency of all the persons matched by rela-tion r in d. This estimation can further be smoothed by using the evidence collection as follows:  
?
?
?+=
Dd
S
D
drpP
drpPdrpP
'
||
)',|(
)1(),|(),|( ?? , (5) 
where D denotes the whole document collection. |D| is the total number of documents.  We use Dirichlet prior in smoothing of parame-ter ?:  
KdrL
drL
+
=
),(
),(? , (6) 
where K is the average frequency of all the experts in the collection.  
3.3 Evidence Matching Model By expanding the evidence e and employing inde-pendence assumption, we have the following for-mula for evidence matching: 
)|()|()|()|(
)|,,,()|(
qdPqrPqpPqtP
qdrptPqeP
=
= . (7) 
In the following, we are to explain what these four terms represent and how they can be estimated. The first term P(t|q) represents the probability that a query q matches to a topic t in evidence. Re-call that a query q may match a topic t in various ways, not necessarily being identical to t. For ex-ample, both topic ?semantic web? and ?semantic web search engine? can match the query ?semantic web search engine?. The probability is defined as 
916
( )),()|( qttypePqtP ? , (8) where type(t, q) represents the way that q matches to t, e.g., phrase matching. Different matching methods are associated with different probabilities. The second term P(p|q) represents the probabil-ity that a person p is generated from a query q. The probability is further approximated by the prior probability of p, 
)()|( pPqpP ? . (9) The prior probability can be estimated by MLE, i.e., the ratio of total occurrences of person p in the collection. The third term represents the probability that a relation r is generated from a query q. Here, we approximate the probability as  
))(()|( rtypePqrP ?
, (10) where type(r) represents the way r connecting query and expert. P(type(r)) represents the reliabil-ity of relation type of r. Following the Bayes rule, the last term can be transformed as 
)()|(
)(
)()|(
)|( dPdqP
qP
dPdqP
qdP ?= , (11) 
where priority distribution P(d) can be estimated based on static rank, e.g., PageRank (Brin and Page, 1998). P(q|d) can be estimated by using a standard language model for IR (Ponte and Croft, 1998). In summary, Equation (7) is converted to 
( ) )()|())(()(),()|( dPdqPrtypePpPqttypePqeP ?
. (12) 
3.4 Evidence Merging  We assume that the ranking score of an expert can be acquired by summing up together all scores of the supporting evidences. Thus we calculate ex-perts? scores by aggregating the scores from all evidences as in Equation (1). 4 Implementation  The implementation of the proposed model con-sists of two stages, namely evidence extraction and evidence quality evaluation.  
4.1 Evidence Extraction Recall that we define an evidence for expert search as a quadruple <topic, person, relation, document>. The evidence extraction covers the extraction of the first three elements, namely person identifica-tion, topic discovering and relation extraction. 4.1.1 Person Identification  The occurrences of an expert can be in various forms, such as name and email address. We call each type of form an expert mask. Table 1 provides a statistic on various masks on the basis of W3C corpus. In Table 1, rate is the proportion of the person occurrences with relevant masks to the per-son occurrences with any of the masks, and ambi-guity is defined as the probability that a mask is shared by more than one expert.   Mask Rate/Ambiguity Sample Full Name(NF) 48.2% / 0.0000 Ritu Raj Tiwari  Email Name(NE) 20.1% / 0.0000 rtiwari@nuance.com Combined Name (NC) 4.2% /0.3992 Tiwari, Ritu R;            R R Tiwari Abbr. Name(NA) 21.2% / 0.4890 Ritu Raj ; Ritu Short Name(NS) 0.7% / 0.6396 RRT Alias, new email (NAE)  7% / 0.4600 Ritiwari rti-wari@hotmail.com Table 1. Various masks and their ambiguity 
1) Every occurrence of a candidate?s email address is normalized to the appropriate candidate_id. 2) Every occurrence of a candidate?s full_name is normalized to the appropriate candidate_id if there is no ambiguity; otherwise, the occurrence is normalized to the candidate_id of the most frequent candidate with that full_name. 3) Every occurrence of combined name, abbrevi-ated name, and email alias is normalized to the appropriate candidate_id if there is no ambigu-ity; otherwise, the occurrence may be normal-ized to the candidate_id of a candidate whose full name also appears in the document. 4) All the personal occurrences other than those covered by Heuristic 1) ~ 3) are ignored. Table 2. Heuristic rules for expert extraction As Table 1 demonstrates, it is not an easy task to identify all the masks with regards to an expert. On one hand, the extraction of full name and email address is straightforward but suffers from low coverage. On the other hand, the extraction of 
917
combined name and abbreviated name can com-plement the coverage, while needs handling of am-biguity.  Table 2 provides the heuristic rules that we use for expert identification. In the step 2) and 3), the rules use frequency and context discourse for re-solving ambiguities respectively. With frequency, each expert candidate actually is assigned a prior probability. With context discourse, we utilize the intuition that person names appearing similar in a document usually refers to the same person. 4.1.2 Topic Discovering A queried topic can occur within documents in various forms, too. We use a set of query process-ing techniques to handle the issue. After the proc-essing, a set of topics transformed from an original query will be obtained and then be used in the search for experts. Table 3 shows five forms of topic discovering from a given query.   Forms Description Sample Phrase Match(QP) The exact match with origi-nal query given by users ?semantic web search engine? Bi-gram Match(QB) A set of matches formed by extracting bi-gram of words in the original query ?semantic web? ?search en-gine? Proximity Match(QPR) Each query term appears as a neighborhood within a window of specified size ?semantic web enhanced search engine? Fuzzy Match(QF) A set of matches, each of which resembles the origi-nal query in appearance. ?sementic web seerch engine? Stemmed Match(QS) A match formed by stem-ming the original query. ?sementic web seerch engin? Table 3. Discovered topics from query ?semantic web search engine? 
4.1.3 Relation Extraction We focus on extracting relations between topics and expert candidates within a span of a document. To make the extraction easier, we partition a document into a pre-defined layout. Figure 1 pro-vides a template in Backus?Naur form. Figure 2 provides a practical use of the template. Note that we are not restricting the use of the template only for certain corpus. Actually the tem-plate can be applied to many kinds of documents. For example, for web pages, we can construct the <Title> from either the ?title? metadata or the con-
tent of web pages (Hu et al, 2006). As for e-mail, we can use the ?subject? field as the <Title>. 
 Figure. 1. A template of document layout 
...
...
...
RDF Primer
Editors: Frank Manola, fmanola@acm.org
   Eric Miller, W3C, em@w3.org
2. Making Statements About Resources
RDF is intended to provide a simple way to make state
These capabilities (the normative specification describe)
2.1 Basic Concepts
Imagine trying to state that someone named John Smith
The form of a simple statement such as:
<Title>
<Author>
<Body>
<Section Title>
<Section>
<Section Body>
 Figure 2. An example use of the layout template  With the layout of partitioned documents, we can then explore many types of relations among different blocks. In this paper, we demonstrate the use of five types of relations by extending the study in (Cao et al, 2005).  Section Relation (RS): The queried topic and the expert candidate occur in the same <Section>. Windowed Section Relation (RWS): The que-ried topic and the expert candidate occur within a fixed window of a <Section>. In our experiment, we used a window of 200 words. Reference Section Relation (RRS): Some <Sec-tion>s should be treated specially. For example, the <Section> consisting of reference information like a list of <book, author> can serve as a reliable source connecting a topic and an expert candidate. We call the relation appearing in a special type of <Section> a special reference section relation. It might be argued whether the use of special sections can be generalized. According to our survey, the special <Section>s can be found in various sites such as Wikipedia as well as W3C. Title-Author Relation (RTA): The queried topic appears in the <Title> and the expert candidate appears in the <Author>.  
918
Section Title-Body Relation (RSTB): The que-ried topic and the expert candidate appear in the <Section Title> and <Section Body> of the same <Section>, respectively. Reversely, the queried topic and the expert candidate can appear in the <Section Body> and <Section Title> of a <Section>. The latter case is used to characterize the docu-ments introducing certain expert or the expert in-troducing certain document. Note that our model is not restricted to use these five relations. We use them only for the aim of demonstrating the flexibility and effectiveness of fine-grained expert search. 4.2 Evidence Quality Evaluation In this section, we elaborate the mechanism used for evaluating the quality of evidence.  4.2.1 Topic-Matching Quality In Section 4.1.2, we use five techniques in process-ing query matches, which yield five sets of match types for a given query. Obviously, the different query matches should be associated with different weights because they represent different qualities.  We further note that different bi-grams gener-ated from the same query with the bi-gram match-ing method might also present different qualities. For example, both topic ?css test? and ?test suite? are the bi-gram matching for query ?css test suite?; however, the former might be more informative. To model that, we use the number of returned documents to refine the query weight. The intuition behind that is similar to the thought of IDF popu-larly used in IR as we prefer to the distinctive bi-grams. Taking into consideration the above two factors, we calculate the topic-matching quality Qt (corre-sponding to P(type(t,q)) in Equation (12) ) for the given query q as 
t
tt
t
df
dfMIN
qttypeWQ
)(
)),((
''
= , (13) 
where t means the discovered topic from a docu-ment and type(t,q) is the matching type between topic t and query q. W(type(t,q)) is the weight for a certain query type, dft is the number of returned documents matched by topic t. In our experiment, we use the 10 training topics of TREC2005 as our training data, and the best quality scores for phrase match, bi-gram match, proximity match, fuzzy 
match, and stemmed match are 1, 0.01, 0.05, 10-8, and 10-4, respectively.  4.2.2 Person-Matching Quality An expert candidate can occur in the documents in various ways. The most confident occurrence should be the ones in full name or email address. Others can include last name only, last name plus initial of first name, etc. Thus, the action of reject-ing or accepting a person from his/her mask (the surface expression of a person in the text) is not simply a Boolean decision, but a probabilistic one with a reliability weight Qp (corresponding to P(c|p) in Equation (3) ). Similarly, the best trained weights for full name, email name, combined name, abbreviated name, short name, and alias email are set to 1, 1, 0.8, 0.2, 0.2, and 0.1, respectively. 4.2.3 Relation Type Quality The relation quality consists of two factors. One factor is about the type of the relation. Different types of relations indicate different strength of the connection between expert candidates and queried topics. In our system, the section title-body rela-tion is given the highest confidence. The other fac-tor is about the degree of proximity between a query and an expert candidate. The intuition is that, the more distant are a query and an expert candi-date within a relation, the looser the connection between them is. To include these two factors, the quality score Qr (corresponding to P(type(r)) in Equation (12) )of a relation r is defined as:  
1),( +
=
tpdis
C
WQ
r
rr
, (14) 
where Wr is the weight of relation type r, dis(p, t) is the distance from the person occurrence p to the queried topic t and Cr is a constant for normaliza-tion. Again, we optimize the Wr based on the train-ing topics, the best weights for section relation, windowed section relation, reference section rela-tion, title-author relation, and section title-body relation  are 1, 4, 10, 45, and 1000 respectively.  4.2.4 Document Quality The quality of evidence also depends on the quality of the document, the context in which it is found. The document context can affect the credibility of the evidence in two ways:  
919
Static quality: indicating the authority of a document. In our experiment, the static quality Qd (corresponding to P(d) in Equation (12) ) is esti-mated by the PageRank, which is calculated using a standard iterative algorithm with a damping fac-tor of 0.85 (Brin and Page, 1998).  Dynamic quality: by ?dynamic?, we mean the quality score varies for different queries q. We de-note the dynamic quality as QDY(d,q) (correspond-ing to P(q|d) in Equation (12) ), which is actually the document relevance score returned by a stan-dard language model for IR(Ponte and Croft, 1998). 5 Experimental Results 
5.1  The Evaluation Data In our experiment, we used the data set in the ex-pert search task of enterprise search track at TREC 2005 and 2006. The document collection is a crawl of the public W3C sites in June 2004. The crawl comprises in total 331,307 web pages. In the fol-lowing experiments, we used the training set of 10 topics of TREC 2005 for tuning the parameters aforementioned in Section 4.2, and used the test set of 50 topics of TREC 2005 and 49 topics of TREC 2006 as the evaluation data sets.  5.2 Evaluation Metrics We used three measures in evaluation: Mean aver-age precision (MAP), R-precision (R-P), and Top N precision (P@N). They are also the standard measures used in the expert search task of TREC. 5.3 Evidence Extraction In the following experiments, we constructed the baseline by using the query matching methods of phrase matching, the expert matching methods of full name matching and email matching, and the relation of section relation. To show the contribu-tion of each individual method for evidence extrac-tion, we incrementally add the methods to the baseline method. In the following description, we will use ?+? to denote applying new method on the previous setting. 5.3.1 Query Matching Table 4 shows the results of expert search achieved by applying different methods of query matching. 
QB, QPR, QF, and QS denote bi-gram match, prox-imity match, fuzzy match, and stemmed match, respectively. The performance of the proposed model increases stably on MAP when new query matches are added incrementally. We also find that the introduction of QF and QS bring some drop on R-Precision and P@10. It is reasonable because both QF and QS bring high recall while affect the precision a bit. The overall relative improvement of using query matching compared to the baseline is presented in the row ?Improv.?. We performed t-tests on MAP. The p-values (< 0.05) are presented in the ?T-test? row, which shows that the im-provement is statistically significant.   TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.1840 0.2136 0.3060 0.3752 0.4585 0.5604 +QB 0.1957 0.2438 0.3320 0.4140 0.4910 0.5799 +QPR  0.2024 0.2501 0.3360 0.4530 0.5137 0.5922 +QF ,QS 0.2030 0.2501 0.3360 0.4580 0.5112 0.5901 Improv. 10.33% 17.09% 9.80% 22.07% 11.49% 5.30% T-test 0.0084   0.0000   Table 4. The effects of query matching 
5.3.2 Person Matching For person matching, we considered four types of masks, namely combined name (NC), abbreviated name (NA), short name (NS) and alias and new email (NAE). Table 5 provides the results on person matching at TREC 2005 and 2006. The baseline is the best model achieved in previous section. It seems that there is little improvement on P@10 while an improvement of 6.21% and 14.00% is observed on MAP. This might be due to the fact that the matching method such as NC has a higher recall but lower precision.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2030 0.2501 0.3360 0.4580 0.5112 0.5901 +NC 0.2056 0.2539 0.3463 0.4709 0.5152 0.5931 +NA  0.2106 0.2545 0.3400 0.5010 0.5181 0.6000 +NS  0.2111 0.2578 0.3400 0.5121 0.5192 0.6000 +NAE  0.2156 0.2591 0.3400 0.5221 0.5212 0.6000 Improv. 6.21% 3.60% 1.19% 14.00% 1.96% 1.68% T-test 0.0064   0.0057   Table 5. The effects of person matching 
920
5.3.3 Multiple Relations For relation extraction, we experimentally demon-strated the use of each of the five relations pro-posed in Section 4.1.3, i.e., section relation (RS), windowed section relation (RWS), reference section relation (RRS), title-author relation (RTA), and sec-tion title-body relation (RSTB). We used the best model achieved in previous section as the baseline. From Table 6, we can see that the section title-body relation contributes the most to the improve-ment of the performance. By using all the discov-ered relations, a significant improvement of 19.94% and 8.35% is achieved.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2156 0.2591 0.3400 0.5221 0.5212 0.6000 +RWS 0.2158 0.2633 0.3380 0.5255 0.5311 0.6082 +RRS 0.2160 0.2630 0.3380 0.5272 0.5314 0.6061 +RTA 0.2234 0.2634 0.3580 0.5354 0.5355 0.6245 +RSTB 0.2586 0.3107 0.3740 0.5657 0.5669 0.6510 Improv. 19.94% 19.91% 10.00% 8.35% 8.77% 8.50% T-test 0.0013   0.0043   Table 6. The effects of relation extraction 
5.4 Evidence Quality  The performance of expert search can be further improved by considering the evidence quality. Ta-ble 7 shows the results by considering the differ-ences in quality.  We evaluated two kinds of evidence quality: context static quality (Qd) and context dynamic quality (QDY). Each of the evidence quality con-tributes about 1%-2% improvement for MAP. The improvement from the PageRank that we calcu-lated from the corpus implies that the web scaled rank technique is also effective in the corpus of documents. Finally, we find a significant relative improvement of 6.13% and 2.86% on MAP by us-ing evidence qualities.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2586 0.3107 0.3740 0.5657 0.5669 0.6510 +Qd 0.2711 0.3188 0.3720 0.5900 0.5813 0.6796 +QDY 0.2755 0.3252 0.3880 0.5943 0.5877 0.7061 Improv. 6.13% 4.67% 3.74% 2.86% 3.67% 8.61% T-test 0.0360   0.0252   Table 7. The effects of using evidence quality  
5.5 Comparison with Other Systems In Table 8, we juxtapose the results of our prob-abilistic model for fine-grained expert search with automatic expert search systems from the TREC evaluation. The performance of our proposed model is rather encouraging, which achieved com-parable results to the best automatic systems on the TREC 2005 and 2006.    MAP R-prec Prec@10 TREC2005 0.2749 0.3330 0.4520 Rank-1 System TREC20061 0.5947 0.5783 0.7041 TREC2005 0.2755 0.3252 0.3880 Our System TREC2006 0.5943 0.5877 0.7061 Table 8. Comparison with other systems 
6 Conclusions This paper proposed to conduct expert search using a fine-grained level of evidence. Specifically, quadruple evidence was formally defined and served as the basis of the proposed model. Differ-ent implementations of evidence extraction and evidence quality evaluation were also comprehen-sively studied. The main contributions are:  1. The proposal of fine-grained expert search, which we believe to be a promising direc-tion for exploring subtle aspects of evidence.  2. The proposal of probabilistic model for fine-grained expert search. The model facilitates investigating the subtle aspects of evidence.  3. The extensive evaluation of the proposed probabilistic model and its implementation on the TREC data set. The evaluation shows promising expert search results.   In future, we are to explore more domain inde-pendent evidences and evaluate the proposed model on the basis of the data from other domains. Acknowledgments The authors would like to thank the three anony-mous reviewers for their elaborate and helpful comments. The authors also appreciate the valu-able suggestions of Hang Li, Nick Craswell, Yangbo Zhu and Linyun Fu.                                                            1 This system, where cluster-based re-ranking is used, is a variation of the fine-grained model proposed in this paper. 
921
References  Bailey, P.,  Soboroff , I., Craswell, N., and Vries A.P., Overview of the TREC 2007 Enterprise Track. In: Proc. of TREC 2007.  Balog, K., Azzopardi, L., and Rijke, M. D., 2006. Formal models for expert finding in enterprise cor-pora. In: Proc. of SIGIR?06,pp.43-50. Brin, S. and Page, L., 1998. The anatomy of a rlarge-scale hypertextual Web search engine, Computer Networks and ISDN Systems (30), pp.107-117. Campbell, C.S., Maglio, P., Cozzi, A. and Dom, B., 2003. Expertise identification using email communi-cations. In: Proc. of CIKM ?03 pp.528?531.  Cao, Y., Liu, J., and Bao, S., and Li, H., 2005. Research on expert search at enterprise track of TREC 2005. In: Proc. of TREC 2005. Craswell, N., Hawking, D., Vercoustre, A. M. and Wil-kins, P., 2001. P@NOPTIC Expert: searching for ex-perts not just for documents. In: Proc. of Ausweb?01. Craswell, N., Vries, A.P., and Soboroff, I., 2005. Over-view of the TREC 2005 Enterprise Track. In: Proc. of TREC 2005. Davenport, T. H. and Prusak, L., 1998. Working Knowledge: how organizations manage what they know. Howard Business, School Press, Boston, MA. Dom, B., Eiron, I., Cozzi A. and Yi, Z., 2003. Graph-based ranking algorithms for e-mail expertise analy-sis, In: Proc. of SIGMOD?03 workshop on Research issues in data mining and knowledge discovery. Fang, H., Zhou, L., Zhai, C., 2006. Language models for expert finding-UIUC TREC 2006 Enterprise Track Experiments, In: Proc. of TREC2006. Fu, Y., Xiang, R., Liu, Y., Zhang, M., Ma, S., 2007. A CDD-based Formal Model for Expert Finding. In Proc. of CIKM 2007. Hertzum, M. and Pejtersen, A. M., 2000. The informa-tion-seeking practices of engineers: searching for documents as well as for people. Information Proc-essing and Management, 36(5), pp.761?778. Hu, Y., Li, H., Cao, Y., Meyerzon, D. Teng, L., and Zheng, Q., 2006. Automatic extraction of titles from general documents using machine learning, IPM. Kautz, H., Selman, B. and Milewski, A., 1996. Agent amplified communication. In: Proc. of AAAI?96, pp. 3?9. Mattox, D., Maybury, M. and Morey, D., 1999. Enter-prise expert and knowledge discovery. Technical Re-port.  McDonald, D. W. and Ackerman, M. S., 1998. Just Talk to Me: a field study of expertise location. In: Proc. of CSCW?98, pp.315-324. Mockus, A. and Herbsleb, J.D., 2002. Expertise Browser: a quantitative approach to identifying ex-pertise, In: Proc. of ICSE?02. 
Maconald, C. and Ounis, I., 2006. Voting for candi-dates: adapting data fusion techniques for an expert search task. In: Proc. of CIKM'06, pp.387-396. Macdonald, C. and Ounis, I., 2007. Expertise Drift and Query Expansion in Expert Search. In Proc. of CIKM 2007.  Petkova, D., and Croft, W. B., 2006. Hierarchical lan-guage models for expert finding in enterprise cor-pora, In: Proc. of ICTAI?06, pp.599-608. Ponte, J. and Croft, W., 1998. A language modeling approach to information retrieval, In: Proc. of SIGIR?98, pp.275-281. Sihn, W. and Heeren F., 2001. Xpertfinder-expert find-ing within specified subject areas through analysis of e-mail communication. In: Proc. of the 6th Annual Scientific conference on Web Technology.  Soboroff, I., Vries, A.P., and Craswell, N., 2006. Over-view of the TREC 2006 Enterprise Track. In: Proc. of TREC 2006. Steer, L.A. and Lochbaum, K.E., 1988. An ex-pert/expert locating system based on automatic repre-sentation of semantic structure, In: Proc. of the 4th IEEE Conference on Artificial Intelligence Applica-tions. Yimam, D., 1996. Expert finding systems for organiza-tions: domain analysis and the DEMOIR approach. In: ECSCW?99 workshop of beyond knowledge man-agement: managing expertise, pp. 276?283. 
922
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 734?742,
Beijing, August 2010
Exploiting Structured Ontology to Organize Scattered Online Opinions
Yue Lu, Huizhong Duan, Hongning Wang, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
{yuelu2,duan9,wang296,czhai}@illinois.edu
Abstract
We study the problem of integrating scat-
tered online opinions. For this purpose,
we propose to exploit structured ontology
to obtain well-formed relevant aspects to
a topic and use them to organize scattered
opinions to generate a structured sum-
mary. Particularly, we focus on two main
challenges in implementing this idea, (1)
how to select the most useful aspects from
a large number of aspects in the ontology
and (2) how to order the selected aspects
to optimize the readability of the struc-
tured summary. We propose and explore
several methods for solving these chal-
lenges. Experimental results on two dif-
ferent data sets (US Presidents and Digital
Cameras) show that the proposed methods
are effective for selecting aspects that can
represent the major opinions and for gen-
erating coherent ordering of aspects.
1 Introduction
The explosive growth of online opinions raises in-
teresting challenges for opinion integration and
summarization. It is especially interesting to in-
tegrate and summarize scattered opinions in blog
articles and forums as they tend to represent the
general opinions of a large number of people and
get refreshed quickly as people dynamically gen-
erate new content, making them valuable for un-
derstanding the current views of a topic.
However, opinions in blogs and forums are
usually fragmental, scattered around, and buried
among other off-topic content, so it is quite chal-
lenging to organize them in a meaningful way.
Traditional text summarization techniques gener-
ate an unstructured list of sentences as a sum-
mary, which cannot reveal representative opinions
on different aspects of a topic or effectively facil-
itate navigation into the huge opinion space. To
address this limitation, recent work has shown the
usefulness of generating a structured summary of
opinions, in which related opinions are grouped
into topical aspects with explicit labeling of all the
aspects. A major challenge in producing such a
structured summary is how to generate these as-
pects for an arbitrary topic (e.g., products, politi-
cal figures, policies, etc.). Intuitively, the aspects
should be concise phrases that can both be easily
interpreted in the context of the topic under con-
sideration and capture the major opinions. How-
ever, where can we find such phrases and which
phrases should we select as aspects? Furthermore,
once we selected aspects, how should we order
them to improve the readability of a structured
summary? One way to generate aspects is to clus-
ter all the opinion sentences and then identify rep-
resentative phrases in each cluster. Although as-
pects selected in this way can effectively capture
the major opinions, a major limitation is that it is
generally hard to ensure that the selected phrases
are well connected with the given topic (Chen and
Dumais, 2000).
In this paper, we propose a novel approach
to generating aspects by leveraging the ontolo-
gies with structured information that are available
online, such as open domain knowledge base in
Freebase1. Such kind of ontology data is not in
small scale by any measure. For example, Free-
base alone contains more than 10 million topics,
3000 types, and 30,000 properties; moreover, it is
constantly growing as people collaboratively con-
tribute. Freebase provides different properties for
different types of topics such as personal infor-
mation for a ?US President? and product features
for a ?Digital Camera?. Since this kind of re-
sources can provide related entities/relations for a
1http://www.freebase.com
734
wide range of topics , our general idea is to lever-
age them as guidance for more informed organi-
zation of scattered online opinions, and in partic-
ular, to select the most important properties of a
topic from such structured ontology as aspects to
generate a structured opinion summary. A signif-
icant advantage of this approach to aspect genera-
tion is that the selected aspects are guaranteed to
be very well connected with the topic, but it also
raises an additional challenge in selecting the as-
pects to best capture the major opinions from a
large number of aspects provided for each topic in
the ontology. Different from some existing work
on exploiting ontologies, e.g., (Sauper and Barzi-
lay, 2009), which relies on training data, we focus
on exploring unsupervised approaches, which can
be applied to a larger scope of topics.
Specifically, given a topic with entries in an on-
tology and a collection of scattered online opin-
ions about the topic, our goal is to generate a
structured summary where representative major
opinions are organized with well aligned aspects
and in an order easy for human to follow. We
propose the following general approach: First, re-
trieval techniques are employed to align opinions
to relevant aspects. Second, a subset of most inter-
esting aspects are selected. Third, we will further
order the selected aspects to present them in a rea-
sonable order. Finally, for the opinions uncovered
by the selected aspects from the ontology, we use
a phrase ranking method to suggest new aspects to
add to the ontology for increasing its coverage.
Implementing the second and third steps in-
volves special challenges. In particular, without
any training data, it is unclear how we should
show the most interesting aspects in ontology with
major opinions aligned and which presentation
order of aspects is natural and intuitive for hu-
man. Solving these two challenges is the main
focus of this paper. We propose three meth-
ods for aspect selection, i.e., size-based, opinion
coverage-based, and conditional entropy-based
methods, and two methods for aspect ordering,
i.e., ontology-ordering and coherence ordering.
We evaluate our methods on two different types of
topics: US Presidents and Digital Cameras. Qual-
itative results demonstrate the utility of integrating
opinions based on structured ontology as well as
the generalizability of proposed methods. Quan-
titative evaluation is also conducted to show the
effectiveness of our methods.
Note that we use the term ?opinion? to broadly
refer to any discussion in opinionated sources
such as blogs and reviews. This allows us to for-
mulate and solve the problem in a general way.
Indeed, the main goal of our work is to extract
and organize the major opinions about a topic that
are buried in many scattered opinionated sources
rather than perform deeper understanding of opin-
ions (e.g., distinguishing positive from negative
opinions), which can be done by using any exist-
ing sentiment analysis technique as an orthogonal
post-processing step after applying our method.
2 Related Work
Aspect summarization, i.e., structured opinion
summarization over topical aspects, has attracted
much attention recently. Existing work iden-
tifies aspects using frequent-pattern/association-
rule mining, e.g. (Liu et al, 2005; Popescu and
Etzioni, 2005), sentence clustering, e.g. (Ga-
mon et al, 2005; Leouski and Croft, 1996), or
topic modeling, e.g. (Mei et al, 2006; Titov and
McDonald, 2008). After that, meaningful and
prominent phrases need to be selected to repre-
sent the aspects, e.g. (Zhao and He, 2006; Mei
et al, 2007). However, these methods suffer from
the problem of producing trivial aspects. Conse-
quently, some of the aspects generated are very
difficult to interpret (Chen and Dumais, 2000). In
this paper, we propose a different kind of approach
that is to use aspects provided by ontology which
are known to be relevant and easy to interpret.
Ontology is used in (Carenini et al, 2005) but
only for mapping product features. The closest
work to ours are (Lu and Zhai, 2008; Sauper and
Barzilay, 2009); both try to use well-written arti-
cles for summarization. However, (Lu and Zhai,
2008) assumes the well-written article is struc-
tured with explicit or implicit aspect information,
which does not always hold in practice, while
(Sauper and Barzilay, 2009) needs a relatively
large amount of training data in the given domain.
In comparison, our work only needs the ontology
information for the given topic which is much eas-
ier to obtain from resources such as Freebase.
735
3 Methods
Given (1) an input topic T , (2) a large number of
aspects/properties A = {A1, ..., Am} from an on-
tology that are related to T , and (3) a huge col-
lection of scattered opinion sentences about the
topic DT = {s1, . . . , sn}, our goal is to gener-
ate a structured organization of opinions that are
both aligned well with the interesting aspects and
representative of major opinions about the topic.
The envisioned structured organization consists
of a sequence of selected aspects from ontol-
ogy ordered to optimize readability and a set of
sentences matching each selected aspect. Once
we obtain a set of sentences in each aspect, we
can easily apply a standard text summarization
method to further summarize these sentences, thus
the unique challenges related to our main idea of
exploiting ontology are the following, which are
also the main focus of our study:
Aspect Selection: How can we select a subset of
aspects A? ? A to capture the major opinions in
our opinion set DT ?
Aspect Ordering: How can we order a subset of
selected aspects A? so as to present them in an or-
der pi(A?) that is most natural with respect to hu-
man perception?
New Aspects Suggestion: Can we exploit the
opinions in DT to suggest new aspects to be added
to the ontology?
3.1 Aspect Selection
In order to align the scattered opinions to the
most relevant aspects, we first use each aspect la-
bel Ai ? A as a query to retrieve a set of rel-
evant opinions in the collection Si ? DT with
a standard language modeling approach, i.e., the
KL-divergence retrieval model (Zhai and Lafferty,
2001). Up to 1000 opinion sentences are retrieved
for each aspect; each opinion sentence can be po-
tentially aligned to several aspects. In this way,
scattered online discussion are linked to the most
relevant aspects in the ontology, which enables a
user to use aspects as ?semantic bridges? to navi-
gate into the opinion space..
However, there are usually a lot of candidate
aspects in an ontology, and only some are heav-
ily commented in online discussions, so showing
all the aspects is not only unnecessary, but also
overwhelming for users. To solve this problem,
we propose to utilize the aligned opinions to fur-
ther select a subset of the most interesting aspects
A? ? A with size k. Several approaches are pos-
sible for this subset selection problem.
Size-based: Intuitively, the selected subset A?
should reflect the major opinions. So a straightfor-
ward method is to order the aspects Ai by the size
of the aligned opinion sentences Si, i.e., the num-
ber of relevant opinion sentences, and then select
the top k ones.
Opinion Coverage-based: The previous method
does not consider possible redundancy among the
aspects. A better approach is to select the subset
that covers as many distinct opinion sentences as
possible. This can be formulated as a maximum
coverage problem, for which a greedy algorithm
is known to be a good approximation: we select
one aspect at a time that is aligned with the largest
number of uncovered sentences.
Conditional Entropy-based: Aspects from a struc-
tured ontology are generally quite meaningful, but
they are not designed specifically for organizing
the opinions in our data set. Thus, they do not
necessarily correspond well to the natural clus-
ters in scattered opinions. To obtain aspects that
are aligned well with the natural clusters in scat-
tered opinions, we can first cluster DT into l
clusters C = {C1, . . . , Cl} using K-means with
TF ? IDF as features, and then choose the sub-
set of aspects that minimize Conditional Entropy
of the cluster label given the aspect:
A? = argminH(C|A?) = argmin?
??
?
Ai?A?,Ci?C
p(Ai, Ci) log
p(Ai, Ci)
p(Ai)
?
?
This Conditional Entropy measures the uncer-
tainty about the cluster label of a sentence given
the knowledge of its aspect. Intuitively, if the as-
pects are aligned well with the clusters, we would
be able to predict well the cluster label of a sen-
tence if we know its aspect, thus there would be
less uncertainty about the cluster label. In the
extreme case when the cluster label can be com-
pletely determined by the aspect, the conditional
entropy would reach its minimum (i.e., 0). Intu-
itively, the conditional entropy-based method es-
sentially selects the most appropriate aspects from
736
Algorithm 1 Greedy Algorithm for
Conditional Entropy Based Aspect Selection
Input: A = {A1, ..., Am}
Output: k-sized A? ? A
1: A? = {?mi=1Ai}2: for j=1 to k do
3: bestH = ?; bestA = A0
4: for each Ai in A do
5: tempA? = {Ai, A? \Ai}
6: if H(C|tempA?) < bestH then
7: bestH = H(C|tempA?)
8: bestA = Ai
9: A? = {bestA,A? \ bestA}
10: output A?
the ontology to label clusters of opinions.
The exact solution of this combinatorial optimiza-
tion problem is NP-complete, so we employ a
polynomial time greedy algorithm to approximate
it: in the i-th iteration, we select the aspect that
can minimize the conditional entropy given the
previous i ? 1 selected aspects. Pseudo code is
given in Algorithm 1.
3.2 Aspect Ordering
In order to present the selected aspects to users
in a most natural way, it is important to obtain a
coherent order of them, i.e., generating an order
consistent with human perception. To achieve this
goal, our idea is to use human written articles on
the topic to learn how to organize the aspects au-
tomatically. Specifically, we would order aspects
so that the relative order of the sentences in all the
aspects would be as consistent with their order in
the original online discussions as possible.
Formally, the input is a subset of selected as-
pects A?; each Ai ? A? is aligned with a set of
relevant opinion sentences Si = {Si,1, Si,2, ...}.
We define a coherence measurement function over
sentence pairs Co(Si,k, Sj,l), which is set to 1 iff
Si,k appears before Sj,l in the same article. Other-
wise, it is set to 0. Then a coherence measurement
function over an aspect pair can be calculated as
Co(Ai, Aj) =
?
Si,k?Si,Sj,l?Sj Co(Si,k, Sj,l)
|Si||Sj |
As an output, we would like to find a permutation
p?i(A?) that maximizes the coherence of all pair-
wise aspects, i.e.,
p?i(A?) = arg max
pi(A?)
?
Ai,Aj?A?,Ai?Aj
Co(Ai, Aj)
Algorithm 2 Greedy Algorithm for
Coherence Based Aspect Ordering
Input: A
Output: pi(A)
1: for each Ai, Aj in A do
2: calculate Co(Ai, Aj)
3: for p = 1 to len = A.size() do
4: Max = A[1]
5: for each aspect Ai in A do
6: Ai.coherence = 0
7: for each aspect Aj in pi(A) do
8: Ai.coherence+ = Co(Aj , Ai)
9: for each aspect Aj in A, j 6= i do
10: Ai.coherence+ = Co(Ai, Aj)
11: if Ai.coherence > Max.coherence then
12: Max = Ai
13: remove Max from A; add Max to pi(A)
14: output pi(A)
where Ai ? Aj means that Ai is before Aj . It
is easy to prove that the problem is NP-complete.
Therefore, we resort to greedy algorithms to find
approximations of the solution. Particularly we
view the problem as a ranking problem. The al-
gorithm proceeds by finding at each ranking po-
sition an aspect that can maximize the coherence
measurement, starting from the top of the rank list.
The detailed algorithm is given in Algorithm 2.
3.3 New Aspects Suggestion
Finally, if the opinions cover more aspects than in
the ontology, we also want to identify informative
phrases to label such extra aspects; such phrases
can also be used to further augment the ontology
with new aspects.
This problem is similar to existing work on gen-
erating labels for clusters (Zeng et al, 2004) or
topic models (Mei et al, 2007). Here we employ
a simple but representative technique to demon-
strate the feasibility of discovering interesting new
aspects for augmenting the ontology. We first ex-
tract named entities from scattered opinions DT
using Stanford Named Entity Recognizer (Finkel
et al, 2005). After that, we rank the phrases by
pointwise Mutual Information (MI):
MI(T, ph) = log P (T, ph)P (T )P (ph)
where T is the given topic and ph refers to a candi-
date entity phrase. P (T, ph) is proportional to the
number of opinion sentences they co-occur; P (T )
or P (ph) are proportional to the number of times
T or ph appears. A higher MI value indicates a
737
Statistics Category 1 Category 2
US president Digital Camera
Number of Topics 36 110
Number of Aspects 65?26 32?4
Number of Opinions 1001?1542 170?249
Table 1: Statistics of Data Sets
stronger association. We can then suggest the top
ranked entity phrases that are not in the selected
aspects as new aspects.
4 Experiments
4.1 Data Sets
To examine the generalizability of our methods,
we test on two very different categories of top-
ics: US Presidents and Digital Cameras.2 For the
ontology, we leverage Freebase, downloading the
structured ontology for each topic. For the opin-
ion corpus, we use blog data for US Presidents and
customer reviews for Digital Cameras. The blog
entries for US Presidents were collected by using
Google Blog Search3 with the name of a president
as the query. Customer reviews for Digital Cam-
eras were crawled from CNET4. The basic statis-
tics of our data sets is shown in Table 1. For all the
data collections, Porter stemmer (Porter, 1997) is
applied and stop words are removed.
4.2 Sample Results
We first show sample results of automatic orga-
nization of online opinions. We use the opin-
ion coverage-based algorithm to select 10 aspects
(10-20 aspects were found to be optimal in (Ka?ki,
2005)) and then apply the coherence-based aspect
ordering method. The number of clusters is set so
that there are on average 15 opinions per cluster.
Opinion Organization: Table 2 and Table 3
present sample results for President Ronald Rea-
gan and Sony Cybershot DSC-W200 camera re-
spectively5. We can see that (1) although Freebase
aspects provide objective and accurate informa-
tion about the given topics, extracted opinion sen-
tences offer additional subjective information; (2)
aligning scattered opinion sentences to most rel-
evant aspects in the ontology helps digestion and
2We have made our data sets available at http://
timan.cs.uiuc.edu/downloads.html .
3http://blogsearch.google.com
4http://www.cnet.com
5Due to space limit, we only show the first few aspects as
output by our methods.
navigation; and (3) the support number, which is
the number of opinion sentences aligned to an as-
pect, can show the popularity of the aspect in the
online discussions.
Adaptability of Aspect Selection: Being un-
supervised is a significant advantage of our meth-
ods over most existing work. It provides flexibil-
ity of applying the methods in different domains
without the requirement of training data, benefit-
ing from both the ontology based template guid-
ance as well as data-driven approaches. As a re-
sult, we can generate different results for differ-
ent topics even in the same domain. In Table 4,
we show the top three selected and ordered as-
pects for Abraham Lincoln and Richard Nixon.
Although they belong to the same category, differ-
ent aspects are picked up due to the differences in
online opinions. People talk a lot about Lincoln?s
role in American Civil War and his famous quo-
tation, but when talking about Nixon, people fo-
cus on ending the Vietnam war and the Watergate
scandal. ?Date of birth? and ?Government posi-
tion? are ranked first because people tend to start
talking from these aspects, which is more natural
than starting from aspects like ?Place of death?.
Baseline Comparison: We also show below the
aspects for Lincoln generated by a representative
approach using clustering method (e.g. (Gamon et
al., 2005)). i.e., we label the largest clusters by se-
lecting phrases with top mutual information. We
can see that although some phrases make sense,
not all are well connected with the given topic;
using aspects in ontology circumvents this prob-
lem. This example confirms the finding in pre-
vious work that the popular existing clustering-
based approach to aspects generation cannot gen-
erate meaningful labels (Chen and Dumais, 2000).
Vincent
New Salem State Historic Site
USS Abraham Lincoln
Martin Luther King Jr
Gettysburg
John F.
New Aspect Discovery: Finally, in Table 5 we
show some phrases ranked among top 10 using
the method described in Section 3.3. They reveal
additional aspects covered in online discussions
and serve as candidate new aspects to be added to
Freebase. Interestingly, John Wilkes Booth, who
assassinated President Lincoln, is not explicitly
738
FreeBase Aspects Supt Representative Opinion Sentences
Appointees: 897 Martin Feldstein, whose criticism of Reagan era deficits has not been forgotten.
- Martin Feldstein Reagan?s first National Security advisor was quoted as declaring...
- Chief Economic Advisor
Government Positions Held: 967 1981 Jan 20, Ronald Reagan was sworn in as president as 52 American hostages
- President of the United States boarded a plane in Tehran and headed toward freedom.
- Jan 20, 1981 to Jan 20, 1989 40th president of the US Ronald Reagan broke the so called ?20 year curse?...
Vice president: 847 8 years, 1981-1988 George H. W. Bush as vice president under Ronald Reagan...
- George H. W. Bush ...exception to the rule was in 1976, when George H W Bush beat Ronald.
Table 2: Opinion Organization Result for President Ronald Reagan
FreeBase Aspects Supt Representative Opinion Sentences
Format: 13 Quality pictures in a compact package.
- Compact ... amazing is that this is such a small and compact unit but packs so much power.
Supported Storage Types: 11 This camera can use Memory Stick Pro Duo up to 8 GB
- Memory Stick Duo Using a universal storage card and cable (c?mon Sony)
Sensor type: 10 I think the larger ccd makes a difference.
- CCD but remember this is a small CCD in a compact point-and-shoot.
Digital zoom: 47 once the digital :smart? zoom kicks in you get another 3x of zoom
-2? I would like a higher optical zoom, the W200 does a great digital zoom translation...
Table 3: Opinion Organization Result for Sony Cybershot DSC-W200 Camera
listed in Freebase, but we can find it in people?s
online discussion using mutual information.
4.3 Evaluation of Aspect Selection
Measures: Aspect selection is a new challenge,
so there is no standard way to evaluate it. It is also
very hard for human to read all of the aspects and
opinions and then select a gold standard subset.
Therefore, we opt to use indirect measures captur-
ing different characteristics of the aspect selection
problem (1) Aspect Coverage (AC): we first as-
sign each aspect Ai to the cluster Cj that has the
most overlapping sentences with Ai, approximat-
ing the cluster that would come into mind when
a reader sees Ai. Then AC is defined as the per-
centage of the clusters covered by at least one as-
pect. (2) Aspect Precision (AP ): for each cov-
ered cluster Ci, AP measures the Jaccard similar-
ity between Ci as a set of opinions and the union
of all aspects assigned to Ci. (3) Average Aspect
Precision (AAP ): defines averaged AP for all
clusters where an uncovered Ci has a zero AP ;
it essentially combines AC and AP . We also re-
port Sentence Coverage (SC), i.e., how many dis-
tinct opinion sentences can be covered by the se-
lected aspects and Conditional Entropy (H), i.e.,
how well the selected aspects align with the nat-
ural clusters in the opinions; a smaller H value
indicates a better alignment.
Results: We summarize the evaluation results in
Measures SC H AC AP AAP
PRESIDENTS
Random 503 1.9069 0.5140 0.0933 0.1223
Size-based 500 1.9656 0.3108 0.1508 0.0949
Opin Cover 746 1.8852 0.5463 0.0913 0.1316
Cond Ent. 479 1.7687 0.5770 0.0856 0.1552
CAMERAS
Random 55 1.6389 0.6554 0.0871 0.1271
Size-based 70 1.6463 0.6071 0.1077 0.1340
Opin Cover 82 1.5866 0.6998 0.0914 0.1564
Cond Ent. 70 1.5598 0.7497 0.0789 0.1574
Table 6: Evaluation Results for Aspect Selection
Table 6. In addition to the three methods de-
scribed in Section 3.1, we also include one base-
line of averaging 10 runs of random selection. The
best performance by each measure on each data
set is highlighted in bold font. Not surprisingly,
opinion coverage-based approach has the best
sentence coverage (SC) performance and condi-
tional entropy-based greedy algorithm achieves
the lowest H . Size-based approach is best in as-
pect precision but at the cost of lowest aspect cov-
erage. The trade-off between AP and AC is com-
parable to that between precision and recall as
in information retrieval while AAP summarizes
the combination of these two. The greedy algo-
rithm based on conditional entropy outperforms
all other approaches in AC and also in AAP , sug-
gesting that it can provide a good balance between
AP and AC.
739
Supt Richard-Nixon Supt Abraham-Lincoln
50 Date of birth: 419 Government Positions Held:
- Jan 9, 1913 - United States Representative Mar 4,1847-Mar 3,1849
108 Tracks Recorded: 558 Military Commands:
- 23-73 Broadcast: End of the Vietnam War - American Civil War - United States of America
120 Works Written About This Topic: 810 Quotations: - Nearly all men can stand adversity, but if
- Watergate you want to test a man?s character, give him power.
Table 4: Comparison of Aspect Selection for Two Presidents (aligned opinions are omitted here)
Suggested Phrases Supporting Opinion Sentences
Abraham Lincoln Presidential Library CDB projects include the Abraham Lincoln Presidential Library and Museum
Abraham Lincoln Memorial ..., eventually arriving at Abraham Lincoln Memorial.
John Wilkes Booth John Wilkes Booth shoots President Abraham Lincoln at Ford?s Theatre ...
Table 5: New Phrases for Abraham Lincoln
4.4 Evaluation of Aspect Ordering
Human Annotation: In order to quantitatively
evaluate the effectiveness of aspect ordering, we
conduct user studies to establish gold standard or-
dering. Three users were each given k selected as-
pects and asked to perform two tasks for each US
President: (1) identify clusters of aspects that are
more natural to be presented together (cluster con-
straints) and (2) identify aspect pairs where one
aspect is preferred to appear before the other from
the viewpoint of readability. (order constraints).
We did not ask them to provide a full order of
the k aspects, because we suspect that there are
usually more than one ?perfect? order. Instead,
identifying partial orders or constraints is easier
for human to perform, thus provides more robust
gold standard.
Human Agreement: After obtaining the human
annotation results, we first study human consen-
sus on the ordering task. For both types of human
identified constraints, we convert them into pair-
wise relations of aspects, e.g., ?Ai and Aj should
be presented together? or ?Ai should be displayed
before Aj?. Then we calculate the agreement per-
centage among the three users. In Table 7, we can
see that only a very small percentage of pair-wise
partial orders (15.92% of the cluster constraints
and none of the order constraints) are agreed by
all the three users, though the agreement of clus-
tering is much higher than that of ordering. This
indicates that ordering the aspects is a subjective
and difficult task.
Measures: Given the human generated gold stan-
dard of partial constraints, we use the follow-
ing measures to evaluate the automatically gen-
AgreedBy Cluster Constraint Order Constraint
1 37.14% 89.22%
2 46.95% 10.78%
3 15.92% 0.00%
Table 7: Human Agreement on Ordering
erated full ordering of aspects: (1) Cluster Pre-
cision (prc): for all the aspect pairs placed in
the same cluster by human, we calculate the per-
centage of them that are also placed together in
the system output. (2) Cluster Penalty (pc): for
each aspect pair placed in the same cluster by hu-
man, we give a linear penalty proportional to the
number of aspects in between the pair that the
system places; pc can be interpreted as the aver-
age number of aspects between aspect pairs that
should be presented together in the case of mis-
ordering. Smaller penalty corresponds to better
ordering performance. (3) Order Precision (pro):
the percentage of correctly predicted aspect pairs
compared with human specified order.
Results: In Table 8, we report the ordering
performance based on two selection algorithms:
opinion coverage-based and conditional entropy-
based. Different selection algorithms provide dif-
ferent subsets of aspects for the ordering algo-
rithms to operate on. For comparison with our
coherence-based ordering algorithm, we include a
random baseline and Freebase ontology ordering.
Note that Freebase order is a very strong baseline
because it is edited by human even though the pur-
pose was not for organizing opinions. To take into
account the variation of human annotation, we use
four versions of gold standard: three are from the
individual annotators and one from the union of
their annotation. We did not include the gold stan-
740
Selection Gold Cluster Precision (prc) Cluster Penalty (pc) Order Precision (pro)
Algo STD Random Freebase Coherence Random Freebase Coherence Random Freebase Coherence
Opin Cover 1 0.3290 0.9547 0.9505 1.8798 0.1547 0.1068 0.4804 0.7059 0.4510
Opin Cover 2 0.3266 0.9293 0.8838 1.7944 0.3283 0.1818 0.4600 0.4000 0.4000
Opin Cover 3 0.2038 0.4550 0.4417 2.5208 1.3628 1.7994 0.5202 0.4561 0.5263
Opin Cover union 0.3234 0.7859 0.7237 1.8378 0.6346 0.4609 0.4678 0.4635 0.4526
Cond Entropy 1 0.2540 0.9355 0.8978 2.0656 0.2957 0.2016 0.5106 0.7111 0.5444
Cond Entropy 2 0.2535 0.7758 0.8323 2.1790 0.7530 0.5222 0.4759 0.6759 0.5093
Cond Entropy 3 0.2523 0.4030 0.5545 2.3079 2.1328 1.1611 0.5294 0.7143 0.8175
Cond Entropy union 0.3067 0.7268 0.7488 1.9735 1.0720 0.7196 0.5006 0.6500 0.6833
Table 8: Evaluation Results on Aspect Ordering
dard that is the intersection of three annotators be-
cause that would leave us with too little overlap.
We have several observations: (1) In general, re-
sults show large variations when using different
versions of gold standard, indicating the subjec-
tive nature of the ordering task. (2) Coherence-
based ordering shows similar performance to
Freebase order-based in cluster precision (prc),
but when we take into consideration the distance-
based penalty (pc) of separating aspects pairs in
the same cluster, coherence-based ordering is al-
most always significantly better except in one
case. This shows that our method can effectively
learn the coherence of aspects based on how their
aligned opinion sentences are presented in online
discussions. (3) Order precision (pro) can hardly
distinguish different ordering algorithm. This in-
dicates that people vary a lot in their preferences
as which aspects should be presented first. How-
ever, in cases when the random baseline outper-
forms others the margin is fairly small, while
Freebase order and coherence-based order have a
much larger margin of improvement when show-
ing superior performance.
5 Conclusions and Future Work
A major challenge in automatic integration of
scattered online opinions is how to organize all
the diverse opinions in a meaningful way for any
given topic. In this paper, we propose to solve this
challenge by exploiting related aspects in struc-
tured ontology which are guaranteed to be mean-
ingful and well connected to the topic. We pro-
posed three different methods for selecting a sub-
set of aspects from the ontology that can best
capture the major opinions, including size-based,
opinion coverage-based, and conditional entropy-
based methods. We also explored two ways to
order aspects, i.e., ontology-order and coherence
optimization. In addition, we also proposed ap-
propriate measures for quantitative evaluation of
both aspect selection and ordering.
Experimental evaluation on two data sets (US
President and Digital Cameras) shows that by ex-
ploiting structured ontology, we can generate in-
teresting aspects to organize scattered opinions.
The conditional entropy method is shown to be
most effective for aspect selection, and the coher-
ence optimization method is more effective than
ontology-order in optimizing the coherence of the
aspect ordering, though ontology-order also ap-
pears to perform reasonably well. In addition, by
extracting salient phrases from the major opinions
that cannot be covered well by any aspect in an
existing ontology, we can also discover interest-
ing new aspects to extend the existing ontology.
Complementary with most existing summariza-
tion work, this work proposes a new direction of
using structured information to organize and sum-
marize unstructured opinions, opening up many
interesting future research directions. For in-
stance, in order to focus on studying aspect selec-
tion and ordering, we have not tried to optimize
sentences matching with aspects in the ontology;
it would be very interesting to further study how
to accurately retrieve sentences matching each as-
pect. Another promising future work is to orga-
nize opinions using both structured ontology in-
formation and well-written overview articles.
Acknowledgment
We thank the anonymous reviewers for their use-
ful comments. This paper is based upon work sup-
ported in part by an IBM Faculty Award, an Alfred
P. Sloan Research Fellowship, an AFOSR MURI
Grant FA9550-08-1-0265, and by the National
Science Foundation under grants IIS-0347933,
IIS-0713581, IIS-0713571, and CNS-0834709.
741
References
Carenini, Giuseppe, Raymond T. Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In K-CAP ?05: Proceedings of the 3rd international
conference on Knowledge capture, pages 11?18,
New York, NY, USA. ACM.
Chen, Hao and Susan Dumais. 2000. Bringing or-
der to the web: automatically categorizing search
results. In CHI ?00: Proceedings of the SIGCHI
conference on Human factors in computing systems,
pages 145?152, New York, NY, USA. ACM.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Gamon, Michael, Anthony Aue, Simon Corston-
Oliver, and Eric K. Ringger. 2005. Pulse: Min-
ing customer opinions from free text. In Famili,
A. Fazel, Joost N. Kok, Jose? Mar??a Pen?a, Arno
Siebes, and A. J. Feelders, editors, IDA, volume
3646 of Lecture Notes in Computer Science, pages
121?132. Springer.
Ka?ki, Mika. 2005. Optimizing the number of search
result categories. In CHI ?05: CHI ?05 extended
abstracts on Human factors in computing systems,
pages 1517?1520, New York, NY, USA. ACM.
Leouski, Anton V. and W. Bruce Croft. 1996. An eval-
uation of techniques for clustering search results.
Technical report.
Liu, Bing, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In WWW ?05: Proceedings of the
14th international conference on World Wide Web,
pages 342?351, New York, NY, USA. ACM.
Lu, Yue and Chengxiang Zhai. 2008. Opinion in-
tegration through semi-supervised topic modeling.
In Huai, Jinpeng, Robin Chen, Hsiao-Wuen Hon,
Yunhao Liu, Wei-Ying Ma, Andrew Tomkins, and
Xiaodong Zhang, editors, WWW, pages 121?130.
ACM.
Mei, Qiaozhu, Chao Liu, Hang Su, and ChengXiang
Zhai. 2006. A probabilistic approach to spatiotem-
poral theme pattern mining on weblogs. In WWW
?06: Proceedings of the 15th international confer-
ence on World Wide Web, pages 533?542.
Mei, Qiaozhu, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In Berkhin, Pavel, Rich Caruana, and Xin-
dong Wu, editors, KDD, pages 490?499. ACM.
Pang, Bo and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
Popescu, Ana-Maria and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In HLT ?05, pages 339?346, Morristown, NJ, USA.
Association for Computational Linguistics.
Porter, M. F. 1997. An algorithm for suffix stripping.
pages 313?316.
Sauper, Christina and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 208?216,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Titov, Ivan and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
WWW ?08: Proceeding of the 17th international
conference on World Wide Web, pages 111?120,
New York, NY, USA. ACM.
Zeng, Hua-Jun, Qi-Cai He, Zheng Chen, Wei-Ying
Ma, and Jinwen Ma. 2004. Learning to cluster
web search results. In SIGIR ?04: Proceedings
of the 27th annual international ACM SIGIR con-
ference on Research and development in informa-
tion retrieval, pages 210?217, New York, NY, USA.
ACM.
Zhai, Chengxiang and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of CIKM
2001, pages 403?410.
Zhao, Jing and Jing He. 2006. Learning to generate
labels for organizing search results from a domain-
specified corpus. In WI ?06: Proceedings of the
2006 IEEE/WIC/ACM International Conference on
Web Intelligence, pages 390?396, Washington, DC,
USA. IEEE Computer Society.
742
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1511?1521, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Discriminative Model for Query Spelling Correction with Latent
Structural SVM
Huizhong Duan, Yanen Li, ChengXiang Zhai and Dan Roth
University of Illinois at Urbana-Champaign
201 N Goodwin Ave
Urbana, IL 61801
{duan9, yanenli2, czhai, danr}@illinois.edu
Abstract
Discriminative training in query spelling cor-
rection is difficult due to the complex inter-
nal structures of the data. Recent work on
query spelling correction suggests a two stage
approach a noisy channel model that is used
to retrieve a number of candidate corrections,
followed by discriminatively trained ranker
applied to these candidates. The ranker, how-
ever, suffers from the fact the low recall of the
first, suboptimal, search stage.
This paper proposes to directly optimize the
search stage with a discriminative model
based on latent structural SVM. In this model,
we treat query spelling correction as a multi-
class classification problem with structured in-
put and output. The latent structural informa-
tion is used to model the alignment of words
in the spelling correction process. Experiment
results show that as a standalone speller, our
model outperforms all the baseline systems. It
also attains a higher recall compared with the
noisy channel model, and can therefore serve
as a better filtering stage when combined with
a ranker.
1 Introduction
Query spelling correction has become a crucial com-
ponent in modern information systems. Particularly,
search engine users rely heavily on the query cor-
rection mechanism to formulate effective queries.
Given a user query q, which is potentially mis-
spelled, the goal of query spelling correction is to
find a correction of the query c that could lead to a
better search experience. A typical query spelling
correction system employs a noisy channel model
(Kernighan et al 1990). The model assumes that
the correct query c is formed in the user?s mind be-
fore entering the noisy channels, e.g., typing, and
get misspelled. Formally, the model maximizes the
posterior probability p(c|q):
c? = arg max
c
p(c|q). (1)
Applying Bayes rule, the formulation can be
rewritten as:
c? = arg max
c
p(q|c)p(c)
= arg max
c
[log p(q|c) + log p(c)].
(2)
The model uses two probabilities. The prior prob-
ability p(c) represents how likely it is that c is the
original correct query in the user?s mind. The prob-
ability is usually modeled by a language model es-
timated from a sizable corpus. The transformation
probability p(q|c) measures how likely it is that q is
the output given that c has been formed by the user.
This probability can be either heuristic-based (edit
distance) or learned from samples of well aligned
corrections. One problem with the noisy channel
model is that there is no weighting for the two kinds
of probabilities, and since they are estimated from
different sources, there are usually issues regarding
their scale and comparability, resulting in subopti-
mal performance (Gao et al 2010). Another limita-
tion of this generative model is that it is not able to
take advantage of additional useful features.
1511
A discriminative model may solve these problems
by adding the flexibility of using features and apply-
ing weights. But training such a model is not easy.
The difficulty is that the output space of query cor-
rection is enormous, as the candidate corrections for
each a query term could be the entire vocabulary.
This is even worse when word boundary errors (i.e.
merging and splitting of words) exist. The problem
is intractable with standard discriminative models as
we cannot enumerate every candidate correction.
To solve the problem, (Gao et al 2010) proposed
a two stage approach. In this approach, a ranker is
trained to score each candidate correction of a query.
When a query is issued, the system first uses the
noisy channel model with a standard search algo-
rithm to find the 20 best candidates. Then the ranker
is used to re-rank these candidates and find the best
correction for the query. This ranker based system
has one critical limitation, though. Since the ranking
stage is decoupled from the search, it relies on the
outsourced search algorithm to find the candidates.
Because query spelling correction is an online oper-
ation, only a small number of candidates can enter
the ranker due to efficiency concerns, thus limiting
the ability of the ranker to the ceiling of recall set by
the suboptimal search phase.
The research question we address here is whether
we can directly optimize the search phase of query
spelling correction using a discriminative model
without loss of efficiency. More specifically, we
want 1) a learning process that is aware of the
search phase and interacts with its result; 2) an ef-
ficient search algorithm that is able to incorporate
the learned model and guide the search to the target
spelling correction.
In this paper, we propose a new discriminative
model for query correction that maintains the ad-
vantage of a discriminative model in accommodat-
ing flexible combination of features and naturally in-
corporates an efficient search algorithm in learning
and inference. Similarly to (Chang et al 2010) we
collapse a two stage process into a single discrim-
inatively trained process, by considering the output
of the first stage as an intermediate latent represen-
tation for the joint learning process. Specifically, we
make use of the latent structural SVM (LS-SVM)
(Yu and Joachims, 2009) formulation. We formu-
late the problem query spelling correction as a multi-
class classification problem on structured inputs and
outputs. The advantage of the structural SVM model
is that it allows task specific, customizable solutions
for the inference problem. This allows us to adapt
the model to make it work directly with the search
algorithm we use for finding the best correction of
the query. To account for word boundary errors, we
model the word alignment between the query and
the correction as a latent structural variable. The
LS-SVM model allows us to jointly search over the
output space and the latent structure space.
As the inference algorithm in the proposed dis-
criminative model we use an algorithm that resem-
bles a traditional noisy channel model. To adapt
the LS-SVM model to enable the efficient search of
query spelling correction, we study how features can
be designed. We analyze the properties of features
that can be used in the search algorithm and propose
a criteria for selecting and designing new features.
We demonstrate the use of the criteria by design-
ing separate features for different types of spelling
errors (e.g. splitting, merging). With the proposed
discriminative model, we can directly optimize the
search phase of query spelling correction without
loss of efficiency. Our model can be used not only as
a standalone speller with high accuracy, but also as
a high recall candidate generation stage for a ranker
based system.
Experiments verify the effectiveness of the dis-
criminative model, as the accuracy of correction can
be improved significantly over baseline systems in-
cluding an award winning query spelling system.
Even though the optimization is primarily based on
the top correction, the weights trained by LS-SVM
can be used to search for more candidate corrections.
The improvement in recall at different levels over the
noisy channel model demonstrates that our model is
superior even when used in the two-stage approach..
2 Related Work
Spelling correction has a long history (Levenshtein,
1966). Traditional techniques were on small scale
and depended on having a small trusted lexicons
(Kukich, 1992). Later, statistical generative mod-
els were shown to be effective in spelling correc-
tion, where a source language model and an er-
ror model were identified as two major components
1512
(Brill and Moore, 2000). Note that we are not deal-
ing here with the standard models in context sen-
sitive spelling (Golding and Roth, 1999) where the
set of candidate correction is a known ?confusion
set?. Query spelling correction, a special form of
the problem, has received much attention in recent
years. Compared with traditional spelling correc-
tion task, query spelling deals with more complex
types of misspellings and a much larger scale of lan-
guage. Research in this direction includes utiliz-
ing large web corpora and query log (Chen et al
2007; Cucerzan and Brill, 2004; Ahmad and Kon-
drak, 2005), employing large-scale n-gram models,
training phrase-based error model from clickthrough
data (Sun et al 2010) and developing additional fea-
tures (Gao et al 2010).
Query alteration/refinement is a very relevant
topic to query spelling correction. The goal of
query alteration/refinement is to modify the inef-
fective query so that it could . Researches on this
track include query expansion (Xu and Croft, 1996;
Qiu and Frei, 1993; Mitra et al 1998), query con-
traction(Kumaran and Allan, 2008; Bendersky and
Croft, 2008; Kumaran and Carvalho, 2009) and
other types of query reformulations for bridging the
vocabulary gap (Wang and Zhai, 2008). (Guo et al
2008) proposed a unified model to perform a broad
set of query refinements including correction, seg-
mentation and stemming. However, it has very lim-
ited ability in query correction. In this paper, we
study the discriminative training of query spelling
correction, which is potentially beneficial to many
existing studies.
Noisy channel model (or source channel model)
has been widely used in NLP. Many approaches have
been proposed to perform discriminative training of
the model (McCallum et al 2000; Lafferty, 2001).
However, these approaches mostly deal with a rela-
tively small search space where the number of can-
didates at each step is limited (e.g. POS tagging). A
typically used search algorithm is dynamic program-
ming. In spelling correction, however, the search
space is much bigger and the existing approaches
featuring dynamic programming are difficult to be
applied.
Structural learning and latent structural learning
has been studied a lot in NLP in recent years(Chang
et al 2010; Dyer et al 2011), and has been
shown to be useful in a range of NLP applications
from Textual Entailment, Paraphrasing and Translit-
eration (Chang et al 2010) to sentiment analysis
(Yessenalina et al 2010).
Work has also been done on integrating discrimi-
native learning in search. Freitag and Khadivi used a
perceptron algorithm to train for sequence alignment
problem. A beam search algorithm was utilized in
the search (Freitag and Khadivi, 2007). Daume et
al. proposed the Searn framework for search based
structural prediction (Daume et al 2009). Our
model differs from the Searn framework in that it
learns to make global decisions rather than accumu-
lating local decisions. The global decision was made
possible by an efficient search algorithm.
Query spelling correction also shares many sim-
ilarities with statistical machine translation (SMT).
Sun et al(2010) has formulated the problem within
an SMT framework. However, SMT usually in-
volves more complex alignments, while in query
spelling correction search is the more challenging
part. Our main contribution in this paper is a novel
unified way to directly optimize the search phase of
query spelling correction with the use of LS-SVM.
3 Discriminative Model for Query Spelling
Correction Based on LS-SVM
In this section, we first present the discriminative
formulation of the problem of query spelling correc-
tion. Then we introduce in detail the model we use
for solving the problem.
3.1 The Discriminative Form of Query Spelling
Correction
In query spelling correction, given a user entered
query q, which is potentially misspelled, the goal is
to find a correction c, such that it could be a more
effective query which improves the quality of search
results. A general discriminative formulation of the
problem is of the following form:
f(q) = arg max
c?V?
[w ??(q, c)], (3)
where ?(q, c) is a vector of features and w is the
model parameter. This discriminative formulation is
more general compared to the noisy channel model.
It has the flexibility of using features and applying
1513
weights. The noisy channel model is a special case
of the discriminative form where only two features,
the source probability and the transformation proba-
bility, are used and uniform weightings are applied.
However, this problem formulation does not give us
much insight on how to proceed to design the model.
Especially, it is unclear how ?(q, c) can be com-
puted.
To enhance the formulation, we explore the fact
that spelling correction follows a word-by-word pro-
cedure. Let us first consider a scenario where word
boundary errors does not exist. In this scenario,
each query term matches and only matches to a sin-
gle term in the correction. Formally, let us denote
q = q1, ..., qn and c = c1, ..., cm as structured ob-
jects from the space of V?, where V is our vocabu-
lary of words and V? is all possible phrases formed
by words in V . Both q and c have an intrinsic se-
quential structure. When no word boundary error
exists, |c| = |q| holds for any candidate correction
c. qi and ci establish a one-to-one mapping. In this
case, we have a more detailed discriminative form:
f(q) = arg max
c?V|q|
[w ? (?0 +
|q|?
i=1
?1(qi, ci))], (4)
where ?0 is a vector of normalizing factors,
?1(qi, ci) is the decomposed computation of ?(q, c)
for each query term qi and ci, for i = 1 to |q|.
Equation 4 is a clearer formulation. The major
challenge of solving this discriminative problem is
the complexity. Theoretically, each term has |V|
candidates and it is impossible to enumerate over
all possible combinations. To make it even worse,
merging and splitting errors are quite common in
misspelling. As a result, the assumption of one-to-
one mapping does not hold in practice.
To account for these word boundary errors and
enhance the discriminative formulation, we intro-
duce a latent variable a to model the unobserved
structural information. More specifically, a =
a1, a2, ...a|a| is the alignment between q and c. Each
alignment node at is a represented by a quadruple
(qstart, qend, cstart, cend). Figure 1 shows a com-
mon merge error and its best alignment. The phrase
?credit card?, in this case, is incorrectly merged into
one word ?creditcard? by the user. Figure 2 shows
Figure 1: Example of Merge Error and Alignment
Figure 2: Example of Split Error and Alignment
the best alignment for a common split error, where
the word ?gamespot? is incorrectly split into a two
word phrase ?game spot?.
Taking into consideration the latent variable, we
arrive at our final discriminative form of query
spelling correction:
f(q) = arg max(c,a)?Vn?A[w ??(q, c, a)]
= arg max(c,a)?V??A[w ? (?0
+
?|a|
t=0 ?1(qat , cat , at))],
(5)
The challenges of successfully applying a dis-
criminative model to this problem formulation are
1) how can we design a learning algorithm to learn
the model parameter w to directly optimize the max-
imization problem; 2) how can we solve the maxi-
mization efficiently without having to enumerate all
candidates; 3) how can we design features to guar-
antee the correctness of the search algorithm. In the
following subsections we introduce our solutions to
the three challenges in detail.
3.2 Latent Structural SVM
We employ the latent structural SVM (LS-SVM)
model for learning the discriminative model of query
spelling correction. LS-SVM is a large margin
method that deals with structured prediction prob-
lems with latent structural information (Yu and
Joachims, 2009). LS-SVM has the merit of allowing
1514
task specific, customizable solutions for the infer-
ence problem. This makes it easy to adapt to learn-
ing the model parameters for different problems.
The following is a brief introduction of LS-SVM
that largely mirrors the work by (Yu and Joachims,
2009).
Without loss of generality, let us aim at learning
a prediction function f : X ? Y that maps input
x ? X to an output y ? Y with latent structural
information h ? H. The decision function is of the
following form:
f(x) = arg max
(y,h)?Y?H
[w ??(x, y, h)], (6)
where ?(x, y, h) is the set of feature functions de-
fined jointly over the input x, the output y and the
latent variable h. w is the parameter of the model.
Given a set of training examples that consist of input
and output pairs {(x1, y1), ...(xn, yn)} ? (X ?Y)n,
the LS-SVM method solves the following optimiza-
tion problem:
minw
1
2
?w?2
+C
n?
i=1
max
(y?,h?)?Y?H
[w ??(xi, y?, h?) + ?(yi, y?)]
?C
n?
i=1
max
h?H
[w ??(xi, yi, h)],
(7)
where ?(yi, y?) is the loss function for the ith ex-
ample. The details of the derivation is omitted in
this paper. Readers who are interested can read more
from (Yu and Joachims, 2009).
There are two maximization problems that are es-
sential in Equation 7. The first one is the loss aug-
mented decision function:
max
(y?,h?)?Y?H
[w ??(xi, y?, h?) + ?(yi, y?)], (8)
and the second is the inference of latent variable
given the label of the training data:
max
h?H
[w ??(xi, yi, h)]. (9)
The Latent Structural SVM framework does not
specify how the maximization problems in Equation
8 and Equation 9 are solved, as well as the infer-
ence problem in 6. These maximization problems
are task dependent. Being able to efficiently solve
them is the key to successfully applying the Latent
Structural SVM method. We will show in detail how
we solve these maximization problems to make LS-
SVM work for query spelling correction in the fol-
lowing subsection.
For training the LS-SVM model, a Concave-
Convex Procedure (CCCP) was proposed to solve
this optimization problem (Yu and Joachims, 2009).
The method resembles the Expect-Maximization
(EM) training method as it updates the model by it-
eratively recomputing the latent variable. However,
rather than performing ?sum-product? training as in
EM where a distribution over the hidden variable is
maintained, the CCCP method used for LS-SVM is
more similar to the ?max-product? paradigm where
we ?guess? the best hidden variable in each iteration,
except here we ?guess? by minimizing a regularized
loss function instead of maximizing the likelihood.
3.3 Solving the Inference Problems
The essential inference problem is to find the correc-
tion that maximizes the scoring function according
to the model (i.e., the decision function in Equation
6). For this purpose we design a best first search al-
gorithm similar to the standard search algorithm in
the noisy channel model. The essence of the search
algorithm is to bound the score of each candidate
so that we could evaluate the most promising candi-
dates first. The algorithm is given in Algorithm 1.
Essentially, the algorithm maintains a priority
queue of all search paths. Each time the best path is
de-queued, it is expanded with up to m ? 1 words
in q by searching over a vocabulary trie of up to
m-gram. Each path is represented as a quadruple
(pos, str, sc, a), representing the current term posi-
tion in query, the string of the path, the path?s score
and the alignment so far. The priority queue is sorted
according to the score of each path in descending or-
der. The GetSuggestions() function retrieves the
top n similar words to the given word with a vocab-
ulary trie according to an error model.
Splitting errors are dealt with in Algorithm 1 by
?looking forward? m words in the query when gen-
erating candidate words. Merging errors are ac-
counted for by including up to m-gram in the vocab-
1515
ulary trie. It is worth mentioning that performance
of Algorithm 1 could be further improved by com-
puting heuristic scores for each path.
Algorithm 1: Best First Search Algorithm
Input: Vocabulary Trie V , query q, output size k,
max order m, candidate pool size n
Output: List l of top k corrections for q
1 Initialize List l;
2 Initialize PriorityQueue pq;
3 Enqueue to pq a start path with position set to 0,
string set to empty string, score set to w ??0, and
path alignment set to empty set;
4 while pq is not Empty do
5 Path pi ? pq.Dequeue();
6 if pi.pos < q.terms.length then
7 for i? 0 tom do
8 ph? q.terms[pi.pos+ 1...pi.pos+ i];
9 sug ? GetSuggestions(ph, V, n);
10 foreach s in sug do
11 pos? ? pi.pos+ i;
12 str? ? concat(pi.str, s.str);
13 a? ? pi.a ? s.a;
14 sc? ? pi.sc+w ??1(qs.a, cs.a, s.a);
15 Enqueue pq with the new path
(pos?, str?, sc?, a?);
16 else
17 Add suggestion string pi.str to l;
18 if l.Count > k then return l;
19 return l;
As Algorithm 1 originates from the noisy channel
model, the two known features that work with the
algorithm are log p(c) and log p(q|c) from the noisy
channel model. However, it is unknown whether
other features can work with the search algorithm
and how we can develop new features to ensure it.
After analyzing the properties of the features and the
search algorithm, we find that a feature ? has to sat-
isfy the following monotonicity constraint in order
to be used in Algorithm 1.
Monotonicity Property. Given query q, for
any alignment At = At?1 ? {at} at time t,
?(qAt , cAt , At) ? ?(qAt?1 , cAt?1 , At?1), where
qAt is the concatenation of qa0 to qat and cAt is the
concatenation of ca0 to cat .
That is, the value of the feature (which is com-
puted in an accumulative manner) cannot increase
as the candidate is extended with a new term at
any search step. This ensures that the score of the
best candidate at any search step is guaranteed to be
higher than the score of any future candidates. It
also implies ?t(qat , cat , at) ? 0 for any t ? T . The
monotonicity feature ensures the correctness of Al-
gorithm 1. We show how we design features with
the guidance of the monotonicity constraint in Sec-
tion 4.
The solution to to the loss augmented inference
depends on the loss function we use. In spelling cor-
rection, usually only one correction is valid for an
input query. Therefore, we apply the 0-1 loss to our
model:
?(c, c?) =
{
0 c = c?
1 c 6= c?
(10)
Given this loss function, the loss augmented infer-
ence problem can be solved easily with an algorithm
similar to Algorithm 1. This is done by initializing
the loss to be 1 at the beginning of each search path.
During the search procedure, we check if the loss
decreases to 0 given the correction string so far. If
this is the case, we decreases the score by 1 and add
the path back to the priority queue. More advanced
functions may also be used (Dreyer et al 2006),
which may lead to better training performance. We
plan to further study different loss functions in our
future work.
The inference of the latent alignment variable can
be solved with dynamic programming, as the num-
ber of possible alignments is limited given the query
and the correction.
4 Features
In the following discussions, we will describe how
the features in our discriminative model are devel-
oped under the guidance of the monotonicity con-
straint.
4.1 Source Probability and Transformation
Probability
We know from empirical experience that the source
probability and the transformation probability are
the two most important features in query spelling
correction. We include them in our model in a nor-
malized form. Taking the source probability for ex-
ample, we define the following feature:
1516
?(q, c, a) = ?+
?|a|
1 log p(c)
?
= 1 +
?|a|
1
log p(c)
? ,
(11)
where ? is a normalizing factor computed as:
? = ?|q| log pmin, (12)
where pmin is the smallest probability we use in
practice.
The formula fits the general form we define in 5
in that ?0 = 1 and ?1(qat , cat , at) =
log p(c)
? for any
t = 1 to |a|.
Similarly, we have the follow feature for the trans-
formation probability:
??(q, c, a) = ?+
?|a|
1 log p(q|c)
?
= 1 +
?|a|
1
log p(q|c)
? .
(13)
We use the web Microsoft n-gram model1 to com-
pute source model p(c). We train the unigram trans-
formation model for the transformation probability
p(q|c) according to (Duan and Hsu, 2011).
In generative models, we treat transformation
probabilities from merging and splitting errors in the
same way as single word errors. In our discrimi-
native model we can assign separate weight to the
transformation probabilities resulted from different
types of errors. This allows fine tuning of the query
spelling correction system, making it more adaptive
to environments where the ratio of different types of
errors may vary. Moreover, the model also allows
us to include language models trained over different
resources, such as query log, title of webpages or
anchor texts.
4.2 Local Heuristic Features
Despite the goal of query spelling correction is to
deal with misspellings, in real world most queries
are correctly spelled. A good query spelling correc-
tion system shall prevent as much as possible from
misjudging an correctly spelled query as misspelled.
With this idea in mind, we invent some heuristic
functions to avoid misjudging.
1http://research.microsoft.com/en-
us/collaboration/focus/cs/web-ngram.aspx
Local Heuristic 1. When a query term is matched
against trustable vocabulary, it increases the chance
that the term is already in its correct form. For ex-
ample, we extract a reliable vocabulary from the title
field of Wikipedia2. We therefore design the follow-
ing feature:
?(q, c, a) = 1 +
|a|?
t=1
?1(qat , cat , at), (14)
where ?1(qat , cat , at) is defined as:
?1(qat , cat , at) =
?
?
?
0 qat /? W
0 qat ? W, qat = ct
? 1|q| qat ? W, qat 6= cat
(15)
where W is the vocabulary of Wikipedia titles.
Since |q| > |a| always holds, the feature is normal-
ized between 0 and 1.
Local Heuristic 2. Another heuristic is that
words with numbers in it, despite usually not in-
cluded in any vocabulary, should be treated care-
fully as they tend to be correct words. Such words
could be a model, a serial number or a special en-
tity name. Since the number keys on keyboard are
away from the letter keys, they are more likely to be
intentionally typed in if found in user queries. Simi-
lar to Heuristic 1, we design the following feature to
capture this heuristic:
??(q, c, a) = 1 +
|a|?
t=1
??1(qat , cat , at), (16)
where ??1(qat , cat , at) is defined as:
??1(qat , cat , aat) =
?
?
?
0 [0...9] /? qat
0 [0...9] ? qat , qat = cat
? 1|q| [0...9] ? qat , qat 6= cat
(17)
4.3 Global Heuristic Features
Some global heuristics are also important in query
spelling correction. For instance, the total number
2http://www.wikipedia.org
1517
of words being corrected in the query may be an
indicator of whether the system has leaned towards
overcorrecting. To account for this global heuristic,
we design the following feature:
?(q, c, a) =
{
1 wc(q, c, a) < wcmax
0 otherwise
(18)
where wc(q, c, a) is the number of word changes
at step t, wcmax is the maximum number of word
changes we allow in our system (in a soft way). Sim-
ilarly, other thresholded features can be designed
such as the number of total edit operations. The use
of global features is similar to the use of loss func-
tion in the search algorithm.
5 Experiments
In order to test the effectiveness and efficiency of our
proposed discriminative training method, in this sec-
tion we conduct extensive experiments on two web
query spelling datasets. Below we first present the
dataset and evaluation metrics, followed by the ex-
periment results on query spelling correction.
5.1 Dataset Preparation
The experiments are conducted on two query
spelling correction datasets. One is the TREC
dataset based on the publicly available TREC
queries (2008 Million Query Track). This dataset
contains 5892 queries and the corresponding correc-
tions annotated by the MSR Speller Challenge 3 or-
ganizers. There could be more than one plausible
corrections for a query. In this dataset only 5.3% of
queries are judged as misspelled.
We have also annotated another dataset that con-
tains 4926 MSN queries, where for each query there
is at most one correction. Three experts are involved
in the annotation process. For each query, we con-
sult the speller from two major search engines (i.e.
Google and Bing). If they agree on the returned
results (including the case if the query is just un-
changed), we take it as the corrected form of the in-
put query. If the results are not the same from the
two, as least one human expert will manually anno-
tate the most likely corrected form of the query. Fi-
nally, about 13% of queries are judged as misspelled
3http://web-ngram.research.microsoft.com/spellerchallenge/
in this dataset, which is close to the error rate of real
web queries. We?ve made this dataset publicly avail-
able to all researchers4.
Both the two datasets are split randomly into two
equal subsets for training and testing.
5.2 Evaluation Metrics
We evaluate our system based on the evaluation met-
rics proposed in Microsoft Speller Challenge, in-
cluding expected precision, expected recall and ex-
pected F1 measure.
Let q be a user query and C(q) = (c1, c2, , ck)
be the set of system output with posterior probabil-
ities P (ci|q). Let S(q) denote the set of plausible
spelling variations annotated by the human experts
for q. Expected Precision is computed as:
Precision =
1
|Q|
?
q?Q
?
c?C(q)
Ip(c, q)P (c|q), (19)
where Ip(c, q) = 1 if c ? S(q), and 0 otherwise.
And expected recall is defined as:
Recall =
1
|Q|
?
q?Q
?
a?S(q)
Ir(C(q), a)/|S(q)|, (20)
where Ir(C(q), a) = 1 if a ? C(q) for a ? S(q),
and 0 otherwise. We use R@N to denote recall for
systems limited to output top N corrections.
Expected F1 measure can be computed as:
F1 =
2 ? precision ? recall
precision+ recall
(21)
5.3 Experiment Results
Table 1 compares the performance of our LS-SVM
based model with two strong baseline systems. The
first baseline system is an Echo system which sim-
ply echos the input. The echo system is usually con-
sidered as a strong baseline in query spelling cor-
rection as the majority of the queries are correctly
spelled queries. The second baseline Lueck-2011
we use is a award winning speller system5 (Luec,
2011), which was ranked at the first place in Mi-
crosoft Spelling Challenge 2011.
4http://times.cs.uiuc.edu/duan9/msn speller.tar.gz
5http://www.phraselink.com
1518
Table 1: LSSVM vs Baselines Serving as Standalone Speller
All Queries Misspelled Queries
Dataset Method Precision R@10 F1 Precision R@10 F1
Echo 0.949 0.876 0.911 0 0 0
TREC Lueck-2011 0.963 0.932 0.947 0.391 0.479 0.430
LS-SVM 0.955 0.944 0.949 0.331 0.678? 0.445?
Echo 0.869 0.869 0.869 0 0 0
MSN Lueck-2011 0.896 0.921 0.908 0.334 0.397 0.363
LS-SVM 0.903 0.953 0.928 0.353? 0.662? 0.461?
We show performances for the entire query sets
as well as the query sets consisting only the mis-
spelled queries. As we can see, our system out-
performs both baseline systems on almost all met-
rics, except the precision of Lueck-2011 is better
than ours on TREC dataset. We perform statistical
test and measures where our system shows statisti-
cal significant improvement over both baseline sys-
tems are noted by ?. It is theoretically impossible
to achieve statistical significance in the entire query
set as majority queries have almost identical perfor-
mance in different systems due to the large amount
of correct queries. But our method shows signifi-
cant improvement in the dealing with the misspelled
queries. This experiment verified the effectiveness
of our proposed discriminative model. As a stan-
dalone speller, our system achieves very high accu-
racy.
Despite we are primarily focused on optimizing
the top correction in our discriminative model, we
can also use the trained system to output multiple
candidate corrections. Table 2 compare our system
with the noisy channel model (N-C) in terms of re-
call at different levels of cutoff. For all levels, we see
that our system achieves higher recall than the noisy
channel model. This indicates that when used to-
gether with a secondary ranker, our system serves as
a better filtering method than the unoptimized noisy
channel model. Since the ranker makes use of arbi-
trary features, it has the potential of further improv-
ing the accuracy of query spelling correction. We
plan to further explore this idea as a future work.
In Table 3 we study the effect of treating the trans-
formation probability of merging and splitting er-
rors as separate features and including the local and
global heuristic features (rich features). We see that
Table 2: LS-SVM vs Noisy Channel Model Serving as
Filtering Method
Dataset Method R@5 R@10 R@20
TREC N-C 0.896 0.899 0.901
LS-SVM 0.923 0.944 0.955
MSN N-C 0.870 0.873 0.876
LS-SVM 0.950 0.953 0.960
the precision of query spelling correction can bene-
fits from the use of rich features. However, it does
not result in much improvement in recall. This is
reasonable as the additional features are primarily
designed to improve the accuracy of the top correc-
tion generated by the system. In doing so, it actu-
ally regularizes the ability of the system in retrieving
diversified results. For instance, the global heuris-
tic feature on the number of word change tries to
prevent the system from returning candidates hav-
ing more than a certain number of changed words.
For the TREC collection where more than one cor-
rections can be labeled for a query, this phenomena
is aggravated.
Table 3: LSSVM w/ and w/o Rich Features
Dataset Method Precision R@10 F1
TREC w/o 0.942 0.946 0.944
w/ 0.955 0.944 0.949
MSN w/o 0.898 0.952 0.924
w/ 0.903 0.953 0.928
6 Conclusions
In this paper, we present a novel discriminative
model for query spelling correction. The paper made
the following contributions:
1519
First, to the best of our knowledge, this is a novel
exploration of directly optimizing the search phase
in query spelling correction with a discriminative
model. By modeling word alignment as the latent
structural information, our formulation also deals
with word boundary errors. We propose to use LS-
SVM for learning the discriminative model which
naturally incorporates search in the learning process.
Second, we develop an efficient search algorithm
that solves the inference problems in the LS-SVM
based model. We analyze the criteria for selecting
and designing features to ensure the correctness and
efficiency of the search algorithm. Third, we explore
effective features to improve the accuracy of the
model. Finally, experiments are conducted to verify
the effectiveness of the proposed model. It is shown
that as a standalone speller our system achieves high
accuracy. When used in a two stage approach, it at-
tains higher recall than the noisy channel model and
can thus serve as a superior method for candidate
generation. We also verify that through the use of
rich features, we can further improve the accuracy
of our query spelling correction system.
7 Acknowledgments
This paper is based upon work supported in part by
MIAS, the Multimodal Information Access and Syn-
thesis center at UIUC, part of CCICADA, a DHS
Center of Excellence, and by the National Science
Foundation under grant CNS-1027965, and by a Mi-
crosoft grant.
References
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In HLT/EMNLP.
The Association for Computational Linguistics.
M. Bendersky and W. B. Croft. 2008. Discovering key
concepts in verbose queries. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?08. ACM, New York, NY, USA, 491-498.
E. Brill and R. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceed-
ings of the 38th Annual Meeting of the Association for
Computational Linguistics, Hong Kong.
M. Chang, D. Goldwasser, D. Roth and V. Srikumar.
2010. Discriminative Learning over Constrained La-
tent Representations. In Proceedings of NAACL.
Q. Chen, M. Li, and M. Zhou. 2007. Improving
query spelling correction using web search results. In
EMNLP-CoNLL, pages 181?189.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
H. Daume, J. Langford and D. Marcu. 2009. Search-
based Structured Prediction. Machine Learning Jour-
nal (MLJ).
M. Dreyer, D. Smith and N. Smith. 2006. Vine parsing
and minimum risk reranking for speed and precision.
In Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning. 201-205.
H. Duan and B.-J. P. Hsu. 2011. Online spelling correc-
tion for query completion. In Proceedings of the 20th
international conference on World wide web, WWW
?11, pages 117?126, New York, NY, USA.
C. Dyer, J. H. Clark, A. Lavie, and N. A. Smith. 2011.
Unsupervised Word Alignment with Arbitrary Fea-
tures. In Proceedings of ACL.
D. Freitag, S. Khadivi. 2007. A Sequence Alignment
Model Based on the Averaged Perceptron. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning. 238-247.
J. Gao, X. Li, D. Micol, C. Quirk, and X. Sun. 2010.
A large scale ranker-based system for search query
spelling correction. In COLING, pages 358?366.
A. R. Golding and D. Roth 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. In
Machine Learning, vol 34, pages 107?130.
J. Guo, G. Xu, H. Li, and X. Cheng. 2008. A unified and
discriminative model for query refinement. In Pro-
ceedings of the 31st annual international ACM SIGIR,
SIGIR ?08, pages 379?386, New York, NY, USA.
C. John Yu and T. Joachims. 2009. Learning structural
SVMs with latent variables. In Proceedings of the 26th
Annual International Conference on Machine Learn-
ing (ICML ?09). ACM, New York, NY, USA, 1169-
1176.
M. D. Kernighan , K. W. Church , W. A. Gale. 1990. A
spelling correction program based on a noisy channel
model. In Proceedings of the 13th conference on Com-
putational linguistics. 205-210. August 20-25, 1990,
Helsinki, Finland.
K. Kukich. 1992. Techniques for automatically correct-
ing words in text. ACM computing surveys, 24(4).
G. Kumaran and J. Allan. 2008. Effective and efficient
user interaction for long queries. In Proceedings of
the 31st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?08. ACM, New York, NY, USA.
1520
G. Kumaran and V. R. Carvalho. 2009. Reducing long
queries using query quality predictors. In Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?09. ACM, New York, NY, USA, 564-571.
J. Lafferty. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning (ICML ?01). 282?289.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet
Physics Doklady, 10(8), 707-710.
G. Luec. 2011. A data-driven approach for correcting
search quaries. In Spelling Alteration for Web Search
Workshop.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum Entropy Markov Models for Information Extrac-
tion and Segmentation. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML ?00). 591-598.
M. Mitra, A. Singhal, and C. Buckley. 1998. Improving
automatic query expansion. In Proceedings of the 21st
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?98.
Y. Qiu and H. Frei. 1993. Concept based query expan-
sion. In Proceedings of the 16th annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ?93. ACM, New York,
NY, USA, 160-169.
X. Sun, J. Gao, D. Micol, and C. Quirk. 2010. Learning
phrase-based spelling error models from clickthrough
data. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
?10, pages 266?274, Stroudsburg, PA, USA.
X. Wang, C. Zhai. 2008. Mining Term Association Pat-
terns from Search Logs for Effective Query Reformu-
lation. In Proceedings of the 17th ACM International
Conference on Information and Knowledge Manage-
ment 2008, CIKM?08. 479-488.
J. Xu and W. B. Croft. 1996. Query expansion using
local and global document analysis. In Proceedings of
the 19th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?96. ACM, New York, NY.
A. Yessenalina, Y. Yue, C. Cardie. 2010. Multi-
level Structured Models for Document-level Sentiment
Classification. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP ?10). 10461056.
1521
