Comparing Knowledge Sources for Nominal
Anaphora Resolution
Katja Markert?
University of Leeds
Malvina Nissim?
University of Edinburgh
We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora
and definite noun phrase coreference. Specifically, we compare an algorithm that relies on links
encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora
by means of shallow lexico-semantic patterns. As corpora we use the British National Corpus
(BNC), as well as the Web, which has not been previously used for this task. Our results
show that (a) the knowledge encoded in WordNet is often insufficient, especially for anaphor?
antecedent relations that exploit subjective or context-dependent knowledge; (b) for other-
anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite NP
coreference, the Web-based method yields results comparable to those obtained using WordNet
over the whole data set and outperforms the WordNet-based method on subsets of the data
set; (d) in both case studies, the BNC-based method is worse than the other methods because
of data sparseness. Thus, in our studies, the Web-based method alleviated the lexical knowledge
gap often encountered in anaphora resolution and handled examples with context-dependent
relations between anaphor and antecedent. Because it is inexpensive and needs no hand-modeling
of lexical knowledge, it is a promising knowledge source to integrate into anaphora resolution
systems.
1. Introduction
Most work on anaphora resolution has focused on pronominal anaphora, often
achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube,
Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an
F-measure of 82.8% for personal pronouns, respectively. Less attention has been paid
to nominal anaphors with full lexical heads, which cover a variety of phenomena, such
as coreference (Example (1)), bridging (Clark 1975; Example (2)), and comparative
anaphora (Examples (3?4)).1
? School of Computing, University of Leeds, Woodhouse Lane, LS2 9JT Leeds, UK. E-mail:
markert@comp.leeds.ac.uk.
? School of Informatics, University of Edinburgh, 2 Buccleuch Place, EH8 9LW Edinburgh, UK. E-mail:
mnissim@inf.ed.ac.uk.
1 In all examples presented in this article, the anaphor is typed in boldface and the correct antecedent in
italics. The abbreviation in parentheses at the end of each example specifies the corpus from which the
example is taken: WSJ stands for the Wall Street Journal, Penn Treebank, release 2; BNC stands for British
National Corpus (Burnard 1995), and MUC-6 for the combined training/test set for the coreference task
of the Sixth Message Understanding Conference (Hirschman and Chinchor 1997).
Submission received: 15 December 2003; revised submission received: 21 November 2004; accepted for
publication: 19 March 2005.
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
(1) The death of Maxwell, the British publishing magnate whose empire
collapsed in ruins of fraud, and who was the magazine?s publisher, gave the
periodical a brief international fame. (BNC)
(2) [. . . ] you don?t have to undo the jacket to get to the map?particularly
important when it?s blowing a hooley. There are elasticated adjustable
drawcords on the hem, waist and on the hood. (BNC)
(3) In addition to increasing costs as a result of greater financial exposure for
members, these measures could have other, far-reaching repercussions.
(WSJ)
(4) The ordinance, in Moon Township, prohibits locating a group home for the
handicapped within a mile of another such facility. (WSJ)
In Example (1), the definite noun phrase (NP) the periodical corefers with the
magazine.2 In Example (2), the definite NP the hood can be felicitously used because
a related entity has already been introduced by the NP the jacket, and a part-of
relation between the two entities can be established. Examples (3)?(4) are instances
of other-anaphora. Other-anaphora are a subclass of comparative anaphora (Halliday
and Hasan 1976; Webber et al 2003) in which the anaphoric NP is introduced by
a lexical modifier (such as other, such, and comparative adjectives) that specifies
the relationship (such as set-complement, similarity and comparison) between the
entities invoked by anaphor and antecedent. For other-anaphora, the modifiers
other or another provide a set-complement to an entity already evoked in the
discourse model. In Example (3), the NP other, far-reaching repercussions refers to
a set of repercussions excluding increasing costs and can be paraphrased as other
(far-reaching) repercussions than (increasing) costs. Similarly, in Example (4), the NP
another such facility refers to a group home which is not identical to the specific (planned)
group home mentioned before.
A large and diverse amount of lexical or world knowledge is usually necessary
to understand anaphors with full lexical heads. For the examples above, we need
the knowledge that magazines are periodicals, that hoods are parts of jackets,
that costs can be or can be viewed as repercussions of an event, and that institutional
homes are facilities. Therefore, many resolution systems that handle these phenomena
(Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie
2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on
hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical
hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has
given strong indications that such resources are insufficient for the entire range of
full NP anaphora. Additionally, we discuss some serious methodological problems
that arise when fixed ontologies are used that have been encountered by previous
researchers and/or us: the costs of building, maintaining and mining ontolo-
gies; domain-specific and context-dependent knowledge; different ways of encoding
information; and sense ambiguity.
2 In this article, we restrict the notion of definite NPs to NPs modified by the article ?the.?
3 These systems also use surface-level features (such as string matching), recency, and grammatical
constraints. In this article, we concentrate on the lexical and semantic knowledge employed.
368
Markert and Nissim Knowledge Sources for Anaphora Resolution
In Section 3, we discuss an alternative to the manual construction of knowledge
bases, which we call the corpus-based approach. A number of researchers (Hearst
1992; Berland and Charniak 1999, among others) have suggested that knowledge
bases be enhanced via (semi)automatic knowledge extraction from corpora, and such
enhanced knowledge bases have also been used for anaphora resolution, specifically
for bridging (Poesio et al 2002; Meyer and Dale 2002). Building on our previous work
(Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in two
ways. First, we suggest using the Web for anaphora resolution instead of the smaller-
size, but less noisy and more balanced, corpora used previously, making available
a huge additional source of knowledge.4 Second, we do not induce a fixed lexical
knowledge base from the Web but use shallow lexicosyntactic patterns and their Web
frequencies for anaphora resolution on the fly. This allows us to circumvent some of the
above-mentioned methodological problems that occur with any fixed ontology, whether
constructed manually or automatically.
The core of this article consists of an empirical comparison of these different
sources of lexical knowledge for the task of antecedent selection or antecedent ranking
in anaphora resolution. We focus on two types of full NP anaphora: other-anaphora
(Section 4) and definite NP coreference (Section 5).5 In both case studies, we compare
an algorithm that relies mainly on the frequencies of lexico-syntactic patterns in corpora
(both the Web and the BNC) with an algorithm that relies mainly on a fixed ontology
(WordNet 1.7.1). We specifically address the following questions:
1. Can the shortcomings of using a fixed ontology that have been stipulated
by previous research on definite NPs be confirmed in our coreference
study? Do they also hold for other-anaphora, a phenomenon less studied
so far?
2. How does corpus-based knowledge acquisition compare to using
manually constructed lexical hierarchies in antecedent selection? And is
the use of the Web an improvement over using smaller, but manually
controlled, corpora?
3. To what extent is the answer to the previous question dependent on the
anaphoric phenomenon addressed?
In Section 6 we discuss several aspects of our findings that still need elaboration
in future work. Specifically, our work is purely comparative and regards the different
lexical knowledge sources in isolation. It remains to be seen how the results carry
forward when the knowledge sources interact with other features (for example,
grammatical preferences). A similar issue concerns the integration of the methods
into anaphoricity determination in addition to antecedent selection. Additionally,
future work should explore the contribution of different knowledge sources for yet
other anaphora types.
4 There is a growing body of research that uses the Web for NLP. As we concentrate on anaphora resolution
in this article, we refer the reader to Grefenstette (1999) and Keller and Lapata (2003), as well as the
December 2003 special issue of Computational Linguistics, for an overview of the use of the Web for other
NLP tasks.
5 As described above, in other-anaphora the entities invoked by the anaphor are a set complement to the
entity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor and
antecedent are identical.
369
Computational Linguistics Volume 31, Number 3
2. The Knowledge Gap and Other Problems for Lexico-semantic Resources
A number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio
2000; Harabagiu, Bunescu, and Maiorano 2001; Strube, Rapp, and Mueller 2002;
Modjeska 2002; Gardent, Manuelian, and Kow 2003) point to the importance of lexical
and world knowledge for the resolution of full NP anaphora and the lack of such
knowledge in existing ontologies (Section 2.1). In addition to this knowledge gap, we
summarize other, methodological problems with the use of ontologies in anaphora
resolution (Section 2.2).
2.1 The Knowledge Gap for Nominal Anaphora with Full Lexical Heads
In the following, we discuss previous studies on the automatic resolution of coreference,
bridging and comparative anaphora, concentrating on work that yields insights into the
use of lexical and semantic knowledge.
2.1.1 Coreference. The prevailing current approaches to coreference resolution are
evaluated on MUC-style (Hirschman and Chinchor 1997) annotated text and treat
pronominal and full NP anaphora, named-entity coreference, and non-anaphoric
coreferential links that can be stipulated by appositions and copula. The performance of
these approaches on definite NPs is often substantially worse than on pronouns and/or
named entities (Connolly, Burger, and Day 1997; Strube, Rapp, and Mueller 2002; Ng
and Cardie 2002b; Yang et al 2003). For example, for a coreference resolution algorithm
on German texts, Strube, Rapp, and Mueller (2002) report an F-measure of 33.9% for
definite NPs that contrasts with 82.8% for personal pronouns.
Several reasons for this performance difference have been established. First,
whereas pronouns are mostly anaphoric in written text, definite NPs do not have
to be so, inducing the problem of whether a definite NP is anaphoric in addition to
determining an antecedent from among a set of potential antecedents (Fraurud 1990;
Vieira and Poesio 2000).6 Second, the antecedents of definite NP anaphora can occur at
considerable distance from the anaphor, whereas antecedents to pronominal anaphora
tend to be relatively close (Preiss, Gasperin, and Briscoe 2004; McCoy and Strube 1999).
An automatic system can therefore more easily restrict its antecedent set for pronominal
anaphora.
Third, it is in general believed that pronouns are used to refer to entities in focus,
whereas entities that are not in focus are referred to by definite descriptions (Hawkins
1978; Ariel 1990; Gundel, Hedberg, and Zacharski 1993), because the head nouns of
anaphoric definite NPs provide the reader with lexico-semantic knowledge. Antecedent
accessibility is therefore additionally restricted via semantic compatibility and does not
need to rely on notions of focus or salience to the same extent as for pronouns. Given this
lexical richness of common noun anaphors, many resolution algorithms for coreference
have incorporated manually controlled lexical hierarchies, such as WordNet. They use,
for example, a relatively coarse-grained notion of semantic compatibility between a few
high-level concepts in WordNet (Soon, Ng, and Lim 2001), or more detailed hyponymy
and synonymy links between anaphor and antecedent head nouns (Vieira and Poesio
6 A two-stage process in which the first stage identifies anaphoricity of the NP and the second the
antecedent for anaphoric NPs (Uryupina 2003; Ng 2004) can alleviate this problem. In this article, we
focus on the second stage, namely, antecedent selection.
370
Markert and Nissim Knowledge Sources for Anaphora Resolution
2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b, among others).
However, several researchers have pointed out that the incorporated information is
still insufficient. Harabagiu, Bunescu, and Maiorano (2001) (see also Kameyama 1997)
report that evaluation of previous systems has shown that ?more than 30% of the
missed coreference links are due to the lack of semantic consistency information
between the anaphoric noun and its antecedent noun? (page 59). Vieira and Poesio
(2000) report results on anaphoric definite NPs in the WSJ that stand in a synonymy
or hyponymy relation to their antecedents (as in Example (1)). Using WordNet links
to retrieve the appropriate knowledge proved insufficient, as only 35.0% of synonymy
relations and 56.0% of hyponymy relations needed were encoded in WordNet as direct
or inherited links.7 The semantic knowledge used might also not necessarily improve
on string matching: Soon, Ng, and Lim (2001) final, automatically derived decision tree
does not incorporate their semantic-compatibility feature and instead relies heavily on
string matching and aliasing, thereby leaving open how much information in a lexical
hierarchy can improve over string matching.
In this article, we concentrate on this last of the three problems (insufficient lexical
knowledge). We investigate whether the knowledge gap for definite NP coreference
can be overcome by using corpora as knowledge sources as well as whether the
incorporation of lexical knowledge sources improves over simple head noun matching.
2.1.2 Comparative Anaphora. Modjeska (2002)?one of the few computational studies
on comparative anaphora?shows that lexico-semantic knowledge plays a larger role
than grammatical salience for other-anaphora. In this article, we show that the semantic
knowledge provided via synonymy and hyponymy links in WordNet is insufficient
for the resolution of other-anaphora, although the head of the antecedent is normally
a synonym or hyponym of the head of the anaphor in other-anaphora (Section 4.4).8
2.1.3 Bridging. Vieira and Poesio (2000) report that 62.0% of meronymy relations (see
Example (2)) needed for bridging resolution in their corpus were not encoded in
WordNet. Gardent, Manuelian, and Kow (2003) identified bridging descriptions in a
French corpus, of which 187 (52%) exploited meronymic relations. Almost 80% of these
were not found in WordNet. Hahn, Strube, and Markert (1996) report experiments on
109 bridging cases from German information technology reports, using a hand-crafted,
domain-specific knowledge base of 449 concepts and 334 relations. They state that 42
(38.5%) links between anaphor and antecedents were missing in their knowledge base,
a high proportion given the domain-specific task. In this article, we will not address
bridging, although we will discuss the extension of our work to bridging in Section 6.
2.2 Methodological Problems for the Use of Ontologies in Anaphora Resolution
Over the years, several major problems have been identified with the use of ontologies
for anaphora resolution. In the following we provide a summary of the different issues
raised, using the examples in the Introduction.
7 Whenever we refer to ?hyponymy/meronymy (relations/links)? in WordNet, we include both direct and
inherited links.
8 From this point on, we will often use the terms anaphor and antecedent instead of head of anaphor and head
of antecedent if the context is non-ambiguous.
371
Computational Linguistics Volume 31, Number 3
2.2.1 Problem 1: Knowledge Gap. As discussed above, even in large ontologies the
lack of knowledge can be severe, and this problem increases for non-hyponymy rela-
tions. None of the examples in Section 1 are covered by synonymy, hyponymy, or
meronymy links in WordNet; for example, hoods are not encoded as parts of jackets,
and homes are not encoded as a hyponym of facilities. In addition, building, extending,
and maintaining ontologies by hand is expensive.
2.2.2 Problem 2: Context-Dependent Relations. Whereas the knowledge gap might
be reduced as (semi)automatic efforts to enrich ontologies become available (Hearst
1992; Berland and Charniak 1999; Poesio et al 2002), the second problem is intrinsic to
fixed context-independent ontologies: How much and which knowledge should they
include? Thus, Hearst (1992) raises the issue of whether underspecified, context- or
point-of-view-dependent hyponymy relations (like the context-dependent link between
costs and repercussions in Example (3)) should be included in a fixed ontology, in
addition to universally true hyponymy relations. Some other hyponymy relations that
we encountered in our studies whose inclusion in ontologies is debatable are age:(risk)
factor, coffee:export, pilots:union, country:member.
2.2.3 Problem 3: Information Encoding. Knowledge might be encoded in many
different ways in a lexical hierarchy, and this can pose a problem for anaphora resolution
(Humphreys et al 1997; Poesio, Vieira, and Teufel 1997). For example, although
magazine and periodical are not linked in WordNet via synonymy/hyponymy, the gloss
records magazine as a periodic publication. Thus, the desired link might be derived
through the analysis of the gloss together with derivation of periodical from periodic.
However, such extensive mining of the ontology (as performed, e.g., by Harabagiu,
Bunescu, and Maiorano [2001]) can be costly. In addition, different information sources
must be weighed (e.g., is a hyponymy link preferred over a gloss inclusion?) and
combined (should hyponyms/hyperonyms/sisters of gloss expressions be considered
recursively?). Extensive combinations also increase the risk of false positives.9
2.2.4 Problem 4: Sense Proliferation. Using all senses of anaphor and potential an-
tecedents in the search for relations might yield a link between an incorrect antecedent
candidate and the anaphor due to an inappropriate sense selection. On the other hand,
considering only the most frequent sense for anaphor and antecedent (as is done in
Soon, Ng, and Lim [2001]) might lead to wrong antecedent assignment if a minority
sense is intended in the text. So, for example, the most frequent sense of hood in
WordNet is criminal, whereas the sense used in Example (2) is headdress. The alterna-
tives are either weighing senses according to different domains or a more costly sense
disambiguation procedure before anaphora resolution (Preiss 2002).
3. The Alternative: Corpus-Based Knowledge Extraction
There have been a considerable number of efforts to extract lexical relations from
corpora in order to build new knowledge sources and enrich existing ones without time-
9 Even without extensive mining, this risk can be high: Vieira and Poesio (2000) report a high number of
false positives for one of their data sets, although they use only WordNet-encoded links.
372
Markert and Nissim Knowledge Sources for Anaphora Resolution
consuming hand-modeling. This includes the extraction of hyponymy and synonymy
relations (Hearst 1992; Caraballo 1999, among others) as well as meronymy (Berland
and Charniak 1999; Meyer 2001).10 One approach to the extraction of instances of a
particular lexical relation is the use of patterns that express lexical relations structurally
explicitly in a corpus (Hearst 1992; Berland and Charniak 1999; Caraballo 1999; Meyer
2001), and this is the approach we focus on here. As an example, the pattern NP1 and
other NP2 usually expresses a hyponymy/similarity relation between the hyponym
NP1 and its hypernym NP2 (Hearst 1992), and it can therefore be postulated that two
noun phrases that occur in such a pattern in a corpus should be linked in an ontology via
a hyponymy link. Applications of the extracted relations to anaphora resolution are less
frequent. However, Poesio et al (2002) and Meyer and Dale (2002) have used patterns
for the corpus-based acquisition of meronymy relations: these patterns are subsequently
exploited for bridging resolution.
Although automatic acquisition can help bridge the knowledge gap (see Prob-
lem 1 in Section 2.2.1), the incorporation of the acquired knowledge into a fixed
ontology yields other problems. Most notably, it has to be decided which knowl-
edge should be included in ontologies, because pattern-based acquisition will
also find spurious, subjective and context-dependent knowledge (see Problem 2 in
Section 2.2.2). There is also the problem of pattern ambiguity, since patterns do
not necessarily have a one-to-one correspondence to lexical relations (Meyer 2001).
Following our work in Markert, Nissim, and Modjeska (2003), we argue that for the
task of antecedent ranking, these problems can be circumvented by not constructing
a fixed ontology at all. Instead, we use the pattern-based approach to find lexical
relationships holding between anaphor and antecedent in corpora on the fly. For
instance, in Example (3), we do not need to know whether costs are always repercus-
sions (and should therefore be linked via hyponymy in an ontology) but only that
they are more likely to be viewed as repercussions than the other antecedent candidates.
We therefore adapt the pattern-based approach in the following way for antecedent
selection.
Step 1: Relation Identification. We determine which lexical relation usu-
ally holds between anaphor and antecedent head nouns for a partic-
ular anaphoric phenomenon. For example, in other-anaphora, a hyponymy/
similarity relation between anaphor and antecedent is exploited (homes are
facilities) or stipulated by the context (costs are viewed as repercussions).
Step 2: Pattern Selection. We select patterns that express this lexical relation
structurally explicitly. For example, the pattern NP1 and other NP2 usually
expresses hyponymy/similarity relations between the hyponym NP1 and its
hypernym NP2 (see above).
Step 3: Pattern Instantiation. If the lexical relation between anaphor and
antecedent head nouns is strong, then it is likely that the anaphor and
antecedent also frequently co-occur in the selected explicit patterns. We
extract all potential antecedents for each anaphor and instantiate the explicit
10 There is also a long history in the extraction of other lexical knowledge, which is also potentially useful
for anaphora resolution, for example, of selectional restrictions/preferences. In this article we focus on
the lexical relations that can hold between antecedent and anaphor head nouns.
373
Computational Linguistics Volume 31, Number 3
for all anaphor/antecedent pairs. In Example (4) the pattern NP1 and
other NP2 can be instantiated with ordinances and other facilities, Moon
Township and other facilities, homes and other facilities, handicapped
and other facilities, and miles and other facilities.11
Step 4: Antecedent Assignment. The instantiation of a pattern can be searched
in any corpus to determine its frequency. We follow the rationale that the
most frequent of these instantiated patterns determines the most likely
antecedent. Therefore, should the head noun of an antecedent candidate
and the anaphor co-occur in a pattern although they do not stand in the
lexical relationship considered (because of pattern ambiguity, noise in the
corpus, or spurious occurrences), this need not prove a problem as long
as the correct antecedent candidate co-occurs more frequently with the
anaphor.
As the patterns can be elaborate, most manually controlled and linguistically processed
corpora are too small to determine the pattern frequencies reliably. Therefore, the size
of the corpora used in some previous approaches leads to data sparseness (Berland and
Charniak 1999), and the extraction procedure can therefore require extensive smoothing.
Thus as a further extension, we suggest using the largest corpus available, the Web,
in the above procedure. The instantiation for the correct antecedent homes and other
facilities in Example (4), for instance, does not occur at all in the BNC but yields over
1,500 hits on the Web.12 The competing instantiations (listed in Step 3) yield 0 hits in the
BNC and fewer than 20 hits on the Web.
In the remainder of this article, we present two comparative case studies on
coreference and other-anaphora that evaluate the ontology- and corpus-based ap-
proaches in general and our extensions in particular.
4. Case Study I: Other-Anaphora
We now describe our first case study for antecedent selection in other-anaphora.
4.1 Corpus Description and Annotation
We use Modjeska?s (2003) annotated corpus of other-anaphors from the WSJ. All
examples in this section are from this corpus. Modjeska restricts the notion of other-
anaphora to anaphoric NPs with full lexical heads modified by other or another
(Examples (3)?(4)), thereby excluding idiomatic non-referential uses (e.g., on the other
hand), reciprocals such as each other, ellipsis, and one-anaphora. The excluded cases
either are non-anaphoric or do not have a full lexical head and would therefore re-
quire a mostly non-lexical approach to resolution. Modjeska?s corpus also excludes
11 These simplified instantiations serve as an example; for final instantiations, see Section 4.5.1.
12 This search and all searches for the Web experiments in Section 4 were executed on August 29, 2003. All
Web searches for Section 5 were executed August 27, 2004.
374
Markert and Nissim Knowledge Sources for Anaphora Resolution
other-anaphors with structurally available antecedents: In list contexts such as
Example (5), the antecedent is normally given as the left conjunct of the list:
(5) [. . .] AZT can relieve dementia and other symptoms in children [. . .]
A similar case is the construction Xs other than Ys. For a computational treatment of
other-NPs with structural antecedents, see Bierner (2001).
The original corpus collected and annotated by Modjeska (2003) contains 500
instances of other-anaphors with NP antecedents in a five-sentence window. In this
study we use the 408 (81.6%) other-anaphors in the corpus that have NP antecedents
within a two-sentence window (the current or previous sentence).13 An antecedent
candidate is manually annotated as correct if it is the latest mention of the entity to
which the anaphor provides the set complement. The tag lenient was used to annotate
previous mentions of the same entity. In Example (6), all other bidders refers to all bidders
excluding United Illuminating Co., whose latest mention is it. In this article, lenient
antecedents are underlined. All other potential antecedents (e.g., offer in Example (6)),
are called distractors.
(6) United Illuminating Co. raised its proposed offer to one it valued at $2.29
billion from $2.19 billion, apparently topping all other bidders.
The antecedent can be a set of separately mentioned entities, like May and July in
Example (7). For such split antecedents (Modjeska 2003), the latest mention of each set
member is annotated as correct, so that there can be more than one correct antecedent to
an anaphor.14
(7) The May contract, which also is without restraints, ended with a gain of
0.45 cent to 14.26 cents. The July delivery rose its daily permissible limit of
0.50 cent a pound to 14.00 cent, while other contract months showed
near-limit advances.
4.2 Antecedent Extraction and Preprocessing
For each anaphor, all previously occurring NPs in the two-sentence window were
automatically extracted exploiting the WSJ parse trees. NPs containing a possessive NP
modifier (e.g., Spain?s economy) were split into a possessor phrase (Spain) and a possessed
entity (Spain?s economy).15 Modjeska (2003) identifies several syntactic positions that
cannot serve as antecedents of other-anaphors. We automatically exclude only NPs
preceding an appositive other-anaphor from the candidate antecedent set. In ?Mary
Elizabeth Ariail, another social-studies teacher,? the NP Mary Elizabeth Ariail cannot
13 We concentrate on this majority of cases to focus on the comparison of different sources of lexical
knowledge without involving discourse segmentation or focus tracking. In Section 5 we expand the
window size to allow equally high coverage for definite NP coreference.
14 The occurrence of split antecedents also motivated the distinction between correct and lenient
antecedents in the annotation. Anaphors with split antecedents have several antecedent candidates
annotated as correct. All other anaphors have only one antecedent candidate annotated as correct, with
previous mentions of the same entity marked as lenient.
15 We thank Natalia Modjeska for the extraction and for making the resulting sets of candidate antecedents
available to us.
375
Computational Linguistics Volume 31, Number 3
be the antecedent of another social-studies teacher as the two phrases are coreferential and
cannot provide a set complement to each other.
The resulting set of potential NP antecedents for an anaphor ana (with a unique
identifier anaid) is called Aanaid.16 The final number of extracted antecedents for the
whole data set is 4,272, with an average of 10.5 antecedent candidates per anaphor.
After extraction, all modification was eliminated, and only the rightmost noun of
compounds was retained, as modification results in data sparseness for the corpus-
based methods, and compounds are often not recorded in WordNet.
For the same reasons we automatically resolved named entities (NEs). They
were classified into the ENAMEX MUC-7 categories (Chinchor 1997) PERSON,
ORGANIZATION and LOCATION, using the software ANNIE (GATE2; http://gate.ac.
uk). We then automatically obtained more-fine-grained distinctions for the NE cate-
gories LOCATION and ORGANIZATION, whenever possible. We classified LOCATIONS
into COUNTRY, (US) STATE, CITY, RIVER, LAKE, and OCEAN in the following way. First,
small gazetteers for these subcategories were extracted from the Web. Second, if an
entity marked as LOCATION by ANNIE occurred in exactly one of these gazetteers
(e.g., Texas in the (US) STATE gazetteer) it received the corresponding specific label;
if it occurred in none or in several of the gazetteers (e.g., Mississippi occurred in
both the state and the river gazetteer), then the label was left at the LOCATION level.
We further classified an ORGANIZATION entity by using its internal makeup as follows.
We extracted all single-word hyponyms of the noun organization from WordNet and
used the members of this set, OrgSet, as the target categories for the fine-grained
distinctions. If an entity was classified by ANNIE as ORGANIZATION and it had
an element <ORG> of OrgSet as its final lemmatized word (e.g., Deutsche Bank) or
contained the pattern <ORG> of (for example, Bank of America), it was subclassified
as <ORG> (here, BANK). In cases of ambiguity, again, no subclassification was carried
out. No further distinctions were developed for the category PERSON. We used regular
expression matching to classify numeric and time entities into DAY, MONTH, and YEAR
as well as DOLLAR or simply NUMBER. This subclassification of the standard cate-
gories provides us with additional lexical information for antecedent selection. Thus, in
Example (8), for instance, a finer-grained classification of South Carolina into STATE
provides more useful information than resolving both South Carolina and Greenville
County as LOCATION only:
(8) Use of Scoring High is widespread in South Carolina and common in
Greenville County. . . . Experts say there isn?t another state in the country
where . . .
Finally, all antecedent candidates and anaphors were lemmatized. The procedure of
extraction and preprocessing results in the following antecedent sets and anaphors
for Examples (3) and (4): A3 = {..., addition, cost, result, exposure, member, measure} and
ana = repercussion and A4 = {..., ordinance, Moon Township [= location], home, handicapped,
mile} and ana = facility.
Table 1 shows the distribution of antecedent NP types in the other-anaphora data
set.17 NE resolution is clearly important as 205 of 468 (43.8%) of correct antecedents
are NEs.
16 In this article the anaphor ID corresponds to the example numbers.
17 Note that there are more correct antecedents than anaphors because the data include split antecedents.
376
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 1
Distribution of antecedent NP types in the other-anaphora data set.
Correct Lenient Distractors All
Pronouns 49 19 329 397
Named entities 205 56 806 1,067
Common nouns 214 104 2,490 2,808
Total 468 179 3,625 4,272
4.3 Evaluation Measures and Baselines
For each anaphor, each algorithm selects at most one antecedent as the correct one. If
this antecedent provides the appropriate set complement to the anaphor (i.e., is marked
in the gold standard as correct or lenient), the assignment is evaluated as correct.18
Otherwise, it is evaluated as wrong. We use the following evaluation measures: Precision
is the number of correct assignments divided by the number of assignments, recall is the
number of correct assignments divided by the number of anaphors, and F-measure is
based on equal weighting of precision and recall. In addition, we also give the coverage
of each algorithm as the number of assignments divided by the number of anaphors.
This last measure is included to indicate how often the algorithm has any knowledge to
go on, whether correct or false. For algorithms in which the coverage is 100%, precision,
recall, and F-measure all coincide.
We developed two simple rule-based baseline algorithms. The first, a recency-based
baseline (baselineREC), always selects the antecedent candidate closest to the anaphor.
The second (baselineSTR) takes into account that the lemmatized head of an other-
anaphor is sometimes the same as that of its antecedent, as in the pilot?s claim . . . other
bankruptcy claims. For each anaphor, baselineSTR string-compares its last (lemmatized)
word with the last (lemmatized) word of each of its potential antecedents. If the strings
match, the corresponding antecedent is chosen as the correct one. If several antecedents
produce a match, the baseline chooses the most recent one among them. If no antecedent
produces a match, no antecedent is assigned.
We tested two variations of this baseline.19 The algorithm baselineSTRv1 uses
only the original antecedents for string matching, disregarding named-entity res-
olution. If string-comparison returns no match, a back-off version (baselineSTR?v1)
chooses the antecedent closest to the anaphor among all antecedent candidates, thereby
yielding a 100% coverage. The second variation (baselineSTRv2) uses the replacements
for named entities for string matching; again a back-off version (baselineSTR?v2) uses
a recency back-off. This baseline performs slightly better, as now cases such as that in
Example (8) (South Carolina . . . another state, in which South Carolina is resolved to STATE)
can also be resolved. The results of all baselines are summarized in Table 2. Results of
the 100% coverage backoff algorithms are indicated by Precision? in all tables. The sets
of anaphors covered by the string-matching baselines baselineSTRv1 and baselineSTRv2
18 This does not hold for anaphors with split antecedents, for which all antecedents marked as correct need
to be found in order to provide the complete set complement. Therefore, all our algorithms? assignments
in these cases are evaluated as wrong, as they select at most one antecedent.
19 Different versions of the same prototype algorithm are indicated via an index of v1, v2, . . . . The general
prototype algorithm is referred to without indices.
377
Computational Linguistics Volume 31, Number 3
Table 2
Overview of the results for all baselines for other-anaphora.
Algorithm Coverage Precision Recall F-measure Precision?
baselineREC 1.000 0.178 0.178 0.178 0.178
baselineSTRv1 0.282 0.686 0.194 0.304 0.333
baselineSTRv2 0.309 0.698 0.216 0.329 0.350
will be called StrSetv1 and StrSetv2, respectively. These sets do not include the cases
assigned by the recency back-off in baselineSTR?v1 and baselineSTR
?
v2.
For our WordNet and corpus-based algorithms we additionally deleted pronouns
from the antecedent sets, since they are lexically not very informative and are also
not encoded in WordNet. This removes 49 (10.5%) of the 468 correct antecedents
(see Table 1); however, we can still resolve some of the anaphors with pronoun
antecedents if they also have a lenient non-pronominal antecedent, as in Example (6).
After pronoun deletion, the total number of antecedents in our data set is 3,875
for 408 anaphors, of which 419 are correct antecedents, 160 are lenient, and 3,296
are distractors.
4.4 Wordnet as a Knowledge Source for Other-Anaphora Resolution
4.4.1 Descriptive Statistics. As most antecedents are hyponyms or synonyms of their
anaphors in other-anaphora, for each anaphor ana, we look up which elements of its
antecedent set Aanaid are hyponyms/synonyms of ana in WordNet, considering all
senses of anaphor and candidate antecedent. In Example (4), for example, we look
up whether ordinance, Moon Township, home, handicapped, and mile are hyponyms or
synonyms of facility in WordNet. Similarly, in Example (9), we look up whether Will
Quinlan [= PERSON], gene, and risk are hyponyms/synonyms of child.
(9) Will Quinlan had not inherited a damaged retinoblastoma supressor gene
and, therefore, faced no more risk than other children . . .
As proper nouns (e.g., Will Quinlan) are often not included in WordNet, we also
look up whether the NE category of an NE antecedent is a hyponym/synonym of
the anaphor (e.g., whether person is a synonym/hyponym of child) and vice versa
(e.g., whether child is a synonym/hyponym of person). This last inverted look-up
is necessary, as the NE category of the antecedent is often too general to preserve
the normal hyponymy relationship to the anaphor. Indeed, in Example (9), it is the
inverted look-up that captures the correct hyponymy relation between person and
child. If the single look-up for common nouns or any of the three look-ups for
proper nouns is successful, we say that a hyp/syn relation between candidate
antecedent and anaphor holds in WordNet. Note that each noun in WordNet
stands in a hyp/syn relation to itself. Table 3 summarizes how many correct/
lenient antecedents and distractors stand in a hyp/syn relation to their anaphor in
WordNet.
Correct/lenient antecedents stand in a hyp/syn relation to their anaphor sig-
nificantly more often than distractors do (p < 0.001, t-test). The use of WordNet
hyponymy/synonymy relations to distinguish between correct/lenient antecedents
and distractors is therefore plausible. However, Table 3 also shows two limitations
378
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 3
Descriptive statistics for WordNet hyp/syn relations for other-anaphora.
Hyp/syn relation No hyp/syn relation Total
to anaphor to anaphor
Correct antecedents 180 (43.0%) 239 (57.0%) 419 (100%)
Lenient antecedents 68 (42.5%) 92 (57.5%) 160 (100%)
Distractors 296 (9.0%) 3,000 (91.0%) 3,296 (100%)
All antecedents 544 (14.0%) 3,331 (86.0%) 3,875 (100%)
of relying on WordNet in resolution algorithms. First, 57% of correct and lenient
antecedents are not linked via a hyp/syn relation to their anaphor in WordNet.
This will affect coverage and recall (see also Section 2.2.1). Examples from our data
set that are not covered are home:facility, cost:repercussion, age:(risk) factor, pension:benefit,
coffee:export, and pilot(s):union, including both missing universal hyponymy links and
context-stipulated ones. Second, the raw frequency (296) of distractors that stand
in a hyp/syn relation to their anaphor is higher than the combined raw frequency
for correct/lenient antecedents (248) that do so, which can affect precision. This is
due to both sense proliferation (Section 2.2.4) and anaphors that require more than just
lexical knowledge about antecedent and anaphor heads to select a correct antecedent
over a distractor. In Example (10), the distractor product stands in a hyp/syn relation-
ship to the anaphor commodity and?disregarding other factors?is a good antecedent
candidate.20
(10) . . . the move is designed to more accurately reflect the value of products
and to put steel on a more equal footing with other commodities.
4.4.2 The WordNet-Based Algorithm. The WordNet-based algorithm resolves each
anaphor ana to a hyponym or synonym in Aanaid, if possible. If several antecedent
candidates are hyponyms or synonyms of ana, it uses a tiebreaker based on string
match and recency. When no candidate antecedent is a hyponym or synonym of
ana, string match and recency can be used as a possible back-off.21 String compari-
son for tiebreaker and back-off can again use the original or the replaced anteced-
ents, yielding two versions, algoWNv1 (original antecedents) and algoWNv2 (replaced
antecedents).
The exact procedure for the version algoWNv1 given an anaphor ana is as follows:22
(i) for each antecedent a in Aanaid, look up whether a hyp/syn relation
between a and ana holds in WordNet; if this is the case, push a into a set
Ahyp/synanaid ;
20 This problem is not WordNet-specific but affects all algorithms that rely on lexical knowledge only.
21 Because each noun is a synonym of itself, anaphors in StrSetv1/StrSetv2 that do have a string-matching
antecedent candidate will already be covered by the WordNet look-up prior to back-off in almost all
cases: Back-off string matching will take effect only if the anaphor/antecedent head noun is not in
WordNet at all. Therefore, the described back-off will most of the time just amount to a recency back-off.
22 The algorithm algoWNv2 follows the same procedure apart from the variation in string matching.
379
Computational Linguistics Volume 31, Number 3
(ii) if Ahyp/synanaid contains exactly one element, choose this element and stop;
(iii) otherwise, if Ahyp/synanaid contains more than one element, string-compare
each antecedent in Ahyp/synanaid with ana (using original antecedents only).
If exactly one element of Ahyp/synanaid matches ana, select this one and stop;
if several match ana, select the closest to ana within these matching
antecedents and stop; if none match, select the closest to ana within
Ahyp/synanaid and stop;
(iv) otherwise, if Ahyp/synanaid is empty, make no assignment and stop.
The back-off algorithm algoWN?v1 uses baselineSTR
?
v1 as a back-off (iv?) if no antecedent
can be assigned:
(iv?) otherwise, if Ahyp/synanaid is empty, use baselineSTR?v1 to assign an
antecedent to ana and stop;
Both algoWNv1 and algoWNv2 achieved the same results, namely, a coverage of
65.2%, precision of 56.8%, and recall of 37.0%, yielding an F-measure of 44.8%.
The low coverage and recall confirm our predictions in Section 4.4.1. Using backoff
algoWN?v1/algoWN
?
v2 achieves a coverage of 100% and a precision/recall/F-measure
of 44.4%.
4.5 Corpora as Knowledge Sources for Other-Anaphora Resolution
In Section 3 we suggested the use of shallow lexico-semantic patterns for obtaining
anaphor?antecedent relations from corpora. In our first experiment we use the Web,
which with its approximately 8,058M pages23 is the largest corpus available to the
NLP community. In our second experiment we use the same technique on the BNC,
a smaller (100 million words) but virtually noise-free and balanced corpus of contem-
porary English.
4.5.1 Pattern Selection and Instantiation. The list-context Xs and other Ys explicitly
expresses a hyponymy/synonymy relationship with X being hyponyms/synonyms
of Y (see also Example (5) and [Hearst 1992]). This is only one of the possible
structures that express hyponymy/synonymy. Others involve such, including, and
especially (Hearst 1992) or appositions and coordination. We derive our patterns from the
list-context because it corresponds relatively unambigously to hyponymy/synonymy
relations (in contrast to coordination, which often links sister concepts instead of a
hyponym and its hyperonym, as in tigers and lions, or even completely unrelated
concepts). In addition, it is quite frequent (for example, and other occurs more
frequently on the Web than such as and other than). Future work has to explore
which patterns have the highest precision and/or recall and how different patterns
can be combined effectively without increasing the risk of false positives (see also
Section 2.2.3).
23 Google (http://www.google.com), estimate from November 2004.
380
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 4
Patterns and instantiations for other-anaphora.
Common-noun patterns Common-noun instantiations
W1: (N1{sg} OR N1{pl}) and other N2{pl} WIc1: (home OR homes) and other facilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B1: (...) and D* other A* N2{pl} BIc1: (home OR homes) and D* other A* facilities
Proper-noun patterns Proper-noun instantiations
W1: (N1{sg} OR N1{pl}) and other N2{pl} WIp1: (person OR persons) and other children
WIp2: (child OR children) and other persons
W2: N1 and other N2{pl} WIp3: Will Quinlan and other children
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B1: (...) and D* other A* N2{pl} BIp1: (person OR persons) and D* other A* children
BIp2: (child OR children) and D* other A* persons
B2: N1 and D* other A* N2{pl} BIp3: Will Quinlan and D* other A* children
Web. For the Web algorithm (algoWeb), we use the following pattern:24
(W1) (N1{sg} OR N1{pl}) and other N2{pl}
Given an anaphor ana and a common-noun antecedent candidate a in Aanaid, we
instantiate (W1) by substituting a for N1 and ana for N2. An instantiated pattern for
Example (4) is (home OR homes) and other facilities (WIc1 in Table 4).
25 This pattern
instantiation is parallel to the WordNet hyp/syn relation look-up for common nouns.
For NE antecedents we instantiate (W1) by substituting the NE category of the
antecedent for N1, and ana for N2. An instantiated pattern for Example (9) is (person
OR persons) and other children (WIp1 in Table 4). In this instantiation, N1 (person) is not
a hyponym of N2 (child); instead N2 is a hyponym of N1 (see the discussion on inverted
queries in Section 4.4.1). Therefore, we also instantiate (W1) by substituting ana for N1
and the NE type of the antecedent for N2 (WI
p
2 in Table 4). Finally, for NE antecedents,
we use an additional pattern:
(W2) N1 and other N2{pl}
which we instantiate by substituting the original NE antecedent for N1 and
ana for N2 (WI
p
3 in Table 4). The three instantiations for NEs are parallel to the three
hyp/syn relation look-ups in the WordNet experiment in Section 4.4.1. We submit
these instantiations as queries to the Google search engine, making use of the Google
API technology.
BNC. For BNC patterns and instantiations, we exploit the BNC?s part-of-speech tagging.
On the one hand, we restrict the instantiation of N1 and N2 to nouns to avoid noise,
and on the other hand, we allow occurrence of modification to improve coverage. We
24 In all patterns and instantiations in this article, OR is the boolean operator, N1 and N2 are variables, and
and and other are constants.
25 All common-noun instantiations are marked by a superscript c and all proper-noun instantiations by a
superscript p.
381
Computational Linguistics Volume 31, Number 3
therefore extend (W1) and (W2) to the patterns (B1) and (B2).26 An instantiation for
(B1), for example, also matches ?homes and the other four facilities.? Otherwise the
instantiations are produced parallel to the Web (see Table 4). We search the instantiations
in the BNC using the IMS Corpus Query Workbench (Christ 1995).
(B1) (N1{sg} OR N1{pl}) and D* other A* N2{pl}
(B2) N1 and D* other A* N2{pl}
For both algoWeb and algoBNC, each antecedent candidate a in Aanaid is assigned a
score. The procedure, using the notation for the Web, is as follows. We obtain the raw
frequencies of all instantiations in which a occurs (WIc1 for common nouns, or WI
p
1, WI
p
2,
and WIp3 for proper names) from the Web, yielding freq(WI
c
1), or freq(WI
p
1 ), freq(WI
p
2 ),
and freq(WIp3 ). The maximum WMa over these frequencies is the score associated with
each antecedent (given an anaphor ana), which we will also simply refer to as the
antecedent?s Web score. For the BNC, we call the corresponding maximum score BMa
and refer to it as the antecedent?s BNC score. This simple maximum score is biased
toward antecedent candidates whose head nouns occur more frequently overall. In a
previous experiment we used mutual information to normalize Web scores (Markert,
Nissim, and Modjeska 2003). However, the results achieved with normalized and
non-normalized scores showed no significant difference. Other normalization methods
might yield significant improvements over simple maximum scoring and can be
explored in future work.
4.5.2 Descriptive Statistics. Table 5 gives descriptive statistics for the Web and BNC
score distributions for correct/lenient antecedents and distractors, including the mini-
mum and maximum score, mean score and standard deviation, median, and number of
zero scores, scores of one, and scores greater than one.
Web scores resulting from simple pattern-based search produce on average signif-
icantly higher scores for correct/lenient antecedents (mean: 2,416.68/807.63; median:
68/68.5) than for distractors (mean: 290.97; median: 1). Moreover, the method produces
significantly fewer zero scores for correct/lenient antecedents (19.6%/22.5%) than for
distractors (42.3%).27 Therefore the pattern-based Web method is a good candidate for
distinguishing correct/lenient antecedents and distractors in anaphora resolution. In
addition, the median for correct/lenient antecedents is relatively high (68/68.5), which
ensures a relatively large amount of data upon which to base decisions. Only 19.6%
of correct antecedents have scores of zero, which indicates that the method might
have high coverage (compared to the missing 57% of hyp/syn relations for correct
antecedents in WordNet; Section 4.4).
Although the means of the BNC score distributions of correct/lenient antecedents
are significantly higher than that of the distractors, this is due to a few outliers; more
interestingly, the median for the BNC score distributions is zero for all antecedent
groups. This will affect precision for a BNC-based algorithm because of the small
amount of data decisions are based on. In addition, although the number of zero scores
26 The star operator indicates zero or more occurrences of a variable. The variable D can be instantiated by
any determiner; the variable A can be instantiated by any adjective or cardinal number.
27 Difference in means was calculated via a t-test; for medians we used chi-square, and for zero counts a
t-test for proportions. The significance level used was 5%.
382
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 5
Descriptive statistics for Web scores and BNC scores for other-anaphora.
Min?Max Mean SD Med 0 scores 1 scores scores > 1
All possible antecedents (Total: 3,875)
BNC 0?22 0.07 0.60 0 3,714 (95.8%) 109 (2.8%) 52 (1.4%)
Web 0?283,000 542.15 8,352.46 2 1,513 (39.0%) 270 (7.0%) 2,092 (54.0%)
Correct antecedents (Total: 419)
BNC 0?22 0.32 1.62 0 360 (85.9%) 39 (9.3%) 20 (4.8%)
Web 0?283,000 2,416.68 15,947.93 68 82 (19.6%) 11 (2.6%) 326 (77.8%)
Lenient antecedents (Total: 160)
BNC 0?4 0.21 0.62 0 139 (86.9%) 13 (8.1%) 8 (5.0%)
Web 0?8,840 807.63 1,718.13 68.5 36 (22.5%) 3 (1.9%) 121 (75.6%)
Distractors (Total: 3,296)
BNC 0?6 0.03 0.25 0 3,215 (97.5%) 57 (1.7%) 24 (0.8%)
Web 0?283,000 290.97 7,010.07 1 1,395 (42.3%) 256 (7.8%) 1,645 (49.9%)
for correct/lenient antecedents (85.9%/86.9%) is significantly lower than for distractors
(97.5%), the number of zero scores is well above 80% for all antecedent groups. Thus,
the coverage and recall of a BNC-based algorithm will be very low. Although the
BNC scores are in general much lower than Web scores and although the Web scores
distinguish better between correct/lenient antecedents and distractors, we observe that
Web and BNC scores still correlate significantly, with correlation coefficients between
0.20 and 0.35, depending on antecedent group.28
To summarize, the pattern-based method yields correlated results on different
corpora, but it is expected to depend on large corpora to be really successful.
4.5.3 The Corpus-Based Algorithms. The prototype Web-based algorithm resolves each
anaphor ana to the antecedent candidate in Aanaid with the highest Web score above zero.
If several potential antecedents achieve the same Web score, it uses a tiebreaker based on
string match and recency. If no antecedent candidate achieves a Web score above zero,
string match and recency can be used as a back-off. String comparison for tiebreaker and
back-off can again use the original or the replaced antecedents, yielding two versions,
algoWebv1 (original antecedents) and algoWebv2 (replaced antecedents).
The exact procedure for the version algoWebv1 for an anaphor ana is as follows:29
(i) for each antecedent a in Aanaid, compute its Web score WMa. Compute
the maximum WM of all Web scores over all antecedents in Aanaid. If WMa
is equal to WM and bigger than zero, push a into a set AWManaid;
28 Correlation significance was measured by both a t-test for the correlation coefficient and also by the
nonparametric paired Kendall rank correlation test, both yielding significance at the 1% level.
29 The algorithm algoWebv2 follows the same basic procedure apart from the variation regarding
original/replaced antecedents in string matching.
383
Computational Linguistics Volume 31, Number 3
(ii) if AWManaid contains exactly one element, select this element and stop;
(iii) otherwise, if AWManaid contains more than one element, string-compare
each antecedent in AWManaid with ana (using original antecedents). If exactly
one element of AWManaid matches ana, select this one and stop; if several match
ana, select the closest to ana within these matching antecedents and stop; if
none match, select the closest to ana within AWManaid and stop;
(iv) otherwise, if AWManaid is empty, make no assigment and stop.
The back-off algorithm algoWeb?v1 uses baselineSTR
?
v1 as a back-off (iv?) if no antecedent
can be assigned (parallel to the back-off in algoWN?v1):
(iv?) otherwise, if AWManaid is empty, use baselineSTR?v1 to assign an antecedent
to ana and stop;
algoWebv1 and algoWebv2 can overrule string matching for anaphors in StrSetv1/StrSetv2.
This happens when the Web score of an antecedent candidate that does not match
the anaphor is higher than the Web scores of matching antecedent candidates. In
particular, there is no guarantee that matching antecedent candidates are included
in AWManaid. In that respect, algoWebv1 and algoWebv2 differ from the corresponding
WordNet alorithms: Matching antecedent candidates are always synonyms of the
anaphor (as each noun is a synonym of itself) and therefore always included in Ahyp/synanaid .
Therefore the WordNet alorithms can be seen as a direct extension of baselineSTR;
that is, they achieve the same results as the string-matching baseline on the sets
StrSetv1/StrSetv2.
Given the high precision of baselineSTR, we might want to exclude the possibility
that the Web algorithms overrule string matching. Instead we can use string matching
prior to Web scoring, use the Web scores only when there are no matching antecedent
candidates, and use recency as the final back-off. This variation then achieves the same
results on the sets StrSetv1/StrSetv2 as the WordNet alorithms and the string-matching
baselines. In combination with the possibility of using original or replaced antecedents
for string matching this yields four algorithm variations overall (see Table 6). The
results (see Table 7) do not show any significant differences according to the variation
explored.
The BNC-based algorithms follow the same procedures as the Web-based algo-
rithms, using the BNC scores instead of Web scores. The results (see Table 8) are
disappointing because of data sparseness (see above). No variation yields considerable
improvement over baselineSTRv2 in the final precision?; in fact, in most cases the varia-
Table 6
Properties of the variations for the corpus-based algorithms for other-anaphora.
Replaced/original antecedent Overrule string matching?
v1 original yes
v2 replaced yes
v3 original no
v4 replaced no
384
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 7
Web results for other-anaphora.
Algorithm Coverage Precision Recall F-measure Precision?
algoWebv1 0.950 0.520 0.495 0.507 0.512
algoWebv2 0.950 0.518 0.493 0.505 0.509
algoWebv3 0.958 0.534 0.512 0.523 0.519
algoWebv4 0.961 0.538 0.517 0.527 0.524
Table 8
BNC results for other-anaphora.
Algorithm Coverage Precision Recall F-measure Precision?
algoBNCv1 0.210 0.488 0.103 0.170 0.355
algoBNCv2 0.210 0.488 0.103 0.170 0.360
algoBNCv3 0.417 0.618 0.257 0.363 0.370
algoBNCv4 0.419 0.626 0.262 0.369 0.375
tions just apply a string-matching baseline, either as a back-off or prior to checking BNC
scores, depending on the variation used.
4.6 Discussion and Error Analysis
The performances of the best versions of all algorithms for other-anaphora are summa-
rized in Table 9.
4.6.1 Algorithm Comparison. Algorithms are compared on their final precision? using
two tests throughout this article. We used a t-test to measure the difference between two
algorithms in the proportion of correctly resolved anaphors. However, there are many
examples which are easy (for example, string-matching examples) and that therefore
most or all algorithms will resolve correctly, as well as many that are too hard for all
algorithms. Therefore, we also compare two algorithms using McNemar?s test, which
only relies on the part of the data set in which the algorithms do not give the same
answer.30 If not otherwise stated, all significance claims hold at the 5% level for both the
t-test and McNemar?s test.
The algorithm baselineSTR significantly outperforms baselineREC in precision?,
showing that the ?same predicate match? is quite accurate even though not very
frequent (coverage is only 30.9%). The WordNet-based and Web-based algorithms
achieve a final precision that is significantly better than the baselines? as well as
algoBNC?s. Most interestingly, the Web-based algorithms significantly outperform the
WordNet-based algorithms, confirming our predictions based on the descriptive statis-
tics. The Web approach, for example, resolves Examples (3), (4), (6), and (11) (which
WordNet could not resolve) in addition to Examples (8) and (9), which both the Web
and WordNet alorithms could resolve.
30 We thank an anonymous reviewer for suggesting the use of McNemar?s test for this article.
385
Computational Linguistics Volume 31, Number 3
Table 9
Overview of the results for the best algorithms for other-anaphora.
Algorithm Coverage Precision Recall F-measure Precision?
baselineREC 1.000 0.178 0.178 0.178 0.178
baselineSTRv2 0.309 0.698 0.216 0.329 0.350
algoBNCv4 0.419 0.626 0.262 0.369 0.375
algoWNv2 0.652 0.568 0.370 0.448 0.444
algoWebv4 0.961 0.538 0.517 0.527 0.524
As expected, the WordNet-based algorithms suffer from the problems discussed
in Section 2.2. In particular, Problem 1 proved to be quite severe, as algoWN achieved
a coverage of only 65.2%. Missing links in WordNet alo affect precision if a good
distractor has a link to the anaphor in WordNet, whereas the correct antecedent does
not (Example (10)). Missing links are both universal relations that should be included
in an ontology (such as home:facility) and context-dependent links (e.g., age:(risk) factor,
costs:repercussions; see Problem 2 in Section 2.2.2). Further mining of WordNet beyond
following hyponymy/synonymy links might alleviate Problem 1 but is more costly
and might lead to false positives (Problem 3). To a lesser degree, the WordNet alo-
rithms also suffer from sense proliferation (Problem 4), as all senses of both anaphor
and antecedent candidates were considered. Therefore, some hyp/syn relations based
on a sense not intended in the text were found, leading to wrong-antecedent selection
and lowering precision. In Example (11), for instance, there is no hyponymy link be-
tween the head noun of the correct antecedent (question) and the head noun of the
anaphor (issue), whereas there is a hyponymy link between issue and person = [Mr.
Dallara] (using the sense of issue as offspring) as well as a synonymy link between number
and issue. While in this case considering the most frequent sense of the anaphor issue as
indicated in WordNet would help, this would backfire in other cases in our data set in
which issue is mostly used in the minority sense of stock, share. Obviously, prior word
sense disambiguation would be the most principled but also a more costly solution.
(11) While Mr. Dallara and Japanese officials say the question of investors
access to the U.S. and Japanese markets may get a disproportionate
share of the public?s attention, a number of other important economic
issues [. . . ]
The Web-based method does not suffer as much from these problems. The linguistically
motivated patterns we use reduce long-distance dependencies between anaphor and
antecedent to local dependencies. By looking up these patterns on the Web we make
use of a large amount of data that is very likely to encode strong semantic links
via these local dependencies and to do so frequently. This holds both for universal
hyponymy relations (addressing Problem 1) and relations that are not necessarily to
be included in an ontology (addressing Problem 2). The problem of whether to include
subjective and context-dependent relations in an ontology (Problem 2) is circumvented
by using Web scores only in comparison to Web scores of other antecedent candidates.
In addition, the Web-based algorithm needs no hand-processing or hand-modeling
whatsoever, thereby avoiding the manual effort of building ontologies. Moreover, the
local dependencies we use reduce the need for prior word sense disambiguation (Prob-
lem 4), as the anaphor and the antecedent constrain each other?s sense within the
386
Markert and Nissim Knowledge Sources for Anaphora Resolution
Figure 1
Decision tree for error classification.
context of the pattern. Furthermore, the Web scores are based on frequency, which
biases the Web-based algorithms toward frequent senses as well as sense pairs that
occur together frequently. Thus, the Web algorithm has no problem resolving issue
to question in Example (11) because of the high frequency of the query question OR
questions and other issues. Problem 3 is still not addressed, however, as any corpus can
encode the same semantic relations via different patterns. Combining patterns might
therefore yield problems similar to those presented by combining information sources
in an ontology.
Our pattern-based method, though, seems to work on very large corpora only.
Unlike the Web-based algorithms, the BNC-based ones make use of POS tagging
and observe sentence boundaries, thus reducing the noise intrinsic to an unprocessed
corpus like the Web. Moreover, the instantiations used in algoBNC allow for modifi-
cation to occur (see Table 4), thus increasing chances of a match. Nevertheless, the
BNC-based algorithms performed much worse than the Web-based ones: Only 4.2% of
all pattern instantiations were found in the BNC, yielding very low coverage and recall
(see Table 5).
4.6.2 Error Analysis. Although the Web algorithms perform best, algoWEBv4 still incurs
194 errors (47.6% of 408). Because in several cases there is more than one reason for a
wrong assignment, we use the decision tree in Figure 1 for error classification. By using
this decision tree, we can, for example, exclude from further analysis those cases that
none of the algorithms could resolve because of their intrinsic design.
As can be seen in Table 10, quite a large number of errors result from deleting
pronouns as well as not dealing with split antecedents (44 cases, or 22.7% of all mis-
takes).31 Out of these 44, 30 involve split antecedents. In 19 of these 30 cases, one
of the several correct antecedents has indeed been chosen by our algorithm, but all
the correct antecedents need to be found to allow for the resolution to be counted as
correct.
Given the high number of NE antecedents in our corpus (43.8% of correct, 25%
of all antecedents; see Table 1), NE resolution is crucial. In 11.3% of the cases, the
algorithm selects a distractor instead of the correct antecedent because the NER module
31 Percentages of errors are rounded to the first decimal; rounding errors account for the coverage of 99.9%
of errors instead of 100%.
387
Computational Linguistics Volume 31, Number 3
Table 10
Occurrences of error types for the best other-anaphora algorithm algoWebv4.
Error type Number of cases Percentage of cases
Design 44 22.7
Named entity 22 11.3
String matching 19 9.8
Zero score 48 24.7
Tiebreaker 13 6.7
Other 48 24.7
Total 194 99.9
either leaves the correct antecedent unresolved (which could then lead to very few or
zero hits in Google) or resolves the named entity to the wrong NE category. String
matching is a minor cause of errors (under 10%). This is because, apart from its being
generally reliable, there is also a possible string match only in just about 30% of the cases
(see Table 2).
Many mistakes, instead, occur because other-anaphora can express heavily context-
dependent and very unconventional relations, such as the description of dolls as winners
in Example (12).
(12) Coleco bounced back with the introduction of the Cabbage Patch
dolls. [. . . ] But as the craze died, Coleco failed to come up with another
winner. [. . . ]
In such cases, the relation between the anaphor and antecedent head nouns is not
frequent enough to be found in a corpus even as large as the Web.32 This is mir-
rored in the high percentage of zero-score errors (24.7% of all mistakes). Although the
Web algorithm suffers from a knowledge gap to a smaller degree than WordNet,
there is still a substantial number of cases in which we cannot find the right lexical
relation.
Errors of type other are normally due to good distractors that achieve higher Web
scores than the correct antecedent. A common reason is that the wished-for relation
is attested but rare and therefore other candidates yield higher scores. This is simi-
lar to zero-score errors. Furthermore, the elimination of modification, although useful to
reduce data sparseness, can sometimes lead to the elimination of information that
could help disambiguate among several candidate antecedents. Lastly, lexical informa-
tion, albeit crucial and probably more important than syntactic information (Modjeska
2002), is not sufficient for the resolution of other-anaphora. The integration of other
features, such as grammatical function, NP form, and discourse structure, could prob-
ably help when very good distractors cannot be ruled out by purely lexical methods
(Example (10)). The integration of the Web feature in a machine-learning algorithm
using several other features has yielded good results (Modjeska, Markert, and Nissim
2003).
32 Using different or simply more patterns might yield some hits for anaphor?antecedent pairs that return a
zero score when instantiated in the pattern we use in this article.
388
Markert and Nissim Knowledge Sources for Anaphora Resolution
5. Case Study II: Definite NP Coreference
The Web-based method we have described outperforms WordNet as a knowledge
source for antecedent selection in other-anaphora resolution. However, it is not clear
how far the method and the achieved comparative results generalize to other kinds of
full NP anaphora. In particular, we are interested in the following questions:
 Is the knowledge gap encountered in WordNet for other-anaphora equally
severe for other kinds of full NP anaphora? A partial (mostly affirmative)
answer to this is given by previous researchers, who put the knowledge
gap for coreference at 30?50% and for bridging at 38?80%, depending on
language, domain, and corpus (see Section 2).
 Do the Web-based method and the specific search patterns we use
generalize to other kinds of anaphora?
 Do different anaphoric phenomena require different lexical knowledge
sources?
As a contribution, we investigate the performance of the knowledge sources discussed
for other-anaphora in the resolution of coreferential NPs with full lexical heads,
concentrating on definite NPs (see Example (1)). The automatic resolution of such
anaphors has been the subject of quite significant interest in the past years, but results
are much less satisfactory than those obtained for the resolution of pronouns (see
Section 2).
The relation between the head nouns of coreferential definite NPs and their
antecedents is again, in general, one of hyponymy or synonymy, making an extension
of our approach feasible. However, other-anaphors are especially apt at conveying
context-specific or subjective information by forcing the reader via the other-expression
to accommodate specific viewpoints. This might not hold for definite NPs.33
5.1 Corpus Collection
We extracted definite NP anaphors and their candidate antecedents from the MUC-6
coreference corpus, including both the original training and test material, for a total
of 60 documents. The documents were automatically preprocessed in the following
way: All meta-information about each document indicated in XML (such as WSJ cat-
egory and date) was discarded, and the headline was included and counted as one
sentence. Whenever headlines contained three dashes, everything after the dashes was
discarded.
We then converted the MUC coreference chains into an anaphor?antecedent anno-
tation concentrating on anaphoric definite NPs. All definite NPs which are in, but not
at the beginning of, a coreference chain are potential anaphors. We excluded definite
NPs with proper noun heads (such as the United States) from this set, since these do
not depend on an antecedent for interpretation and are therefore not truly anaphoric.34
We also excluded appositives, which provide coreference structurally and are therefore
33 We thank an anonymous reviewer for pointing out that this role for coreference is more likely to be
provided by demonstratives than definite NPs.
34 Proper-noun heads are approximated by capitalization in the exclusion procedure.
389
Computational Linguistics Volume 31, Number 3
not anaphoric. Otherwise, we strictly followed the MUC annotation for coreference in
our extraction, although it is not entirely consistent and not necessarily comprehensive
(van Deemter and Kibble 2000). This extraction method yielded a set of 565 anaphoric
definite NPs.
For each extracted anaphor in a coreference chain C we regard the NP in C that is
closest to the anaphor as the correct antecedent, whereas all other previous mentions
in C are regarded as lenient. NPs that occur before the anaphor but are not marked as
being in the same coreference chain are distractors. Since anaphors with split antecedents
are not annotated in MUC, anaphors cannot have more than one correct antecedent. In
Example (13), the NPs with the head nouns Pact, contract, and settlement are marked
as coreferent in MUC: In our annotation, the settlement is an anaphor with a correct
antecedent headed by contract and a lenient antecedent Pact. Other NPs prior to the
anaphor (e.g., Canada or the IWA-Canada union) are distractors.35
(13) Forest Products Firms Tentatively Agree On Pact in Canada. A group of
large British Columbia forest products companies has reached a tentative,
three-year labor contract with about 18,000 members of the IWA-Canada union,
. . .The settlement involves . . .
With respect to other-anaphora, we expanded our window size from two to five sen-
tences (the current and the four previous sentences) and excluded all anaphors with
no correct or lenient antecedent within this window size, thus yielding a final set of 477
anaphors (84.4% of 565). This larger window size is motivated by the fact that a window
size of two would cover only 62.3% of all anaphors (352 out 565).
5.2 Antecedent Extraction, Preprocessing, and Baselines
All NPs prior to the anaphor within the five-sentence window were extracted
as antecedent candidates.36 We further processed anaphors and antecedents as in
Case Study I (see Section 4.2): Modification was stripped and all NPs were lemmatized.
In this experiment, named entities were resolved using Curran and Clark?s (2003) NE
tagger rather than GATE.37 The identified named entities were further subclassified into
finer-grained entities, as described for Case Study I.
The final number of extracted antecedents for the whole data set of 477 anaphors is
14,233, with an average of 29.84 antecedent candidates per anaphor. This figure is much
higher than the average number of antecedent candidates for other-anaphors (10.5)
because of the larger window size used. The data set includes 473 correct antecedents,
803 lenient antecedents, and 12,957 distractors. Table 11 shows the distribution of NP
types for correct and lenient antecedents and for distractors.
There are fewer correct antecedents (473) than anaphors (477) because the MUC
annotation also includes anaphors whose antecedent is not an NP but, for exam-
ple, a nominal modifier in a compound. Thus, in Example (14), the bankruptcy code
is annotated in MUC as coreferential to bankruptcy-law, a modifier in bankruptcy-law
protection.
35 All examples in the coreference study are from the MUC-6 corpus.
36 This extraction was conducted manually, to put this study on an equal footing with Case Study I. It
presupposes perfect NP chunking. A further discussion of this issue can be found in Section 6.
37 Curran and Clark?s (2003) tagger was not available to us during the first case study. Both NE taggers are
state-of-the-art taggers trained on newspaper text.
390
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 11
Distribution of antecedent NP types for definite NP anaphora.
Correct Lenient Distractors All
Pronouns 70 145 1,078 1,293
Named entities 123 316 3,108 3,547
Common nouns 280 342 8,771 9,133
Total 473 803 12,957 14,233
(14) All legal proceedings against Eastern, a unit of Texas Air Corp., were put
on hold when Eastern filed for bankruptcy-law protection March 9. . . . If it
doesn?t go quickly enough, the judge said he may invoke a provision of
the bankruptcy code [. . . ]
In our scheme we extract the bankruptcy code as anaphoric but our method of extract-
ing candidate antecedents does not include bankruptcy-law. Therefore, there are four
anaphors in our data set with no correct/lenient antecedent extracted. These cannot be
resolved by any of the suggested approaches.
We use the same evaluation measures as for other-anaphora as well as the same
significance tests for precision?. We also use the same baseline variations baselineREC,
baselineSTRv1, and baselineSTRv2 (see Table 12 and cf. Table 2). The recency baseline per-
forms worse than for other-anaphora. String matching improves dramatically on simple
recency. It also seems to be more relevant than for our other-anaphora data set, achieving
higher coverage, precision, and recall. This confirms the high value of string matching
that has been assigned to coreference resolution by previous researchers (Soon, Ng, and
Lim 2001; Strube, Rapp, and Mueller 2002, among others).
As the MUC data set does not include split antecedents, an anaphor ana usually
agrees in number with its antecedent. Therefore, we also explored variations of all
algorithms that as a first step delete from Aanaid all candidate antecedents that do not
agree in number with ana.38 The algorithms then proceed as usual. Algorithms that
use number checking are marked with an additional n in the subscript. Using number
checking leads to small but consistent gains for all baselines.
As in Case Study I, we deleted pronouns for the WordNet- and corpus-based meth-
ods, thereby removing 70 of 473 (14.8%) of correct antecedents (see Table 11). After
pronoun deletion, the total number of antecedents in our data set is 12,940 for 477
anaphors, of which 403 are correct antecedents, 658 are lenient antecedents, and 11,879
are distractors.
38 The number feature can have the values singular, plural, or unknown. All NE antecedent candidates
received the value singular, as this was by far the most common occurrence in the data set. Information
about the grammatical number of anaphors and common-noun antecedent candidates was calculated
and retained as additional information during the lemmatization process. If lemmatization to both a
plural and a singular noun (as determined by WordNet and CELEX) was possible (for example, the word
talks could be lemmatized to talk or talks), the value unknown was used. An anaphor and an antecedent
candidate were said to agree in number if they had the same value or if at least one of the two values was
unknown.
391
Computational Linguistics Volume 31, Number 3
Table 12
Overview of the results for all baselines for coreference.
Algorithm Coverage Precision Recall F-measure Precision?
baselineREC 1.000 0.031 0.031 0.031 0.031
baselineSTRv1 0.637 0.803 0.511 0.625 0.532
baselineSTRv2 0.717 0.775 0.555 0.647 0.570
With number checking
baselineRECn 1.000 0.086 0.086 0.086 0.086
baselineSTRv1n 0.614 0.833 0.511 0.634 0.549
baselineSTRv2n 0.694 0.809 0.562 0.664 0.591
5.3 WordNet for Antecedent Selection in Definite NP Coreference
We hypothesize that again most antecedents are hyponyms or synonyms of their
anaphors in definite NP coreference (see Examples (1) and (13)). Therefore we use the
same look-up for hyp/syn relations that was used for other-anaphora (see Section 4.4),
including the specifications for common noun and proper name look-ups. Parallel to
Table 3, Table 13 summarizes how many correct and lenient antecedents and distractors
stand in a hyp/syn relation to their anaphor in WordNet.
As already observed for other-anaphora, correct and lenient antecedents stand in a
hyp/syn relation to their anaphor significantly more often than distractors do (t-test,
p < 0.001). Hyp/syn relations in WordNet might be better at capturing the relation
between antecedent and anaphors for definite NP coreference than for other-anaphora:39
A higher percentage of correct and lenient antecedents of definite NP coreference
(71.96%/67.78%) stand in a hyp/syn relation to their anaphors than is the case for
other-anaphora (43.0%/42.5%). At the same time, though, there is no difference in the
percentage of distractors that stand in a hyp/syn relation to their anaphors (9% for other-
anaphora, 8.80% for definite NP coreference). For our WordNet alorithms, this is likely
to translate directly into higher coverage and recall and potentially into higher precision
than in Case Study I. Still, about 30% of correct antecedents are not in a hyp/syn
relation to their anaphor in the current case study, confirming results by Harabagiu,
Bunescu, and Maiorano (2001), who also look at MUC-style corpora.40 This gap, though,
is alleviated by a quite high number of lenient antecedents, whose resolution can make
up for a missing link between anaphor and correct antecedent.41
The WordNet-based algorithms are defined exactly as in Section 4.4, with the
additional two algorithms that include number checking. Results are summarized in
Table 14.
All variations of the WordNet alorithms perform significantly better than the
corresponding versions of the string-matching baseline (i.e., algoWNv1 is better than
baselineSTRv1, . . . , algoWNv2n is better than baselineSTRv2n), showing that they add
39 Some of this difference might be due to the corpus used instead of the phenomenon as such.
40 Harabagiu, Bunescu, and Maiorano (2001) include all common-noun coreference links in their countings,
whereas we concentrate on definite NPs only, so that the results are not exactly the same.
41 The possibility of resolving to lenient antecedents follows a similar approach as that of Ng and Cardie
(2002b), who suggest a ?best-first? coreference resolution approach instead of a ?most recent first?
approach.
392
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 13
Descriptive statistics for WordNet hyp/syn relations on the coreference data set.
Hyp/syn relation to anaphor No hyp/syn relation Total
Correct antecedents 290 (71.96%) 113 (28.04%) 403 (100%)
Lenient antecedents 446 (67.78%) 212 (32.22%) 658 (100%)
Distractors 1,046 (8.80%) 10,833 (91.20%) 11,879 (100%)
All antecedents 1,782 (13.77%) 11,158 (86.23%) 12,940 (100%)
additional lexical knowledge to string matching. As expected from the descriptive
statistics discussed above, the results are better than those obtained by the WordNet
algorithms for other-anaphora, even if we disregard the additional morphosyntactic
number constraint.
5.4 The Corpus-Based Approach for Definite NP Coreference
Following the assumption that most antecedents are hyponyms or synonyms of
their anaphors in definite NP coreference, we use the same list-context pattern and
instantiations that were used for other-anaphora, allowing us to evaluate whether they
are transferrable. The corpora we use are again the Web and the BNC.
As with other-anaphora, the Web scores do well in distinguishing between cor-
rect/lenient antecedents and distractors, with significantly higher means/medians for
correct/lenient antecedents (median 472/617 vs. 2 for distractors), as well as signifi-
cantly fewer zero scores (8% for correct/lenient vs. 41% for distractors). This indicates
transferability of the Web-based approach to coreference. Compared to other-anaphora,
the number of zero-scores is lower for correct/lenient antecedent types, so that we
expect better overall results, similar to our expectations for the WordNet alorithm.
The BNC scores can also distinguish between correct/lenient antecedents and
distractors, since the number of zero scores for correct/lenient antecedents (68.98%/
58.05%) is significantly lower than for distractors (96.97%). Although more than
50% of correct/lenient antecedents receive a zero score, there are fewer zero scores
than for other-anaphora (for which more than 80% of correct/lenient antecedents re-
ceived zero scores). However, BNC scores are again in general much lower than Web
scores, as measured by means, medians, and zero scores. Nevertheless, Web scores
and BNC scores correlate significantly, with the correlations reaching higher coeffi-
Table 14
Overview of the results for all WordNet alorithms for coreference.
Algorithm Coverage Precision Recall F-measure Precision?
algoWNv1 0.874 0.715 0.625 0.666 0.631
algoWNv2 0.874 0.724 0.633 0.676 0.639
With number checking
algoWNv1n 0.866 0.734 0.635 0.681 0.648
algoWNv2n 0.866 0.751 0.649 0.697 0.662
393
Computational Linguistics Volume 31, Number 3
cients (0.53 to 0.65, depending on antecedent group) than they did in the case study for
other-anaphora.
The corpus-based algorithms for coreference resolution are parallel to those
described for other-anaphora and are marked by the same subscripts. The variations that
include number checking are again marked by a subscript n. Tables 15 and 16 report the
results for all the Web and BNC algorithms, respectively.
5.5 Discussion and Error Analysis
5.5.1 Algorithm Comparison. Using the original or the replaced antecedent for string
matching (versions v1 vs. v2, v1n vs. v2n, v3 vs. v4, and v3n vs. v4n) never results
in interesting differences in any of the approaches discussed. Also, number matching
provides consistent improvements. Therefore, from this point on, our discussion will
disregard those variations, that use original antecedents only (v1, v1n, v3, and v3n) as
well as algorithms that do not use number matching (v2, v4). We will also concentrate
on the final precision? of the full-coverage algorithms. The set of anaphors that are cov-
ered by the best string-matching baseline, prior to recency back-off, will again be denoted
by StrSetv2n. Again, both a t-test and McNemar?s test will be used, when statements
about significance are made.
The results for the string-matching baselines and for the lexical methods are higher
for definite coreferential NPs than for other-anaphora. This is largely a result of the
higher number of string-matching antecedent/anaphor pairs in coreference, the higher
precision of string matching, and to a lesser degree, the lower number of unusual
redescriptions.
Similar to the results for other-anaphora, the WordNet-based algorithms beat the
corresponding baselines. The first striking result is that the Web algorithm variation
algoWebv2n, which relies only on the highest Web scores and is therefore allowed
to overrule string matching, does not outperform the corresponding string-matching
baseline baselineSTRv2n and performs significantly worse than the corresponding
WordNet alorithm algoWNv2n. This contrasts with the results for other-anaphora. When
the results were examined in detail, it emerged that for a considerable number of
anaphors in StrSetv2n, the highest Web score was indeed achieved by a distractor with
a high-frequency head noun when the correct or lenient antecedent could be instead
found by a simple string match to the anaphor. This problem is much more severe than
Table 15
Overview of the results for all Web algorithms for coreference.
Algorithm Coverage Precision Recall F-measure Precision?
algoWebv1 0.994 0.561 0.558 0.559 0.562
algoWebv2 0.994 0.553 0.549 0.550 0.554
algoWebv3 0.998 0.674 0.673 0.673 0.673
algoWebv4 0.998 0.679 0.677 0.678 0.677
With number checking
algoWebv1n 0.992 0.613 0.608 0.610 0.612
algoWebv2n 0.992 0.607 0.602 0.604 0.606
algoWebv3n 0.996 0.705 0.702 0.703 0.703
algoWebv4n 0.996 0.716 0.713 0.714 0.713
394
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 16
Overview of the results for all BNC algorithms for coreference.
Algorithm Coverage Precision Recall F-measure Precision?
algoBNCv1 0.438 0.559 0.245 0.341 0.524
algoBNCv2 0.438 0.559 0.245 0.341 0.526
algoBNCv3 0.769 0.749 0.576 0.651 0.589
algoBNCv4 0.777 0.757 0.589 0.663 0.599
With number checking
algoBNCv1n 0.411 0.612 0.251 0.356 0.562
algoBNCv2n 0.411 0.622 0.256 0.369 0.570
algoBNCv3n 0.753 0.769 0.579 0.661 0.610
algoBNCv4n 0.761 0.785 0.597 0.678 0.627
for other-anaphora because of (1) the larger window size that includes more distractors
and (2) the higher a priori precision of the string-matching baseline, which means that
overruling string matching leads to wrong results more frequently. Typical examples
involve named-entity recognition and inverted queries. Thus, in Example (15), the
anaphor the union is coreferent with the first occurrence of the union, a case easily
resolved by string matching. However, the distractor organization [= Chrysler Canada]
achieves a higher Web score, because of the score of the inverted query union OR unions
and other organizations.42
(15) [. . . ] The union struck Chrysler Canada Tuesday after rejecting a company
offer on pension adjustments. The union said the size of the adjustments
was inadequate.
Several potential solutions exist to this problem, such as normalization of Web scores
or penalizing of inverted queries. The solution we have adopted in algoWebv4n is to
use Web scores only after string matching, thereby making the Web-based approach
more comparable to the WordNet approach. Therefore, baselineSTRv2n, algoWebv4n, and
algoWNv2n (as well as algoBNCv4n) all coincide in their decisions for anaphors in StrSetv2n
and only differ in the decisions made for anaphors that do not have a matching
antecedent candidate. Indeed, algoWebv4n performs significantly better than the base-
lines at the 1% level, and results rise from a precision? of 60.6% for algoWebv2n to 71.3%
for algoWebv4n. It also significantly outperforms the best BNC results, thus showing that
overcoming data sparseness is more important than working with a controlled, tagged,
and representative corpus. Furthermore, shows better performance than WordNet in the
final algorithm variation (71.3% vs. 66.2%).43 According to results of a t-test, however,
this last difference is not significant. McNemar?s test, concentrating on the part of the
data in which the methods differ, shows instead significance at the 1% level.
Indeed, one of the problems in comparing algorithm results for coreference is that
such a large number of anaphors are covered by simple string matching, leaving only
42 Remember that this problem does not affect the WordNet-based algorithm, which always achieves the
same results as the string-matching baseline on StrSetv2n. Both the correct antecedent and the organization
[= Chrysler Canada] distractor stand in a hyp/syn relation to the anaphor, and then string matching is
used as a tiebreaker.
43 In general, the WordNet methods achieve higher precision, with the Web method achieving higher recall.
395
Computational Linguistics Volume 31, Number 3
a small data set on which the lexical methods can differ. Thus, StrSetv2n contains 331 of
477 cases (268 of which are assigned correctly by baselineStrv2n), so that improvements by
the other methods are confined to the set of the remaining 146 anaphors. Of these 146,
baselineStr?v2n assigns the correct antecedent to 13 (8.9%) anaphors by using a recency
back-off, the best WordNet method to 55 anaphors (37.67%), and the best Web method
to 72 anaphors (49.31%). Therefore the Web-based method is a better complement to
string matching than WordNet, which is reflected in the results of McNemar?s test.
Anaphor?antecedent relations that were not covered in WordNet but that did not prove
a problem for the Web algorithm were again both general hyponymy relations, such as
retailer:organization, bill:legislation and month:time, and more subjective relations like (wage)
cuts:concessions and legislation:attack.
5.5.2 Error Analysis. The best-performing Web-based algorithm, algoWebv4n, still selects
the wrong antecedent for a given anaphor in 137 of 477 cases (28.7%). Again, we use
the decision tree in Figure 1 to classify errors. Design errors now do not include split
antecedents but do include errors that occur because the condition of number agreement
was violated, pronoun deletion errors, and the four cases in which the antecedent is a
non-NP antecedent and therefore not extracted in the first place (see Section 5.1 and
Example (14)). Table 17 reports the frequency of each error type.
Differently from other-anaphora, the design and NE errors together account for
under 15% of the mistakes. Also rare are zero-score errors (only 8%). When compared
to the number of zero-score errors in other anaphora (24.7%), this low figure suggests
that other-anaphora is more prone to exploit rare, unusual, and context-dependent
redescriptions than full NP coreference. Nevertheless, it is yet possible to find non-
standard redescriptions in coreference as well which yield zero scores, such as the use
of transaction to refer to move in Example (16).
(16) Conseco Inc., in a move to generate about $200 million in tax deductions,
said it induced five of its top executives to exercise stock options to
purchase about 3.6 million common shares of the financial-services
concern. As a result of the transaction, . . .
Much more substantial is the weight of errors due to string matching, tiebreaker deci-
sions, and the presence of good distractors (the main reason for errors of type other),
which together account for over three-quarters of all mistakes.
String matching is quite successful for coreference (baselineSTRv2n covers nearly
70% of the cases with a precision of 80.9%). However, because algoWebv4n never over-
Table 17
Occurrences of error types for the best coreference algorithm algoWebv4n.
Error type Number of cases Percentage of cases
Design 12 8.7
Named entity 7 5.1
String matching 33 24.1
Zero scores 11 8.0
Tiebreaker 34 24.8
Other 40 29.2
Total 137 99.9
396
Markert and Nissim Knowledge Sources for Anaphora Resolution
rules string matching, the errors of baselineSTRv2n are preserved here and account for
24.1% of all mistakes.44 Tiebreaker errors are quite frequent too (24.8%), as our far-from-
sophisticated tiebreaker was needed in nearly half of the cases (224 times; 47.0%).
The remaining errors (29.2%) are due to the presence of good distractors that score
higher than the correct/lenient antecedent. In Example (17), for instance, a distractor
with a higher Web score (comment) prevents the algorithm from selecting the correct
antecedent (investigation) for the anaphor the inquiry.
(17) Mr. Adams couldn?t be reached for comment. Though the investigation has
barely begun, persons close to the board said Messrs. Lavin and Young
will get a ?hard look? as to whether they were involved, and are both
considered a ?natural focus? of the inquiry.
Example (18) shows how stripping modification might have eliminated information
crucial to identifying the correct antecedent: Only the head process was retained of the
anaphor arbitration process, so that the surface link between anaphor and antecedent
(arbitration) was lost and the distractor securities industry, reduced to industry, was
instead selected.
(18) The securities industry has favored arbitration because it keeps brokers and
dealers out of court. But consumer advocates say customers sometimes
unwittingly sign away their right to sue. ?We don?t necessarily have a beef
with the arbitration process,? says Martin Meehan, [. . . ]
6. Open Issues
6.1 Preprocessing and Prior Assumptions
Our algorithms build on two main preprocessing assumptions. First, we assume perfect
base-NP chunking and expect results to be lower with automatic chunking. Neverthe-
less, since automatic chunking will affect all algorithms in the same way, we do expect
comparative results to stand. We are not, however, dependent on full parsing, as no
parsing-dependent grammatical features are used by the algorithms.
Second, the anaphoricity of the definite NPs in Case Study II has de facto been
manually determined, as we restrict our study to antecedent selection for the NPs
that are marked in the MUC corpus as coreferent. One of the reasons why pronoun res-
olution has been more successful than definite NP resolution is that whereas pronouns
are mostly anaphoric, definite NPs do not have to be so (see Section 2). In fact, it has been
argued by several researchers that an anaphora resolution algorithm should proceed to
antecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng
2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a two-
stage process which we also follow in this article. Although recent work on automatic
anaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), our
algorithms will perform worse when building on non-manually determined anaphors.
Future work will explore the extent of such a decrease in performance.
44 Some of the errors incured by baselineSTRv2n are here classified as design, NE, or tiebreaker errors.
397
Computational Linguistics Volume 31, Number 3
6.2 Directions for Improvement
All algorithms we have described can be considered blueprints for more complex
versions. Specifically, the WordNet-based algorithms could be improved by exploiting
information encoded in WordNet beyond explicitly encoded links (glosses could be
mined, too, for example; see also Harabagiu, Bunescu, and Maiorano [2001]). The Web-
based algorithms could similarly benefit from the exploration of different patterns and
their combination, as well as from using non-pattern-based approaches for hyponymy
detection (Shinzato and Torisawa 2004). In addition, we have evaluated the contribution
of lexical resources in isolation rather than within a more sophisticated system that
integrates additional non-lexical features. It is unclear whether integrating such knowl-
edge sources in a full-resolution system might even out the differences between the
Web-based and the WordNet-based algorithms or exacerbate them. Modjeska, Markert,
and Nissim (2003) included a feature based on Web scores in a naive Bayes model
for other-anaphora resolution that also used grammatical features and showed that
the addition of the Web feature yielded an 11.4-percentage-point improvement over
using a WordNet-based feature. This gives some indication that additional grammatical
features might not be able to compensate fully for the knowledge gap encountered in
WordNet.
6.3 Extension to Yet Other Anaphora Types
Using the Web for antecedent selection in anaphora resolution is novel and needs
further study for other types of full NP anaphora than the ones studied in this article.
If an anaphora type exploits hyponymy/synonymy relationships between anaphor and
antecedent head nouns, it can in principle be treated with the exact same pattern we
used in this article. This holds, for example, for demonstratives and such-anaphors. The
latter, in particular, are similar to other-anaphora in that they establish a comparison
between the entity they invoke and that invoked by the antecedent and are also easily
used to accommodate subjective viewpoints. They should therefore benefit especially
from not relying wholly on standard taxonomic links.
Different patterns can be developed for anaphora types that build on non-
hyponymy relations. For example, bridging exploits meronymy and/or causal rela-
tions (among others). Therefore, patterns that express ?part-of? links, for example,
such as X of Y and genitives, would be appropriate. Indeed, these patterns have been
recently used in Web search for antecedent selection for bridging anaphora by Poesio
et al (2004). They compare accuracy in antecedent selection for a method that inte-
grates Web hits and focusing techniques with a method that uses WordNet and fo-
cusing, achieving comparable results for both methods. This strenghtens our hypothesis
that antecedent selection for full NP anaphora without hand-modeled lexical knowl-
edge has become feasible.
7. Conclusions
We have explored two different ways of exploiting lexical knowledge for antecedent
selection in other-anaphora and definite NP coreference. Specifically, we have compared
a hand-crafted and -structured source of information such as WordNet and a simple
and inexpensive pattern-based method operating on corpora. As corpora we have used
the BNC and also suggested the Web as the biggest corpus available.
398
Markert and Nissim Knowledge Sources for Anaphora Resolution
We confirmed results by other researchers that show that a substantial number of
lexical links often exploited in coreference are not included in WordNet. We have also
shown the presence of an even more severe knowledge gap for other-anaphora (see also
Question 1 in Section 1). Largely because of this knowledge gap, the novel Web-based
method that we proposed proved better than WordNet at resolving other-anaphora.
Although the gains for coreference are not as high, the Web-based method improves
more substantially on string-matching techniques for coreference than WordNet does
(see the success rate beyond StrSetv2n for coreference, Section 5.5). In both studies, the
Web-based method clearly outperformed the BNC-based one. This shows that, for our
tasks, overcoming data sparseness was more important than working with a manually
controlled, virtually noise-free, but relatively small corpus, which addresses Question 2
in Section 1: Corpus-induced knowledge can indeed rival and even outperform the
knowledge obtained via lexical hierarchies, as long as the corpus is large enough.
Corpus-based methods can therefore be a very useful complement to resolution al-
gorithms for languages for which hand-crafted taxonomies have not yet been created
but for which large corpora do exist. In answer to Question 3 in Section 1, our results
suggest that different anaphoric phenomena suffer in varying degrees from missing
knowledge and that the Web-based method performs best when used to deal with
phenomena that standard taxonomy links do not capture that easily or that frequently
exploit subjective and context-dependent knowledge.
In addition, the Web-based method that we propose does not suffer from some
of the intrinsic limitations of ontologies, specifically, the problem of what knowledge
should be included (see Section 2.2). It is also inexpensive and does not need any
postprocessing of the Web pages returned or any hand-modeling of lexical knowledge.
To summarize, antecedent selection for other-anaphora and definite NP coreference
without hand-crafted lexical knowledge is feasible. This might also be the case for yet
other full NP anaphora types with similar properties?an issue that we will explore in
future work.
Acknowledgments
We especially thank Natalia Modjeska for
providing us with her annotated corpus of
other-anaphors as well as with the extracted
and partially preprocessed sets of candidate
antecedents for Case Study I. She also
collaborated on previous related work on
other-anaphora (Markert, Nissim, and
Modjeska 2003; Modjeska, Markert, and
Nissim 2003) on which this article builds. We
would also like to thank Johan Bos, James
Curran, Bonnie Webber, and four
anonymous reviewers for helpful comments,
which allowed us to greatly improve this
article. Malvina Nissim was partially
supported by Scottish Enterprise
Stanford-Link Grants R36766 (Paraphrase
Generation) and R36759 (SEER).
References
Ariel, Mira. 1990. Accessing Noun Phrase
Antecedents. Routledge, London and
New York.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large corpora.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics,
pages 57?64, Providence, RI.
Bierner, Gann. 2001. Alternative phrases
and natural language information
retrieval. In Proceedings of the 39th
Annual Meeting of the Association for
Computational Linguistics, pages 58?65,
Toulouse, France.
Burnard, Lou, 1995. Users? Reference Guide,
British National Corpus. British National
Corpus Consortium, Oxford.
Caraballo, Sharon. 1999. Automatic
acquisition of a hypernym-labelled noun
hierarchy from text. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 120?126,
Providence, RI.
Chinchor, Nancy. 1997. MUC-7 named entity
task definition. In Proceedings of the Seventh
Conference on Message Understanding,
Washington, DC.
399
Computational Linguistics Volume 31, Number 3
Christ, Oliver, 1995. The XKWIC User Manual.
Institute for Computational Linguistics,
University of Stuttgart.
Clark, Herbert H. 1975. Bridging. In
Proceedings of the Conference on Theoretical
Issues in Natural Language Processing,
pages 169?174, Cambridge, MA.
Connolly, Dennis, John D. Burger, and
David S. Day. 1997. A machine learning
approach to anaphoric reference. In
Daniel Jones and Harold Somers, editors,
New Methods in Language Processing.
University College London Press, London,
pages 133?144.
Curran, James and Stephen Clark. 2003.
Language independent NER using
a maximum entropy tagger. In
Proceedings of the Seventh Conference on
Natural Language Learning (CoNLLO3),
pages 164?167, Edmonton, Alberta,
Canada.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fraurud, Kari. 1990. Definiteness and the
processing of NPs in natural discourse.
Journal of Semantics, 7:395?433.
Gardent, Claire, Helene Manuelian, and
Eric Kow. 2003. Which bridges for
bridging definite descriptions? In
Proceedings of the EACL 2003 Workshop
on Linguistically Interpreted Corpora,
pages 69?76, Budapest.
Grefenstette, Gregory. 1999. The WWW as a
resource for example-based MT tasks. In
Proceedings of ASLIB?99: Translating and the
Computer 21, London.
Gundel, Jeanette, Nancy Hedberg, and Ron
Zacharski. 1993. Cognitive status and the
form of referring expressions in discourse.
Language, 69(2):274?307.
Hahn, Udo, Michael Strube, and Katja
Markert. 1996. Bridging textual ellipses. In
Proceedings of the 16th International
Conference on Computational Linguistics,
pages 496?501, Copenhagen.
Halliday, Michael A. K. and Ruqaiya Hasan.
1976. Cohesion in English. Longman,
London.
Harabagiu, Sanda. 1997. WordNet-Based
Inference of Textual Context, Cohesion and
Coherence. Ph.D. thesis, University of
Southern California.
Harabagiu, Sanda, Razvan Bunescu, and
Steven J. Maiorano. 2001. Text and
knowledge mining for coreference
resolution. In Proceedings of the Second
Conference of the North American Chapter of
the ACL, pages 55?62, Pittsburgh.
Hawkins, John A. 1978. Definiteness and
Indefiniteness. Croom Helm, London.
Hearst, Marti. 1992. Automatic acquisition of
hyponyms from large text corpora. In
Proceedings of the 14th International
Conference on Computational Linguistics,
Nantes, France.
Hirschman, Lynette and Nancy Chinchor.
1997. MUC-7 coreference task definition.
In Proceedings of the Seventh Conference on
Message Understanding, Washington, DC.
Humphreys, Kevin, Robert Gaizauskas,
Saliha Azzam, Chris Huyck, Brian
Mitchell, and Hamish Cunningham. 1997.
University of Sheffield: Description of the
LaSie-II system as used for MUC-7. In
Proceedings of the Seventh Message
Understanding Conference (MUC-7),
Washington, DC.
Kameyama, Megumi. 1997. Recognizing
referential links: An information extraction
perspective. In Proceedings of the ACL-1997
Workshop on Operational Factors in Practical,
Robust Anaphora Resolution for Unrestricted
Texts, pages 46?53, Madrid.
Keller, Frank and Maria Lapata. 2003. Using
the Web to obtain frequencies for unseen
bigrams. Computational Linguistics,
29(3):459?484.
Kennedy, Christopher and Branimir
Boguraev. 1996. Anaphora for everyone:
Pronominal anaphora resolution without a
parser. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 113?118, Copenhagen.
Markert, Katja, Malvina Nissim, and
Natalia N. Modjeska. 2003. Using the
Web for nominal anaphora resolution. In
Robert Dale, Kees van Deemter, and
Ruslan Mitkov, editors, Proceedings of the
EACL Workshop on the Computational
Treatment of Anaphora, pages 39?46,
Budapest.
McCoy, Kathleen and Michael Strube. 1999.
Generating anaphoric expressions:
Pronoun or definite description? In ACL-99
Workshop on the Relation of
Discourse/Dialogue Structure and Reference,
pages 63?71, College Park, MD.
Meyer, Ingrid. 2001. Extracting
knowledge-rich contexts for
terminography. In Didier Bourigault,
Christian Jacquemin, and Marie-Claude
L?Homme, editors, Recent Advances in
Computational Terminology. John Benjamins,
Amsterdam, pages 279?301.
Meyer, Josef and Robert Dale. 2002.
Mining a corpus to support associative
anaphora resolution. In Proceedings of
400
Markert and Nissim Knowledge Sources for Anaphora Resolution
the Fourth International Conference on
Discourse Anaphora and Anaphor Resolution,
Lisbon.
Mitkov, Ruslan. 1998. Robust pronoun
resolution with limited knowledge. In
Proceedings of the 17th International
Conference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 869?879,
Montreal.
Modjeska, Natalia N. 2002. Lexical and
grammatical role constraints in resolving
other-anaphora. In Proceedings of DAARC
2002, pages 129?134, Lisbon.
Modjeska, Natalia N. 2003. Resolving
other-anaphora. Ph.D. thesis, School of
Informatics, University of Edinburgh.
Modjeska, Natalia N., Katja Markert, and
Malvina Nissim. 2003. Using the Web in
machine learning for other-anaphora
resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, pages 176?183,
Sapporo, Japan.
Ng, Vincent. 2004. Learning noun phrase
anaphoricity to improve coreference
resolution: Issues in representation and
optimization. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 151?158,
Barcelona.
Ng, Vincent and Claire Cardie. 2002a.
Identifying anaphoric and non-anaphoric
noun phrases to improve coreference
resolution. In Proceedings of the 19th
International Conference on Computational
Linguistics, pages 730?736, Taipei.
Ng, Vincent and Claire Cardie. 2002b.
Improving machine learning approaches
to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
Philadelphia.
Poesio, Massimo, Tomonori Ishikawa, Sabine
Schulte im Walde, and Renata Vieira. 2002.
Acquiring lexical knowledge for anaphora
resolution. In Proceedings of the Third
International Conference on Language
Resources and Evaluation, pages 1220?1224,
Las Palmas, Canary Islands.
Poesio, Massimo, Rahul Mehta, Axel
Maroudas, and Janet Hitzeman. 2004.
Learning to resolve bridging references. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 143?150, Barcelona.
Poesio, Massimo, Renata Vieira, and Simone
Teufel. 1997. Resolving bridging references
in unrestricted text. In Ruslan Mitkov,
editor, Proceedings of the ACL Workshop on
Operational Factors in Robust Anaphora
Resolution, pages 1?6, Madrid.
Preiss, Judita. 2002. Anaphora resolution
with word sense disambiguation. In
Proceedings of SENSEVAL-2, pages 143?146,
Philadelphia.
Preiss, Judita, Caroline Gasperin, and Ted
Briscoe. 2004. Can anaphoric definite
descriptions be replaced by pronouns? In
Proceedings of the Fourth International
Conference on Language Resources and
Evaluation, pages 1499?1502, Lisbon.
Shinzato, Keiji and Kentaro Torisawa. 2004.
Acquiring hyponymy relations from Web
documents. In Proceedings of the Conference
of the North American Chapter of the ACL,
pages 73?80, Boston.
Soon, Wee Meng, Hwee Tou Ng Ng, and
Daniel Chung Yung Lim. 2001. A machine
learning approach to coreference
resolution of noun phrases. Computational
Linguistics, 27(4):521?544.
Strube, Michael, Stefan Rapp, and
Christoph Mueller. 2002. The influence
of minimum edit distance on reference
resolution. In Proceedings of the 2002
Conference on Empirical Methods in Natural
Language Processing, pages 312?319,
Philadelphia.
Uryupina, Olga. 2003. High-precision
identification of discourse new and unique
noun phrases. In Proceedings of the ACL
2003 Student Workshop, pages 80?86,
Sapporo, Japan.
van Deemter, Kees and Rodger Kibble. 2000.
On coreferring: Coreference in MUC and
related annotation schemes. Computational
Linguistics, 26(4):615?662.
Vieira, Renata and Massimo Poesio. 2000. An
empirically-based system for processing
definite descriptions. Computational
Linguistics, 26(4):539?593.
Webber, Bonnie, Matthew Stone, Aravind
Joshi, and Alistair Knott. 2003. Anaphora
and discourse structure. Computational
Linguistics, 29(4):545?587.
Yang, Xiaofeng, Guodong Zhou, Jian Su,
and Chew Lim Tan. 2003. Coreference
resolution using competition learning
approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics, pages 176?183, Sapporo, Japan.
401

Syntactic Features and Word Similarity for Supervised Metonymy
Resolution
Malvina Nissim
ICCS, School of Informatics
University of Edinburgh
mnissim@inf.ed.ac.uk
Katja Markert
ICCS, School of Informatics
University of Edinburgh and
School of Computing
University of Leeds
markert@inf.ed.ac.uk
Abstract
We present a supervised machine learning
algorithm for metonymy resolution, which
exploits the similarity between examples
of conventional metonymy. We show
that syntactic head-modifier relations are
a high precision feature for metonymy
recognition but suffer from data sparse-
ness. We partially overcome this problem
by integrating a thesaurus and introduc-
ing simpler grammatical features, thereby
preserving precision and increasing recall.
Our algorithm generalises over two levels
of contextual similarity. Resulting infer-
ences exceed the complexity of inferences
undertaken in word sense disambiguation.
We also compare automatic and manual
methods for syntactic feature extraction.
1 Introduction
Metonymy is a figure of speech, in which one ex-
pression is used to refer to the standard referent of
a related one (Lakoff and Johnson, 1980). In (1),1
?seat 19? refers to the person occupying seat 19.
(1) Ask seat 19 whether he wants to swap
The importance of resolving metonymies has
been shown for a variety of NLP tasks, e.g., ma-
chine translation (Kamei and Wakao, 1992), ques-
tion answering (Stallard, 1993) and anaphora reso-
lution (Harabagiu, 1998; Markert and Hahn, 2002).
1(1) was actually uttered by a flight attendant on a plane.
In order to recognise and interpret the metonymy
in (1), a large amount of knowledge and contextual
inference is necessary (e.g. seats cannot be ques-
tioned, people occupy seats, people can be ques-
tioned). Metonymic readings are also potentially
open-ended (Nunberg, 1978), so that developing a
machine learning algorithm based on previous ex-
amples does not seem feasible.
However, it has long been recognised that many
metonymic readings are actually quite regular
(Lakoff and Johnson, 1980; Nunberg, 1995).2 In (2),
?Pakistan?, the name of a location, refers to one of
its national sports teams.3
(2) Pakistan had won the World Cup
Similar examples can be regularly found for many
other location names (see (3) and (4)).
(3) England won the World Cup
(4) Scotland lost in the semi-final
In contrast to (1), the regularity of these exam-
ples can be exploited by a supervised machine learn-
ing algorithm, although this method is not pursued
in standard approaches to regular polysemy and
metonymy (with the exception of our own previous
work in (Markert and Nissim, 2002a)). Such an al-
gorithm needs to infer from examples like (2) (when
labelled as a metonymy) that ?England? and ?Scot-
land? in (3) and (4) are also metonymic. In order to
2Due to its regularity, conventional metonymy is also known
as regular polysemy (Copestake and Briscoe, 1995). We use the
term ?metonymy? to encompass both conventional and uncon-
ventional readings.
3All following examples are from the British National Cor-
pus (BNC, http://info.ox.ac.uk/bnc).
Scotland
subj-of subj-of
win lose
context reduction
Pakistan
Scotland-subj-of-losePakistan-subj-of-win
similarity
semantic class
head similarity
role similarity
Pakistan
had won the World Cup lost in the semi-finalScotland
Figure 1: Context reduction and similarity levels
draw this inference, two levels of similarity need to
be taken into account. One concerns the similarity of
the words to be recognised as metonymic or literal
(Possibly Metonymic Words, PMWs). In the above
examples, the PMWs are ?Pakistan?, ?England? and
?Scotland?. The other level pertains to the similar-
ity between the PMW?s contexts (?<subject> (had)
won the World Cup? and ?<subject> lost in the
semi-final?). In this paper, we show how a machine
learning algorithm can exploit both similarities.
Our corpus study on the semantic class of lo-
cations confirms that regular metonymic patterns,
e.g., using a place name for any of its sports teams,
cover most metonymies, whereas unconventional
metonymies like (1) are very rare (Section 2). Thus,
we can recast metonymy resolution as a classifica-
tion task operating on semantic classes (Section 3).
In Section 4, we restrict the classifier?s features to
head-modifier relations involving the PMW. In both
(2) and (3), the context is reduced to subj-of-win.
This allows the inference from (2) to (3), as they
have the same feature value. Although the remain-
ing context is discarded, this feature achieves high
precision. In Section 5, we generalize context simi-
larity to draw inferences from (2) or (3) to (4). We
exploit both the similarity of the heads in the gram-
matical relation (e.g., ?win? and ?lose?) and that of
the grammatical role (e.g. subject). Figure 1 illus-
trates context reduction and similarity levels.
We evaluate the impact of automatic extraction of
head-modifier relations in Section 6. Finally, we dis-
cuss related work and our contributions.
2 Corpus Study
We summarize (Markert and Nissim, 2002b)?s an-
notation scheme for location names and present an
annotated corpus of occurrences of country names.
2.1 Annotation Scheme for Location Names
We identify literal, metonymic, and mixed readings.
The literal reading comprises a locative (5)
and a political entity interpretation (6).
(5) coral coast of Papua New Guinea
(6) Britain?s current account deficit
We distinguish the following metonymic patterns
(see also (Lakoff and Johnson, 1980; Fass, 1997;
Stern, 1931)). In a place-for-people pattern,
a place stands for any persons/organisations associ-
ated with it, e.g., for sports teams in (2), (3), and (4),
and for the government in (7).4
(7) a cardinal element in Iran?s strategy when
Iranian naval craft [...] bombarded [...]
In a place-for-event pattern, a location
name refers to an event that occurred there (e.g., us-
ing the word Vietnam for the Vietnam war). In a
place-for-product pattern a place stands for
a product manufactured there (e.g., the word Bor-
deaux referring to the local wine).
The category othermet covers unconventional
metonymies, as (1), and is only used if none of the
other categories fits (Markert and Nissim, 2002b).
We also found examples where two predicates are
involved, each triggering a different reading.
(8) they arrived in Nigeria, hitherto a leading
critic of the South African regime
In (8), both a literal (triggered by ?arriving in?)
and a place-for-people reading (triggered by
?leading critic?) are invoked. We introduced the cat-
egory mixed to deal with these cases.
2.2 Annotation Results
Using Gsearch (Corley et al, 2001), we randomly
extracted 1000 occurrences of country names from
the BNC, allowing any country name and its variants
listed in the CIA factbook5 or WordNet (Fellbaum,
4As the explicit referent is often underspecified, we intro-
duce place-for-people as a supertype category and we
evaluate our system on supertype classification in this paper. In
the annotation, we further specify the different groups of people
referred to, whenever possible (Markert and Nissim, 2002b).
5http://www.cia.gov/cia/publications/
factbook/
1998) to occur. Each country name is surrounded by
three sentences of context.
The 1000 examples of our corpus have been inde-
pendently annotated by two computational linguists,
who are the authors of this paper. The annotation
can be considered reliable (Krippendorff, 1980) with
95% agreement and a kappa (Carletta, 1996) of .88.
Our corpus for testing and training the algorithm
includes only the examples which both annotators
could agree on and which were not marked as noise
(e.g. homonyms, as ?Professor Greenland?), for a
total of 925. Table 1 reports the reading distribution.
Table 1: Distribution of readings in our corpus
reading freq %
literal 737 79.7
place-for-people 161 17.4
place-for-event 3 .3
place-for-product 0 .0
mixed 15 1.6
othermet 9 1.0
total non-literal 188 20.3
total 925 100.0
3 Metonymy Resolution as a Classification
Task
The corpus distribution confirms that metonymies
that do not follow established metonymic patterns
(othermet) are very rare. This seems to be the
case for other kinds of metonymies, too (Verspoor,
1997). We can therefore reformulate metonymy res-
olution as a classification task between the literal
reading and a fixed set of metonymic patterns that
can be identified in advance for particular semantic
classes. This approach makes the task comparable to
classic word sense disambiguation (WSD), which is
also concerned with distinguishing between possible
word senses/interpretations.
However, whereas a classic (supervised) WSD
algorithm is trained on a set of labelled instances
of one particular word and assigns word senses to
new test instances of the same word, (supervised)
metonymy recognition can be trained on a set of
labelled instances of different words of one seman-
tic class and assign literal readings and metonymic
patterns to new test instances of possibly different
words of the same semantic class. This class-based
approach enables one to, for example, infer the read-
ing of (3) from that of (2).
We use a decision list (DL) classifier. All features
encountered in the training data are ranked in the DL
(best evidence first) according to the following log-
likelihood ratio (Yarowsky, 1995):
Log
(
Pr(reading
i
|feature
k
)
?
j 6=i
Pr(reading
j
|feature
k
)
)
We estimated probabilities via maximum likeli-
hood, adopting a simple smoothing method (Mar-
tinez and Agirre, 2000): 0.1 is added to both the de-
nominator and numerator.
The target readings to be distinguished are
literal, place-for-people,place-for-
event, place-for-product, othermet and
mixed. All our algorithms are tested on our an-
notated corpus, employing 10-fold cross-validation.
We evaluate accuracy and coverage:
Acc = # correct decisions made
# decisions made
Cov = # decisions made
# test data
We also use a backing-off strategy to the most fre-
quent reading (literal) for the cases where no
decision can be made. We report the results as ac-
curacy backoff (Acc
b
); coverage backoff is always
1. We are also interested in the algorithm?s perfor-
mance in recognising non-literal readings. There-
fore, we compute precision (P ), recall (R), and F-
measure (F ), where A is the number of non-literal
readings correctly identified as non-literal (true pos-
itives) and B the number of literal readings that are
incorrectly identified as non-literal (false positives):
P = A/(A + B)
R = A
#non-literal examples in the test data
F = 2PR/(R + P )
The baseline used for comparison is the assign-
ment of the most frequent reading literal.
4 Context Reduction
We show that reducing the context to head-modifier
relations involving the Possibly Metonymic Word
achieves high precision metonymy recognition.6
6In (Markert and Nissim, 2002a), we also considered local
and topical cooccurrences as contextual features. They con-
stantly achieved lower precision than grammatical features.
Table 2: Example feature values for role-of-head
role-of-head (r-of-h) example
subj-of-win England won the World Cup (place-for-people)
subjp-of-govern Britain has been governed by . . . (literal)
dobj-of-visit the Apostle had visited Spain (literal)
gen-of-strategy in Iran?s strategy . . . (place-for-people)
premod-of-veteran a Vietnam veteran from Rhode Island (place-for-event)
ppmod-of-with its border with Hungary (literal)
Table 3: Role distribution
role freq #non-lit
subj 92 65
subjp 6 4
dobj 28 12
gen 93 20
premod 94 13
ppmod 522 57
other 90 17
total 925 188
We represent each example in our corpus by a sin-
gle feature role-of-head, expressing the grammat-
ical role of the PMW (limited to (active) subject,
passive subject, direct object, modifier in a prenom-
inal genitive, other nominal premodifier, dependent
in a prepositional phrase) and its lemmatised lexi-
cal head within a dependency grammar framework.7
Table 2 shows example values and Table 3 the role
distribution in our corpus.
We trained and tested our algorithm with this fea-
ture (hmr).8 Results for hmr are reported in the
first line of Table 5. The reasonably high precision
(74.5%) and accuracy (90.2%) indicate that reduc-
ing the context to a head-modifier feature does not
cause loss of crucial information in most cases. Low
recall is mainly due to low coverage (see Problem 2
below). We identified two main problems.
Problem 1. The feature can be too simplistic, so
that decisions based on the head-modifier relation
can assign the wrong reading in the following cases:
? ?Bad? heads: Some lexical heads are semanti-
cally empty, thus failing to provide strong evi-
dence for any reading and lowering both recall
and precision. Bad predictors are the verbs ?to
have? and ?to be? and some prepositions such
as ?with?, which can be used with metonymic
(talk with Hungary) and literal (border with
Hungary) readings. This problem is more se-
rious for function than for content word heads:
precision on the set of subjects and objects is
81.8%, but only 73.3% on PPs.
? ?Bad? relations: The premod relation suffers
from noun-noun compound ambiguity. US op-
7We consider only one link per PMW, although cases like (8)
would benefit from including all links the PMW participates in.
8The feature values were manually annotated for the follow-
ing experiments, adapting the guidelines in (Poesio, 2000). The
effect of automatic feature extraction is described in Section 6.
eration can refer to an operation in the US (lit-
eral) or by the US (metonymic).
? Other cases: Very rarely neglecting the remain-
ing context leads to errors, even for ?good?
lexical heads and relations. Inferring from the
metonymy in (4) that ?Germany? in ?Germany
lost a fifth of its territory? is also metonymic,
e.g., is wrong and lowers precision.
However, wrong assignments (based on head-
modifier relations) do not constitute a major problem
as accuracy is very high (90.2%).
Problem 2. The algorithm is often unable to make
any decision that is based on the head-modifier re-
lation. This is by far the more frequent problem,
which we adress in the remainder of the paper. The
feature role-of-head accounts for the similarity be-
tween (2) and (3) only, as classification of a test in-
stance with a particular feature value relies on hav-
ing seen exactly the same feature value in the train-
ing data. Therefore, we have not tackled the infer-
ence from (2) or (3) to (4). This problem manifests
itself in data sparseness and low recall and coverage,
as many heads are encountered only once in the cor-
pus. As hmr?s coverage is only 63.1%, backoff to a
literal reading is required in 36.9% of the cases.
5 Generalising Context Similarity
In order to draw the more complex inference from
(2) or (3) to (4) we need to generalise context sim-
ilarity. We relax the identity constraint of the orig-
inal algorithm (the same role-of-head value of the
test instance must be found in the DL), exploiting
two similarity levels. Firstly, we allow to draw infer-
ences over similar values of lexical heads (e.g. from
subj-of-win to subj-of-lose), rather than over iden-
tical ones only. Secondly, we allow to discard the
Table 4: Example thesaurus entries
lose[V]: win
1
0.216, gain
2
0.209, have
3
0.207, ...
attitude[N]:stance
1
0.181, behavior
2
0.18, ..., strategy
17
0.128
lexical head and generalise over the PMW?s gram-
matical role (e.g. subject). These generalisations al-
low us to double recall without sacrificing precision
or increasing the size of the training set.
5.1 Relaxing Lexical Heads
We regard two feature values r-of-h and r-of-h? as
similar if h and h? are similar. In order to capture the
similarity between h and h? we integrate a thesaurus
(Lin, 1998) in our algorithm?s testing phase. In Lin?s
thesaurus, similarity between words is determined
by their distribution in dependency relations in a
newswire corpus. For a content word h (e.g., ?lose?)
of a specific part-of-speech a set of similar words ?
h
of the same part-of-speech is given. The set mem-
bers are ranked in decreasing order by a similarity
score. Table 4 reports example entries.9
Our modified algorithm (relax I) is as follows:
1. train DL with role-of-head as in hmr; for each test in-
stance observe the following procedure (r-of-h indicates
the feature value of the test instance);
2. if r-of-h is found in the DL, apply the corresponding rule
and stop;
2? otherwise choose a number n ? 1 and set i = 1;
(a) extract the ith most similar word h
i
to h from the
thesaurus;
(b) if i > n or the similarity score of h
i
< 0.10, assign
no reading and stop;
(b?) otherwise: if r-of-h
i
is found in the DL, apply cor-
responding rule and stop; if r-of-h
i
is not found in
the DL, increase i by 1 and go to (a);
The examples already covered by hmr are clas-
sified in exactly the same way by relax I (see Step
2). Let us therefore assume we encounter the test
instance (4), its feature value subj-of-lose has not
been seen in the training data (so that Step 2 fails
and Step 2? has to be applied) and subj-of-win is in
the DL. For all n ? 1, relax I will use the rule for
subj-of-win to assign a reading to ?Scotland? in (4)
as ?win? is the most similar word to ?lose? in the
thesaurus (see Table 4). In this case (2b?) is only
9In the original thesaurus, each ?
h
is subdivided into clus-
ters. We do not take these divisions into account.
0 10 20 30 40 50
Thesaurus Iterations (n)
0.1 0.1
0.2 0.2
0.3 0.3
0.4 0.4
0.5 0.5
0.6 0.6
0.7 0.7
0.8 0.8
0.9 0.9
R
es
ul
ts
Precision
Recall
F-Measure
Figure 2: Results for relax I
applied once as already the first iteration over the
thesaurus finds a word h
1
with r-of-h
1
in the DL.
The classification of ?Turkey? with feature value
gen-of-attitude in (9) required 17 iterations to find
a word h
17
(?strategy?; see Example (7)) similar to
?attitude?, with r-of-h
17
(gen-of-strategy) in the DL.
(9) To say that this sums up Turkey?s attitude as
a whole would nevertheless be untrue
Precision, recall and F-measure for n ?
{1, ..., 10, 15, 20, 25, 30, 40, 50} are visualised in
Figure 2. Both precision and recall increase with
n. Recall more than doubles from 18.6% in hmr
to 41% and precision increases from 74.5% in hmr
to 80.2%, yielding an increase in F-measure from
29.8% to 54.2% (n = 50). Coverage rises to 78.9%
and accuracy backoff to 85.1% (Table 5).
Whereas the increase in coverage and recall is
quite intuitive, the high precision achieved by re-
lax I requires further explanation. Let S be the set
of examples that relax I covers. It consists of two
subsets: S1 is the subset aleady covered by hmr and
its treatment does not change in relax I, yielding the
same precision. S2 is the set of examples that re-
lax I covers in addition to hmr. The examples in S2
consist of cases with highly predictive content word
heads as (a) function words are not included in the
thesaurus and (b) unpredictive content word heads
like ?have? or ?be? are very frequent and normally
already covered by hmr (they are therefore members
of S1). Precision on S2 is very high (84%) and raises
the overall precision on the set S.
Cases that relax I does not cover are mainly due
to (a) missing thesaurus entries (e.g., many proper
Table 5: Results summary for manual annotation.
For relax I and combination we report best results
(50 thesaurus iterations).
algorithm Acc Cov Acc
b
P R F
hmr .902 .631 .817 .745 .186 .298
relax I .877 .789 .851 .802 .410 .542
relax II .865 .903 .859 .813 .441 .572
combination .894 .797 .870 .814 .510 .627
baseline .797 1.00 .797 n/a .000 n/a
names or alternative spelling), (b) the small num-
ber of training instances for some grammatical roles
(e.g. dobj), so that even after 50 thesaurus iterations
no similar role-of-head value could be found that is
covered in the DL, or (c) grammatical roles that are
not covered (other in Table 3).
5.2 Discarding Lexical Heads
Another way of capturing the similarity between (3)
and (4), or (7) and (9) is to ignore lexical heads and
generalise over the grammatical role (role) of the
PMW (with the feature values as in Table 3: subj,
subjp, dobj, gen, premod, ppmod). We therefore de-
veloped the algorithm relax II.
1. train decision lists:
(a) DL1 with role-of-head as in hmr
(b) DL2 with role;
for each test instance observe the following procedure (r-
of-h and r are the feature values of the test instance);
2. if r-of-h is found in the DL1, apply the corresponding rule
and stop;
2? otherwise, if r is found in DL2, apply the corresponding
rule.
Let us assume we encounter the test instance
(4), subj-of-lose is not in DL1 (so that Step 2 fails
and Step 2? has to be applied) and subj is in DL2.
The algorithm relax II will assign a place-for-
people reading to ?Scotland?, as most subjects in
our corpus are metonymic (see Table 3).
Generalising over the grammatical role outper-
forms hmr, achieving 81.3% precision, 44.1% re-
call, and 57.2% F-measure (see Table 5). The algo-
rithm relax II also yields fewer false negatives than
relax I (and therefore higher recall) since all sub-
jects not covered in DL1 are assigned a metonymic
reading, which is not true for relax I.
5.3 Combining Generalisations
There are several ways of combining the algorithms
we introduced. In our experiments, the most suc-
cessful one exploits the facts that relax II performs
better than relax I on subjects and that relax I per-
forms better on the other roles. Therefore the algo-
rithm combination uses relax II if the test instance
is a subject, and relax I otherwise. This yields the
best results so far, with 87% accuracy backoff and
62.7% F-measure (Table 5).
6 Influence of Parsing
The results obtained by training and testing our clas-
sifier with manually annotated grammatical relations
are the upper bound of what can be achieved by us-
ing these features. To evaluate the influence pars-
ing has on the results, we used the RASP toolkit
(Briscoe and Carroll, 2002) that includes a pipeline
of tokenisation, tagging and state-of-the-art statisti-
cal parsing, allowing multiple word tags. The toolkit
also maps parse trees to representations of gram-
matical relations, which we in turn could map in a
straightforward way to our role categories.
RASP produces at least partial parses for 96% of
our examples. However, some of these parses do
not assign any role of our roleset to the PMW ?
only 76.9% of the PMWs are assigned such a role
by RASP (in contrast to 90.2% in the manual anno-
tation; see Table 3). RASP recognises PMW sub-
jects with 79% precision and 81% recall. For PMW
direct objects, precision is 60% and recall 86%.10
We reproduced all experiments using the auto-
matically extracted relations. Although the relative
performance of the algorithms remains mostly un-
changed, most of the resulting F-measures are more
than 10% lower than for hand annotated roles (Ta-
ble 6). This is in line with results in (Gildea and
Palmer, 2002), who compare the effect of man-
ual and automatic parsing on semantic predicate-
argument recognition.
7 Related Work
Previous Approaches to Metonymy Recognition.
Our approach is the first machine learning algorithm
to metonymy recognition, building on our previous
10We did not evaluate RASP?s performance on relations that
do not involve the PMW.
Table 6: Results summary for the different algo-
rithms using RASP. For relax I and combination
we report best results (50 thesaurus iterations).
algorithm Acc Cov Acc
b
P R F
hmr .884 .514 .812 .674 .154 .251
relax I .841 .666 .821 .619 .319 .421
relax II .820 .769 .823 .621 .340 .439
combination .850 .672 .830 .640 .388 .483
baseline .797 1.00 .797 n/a .000 n/a
work (Markert and Nissim, 2002a). The current ap-
proach expands on it by including a larger number
of grammatical relations, thesaurus integration, and
an assessment of the influence of parsing. Best F-
measure for manual annotated roles increased from
46.7% to 62.7% on the same dataset.
Most other traditional approaches rely on hand-
crafted knowledge bases or lexica and use vi-
olations of hand-modelled selectional restrictions
(plus sometimes syntactic violations) for metonymy
recognition (Pustejovsky, 1995; Hobbs et al, 1993;
Fass, 1997; Copestake and Briscoe, 1995; Stallard,
1993).11 In these approaches, selectional restric-
tions (SRs) are not seen as preferences but as ab-
solute constraints. If and only if such an absolute
constraint is violated, a non-literal reading is pro-
posed. Our system, instead, does not have any a
priori knowledge of semantic predicate-argument re-
strictions. Rather, it refers to previously seen train-
ing examples in head-modifier relations and their la-
belled senses and computes the likelihood of each
sense using this distribution. This is an advantage as
our algorithm also resolved metonymies without SR
violations in our experiments. An empirical compar-
ison between our approach in (Markert and Nissim,
2002a)12 and an SRs violation approach showed that
our approach performed better.
In contrast to previous approaches (Fass, 1997;
Hobbs et al, 1993; Copestake and Briscoe, 1995;
Pustejovsky, 1995; Verspoor, 1996; Markert and
Hahn, 2002; Harabagiu, 1998; Stallard, 1993), we
use a corpus reliably annotated for metonymy for
evaluation, moving the field towards more objective
11(Markert and Hahn, 2002) and (Harabagiu, 1998) en-
hance this with anaphoric information. (Briscoe and Copes-
take, 1999) propose using frequency information besides syn-
tactic/semantic restrictions, but use only a priori sense frequen-
cies without contextual features.
12Note that our current approach even outperforms (Markert
and Nissim, 2002a).
evaluation procedures.
Word Sense Disambiguation. We compared our
approach to supervised WSD in Section 3, stressing
word-to-word vs. class-to-class inference. This al-
lows for a level of abstraction not present in standard
supervised WSD. We can infer readings for words
that have not been seen in the training data before,
allow an easy treatment of rare words that undergo
regular sense alternations and do not have to anno-
tate and train separately for every individual word to
treat regular sense distinctions.13
By exploiting additional similarity levels and inte-
grating a thesaurus we further generalise the kind of
inferences we can make and limit the size of anno-
tated training data: as our sampling frame contains
553 different names, an annotated data set of 925
samples is quite small. These generalisations over
context and collocates are also applicable to stan-
dard WSD and can supplement those achieved e.g.,
by subcategorisation frames (Martinez et al, 2002).
Our approach to word similarity to overcome data
sparseness is perhaps most similar to (Karov and
Edelman, 1998). However, they mainly focus on the
computation of similarity measures from the train-
ing data. We instead use an off-the-shelf resource
without adding much computational complexity and
achieve a considerable improvement in our results.
8 Conclusions
We presented a supervised classification algorithm
for metonymy recognition, which exploits the simi-
larity between examples of conventional metonymy,
operates on semantic classes and thereby enables
complex inferences from training to test examples.
We showed that syntactic head-modifier relations
are a high precision feature for metonymy recogni-
tion. However, basing inferences only on the lex-
ical heads seen in the training data leads to data
sparseness due to the large number of different lex-
ical heads encountered in natural language texts. In
order to overcome this problem we have integrated
a thesaurus that allows us to draw inferences be-
13Incorporating knowledge about particular PMWs (e.g., as
a prior) will probably improve performance, as word idiosyn-
cracies ? which can still exist even when treating regular sense
distinctions ? could be accounted for. In addition, knowledge
about the individual word is necessary to assign its original se-
mantic class.
tween examples with similar but not identical lex-
ical heads. We also explored the use of simpler
grammatical role features that allow further gener-
alisations. The results show a substantial increase in
precision, recall and F-measure. In the future, we
will experiment with combining grammatical fea-
tures and local/topical cooccurrences. The use of
semantic classes and lexical head similarity gener-
alises over two levels of contextual similarity, which
exceeds the complexity of inferences undertaken in
standard supervised word sense disambiguation.
Acknowledgements. The research reported in this
paper was supported by ESRC Grant R000239444.
Katja Markert is funded by an Emmy Noether Fel-
lowship of the Deutsche Forschungsgemeinschaft
(DFG). We thank three anonymous reviewers for
their comments and suggestions.
References
E. Briscoe and J. Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proc. of LREC, 2002,
pages 1499?1504.
T. Briscoe and A. Copestake. 1999. Lexical rules in
constraint-based grammar. Computational Linguis-
tics, 25(4):487?526.
J. Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
A. Copestake and T. Briscoe. 1995. Semi-productive
polysemy and sense extension. Journal of Semantics,
12:15?67.
S. Corley, M. Corley, F. Keller, M. Crocker, and S.
Trewin. 2001. Finding syntactic structure in unparsed
corpora: The Gsearch corpus query system. Comput-
ers and the Humanities, 35(2):81?94.
D. Fass. 1997. Processing Metaphor and Metonymy.
Ablex, Stanford, CA.
C. Fellbaum, ed. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, Mass.
D. Gildea and M. Palmer. 2002. The necessity of parsing
for predicate argument recognition. In Proc. of ACL,
2002, pages 239?246.
S. Harabagiu. 1998. Deriving metonymic coercions
from WordNet. In Workshop on the Usage of WordNet
in Natural Language Processing Systems, COLING-
ACL, 1998, pages 142?148.
J. R. Hobbs, M. E. Stickel, D. E. Appelt, and P. Martin.
1993. Interpretation as abduction. Artificial Intelli-
gence, 63:69?142.
S. Kamei and T. Wakao. 1992. Metonymy: Reassess-
ment, survey of acceptability and its treatment in ma-
chine translation systems. In Proc. of ACL, 1992,
pages 309?311.
Y. Karov and S. Edelman. 1998. Similarity-based
word sense disambiguation. Computational Linguis-
tics, 24(1):41-59.
K. Krippendorff. 1980. Content Analysis: An Introduc-
tion to Its Methodology. Sage Publications.
G. Lakoff and M. Johnson. 1980. Metaphors We Live By.
Chicago University Press, Chicago, Ill.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proc. of International Conference on
Machine Learning, Madison, Wisconsin.
K. Markert and U. Hahn. 2002. Understanding
metonymies in discourse. Artificial Intelligence,
135(1/2):145?198.
K. Markert and M. Nissim. 2002a. Metonymy resolu-
tion as a classification task. In Proc. of EMNLP, 2002,
pages 204?213.
Katja Markert and Malvina Nissim. 2002b. Towards a
corpus annotated for metonymies: the case of location
names. In Proc. of LREC, 2002, pages 1385?1392.
D. Martinez and E. Agirre. 2000. One sense per collo-
cation and genre/topic variations. In Proc. of EMNLP,
2000.
D. Martinez, E. Agirre, and L. Marquez. 2002. Syntactic
features for high precision word sense disambiguation.
In Proc. of COLING, 2002.
G. Nunberg. 1978. The Pragmatics of Reference. Ph.D.
thesis, City University of New York, New York.
G. Nunberg. 1995. Transfers of meaning. Journal of
Semantics, 12:109?132.
M. Poesio, 2000. The GNOME Annotation Scheme Man-
ual. University of Edinburgh, 4th version. Available
from http://www.hcrc.ed.ac.uk/?gnome.
J. Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge, Mass.
D. Stallard. 1993. Two kinds of metonymy. In Proc. of
ACL, 1993, pages 87?94.
G. Stern. 1931. Meaning and Change of Meaning.
Go?teborg: Wettergren & Kerbers Fo?rlag.
C. Verspoor. 1996. Lexical limits on the influence of
context. In Proc. of CogSci, 1996, pages 116?120.
C. Verspoor. 1997. Conventionality-governed logical
metonymy. In H. Bunt et al, editors, Proc. of IWCS-2,
1997, pages 300?312.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL, 1995, pages 189?196.
  	
Using the Web in Machine Learning for Other-Anaphora Resolution
Natalia N. Modjeska
School of Informatics
University of Edinburgh and
Department of Computer Science
University of Toronto
natalia@cs.utoronto.ca
Katja Markert
School of Computing
University of Leeds and
School of Informatics
University of Edinburgh
markert@inf.ed.ac.uk
Malvina Nissim
School of Informatics
University of Edinburgh
mnissim@inf.ed.ac.uk
Abstract
We present a machine learning frame-
work for resolving other-anaphora. Be-
sides morpho-syntactic, recency, and se-
mantic features based on existing lexi-
cal knowledge resources, our algorithm
obtains additional semantic knowledge
from the Web. We search the Web via
lexico-syntactic patterns that are specific
to other-anaphors. Incorporating this in-
novative feature leads to an 11.4 percent-
age point improvement in the classifier?s
F -measure (25% improvement relative to
results without this feature).
1 Introduction
Other-anaphors are referential NPs with the mod-
ifiers ?other? or ?another? and non-structural an-
tecedents:1
(1) An exhibition of American design and architec-
ture opened in September in Moscow and will
travel to eight other Soviet cities.
(2) [. . . ] the alumni director of a Big Ten university
?I?d love to see sports cut back and so would a
lot of my counterparts at other schools, [. . . ]?
(3) You either believe Seymour can do it again or
you don?t. Beside the designer?s age, other
risk factors for Mr. Cray?s company include
the Cray-3?s [. . . ] chip technology.
1All examples are from the Wall Street Journal; the correct
antecedents are in italics and the anaphors are in bold font.
In (1), ?eight other Soviet cities? refers to a set of So-
viet cities excluding Moscow, and can be rephrased
as ?eight Soviet cities other than Moscow?. In (2),
?other schools? refers to a set of schools excluding
the mentioned Big Ten university. In (3), ?other risk
factors for Mr. Cray?s company? refers to a set of
risk factors excluding the designer?s age.
In contrast, in list-contexts such as (4), the an-
tecedent is available both anaphorically and struc-
turally, as the left conjunct of the anaphor.2
(4) Research shows AZT can relieve dementia and
other symptoms in children [. . . ]
We focus on cases such as (1?3).
Section 2 describes a corpus of other-anaphors.
We present a machine learning approach to other-
anaphora, using a Naive Bayes (NB) classifier (Sec-
tion 3) with two different feature sets. In Section 4
we present the first feature set (F1) that includes
standard morpho-syntactic, recency, and string com-
parison features. However, there is evidence that,
e.g., syntactic features play a smaller role in resolv-
ing anaphors with full lexical heads than in pronom-
inal anaphora (Strube, 2002; Modjeska, 2002). In-
stead, a large and diverse amount of lexical or
world knowledge is necessary to understand exam-
ples such as (1?3), e.g., that Moscow is a (Soviet)
city, that universities are informally called schools
in American English and that age can be viewed as
a risk factor. Therefore we add lexical knowledge,
which is extracted from WordNet (Fellbaum, 1998)
and from a Named Entity (NE) Recognition algo-
rithm, to F1.
2Antecedents are also available structurally in constructions
?other than?, e.g., ?few clients other than the state?. For a com-
putational treatment of ?other? with structural antecedents see
(Bierner, 2001).
The algorithm?s performance with this feature set
is encouraging. However, the semantic knowledge
the algorithm relies on is not sufficient for many
cases of other-anaphors (Section 4.2). Many expres-
sions, word senses and lexical relations are miss-
ing from WordNet. Whereas it includes Moscow
as a hyponym of city, so that the relation between
anaphor and antecedent in (1) can be retrieved, it
does not include the sense of school as university,
nor does it allow to infer that age is a risk factor.
There have been efforts to extract missing lexical
relations from corpora in order to build new knowl-
edge sources and enrich existing ones (Hearst, 1992;
Berland and Charniak, 1999; Poesio et al, 2002).3
However, the size of the used corpora still leads
to data sparseness (Berland and Charniak, 1999)
and the extraction procedure can therefore require
extensive smoothing. Moreover, some relations
should probably not be encoded in fixed context-
independent ontologies at all. Should, e.g., under-
specified and point-of-view dependent hyponymy
relations (Hearst, 1992) be included? Should age,
for example, be classified as a hyponym of risk fac-
tor independent of context?
Building on our previous work in (Markert et al,
2003), we instead claim that the Web can be used
as a huge additional source of domain- and context-
independent, rich and up-to-date knowledge, with-
out having to build a fixed lexical knowledge base
(Section 5). We describe the benefit of integrating
Web frequency counts obtained for lexico-syntactic
patterns specific to other-anaphora as an additional
feature into our NB algorithm. This feature raises
the algorithm?s F -measure from 45.5% to 56.9%.
2 Data Collection and Preparation
We collected 500 other-anaphors with NP an-
tecedents from the Wall Street Journal corpus (Penn
Treebank, release 2). This data sample excludes
several types of expressions containing ?other?: (a)
list-contexts (Ex. 4) and other-than contexts (foot-
note 2), in which the antecedents are available struc-
turally and thus a relatively unsophisticated proce-
dure would suffice to find them; (b) idiomatic and
discourse connective ?other?, e.g., ?on the other
3In parallel, efforts have been made to enrich WordNet by
adding information in glosses (Harabagiu et al, 1999).
hand?, which are not anaphoric; and (c) reciprocal
?each other? and ?one another?, elliptic phrases e.g.
?one X . . . the other(s)? and one-anaphora, e.g., ?the
other/another one?, which behave like pronouns and
thus would require a different search method. Also
excluded from the data set are samples of other-
anaphors with non-NP antecedents (e.g., adjectival
and nominal pre- and postmodifiers and clauses).
Each anaphor was extracted in a 5-sentence con-
text. The correct antecedents were manually an-
notated to create a training/test corpus. For each
anaphor, we automatically extracted a set of po-
tential NP antecedents as follows. First, we ex-
tracted all base NPs, i.e., NPs that contain no further
NPs within them. NPs containing a possessive NP
modifier, e.g., ?Spain?s economy?, were split into a
possessor phrase, ?Spain?, and a possessed entity,
?economy?. We then filtered out null elements and
lemmatised all antecedents and anaphors.
3 The Algorithm
We use a Naive Bayes classifier, specifically the im-
plementation in the Weka ML library.4
The training data was generated following the
procedure employed by Soon et al (2001) for
coreference resolution. Every pair of an anaphor
and its closest preceding antecedent created a pos-
itive training instance. To generate negative train-
ing instances, we paired anaphors with each of the
NPs that intervene between the anaphor and its an-
tecedent. This procedure produced a set of 3,084
antecedent-anaphor pairs, of which 500 (16%) were
positive training instances.
The classifier was trained and tested using 10-fold
cross-validation. We follow the general practice of
ML algorithms for coreference resolution and com-
pute precision (P), recall (R), and F-measure (F ) on
all possible anaphor-antecedent pairs.
As a first approximation of the difficulty of our
task, we developed a simple rule-based baseline al-
gorithm which takes into account the fact that the
lemmatised head of an other-anaphor is sometimes
the same as that of its antecedent, as in (5).
4http://www.cs.waikato.ac.nz/ml/weka/.
We also experimented with a decision tree classifier, with
Neural Networks and Support Vector Machines with Sequential
Minimal Optimization (SMO), all available from Weka. These
classifiers achieved worse results than NB on our data set.
Table 1: Feature set F1
Type Feature Description Values
Gramm NP FORM Surface form (for all NPs) definite, indefinite, demonstrative, pronoun,
proper name, unknown
Match RESTR SUBSTR Does lemmatized antecedent string contain lemma-
tized anaphor string?
yes, no
Syntactic GRAM FUNC Grammatical role (for all NPs) subject, predicative NP, dative object, direct
object, oblique, unknown
Syntactic SYN PAR Anaphor-antecedent agreement with respect to
grammatical function
yes, no
Positional SDIST Distance between antecedent and anaphor in sen-
tences
1, 2, 3, 4, 5
Semantic SEMCLASS Semantic class (for all NPs) person, organization, location, date, money,
number, thing, abstract, unknown
Semantic SEMCLASS AGR Anaphor-antecedent agreement with respect to se-
mantic class
yes, no, unknown
Semantic GENDER AGR Anaphor-antecedent agreement with respect to gen-
der
same, compatible, incompatible, unknown
Semantic RELATION Type of relation between anaphor and antecedent same-predicate, hypernymy, meronymy,
compatible, incompatible, unknown
(5) These three countries aren?t completely off the
hook, though. They will remain on a lower-
priority list that includes other countries [. . . ]
For each anaphor, the baseline string-compares its
last (lemmatised) word with the last (lemmatised)
word of each of its possible antecedents. If the
words match, the corresponding antecedent is cho-
sen as the correct one. If several antecedents pro-
duce a match, the baseline chooses the most re-
cent one among them. If string-comparison returns
no antecedent, the baseline chooses the antecedent
closest to the anaphor among all antecedents. The
baseline assigns ?yes? to exactly one antecedent per
anaphor. Its P, R and F -measure are 27.8%.
4 Naive Bayes without the Web
First, we trained and tested the NB classifier with
a set of 9 features motivated by our own work on
other-anaphora (Modjeska, 2002) and previous ML
research on coreference resolution (Aone and Ben-
nett, 1995; McCarthy and Lehnert, 1995; Soon et
al., 2001; Ng and Cardie, 2002; Strube et al, 2002).
4.1 Features
A set of 9 features, F1, was automatically acquired
from the corpus and from additional external re-
sources (see summary in Table 1).
Non-semantic features. NP FORM is based on the
POS tags in the Wall Street Journal corpus and
heuristics. RESTR SUBSTR matches lemmatised
strings and checks whether the antecedent string
contains the anaphor string. This allows to resolve
examples such as ?one woman ringer . . . another
woman?. The values for GRAM FUNC were approxi-
mated from the parse trees and Penn Treebank anno-
tation. The feature SYN PAR captures syntactic par-
allelism between anaphor and antecedent. The fea-
ture SDIST measures the distance between anaphor
and antecedent in terms of sentences.5
Semantic features. GENDER AGR captures agree-
ment in gender between anaphor and antecedent,
gender having been determined using gazetteers,
kinship and occupational terms, titles, and Word-
Net. Four values are possible: ?same?, if both NPs
have same gender; ?compatible?, if antecedent and
anaphor have compatible gender, e.g., ?lawyer . . .
other women?; ?incompatible?, e.g., ?Mr. Johnson
. . . other women?; and ?unknown?, if one of the
NPs is undifferentiated, i.e., the gender value is ?un-
known?. SEMCLASS: Proper names were classified
using ANNIE, part of the GATE2 software package
(http://gate.ac.uk). Common nouns were
looked up in WordNet, considering only the most
frequent sense of each noun (the first sense in Word-
Net). In each case, the output was mapped onto one
of the values in Table 1. The SEMCLASS AGR fea-
5We also experimented with a feature MDIST that measures
intervening NP units. This feature worsened the overall perfor-
mance of the classifier.
ture compares the semantic class of the antecedent
with that of the anaphor NP and returns ?yes? if
they belong to the same class; ?no?, if they belong
to different classes; and ?unknown? if the seman-
tic class of either the anaphor or antecedent has not
been determined. The RELATION between other-
anaphors and their antecedents can partially be de-
termined by string comparison (?same-predicate?)6
or WordNet (?hypernymy? and ?meronymy?). As
other relations, e.g. ?redescription? (Ex. (3), cannot
be readily determined on the basis of the information
in WordNet, the following values were used: ?com-
patible?, for NPs with compatible semantic classes,
e.g., ?woman . . . other leaders?; and ?incompati-
ble?, e.g., ?woman . . . other economic indicators?.
Compatibility can be defined along a variety of pa-
rameters. The notion we used roughly corresponds
to the root level of the WordNet hierarchy. Two
nouns are compatible if they have the same SEM-
CLASS value, e.g., ?person?. ?Unknown? was used
if the type of relation could not be determined.
4.2 Results
Table 2 shows the results for the Naive Bayes clas-
sifier using F1 in comparison to the baseline.
Table 2: Results with F1
Features P R F
baseline 27.8 27.8 27.8
F1 51.7 40.6 45.5
Our algorithm performs significantly better than the
baseline.7 While these results are encouraging, there
were several classification errors.
Word sense ambiguity is one of the reasons for
misclassifications. Antecedents were looked up in
WordNet for their most frequent sense for a context-
independent assignment of the values of semantic
class and relations. However, in many cases either
the anaphor or antecedent or both are used in a sense
that is ranked as less frequent in Wordnet. This
might even be a quite frequent sense for a specific
corpus, e.g., the word ?issue? in the sense of ?shares,
stocks? in the WSJ. Therefore there is a strong inter-
6Same-predicate is not really a relation. We use it when the
head noun of the anaphor and antecedent are the same.
7We used a t-test with confidence level 0.05 for all signifi-
cance tests.
action between word sense disambiguation and ref-
erence resolution (see also (Preiss, 2002)).
Named Entity resolution is another weak link.
Several correct NE antecedents were classified as
?antecedent=no? (false negatives) because the NER
module assigned the wrong class to them.
The largest class of errors is however due to insuf-
ficient semantic knowledge. Problem examples can
roughly be classified into five partially overlapping
groups: (a) examples that suffer from gaps in Word-
Net, e.g., (2); (b) examples that require domain-,
situation-specific, or general world knowledge, e.g.,
(3); (c) examples involving bridging phenomena
(sometimes triggered by a metonymic or metaphoric
antecedent or anaphor), e.g., (6); (d) redescriptions
and paraphrases, often involving semantically vague
anaphors and/or antecedents, e.g., (7) and (3); and
(e) examples with ellipsis, e.g., (8).
(6) The Justice Department?s view is shared by
other lawyers [. . . ]
(7) While Mr. Dallara and Japanese officials say
the question of investors? access to the U.S.
and Japanese markets may get a disproportion-
ate share of the public?s attention, a number of
other important economic issues will be on
the table at next week?s talks.
(8) He sees flashy sports as the only way the last-
place network can cut through the clutter of ca-
ble and VCRs, grab millions of new viewers
and tell them about other shows premiering a
few weeks later.
In (6), the antecedent is an organization-for-people
metonymy. In (7), the question of investors? access
to the U.S. and Japanese markets is characterized as
an important economic issue. Also, the head ?is-
sues? is lexically uninformative to sufficiently con-
strain the search space for the antecedent. In (8), the
antecedent is not the flashy sports, but rather flashy
sport shows, and thus an important piece of infor-
mation is omitted. Alternatively, the antecedent is a
content-for-container metonymy.
Overall, our approach misclassifies antecedents
whose relation to the other-anaphor is based on sim-
ilarity, property-sharing, causality, or is constrained
to a specific domain. These relation types are not ?
and perhaps should not be ? encoded in WordNet.
5 Naive Bayes with the Web
With its approximately 3033M pages8 the Web is
the largest corpus available to the NLP community.
Building on our approach in (Markert et al, 2003),
we suggest using the Web as a knowledge source
for anaphora resolution. In this paper, we show how
to integrate Web counts for lexico-syntactic patterns
specific to other-anaphora into our ML approach.
5.1 Basic Idea
In the examples we consider, the relation between
anaphor and antecedent is implicitly expressed, i.e.,
anaphor and antecedent do not stand in a structural
relationship. However, they are linked by a strong
semantic relation that is likely to be structurally ex-
plicitly expressed in other texts. We exploit this in-
sight by adopting the following procedure:
1. In other-anaphora, a hyponymy/similarity rela-
tion between the lexical heads of anaphor and
antecedent is exploited or stipulated by the con-
text,9 e.g. that ?schools? is an alternative term
for universities in Ex. (2) or that age is viewed
as a risk factor in Ex. (3).
2. We select patterns that structurally explicitly
express the same lexical relations. E.g., the list-
context NP
1
and other NP
2
(as Ex. (4))
usually expresses hyponymy/similarity rela-
tions between the hyponym NP
1
and its hyper-
nym NP
2
(Hearst, 1992).
3. If the implicit lexical relationship between
anaphor and antecedent is strong, it is likely
that anaphor and antecedent also frequently
cooccur in the selected explicit patterns. We
instantiate the explicit pattern for all anaphor-
antecedent pairs. In (2) the pattern NP
1
and other NP
2
is instantiated with e.g.,
counterparts and other schools, sports
and other schools and universities and
other schools.10 These instantiations can be
8http://www.searchengineshowdown.com/
stats/sizeest.shtml, data from March 2003.
9In the Web feature context, we will often use
?anaphor/antecedent? instead of the more cumbersome
?lexical heads of the anaphor/antecedent?.
10These simplified instantiations serve as an example and are
neither exhaustive nor the final instantiations we use; see Sec-
tion 5.3.
searched in any corpus to determine their fre-
quencies. The rationale is that the most fre-
quent of these instantiated patterns is a good
clue for the correct antecedent.
4. As the patterns can be quite elaborate, most
corpora will be too small to determine the cor-
responding frequencies reliably. The instantia-
tion universities and other schools, e.g.,
does not occur at all in the British National Cor-
pus (BNC), a 100M words corpus of British
English.11 Therefore we use the largest corpus
available, the Web. We submit all instantiated
patterns as queries making use of the Google
API technology. Here, universities and
other schools yields over 700 hits, whereas
the other two instantiations yield under 10 hits
each. High frequencies do not only occur
for synonyms; the corresponding instantiation
for the correct antecedent in Ex. (3) age and
other risk factors yields over 400 hits on
the Web and again none in the BNC.
5.2 Antecedent Preparation
In addition to the antecedent preparation described
in Section 2, further processing is necessary. First,
pronouns can be antecedents of other-anaphors but
they were not used as Web query input as they are
lexically empty. Second, all modification was elim-
inated and only the rightmost noun of compounds
was kept, to avoid data sparseness. Third, using pat-
terns containing NEs such as ?Will Quinlan? in (9)
also leads to data sparseness (see also the use of NE
recognition for feature SEMCLASS).
(9) [. . . ] Will Quinlan had not inherited a damaged
retinoblastoma supressor gene and, therefore,
faced no more risk than other children [. . . ]
We resolved NEs in two steps. In addition
to GATE?s classification into ENAMEX and NU-
MEX categories, we used heuristics to automati-
cally obtain more fine-grained distinctions for the
categories LOCATION, ORGANIZATION, DATE and
MONEY, whenever possible. No further distinc-
tions were made for the category PERSON. We
classified LOCATIONS into COUNTRY, (US) STATE,
CITY, RIVER, LAKE and OCEAN, using mainly
11http://info.ox.ac.uk/bnc
Table 3: Patterns and Instantiations for other-anaphora
ANTECEDENT PATTERN INSTANTIATIONS
common noun (O1): (N
1
fsgg OR N
1
fplg) and other N
2
fplg Ic
1
: ?(university OR universities) and other schools?
proper name (O1): (N
1
fsgg OR N
1
fplg) and other N
2
fplg Ip
1
: ?(person OR persons) and other children?
Ip
2
: ?(child OR children) and other persons?
(O2): N
1
and other N
2
fplg Ip
3
: ?Will Quinlan and other children?
gazetteers.12 If an entity classified by GATE as
ORGANIZATION contained an indication of the or-
ganization type, we used this as a subclassifica-
tion; therefore ?Bank of America? is classified as
BANK. For DATE and MONEY entities we used
simple heuristics to classify them further into DAY,
MONTH, YEAR as well as DOLLAR.
From now on we call A the list of possible an-
tecedents and ana the anaphor. For (2), this list
is A
2
=fcounterpart, sport, universityg (the pronoun
?I? has been discarded) and ana
2
=school. For (9),
they are A
9
=frisk, gene, person [=Will Quinlan]g
and ana
9
=child.
5.3 Queries and Scoring Method
We use the list-context pattern:13
(O1) (N
1
fsgg OR N
1
fplg) and other N
2
fplg
For common noun antecedents, we instantiate the
pattern by substituting N
1
with each possible an-
tecedent from set A, and N
2
with ana, as normally
N
1
is a hyponym of N
2
in (O1), and the antecedent
is a hyponym of the anaphor. An instantiated pat-
tern for Ex. (2) is (university OR universities)
and other schools (Ic
1
in Table 3).14
For NE antecedents we instantiate (O1) by substi-
tuting N
1
with the NE category of the antecedent,
and N
2
with ana. An instantiated pattern for
Example (9) is (person OR persons) and other
children (Ip
1
in Table 3). In this instantiation, N
1
(?person?) is not a hyponym of N
2
(?child?), instead
N
2
is a hyponym of N
1
. This is a consequence of
the substitution of the antecedent (?Will Quinlan?)
12They were extracted from the Web. Small gazetteers, con-
taining in all about 500 entries, are sufficient. This is the only
external knowledge collected for the Web feature.
13In all patterns in this paper, ?OR? is the boolean operator,
?N
1
? and ?N
2
? are variables, all other words are constants.
14Common noun instantiations are marked by a superscript
?c? and proper name instantiations by a superscript ?p?.
with its NE category (?person?); such an instanti-
ation is not frequent, since it violates standard re-
lations within (O1). Therefore, we also instantiate
(O1) by substituting N
1
with ana, and N
2
with the
NE type of the antecedent (Ip
2
in Table 3). Finally,
for NE antecedents, we use an additional pattern:
(O2) N
1
and other N
2
fplg
which we instantiate by substituting N
1
with the
original NE antecedent and N
2
with ana (Ip
3
in Ta-
ble 3).
Patterns and instantiations are summarised in Ta-
ble 3. We submit these instantiations as queries to
the Google search engine.
For each antecedent ant in A we obtain the raw
frequencies of all instantiations it occurs in (Ic
1
for
common nouns, or I
p
1
, I
p
2
, I
p
3
for proper names) from
the Web, yielding freq(Ic
1
), or freq(I
p
1
), freq(I
p
2
)
and freq(Ip
3
). We compute the maximum M
ant
over these frequencies for proper names. For com-
mon nouns M
ant
corresponds to freq(Ic
1
). The in-
stantiation yielding M
ant
is then called Imax
ant
.
Our scoring method takes into account the indi-
vidual frequencies of ant and ana by adapting mu-
tual information. We call the first part of Imax
ant
(e.g. ?university OR universities?, or ?child OR chil-
dren?) X
ant
, and the second part (e.g. ?schools?
or ?persons?) Y
ant
. We compute the probability of
Imax
ant
, X
ant
and Y
ant
, using Google to determine
freq(X
ant
) and freq(Y
ant
).
Pr(Imax
ant
) =
M
ant
number of GOOGLE pages
Pr(X
ant
) =
freq(X
ant
)
number of GOOGLE pages
Pr(Y
ant
) =
freq(Y
ant
)
number of GOOGLE pages
We then compute the final score MI
ant
.
MI
ant
= log
Pr(Imax
ant
)
Pr(X
ant
)Pr(Y
ant
)
5.4 Integration into ML Framework and
Results
For each anaphor, the antecedent in A with the
highest MI
ant
gets feature value ?webfirst?.15 All
other antecedents (including pronouns) get the fea-
ture value ?webrest?. We chose this method instead
of e.g., giving score intervals for two reasons. First,
since score intervals are unique for each anaphor,
it is not straightforward to incorporate them into a
ML framework in a consistent manner. Second, this
method introduces an element of competition be-
tween several antecedents (see also (Connolly et al,
1997)), which the individual scores do not reflect.
We trained and tested the NB classifier with the
feature set F1, plus the Web feature. The last row
in Table 4 shows the results. We obtained a 9.1 per-
centage point improvement in precision (an 18% im-
provement relative to the F1 feature set) and a 12.8
percentage point improvement in recall (32% im-
provement relative to F1), which amounts to an 11.4
percentage point improvement in F -measure (25%
improvement relative to F1 feature set). In particu-
lar, all the examples in this paper were resolved.
Our algorithm still misclassified several an-
tecedents. Sometimes even the Web is not large
enough to contain the instantiated pattern, espe-
cially when this is situation or speaker specific. An-
other problem is the high number of NE antecedents
(39.6%) in our corpus. While our NER module is
quite good, any errors in NE classification lead to
incorrect instantiations and thus to incorrect classi-
fications. In addition, the Web feature does not yet
take into account pronouns (7.43% of all correct and
potential antecedents in our corpus).
6 Related Work and Discussion
Modjeska (2002) presented two hand-crafted algo-
rithms, SAL and LEX, which resolve the anaphoric
references of other-NPs on the basis of grammati-
cal salience and lexical information from WordNet,
respectively. In our own previous work (Markert et
15If several antecedents have the highest MI
ant
they all get
value ?webfirst?.
Table 4: Results with F1 and F1+Web
Features P R F
baseline 27.8 27.8 27.8
F1 51.7 40.6 45.5
F1+Web 60.8 53.4 56.9
al., 2003) we presented a preliminary symbolic ap-
proach that uses Web counts and a recency-based
tie-breaker for resolution of other-anaphora and
bridging descriptions. (For another Web-based sym-
bolic approach to bridging see (Bunescu, 2003).)
The approach described in this paper is the first ma-
chine learning approach to other-anaphora. It is
not directly comparable to the symbolic approaches
above for two reasons. First, the approaches dif-
fer in the data and the evaluation metrics they used.
Second, our algorithm does not yet constitute a
full resolution procedure. As the classifier oper-
ates on the whole set of antecedent-anaphor pairs,
more than one potential antecedent for each anaphor
can be classified as ?antecedent=yes?. This can
be amended by e.g. incremental processing. Also,
the classifier does not know that each other-NP is
anaphoric and therefore has an antecedent. (This
contrasts with e.g. definite NPs.) Thus, it can clas-
sify all antecedents as ?antecedent=no?. This can be
remedied by using a back-off procedure, or a compe-
tition learning approach (Connolly et al, 1997). Fi-
nally, the full resolution procedure will have to take
into account other factors, e.g., syntactic constraints
on antecedent realization.
Our approach is the first ML approach to any kind
of anaphora that integrates the Web. Using the Web
as a knowledge source has considerable advantages.
First, the size of the Web almost eliminates the prob-
lem of data sparseness for our task. For this rea-
son, using the Web has proved successful in sev-
eral other fields of NLP, e.g., machine translation
(Grefenstette, 1999) and bigram frequency estima-
tion (Keller et al, 2002). In particular, (Keller et al,
2002) have shown that using the Web handles data
sparseness better than smoothing. Second, we do
not process the returned Web pages in any way (tag-
ging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poe-
sio et al, 2002). Third, the linguistically motivated
patterns we use reduce long-distance dependencies
between anaphor and antecedent to local dependen-
cies. By looking up these patterns on the Web we
obtain semantic information that is not and perhaps
should not be encoded in an ontology (redescrip-
tions, vague relations, etc.). Finally, these local de-
pendencies also reduce the need for prior word sense
disambiguation, as the anaphor and the antecedent
constrain each other?s sense within the context of the
pattern.
7 Conclusions
We presented a machine learning approach to other-
anaphora, which uses a NB classifier and two sets
of features. The first set consists of standard
morpho-syntactic, recency, and semantic features
based on WordNet. The second set alo incorpo-
rates semantic knowledge obtained from the Web via
lexico-semantic patterns specific to other-anaphora.
Adding this knowledge resulted in a dramatic im-
provement of 11.4% points in the classifier?s F -
measure, yielding a final F -measure of 56.9%.
To our knowledge, we are the first to integrate a
Web feature into a ML framework for anaphora reso-
lution. Adding this feature is inexpensive, solves the
data sparseness problem, and allows to handle ex-
amples with non-standard relations between anaphor
and antecedent. The approach is easily applicable to
other anaphoric phenomena by developing appropri-
ate lexico-syntactic patterns (Markert et al, 2003).
Acknowledgments
Natalia N.Modjeska is supported by EPSRC grant
GR/M75129; Katja Markert by an Emmy Noether
Fellowship of the Deutsche Forschungsgemen-
schaft. We thank three anonymous reviewers for
helpful comments and suggestions.
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proc. of ACL?95, pages 122?129.
M. Berland and E. Charniak. 1999. Finding parts in very
large corpora. In Proc. of ACL?99, pages 57?64.
G. Bierner. 2001. Alternative phrases and natural lan-
guage information retrieval. In Proc. of ACL?01.
R. Bunescu. 2003. Associative anaphora resolution: A
Web-based approach. In R. Dale, K. van Deemter, and
R. Mitkov, editors, Proc. of the EACL Workshop on the
Computational Treatment of Anaphora.
D. Connolly, J. D. Burger, and D. S. Day. 1997. A
machine learning approach to anaphoric reference. In
Daniel Jones and Harold Somers, editors, New Meth-
ods in Language Processing, pages 133?144. UCL
Press, London.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. The MIT Press.
G. Grefenstette. 1999. The WWW as a resource for
example-based MT tasks. In Proc. of ASLIB?99 Trans-
lating and the Computer 21, London.
S. Harabagiu, G. Miller, and D. Moldovan. 1999. Word-
net 2 - a morphologically and semantically enhanced
resource. In Proc. of SIGLEX-99, pages 1?8.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of COLING-92.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using the
Web to overcome data sparseness. In Proc. of EMNLP
2002, pages 230?237.
K. Markert, M. Nissim, and N. N. Modjeska. 2003.
Using the Web for nominal anaphora resolution. In
R. Dale, K. van Deemter, and R. Mitkov, editors, Proc.
of the EACL Workshop on the Computational Treat-
ment of Anaphora, pages 39?46.
J. F. McCarthy and W. G. Lehnert. 1995. Using decision
trees for coreference resolution. In Proc. of IJCAI-95,
pages 1050?1055.
N. N. Modjeska. 2002. Lexical and grammatical role
constraints in resolving other-anaphora. In Proc. of
DAARC 2002, pages 129?134.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proc. of
ACL?02, pages 104?111.
M. Poesio, T. Ishikawa, S. Schulte im Walde, and
R. Viera. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proc. of LREC 2002, pages
1220?1224.
J. Preiss. 2002. Anaphora resolution with word sense
disambiguation. In Proc. of SENSEVAL-2, pages 143?
146.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
M. Strube, S. Rapp, and C. Mu?ller. 2002. The influence
of minimum edit distance on reference resolution. In
Proc. of EMNLP 2002, pages 312?319.
M. Strube. 2002. NLP approaches to reference resolu-
tion. Tutorial notes, ACL?02.
Exploiting Context for Biomedical Entity Recognition:
From Syntax to the Web
Jenny Finkel,* Shipra Dingare,? Huy Nguyen,*
Malvina Nissim,? Christopher Manning,* and Gail Sinclair?
*Department of Computer Science
Stanford University
Stanford, CA 93405-9040
United States
{jrfinkel|htnguyen|manning}
@cs.stanford.edu
?Institute for Communicating and
Collaborative Systems
University of Edinburgh
Edinburgh EH8 9LW
United Kingdom
{sdingar1|mnissim|csincla1}
@inf.ed.ac.uk
Abstract
We describe a machine learning system for the
recognition of names in biomedical texts. The sys-
tem makes extensive use of local and syntactic fea-
tures within the text, as well as external resources
including the web and gazetteers. It achieves an F-
score of 70% on the Coling 2004 NLPBA/BioNLP
shared task of identifying five biomedical named en-
tities in the GENIA corpus.
1 Introduction
The explosion of information in the fields of molec-
ular biology and genetics has provided a unique
opportunity for natural language processing tech-
niques to aid researchers and curators of databases
in the biomedical field by providing text mining
services. Yet typical natural language processing
tasks such as named entity recognition, informa-
tion extraction, and word sense disambiguation are
particularly challenging in the biomedical domain
with its highly complex and idiosyncratic language.
With the increasing use of shared tasks and shared
evaluation procedures (e.g., the recent BioCreative,
TREC, and KDD Cup), it is rapidly becoming clear
that performance in this domain is markedly lower
than the field has come to expect from the standard
domain of newswire. The Coling 2004 shared task
focuses on the problem of Named Entity Recogni-
tion, requiring participating systems to identify the
five named entities of protein, RNA, DNA, cell line,
and cell type in the GENIA corpus of MEDLINE
abstracts (Ohta et al, 2002). In this paper we de-
scribe a machine learning system incorporating a di-
verse set of features and various external resources
to accomplish this task. We describe our system in
detail and also discuss some sources of error.
2 System Description
Our system is a Maximum Entropy Markov Model,
which further develops a system earlier used for the
CoNLL 2003 shared task (Klein et al, 2003) and the
2004 BioCreative critical assessment of information
extraction systems, a task that involved identifying
gene and protein name mentions but not distinguish-
ing between them (Dingare et al, 2004). Unlike
the above two tasks, many of the entities in the cur-
rent task do not have good internal cues for distin-
guishing the class of entity: various systematic pol-
ysemies and the widespread use of acronyms mean
that internal cues are lacking. The challenge was
thus to make better use of contextual features, in-
cluding local and syntactic features, and external re-
sources in order to succeed at this task.
2.1 Local Features
We used a variety of features describing the imme-
diate content and context of each word, including
the word itself, the previous and next words, word
prefixes and suffix of up to a length of 6 characters,
word shapes, and features describing the named en-
tity tags assigned to the previous words. Word
shapes refer to a mapping of each word onto equiva-
lence classes that encodes attributes such as length,
capitalization, numerals, greek letters, and so on.
For instance, ?Varicella-zoster? would become Xx-
xxx, ?mRNA? would become xXXX, and ?CPA1?
would become XXXd. We also incorporated part-of-
speech tagging, using the TnT tagger(Brants, 2000)
retrained on the GENIA corpus gold standard part-
of-speech tagging. We also used various interaction
terms (conjunctions) of these base-level features in
various ways. The full set of local features is out-
lined in Table 1.
2.2 External Resources
We made use of a number of external resources, in-
cluding gazetteers, web-querying, use of the sur-
rounding abstract, and frequency counts from the
British National Corpus.
88
Word Features wi, wi?1, wi+1
Disjunction of 5 prev words
Disjunction of 5 next words
TnT POS POSi, POSi?1, POSi+1
Prefix/suffix Up to a length of 6
Abbreviations abbri
abbri?1 + abbri
abbri + abbri+1
abbri?1 + abbri + abbri+1
Word Shape shapei, shapei?1, shapei+1
shapei?1 + shapei
shapei + shapei+1
shapei?1 + shapei + shapei+1
Prev NE NEi?1, NEi?2 + NEi?1
NEi?3 + NEi?2 + NEi?1
Prev NE + Word NEi?1 + wi
Prev NE + POS NEi?1 + POSi?1 + POSi
NEi?2 + NEi?1 + POSi?2 +
POSi?1 + POSi
Prev NE + Shape NEi?1 + shapei
NEi?1 + shapei+1
NEi?1 + shapei?1 + shapei
NEi?2 + NEi?1 + shapei?2 +
shapei?1 + shapei
Paren-Matching Signals when one parenthesis
in a pair has been assigned a
different tag than the other in a
window of 4 words
Table 1: Local Features (+ indicates conjunction)
2.2.1 Frequency
Many entries in gazetteers are ambiguous words,
occasionally used in the sense that the gazetteer
seeks to represent, but at least as frequently not.
So while the information that a token was seen in
a gazetteer is an unreliable indicator of whether it
is an entity, less frequent words are less likely to be
ambiguous than more frequent ones. Additionally,
more frequent words are likely to have been seen
often in the training data and the system should be
better at classifying them, while less frequent words
are a common source of error and their classifica-
tion is more likely to benefit from the use of external
resources. We assigned each word in the training
and testing data a frequency category correspond-
ing to its frequency in the British National Corpus,
a 100 million word balanced corpus, and used con-
junctions of this category and certain other features.
2.2.2 Gazetteers
Our gazetteer contained only gene names and was
compiled from lists from biomedical websites (such
as LocusLink) as well as from the Gene Ontol-
ogy and the data provided for the BioCreative 2004
tasks. The final gazetteer contained 1,731,496 en-
tries. Because it contained only gene names, and for
the reasons discussed earlier, we suspect that it was
not terribly useful for identifying the presences of
entities, but rather that it mainly helped to establish
the exact beginning and ending point of multi-word
entities recognized mainly through other features.
2.2.3 Web
For each of the named entity classes, we built in-
dicative contexts, such as ?X mRNA? for RNA, or
?X ligation? for protein. For each entity X which
had a frequency lower than 10 in the British Na-
tional Corpus, we submitted instantiations of each
pattern to the web, using the Google API, and ob-
tained the number of hits. The pattern that returned
the highest number of hits determined the feature
value (e.g., ?web-protein?, or ?web-RNA?). If no
hits were returned by any pattern, a value ?O-web?
was assigned. This value was also assigned to all
words whose frequency was higher than 10 (using
yet another value for words with higher frequency
did not improve the tagger?s performance).
2.2.4 Abstracts
A number of NER systems have made effective use
of how the same token was tagged in different parts
of the same document (see (Curran and Clark, 2003)
and (Mikheev et al, 1999)). A token which appears
in an unindicative context in one sentence may ap-
pear in a very obvious context in another sentence
in the same abstract. To leverage this we tagged
each abstract twice, providing for each token a fea-
ture indicating whether it was tagged as an entity
elsewhere in the abstract. This information was
only useful when combined with information on fre-
quency.
2.3 Deeper Syntactic Features
While the local features discussed earlier are all
fairly surface level, our system also makes use of
deeper syntactic features. We fully parsed the train-
ing and testing data using the Stanford Parser of
(Klein and Manning, 2003) operating on the TnT
part-of-speech tagging ? we believe that the un-
lexicalized nature of this parser makes it a partic-
ularly suitable statistical parser to use when there
is a large domain mismatch between the training
material (Wall Street Journal text) and the target
domain, but have not yet carefully evaluated this.
Then, for each word in the sentence which is in-
side a noun phrase, the head and governor of the
noun phrase are extracted. These features are not
very useful when identifying only two classes (such
as GENE and OTHER in the BioCreative task), but
they were quite useful for this task because of the
large number of classes which the system needed to
distinguish between. Because the classifier is now
89
choosing between classes where members can look
very similar, longer distance information can pro-
vide a better representation of the context in which
the word appears. For instance, the word phospho-
rylation occurs in the training corpus 492 times, 482
of which it is was classified as other. However,
it is the governor of 738 words, of which 443 are
protein, 292 are other and only 3 are cell
line.
We also made use of abbreviation matching to
help ensure consistency of labels. Abbreviations
and long forms were extracted from the data using
the method of (Schwartz and Hearst, 2003). This
data was combined with a list of other abbreviations
and long forms extracted from the BioCreative 2004
task. Then all occurrences of either the long or short
forms in the data was labeled. These labels were in-
cluded in the system as features and helped to im-
prove boundary detection.
2.4 Adjacent Entities
When training our classifier, we merged the B- and
I- labels for each class, so it did not learn how to
differentiate between the first word of a class and
internal word. There were several motivations for
doing this. Foremost was memory concerns; our fi-
nal system trained on just the six classes had 1.5
million features ? we just did not have the resources
to train it over more classes without giving up many
of our features. Our second motivation was that by
merging the beginning and internal labels for a par-
ticular class, the classifier would see more examples
of that class and learn better how to identify it. The
drawback of this move is that when two entities be-
longing to the same class are adjacent, our classifier
will automatically merge them into one entity. We
did attempt to split them back up using NP chunks,
but this severely reduced performance.
3 Results and Discussion
Our results on the evaluation data and a confusion
matrix are shown in Tables 2 and 4. Table 4 sug-
gests areas for further work. Collapsing the B- and
I- tags does cost us quite a bit. Otherwise confusions
between some named entity and being nothing are
most of the errors, although protein/DNA and cell-
line/cell-type confusions are also noticeable.
Analysis of performance in biomedical Named
Entity Recognition tends to be dominated by the
perceived poorness of the results, stemming from
the twin beliefs that performance of roughly ninety
percent is the state-of-the-art and that performance
of 100% (or close to that) is possible and the goal
to be aimed for. Both of these beliefs are ques-
tionable, as the top MUC 7 performance of 93.39%
Entity Precision Recall F-Score
Fully Correct
protein 77.40% 68.48% 72.67%
DNA 66.19% 69.62% 67.86%
RNA 72.03% 65.89% 68.83%
cell line 59.00% 47.12% 52.40%
cell type 62.62% 76.97% 69.06%
Overall 71.62% 68.56% 70.06%
Left Boundary Correct
protein 82.89% 73.34% 77.82%
DNA 68.47% 72.01% 70.19%
RNA 75.42% 68.99% 72.06%
cell line 63.80% 50.96% 56.66%
cell type 63.93% 78.57% 70.49%
Overall 75.72% 72.48% 74.07%
Right Boundary Correct
protein 84.70% 74.96% 79.53%
DNA 74.43% 78.29% 76.31%
RNA 78.81% 72.09% 75.30%
cell line 70.2% 56.07% 62.34%
cell type 71.68% 88.10% 79.05%
Overall 79.65% 76.24% 77.91%
Table 2: Results on the evaluation data
(Mikheev et al, 1998) in the domain of newswire
text used an easier performance metric where incor-
rect boundaries were given partial credit, while both
the biomedical NER shared tasks to date have used
an exact match criterion where one is doubly penal-
ized (both as a FP and as a FN) for incorrect bound-
aries. However, the difference in metric clearly can-
not account entirely for the performance discrep-
ancy between newswire NER and biomedical NER.
Biomedical NER appears to be a harder task due
to the widespread ambiguity of terms out of con-
text, the complexity of medical language, and the
apparent need for expert domain knowledge. These
are problems that more sophisticated machine learn-
ing systems using resources such as ontologies and
deep processing might be able to overcome. How-
ever, one should also consider the inherent ?fuzzi-
ness? of the classification task. The few existing
studies of inter-annotator agreement for biomedi-
cal named entities have measured agreement be-
tween 87%(Hirschman, 2003) and 89%(Demetrious
and Gaizauskas, 2003). As far as we know there
are no inter-annotator agreement results for the GE-
NIA corpus, and it is necessary to have such results
before properly evaluating the performance of sys-
tems. In particular, the fact that BioNLP sought to
distinguish between gene and protein names, when
these are known to be systematically ambiguous,
and when in fact in the GENIA corpus many enti-
ties were doubly classified as ?protein molecule or
90
DNA RNA cell line cell type protein
gold\ans B- I- B- I- B- I- B- I- B- I- O
B-DNA 723 39 0 0 1 0 0 0 154 1 138
I-DNA 52 1390 0 0 0 0 0 0 19 71 257
B-RNA 1 0 89 3 0 0 0 0 14 0 11
I-RNA 0 1 5 164 0 0 0 0 2 0 15
B-cell line 3 0 0 0 319 41 37 5 12 1 82
I-cell line 0 6 0 0 24 713 5 104 0 14 123
B-cell type 1 0 0 0 164 22 1228 90 31 5 380
I-cell type 0 0 0 0 13 383 88 2101 8 27 371
B-protein 48 5 10 3 20 1 19 3 4200 192 566
I-protein 6 66 0 11 0 10 2 25 245 3630 779
O 170 240 25 26 85 142 184 132 1042 656 78945
Table 3: Our confusion matrix over the evaluation data
human B-cell type
monocytes I-cell type
human O
monocytes B-cell type
macrophages B-cell type
primary B-cell type JJ
T I-cell type NN
lymphocytes I-cell type NNS
primary O JJ
peripheral B-cell type JJ
blood I-cell type NN
lymphocytes I-cell type NNS
Table 4: Examples of annotation inconsistencies
region? and ?DNA molecule or region?, suggests
that inter-annotator agreement could be low, and
that many entities in fact have more than one classi-
fication.
One area where GENIA appears inconsistent is
in the labeling of preceding adjectives. The data
was selected by querying for the term human, yet
the term is labeled inconsistently, as is shown in Ta-
ble 4. Of the 1790 times the term human occurred
before or at the beginning of an entity in the train-
ing data, it was not classified as part of the entity
110 times. In the test data, there is only on instance
(out of 130) where the term is excluded. Adjectives
are excluded approximately 25% of the time in both
the training and evaluation data. There are also in-
consistencies when two entities are separated by the
word and.
4 Acknowledgements
This paper is based on work supported in part by a
Scottish Enterprise Edinburgh-Stanford Link Grant
(R36759), as part of the SEER project, and in part
the National Science Foundation under the Knowl-
edge Discovery and Dissemination program.
References
Thorsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In ANLP 6, pages 224?231.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of the Seventh Conference on Natural
Language Learning (CoNLL-03), pages 164?167.
George Demetrious and Rob Gaizauskas. 2003. Corpus
resources for development and evaluation of a biolog-
ical text mining system. In Proceedings of the Third
Meeting of the Special Interest Group on Text Mining,
Brisbane, Australia, July.
Shipra Dingare, Jenny Rose Finkel, Christopher Man-
ning, Malvina Nissim, and Beatrice Alex. 2004. Ex-
ploring the boundaries: Gene and protein identifica-
tion in biomedical text. In Proceedings of the BioCre-
ative Workshop.
Lynette Hirschman. 2003. Using biological resources to
bootstrap text mining.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 41, pages 423?430.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In CoNLL 7, pages 180?
183.
Andrei Mikheev, Claire Grover, and Mark Moens. 1998.
Description of the LTG system used for MUC-7. In
Proceedings of MUC-7.
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named entity recognition without gazetteers. In Pro-
ceedings of the ninth conference on European chap-
ter of the Association for Computational Linguistics,
pages 1?8. Association for Computational Linguis-
tics.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA corpus: an annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of he Human Language Technology Confer-
ence, pages 73?77.
Ariel Schwartz and Marti Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in
biomedical text. In Pacific Symposium on Biocomput-
ing, Kauai, Jan.
91
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 45?52,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Framework for Annotating Information Structure in Discourse
Sasha Calhoun   , Malvina Nissim   , Mark Steedman   and Jason Brenier 
 
Institute for Communicating and Collaborative Systems, University of Edinburgh, UK
Sasha.Calhoun@ed.ac.uk,

steedman,mnissim  @inf.ed.ac.uk

Department of Linguistics, University of Colorado at Boulder
jbrenier@colorado.edu
Abstract
We present a framework for the integrated
analysis of the textual and prosodic char-
acteristics of information structure in the
Switchboard corpus of conversational En-
glish. Information structure describes the
availability, organisation and salience of
entities in a discourse model. We present
standards for the annotation of informa-
tion status (old, mediated and new), and
give guidelines for annotating informa-
tion structure, i.e. theme/rheme and back-
ground/kontrast. We show that informa-
tion structure in English can only be anal-
ysed concurrently with prosodic promi-
nence and phrasing. This annotation, us-
ing stand-off XML in NXT, can help es-
tablish standards for the annotation of in-
formation structure in discourse.
1 Introduction
We present a framework for the integrated analysis
of the textual and prosodic characteristics of infor-
mation structure in a corpus of conversational En-
glish. Section 2 introduces the corpus as well as the
tools we employ in the annotation process. We pro-
pose two complementary annotation efforts within
this framework. The rst, information status (old,
mediated, new), expresses the availability of entities
in discourse (Section 3). The second scheme will
rstly annotate theme/rheme, i.e. how each intona-
tion phrase is organised in the discourse model, and
secondly kontrast: how salient the speaker wishes
to make each entity, property or relation (Section 4).
We will demonstrate that the perception of both of
these is intimately affected by prosodic structure. In
particular, the theme/rheme division affects prosodic
phrasing; and information status and kontrast affect
relative prosodic prominence. Therefore we also
propose to annotate a subset of the corpus for this
prosodic information (Section 5). In conjunction
with existing annotations of the corpus, our inte-
grated framework using NXT will be unique in the
eld of conversational speech in terms of size and
richness of annotation.
2 Corpus and Tools
The Switchboard Corpus (Godfrey et al, 1992) con-
sists of 2430 spontaneous phone conversations (av-
erage six minutes), between speakers of American
English, for three million words. The corpus is
distributed as stereo speech signals with an ortho-
graphic transcription per channel time-stamped at
the word level. A third of this is syntactically parsed
as part of the Penn Treebank (Marcus et al, 1993)
and has dialog act annotation (Shriberg et al, 1998).
We used a subset of this. In adherence with current
standards, we converted all the existing annotations,
and are producing the new discourse annotations in
a coherent multi-layered XML-conformant schema,
using NXT technology (Carletta et al, 2004).1 This
allows us to search over and integrate information
from the many layers of annotation, including the
1Beside the NXT tools, we also used the TIGER Switch-
board filter (Mengel and Lezius, 2000) for the XML-
conversion. Using existing markup we automatically selected
and filtered NPs to be annotated, excluding locative, directional,
and adverbial NPs and disfluencies, and adding possessive pro-
nouns. See (Nissim et al, 2004) for technical details.
45
sound les. NXT tools can be easily customised
to accommodate different layers of annotation users
want to add, including data sets that have low-level
annotations time-stamped against a set of synchro-
nized signals, multiple, crossing tree structures, and
connection to external corpus resources such as ges-
ture ontologies and lexicons (Carletta et al, 2004).
3 Information Status
Information Status describes how available an en-
tity is in the discourse. We dene this in terms of
the speaker?s assumptions about the hearer?s knowl-
edge/beliefs, and we express it by the well-known
old/new distinction.2
3.1 Annotation Scheme
Our annotation scheme for the discourse layer
mainly builds on (Prince, 1992) and (Eckert and
Strube, 2001), as well as on related work on
annotation of anaphoric links (Passonneau, 1996;
Hirschman and Chinchor, 1997; Davies et al, 1998;
Poesio, 2000). Prince denes ?old? and ?new? with
respect to the discourse model as well as the hearer?s
point of view. Considering the interaction of both
these aspects, we dene as new an entity which has
not been previously referred to and is yet unknown
to the hearer, and as mediated an entity that is newly
mentioned in the dialogue but that the hearer can in-
fer from the prior context.3 This is mainly the case
of generally known entities (such as ?the sun?, or
?the Pope? (L?obner, 1985)), and bridging (Clark,
1975), where an entity is related to a previously in-
troduced one. Whenever an entity is not new nor
mediated is considered as old.
Because ner-grained distinctions (e.g. (Prince,
1981; Lambrecht, 1994)) have proved hard to distin-
guish reliably in practice, we organise our scheme
hierarchically: we use the three main classes de-
scribed above as top level categories for which more
specic subtypes can assigned. This approach pre-
serves a high-level, more reliable distinction while
allowing a ner-grained classication that can be ex-
ploited for specic tasks.
Besides the main categories, we introduce two
more classes. A category non-applicable is used for
2We follow Prince in using ? old? rather than ?given? to refer
to ?not-new? information, but regard the two as identical.
3This type corresponds to Prince?s (1981; 1992) inferrables.
wrongly extracted markables (such as ?course? in
?of course?), for idiomatic occurrences, and exple-
tive uses of ?it?. Traces are automatically extracted
as markables, but are left unannotated. In the rare
event the annotators nd some fragments too dif-
cult to understand, a category not-understood can be
assigned. Entities marked as non-applicable or not-
understood are excluded from any further annotation.
For all other markables, the annotators must choose
between old, mediated, and new. For the rst two,
subtypes can also be specied: subtype assignment
is encouraged but not compulsory.
New The category new is assigned to entities that
have not yet been introduced in the dialogue and that
the hearer cannot infer from previously mentioned
entities. No subtypes are specied for this category.
Mediated Mediated entities are inferrable from
previously mentioned ones, or generally known to
the hearer. We specify nine subtypes: general, bound,
part, situation, event, set, poss, func value, aggrega-
tion.4 Generally known entities such as ?the moon?
or ?Italy? are assigned a subtype general. Most
proper nouns fall into this subclass, but the anno-
tator could opt for a different tag, depending on the
context. Also mediated are bound pronouns, such as
?them? in (1), which are assigned a subtype bound.5
(1) [. . . ] it?s hard to raise one child without them
thinking they?re the pivot point of the universe.
A subtype poss is used to mark all kinds of intra-
phrasal possessive relations (pre- and postnominal).
Four subtypes (part, situation, event, and set) are
used to mark instances of bridging. The subtype part
is used to mark part-whole relations for physical ob-
jects, both as intra- and inter-phrasal relations. (This
category is to be preferred to poss whenever appli-
cable.) The occurrence of ?the door? in (2), for in-
stance, is annotated as mediated/part.
(2) When I come home in the evenings my dog
greets me at the door.
For similar relations that do not involve physical ob-
jects, i.e. if an entity is part of a situation set up by
4Some of the subtypes are inspired by categories developed
for bridging markup (Passonneau, 1996; Davies et al, 1998).
5All examples in this paper are from the Switchboard Cor-
pus. The markable in question is typed in boldface; antecedents
or trigger entities, where present, are in italics. For the sake of
space we do not provide examples for each category (see (Nis-
sim, 2003)).
46
a previously introduced entity, we use the subtype
situation.6,as for the NP ?the specications? in (3).
(3) I guess I don?t really have a problem with cap-
ital punishment. I?m not really sure what the
exact specifications are for Texas.
The subtype event is applied whenever an entity is
related to a previously mentioned verb phrase (VP).
In (4), e.g., ?the bus? is triggered by travelling
around Yucatan.
(4) We were travelling around Yucatan, and the
bus was really full.
Whenever an entity referred to is a subset of, a super-
set of, or a member of the same set as a previously
mentioned entity, the subtype set is applied.
Rarely, an entity refers to a value of a previously
mentioned function, as ?zero? and ?ten? in (5). In
such cases a subtype func-value is assigned.
(5) I had kind of gotten used to centigrade temper-
ature [. . . ] if it?s between zero and ten it?s cold.
Lastly, a subtype aggregation is used to classify co-
ordinated NPs. Two old or med entities, for instance
do not give rise to an old coordinated NP, unless it
has been previously introduced as such. A medi-
ated/aggregation tag is assigned instead.
Old An entity is old when it is not new nor medi-
ated. This is usually the case if an entity is coref-
erential with an already introduced entity, if it is
a generic pronoun, or if it is a personal pronoun
referring to the dialogue participants. Six differ-
ent subtypes are available for old entities: identity,
event, general, generic, ident generic, relative. In (6),
for instance, ?us? would be marked as old because it
corefers with ?we?, and a subtype identity would also
be assigned.
(6) [. . . ] we camped in a tent, and uh there were
two other couples with us.
In addition, a coreference link is marked up between
anaphor and antecedent, thus creating anaphoric
chains (see also (Carletta et al, 2004)). The subtype
event applies whenever the antecedent is a VP. In (7),
?it? is old/event, as its antecedent is the VP ?educate
three?. As we do not extract VPs as markables, no
link can be marked up.
(7) I most certainly couldn?t educate three. I don?t
know how my parents did it.
6This includes elements of the thematic grid of an already
introduced entity. It subsumes Passonneau?s (1996) class ?arg?.
Also classied as old are personal pronouns refer-
ring to the dialogue participants as well as generic
pronouns. In the rst case, a subtype general is spec-
ied, whereas the subtype for the second is generic.
An instance of old/generic is ?you? in (8).
(8) up here you got to wait until Aug- August until
the water warms up.
In a chain of generic references, the subtype
ident generic is assigned, and a coreference link is
marked up. Coreference is also marked up for rel-
ative pronouns: they receive a subtype relative and
are linked back to their head.
The guidelines contain a decision tree the annota-
tors use to establish priority in case more than one
class is appropriate for a given entity. For example,
if a mediated/general entity is also old/identity the latter
is to be preferred to the former. Similar precedence
relations hold among subtypes.
To provide more robust and reliable clues in an-
notating bridging types (e.g. for distinguishing
between poss and part), we provided replacement
tests and referred to relations encoded in knowledge
bases such as WordNet (Fellbaum, 1998) (for part)
and FrameNet (Baker et al, 1998) (for situation).
3.2 Validation of the Scheme
Three Switchboard dialogues (for a total of 1738
markables) were marked up by two different anno-
tators for assessing the validity of the scheme. We
evaluated annotation reliability by using the Kappa
statistic (Carletta, 1996). Good quality annotation
of discourse phenomena normally yields a kappa
(  ) of about .80. We assessed the validity of the
scheme on the four-way classication into the three
main categories (old, mediated and new) and the non-
applicable category. We also evaluated the annota-
tion including the subtypes. All cases where at least
one annotator assigned a not-understood tag were ex-
cluded from the agreement evaluation (14 mark-
ables). Also excluded were all traces (222 mark-
ables), which the annotators left unmarked. The
total markables considered for evaluation over the
three dialogues was therefore 1502.
The annotation of the three dialogues yielded
 	
 for the high-level categories, and  
 when including subtypes (  ; Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 36?41,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 08: Metonymy Resolution at SemEval-2007
Katja Markert
School of Computing
University of Leeds, UK
markert@comp.leeds.ac.uk
Malvina Nissim
Dept. of Linguistics and Oriental Studies
University of Bologna, Italy
malvina.nissim@unibo.it
Abstract
We provide an overview of the metonymy
resolution shared task organised within
SemEval-2007. We describe the problem,
the data provided to participants, and the
evaluation measures we used to assess per-
formance. We also give an overview of the
systems that have taken part in the task, and
discuss possible directions for future work.
1 Introduction
Both word sense disambiguation and named entity
recognition have benefited enormously from shared
task evaluations, for example in the Senseval, MUC
and CoNLL frameworks. Similar campaigns have
not been developed for the resolution of figurative
language, such as metaphor, metonymy, idioms and
irony. However, resolution of figurative language is
an important complement to and extension of word
sense disambiguation as it often deals with word
senses that are not listed in the lexicon. For exam-
ple, the meaning of stopover in the sentence He saw
teaching as a stopover on his way to bigger things
is a metaphorical sense of the sense ?stopping place
in a physical journey?, with the literal sense listed
in WordNet 2.0 but the metaphorical one not being
listed.1 The same holds for the metonymic reading
of rattlesnake (for the animal?s meat) in Roast rat-
tlesnake tastes like chicken.2 Again, the meat read-
1This example was taken from the Berkely Master Metaphor
list (Lakoff and Johnson, 1980) .
2From now on, all examples in this paper are taken from the
British National Corpus (BNC) (Burnard, 1995), but Ex. 23.
ing of rattlesnake is not listed in WordNet whereas
the meat reading for chicken is.
As there is no common framework or corpus for
figurative language resolution, previous computa-
tional works (Fass, 1997; Hobbs et al, 1993; Barn-
den et al, 2003, among others) carry out only small-
scale evaluations. In recent years, there has been
growing interest in metaphor and metonymy resolu-
tion that is either corpus-based or evaluated on larger
datasets (Martin, 1994; Nissim and Markert, 2003;
Mason, 2004; Peirsman, 2006; Birke and Sarkaar,
2006; Krishnakamuran and Zhu, 2007). Still, apart
from (Nissim and Markert, 2003; Peirsman, 2006)
who evaluate their work on the same dataset, results
are hardly comparable as they all operate within dif-
ferent frameworks.
This situation motivated us to organise the first
shared task for figurative language, concentrating on
metonymy. In metonymy one expression is used to
refer to the referent of a related one, like the use of
an animal name for its meat. Similarly, in Ex. 1,
Vietnam, the name of a location, refers to an event (a
war) that happened there.
(1) Sex, drugs, and Vietnam have haunted Bill
Clinton?s campaign.
In Ex. 2 and 3, BMW, the name of a company, stands
for its index on the stock market, or a vehicle manu-
factured by BMW, respectively.
(2) BMW slipped 4p to 31p
(3) His BMW went on to race at Le Mans
The importance of resolving metonymies has been
shown for a variety of NLP tasks, such as ma-
36
chine translation (Kamei and Wakao, 1992), ques-
tion answering (Stallard, 1993), anaphora resolution
(Harabagiu, 1998; Markert and Hahn, 2002) and
geographical information retrieval (Leveling and
Hartrumpf, 2006).
Although metonymic readings are, like all figu-
rative readings, potentially open ended and can be
innovative, the regularity of usage for word groups
helps in establishing a common evaluation frame-
work. Many other location names, for instance, can
be used in the same fashion as Vietnam in Ex. 1.
Thus, given a semantic class (e.g. location), one
can specify several regular metonymic patterns (e.g.
place-for-event) that instances of the class are likely
to undergo. In addition to literal readings, regu-
lar metonymic patterns and innovative metonymic
readings, there can also be so-called mixed read-
ings, similar to zeugma, where both a literal and a
metonymic reading are evoked (Nunberg, 1995).
The metonymy task is a lexical sample task for
English, consisting of two subtasks, one concentrat-
ing on the semantic class location, exemplified by
country names, and another one concentrating on or-
ganisation, exemplified by company names. Partici-
pants had to automatically classify preselected coun-
try/company names as having a literal or non-literal
meaning, given a four-sentence context. Addition-
ally, participants could attempt finer-grained inter-
pretations, further specifying readings into prespec-
ified metonymic patterns (such as place-for-event)
and recognising innovative readings.
2 Annotation Categories
We distinguish between literal, metonymic, and
mixed readings for locations and organisations. In
the case of a metonymic reading, we also specify
the actual patterns. The annotation categories were
motivated by prior linguistic research by ourselves
(Markert and Nissim, 2006), and others (Fass, 1997;
Lakoff and Johnson, 1980).
2.1 Locations
Literal readings for locations comprise locative
(Ex. 4) and political entity interpretations (Ex. 5).
(4) coral coast of Papua New Guinea.
(5) Britain?s current account deficit.
Metonymic readings encompass four types:
- place-for-people a place stands for any per-
sons/organisations associated with it. These can be
governments (Ex. 6), affiliated organisations, incl.
sports teams (Ex. 7), or the whole population (Ex. 8).
Often, the referent is underspecified (Ex. 9).
(6) America did once try to ban alcohol.
(7) England lost in the semi-final.
(8) [. . . ] the incarnation was to fulfil the
promise to Israel and to reconcile the world
with God.
(9) The G-24 group expressed readiness to pro-
vide Albania with food aid.
- place-for-event a location name stands for an
event that happened in the location (see Ex. 1).
- place-for-product a place stands for a product
manufactured in the place, as Bordeaux in Ex. 10.
(10) a smooth Bordeaux that was gutsy enough
to cope with our food
- othermet a metonymy that does not fall into any
of the prespecified patterns, as in Ex. 11, where New
Jersey refers to typical local tunes.
(11) The thing about the record is the influ-
ences of the music. The bottom end is very
New York/New Jersey and the top is very
melodic.
When two predicates are involved, triggering a dif-
ferent reading each (Nunberg, 1995), the annotation
category is mixed. In Ex. 12, both a literal and a
place-for-people reading are involved.
(12) they arrived in Nigeria, hitherto a leading
critic of [. . . ]
2.2 Organisations
The literal reading for organisation names describes
references to the organisation in general, where an
organisation is seen as a legal entity, which consists
of organisation members that speak with a collec-
tive voice, and which has a charter, statute or defined
aims. Examples of literal readings include (among
others) descriptions of the structure of an organisa-
tion (see Ex. 13), associations between organisations
(see Ex. 14) or relations between organisations and
products/services they offer (see Ex. 15).
37
(13) NATO countries
(14) Sun acquired that part of Eastman-Kodak
Cos Unix subsidary
(15) Intel?s Indeo video compression hardware
Metonymic readings include six types:
- org-for-members an organisation stands for
its members, such as a spokesperson or official
(Ex. 16), or all its employees, as in Ex. 17.
(16) Last February IBM announced [. . . ]
(17) It?s customary to go to work in black or
white suits. [. . . ] Woolworths wear them
- org-for-event an organisation name is used to re-
fer to an event associated with the organisation (e.g.
a scandal or bankruptcy), as in Ex. 18.
(18) the resignation of Leon Brittan from Trade
and Industry in the aftermath of Westland.
- org-for-product the name of a commercial or-
ganisation can refer to its products, as in Ex. 3.
- org-for-facility organisations can also stand for
the facility that houses the organisation or one of its
branches, as in the following example.
(19) The opening of a McDonald?s is a major
event
- org-for-index an organisation name can be used
for an index that indicates its value (see Ex. 2).
- othermet a metonymy that does not fall into any
of the prespecified patterns, as in Ex. 20, where Bar-
clays Bank stands for an account at the bank.
(20) funds [. . . ] had been paid into Barclays
Bank.
Mixed readings exist for organisations as well.
In Ex. 21, both an org-for-index and an org-for-
members pattern are invoked.
(21) Barclays slipped 4p to 351p after confirm-
ing 3,000 more job losses.
2.3 Class-independent categories
Apart from class-specific metonymic readings, some
patterns seem to apply across classes to all names. In
the SemEval dataset, we annotated two of them.
object-for-name all names can be used as mere
signifiers, instead of referring to an object or set of
objects. In Ex. 22, both Chevrolet and Ford are used
as strings, rather than referring to the companies.
(22) Chevrolet is feminine because of its sound
(it?s a longer word than Ford, has an open
vowel at the end, connotes Frenchness).
object-for-representation a name can refer to a
representation (such as a photo or painting) of the
referent of its literal reading. In Ex. 23, Malta refers
to a drawing of the island when pointing to a map.
(23) This is Malta
3 Data Collection and Annotation
We used the CIA Factbook3 and the Fortune 500
list as sampling frames for country and company
names respectively. All occurrences (including plu-
ral forms) of all names in the sampling frames were
extracted in context from all texts of the BNC, Ver-
sion 1.0. All samples extracted are coded in XML
and contain up to four sentences: the sentence in
which the country/company name occurs, two be-
fore, and one after. If the name occurs at the begin-
ning or end of a text the samples may contain less
than four sentences.
For both the location and the organisation subtask,
two random subsets of the extracted samples were
selected as training and test set, respectively. Before
metonymy annotation, samples that were not under-
stood by the annotators because of insufficient con-
text were removed from the datsets. In addition, a
sample was also removed if the name extracted was
a homonym not in the desired semantic class (for ex-
ample Mr. Greenland when annotating locations).4
For those names that do have the semantic class
location or organisation, metonymy anno-
tation was performed, using the categories described
in Section 2. All training set annotation was carried
out independently by both organisers. Annotation
was highly reliable with a kappa (Carletta, 1996) of
3https://www.cia.gov/cia/publications/
factbook/index.html
4Given that the task is not about standard Named Entity
Recognition, we assume that the general semantic class of the
name is already known.
38
Table 1: Reading distribution for locations
reading train test
literal 737 721
mixed 15 20
othermet 9 11
obj-for-name 0 4
obj-for-representation 0 0
place-for-people 161 141
place-for-event 3 10
place-for-product 0 1
total 925 908
Table 2: Reading distribution for organisations
reading train test
literal 690 520
mixed 59 60
othermet 14 8
obj-for-name 8 6
obj-for-representation 1 0
org-for-members 220 161
org-for-event 2 1
org-for-product 74 67
org-for-facility 15 16
org-for-index 7 3
total 1090 842
.88/.89 for locations/organisations.5 As agreement
was established, annotation of the test set was car-
ried out by the first organiser. All cases which were
not entirely straightforward were then independently
checked by the second organiser. Samples whose
readings could not be agreed on (after a reconcil-
iation phase) were excluded from both training and
test set. The reading distributions of training and test
sets for both subtasks are shown in Tables 1 and 2.
In addition to a simple text format including only
the metonymy annotation, we provided participants
with several linguistic annotations of both training
and testset. This included the original BNC tokeni-
sation and part-of-speech tags as well as manually
annotated dependency relations for each annotated
name (e.g. BMW subj-of-slip for Ex. 2).
4 Submission and Evaluation
Teams were allowed to participate in the location
or organisation task or both. We encouraged super-
vised, semi-supervised or unsupervised approaches.
Systems could be tailored to recognise
metonymies at three different levels of granu-
5The training sets are part of the already available Mascara
corpus for metonymy (Markert and Nissim, 2006). The test sets
were newly created for SemEval.
larity: coarse, medium, or fine, with an increasing
number and specification of target classification
categories, and thus difficulty. At the coarse level,
only a distinction between literal and non-literal was
asked for; medium asked for a distinction between
literal, metonymic and mixed readings; fine needed
a classification into literal readings, mixed readings,
any of the class-dependent and class-independent
metonymic patterns (Section 2) or an innovative
metonymic reading (category othermet).
Systems were evaluated via accuracy (acc) and
coverage (cov), allowing for partial submissions.
acc = # correct predictions# predictions cov =
# predictions
# samples
For each target category c we also measured:
precisionc = # correct assignments of c# assignments of c
recallc = # correct assignments of c# dataset instances of c
fscorec = 2precisioncrecallcprecisionc+recallc
A baseline, consisting of the assignment of the most
frequent category (always literal), was used for each
task and granularity level.
5 Systems and Results
We received five submissions (FUH, GYDER,
up13, UTD-HLT-CG, XRCE-M). All tackled
the location task; three (GYDER, UTD-HLT-CG,
XRCE-M) also participated in the organisation task.
All systems were full submissions (coverage of 1)
and participated at all granularity levels.
5.1 Methods and Features
Out of five teams, four (FUH, GYDER, up13,
UTD-HLT-CG) used supervised machine learning,
including single (FUH,GYDER, up13) as well
as multiple classifiers (UTD-HLT-CG). A range
of learning paradigms was represented (including
instance-based learning, maximum entropy, deci-
sion trees, etc.). One participant (XRCE-M) built a
hybrid system, combining a symbolic, supervised
approach based on deep parsing with an unsuper-
vised distributional approach exploiting lexical in-
formation obtained from large corpora.
Systems up13 and FUH used mostly shallow fea-
tures extracted directly from the training data (in-
cluding parts-of-speech, co-occurrences and collo-
39
cations). The other systems made also use of syn-
tactic/grammatical features (syntactic roles, deter-
mination, morphology etc.). Two of them (GYDER
and UTD-HLT-CG) exploited the manually anno-
tated grammatical roles provided by the organisers.
All systems apart from up13 made use of exter-
nal knowledge resources such as lexical databases
for feature generalisation (WordNet, FrameNet,
VerbNet, Levin verb classes) as well as other cor-
pora (the Mascara corpus for additional training ma-
terial, the BNC, and the Web).
5.2 Performance
Tables 3 and 4 report accuracy for all systems.6 Ta-
ble 5 provides a summary of the results with lowest,
highest, and average accuracy and f-scores for each
subtask and granularity level.7
The task seemed extremely difficult, with 2 of the
5 systems (up13,FUH) participating in the location
task not beating the baseline. These two systems re-
lied mainly on shallow features with limited or no
use of external resources, thus suggesting that these
features might only be of limited use for identify-
ing metonymic shifts. The organisers themselves
have come to similar conclusions in their own ex-
periments (Markert and Nissim, 2002). The sys-
tems using syntactic/grammatical features (GYDER,
UTD-HLT-CG, XRCE-M) could improve over the
baseline whether using manual annotation or pars-
ing. These systems also made heavy use of feature
generalisation. Classification granularity had only a
small effect on system performance.
Only few of the fine-grained categories could be
distinguished with reasonable success (see the f-
scores in Table 5). These include literal readings,
and place-for-people, org-for-members, and org-for-
product metonymies, which are the most frequent
categories (see Tables 1 and 2). Rarer metonymic
targets were either not assigned by the systems
at all (?undef? in Table 5) or assigned wrongly
6Due to space limitations we do not report precision, recall,
and f-score per class and refer the reader to each system de-
scription provided within this volume.
7The value ?undef? is used for cases where the system did
not attempt any assignment for a given class, whereas the value
?0? signals that assignments were done, but were not correct.
8Please note that results for the FUH system are slightly dif-
ferent than those presented in the FUH system description pa-
per. This is due to a preprocessing problem in the FUH system
that was fixed only after the run submission deadline.
Table 5: Overview of scores
base min max ave
LOCATION-coarse
accuracy 0.794 0.754 0.852 0.815
literal-f 0.849 0.912 0.888
non-literal-f 0.344 0.576 0.472
LOCATION-medium
accuracy 0.794 0.750 0.848 0.812
literal-f 0.849 0.912 0.889
metonymic-f 0.331 0.580 0.476
mixed-f 0.000 0.083 0.017
LOCATION-fine
accuracy 0.794 0.741 0.844 0.801
literal-f 0.849 0.912 0.887
place-for-people-f 0.308 0.589 0.456
place-for-event-f 0.000 0.167 0.033
place-for-product-f 0.000 undef 0.000
obj-for-name-f 0.000 0.667 0.133
obj-for-rep-f undef undef undef
othermet-f 0.000 undef 0.000
mixed-f 0.000 0.083 0.017
ORGANISATION-coarse
accuracy 0.618 0.732 0.767 0.746
literal-f 0.800 0.825 0.810
non-literal-f 0.572 0.652 0.615
ORGANISATION-medium
accuracy 0.618 0.711 0.733 0.718
literal-f 0.804 0.825 0.814
metonymic-f 0.553 0.604 0.577
mixed-f 0.000 0.308 0.163
ORGANISATION-fine
accuracy 0.618 0.700 0.728 0.713
literal-f 0.808 0.826 0.817
org-for-members-f 0.568 0.630 0.608
org-for-event-f 0.000 undef 0.000
org-for-product-f 0.400 0.500 0.458
org-for-facility-f 0.000 0.222 0.141
org-for-index-f 0.000 undef 0.000
obj-for-name-f 0.250 0.800 0.592
obj-for-rep-f undef undef undef
othermet-f 0.000 undef 0.000
mixed-f 0.000 0.343 0.135
(low f-scores). An exception is the object-for-
name pattern, which XRCE-M and UTD-HLT-CG
could distinguish with good success. Mixed read-
ings also proved problematic since more than one
pattern is involved, thus limiting the possibilities
of learning from a single training instance. Only
GYDER succeeded in correctly identifiying a variety
of mixed readings in the organisation subtask. No
systems could identify unconventional metonymies
correctly. Such poor performance is due to the non-
regularity of the reading by definition, so that ap-
proaches based on learning from similar examples
alone cannot work too well.
40
Table 3: Accuracy scores for all systems for all the location tasks.8
task ? / system ? baseline FUH UTD-HLT-CG XRCE-M GYDER up13
LOCATION-coarse 0.794 0.778 0.841 0.851 0.852 0.754
LOCATION-medium 0.794 0.772 0.840 0.848 0.848 0.750
LOCATION-fine 0.794 0.759 0.822 0.841 0.844 0.741
Table 4: Accuracy scores for all systems for all the organisation tasks
task ? / system ? baseline UTD-HLT-CG XRCE-M GYDER
ORGANISATION-coarse 0.618 0.739 0.732 0.767
ORGANISATION-medium 0.618 0.711 0.711 0.733
ORGANISATION-fine 0.618 0.711 0.700 0.728
6 Concluding Remarks
There is a wide range of opportunities for future fig-
urative language resolution tasks. In the SemEval
corpus the reading distribution mirrored the actual
distribution in the original corpus (BNC). Although
realistic, this led to little training data for several
phenomena. A future option, geared entirely to-
wards system improvement, would be to use a strat-
ified corpus, built with different acquisition strate-
gies like active learning or specialised search proce-
dures. There are also several options for expand-
ing the scope of the task, for example to a wider
range of semantic classes, from proper names to
common nouns, and from lexical samples to an all-
words task. In addition, our task currently covers
only metonymies and could be extended to other
kinds of figurative language.
Acknowledgements
We are very grateful to the BNC Consortium for let-
ting us use and distribute samples from the British
National Corpus, version 1.0.
References
J.A. Barnden, S.R. Glasbey, M.G. Lee, and A.M. Walling-
ton. 2003. Domain-transcending mappings in a system for
metaphorical reasoning. In Proc. of EACL-2003, 57-61.
J. Birke and A Sarkaar. 2006. A clustering approach for the
nearly unsupervised recognition of nonliteral language. In
Proc. of EACL-2006.
L. Burnard, 1995. Users? Reference Guide, British National
Corpus. BNC Consortium, Oxford, England.
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22:249-254.
D. Fass. 1997. Processing Metaphor and Metonymy. Ablex,
Stanford, CA.
S. Harabagiu. 1998. Deriving metonymic coercions from
WordNet. In Workshop on the Usage of WordNet in Natural
Language Processing Systems, COLING-ACL ?98, 142-148,
Montreal, Canada.
J.R. Hobbs, M.E. Stickel, D.E. Appelt, and P. Martin. 1993.
Interpretation as abduction. Artificial Intelligence, 63:69-
142.
S. Kamei and T. Wakao. 1992. Metonymy: Reassessment, sur-
vey of acceptability and its treatment in machine translation
systems. In Proc. of ACL-92, 309-311.
S. Krishnakamuran and X. Zhu. 2007. Hunting elusive
metaphors using lexical resources. In NAACL 2007 Work-
shop on Computational Approaches to Figurative Language.
G. Lakoff and M. Johnson. 1980. Metaphors We Live By.
Chicago University Press, Chicago, Ill.
J. Leveling and S. Hartrumpf. 2006. On metonymy recogni-
tion for gir. In Proceedings of GIR-2006: 3rd Workshop on
Geographical Information Retrieval.
K. Markert and U. Hahn. 2002. Understanding metonymies in
discourse. Artificial Intelligence, 135(1/2):145?198.
K. Markert and M. Nissim. 2002. Metonymy resolution as a
classification task. In Proc. of EMNLP-2002, 204-213.
K. Markert and M. Nissim. 2006. Metonymic proper names: A
corpus-based account. In A. Stefanowitsch, editor, Corpora
in Cognitive Linguistics. Vol. 1: Metaphor and Metonymy.
Mouton de Gruyter, 2006.
J. Martin. 1994. Metabank: a knowledge base of
metaphoric language conventions. Computational Intelli-
gence, 10(2):134-149.
Z. Mason. 2004. Cormet: A computational corpus-based con-
ventional metaphor extraction system. Computational Lin-
guistics, 30(1):23-44.
M. Nissim and K. Markert. 2003. Syntactic features and word
similarity for supervised metonymy resolution. In Proc. of
ACL-2003, 56-63.
G. Nunberg. 1995. Transfers of meaning. Journal of Seman-
tics, 12:109-132.
Y Peirsman. 2006. Example-based metonymy recognition for
proper nouns. In Student Session of EACL 2006.
D. Stallard. 1993. Two kinds of metonymy. In Proc. of ACL-
93, 87-94.
41
Proceedings of the 8th International Conference on Computational Semantics, pages 45?60,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Automatic identification of semantic relations
in Italian complex nominals
Fabio Celli
CLIC-CIMeC
University of Trento
fabio.celli@email.unitn.it
Malvina Nissim
Dipartimento di Studi Linguistici e Orientali
University of Bologna
malvina.nissim@unibo.it
Abstract
This paper addresses the problem of the identification of the seman-
tic relations in Italian complex nominals (CNs) of the type N+P+N.
We exploit the fact that the semantic relation, which is underspeci-
fied in most cases, is partially made explicit by the preposition. We
develop an annotation framework around five different semantic rela-
tions, which we use to create a corpus of 1700 Italian CNs, obtaining an
inter-annotator agreement of K=.695. Exploiting this data, for each
preposition p we train a classifier to assign one of the five semantic
relations to any CN of the type N+p+N, by using both string and
supersense features. To obtain supersenses, we experiment with a se-
quential tagger as well as a plain lookup in MultiWordNet, and find
that using information obtained from the former yields better results.
1 Introduction
Complex nominals are pervasive in language, and include noun-noun (N+N)
and adjective-noun (A+N) combinations (Levi, 1978), as in Ex. 1 and 2.
(1) dessert fork
(2) medieval historian
45
A ?dessert fork? is ?a fork for eating dessert?, and a ?medieval historian?
can be also described as ?a historian who studies medieval times?.
1
In
both cases the relation is not overtly marked. Indeed, syntactically, there is
nothing that tells us that the semantic relation between ?dessert? and ?fork?
in Ex. 1 is different than the one binding ?plastic? and ?fork? in Ex. 3.
(3) plastic fork
However, it is well known that whereas English composes CNs of the type
N+N, Romance languages must glue the two nouns by means of a prepo-
sition, thus yielding CNs of the form N+P+N, thereby partially making
explicit the underlying semantic relation (Busa and Johnston, 1996). So, in
Ex. 4, the ?purpose? relation between dessert and fork is (partially) made
explicit by the preposition ?da?. In contrast, the ?property? relation binding
plastic and fork (a fork made of plastic) is expressed using ?di? (Ex. 5).
(4) forchetta da dessert (en: dessert fork)
(5) forchetta di plastica (en: plastic fork)
Recently, Girju (2007) has exploited this observation including cross-language
information in a system for the automatic interpretation of NN compounds
in English. However, whereas it is true that the overt preposition restricts
the set of possible relations, it is also true that prepositions are still se-
mantically ambiguous, since there is no one-to-one correspondence between
prepositions and relations. So, ?di?, used in a ?property? relation above,
can also express a ?part-whole? (Ex. 6), a ?theme? (Ex. 7), and several other
relations.
(6) dorso della mano (the back of the hand)
(7) suonatore di chitarra (guitar player)
In this work, we also exploit the presence of a preposition in Italian CNs as
an aid to detect the semantic relation. We extract and annotate CNs in a
corpus of written Italian, and develop a supervised system for determining
the semantics of the CN, comparing the contribution of plain nouns with
that of hypernym classes, and different ways in which such hypernyms can be
obtained. In the next section, we discuss previous work on the semantics of
complex nominals. In Section 3, we define a set of five semantic relations for
the annotation of Italian CNs and the details of the annotation framework,
and discuss the corpus distribution. In Section 4 we describe the experiments
for the automatic identification of semantic relations, and discuss the results.
We conclude with ideas for future work in Section 5.
1
In this work we will only consider N+N CNs, thereby excluding A+N CNs.
46
2 Previous work
Given their underspecified nature, CNs, especially in English, have received
a large amount of attention in the linguistic and computational linguistic
literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; John-
ston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007,
among others). Current interest in NLP is also shown in the organisation of
a SemEval task especially dedicated to noun-noun compound interpretation
(Task 4, (Girju et al, 2007)). Indeed, NLP systems which aim at full text
understanding for higher NLP tasks, such as question answering, recognis-
ing textual entailment and machine translation, need to grasp the semantic
relation which noun compounds mostly leave underspecified.
One main issue in noun-noun compound interpretation is the lack of
general agreement on a well-defined set of semantic relations. Nastase and
Szpakowicz (2003), for instance, propose a two-level taxonomy, in which
fifteen fine-grained relations are subsumed into five general classes (causal,
participant, spatial, temporal, quality). An example of a causal relation
(with subtype ?purpose?) is ?concert hall?, and an example of a participant
relation (with subtype ?beneficiary?) is ?student discount?.
Girju et al (2007) propose the smaller set reported in Table 1, which
was tested on English N+N complex nominals within the SemEval 2007 task.
They specifically spell out semantic relations as two-poles relationships: for
example an effect is an effect always with respect to a cause.
Table 1: The set of 7 semantic relations from Girju et al (2007)
Semantic relation Examples
Cause-Effect laugh (cause) wrinkles (effect)
Instrument-Agency laser (instrument) printer (agency)
Product-Producer honey (product) bee (producer)
Origin-Entity message (entity) from outer-space (origin)
Theme-Tool news (theme) conference(tool)
Part-Whole the door (part) of the car (whole)
Content-Container apples (content) in the basket (container)
As far as relation detection is concerned, Johnston and Busa (1996),
working specifically on Italian, have suggested using information included
in qualia structures (Pustejovsky, 1995) for deriving the compound?s inter-
pretation. The use of qualia structures for this task is appropriate and
semantically sound but absolutely not straightforward to implement, since
there does not exist an electronic repository of qualias, so that the structures
47
would need to be constructed by hand, thereby involving a large amount of
manual work. Recent work has shown that the automatic acquisition of
qualias can be performed with reasonable success exploiting information
obtained using lexico-syntactic patterns over the Web (Cimiano and Wen-
deroth, 2005). For our purposes, though, if lexico-syntactic patterns can be
used successfully to induce qualia roles, we could directly use the informa-
tion we obtain from them, thus bypassing the qualia structure representa-
tion. We plan to include features based on such kinds of patterns in future
development of this work (see also (Nakov and Hearst, 2008)).
More purely computational approaches include both supervised (Lauer,
1995) as well as unsupervised models, such as (Lapata and Keller, 2005),
who use frequencies obtained over the Web. Some researchers also suggest
solutions to the data sparsness problem, which affects our approach as well,
by using lexical similarity (Turney, 2006) or clustering techniques (Pantel
and Pennacchiotti, 2006).
Finally, there exists specific work on compound nouns whose head is
derived from a verb (Lapata, 2002), and information about verbs deverbal
nouns are linked to has proved a useful feature in previous approaches (Girju,
2007). Whereas we have exploited this information in the annotation phase,
we have not included corresponding features yet in the statistical model we
use, but we plan to do so in future extensions.
3 Annotation Framework and Data
For developing an annotation framework, we built on Italian grammars,
existing classifications (see Section 2), and a preliminary study of corpus
data.
3.1 Annotation framework
In determining the set of relations to be annotated, following (Girju, 2007),
we also define two-pole relations between the involved nominals.
We assume that relations can be extracted and subsumed in general
classes starting from ?-roles, which are partially made explicit by the prepo-
sitional phrase. Since there is no general agreement on a complete list of
?-roles we chose to work with types of complements, which are provided by
traditional Italian grammars and can be found in almost every Italian dic-
tionary. In (Zingarelli, 2008), we found 33 different types of prepositional
phrases (PPs), which we grouped into 21 classes (for instance, all of the
48
location-related PPs were grouped under a single LOC class). This infor-
mation was included in the annotation scheme (Celli, 2008), although is not
used in the current relation identification model.
Following (Langacker, 1987), the nouns within each CNs were also re-
visited within a trajector (Tr) and landmark (Lm) approach mirroring the
two-pole interpretation of the semantic relations.
The set of five semantic relations we arrived at is given in Table 2. These
five relations are the target of our classification experiments (Section 4).
Table 2: Relations for Italian prepositions.
Relation(Tag) Description Examples
cause-effect (CE) tr. causes lm. death
Lm
by privations
Tr
located-location (LL) lm. localizes tr. window
Lm
passage
Tr
owner-property (OP) tr. possess lm. stone
Lm
statue
Tr
included-set (IS) lm. includes tr. thousands
Tr
of men
Lm
bound-bounded (RR) lm. undergoes tr. city
Lm
destruction
Tr
In the cause-effect (CE) relation the trajector is the cause or the agent
and the landmark is the product or the effect produced by the agent/causer,
as in ?morte per stenti? (en: death by privations). In located-location (LL),
a trajector is located in space or time with respect to a landmark, as ?casa
in montagna? (en: mountain house). The owner-property (OP) relation as-
sociates a trajector (owner) with its property, part, or characteristic, which
is the landmark. Examples are ?statua di pietra? (en: stone statue) and
?cane da caccia? (en: hunting dog). In included-set (IS) the trajector is the
included object and the landmark is the set: in ?migliaia di uomini? (en:
thousands of men), ?migliaia? (en: thousands) is the subset and ?uomini?
(en: men) is the set. The bound-bounded (RR) relation is a direct rela-
tionship between an event, usually a deverbal (trajector), and its undergoer
(landmark), as ?distruzione della citta`? (en: destruction of the city). Clas-
sic relations such as part-whole, producer-product, and is-a are covered in
this account by the owner-property, cause-effect and included-set relations,
respectively.
Annotation categories Each extracted CN (see Section 3.2) was anno-
tated with the following information:
? the lemma (A, CON, DI, DA, IN, PER, SU, TRA)
2
2
The preposition ?tra? can also be written as ?fra?. They are semantically equivalent.
Occurrences of both variants were extracted, but we refer to them always as ?tra?.
49
? the relation (CE, OP, LL, IS, RR)
? the type of prepositional phrase (21 tags)
? the semantic type of n1/n2 (natural, abstract, artifact, metaphorical usage)
? the position of trajector and landmark in the CN (TL, LT)
? the order of the head and the modifier in the CN (HM, MH)
The following CN types were to be excluded from annotation:
? CNs including proper nouns, such as ?problema di Marco? (en: Mark?s prob-
lem);
? CNs involving complex prepositions, such as ?hotel nel mezzo del deserto?
(en: hotel in the middle of the desert);
? CNs involving n1 and/or n2 of categories other than noun, due to POS-
tagging errors;
? CNs containing bisyllabic prepositions, such as ?macchina senza benzina?
(en: car without fuel);
3
? CNs used as adverbs, e.g. ?accordo di massima? (en: generally agreed with)
3.2 Data
Corpus Selection We used CORISsmall, a reduced version of CORIS,
a 100M-word, balanced corpus of written Italian (Rossini Favretti, 2000).
CORISsmall was sampled by randomly extracting sentences with a length
between 2 and 40 words. We discarded a few domain-specific subcorpora
which were likely to contain prepositions used in ways different from common
usage, as the legal subcorpus. The resulting corpus, henceforth CORISnom-
inals, contains 75,000 words. The corpus was then automatically tagged
with part-of-speech information, using TreeTagger (Schmid, 1994).
CN detection We chose to annotate monosyllabic prepositions only, namely
a (to), con (with), di (of), da (from), in (in),per (for), su (on) and tra
(within), because they are more frequent in CNs, more polysemous and
not occurring as any other grammatical category, differently from bisyllabic
prepositions which can be used adverbially. In any case, bisyllabic preposi-
tions occurr in less than 2% of all the extracted CNs (42 out of 2298).
Exploiting part-of-speech information, we extracted all the N+P+N com-
binations with a context window of 10 words left and right. The frequency
of the CNs found in CORISnominals is reported in Table 3.
3
Prepositions which incorporate the determiner, such as ?della? (di+la, en: of the)
or ?sulla? (su+la, en: on the), although possible bisyllabic, are definite variants of their
corresponding monosyllabic prepositions, and are therefore included in the dataset.
50
Table 3: Frequency of CN types in CORISnominals
CNs extracted #inst example
N+P+N 1125 lampada a olio (oil lamp)
N+Pdet+N 1044 dorso della mano (back of the hand)
N+P+D+N 129 casa per le vacanze (holiday home)
total 2298
Annotation procedure and evaluation The annotation was performed
by a native speaker of Italian, with experience in the semantic analysis of
complex nominals. After discarding some CNs according to the rules defined
in the annotation scheme, the final number of annotated instances is 1700.
In order to assess the difficulty of the relation assignment task, a randomly
extracted portion of the data (186 CNs) was further annotated by a second
native speaker of Italian. The second annotator marked them up following
specific guidelines and some training material composed of about 50 already
annotated CNs as examples. We calculated inter-annotator agreement using
Cohen?s kappa statistics (Cohen, 1960), obtaining a kappa of .695. While
this relatively not so high value can be considered satisfactory in the field
of semantic annotation (this score is also in the same ballpark as the 70.3%
agreement reported for the SemeEval Task 4 annotation (Girju et al, 2007)),
it still indicates that the phenomenon involves a good amount of ambiguity
thus making the classification task far from straightforward. Table 4 reports
the confusion matrix for the annotated subset.
Table 4: Confusion matrix for annotator A and annotator B
A/B CE IS LL OP RR total
CE 2 ? ? ? ? 2
IS 1 22 4 5 1 33
LL ? ? 12 4 1 17
OP 34 4 8 44 6 96
RR 5 ? 1 3 29 38
total 42 26 25 56 37 186
The largest area of disagreement is in the opposition between CE and
OP: annotator B assigned the type CE to a large number of CNs which
annotator A had marked as OP. This might be due to the fact that CE
relations can be triggered by parts of objects (or features of concepts), which
are expressed by the OP relation. A prime example of such overlap is ?fumo
51
di sigaretta? (en: cigarette smoke), which can be seen both as a cause-effect
relation as well as a owner-property relation. Thus, future work will involve
a reassessment of these two categories and a revision of the guidelines.
Corpus Distribution Table 5 illustrates the distribution of semantic re-
lations across each preposition.
Table 5: Distribution of relations across prepositions in CORISnominals
prep/rel CE IS LL OP RR total
a 0 8 29 34 28 99
con 0 5 0 10 14 29
di 62 262 69 646 289 1328
da 2 0 7 18 8 35
in 0 5 50 31 14 100
per 8 2 2 29 7 48
su 3 0 18 12 11 44
tra 0 0 4 3 10 17
total 75 282 179 783 381 1700
The most striking figure is the overwhelming predominance of ?di?,
which features in 78% of all CNs. This is in line with the extremely high
overall frequency of ?di? in Italian, which is ranked as the most frequent
word in CoLFIS (an Italian frequency lexicon based on a 3M word corpus,
Laudanna et al (1995)), and also with Girju?s 2007 observation that 77.7%
of the English noun-noun compounds in her data can be rephrased as ?of?
phrases. We can also observe that some prepositions, namely ?a? and ?con?,
show more than one predominant relation usage in CNs. Overall, OP is by
far the most frequent relation, occurring in nearly half of the CNs.
As an additional observation, for each preposition we compared its fre-
quency of occurrence in CNs and in any other constructions. We found that
while ?di? and ?su? are particularly CN-oriented prepositions, both with
over 55% of their occurrences being in CNs, the others appear in CNs about
10% or less of their total occurrences.
4 Automatic identification of CN relations
We can see the problem of semantic relations in CNs from at least two (con-
verging) points of view. From a more language understanding side, given a
CN (two nouns connected by a preposition), we might want to know what the
52
Table 6: Accuracy for most frequent relation baseline and for basic system
prep #inst most freq rel baseline basic system
a 99 OP (34) 34.34 47.47
con 29 RR (14) 48.28 48.28
da 35 OP (18) 51.43 51.43
di 1328 OP (646) 48.64 56.40
in 100 LL (50) 50.00 52.00
per 48 OP (29) 60.42 60.42
tra 17 RR (10) 58.82 58.82
su 44 LL (18) 40.91 50.00
underlying semantic relation is. From a more language generation perspec-
tive, though, we might want to be able to select the appropriate preposition,
given two nouns and a relation between the concepts they express.
This translates into two different classification tasks. One where the
target categories are relations, the other where they are prepositions. In
the work we describe in this paper we concentrate on the first task. For
each preposition we build a supervised model where the target categories
correspond to the annotation tags for the semantic relations: CE, IS, LL,
OP, RR. As evaluation measures, we report accuracy and coverage. Coverage
amounts to the portion of data for which supersenses could be found for both
n1 and n2, thus providing insights in assessing the contribution of different
supersense assignment methods (see Section 4.2 and Section 4.3).
For assessing the difficulty of the task, beside inter-annotator agreement,
we take a simple baseline where we assign to each CN the semantic relation
which is most frequently associated with the CN?s preposition (Table 6).
In the learning experiments, we use the Weka implementation (Wit-
ten and Frank, 2000) of the sequential minimal optimization algorithm for
training a support vector classifier, within a ten-fold cross-validation setting.
Girju (2007) has shown SVMs to be most efficient for this task.
4.1 Basic system
The basic system uses as features only n1 and n2 as simple strings. Table 6
shows accuracy per preposition for the basic system and for the baseline.
The most evident limitation of this basic approach is data sparseness.
Out of 1700 CNs, 1662 involve a combination of n1 and n2 which occurs only
once, independently of the preposition used. The most frequent n1 (?parte?,
part) occurs 13 times with two different prepositions, and the most frequent
53
n2 (?lavoro?, job/work) 16 across four different prepositions.
One intuitive way to alleviate the data sparseness problem without in-
creasing the corpus size, is to cluster instances. Following Girju (2007), who
uses hypernyms obtained from WordNet (Fellbaum, 1998) in place of strings,
we reduce each noun in our data set to its hypernym. In this supersense
assignment, we experimented with two procedures: a more sophisticated one
involving sequential sense tagging, thus dealing with sense disambiguation,
and a simpler one involving plain assignment of hypernyms.
4.2 Hypernym selection via sense tagging
Two major problems related to finding a hypernym for a word are sense
ambiguity (one term can easily have more than one hypernym if it has
more than one sense) and coverage (even large ontologies/databases might
not include some of the encountered terms). A supersense tagger alleviates
such limitations by tagging words in context, thus tackling the ambiguity
issue, and by using a combination of features rather than just the lexical
entry, thereby being able to classify also words that are not included in the
dictionary. Picca et al (2008) have developed such a tagger for Italian,
building on an existing version for English (Ciaramita and Altun, 2006),
retrained on MultiSemCor (Bentivogli and Pianta, 2005), a word-aligned
English-Italian corpus which contains the translation of the English texts
in SemCor. The set of 26 noun supersense labels come from MultiWordNet
(Pianta et al, 2002), a multilingual lexical database in which the Italian
WordNet is strictly aligned with Princeton WordNet 1.6, and which is linked
to MultiSemCor.
The average reported performance of the tagger is about 60% (Picca
et al, 2008). This relatively low accuracy introduces a large portion of errors
in the classification, thus reducing the advantage of dealing with supersenses
rather than words in the identification of semantic relations in CNs. Errors
can be of three types: (i) the assignment of a wrong noun class, (ii) the
assignment of a class of the wrong part-of-speech type (any non-noun tag),
and (iii) the non-assignment of any class (tag ?0?). Whereas errors of type
(i) can only be spotted via manual investigation, mistakes of type (ii) and
(iii) can be detected automatically and a backoff strategy can be deployed.
In 228 CNs out of 1700 both nouns have been assigned a ?0? tag. In a
further 751 CNs, one of the two nouns is tagged as ?0?. Out of these, there
are 33 cases where the other noun is assigned a non-noun tag (adj or verb).
A non-noun tag for n1 or n2 is also found in a further 57 cases.
As a backoff strategy for all cases that fall under (ii) and (iii), we searched
54
Table 7: Results using supersenses obtained via tagging, in combination
with string features, and alone, and with and without backoff.
no backoff backoff
prep #inst cov%
acc%
#inst cov%
acc%
string nostring string nostring
a 32 32.32 68.75 75.00 99 100 45.46 44.44
con 11 37.93 72.73 63.64 29 100 62.07 58.62
da 14 40.00 64.29 57.14 35 100 65.71 65.71
di 526 39.61 58.55 51.71 1328 100 59.71 50.75
in 36 36.00 63.89 61.11 100 100 64.00 62.00
per 16 33.33 68.75 56.25 48 100 56.25 54.17
tra 10 58.82 70.00 70.00 17 100 64.71 64.71
su 20 45.45 45.00 50.00 44 100 54.54 47.73
hypernyms directly in MultiWordNet (MWN). (The set of possible hyper-
nyms is identical to the set of the 26 supersenses used by the tagger.) As a
first step, we lemmatised the string using Morph-it!, an existing lemmatiser
for Italian (Zanchetta and Baroni, 2005), since MWN contains lemmata but
not their morphological variants. Whenever we found more than one synset
associated to a term, a corresponding number of hypernyms was also found.
If one of the hypernyms was recurring more than the others, this was se-
lected. Otherwise, the hypernym associated to the first sense was selected.
4
Whenever the lemmatised noun was not in MWN (106 cases), we assigned
the most frequent supersense in the dataset (?act? for both n1 and n2).
We then ran classification experiments using the obtained supersenses
for n1 and n2 as additional features, as well as on their own (thus ignoring
the original string?this is reported as ?nostring? in Tables 7?8), both with
and without the backoff strategy. In the latter case, we excluded all CNs
where at least one of the two nouns had been tagged as a non-noun or had no
supersense assignment. Under these settings coverage was seriously affected,
but accuracy was generally higher than when deploying the backoff strategy.
Table 7 reports results.
4
Optimally, we would select the hypernym for the most frequent sense (the one ranked
first in Princeton WordNet). However, synsets for a given term are not ordered by fre-
quency in MWN. One option would be to exploit frequencies from MultiSemCor, but the
corpus is rather small and might not be very reliable.
55
Table 8: Results using supersenses obtained via plain assignment, in com-
bination with string features, and alone, and with and without backoff.
no backoff backoff
prep #inst cov%
acc%
#inst cov%
acc%
string nostring string nostring
a 90 90.91 47.78 42.22 99 100 39.39 34.34
con 26 89.65 61.54 57.69 29 100 55.17 55.17
da 30 85.71 60.00 63.33 35 100 62.86 65.71
di 1178 88.70 61.88 52.63 1328 100 60.54 51.13
in 88 88.00 50.00 52.27 100 100 56.00 53.00
per 41 85.42 65.85 60.98 48 100 56.25 47.92
tra 14 82.35 42.86 42.86 17 100 47.06 35.29
su 36 81.82 52.78 52.78 44 100 59.09 40.91
4.3 Hypernym selection via plain assignment
Given the large number of cases where we had to resort to a backoff strategy
on the tagger?s output, we tried to obtain hypernyms from MWN directly,
thus bypassing the tagging stage. Whenever necessary, we employed the
backoff strategies described above: most frequent hypernym found for an
ambiguous term (or first sense?s hypernym in case of equal frequency), and
overall most frequent assigned hypernym in the corpus (?act? in this case as
well) for all those nouns that were not found in MWN. This direct lookup
approach should improve on coverage but suffer more from ambiguity-related
problems. Table 8 summarises the results.
4.4 Discussion
Under the best settings, at full coverage, our average performance is around
59% (using tagger-assigned supersenses, backoff, the string feature), with
wide variation across prepositions. Given the currently limited set of fea-
tures, results are in general promising, especially if compared to the inter-
annotator agreement, and to previous work (see below).
When using supersenses obtained from the tagger, results are steadily
better than when using hypernyms directly looked up in MWN (both with
and without backoff) with the exception of ?di? and ?su?. The low coverage
but higher accuracy yielded when using the tagger?s senses without resorting
to a backoff strategy were both expected, as mentioned above.
Results suggest that the utility of a backoff strategy varies from one
56
preposition to another. For instance, for ?a?, ?con?, and ?per?, backoff
appears to lower performance, independently on how the supersenses were
obtained. These three prepositions had the three lowest coverage scores
when using the tagger, which suggests that if too large a proportion is left
to the approximation of backoff, the benefits of accurate sense tagging are
lost. This is not however true for the MWN lookup, where the coverage for
these three prepositions is rather high.
Additionally, we can observe that in most cases, in the back-off settings,
including the string as a feature helps improve the performance (both in the
tagging and in the plain assignment). This is likely due to the fact that the
approximation given by not having precise information about the supersense
and needing to resort to a backoff strategy is (partially) compensated by
taking into account the original noun. In contrast, using the string without
the backoff strategy on the tagger?s output yields a decrease in performance,
proving supersenses useful.
For a better assessment of the actual contribution of using hypernyms
for detecting the semantic relation without incurring in the noise introduced
by wrong hypernym assignments or the backoff strategy, we manually cor-
rected the tagger?s output in 60% of the data. This allowed us to evaluate
the tagger?s performance on supersense assignment for this 60% portion as
well as to compare on this subset, contaning 1024 CNs, an algorithm us-
ing ?gold? supersenses with that built on the tagger?s output (using the
backoff strategy, and including string features, see Section 4.2). We found
that supersenses were assigned by the tagger with an accuracy of 63.9%,
a result in line with previously reported performance (Picca et al, 2008).
We also observed that using the manually assigned hypernyms yielded an
average improvement of about seven percentage points over using the tag-
ger?s senses, although for some prepositions, instances in this smaller dataset
were just too few to draw any solid conclusion. Although more accurate,
the gold tags do not boost the performance as much as one might expect.
On the one hand, this might suggest that hypernyms can contribute only
to a certain extent to this task, and other more expressive features must be
found. On the other, it is also possible that the chosen set of 26 supersenses
is too large, especially for a dataset like ours which is rather small, thereby
not really overcoming the data sparseness problem.
Comparison to previous work in terms of performance is not straight-
forward, because of the language difference, the relation sets used, and the
evaluation settings. In the SemEval-2007 exercise, for example, for each
of the seven semantic relations used (see Table 1), a system must decide
whether a given instance expresses that relation or not within an ad hoc-
57
built dataset, so that the overall semantic relation identification of the task
is actually split in seven different binary classification tasks, one per relation.
The highest reported average accuracy is 76.3% (Girju et al, 2007).
Girju (2007) classifies noun-noun compounds in 22 different semantic
relations. Best results on English are obtained when using a rich feature
set including cross-linguistic information. Reported figures differ slightly ac-
cording to the dataset used, with an average accuracy of 76.1%. When using
only language-internal supersense features, the average accuracy is 44.15%.
Girju (2007) also trains and tests another state-of-the-art supervised model
for English, namely Semantic Scattering (Moldovan and Badulescu, 2005),
reporting an average accuracy of 59.07%.
5 Conclusions and future work
We have presented a framework for the annotation of Italian complex nom-
inals in a very high data sparseness condition, and supervised models for
the identification of the underlying semantic relation for monosyllabic Ital-
ian prepositions. We exploited both string and supersense features, showing
that the importance of including string information varies from one prepo-
sition to another and from whether we are using backoff strategies or not.
We have also seen that for obtaining the supersenses, a sequential sense tag-
ging approach yields better overall results than a simple lookup in MWN,
although it dramatically cuts on coverage.
Future work will involve further classification experiments with addi-
tional features, including web counts obtained via lexico-syntactic patterns
(Lapata and Keller, 2005; Nakov and Hearst, 2008). We will exploit part of
the annotation which we have not considered in this study (see Section 3),
namely the type of prepositional phrase (see Appendix), a very general con-
ceptual clustering which also marks metaphorical usage, the position of tra-
jector and landmark in the CN, and the order of the head and the modifier.
References
Bentivogli, L. and E. Pianta (2005). Exploiting parallel texts in the cre-
ation of multilingual semantically annotated resources: the MultiSemCor
Corpus. Natural Language Engineering 11 (3), 247?261.
Busa, F. and M. Johnston (1996). Cross-linguistic semantics for complex
nominals in the generative lexicon. In AISB Workshop on Multilinguality
in the Lexicon.
58
Celli, F. (2008). La semantica delle preposizioni italiane nella combinazione
concettuale. Master thesis in Linguistics, Universita` di Bologna.
Ciaramita, M. and Y. Altun (2006). Broad-coverage sense disambiguation
and information extraction with a supersense sequence tagger. In Pro-
ceedings of EMNLP 2006, pp. 594?602.
Cimiano, P. and J. Wenderoth (2005). Automatically learning qualia struc-
tures from the web. In Proceedings of the ACL-SIGLEX Workshop on
Deep Lexical Acquisition, Ann Arbor, Michigan, pp. 28?37.
Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational
and Psychological Measurement 20, 37?46.
Downing, P. (1977). On the creation and use of English compound nouns.
Language 53, 810?842.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database. Cam-
bridge: MIT Press.
Girju, R. (2007). Improving the interpretation of noun phrases with cross-
linguistic information. In Proceedings of ACL?07, pp. 568?575.
Girju, R., P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret
(2007, June). SemEval-2007 Task 04: Classification of Semantic Relations
between Nominals. In Proceedings of SemEval-2007, pp. 13?18.
Johnston, M. and F. Busa (1996). Qualia structure and the compositional
interpretation of compounds. In Proceedings of the ACL Workshop on
breadth and depth of semantic lexicons.
Langacker, R.W. (1987). Foundations of cognitive grammar. Univ. Press.
Lapata, M. (2002). The disambiguation of nominalisations. Computational
Linguistics 28 (3), 357?388.
Lapata, M. and F. Keller (2005). Web-based models for natural language
processing. ACM Transactions on Speech and Language Processing 2.
Laudanna, A., A. Thornton, G. Brown, C. Burani, and L. Marconi (1995).
Un corpus dell?italiano scritto contemporaneo dalla parte del ricevente. In
S. Bolasco, L. Lebart, and A. Salem (Eds.), III Giornate internazionali
di Analisi Statistica dei Dati Testuali. Volume I, pp. 103?109. Cisu.
Lauer, M. (1995). Corpus statistics meet the noun compound: some empir-
ical results. In Proceedings of ACL?95.
Levi, J. (1978). The Syntax and Semantics of Complex Nominals. Academic
Press.
Moldovan, D. and A. Badulescu (2005). A semantic scattering model for
the automatic interpretation of genitives. In Proceedings of HLT-EMNLP
2005, pp. 891?898.
59
Nakov, P. and M. A. Hearst (2008). Solving relational similarity problems
using the web as a corpus. In Proceedings of ACL-08: HLT, Columbus,
Ohio, pp. 452?460. Association for Computational Linguistics.
Nastase, V. and S. Szpakowicz (2003). Exploring noun-modifier semantic
relations. In Proceedings of IWCS-5, pp. 285?301.
Pantel, P. and M. Pennacchiotti (2006). Espresso: Leveraging generic pat-
terns for automatically harvesting semantic relations. In Proceedings of
ACL?06, Sydney, Australia, pp. 113?120.
Pianta, E., L. Bentivogli, and C. Girardi (2002). MultiWordNet: developing
an aligned multilingual database. In Proceedings of the First International
Conference on Global WordNet, pp. 293?302.
Picca, D., A. M. Gliozzo, and M. Ciaramita (2008). Supersense Tagger for
Italian. In Proceedings of LREC 2008.
Pustejovsky, J. (1995). The Generative Lexicon. The MIT Press.
Rosario, B. and M. Hearst (2001). Classifying the semantic relations in
noun compounds via a domain-specific lexical hierarchy. In L. Lee and
D. Harman (Eds.), Proceedings of EMNLP 2001, pp. 82?90.
Rossini Favretti, R. (2000). Progettazione e costruzione di un corpus di ital-
iano scritto: CORIS/CODIS. In R. Rossini Favretti (Ed.), Linguistica e
informatica. Multimedialita`, corpora e percorsi di apprendimento. Bulzoni.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees.
In Proc. of the Conference on New Methods in Language Processing, 44-49.
Turney, P. D. (2006). Expressing implicit semantic relations without super-
vision. In Proceedings of ACL?06, Sydney, Australia, pp. 313?320.
Warren, B. (1978). Semantic patterns of noun-noun compounds. Gothenburg
Studies in English 41.
Witten, I. H. and E. Frank (2000). Data Mining: Practical Machine Learning
Tools and Techniques with Java Implementations. Morgan Kaufmann.
Zanchetta, E. and M. Baroni (2005). Morph-it! a free corpus-based morpho-
logical resource for the italian language. Corpus Linguistics 2005 1 (1).
Zingarelli, N. (2008). Lo Zingarelli 2008. Vocabolario della Lingua Italiana.
Zanichelli.
60
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 9?17,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Empirical Approach to the Interpretation of Superlatives
Johan Bos
Laboratory for Computational Linguistics
Department of Computer Science
University of Rome ?La Sapienza?
bos@di.uniroma1.it
Malvina Nissim
Laboratory for Applied Ontology
Institute for Cognitive Science and Technology
National Research Council (CNR), Rome
malvina.nissim@loa-cnr.it
Abstract
In this paper we introduce an empirical
approach to the semantic interpretation of
superlative adjectives. We present a cor-
pus annotated for superlatives and pro-
pose an interpretation algorithm that uses
a wide-coverage parser and produces se-
mantic representations. We achieve F-
scores between 0.84 and 0.91 for detecting
attributive superlatives and an accuracy in
the range of 0.69?0.84 for determining the
correct comparison set. As far as we are
aware, this is the first automated approach
to superlatives for open-domain texts and
questions.
1 Introduction
Although superlative noun phrases (the nation?s
largest milk producer, the most complex arms-
control talks ever attempted, etc.) received consid-
erable attention in formal linguistics (Szabolcsi,
1986; Gawron, 1995; Heim, 1999; Farkas and
Kiss, 2000), this interest is not mirrored in com-
putational linguistics and NLP. On the one hand,
this seems remarkable, since superlatives are fairly
frequently found in natural language. On the other
hand, this is probably not that surprising, given
that their semantic complexity requires deep lin-
guistic analysis that most wide-coverage NLP sys-
tems do not provide.
But even if NLP systems incorporated linguistic
insights for the automatic processing of superla-
tives, it might not be of help: the formal semantics
literature on superlatives focuses on linguistically
challenging examples (many of them artificially
constructed) which might however rarely occur in
real data and would therefore have little impact
on the performance of NLP systems. Indeed, no
corpus-based studies have been conducted to get a
comprehensive picture of the variety of configura-
tions superlatives exhibit, and their distribution in
real occurring data.
In this paper we describe our work on the anal-
ysis of superlative adjectives, which is empiri-
cally grounded and is implemented into an exist-
ing wide-coverage text understanding system. To
get an overview of the behaviour of superlatives
in text, we annotated newswire data, as well as
queries obtained from search engines logs. On
the basis of this corpus study, we propose, imple-
ment and evaluate a syntactic and semantic analy-
sis for superlatives. To the best of our knowledge,
this is the first automated approach to the interpre-
tation of superlatives for open-domain texts that
is grounded on actual corpus-evidence and thor-
oughly evaluated. Some obvious applications that
would benefit from this work are question answer-
ing, recognition of entailment, and more generally
relation extraction systems.
2 Syntax and Semantics of Superlatives
2.1 Surface Forms
In English, superlative adjectives appear in a large
variety of syntactic and morphological forms.
One-syllable adjectives and some two-syllable ad-
jectives are directly inflected with the suffix ?-est?.
Some words of two syllables and all words of three
or more syllables are instead introduced by ?most?
(or ?least?). Superlatives can be modified by ordi-
nals, cardinals or adverbs, such as intensifiers or
modals, and are normally preceeded by the defi-
nite article or a possessive. The examples below
illustrate the wide variety and uses of superlative
adjectives.
9
the tallest woman
AS Roma?s quickest player
the Big Board?s most respected floor traders
France?s third-largest chemical group
the most-recent wave of friendly takeovers
the two largest competitors
the the southern-most tip of England
its lowest possible prices
Superlative adjectives can manifest themselves
in predicative (?Mia is the tallest.?) or attributive
form (?the tallest woman?). Furthermore, there
are superlative adverbs, such as ?most recently?,
and idiomatic usages.
2.2 The Comparison Set
It is well known that superlatives can be analysed
in terms of comparative constructions (Szabolcsi,
1986; Alshawi, 1992; Gawron, 1995; Heim, 1999;
Farkas and Kiss, 2000). Accordingly, ?the oldest
character? can be interpreted as the character such
that there is no older character, in the given con-
text. Therefore, a correct semantic interpretation
of the superlative depends on the correct charac-
terisation of the comparison set. The comparison
set denotes the set of entities that are compared to
each other with respect to a certain dimension (see
Section 2.3). In ?the oldest character in the book?,
the members of the comparison set are characters
in the book, and the dimension of comparison is
age.
The computation of the comparison set is com-
plicated by complex syntactic structure involving
the superlative. The presence of possessives for
example, as in ?AS Roma?s quickest player?, ex-
tends the comparison set to players of AS Roma.
Prepositional phrases (PPs), gerunds, and relative
clauses introduce additional complexity. PPs that
are attached to the head noun of the superlative are
part of the comparison set ? those that modify
the entire NP are not. Similarly, restrictive rel-
ative clause are included in the comparison set,
non-restrictive aren?t.
We illustrate this complexity in the following
examples, taken from the Wall Street Journal,
where the comparison set is underlined:
The oldest designer got to work on the dash-
board, she recalls. (WSJ02)
A spokesman for Borden Inc., the nation?s
largestmilk producer, concedes Goya may be on
to something. (WSJ02)
Right now, the largest loan the FHA can
insure in high-cost housing markets is $101,250.
(WSJ03)
With newspapers being the largest single
component of solid waste in our landfills ...
(WSJ02)
... questions being raised by what gen-
erally are considered the most complex
arms-control talks ever attempted. (WSJ02)
Besides syntactic ambiguities, the determina-
tion of the comparison set can be further compli-
cated by semantic ambiguities. Some occurrences
of superlatives licence a so-called ?comparitive?
reading, as in the following example discussed in
the formal semantics literature (Heim, 1999; Sz-
abolcsi, 1986):
John climbed the highest mountain.
Here, in the standard interpretion, the moun-
tain referred to is the highest available in the con-
text. However, another interpretation might arise
in a situation where several people climbed several
mountains, and John climbed a mountain higher
than anyone else did, but not necessarily the high-
est of all mountains in the context. Our corpus
study reveals that these readings are rare, although
they tend to be more frequent in questions than in
newspaper texts.
2.3 Dimension
Part of the task of semantically interpretating su-
perlative adjectives is the selection of the dimen-
sion on which entities are compared. In ?the
highest mountain? we compare mountains with re-
spect to the dimension height, in ?the best paper?
we compare papers with respect to the dimension
quality, and so on. A well-known problem is that
some adjectives can be ambiguous or vague in
choosing their dimension. Detecting the appropri-
ate dimension is not covered in this paper, but is
orthogonal to the analysis we provide.
2.4 Superlatives and Entailment
Superlatives exhibit a non-trivial semantics. Some
examples of textual entailment make this very ev-
ident. Consider the contrasts in the following en-
tailment tests with indefinite and universally quan-
tified noun phrases:
I bought a blue car |= I bought a car
I bought a car 6|= I bought a blue car
I bought every blue car 6|= I bought every car
I bought every car |= I bought every blue car
10
Observe that the directions of entailments are
mirrorred. Now consider a similar test with su-
perlatives, where the entailments fail in both di-
rections:
I bought the cheapest blue car 6|= I bought the cheapest car
I bought the cheapest car 6|= I bought the cheapest blue car.
These entailment tests underline the point that
the meaning of superlatives is rather complicated,
and that a shallow semantic representation, say
?x.[cheapest(x) ? car(x)] for ?cheapest car?, sim-
ply won?t suffice. A semantic represention captur-
ing the meaning of a superlative requires a more
sophisticated analysis. In particular, it is impor-
tant to explicitly represent the comparison set of
a superlative. In ?the cheapest car?, the compar-
ison set is formed by the set of cars, whereas in
?the cheapest blue car?, the comparison set is the
set of blue cars. Semantically, we can represent
?cheapest blue car? as follows, where the compar-
ison set is made explicit in the antecedent of the
conditional:
?x.[car(x) ? blue(x) ?
?y((car(y) ? blue(y) ? x 6=y) ? cheaper(x,y))]
Paraphrased in English, this stipulates that some
blue car is cheaper than any other blue car. A
meaning representation like this will logically pre-
dict the correct entailment relations for superla-
tives.
3 Annotated Corpus of Superlatives
In order to develop and evaluate our system we
manually annotated a collection of newspaper arti-
cle and questions with occurrences of superlatives.
The design of the corpus and its characteristics are
described in this section.
3.1 Classification and Annotation Scheme
Instances of superlatives are identified in text and
classified into one of four possible classes: at-
tributive, predicative, adverbial, or idiomatic:
its rates will be among the highest (predicative)
the strongest dividend growth (attributive)
free to do the task most quickly (adverbial)
who won the TONY for best featured actor? (idiom)
For all cases, we annotate the span of the su-
perlative adjective in terms of the position of the
tokens in the sentence. For instance, in ?its1 rates2
will3 be4 among5 the6 highest7?, the superlative
span would be 7?7.
Additional information is encoded for the at-
tributive case: type of determiner (possessive, def-
inite, bare, demonstrative, quantifier), number (sg,
pl, mass), cardinality (yes, no), modification (ad-
jective, ordinal, intensifier, none). Table 1 shows
some examples from the WSJ with annotation val-
ues.
Not included in this study are adjectives such
as ?next?, ?past?, ?last?, nor the ordinal ?first?,
although they somewhat resemble superlatives in
their semantics. Also excluded are adjectives that
lexicalise a superlative meaning but are not su-
perlatives morphologically, like ?main?, ?princi-
pal?, and the like. For etymological reasons we
however include ?foremost? and ?uttermost.?
3.2 Data and Annotation
Our corpus consists of a collection of newswire
articles from the Wall Street Journal (Sections 00,
01, 02, 03, 04, 10, and 15) and the Glasgow Her-
ald (GH950110 from the CLEF evaluation forum),
and a large set of questions from the TREC QA
evaluation exercise (years 2002 and 2003) and
natural language queries submitted to the Excite
search engine (Jansen and Spink, 2000). The data
was automatically tokenised, but all typos and
extra-grammaticalities were preserved. The cor-
pus was split into a development set used for tun-
ing the system and a test set for evaluation. The
size of each sub-corpus is shown in Table 2.
Table 2: Size of each data source (in number of
sentences/questions)
source dev test total
WSJ 8,058 6,468 14,526
GH ? 2,553 2,553
TREC 1,025 ? 1,025
Excite ? 67,140 67,140
total 9,083 76,161 85,244
The annotation was performed by two trained
linguists. One section of the WSJ was anno-
tated by both annotators independently to calcu-
late inter-annotator agreement. All other docu-
ments were first annotated by one judge and then
checked by the second, in order to ensure max-
imum correctness. All disagreements were dis-
cussed and resolved for the creation of a gold stan-
dard corpus.
Inter-annotator agreement was assessed mainly
using f-score and percentage agreement as well as
11
Table 1: Annotation examples of superlative adjectives
example sup span det num car mod comp set
The third-largest thrift institution in Puerto Rico
also [. . . ]
2?2 def sg no ord 3?7
The Agriculture Department reported that feedlots
in the 13 biggest ranch states held [. . . ]
9?10 def pl yes no 11?12
The failed takeover would have given UAL em-
ployees 75 % voting control of the nation ?s
second-largest airline [. . . ]
17?17 pos sg no ord 14?18
the kappa statistics (K), where applicable (Car-
letta, 1996). In using f-score, we arbitrarily take
one of the annotators? decisions (A) as gold stan-
dard and compare them with the other annotator?s
decisions (B). Note that here f-score is symmetric,
since precision(A,B) = recall(B,A), and (balanced)
f-score is the harmonic mean of precision and re-
call (Tjong Kim Sang, 2002; Hachey et al, 2005,
see also Section 5).
We evaluated three levels of agreement on a
sample of 1967 sentences (one full WSJ section).
The first level concerns superlative detection: to
what extent different human judges can agree on
what constitutes a superlative. For this task, f-
score was measured at 0.963 with a total of 79 su-
perlative phrases agreed upon.
The second level of agreement is relative to type
identification (attributive, predicative, adverbial,
idiomatic), and is only calculated on the subset
of cases both annotators recognised as superlatives
(79 instances, as mentioned). The overall f-score
for the classification task is 0.974, with 77 cases
where both annotators assigned the same type to
a superlative phrase. We also assessed agreement
for each class, and the attributive type resulted the
most reliable with an f-score of 1 (total agree-
ment on 64 cases), whereas there was some dis-
agreement in classifying predicative and adverbial
cases (0.9 and 0.8 f-score, respectively). Idiomatic
uses where not detected in this portion of the data.
To assess this classification task we also used the
kappa statistics which yielded KCo=0.922 (fol-
lowing (Eugenio and Glass, 2004) we report K
as KCo, indicating that we calculate K a` la Co-
hen (Cohen, 1960). KCo over 0.9 is considered to
signal very good agreement (Krippendorff, 1980).
The third and last level of agreement deals with
the span of the comparison set and only concerns
attributive cases (64 out of 79). Percentage agree-
ment was used since this is not a classification task
and was measured at 95.31%.
The agreement results show that the task ap-
pears quite easy to perform for linguists. Despite
the limited number of instances compared, this has
also emerged from the annotators? perception of
the difficulty of the task for humans.
3.3 Distribution
The gold standard corpus comprises a total of
3,045 superlatives, which roughly amounts to one
superlative in every 25 sentences/questions. The
overwhelming majority of superlatives are attribu-
tive (89.1%), and only a few are used in a pred-
icative way (6.9%), adverbially (3.0%), or in id-
iomatic expressions (0.9%).1 Table 3 shows the
detailed distribution according to data source and
experimental sets. Although the corpus also in-
cludes annotation about determination, modifica-
tion, grammatical number, and cardinality of at-
tributive superlatives (see Section 3.1), this infor-
mation is not used by the system described in this
paper.
Table 3: Distribution of superlative types in the
development and evaluation sets.
dev test
type WSJ TREC WSJ GH Excite total
att 240 43 218 68 2,145 2,714
pre 40 3 26 17 125 211
adv 17 2 22 9 41 91
idi 6 5 1 2 15 29
total 303 53 267 96 2,326 3,045
4 Automatic Analysis of Superlatives
The system that we use to analyse superlatives is
based on two linguistic formalisms: Combinatory
Categorial Grammar (CCG), for a theory of syn-
tax; and Discourse Representation Theory (DRT)
1Percentages are rounded to the first decimal and do not
necessarily sum up to 100%.
12
for a theory of semantics. In this section we will il-
lustrate how we extend these theories to deal with
superlatives and how we implemented this into a
working system.
4.1 Combinatory Categorial Grammar
(CCG)
CCG is a lexicalised theory of grammar (Steed-
man, 2001). We used Clark & Curran?s wide-
coverage statistical parser (Clark and Curran,
2004) trained on CCG-bank, which in turn is de-
rived from the Penn-Treebank (Hockenmaier and
Steedman, 2002). In CCG-bank, the majority of
superlative adjective of cases are analysed as fol-
lows:
the tallest woman
NP/N N/N N
N
NP
most devastating droughts
(N/N)/(N/N) N/N N
N/N
N
third largest bank
N/N (N/N)\(N/N) N
N/N
N
Clark & Curran?s parser outputs besides a CCG
derivation of the input sentence also a part-of-
speech (POS) tag and a lemmatised form for each
input token. To recognise attributive superla-
tives in the output of the parser, we look both
at the POS tag and the CCG-category assigned
to a word. Words with POS-tag JJS and CCG-
category N/N, (N/N)/(N/N), or (N/N)\(N/N) are
considered attributive superlatives adjectives, and
so are the words ?most? and ?least? with CCG cat-
egory (N/N)/(N/N).
However, most hyphenated superlatives are not
recognised by the parser as JJ instead of JJS, and
are corrected in a post-processing step.2 Examples
that fall in this category are ?most-recent wave?
and ?third-highest?.
4.2 Discourse Representation Theory (DRT)
The output of the parser, a CCG derivation of the
input sentence, is used to construct a Discourse
Representation Structure (DRS, the semantic rep-
resentation proposed by DRT (Kamp and Reyle,
2This is due to the fact that the Penn-Treebank annotation
guidelines prescribe that all hyphenated adjectives ought to
be tagged as JJ.
1993)). We follow (Bos et al, 2004; Bos, 2005) in
automatically building semantic representation on
the basis of CCG derivations in a compositional
fashion. We briefly summarise the approach here.
The semantic representation for a word is deter-
mined by its CCG category, POS-tag, and lemma.
Consider the following lexical entries:
the: ?p.?q.(
x
;p(x);q(x))
tallest: ?p.?x.( (
y
y 6=x
;p(y))?
taller(x,y)
;p(x))
man: ?x.
man(x)
These lexical entries are combined in a compo-
sitional fashion following the CCG derivation, us-
ing the ?-calculus as a glue language:
tallest man: ?x.
man(x)
y
y6=x
man(y)
?
taller(x,y)
the tallest man: ?q.(
x
man(x)
y
y6=x
man(y)
?
taller(x,y)
;q(x))
In this way DRSs can be produced in a robust
way, achieving high-coverage. An example output
representation of the complete system is shown in
Figure 1.
As is often the case, the output of the parser is
not always what one needs to construct a meaning-
ful semantic representation. There are two cases
where we alter the CCG derivation output by the
parser in order to improve the resulting DRSs. The
first case concerns modifiers following a superla-
tive construction, that are attached to the NP node
rather than N. A case in point is
... the largest toxicology lab in New
England ...
where the PP in New England has the CCG cate-
gory NP\NP rather than N\N. This would result
in a comparison set containing of toxicology labs,
rather than a set toxicology labs in New England.
The second case are possessive NPs preceding
a superlative construction. An example here is
... Jaguar?s largest shareholder ...
13
_______________________________________________
| x0 x1 x2 x3 x4 x5 x6 |
|-----------------------------------------------|
| acquisition(x1) |
| nn(x0,x1) |
| named(x0,georgia-pacific,nam) |
| named(x2,nekoosa,loc) |
| of(x1,x2) |
| company(x5) |
| nn(x3,x5) |
| forest-product(x3) |
| nn(x4,x5) |
| named(x4,us,loc) |
| ____________________ ________________ |
| | x7 x8 x9 | | | |
| |--------------------| |----------------| |
| | company(x9) | ==> | largest(x5,x9) | |
| | nn(x7,x9) | |________________| |
| | forest-product(x7) | |
| | nn(x8,x9) | |
| | named(x8,us,loc) | |
| | _________ | |
| | | | | |
| | __ |---------| | |
| | | | x5 = x9 | | |
| | |_________| | |
| |____________________| |
| create(x6) |
| agent(x6,x1) |
| patient(x6,x5) |
| event(x6) |
|_______________________________________________|
Figure 1: Example DRS output
where a correct interpretation of the superlative
requires a comparison set of shareholders from
Jaguar, rather than just any shareholder. However,
the parser outputs a derivation where ?largest? is
combined with ?shareholder?, and then with the
possessive construction, yielding the wrong se-
mantic interpretation. To deal with this, we anal-
yse possessives that interact with the superlative as
follows:
Rome ?s oldest church
NP ((NP/N)/(N/N)\NP N/N N
(NP/N)/(N/N)
NP/N
NP
This analysis yields the correct comparison set for
superlative that follow a possessive noun phrase,
given the following lexical semantics for the geni-
tive:
?n.?S.?p.?q.(
u
;S(?x.(p(x);n(?y.
of(y,x)
)(u);q(u))))
For both cases, we apply some simple post-
processing rules to the output of the parser to ob-
tain the required derivations. The effect of these
rules is reported in the next section, where we as-
sess the accuracy of the semantic representations
produced for superlatives by comparing the auto-
matic analysis with the gold standard.
5 Evaluation
The automatic analysis of superlatives we present
in the following experiments consists of two se-
quential tasks: superlative detection, and compar-
ison set determination.
The first task is concerned with finding a su-
perlative in text and its exact span (?largest?,
?most beautiful?, ?10 biggest?). For a found string
to to be judged as correct, its whole span must cor-
respond to the gold standard. The task is evaluated
using precision (P), recall (R), and f-score (F), cal-
culated as follows:
P = correct assignments of ctotal assignments of c
R = correct assignments of ctotal corpus instances of c
F = 2PcRcPc+Rc
The second task is conditional on the first: once
a superlative is found, its comparison set must
also be identified (?rarest flower in New Zealand?,
?New York?s tallest building?, see Section 2.2). A
selected comparison set is evaluated as correct if
it corresponds exactly to the gold standard anno-
tation: partial matches are counted as wrong. As-
signments are evaluated using accuracy (number
of correct decisions made) only on the subset of
previously correctly identified superlatives.
For both tasks we developed simple baseline
systems based on part-of-speech tags, and a more
sophisticated linguistic analysis based on CCG
and DRT (i.e. the system described in Section 4).
In the remainder of the paper we refer to the latter
system as DLA (Deep Linguistic Analysis).
5.1 Superlative Detection
Baseline system For superlative detection we
generated a baseline that solely relies on part-of-
speech information. The data was tagged using
TnT (Brants, 2000), using a model trained on the
Wall Street Journal. In the WSJ tagset, superla-
tives can be marked in two different ways, depend-
ing on whether the adjective is inflected or modi-
fied by most/least. So, ?largest?, for instance, is
tagged as JJS, whereas ?most beautiful? is a se-
quence of RBS (most) and JJ (beautiful). We also
checked that they are followed by a common or
proper noun (NN.*), allowing one word to oc-
cur in between. To cover more complex cases,
we also considered pre-modification by adjectives
(JJ), and cardinals (CD). In summary, we matched
on sequences found by the following pattern:
[(CD || JJ)* (JJS || (RBS JJ)) * NN.*]
This rather simple baseline is capable of de-
tecting superlatives such as ?100 biggest banks?,
?fourth largest investors?, and ?most important
14
element?, but will fail on expressions such as
?fastest growing segments? or ?Scotland ?s lowest
permitted 1995-96 increase?.
DLA system For evaluation, we extrapolated
superlatives from the DRSs output by the system.
Each superlative introduces an implicational DRS
condition, but not all implicational DRS condi-
tions are introduced by superlatives. Hence, for
the purposes of this experiment superlative DRS
conditions were assigned a special mark. While
traversing the DRS, we use this mark to retrieve
superlative instances. In order to retrieve the orig-
inal string that gave rise to the superlative interpre-
tation, we exploit the meta information encoded in
each DRS about the relation between input tokens
and semantic information. The obtained string po-
sition can in turn be evaluated against the gold
standard.
Table 4 lists the results achieved by the base-
line system and the DLA system on the detection
task. The DLA system outperforms the baseline
system on precision in all sub-corpora. However,
the baseline achieves a higher recall on the Excite
queries. This is not entirely surprising given that
the coverage of the parser is between 90?95% on
unseen data. Moreover, Excite queries are often
ungrammatical, thus further affecting the perfor-
mance of parsing.
Table 4: Detection of Attributive Superlatives, re-
porting P (precision), R (Recall) and F-score, for
WSJ sections, extracts of the Glasgow Herald,
TREC questions, and Excite queries. D indicates
development data, T test data.
Baseline DLA
Corpus P R F P R F
WSJ (D) 0.93 0.86 0.89 0.96 0.90 0.93
WSJ (T) 0.91 0.83 0.87 0.95 0.87 0.91
GH (T) 0.80 0.76 0.78 0.87 0.81 0.84
TREC (D) 0.76 0.91 0.83 0.85 0.91 0.88
Excite (T) 0.92 0.92 0.92 0.97 0.84 0.90
5.2 Comparison Set Determination
Baseline For comparison set determination we
developed two baseline systems. Both use the
same match on sequences of part-of-speech tags
described above. For Baseline 1, the beginning
of the comparison set is the first word following
the superlative. The end of the comparison set is
the first word tagged as NN.* in that sequence (the
same word could be the beginning and end of the
comparison set, as it often happens).
The second baseline takes the first word after
the superlative as the beginning of the comparison
set, and the end of the sentence (or question) as the
end (excluding the final punctuation mark). We
expect this strategy to perform well on questions,
as the following examples show.
Where is the oldest synagogue in the United States?
What was the largest crowd to ever come see Michael Jordan?
This approach is obviously likely to generate com-
parison sets much wider than required.
More complex examples that neither baseline
can tackle involve possessives, since on the sur-
face the comparison set lies at both ends of the
superlative adjective:
The nation?s largest pension fund
the world?s most corrupt organizations
DLA 1 We first extrapolate superlatives from the
DRS output by the system (see procedure above).
Then, we exploit the semantic representation to se-
lect the comparison set: it is determined by the in-
formation encoded in the antecedent of the DRS-
conditional introduced by the superlative. Again,
we exploit meta information to reconstruct the
original span, and we match it against the gold
standard for evaluation.
DLA 2 DLA 2 builds on DLA 1, to which it adds
post-processing rules to the CCG derivation, i.e.
before the DRSs are constructed. This set of rules
deal with NP post-modification of the superlative
(see Section 4).
DLA 3 In this version we include a set of post-
processing rules that apply to the CCG derivation
to deal with possessives preceding the superlative
(see Section 4).
DLA 4 This is a combination of DLA 2 and
DLA 3. This system is clearly expected to per-
form best.
Results for both baseline systems and all versions
of DLA are shown in Table 5
On text documents, DLA 2/3/4 outperform the
baseline systems. DLA 4 achieves the best per-
formance, with an accuracy of 69?83%. On ques-
tions, however, DLA 4 competes with the base-
line: whereas it is better on TREC questions, it
performs worse on Excite questions. One of the
obvious reasons for this is that the parser?s model
15
Table 5: Determination of Comparison Set of
Attributive Superlatives (Accuracy) for WSJ sec-
tions, extracts of the Glasgow Herald, TREC and
Excite questions. D indicates development data, T
test data.
Corpus Base 1 Base 2 DLA 1 DLA 2 DLA3 DLA 4
WSJ (D) 0.29 0.17 0.29 0.52 0.53 0.78
WSJ (T) 0.31 0.22 0.32 0.59 0.53 0.83
GH (T) 0.23 0.31 0.22 0.51 0.38 0.69
TREC (D) 0.10 0.69 0.13 0.69 0.23 0.82
Excite (T) 0.23 0.90 0.32 0.82 0.33 0.84
for questions was trained on TREC data. Addi-
tionally, as noted earlier, Excite questions are of-
ten ungrammatical and make parsing less likely to
succeed. However, the baseline system, by defini-
tion, does not output semantic representations, so
that its outcome is of little use for further reason-
ing, as required by question answering or general
information extraction systems.
6 Conclusions
We have presented the first empirically grounded
study of superlatives, and shown the feasibility of
their semantic interpretation in an automatic fash-
ion. Using Combinatory Categorial Grammar and
Discourse Representation Theory we have imple-
mented a system that is able to recognise a superla-
tive expression and its comparison set with high
accuracy.
For developing and testing our system, we have
created a collection of over 3,000 instances of su-
perlatives, both in newswire text and in natural
language questions. This very first corpus of su-
perlatives allows us to get a comprehensive picture
of the behaviour and distribution of superlatives in
real occurring data. Thanks to such broad view
of the phenomenon, we were able discover issues
previously unnoted in the formal semantics liter-
ature, such as the interaction of prenominal pos-
sessives and superlatives, which cause problems
at the syntax-semantics interface in the determina-
tion of the comparison set. Similarly problematic
are hyphenated superlatives, which are tagged as
normal adjectives in the Penn Treebank.
Moreover, this work provides a concrete way
of evaluating the output of a stochastic wide-
coverage parser trained on the CCGBank (Hock-
enmaier and Steedman, 2002). With respect to
superlatives, our experiments show that the qual-
ity of the raw output is not entirely satisfactory.
However, we have also shown that some sim-
ple post-processing rules can increase the perfor-
mance considerably. This might indicate that the
way superlatives are annotated in the CCGbank,
although consistent, is not fully adequate for the
purpose of generating meaningful semantic repre-
sentations, but probably easy to amend.
7 Future Work
Given the syntactic and semantic complexity of
superlative expressions, there is still wide scope
for improving the coverage and accuracy of our
system. One obvious improvement is to amend
CCGbank in order to avoid the need for postpro-
cessing rules, thereby also allowing the creation
of more accurate language models. Another as-
pect which we have neglected in this study but
want to consider in future work is the interac-
tion between superlatives and focus (Heim, 1999;
Gawron, 1995). Also, only one of the possible
types of superlative was considered, namely the at-
tributive case. In future work we will consider the
interpretation of predicative and adverbial superla-
tives, as well as comparative expressions. Finally,
we would like to investigate the extent to which
existing NLP systems (such as open-domain QA
systems) can benefit from a detailed analysis of
superlatives.
Acknowledgements
We would like to thank Steve Pulman (for in-
formation on the analysis of superlatives in the
Core Language Engine), Mark Steedman (for use-
ful suggestions on an earlier draft of this paper),
and Jean Carletta (for helpful comments on anno-
tation agreement issues), as well as three anony-
mous reviewers for their comments. We are ex-
tremely grateful to Stephen Clark and James Cur-
ran for making their parser available to us. Johan
Bos is supported by a ?Rientro dei Cervelli? grant
(Italian Ministry for Research); Malvina Nissim is
supported by the EU FP6 NeOn project.
References
Hiyan Alshawi, editor. 1992. The Core Language En-
gine. The MIT Press, Cambridge, Massachusetts.
J. Bos, S. Clark, M. Steedman, J.R. Curran, and Hock-
enmaier J. 2004. Wide-Coverage Semantic Rep-
resentations from a CCG Parser. In Proceedings of
16
the 20th International Conference on Computational
Linguistics (COLING ?04), Geneva, Switzerland.
Johan Bos. 2005. Towards wide-coverage semantic in-
terpretation. In Proceedings of Sixth International
Workshop on Computational Semantics IWCS-6,
pages 42?53.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference ANLP-
2000, Seattle, WA.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
Linguistics, 22(2):249?254.
S. Clark and J.R. Curran. 2004. Parsing the WSJ using
CCG and Log-Linear Models. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL ?04), Barcelona, Spain.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurements, 20:37?46.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: a second look. Computational Lin-
guistics, 30(1).
Donka F. Farkas and Katalin E`. Kiss. 2000. On the
comparative and absolute readings of superlatives.
Natural Language and Linguistic Theory, 18:417?
455.
Jean Mark Gawron. 1995. Comparatives, superlatives,
and resolution. Linguistics and Philosophy, 18:333?
380.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the 9th Confer-
ence on Computational Natural Language Learning,
Ann Arbor, Michigan, USA.
Irene Heim. 1999. Notes on superlatives. MIT.
J. Hockenmaier and M. Steedman. 2002. Genera-
tive Models for Statistical Parsing with Combinatory
Categorial Grammar. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, Philadelphia, PA.
Bernard J. Jansen and Amanda Spink. 2000. The ex-
cite research project: A study of searching by web
users. Bulletin of the American Society for Informa-
tion Science and Technology, 27(1):5?17.
H. Kamp and U. Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications.
M. Steedman. 2001. The Syntactic Process. The MIT
Press.
Anna Szabolcsi. 1986. Comparative superlatives. In
N. Fukui et al, editor, Papers in Theoretical Lin-
guistics, MITWPL, volume 8. MIT.
Erik F. Tjong Kim Sang. 2002. Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of
CoNLL-2002, pages 155?158. Taipei, Taiwan.
17
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 94?102,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Information Status of Discourse Entities
Malvina Nissim?
Laboratory for Applied Ontology
Institute for Cognitive Science and Technology
National Research Council (ISTC-CNR), Roma, Italy
malvina.nissim@loa-cnr.it
Abstract
In this paper we address the issue of au-
tomatically assigning information status to
discourse entities. Using an annotated cor-
pus of conversational English and exploit-
ing morpho-syntactic and lexical features,
we train a decision tree to classify entities
introduced by noun phrases as old, medi-
ated, or new. We compare its performance
with hand-crafted rules that are mainly
based on morpho-syntactic features and
closely relate to the guidelines that had
been used for the manual annotation. The
decision tree model achieves an overall ac-
curacy of 79.5%, significantly outperform-
ing the hand-crafted algorithm (64.4%).
We also experiment with binary classifica-
tions by collapsing in turn two of the three
target classes into one and retraining the
model. The highest accuracy achieved on
binary classification is 93.1%.
1 Introduction
Information structure is the way a speaker or
writer organises known and new information in
text or dialogue. Information structure has been
the subject of numerous and very diverse linguistic
studies (Halliday, 1976; Prince, 1981; Hajic?ova?,
1984; Vallduv??, 1992; Lambrecht, 1994; Steed-
man, 2000, for instance), thus also yielding a
wide range of terms and definitions (see (Vallduv??,
?The work reported in this paper was carried out while
the author was a research fellow at the Institute for Com-
municating and Collaborative Systems of the University
of Edinburgh, United Kingdom, and was supported by a
Scottish Enterprise Edinburgh-Stanford Link grant (265000-
3102-R36766).
1992; Kruijff-Korbayova? and Steedman, 2003) for
a discussion). In the present study, we adopt the
term ?Information Status?, following the defini-
tion employed for the annotation of the corpus we
use for our experiments (Nissim et al, 2004). In-
formation status describes to which degree a dis-
course entity is available to the hearer, in terms
of the speaker?s assumptions about the hearer?s
knowledge and beliefs. Although there is a fine
line in the distinction between Information Sta-
tus and Information Structure, it is fair to say that
whereas the latter models wider discourse coher-
ence, the former focuses mainly on the local level
of discourse entities. Section 2 provides more de-
tails on how this notion is encoded in our corpus.
Information status has generated large interest
among researchers because of its complex interac-
tion with other linguistic phenomena, thus affect-
ing several Natural Language Processing tasks.
Since it correlates with word order and pitch ac-
cent (Lambrecht, 1994; Hirschberg and Nakatani,
1996), for instance, incorporating knowledge on
information status would be helpful for natural
language generation, and in particular text-to-
speech systems. Sto?ber and colleagues, for ex-
ample, ascribe to the lack of such information the
lower performance of text-to-speech compared to
concept-to-speech generation, where such knowl-
edge could be made directly available to the sys-
tem (Sto?ber et al, 2000).
Another area where information status can play
an important role is anaphora resolution. A major
obstacle in the resolution of definite noun phrases
with full lexical heads is that only a small pro-
portion of them is actually anaphoric (ca. 30%
(Vieira and Poesio, 2000)). Therefore, in the ab-
sence of anaphoricity information, a resolution
system will try to find an antecedent also for non-
94
anaphoric definite noun phrases, thus severely af-
fecting performance. There has been recent in-
terest in determining anaphoricity before perform-
ing anaphora resolution (Ng and Cardie, 2002;
Uryupina, 2003), but results have not been en-
tirely satisfactory. Given that old entities are more
likely to be referred to by anaphors, for instance,
identification of information status could improve
anaphoricity determination.
Postolache et al (2005) have recently shown
that learning information structure with high ac-
curacy is feasible for Czech. However, there are
yet no studies that explore such a task for English.
Exploiting an existing annotated corpus, in this pa-
per we report experiments on learning a model for
the automatic identification of information status
in English.
2 Data
For our experiments we annotated a portion of the
transcribed Switchboard corpus (Godfrey et al,
1992), consisting of 147 dialogues (Nissim et al,
2004).1 In the following section we provide a brief
description of the annotation categories.
2.1 Annotation
Our annotation of information status mainly builds
on (Prince, 1992), and employs a distinction into
old, mediated, and new entities similar to the work
of (Strube, 1998; Eckert and Strube, 2001).
All noun phrases (NPs) were extracted as mark-
able entities using pre-existing parse information
(Carletta et al, 2004). An entity was annotated as
new if it has not been previously referred to and
is yet unknown to the hearer. The tag mediated
was instead used whenever an entity that is newly
mentioned in the dialogue can be inferred by the
hearer thanks to prior or general context.2 Typ-
ical examples of mediated entities are generally
known objects (such as ?the sun?, or ?the Pope?
(Lo?bner, 1985)), and bridging anaphors (Clark,
1975; Vieira and Poesio, 2000), where an entity
is related to a previously introduced one. When-
ever an entity was neither new nor mediated, it was
considered as old.
1Switchboard is a collection of spontaneous phone con-
versations, averaging six minutes in length, between speakers
of American English on predetermined topics. A third of the
corpus is syntactically parsed as part of the Penn Treebank
(Marcus et al, 1993)
2This type corresponds to Prince?s (1981; 1992) in-
ferrables.
In order to account for the complexity of the
notion of information status, the annotation also
includes a sub-type classification for old and me-
diated entities that provides a finer-grained dis-
tinction with information on why a given entity is
mediated (e.g., set-relation, bridging) or old (e.g.,
coreference, generic pronouns). In order to test
the feasibility of automatically assigning informa-
tion status to discourse entities, we took a modular
approach and only considered the coarser-grained
distinctions for this first study. Information about
the finer-grained subtypes will be used in future
work.
In addition to the main categories, we used two
more annotation classes: a tag non-applicable,
used for entities that were wrongly extracted in the
automatic selection of markables (e.g. ?course? in
?of course?), for idiomatic occurrences, and ex-
pletive uses of ?it?; and a tag not-understood to be
applied whenever an annotator did not fully under-
stand the text. Instances annotated with these two
tags, as well as all traces, which were left unanno-
tated, were excluded from all our experiments.
Inter-annotator agreement was measured using
the kappa (K) statistics (Cohen, 1960; Carletta,
1996) on 1,502 instances (three Switchboard dia-
logues) marked by two annotators who followed
specific written guidelines. Given that the task
involves a fair amount of subjective judgement,
agreement was remarkably high. Over the three
dialogues, the annotation yielded K = .845 for
the old/med/new classification (K = .788 when
including the finer-grained subtype distinction).
Specifically, ?old? proved to be the easiest to dis-
tinguish, with K = .902; for ?med? and ?new?
agreement was measured at K = .800 and K =
.794, respectively. A value of K > .76 is usually
considered good agreement. Further details on the
annotation process and corpus description are pro-
vided in (Nissim et al, 2004)
2.2 Setup
We split the 147 dialogues into a training, a de-
velopment and an evaluation set. The training set
contains 40,865 NPs distributed over 94 dialogues,
the development set consists of 23 dialogues for a
total of 10,565 NPs, and the evaluation set com-
prises 30 dialogues with 12,624 NPs. Instances
were randomised, so that occurrences of NPs from
the same dialogue were possibly split across the
different sets.
95
Table 1 reports the distribution of classes for
the training, development and evaluation sets. The
distributions are similar, with a majority of old en-
tities, followed by mediated entities, and lastly by
new ones.
Table 1: Information status distribution of NPs in
training, development and evaluation sets
TRAIN DEV EVAL
old 19730 (48.3%) 5181 (49.0%) 6049 (47.9%)
med 15184 (37.1%) 3762 (35.6%) 4644 (36.8%)
new 5951 (14.6%) 1622 (15.4%) 1931 (15.3%)
total 40865 (100%) 10565 (100%) 12624 (100%)
3 Classification with hand-crafted rules
The target classes for our classification experi-
ments are the annotation tags: old, mediated, and
new. As baseline, we could take a simple ?most-
frequent-class? assignment that would classify all
entities as old, thus yielding an accuracy of 47.9%
on the evaluation set (see Table 1). Although the
?all-old? assumption makes a reasonable baseline,
it would not provide a particularly interesting solu-
tion from a practical perspective, since a dialogue
should also contain not-old information. Thus,
rather than adopting this simple strategy, we de-
veloped a more sophisticated baseline working on
a set of hand-crafted rules.
This hand-crafted algorithm is based on rather
straightforward, intuitive rules, partially reflecting
the instructions specified in the annotation guide-
lines. As shown in Figure 1, the top split is the
NP type: whether the instance to classify is a pro-
noun, a proper noun, or a common noun. The
other information that the algorithm uses is about
complete or partial string overlapping with respect
to the dialogue?s context. For common nouns we
also consider the kind of determiner (definite, in-
definite, demonstrative, possessive, or bare).
In order to obtain the NP type information, we
exploited the pre-existing morpho-syntactic tree-
bank annotation of Switchboard. Whenever the
extraction failed, we assigned a type ?other? and
always backed-off these cases to old (the most fre-
quent class in training data). Values for the other
features were obtained by simple pattern matching
and NP extraction.
Evaluation measures The algorithm?s perfor-
mance is evaluated with respect to its general ac-
curacy (Acc): the number of correctly classified
instances over all assignments. Moreover, for each
case NP is a pronoun
status := old
case NP is a proper noun
if first occurrence then
status := med
else
status := old
endif
case NP is a common noun
if identical string already mentioned then
status := old
else
if partial string already mentioned then
status := med
else
if determiner is def/dem/poss then
status := med
else
status := new
endif
endif
endif
otherwise
status := old
Figure 1: Hand-crafted rule-based algorithm for
the assignment of information status to NPs.
class (c), we report precision (P), recall (R), and f-
score (F) thus calculated:
Pc =
correct assignments of c
total assignments of c
Rc =
correct assignments of c
total corpus instances of c
Fc = 2PcRcPc+Rc
The overall accuracy of the rule-based algo-
rithm is 65.8%. Table 2 shows the results for each
target class in both the development and evaluation
sets. We discuss results on the latter.
Although a very high proportion of old entities
is correctly retrieved (93.5%), this is done with
relatively low precision (66.7%). Moreover, both
precision and recall for the other classes are dis-
appointing. Unsurprisingly, the rules that apply
to common nouns (the most ambiguous with re-
spect to information status) generate a large num-
96
Table 2: Per class performance of hand-crafted
rules on the development and evaluation sets
DEV EVAL
P R F P R F
old .677 .932 .784 .667 .935 .779
med .641 .488 .554 .666 .461 .545
new .517 .180 .267 .436 .175 .250
ber of false positives. The rule that predicts an
old entity in case of a full previous mention, for
example, has a precision of only 39.8%. Better,
but not yet satisfactory, is the precision of the rule
that predicts a mediated entity for a common noun
that has a previous partial mention (64.7%). The
worst performing rule is the one that assigns the
most frequent class (old) to entities of syntactic
type ?other?, with a precision of 35.4%. To give an
idea of the correlation between NP type and infor-
mation status, in Table 3 we report the distribution
observed in the evaluation set.
Table 3: Distribution of information status over
NP types in the evaluation set
old med new
pronoun 4465 159 13
proper 107 198 27
common 752 2874 1256
other 725 1413 635
4 Learning Information Status
Our starting point for the automatic assignment
of information status are the three already intro-
duced classes: old, mediated and new. Addition-
ally, we experiment with binary classifications, by
collapsing mediated entities in turn with old and
new ones.
For training, developing and evaluating the
model we use the split described in Section 2.2
(see Table 1). Performance is evaluated accord-
ing to overall accuracy and per class precision, re-
call, and f-score as described in Section 3. To train
a C4.5 decision tree model we use the J48 Weka
implementation (Witten and Frank, 2000). The
choice of features to build the tree is described in
the following section.
4.1 Features
The seven features we use are automatically ex-
tracted from the annotated data exploiting pre-
existing morpho-syntactic markup and using sim-
Table 4: Feature set for learning experiments
FEATURE VALUES
full prev mention numeric
mention time {first,second,more}
partial prev mention {yes,no,na}
determiner {bare,def,dem,indef,poss,na}
NP length numeric
grammatical role {subject,subjpass,object,pp,other}
NP type {pronoun,common,proper,other}
ple pattern matching techniques. They are sum-
marised in Table 4.
The choice of features is motivated by the fol-
lowing observations. The information coming
from partial previous mentions is particularly use-
ful for the identification of mediated entities. This
should account specifically for cases of media-
tion via set-relations; for example, ?your children?
would be considered a partial previous mention of
?my children? or ?your four children?. The value
?na? stands for ?non-applicable? and is mainly
used for pronouns. Full previous mention is likely
to be a good indicator of old entities. Both full and
partial previous mentions are calculated within
each dialogue without any constraints based on
distance.
NP type and determiner type are expected to be
helpful for all categories, with pronouns, for in-
stance, tending to be old and indefinite NPs being
often new. We included the length of NPs (mea-
sured in number of words) since linguistic studies
have shown that old entities tend to be expressed
with less lexical material (Wasow, 2002). In exper-
iments on the development data we also included
the NP string itself, on the grounds that it might
be of use in cases of general mediated instances
(common knowledge entities), such as ?the sun?,
?people?, ?Mickey Mouse?, and so on. However,
this feature turned out to negatively affect perfor-
mance, and was not included in the final model.
4.2 Results
With an overall final accuracy of 79.5% on the
evaluation set, C4.5 significantly outperforms the
hand-crafted algorithm (65.8%). Although the
identification of old entities is quite successful
(F=.928), performance is not entirely satisfactory.
This is especially true for the classification of new
entities, for which the final f-score is .320, mainly
due to extremely low recall (.223). Mediated enti-
ties, instead, are retrieved with a fairly low preci-
sion but higher recall. Table 5 summarises preci-
sion, recall, and f-score for each class.
97
Table 5: Per class performance of C4.5 on the de-
velopment and evaluation sets
DEV EVAL
P R F P R F
old .935 .911 .923 .941 .915 .928
med .673 .878 .762 .681 .876 .766
new .623 .234 .341 .563 .223 .320
The major confusion in the classification arises
between mediated and new (the most difficult de-
cision to make for human annotators too, see Sec-
tion 2.1), which are often distinguished on the ba-
sis of world knowledge, not available to the classi-
fier. This is clearly shown by the confusion matrix
in Table 6: the highest proportion of mistakes is
due to 1,453 new instances classified as mediated.
Also significant is the wrong assignment of me-
diated tags to old entities. Such behaviour of the
classifier is to be expected, given the ?in-between?
nature of mediated entities.
Table 6: Confusion matrix for evaluation set.
C=Classifier tag; G=Gold tag
C ?
G ?
old med new
old 5537 452 60
med 303 4066 275
new 47 1453 431
4.3 Classification with two categories only
Given the above observations, we collapsed me-
diated entities in turn with old ones (focusing on
their non-newness) or new ones (enhancing their
non complete givenness), thus reducing the task to
a binary classification.
Since it appears to be more difficult to distin-
guish mediated and new rather than mediated and
old (Table 6), we expect the classifier to perform
better when mediated is binned with new rather
than old. Also, in the case where mediated and old
entities are collapsed into one single class as op-
posed to new ones, the distribution of classes be-
comes highly skewed towards old entities (84.7%)
so that the learner is likely to lack sufficient infor-
mation for identifying new entities.
Table 7 shows the final accuracy for the two bi-
nary classifications (and the three-way one). As
expected, when mediated entities are joint with
new ones, the classifier performs best (93.1%),
with high f-scores for both old and new, and is sig-
nificantly better than the alternative binary classi-
fication (t-test, p < 0.001). Indeed, the old+med
vs new classification is nearly an all-old assign-
ment and its overall final accuracy (85.5%) is not
a significant improvement over the all-old baseline
(84.7%). Results suggest that mediated NPs are
more similar to new than to old entities and might
provide interesting feedback for the theoretical as-
sumptions underlying the corpus annotation.
4.4 Comparison with two categories only
For a fair comparison, we performed a two-way
classification using the hand-crafted algorithm,
which had to be simplified to account for the lack
of a mediated class.
In the case where all mediated instances where
collapsed together with the old ones, the decision
rules are very simple: pronouns, proper nouns, and
common nouns that have been previously fully or
partially mentioned are classified as old; first men-
tion common nouns are new; everything else is
old. Both precision and recall for old instances
are quite high (.868 and .906 respectively), for a
resulting f-score of .887. Conversely, the perfor-
mance on identifying new entities is very poor,
with a precision of .337 and a recall of .227, for
a combined f-score of .271. The overall accuracy
is .803, and this is significantly lower than the per-
formance of C4.5, which achieves an overall accu-
racy of .850 (t-test, p < 0.001).
When mediated entities are collapsed with new
ones, rule-based classification is done again with
a very basic algorithm derived from the rules in
Figure 1: pronouns are old; proper nouns are new
if first mention, old otherwise; common nouns
that have been fully previously mentioned are old,
otherwise new. Everything else is new, which in
the training set is now the most frequent class
(51.7%). The overall accuracy of .849 is signif-
icantly lower than that achieved by C4.5, which
is .931 (t-test, p < 0.001). Differently from the
previous case (mediated collapsed with old), the
performance on each class is comparable, with a
precision, recall and f-score of .863, .815, and .838
for old and of .838, .881, and .859 for new.
5 Discussion
5.1 Influence of training size
In order to assess the contribution of training size
to performance, we experimented with increas-
98
Table 7: Overview of accuracy for hand-crafted
rules and C4.5 on three-way and binary classifica-
tions on development and evaluation sets
DEV EVAL
classification rules C4.5 rules C4.5
old vs med vs new .658 .796 .644 .795
old+med vs new .810 .861 .803 .855
old vs med+new .844 .926 .849 .931
ingly larger portions of the training data (from 50
to 30,000 instances). For each training size we ran
the classifier 5 times, each with a different ran-
domly picked set of instances. This was done for
the three-way and the two binary classifications.
Reported results are always averaged over the 5
runs. Figure 2 shows the three learning curves.
Figure 2: Learning curves for three- and two-way
classifications
The curve for the three-way classification shows a
slight constant improvement, though it appears to
reach a plateau after 5,000 instances. The result
obtained training on the full set (40865 instances)
is significantly better only if compared to a train-
ing set of 4,000 or less (t-test, p < 0.05). No other
significant difference in accuracy can be observed.
Increasing the training size over 5,000 instances
when learning to classify old+mediated vs new
leads to a slight improvement due to the learner
being able to identify some new entities. With a
smaller training set the proportion of new entities
is far too small to be of use. However, as said, the
overall final accuracy of 85.5% (see Table 7) does
not significantly improve over the baseline.
Table 8: Performance of leave-one-out and single-
feature classifiers on three-way classification
FEATURE
ACCURACY
removed single
full prev mention .793 .730
mention time .795 .730
partial prev mention .791 .769
determiner .789 .775
NP length .793 .733
gram role .782 .656
NP type .784 .701
full set .795
5.2 Feature contribution
We are also interested in the contribution of each
single feature. Therefore, we ran the classifier
again, leaving out one feature at a time. No sig-
nificant drop or gain was observed in any of the
runs (t-test, p < 0.01), though the worst detri-
ments were yielded by removing the grammati-
cal role and the NP type. These two features,
however, also appear to be the least informative
in single-feature classification experiments, thus
suggesting that such information comes very use-
ful only when combined with other evidence (see
also Section 5.4. All results for leave-one-out and
single-feature classifiers are shown in Table 8.
5.3 Error Analysis
The overwhelming majority of mistakes (1,453,
56.1% of all errors) in the three-way classification
stems from classifying as mediated entities that
are in fact new (Table 6). Significant confusion
arises from proper nouns, as they are annotated as
mediated or new entities, depending on whether
they are generally known (such as names of US
presidents, for example), or domain/community-
specific (such as the name of a local store that only
the speaker knows). This inconsistency in the an-
notation might reflect well the actual status of en-
tities in the dialogues, but it can be misleading for
the classifier.
Another large group of errors is formed by old
entities classified as mediated (452 cases). This is
probably due to the fact that the first node in the
decision tree is the ?partial mention? feature (see
Figure 3). The tree correctly captures the fact that
a firstly mentioned entity which has been partially
mentioned before is mediated. An entity that has
a previous partial mention but also a full previous
mention is classified as old only if it is a proper
noun or a pronoun, but as mediated if it is a com-
mon noun. This yields a large number of mis-
99
takes, since many common nouns that have been
previously mentioned (both in full and partially)
are in fact old. Another problem with previous
mentions is the lack of restriction in distance: we
consider a previous mention any identical mention
of a given NP anywhere in the dialogue, and we
have no means of checking that it is indeed the
same entity that is referred to. A way to alleviate
this problem might be exploiting speaker turn in-
formation. Using anaphoric chains could also be
of help, but see Section 6.
5.4 Learnt trees meet hand-crafted rules
The learnt trees provide interesting insights on the
intuitions behind the choice of hand-crafted rules.
partial = yes
| full <= 1
| | det = def: med
| | det = indef
| | | length <= 2
| | | | gramm = subj: med
| | | | gramm = subjpassive: new
| | | | gramm = obj: med
| | | | gramm = pp: med
| | | | gramm = other
| | | | | type = proper: med
| | | | | type = common: new
| | | | | type = pronoun: new
| | | | | type = other: med
| | | length > 2: med
| | det = dem
| | | gramm = subj
. . .
Figure 3: Top of C.5, full training set, three classes
Figure 3 shows the top of C4.5 (trained on the full
training set for the three-way classification), which
looks remarkably different from the rules in Fig-
ure 1. We had based our decision of emphasising
the importance of the NP type on the linguistic ev-
idence that different syntactic realisations reflect
different degrees of availability of discourse enti-
ties (Givo?n, 1983; Ariel, 1990; Grosz et al, 1995).
In the learnt model, however, knowledge about NP
type is only used as subordinate to other features.
This is indeed mirrored in the fact that removing
NP type information from the feature set causes
accuracy to drop, but a classifier building on NP
type alone performs poorly (see Table 8).3 In-
terestingly, though, more informative knowledge
about syntactic form seems to be derived from the
determiner type, which helps distinguish degrees
of oldness among common nouns.
3The NPtype-only classifier assigns old to pronouns and
med to all other types; it never assigns new.
5.5 Naive Bayes model
For additional comparison, we also trained a Naive
Bayes classifier with the same experimental set-
tings. Results are significantly worse than C4.5?s
in all three scenarios (t-test, p < 0.005), with an
accuracy of 74.6% in the three-way classification,
63.3% for old+mediated vs new, and 91.0% for old
vs mediated+new. The latter distribution appears
again to be the easiest to learn.
6 Related Work
To our knowledge, there are no other studies on the
automatic assignment of information status in En-
glish. Recently, (Postolache et al, 2005) have re-
ported experiments on learning information struc-
ture in the Prague TreeBank. The Czech tree-
bank is annotated following the Topic-Focus artic-
ulation theory (Hajic?ova? et al, 1998). The theo-
retical definitions underlying the Prague Treebank
and the corpus we are using are different, with the
former giving a more global picture of informa-
tion structure, and the latter a more entity-specific
one. For this reason, and due to the fact that Pos-
tolache et al?s experiments are on Czech (with a
freer word order than English), comparing results
is not straightforward.
Their best system (C4.5 decision tree) achieves
an accuracy of 90.69% on the topic/focus identi-
fication task. This result is comparable with the
result we obtain when training and testing on the
corpus where mediated and new entities are not
distinguished (93.1%). Postolache and colleagues
also observe a slowly flattening learning curve af-
ter a very small amount of data (even 1%, in their
case). Therefore, they predict an increase in per-
formance will mainly come from better features
rather than more training data. This is likely to be
true in our case as well, also because our feature
set is currently small and we will further benefit
from incorporating additional features. Postolache
et al use a larger feature set, which also includes
coreference information. The corpus we use has
manually annotated coreference links. However,
because we see anaphoricity determination as a
task that could benefit from automatic information
status assignment, we decided not to exploit this
information in the current experiments. Moreover,
we did not want our model to rely too heavily on a
feature that is not easy to obtain automatically.
100
7 Conclusions and Future Work
We have presented a model for the automatic as-
signment of information status in English. On the
three-way classification into old, mediated, and
new that reflects the corpus annotation tags, the
learnt tree outperforms a hand-crafted algorithm
and achieves an accuracy of 79.5%, with high pre-
cision and recall for old entities, high recall for
mediated entities, and a fair precision, but very
poor recall, for new ones. When we collapsed me-
diated and new entities into one category only op-
posing this to old ones, the classifier performed
with an accuracy of 93.1%, with high f-scores for
both classes. Binning mediated and old entities to-
gether did not produce interesting results, mainly
due to the highly skewed distribution of the result-
ing corpus towards old entities. This suggests that
mediated entities are more similar to new than to
old ones, and might provide interesting feedback
for the theoretical assumptions underlying the an-
notation. Future work will examine specific cases
and investigate how such insights can be used to
make the theoretical framework more accurate.
As the first experiments run on English to learn
information status, we wanted to concentrate on
the task itself and avoid noise introduced by au-
tomatic processing. More realistic settings for in-
tegrating an information status model in a large-
scale NLP system would imply obtaining syntactic
information via parsing rather than directly from
the treebank. Future experiments will assess the
impact of automatic preprocessing of the data.
Results are very promising but there is room for
improvement. First, the syntactic category ?other?
is far too large, and finer distinctions must be made
by means of better extraction rules from the trees.
Second, and most importantly, we believe that us-
ing more features will be the main trigger of higher
accuracy. In particular, we plan to use additional
lexical and relational features derived from knowl-
edge sources such as WordNet (Fellbaum, 1998)
and FrameNet (Baker et al, 1998) which should
be especially helpful in distinguishing mediated
from new entities, the most difficult decision to
make. For example, an entity that is linked in
WordNet (within a given depth) and/or FrameNet
to a previously introduced one is more likely to be
mediated than new.
Additionally, we will attempt to exploit dia-
logue turns, since knowing which speaker said
what is clearly very valuable information. In a
similar vein, we will experiment with distance
measures, in terms of turns, sentences, or even
time, for determining when an introduced entity
might stop to be available.
We also plan to run experiments on the auto-
matic classification of old and mediated subtypes
(the finer-grained classification) that is included
in the corpus but that we did not consider for the
present study (see Section 2.1). The major benefit
of this would be a contribution to the resolution of
bridging anaphora.
References
Mira Ariel. 1990. Accessing Noun Phrase An-
tecedents. Routledge, London-New York.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, Proceedings of
COLING-ACL, pages 86?90.
Jean Carletta, Shipra Dingare, Malvina Nissim, and
Tatiana Nikitina. 2004. Using the NITE XML
Toolkit on the Switchboard Corpus to study syntac-
tic choice: a case study. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC2004), Lisbon, May 2004.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
Linguistics, 22(2):249?254.
Herbert H. Clark. 1975. Bridging. In Roger Schank
and Bonnie Nash-Webber, editors, Theoretical Is-
sues in Natural Language Processing. The MIT
Press, Cambridge, MA.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurements, 20:37?46.
Miriam Eckert and Michael Strube. 2001. Dialogue
acts, synchronising units and anaphora resolution.
Journal of Semantics, 17(1):51?89.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA.
Talmy Givo?n. 1983. Introduction. In Talmy Givo?n,
editor, Topic Continuity in Discourse: A Quantita-
tive Cross-language Study. John Benjamins, Ams-
terdam/Philadelphia.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICASSP-92, pages 517?520.
Barbara Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: a framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
101
Eva Hajic?ova?, Barbara Partee, and Petr Sgall. 1998.
Topic-focus articulation, tripartite structures, and se-
mantic content. In Studies in Linguistics and Philos-
ophy, volume 71. Dordrecht.
Eva Hajic?ova?. 1984. Topic and focus. In Petr
Sgall, editor, Contributions to Functional Syntax.
Semantics and Language Comprehension (LLSEE
16), pages 189?202. John Benjamins, Amsterdam.
M.A.K. Halliday. 1976. Notes on transitivity and
theme in English. Part 2. Journal of Linguistics,
3(2):199?244.
Julia Hirschberg and Christine H. Nakatani. 1996. A
prosodic analysis of discourse segments in direction
giving monologues. In Proceedings of 34th Annual
Meeting of the Association for Computational Lin-
guistics.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003.
Discourse and information structure. Journal of
Logic, Language, and Information, 12:249?259.
Knud Lambrecht. 1994. Information structure and
sentence form. Topic, focus, and the mental repre-
sentation of discourse referents. Cambridge Univer-
sity Press, Cambridge.
Sebastian Lo?bner. 1985. Definites. Journal of Seman-
tics, 4:279?326.
Mitchell Marcus, Beatrice Santorini, and May Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn treebank. Computa-
tional Linguistics, 19:313?330.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In Proc of the 19th In-
ternational Conference on Computational Linguis-
tics; Taipei, Taiwan, pages 730?736.
Malvina Nissim, Shipra Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for
information status in dialogue. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC2004), Lisbon, May
2004.
Oana Postolache, Ivana Kruijff-Korbayova, and Geert-
Jan Kruijff. 2005. Data-driven approaches for in-
formation structure identification. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 9?16, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
Ellen F. Prince. 1981. Toward a taxonomy of given-
new information. In Peter Cole, editor, Radical
Pragmatics. Academic Press, New York.
Ellen Prince. 1992. The ZPG letter: subjects, definite-
ness, and information-status. In Sandra Thompson
and William Mann, editors, Discourse description:
diverse analyses of a fund raising text, pages 295?
325. John Benjamins, Philadelphia/Amsterdam.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
K. Sto?ber, P. Wagner, Jo?rg Helbig, S. Ko?ster, D. Stall,
M. Thomas, J. Blauert, W. Hess, R. Hoffmann, and
H. Mangold. 2000. Speech synthesis using multi-
level selection and concatenation of units from large
speech corpora. In W. Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translation, pages
519?534. Springer-Verlag, Berlin.
Michael Strube. 1998. Never look back: An alterna-
tive to centering. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics and
36th Annual Meeting of the Association for Com-
putational Linguistics, pages 1251?1257, Montre?al,
Que?bec, Canada.
Olga Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proc. of
the ACL 2003 Student Workshop, pages 80?86.
Enric Vallduv??. 1992. The Informational Component.
Garland, New York.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4).
Thomas Wasow. 2002. Postverbal Behavior. CSLI
Publications.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San
Diego, CA.
102
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642?646,
Dublin, Ireland, August 23-24, 2014.
The Meaning Factory: Formal Semantics for Recognizing Textual
Entailment and Determining Semantic Similarity
Johannes Bjerva
Univ. of Groningen
j.bjerva@rug.nl
Johan Bos
Univ. of Groningen
johan.bos@rug.nl
Rob van der Goot
Univ. of Groningen
r.van.der.goot@rug.nl
Malvina Nissim
Univ. of Bologna
malvina.nissim@unibo.it
Abstract
Shared Task 1 of SemEval-2014 com-
prised two subtasks on the same dataset
of sentence pairs: recognizing textual en-
tailment and determining textual similar-
ity. We used an existing system based on
formal semantics and logical inference to
participate in the first subtask, reaching
an accuracy of 82%, ranking in the top
5 of more than twenty participating sys-
tems. For determining semantic similar-
ity we took a supervised approach using a
variety of features, the majority of which
was produced by our system for recogniz-
ing textual entailment. In this subtask our
system achieved a mean squared error of
0.322, the best of all participating systems.
1 Introduction
The recent popularity of employing distributional
approaches to semantic interpretation has also lead
to interesting questions about the relationship be-
tween classic formal semantics (including its com-
putational adaptations) and statistical semantics.
A promising way to provide insight into these
questions was brought forward as Shared Task 1 in
the SemEval-2014 campaign for semantic evalua-
tion (Marelli et al., 2014). In this task, a system is
given a set of sentence pairs, and has to predict for
each pair whether the sentences are somehow re-
lated in meaning. Interestingly, this is done using
two different metrics: the first stemming from the
formal tradition (contradiction, entailed, neutral),
and the second in a distributional fashion (a simi-
larity score between 1 and 5). We participated in
this shared task with a system rooted in formal se-
mantics. In particular, we were interested in find-
ing out whether paraphrasing techniques could in-
crease the accuracy of our system, whether mean-
ing representations used for textual entailment are
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
useful for predicting semantic similarity, and con-
versely, whether similarity features could be used
to boost accuracy of recognizing textual entail-
ment. In this paper we outline our method and
present the results for both the textual entailment
and the semantic similarity task.
1
2 Recognizing Textual Entailment
2.1 Overview
The core of our system for recognizing textual en-
tailment works as follows: (i) produce a formal se-
mantic representation for each sentence for a given
sentence pair; (ii) translate these semantic repre-
sentations into first-order logic; (iii) use off-the-
shelf theorem provers and model builders to check
whether the first sentence entails the second, or
whether the sentences are contradictory. This is
essentially an improved version of the framework
introduced by Bos & Markert (2006).
To generate background knowledge that could
assist in finding a proof we used the lexical
database WordNet (Fellbaum, 1998). We also
used a large database of paraphrases (Ganitkevitch
et al., 2013) to alter the second sentence in case no
proof was found at the first attempt, inspired by
Bosma & Callison-Burch (2006). The core sys-
tem reached high precision on entailment and con-
tradiction. To increase recall, we used a classifier
trained on the output from our similarity task sys-
tem (see Section 3) to reclassify the ?neutrals? into
possible entailments.
2.2 Technicalities
The semantic parser that we used is Boxer (Bos,
2008). It is the last component in the pipeline of
the C&C tools (Curran et al., 2007), comprising
a tokenizer, POS-tagger, lemmatizer (Minnen et
1
To reproduce these results in a linux environment (with
SWI Prolog) one needs to install the C&C tools (this in-
cludes Boxer and the RTE system), the Vampire theorem
prover, the two model builders Paradox and Mace-2, and the
PPDB-1.0 XL database. Detailed instructions can be found in
the src/scripts/boxer/sick/README folder of the
C&C tools.
642
al., 2001), and a robust parser for CCG (Steed-
man, 2001). Boxer produces semantic represen-
tations based on Discourse Representation Theory
(Kamp and Reyle, 1993). We used the standard
translation from Discourse Representation Struc-
tures to first-order logic, rather than the one based
on modal first-order logic (Bos, 2004), since the
shared task data did not contain any sentences with
propositional argument verbs.
After conversion to first-order logic, we
checked with the theorem prover Vampire (Ri-
azanov and Voronkov, 2002) whether a proof
could be found for the first sentence entailing the
second, and whether a contradiction could be de-
tected for the conjunction of both sentences trans-
lated into first-order logic. If neither a proof nor
a contradiction could be found within 30 seconds,
we used the model builder Paradox (Claessen and
S?orensson, 2003) to produce a model of the two
sentences separately, and one of the two sentences
together. However, even though Paradox is an ef-
ficient piece of software, it does not always return
minimal models with respect to the extensions of
the non-logical symbols. Therefore, in a second
step, we asked the model builder Mace-2 (Mc-
Cune, 1998) to construct a minimal model for the
domain size established by Paradox. These mod-
els are used as features in the similarity task (Sec-
tion 3).
Background knowledge is important to increase
recall of the theorem prover, but hard to acquire
automatically (Bos, 2013). Besides translating hy-
pernym relations of WordNet to first-order logic
axioms, we also reasoned that it would be benefi-
cial to have a way of dealing with multi-word ex-
pressions. But instead of translating paraphrases
into axioms, we used them to rephrase the input
sentence in case no proof or contradiction was
found for the original sentence pair. Given a para-
phrase SRC7?TGT, we rephrased the first sen-
tence of a pair only if SRC matches with up to
four words, no words of TGT were already in the
first sentence, and every word of TGT appeared in
the second sentence. The paraphrases themselves
were taken from PPDB-1.0 (Ganitkevitch et al.,
2013). In the training phrase we found that the XL
version (comprising o2m, m2o, phrasal, lexical)
gave the best results (using a larger version caused
a strong decrease in precision, while smaller ver-
sions lead to a decrease in recall).
We trained a separate classifier in order to re-
classify items judged by our RTE system as be-
ing neutral. This classifier uses a single feature,
namely the relatedness score for each sentence
pair. As training material, we used the gold relat-
edness scores from the training and trial sets. For
classification of the test set, we used the related-
ness scores obtained from our Semantic Similarity
system (see Section 3). The classifier is a Support
Vector Machine classifier, in the implementation
provided by Scikit-Learn (Pedregosa et al., 2011),
based on the commonly used implementation LIB-
SVM (Chang and Lin, 2011). We used the imple-
mentation?s standard parameters.
2.3 Results
We submitted two runs. The first (primary) run
was produced by a configuration that included re-
classifying the ?neutrals?. The second run is with-
out the reclassification of the neutrals. After sub-
mission we ran a system that did not use the para-
phrasing technique in order to measure what in-
fluence the PPDB had on our performance. The
results are summarized in Table 1. In the train-
ing phase we got the best results for the configu-
ration using the PPDB and reclassication, which
was submitted as our primary run.
Table 1: Results on the entailment task for various
system configurations.
System Configuration Accuracy
most frequent class baseline 56.7
?PPDB, ?reclassification 77.6
+PPDB, ?reclassification 79.6
+PPDB, +reclassification 81.6
In sum, our system for recognizing entailment
performed well reaching 82% accuracy and by
far outperforming the most-frequent class baseline
(Table 1). We show some selected examples illus-
trating the strengths of our system below.
Example 1627 (ENTAILMENT)
A man is mixing a few ingredients in a bowl
Some ingredients are being mixed in a bowl by a person
Example 2709 (CONTRADICTION)
There is no person boiling noodles
A woman is boiling noodles in water
Example 9051 (ENTAILMENT)
A pair of kids are sticking out blue and green colored tongues
Two kids are sticking out blue and green colored tongues
A proof for entailment is found for Ex. 1627,
because for passive sentences Boxer produces
a meaning representation equivalent to their ac-
tive variants. A contradiction is detected for
Ex. 2709 because of the way negation is han-
dled by Boxer. Both examples trigger background
knowledge from WordNet hyperonyms (man ?
person; woman ? person) that is used in the
643
proofs.
2
Ex. 9051 shows how paraphrasing helps,
here ?a pair of? 7? ?two?.
3 Determining Semantic Similarity
3.1 Overview
The Semantic Similarity system follows a super-
vised approach to solving the regression problem
of determining the similarity between each given
sentence pair. The system uses a variety of fea-
tures, ranging from simpler ones such as word
overlap, to more complex ones in the form of
deep semantic features and features derived from a
compositional distributional semantic model. The
majority of these features are derived from the
models from our RTE system (see Section 2).
3.2 Technicalities
3.2.1 Regressor
The regressor used is a Random Forest Regressor
in the implementation provided by Scikit-Learn
(Pedregosa et al., 2011). Random forests are ro-
bust with respect to noise and do not overfit easily
(Breiman, 2001). These two factors make them a
highly suitable choice for our approach, since we
are dealing with a relatively large number of weak
features, i.e., features which may be seen as indi-
vidually containing a rather small amount of infor-
mation for the problem at hand.
Our parameter settings for the regressor is fol-
lows. We used a total of 1000 trees, with a maxi-
mum tree depth of 20. At each node in a tree the
regressor looked at maximum 3 features in order
to decide on the split. The quality of each such
split is determined using mean squared error as
measure. These parameter values were optimised
when training on the training set, with regards to
performance on the trial set.
3.2.2 Feature overview
We used a total of 32 features for our regres-
sor. Due to space constraints, we have sub-divided
our features into groups by the model/method in-
volved. For all features we compared the outcome
of the original sentence pair with the outcome of
the paraphrased sentence pairs (see Section 2.2)
3
.
If the paraphrased sentence pair yielded a higher
feature overlap score than the original sentence
pair, we utilized the former. In other words, we
2
In the training data around 20% of the proofs for entail-
ment were established with the help of WordNet, but only 4%
for detecting contradictions.
3
In addition to the PPDB we added handling of negations,
by removing some negations {not, n?t} and substituting oth-
ers {no:a, none:some, nobody:somebody}.
assume that the sentence pair generated with para-
phrases is a good representation of the original
pair, and that similarities found here are an im-
provement on the original score.
Logical model We used the logical models cre-
ated by Paradox and Mace for the two sentences
separately, as well as a combined model (see Sec-
tion 2.2). The features extracted from this model
are the proportion of overlap between the in-
stances in the domain, and the proportion of over-
lap between the relations in the model.
Noun/verb overlap We first extracted and lem-
matised all nouns and verbs from the sentence
pairs. With these lemmas we calculated two new
separate features, the overlap of the noun lemmas
and the overlap of the verb lemmas.
Discourse Representation Structure (DRS)
The two most interesting pieces of information
which easily can be extracted from the DRS mod-
els are the agents and patients. We first extracted
the agents for both sentences in a sentence pair,
and then computed the overlap between the two
lists of agents. Secondly, since all sentences in the
corpus have exactly one patient, we extracted the
patient of each sentence and used this overlap as a
binary feature.
Wordnet novelty We build one tree containing
all WordNet concepts included in the first sen-
tence, and one containing all WordNet concepts
of both sentences together. The difference in size
between these two trees is used as a feature.
RTE The result from our RTE system (entail-
ment, neutral or contradiction) is used as a feature.
Compositional Distributional Semantic Model
Our CDSM feature is based on word vectors de-
rived using a Skip-Gram model (Mikolov et al.,
2013a; Mikolov et al., 2013b). We used the pub-
licly available word2vec
4
tool to calculate these
vectors. We trained the tool on a data set con-
sisting of the first billion characters of Wikipedia
5
and the English part of the French-English 10
9
corpus used in the wmt11 translation task
6
. The
Wikipedia section of the data was pre-processed
using a script
7
which made the text lower case, re-
moved tables etc. The second section of the data
was also converted to lower case prior to training.
We trained the vectors using the following pa-
rameter settings. Vector dimensionality was set
4
code.google.com/p/word2vec/
5
mattmahoney.net/dc/enwik9.zip
6
statmt.org/wmt11/translation-task.html#download
7
mattmahoney.net/dc/textdata.html
644
Table 2: Pearson correlation and MSE obtained on the test set for each feature group in isolation.
Feature group p [?PPDB] p [+PPDB] MSE [?PPDB] MSE [+PPDB]
Logical model 0.649 0.737 0.590 0.476
Noun/verb overlap 0.647 0.676 0.592 0.553
DRS 0.634 0.667 0.610 0.569
Wordnet novelty 0.652 0.651 0.590 0.591
RTE 0.621 0.620 0.626 0.627
CDSM 0.608 0.609 0.681 0.679
IDs 0.493 0.493 0.807 0.807
Synset 0.414 0.417 0.891 0.889
Word overlap 0.271 0.340 0.944 0.902
Sentence length 0.227 0.228 0.971 0.971
All with IDs 0.836 0.842 0.308 0.297
All without IDs 0.819 0.827 0.336 0.322
to 1600 with a context window of 10 words. The
skip-gram model with hierarchical softmax, and a
negative sampling of 1e-3 was used.
To arrive at the feature used for our regressor,
we first calculated the element-wise sum of the
vectors of each word in the given sentences. We
then calculated the cosine distance between the
sentences in the sentence pair.
IDs One surprisingly helpful feature was each
sentence pair?s ID in the corpus.
8
Since this
feature clearly is not representative of what one
would have access to in a real-world scenario, it
was not included in the primary run.
Synset Overlap We built one set for each sen-
tence pair consisting of each possible lemma form
of all possible noun synsets for each word. The
proportion of overlap between the two resulting
sets was then used as a feature. Given cases where
relatively synonymous words are used (e.g. kid
and child), these will often belong to the same
synset, thus resulting in a high overlap score.
Synset Distance We first generated each possi-
ble word pair consisting of one word from each
sentence. Using these pairings, we calculated
the maximum path similarity between the noun
synsets available for these words. This calculation
is restricted so that each word in the first sentence
in each pair is only used once.
Word overlap Our word overlap feature was
calculated by first creating one set per sentence,
containing each word occurring in that sentence.
8
We discovered that the ordering of the entire data set was
informative for the prediction of sentence relatedness. We
have illustrated this by using the ordering of the sentences
(i.e. the sentence IDs) as a feature in our model, and thereby
obtaining better results. Relying on such a non-natural order-
ing of the sentences would be methodologically flawed, and
therefore this feature was not used in our primary run.
The four most common words in the corpus were
used as a stop list, and removed from each set. The
proportion of overlap between the two sets was
then used as our word overlap feature.
Sentence Lengths The difference in length be-
tween the sentence pairs proved to be a somewhat
useful feature. Although mildly useful for this par-
ticular data set, we do not expect this to be a par-
ticularly helpful feature in real world applications.
3.3 Results
We trained our system on 5000 sentence pairs, and
evaluated it on 4927 sentence pairs. Table 2 con-
tains our scores for the evaluation, broken up per
feature group. Our relatedness system yielded the
highest scores compared to all other systems in
this shared task, as measured by MSE and Spear-
man correlation scores. Although our system per-
formed slightly worse as measured by Pearson
correlation, there is no significant difference to the
scores obtained by the two higher ranked systems.
4 Conclusion
Our work shows that paraphrasing techniques can
be used to improve the results of a textual entail-
ment system. Additionally, the scores from our
semantic similarity measure could be used to im-
prove the scores of the textual entailment system.
Our work also shows that deep semantic features
can be used to predict semantic relatedness.
Acknowledgements
We thank Chris Callison-Burch, Juri Ganitkevitch and Ellie
Pavlick for getting the most out of PPDB. We also thank our
colleagues Valerio Basile, Harm Brouwer, Kilian Evang and
Noortje Venhuizen for valuable comments and feedback.
645
References
Johan Bos and Katja Markert. 2006. Recognising
textual entailment with robust logical inference. In
Joaquin Quinonero-Candela, Ido Dagan, Bernardo
Magnini, and Florence d?Alch?e Buc, editors, Ma-
chine Learning Challenges, MLCW 2005, volume
3944 of LNAI, pages 404?426.
Johan Bos. 2004. Computational Semantics in Dis-
course: Underspecification, Resolution, and Infer-
ence. Journal of Logic, Language and Information,
13(2):139?157.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Johan Bos. 2013. Is there a place for logic in rec-
ognizing textual entailment? Linguistic Issues in
Language Technology, 9(3):1?18.
Wauter Bosma and Chris Callison-Burch. 2006. Para-
phrase substitution for recognizing textual entail-
ment. In Proceedings of CLEF.
Leo Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
K. Claessen and N. S?orensson. 2003. New techniques
that improve mace-style model finding. In P. Baum-
gartner and C. Ferm?uller, editors, Model Computa-
tion ? Principles, Algorithms, Applications (Cade-
19 Workshop), pages 11?27, Miami, Florida, USA.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically Motivated Large-Scale NLP with
C&C and Boxer. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 33?36, Prague,
Czech Republic.
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
Juri Ganitkevitch, Benjamin VanDurme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL 2013), Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi,
S. Menini, and R. Zamparelli. 2014. Semeval-2014
task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
W. McCune. 1998. Automatic Proofs and Counterex-
amples for Some Ortholattice Identities. Informa-
tion Processing Letters, 65(6):285?291.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of english. Jour-
nal of Natural Language Engineering, 7(3):207?
223.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
A. Riazanov and A. Voronkov. 2002. The Design and
Implementation of Vampire. AI Communications,
15(2?3):91?110.
Mark Steedman. 2001. The Syntactic Process. The
MIT Press.
646
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), page 51,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Modelling the internal variability of MWEs
Malvina Nissim
Department of Linguistics and Oriental Studies
University of Bologna
Via Zamboni 33, 40126 Bologna, Italy
malvina.nissim@unibo.it
Abstract
The issue of flexibility of multiword expressions
(MWEs) is crucial towards their identification and
extraction in running text, as well as their better
understanding from a linguistic perspective. If we
project a large MWE lexicon onto a corpus, project-
ing fixed forms suffers from low recall, while an un-
constrained flexible search for lemmas yields a loss
in precision. In this talk, I will describe a method
aimed at maximising precision in the identification
of MWEs in flexible mode, building on the idea that
internal variability can be modelled via so-called
variation patterns. I will discuss the advantages and
limitations of using variation patterns, compare their
performance to that of association measures, and ex-
plore their usability in MWE extraction, too.
About the Speaker
Malvina Nissim is a tenured researcher in computa-
tional linguistics at the University of Bologna. Her
research focuses on the computational handling of
several lexical semantics and discourse phenomena,
such as the choice of referring expressions, semantic
relations within compounds and in argument struc-
ture, multiword expressions, and, more recently, on
the annotation and automatic detection of modality.
She is also a co-founder and promoter of the Senso
Comune project, devoted to the creation of a com-
mon knowledge base for Italian via crowdsourcing.
She graduated in Linguistics from the University of
Pisa, and obtained her PhD in Linguistics from the
University of Pavia. Before joining the University
of Bologna she was a post-doc at the University of
Edinburgh and at the Institute for Cognitive Science
and Technology in Rome.
51
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 101?105,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
A Repository of Variation Patterns for Multiword Expressions
Malvina Nissim
FICLIT, University of Bologna
malvina.nissim@unibo.it
Andrea Zaninello
Zanichelli Editore, Humanities Department
andrea.zaninello@gmail.com
1 Introduction and Background
One of the crucial issues in the analysis and process-
ing of MWEs is their internal variability. Indeed,
the feature that mostly characterises MWEs is their
fixedness at some level of linguistic analysis, be it
morphology, syntax, or semantics. The morphologi-
cal aspect is not trivial in languages which exhibit a
rich morphology, such as Romance languages.
The issue is relevant in at least three aspects of
MWE representation and processing: lexicons, iden-
tification, and extraction (Calzolari et al, 2002). At
the lexicon level, MWEs are usual stored as one
form only, the so-called quotation form (or citation
form). However, some variations of the quotation
form might also be valid instances of MWEs (Bond
et al, 2005) ? some but not all, as some of them
might actually be plain compositional phrases.
This becomes relevant for automatic identification
and extraction. If a lexicon stores the quotation form
only, identification on a corpus done via matching
lexicon strings as such would miss valid variations
of a given MWE. Identification could be done ex-
ploiting lemmas rather than quotation forms, but an
unrestricted match would also possibly return com-
positional phrases. Extraction is usually done ap-
plying association measures over instances of given
POS patterns (Evert and Krenn, 2005), and because
lemmas are matched, no restrictions on internal vari-
ation is enforced as such. Knowing which variations
should be allowed for the quotation form of a given
MWE would help in increasing recall while keeping
precision high. However, specifying such variations
for each MWE would be too costly and wouldn?t
help in extraction, as no specifications could be done
a priori on yet unknown MWEs. Optimally, one
would need to find more general variation patterns
that could be applied to classes of MWEs. Indeed,
the main idea behind this work is that MWEs can
be handled through more general patterns. This is
also claimed, for instance, by Masini (2007) whose
analysis on Italian MWEs takes a constructionist
perspective (Goldberg, 2003), by Weller and Heid
(2010), who treat verbal expressions in German, and
also by Gre?goire (2010), who bases his work on the
Equivalence Class Method (ECM, (Odijk, 2004)) as-
suming that MWEs may be clustered according to
their syntactic pattern and treated homogeneously.
We suggest that variation patterns can be found and
defined over POS sequences. Working on Italian, in
this paper we report the results of ongoing research
and show how such patterns can be derived, we then
propose a way to encode them in a repository, which
can be combined with existing lexicons of MWEs.
For the moment, we restrict our study to contiguous
MWEs although we are aware that non-contiguous
expressions are common and should be treated, too
(see also (Pianta and Bentivogli, 2004)). Thus, only
morphological variation is considered at this stage,
while phenomena such as insertion and word order
variation are left for future work.
2 Obtaining Variation Patterns
Variation patterns refer to POS sequences and rely
on frequencies. The main resources needed for ob-
taining them are a MWE lexicon and a reference cor-
pus (pos-tagged and lemmatised).We use a MWE
lexicon derived from an existing online dictionary
101
for Italian (Zaninello and Nissim, 2010), and the
corpus ?La Repubblica? (Baroni et al, 2004) for ob-
taining frequencies.
A variation pattern encodes the way a given in-
stance of a MWE morphologically differs from its
original quotation form in each of its parts. All
tokens that correspond to the quotation form are
marked as fix whereas all tokens that do not are
marked as flex. Consider Example (1):
(1) a. quotation form: ?casa di cura? (nursing
home)
b. instance: ?case di cura? (nursing homes)
c. variation pattern: flex fix fix
The pattern for the instance in (1b) is flex fix fix
because the first token, ?case? (houses) is a plu-
ral whereas the quotation form features a singu-
lar (?casa?, house), thus is assigned a flex label,
whereas the other two tokens are found exactly as
they appear in the quotation form, and are therefore
labelled as fix.
At this point, it is quite important to note that a
binary feature applied to each token makes flexibil-
ity underspecified in at least two ways. First, the
value flex does not account by itself for the degree
of variation: a token is flex if it can be found in one
variation as well as many. We have addressed this is-
sue elsewhere via a dedicated measure (Nissim and
Zaninello, 2011), but we do not pick it up here again.
In any case, the degree of variation could indeed be
included as additional information. Second, we only
specify which part of the MWEs varies but do not
make assumptions on the type of variation encoun-
tered (for example, it doesn?t distinguish at the level
of gender or number).
We believe this is a fair tradeoff which cap-
tures generalisations at a level which is intermedi-
ate between a word-by-word analysis and consider-
ing the entire MWE as a single unit. Additionally, it
does not require finer-grained annotation than POS-
tagging and lemmatisation, and allows for the dis-
covery of possibly unknown and unpredicted varia-
tions. Morphological analysis, when needed, is of
course still possible a posteriori on the instances
found, but it is useful that at this stage flexibility is
left underspecified.
As said, validating variation patterns per MWE
would be impractical and uninformative with respect
to the extraction of previously unseen MWEs. Thus,
we define variation patterns over part-of-speech se-
quences. More specifically, we operate as follows:
1. search all MWEs contained in a given lexicon
on a large corpus, matching all possible varia-
tions (lemma-based, or unconstrained, search);
2. obtain variation patterns for all MWEs by com-
paring each instance to its quotation form;
3. group all MWEs with the same POS sequence;
4. for each POS sequence collect all variation pat-
terns of all pertinent MWEs.
In previous work (Nissim and Zaninello, 2013), we
have observed that frequency is a good indicator
of valid patterns: the most frequent variation pat-
terns correlate with variations annotated as correct
by manual judges. Patterns for two nominal POS
were evaluated, and they were found to be success-
ful. In this paper we pick three further POS se-
quences per expression type for a total of nine POS
patterns, and evaluate the precision of a pattern se-
lection measure.
The availability of variation patterns per POS se-
quences (and expression type) can be of use both in
identification as well as in extraction. In identifica-
tion, patterns can be used as a selection strategy for
all of the matched instances. One could just use fre-
quency directly from the corpus where the identifi-
cation is done, but this might not always be possible
due to corpus size. This is why using an external
repository of patterns evaluated against a large ref-
erence corpus for a given language might be useful.
In extraction tasks, patterns can be used as fil-
ters, either as a post-processing phase after match-
ing lemmas for given POS sequences, or directly
extracting only allowed configurations which could
be specified for instance in extraction tools such as
mwetoolkit (Ramisch et al, 2010). In previous
work we have shown that patterns can be derived
comparing found instances against their lemmatised
form, making this a realistic setting even in extrac-
tion where quotation forms are not known (Nissim
and Zaninello, 2013).
102
3 Ranking
For ranking variation patterns we take into account
the following figures:
? the total number of different variation patterns
per POS sequence
? the total number of instances (hits on the cor-
pus) with a given variation pattern
For example, the POS sequence ADJ PRE NOUN
characterising some adjectival expressions is fea-
tured by 9 different original multiword expres-
sions that were found in the corpus. The vari-
ations with respect to the quotation form (indi-
cated as fix fix fix and found for seven differ-
ent types) in which instances have been found are
four: flex fix fix (13 times), flex fix flex (7
times), fix fix flex (3 times), and fix flex flex
(one time), for a total of 31 variations. Each in-
stance yielding a given pattern was found at least
once in the corpus, but possibly more times. We take
into account this value as well, thus counting the
number of single instances of a given pattern. So,
while ?degni di nota? (?worthpl mentioning?, quota-
tion form: ?degno di nota?, ?worthsg mentioning?)
would serve as one variation of type flex fix fix,
counting instances would account for the fact that
this expression was found in the corpus 38 times.
For the ADJ PRE NOUN sequence, instances of
pattern fix fix fix were found 130 times, in-
stances of flex fix fix 219, flex fix flex 326,
fix fix flex 90, and fix flex flex just once, for
a total of 766 instances.
Such figures are the basis for pattern ranking and
are used in the repository to contribute to the de-
scription of variation patterns (Figure 1). We use the
share of a given variation pattern (vp) over the total
number of variations (pattern share). In the exam-
ple above, the share of flex fix fix (occurring 13
times) would be 13/31 (41.9%), as 31 is the total
of encountered variations for the ADJ PRE NOUN
POS sequence. We also use the instance share,
which for the same variation pattern would be
219/766 (12.0%) and combine it with the pattern
share to obtain an overall share (sharevp):
sharevp = (
#variationsvp
#variationspos
+ #instancesvp#instancespos )/2
As a global ranking score (GRSvp), the resulting av-
erage share is combined with the spread, namely
the ratio of instances over variations (219/13 for
flex fix fix), a pattern-internal measure indicat-
ing the average instances per variation pattern.
spreadvp =
#instancesvp
#variationsvp
GRSvp = sharevp ? spreadvp
Only patterns with GRS > 1 are kept, with the aim
of maximising precision. Evaluation is done against
some POS sequences for which extracted instances
have been manually annotated. Precision, recall, and
f-score are reported in Table 1. Results for an un-
constrained search (no pattern selection) are also in-
cluded for comparison. The number of variation pat-
terns that we keep on the basis of the ranking score
includes the fix fix fix pattern.
From the table, we can see that in most cases
precision is increased over an unconstrained match.
However, while for verbal expressions the boost
in precision preserves recall high, thus yielding f-
scores that are always higher than for an uncon-
strained search, the same isn?t true for adjectives
and adverbs. In two cases, both featuring the same
POS sequence (PRE NOUN ADJ) though for dif-
ferent expression types, recall is heavily sacrificed.
In three cases, the GRS doesn?t let discard any pat-
terns, thus being of no use in boosting precision.
These are cases where only two variation patterns
were observed, indicating that possibly other rank-
ing measures could be explored for better results un-
der such conditions. In previous work we have seen
that selecting variation patterns works well for nom-
inal expressions (Nissim and Zaninello, 2013).
Overall, even though in some cases our method
does not yield different results than an unconstrained
search, whenever it does, precision is always higher.
It is therefore worth applying whenever boosting
precision is desirable.
4 Repository and Encoding
We create an XML-based repository of POS patterns
with their respective variation patterns. Variation
patterns per POS sequence are reported according
to the ranking produced by the GRS. However, we
103
Table 1: Evaluation of pattern selection for some POS sequences according to the Global Ranking Score.
GRS unconstrained
expr type POS sequence # vp kept prec rec f-score prec rec f-score
verbal
VER:infi ARTPRE NOUN 2/4 1.000 0.998 0.999 0.979 1.000 0.989
VER:infi:cli ART NOUN 2/7 0.965 0.981 0.973 0.943 1.000 0.971
VER:infi ADV 2/4 0.997 0.978 0.987 0.951 1.000 0.975
adjectival
ADJ PRE NOUN 2/2 0.379 1.000 0.550 0.379 1.000 0.550
PRE NOUN ADJ 1/4 1.000 0.590 0.742 0.848 1.000 0.918
PRE VER:fin 4/5 1.000 0.968 0.984 1.000 1.000 1.000
adverbial
PRE ADV 2/2 0.671 1.000 0.803 0.671 1.000 0.803
PRE NOUN ADJ 1/4 1.000 0.746 0.854 0.899 1.000 0.947
PRE ADJ 2/2 0.362 1.000 0.532 0.362 1.000 0.532
include all observed patterns equipped with the fre-
quency information we used, so that other ranking
measures or different thresholds could be applied.
The repository is intended as connected to two
sources, namely a lexicon to obtain quotation forms
of MWEs to be searched, and the corpus where ex-
pressions were searched, which provides the figures.
POS patterns are listed as elements for each
expression element, whose attribute type spec-
ifies the grammatical type?for example ?verbal?.
The same POS pattern can feature under differ-
ent expression types, and could have different con-
straints on variation according to the grammatical
category of the MWE (in extraction this issue would
require dedicated handling, as the grammatical cat-
egory is not necessarily known in advance). For
the element pattern, which specifies the POS se-
quence, the attribute mwes indicates how many dif-
ferent original mews were found for that sequence,
and the attributes variations and instances
the number of variations and instances (Section 3).
Actual patterns are listed as data of a vp (variation
pattern) element, according to decreasing GRS, with
values obtained from the reference corpus (specified
via a corpus element). Attributes for the vp ele-
ment are vshare (variation share), ishare (in-
stance share), spread, and grs (see again Sec-
tion 3). In Figure 1 we provide a snapshot of what
the repository looks like.
The POS sequence of a MWE in the original lex-
icon can be matched to the same value in the repos-
itory, and so can the expression type, which should
also be specified in the lexicon, so that the relative
variation patterns can be inherited by the MWE.
References
M. Baroni, S. Bernardini, F. Comastri, L. Piccioni,
A. Volpi, G. Aston, and M. Mazzoleni. 2004. In-
troducing the La Repubblica Corpus: A Large, An-
notated, TEI(XML)-Compliant Corpus of Newspaper
Italian. In Proceedings of LREC 2004, pages 1771?
1774.
F. Bond, A. Korhonen, D. McCarthy, and A. Villavicen-
cio. 2005. Multiword Expressions: Having a crack at
a hard nut. Computer Speech and Language, 19:365?
367.
N. Calzolari, C. J. Fillmore, R. Grishman, N. Ide,
A. Lenci, C. MacLeod, and A. Zampolli. 2002. To-
wards best practice for multiword expressions in com-
putational lexicons. In Proceedings of LREC 2002,
pages 1934?1940.
Stefan Evert and Brigitte Krenn. 2005. Using small ran-
dom samples for the manual evaluation of statistical
association measures. Computer Speech & Language,
19(4):450?466. Special issue on Multiword Expres-
sions.
Adele Goldberg. 2003. Constructions: a new theoretical
approach to language. Trends in Cognitive Sciences,
7(5):219?224.
Nicole Gre?goire. 2010. DuELME: a Dutch electronic
lexicon of multiword expressions. Language Re-
sources and Evaluation, 44(1-2):23?39.
Francesca Masini. 2007. Parole sintagmatiche in ital-
iano. Ph.D. thesis, University of Roma Tre, Rome,
Italy.
Malvina Nissim and Andrea Zaninello. 2011. A quan-
titative study on the morphology of Italian multiword
expressions. Lingue e Linguaggio, X:283?300.
Malvina Nissim and Andrea Zaninello. 2013. Mod-
elling the internal variability of multiword expressions
through a pattern-based method. ACM Transactions
on Speech and Language Processing, Special issue on
Multiword Expressions.
104
<corpus name="larepubblica">
<expression type="verbal">
<patterns>
<pattern pos="VER:infi_ARTPRE_NOUN" mwes="55" variations="671" instances="9046">
<vp vshare="0.896" ishare"0.740" spread="42.1" grs="9.109">flex fix fix</vp>
<vp vshare="0.082" ishare"0.256" spread="11.1" grs="7.127">fix fix fix</vp>
<vp vshare="0.016" ishare"0.003" spread="2.6" grs="0.026">flex flex fix</vp>
<vp vshare="0.006" ishare"0.000" spread="1.2" grs="0.004">flex flex flex</vp>
</pattern>
<pattern pos="VER:infi:cli_ART_NOUN" mwes="41" variations="600" instances="3703">
<vp vshare="0.065" ishare"0.267" spread="25.3" grs="4.203">fix fix fix</vp>
<vp vshare="0.893" ishare"0.723" spread="5" grs="4.040">flex fix fix</vp>
<vp vshare="0.030" ishare"0.008" spread="1.6" grs="0.029">flex flex flex</vp>
<vp vshare="0.005" ishare"0.000" spread="1" grs="0.003">flex flex fix</vp>
<vp vshare="0.003" ishare"0.000" spread="1" grs="0.002">fix flex flex</vp>
<vp vshare="0.002" ishare"0.000" spread="2" grs="0.002">fix flex fix</vp>
<vp vshare="0.002" ishare"0.000" spread="1" grs="0.000">flex fix flex</vp>
</pattern>
<pattern ...>
...
</pattern>
</patterns>
</expression>
<expression type="adverbial">
<patterns>
<pattern pos="PRE_NOUN_ADJ" mwes="53" variations="79" instances="12202">
<vp vshare="0.671" ishare"0.989" spread="227.7" grs="189.0">fix fix fix</vp>
<vp vshare="0.076" ishare"0.007" spread="14" grs="0.580">fix flex flex</vp>
<vp vshare="0.190" ishare"0.004" spread="2.9" grs="0.284">fix fix flex</vp>
<vp vshare="0.063" ishare"0.000" spread="1" grs="0.032">fix fix fix</vp>
</pattern>
...
</patterns>
</expression>
<expression type="adjectival">
<patterns>
...
</patterns>
</expression>
</corpus>
Figure 1: Snapshot of the XML repository of variation patterns over POS patterns, listed by expression types. See text
for element and attribute explanation..
J. Odijk. 2004. A proposed standard for the lexical rep-
resentation of idioms. In Proceedings of EURALEX
2004, pages 153?164.
Emanuele Pianta and Luisa Bentivogli. 2004. Anno-
tating discontinuous structures in xml: the multiword
case. In Proceedings of LREC Workshop on XML-
based Richly Annotated Corpora, pages 30?37, Lis-
bon, Portugal.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for multi-
word expression identification. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, Proceedings of the Interna-
tional Conference on Language Resources and Eval-
uation, LREC 2010, 17-23 May 2010, Valletta, Malta.
European Language Resources Association.
Marion Weller and Ulrich Heid. 2010. Extraction of Ger-
man Multiword Expressions from Parsed Corpora Us-
ing Context Features. In Proceedings of the seventh
conference on International Language Resources and
Evaluation (LREC 2010), pages 3195?3201. European
Language Resources Association.
Andrea Zaninello and Malvina Nissim. 2010. Creation
of Lexical Resources for a Characterisation of Multi-
word Expressions in Italian. In Proceedings of LREC
2010, pages 655?661, Valletta, Malta, may. European
Language Resources Association (ELRA).
105
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 100?107,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sentiment analysis on Italian tweets
Valerio Basile
University of Groningen
v.basile@rug.nl
Malvina Nissim
University of Bologna
malvina.nissim@unibo.it
Abstract
We describe TWITA, the first corpus of Italian
tweets, which is created via a completely au-
tomatic procedure, portable to any other lan-
guage. We experiment with sentiment anal-
ysis on two datasets from TWITA: a generic
collection and a topic-specific collection. The
only resource we use is a polarity lexicon,
which we obtain by automatically matching
three existing resources thereby creating the
first polarity database for Italian. We observe
that albeit shallow, our simple system captures
polarity distinctions matching reasonably well
the classification done by human judges, with
differences in performance across polarity val-
ues and on the two sets.
1 Introduction
Twitter is an online service which lets subscribers
post short messages (?tweets?) of up to 140 charac-
ters about anything, from good-morning messages
to political stands.
Such micro texts are a precious mine for grasping
opinions of groups of people, possibly about a spe-
cific topic or product. This is even more so, since
tweets are associated to several kinds of meta-data,
such as geographical coordinates of where the tweet
was sent from, the id of the sender, the time of the
day ? information that can be combined with text
analysis to yield an even more accurate picture of
who says what, and where, and when. The last years
have seen an enormous increase in research on de-
veloping opinion mining systems of various sorts
applying Natural Language Processing techniques.
Systems range from simple lookups in polarity or
affection resources, i.e. databases where a polarity
score (usually positive, negative, or neutral) is asso-
ciated to terms, to more sophisticated models built
through supervised, unsupervised, and distant learn-
ing involving various sets of features (Liu, 2012).
Tweets are produced in many languages, but most
work on sentiment analysis is done for English (even
independently of Twitter). This is also due to the
availability of tools and resources. Developing sys-
tems able to perform sentiment analysis for tweets in
a new language requires at least a corpus of tweets
and a polarity lexicon, both of which, to the best of
our knowledge, do not exist yet for Italian.
This paper offers three main contributions in this
respect. First, we present the first of corpus of tweets
for Italian, built in such a way that makes it possi-
ble to use the exact same strategy to build similar
resources for other languages without any manual
intervention (Section 2). Second, we derive a polar-
ity lexicon for Italian, organised by senses, also us-
ing a fully automatic strategy which can replicated
to obtain such a resource for other languages (Sec-
tion 3.1). Third, we use the lexicon to automatically
assign polarity to two subsets of the tweets in our
corpus, and evaluate results against manually anno-
tated data (Sections 3.2?3.4).
2 Corpus creation
We collected one year worth of tweets, from Febru-
ary 2012 to February 2013, using the Twitter fil-
ter API1 and a language recognition strategy which
1https://dev.twitter.com/docs/api/1/
post/statuses/filter
100
we describe below. The collection, named TWITA,
consists of about 100 million tweets in Italian en-
riched with several kinds of meta-information, such
as the time-stamp, geographic coordinates (when-
ever present), and the username of the twitter. Addi-
tionally, we used off-the-shelf language processing
tools to tokenise all tweets and tag them with part-
of-speech information.
2.1 Language detection
One rather straightforward way of creating a corpus
of language-specific tweets is to retrieve tweets via
the Twitter API which are matched with strongly
language-representative words. Tjong Kim Sang
and Bos (2012) compile their list of highly typ-
ical Dutch terms manually to retrieve Dutch-only
tweets. While we also use a list of strongly repre-
sentative Italian words, we obtain such list automat-
ically. This has the advantage of making the proce-
dure more objective and fully portable to any other
language for which large reference corpora are avail-
able. Indeed, we relied on frequency information de-
rived from ItWac, a large corpus of Italian (Baroni et
al., 2009), and exploited Google n-grams to rule out
cross-language homographs. For boosting precision,
we also used the publicly available language recog-
nition software langid.py (Lui and Baldwin, 2012).
The details of the procedure are given below:
1. extract the 1.000 most frequent lemmas from
ItWaC;
2. extract tweets matched by the selected repre-
sentative words and detect the language using a
freely available software;2
3. filter out the terms in the original list which
have high frequency in a conflicting language.
Frequency is obtained from Google N-grams;
4. use high frequency terms in the resulting
cleaner list to search the Twitter API.
The 20 top terms which were then used to match
Italian-only tweets are: vita Roma forza alla quanto
amore Milano Italia fare grazie della anche peri-
odo bene scuola dopo tutto ancora tutti fatto. In the
2Doing so, we identify other languages that share charac-
ter sequences with Italian. The large majority of tweets in the
first search were identified as Portuguese, followed by English,
Spanish and then Italian.
extraction, we preserved metadata about user, time,
and geographical coordinates whenever available.
Both precision and recall of this method are hard
to assess. We cannot know how many tweets that
are in fact Italian we?re actually missing, but the
amount of data we can in any case collect is so high
that the issue is not so relevant.3 Precision is more
important, but manual checking would be too time-
consuming. We inspected a subset of 1,000 tweets
and registered a precision of 99.7% (three very short
tweets were found to be in Spanish). Considering
that roughly 2.5% of the tweets also include the ge-
ographical coordinates of the device used to send the
message, we assessed an approximate precision in-
directly. We plotted a one million tweets randomly
chosen from our corpus and obtained the map shown
in Figure 1 (the map is clipped to the Europe area for
better identifiability). We can see that Italy is clearly
outlined, indicating that precision, though not quan-
tifiable, is likely to be satisfactory.
Figure 1: Map derived by plotting geo-coordinates of
tweets obtained via our language-detection procedure.
2.2 Processing
The collected tweets have then been enriched with
token-level, POS-tags, and lemma information.
Meta-information was excluded from processing.
So for POS-tagging and lemmatisation we substi-
tuted hashtags, mentions (strings of the form @user-
3This is because we extract generic tweets. Should one want
to extract topic-specific tweets, a more targeted list of charac-
terising terms should be used.
101
name referring to a specific user) and URLs with a
generic label. All the original information was re-
inserted after processing. The tweets were tokenised
with the UCTO rule-based tokeniser4 and then POS-
tagged using TreeTagger (Schmid, 1994) with the
provided Italian parameter file. Finally, we used the
morphological analyser morph-it! (Zanchetta and
Baroni, 2005) for lemmatisation.
3 Sentiment Analysis
The aim of sentiment analysis (or opinion mining) is
detecting someone?s attitude, whether positive, neu-
tral, or negative, on the basis of some utterance or
text s/he has produced. While a first step would be
determining whether a statement is objective or sub-
jective, and then only in the latter case identify its
polarity, it is often the case that only the second task
is performed, thereby also collapsing objective state-
ments and a neutral attitude.
In SemEval-2013?s shared task on ?Sentiment
Analysis in Twitter?5 (in English tweets), which is
currently underway, systems must detect (i) polar-
ity of a given word in a tweet, and (ii) polarity of
the whole tweet, in terms of positive, negative, or
neutral. This is also what we set to do for Italian.
We actually focus on (ii) in the sense that we do not
evaluate (i), but we use and combine each word?s
polarity to obtain the tweet?s overall polarity.
Several avenues have been explored for polar-
ity detection. The simplest route is detecting the
presence of specific words which are known to ex-
press a positive, negative or neutral feeling. For
example, O?Connor et al (2010) use a lexicon-
projection strategy yielding predictions which sig-
nificantly correlate with polls regarding ratings of
Obama. While it is clear that deeper linguistic anal-
ysis should be performed for better results (Pang and
Lee, 2008), accurate processing is rather hard on
texts such as tweets, which are short, rich in abbrevi-
ations and intra-genre expressions, and often syntac-
tically ill-formed. Additionally, existing tools for the
syntactic analysis of Italian, such as the DeSR parser
(Attardi et al, 2009), might not be robust enough for
processing such texts.
Exploiting information coming from a polarity
4http://ilk.uvt.nl/ucto/
5www.cs.york.ac.uk/semeval-2013/task2/.
lexicon, we developed a simple system which as-
signs to a given tweet one of three possible values:
positive, neutral or negative. The only input to the
system is the prior polarity coded in the lexicon per
word sense. We experiment with several ways of
combining all the polarities obtained for each word
(sense) in a given tweet. Performance is evaluated
against manually annotated tweets.
3.1 Polarity lexicon for Italian
Most polarity detection systems make use, in some
way, of an affection lexicon, i.e. a language-specific
resource which assigns a negative or positive prior
polarity to terms. Such resources have been built by
hand or derived automatically (Wilson et al, 2005;
Wiebe and Mihalcea, 2006; Esuli and Sebastiani,
2006; Taboada et al, 2011, e.g.). To our knowl-
edge, there isn?t such a resource already available
for Italian. Besides hand-crafting, there have been
proposals for creating resources for new languages
in a semi-automatic fashion, using manually anno-
tated sets of seeds (Pitel and Grefenstette, 2008),
or exploiting twitter emoticons directly (Pak and
Paroubek, 2011). Rather than creating a new po-
larity lexicon from scratch, we exploit three exist-
ing resources, namely MultiWordNet (Pianta et al,
2002), SentiWordNet (Esuli and Sebastiani, 2006;
Baccianella et al, 2010), and WordNet itself (Fell-
baum, 1998) to obtain an annotated lexicon of senses
for Italian. Basically, we port the SentiWordNet an-
notation to the Italian portion of MultiWordNet, and
we do so in a completely automatic fashion.
Our starting point is SentiWordNet, a version
of WordNet where the independent values positive,
negative, and objective are associated to 117,660
synsets, each value in the zero-one interval. Mul-
tiWordNet is a resource which aligns Italian and En-
glish synsets and can thus be used to transfer polar-
ity information associated to English synsets in Sen-
tiWordNet to Italian synsets. One obstacle is that
while SentiWordNet refers to WordNet 3.0, Multi-
WordNet?s alignment holds for WordNet 1.6, and
synset reference indexes are not plainly carried over
from one version to the next. We filled this gap using
an automatically produced mapping between synsets
of Wordnet versions 1.6 and 3.0 (Daud et al, 2000),
making it possible to obtain SentiWordNet annota-
tion for the Italian synsets of MultiWordNet. The
102
coverage of our resource is however rather low com-
pared to the English version, and this is due to the
alignment procedure which must exploit an earlier
version of the resource. The number of synsets is
less than one third of that of SentiWordNet.
3.2 Polarity assignment
Given a tweet, our system assigns a polarity score to
each of its tokens by matching them to the entries in
SentiWordNet. Only matches of the correct POS are
allowed. The polarity score of the complete tweet is
given by the sum of the polarity scores of its tokens.
Polarity is associated to synsets, and the same
term can occur in more than one synset. One option
would be to perform word sense disambiguation and
only pick the polarity score associated with the in-
tended sense. However, the structure of tweets and
the tools available for Italian do not make this op-
tion actually feasible, although we might investigate
it in the future. As a working solution, we compute
the positive and negative scores for a term occurring
in a tweet as the means of the positive and negative
scores of all synsets to which the lemma belongs to
in our lexical resource. The resulting polarity score
of a lemma is the difference between its positive and
negative scores. Whenever a lemma is not found in
the database, it is given a polarity score of 0.
One underlying assumption to this approach is
that the different senses of a given word have simi-
lar sentiment scores. However, because this assump-
tion might not be true in all cases, we introduce the
concept of ?polypathy?, which is the characterising
feature of a term exhibiting high variance of polarity
scores across its synsets. The polypathy of a lemma
is calculated as the standard deviation of the polar-
ity scores of the possible senses. This information
can be used to remove highly polypathic words from
the computation of the polarity of a complete tweet,
for instance by discarding the tokens with a poly-
pathy higher than a certain threshold. In particular,
for the experiments described in this paper, a thresh-
old of 0.5 has been empirically determined. To give
an idea, among the most polypathic words in Senti-
WordNet we found weird (.62), stunning (.61), con-
flicting (.56), terrific (.56).
Taboada et al (2011) also use SentiWordNet for
polarity detection, either taking the first sense of a
term (the most frequent in WordNet) or taking the
average across senses, as we also do ? although
we also add the polypathy-aware strategy. We can-
not use the first-sense strategy because through the
alignment procedure senses are not ranked accord-
ing to frequency anymore.
3.3 Gold standard
For evaluating the system performance we created
two gold standard sets, both annotated by three inde-
pendent native-speakers, who were given very sim-
ple and basic instructions and performed the anno-
tation via a web-based interface. The value to be
assigned to each tweet is one out of positive, neu-
tral, or negative. As mentioned, the neutral value
includes both objective statements as well as subjec-
tive statements where the twitter?s position is neutral
or equally positive and negative at the same time (see
also (Esuli and Sebastiani, 2007)).
All data selected for annotation comes from
TWITA. The first dataset consists of 1,000 ran-
domly selected tweets. The second dataset is topic-
oriented, i.e. we randomly extracted 1,000 tweets
from all those containing a given topic. Topic-
oriented, or target-dependent (Jiang et al, 2011),
classification involves detecting opinions about a
specific target rather than detecting the more gen-
eral opinion expressed in a given tweet. We identify
a topic through a given hashtag, and in this experi-
ment we chose the tag ?Grillo?, the leader of an Ital-
ian political movement. While in the first set the an-
notators were asked to assign a polarity value to the
message of the tweet as a whole, in the second set
the value was to be assigned to the author?s opinion
concerning the hashtag, in this case Beppe Grillo.
This is a relevant distinction, since it can happen
that the tweet is, say, very negative about someone
else while being positive or neutral about Grillo at
the same time. For example, the tweet in (1), ex-
presses a negative opinion about Vendola, another
Italian politician, but is remaining quite neutral to-
wards Grillo, the target of the annotation exercise.
(1) #Vendola da` del #populista a #Grillo e` una
barzelletta o ancora non si e` accorto che il
#comunismo e` basato sul populismo?
Thus, in the topic-specific set we operate a more
subtle distinction when assigning polarity, some-
103
thing which should make the task simpler for a hu-
man annotator while harder for a shallow system.
As shown in Table 1, for both sets the annotators
detected more than half of the tweets as neutral, or
they were disagreeing ? without absolute majority,
a tweet is considered neutral; however these cases
account for only 7.7% in the generic set and 6.9% in
the topic-specific set.
Table 1: Distribution of the tags assigned by the absolute
majority of the raters
set positive negative neutral
generic 94 301 605
topic-specific 293 145 562
Inter-annotator agreement was measured via Fleiss?
Kappa across three annotators. On the generic set,
we found an agreement of Kappa = 0.321, while
on the topic-specific set we found Kappa = 0.397.
This confirms our expectation that annotating topic-
specific tweets is actually an easier task. We might
also consider using more sophisticated and fine-
grained sentiment annotation schemes which have
proved to be highly reliable in the annotation of En-
glish data (Su and Markert, 2008a).
3.4 Evaluation
We ran our system on both datasets described in Sec-
tion 3.3, using all possible variations of two parame-
ters, namely all combinations of part-of-speech tags
and the application of the threshold scheme, as dis-
cussed in Section 3.2. We measure overall accuracy
as well as precision, recall, and f-score per polar-
ity value. In Tables 2 and 3, we report best scores,
and indicate in brackets the associated POS combi-
nation. For instance, in Table 2, we can read that the
recall of 0.701 for positive polarity is obtained when
the system is run without polypathy threshold and
using nouns, verbs, and adjectives (nva).
We can draw several observations from these re-
sults. First, a fully automatic approach that lever-
ages existing lexical resources performs better than
a wild guess. Performance is boosted when highly
polypathic words are filtered out.
Second, while the system performs well at recog-
nising especially neutral but also positive polarity,
it is really bad at detecting negative polarity. Es-
pecially in the topic-specific set, the system assigns
Table 2: Best results on the generic set. In brackets POS
combination: (n)oun, (v)erb, (a)djective, adve(r)b.
without polypathy threshold, best accuracy: 0.505 (a)
positive negative neutral
best precision 0.440 (r) 0.195 (v) 0.664 (nar)
best recall 0.701 (nva) 0.532 (var) 0.669 (a)
best F-score 0.485 (nvar) 0.262 (vr) 0.647 (a)
with polypathy threshold, best accuracy: 0.554 (r)
positive negative neutral
best precision 0.420 (r) 0.233 (v) 0.685 (nar)
best recall 0.714 (nvar) 0.457 (var) 0.785 (r)
best F-score 0.492 (nar) 0.296 (vr) 0.698 (r)
Table 3: Best results on the topic-specific set. In brackets
POS combination: (n)oun, (v)erb, (a)djective, adve(r)b.
without polypathy threshold, best accuracy: 0.487 (r)
positive negative neutral
best precision 0.164 (a) 0.412 (a) 0.617 (nar)
best recall 0.593 (nva) 0.150 (nr) 0.724 (a)
best f-score 0.251 (nv) 0.213 (nr) 0.637 (a)
with polypathy threshold, best accuracy: 0.514 (r)
positive negative neutral
best precision 0.163 (nvar) 0.414 (a) 0.623 (nar)
best recall 0.593 (nvar) 0.106 (nar) 0.829 (r)
best f-score 0.256 (nvar) 0.166 (nar) 0.676 (r)
too many positive labels in place of negative ones,
causing at the same time positive?s precision and
negative?s recall to drop. We believe there are two
explanations for this. The first one is the ?positive-
bias? of SentiWordNet, as observed by Taboada et
al. (2011), which causes limited performance in the
identification of negative polarity. The second one
is that we do not use any syntactic clues, such as for
detecting negated statements. Including some strat-
egy for dealing with this should improve recognition
of negative opinions, too.
Third, the lower performance on the topic-specific
dataset confirms the intuition that this task is harder,
mainly because we operate a more subtle distinc-
tion when assigning a polarity label as we refer to
one specific subject. Deeper linguistic analysis, such
as dependency parsing, might help, as only certain
words would result as related to the intended target
while others wouldn?t.
As far as parts of speech are concerned, there
is a tendency for adverbs to be good indicators to-
wards overall accuracy, and best scores are usually
obtained exploiting adjectives and/or adverbs.
104
4 Related work
We have already discussed some related work con-
cerning corpus creation, the development of an
affection lexicon, and the use of such polarity-
annotated resources for sentiment analysis (Sec-
tion 3). As for results, because this is the first experi-
ment on detecting polarity in Italian tweets, compar-
ing performance is not straightforward. Most work
on sentiment analysis in tweets is on English, and al-
though there exist relatively complex systems based
on statistical models, just using information from a
polarity resource is rather common. Su and Markert
(2008b) test SentiWordNet for assigning a subjec-
tivity judgement to word senses on a gold standard
corpus, observing an accuracy of 75.3%. Given that
SentiWordNet is the automatic expansion over a set
of manually annotated seeds, at word-level, this can
be considered as an upper bound in sense subjectiv-
ity detection. Taboada et al (2011) offer a survey of
lexicon-based methods which are evaluated on ad-
jectives only, by measuring overall accuracy against
a manually annotated set of words. Using Senti-
WordNet in a lexicon-projection fashion yields an
accuracy of 61.47% under best settings. These are
however scores on single words rather than whole
sentences or microtexts.
Considering that we assign polarity to tweets
rather than single words, and that in the creation of
our resource via automatic alignment we lose more
than two thirds of the original synsets (see Sec-
tion 3.1), our results are promising. They are also
not that distant from results reported by Agarwal et
al. (2011), whose best system, a combination of un-
igrams and the best set of features, achieves an ac-
curacy of 60.50% on a three-way classification like
ours, evaluated against a manually annotated set of
English tweets. Best f-scores reported for positive,
negative, and neutral are comprised between 59%
and 62%. Similar results are obtained by Pak and
Paroubek (2010), who train a classifier on automati-
cally tagged data, and evaluate their model on about
200 English tweets. Best reported f-score on a three-
way polarity assignment is just over 60%.
5 Conclusions and future work
We have presented the first corpus of Italian tweets
obtained in a completely automatic fashion, the first
polarity lexicon for Italian, and the first experiment
on sentiment analysis on Italian tweets using these
two resources. Both the corpus and the lexicon are
as of now unique resources for Italian, and were pro-
duced in a way which is completely portable to other
languages. In compliance with licensing terms of the
sources we have used, our resources are made avail-
able for research purposes after reviewing.
Simply projecting the affection lexicon, using two
different polarity scoring methods, we experimented
with detecting a generic sentiment expressed in a mi-
crotext, and detecting the twitter?s opinion on a spe-
cific topic. As expected, we found that topic-specific
classification is harder for an automatic system as it
must discern what is said about the topic itself and
what is said more generally or about another entity
mentioned in the text.
Indeed, this contribution can be seen as a first
step towards polarity detection in Italian tweets. The
information we obtain from SentiWordNet and the
ways we combine it could obviously be used as fea-
ture in a learning setting. Other sources of infor-
mation, to be used in combination with our polarity
scores or integrated in a statistical model, are the so-
called noisy labels, namely strings (such as emoti-
cons or specific hashtags (Go et al, 2009; Davi-
dov et al, 2010)) that can be taken as positive or
negative polarity indicators as such. Speriosu et al
(2011) have shown that training a maximum entropy
classier using noisy labels as class predictors in the
training set yields an improvement of about three
percentage points over a lexicon-based prediction.
Another important issue to deal with is figurative
language. During manual annotation we have en-
countered many cases of irony or sarcasm, which is a
phenomenon that must be obviously tackled. There
have been attempts at identifying it automatically in
the context of tweets (Gonza?lez-Iba?n?ez et al, 2011),
and we plan to explore this issue in future work.
Finally, the co-presence of meta and linguistic
information allows for a wide range of linguistic
queries and statistical analyses on the whole of the
corpus, also independently of sentiment informa-
tion, of course. For example, correlations between
parts-of-speech and polarity have been found (Pak
and Paroubek, 2010), and one could expect also
correlations with sentiment and time of the day, or
month of the year, and so on.
105
Acknowledgments
We would like to thank Manuela, Marcella e Silvia
for their help with annotation, and the reviewers for
their useful comments. All errors remain our own.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ?11, pages 30?38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Giuseppe Attardi, Felice Dell?Orletta, Maria Simi, and
Joseph Turian. 2009. Accurate dependency parsing
with a stacked multilayer perceptron. In Proceeding
of Evalita 2009, LNCS. Springer.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Nicoletta Calzolari et al, editor, Proceedings of LREC
2010.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Jordi Daud, Llus Padr, and German Rigau. 2000. Map-
ping wordnets using structural information. In 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?2000)., Hong Kong.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 241?249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417?422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
424?431, Prague, Czech Republic, June. Association
for Computational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
sentiment analysis using distant supervision. http:
//cs.wmich.edu/?tllake/fileshare/
TwitterDistantSupervision09.pdf.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 581?586, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 151?160, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In ACL (Sys-
tem Demonstrations), pages 25?30. The Association
for Computer Linguistics.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen and
Samuel Gosling, editors, Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM 2010, Washington, DC, USA, May
23-26. The AAAI Press.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Nicoletta Calzolari et al, editor, Proceedings of the
International Conference on Language Resources and
Evaluation, LREC 2010, 17-23 May 2010, Valletta,
Malta. European Language Resources Association.
Alexander Pak and Patrick Paroubek. 2011. Twitter for
sentiment analysis: When language resources are not
available. 23rd International Workshop on Database
and Expert Systems Applications, 0:111?115.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In Proceedings of the First In-
ternational Conference on Global WordNet, pages 21?
25.
Guillaume Pitel and Gregory Grefenstette. 2008. Semi-
automatic building method for a multidimensional af-
fect dictionary for a new language. In Proceedings of
LREC 2008.
106
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First workshop on
Unsupervised Learning in NLP, pages 53?63, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Fangzhong Su and Katja Markert. 2008a. Eliciting sub-
jectivity and polarity judgements on word senses. In
Proceedings of COLING 2008 Workshop on Human
Judgements in Computational Linguistics, Manch-
ester, UK.
Fangzhong Su and Katja Markert. 2008b. From words
to senses: A case study of subjectivity recognition. In
Donia Scott and Hans Uszkoreit, editors, Proceedings
of COLING 2008, Manchester, UK, pages 825?832.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-based
methods for sentiment analysis. Comput. Linguist.,
37(2):267?307, June.
Erik Tjong Kim Sang and Johan Bos. 2012. Predicting
the 2011 dutch senate election results with twitter. In
Proceedings of the Workshop on Semantic Analysis in
Social Media, pages 53?60, Avignon, France, April.
Association for Computational Linguistics.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Nicoletta Calzolari, Claire Cardie,
and Pierre Isabelle, editors, ACL. The Association for
Computer Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing Conference, 6-8 October, Van-
couver, British Columbia, Canada.
Eros Zanchetta and Marco Baroni. 2005. Morph-it! a
free corpus-based morphological resource for the ital-
ian language. In Proceedings of Corpus Linguistics
2005.
107
