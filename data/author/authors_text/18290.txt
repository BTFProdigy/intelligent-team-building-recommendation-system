Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1786?1791,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Comparing Representations of Semantic Roles for
String-To-Tree Decoding
Marzieh Bazrafshan and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We introduce new features for incorpo-
rating semantic predicate-argument struc-
tures in machine translation (MT). The
methods focus on the completeness of the
semantic structures of the translations, as
well as the order of the translated seman-
tic roles. We experiment with translation
rules which contain the core arguments
for the predicates in the source side of a
MT system, and observe that using these
rules significantly improves the translation
quality. We also present a new semantic
feature that resembles a language model.
Our results show that the language model
feature can also significantly improve MT
results.
1 Introduction
In recent years, there have been increasing ef-
forts to incorporate semantics in statistical ma-
chine translation (SMT), and the use of predicate-
argument structures has provided promising im-
provements in translation quality. Wu and Fung
(2009) showed that shallow semantic parsing can
improve the translation quality in a machine trans-
lation system. They introduced a two step model,
in which they used a semantic parser to rerank
the translation hypotheses of a phrase-based sys-
tem. Liu and Gildea (2010) used semantic fea-
tures for a tree-to-string syntax based SMT sys-
tem. Their features modeled deletion and reorder-
ing for source side semantic roles, and they im-
proved the translation quality. Xiong et al. (2012)
incorporated the semantic structures into phrase-
based SMT by adding syntactic and semantic fea-
tures to their translation model. They proposed
two discriminative models which included fea-
tures for predicate translation and argument re-
ordering from source to target side. Bazrafshan
and Gildea (2013) used semantic structures in
a string-to-tree translation system by extracting
translation rules enriched with semantic informa-
tion, and showed that this can improve the trans-
lation quality. Li et al. (2013) used predicate-
argument structure reordering models for hierar-
chical phrase-based translation, and they used lin-
guistically motivated constraints for phrase trans-
lation.
In this paper, we experiment with methods for
incorporating semantics in a string-to-tree MT
system. These methods are designed to model the
order of translation, as well as the completeness
of the semantic structures. We extract translation
rules that include the complete semantic structure
in the source side, and compare that with using
semantic rules for the target side predicates. We
present a method for modeling the order of seman-
tic role sequences that appear spread across multi-
ple syntax-based translation rules, in order to over-
come the problem that a rule representing the en-
tire semantic structure of a predicate is often too
large and too specific to apply to new sentences
during decoding. For this method, we compare the
verb-specific roles of PropBank and the more gen-
eral thematic roles of VerbNet.
These essential arguments of a verbal predicate
are called the core arguments. Standard syntax-
based MT is incapable of ensuring that the tar-
get translation includes all of the core arguments
of a predicate that appear in the source sentence.
To encourage the translation of the likely core ar-
guments, we follow the work of Bazrafshan and
Gildea (2013), who use special translation rules
with complete semantic structures of the predi-
cates in the target side of their MT system. Each
of these rules includes a predicate and all of its
core arguments. Instead of incorporating only the
target side semantic rules, we extract the special
rules for both the source and the target sides, and
compare the effectiveness of adding these rules to
1786
S-8
NP-7-ARG1
1
victimized by
NP-7-ARG0
2
NP-7-ARG1
1 ? NP-7-ARG0 2
Figure 1: A complete semantic rule (Bazrafshan
and Gildea (2013)).
the system separately and simultaneously.
Besides the completeness of the arguments, it is
also important for the arguments to appear in the
correct order. Our second method is designed to
encourage correct order of translation for both the
core and the non-core roles in the target sentence.
We designed a new feature that resembles the lan-
guage model feature in a standard MT system. We
train a n-gram language model on sequences of se-
mantic roles, by treating the semantic roles as the
words in what we call the semantic language. Our
experimental results show that the language model
feature significantly improves translation quality.
Semantic Role Labeling (SRL): We use se-
mantic role labelers to annotate the training data
that we use to extract the translation rules. For tar-
get side SRL, the role labels are attached to the
nonterminal nodes in the syntactic parse of each
sentence. For source side SRL, the labels annotate
the spans from the source sentence that they cover.
We train our semantic role labeler using two differ-
ent standards: Propbank (Palmer et al., 2005) and
VerbNet (Kipper Schuler, 2005).
PropBank annotates the Penn Treebank with
predicate-argument structures.It use generic labels
(such as Arg0, Arg1, etc.) which are defined
specifically for each verb. We trained a semantic
role labeler on the annotated Penn Treebank data
and used the classifier to tag our training data.
VerbNet is a verb lexicon that categorizes En-
glish verbs into hierarchical classes, and annotates
them with thematic roles for the arguments that
they accept. Since the thematic roles use more
meaningful labels (e.g. Agent, Patient, etc.), a lan-
guage model trained on VerbNet labels may be
more likely to generalize across verbs than one
trained on PropBank labels. It may also provide
more information, since VerbNet has a larger set
of labels than PropBank. To train the semantic
role labeler on VerbNet, we used the mappings
A? BC c
0
[B, i, j] c
1
[C, j, k] c
2
[A, i, k] c
0
+ c
1
+ c
2
Figure 2: A deduction step in our baseline decoder
provided by the SemLink project (Palmer, 2009)
to annotate the Penn Treebank with the VerbNet
roles. These mappings map the roles in PropBank
to the thematic roles of VerbNet. When there is no
mapping for a role, we keep the role from Prop-
bank.
2 Using Semantics in Machine
Translation
In this section, we present our techniques for in-
corporating semantics inMT: source side semantic
rules, and the semantic language model.
2.1 Source Side Semantic Rules
Bazrafshan and Gildea (2013) extracted transla-
tion rules that included a predicate and all of its
arguments from the target side, and added those
rules to the baseline rules of their string-to-tree
MT system. Figure 1 shows an example of such
rules, which we refer to as complete semantic
rules. The new rules encourage the decoder to
generate translations that include all of the seman-
tic roles that appear in the source sentence.
In this paper, we use the same idea to extract
rules from the semantic structures of the source
side. The complete semantic rules consist of the
smallest fragments of the combination of GHKM
(Galley et al., 2004) rules that include one pred-
icate and all of its core arguments that appear in
the sentence. Rather than keeping the predicate
and argument labels attached to the non-terminals,
we remove those labels from our extracted seman-
tic rules, to keep the non-terminals in the semantic
rules consistent with the non-terminals of the base-
line GHKM rules. This is also important when us-
ing both the source and the target semantic rules
(i.e. Chinese and English rules), as it has been
shown that there are cross lingual mismatches be-
tween Chinese and English semantic roles in bilin-
gual sentences (Fung et al., 2007).
We extract a complete semantic rule for each
verbal predicate of each sentence pair in the train-
ing data. To extract the target side complete se-
mantic rules, using the target side SRL anno-
1787
A? BC to space c
0
(x1 x2 Destination)
[B, i, j, (Agent, )] c
1
[C, j, k, (PRED bring, Theme, )] c
2
[A, i, k, (Agent, PRED bring,-*-, Theme, Destination)] c
0
+ c
1
+ c
+
+ LMcost(Agent, PRED bring,-*-, Theme, Destination)
Figure 3: A deduction step in the semantic language model method.
tated training data, we follow the general GHKM
method, and modify it to ensure that each fron-
tier node (Galley et al., 2004) in a rule includes ei-
ther all or none of the semantic role labels (i.e. the
predicate and all of its present core arguments) in
its descendants in the target side tree. The result-
ing rule then includes the predicate and all of its
arguments. We use the source side SRL annotated
training data to extract the source side semantic
rules. Since the annotations specify the spans of
the semantic roles, we extract the semantic rules
by ensuring that the span of the root (in the target
side) of the extracted rule covers all of the spans
of the roles in the predicate-argument structure.
The semantic rules are then used together with
the original GHKM rules. We add a binary feature
to distinguish the semantic rules from the rest. We
experiment with adding the semantic rules from
the source side, and compare that with adding se-
mantic rules of both the source and the target side.
In all of the experiments in this paper, we use
a string-to-tree decoder which uses a CYK style
parser (Yamada and Knight, 2002). Figure 2 de-
picts a deduction step in the baseline decoder. The
CFG rule in the first line is used to generate a
new item A with span (i, k) using items B and
C, which have spans (i, j) and (j, k) respectively.
The cost of each item is shown on the right. For
experimenting with complete semantic rules, in
addition having more rules, the only other modi-
fication made to the baseline system is extending
the feature vector to include the new feature. We
do not modify the decoder in any significant way.
2.2 Semantic Language Model
The semantic language model is designed to en-
courage the correct order of translation for the se-
mantic roles. While the complete translation rules
of Section 2.1 contain the order of the translation
for core semantic roles, they do not include the
non-core semantic roles, that is, semantic roles
which are not essential for the verbal predicates,
but do contribute to the meaning of the predicate.
In addition, the semantic LM can help in cases
where no specific complete semantic rule can ap-
ply, which makes the system more flexible.
The semantic language model resembles a reg-
ular language model, but instead of words, it de-
fines a probability distribution over sequences of
semantic roles. For this method we also use a se-
mantic role labeler on our training data, and use
the labeled data to train a tri-gram semantic lan-
guage model.
The rules are extracted using the baseline rule
extraction method. As opposed to the previous
method, the rules for this method are not derived
by combining GHKM rules, but rather are reg-
ular GHKM rules which are annotated with se-
mantic roles. We make a new field in each rule
to keep the ordered list of the semantic roles in
that rule. We also include the nonterminals of the
right-hand-side of the rule in that ordered list, to
be able to substitute the semantic roles from the
input translation items in the correct order. The
decoder uses this new field to save the semantic
roles in the translation items, and propagates the
semantic LM states in the same way that the reg-
ular language model states are propagated by the
decoder.
We define a new feature for the semantic lan-
guage model, and score the semantic states in each
translation item, again analogously to a regular
language model. Figure 3 depicts how the de-
duction for this method is different from our base-
line. In this example, the semantic roles ?Agent?,
?PRED bring? and ?Theme? come from the input
items, and the role ?Destination? (which tags the
terminals ?to space?) comes from the translation
rule.
We stemmed the verbs for training this feature,
and also annotated our rules with stemmed verbal
predicates. The stemming helps the training since
the argument types of a verb are normally inde-
pendent of its inflected variants.
1788
avg. BLEU Score
dev test p-value
Baseline 26.01 25.00 -
Source 26.44 25.17 0.048
Source and target 26.39 25.63 < 10
?10
Propbank LM 26.38 25.08 0.108
VerbNet LM 26.58 25.23 0.025
Table 1: Comparisons of the methods with the
baseline. The BLEU scores are calculated on the
top 3 results from 15 runs MERT for each experi-
ments. The p-values are calculated by comparing
each method against the baseline system.
3 Experiments
3.1 Experimental Setup
The data that we used for training the MT sys-
tem was a Chinese-English corpus derived from
newswire text from LDC.
1
The data consists of
250K sentences, which is 6.3M words in the En-
glish side. Our language model was trained on
the English side of the entire data, which consisted
of 1.65M sentences (39.3M words). Our develop-
ment and test sets are from the newswire portion
of NIST evaluations (2004, 2005, 2006). We used
392 sentences for the development set and 428
sentences for the test set. These sentences have
lengths smaller than 30, and they each have 4 ref-
erence translations. We used our in-house string-
to-tree decoder that uses Earley parsing. Other
than the features that we presented for our new
methods, we used a set of nine standard features.
The rules for the baseline system were extracted
using the GHKM method. Our baseline GHKM
rules also include composed rules, where larger
rules are constructed by combining two levels of
the regular GHKM rules. We exclude any unary
rules (Chung et al., 2011), and only keep rules
that have scope up to 3 (Hopkins and Langmead,
2010). For the semantic language model, we used
the SRILM package (Stolcke, 2002) and trained
a tri-gram language model with the default Good-
Turing smoothing.
Our target side semantic role labeler uses a max-
imum entropy classifier to label parsed sentences.
We used Sections 02-22 of the Penn TreeBank to
1
The data was randomly selected from the follow-
ing sources: LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,
LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2006E24, LDC2006E92, LDC2006E24
train the labeler, and sections 24 and 23 for devel-
opment set and training set respectively. The la-
beler has a precision of 90% and a recall of 88%.
We used the Chinese semantic role labeler of Wu
and Palmer (2011) for source side SRL, which
uses the LIBLINEAR (Fan et al., 2008) as a classi-
fier. Minimum Error Rate Training (MERT) (Och,
2003) was used for tuning the feature weights.
For all of our experiments, we ran 15 instances
of MERT with random initial weight vectors, and
used the weights of the top 3 results on the de-
velopment set to test the systems on the test set.
We chose to use the top 3 runs (rather than the
best run) of each system to account for the insta-
bility of MERT (Clark et al., 2011). This method
is designed to reflect the average performance of
the MT system when trained with random restarts
of MERT: we wish to discount runs in which the
optimizer is stuck in a poor region of the weight
space, but also to average across several good runs
in order not to be mislead by the high variance of
the single best run. For each of our MT systems,
we merged the results of the top 3 runs on the test
set into one file, and ran a statistical significance
test, comparing it to the merged top 3 results from
our baseline system. The 3 runs were merged by
duplicating each run 3 times, and arranging them
in the file so that the significance testing compares
each run with all the runs of the baseline. We per-
formed significance testing using paired bootstrap
resampling (Koehn, 2004). The difference is con-
sidered statistically significant if p < 0.05 using
1000 iterations of paired bootstrap resampling.
3.2 Results
Our results are shown in Table 1. The second
and the third columns contain the average BLEU
score (Papineni et al., 2002) on the top three re-
sults on the development and test sets. The fourth
column is the p-value for statistical significance
testing against the baseline. The first row shows
the results for our baseline. The second row con-
tains the results for using the source (Chinese)
side complete semantic rules of Section 2.1, and
the third row is the results for combining both
the source and the target side complete semantic
rules. As noted before, in both of these experi-
ments we also use the regular GHKM rules. The
result show that the source side complete seman-
tic rules improve the system (p = 0.048), and as
we expected, combining the source and the tar-
1789
Source Sentence ?? ,??????????????????????? .
Reference therefore , it is the international community ?s responsibility to protect the children from harms resulted
from armed conflicts .
Baseline the armed conflicts will harm the importance of the international community the responsibilities . there-
fore , from child protection
Verbet LM therefore , the importance of the international community is to protect children from the harm affected
by the armed conflicts .
Source Sentence ???????? ,??????????? ,???????? .
Reference compared with last year ?s meeting , the smell of gunpowder has disappeared in this year ?s meeting and
the two sides ? standpoints are getting closer .
Baseline disappears on gunpowder , near the stance of the two sides compared with last year ?s meeting , the
meeting of this year .
Verbet LM the smells of gunpowder has disappeared , the position in the two sides approach . compared with last
year ?s meeting , this meeting
(a) Comparison of the language model method (using VerbNet) and the baseline system.
Source Sentence ???????? ,??????????????? .
Reference scientists have boldly predicted that the british spacecraft might have been stuck in a hole .
Baseline scientists boldly expected , this vessel uk may have in the space ship in hang tung .
Semantic Rules scientists have boldly expected this vessel and the possible settlement of the space ship in hang tung .
Source Sentence ????????????????? .
Reference the us government should show goodwills to north korea ?s stand .
Baseline this position of the government of the united states to goodwill toward the dprk .
Semantic Rules this position that the us government should use goodwill toward the dprk .
(b) Comparison of the experiments with source and target side semantic rules and the baseline system.
Figure 4: Comparison of example translations from our semantic methods and the baseline system.
get side rules improves the system even more sig-
nificantly (p < 10
?10
). To measure the effect
of combining the rules, in a separate experiment
we replicated the complete semantic rules exper-
iments of Bazrafshan and Gildea (2013), and ran
statistical significance tests comparing the combi-
nation of the source and target rules with using
only the source or the target semantic rules sep-
arately. The results showed that combining the se-
mantic rules outperforms both of the experiments
that used rules from only one side (with p < 0.05
in both cases).
The results for the language model feature are
shown in the last two rows of the table. Us-
ing Propbank for language model training did not
change the system in any significant way (p =
0.108), but using VerbNet significantly improved
the results (p = 0.025). Figure 4(a) contains an
example comparing the baseline system with the
VerbNet language model. We can see how the
VerbNet language model helps the decoder trans-
late the argument in the correct order. The baseline
system has also generated the correct arguments,
but the output is in the wrong order. Figure 4(b)
compares the experiment with semantic rules of
both target and source side and the baseline sys-
tem. Translation of the word ?use? by our seman-
tic rules is a good example showing how the de-
coder uses these semantic rules to generate a more
complete predicate-argument structure.
4 Conclusions
We experimented with two techniques for incor-
porating semantics in machine translation. The
models were designed to help the decoder trans-
late semantic roles in the correct order, as well
as generating complete predicate-argument struc-
tures. We observed that using a semantic lan-
guage model can significantly improve the trans-
lations, and help the decoder to generate the se-
mantic roles in the correct order. Adding transla-
tion rules with complete semantic structures also
improved our MT system. We experimented with
using source side complete semantic rules, as well
as using rules for both the source and the target
sides. Both of our experiments showed improve-
ments over the baseline, and as expected, the sec-
ond one had a higher improvement.
Acknowledgments
Partially funded by NSF grant IIS-0910611.
1790
References
Marzieh Bazrafshan and Daniel Gildea. 2013. Seman-
tic roles for string to tree machine translation. In
Association for Computational Linguistics (ACL-13)
short paper.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon.
Association for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, HLT ?11, pages 176?181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and
Dekai Wu. 2007. Learning Bilingual Seman-
tic Frames: Shallow Semantic Parsing vs. Seman-
tic Role Projection. In TMI-2007: Proceedings of
the 11 th International Conference on Theoretical
and Methodological Issues in Machine Translation,
Sk?ovde, Sweden.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Proceedings of NAACL-04, pages 273?280,
Boston, MA.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646?655, Cambridge,
MA, October.
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013.
Modeling syntactic and semantic structures in hier-
archical phrase-based translation. In HLT-NAACL,
pages 540?549.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In COLING-10, Bei-
jing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL-03, pages 160?167, Sapporo, Japan.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Martha. Palmer. 2009. SemLink: Linking PropBank,
VerbNet and FrameNet. In Proceedings of the Gen-
erative Lexicon ConferenceGenLex-09, Pisa, Italy,
Sept.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL-02, pages 311?318, Philadelphia, PA.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In International Confer-
ence on Spoken Language Processing, volume 2,
pages 901?904.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
the HLT-NAACL 2009: Short Papers, Boulder, Col-
orado.
Shumin Wu and Martha Palmer. 2011. Semantic map-
ping using automatic word alignment and seman-
tic role labeling. In Proceedings of the Fifth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation, SSST-5, pages 21?30, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In ACL (1), pages 902?911.
Kenji Yamada and Kevin Knight. 2002. A decoder
for syntax-based statistical MT. In Proceedings of
ACL-02, pages 303?310, Philadelphia, PA.
1791
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 543?547,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Tuning as Linear Regression
Marzieh Bazrafshan, Tagyoung Chung and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We propose a tuning method for statistical ma-
chine translation, based on the pairwise rank-
ing approach. Hopkins and May (2011) pre-
sented a method that uses a binary classifier.
In this work, we use linear regression and
show that our approach is as effective as us-
ing a binary classifier and converges faster.
1 Introduction
Since its introduction, the minimum error rate train-
ing (MERT) (Och, 2003) method has been the most
popular method used for parameter tuning in ma-
chine translation. Although MERT has nice proper-
ties such as simplicity, effectiveness and speed, it is
known to not scale well for systems with large num-
bers of features. One alternative that has been used
for large numbers of features is the Margin Infused
Relaxed Algorithm (MIRA) (Chiang et al, 2008).
MIRA works well with a large number of features,
but the optimization problem is much more compli-
cated than MERT. MIRA also involves some modi-
fications to the decoder itself to produce hypotheses
with high scores against gold translations.
Hopkins and May (2011) introduced the method
of pairwise ranking optimization (PRO), which casts
the problem of tuning as a ranking problem be-
tween pairs of translation candidates. The problem
is solved by doing a binary classification between
?correctly ordered? and ?incorrectly ordered? pairs.
Hopkins and May (2011) use the maximum entropy
classifier MegaM (Daume? III, 2004) to do the binary
classification. Their method compares well to the
results of MERT, scales better for high dimensional
feature spaces, and is simpler than MIRA.
In this paper, we use the same idea for tuning, but,
instead of using a classifier, we use linear regression.
Linear regression is simpler than maximum entropy
based methods. The most complex computation that
it needs is a matrix inversion, whereas maximum en-
tropy based classifiers use iterative numerical opti-
mization methods.
We implemented a parameter tuning program
with linear regression and compared the results to
PRO?s results. The results of our experiments are
comparable to PRO, and in many cases (also on av-
erage) we get a better maximum BLEU score. We
also observed that on average, our method reaches
the maximum BLEU score in a smaller number of
iterations.
The contributions of this paper include: First, we
show that linear regression tuning is an effective
method for tuning, and it is comparable to tuning
with a binary maximum entropy classifier. Second,
we show linear regression is faster in terms of the
number of iterations it needs to reach the best re-
sults.
2 Tuning as Ranking
The parameter tuning problem in machine transla-
tion is finding the feature weights of a linear trans-
lation model that maximize the scores of the candi-
date translations measured against reference transla-
tions. Hopkins and May (2011) introduce a tuning
method based on ranking the candidate translation
pairs, where the goal is to learn how to rank pairs of
candidate translations using a gold scoring function.
543
PRO casts the tuning problem as the problem of
ranking pairs of sentences. This method iteratively
generates lists of ?k-best? candidate translations for
each sentence, and tunes the weight vector for those
candidates. MERT finds the weight vector that max-
imizes the score for the highest scored candidate
translations. In contrast, PRO finds the weight vec-
tor which classifies pairs of candidate translations
into ?correctly ordered? and ?incorrectly ordered,?
based on the gold scoring function. While MERT
only considers the highest scored candidate to tune
the weights, PRO uses the entire k-best list to learn
the ranking between the pairs, which can help pre-
vent overfitting.
Let g(e) be a scoring function that maps each
translation candidate e to a number (score) using a
set of reference translations. The most commonly
used gold scoring function in machine translation
is the BLEU score, which is calculated for the en-
tire corpus, rather than for individual sentences. To
use BLEU as our gold scoring function, we need to
modify it to make it decomposable for single sen-
tences. One way to do this is to use a variation of
BLEU called BLEU+1 (Lin and Och, 2004), which
is a smoothed version of the BLEU score.
We assume that our machine translation system
scores translations by using a scoring function which
is a linear combination of the features:
h(e) = wTx(e) (1)
where w is the weight vector and x is the feature vec-
tor. The goal of tuning as ranking is learning weights
such that for every two candidate translations e1 and
e2, the following inequality holds:
g(e1) > g(e2) ? h(e1) > h(e2) (2)
Using Equation 1, we can rewrite Equation 2:
g(e1) > g(e2) ? wT(x(e1) ? x(e2)) > 0 (3)
This problem can be viewed as a binary classifica-
tion problem for learning w, where each data point is
the difference vector between the feature vectors of
a pair of translation candidates, and the target of the
point is the sign of the difference between their gold
scores (BLEU+1). PRO uses the MegaM classifier
to solve this problem. MegaM is a binary maximum
entropy classifier which returns the weight vector
w as a linear classifier. Using this method, Hop-
kins and May (2011) tuned the weight vectors for
various translation systems. The results were close
to MERT?s and MIRA?s results in terms of BLEU
score, and the method was shown to scale well to
high dimensional feature spaces.
3 Linear Regression Tuning
In this paper, we use the same idea as PRO for tun-
ing, but instead of using a maximum entropy clas-
sifier, we use a simple linear regression to estimate
the vector w in Equation 3. We use the least squares
method to estimate the linear regression. For a ma-
trix of data points X, and a target vector g, the
weight vector can be calculated as:
w = (XTX)?1XTg (4)
Adding L2 regularization with parameter ? has the
following closed form solution:
w = (XTX + ?I)?1XTg (5)
Following the sampling method used in PRO, the
matrices X and vector g are prepared as follows:
For each sentence,
1. Generate a list containing the k best transla-
tions of the sentence, with each translation e
scored by the decoder using a function of the
form h(e) = wTx(e).
2. Use the uniform distribution to sample n ran-
dom pairs from the set of candidate transla-
tions.
3. Calculate the gold scores g for the candidates in
each pair using BLEU+1. Keep a pair of can-
didates as a potential pair if the difference be-
tween their g scores is bigger than a threshold
t.
4. From the potential pairs kept in the previous
step, keep the s pairs that have the highest dif-
ferences in g and discard the rest.
5. For each pair e1 and e2 kept in step 4, make two
data points (x(e1)? x(e2), g(e1)? g(e2)) and
(x(e2) ? x(e1), g(e2) ? g(e1)).
544
The rows of X consist of the inputs of the data points
created in step 5, i.e., the difference vectors x(e1)?
x(e2). Similarly, the corresponding rows in g are
the outputs of the data points, i.e., the gold score
differences g(e1) ? g(e2).
One important difference between the linear re-
gression method and PRO is that rather than using
the signs of the gold score differences and doing a
binary classification, we use the differences of the
gold scores directly, which allows us to use the in-
formation about the magnitude of the differences.
4 Experiments
4.1 Setup
We used a Chinese-English parallel corpus with the
English side parsed for our experiments. The cor-
pus consists of 250K sentence pairs, which is 6.3M
words on the English side. The corpus derives from
newswire texts available from LDC.1 We used a 392-
sentence development set with four references for
parameter tuning, and a 428-sentence test set with
four references for testing. They are drawn from the
newswire portion of NIST evaluations (2004, 2005,
2006). The development set and the test set only
had sentences with less than 30 words for decoding
speed.
We extracted a general SCFG (GHKM) grammar
using standard methods (Galley et al, 2004; Wang
et al, 2010) from the parallel corpus with a mod-
ification to preclude any unary rules (Chung et al,
2011). All rules over scope 3 are pruned (Hopkins
and Langmead, 2010). A set of nine standard fea-
tures was used for the experiments, which includes
globally normalized count of rules, lexical weight-
ing (Koehn et al, 2003), and length penalty. Our
in-house decoder was used for experiments with a
trigram language model. The decoder is capable
of both CNF parsing and Earley-style parsing with
cube-pruning (Chiang, 2007).
We implemented linear regression tuning using
1We randomly sampled our data from various differ-
ent sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,
LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2006E24, LDC2006E92, LDC2006E24) The language
model is trained on the English side of entire data (1.65M sen-
tences, which is 39.3M words.)
Average of max BLEU Max BLEU
dev test dev test
Regression 27.7 (0.91) 26.4 (0.82) 29.0 27.6
PRO 26.9 (1.05) 25.6 (0.84) 28.0 27.2
Table 1: Average of maximum BLEU scores of the ex-
periments and the maximum BLEU score from the ex-
periments. Numbers in the parentheses indicate standard
of deviations of maximum BLEU scores.
the method explained in Section 3. Following Hop-
kins and May (2011), we used the following param-
eters for the sampling task: For each sentence, the
decoder generates the 1500 best candidate transla-
tions (k = 1500), and the sampler samples 5000
pairs (n = 5000). Each pair is kept as a potential
data point if their BLEU+1 score difference is big-
ger than 0.05 (t = 0.05). Finally, for each sentence,
the sampler keeps the 50 pairs with the highest dif-
ference in BLEU+1 (s = 50) and generates two data
points for each pair.
4.2 Results
We ran eight experiments with random initial weight
vectors and ran each experiment for 25 iterations.
Similar to what PRO does, in each iteration, we lin-
early interpolate the weight vector learned by the re-
gression (w) with the weight vector of the previous
iteration (wt?1) using a factor of 0.1:
wt = 0.1 ? w + 0.9 ? wt?1 (6)
For the sake of comparison, we also implemented
PRO with exactly the same parameters, and ran it
with the same initial weight vectors.
For each initial weight vector, we selected the iter-
ation at which the BLEU score on the development
set is highest, and then decoded using this weight
vector on the test set. The results of our experi-
ments are presented in Table 1. In the first column,
we show the average over the eight initial weight
vectors of the BLEU score achieved, while in the
second column we show the results from the ini-
tial weight vector with the highest BLEU score on
the development set. Thus, while the second col-
umn corresponds to a tuning process where the sin-
gle best result is retained, the first column shows the
expected behavior of the procedure on a single ini-
tial weight vector. The linear regression method has
545
 12
 14
 16
 18
 20
 22
 24
 26
 28
 0  5  10  15  20  25
BL
EU
Iteration
reg-avg
pro-avg
Figure 1: Average of eight runs of regression and PRO.
higher BLEU scores on both development and test
data for both the average over initial weights and the
maximum over initial weights.
Figure 1 shows the average of the BLEU scores
on the development set of eight runs of the experi-
ments. We observe that on average, the linear regres-
sion experiments reach the maximum BLEU score
in a smaller number of iterations. On average, linear
regression reached the maximum BLEU score after
14 iterations and PRO reached the maximum BLEU
score after 20 iterations. One iteration took several
minutes for both of the algorithms. The largest por-
tion of this time is spent on decoding the develop-
ment set and reading in the k-best list. The sampling
phase, which includes performing linear regression
or running MegaM, takes a negligible amount of
time compared to the rest of the operations.
We experimented with adding L2 regularization
to linear regression. As expected, the experiments
with regularization produced lower variance among
the different experiments in terms of the BLEU
score, and the resulting set of the parameters had a
smaller norm. However, because of the small num-
ber of features used in our experiments, regulariza-
tion was not necessary to control overfitting.
5 Discussion
We applied the idea of tuning as ranking and modi-
fied it to use linear regression instead of binary clas-
sification. The results of our experiments show that
tuning as linear regression is as effective as PRO,
and on average it reaches a better BLEU score in a
fewer number of iterations.
In comparison with MERT, PRO and linear re-
gression are different in the sense that the latter two
approaches take into account rankings of the k-best
list, whereas MERT is only concerned with separat-
ing the top 1-best sentence from the rest of the k-
best list. PRO and linear regression are similar in
the sense that both are concerned with ranking the
k-best list. Their difference lies in the fact that PRO
only uses the information on the relative rankings
and uses binary classification to rank the points; on
the contrary, linear regression directly uses the infor-
mation on the magnitude of the differences. This dif-
ference between PRO and linear regression explains
why linear regression converges faster and also may
explain the fact that linear regression achieves a
somewhat higher BLEU score. In this sense, lin-
ear regression is also similar to MIRA since MIRA?s
loss function also uses the information on the magni-
tude of score difference. However, the optimization
problem for linear regression is simpler, does not re-
quire any changes to the decoder, and therefore the
familiar MERT framework can be kept.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work was
supported by NSF grant IIS-0910611.
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon. As-
sociation for Computational Linguistics.
Hal Daume? III. 2004. Notes on CG and LM-BFGS opti-
mization of logistic regression. August.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280, Boston.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
546
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of Coling 2004,
pages 501?507, Geneva, Switzerland, Aug 23?Aug
27. COLING.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36:247?277, June.
547
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 419?423,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Roles for String to Tree Machine Translation
Marzieh Bazrafshan and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We experiment with adding semantic role
information to a string-to-tree machine
translation system based on the rule ex-
traction procedure of Galley et al (2004).
We compare methods based on augment-
ing the set of nonterminals by adding se-
mantic role labels, and altering the rule
extraction process to produce a separate
set of rules for each predicate that encom-
pass its entire predicate-argument struc-
ture. Our results demonstrate that the sec-
ond approach is effective in increasing the
quality of translations.
1 Introduction
Statistical machine translation (SMT) has made
considerable advances in using syntactic proper-
ties of languages in both the training and the de-
coding of translation systems. Over the past few
years, many researchers have started to realize that
incorporating semantic features of languages can
also be effective in increasing the quality of trans-
lations, as they can model relationships that often
are not derivable from syntactic structures.
Wu and Fung (2009) demonstrated the promise
of using features based on semantic predicate-
argument structure in machine translation, using
these feature to re-rank machine translation out-
put. In general, re-ranking approaches are lim-
ited by the set of translation hypotheses, leading
to a desire to incorporate semantic features into
the translation model used during MT decoding.
Liu and Gildea (2010) introduced two types of
semantic features for tree-to-string machine trans-
lation. These features model the reorderings and
deletions of the semantic roles in the source sen-
tence during decoding. They showed that addition
of these semantic features helps improve the qual-
ity of translations. Since tree-to-string systems are
trained on parse trees, they are constrained by the
tree structures and are generally outperformed by
string-to-tree systems.
Xiong et al (2012) integrated two discrimi-
native feature-based models into a phrase-based
SMT system, which used the semantic predicate-
argument structure of the source language. Their
first model defined features based on the context of
a verbal predicate, to predict the target translation
for that verb. Their second model predicted the re-
ordering direction between a predicate and its ar-
guments from the source to the target sentence.
Wu et al (2010) use a head-driven phrase struc-
ture grammar (HPSG) parser to add semantic rep-
resentations to their translation rules.
In this paper, we use semantic role labels to en-
rich a string-to-tree translation system, and show
that this approach can increase the BLEU (Pap-
ineni et al, 2002) score of the translations. We
extract GHKM-style (Galley et al, 2004) transla-
tion rules from training data where the target side
has been parsed and labeled with semantic roles.
Our general method of adding information to the
syntactic tree is similar to the ?tree grafting? ap-
proach of Baker et al (2010), although we fo-
cus on predicate-argument structure, rather than
named entity tags and modality. We modify the
rule extraction procedure of Galley et al (2004) to
produce rules representing the overall predicate-
argument structure of each verb, allowing us to
model alternations in the mapping from syntax to
semantics of the type described by Levin (1993).
2 Semantic Roles for String-to-Tree
Translation
2.1 Semantic Role Labeling
Semantic Role Labeling (SRL) is the task of iden-
tifying the arguments of the predicates in a sen-
tence, and classifying them into different argu-
ment labels. Semantic roles can provide a level
419
of understanding that cannot be derived from syn-
tactic analysis of a sentence. For example, in
sentences ?Ali opened the door.? and ?The door
opened?, the word door has two different syntac-
tic roles but only one semantic role in the two sen-
tences.
Semantic arguments can be classified into core
and non-core arguments (Palmer et al, 2010).
Core arguments are necessary for understanding
the sentence. Non-core arguments add more infor-
mation about the predicate but are not essential.
Automatic semantic role labelers have been de-
veloped by training classifiers on hand annotated
data (Gildea and Jurafsky, 2000; Srikumar and
Roth, 2011; Toutanova et al, 2005; Fu?rstenau and
Lapata, 2012). State-of-the-art semantic role la-
belers can predict the labels with accuracies of
around 90%.
2.2 String-to-Tree Translation
We adopt the GHKM framework of Galley et al
(2004) using the parses produced by the split-
merge parser of Petrov et al (2006) as the English
trees. As shown by Wang et al (2010), the refined
nonterminals produced by the split-merge method
can aid machine translation. Furthermore, in all
of our experiments, we exclude unary rules during
extraction by ensuring that no rules will have the
same span in the source side (Chung et al, 2011).
2.3 Using Semantic Role Labels in SMT
To incorporate semantic information into a string-
to-tree SMT system, we tried two approaches:
? Using semantically enriched GHKM rules,
and
? Extracting semantic rules separately from the
regular GHKM rules, and adding a new fea-
ture for distinguishing the semantic rules.
The next two sections will explain these two
methods in detail.
2.4 Semantically Enriched Rules (Method 1)
In this method, we tag the target trees in the train-
ing corpus with semantic role labels, and extract
the translation rules from the tagged corpus. Since
the SCFG rule extraction methods do not assume
any specific set of non-terminals for the target
parse trees, we can attach the semantic roles of
each constituent to its label in the tree, and use
S
NP?ARG0
NPB
NN
everybody
VP
VBG?PRED
lending
NP?ARG1
NPB
DT
a
NN
hand
Figure 1: A target tree after inserting semantic
roles. ?Lending? is the predicate, ?everybody? is
argument 0, and ?a hand? is argument 1 for the
predicate.
S-8
NP-7-ARG1 1 victimized by NP-7-ARG0 2
NP-7-ARG1 1 ? NP-7-ARG0 2
Figure 2: A complete semantic rule.
these new labels for rule extraction. We only la-
bel the core arguments of each predicate, to make
sure that the rules are not too specific to the train-
ing data. We attach each semantic label to the root
of the subtree that it is labeling. Figure 1 shows
an example target tree after attaching the semantic
roles. We then run a GHKM rule extractor on the
labeled training corpus and use the semantically
enriched rules with a syntax-based decoder.
2.5 Complete Semantic Rules with Added
Feature (Method 2)
This approach uses the semantic role labels to
extract a set of special translation rules, that on
the target side form the smallest tree fragments in
which one predicate and all of its core arguments
are present. These rules model the complete se-
mantic structure of each predicate, and are used
by the decoder in addition to the normal GHKM
rules, which are extracted separately.
Starting by semantic role labeling the target
parse trees, we modify the GHKM component of
the system to extract a semantic rule for each pred-
icate. We define labels p as the set of semantic
role labels related to predicate p. That includes all
420
Number of rules
dev test
Baseline 1292175 1300589
Method 1 1340314 1349070
Method 2 1416491 1426159
Table 1: The number of the translation rules used
by the three experimented methods
of the labels of the arguments of p, and the label
of p itself. Then we add the following condition
to the definition of the ?frontier node? defined in
Galley et al (2004):
A frontier node must have either all or none of
the semantic role labels from labels p in its de-
scendants in the tree.
Adding this new condition, we extract one se-
mantic rule for each predicate, and for that rule we
discard the labels related to the other predicates.
This semantic rule will then have on its target side,
the smallest tree fragment that contains all of the
arguments of predicate p and the predicate itself.
Figure 2 depicts an example of a complete se-
mantic rule. Numbers following grammatical cat-
egories (for example, S-8 at the root) are the re-
fined nonterminals produced by the split-merge
parser. In general, the tree side of the rule may
extend below the nodes with semantic role labels
because of the general constraint on frontier nodes
that they must have a continuous span in the source
(Chinese) side. Also, the internal nodes of the
rules (such as a node with PRED label in Figure
2) are removed because they are not used in de-
coding.
We also extract the regular GHKM rules using
the original definition of the frontier nodes, and
add the semantic rules to them. To differentiate
the semantic rules from the non-semantic ones, we
add a new binary feature that is set to 1 for the
semantic rules and to 0 for the rest of the rules.
3 Experiments
Semantic role labeling was done using the Prop-
Bank standard (Palmer et al, 2005). Our labeler
uses a maximum entropy classifier and for iden-
tification and classification of semantic roles, and
has a percision of 90% and a recall of 88%. The
features used for training the labeler are a subset of
the features used by Gildea and Jurafsky (2000),
Xue and Palmer (2004), and Pradhan et al (2004).
The string-to-tree training data that we used is
a Chinese to English parallel corpus that contains
more than 250K sentence pairs, which consist of
6.3M English words. The corpus was drawn from
the newswire texts available from LDC.1 We used
a 392-sentence development set with four refer-
ences for parameter tuning, and a 428-sentence
test set with four references for testing. They are
drawn from the newswire portion of NIST evalua-
tion (2004, 2005, 2006). The development set and
the test set only had sentences with less than 30
words for decoding speed. A set of nine standard
features, which include globally normalized count
of rules, lexical weighting (Koehn et al, 2003),
length penalty, and number of rules used, was used
for the experiments. In all of our experiments, we
used the split-merge parsing method of Petrov et
al. on the training corpus, and mapped the seman-
tic roles from the original trees to the result of the
split-merge parser. We used a syntax-based de-
coder with Earley parsing and cube pruning (Chi-
ang, 2007). We used the Minimum Error Rate
Training (Och, 2003) to tune the decoding param-
eters for the development set and tested the best
weights that were found on the test set.
We ran three sets of experiments: Baseline
experiments, where we did not do any seman-
tic role labeling prior to rule extraction and only
extracted regular GHKM rules, experiments with
our method of Section 2.4 (Method 1), and a set
of experiments with our method of Section 2.5
(Method 2).
Table 1 contains the numbers of the GHKM
translation rules used by our three method. The
rules were filtered by the development and the test
to increase the decoding speed. The increases in
the number of rules were expected, but they were
not big enough to significantly change the perfor-
mance of the decoder.
3.1 Results
For every set of experiments, we ran MERT on the
development set with 8 different starting weight
vectors picked randomly. For Method 2 we added
a new random weight for the new feature. We then
tested the system on the test set, using for each
experiment the weight vector from the iteration of
MERT with the maximum BLEU score on the de-
velopment set. Table 3 shows the BLEU scores
that we found on the test set, and their correspond-
ing scores on the development set.
1We randomly sampled our data from various different
sources. The language model is trained on the English side
of entire data (1.65M sentences, which is 39.3M words.)
421
Source ?? 13????? ,????? ,????? .
Reference to solve the problem of 1.3 billion people , we can only rely on ourselves and nobody else .
Baseline cannot rely on others , can only resolve the problem of 13 billion people , on their own .
Method 2 to resolve the issue of 1.3 billion people , they can?t rely on others , and it can only rely on themselves .
Source ???????? ,???????????? .
Reference in the new situation of the millennium , the development of asia is facing new opportunities .
Baseline facing new opportunities in the new situation in the new century , the development of asia .
Method 2 under the new situation in the new century , the development of asia are facing a new opportunity .
Source ?? ,???????????? ??????????? .
Reference he said the arab league is the best partner to discuss with the united states about carrying out democratic reforms in the middle east .
Baseline arab league is the best with democratic reform in the middle east region in the discussion of the united states , he said .
Method 2 arab league is the best partner to discuss the middle east region democratic reform with the united states , he said .
Table 2: Comparison of example translations from the baseline method and our Method 2.
The best BLEU score on the test set is 25.92,
which is from the experiments of Method 2.
Method 1 system seems to behave slightly worse
than the baseline and Method 2. The reason for
this behavior is that the rules that were extracted
from the semantic role labeled corpus could have
isolated semantic roles in them which would not
necessarily get connected to the right predicate
or argument during decoding. In other words,
it is possible for a rule to only contain one or
some of the semantic arguments of a predicate,
and not even include the predicate itself, and there-
fore there is no guarantee that the predicate will be
translated with the right arguments and in the right
order. The difference between the BLEU scores
of the best Method 2 results and the baseline is
0.92. This improvement is statistically significant
(p = 0.032) and it shows that incorporating se-
mantic roles in machine translation is an effective
approach.
Table 2 compares some translations from the
baseline decoder and our Method 2. The first line
of each example is the Chinese source sentence,
and the second line is one of the reference trans-
lations. The last two lines compare the baseline
and Method 2. These examples show how our
Method 2 can outperform the baseline method, by
translating complete semantic structures, and gen-
erating the semantic roles in the correct order in
the target language. In the first example, the pred-
icate rely on for the argument themselves was not
translated by the baseline decoder, but it was cor-
rectly translated by Method 2. The second ex-
ample is a case where the baseline method gener-
ated the arguments in the wrong order (in the case
of facing and development), but the translation by
Method 2 has the correct order. In the last example
we see that the arguments of the predicate discuss
have the wrong order in the baseline translation,
BLEU Score
dev test
Baseline 26.01 25.00
Method 1 26.12 24.84
Method 2 26.5 25.92
Table 3: BLEU scores on the test and development
sets, of 8 experiments with random initial feature
weights.
but Method 2 generated the correct oder.
4 Conclusion
We proposed two methods for incorporating se-
mantic role labels in a string-to-tree machine
translation system, by learning translation rules
that are semantically enriched. In one approach,
the system learned the translation rules by us-
ing a semantic role labeled corpus and augment-
ing the set of nonterminals used in the rules, and
in the second approach, in addition to the regu-
lar SCFG rules, the system learned semantic roles
which contained the complete semantic structure
of a predicate, and added a feature to distinguish
those rules.
The first approach did not perform any better
than the baseline, which we explained as being due
to having rules with only partial semantic struc-
tures and not having a way to guarantee that those
rules will be used with each other in the right way.
The second approach significantly outperformed
the baseline of our experiments, which shows that
complete predicate-argument structures can im-
prove the quality of machine translation.
Acknowledgments Partially funded by NSF
grant IIS-0910611.
422
References
Kathryn Baker, Michael Bloodgood, Chris Callison-
Burch, Bonnie J. Dorr, Nathaniel W. Filardo, Lori
Levin, Scott Miller, and Christine Piatko. 2010.
Semantically-informed machine translation: A tree-
grafting approach. In Proceedings of The Ninth Bi-
ennial Conference of the Association for Machine
Translation in the Americas, Denver, Colorado.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon.
Association for Computational Linguistics.
Hagen Fu?rstenau and Mirella Lapata. 2012. Semi-
supervised semantic role labeling via structural
alignment. Computational Linguistics, 38(1):135?
171.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Proceedings of NAACL-04, pages 273?280,
Boston.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In Proceedings of ACL-
00, pages 512?520, Hong Kong, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, pages 48?54, Edmonton,
Alberta.
Beth Levin. 1993. English Verb Classes And Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In COLING-10, Bei-
jing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL-03, pages 160?167.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic Role Labeling. Synthesis Lec-
tures on Human Language Technology Series. Mor-
gan and Claypool.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL-02, pages 311?318.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of NAACL-04.
V. Srikumar and D. Roth. 2011. A joint model for
extended semantic role labeling. In EMNLP, Edin-
burgh, Scotland.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-05, pages 589?
596.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and
re-aligning for syntax-based machine translation.
Computational Linguistics, 36:247?277, June.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
the HLT-NAACL 2009: Short Papers, Boulder, Col-
orado.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL ?10, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In ACL (1), pages 902?911.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP.
423
