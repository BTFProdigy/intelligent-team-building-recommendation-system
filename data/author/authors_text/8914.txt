 
Applying Machine Learning to Chinese Temporal Relation Resolution 
 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
cswjli@comp.polyu.edu.hk 
Kam-Fai Wong 
Department of Systems Engineering and Engineering 
Management 
The Chinese University of Hong Kong, Hong Kong
kfwong@se.cuhk.edu.hk 
Guihong Cao 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
csghcao@comp.polyu.edu.hk 
Chunfa Yuan 
Department of Computer Science and Technology 
Tsinghua University, Beijing, China. 
cfyuan@tsinghua.edu.cn 
 
 
 
Abstract 
Temporal relation resolution involves extraction 
of temporal information explicitly or implicitly 
embedded in a language. This information is of-
ten inferred from a variety of interactive gram-
matical and lexical cues, especially in Chinese. 
For this purpose, inter-clause relations (tempo-
ral or otherwise) in a multiple-clause sentence 
play an important role. In this paper, a computa-
tional model based on machine learning and 
heterogeneous collaborative bootstrapping is 
proposed for analyzing temporal relations in a 
Chinese multiple-clause sentence. The model 
makes use of the fact that events are represented 
in different temporal structures. It takes into ac-
count the effects of linguistic features such as 
tense/aspect, temporal connectives, and dis-
course structures. A set of experiments has been 
conducted to investigate how linguistic features 
could affect temporal relation resolution.  
 
1 Introduction 
In language studies, temporal information de-
scribes changes and time of changes expressed in a 
language. Such information is critical in many typi-
cal natural language processing (NLP) applications, 
e.g. language generation and machine translation, etc. 
Modeling temporal aspects of an event in a written 
text is more complex than capturing time in a physi-
cal time-stamped system. Event time may be speci-
fied explicitly in a sentence, e.g. ???? 1997??
????????? (They solved the traffic prob-
lem of the city in 1997)?; or it may be left implicit, to 
be recovered by readers from context. For example, 
one may know that ???????????????
?????? (after the street bridge had been built, 
they solved the traffic problem of the city)?, yet 
without knowing the exact time when the street 
bridge was built. As reported by Partee (Partee, 
1984), the expression of relative temporal relations 
in which precise times are not stated is common in 
natural language. The objective of relative temporal 
relation resolution is to determine the type of rela-
tive relation embedded in a sentence. 
In English, temporal expressions have been 
widely studied. Lascarides and Asher (Lascarides, 
Asher and Oberlander, 1992) suggested that tempo-
ral relations between two events followed from dis-
course structures. They investigated various 
contextual effects on five discourse relations 
(namely narration, elaboration, explanation, back-
ground and result) and then corresponded each of 
them to a kind of temporal relations. Hitzeman et al 
(Hitzeman, Moens and Grover, 1995) described a 
method for analyzing temporal structure of a dis-
course by taking into account the effects of tense, 
aspect, temporal adverbials and rhetorical relations 
(e.g. causation and elaboration) on temporal order-
ing. They argued that rhetorical relations could be 
further constrained by event temporal classification. 
Later, Dorr and Gaasterland (Dorr and Gaasterland, 
2002) developed a constraint-based approach to 
generate sentences, which reflect temporal relations, 
by making appropriate selections of tense, aspect 
and connecting words (e.g. before, after and when). 
Their works, however, are theoretical in nature and 
have not investigated computational aspects. 
The pioneer work on Chinese temporal relation 
extraction was first reported by Li and Wong (Li and 
Wong, 2002). To discover temporal relations em-
bedded in a sentence, they devised a set of simple 
rules to map the combined effects of temporal indi-
cators, which are gathered from different grammati-
cal categories, to their corresponding relations. 
However, their work did not focus on relative tem-
poral relations. Given a sentence describing two 
temporally related events, Li and Wong only took 
the temporal position words (including before, after 
and when, which serve as temporal connectives) and 
the tense/aspect markers of the second event into 
consideration. The proposed rule-based approach 
was simple; but it suffered from low coverage and 
was particularly ineffective when the interaction be-
tween the linguistic elements was unclear. 
This paper studies how linguistic features in Chi-
nese interact to influence relative relation resolution. 
For this purpose, statistics-based machine learning 
approaches are applied. The remainder of the paper 
is structured as follows: Section 2 summarizes the 
linguistic features, which must be taken into account 
in temporal relation resolution, and introduces how 
these features are expressed in Chinese. In Section 3, 
the proposed machine learning algorithms to identify 
temporal relations are outlined; furthermore, a het-
erogeneous collaborative bootstrapping technique 
for smoothing is presented. Experiments designed 
for studying the impact of different approaches and 
linguistic features are described in Section 4. Finally, 
Section 5 concludes the paper. 
2 Modeling Temporal Relations 
2.1 Temporal Relation Representations 
As the importance of temporal information proc-
essing has become apparent, a variety of temporal 
systems have been introduced, attempting to ac-
commodate the characteristics of relative temporal 
information. Among those who worked on temporal 
relation representations, many took the work of Rei-
chenbach (Reichenbach, 1947) as a starting point, 
while some others based their works on Allen?s (Al-
len, 1981). 
Reichenbach proposed a point-based temporal 
theory. This was later enhanced by Bruce who de-
fined seven relative temporal relations (Bruce. 1972). 
Given two durative events, the interval relations be-
tween them were modeled by the order between the 
greatest lower bounding points and least upper 
bounding points of the two events. In the other camp, 
instead of adopting time points, Allen took intervals 
as temporal primitives and introduced thirteen basic 
binary relations. In this interval-based theory, points 
are relegated to a subsidiary status as ?meeting 
places? of intervals. An extension to Allen?s theory, 
which treated both points and intervals as primitives 
on an equal footing, was later investigated by Ma 
and Knight (Ma and Knight, 1994). 
In natural language, events can either be punctual 
(e.g. ?? (explore)) or durative (e.g. ?? (built a 
house)) in nature. Thus Ma and Knight?s model is 
adopted in our work (see Figure 1). Taking the sen-
tence ????????????????????? 
(after the street bridge had been built, they solved 
the traffic problem of the city)? as an example, the 
relation held between building the bridge (i.e. an 
interval) and solving the problem (i.e. a point) is 
BEFORE. 
Figure 1. Thirteen temporal relations between points and 
intervals 
2.2 Linguistic Features for Determining Relative 
Relations 
Relative relations are generally determined by 
tense/aspect, connecting words (temporal or other-
wise) and event classes.  
Tense/Aspect in English is manifested by verb in-
flections. But such morphological variations are in-
applicable to Chinese verbs; instead, they are 
conveyed lexically (Li and Wong, 2002). In other 
words, tense and aspect in Chinese are expressed 
using a combination of time words, auxiliaries, tem-
poral position words, adverbs and prepositions, and 
particular verbs. 
Temporal Connectives in English primarily in-
volve conjunctions, e.g. after, before and when (Dorr 
and Gaasterland, 2002). They are key components in 
discourse structures. In Chinese, however, conjunc-
tions, conjunctive adverbs, prepositions and position 
words are required to represent connectives. A few 
verbs which express cause and effect also imply a 
forward movement of event time. The words, which 
contribute to the tense/aspect and temporal connec-
tive expressions, are explicit in a sentence and gen-
erally known as Temporal Indicators. 
Event Class is implicit in a sentence. Events can 
be classified according to their inherent temporal 
characteristics, such as the degree of telicity and/or 
atomicity (Li and Wong, 2002). The four widespread 
accepted temporal classes1 are state, process, punc-
tual event and developing event. Based on their 
classes, events interact with the tense/aspect of verbs 
to define the temporal relations between two events. 
Temporal indicators and event classes are together 
referred to as Linguistic Features (see Table 1). For 
example, linguistic features are underlined in the 
sentence ?(??)?????(??)????????
????? after/because the street bridge had been 
built (i.e. a developing event), they solved the traffic 
problem of the city (i.e. a punctual event)?. 
                                                          
1 Temporal classification refers to aspectual classification. 
A punctual event (i.e. represented in time point) 
A durative event (i.e. represented in time interval) 
BEFORE/AFTER 
MEETS/MET-BY 
OVERLAPS/OVERLAPPED-BY
STARTS/STARTED-BY 
DURING/CONTAINS 
FINISHES/FINISHED-BY 
SAME-AS 
Table 1 shows the mapping between a temporal 
indicator and its effects. Notice that the mapping is 
not one-to-one. For example, adverbs affect 
tense/aspect as well as discourse structure. For an-
other example, tense/aspect can be affected by auxil-
iary words, trend verbs, etc. This shows that 
classification of temporal indicators based on part-
of-speech (POS) information alone cannot determine 
relative temporal relations. 
3 Machine Learning Approaches for Relative 
Relation Resolution 
Previous efforts in corpus-based natural language 
processing have incorporated machine learning 
methods to coordinate multiple linguistic features 
for example in accent restoration (Yarowsky, 1994) 
and event classification (Siegel and McKeown, 
1998), etc. 
Relative relation resolution can be modeled as a 
relation classification task. We model the thirteen 
relative temporal relations (see Figure 1) as the 
classes to be decided by a classifier. The resolution 
process is to assign an event pair (i.e. the two events 
under concern)2 to one class according to their lin-
guistic features. For this purpose, we train two clas-
sifiers, a Probabilistic Decision Tree Classifier 
(PDT) and a Na?ve Bayesian Classifier (NBC). We 
then combine the results by the Collaborative Boot-
strapping (CB) technique which is used to mediate 
the sparse data problem arose due to the limited 
number of training cases. 
                                                          
2 It is an object in machine learning algorithms. 
3.1 Probabilistic Decision Tree (PDT) 
Due to two domain-specific characteristics, we 
encounter some difficulties in classification. (a) Un-
known values are common, for many events are 
modified by less than three linguistic features. (b) 
Both training and testing data are noisy. For this rea-
son, it is impossible to obtain a tree which can com-
pletely classify all training examples. To overcome 
this predicament, we aim to obtain more adjusted 
probability distributions of event pairs over their 
possible classes. Therefore, a probabilistic decision 
tree approach is preferred over conventional deci-
sion tree approaches (e.g. C4.5, ID3). We adopt a 
non-incremental supervised learning algorithm in 
TDIDT (Top Down Induction of Decision Trees) 
family. It constructs a tree top-down and the process 
is guided by distributional information learned from 
examples (Quinlan, 1993). 
3.1.1 Parameter Estimation 
Based on probabilities, each object in the PDT ap-
proach can belong to a number of classes. These 
probabilities could be estimated from training cases 
with Maximum Likelihood Estimation (MLE). Let l 
be the decision sequence, z the object and c the class. 
The probability of z belonging to c is: 
?? ?=
ll
zlplcpzclpzcp )|()|()|,()|(  (1)
let nBBBl ...21= , by MLE we have: 
)(
),(
)|()|(
n
n
n Bf
Bcf
Bcplcp =?   (2)
),( nBcf  is the count of the items whose leaf nodes 
are Bn and belonging to class c. And 
Linguistic Feature Symbol POS Tag Effect Example 
With/Without 
punctuations 
PT Not Applica-
ble 
Not Applicable Not Applicable 
Speech verbs VS TI_vs Tense ??, ??, ? 
Trend verbs TR TI_tr Aspect ??, ?? 
Preposition words P TI_p Discourse Structure/Aspect ?, ?, ? 
Position words PS TI_f Discourse Structure ?, ?, ?? 
Verbs with verb 
objects 
VV TI_vv Tense/Aspect ??, ??, ? 
Verbs expressing 
wish/hope 
VA TI_va Tense ??, ?, ? 
Verbs related to 
causality 
VC TI_vc Discourse Structure ??, ??, ?? 
Conjunctive words C TI_c Discourse Structure ?, ??, ?? 
Auxiliary words U TI_u Aspect ?, ?, ? 
Time words T TI_t Tense ??, ??, ?? 
Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ??, ? 
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event, 
Developing Event, 
Process 
Table 1. Linguistic features: eleven temporal indicators and one event class 
),...|(...
),,|(),|()|()|(
11
213121
zBBBp
zBBBpzBBpzBpzlp
nn ?
=
 
 
(3)
where 
)|...(
)|...(
),...|(
121
121
121 zBBBp
zBBBBp
zBBBBp
mm
mmm
mmm
??
??
?? =
)|...(
)|...(
121
121
zBBBf
zBBBBf
mm
mmm
??
??= , ( nm ,...,3,2= ).  
An object might traverse more than one decision 
path if it has unknown attribute values. 
)|...( 121 zBBBBf mmm ??  is the count of the item z, 
which owns the decision paths from B1 to Bm. 
3.1.2 Classification Attributes 
Objects are classified into classes based on their 
attributes. In the context of temporal relation resolu-
tion, how to categorize linguistic features into classi-
fication attributes is a major design issue. We extract 
all temporal indicators surrounding an event. As-
sume m and n are the anterior and posterior window 
size. They represent the numbers of the indicators 
BEFORE and AFTER respectively. Consider the 
most extreme case where an event consists of at 
most 4 temporal indicators before and 2 after. We 
set m and n to 4 and 2 initially. Experiments show 
that learning performance drops when m>4 and n>2 
and there is only very little difference otherwise (i.e. 
when m?4 and n?2).  
In addition to temporal indicators alone, the posi-
tion of the punctuation mark separating the two 
clauses describing the events and the classes of the 
events are also useful classification attributes.  We 
will outline why this is so in Section 4.1. Altogether, 
the following 15 attributes are used to train the PDT 
and NBC classifiers: 
,,),(,,,, 2
1
1
1
1
1
2
1
3
1
4
1 1
r
e
r
e
l
e
l
e
l
e
l
e TITIeclassTITITITI  
2
2
1
2
1
2
2
2
3
2
4
2
,),(,,,,, / 2
r
e
r
e
l
e
l
e
l
e
l
e TITIeclassTITITITIpuncwowi  
li (i=1,2,3,4) and rj (j=1,2) are the ith indictor before 
and the jth indicator after the event ek (k=1,2). Given 
a sentence, for example, ?/TI_d ?/E0 ?/TI_u ??
/n ?/w ?/TI_d ?/E2 ?/TI_u ??/n ?/w, the at-
tribute vector could be represented as: [0, 0, 0, ?, 
E0, ?, 0, 1, 0, 0, 0, ?, E2, ?, 0]. 
3.1.3 Attribute Selection Function 
Many similar attribute selection functions were 
used to construct a decision tree (Marquez, 2000). 
These included information gain and information 
gain ratio (Quinlan, 1993), 2? Test and Symmetrical 
Tau (Zhou and Dillon, 1991). We adopt the one pro-
posed by Lopez de Mantaraz (Mantaras, 1991) for it 
shows more stable performance than Quinlan?s 
information gain ratio in our experiments. Compared 
with Quinlan?s information gain ratio, Lopez?s dis-
tance-based measurement is unbiased towards the 
attributes with a large number of values and is capa-
ble of generating smaller trees with no loss of accu-
racy (Marquez, Padro and Rodriguez, 2000). This 
characteristic makes it an ideal choice for our work, 
where most attributes have more than 200 values. 
3.2 Na?ve Bayesian Classifier (NBC) 
NBC assumes independence among features. 
Given the class label c, NBC learns from training 
data the conditional probability of each attribute Ai 
(see Section 3.1.2). Classification is then performed 
by applying Bayes rule to compute the probability of 
c given the particular instance of A1,?,An, and then 
predicting the class with the highest posterior prob-
ability ratio. 
),...,,,|(maxarg 321
*
n
c
AAAAcscorec =  (4)
),...,,,|(
),...,,,|(
),...,,,|(
321
321
321
n
n
n AAAAcp
AAAAcp
AAAAcscore =  (5)
Apply Bayesian rule to (5), we have: 
),...,,,|(
),...,,,|(
),...,,,|(
321
321
321
n
n
n AAAAcp
AAAAcp
AAAAcscore =
)()|,...,,,(
)()|,...,,,(
321
321
cpcAAAAp
cpcAAAAp
n
n=
)()|(
)()|(
1
1
cpcAp
cpcAp
n
i
i
n
i
i
?
?
=
=?  
 
 
 
(6)
)|( cAp i and )|( cAp i  are estimated by MLE from 
training data with Dirichlet Smoothing method: 
?
=
?+
+= n
j
j
i
i
nucAc
ucAc
cAp
1
),(
),(
)|(   (7)
?
=
?+
+= n
j
j
i
i
nucAc
ucAc
cAp
1
),(
),(
)|(   (8)
3.3 Collaborative Bootstrapping (CB) 
PDT and NB are both supervised learning ap-
proach. Thus, the training processes require many 
labeled cases. Recent results (Blum and Mitchell, 
1998; Collins, 1999) have suggested that unlabeled 
data could also be used effectively to reduce the 
amount of labeled data by taking advantage of col-
laborative bootstrapping (CB) techniques. In previ-
ous works, CB trained two homogeneous classifiers 
based on different independent feature spaces. How-
ever, this approach is not applicable to our work 
since only a few temporal indicators occur in each 
case. Therefore, we develop an alternative CB algo-
rithm, i.e. to train two different classifiers based on 
the same feature spaces. PDT (a non-linear classifier) 
and NBC (a linear classifier) are under consideration. 
This is inspired by Blum and Mitchell?s theory that 
two collaborative classifiers should be conditionally 
independent so that each classifier can make its own 
contribution (Blum and Mitchell, 1998). The learn-
ing steps are outlined in Figure 2. 
Inputs: A collection of the labeled cases and unla-
beled cases is prepared. The labeled cases 
are separated into three parts, training 
cases, test cases and held-out cases.  
Loop: While the breaking criteria is not satisfied 
1 Build the PDT and NBC classifiers us-
ing training cases 
2 Use PDT and NBC to classify the unla-
beled cases, and exchange with the se-
lected cases which have higher 
Classification Confidence (i.e. the un-
certainty is less than a threshold). 
3 Evaluate the PDT and NBC classifiers 
with the held-out cases. If the error rate 
increases or its reduction is below a 
threshold break the loop; else go to step 
1. 
Output: Use the optimal classifier to label the test 
cases 
Figure 2. Collaborative bootstrapping algorithm 
3.4 Classification Confidence Measurement 
Classification confidence is the metric used to 
measure the correctness of each labeled case auto-
matically (see Step 2 in Figure 2). The desirable 
metric should satisfy two principles:  
? It should be able to measure the uncertainty/ cer-
tainty of the output of the classifiers; and 
? It should be easy to calculate. 
We adopt entropy, i.e. an information theory 
based criterion, for this purpose. Let x be the classi-
fied object, and },...,,,{ 321 nccccC = the set of output. 
x is classified as ci with the probability 
)|( xcp i ni ,..,3,2,1= . The entropy of the output is 
then calculated as: 
?
=
?= n
i
ii xcpxcpxCe
1
)|(log)|()|(  (9)
Once )|( xcp i is known, the entropy can be deter-
mined. These parameters can be easily determined in 
PDT, as each incoming case is classified into each 
class with a probability. However, the incoming 
cases in NBC are grouped into one class which is 
assigned the highest score. We then have to estimate 
)|( xcp i  from those scores. Without loss of general-
ity, the probability is estimated as: 
?
=
= n
j
j
i
i
xcscore
xcscore
xcp
1
)|(
)|(
 )|(   (10)
where )|( xcscore i  is the ranking score of x be-
longing to ci. 
4 Experiment Setup and Evaluation 
Several experiments have been designed to evalu-
ate the proposed learning approaches and to reveal 
the impact of linguistic features on learning per-
formance. 700 sentences are extracted from Ta Kong 
Pao (a local Hong Kong Chinese newspaper) finan-
cial version. 600 cases are labeled manually and 100 
left unlabeled. Among those labeled, 400 are used as 
training data, 100 as test data and the rest as held-out 
data. 
4.1 Use of Linguistic Features As Classification 
Attributes 
The impact of a temporal indicator is determined 
by its position in a sentence. In PDT and NBC, we 
consider an indicator located in four positions: (1) 
BEFORE the first event; (2) AFTER the first event 
and BEFORE the second and it modifies the first 
event; (3) the same as (2) but it modifies the second 
event; and (4) AFTER the second event. Cases (2) 
and (3) are ambiguous. The positions of the temporal 
indicators are the same. But it is uncertain whether 
these indicators modify the first or the second event 
if there is no punctuation separating their roles. We 
introduce two methods, namely NA and SAP to 
check if the ambiguity affects the two learning ap-
proaches. 
N(atural) O(rder): the temporal indicators between 
the two events are extracted and compared accord-
ing to their occurrence in the sentences regardless 
which event they modify.  
S(eparate) A(uxiliary) and P(osition) words: we 
try to resolve the above ambiguity with the gram-
matical features of the indicators. In this method, 
we assume that an indicator modifies the first 
event if it is an auxiliary word (e.g. ?), a trend 
verb (e.g. ??) or a position word (e.g. ?); oth-
erwise it modifies the second event. 
Temporal indicators are either tense/aspect or con-
nectives (see Section 2.2). Intuitively, it seems that 
classification could be better achieved if connective 
features are isolated from tense/ aspect features, 
allowing like to be compared with like. Methods 
SC1 and SC2 are designed based on this assumption. 
Table 2 shows the effect the different classification 
methods. 
SC1 (Separate Connecting words 1): it separates 
conjunctions and verbs relating to causality from 
others. They are assumed to contribute to dis-
course structure (intra- or inter-sentence structure), 
and the others contribute to the tense/aspect ex-
pressions for each individual event. They are built 
into 2 separate attributes, one for each event. 
SC2 (Separate Connecting words 2): it is the same 
as SC1 except that it combines the connecting 
word pairs (i.e. as a single pattern) into one attrib-
ute. 
EC (Event Class): it takes event classes into con-
sideration. 
Accuracy Method PDT NBC 
NO 82.00% 81.00% 
SAP 82.20% 81.50% 
SAP +SC1 80.20% 78.00% 
SAP +SC2 81.70% 79.20% 
SAP +EC 85.70% 82.25% 
Table 2. Effect of encoding linguistic features in the dif-
ferent ways 
4.2 Impact of Individual Features 
From linguistic perspectives, 13 features (see Ta-
ble 1) are useful for relative relation resolution. To 
examine the impact of each individual feature, we 
feed a single linguistic feature to the PDT learning 
algorithm one at a time and study the accuracy of the 
resultant classifier. The experimental results are 
given in Table 3. It shows that event classes have 
greatest accuracy, followed by conjunctions in the 
second place, and adverbs in the third. 
Feature Accuracy Feature Accuracy
PT 50.5% VA 56.5% 
VS 54% C 62% 
VC 54% U 51.5% 
TR 50.5% T 57.2% 
P 52.2 % D 61.7% 
PS 58.7% EC 68.2% 
VS 51.2% None 50.5% 
Table 3. Impact of individual linguistic features 
4.3 Discussions 
Analysis of the results in Tables 2 and 3 reveals 
some linguistic insights: 
1. In a situation where temporal indicators appear 
between two events and there is no punctuation 
mark separating them, POS information help re-
duce the ambiguity. Compared with NO, SAP 
shows a slight improvement from 82% to 82.2%. 
But the improvement seems trivial and is not as 
good as our prediction. This might due to the 
small percent of such cases in the corpus. 
2. Separating conjunctions and verbs relating to 
causality from others is ineffective. This reveals 
the complexity of Chinese in connecting expres-
sions. It is because other words (such as adverbs, 
proposition and position words) also serve such 
a function. Meanwhile, experiments based on 
SC1 and SC2 suggest that the connecting ex-
pressions generally involve more than one word 
or phrase. Although the words in a connecting 
expression are separated in a sentence, the action 
is indeed interactive. It would be more useful to 
regard them as one attribute. 
3. The effect of event classification is striking. 
Taking this feature into account, the accuracies 
of both PDT and NB improved significantly. As 
a matter of fact, different event classes may in-
troduce different relations even if they are con-
strained by the same temporal indicators. 
4.4 Collaborative Bootstrapping 
Table 4 presents the evaluation results of the four 
different classification approaches. DM is the default 
model, which classifies all incoming cases as the 
most likely class. It is used as evaluation baseline. 
Compare with DM, PDT and NBC show improve-
ment in accuracy (i.e. above 60% improvement). 
And CB in turn outperforms PDT and NBC. This 
proves that using unlabeled data to boost the per-
formance of the two classifiers is effective. 
Accuracy Approach Close test Open test 
DM 50.50% 55.00% 
NBC 82.25% 72.00% 
PDT 85.70% 74.00% 
CB 88.70% 78.00% 
Table 4. Evaluation of NBC, PDT and CB approaches 
5 Conclusions 
Relative temporal relation resolution received 
growing attentions in recent years. It is important for 
many natural language processing applications, such 
as information extraction and machine translation. 
This topic, however, has not been well studied, es-
pecially in Chinese. In this paper, we propose a 
model for relative temporal relation resolution in 
Chinese. Our model combines linguistic knowledge 
and machine learning approaches. Two learning ap-
proaches, namely probabilistic decision tree (PDT) 
and naive Bayesian classifier (NBC) and 13 linguis-
tic features are employed. Due to the limited labeled 
cases, we also propose a collaborative bootstrapping 
technique to improve learning performance. The 
experimental results show that our approaches are 
encouraging. To our knowledge, this is the first at-
tempt of collaborative bootstrapping, which involves 
two heterogeneous classifiers, in NLP application. 
This lays down the main contribution of our research. 
In this pilot work, temporal indicators are selected 
based on linguistic knowledge. It is time-consuming 
and could be error-prone. This suggests two direc-
tions for future studies. We will try to automate or at 
least semi-automate feature selection process. An-
other future work worth investigating is temporal 
indicator clustering. There are two methods we 
could investigate, i.e. clustering the recognized indi-
cators which occur in training corpus according to 
co-occurrence information or grouping them into 
two semantic roles, one related to tense/aspect ex-
pressions and the other to connecting expressions 
between two events. 
 
Acknowledgements 
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong 
(RGC reference number PolyU5085/02E) and 
CUHK Strategic Grant (account number 4410001). 
 
References 
Allen J., 1981. An Interval-based Represent Action 
of Temporal Knowledge. In Proceedings of 7th In-
ternational Joint Conference on Artificial Intelli-
gence, pages 221-226. Los Altos, CA. 
Blum, A. and Mitchell T., 1998. Combining Labeled 
and Unlabeled Data with Co-Training. In Proceed-
ings of the Eleventh Annual Conference on Com-
putational Learning Theory, Madison, Wisconsin, 
pages 92-100 
Bruce B., 1972. A Model for Temporal References 
and its Application in Question-Answering Pro-
gram. Artificial Intelligence, 3(1):1-25. 
Collins M. and Singer Y, 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceed-
ings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 189-196. Uni-
versity of Maryland. 
Dorr B. and Gaasterland T., 2002. Constraints on the 
Generation of Tense, Aspect, and Connecting 
Words from Temporal Expressions. (submitted to 
JAIR) 
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of the 7th European 
Meeting of the Association for Computational 
Linguistics, pages 253-260. Dublin, Ireland.  
Lascarides A., Asher N. and Oberlander J., 1992. 
Inferring Discourse Relations in Context. In 
Proceedings of the 30th Meeting of the 
Association for Computational Linguistics, pages 
1-8, Newark, Del. 
Li W.J. and Wong K.F., 2002. A Word-based Ap-
proach for Modeling and Discovering Temporal 
Relations Embedded in Chinese Sentences, ACM 
Transaction on Asian Language Processing, 
1(3):173-206. 
Ma J. and Knight B., 1994. A General Temporal 
Theory. The Computer Journal, 37(2):114- 123. 
M?ntaras L., 1991. A Distance-based Attribute Se-
lection Measure for Decision Tree Induction. Ma-
chine Learning, 6(1): 81?92. 
M?rquez L., Padr? L. and Rodr?guez H., 2000. A 
Machine Learning Approach to POS Tagging. 
Machine Learning, 39(1):59-91. Kluwer Aca-
demic Publishers. 
Partee, B., 1984. Nominal and Temporal Anaphora. 
Linguistics and Philosophy, 7(3):287-324. 
Quinlan J., 1993. C4.5 Programs for Machine 
Learning. Morgan Kauman Press. 
Reichenbach H., 1947. Elements of Symbolic Logic. 
Berkeley CA, University of California Press.  
Siegel E. and McKeown K., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving 
Aspectual Classification and Revealing Linguistic 
Insights. Computational Linguistics, 26(4): 595-
627. 
Wiebe, J.M., O'Hara, T.P., Ohrstrom-Sandgren, T. 
and McKeever, K.J, 1998. An Empirical Approach 
to Temporal Reference Resolution. Journal of Ar-
tificial Intelligence Research, 9:247-293. 
Wong F., Li W., Yuan C., etc., 2002. Temporal Rep-
resentation and Classification in Chinese. Interna-
tional Journal of Computer Processing of Oriental 
Languages, 15(2):211-230. 
Yarowsky D., 1994. Decision Lists for Lexical Am-
biguity Resolution: Application to the Accent Res-
toration in Spanish and French. In Proceeding of 
the 32rd Annual Meeting of ACL, San Francisco, 
CA. 
Zhou X., Dillon T., 1991. A Statistical-heuristic Fea-
ture Selection Criterion for Decision Tree Induc-
tion. IEEE Transaction on Pattern Analysis and 
Machine Intelligence, 13(8): 834-841. 
 
  
Combining Linguistic Features with Weighted Bayesian Classifier 
for Temporal Reference Processing 
 
Guihong Cao 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong 
csghcao@comp.polyu.edu.hk 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
cswjli@comp.polyu.edu.hk 
Kam-Fai Wong 
Department of Systems Engineering and Engineering 
Management 
The Chinese University of Hong Kong, Hong Kong 
kfwong@se.cuhk.edu.hk 
Chunfa Yuan 
Department of Computer Science and Technology 
Tsinghua University, Beijing, China. 
cfyuan@tsinghua.edu.cn 
 
Abstract 
Temporal reference is an issue of determining 
how events relate to one another. Determining 
temporal relations relies on the combination of 
the information, which is explicit or implicit in 
a language. This paper reports a computational 
model for determining temporal relations in 
Chinese. The model takes into account the ef-
fects of linguistic features, such as tense/aspect, 
temporal connectives, and discourse structures, 
and makes use of the fact that events are repre-
sented in different temporal structures. A ma-
chine learning approach, Weighted Bayesian 
Classifier, is developed to map their combined 
effects to the corresponding relations. An em-
pirical study is conducted to investigate differ-
ent combination methods, including lexical-
based, grammatical-based, and role-based 
methods. When used in combination, the 
weights of the features may not be equal. Incor-
porating with an optimization algorithm, the 
weights are fine tuned and the improvement is 
remarkable. 
 
1 Introduction 
Temporal information describes changes and time 
of the changes. In a language, the time of an event 
may be specified explicitly, for example ????
1997??????????? (They solved the traf-
fic problem of the city in 1997)?; or it may be related 
to the time of another event, for example ?????
???, ???????????? (They solved the 
traffic problem of the city after the street bridge had 
been built?. Temporal reference describes how 
events relate to one another, which is essential to 
natural language processing (NLP). Its major appli-
cations cover syntactic structural disambiguation 
(Brent, 1990), information extraction and question 
answering (Li, 2002), language generation and ma-
chine translation (Dorr, 2002). 
Many researchers have attempted to characterize 
the nature of temporal reference in a discourse. Iden-
tifying temporal relations1 between two events de-
                                                 
1 The relations under examined include both intra-sentence and inter-
pends on a combination of information resources. 
This information is provided by explicit tense and 
aspect markers, implicit event classes or discourse 
structures. It has been used to explain semantics of 
temporal expressions (Moens, 1988; Webber, 1988), 
to constrain possible temporal interpretations 
(Hitzeman, 1995; Sing, 1997), or to generate appro-
priate temporally conjoined clauses (Dorr, 2002). 
The purpose of our work is to develop a computa-
tional model, which automatically determines tempo-
ral relations in Chinese. While temporal reference 
interpretation in English has been well studied, Chi-
nese has been rarely discussed. In our study, thirteen 
related features are identified from linguistic per-
spective. How to combine these features and how to 
map their combined effects to the corresponding rela-
tions are the critical issues to be addressed in this 
paper. 
Previous work was limited in that they just con-
structed constraint or preference rules for some rep-
resentative examples. These methods are ineffective 
for computing purpose, especially when a large 
number of the features are involved and the interac-
tion among them is unclear. Therefore, a machine 
learning approach is applied and the empirical stud-
ies are carried out in our work. 
The rest of this paper is organized as follows. Sec-
tion 2 introduces temporal relation representations. 
Section 3 provides linguistic background of temporal 
reference and investigates linguistic features for de-
termining temporal relations in Chinese. Section 4 
explains the methods used to combine linguistic fea-
tures with Bayesian Classifier. It is followed by a 
description of the optimization algorithm which is 
used for estimating feature weights in Section 5. Fi-
nally, Section 6 concludes the paper. 
2 Representing Temporal Relations 
With the growing interests to temporal information 
processing in NLP, a variety of temporal systems 
have been introduced to accommodate the character-
istics of temporal information. In order to process 
temporal reference in a discourse, a formal represen-
                                                                            
sentence relations. 
  
tation of temporal relations is required. Among those 
who worked on representing or explaining temporal 
relations, some have taken the work of Reichenbach 
(Reichenbach, 1947) as a starting point, while others 
based their works on Allen?s (Allen, 1983). 
Reichenbach proposed a point-based temporal the-
ory. Reichenbach?s representation associated English 
tenses and aspects with three time points, namely 
event time (E), speech time (S) and reference time 
(R). The reference of E-R and R-S was either before 
(or after in reverse order) or simultaneous. This the-
ory was later enhanced by Bruce who defined seven 
temporal relations (Bruce, 1972). Given two durative 
events, the interval relations between them were 
modeled by the order between the greatest lower 
bounding point and least upper bounding point of the 
two events. In the other camp, instead of adopting 
time points, Allen took intervals as temporal primi-
tives to facilitate temporal reasoning and introduced 
thirteen basic relations. In this interval-based repre-
sentation, points were relegated to a subsidiary status 
as ?meeting places? of intervals. An extension to 
Allen?s theory, which treated both points and inter-
vals as primitives on an equal footing, was later in-
vestigated by Knight and Ma (Knight, 1994). 
In natural languages, events described can be ei-
ther punctual or durative in nature. A punctual event, 
e.g., ?? (explore), occurs instantaneously. It takes 
time but does not last in a sense that it lacks of a 
process of change. It is adequate to represent a punc-
tual event with a simple point structure. Whilst, a 
durative event, e.g., ?? (built a house), is more 
complex and its accomplishment as a whole involves 
a process spreading in time. Representing a durative 
event requires an interval representation. For this 
reason, Knight and Ma?s model is adopted in our 
work (see Figure 1). Taking the sentence ?????
???, ???????????? (They solved the 
traffic problem of the city after the street bridge had 
been built)? as an example, the relation held between 
building the bridge (i.e., an interval) and solving the 
problem (i.e., a point) is BEFORE. 
 
Figure 1 13 relations represented with points and intervals 
3 Linguistic Background of Temporal Refer-
ence in a Discourse 
3.1 Literature Review 
There were a number of theories in the literature 
about how temporal relations between events can be 
determined in English. Most of the researches on 
temporal reference were based on Reichenbach?s 
notion of tense/aspect structure, which was known as 
Basic Tense Structure (BTS). As for relating two 
events adjoined by a temporal/causal connective, 
Hornstein (Hornstein, 1990) proposed a neo-
Reichenbach structure which organized the BTSs 
into a Complex Tense Structure (CTS). It has been 
argued that all sentences containing a matrix and an 
adjunct clause were subject to linguistic constraints 
on tense structure regardless of the lexical words in-
cluded in the sentence. Generally, constraints were 
used to support syntactic disambiguation (Brent, 
1990) or to generate acceptable sentences (Dorr, 
2002). 
In a given CTS, a past perfect clause should pre-
cede the event described by a simple past clause. 
However, the order of two events in CTS does not 
necessarily correspond to the order imposed by the 
interpretation of the connective (Dorr, 2002). Tem-
poral/casual connective, such as ?after?, ?before? or 
?because?, can supply explicit information about the 
temporal ordering of events. Passonneau (Passon-
neau, 1988), Brent (Brent, 1990 and Sing (Sing, 1997) 
determined intra-sentential relations by accounting 
for temporal or causal connectives. Dorr and Gaast-
erland (Dorr, 2002), on the other hand, studied how 
to generate the sentences which reflect event tempo-
ral relations by selecting proper connecting words. 
However, temporal connectives can be ambiguous. 
For instance, a ?when? clause permits many possible 
temporal relations. 
Several researchers have developed the models 
that incorporated aspectual types (such as those dis-
tinct from states, processes and events) to interpret 
temporal relations between clauses connected with 
?when?. Moens and Steedmen (Moens, 1988) devel-
oped a tripartite structure of events2, and emphasized 
it was the notion of causation and consequence that 
played a central role in defining temporal relations of 
events. Webber (Webber, 1988) improved upon the 
above work by specifying rules for how events are 
related to one another in a discourse and Sing and 
Sing defined semantic constraints through which 
events can be related (Sing, 1997). The importance 
of aspectual information in retrieving proper aspects 
and connectives for sentence generation was also 
recognized by Dorr and Gaasterland (Dorr, 2002). 
Some literature claimed that discourse structures 
suggested temporal relations. Lascarides and Asher 
(Lascarides, 1991) investigated various contextual 
effects on rhetorical relations (such as narration, 
elaboration, explanation, background and result). 
They corresponded each of the discourse relations to 
a kind of temporal relation. Later, Hitzeman (Hitze-
man, 1995) described a method for analyzing tempo-
ral structure of a discourse by taking into account the 
effects of tense, aspect, temporal adverbials and rhe-
                                                 
2  The structure comprises a culmination, an associated preparatory 
process and a consequence state. 
A punctual event (i.e. represented in time point) 
A durative event (i.e. represented in time interval) 
BEFORE/AFTER 
MEETS/MET-BY 
OVERLAPS/OVERLAPPED-BY 
STARTS/STARTED-BY 
DURING/CONTAINS 
FINISHES/FINISHED-BY 
SAME-AS 
  
torical relations. A hierarchy of rhetorical and tempo-
ral relations was adopted so that they could mutually 
constrain each other.  
 To summarize, the interpretation of temporal rela-
tions draws on the combination of various informa-
tion resources, including explicit tense/aspect and 
connectives (temporal or otherwise), temporal 
classes implicit in events, or rhetorical relations hid-
den in a discourse. This conclusion, although drawn 
from the studies of English, provides the common 
understanding on what information is required for 
determining temporal relations across languages. 
3.2 Linguistic Features for Determining Tem-
poral Relations in Chinese 
Thirteen related linguistic features are recognized 
for determining Chinese temporal relations in this 
paper (See Table 1). The selected features are scat-
tered in various grammatical categories due to the 
unique nature of language, but they fall into the fol-
lowing three groups. 
(1) Tense/aspect in English is manifested by verb 
inflections. But such morphological variations are 
inapplicable to Chinese verbs. Instead, they are 
conveyed lexically. In other words, tense and as-
pect in Chinese are expressed using a combination 
of, for example, time words, auxiliaries, temporal 
position words, adverbs and prepositions, and 
particular verbs. They are known as Tense/Aspect 
Markers. 
(2) Temporal Connectives in English primarily in-
volve conjunctions, such as ?after? and ?before?, 
which are the key components in discourse struc-
tures. In Chinese, however, conjunctions, conjunc-
tive adverbs, prepositions and position words, or 
their combinations are required to represent 
connectives. A few verbs that express cause/effect 
imply a temporal relation. They are also regarded 
as a feature relating to discourse structure3. The 
words which contribute to the tense/aspect and 
temporal connective expressions are explicit in a 
sentence and generally known as Temporal Indica-
                                                 
3 The casual conjunctions such as ?because? are included in this 
group. 
tors. 
(3) Event Classes are implicit in a sentence. Events 
can be classified according to their inherent tem-
poral characteristics, such as the degree of telicity 
and atomicity. The four widespread accepted tem-
poral classes are state, process, punctual event and 
developing event (Li, 2002). Based on their 
classes, events interact with the tense/aspect of 
verbs to determine the temporal relations between 
two events. 
Temporal indicators and event classes are both re-
ferred to as Linguistic Features. Table 1 shows the 
association between a temporal indicator and its ef-
fects. Note that the association is not one-to-one. For 
example, adverbs affect tense/aspect (e.g. ?, being) 
as well as discourse structure (e.g. ?, at the same 
time). For another example, tense/aspect can be 
jointly affected by auxiliary words (e.g. ? , 
were/was), trend verbs (??, begin to), and so on. 
Obviously, it is not a simple task to map the com-
bined effects of the thirteen linguistic features to the 
corresponding relations. Therefore, a machine learn-
ing approach is proposed, which investigates how 
these features contribute to the task and how they 
should be combined. 
4 Combining Linguistic Features with Machine 
Learning Approach 
Previous efforts in corpus-based NLP have incor-
porated machine learning methods to coordinate mul-
tiple linguistic features, for example, in accent resto-
ration (Yarowsky, 1994) and event classification 
(Siegel, 1998).  
Temporal relation determination can be modeled 
as a relation classification task. We formulate the 
thirteen temporal relations (see Figure 1) as the 
classes to be decided by a classifier. The classifica-
tion process is to assign an event pair to one class 
according to their linguistic features. There existed 
numerous classification algorithms based upon su-
pervised learning principle. One of the most effective 
classifiers is Bayesian Classifier, introduced by Duda 
and Hart (Duda, 1973) and analyzed in more detail 
by Langley and Thompson (Langley, 1992). Its pre-
dictive performance is competitive with state-of-the-
Linguistic Feature Symbol POS Tag Effect Example 
With/Without punctuations PT Not Applicable Not Applicable Not Applicable 
Speech verbs VS TI_vs Tense ??, ??, ? 
Trend verbs TR TI_tr Aspect ??, ?? 
Preposition words P TI_p Discourse Structure/Aspect ?, ?, ? 
Position words PS TI_f Discourse Structure ?, ?, ?? 
Verbs with verb objects VV TI_vv Tense/Aspect ??, ??, ? 
Verbs expressing wish/hope VA TI_va Tense ??, ?, ? 
Verbs related to causality VC TI_vc Discourse Structure ??, ??, ?? 
Conjunctive words C TI_c Discourse Structure ?, ??, ?? 
Auxiliary words U TI_u Aspect ?, ?, ? 
Time words T TI_t Tense ??, ??, ?? 
Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ??, ? 
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event, De-
veloping Event, Process 
Table 1 Linguistic features: eleven temporal indicators and one event class 
  
art classifiers, such as C4.5 and SVM (Friedman, 
1997). 
4.1 Bayesian Classifier 
Given the class c , Bayesian Classifier learns from 
training data the conditional probability of each at-
tribute. Classification is performed by applying 
Bayes rule to compute the posterior probability of c  
given a particular instance x , and then predicting the 
class with the highest posterior probability ratio. Let 
],...,,,,[ 2121 nttteex = , Eee ?21 ,  are the two event 
classes and Tttt n ?,...,, 21 are the temporal indicators 
(i.e. the words). E is the set of event classes. T is the 
set of temporal indicators. Then x is classified as: 
???
?
???
?=
),...,,,,|(
),...,,,,|(
logmaxarg
2121
2121*
n
n
c ttteecP
ttteecP
c  (E1)
where c denotes the classes different from c . As-
suming event classes are independent of temporal 
indicators given c , we have: 
???
?
???
?=
???
?
???
?
)()|,...,,,,(
)()|,...,,,,(
log
),...,,,,|(
),...,,,,|(
log
2121
2121
2121
2121
cPcttteeP
cPcttteeP
ttteecP
ttteecP
n
n
n
n
                          (E2)
???
?
???
?+???
?
???
?+???
?
???
?=
)|,...,,(
)|,...,,(
log
)|,(
)|,(
log
)(
)(log
21
21
21
21
ctttP
ctttP
ceeP
ceeP
cP
cP
n
n  
Assuming temporal indicators are independent of 
each other, we have 
?
=
= n
i i
i
n
n
ctP
ctP
ctttP
ctttP
121
21
)|(
)|(
)|,...,,(
)|,...,,( ,    ( ni ,...2,1= ) (E3)
A Na?ve Bayesian Classifier assumes strict inde-
pendence among all attributes. However, this as-
sumption is not satisfactory in the context of tempo-
ral relation determination. For example, if the 
relation between 1e  and 2e  is SAME_AS, 1e  and 2e  
have to be identical. We release the independence 
assumption for 1e and 2e , and decompose the second 
part of (E2) as: 
),|()|(
),|()|(
)|,(
)|,(
121
121
21
21
ceePceP
ceePceP
ceeP
ceeP =  (E4)
Estimation of ),|( 12 ceep is motivated by Absolute 
Discounting N-Gram language model (Goodman, 
2001): 
??
??
?
=
>?=
0),,( if     )|(),(
0),,( if    
),(
),,(
),|(
1221
12
1
12
12
ceeCcePce
ceeC
ceC
DceeC
ceP e
?
 
 
(E5) 
here D is the discount factor and is set to 0.5 experi-
mentally. From the fact that 1),|(
2
12 =?
e
ceeP , we get: 
?
?
>
>
?
?
=
0),,(|
2
0),,(|
12
1
122
122
)|(1
),|(1
),(
ceeCe
ceeCe
ceP
ceeP
ce?  
 
(E6)
)|( ctp i and )|( cep i  are estimated by MLE with 
Dirichlet Smoothing method: 
?
?
+
+=
Tt
i
i
i
i
TuctC
uctC
ctP
||),(
),(
)|(      ( ni ,...2,1= )  
(E7)
?
?
+
+=
Ee
i
i
i
i
EuceC
uceC
ceP
||),(
),(
)|(     ( 2,1=i )  
(E8)
where u (=0.5) is the smoothing factor. Then, 
)|( ctp i , )|( cep i and ),|( 12 ceeP  can be estimated 
with (E5) - (E8) by substituting c  with c . 
4.2 Estimating )|,...,( 21 ctttP n  with Lexical-POS 
Information 
The effects of a temporal indicator are constrained 
by its positions in a sentence. For instance, the con-
junctive word ?? (because) may represent the dif-
ferent relations when it occurs before or after the first 
event. Therefore, in estimating )|,...,( 21 ctttp n , we 
consider an indicator located in three positions: (1) 
BEFORE the first event; (2) AFTER the first event 
and BEFORE the second and it modifies the first 
event; (3) the same as (2) but it modifies the second 
event; and (4) AFTER the second event. Note that 
cases (2) and (3) are ambiguous. The positions of the 
temporal indicators are the same. But it is uncertain 
whether these indicators modify the first or the sec-
ond event if there is no punctuation (such as comma, 
period, exclamation or question mark) separating 
their roles. The ambiguity is resolved by using POS 
information. We assume that an indicator modifies 
the first event if it is an auxiliary word, a trend word 
or a position word; otherwise it modifies the second. 
Thus, we rewrite )|,...,( 21 ctttP n as ,,...,( 1111 nttP  
)|,...,,...,,,...,
432 441331221
ctttttt nnn , where jn is the total 
number of the temporal indicators occurring in the 
position j . 4,3,2,1=j  represents the four positions 
and nn
j
j =?=
4
1
. Assuming jit are independent of each 
other, then ?
=
n
i
i ctP
1
)|( in (E3) is revised as 
??
= =
4
1 1
)|(
j
n
i
ji
j
ctP . Accordingly, (E7) is revised as: 
?
?
+
+=
Tt
ji
ji
ji
ji
TuctC
uctC
ctP
||),(
),(
)|(  
( 4,3,2,1=j  and jni ,...2,1= ) 
 
(E7?)
In addition to taking positions into account, we 
further classify the temporal indicators into two 
groups according to their grammatical categories or 
semantic roles. The rationale of grouping will be 
demonstrated in Section 4.3. 
4.3 Experimental Results 
Several experiments have been designed to evalu-
ate the proposed Bayesian Classifier in combining 
linguistic features for temporal relation determination 
and to reveal the impact of linguistic features on 
learning performance. 700 instances are extracted 
from Ta Kong Pao (a local Hong Kong Chinese 
newspaper) financial version. Among them, 500 are 
used as training data, and 200 as test data, which are 
  
partitioned equally into two sets. One is similar as 
training data in class distribution, while the other is 
quite different. 209 lexical words, gathered from lin-
guistic books and corpus, are used as the temporal 
indicators and manually marked with the tags given 
in Table 1. 
4.3.1 Impact of Individual Features 
From linguistic perspective, the thirteen features 
(see Table 1) are useful for temporal relation deter-
mination. To examine the impact of each individual 
feature, we feed a single linguistic feature to the 
Bayesian Classifier learning algorithm one at a time 
and study the accuracy of the resultant classifier. The 
experimental results are given in Table 2. It shows 
that event classes have greatest accuracy, followed 
by conjunctions in the second place, and adverbs in 
the third in the close test. Since punctuation shows 
no contribution, we only use it as a syntactic feature 
to differentiate cases (2) and (3) mentioned in Sec-
tion 4.2. 
4.3.2 Features in Combination 
We now use Bayesian Classifier introduced in Sec-
tions 4.1 and 4.2 to combine all the related temporal 
indicators and event classes, since none of the fea-
tures can achieve a good result alone. The simplest 
way is to combine the features without distinction. 
The conditional probability )|( ctP ji is estimated by 
(E7?). This model is called Ungrouped Model (UG). 
However, as illustrated in table 1, the temporal in-
dicators play different roles in building temporal ref-
erence. It is not reasonable to treat them equally. We 
claim that the temporal indicators have two functions, 
i.e., representing the connections of the clauses, or 
representing the tense/aspect of the events. We iden-
tify them as connective words or tense/aspect mark-
ers and separate them into two groups. This allows 
features to be compared with those in the same group. 
Let ],[ 21 TTT = , where 1T is the set of connective 
words and 2T is the set of tense/aspect markers. We 
have 1112
1
1 ,..,, Tttt m ? and 222221 ,..,, Tttt l ? , m and l are 
the number of the connective words and the 
tense/aspect markers in a sentence respectively. We 
assume that the occurrences of the two groups are 
independent. By taking both grouping and position 
features into account, we replace ?
=
n
i
i ctP
1
)|(  with 
???
= = =
2
1
4
1 1
)|(
k j
n
i
k
ji
k
j
ctP , 2,1=k  represents the two groups 
and j
k
k
j nn =?=
2
1
. To build the grouping-based Bayes-
ian Classifier, (E7?) is modified as: 
?
?
+
+=
kk
ji Tt
kk
ji
k
jik
ji TuctC
uctC
ctP
||),(
),(
)|(  
( 2,1=k , 4,3,2,1=j  and jni ,...2,1= ) 
 
(E7??)
4.3.3 Grouping Features by Grammatical Cate-
gories or Semantic Roles 
We partition temporal indicators into connective 
words and tense/aspect markers in two ways. One is 
simply based on their grammatical categories (i.e. 
POS information). It separates conjunctions (e.g., ?
?, after; ??, because) and verbs relating to causal-
ity (e.g., ??, cause) from others. They are assumed 
to be connective words (i.e. 1T? ), while others are 
tense/aspect markers (i.e. 2T? ). This model is called 
Grammatical Function based Grouping Model (GFG). 
Unfortunately, such a separation is ineffective. In 
comparison with UG, the performance of GFG de-
creases as shown in figure 2. This reveals the com-
plexity of Chinese in connecting expressions. It 
arises from the fact that some other words, such as 
adverbs (e.g., ???, meanwhile), prepositions (e.g., 
?, at) and position words (e.g., ??, before), can 
also serve such a connecting function (see Table 1). 
Actually, the roles of the words falling into these 
grammatical categories are ambiguous. For instance, 
the adverb? can express an event happened in the 
past, e.g., ????????? (He just finished the 
report)?. It can be also used in a connecting expres-
sion (such as ????), e.g., ??????????
??? (He went to the library after he had finished 
the report)?. 
This finding suggests that temporal indicators 
should be divided into two groups according to their 
semantic roles rather than grammatical categories. 
Therefore we propose the third model, namely 
Semantic Role based Grouping Model (SRG), in 
which the indicators are manually re-marked as 
TI_j_pos or TI_at_pos4. 
Figure 2 shows the accuracies of four models (i.e. 
DM. UG, GFG and SRG) based on the three tests. 
Test 1 is the close test carried out on training data 
and tests 2 and 3 are open tests performed on differ-
ent test data. DM (i.e., Default Model) assigns all 
incoming cases with the most likely class and it is 
used as evaluation baseline. In our case, it is 
SAME_AS, which holds 50.2% in training data. 
SRG model outperforms UG and GFG models. 
These results validate our previous assumption em-
pirically. 
                                                 
4 ?j? and ?at? are the tags representing connecting and tense/aspect 
roles respectively. ?pos? is the POS tag of the temporal indicator TI. 
Accuracy Accuracy  
Feature Close 
test 
Open 
test 1 
Open 
test 2 
 
Feature Close 
test 
Open 
test 1
Open 
test 2
VS 53.4% 48% 30% VA 57% 50% 37%
VC 56.6% 56% 49% C 62.6% 52% 45%
TR 50.2% 46% 28% U 51.8% 50% 32%
P 52.4% 49% 30% T 57.2% 48% 32%
PS 59% 53% 38% D 59.6% 55% 47%
VV 51% 49% 29% EC 72.4% 69% 68%
Table 2 Impact of each individual linguistic feature 
  
20%
30%
40%
50%
60%
70%
80%
90%
Close Test Open Test1 Open Test2
A
cc
ur
ac
y
DM UG GFG SRG
 
Figure 2 Comparing DM, UG, GFG and SRG models 
4.3.4 Impact of Semantic Roles in SRG Model 
When the temporal indicators are classified into 
two groups based on their semantic roles in SRG 
model, there are three types of linguistic features 
used in the Bayesian Classifier, i.e., tense/aspect 
markers, connective words and event classes. A set 
of experiments are conducted to investigate the im-
pacts of each individual feature type and the impacts 
when they are used in combination (shown in Table 
3). We find that the performance of methods 1 and 2 
in the open tests drops dramatically compared with 
those in the close test. But the predictive strength of 
event classes in method 3 is surprisingly high. Two 
conclusions are thus drawn. Firstly, the models using 
tense/aspect markers and connective words are more 
likely to encounter over-fitting problem with insuffi-
cient training data. Secondly, different features have 
varied weights. We then incorporate an optimization 
approach to adjust the weights of the three types of 
features, and propose an algorithm to tackle over-
fitting problem in the next section. 
Method Semantic Groups 
Close 
test 
Open 
test 1 
Open 
test 2
1 Tense/aspect markers 71% 58% 40%
2 Connective words 75% 65% 57%
3 Event classes 66.6% 69% 68%
4 1+2 84.8% 70% 56%
5 1+3 76.6% 72% 66%
6 2+3 82.4% 84% 81%
7 1+2+3 89.8% 84% 80%
8 Default 50.2% 46% 28%
Table 3: Impact of Semantic Role based Groups 
5. Weighted Bayesian Classifier  
Let 1? , 2? , 3? be the weights of event classes, con-
nective words and tense/aspect markers respectively. 
Then the Weighted Bayesian Classifier is: 
???
?
???
?
),...,,,,|(
),...,,,,|(
log
2121
2121
n
n
ttteecP
ttteecP  
???
?
???
?+???
?
???
?=
)|,(
)|,(
log
)(
)(log
21
21
1 ceeP
ceeP
cP
cP ?                             (E9)
???
?
???
?+???
?
???
?+
)|,...,,(
)|,...,,(
log
)|,...,,(
)|,...,,(
log 22
2
2
1
22
2
2
1
311
2
1
1
11
2
1
1
2 ctttP
ctttP
ctttP
ctttP
l
l
m
m ??  
In order to estimate the weights, we need a suit-
able optimization approach to search for the opti-
mal value of ],,[ 321 ???  automatically. 
5.1 Estimating Weights with Simulated Anneal-
ing Algorithm 
Quite a lot optimization approaches are available 
to compute the optimal value of ],,[ 321 ??? . Here, 
Simulated Annealing algorithm is employed to per-
form the task, which is a general and powerful opti-
mization approach with excellent global convergence 
(Kirkpatrick, 1983). Figure 3 shows the procedure of 
searching for an optimal weight vector with the algo-
rithm. 
1. 1=k , )( 1?= kk tTt  
2. Generates a random change from the current weight vec-
tor iv . The updated weight vector is denoted by jv . Then 
computes the increasement of the objective function, i.e. 
)()( ij vfvf ?=? . 
3 Accepts jv as an optimal vector and substitutes iv with the 
following accept rate: 
??
??
?
??
?
? <
>
= 0 if  )exp(
0 if             1
)(
k
ji
t
vvP  
4 If kLk < , lets 1+= kk , goes to step 2. 
5 Else if fk Tt < , goes to step 1. 
6 Else stops looping and outputs the current optimal weight 
vector.  
Figure 3 Simulated Annealing algorithm 
In Figure 3, Markov chain length 20=kL ; tem-
perature update function ttT *9.0)( = ; starting point 
],,[ 03
0
2
0
1
0 ???=v =[1,1,1]; initial temperature 200 =t  
and final temperature 810?=ft . Note that the initial 
temperature is critical for a simulated annealing algo-
rithm (Kirkpatrick, 1983). Its value should assure 
that the initial accept rate is greater than 90%. 
5.2 K-fold Cross-Validation 
The accuracy of the classifier is defined as the ob-
jective function of the Simulated Annealing algo-
rithm illustrated in Figure 3. If it is evaluated with 
the accuracy over all training data, the Weighted 
Bayesian Classifier may trap into over-fitting prob-
lem and lower the performance due to insufficient 
data. To avoid this, we employ K-fold Cross-
Validation technique. It partitions the original set of 
data into K parts. One part is selected arbitrarily as 
evaluating data and the other K-1 parts as training 
data. Then K accuracies on evaluating data are ob-
tained after K iterations and their average is used as 
the objective function. 
5.3 Experimental Results 
Table 4 shows the result of the experiment which 
compares WSRG (Weighted SRG) with SRG. We 
use error reduction to evaluate the benefit from in-
corporating weight parameters into Bayesian Classi-
fier. It is defined as: 
SRG
WSRGSRG
rateerror
rateerrorrateerror
_
__
reductionerror 
?=  
  
The experimental results show that the Weighted 
Bayesian Classifier outperforms the Bayesian Classi-
fier significantly in the two open tests and it tackles 
the over-fitting problem well. To test Simulated An-
nealing algorithm?s global convergence, we ran-
domly choose several initial values and they finally 
converge to a small area [7.2?0.09, 5.8?0.02, 
3.0?0.02]. The empirical result demonstrates that the 
output of a Simulated Annealing algorithm is a 
global optimal weighting vector. 
6 Conclusions 
Temporal reference processing has received grow-
ing attentions in last decades. However this topic has 
not been well studied in Chinese. In this paper, we 
proposed a method to determine temporal relations in 
Chinese by employing linguistic knowledge and ma-
chine learning approaches. Thirteen related linguistic 
features were recognized and temporal indicators 
were further grouped with respect to grammatical 
functions or semantic roles. This allows features to 
be compared with those in the same group. To ac-
commodate the fact that the different types of fea-
tures support varied importance, we extended Na?ve 
Bayesian Classifier to Weighted Bayesian Classifier 
and applied Simulated Annealing algorithm to opti-
mize weight parameters. To avoid over-fitting prob-
lem, K-fold Cross-Validation technique was incorpo-
rated to evaluate the objective function of the optimi-
zation algorithm. Establishing the temporal relations 
between two events could be extended to provide a 
determination of the temporal relations among multi-
ple events in a discourse. With such an extension, 
this temporal analysis approach could be incorpo-
rated into various NLP applications, such as question 
answering and machine translation. 
Acknowledgements 
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong 
(RGC reference number PolyU5085/02E) and CUHK 
Strategic Grant (account number 4410001). 
References 
Allen J., 1983. Maintaining Knowledge about Temporal 
Intervals. Communications of the ACM, 26(11):832-
843. 
Brent M., 1990. A Simplified Theory of Tense Repre-
sentations and Constraints on Their Composition, In 
Proceedings of the 28th Annual Conference of the As-
sociation for Computational Linguistics, pages 119-
126. Pittsburgh. 
Bruce B., 1972. A Model for Temporal References and 
its Application in Question-Answering Program. Arti-
ficial Intelligence, 3(1):1-25. 
Dorr B. and Gaasterland T., 2002. Constraints on the 
Generation of Tense, Aspect, and Connecting Words 
from Temporal Expressions. submitted to Journal of 
Artificial Intelligence Research. 
Duda, R. O. and P. E. Hart, 1973. Pattern Classification 
and Scene Analysis. New York. 
Friedman N., Geiger D. and Goldszmidt M., 1997. 
Bayesian Network Classifiers. Machine Learning 
29:131-163, Kluwer Academic Publisher. 
Goodman J., 2001. A Bit of Progress in Language Mod-
eling. Microsoft Research Technical Report MSR-
TR-2001-72. 
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of Dis-
course. In Proceedings of the 7th European Meeting 
of the Association for Computational Linguistics, 
pages 253-260. Dublin, Ireland.  
Hornstein N., 1990. As Time Goes By. MIT Press, Cam-
bridge, MA. 
Kirkpatrick, S., Gelatt C.D., and Vecchi M.P., 1983. 
Optimization by Simulated Annealing. Science, 
220(4598): 671-680. 
Knight B. and Ma J., 1997. Temporal Management Us-
ing Relative Time in Knowledge-based Process Con-
trol, Engineering Applications of Artificial Intelli-
gence, 10(3):269-280.  
Langley, P.W. and Thompson K., 1992. An Analysis of 
Bayesian Classifiers. In Proceedings of the 10th Na-
tional Conference on Artificial Intelligence, pages 
223?228. San Jose, CA. 
Lascarides A. and Asher N., 1991. Discourse Relations 
and Defensible Knowledge. In Proceedings of the 29th 
Meeting of the Association for Computational Lin-
guistics, pages 55-62. Berkeley, USA. 
Li W.J. and Wong K.F., 2002. A Word-based Approach 
for Modeling and Discovering Temporal Relations 
Embedded in Chinese Sentences, ACM Transaction 
on Asian Language Processing, 1(3):173-206. 
Moens M. and Steedmen M., 1988. Temporal Ontology 
and Temporal Reference. Computational Linguistics, 
14(2):15-28. 
Passonneau R., 1988. A Computational Model of the 
Semantics of Tense and Aspect. Computational Lin-
guistics, 14(2):44-60. 
Reichenbach H., 1947. The Elements of Symbolic Logic. 
The Free Press, New York. 
Siegel E.V. and McKeown K.R., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving As-
pectual Classification and Revealing Linguistic In-
sights. Computational Linguistics, 26(4):595-627. 
Singh M. and Singh M., 1997. On the Temporal Struc-
ture of Events. In Proceedings of AAAI-97 Workshop 
on Spatial and Temporal Reasoning, pages 49-54. 
Providence, Rhode Island. 
Webber B., 1988. Tense as Discourse Anaphor. Compu-
tational Linguistics, 14(2):61-73.  
Yarowsky D., 1994. Decision Lists for Lexical Ambi-
guity Resolution: Application to the Accent Restora-
tion in Spanish and French. In Proceeding of the 32nd 
Annual Meeting of the Association for Computational 
Linguistics, pages 88-95. San Francisco, CA. 
Error Rate 
Model 
Close Test Open Test1 Open Test2
SRG 10.2% 16% 20% 
WSRG 12.4%% 11% 13% 
Error Reduction -21.57% 31.25% 35% 
Table 4 Compare WSRG with SRG on error rates 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 985?992
Manchester, August 2008
Extractive Summarization Using Supervised and Semi-supervised 
Learning 
Kam-Fai Wong*, Mingli Wu*?  
*Department of Systems Engineering and 
Engineering Management  
The Chinese University of Hong Kong 
New Territories, Hong Kong 
{kfwong,mlwu}@se.cuhk.edu.hk
Wenjie Li? 
?Department of Computing 
The Hong Kong Polytechnic University 
Kowloon, Hong Kong 
cswjli@comp.polyu.edu.hk 
 
 
Abstract 
It is difficult to identify sentence impor-
tance from a single point of view. In this 
paper, we propose a learning-based ap-
proach to combine various sentence fea-
tures. They are categorized as surface, 
content, relevance and event features. 
Surface features are related to extrinsic 
aspects of a sentence. Content features 
measure a sentence based on content-
conveying words. Event features repre-
sent sentences by events they contained. 
Relevance features evaluate a sentence 
from its relatedness with other sentences. 
Experiments show that the combined fea-
tures improved summarization perform-
ance significantly. Although the evalua-
tion results are encouraging, supervised 
learning approach requires much labeled 
data. Therefore we investigate co-training 
by combining labeled and unlabeled data. 
Experiments show that this semi-
supervised learning approach achieves 
comparable performance to its supervised 
counterpart and saves about half of the 
labeling time cost. 
1 Introduction 
1 Automatic text summarization involves con-
densing a document or a document set to produce 
a human comprehensible summary. Two kinds of 
summarization approaches were suggested in the 
past, i.e., extractive (Radev et al, 2004; Li et al, 
2006) and abstractive summarization (Dejong, 
1978). The abstractive approaches typically need 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
to ?understand? and then paraphrase the salient 
concepts across documents. Due to the limita-
tions in natural language processing technology, 
abstractive approaches are restricted to specific 
domains. In contrast, extractive approaches 
commonly select sentences that contain the most 
significant concepts in the documents. These ap-
proaches tend to be more practical. 
Recently various effective sentence features 
have been proposed for extractive summarization, 
such as signature word, event and sentence rele-
vance. Although encouraging results have been 
reported, most of these features are investigated 
individually. We argue that it is ineffective to 
identify sentence importance from a single point 
of view. Each sentence feature has its unique 
contribution, and combing them would be advan-
tageous. Therefore we investigate combined sen-
tence features for extractive summarization. To 
determine weights of different features, we em-
ploy a supervised learning framework to identify 
how likely a sentence is important. Some re-
searchers explored learning based summarization, 
but the new emerging features are not concerned, 
such as event features (Li et. al, 2006). 
We investigate the effectiveness of different 
sentence features with supervised learning to de-
cide which sentences are important for summari-
zation. After feature vectors of sentences are ex-
amined, a supervised learning classifier is then 
employed.  Particularly, considering the length of 
final summaries is fixed, candidate sentences are 
re-ranked. Finally, the top sentences are ex-
tracted to compile the final summaries. Experi-
ments show that combined features improve 
summarization performance significantly. 
Our supervised learning approach generates 
promising results based on combined features. 
However, it requires much labeled data. As this 
procedure is time consuming and costly, we in-
vestigate semi-supervised learning to combine 
labeled data and unlabeled data. A semi-
985
supervised learning classifier is used instead of a 
supervised one in our extractive summarization 
framework. Two classifiers are co-trained itera-
tively to exploit unlabeled data. In each iteration 
step, the unlabeled training examples with top 
classifying confidence are included in the labeled 
training set, and the two classifiers are trained on 
the new training data. Experiments show that the 
performance of our semi-supervised learning 
approach is comparable to its supervised learning 
counterpart and it can reduce the labeling time 
cost by 50%. 
The remainder of this paper is organized as 
follows. Section 2 gives related work and Section 
3 describes our learning-based extractive summa-
rization framework. Section 4 outlines the vari-
ous sentence features and Section 5 describes 
supervised/semi-supervised learning approaches. 
Section 6 presents experiments and results. Fi-
nally, Section 7 concludes the paper. 
2 Related Work 
Traditionally, features for summarization were 
studied separately. Radev et al (2004) reported 
that position and length are useful surface fea-
tures. They observed that sentences located at the 
document head most likely contained important 
information. Recently, content features were also 
well studied, including centroid (Radev et al, 
2004), signature terms (Lin and Hovy, 2000) and 
high frequency words (Nenkova e t al., 2006). 
Radev et al (2004) defined centroid words as 
those whose average tf*idf score were higher 
than a threshold. Lin and Hovy (2000) identified 
signature terms that were strongly associated 
with documents based on statistics measures. 
Nenkova et al (2006) later reported that high 
frequency words were crucial in reflecting the 
focus of the document.  
Bag of words is somewhat loose and omits 
structural information. Document structure is 
another possible feature for summarization. Bar-
zilay and Elhadad (1997) constructed lexical 
chains and extracted strong chains in summaries. 
Marcu (1997) parsed documents as rhetorical 
trees and identified important sentences based on 
the trees. However, only moderate results were 
reported. On the other hand, Dejong (1978) rep-
resented documents using predefined templates. 
The procedure to create and fill the templates 
was time consuming and it was hard to adapt the 
method to different domains.  
Recently, semi-structure events (Filatovia and 
Hatzivassiloglou, 2004; Li et al, 2006; Wu, 2006) 
have been investigated by many researchers as 
they balanced document representation with 
words and structures. They defined events as 
verbs (or action nouns) plus the associated 
named entities. For instance, given the sentence 
?Yasser Arafat on Tuesday accused the United 
States of threatening to kill PLO officials?, they 
first identified ?accused?, ?threatening? and 
?kill? as event terms; and ?Yasser Arafat?, 
?United States?, ?PLO? and ?Tuesday? as event 
elements. Encouraging results based on events 
were reported for news stories.  
From another point of view, sentences in a 
document are somehow connected. Sentence 
relevance has been used as an alternative means 
to identify important sentences. Erkan and Radev 
(2004) and Yoshioka (2004) evaluate the rele-
vance (similarity) between any two sentences 
first. Then a web analysis approach, PageRank, 
was used to select important sentences from a 
sentence map built on relevance. Promising re-
sults were reported. However, the combination of 
these features is not well studied. Wu et al (2007) 
conducted preliminary research on this problem, 
but event features were not considered. 
Normally labeling procedure in supervised 
learning is very time consuming. Blum and 
Mitchell (1998) proposed co-training approach to 
exploit labeled and unlabeled data. Promising 
results were reported from their experiments on 
web page classification. A number of successful 
studies emerged thereafter for other natural lan-
guage processing tasks, such as text classification 
(Denis and Gilleron, 2003), noun phrase chunk-
ing (Pierce and Cardie, 2001), parsing (Sarkar, 
2001) and reference or relation resolution (Mul-
ler et al, 2001; Li et al, 2004). To our knowl-
edge, there is little research in the application of 
co-training techniques to extractive summariza-
tion. 
3 The Framework for Extractive Sum-
marization 
Extractive summarization can be regarded as a 
classification problem. Given the features of a 
sentence, a machine-learning based classification 
model will judge how likely the sentence is im-
portant. The classification model can be super-
vised or semi-supervised learning. Supervised 
approaches normally perform better, but require 
more labeled training data. SVMs perform well 
in many classification problems. Thus we em-
ploy it for supervised learning. For semi-
supervised learning, we co-trained a probabilistic 
986
SVM and a Na?ve Bayesian classifier to exploit 
unlabeled data. 
 
Figure 1. Learning-based Extractive Summariza-
tion Framework 
The automatic summarization procedure is 
shown in Figure 1. First, each input sentence is 
examined by going through the pre-specified fea-
ture functions. The classification model will then 
predict the importance of each sentence accord-
ing to its feature values. A re-ranking algorithm 
is then used to revise the order. Finally, the top 
sentences are included in the summaries until the 
length limitation is reached. The re-ranking algo-
rithm is crucial, as more important content are 
expected to be contained in the final summary 
with fixed length. Important sentences above a 
threshold are regarded as candidates. The one 
with less words and located at the beginning part 
of a document is ranked first. The re-ranking al-
gorithm is described as follows. 
Ranki = RankPosi + RankLengthi  
where RankPosi is the rank of sentence i accord-
ing to its position in a document (i.e. the sentence 
no.) and RankLengthi is rank of sentence i ac-
cording to its length. 
4 Sentence Features for Extractive 
Summarization 
This section provides a detailed description on 
the four types of sentence features, i.e., surface, 
content, event and relevance features, which will 
be examined systematically. 
4.1 Surface Features 
Surface features are based on structure of 
documents or sentences, including sentence 
position in the document, the number of words in 
the sentence, and the number of quoted words in 
the sentence (see Table 1).  
 
Name Description 
Position 1/sentence no. 
Doc_First Whether it is the first sentence of a document  
Para_First Whether it is the first sentence of a paragraph 
Length The number of words in a sentence 
Quote The number of quoted words in a sen-tence  
Table 1. Types of surface features 
 
The intuition with respect to the importance of 
a sentence stems from the following observations: 
(1) the first sentence in a document or a para-
graph is important; (2) the sentences in the ear-
lier parts of a document is more important than 
sentences in later parts; (3) a sentence is impor-
tant if the number of words (except stop words) 
in it is within a certain range; (4) a sentence con-
taining too many quoted words is unimportant.  
4.2 Content Features 
We integrate three well-known sentence features 
based on content-bearing words i.e., centroid 
words, signature terms, and high frequency 
words. Both unigram and bigram representations 
have been investigated. Table 2 summarizes the 
six content features we studied.  
 
Name Description 
Centroid_Uni The sum of  the weights of cen-troid uni-gram  
Centroid_Bi The sum of  the weights of cen-troid bi-grams  
SigTerm_Uni The number of signature uni-grams  
SigTerm_Bi The number of signature bi-grams 
FreqWord_Uni The sum of  the weights of fre-quent uni-grams  
FreqWord_Bi The sum of  the weights of fre-quent bi-grams  
Table 2. Types of content features 
4.3 Event Features 
An event is comprised of an event term and asso-
ciated event elements. In this study, we choose 
verbs (such as ?elect and incorporate?) and ac-
tion nouns (such as ?election and incorporation?) 
as event terms that can characterize actions. They 
relate to ?did what?. One or more associated 
named entities are considered as event elements. 
Four types of named entities are currently under 
987
consideration. The GATE system (Cunningham 
et al, 2002) is used to tag named entities, which 
are categorized as <Person>, <Organization>, 
<Location> and <Date>. They convey the infor-
mation about ?who?, ?whom?, ?when? and 
?where?. A verb or an action noun is deemed an 
event term only when it appears at least once 
between two named entities. 
Event summarization approaches based on in-
stances or concepts are investigated. An occur-
rence of an event term (or event element) in a 
document is considered as an instance, while the 
collection of the same event terms (or event ele-
ments) is considered as a concept. Given a 
document set, instances of event terms and event 
elements are identified first. An event map is 
then built based on event instances or concepts 
(Wu , 2006; Li et al, 2006). PageRank algorithm 
is used to assign weight to each node (an instance 
or concept) in the event map. The final weight of 
a sentence is the sum of weights of event in-
stances contained in the sentence. 
4.4 Relevance Features 
Relevance features are incorporated to exploit 
inter-sentence relationships. It is assumed that: (1) 
sentences related to important sentences are im-
portant; (2) sentences related to many other sen-
tences are important. The first sentence in a 
document or a paragraph is important, and other 
sentences in a document are compared with the 
leading ones. Two types of sentence relevance, 
FirstRel_Doc and FirstRel_Para (see Table 3), 
are measured by comparing pairs of sentences 
using word-based cosine similarity. 
Another way to exploit sentence relevance is 
to build a sentence map. Every two sentences are 
regarded relevant if their similarity is above a 
threshold. Every two relevant sentences are con-
nected with a unidirectional link. Based on this 
map, PageRank algorithm is applied to evaluate 
the importance of a sentence. These relevance 
features are shown in Table 3.  
 
Name Description 
FirstRel_Doc Similarity with the first sentence in the document  
FirstRel_Para Similarity with the first sentence in the paragraph  
PageRankRel PageRank value of the sentence based on the sentence map  
 
Table 3. Types of relevance features 
5 Supervised/Semi-supervised Learning 
Approaches  
To incorporate features described in Section 4, 
we investigate supervised and semi-supervised 
learning approaches. Probabilistic Support Vec-
tor Machine (PSVM)  is employed as supervised 
learning (Wu et al, 2004), while the co-training 
of PSVM and Na?ve Bayesian Classifier (NBC) 
is used for semi-supervised learning. The two 
learning-based classification approaches, PSVM 
and NBC, are described in following sections. 
5.1 Probabilistic Support Vector Machine 
(PSVM) 
For a set of training examples ( ix , iy ), 
li ,...,1= , where ix  is an instance and iy  the 
corresponding label, basic SVM requires the so-
lution of the following optimization problem. 
?
=
+
l
i
i
T
bw
Cww
1
,, 2
1  min ??  
subject to    
0
1  ))(( ,
?
??+
i
ii
T
i bxwy
?
??
 
 
Here the SVM classifier is expected to find a 
hyper-plane to separate testing examples as posi-
tive and negative. Wu et al (2004) extend the 
basic SVM to a probabilistic version. Its goal is 
to estimate  
 
kixiyppi ,...1 ),|( === . 
First the pairwise (one-against-one) probabilities 
) ,or  |( xjiyiyprij ==?  is estimated using 
BAfij e
r ++? 1
1
 
where A and B are estimated by minimizing the 
negative log-likelihood function using training 
data and their decision values f. Then ip  is ob-
tained by solving the following optimization 
problem 
? ?
= ?
?
k
i ijj
jijijip
prpr
1 :
2)(
2
1  min  
 
subject to    
0
1  ))(( ,
?
??+
i
ii
T
i bxwy
?
??
 
The problem can be reformulated as  
QPP T
P 2
1  min  
988
where      ??
??
?
??
== ? ?
ji if       
ji if  
Q
2
:
ij
ijji
siiss
rr
r
 
The problem is convex and the optimality condi-
tions a scalar b such that   
??
???
?=??
???
???
???
?
1
z
  
b
P
 
0Te
eQ
 
where e is the vector of all 1s and z is the vector 
of all 0s, and b is the Lagrangian multiplier of the 
equality constraint ?
=
=
k
i
ip
1
1 . 
5.2 Na?ve Bayesian Classier (NBC) 
Na?ve Bayesian Classier assumes features are 
independent. It learns prior probability and con-
ditional probability of each feature, and predicts 
the class label by highest posterior probability. 
Given a feature vector (F1, F2, F3,?, Fn), the 
classifier need to decide the label c: 
 
),...,,|(maxarg 321 n
c
FFFFcPc =  
 
By applying Bayesian rule, we have  
 
),...,,,(
)|,...,,,()(
),...,,,|(
321
321
321
n
n
n FFFFP
cFFFFPcP
FFFFcP =
 
Since the denominator does not depend on c and 
the values of Fi are given, therefore the denomi-
nator is a constant and we are only interested in 
the numerator. As features are assumed inde-
pendent,  
 
?
=
?
=
n
i
i
nn
cFPcP
cFFFFPcPFFFFcP
1
321321
)|()(
)|,...,,()(),...,,|(
 
 
where )|( cFP i is estimated with MLE from 
training data with Laplace Smoothing. 
5.3 Co-Training (COT) 
Supervised learning approaches require much 
labeled data and the labeling procedure is very 
time-consuming. Literature (Blum and Mitchell, 
1998; Collins, 1999) has suggested that unla-
beled data can be exploited together with labeled 
data by co-training two classifiers. (Blum and 
Mitchell, 1998) trained two classifiers of same 
type on different features, and (Li et al, 2004) 
trained two classifiers of different types. In this 
paper, as the number of involved features is not 
too many, we train two different classifiers, 
PSVM and NBC, on the same feature spaces. 
The co-training algorithm is described as follows. 
 
Given: 
L is the set of labeled training examples 
U is the set of unlabeled training examples 
Loop: until the unlabeled data is exhausted 
Train the first classifier C1 (PSVM) on L 
Train the second classifier C2 (NBC) on L 
For each classifier Ci 
Ci labels examples from U 
Ci chooses p positive and n negative ex-
amples E from U. These examples have 
top classifying confidence. 
Ci removes examples E from U 
Ci adds examples E with the correspond-
ing labels to L 
End 
Output: label the test examples by the optimal 
classifier which is evaluated on training data ac-
cording to the classification performance. 
6 Experiments 
DUC 20012 has been used in our experiments. It 
contains 30 clusters of relevant documents and 
308 documents in total. Each cluster deals with a 
specific topic (e.g. a hurricane) and comes with 
model summaries created by NIST assessors. 50, 
100, 200 and 400 word summaries are provided. 
Twenty-five of the thirty document clusters are 
used as training data and the remaining five are 
used as testing. The training/testing configuration 
is same in experiments of supervised learning 
and semi-supervised learning, while the differ-
ence is that some sentences in training data are 
not tagged for semi-supervised learning. 
An automatic evaluation package, i.e., 
ROUGE (Lin and Hovy, 2003) is employed to 
evaluate the summarization performance. It 
compares machine-generated summaries with 
model summaries based on the overlap. Precision 
and recall measures are used to evaluate the clas-
sification performance. For comparison, we 
evaluate our approaches on DUC 2004 data set 
also. It contains 50 clusters of documents. Only 
665-character summaries are given by assessors 
for each cluster. 
6.1 Experiments on Supervised Learning 
Approach 
We use LibSVM3 as our classification model for 
SVM classifiers normally perform better. Types 
of features presented in previous section are 
evaluated individually first. Precision measures 
                                                 
2 http://duc.nist.gov/ 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
989
the percentage of true important sentences 
among all important sentences labeled by the 
classifier. Recall measures the percentage of true 
important sentences labeled by the classifier 
among all true important sentences.  
Table 4 shows the precisions and recalls of 
different feature groups under the PSVM classi-
fier. Table 5 records the ROUGE evaluation re-
sults ? ROUGE-1, ROUGE-2 and ROUGE-L. 
They evaluate the overlap between machine-
generated summaries and model summaries 
based on unigram, bigram and long distance re-
spectively. The summary length is limited to 200 
words here.  
 
Feature Precision Recall 
Sur 0.488 0.146 
Con 0.407 0.167 
Rel 0.488 0.146 
Event 0.344 0.146 
Sur+Con 0.575 0.160 
Sur+Rel 0.488 0.146 
Con+Rel 0.588 0.139 
Sur+Event 0.600 0.125 
Con+Event 0.384 0.194 
Rel+Event 0.543 0.132 
Sur+Con+Event 0.595 0.153 
Sur+Rel+Event 0.553 0.146 
Con+Rel+Event 0.581 0.125 
Sur+Con+Rel 0.595 0.174 
Sur+Con+Rel+Event 0.579 0.153 
Table 4. Classification performance based on 
different feature groups 
 
 
Feature Rouge-1 
Rouge-
2 
Rouge-
L 
Sur 0.373 0.103 0.356 
Con 0.352 0.074 0.334 
Rel 0.373 0.103 0.356 
Event 0.344 0.064 0.325 
Sur+Con 0.380 0.109 0.363 
Sur+Rel 0.373 0.103 0.356 
Con+Rel 0.375 0.103 0.358 
Sur+Event 0.348 0.091 0.332 
Con+Event 0.344 0.071 0.330 
Rel+Event 0.349 0.089 0.356 
Sur+Con+Event 0.379 0.106 0.363 
Sur+Rel+Event 0.371 0.101 0.353 
Sur+Con+Rel 0.396 0.116 0.358 
Sur+Con+Rel+Event 0.375 0.106 0.359 
Table 5. ROUGE evaluation results for differ-
ent feature groups 
From Table 4, we can see the most useful fea-
ture groups are ?surface? and ?relevance?, i.e. 
the external characteristics of a sentence in the 
document and the relationships of a sentence 
with other sentences in a cluster. The evaluation 
scores from surface features and relevance fea-
tures are the same. We found that the reason is 
that the dominating feature in each feature group 
is about whether a sentence is the first sentence 
in a document. The influence of event features is 
not very positive. Based on our analysis the rea-
son is that not all clusters contain enough event 
terms/elements to build a good event map. 
From Table 5, it can be seen that the combina-
tion of multiple features or multiple feature 
groups outperforms individual feature or feature 
groups. When surface, content and relevance fea-
tures are employed, the best performance is 
achieved, i.e., ROUGE-1 and ROUGE-2 score 
are 0.396 and 0.116 respectively. In our prelimi-
nary experiments, we find ROUGE-1 score of a 
model summary is 0.422 (without stemming and 
filtering stop words). Therefore summaries gen-
erated by our supervised learning approach re-
ceived comparable performance with model 
summaries when evaluated by ROUGE. Al-
though ROUGE is not perfect at this time, it is 
automatic and good complement to subjective 
evaluations.  
 We also find that the Rouge scores are similar 
for variations on the feature set. Sentences from 
original documents are selected to build the final 
summaries. Normally, only four to six sentences 
are contained in one 200-word summary in our 
experiments, i.e., few sentences will be kept in a 
summary. As variations of the feature set only 
induce little change of the order of most impor-
tant sentences, the ROUGE scores change little. 
6.2 Experiments on Semi-supervised Learn-
ing Approach 
Supervised learning approaches normally 
achieve good performance but require manually 
labeled data. Recent literature (Blum and 
Mitchell, 1998; Collins, 199) has suggested that 
co-training techniques reduce the amount of la-
beled data. They trained two homogeneous clas-
sifiers based on different feature spaces. How-
ever this method is unsuitable for our application 
as the number of required features in our case is 
not too many. Therefore we develop a co-
training approach to train different classifiers 
based on same feature space. PSVM and NBC 
are applied to the combination of surface, content 
and relevance features. 
The capability of different learning approaches 
to identify important sentences is shown in Fig-
990
ure 2. The ?x? axis shows the number of labeled 
sentences employed. The remained training sen-
tences in DUC 2001 are employed as unlabeled 
training data. The y axis shows f-measures of 
important sentences identified from the test set. 
The size of the training seed set is investigated. 
For each size, three different seed sets which are 
chose randomly are used. The average evaluation 
scores are used as the final performance. This 
procedure avoids the variance of the final evalua-
tion results. The ROUGE evaluation results of 
these supervised learning approaches and semi-
supervised learning approaches are shown in Ta-
ble 6 (2000 labeled sentences). It can be seen that 
the ROUGE performance of co-trained classifiers 
is better than that of individual classifiers. 
0
0.1
0.2
0.3
0.4
50 100 200 500 1000 2000
 Number of Labeled Sentences
F
-M
ea
su
re Cotrain
Bayes
Svm
 
Figure 2. Performance of supervised learning 
and semi-supervised learning approaches 
 
Learning  
Approaches Rouge-1 Rouge-2 Rouge-L
PSVM 0.358 0.082 0.323 
NBC 0.353 0.061 0.317 
COT 0.366 0.090 0.329 
Table 6. ROUGE evaluation results of supervised 
learning and semi-supervised learning 
6.3 Experiments on Summary Length 
In DUC 2001 dataset, 50, 100, 200 and 400-word 
summaries are provided to evaluate summaries 
with different length. Our supervised approach, 
which generates the best performance in previous 
experiments, is employed. The ROUGE scores of 
evaluations on different summary length are 
shown in Table 7. Our summaries consist of ex-
tracted sentences. It can be seen that these sum-
maries achieve lower ROUGE scores when the 
length of summary is reduced. The reason is that 
when people try to write a more concise sum-
mary, condensed contents are included in the 
summaries, which may not use the original con-
tents directly. Therefore the word-overlapping 
test tool in ROUGE generates lower scores.  
We then tested the same classifier and same 
features on DUC 2004. The length of summaries 
is only 665 characters (about 100 words). 
ROUGE-1 and ROUGE-2 are 0.329 and 0.073 
respectively. It confirms that the performance of 
our approach is sensitive to the length of the 
summary.  
Sum_length Rouge-1 Rouge-2 Rouge-L
50 0.241 0.036 0.205 
100 0.309 0.085 0.277 
200 0.396 0.116 0.358 
400 0.423 0.118 0.402 
Table 7. ROUGE evaluation results for differ-
ent summary length 
7 Conclusions and Future Work 
We explore surface, content, event, relevance 
features and their combinations for extractive 
summarization with supervised learning ap-
proach. Experiments show that the combination 
of surface, content and relevance features per-
form best. The highest ROUGE-1, ROUGE-2 
scores are 0.396 and 0.116 respectively. The 
Rouge-1 score of manually generated summaries 
is 0.422. This shows the ROUGE performance of 
our supervised learning approach is comparable 
to that of manually generated summaries. The 
ROUGE-1 scores of extractive summarization 
based on centroid, signature word, high fre-
quency word and event individually are 0.319, 
0.356, 0.371 and 0.374 respectively. It can be 
seen that our summarization approach based on 
combination of features improves the perform-
ance obviously.  
Although the results of supervised learning 
approach are encouraging, it required much la-
beled data. To reduce labeling cost, we apply co-
training to combine labeled and unlabeled data. 
Experiments show that compare with supervised 
learning, semi-supervised learning approach 
saves half of the labeling cost and maintains 
comparable performance (0.366 vs 0.396). We 
also find that our extractive summarization is 
sensitive to length of the summary. When the 
length is extended, the ROUGE scores of same 
summarization method are improved. In the fu-
ture, we plan to investigate sentence compression 
to improve performance of our summarization 
approaches on short summaries. 
Acknowledgement 
The research described in this paper is partially 
supported by Research Grants Council of Hong 
Kong (RGC: PolyU5217/07E), CUHK Strategic 
Grant Scheme (No: 4410001) and Direct Grant 
Scheme (No: 2050417).  
991
References 
Regina Barzilay, and Michael Elhadad. 1997. Using 
lexical chains for text summarization. In Proceed-
ings of the 35th Annual Meeting of the Association 
for Computational Linguistics Workshop on Intel-
ligent Scalable Text Summarization, pages 10-17. 
Avrim Blum and Tom Mitchell. 1998. Combining 
labeled and unlabeled data with co-training. In 
Proceedings of the 11th Annual Conference on 
Computational Learning Theory, pages 92-100. 
Hamish Cunningham, Diana Maynard, Kalina 
Bontcheva, Valentin Tablan. 2002. GATE: a 
framework and graphical development environ-
ment for robust NLP tools and applications. In 
Proceedings of the 40th Annual Meeting of the As-
sociation for computational Linguistics. 
Francois Denis and Remi Gilleron. 2003. Text classi-
fication and co-training from positive and unla-
beled examples. In Proceedings of the 20th Inter-
national Conference on Machine Learning Work-
shop: the Continuum from Labeled Data to Unla-
beled Data in Machine Learning and Data Mining. 
Gunes Erkan and Dragomir R. Radev. 2004. LexPag-
eRank: prestige in multi-document text summariza-
tion. In Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Process-
ing, pages 365-371. 
Elena Filatova and Vasileios Hatzivassiloglou. Event-
based extractive summarization. 2004. In Proceed-
ings of the 42nd Annual Meeting of the Association 
for Computational Linguistics Workshop, pages 
104-111. 
Gerald Francis DeJong. 1978. Fast skimming of news 
stories: the FRUMP system. Ph.D. thesis, Yale 
University. 
Wenjie Li, Guihong Cao, Kam-Fai Wong and Chunfa 
Yuan. 2004. Applying machine learning to Chinese 
temporal relation resolution. In Proceedings of the 
42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 583-589. 
Wenjie Li, Wei Xu, Mingli Wu, Chunfa Yuan, Qin Lu. 
2006. Extractive summarization using inter- and in-
tra- event relevance. In proceedings of Proceedings 
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics, pages 
369-376. 
Chin-Yew Lin; Eduard Hovy. 2000. The automated 
acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 495-
501. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, Edmonton, Canada. 
Daniel Marcu. 1997. The rhetorical parsing of natural 
language texts. In Proceedings of the 35th Annual 
Meeting of the Association for computational Lin-
guistics, pages 96-103. 
Christoph Muller, Stefan Rapp and Michael Strube. 
2001. Applying co-training to reference resolution. 
In Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics.  
Ani Nenkova, Lucy Vanderwende and Kathleen 
McKeown. 2006. A compositional context sensi-
tive multi-document summarizer: exploring the 
factors that influence summarization. In Proceed-
ings of the 29th Annual International ACM SIGIR 
Conference on Research and Development in In-
formation Retrieval. 
David Pierce and Claire Cardie. 2001. Limitations of 
co-training for natural language learning from large 
datasets. In Proceedings of the 2001 Conference on 
Empirical Methods in Natural Language Process-
ing, pages 1-9. 
Dragomir R. Radev, Timothy Allison, et al 2004. 
MEAD - a platform for multidocument multilin-
gual text summarization. In Proceedings of 4th In-
ternational Conference on Language Resources 
and Evaluation. 
Anoop Sarkar. 2001. Applying co-training methods to 
statistical parsing. In Proceedings of 2nd Meeting 
of the North American Chapter of the Association 
for Computational Linguistics on Language Tech-
nologies. 
Mingli Wu. 2006. Investigations on event-based 
summarization. In proceedings of the 21st Interna-
tional Conference on Computational Linguistics 
and 44th Annual Meeting of the Association for 
Computational Linguistics Student Research Work-
shop, pages 37-42. 
Mingli Wu, Wenjie Li, Furu Wei, Qin Lu and Kam-
Fai Wong. 2007. Exploiting surface, content and 
relevance features for learning-based extractive 
summarization. In Proceedings of 2007 IEEE In-
ternational Conference on Natural Language 
Processing and Knowledge Engineering.  
Ting-Fan Wu, Chih-Jen Lin and Ruby C. Weng. 2004. 
Probability estimates for multi-class classification 
by pairwise coupling. Journal of Machine Learning 
Research, 5:975-1005.  
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple news articles summarization based on 
event reference information. In Working Notes of 
the Fourth NTCIR Workshop Meeting, National In-
stitute of Informatics. 
992
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 414 ? 425, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Preliminary Work on Classifying Time Granularities 
of Temporal Questions 
Wei Li1, Wenjie Li1, Qin Lu1, and Kam-Fai Wong2  
1
 Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong 
{cswli, cswjli, csluqin}@comp.polyu.edu.hk 
2
 Department of Systems Engineering, the Chinese University of Hong Kong, 
Shatin, Hong Kong 
kfwong@se.cuhk.edu.hk 
Abstract. Temporal question classification assigns time granularities to tempo-
ral questions ac-cording to their anticipated answers. It is very important for an-
swer extraction and verification in the literature of temporal question answer-
ing. Other than simply distinguishing between "date" and "period", a more fine-
grained classification hierarchy scaling down from "millions of years" to "sec-
ond" is proposed in this paper. Based on it, a SNoW-based classifier, combining 
user preference, word N-grams, granularity of time expressions, special patterns 
as well as event types, is built to choose appropriate time granularities for the 
ambiguous temporal questions, such as When- and How long-like questions. 
Evaluation on 194 such questions achieves 83.5% accuracy, almost close to 
manually tagging accuracy 86.2%. Experiments reveal that user preferences 
make significant contributions to time granularity classification. 
1   Introduction 
Temporal questions, such as the questions with the interrogatives ?when?, ?how long? 
and ?which year?, seek for the occurrence time of the events or the temporal attributes 
of the entities. Temporal question classification plays an important role in the litera-
ture of question answering and temporal information processing. In the evaluation of 
TREC 10 Question-Answering (QA) track [1], more than 10% of questions in the test 
question corpus are temporal questions. Different from TREC QA track, Workshop 
TERQAS (http://www.timeml.org/terqas/) particularly investigated on temporal ques-
tion answering instead of a general one. It focused on temporal and event recognition 
in question answering systems and paid great attention to temporal relations among 
states, events and time expressions in temporal questions. TimeML (http://www.ti-
meml.org), a temporal information (e.g. time expression, tense & aspect) annotation 
standard, has also been used for temporal question answering in this workshop [2]. 
Correct understanding of a temporal question will greatly help extracting and verify-
ing its answers and certainly improve the performance of any question answering 
system. Look at the following examples. 
[Ea]. What is the birthday of Abraham Lincoln? 
[Eb]. When did the Neanderthal man live? 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 415 
In a general question answering system, the question classifier commonly classifies 
temporal questions into two classes, i.e. ?date? and ?period?. With such a system, the 
above two questions are both assigned a ?date?. Whereas it is natural for the question 
[Ea] to be answered with a particular data (e.g. ?12/02/1809?), it is not the case for 
question [Eb], because a proper answer could be ?35,000 years ago?. However, if it is 
known that the time granularity concerned is ?thousands of years?, answer extraction 
turn to be more targeted. The need for a more fine-grained classification is obvious. 
Although there were different question classification hierarchies, as reported 
[3,4,12,13,14], few inclined to introducing the classification hierarchy (e.g. ?year?, 
?month? and ?day?) which could give a clearer direction to guide answer extraction 
and verification of temporal questions. In the following, we try to find out whether 
temporal questions can be further classified into finer time granularity and how to 
classify them. 
By examining a temporal question corpus consisting of 348 questions, 293 of 
which are gathered from UIUC question answering labelled data (http://l2r.cs. 
uiuc.edu/~cogcomp/Data/QA/QC), and the rest 55 from TREC 10 test corpus, we find 
two different cases. On the one hand, some questions are very straightforward in ex-
pressing the time granularities of the answers expected, e.g. the questions beginning 
with ?which year? or ?for how many years?. On the other hand, some questions are 
not so obvious, e.g. the questions headed by ?when? or ?for how long?. We call such 
questions ambiguous questions. Not surprisingly, the ambiguous When- and How 
long-like questions account for a large proportion in this temporal question corpus, 
i.e. 197 from 348 in total. 
We further investigate on those 197 ambiguous questions in order to find out 
whether they can be classified into finer time granularity. Three experimenters are 
requested to tag a time granularity to each question independently1. Answers are not 
provided. The tag with two agreements is taken as the time granularity class of the 
corresponding temporal question. Otherwise the tag ?UNKNOWN? is assigned. Ref-
erence answers for the questions are extracted from AltaVista Web Search 
(http://www.altavista.com). Comparing the time granularities tagged manually with 
those provided by the reference answers, we find that only 27 out of 197 questions are 
incorrectly tagged, in other words, the manually tagging accuracy is 86.2%. Errors 
exist though, the relatively high agreement between users? tagging and reference 
answers lights the hope of automatically determining the time granularities of tempo-
ral questions. 
Analysing the tagging results, it is revealed that the tagging errors arouse from 
three sources: insufficient world knowledge, different speaking habits and different 
expected information granularity among human. See the following examples: 
[Ec]. When did the Neanderthal man live?   
User: year; Ref.: thousands of years 
[Ed]. How long is human gestation?  
User: month; Ref.: week 
[Ee]. When was the first Wall Street Journal published?   
User: year; Ref.: day 
                                                          
1
  The granularity hierarchy and the tagging principle will be detailed later. 
416 W. Li et al 
For question [Ec], the time granularity should be ?thousands of years?, rather than 
?year?. This error could be corrected if one knows that Neanderthal man existed 
35,000 years ago. The time granularity of question [Ed] should be ?week?, but not 
?month? in accordance with the habit. For question [Ee], users? tag is ?year?, different 
from the reference answer?s tag ?day?. However, both granularities are acceptable in 
commonsense, because the different users may want coarser or finer information. This 
observation suggests that incorporating question context, world knowledge, and 
speaking habits would help determine the time granularities of temporal questions. 
In this paper, we propose a fine-grained temporal question classification scheme, 
i.e. time granularity hierarchy, consisting of sixteen non-exclusive classes and scaling 
down from ?millions of years? to ?second?. The SNoW-based classifier is then built 
to combine linguistic features (including word N-grams, granularity of time expres-
sions and special patterns), user preferences and event types, and assign one of the 
sixteen classes to each temporal question. In our work, user preference, which charac-
terizes world knowledge and speaking habits, is estimated by means of the time 
granularities of the entities and/or events involved. The SNoW-based classifier 
achieves 83.5% accuracy, almost close to 86.2% of manually tagging accuracy. Ex-
periments also show that user preference makes a great contribution to time granular-
ity classification. 
The rest of this paper is organized as follows. In the next section various related 
works in this literature are introduced. In Sect. 3, we demonstrate the time granularity 
hierarchy and principles. User preference is fully investigated in Sect. 4. Feature de-
sign is depicted in Sect. 5. Time granularity classifiers are introduced in Sect. 6 and 
the experiment results are presented in Sect. 7. We finally conclude this paper in the 
last section. 
2   Related Works 
In TREC QA track, almost every QA system joining in the evaluation has a question 
classification module. This makes question classification a hot topic. Questions can be 
classified from several aspects. Most classification hierarchies [3,4,12,13,14] adopt the 
anticipated answer types as its classification criteria. Abney et al [4] gave a coarse 
classification hierarchy with seven classes (person, location, etc.). Hovy et al [13] 
introduced a finer classification with forty-seven classes manually constructed from 
17,000 practical questions. Li et al [3] proposed a two-level classification hierarchy, a 
coarser one with six classes and a finer one with fifty classes. In all these classification 
hierarchies, temporal questions are simply classified into two classes, i.e. ?date? and 
?period?. Some works classified temporal questions from other aspects. In [2], a tem-
poral question classification hierarchy is proposed according to the temporal relation 
among state, event and time expression. In [5], temporal questions are classified into 
three types with regard to question structure: non-temporal, simple and complex. Diaz 
F. et al [6] did an interesting work on the statistics of the number of topics along time-
line. According to whether questions or topics have a clear distribution along timeline, 
they can be classified into three types: atemporal, temporal clear and temporal ambigu-
ous.  Focusing on ambiguous temporal questions, e.g. when and how long-like ques-
tions, we introduce a classification hierarchy in terms of the anticipated answer types. 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 417 
It is an extension of two classes ?date? and ?period? and includes sixteen non-
exclusive classes scaling down from ?millions of years? to ?second?. 
Related to the work of features design, Li et al [3] built the question classifier 
based on three types of features, including surface text (e.g. N-grams), syntactic fea-
tures (e.g. part-of-speech and name entity tags), and semantic related words (words 
that often occur with a specific question class). Later works of Li et al [10] intro-
duced semantic information and world knowledge from external resources such as 
WordNet. In this paper, we introduce a new feature, user preference, which is ex-
pected to imply the world knowledge in time granularity in the experiment. User 
preference is estimated from statistics with which Diaz F. et al [6] determine whether 
a question is temporal ambiguous or not. E. Saquete et al [5] suggested that questions 
had different structures, i.e. non-temporal, simple and complex, which is helpful to 
handle questions more orderly. It gives us inspiration to use question focus, i.e. 
whether a question is event-based or entity-based.  
Many machine-learning methods have been used in question classification, such 
as language model [7], SNoW [3,10], maximum entropy [15] and support vector ma-
chine [8,9]. In our experiments, language model is selected as the baseline model, and 
SNoW is selected to tackle to the large feature space and build the classifier. In fact, 
SNoW has already been used in many other fields, such as text categorization, word 
sense disambiguation and even facial feature detection. 
3   Time Granularity Hierarchy and Tagging Principles 
In traditional question answering systems, only two question types are time-related, i.e. 
?date? and ?period?. For the reasons explained in Sect. 1, we propose a more detailed 
temporal question classification scheme, namely time granularity hierarchy scaling 
down from ?millions of years? to ?second? in order to facilitate answer extraction and 
verification. The initial time granularity hierarchy includes the following twelve 
classes: ?second?, ?minute?, ?hour?, ?day?, ?week?, ?month?, ?season?, ?year?, ?dec-
ade?, ?century?,  ?thousands of years? and ?millions of years?.  
Granularity ?weekday? is added to the initial hierarchy because some temporal 
questions favor ?weekday? instead of ?day?, although both of them indicate one day. 
Some questions favour a region of time granularity. Look at the following examples. 
[Ef]. What time of year has the most air travel? 
[Eg]. What time of day did Emperor Hirohito die? 
For [Ef] question, its time granularity could be ?season?, ?month? or even ?day?; and 
for question [Eg], the time granularity could be ?hour? or ?minute?. We can only 
determine that their time granularities are less than ?year? or ?day? respectively, but 
cannot go any further. Such situations only occur to time granularity ?year? and 
?day?, so we expand the original classification hierarchy by adding another two types: 
?less than day?, ?less than year?. Besides, the questions asking for festivals are classi-
fied into ?special date?.  
Up to now, the time granularity hierarchy has sixteen classes. The less frequent 
temporal measures, such as ?microsecond? and ?billions of years? are ignored. As 
mentioned above, the class ?less than day? overlaps several granularities, e.g. ?hour? 
and ?minute?, so the time granularity hierarchy we proposed is non-exclusive.  
418 W. Li et al 
In reality, some temporal questions can be answered in several different time 
granularities. For example, question ?when was Abraham Lincoln born??,  its answers 
can be a ?day? (?12/02/1809?) or a ?year? (?1809?). To resolve this confliction, we 
adopt two principles for time granularity annotation.  
[Pa]. Assign the minimum time granularity we can determine to a given temporal 
question if several time granularities are applicable. 
[Pb]. Select the time granularity with regard to speaking habits or user preferences.  
When the two principles conflict to each other, principle [Pb] takes the priority. With 
principle [Pa], time granularity of the above question can only be ?day?. 
4   User Preference 
In general, temporal questions have two different focuses: entity-based and event-based. 
[a]. Entity-based question: temporal interrogative words + (be) + entity, e.g. 
?When was the World War II?? 
[b]. Event-based question: temporal interrogatives + event, e.g. ?When did 
Mount St. Helen last have a significant eruption?? 
Time granularities of entities (or events) have great significance to those of entity-
based (or event-based) temporal questions. So, in the following, we make estimation 
of the time granularities of entities and events from statistics, based on the intuition 
that some entities or events may favor certain types of time granularities, which is 
called user preference here.  
4.1   Estimation of Time Granularities of Entities and Events 
4.1.1   Time Granularity of Entities 
The time granularity of the entity is derived by counting the co-occurrences of the 
entity and time granularities. The statistics is gathered from AltaVista Web Search. 
The sentences containing both the entity and time expressions are extracted from the 
first one hundred results returned by AltaVista with the entity as the searching key-
word. The probability P of a time granularity class tgi on the occurrence of the entity 
is calculated as the following Equation (1).  
)(#
)(#)|(
entity
entitytg
entitytgP ii
?
=
  )|(max)( entitytgPArgentityTG itgi=          (1) 
#( ) is the number of the sentences containing the expressions between the parenthe-
sis. TG(entity) represents the time granularity of the entity. 
4.1.2   Time Granularity of Events 
The time granularities of the events are not directly extracted as what is done to the 
entities, because they have little chance to be reused on the observation that there are 
rarely two identical events in a question corpus. As an alternative, the time granularity 
of an event is estimated from a sequence of entity-verb-entity? approximating the 
event. The time granularity of the verb is determined as Equation (1) by substituting 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 419 
?verb? for ?entity?. We choose two strategies for the estimation: maximum product 
and one-win-all.  
Maximum  product: )'|()|()|(1)|( entitytgPverbtgPentitytgP
Z
eventtgP iiii =
 
                                 )|(max)( eventtgPArgeventTG itgi=                                        (2) 
TG(event) represents time granularity of event. Z is used for normalization. 
One-win-all: )}'|(),|(),|({max)( entitytgPverbtgPentitytgPArgeventTG iiitgi=               (3) 
Equation (1) is smoothed in order to avoid 0 values in Equation (2). 
                          
i
i
i tgttw
wtg
wtgP =
+
+?
= )(#
1)(#)|(                           (4) 
t is the number of the time granularity classes, w is either an entity or a verb. 
4.1.3   Experiment: Evaluating the Estimation 
In the 197 ambiguous questions, 12 questions are entity-based, and the rest 185 ques-
tions are event-based. If all the 197 questions are arbitrarily assigned a tag ?year?, the 
tagging accuracy is 48.2%. 
For each entity-based or event-based question, the time granularity of the entity or 
event within it are assumed as the time granularity of the question. Compared with the 
time granularity of the reference answer, for the entity-based questions, we achieve 
75% accuracy; for the event-based question, the accuracy of maximum product strat-
egy and one-win-all strategy are 67.0% and 64.3% respectively. It seems that maxi-
mum product strategy is more effective than one-win-all strategy in this application. 
With maximum product strategy, the overall accuracy on all the 197 ambiguous ques-
tions is 67.4%. Notice that the accuracy of arbitrarily tagging is only 48.2%, so the 
estimation of the time granularities of the entities and the events is useful for deter-
mining the time granularities of temporal questions. 
4.2   Distribution of the Time Granularity of Entities and Events 
4.2.1   Observation of Distribution 
In the experiments of estimation, we find that some entities or events tend to favor 
only one certain time granularity, some others tend to favor several time granularities, 
and the rest may have a uniform distribution almost on every time granularity. 
-1 0 1 2 3 4 5 6 7 8 9
0
20
40
60
80
100
Season
Week
Pr
o
p o
rti
o n
(%
)
Time Granularity of "gestation"
   
-1 0 1 2 3 4 5 6 7 8 9
0
10
20
30
40
50
60
Century
Decade
Year
Month
Day
Pr
o
po
rti
o
n(%
)
Time Granularity of "Lincoln born"
-1 0 1 2 3 4 5 6 7 8 9
0
10
20
30
40
50
Year
Month
Weekday
Day
Hour
Pr
op
or
tio
n
(%
)
Time Granularity of "take place"
 
(a)   (b)   (c) 
Fig. 1. Distribution of the time granularities of the entities and events 
420 W. Li et al 
In Fig. 1(a), time granularity ?day? takes a preponderant proportion, i.e. more than 
80%, in the distribution of ?gestation?, which is called single-peak-distribution. In 
Fig. 1(b), both ?day? and ?year? take a large proportion, so ?Lincoln born? is multi-
peak-distributed. In Fig. 1(c), for ?take place?, all the time granularities almost take a 
similar proportion and it is a uniform distribution. 
4.2.2   Experiments on Distribution 
Assume an entity (or event) E, its possible time granularities {tgi, i=1,?t} and the 
corresponding probabilities {Pi, i=1,?t} (calculated by Equation 1 and 2).  
       ?= i iPt1? ;  ?= i iPId ),( ? ; ?
??
?
>
??
?
=
i
i
i P
P
PI
0
1),(                        (5) 
d is the number of time granularities tgi with higher probability Pi than average prob-
ability ? . For simplicity, distribution DE of the time granularity of E is determined as 
follows, 
                                              
3
31
1
>
?<
=
??
??
?
=
d
d
d
Uniform
Multi
Single
DE
                                                  (6)  
Observing the experiment results in Sect. 4.1.3, 88.7%, 56.3% and 18.9% accuracy 
are achieved on the questions within which the time granularities of the entities or 
events are estimated to be single-peak-, multi-peak-, and uniform-distributed respec-
tively. So whether the estimated time granularity of the entity or event is single-peak-, 
multi-peak-, or uniform-distributed highlights the confidence on the estimation, which 
can be taken as a feature associated with the estimation of the time granularities. 
5   Feature Design 
As described in the above section, estimation of the time granularities of the entities 
and the events is useful for determining the time granularities of temporal questions; 
whether a question is entity-based or not and the distribution of time granularities of 
the entities and events within the questions will also be taken as associated features. 
These three features are named user preference feature in total. Besides, another four 
types of features are considered. 
Word N-grams 
Word N-grams feature, e.g. unigram and bigram is the most straightforward feature 
and commonly used in question classification. In general question classification, uni-
gram ?when? indicates a temporal question. In temporal question classification, uni-
gram ?birthday? always implies a ?day? while bigram ?when ? born? is a strong 
evidence of the time granularity ?day?. From this aspect, word N-grams also reflect 
user preference on time granularity. 
Granularity of Time Expressions 
Time expressions are common in temporal questions, e.g. ?July 11, 1998? and date 
modifier ?1998? in ?1998 Superbowl?. We take the granularities of time expressions 
as features, for example, 
TG(?in 1998?) = ?year?  TG(?July 11, 1998?) = ?day? 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 421 
Granularities of time expressions impose the constraints on the time granularities of 
temporal questions. If there is a time expression whose time granularity is tg in a 
temporal question, time granularity of this question can not be tg. For example, ques-
tion ?When is the 1998 SuperBowl??, its time granularity can not be ?Year?, i.e. the 
time granularity of  ?1998?. 
Special Patterns 
In word N-gram features, words are equally processed, however, some special words 
combining with the verbs or the temporal connectives (e.g. ?when?, ?before? and 
?since?) will produce special patterns and affect the time granularities of temporal 
questions. Look at the following examples. 
[Eh]. Since when hasn?t John Sununu been able to fly on government planes for 
personal business? 
[Ei]. What time of the day does Michael Milken typically wake up? 
For question [Eh], the temporal preposition ?since? combined with ?when? highlights 
that this question is seeking for a beginning point time, which implies a finer time 
granularity; for question [Ei], ?typically? combined with verb ?wake up? indicates a 
generally occurred event, and implies that its time granularity could be ?less than 
day? or ?less than year?. 
Event Types 
In general, there are four event types: states, activities, accomplishments, and 
achievements. States and activities favour larger time granularities, while accom-
plishments and achievements favour smaller ones. For example, the activity ?stay? 
will favour larger time granularity than the accomplishment event ?take place?.  
6   Classifier Building 
In this work, we choose the Sparse Network of Winnow (SNoW) model as the time 
granularity classifier and compare it with a commonly used Language Model (LM) 
classifier. 
6.1   Language Model (LM) 
As language model has already been used in question classification [7], it is taken as 
the baseline model in the experiments. Language model mainly combines two types 
of features, i.e. unigram and bigram. Given a temporal question Q, its time granularity 
TG(Q) is calculated by Equation (7). 
              ?? =
=
+
=
=
?+=
nj
j jji
mj
j jitg wwtgPwtgPArgQTG i 1 11 )|()1()|(max)( ??              (7)  
w represents words. m and n are the numbers of unigrams and bigrams in questions 
respectively. ?  assigns different weights to unigrams and bigrams. In the experiment, 
best accuracy is achieved when 7.0=?  (see Sect. 7.3.1). 
6.2   Sparse Network of Winnow (SNoW) 
SNoW is a learning framework and applicable to the tasks with a very large number 
of features. It selects active features by updating weights of features, and learns a 
422 W. Li et al 
linear function from a corpus consisting of positive and negative examples. Let 
Ac={i1, ?, im} be the set of features that are active and linked to target class c. Let si 
be the real valued strength associated with feature in the example. Then the example?s 
class is c if and only if, 
                                                   ?
?
?
Aci
ciic sw ?,                                                     (8) 
icw , is weight of feature i connected with class c, which is learned from the training 
corpus. SNoW has already been used in question classification [3,10] and good results 
are reported. As mentioned in Sect. 5, five types of features are selected for our task. 
They are altogether counted to more than ten thousand features. Since it is a large 
feature set, SNoW is a good choice.  
7   Experiments 
7.1   Setup 
In this 348-question-corpus (see Sect. 1), time granularities of 151 questions are 
straightforward, while those of the rest 197 questions are ambiguous. For the sixteen 
time granularity classes, we only consider ten classes including more than four ques-
tions. Questions with unconsidered time granularity classes excluded, the question cor-
pus has 339 questions in total, 145 for training and 194 for testing. As a result, the task 
is to learn a model from the 145-question training corpus and classify questions in the 
194-question test corpus into ten classes: ?second?, ?minute?, ?hour?, ?day?, ?week-
day?, ?week?, ?month?, ?season?, ?year? and ?century?. The SNoW classifier is 
downloaded from UIUC (http://l2r.cs.uiuc.edu/~cogcomp/download.php?key=SNOW). 
7.2   Evaluation Criteria 
The primary evaluation standard is accuracy1, i.e. the proportion of the correct classi-
fied questions out of the test questions (see Equation 9). However, if a question seek-
ing for a finer time granularity, e.g. ?day?, has been incorrectly determined as a 
coarser one, e.g. ?year?, it should also be taken as partly correct, which is reflected in 
accuracy2 (see Equation 10).  
                                         
)(#
)(#
1 test
correctAccuracy =                                                  (9) 
#( ) is number of questions. 
       
)(#
)(
2 test
QRR
Accuracy i i?=
??
??
?
>
<
=
+?
=
)()'(
)()'(
)()'(
)1)()'((1
0
1
)(
QQ
QQ
QQ
QQ tgRtgR
tgRtgR
tgRtgR
tgRtgR
QRR
        (10) 
Qtg  and 'Qtg  are the reference and classification result respectively. )( QtgR is the 
rank of the time granularity class Qtg , scaling down from ?millions of years? to 
?second?. Rank of ?second? is 1, while rank of ?year? is 9. The ranks of the last three 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 423 
time granularities, i.e. ?special date?, ?less than day? and ?less than year? are 14, 15 
and 16 respectively. Likewise, )'( QtgR is the rank of 'Qtg .  
7.3   Experimental Results and Analysis 
In the experiments, language model is taken as the baseline model. Performance of 
SNoW-based classifier will be compared with that of language model. Different com-
binations of features are tested in SNoW-based classifier and their performances are 
investigated. 
7.3.1   LM Classifier 
The LM classifier takes two types of features: unigram and bigram. Experiment re-
sults are presented in Fig. 2.  
Accuracy varies with different feature weight ?  and best accuracy (accuracy1 
68.0% and accuracy2 68.9%) achieves when ? =0.7. Accuracy when ? =1.0 is higher 
than that when ? =0. It indicates that, in the framework of language model, unigrams 
achieves better performance than bigrams, which accounts from the sparseness of 
bigram features. 
-0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
45
50
55
60
65
70
 Accuracy1
 Accuracy2
Ac
cu
ra
cy
(%
)
?
 
Fig. 2. Accuracy of LM classifier. Data in circle is the best performance achieved. 
7.3.2   SNOW Classifier 
Our SNoW classifier requires binary features. We then encode each feature with an 
integer label. When a feature is observed in a question, its label will appear in the 
extracted feature set of this question. There are six types of features: 15 user prefer-
ences (10 for the estimation of time granularities, 3 for the estimation distributions, 
and 2 for question focuses) (F1), 951 unigrams (F2), 9277 bigrams (F3), 10 granularity 
of time expressions (F4), 14 special patterns (F5), and 4 event types (F6). Although the 
number of all features is more than ten thousand, the features in one question are no 
more than twenty in general. Accuracies of SNoW classifier on 194 test questions are 
presented in Table 1. It shows that simply using unigram features, SNoW classifier 
has already achieved better accuracy than LM classifier (accuracy1: 69.5% vs. 68.0%; 
accuracy2: 70.3% vs. 68.9%). From this view, SNoW classifier outperforms LM clas-
sifier in handling sparse features. When all the six types of features are used, SNoW 
classifier achieves 83.5% in accuracy1 and 83.9% in accuracy2, almost close to the 
accuracy of user tagging, i.e. 86.2%. 
424 W. Li et al 
Table 1. Accuracy (%) of SNoW classifier 
Feature Set F2 F2, 3 F1~6 
Accuracy1 69.5 72.1 83.5 
Accuracy2 70.3 72.7 83.9 
Table 2. Accuracy1 (%) on different types of time granularities 
TG second minute hour day weekday 
Accuracy1 100 100 100 64.2 100 
TG week month season year century 
Accuracy1 100 60 100 90.5 66.7 
Table 3. Accuracy (%) on combination of different types of features 
Feature Set F2,3 F1,2,3 F2,3,4 F2,3,5 F2,3,6 
Accuracy1 72.1 79.8 73.7 74.7 72.6 
Accuracy2 72.7 80.6 74.7 75.2 73.1 
With all the six types of features, accuracy1 on the questions with different types of 
time granularity is illustrated in Table 2. It reveals that the classification errors mainly 
come from time granularity of ?month?, ?day? and ?century?. Low accuracy on 
?month? and ?century? accounts from absence of enough examples, i.e. examples for 
training and testing both less than five. Many ?day? questions are incorrectly classi-
fied into ?year?, which accounts for the low accuracy on ?day?.  The reason lies in 
that there are more ?year? questions than ?day? questions in the training question 
corpus (116 vs. 56).  
In general, we can extract three F1 features, one F4 feature, less than two F5 fea-
tures, and one F6 feature from one question. It is hard for SNoW classifier to train and 
test independently on each of these types of the features because of the small feature 
number in one example question. However, the numbers of F2 and F3 features in a 
question are normally more than ten. So we take unigrams (F2) and bigrams (F3) as 
the basic feature set. Table 3 presents the accuracy when the rest four types of fea-
tures are added into the basic feature set respectively. As expected user preference 
makes the most significant improvement, 7.82% in accuracy1 and 7.90% in accuracy2. 
Special patterns also play an important role, which makes 2.6% accuracy1 improve-
ment. It is strange that event type makes such a modest improvement (0.5%). After 
analyzing the experimental results, we find that as there are only four event types, it 
makes limited contribution to 10-class time granularity classification. 
8   Conclusion 
Various features for time granularity classification of temporal questions are investi-
gated in this paper. User preference is shown to make a significant contribution  
to classification performance. SNoW classifier, combining user preference, word  
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 425 
N-grams, granularity of time expressions, special patterns and event types, achieves 
83.5% accuracy in classification, close to manually tagging accuracy 86.2%.  
Acknowledgement 
This project is partially supported by Hong Kong RGC CERG (Grant No: 
PolyU5181/03E), and partially by CUHK Direct Grant (No: 2050330).  
References 
1) TREC (ed.): The TREC-8 Question Answering Track Evaluation. Text Retrieval Confer-
ence TREC-8, Gaithersburg, MD (1999) 
2) Radev D. and Sundheim B.: Using TimeML in Question Answering. 
http://www.cs.brandeis.edu/~jamesp/arda/time/documentation/TimeML-use-in-qa-
v1.0.pdf, (2002) 
3) Li, X. and Roth, D.: Learning Question Classifiers. Proceedings of the 19th International 
Conference on Computational Linguistics (2002) 556-562 
4) S. Abney, M. Collins, and A. Singhal: Answer Extraction. Proceedings of the 6th ANLP 
Conference (2000) 296-301 
5) Saquete E., Mart?nez-Barco P., Mu?oz R.: Splitting Complex Temporal Questions for 
Question Answering Systems. Proceedings of the 42nd Annual Meeting of the Association 
for Computational Linguistics (2004) 567-574 
6) Diaz, F. and Jones, R.: Temporal Profiles of Queries. Yahoo! Research Labs Technical 
Report YRL-2004-022 (2004) 
7) Wei Li: Question Classification Using Language Modeling. CIIR Technical Report (2002) 
8) Dell Zhang and Wee Sun Lee: Question Classification Using Support Vector Machines. 
Proceedings of the 26th Annual International ACM SIGIR Conference on Research and 
Development in Information Retrieval (2003) 26-32 
9) Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku Maeda: Question Classification 
Using HDAG Kernel. Proceedings of Workshop on Multilingual Summarization and 
Question Answering (2003) 61-68 
10) Li X., Roth D., and Small K.: The Role of Semantic Information in Learning Question 
Classifiers. Proceedings of the International Joint Conference on Natural Language Proc-
essing (2004) 
11) Schilder, Frank & Habel, Christopher: Temporal Information Extraction for Temporal 
Question Answering. In New Directions in Question Answering. Papers from the 2003 
AAAI Spring Symposium TR SS-03-07 (2003) 34-44 
12) Rohini K. Srihari, Wei Li: A Question Answering System Supported by Information Ex-
traction. Proceedings of Association for Computational Linguistics (2000) 166-172 
13) Eduard Hovy, Laurie Geber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran: 
Towards Semantics-Based Answer Pinpointing. Proceedings of the DARPA Human Lan-
guage Technology Conference (2001) 
14) Hermjacob U.: Parsing and Question Classification for Question Answering. Proceedings 
of the Association for Computational Linguists Workshop on Open-Domain Question An-
swering (2001) 17-22 
15) Ittycheriah, Franz M., Zhu W., Ratnaparki A. and Mammone R.: Question Answering Us-
ing Maximum Entropy Components. Proceedings of the North American chapter of the 
Association for Computational Linguistics (2001) 33-39  
NIL Is Not Nothing: Recognition of Chinese  
Network Informal Language Expressions 
Abstract 
Informal language is actively used in net-
work-mediated communication, e.g. chat 
room, BBS, email and text message. We refer 
the anomalous terms used in such context as 
network informal language (NIL) expres-
sions. For example, ??(ou3)? is used to re-
place ?? (wo3)? in Chinese ICQ. Without 
unconventional resource, knowledge and 
techniques, the existing natural language 
processing approaches exhibit less effective-
ness in dealing with NIL text. We propose to 
study NIL expressions with a NIL corpus and 
investigate techniques in processing NIL ex-
pressions. Two methods for Chinese NIL ex-
pression recognition are designed in NILER 
system. The experimental results show that 
pattern matching method produces higher 
precision and support vector machines 
method higher F-1 measure. These results are 
encouraging and justify our future research 
effort in NIL processing. 
1 Introduction 
The rapid global proliferation of Internet applica-
tions has been showing no deceleration since the 
new millennium. For example, in commerce more 
and more physical customer services/call centers 
are replaced by Internet solutions, e.g. via MSN, 
ICQ, etc. Network informal language (NIL) is ac-
tively used in these applications. Following this 
trend, we forecast that NIL would become a key 
language for human communication via network.  
Today NIL expressions are ubiquitous. They 
appear, for example, in chat rooms, BBS, email, 
text message, etc. There is growing importance in 
understanding NIL expressions from both technol-
ogy and humanity research points of view. For 
instance, comprehension of customer-operator dia-
logues in the aforesaid commercial application 
would facilitate effective Customer Relationship 
Management (CRM).  
Recently, sociologists showed many interests in 
studying impact of network-mediated communica-
tion on language evolution from psychological and 
cognitive perspectives (Danet, 2002; McElhearn, 
2000; Nishimura, 2003). Researchers claim that 
languages have never been changing as fast as to-
day since inception of the Internet; and the lan-
guage for Internet communication, i.e. NIL, gets 
more concise and effective than formal language.  
Processing NIL text requires unconventional 
linguistic knowledge and techniques. Unfortu-
nately, developed to handle formal language text, 
the existing natural language processing (NLP) 
approaches exhibit less effectiveness in dealing 
with NIL text. For example, we use ICTCLAS 
(Zhang et al, 2003) tool to process sentence ???
???????(Is he going to attend 
a meeting?)?. The word segmentation result is 
??|?|?|?|?|??|?|??. In this sentence , ??
?? (xi4 ba1 xi4)? is a NIL expression 
which means ?is he ?.?? in this case. It can be 
concluded that without identifying the expression, 
further Chinese text processing techniques are not 
able to produce reasonable result. 
This problem leads to our recent research in 
?NIL is Not Nothing? project, which aims to pro-
duce techniques for NIL processing, thus  avails 
understanding of change patterns and behaviors in 
language (particularly in Internet language) evolu-
tion. The latter could make us more adaptive to the 
dynamic language environment in the cyber world.  
Recently some linguistic works have been car-
ried out on NIL for English. A shared dictionary 
Yunqing Xia,  Kam-Fai Wong,  Wei Gao
Department of Systems Engineering and Engineering Management 
The Chinese University of Hong Kong, Shatin, Hong Kong 
{yqxia, kfwong, wgao}@se.cuhk.edu.hk 
95
has been compiled and made available online. It 
contains 308 English NIL expressions including 
English abbreviations, acronyms and emoticons. 
Similar efforts for Chinese are rare. This is be-
cause Chinese language has not been widely used 
on the Internet until ten years ago. Moreover, Chi-
nese NIL expression involves processing of Chi-
nese Pinyin and dialects, which results in higher 
complexity in Chinese NIL processing.  
In ?NIL is Not Nothing? project, we develop a 
comprehensive Chinese NIL dictionary. This is a 
difficult task because resource of NIL text is rather 
restricted. We download a collection of BBS text 
from an Internet BBS system and construct a NIL 
corpus by annotating NIL expressions in this col-
lection by hand. An empirical study is conducted 
on the NIL expressions with the NIL corpus and a 
knowledge mining tool is designed to construct the 
NIL dictionary and generate statistical NIL fea-
tures automatically. With these knowledge and 
resources, the NIL processing system, i.e. NILER, 
is developed to extract NIL expressions from NIL 
text by employing state-of-the-art information ex-
traction techniques.  
The remaining sections of this paper are organ-
ized as follow. In Section 2, we observe formation 
of NIL expressions. In Section 3 we present the 
related works. In Section 4, we describe NIL cor-
pus and the knowledge engineering component in 
NIL dictionary construction and NIL features gen-
eration. In Section 5 we present the methods for 
NIL expression recognition. We outline the ex-
periments, discussions and error analysis in Sec-
tion 6, and finally Section 7 concludes the paper. 
2 The Ways NIL Expressions Are Typi-
cally Formed 
NIL expressions were first introduced for expedit-
ing writing or computer input, especially for online 
chat where the input speed is crucial to prompt and 
effective communication. For example, it is rather 
annoying to input full Chinese sentences in text-
based chatting environment, e.g. over the mobile 
phone. Thus abbreviations and acronyms are then 
created by forming words in capital with the first 
letters of a series of either English words or Chi-
nese Pinyin.  
Chinese Pinyin is a popular approach to Chi-
nese character input. Some Pinyin input methods 
incorporate lexical intelligence to support word or 
phrase input. This improves input rate greatly. 
However, Pinyin input is not error free. Firstly, 
options are usually prompted to user and selection 
errors result in homophone, e.g. ???(ban1 
zu2)? and ??? (ban1 zhu3)?. Secondly, 
input with incorrect Pinyin or dialect produces 
wrong Chinese words with similar pronunciation, 
e.g. ???(xi1 fan4)? and ???(xi3 hua-
n1)?. Nonetheless, prompt communication spares 
little time to user to correct such a mistake. The 
same mistake in text is constantly repeated, and the 
wrong word thus becomes accepted by the chat 
community. This, in fact, is one common way that 
a new Chinese NIL expression is created. 
We collect a large number of ?sentences? 
(strictly speaking, not all of them are sentences) 
from a Chinese BBS system and identify NIL ex-
pressions by hand. An empirical study on NIL ex-
pressions in this collection shows that NIL 
expressions can be classified into four classes as 
follow based on their origins. 
1) Abbreviation (A). Many Chinese NIL expres-
sions are derived from abbreviation of Chi-
nese Pinyin. For example, ?PF? equals to ??
?(pei4 fu2)? which means ?admire?.  
2) Foreign expression (F). Popular Informal ex-
pressions from foreign languages such as 
English are adopted, e.g. ?ASAP? is used for 
?as soon as possible?. 
3) Homophone (H). A NIL expression is some-
times generated by borrowing a word with 
similar sound (i.e. similar Pinyin). For exam-
ple ??? ? equals ??? ? which means 
?like?. ???? and ???? hold homophony 
in a Chinese dialect. 
4) Transliteration (T) is a transcription from one 
alphabet to another and a letter-for-letter or 
sound-for-letter spelling is applied to repre-
sent a word in another language. For exam-
ple, ???(bai4 bai4)? is transliteration 
of ?bye-bye?. 
A thorough observation, in turn, reveals that, 
based on the ways NIL expressions are formed 
and/or their part of speech (POS) attributes, we 
observe a NIL expression usually takes one of the 
forms presented in Table 1 and Table 2. 
The above empirical study is essential to NIL 
lexicography and feature definition. 
96
3 Related Works 
NIL expression recognition, in particular, can be 
considered as a subtask of information extraction 
(IE). Named entity recognition (NER) happens to 
hold similar objective with NIL expression recog-
nition, i.e. to extract meaningful text segments 
from unstructured text according to certain pre-
defined criteria. 
NER is a key technology for NLP applications 
such as IE and question & answering. It typically 
aims to recognize names for person, organization, 
location, and expressions of number, time and cur-
rency. The objective is achieved by employing 
either handcrafted knowledge or supervised learn-
ing techniques. The latter is currently dominating 
in NER amongst which the most popular methods 
are decision tree (Sekine et al, 1998; Pailouras et 
al., 2000), Hidden Markov Model (Zhang et al, 
2003; Zhao, 2004), maximum entropy (Chieu and 
Ng, 2002; Bender et al, 2003), and support vector 
machines (Isozaki and Kazawa, 2002; Takeuchi 
and Collier, 2002; Mayfield, 2003). 
From the linguistic perspective, NIL expres-
sions are rather different from named entities in 
nature. Firstly, named entity is typically noun or 
noun phrase (NP), but NIL expression can be any 
kind, e.g. number ?94? in NIL represents ????
which is a verb meaning ?exactly be?. Secondly, 
named entities often have well-defined meanings 
in text and are tractable from a standard dictionary; 
but NIL expressions are either unknown to the dic-
tionary or ambiguous. For example, ???? ap-
pears in conventional dictionary with the meaning 
of Chinese porridge, but in NIL text it represents ?
??? which surprisingly represents ?like?. The 
issue that concerns us is that these expressions like  
???? may also appear in NIL text with their 
formal meaning. This leads to ambiguity and 
makes it more difficult in NIL processing.  
Another notable work is the project of ?Nor-
malization of Non-standard Words? (Sproat et al, 
2001) which aims to detect and normalize the 
?Non-Standard Words (NSW)? such as digit se-
quence; capital word or letter sequence; mixed 
case word; abbreviation; Roman numeral; URL 
and e-mail address. In our work, we consider most 
types of the NSW in English except URL and 
email address. Moreover, we consider Chinese 
NIL expressions that contain same characters as 
the normal words. For example, ??? ? and           
???? both appear in common dictionaries, but 
they carry anomalous meanings in NIL text. Am-
biguity arises and basically brings NIL expressions 
recognition beyond the scope of NSW detection.  
According to the above observations, we pro-
pose to employ the existing IE techniques to han-
dle NIL expressions. Our goal is to develop a NIL 
expression recognition system to facilitate net-
work-mediated communication. For this purpose, 
we first construct the required NIL knowledge re-
sources, namely, a NIL dictionary and n-gram sta-
tistical features. 
Table 2: NIL expression forms based on POS attribute.  
POS
Attribute 
# of NIL  
Expressions Examples 
Number 1 ?W? represents ??(wan4)?and means ?ten thousand?. 
Pronoun 9 ??? represents ??? and means ?I?. 
Noun 29 
?LG? represents ???(lao3 
gong1)? and means ?hus-
band?.
Adjective 250 ?FB? represents ???(fu3 
bai4)? and means ?corrupt?. 
Verb 34 
???(cong1 bai2)? repre-
sents ???(chong3 bai4)?
and means ?adore?. 
Adverb 10 ??(fen3)? represents ??
(hen3)? and means ?very?. 
Exclamation  9 
??(nie0)? represents ??
(ne0)? and equals a descrip-
tive exclamation. 
Phrase 309 ?AFK? represents ?Away From Keyboard?.  
Table 1: NIL expression forms based on word formation. 
Word  
Formation 
# of NIL  
Expressions Examples 
Chinese  
Word or 
Phrase
33 ???? represents ???? and means ?like?. 
Sequence of 
English 
Capitals  
341 ?PF? represents ???? and means ?admire?. 
Number 8
?94(jiu3 si4)? represents
???(jiu4 shi4)? and 
means ?exactly be?. 
Mixture of  
the Above 
Forms
30
?8?(ba1 cuo4)? repre-
sents ???(bu3 cuo4)?
and means ?not bad?. 
Emoticons 239 ?:-(? represents a sad emotion.
97
4 Knowledge Engineering 
Recognition of NIL expressions relies on uncon-
ventional linguistic knowledge such as NIL dic-
tionary and NIL features. We construct a NIL 
corpus and develop a knowledge engineering 
component to obtain these knowledge by running a 
knowledge mining tool on the NIL corpus. The 
knowledge mining tool is a text processing pro-
gram that extracts NIL expressions and their at-
tributes and contextual information, i.e. n-grams, 
from the NIL corpus. Workflow for this compo-
nent is presented in Figure 1. 
4.1  NIL Corpus
The NIL corpus is a collection of network informal 
sentences which provides training data for NIL 
dictionary and statistical NIL features. The NIL 
corpus is constructed by annotating a collection of 
NIL text manually. 
Obtaining real chat text is difficult because of 
the privacy restriction. Fortunately, we find BBS 
text within ????(da4 zui3 qu1)? zone in 
YESKY system (http://bbs.yesky.com/bbs/) re-
flects remarkable colloquial characteristics and 
contains a vast amount of NIL expressions. We 
download BBS text posted from December 2004 
and February 2005 in this zone. Sentences with 
NIL expressions are selected by human annotators, 
and NIL expressions are manually identified and 
annotated with their attributes. We finally col-
lected 22,432 sentences including 451,193 words 
and 22,648 NIL expressions. 
The NIL expressions are marked up with 
SGML. The typical example, i.e. ???????
???? in Section 1, is annotated as follows. 
where NILEX is the SGML tag to label a NIL ex-
pression, which entails NIL linguistic attributes 
including class, normal, pinyin, segments, pos, and 
posseg (see Section 4.2). H is a value of class (see 
Section 2). Value VERB demotes verb, ADJ adjec-
tive, NUM number and AUX auxiliary. 
4.2  NIL Dictionary 
The NIL dictionary is a structured databank that 
contains NIL expression entries. Each entity in 
turn entails nine attributes described as follow. 
1. ID: an unique identification number for the 
NIL expression, e.g. 915800; 
2. string: string of the NIL expression, e.g. ??
???;
3. class: class of the NIL expression (see Sec-
tion 2), e.g. ?H? for homophony;  
4. pinyin: Chinese Pinyin for the NIL expres-
sion, e.g. ?xi4 ba1 xi4?; 
5. normal: corresponding normal text for the 
NIL expression, e.g. ?????;  
6. segments: word segments of the NIL expres-
sion, e.g. ??|?|??; 
7. pos: POS tag associated with the expression, 
e.g. ?VERB? denoting a verb;  
8. posseg: a POS tag list for the word seg-
ments, e.g. ?VERB|AUX|VERB?;  
9. frequency: number of occurrences of the 
NIL expression. 
We run the knowledge mining tool to extract all 
annotated NIL expressions together with their at-
tributes from the NIL corpus. The NIL expressions 
are then each assigned an ID number and inserted 
into an indexed data file, i.e. the NIL dictionary. 
Current NIL dictionary contains 651 NIL entries.  
4.3  NIL Feature Set 
The NIL features are required by support vector 
machines method in NIL expression recognition. 
We define two types of statistical features for NIL 
expressions, i.e. Chinese word n-grams and POS 
tag n-grams. Bigger n leads to more contextual 
?<NILEX string=????? class=?H? normal=??
??? pinyin=?xi4 ba1 xi4? segments=??|?|??
pos=?VERB? posseg=?ADJ|NUM|ADJ?>???
</NILEX>?????
Figure 1: Workflow for NIL knowledge engineering 
component. NILE refers to NIL expression, which is 
identified and annotated by human annotator.  
NILE 
Annotation 
 Original Text  
 Collection 
NIL Corpus 
NIL 
Dictionary 
NIL 
Features 
Extract  
A Sentence 
Knowledge Mining Tool 
Word Segmentation & POS Tagging 
(ICTCLAS) 
98
information, but results in higher computational 
complexity. To compromise, we generate n-grams 
with n = 1, 2, 3, 4. For example,   ???/????
is a bi-gram for ????? in terms of word seg-
mentation, and its POS tag bi-gram is 
?PRONOUN/ VERB?.  
We run the knowledge mining tool on the NIL 
corpus to produce all n-grams for Chinese words 
and their POS tags in which NIL expression ap-
pears. 8379 features were generated including 
7416 word-based n-grams and 963 POS tag-based 
n-grams. These statistical NIL features are linked 
to the corresponding NIL dictionary entries by 
their global NIL expression IDs. 
Besides, we consider some morphological fea-
tures including being/containing a number, some 
English capitals or Chinese characters. These fea-
tures can be extracted by parsing string of the NIL 
expressions. 
5 NILER System 
5.1  Architecture 
We develop NILER system to recognize NIL ex-
pressions in NIL text and convert them to normal 
language text. The latter functionality is discussed 
in other literatures. Architecture of NILER system 
is presented in Figure 2. 
The input chat text is first segmented and POS 
tagged with ICTCLAS tool. Because ICTCLAS is 
not able to identify NIL expressions, some expres-
sions are broken into several segments. NIL ex-
pression recognizer processes the segments and 
POS tags and identifies the NIL expressions.  
5.2  NIL Expression Recognizer 
We implement two methods in NIL expression 
recognition, i.e. pattern matching and support vec-
tor machines. 
5.2.1  Method I: Pattern Matching  
Pattern matching (PM) is a traditional method in 
information extraction systems. It uses a hand-
crafted rule set and dictionary for this purpose. 
Because it?s simple, fast and independent of cor-
pus, this method is widely used in IE tasks. 
By applying NIL dictionary, candidates of NIL 
expressions are first extracted from the input text 
with longest matching. As ambiguity occurs con-
stantly, 24 patterns are produced and employed to 
disambiguate. We first extract those word and POS 
tag n-grams from the NIL corpus and create pat-
terns by generalizing them manually. An illustra-
tive pattern is presented as follows. 
?]_[)_(8]_[ ?!!! anyvunitvnotanyv
where anyv _  and unitv _  are variables denoting 
any word and any unit word respectively;  )(xnot
is the negation operator. The illustrative pattern 
determines ?8? to be a NIL expression if it is suc-
ceeded by a unit word. With this pattern, ?8? 
within sentence ?????  ???? (He has 
been working for eight hours.)? is not recognized 
as a NIL expression.  
5.2.2  Method II: Support Vector Machines  
Support vector machines (SVM) method produces 
high performance in many classification tasks 
(Joachims, 1998; Kudo and Matsumoto, 2001). As 
SVM can handle large numbers of features effi-
ciently, we employ SVM classification method to 
NIL expression recognition. 
Suppose we have a set of training data for a 
two-class classification problem {(x1,y1), (x2,
y2),?,(xN, yN)}, where ),...2,1( NiRx Di  ?  is a fea-
ture vector of the i-th order sample in the training 
set and }1,1{ ?iy  is the label for the sample. 
The goal of SVM is to find a decision function that 
accurately predicts y for unseen x. A non-linear 
SVM classifier gives a decision function 
))(()( xgsignxf   for an input vector x, where  
?
 
 
l
i
ii bzxKxg
1
),()( Y
The szi  are so-called support vectors, and 
represents the training samples. iY  and b  are pa-
rameters for SVM motel. l is number of training 
samples. ),( zxK  is a kernel function that implic-
NIL 
Dictionary 
NIL 
Features 
 Chat Text 
NIL Expression 
 List 
NIL Expression 
Recognizer 
Word Segmentation 
Word POS Tagging 
(ICTCLAS) 
Figure 2: Architecture of NILER system. 
99
itly maps vector x into a higher dimensional space. 
A typical kernel is defined as dot products, i.e.  
)(),( zxkzxK x .
Based on the training process, the SVM algo-
rithm constructs the support vectors and parame-
ters. When text is input for classification, it is first 
converted into feature vector x. The SVM method 
then classifies the vector x by determining sign of 
g(x), in which 1)(  xf  means that word x is posi-
tive and otherwise if 1)(  xf . The SVM algo-
rithm was later extended in SVMmulticlass to predict 
multivariate outputs (Joachims, 1998).  
In NIL expression recognition, we consider 
NIL corpus as training set and the annotated NIL 
expressions as samples. NIL expression recogni-
tion is achieved with the five-class SVM classifi-
cation task, in which four classes are those defined 
in Section 2 and reflected by class attribute within 
NIL annotation scheme. The fifth class is 
NOCLASS, which means the input text is not any 
NIL expression class.  
6 Experiments
6.1  Experiment Description 
We conduct experiments to evaluate the two meth-
ods in performing the task of NIL expression rec-
ognition. In training phase we use NIL corpus to 
construct NIL dictionary and pattern set for PM 
method, and generate statistical NIL features, sup-
port vectors and parameters for SVM methods. To 
observe how performance is influenced by the vol-
ume of training data, we create five NIL corpora, 
i.e. C#1~C#5, with five numbers of NIL sentences, 
i.e. 10,000, 13,000, 16,000, 19,000 and 22,432, by 
randomly selecting sentence from NIL corpus de-
scribed in Section 4.1.  
To generate test set, we download 5,690 sen-
tences from YESKY system which cover BBS text 
in March 2005. We identify and annotate NIL ex-
pressions within these sentences manually and 
consider the annotation results as gold standard.  
We first train the system with the five corpora 
to produce five versions of NIL dictionary, pattern 
set, statistical NIL feature set and SVM model. We 
then run the two methods with each version of the 
above knowledge over the test set to produce rec-
ognition results automatically. We compare these 
results against the gold stand and present experi-
mental results with criteria including precision, 
recall and F1-measure. 
6.2  Experimental Results 
We present experimental results of the two meth-
ods on the five corpora in Table 3. 
Table 3: Experimental results for the two methods on the five 
corpora. PRE denotes precision, REC denotes recall, and F1 
denotes F1-Measure. 
PM SVM Corpus
PRE REC F1 PRE REC F1 
C#1 0.742 0.547 0.630 0.683 0.703 0.693 
C#2 0.815 0.634 0.713 0.761 0.768 0.764 
C#3 0.873 0.709 0.783 0.812 0.824 0.818 
C#4 0.904 0.759 0.825 0.847 0.851 0.849 
C#5 0.915 0.793 0.850 0.867 0.875 0.871 
6.3  Discussion I: The Two Methods 
To compare performance of the two methods, we 
present the experimental results with smoothed 
curves for precision, recall and F1-Mesure in Fig-
ure 3, Figure 4 and Figure 5 respectively. 
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 3: Smoothed precision curves over the five corpora.  
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 4: Smoothed recall curves over the five corpora.   
Figure 3 reveals that PM method produces 
higher precision, i.e. 91.5%, and SVM produces 
higher recall, i.e. 79.3%, and higher F1-Measure, 
i.e. 87.1%, with corpus C#5. It can be inferred that 
PM method is self-restrained. In other words, if a 
NIL expression is identified with this method, it is 
very likely that the decision is right. However, the 
weakness is that more NIL expressions are ne-
glected. On the other hand, SVM method outper-
100
forms PM method regarding overall capability, i.e. 
F1-Measure, according to Figure 5. 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 5: Smoothed F1-Measure curves over the five corpora. 
We argue that each method holds strength and 
weakness. Different methods should be adopted to 
cater to different application demands. For exam-
ple, in CRM text processing, we might favor preci-
sion. So PM method may be the better choice. On 
the other hand, to perform the task of chat room 
security monitoring, recall is more important. Then 
SVM method becomes the better option. We claim 
that there exists an optimized approach which 
combines the two methods and yields higher preci-
sion and better robustness at the same time. 
6.4  Discussion II: How Volume Influences Per-
formance 
To observe how training corpus influences per-
formance in the two methods regarding volume, 
we present experimental results with smoothed 
quality curves for the two method in Figure 6 and 
Figure 7 respectively. 
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0 1 2 3 4 5 6
PRE
REC
F1
Figure 6: Smoothed quality curves for PM method over the 
five corpora.
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
PRE
REC
F1
Figure 7: Smoothed quality curves for SVM method 
over the five corpora. 
The smoothed quality curves in Figure 6 and 
Figure 7 reveal the tendency that bigger volume of 
training data leads to better processing quality. 
Meanwhile, the improvement tends to decrease 
along with increasing of volume. It thus predicts 
that there exists a corpus with a certain volume 
that produces the best quality according to the ten-
dency. Although current corpus is not big enough 
to prove the optimal volume, the tendency re-
vealed by the curves is obvious. 
6.5  Error Analysis 
We present two examples to analyze errors occur 
within our experiments. 
Err.1 Ambiguous NIL Expression 
Example 1:
[Sentence]: ??? 8??
[Meaning]:  I still don?t understand. 
[NIL expression found(Y/N)? ]: Y 
[Normal language text]: ??????
Error in Example 1 is caused by failure in iden-
tifying ? ? ? (mi3 bai2)?. Because ? ?
(mi3)? succeeds ?8(ba1)? in the word seg-
ments, i.e. ??|??|8|?|??, and it can be used as 
a unit word, PM method therefore refuses to iden-
tify ?8(ba)? as a NIL expression according to the 
pattern described in Section 5.2.1. In fact, ????
is an unseen NIL expression. SVM method suc-
cessfully recognizes ??? ? to be ??? (mi3
you3)?, thus recognizes ?8?. In our experiments 
56 errors in PM method suffer the same failure, 
while SVM method identifies 48 of them. This 
demonstrates that PM method is self-restrained 
and SVM method is relatively scalable in process-
ing NIL text. 
Err.2 Unseen NIL expression 
Example 2: 
[Sentence]: ??? 4U??
[Meaning]: Just came back from 4U. 
[NIL expression found (Y/N)?] : N
Actually, there is no NIL expression in example 
2. But because of a same 1-gram with ?4D?, i.e. 
?4?, SVM outputs ?4U? as a NIL expression. In 
fact, it is the name for a mobile dealer. There are 
78 same errors in SVM method in our experi-
ments, which reveals that SVM method is some-
times over-predicting. In other words, some NIL 
expressions are recognized with SVM method by 
mistake, which results in lower precision. 
101
7 Conclusions and Future Works 
Network informal language processing is a new 
NLP research application, which seeks to recog-
nize and normalize NIL expressions automatically 
in a robust and adaptive manner. This research is 
crucial to improve capability of NLP techniques in 
dealing with NIL text.  With empirical study on 
Chinese network informal text and NIL expres-
sions, we propose two NIL expression recognition 
methods, i.e. pattern matching and support vector 
machines. The experimental results show that PM 
method produces higher precision, i.e. 91.5%, and 
SVM method higher F-1 measure, i.e. 87.1%. 
These results are encouraging and justify our fu-
ture research effort in NIL processing. 
Research presented in this paper is preliminary 
but significant. We address future works as follow. 
Firstly, NIL corpus constructed in our work is fun-
damental. Not only will difficulty in seeking for 
text resource be overcome, but a large quantity of 
manpower will be allocated to this laborious and 
significant work. Secondly, new NIL expressions 
will appear constantly with booming of network-
mediated communication. A powerful NIL expres-
sion recognizer will be designed to improve adap-
tivity of the recognition methods and handle the 
unseen NIL expressions effectively. Finally, we 
state that research in this paper targets in special at 
NIL expressions in China mainland. Due to cul-
tural/geographical variance, NIL expressions in 
Hong Kong and Taiwan could be different. Further  
research will be conducted to adapt our methods to 
other NIL communities.  
References 
Bender, O., Och, F. J. and Ney, H. 2003. Maximum En-
tropy Models for Named Entity Recognition,
CoNLL-2003,  pp. 148-151.  
Chieu, H. L. and Ng, H. T. 2002. Named Entity Recog-
nition: A Maximum Entropy Approach Using Global 
Information. COLING-02, pp. 190-196. 
Danet, B. 2002. The Language of Email, European Un-
ion Summer School, University of Rome. 
Isozaki, H. and Kazawa, H. 2002. Efficient Support 
Vector Classifiers for Named Entity Recognition,
COLING-02, pp. 390-396.. 
Joachims, T. 1998. Text categorization with Support 
Vector Machines: Learning with many relevant fea-
tures. ECML?98, pp. 137-142. 
Kudo, T. and Matsumoto, Y. 2001. Chunking with Sup-
port Vector Machines. NAACL 2001, pp.192-199. 
Mayfield, J. 2003. Paul McNamee; Christine Piatko, 
Named Entity Recognition using Hundreds of Thou-
sands of Features, CoNLL-2003, pp. 184-187. 
McElhearn, K. 2000. Writing Conversation - An Analy-
sis of Speech Events in E-mail Mailing Lists,
http://www.mcelhearn.com/cmc.html, Revue Fran-
?aise de Linguistique Appliqu?e, volume V-1.  
Nishimura, Y. 2003. Linguistic Innovations and Inter-
actional Features of Casual Online Communication 
in Japanese, JCMC 9 (1). 
Pailouras, G., Karkaletsis, V. and Spyropoulos, C. D. 
2000. Learning Decision Trees for Named-Entity 
Recognition and Classification. Workshop on Ma-
chine Learning for Information Extraction, 
ECAI(2000).   
Sekine, S., Grishman, R. and Shinnou, H. 1998. A Deci-
sion Tree Method for Finding and Classifying Names 
in Japanese Texts, WVLC 98. 
Snitt, E. N. 2000. The Use of Language on the Internet,
http://www.eng.umu.se/vw2000/Emma/lin-
guistics1.htm. 
Sproat, R., Black, A.,  Chen, S., Kumar, S., Ostendorf, 
M. and Richards, M. 2001. Normalization of Non-
standard Words. Computer Speech and Languages, 
15(3):287- 333. 
Takeuchi, K. and Collier, N. 2002. Use of Support Vec-
tor Machines in Extended Named Entity Recognition.
CoNLL-2002, pp. 119-125.  
Zhang, Z., Yu, H., Xiong, D. and Liu, Q. 2003. HMM-
based Chinese Lexical Analyzer ICTCLAS. In the 2nd
SIGHAN workshop affiliated with ACL?03, pp. 184-
187.  
Zhao, S. 2004. Named Entity Recognition in Biomedical 
Texts Using an HMM model, COLING-04 workshop 
on Natural Language Processing in Biomedicine and 
its Applications.  
102
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 993?1000,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Phonetic-Based Approach to Chinese Chat Text Normalization 
 
Yunqing Xia, Kam-Fai Wong 
Department of S.E.E.M. 
The Chinese University of Hong Kong 
Shatin, Hong Kong 
{yqxia, kfwong}@se.cuhk.edu.hk 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University 
Kowloon, Hong Kong 
cswjli@comp.polyu.edu.hk 
 
 
Abstract 
Chatting is a popular communication 
media on the Internet via ICQ, chat 
rooms, etc. Chat language is different 
from natural language due to its anoma-
lous and dynamic natures, which renders 
conventional NLP tools inapplicable. The 
dynamic problem is enormously trouble-
some because it makes static chat lan-
guage corpus outdated quickly in repre-
senting contemporary chat language. To 
address the dynamic problem, we pro-
pose the phonetic mapping models to 
present mappings between chat terms and 
standard words via phonetic transcrip-
tion, i.e. Chinese Pinyin in our case. Dif-
ferent from character mappings, the pho-
netic mappings can be constructed from 
available standard Chinese corpus. To 
perform the task of dynamic chat lan-
guage term normalization, we extend the 
source channel model by incorporating 
the phonetic mapping models. Experi-
mental results show that this method is 
effective and stable in normalizing dy-
namic chat language terms. 
1 Introduction 
Internet facilitates online chatting by providing 
ICQ, chat rooms, BBS, email, blogs, etc. Chat 
language becomes ubiquitous due to the rapid 
proliferation of Internet applications. Chat lan-
guage text appears frequently in chat logs of 
online education (Heard-White, 2004), customer 
relationship management (Gianforte, 2003), etc. 
On the other hand, wed-based chat rooms and 
BBS systems are often abused by solicitors of 
terrorism, pornography and crime (McCullagh, 
2004). Thus there is a social urgency to under-
stand online chat language text. 
Chat language is anomalous and dynamic. 
Many words in chat text are anomalous to natural 
language. Chat text comprises of ill-edited terms 
and anomalous writing styles. We refer chat 
terms to the anomalous words in chat text. The 
dynamic nature reflects that chat language 
changes more frequently than natural languages. 
For example, many popular chat terms used in 
last year have been discarded and replaced by 
new ones in this year. Details on these two fea-
tures are provided in Section 2.  
The anomalous nature of Chinese chat lan-
guage is investigated in (Xia et al, 2005). Pattern 
matching and SVM are proposed to recognize 
the ambiguous chat terms. Experiments show 
that F-1 measure of recognition reaches 87.1% 
with the biggest training set. However, it is also 
disclosed that quality of both methods drops sig-
nificantly when training set is older. The dy-
namic nature is investigated in (Xia et al, 
2006a), in which an error-driven approach is pro-
posed to detect chat terms in dynamic Chinese 
chat terms by combining standard Chinese cor-
pora and NIL corpus (Xia et al, 2006b). Lan-
guage texts in standard Chinese corpora are used 
as negative samples and chat text pieces in the 
NIL corpus as positive ones. The approach calcu-
lates confidence and entropy values for the input 
text. Then threshold values estimated from the 
training data are applied to identify chat terms. 
Performance equivalent to the methods in exis-
tence is achieved consistently. However, the is-
sue of normalization is addressed in their work. 
Dictionary based chat term normalization is not a 
good solution because the dictionary cannot 
cover new chat terms appearing in the dynamic 
chat language. 
In the early stage of this work, a method based 
on source channel model is implemented for chat 
term normalization. The problem we encounter is 
addressed as follows. To deal with the anoma-
lous nature, a chat language corpus is constructed 
with chat text collected from the Internet. How-
993
ever, the dynamic nature renders the static corpus 
outdated quickly in representing contemporary 
chat language. The dilemma is that timely chat 
language corpus is nearly impossible to obtain. 
The sparse data problem and dynamic problem 
become crucial in chat term normalization. We 
believe that some information beyond character 
should be discovered to help addressing these 
two problems.  
Observation on chat language text reveals that 
most Chinese chat terms are created via phonetic 
transcription, i.e. Chinese Pinyin in our case. A 
more exciting finding is that the phonetic map-
pings between standard Chinese words and chat 
terms remain stable in dynamic chat language. 
We are thus enlightened to make use of the pho-
netic mapping models, in stead of character map-
ping models, to design a normalization algorithm 
to translate chat terms to their standard counter-
parts. Different from the character mapping 
models constructed from chat language corpus, 
the phonetic mapping models are learned from a 
standard language corpus because they attempt to 
model mappings probabilities between any two 
Chinese characters in terms of phonetic tran-
scription. Now the sparse data problem can thus 
be appropriately addressed. To normalize the 
dynamic chat language text, we extend the 
source channel model by incorporating phonetic 
mapping models. We believe that the dynamic 
problem can be resolved effectively and robustly 
because the phonetic mapping models are stable.  
The remaining sections of this paper are or-
ganized as follows. In Section 2, features of chat 
language are analyzed with evidences. In Section 
3, we present methodology and problems of the 
source channel model approach to chat term 
normalization. In Section 4, we present defini-
tion, justification, formalization and parameter 
estimation for the phonetic mapping model. In 
Section 5, we present the extended source chan-
nel model that incorporates the phonetic mapping 
models. Experiments and results are presented in 
Section 6 as well as discussions and error analy-
sis. We conclude this paper in Section 7. 
2 Feature Analysis and Evidences 
Observation on NIL corpus discloses the anoma-
lous and dynamic features of chat language. 
2.1 Anomalous 
Chat language is explicitly anomalous in two 
aspects. Firstly, some chat terms are anomalous 
entries to standard dictionaries. For example, ??
?(here, jie4 li3)? is not a standard word in any 
contemporary Chinese dictionary while it is often 
used to replace ???(here, zhe4 li3)? in chat 
language. Secondly, some chat terms can be 
found in  standard dictionaries while their mean-
ings in chat language are anomalous to the dic-
tionaries. For example, ??(even, ou3)? is often 
used to replace ??(me, wo2)? in chat text. But 
the entry that ??? occupies in standard diction-
ary is used to describe even numbers. The latter 
case is constantly found in chat text, which 
makes chat text understanding fairly ambiguous 
because it is difficult to find out whether these 
terms are used as standard words or chat terms. 
2.2 Dynamic 
Chat text is deemed dynamic due to the fact that 
a large proportion of chat terms used in last year 
may become obsolete in this year. On the other 
hand, ample new chat terms are born. This fea-
ture is not as explicit as the anomalous nature. 
But it is as crucial. Observation on chat text in 
NIL corpus reveals that chat term set changes 
along with time very quickly. 
An empirical study is conducted on five chat 
text collections extracted from YESKY BBS sys-
tem (bbs.yesky.com) within different time peri-
ods, i.e. Jan. 2004, July 2004, Jan. 2005, July 
2005 and Jan. 2006. Chat terms in each collec-
tion are picked out by hand together with their 
frequencies so that five chat term sets are ob-
tained. The top 500 chat terms with biggest fre-
quencies in each set are selected to calculate re-
occurring rates of the earlier chat term sets on the 
later ones.  
Set Jul-04 Jan-05 Jul-05 Jan-06 Avg. 
Jan-04 0.882 0.823 0.769 0.706 0.795
Jul-04 - 0.885 0.805 0.749 0.813
Jan-05 - - 0.891 0.816 0.854
Jul-05 - - - 0.875 0.875
Table 1. Chat term re-occurring rates. The rows 
represent the earlier chat term sets and the col-
umns the later ones. 
The surprising finding in Table 1 is that 29.4% 
of chat terms are replaced with new ones within 
two years and about 18.5% within one year. The 
changing speed is much faster than that in stan-
dard language. This thus proves that chat text is 
dynamic indeed. The dynamic nature renders the 
static corpus outdated quickly. It poses a chal-
lenging issue on chat language processing.  
994
3 Source Channel Model and Problems 
The source channel model is implemented as 
baseline  method in this work for chat term nor-
malization. We brief its methodology and prob-
lems as follows. 
3.1 The Model 
The source channel model (SCM) is a successful 
statistical approach in speech recognition and 
machine translation (Brown, 1990). SCM is 
deemed applicable to chat term normalization 
due to similar task nature. In our case, SCM aims 
to find the character string niicC ,...,2,1}{ ==  that 
the given input chat text njitT ,...,2,1}{ ==  is most 
probably translated to, i.e. ii ct ? , as follows. 
)(
)()|(maxarg)|(maxarg?
Tp
CpCTpTCpC
CC
==     (1) 
Since )(Tp  is a constant for C , so C?  should 
also maximize )()|( CpCTp . Now )|( TCp  is 
decomposed into two components, i.e. chat term 
translation observation model )|( CTp  and lan-
guage model )(Cp . The two models can be both 
estimated with maximum likelihood method us-
ing the trigram model in NIL corpus.  
3.2 Problems 
Two problems are notable in applying SCM in 
chat term normalization. First, data sparseness 
problem is serious because timely chat language 
corpus is expensive thus small due to dynamic 
nature of chat language. NIL corpus contains 
only 12,112 pieces of chat text created in eight 
months, which is far from sufficient to train the 
chat term translation model. Second, training 
effectiveness is poor due to the dynamic nature. 
Trained on static chat text pieces, the SCM ap-
proach would perform poorly in processing chat 
text in the future. Robustness on dynamic chat 
text thus becomes a challenging issue in our re-
search.  
Updating the corpus with recent chat text con-
stantly is obviously not a good solution to the 
above problems. We need to find some informa-
tion beyond character to help addressing the 
sparse data problem and dynamic problem. For-
tunately, observation on chat terms provides us 
convincing evidence that the underlying phonetic 
mappings exist between most chat terms and 
their standard counterparts. The phonetic map-
pings are found promising in resolving the two 
problems.  
4 Phonetic Mapping Model 
4.1 Definition of Phonetic Mapping 
Phonetic mapping is the bridge that connects two 
Chinese characters via phonetic transcription, i.e. 
Chinese Pinyin in our case. For example, ??
???? ?? )56.0,,( jiezhe ?? is the phonetic mapping con-
necting ??(this, zhe4)? and ??(interrupt, jie4)?, 
in which ?zhe? and ?jie? are Chinese Pinyin for  
??? and ??? respectively. 0.56 is phonetic 
similarity between the two Chinese characters. 
Technically, the phonetic mappings can be con-
structed between any two Chinese characters 
within any Chinese corpus. In chat language, any 
Chinese character can be used in chat terms, and 
phonetic mappings are applied to connect chat 
terms to their standard counterparts. Different 
from the dynamic character mappings, the pho-
netic mappings can be produced with standard 
Chinese corpus before hand. They are thus stable 
over time.  
4.2 Justifications on Phonetic Assumption  
To make use of phonetic mappings in normaliza-
tion of chat language terms, an assumption must 
be made that chat terms are mainly formed via 
phonetic mappings. To justify the assumption, 
two questions must be answered. First, how 
many percent of chat terms are created via pho-
netic mappings? Second, why are the phonetic 
mapping models more stable than character map-
ping models in chat language? 
Mapping type Count Percentage 
Chinese word/phrase 9370 83.3% 
English capital 2119 7.9% 
Arabic number 1021 8.0% 
Other  1034 0.8% 
Table 2. Chat term distribution in terms of  map-
ping type. 
To answer the first question, we look into chat 
term distribution in terms of mapping type in 
Table 2. It is revealed that 99.2 percent of chat 
terms in NIL corpus fall into the first four pho-
netic mapping types that make use of phonetic 
mappings. In other words, 99.2 percent of chat 
terms can be represented by phonetic mappings. 
0.8% chat terms come from the OTHER type, 
emoticons for instance. The first question is un-
doubtedly answered with the above statistics.  
To answer the second question, an observation 
is conducted again on the five chat term sets de-
scribed in Section 2.2. We create phonetic map-
995
pings manually for the 500 chat terms in each 
set. Then five phonetic mapping sets are ob-
tained. They are in turn compared against the 
standard phonetic mapping set constructed with 
Chinese Gigaword. Percentage of phonetic map-
pings in each set covered by the standard set is 
presented in Table 3.  
Set Jan-04 Jul-04 Jan-05 Jul-05 Jan-06
percentage 98.7 99.3 98.9 99.3 99.1 
Table 3. Percentages of phonetic mappings in 
each set covered by standard set.  
By comparing Table 1 and Table 3, we find 
that phonetic mappings remain more stable than 
character mappings in chat language text. This 
finding is convincing to justify our intention to 
design effective and robust chat language nor-
malization method by introducing phonetic map-
pings to the source channel model. Note that 
about 1% loss in these percentages comes from 
chat terms that are not formed via phonetic map-
pings, emoticons for example. 
4.3 Formalism 
The phonetic mapping model is a five-tuple, i.e. 
>< )|(Pr),(),(,, CTCptTptCT pm , 
which comprises of chat term character T , stan-
dard counterpart character C , phonetic transcrip-
tion of T  and C , i.e. )(Tpt  and )(Cpt , and the 
mapping probability )|(Pr CTpm  that T  is 
mapped to C  via the  phonetic mapping ( ) CT CTCptTpt pm ??????? ?? )|(Pr),(),(  (hereafter briefed by 
CT M??? ). 
As they manage mappings between any two 
Chinese characters, the phonetic mapping models 
should be constructed with a standard language 
corpus. This results in two advantages. One, 
sparse data problem can be addressed appropri-
ately because standard language corpus is used. 
Two, the phonetic mapping models are as stable 
as standard language. In chat term normalization, 
when the phonetic mapping models are used to 
represent mappings between chat term characters 
and standard counterpart characters, the dynamic 
problem can be addressed in a robust manner.   
Differently, the character mapping model used 
in the SCM (see Section 3.1) connects two Chi-
nese characters directly. It is a three-tuple, i.e.  
>< )|(Pr,, CTCT cm , 
which comprises of chat term character T , stan-
dard counterpart character C  and the mapping 
probability )|(Pr CTcm  that T  is mapped to C  
via this character mapping. As they must be con-
structed from chat language training samples, the 
character mapping models suffer from data 
sparseness problem and dynamic problem.  
4.4 Parameter Estimation 
Two questions should be answered in parameter 
estimation. First, how are the phonetic mapping 
space constructed? Second, how are the phonetic 
mapping probabilities estimated?  
To construct the phonetic mapping models, we 
first extract all Chinese characters from standard 
Chinese corpus and use them to form candidate 
character mapping models. Then we generate 
phonetic transcription for the Chinese characters 
and calculate phonetic probability for each can-
didate character mapping model. We exclude 
those character mapping models holding zero 
probability. Finally, the character mapping mod-
els are converted to phonetic mapping models 
with phonetic transcription and phonetic prob-
ability incorporated.  
The phonetic probability is calculated by 
combining phonetic similarity and character fre-
quencies in standard language as follows.  
( )
( )? ?
?=
i iislc
slc
pm
AApsAfr
AApsAfr
AAob
),()(
),()(
),(Pr    (2) 
In Equation (2) }{ iA  is the character set in 
which each element iA  is similar to character A  
in terms of phonetic transcription. )(cfrslc  is a 
function returning frequency of given character 
c  in standard language corpus and ),( 21 ccps  
phonetic similarity between character 1c  and 2c . 
Phonetic similarity between two Chinese char-
acters is calculated based on Chinese Pinyin as 
follows.  
)))(()),(((        
)))(()),(((       
))(),((),(
ApyfinalApyfinalSim
ApyinitialApyinitialSim
ApyApySimAAps
?
=
=
     (3) 
In Equation (3) )(cpy  is a function that returns 
Chinese Pinyin of given character c , and 
)(xinitial  and )(xfinal  return initial (shengmu) 
and final (yunmu) of given Chinese Pinyin x   
respectively. For example, Chinese Pinyin for the 
Chinese character ??? is ?zhe?, in which ?zh? is 
initial and ?e? is final. When initial or final is 
996
empty for some Chinese characters, we only cal-
culate similarity of the existing parts.  
An algorithm for calculating similarity of ini-
tial pairs and final pairs is proposed in (Li et al, 
2003) based on letter matching. Problem of this 
algorithm is that it always assigns zero similarity 
to those pairs containing no common letter. For 
example, initial similarity between ?ch? and ?q? 
is set to zero with this algorithm. But in fact, 
pronunciations of the two initials are very close 
to each other in Chinese speech. So non-zero 
similarity values should be assigned to these spe-
cial pairs before hand (e.g., similarity between 
?ch? and ?q? is set to 0.8). The similarity values 
are agreed by some native Chinese speakers. 
Thus Li et al?s algorithm is extended to output a 
pre-defined similarity value before letter match-
ing is executed in the original algorithm. For ex-
ample, Pinyin similarity between ?chi? and ?qi? 
is calculated as follows.  
8.018.0),(),()( =?=?= iiSimqchSimchi,qiSim  
5 Extended Source Channel Model 
We extend the source channel model by inserting 
phonetic mapping models niimM ,...,2,1}{ ==  into 
equation (1), in which chat term character it  is 
mapped to standard character ic  via im , i.e. 
i
m
i ct i??? . The extended source channel model 
(XSCM) is mathematically addressed as follows. 
)(
)()|(),|(
maxarg    
),|(maxarg?
,
,
Tp
CpCMpCMTp
TMCpC
MC
MC
=
=
   (4) 
Since )(Tp  is a constant, C?  and M?  should 
also maximize )()|(),|( CpCMpCMTp . Now 
three components are involved in XSCM, i.e. 
chat term normalization observation model 
),|( CMTp , phonetic mapping model )|( CMp  
and language model )(Cp . 
Chat Term Normalization Observation 
Model.  We assume that mappings between chat 
terms and their standard Chinese counterparts are 
independent of each other. Thus chat term nor-
malization probability can be calculated as fol-
lows. 
?= i iii cmtpCMTp ),|(),|(              (5) 
The ),|( iii cmtp ?s are estimated using maxi-
mum likelihood estimation method with Chinese 
character trigram model in NIL corpus.  
Phonetic Mapping Model. We assume that the 
phonetic mapping models depend merely on the 
current observation. Thus the phonetic mapping 
probability is calculated as follows. 
?= i ii cmpCMp )|()|(                 (6) 
in which )|( ii cmp ?s are estimated with equation 
(2) and (3) using a standard Chinese corpus.  
Language Model.  The language model )(Cp ?s 
can be estimated using maximum likelihood es-
timation method with Chinese character trigram 
model on NIL corpus.  
In our implementation, Katz Backoff smooth-
ing technique (Katz, 1987) is used to handle the 
sparse data problem, and Viterbi algorithm is 
employed to find the optimal solution in XSCM.   
6 Evaluation 
6.1 Data Description 
Training Sets 
Two types of training data are used in our ex-
periments. We use news from Xinhua News 
Agency in LDC Chinese Gigaword v.2 
(CNGIGA) (Graf et al, 2005) as standard Chi-
nese corpus to construct phonetic mapping mod-
els because of its excellent coverage of standard 
Simplified Chinese. We use NIL corpus (Xia et 
al., 2006b) as chat language corpus. To evaluate 
our methods on size-varying training data, six 
chat language corpora are created based on NIL 
corpus. We select 6056 sentences from NIL cor-
pus randomly to make the first chat language 
corpus, i.e. C#1. In every next corpus, we add 
extra 1,211 random sentences. So 7,267 sen-
tences are contained in C#2, 8,478 in C#3, 9,689 
in C#4, 10,200 in C#5, and 12,113 in C#6.  
Test Sets 
Test sets are used to prove that chat language is 
dynamic and XSCM is effective and robust in 
normalizing dynamic chat language terms. Six 
time-varying test sets, i.e. T#1 ~ T#6, are created 
in our experiments. They contain chat language 
sentences posted from August 2005 to Jan 2006. 
We randomly extract 1,000 chat language sen-
tences posted in each month. So timestamp of the 
six test sets are in temporal order, in which time-
stamp of T#1 is the earliest and that of T#6 the 
newest.  
The normalized sentences are created by hand 
and used as standard normalization answers. 
997
6.2 Evaluation Criteria 
We evaluate two tasks in our experiments, i.e. 
recognition and normalization. In recognition, 
we use precision (p), recall (r) and f-1 measure 
(f) defined as follows.  
 2        
rp
rpf
zx
xr
yx
xp +
??=+=+=      (7) 
where x denotes the number of true positives, y 
the false positives and z the true negatives.  
For normalization, we use accuracy (a), which 
is commonly accepted by machine translation 
researchers as a standard evaluation criterion. 
Every output of the normalization methods is 
compared to the standard answer so that nor-
malization accuracy on each test set is produced.  
6.3 Experiment I: SCM vs. XSCM Using  
Size-varying Chat Language Corpora 
In this experiment we investigate on quality of 
XSCM and SCM using same size-varying train-
ing data. We intend to prove that chat language is 
dynamic and phonetic mapping models used in 
XSCM are helpful in addressing the dynamic 
problem. As no standard Chinese corpus is used 
in this experiment, we use standard Chinese text 
in chat language corpora to construct phonetic 
mapping models in XSCM. This violates the ba-
sic assumption that the phonetic mapping models 
should be constructed with standard Chinese 
corpus. So results in this experiment should be 
used only for comparison purpose. It would be 
unfair to make any conclusion on general per-
formance of XSCM method based on results in 
this experiments.   
We train the two methods with each of the six 
chat language corpora, i.e. C#1 ~ C#6 and test 
them on six time-varying test sets, i.e. T#1 ~ T#6. 
F-1 measure values produced by SCM and 
XSCM in this experiment are present in Table 3.  
Three tendencies should be pointed out ac-
cording to Table 3. The first tendency is that f-1 
measure in both methods drops on time-varying 
test sets (see Figure 1) using same training chat 
language corpora. For example, both SCM and 
XSCM perform best on the earliest test set T#1 
and worst on newest T#4. We find that the qual-
ity drop is caused by the dynamic nature of chat 
language. It is thus revealed that chat language is 
indeed dynamic. We also find that quality of 
XSCM drops less than that of SCM. This proves 
that phonetic mapping models used in XSCM are 
helpful in addressing the dynamic problem. 
However, quality of XSCM in this experiment 
still drops by 0.05 on the six time-varying test 
sets. This is because chat language text corpus is 
used as standard language corpus to model the 
phonetic mappings. Phonetic mapping models 
constructed with chat language corpus are far 
from sufficient. We will investigate in Experi-
ment-II to prove that stable phonetic mapping 
models can be constructed with real standard 
language corpus, i.e. CNGIGA.  
Test Set T#1 T#2 T#3 T#4 T#5 T#6
C#1 0.829 0.805 0.762 0.701 0.739 0.705
C#2 0.831 0.807 0.767 0.711 0.745 0.715
C#3 0.834 0.811 0.774 0.722 0.751 0.722
C#4 0.835 0.814 0.779 0.729 0.753 0.729
C#5 0.838 0.816 0.784 0.737 0.761 0.737
S
C
M
C#6 0.839 0.819 0.789 0.743 0.765 0.743
C#1 0.849 0.840 0.820 0.790 0.805 0.790
C#2 0.850 0.841 0.824 0.798 0.809 0.796
C#3 0.850 0.843 0.824 0.797 0.815 0.800
C#4 0.851 0.844 0.829 0.805 0.819 0.805
C#5 0.852 0.846 0.833 0.811 0.823 0.811
X
S
C
M
C#6 0.854 0.849 0.837 0.816 0.827 0.816
Table 3. F-1 measure by SCM and XSCM on six 
test sets with six chat language corpora. 
0.69
0.71
0.73
0.75
0.77
0.79
0.81
0.83
0.85
0.87
0.89
0.91
T#1 T#2 T#3 T#4 T#5 T#6
SCM-C#1
SCM-C#2
SCM-C#3
SCM-C#4
SCM-C#5
SCM-C#6
XSCM-C#1
XSCM-C#2
XSCM-C#3
XSCM-C#4
XSCM-C#5
XSCM-C#6
 
Figure 1. Tendency on f-1 measure in SCM and 
XSCM on six test sets with six chat language 
corpora. 
The second tendency is f-1 measure of both 
methods on same test sets drops when trained 
with size-varying chat language corpora. For ex-
ample, both SCM and XSCM perform best on 
the largest training chat language corpus C#6 and 
worst on the smallest corpus C#1. This tendency 
reveals that both methods favor bigger training 
chat language corpus. So extending the chat lan-
guage corpus should be one choice to improve 
quality of chat language term normalization.  
The last tendency is found on quality gap be-
tween SCM and XSCM. We calculate f-1 meas-
ure gaps between two methods using same train-
ing sets on same test sets (see Figure 2). Then the 
tendency is made clear. Quality gap between 
SCM and XSCM becomes bigger when test set 
998
becomes newer. On the oldest test set T#1, the 
gap is smallest, while on the newest test set T#6, 
the gap reaches biggest value, i.e. around 0.09. 
This tendency reveals excellent capability of 
XSCM in addressing dynamic problem using the 
phonetic mapping models.  
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
T#1 T#2 T#3 T#4 T#5 T#6
C#1
C#2
C#3
C#4
C#5
C#6
 
Figure 2. Tendency on f-1 measure gap in SCM 
and XSCM on six test sets with six chat language 
corpora. 
6.4 Experiment II: SCM vs. XSCM Using  
Size-varying Chat Language Corpora 
and CNGIGA 
In this experiment we investigate on quality of 
SCM and XSCM when a real standard Chinese 
language corpus is incorporated. We want to 
prove that the dynamic problem can be addressed 
effectively and robustly when CNGIGA is used 
as standard Chinese corpus.  
We train the two methods on CNGIGA and 
each of the six chat language corpora, i.e. C#1 ~ 
C#6. We then test the two methods on six time-
varying test sets, i.e. T#1 ~ T#6. F-1 measure 
values produced by SCM and XSCM in this ex-
periment are present in Table 4. 
Test Set T#1 T#2 T#3 T#4 T#5 T#6
C#1 0.849 0.840 0.820 0.790 0.735 0.703
C#2 0.850 0.841 0.824 0.798 0.743 0.714
C#3 0.850 0.843 0.824 0.797 0.747 0.720
C#4 0.851 0.844 0.829 0.805 0.748 0.727
C#5 0.852 0.846 0.833 0.811 0.758 0.734
S 
C 
M 
C#6 0.854 0.849 0.837 0.816 0.763 0.740
C#1 0.880 0.878 0.883 0.878 0.881 0.878
C#2 0.883 0.883 0.888 0.882 0.884 0.880
C#3 0.885 0.885 0.890 0.884 0.887 0.883
C#4 0.890 0.888 0.893 0.888 0.893 0.887
C#5 0.893 0.892 0.897 0.892 0.897 0.892
X 
S 
C 
M 
C#6 0.898 0.896 0.900 0.897 0.901 0.896
Table 4. F-1 measure by SCM and XSCM on six 
test sets with six chat language corpora and 
CNGIGA. 
Three observations are conducted on our re-
sults. First, according to Table 4, f-1 measure of 
SCM with same training chat language corpora 
drops on time-varying test sets, but XSCM pro-
duces much better f-1 measure consistently using 
CNGIGA and same training chat language cor-
pora (see Figure 3). This proves that phonetic 
mapping models are helpful in XSCM method. 
The phonetic mapping models contribute in two 
aspects. On the one hand, they improve quality 
of chat term normalization on individual test sets. 
On the other hand, satisfactory robustness is 
achieved consistently.  
0.69
0.71
0.73
0.75
0.77
0.79
0.81
0.83
0.85
0.87
0.89
0.91
T#1 T#2 T#3 T#4 T#5 T#6
SCM-C#1
SCM-C#2
SCM-C#3
SCM-C#4
SCM-C#5
SCM-C#6
XSCM-C#1
XSCM-C#2
XSCM-C#3
XSCM-C#4
XSCM-C#5
XSCM-C#6
`
 
Figure 3. Tendency on f-1 measure in SCM and 
XSCM on six test sets with six chat language 
corpora and CNGIGA. 
The second observation is conducted on pho-
netic mapping models constructed with 
CNGIGA. We find that 4,056,766 phonetic map-
ping models are constructed in this experiment, 
while only 1,303,227 models are constructed 
with NIL corpus in Experiment I. This reveals 
that coverage of standard Chinese corpus is cru-
cial to phonetic mapping modeling. We then 
compare two character lists constructed with two 
corpora. The 100 characters most frequently used 
in NIL corpus are rather different from those ex-
tracted from CNGIGA. We can conclude that 
phonetic mapping models should be constructed 
with a sound corpus that can represent standard 
language.  
The last observation is conducted on f-1 meas-
ure achieved by same methods on same test sets 
using size-varying training chat language corpora. 
Both methods produce best f-1 measure with big-
gest training chat language corpus C#6 on same 
test sets. This again proves that bigger  training 
chat language corpus could be helpful to improve 
quality of chat language term normalization. One 
question might be asked whether quality of 
XSCM converges on size of the training chat 
language corpus. This question remains open due 
to limited chat language corpus available to us.  
6.5 Error Analysis 
Typical errors in our experiments belong mainly 
to the following two types.  
999
Err.1 Ambiguous chat terms 
Example-1: ??? 8?  
In this example, XSCM finds no chat term 
while the correct normalization answer is ???
??? (I still don?t understand)?. Error illus-
trated in Example-1 occurs when chat terms 
?8(eight, ba1)? and ??(meter, mi3)? appear in a 
chat sentence together. In chat language, ??? in 
some cases is used to replace ??(understand, 
ming2)?, while in other cases, it is used to repre-
sent a unit for length, i.e. meter. When number 
?8? appears before ???, it is difficult to tell 
whether they are chat terms within sentential 
context. In our experiments, 93 similar errors 
occurred. We believe this type of errors can be 
addressed within discoursal context.  
Err.2 Chat terms created in manners other 
than phonetic mapping 
Example-2: ?? ing    
In this example, XSCM does not recognize 
?ing? while the correct answer is ?(??)?? 
(I?m worrying)?. This is because chat terms cre-
ated in manners other than phonetic mapping are 
excluded by the phonetic assumption in XSCM 
method. Around 1% chat terms fall out of pho-
netic mapping types. Besides chat terms holding 
same form as showed in Example-2, we find that 
emoticon is another major exception type. Fortu-
nately, dictionary-based method is powerful 
enough to handle the exceptions. So, in a real 
system, the exceptions are handled by an extra 
component.  
7 Conclusions 
To address the sparse data problem and dynamic 
problem in Chinese chat text normalization, the 
phonetic mapping models are proposed in this 
paper to represent mappings between chat terms 
and standard words. Different from character 
mappings, the phonetic mappings are constructed 
from available standard Chinese corpus. We ex-
tend the source channel model by incorporating 
the phonetic mapping models. Three conclusions 
can be made according to our experiments. 
Firstly, XSCM outperforms SCM with same 
training data. Secondly, XSCM produces higher 
performance consistently on time-varying test 
sets.  Thirdly, both SCM and XSCM perform 
best with biggest training chat language corpus.  
Some questions remain open to us regarding 
optimal size of training chat language corpus in 
XSCM.  Does the optimal size exist? Then what 
is it? These questions will be addressed in our 
future work. Moreover, bigger context will be 
considered in chat term normalization, discourse 
for instance.  
Acknowledgement 
Research described in this paper is partially sup-
ported by the Chinese University of Hong Kong 
under the Direct Grant Scheme project 
(2050330) and Strategic Grant Scheme project 
(4410001). 
References 
Brown, P. F., J. Cocke, S. A. D. Pietra, V. J. D. Pietra, 
F. Jelinek, J. D. Lafferty, R. L. Mercer and P. S. 
Roossin. 1990.  A statistical approach to machine 
translation. Computational Linguistics, v.16 n.2, 
p.79-85. 
Gianforte, G.. 2003. From Call Center to Contact 
Center: How to Successfully Blend Phone, Email, 
Web and Chat to Deliver Great Service and Slash 
Costs. RightNow Technologies. 
Graf, D., K. Chen, J.Kong and K. Maeda. 2005. Chi-
nese Gigaword Second Edition. LDC Catalog 
Number LDC2005T14. 
Heard-White, M., Gunter Saunders and Anita Pincas. 
2004.  Report into the use of CHAT in education. 
Final report for project of Effective use of CHAT 
in Online Learning, Institute of Education, Univer-
sity of London. 
James, F.. 2000. Modified Kneser-Ney Smoothing of 
n-gram Models. RIACS Technical Report 00.07. 
Katz, S. M.. Estimation of probabilities from sparse 
data for the language model component of a speech 
recognizer. IEEE Transactions on Acoustics, 
Speech and Signal Processing, 35(3):400-401. 
Li, H., W. He and B. Yuan. 2003. An Kind of Chinese 
Text Strings' Similarity and its Application in 
Speech Recognition. Journal of Chinese Informa-
tion Processing, 2003 Vol.17 No.1 P.60-64.  
McCullagh, D.. 2004. Security officials to spy on chat 
rooms. News provided by CNET Networks. No-
vember 24, 2004. 
Xia, Y., K.-F. Wong and W. Gao. 2005. NIL is not 
Nothing: Recognition of Chinese Network Infor-
mal Language Expressions. 4th SIGHAN Work-
shop at IJCNLP'05, pp.95-102. 
Xia, Y. and K.-F. Wong. 2006a. Anomaly Detecting 
within Dynamic Chinese Chat Text. EACL?06 
NEW TEXT workshop, pp.48-55.  
Xia, Y., K.-F. Wong and W. Li. 2006b. Constructing 
A Chinese Chat Text Corpus with A Two-Stage 
Incremental Annotation Approach. LREC?06. 
1000
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 133?136,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Sentiment Vector Space Model for  
Lyric-based Song Sentiment Classification 
 
 
Yunqing Xia Linlin Wang 
Center for Speech and language Tech.  State Key Lab of Intelligent Tech. and Sys. 
RIIT, Tsinghua University Dept. of CST, Tsinghua University 
Beijing 100084, China Beijing 100084, China 
yqxia@tsinghua.edu.cn wangll07@mails.tsinghua.edu.cn 
  
Kam-Fai Wong Mingxing Xu 
Dept. of SE&EM Dept. of CST 
The Chinese University of Hong Kong Tsinghua University 
Shatin, Hong Kong Beijing 100084, China 
kfwong@se.cuhk.edu.hk xumx@tsinghua.edu.cn 
 
 
Abstract 
Lyric-based song sentiment classification 
seeks to assign songs appropriate sentiment 
labels such as light-hearted and heavy-hearted. 
Four problems render vector space model 
(VSM)-based text classification approach in-
effective: 1) Many words within song lyrics 
actually contribute little to sentiment; 2) 
Nouns and verbs used to express sentiment are 
ambiguous; 3) Negations and modifiers 
around the sentiment keywords make particu-
lar contributions to sentiment; 4) Song lyric is 
usually very short. To address these problems, 
the sentiment vector space model (s-VSM) is 
proposed to represent song lyric document. 
The preliminary experiments prove that the s-
VSM model outperforms the VSM model in 
the lyric-based song sentiment classification 
task. 
1 Introduction 
Song sentiment classification nowadays becomes a 
hot research topic due largely to the increasing 
demand of ubiquitous song access, especially via 
mobile phone. In their music phone W910i, Sony 
and Ericsson provide Sense Me component to catch 
owner?s mood and play songs accordingly. Song 
sentiment classification is the key technology for 
song recommendation. Many research works have 
been reported to achieve this goal using audio sig-
nal (Knees et al, 2007). But research efforts on 
lyric-based song classification are very few. 
Preliminary experiments show that VSM-based 
text classification method (Joachims, 2002) is inef-
fective in song sentiment classification (see Sec-
tion 5) due to the following four reasons. Firstly, 
the VSM model considers all content words within 
song lyric as features in text classification. But in 
fact many words in song lyric actually make little 
contribution to sentiment expressing. Using all 
content words as features, the VSM-based classifi-
cation methods perform poorly in song sentiment 
classification. Secondly, observation on lyrics of 
thousands of Chinese pop songs reveals that senti-
ment-related nouns and verbs usually carry multi-
ple senses. Unfortunately, the ambiguity is not 
appropriately handled in the VSM model. Thirdly, 
negations and modifiers are constantly found 
around the sentiment words in song lyric to inverse, 
to strengthen or to weaken the sentiments that the 
sentences carry. But the VSM model is not capable 
of reflecting these functions. Lastly, song lyric is 
usually very short, namely 50 words on average in 
length, rendering serious sparse data problem in 
VSM-based classification. 
To address the aforementioned problems of the 
VSM model, the sentiment vector space model (s-
VSM) is proposed in this work. We adopt the s-
VSM model to extract sentiment features from 
song lyrics and implement the SVM-light 
(Joachims, 2002) classification algorithm to assign 
sentiment labels to given songs. 
133
2 Related Works  
Song sentiment classification has been investigated 
since 1990s in audio signal processing community 
and research works are mostly found relying on 
audio signal to make a decision using machine 
learning algorithms (Li and Ogihara, 2006; Lu et 
al., 2006). Typically, the sentiment classes are de-
fined based on the Thayer?s arousal-valence emo-
tion plane (Thayer, 1989). Instead of assigning 
songs one of the four typical sentiment labels, Lu 
et al (2006) propose the hierarchical framework to 
perform song sentiment classification with two 
steps. In the first step the energy level is detected 
with intensity features and the stress level is de-
termined in the second step with timbre and 
rhythm features. It is proved difficult to detect 
stress level using audio as classification proof. 
Song sentiment classification using lyric as 
proof is recently investigated by Chen et al (2006). 
They adopt the hierarchical framework and make 
use of song lyric to detect stress level in the second 
step. In fact, many literatures have been produced 
to address the sentiment analysis problem in natu-
ral language processing research. Three approaches 
are dominating, i.e. knowledge-based approach 
(Kim and Hovy, 2004), information retrieval-based 
approach (Turney and Littman, 2003) and machine 
learning approach (Pang et al, 2002), in which the 
last approach is found very popular. Pang et al 
(2002) adopt the VSM model to represent product 
reviews and apply text classification algorithms 
such as Na?ve Bayes, maximum entropy and sup-
port vector machines to predict sentiment polarity 
of given product review.  
Chen et al (2006) also apply the VSM model in 
lyric-based song sentiment classification. However, 
our experiments show that song sentiment classifi-
cation with the VSM model delivers disappointing 
quality (see Section 5). Error analysis reveals that 
the VSM model is problematic in representing 
song lyric. It is necessary to design a new lyric rep-
resentation model for song sentiment classification. 
3 Sentiment Vector Space Model 
We propose the sentiment vector space model (s-
VSM) for song sentiment classification. Principles 
of the s-VSM model are listed as follows. 
(1) Only sentiment-related words are used to pro-
duce sentiment features for the s-VSM model.  
(2) The sentiment words are appropriately disam-
biguated with the neighboring negations and 
modifiers.  
(3) Negations and modifiers are included in the s-
VSM model to reflect the functions of invers-
ing, strengthening and weakening.  
Sentiment unit is found the appropriate element 
complying with the above principles.  
To be general, we first present the notation for 
sentiment lexicon as follows. 
 ,...,1},{                         
   ,...,1},{                         
  ,...,1},{  };,,{
LlmM
JjnN
IicCMNCL
l
j
i
==
==
===
 
in which L represents sentiment lexicon, C senti-
ment word set, N negation set and M modifier set. 
These words can be automatically extracted from a 
semantic dictionary and each sentiment word is 
assigned a sentiment label, namely light-hearted or 
heavy-hearted according to its lexical definition.  
Given a piece of song lyric, denoted as follows,  
HhwW h ,...,1},{ ==  
in which W denotes a set of words that appear in 
the song lyric, the semantic lexicon is in turn used 
to locate sentiment units denoted as follows. 
MWmNWnCWc
mncuU
vlvjvi
vlvjviv
??????
==
,,,
,,,
  ;  ;,
 },,{}{
 
Note that sentiment units are unambiguous sen-
timent expressions, each of which contains one 
sentiment word and possibly one modifier and one 
negation. Negations and modifiers are helpful to 
determine the unique meaning of the sentiment 
words within certain context window, e.g. 3 pre-
ceding words and 3 succeeding words in our case.  
Then, the s-VSM model is presented as follows. 
))(),...,(),(( 21 UfUfUfV TS = . 
in which VS represents the sentiment vector for the 
given song lyric and fi(U) sentiment features which 
are usually certain statistics on sentiment units that 
appear in lyric.  
We classify the sentiment units according to oc-
currence of sentiment words, negations and modi-
fiers. If the sentiment word is mandatory for any 
sentiment unit, eight kinds of sentiment units are 
obtained. Let fPSW denote count of positive senti-
134
ment words (PSW), fNSW count of negative senti-
ment words (NSW), fNEG count of negations (NEG) 
and fMOD count of modifiers (MOD). Eight senti-
ment features are defined in Table 1.  
fi Number of sentiment units satisfying ?
f1 fPSW >0, fNSW =fNEG =fMOD =0  
f2 fPSW =0, fNSW >0, fNEG = fMOD =0  
f3 fPSW >0, fNSW =0,  fNEG>0, fMOD =0 
f4 fPSW=0, fNSW >0, fNEG >0, fMOD =0  
f5 fPSW >0, fNSW =0, fNEG =0, fMOD >0  
f6 fPSW=0, fNSW >0, fNEG =0, fMOD >0  
f7 fPSW >0, fNSW =0, fNEG >0, fMOD >0  
f8 fPSW =0, fNSW >0, fNEG >0, fMOD >0  
Table 1. Definition of sentiment features. Note that 
one sentiment unit contains only one sentiment 
word. Thus it is not possible that fPSW and fNSW are 
both bigger than zero. 
Obviously, sparse data problem can be well ad-
dressed using statistics on sentiment units rather 
than on individual words or sentiment units.  
4  Lyric-based Song Sentiment Classifica-
tion 
Song sentiment classification based on lyric can be 
viewed as a text classification task thus can be 
handled by some standard classification algorithms. 
In this work, the SVM-light algorithm is imple-
mented to accomplish this task due to its excel-
lence in text classification.  
Note that song sentiment classification differs 
from the traditional text classification in feature 
extraction. In our case, sentiment units are first 
detected and the sentiment features are then gener-
ated based on sentiment units. As the sentiment 
units carry unambiguous sentiments, it is deemed 
that the s-VSM is model is promising to carry out 
the song sentiment classification task effectively. 
5 Evaluation 
To evaluate the s-VSM model, a song corpus, i.e. 
5SONGS, is created manually. It covers 2,653 Chi-
nese pop songs, in which 1,632 are assigned label 
of light-hearted (positive class) and 1,021 assigned 
heavy-hearted (negative class). We randomly se-
lect 2,001 songs (around 75%) for training and the 
rest for testing. We adopt the standard evaluation 
criteria in text classification, namely precision (p), 
recall (r), f-1 measure (f) and accuracy (a) (Yang 
and Liu, 1999). 
In our experiments, three approaches are imple-
mented in song sentiment classification, i.e. audio-
based (AB) approach, knowledge-based (KB) ap-
proach and machine learning (ML) approach, in 
which the latter two approaches are also referred to 
as text-based (TB) approach. The intentions are 1) 
to compare AB approach against the two TB ap-
proaches, 2) to compare the ML approach against 
the KB approach, and 3) to compare the VSM-
based ML approach against the s-VSM-based one. 
Audio-based (AB) Approach 
We extract 10 timbre features and 2 rhythm fea-
tures (Lu et al, 2006) from audio data of each song. 
Thus each song is represented by a 12-dimension 
vector. We run SVM-light algorithm to learn on the 
training samples and classify test ones.  
Knowledge-based (KB) Approach 
We make use of HowNet (Dong and dong, 
2006), to detect sentiment words, to recognize the 
neighboring negations and modifiers, and finally to 
locate sentiment units within song lyric. Sentiment 
(SM) of the sentiment unit (SU) is determined con-
sidering sentiment words (SW), negation (NEG) 
and modifiers (MOD) using the following rule.  
(1) SM(SU) = label(SW); 
(2) SM(SU) = - SM(SU) iff SU contains NEG; 
(3) SM(SU) = degree(MOD)*SM(SU) iff SU 
contains MOD. 
In the above rule, label(x) is the function to read 
sentiment label(?{1, -1}) of given word in the 
sentiment lexicon and degree(x) to read its modifi-
cation degree(?{1/2, 2}). As the sentiment labels 
are integer numbers, the following formula is 
adopted to obtain label of the given song lyric.  
??
???
?= ?
i
iSUSMsignlabel )(  
Machine Learning (ML) Approach 
The ML approach adopts text classification al-
gorithms to predict sentiment label of given song 
lyric. The SVM-light algorithm is implemented 
based on VSM model and s-VSM model, respec-
tively. For the VSM model, we apply (CHI) algo-
rithm (Yang and Pedersen, 1997) to select effective 
sentiment word features. For the s-VSM model, we 
adopt HowNet as the sentiment lexicon to create 
sentiment vectors.  
Experimental results are presented Table 2.    
135
 p R f-1 a 
Audio-based 0.504 0.701 0.586 0.504
Knowledge-based 0.726 0.584 0.647 0.714
VSM-based 0.587 1.000 0.740 0.587
s-VSM-based 0.783 0.750 0.766 0.732
Table 2. Experimental results 
Table 2 shows that the text-based methods out-
perform the audio-based method. This justifies our 
claim that lyric is better than audio in song senti-
ment detection. The second observation is that ma-
chine learning approach outperforms the 
knowledge-based approach. The third observation 
is that s-VSM-based method outperforms VSM-
based method on f-1 score. Besides, we surpris-
ingly find that VSM-based method assigns all test 
samples light-hearted label thus recall reaches 
100%. This makes results of VSM-based method 
unreliable. We look into the model file created by 
the SVM-light algorithm and find that 1,868 of 
2,001 VSM training vectors are selected as support 
vectors while 1,222 s-VSM support vectors are 
selected. This indicates that the VSM model indeed 
suffers the problems mentioned in Section 1 in 
lyric-based song sentiment classification. As a 
comparison, the s-VSM model produces more dis-
criminative support vectors for the SVM classifier 
thus yields reliable predictions.  
6  Conclusions and Future Works 
The s-VSM model is presented in this paper as a 
document representation model to address the 
problems encountered in song sentiment classifica-
tion. This model considers sentiment units in fea-
ture definition and produces more discriminative 
support vectors for song sentiment classification. 
Some conclusions can be drawn from the prelimi-
nary experiments on song sentiment classification. 
Firstly, text-based methods are more effective than 
the audio-based method. Secondly, the machine 
learning approach outperforms the knowledge-
based approach. Thirdly, s-VSM model is more 
reliable and more accurate than the VSM model. 
We are thus encouraged to carry out more research 
to further refine the s-VSM model in sentiment 
classification. In the future, we will incorporate 
some linguistic rules to improve performance of 
sentiment unit detection. Meanwhile, sentiment 
features in the s-VSM model are currently equally 
weighted. We will adopt some estimation tech-
niques to assess their contributions for the s-VSM 
model. Finally, we will also explore how the s-
VSM model improves quality of polarity classifi-
cation in opinion mining.  
Acknowledgement 
Research work in this paper is partially supported 
by NSFC (No. 60703051) and Tsinghua University 
under the Basic Research Foundation (No. 
JC2007049). 
References  
R.H. Chen, Z.L. Xu, Z.X. Zhang and F.Z. Luo. Content 
Based Music Emotion Analysis and Recognition. 
Proc. of 2006 International Workshop on Computer 
Music and Audio Technology, pp.68-75. 2006.  
Z. Dong and Q. Dong. HowNet and the Computation of 
Meaning. World Scientific Publishing. 2006. 
T. Joachims. Learning to Classify Text Using Support 
Vector Machines, Methods, Theory, and Algorithms. 
Kluwer (2002). 
S.-M. Kim and E. Hovy. Determining the Sentiment of 
Opinions. Proc. COLING?04, pp. 1367-1373. 2004. 
P. Knees, T. Pohle, M. Schedl and G. Widmer. A Music 
Search Engine Built upon Audio-based and Web-
based Similarity Measures. Proc. of SIGIR'07, pp.47-
454. 2007 
T. Li and M. Ogihara. Content-based music similarity 
search and emotion detection. Proc. IEEE Int. Conf. 
Acoustic, Speech, and Signal Processing, pp. 17?21. 
2006. 
L. Lu, D. Liu and H. Zhang. Automatic mood detection 
and tracking of music audio signals. IEEE Transac-
tions on Audio, Speech & Language Processing 
14(1): 5-18 (2006). 
B. Pang, L. Lee and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. Proc. of EMNLP-02, pp.79-86. 2002. 
R. E. Thayer, The Biopsychology of Mood and Arousal, 
New York, Oxford University Press. 1989. 
P. D. Turney and M. L. Littman. Measuring praise and 
criticism: Inference of semantic orientation from as-
sociation. ACM Trans. on Information Systems, 
21(4):315?346. 2003. 
Y. Yang and X. Liu. A Re-Examination of Text Catego-
rization Methods. Proc. of SIGIR?99, pp. 42-49. 1999. 
Y. Yang and J. O. Pedersen. A comparative study on 
feature selection in text categorization. Proc. 
ICML?97, pp.412-420. 1997. 
 
136
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1075?1083,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Exploiting Bilingual Information to Improve Web Search
Wei Gao1, John Blitzer2, Ming Zhou3, and Kam-Fai Wong1
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong, China
{wgao,kfwong}@se.cuhk.edu.hk
2Computer Science Division, University of California at Berkeley, CA 94720-1776, USA
blitzer@cs.berkeley.edu
3Microsoft Research Asia, Beijing 100190, China
mingzhou@microsoft.com
Abstract
Web search quality can vary widely across
languages, even for the same information
need. We propose to exploit this variation
in quality by learning a ranking function
on bilingual queries: queries that appear in
query logs for two languages but represent
equivalent search interests. For a given
bilingual query, along with correspond-
ing monolingual query log and monolin-
gual ranking, we generate a ranking on
pairs of documents, one from each lan-
guage. Then we learn a linear ranking
function which exploits bilingual features
on pairs of documents, as well as standard
monolingual features. Finally, we show
how to reconstruct monolingual ranking
from a learned bilingual ranking. Us-
ing publicly available Chinese and English
query logs, we demonstrate for both lan-
guages that our ranking technique exploit-
ing bilingual data leads to significant im-
provements over a state-of-the-art mono-
lingual ranking algorithm.
1 Introduction
Web search quality can vary widely across lan-
guages, even for a single query and search en-
gine. For example, we might expect that rank-
ing search results for the query Wj? ?Y?
(Thomas Hobbes) to be more difficult in Chinese
than it is in English, even while holding the ba-
sic ranking function constant. At the same time,
ranking search results for the query Han Feizi (8
:) is likely to be harder in English than in Chi-
nese. A large portion of web queries have such
properties that they are originated in a language
different from the one they are searched.
This variance in problem difficulty across lan-
guages is not unique to web search; it appears in
a wide range of natural language processing prob-
lems. Much recent work on bilingual data has fo-
cused on exploiting these variations in difficulty
to improve a variety of monolingual tasks, includ-
ing parsing (Hwa et al, 2005; Smith and Smith,
2004; Burkett and Klein, 2008; Snyder and Barzi-
lay, 2008), named entity recognition (Chang et al,
2009), and topic clustering (Wu and Oard, 2008).
In this work, we exploit a similar intuition to im-
prove monolingual web search.
Our problem setting differs from cross-lingual
web search, where the goal is to return machine-
translated results from one language in response to
a query from another (Lavrenko et al, 2002). We
operate under the assumption that for many mono-
lingual English queries (e.g., Han Feizi), there ex-
ist good documents in English. If we have Chinese
information as well, we can exploit it to help find
these documents. As we will see, machine trans-
lation can provide important predictive informa-
tion in our setting, but we do not wish to display
machine-translated output to the user.
We approach our problem by learning a rank-
ing function for bilingual queries ? queries that
are easily translated (e.g., with machine transla-
tion) and appear in the query logs of two languages
(e.g., English and Chinese). Given query logs
in both languages, we identify bilingual queries
with sufficient clickthrough statistics in both sides.
Large-scale aggregated clickthrough data were
proved useful and effective in learning ranking
functions (Dou et al, 2008). Using these statis-
tics, we can construct a ranking over pairs of docu-
ments, one from each language. We use this rank-
ing to learn a linear scoring function on pairs of
documents given a bilingual query.
We find that our bilingual rankings have good
monolingual ranking properties. In particular,
given an optimal pairwise bilingual ranking, we
show that simple heuristics can effectively approx-
imate the optimal monolingual ranking. Using
1075
1 10 100 1,000 10,000 50,0000
5
10
15
20
25
30
35
40
45
50
Frequency (# of times that queries are issued)
Pro
por
tion
 of 
bilin
gua
l qu
erie
s (%
)
 
 
English
Chinese
Figure 1: Proportion of bilingual queries in the
query logs of different languages.
these heuristics and our learned pairwise scoring
function, we can derive a ranking for new, unseen
bilingual queries. We develop and test our bilin-
gual ranker on English and Chinese with two large,
publicly available query logs from the AOL search
engine1 (English query log) (Pass et al, 2006)
and the Sougou search engine2 (Chinese query
log) (Liu et al, 2007). For both languages, we
achieve significant improvements over monolin-
gual Ranking SVM (RSVM) baselines (Herbrich
et al, 2000; Joachims, 2002), which exploit a va-
riety of monolingual features.
2 Bilingual Query Statistics
We designate a query as bilingual if the concept
has been searched by users of both two languages.
As a result, not only does it occur in the query log
of its own language, but its translation also appears
in the log of the second language. So a bilingual
query yields reasonable queries in both languages.
Of course, most queries are not bilingual. For ex-
ample, our English log contains map of Alabama,
but not our Chinese log. In this case, we wouldn?t
expect the Chinese results for the query?s transla-
tion, ?n?j?C, to be helpful in ranking the
English results.
In total, we extracted 4.8 million English
queries from AOL log, of which 1.3% of their
translations appear in Sogou log. Similarly, of our
3.1 million Chinese queries from Sogou log, 2.3%
of their translations appear in AOL log. By to-
tal number of queries issued (i.e., counting dupli-
1http://search.aol.com
2http://www.sogou.com
cates), the proportion of bilingual queries is much
higher. As Figure 1 shows as the number of times
a query is issued increases, so does the chance of
it being bilingual. In particular, nearly 45% of the
highest-frequency English queries and 35% of the
highest-frequency Chinese queries are bilingual.
3 Learning to Rank Using Bilingual
Information
Given a set of bilingual queries, we now de-
scribe how to learn a ranking function for mono-
lingual data that exploits information from both
languages. Our procedure has three steps: Given
two monolingual rankings, we construct a bilin-
gual ranking on pairs of documents, one from each
language. Then we learn a linear scoring function
for pairs of documents that exploits monolingual
information (in both languages) and bilingual in-
formation. Finally, given this ranking function on
pairs and a new bilingual query, we reconstruct a
monolingual ranking for the language of interest.
This section addresses these steps in turn.
3.1 Creating Bilingual Training Data
Without loss of generality, suppose we rank En-
glish documents with constraints from Chinese
documents. Given an English log Le and a Chi-
nese log Lc, our ranking algorithm takes as input
a bilingual query pair q = (qe, qc) where qe ? Le
and qc ? Lc, a set of returned English documents
{ei}Ni=1 from qe, and a set of constraint Chinese
documents {cj}nj=1 from qc. In order to create
bilingual ranking data, we first generate monolin-
gual ranking data from clickthrough statistics. For
each language-query-document triple, we calcu-
late the aggregated click count across all users and
rank documents according to this statistic. We de-
note the count of a page as C(ei) or C(cj).
The use of clickthrough statistics as feedback
for learning ranking functions is not without con-
troversy, but recent empirical results on large
data sets suggest that the aggregated user clicks
provides an informative indicator of relevance
preference for a query. Joachims et al (2007)
showed that relative feedback signals generated
from clicks correspond well with human judg-
ments. Dou et al (2008) revealed that a straight-
forward use of aggregated clicks can achieve a bet-
ter ranking than using explicitly labeled data be-
cause clickthrough data contain fine-grained dif-
ferences between documents useful for learning an
1076
Table 1: Clickthrough data of a bilingual query
pair extracted from query logs.
Bilingual query pair (Mazda,jH)
doc URL click #
e1 www.mazda.com 229
e2 www.mazdausa.com 185
e3 www.mazda.co.uk 5
e4 www.starmazda.com 2
e5 www.mazdamotosports.com 2
. . . . . .
c1 www.faw-mazda.com 50
c2 price.pcauto.com.cn/brand.
jsp?bid=17
43
c3 auto.sina.com.cn/salon/
FORD/MAZDA.shtml
20
c4 car.autohome.com.cn/brand/
119/
18
c5 jsp.auto.sohu.com/view/
brand-bid-263.html
9
. . . . . .
accurate and reliable ranking. Therefore, we lever-
age aggregated clicks for comparing the relevance
order of documents. Note that there is nothing
specific to our technique that requires clickthrough
statistics. Indeed, our methods could easily be em-
ployed with human annotated data. Table 1 gives
an example of a bilingual query pair and the ag-
gregated click count of each result page.
Given two monolingual documents, a prefer-
ence order can be inferred if one document is
clicked more often than another. To allow for
cross-lingual information, we extend the order of
individual documents into that of bilingual docu-
ment pairs: given two bilingual document pairs,
we will write
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
to indi-
cate that the pair of
(
e(1)i , c
(1)
j
)
is ranked higher
than the pair of
(
e(2)i , c
(2)
j
)
.
Definition 1
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
if and
only if one of the following relations hold:
1. C(e(1)i ) > C(e
(2)
i ) and C(c
(1)
j ) ? C(c
(2)
j )
2. C(e(1)i ) ? C(e
(2)
i ) and C(c
(1)
j ) > C(c
(2)
j )
Note, however, that from a purely monolingual
perspective, this definition introduces orderings on
documents that should not initially have existed.
For English ranking, for example, we may have(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
even when C(e(1)i ) =
C(e(2)i ). This leads us to the following asymmet-
ric definition of  that we use in practice:
Definition 2
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
if and
only if C(e(1)i ) > C(e
(2)
i ) and C(c
(1)
j ) ? C(c
(2)
j )
With this definition, we can unambiguously
compare the relevance of bilingual document pairs
based on the order of monolingual documents.
The advantages are two-fold: (1) we can treat mul-
tiple cross-lingual document similarities the same
way as the commonly used query-document fea-
tures in a uniform manner of learning; (2) with the
similarities, the relevance estimation on bilingual
document pairs can be enhanced, and this in return
can improve the ranking of documents.
3.2 Ranking Model
Given a pair of bilingual queries (qe, qc), we
can extract the set of corresponding bilin-
gual document pairs and their click counts
{(ei, cj), (C(ei), C(cj))}, where i = 1, . . . , N
and j = 1, . . . , n. Based on that, we produce a
set of bilingual ranking instances S = {?ij, zij},
where each ?ij = {xi;yj; sij} is the feature
vector of (ei, cj) consisting of three components:
xi = f(qe, ei) is the vector of monolingual rele-
vancy features of ei, yi = f(qc, cj) is the vector
of monolingual relevancy features of cj , and sij =
sim(ei, cj) is the vector of cross-lingual similari-
ties between ei and cj , and zij = (C(ei), C(cj))
is the corresponding click counts.
The task is to select the optimal function that
minimizes a given loss with respect to the order
of ranked bilingual document pairs and the gold.
We resort to Ranking SVM (RSVM) (Herbrich et
al., 2000; Joachims, 2002) learning for classifica-
tion on pairs of instances. Compared the base-
line RSVM (monolingual), our algorithm learns
to classify on pairs of bilingual document pairs
rather than on pairs of individual documents.
Let f being a linear function:
f~w(ei, cj) = ~wx ? xi + ~wy ? yj + ~ws ? sij (1)
where ~w = {~wx; ~wy; ~ws} denotes the weight vec-
tor, in which the elements correspond to the rele-
vancy features and similarities. For any two bilin-
gual document pairs, their preference relation is
measured by the difference of the functional val-
ues of Equation 1:
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
?
f~w
(
e(1)i , c
(1)
j
)
? f~w
(
e(2)i , c
(2)
j
)
> 0 ?
~wx ?
(
x(1)i ? x
(2)
i
)
+ ~wy ?
(
y(1)j ? y
(2)
j
)
+
~ws ?
(
s(1)ij ? s
(2)
ij
)
> 0
1077
We then create a new training corpus based on the
preference ordering of any two such pairs: S? =
{??ij, z
?
ij}, where the new feature vector becomes
??ij =
{
x(1)i ? x
(2)
i ;y
(1)
j ? y
(2)
j ; s
(1)
ij ? s
(2)
ij
}
,
and the class label
z?ij =
?
?
?
+1, if
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
;
?1, if
(
e(2)i , c
(2)
j
)

(
e(1)i , c
(1)
j
)
is a binary preference value depending on the or-
der of bilingual document pairs. The problem is to
solve SVM objective: min
~w
1
2?~w?
2 + ?
?
i
?
j ?ij
subject to bilingual constraints: z?ij ? (~w ? ?
?
ij) ?
1? ?ij and ?ij ? 0.
There are potentially ? = nN bilingual docu-
ment pairs for each query, and the number of com-
parable pairs may be much larger due to the com-
binatorial nature (but less than ?(? ? 1)/2). To
speed up training, we resort to stochastic gradient
descent (SGD) optimizer (Shalev-Shwartz et al,
2007) to approximate the true gradient of the loss
function evaluated on a single instance (i.e., per
constraint). The parameters are then adjusted by
an amount proportional to this approximate gradi-
ent. For large data set, SGD-RSVM can be much
faster than batch-mode gradient descent.
3.3 Inference
The solution ~w forms a vector orthogonal to the
hyper-plane of RSVM. To predict the order of
bilingual document pairs, the ranking score can
be simply calculated by Equation 1. However, a
prominent problem is how to derive the full order
of monolingual documents for output from the or-
der of bilingual document pairs. To our knowl-
edge, there is no precise conversion algorithm in
polynomial time. We thus adopt two heuristics for
approximating the true document score:
? H-1 (max score): Choose the maximum
score of the pair as the score of document,
i.e., score(ei) = maxj(f(ei, cj)).
? H-2 (mean score): Average over all the
scores of pairs associated with the ranked
document as the score of this document, i.e.,
score(ei) = 1/n
?
j f(ei, cj).
Intuitively, for the rank score of a single docu-
ment, H-2 combines the ?voting? scores from its n
constraint documents weighted equally, while H-1
simply chooses the maximum one. A formal ap-
proach to the problem is to leverage rank aggre-
gation formalism (Dwork et a., 2001; Liu et al,
2007), which will be left for our future work. The
two simple heuristics are employed here because
of their simplicity and efficiency. The time com-
plexity of the approximation is linear to the num-
ber of ranked documents given n is constant.
4 Features and Similarities
Standard features for learning to rank include vari-
ous query-document features, e.g., BM25 (Robert-
son, 1997), as well as query-independent features,
e.g., PageRank (Brin and Page, 1998). Our feature
space consists of both these standard monolingual
features and cross-lingual similarities among doc-
uments. The cross-lingual similarities are valu-
ated using different translation mechanisms, e.g.,
dictionary-based translation or machine transla-
tion, or even without any translation at all.
4.1 Monolingual Relevancy Features
In learning to rank, the relevancy between query
and documents and the measures based on link
analysis are commonly used as features. The dis-
cussion on their details is beyond the scope of this
paper. Readers may refer to (Liu et al, 2007)
for the definitions of many such features. We im-
plement six of these features that are considered
the most typical shown as Table 2. These include
sets of measures such as BM25, language-model-
based IR score, and PageRank. Because most con-
ventional IR and web search relevancy measures
fall into this category, we call them altogether IR
features in what follows. Note that for a given
bilingual document pair (e, c), the monolingual IR
features consist of relevance score vectors f(qe, e)
in English and f(qc, c) in Chinese.
4.2 Cross-lingual Document Similarities
To measure the document similarity across dif-
ferent languages, we define the similarity vector
sim(e, c) as a series of functions mapping a bilin-
gual document pair to positive real numbers. In-
tuitively, a good similarity function is one which
maps cross-lingual relevant documents into close
scores and maintains a large distance between ir-
relevant and relevant documents. Four categories
of similarity measures are employed.
Dictionary-based Similarity (DIC): For
dictionary-based document translation, we use
1078
Table 2: List of monolingual relevancy measures
used as IR features in our model.
IR Feature Description
BM25 Okapi BM25 score (Robertson, 1997)
BM25 PRF Okapi BM25 score with pseudo-
relevance feedback (Robertson and
Jones, 1976)
LM DIR Language-model-based IR score with
Dirichlet smoothing (Zhai and Lafferty,
2001)
LM JM Language-model-based IR score with
Jelinek-Mercer smoothing (Zhai and
Lafferty, 2001)
LM ABS Language-model-based IR score with
absolute discounting (Zhai and Lafferty,
2001)
PageRank PageRank score (Brin and Page, 1998)
the similarity measure proposed by Mathieu et
al. (2004). Given a bilingual dictionary, we let
T (e, c) denote the set of word pairs (we, wc) such
that we is a word in English document e, and wc
is a word in Chinese document c, and we is the
English translation of wc. We define tf(we, e)
and tf(wc, c) to be the term frequency of we in
e and that of wc in c, respectively. Let df(we)
and df(wc) be the English document frequency
for we and Chinese document frequency for
wc. If ne (nc) is the total number of English
(Chinese), then the bilingual idf is defined as
idf(we, wc) = log ne+ncdf(we)+df(wc) . Then the
cross-lingual document similarity is calculated by
sim(e, c) =
?
(we,wc)?T (e,c)
tf(we,e)tf(wc,c)idf(we,wc)2
?
Z
where Z is a normalization coefficient (see Math-
ieu et al (2004) for detail). This similarity func-
tion can be understood as the cross-lingual coun-
terpart of the monolingual cosine similarity func-
tion (Salton, 1998).
Similarity Based on Machine Translation
(MT): For machine translation, the cross-lingual
measure actually becomes a monolingual similar-
ity between one document and another?s transla-
tion. We therefore adopt cosine function for it di-
rectly (Salton, 1998).
Translation Ratio (RATIO): Translation ratio
is defined as two sets of ratios of translatable terms
using a bilingual dictionary: RATIO FOR ? what
percent of words in e can be translated to words in
c; RATIO BACK ? what percent of words in c can
be translated back to words in e.
URL LCS Ratio (URL): The ratio of longest
common subsequence (Cormen et al, 2001) be-
tween the URLs of two pages being compared.
This measure is useful to capture pages in different
languages but with similar URLs such as www.
airbus.com, www.airbus.com.cn, etc.
Note that each set of similarities above except
URL includes 3 values based on different fields of
web page: title, body, and title+body.
5 Experiments and Results
This section presents evaluation metric, data sets
and experiments for our proposed ranker.
5.1 Evaluation Metric
Commonly adopted metrics for ranking, such as
mean average precision (Buckley and Voorhees,
2000) and Normalized Discounted Cumulative
Gain (Ja?rvelin and Keka?la?inen, 2000), is designed
for data sets with human relevance judgment,
which is not available to us. Therefore, we
use the Kendall?s tau coefficient (Kendall, 1938;
Joachims, 2002) to measure the degree of correla-
tion between two rankings. For simplicity, let?s as-
sume strict orderings of any given ranking. There-
fore we ignore all the pairs with ties (instances
with the identical click count). Kendall?s tau is
defined as ?(ra, rb) = (P ? Q)/(P + Q), where
P is the number of concordant pairs and Q is the
number of disconcordant pairs in the given order-
ings ra and rb. The value is a real number within
[?1,+1], where ?1 indicates a complete inver-
sion, and +1 stands for perfect agreement, and a
value of zero indicates no correlation.
Existing ranking techniques heavily depend on
human relevance judgment that is very costly to
obtain. Similar to Dou et al(2008), our method
utilizes the automatically aggregated click count in
query logs as the gold for deriving the true order
of relevancy, but we use the clickthrough of dif-
ferent languages. We average Kendall?s tau values
between the algorithm output and the gold based
on click frequency for all test queries.
5.2 Data Sets
Query logs can be the basis for constructing high
quality ranking corpus. Due to the proprietary
issue of log, no public ranking corpus based on
real-world search engine log is currently avail-
able. Moreover, to build a predictable bilingual
ranking corpus, the logs of different languages are
needed and have to meet certain conditions: (1)
they should be sufficiently large so that a good
number of bilingual query pairs could be identi-
1079
Table 3: Statistics on AOL and Sogou query logs.
AOL(EN) Sogou(CH)
# sessions 657,426 5,131,000
# unique queries 10,154,743 3,117,902
# clicked queries 4,811,650 3,117,590
# clicked URLs 1,632,788 8,627,174
time span 2006/03-05 2006/08
size 2.12GB 1.56GB
fied; (2) for the identified query pairs, there should
be sufficient statistics of associated clickthrough
data; (3) The click frequency should be well dis-
tributed at both sides so that the preference order
between bilingual document pairs can be derived
for SVM learning.
For these reasons, we use two independent and
publicly accessible query logs to construct our
bilingual ranking corpus: English AOL log3 and
Chinese Sogou log4. Table 3 shows some statis-
tics of these two large query logs.
We automatically identify 10,544 bilingual
query pairs from the two logs using the Java
API for Google Translate5, in which each query
has certain number of clicked URLs. To bet-
ter control the bilingual equivalency of queries,
we make sure the bilingual queries in each of
these pairs are bi-directional translations. Then
we download all their clicked pages, which re-
sults in 70,180 English6 and 111,197 Chinese doc-
uments. These documents form two independent
collections, which are indexed separately for re-
trieval and feature calculation.
For good quality, it is necessary to have suffi-
cient clickthrough data for each query. So we fur-
ther identify 1,084 out of 10,544 bilingual query
pairs, in which each query has at least 10 clicked
and downloadable documents. This smaller col-
lection is used for learning our model, which con-
tains 21,711 English and 28,578 Chinese docu-
ments7. In order to compute cross-lingual doc-
ument similarities based on machine translation
3http://gregsadetsky.com/aol-data/
4http://www.sogou.com/labs/dl/q.html
5http://code.google.com/p/
google-api-translate-java/
6AOL log only records the domain portion of the clicked
URLs, which misleads document downloading. We use the
?search within site or domain? function of a major search en-
gine to approximate the real clicked URLs by keeping the
first returned result for each query.
7Because Sogou log has a lot more clicked URLs, for bal-
ancing with the number of English pages, we kept at most 50
pages per Chinese query.
Table 4: Kendall?s tau values of English ranking.
The significant improvements over baseline (99%
confidence) are bolded with the p-values given in
parenthesis. * indicates significant improvement
over IR (no similarity). n = 5.
Models Pair H-1 (max) H-2 (mean)
RSVM (baseline) n/a 0.2424 0.2424
IR (no similarity) 0.2783 0.2445 0.2445
IR+DIC 0.2909 0.2453 0.2496
IR+MT 0.2858
0.2488* 0.2494*
(p=0.0003) (p=0.0004)
IR+DIC+MT 0.2901 0.2481
0.2514*
(p=0.0009)
IR+DIC+RATIO 0.2946 0.2466
0.2519*
(p=0.0004)
IR+DIC+MT
+RATIO
0.2940
0.2473* 0.2539*
(p=0.0009) (p=1.5e-5)
IR+DIC+MT
+RATIO+URL
0.2979
0.2533* 0.2577*
(p=2.2e-5) (p=4.4e-7)
(see Section 4.2), we automatically translate all
these 50,298 documents using Google Translate,
i.e., English to Chinese and vice versa. Then the
bilingual document pairs are constructed, and all
the monolingual features and cross-lingual simi-
larities are computed (see Section 4.1&4.2).
5.3 English Ranking Performance
Here we examine the ranking performance of our
English ranker under different similarity settings.
We use traditional RSVM (Herbrich et al, 2000;
Joachims, 2002) without any bilingual considera-
tion as the baseline, which uses only English IR
features. We conduct this experiment using all the
1,084 bilingual query pairs with 4-fold cross vali-
dation (each fold with 271 query pairs). The num-
ber of constraint documents n is empirically set as
5. The results are shown in Table 4.
Clearly, bilingual constraints are helpful to
improve English ranking. Our pairwise set-
tings unanimously outperforms the RSVM base-
line. The paired two-tailed t-test (Smucker et
al., 2007) shows that most improvements resulted
from heuristic H-2 (mean score) are statistically
significant at 99% confidence level (p<0.01). Rel-
atively fewer significant improvements can be
made by heuristic H-1 (max score). This is be-
cause the maximum score on pair is just a rough
approximation to the optimal document score. But
this simple scheme works surprisingly well and
still consistently outperforms the baseline.
Note that our bilingual model with only IR fea-
tures, i.e., IR (no similarity), also outperforms the
baseline. The reason is that in this setting there are
1080
1 2 3 4 5 6 7 8 9 100.23
0.235
0.24
0.245
0.25
0.255
0.26
# of constraint documents in a different language
Ke
nda
ll?s 
tau
 
 
RSVM (baseline)
IR+DIC
IR+MT
IR+DIC+MT
IR+DIC+RAIO+MT
IR+DIC+RAIO+MT+URL
Figure 2: English ranking results vary with the
number of constraint Chinese documents.
IR features of n Chinese documents introduced in
addition to the IR features of English documents
in the baseline.
The DIC similarity does not work as effectively
as MT. This may be due to the limitation of bilin-
gual dictionary alone for translating documents,
where the issues like out-of-vocabulary words and
translation ambiguity are common but can be bet-
ter dealt with by MT. When DIC is combined with
RATIO, which considers both forward and back-
ward translation of words, it can capture the corre-
lation between bilingually very similar pages, thus
performs better.
We find that the URL similarity, although sim-
ple, is very useful and improves 1.5?2.4% of
Kendall?s tau value than not using it. This is be-
cause the URLs of the top Chinese (constraint)
documents are often similar to many of returned
English URLs which are generally more regu-
lar. For example, in query pair (Toyota Camry,
TA Model for Processing Temporal References in Chinese
Wenjie Li, Kam-Fai Wong
Department of Systems Engineering
and Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
fwjli, kfwongg@se.cuhk.edu.hk
Chunfa Yuan
Department of Computer Science
and Technology
Tsinghua University,
Beijing, 100084, P.R. China
ycf@s1000e.cs.tsinghua.edu.cn
Abstract
Conventional information systems can-
not cater for temporal information eec-
tively. For this reason, it is useful to cap-
ture and maintain the temporal knowl-
edge (especially the relative knowledge)
associated to each action in an informa-
tion system. In this paper, we propose a
model to mine and organize temporal re-
lations embedded in Chinese sentences.
Three kinds of event expressions are ac-
counted for, i.e. single event, multiple
events and declared event(s). Experi-
ments are conducted to evaluate the min-
ing algorithm using a set of news reports
and the results are signicant. Error
analysis has also been performed open-
ing up new doors for future research.
1 Introduction
Information Extraction (IE) is an upcoming chal-
lenging research area to cope with the increas-
ing volume of unwieldy distributed information re-
sources, such as information over WWW. Among
them, temporal information is regarded as an
equally, if not more, important piece of infor-
mation in domains where the task of extract-
ing and tracking information over time occurs
frequently, such as planning, scheduling and
question-answering. It may be as simple as an ex-
plicit or direct expression in a written language,
such as \the company closed down in May, 1997";
or it may be left implicit, to be recovered by read-
ers from the surrounding texts. For example, one
may know the fact that \the company has closed
down before the earthquake", yet without know-
ing the exact time of the bankruptcy. Relative
temporal knowledge such as this where the pre-
cise time is unavailable is typically determined by
human. An information system which does not
account for this properly is thus rather restrictive.
It is hard to separate temporal information (in
particular refers to temporal relations in this pa-
per) discovery from natural language processing.
In English, tenses and aspects reected by dier-
ent verb forms are important elements in a sen-
tence for expressing temporal reference (Steed-
man, 97) and for transforming situations into
temporal logic operators (Bruce, 72). The pio-
neer work of Reichenbach (Reichenbach, 47) on
tenses forms the basis of many subsequent re-
search eorts in temporal natural language pro-
cessing, e.g. the work of Prior in tense logic (Prior,
67), and of Hwang et alin tense tree (Hwang
92) and temporal adverbial analysis (Hwang 94),
etc. Reichenbach argued that the tense system
provided predication over three underlying times,
namely S (speech time), R (reference time), and
E (event time). Later, a multiple temporal refer-
ences model was introduced by Bruce (Bruce, 72).
He dened the set (S
1
; S
2
; :::; S
n
), which is an el-
ement of a tense. S
1
corresponds to the time of
speech. Each S
i
(i = 2; :::; n  1) is a time of ref-
erence, and S
n
, the time of an event. To facilitate
logic manipulation, Bruce proposed seven rst or-
der logic relations based on time intervals and a
method to map nine English tenses into tempo-
ral rst order logic expressions
1
. His work laid
down the foundation of temporal logic in natu-
ral language. These relations were then gradually
expanded to nine in (Allen, 81) and further to
thirteen in (Allen, 83)
2
.
In contrast, Chinese verbs appear in only one
1
The seven relations are symbolized as R(A;B) for
relation R and time intervals A and B, where R in-
cludes before, after, during, contains, same-time, over-
laps or overlapped-by.
2
meet, met-by, starts, started-by, nishes and
nished-by are added into temporal relations.
form. The lack of regular morphological tense
markers renders Chinese temporal expressions
complicated. For quite a long time, linguists ar-
gued whether tenses existed in Chinese; and if
they did how are they expressed. We believe that
Chinese do have tenses. But they are determined
with the assistance of temporal adverbs and aspect
auxiliary words. For example, ? ...  (being), .
? ... ? (was/been) and  ... (will be) express an
ongoing action, a situation started or nished in
the past, and a situation which will occur in the
future, respectively. Therefore, the conventional
theory to determine temporal information based
on verb axations is inapplicable. Over the past
years, there has been considerable progress in the
areas of information extraction and temporal logic
in English (Antony, 87; Bruce, 72; Kaufmann, 97).
Nevertheless, only a few researchers have investi-
gated these areas in Chinese.
The objective of our research is to design and
develop a temporal information extraction sys-
tem. For practical and cultural reason, the appli-
cation target is on-line nancial news in Chinese.
The nal system, referred to as TICS (Tempo-
ral Information-extraction from Chinese Sources),
will accept a series of Chinese nancial texts as in-
put, analyze each sentence one by one to extract
the desirable temporal information, represent each
piece of information in a concept frame, link all
frames together in chronological order based on
inter- or intra-event relations, and nally apply
this linked knowledge to fulll users' queries.
In this paper, we introduce a fundamental
model of TICS, which is designed to mine and
organize temporal relations embedded in Chinese
sentences. Three kinds of event expressions are
accounted for, i.e. single event, multiple events
and declared event(s). This work involved four
major parts, (1) built temporal model; (2) con-
structed rules sets; (3) developed the algorithm;
and (4) set up the experiments and performed the
evaluation.
2 A Model for Temporal Relation
Discovery
2.1 Temporal Concept Frame
In IE, it is impossible as well as impractical to
extract all the information from an incoming doc-
ument. For this reason, all IE systems are geared
for specic application domains. The domain is
determined by a pre-specied concept dictionary.
Then a certain concept is triggered by several lex-
ical items and activated in the specic linguistic
contexts. Each concept denition contains a set
of slots for the extracted information. In addition,
it contains a set of enabling conditions which are
constraints that must be satised in order for the
concept to be activated. Due to its versatility, a
frame structure is generally used to represent con-
cepts (as shown in Figure 1).
Slots in a temporal concept frame are divided
into two types: activity-related and time-related.
Activity-related slots provide the descriptions of
objects and actions concerning the concept. For
example, company predator, company target and
purchase value are the attributes of the concept
(B?, TAKEOVER). Meanwhile, time-related
slots provide information related to when a con-
cept begins or nishes, how long does it last and
how does it relate to another concept, etc.
referenced concept frame
......
(temporal relations
among activities)
{AND, OR}
Absolute relation
relation 1
relation 
time
time
relation n reference
relative relation
relation 1 reference
Delare frame
speaksman
location
reliability
absolute relation
relative relation
source
absolute relation
publish frame
n
......
......
......
......
Temporal slots
absolute relation
declare
duration
reliability
Activity slots
company
Concept frame
......
concept type
publish
company frame
company name
employees no.
turnover
parent company
company name
employees no.
turnover
parent company
company frame
among entities)(static relations
relative relation
Figure 1: Temporal concept frame construction
2.2 Temporal Relations
The system is designed with two sets of temporal
relations, namely absolute and relative relations.
The role of absolute temporal relations is to posi-
tion situation occurrences on a time axis. These
relations depict the beginning and/or ending time
bounds of an occurrence or its relevance to refer-
ence times, see TR(T ) in Section 2.3. Absolute
relations are organized by Time Line in the sys-
tem, see Figure 2.
r:                                   t:                                d:absolute relation      time parameter        duration
13/6/99
Time_Line
17/6/99 21/6/99 3/7/9929/6/99
New World demands
the payment of 10-
million-dollar debt
from CTI
Nan Hua takes over
Si Hai Travel with
1st Pacific takes over
from Lin Shao Liang
40% Yin Duo Fu?s stock
Jing Kuang sells DC
holdings to Dong Fong
Hong
Ba Ling buys 30%
Lian
stock from Zhong
o o ooo
Event B Time specification Event C Time specificationTime specification
t r r t dtr
25/6/99
23 million dollar
Event A
d d ......
Figure 2: The T ime Line organization for abso-
lute relations in TICS
In many cases, the time when an event takes
place may not be known. But its relevance to
another occurrence time is given. Relative tem-
poral knowledge such as this is manifested by rel-
ative relations. Allen has proposed thirteen re-
lations. The same is adopted in our system, see
TR(E
i
; E
j
) in Section 2.3. The relative relations
are derived either directly from a sentence describ-
ing two situations, or indirectly from the absolute
relations of two individual situations. They are or-
ganized by Relational Chains, as shown in Figure
3.
o                                       o
before
o                                       o
before
R d R d R d
R d R d R d
same_as
o o
o
contains
before
before
Event A
Si Tong parent
stock
Event CEvent A
Hua Ruen places
company reorgnizes
Event B
decreases by 1.3%
The price of Hua Ruen
Event D
Event E Event F
CKI acquires 20% of 
Envestra Limited from Richard Li for $3.8 billion
Event G
Ba Ling buye 30%
stock from Zhong Lian
R d
Event B Event C Event D
Event B Event E Event F
R d R d
Event C Event G
Si Tong parent
company goes public 
Tricom purchases 60% of PCC
R:                                     d:relative relation       duration
Figure 3: The Relational Chain organization for
relative relations in TICS
2.3 Temporal Model
This section describes our temporal model for
discovering relations from Chinese sentences.
Suppose TR indicates a temporal relation, E
indicates an event and T indicates time. The
absolute and relative relations are symbolized as:
OCCUR(E
i
; TR(T ))
3
and TR(E
i
; E
j
), respec-
tively. The sets of TR are:
TR(T ) = fON; BEGIN; END; PAST ,
FUTUER; ONGOING; CONTINUEDg
TR(E
i
; E
j
) = fBEFORE; AFTER; MEETS,
METBY; OV ERLAPS; OV ERLAPPED,
DURING; CONTAINS; STAREDBY ,
STARTS; FINISHES; FINISHEDBY ,
SAME ASg
For an absolute relation of a single event, T is
an indispensable parameter, which includes event
time t
e
, reference time t
r
4
and speech time t
s
:
3
OCCUR is a predicate for the happening of a sin-
gle event. Under the situations where there are no am-
biguity, E
i
can be omitted. The OCCUR(E
i
; TR(T )
is simplied as TR(T ).
4
There maybe exist more than one reference time
in a statement.
T = ft
e
; t
r
; t
s
g
Some Chinese words can function as the tem-
poral indicators. These include time word (TW ),
time position word (F ), temporal adverb (ADV ),
auxiliary word (AUX), preposition word (P ),
auxiliary verb (V A), trend verb (V C) and some
special verbs (V V ). They are all regarded as the
elements of the temporal indicator TI :
TI = fTW; F; ADV; AUX; V A; V C; V V; Pg
Each type of the indicators, e.g. TW , con-
tains a set of words, such as TW = twlist =
ftw
1
; tw
2
; :::tw
n
g, with each word having an tem-
poral attribute, indicated by ATT .
The core of the model is thus a rule set R
which maps the combinational eects of all the
indicators, TI , in a sentence to its corresponding
temporal relation, TR,
R : TI !
Improving Document Clustering by Utilizing Meta-Data* 
Kam-Fai Wong 
Department of Systems
Engineering and 
Engineering Management,
The Chinese University of 
Hong Kong
kfwong@se.cuhk.edu.hk
Nam-Kiu Chan
Centre for Innovation and 
Technology,
The Chinese University of 
Hong Kong
jussie@cintec.cuhk.edu.hk
Kam-Lai Wong 
Centre for Innovation and 
Technology,
The Chinese University of
Hong Kong
klwong@cintec.cuhk.edu.hk
Abstract
In this paper, we examine how to im-
prove the precision and recall of docu-
ment clustering by utilizing meta-data.
We use meta-data through NewsML tags 
to assist clustering and show that this ap-
proach is effective through experiments 
on sample news data. Experimental result 
shows that clustering using NewsML 
could improve average recall and preci-
sion over the same without using
NewsML by about 10%. Our algorithm 
facilitates effective e-business for the
news media and publishing industry to
empower e-business.
1
2
2.1
Introduction
Nowadays, people have great demand on
knowledge and information, while information
overload becoming one serious problem. News
media and publishing industry therefore try to suit 
customers? need by using electronic information
management system. Document clustering algo-
rithm has been introduced to group similar docu-
ments together for easier searching and reading.
Document clustering algorithm has been 
widely used in news media and publishing indus-
try, which ensured it effectiveness over manual 
clustering. With labor cost reduced and time saved,
document clustering algorithm provides conven-
ient clustered-news for users. 
To improve the accuracy of document
clustering algorithm, we suggest to provide more
flexible information for each document. Under the
hypothesis that document clustering algorithm can
get better result with more information about data 
used, we suggest that using additional meta-data 
contained in NewsML standard could enhance the 
performance of document clustering algorithms.
We evaluated the effectiveness of using 
meta-data in the proposed clustering algorithm.
We used Chinese electronic news sources for the
evaluation. The experiment showed that using the
meta-data provided by NewsML achieved better 
document clustering.
The remaining of the paper is organized as fol-
lows: In Section 2, we give an overview of current 
document clustering approaches. Section 3, analy-
ses existing problems in document clustering and 
suggests a solution using NewsML. We show the
tags that could be used in the algorithm and how 
they are handled. The performance of algorithm
through experiments, and we will present the ex-
perimental results regarding measures in precision 
and recall. In Section 5, we include a brief sum-
mary and discussion on future work.
Current Approaches
Different document clustering methods have 
been examined. These conventional clustering
methods mainly consist of two parts: construction 
of a similarity matrix between documents and
formulation of clustering algorithm to generate 
clusters.
Similarity Matrix 
The first step of conventional clustering
method is to construct a similarity matrix between
these documents so as to understand how docu-
ments are similar to one another. The constructed
similarity matrix will later be used by the cluster-
ing algorithm for generating clusters. 
* Corresponding Author: Kam-Lai Wong (klwong@cintec.cuhk.edu.hk)
Named entity method (Volk and Clematide, 
2001) is one of the widely used approaches for 
constructing a similarity matrix. Named Entities 
form the major components in a document. When 
fundamental entities like Person, Company and 
Geographical names are detected, the algorithm 
could understand the content of a document to a 
certain extent. 
Named entity method has also been investi-
gated together with keywords to perform cluster-
ing  (Lam, Meng, Wong and Yen, 2001). The 
similarity score is calculated based on the named 
entity vectors and the keyword vector with 
weighting parameters to control the degree of em-
phasis on the corresponding vectors. 
Concept terms method (Wong, Lam and Yen, 
1999) has been proposed in order to deal with the 
problem of vocabulary switching. The potentially 
concept terms are basically the keywords derived 
from a separated concept generation corpus. Con-
cept terms are selected based on the co-occurrence 
between a query and a document. 
However, named entities approach and con-
cept terms approach contain some limitations: The 
accuracy of the clustering algorithm would be di-
rectly proportional to the accuracy of algorithm. 
Thus, any error from identifications of named en-
tities or concept terms will adversely affect the 
quality of the clustering algorithm as well. 
N-gram Algorithm (Lee, Cho and Park, 1999) 
has been introduced in order to avoid the afore-
mentioned limitations. An N-gram is a character 
sequence of length N extracted from a document. 
The main idea of the N-gram approach is that the 
character structure of a term can be used to find 
semantically similar terms. The approach assumes 
no prior linguistic knowledge about the text being 
processed. Moreover, there is no language-
specific information used in the N-grams ap-
proach, which qualifies this method as a language-
independent approach. By using N-grams, fre-
quently appeared terms of each document can be 
extracted and compared to make the similarity 
measure. 
2.2 Clustering Algorithm 
Probabilistic method is one of the com-
monly used methods in document clustering. The 
aim of probabilistic method is to minimize the 
heterogeneity in each group with respect to the 
group representative based on statistical ap-
proaches (Estivill-Castro and Yang, 2001). Neural 
network is also used to perform a cyclic learning 
process for clustering (Grothkopf, Andernach and 
Stevens-Rayburn, 1998).  
Hierarchical clustering methods include 
group-average clustering algorithm and single-
link clustering algorithm (Johnson and Kargupta, 
2000; Tombros, Villa and Van Rijsbergen, 2002). 
Group average clustering is based on creating a 
hierarchical tree by initially creating a singleton 
cluster for each document. The clusters are 
merged to the parent node until the algorithm goal 
is achieved. The algorithm merges document pairs 
in the resulting clusters by merging clusters in a 
greedy, bottom-up fashion. A divide-and-conquer 
strategy can be used to balance the cluster quality 
and computational efficiency. 
The basic steps of group-average clustering 
algorithm are like this: On each iteration, it first 
divides the current pool of clusters into evenly 
sized buckets. Group-average clustering is then 
applied to each bucket locally, merging smaller 
clusters into larger ones. The time complexity for 
the algorithm is O(kn), where k is bucket size and 
n is the number of documents. 
Single link clustering (Dunlop, 2000), on the 
other hand, is based on creating a hierarchical tree 
by continually inserting an additional node that 
satisfies the following criteria: 
- The new node is currently outside the hierar-
chy 
- Of all similarities between nodes inside and 
outside the hierarchy, the new node which has 
the strongest similarity is selected. It is then 
added to the hierarchy at a level based on how 
strong the similarity is. 
The approach is fairly fast and result in hier-
archies where the closest nearest neighbours are at 
lower levels of the hierarchy. However, it leads to 
non-balanced clusters, and many node-node com-
parisons can have the same strength of similarity 
thus many documents can be linked at the same 
level in the hierarchy. 
The accuracy of the above conventional clus-
tering method, however, is generally low. There-
fore a new approach is proposed. 
3 Our Proposed Method 
In our proposal, we suggest the following ap-
proaches for generating clusters. 
1. Use Bi-gram to extract terms from
documents
2. Use <KeywordLine> Tag to look up
keyword terms for documents
3. Compare terms betweens documents to
construct similarity matrix
4. Use <SubjectCode> Tag to group docu-
ments to different subjects 
5. Adjust similarity matrix by data pro-
vided by step 4
6. Apply group-average clustering algo-
rithm to generate clusters 
We have chosen to use Bi-gram algorithm to
extract terms from documents. The idea of bi-
gram Algorithm is similar to N-grams?. The rea-
son of using bi-gram instead of N-gram is that our
experiment mainly deals with Chinese (Big5)
news. Since Chinese terms are typically formed
by two Chinese characters, bi-gram approach is 
sufficient for this application. Using bi-gram,
moreover, would be a more effective approach 
when handling other two-byte code like Japanese 
and Korean languages (Lee, Cho and Park, 1999).
Using bi-grams instead of N-grams can re-
duce system resources. Since hundreds of docu-
ments are handled each time, using N-gram would 
be impractical. For instance, using N-grams ap-
proach for a document with M Chinese characters
would extract (M-1) + (M-2)+?+[M- (N-1)]
terms. This would require more time for compari-
son than only M-1 terms for bi-grams. 
Figure 1: A sample document in NewsML format containing keywords and subject labels
We suggest using NewsML 
(http://www.newsml.org) in our project. NewsML,
which is released by International Press and Tele-
communications Council (IPTC) (www.iptc.org ),
is an XML-based data format for news that is in-
tended to use for the creation, transfer and deliv-
ery of news. All news is created based on 
NewsML data type definition (DTD) file. 
NewsML has been widely recognized globally.
Noticeable users include Reuters, AFP, and Kyo-
donews.
A sample document in NewsML format is 
shown in Figure 1. By using NewsML, a helpful 
tag called <KeywordLine> can be used in order to
look up existing keywords from a document. The 
keywords, which could highly reflect the main
concept of the document, would usually be given
by the author of the document and stored in the 
<KeywordLine> tag under NewsML. Thus, the 
<KeywordLine> Tag is a useful indicator for
keyword extraction.
We used NewsML tagged keywords for 
comparison. A score is computed for each key-
word based on term frequency to reflect the level 
of importance to the document. A given keyword,
however, may exist in more than two Chinese 
characters. But in order to make easy and accurate
comparison, we extracted the keywords in two 
Chinese characters each and compare them with 
the terms extracted by the bi-gram algorithm.
These keywords would be more representative
than those extracted by bi-gram. Thus, they would 
be very useful for identifying the news content. 
We therefore applied higher weighting to these 
terms. In view of this, we assign the weighting of 
NewsML tagged keywords ten times more than
those extracted using bi-grams.
Besides using only the keywords and terms 
from a news document, subject of the news is also
crucial. For example, news referring to entertain-
ment should not be in the same cluster with one 
talking about business news. Understanding news
semantically to a certain extent can help improve
clustering accuracy. <SubjectCode> Tag in 
NewsML enabled the clustering algorithm to dis-
tinguish which subject a particular news is refer-
ring to. Although the <SubjectCode> only gives a 
rough concept of the news, it can significantly
improve accuracy.
In our experiment, we ensured that news with
different <SubjectCode> would not be put in the 
same cluster. The similarity measure of two pieces
of news with different <SubjectCode> would, 
therefore, be zero. 
4 Experimental Results
We assessed the effectiveness of using meta-
data for document clustering empirically. We used 
NewsML meta-data and Hong Kong Chinese
news articles for this purpose. 
Performance metrics are based on Recall and 
Precision, which are defined as follows: 
In event Not in event 
In cluster A B
Not in cluster C D
Recall = A / (A+C) if A+C > 0 
Precision = A / (A+B) if A+B > 0 
To study the effect of NewsML, we calcu-
lated the percentage of recall and precision under
different threshold values. The threshold value (0-
1) is a user-defined variable for clustering. The 
threshold is the value where documents within a 
cluster should have a similarity greater than it, and 
similarity between two documents is calculated by
Jaccard?s coefficient: 
cba
a
www
w

where
a is the set of terms present in both docu-
ments
b is the set of terms present in document 1 
but absent in document 2 
c is the set of terms present in document 2 
but absent in document 1 
cba www ,,  are the sum of weights of the
related set. 
A higher threshold represent higher similarity
is needed for documents to be put in the same
cluster.
Prior to the experiment, we matched each
piece of news with specific Subject Codes defined
by IPTC (http://www.iptc.org/site/subject-
codes/index.html), as well as adding keywords
with respect to their contents. After the prepara-
tion process, we started our experiment and find 
out the recall and precision measures as above 
stated
The main difference between the algorithms
with and without NewsML is on the use of the 
KeywordLine and SubjectCode tags. The evalua-
tion steps are as follows: 
1. Non-overlapping clusters are generated by
our clustering system
2. Each generated cluster is matched to the most
similar sample event using a one-to-one 
matching method.
3. If the number of clusters generated is less 
than the number of events in the data set, dis-
card the excessive sample events after match-
ing.
4. If the number of clusters generated is greater 
than the number of events in the data set, 
empty events are added to the sample and
they are matched with the generated clusters. 
5. Calculate recall and precision for each cluster 
pairs
6. Use the calculated recall and precision, to
obtain the macro-average (Yang, Pierce and
Carbonell, 1998).
7. Repeat the above steps on different threshold 
values.
Clustering Evaluation Result
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
70.00%
80.00%
90.00%
100.00%
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Recall With
NewsML
Recall
Without
NewsML
Precision
With
NewsML
Precision
Without
NewsML
Figure 2: Clustering evaluation result for 158 news articles from three different publications
4.1 4.2Experiment A: News Articles from Mul-
tiple Publications
We began our evaluation with a set of data 
from three different publications. We used 158
news articles containing 31 distinct events (which
was equivalent to 31 non-overlapping clusters) as 
our training set. The events belonged to different 
sector including local news, international news,
sports news, entertainment news and business 
news. Based on the IPTC Subject Reference Sys-
tem (www.iptc.org), these news articles were clas-
sified into 7 different subjects. 
The result is shown in Figure 2 where x-axis 
represents the threshold and y-axis represents the
percentage of the measure.
The graph suggests that both recall and preci-
sion values decrease with increasing threshold. 
The performance of using NewsML is better when
the threshold is smaller. The greatest difference
occurs when threshold value = 0.2 
Moreover, we observe that the variation of 
the performance is greater when using NewsML.
The precision and recall can drop about 40% by
changing the threshold. In the contrast, without 
using NewsML, the result is more steady. 
On average, recall and precision NewsML-
based algorithm gives 16.15% and 12.09% respec-
tively improvement of its non-NewsML counter-
part.
Experiment B: News Articles from the
Same Publication 
We then carried out another experiment us-
ing 503 news articles from one single publication.
We tried to classify these news articles manually
to events of related news, and then carried out the 
same approach as the first experiment. These 503 
news articles contained 126 distinct events and 
classified to 12 different subjects with reference to
the IPTC Subject Reference System.
We find out how the system performs when
dealing with one single publication. As we predict, 
news from only one single publication often uses 
same keywords or terms to express the same con-
cept. This simplified clustering. 
The result is shown in Figure 3 where x-axis 
represents the threshold and y-axis represents the
percentage of the measure.
The performance of the system decrease
when the threshold value increases, which is con-
sistent with the first experiment. From the graph, 
we observe that the performance is best when the
threshold is between 0.2 and 0.4, which means
those values could best match human expectation.
The graph also suggests that both recall and 
precision gives a better result towards using meta-
data. On average 8.02% (recall) and 8.92% (preci-
sion) improvement over the approach without us-
ing NewsML meta-data are achieved.
????????????????????????????
?????
??????
??????
??????
??????
??????
??????
??????
??????
??????
???????
??? ??? ??? ??? ??? ??? ??? ??? ???
???????????????
???????????
??????
??????
???????
??????
?????????
????
??????
?????????
???????
??????
5 Conclusions 
From the results of the two experiments, we 
have found that using NewsML could improve
both the recall and precision of the document clus-
tering algorithm by about 10%, over those without 
using NewsML.
In this paper, we demonstrate that the effec-
tiveness of document clustering algorithm could
be improved by utilizing meta-data in additional 
to the original data content. In our experiment, we 
chose NewsML as a representation of news con-
tent with added meta-data. We proposed to use the 
<KeywordLine> tags and <SubjectCode> tags in
NewsML for clustering.
Our proposed document clustering algorithm
is a refinement of conventional group-average
clustering algorithm and bi-gram algorithm. It has 
been shown that NewsML could help conven-
tional clustering methods improve both the recall
and precision by about 10% on average.
In order to demonstrate the practicality of
NewsML for e-business, we have deployed
NewsML in developing an application called
NewsFocus1. News Focus consists of a clustering 
function. The function is mainly for clustering
similar news from three different news sources in
Hong Kong. News articles under NewsFocus are 
clustered by news events. 
In the future, we try to use different methods
to further improve the clustering performance. 
Inverted document frequency and cosine-angle
formula, for example, have been widely used in 
terms score calculation and matrix similarity cal-
culation. Top relevance terms can be used as key-
words in case of any insufficiency of metadata.
We will also try to use other tags in NewsML like 
<Headline> in supplement with <KeywordLine> 
and <SubjectCode> to give more metadata infor-
mation for the document. Weighting parameters
may also be applied to show the degree of empha-
sis on using those metadata. 
Figure 3: Clustering evaluation result for 503 news articles from the same publication 
References
M. D. Dunlop, Development and evaluation of cluste-
ing techniques for finding people, Proceedings of the
Third International Conference Basel, Volume 34,
2000
V. Estivill-Castro and J. Yang, Non-crisp Clustering by
Fast, Convergent, and Robust Algorithms, Principles
of Data Mining and Knowledge Discovery, Volume
2168, 2001, pp. 103-114
U. Grothkopf, H. Andernach, S. Stevens-Rayburn, and
M. Gomez, Comparison of  Two ?Document Similarity
Search Engines?, Library and Information Services in
Astronomy III, ASP Conference Series, Volume 153,
1998, pp. 85-92
E. L. Johnson and H. Kargupta, Collective, Hierarchi-
cal Clustering from Distributed, Heterogeneous Data,
Large-Scale Parallel Data Mining, Lecture Notes in
Artificial Intelligence, Volume 1759, 2000, pp. 221-
244
Lam, W., Meng, H., Wong, K.L. and Yen, J., Using
Contextual Analysis for News Event Detection,
International Journal of Intelligent Systems, 16(4),
2001, pp.525-546
1 Please visit http://www.cnewsml.org/clustering/jsp/index2.html for demonstration
J. H. Lee, H. Y. Cho and H. R. Park, N-gram-based 
indexing for Korean text retrieval, Information Proc-
essing and Management, Volume 35 Number 4, 1999, 
pp. 427-441 
A. Tombros, R. Villa, C. J. Van Rijsbergen, The  effec-
tiveness of query-specific hierarchic clustering in in-
formation retrieval, Information processing & 
management, Volume 38, 2002 , pp. 559-582 
M. Volk and S. Clematide: Learn-Filter-Apply-Forget. 
Mixed Approaches to Named Entity Recognition.  Pro-
ceedings of 6th International Workshop on Applica-
tions of Natural Language for Information Systems. 
GI-Edition. Lecture Notes in Informatics. vol. 3. Ma-
drid: 2001.  
K.L. Wong, W. Lam, J. Yen, Interactive Chinese News 
Event Detection and Tracking, Proceedings of The 
Second Asian Digital Library Conference, 1999, 
pp.30-43. 
Y. Yang, T. Pierce, J. Carbonell, A Study on Retrospec-
tive and On-Line Event Detection, Proceedings of 
SIGIR-98, 21st ACM International Conference on Re-
search and Development in Information Retrieval, 
1998, pp.28-36. 
Anomaly Detecting within Dynamic Chinese Chat Text 
 
Yunqing Xia 
Department of S.E.E.M. 
The Chinese University of Hong Kong 
Shatin, Hong Kong 
yqxia@se.cuhk.edu.hk 
Kam-Fai Wong 
Department of S.E.E.M. 
The Chinese University of Hong Kong 
Shatin, Hong Kong 
kfwong@se.cuhk.edu.hk 
 
 
Abstract 
The problem in processing Chinese chat 
text originates from the anomalous char-
acteristics and dynamic nature of such a 
text genre. That is, it uses ill-edited terms 
and anomalous writing styles in chat text, 
and the anomaly is created and discarded 
very quickly. To handle this problem, 
one solution is to re-train the recognizer 
periodically. This costs a lot of man-
power in producing the timely chat text 
corpus. The new approaches are pro-
posed in this paper to detect the anomaly 
within dynamic Chinese chat text by in-
corporating standard Chinese corpora and 
chat corpus. We first model standard lan-
guage text using standard Chinese cor-
pora and apply these models to detect 
anomalous chat text. To improve detec-
tion quality, we construct anomalous chat 
language model using one static chat text 
corpus and incorporate this model into 
the standard language models. Our ap-
proaches calculate confidence and en-
tropy for the input text and apply thresh-
old values to help make the decisions. 
The experiments prove that performance 
equivalent to the best ones produced by 
the approaches in existence can be 
achieved stably with our approaches.  
1 Introduction 
Network Informal Language (NIL) refers to the 
special human language widely used in the 
community of network communication via plat-
forms such as chat rooms/tools, mobile phone 
short message services (SMS), bulletin board 
systems (BBS), emails, blogs, etc. NIL is ubiqui-
tous due in special to the rapid proliferation of 
Internet applications. As one important type of 
NIL text, chat text appears frequently within in-
creasing volume of chat logs of online education 
(Heard-White, 2004) and customer relationship 
management (Gianforte, 2003) via chat 
rooms/tools. In wed-based chat rooms and BBS a 
large volume of NIL text is abused by (McCul-
lagh, 2004). A survey by the Global System for 
Mobile Communication (GSM) showed that 
Germans send 200 million messages a year 
(German News, 2004). All the facts disclose the 
growing importance in processing NIL text.  
Chat text holds anomalous characteristics in 
forming non-alphabetical characters, words, and 
phrases. It uses ill-edited terms and anomalous 
writing styles. Typical examples of anomalous 
Chinese chat terms can be found in (Xia et. al., 
2005a). Besides the anomalous characteristics, 
our observations reveal remarkable dynamic na-
ture of the chat text. The anomaly is created and 
discarded very quickly. Although there is no idea 
how tomorrow?s chat text would look like, the 
changing will never stop. Instead, the changing 
gets faster and faster.  
The challenging issues originates from the dy-
namic nature are two-fold. On the one hand, 
anomalous chat terms and writing styles are fre-
quently found in chat text. Knowledge about chat 
text is urgently required to understand the anom-
aly. On the other hand, the dynamic nature of the 
chat text makes it nearly impossible to maintain a 
timely chat text knowledge base. This claim has 
been proved by (Xia et. al., 2005a) in which ex-
periments are conducted with an SVM classifier. 
The classifier is trained on chat text created in an 
earlier period and tested on chat text created in a 
later period. In their experiments, performance of 
the SVM classifier becomes lower when the two 
periods are farther. This reveals that chat text is 
written in such a style that changes constantly 
along with time. A straightforward solution to 
this problem is to re-train the SVM classifier pe-
riodically with timely chat text collections. Un-
fortunately, this solution costs a lot of manpower 
in producing new chat text corpora. The super-
48
vised learning technique becomes ineffective in 
processing chat text.  
This paper proposes approaches to detecting 
anomaly in dynamic Chinese chat text by incor-
porating standard Chinese corpora and a static 
chat corpus. The idea is basically error-driven. 
That is, we first create standard language models 
using trigram on standard Chinese corpora. 
These corpora provide negative training samples. 
We then construct anomalous chat language 
model using one static chat text corpus which 
provides positive training samples. We incorpo-
rate the chat language model with the standard 
language models and calculate confidence and 
entropy to help make decisions whether input 
text is anomalous chat text. We investigate two 
types of trigram, i.e. word trigram and part-of-
speech (POS) tag trigram in this work.  
The remaining sections of this paper are or-
ganized as follow.  In Section 2, the works re-
lated to this paper are addressed. In Section 3, 
approaches of anomaly detection in dynamic 
Chinese chat text with standard Chinese corpora 
are presented. In Section 4, we incorporate the 
NIL corpus into our approaches. In section 5, 
experiments are described to estimate threshold 
values and to evaluate performance of the two 
approaches with various configurations. Com-
parisons and discussions are also reported. We 
conclude this paper and address future works in 
Section 6.  
2 Related Works 
Some works had been carried out in (Xia et. al., 
2005a) in which an SVM classifier is imple-
mented to recognize anomalous chat text terms. 
A within-domain open test is conducted on chat 
text posted in March 2005. The SVM classifier is 
trained on five training sets which contain chat 
text posted from December 2004 to February 
2005. The experiments show that performance of 
the SVM classifier increases when the training 
period and test period are closer. This reveals 
that chat text is written in a style that changes 
quickly with time. Many anomalous popular chat 
terms in last year are forgotten today and new 
ones replace them. This makes SVM based pat-
tern learning technique ineffective to reflect the 
changes.  
The solution to this problem in (Xia et. al., 
2005b) is to re-train the SVM classifier periodi-
cally. This costs a lot of manpower in producing 
the timely chat text corpora, in which each piece 
of anomalous chat text should be annotated with 
several attributes manually.  
We argue that the anomalous chat text can be 
identified using negative training samples in 
static Chinese corpora. Our proposal is that we 
model the standard natural language using stan-
dard Chinese corpora. We incorporate a static 
chat text corpus to provide positive training sam-
ples to reflect fundamental characteristics of 
anomalous chat text. We then apply the models 
to detect the anomalous chat text by calculating 
confidence and entropy.  
Regarding the approaches proposed in this pa-
per, our arguments are, 1) the approaches can 
achieve performance equivalent to the best ones 
produced by the approaches in existence; and 2) 
the good performance can be achieved stably. 
We prove these arguments in the following sec-
tions.  
3 Anomaly Detection with Standard 
Chinese Corpora 
Chat text exhibits anomalous characteristics in 
using or forming words. We argue that the 
anomalous chat text, which is referred as anom-
aly in this article, can be identified with language 
models constructed on standard Chinese corpora 
with some statistical language modeling (SLM) 
techniques, e.g. trigram model. 
The problem of anomaly detection can be ad-
dressed as follows. Given a piece of anomalous 
chat text, i.e. },...,,{ 21 nwwwW = , and a language 
model )}({ xpLM = , we attempt to recognize W  
as anomaly by the language model. We propose 
two approaches to tackle this problem. We de-
sign a confidence-based approach to calculate 
how likely that W  fits into the language model. 
Another approach is designed based on entropy 
calculation. Entropy method was originally pro-
posed to estimate how good a language model is. 
In our work we apply this method to estimate 
how much the constructed language models are 
able to reflect the corpora properly based on the 
assumption that the corpora are sound and com-
plete.  
Although there exist numerous statistical 
methods to construct a natural language model, 
the objective of them is one: to construct a prob-
abilistic distribution model )(xp  which fits to the 
most extent into the observed language data in 
the corpus. We implement the trigram model and 
create language models with three Chinese cor-
pora, i.e. People?s Daily corpus, Chinese  Giga-
word and Chinese Pen Treebank. We investigate 
49
quality of the language models produced with 
these corpora. 
3.1 The N-gram Language Models 
N-gram model is the most widely used in statisti-
cal language modeling nowadays. Without loss 
of generality we express the probability of a 
word sequence },...,{ 1 nwwW =  of n words, i.e. 
)(Wp  as  
?
=
?==
n
i
iin wwwwpwwpWp
1
1101 ),...,,|(),...,()(  
(1) 
where 0w  is chosen appropriately to handle the 
initial condition. The probability of the next 
word iw  depends on the history ih  of words 
that have been given so far. With this factoriza-
tion the complexity of the model grows exponen-
tially with the length of the history. 
One of the most successful models of the past 
two decades is the trigram model (n=3) where 
only the most recent two words of the history are 
used to condition the probability of the next 
word. 
Instead of using the actual words, one can use 
a set of word classes. Classes based on the POS 
tags, or the morphological analysis of words, or 
the semantic information have been tried. Also, 
automatically derived classes based on some sta-
tistical models of co-occurrence have been tried 
(Brown et. al., 1990). The class model can be 
generally described as  
?
=
??=
n
i
iiiii cccpcwpWp
1
12 ),|()|()(       (2) 
if the classes are non-overlapping. These tri-class 
models have had higher perplexities than the cor-
responding trigram model. However, they have 
led to a reduction in perplexity when linearly 
combined with the trigram model. 
3.2 The Confidence-based Approach 
Given a piece of chat text },...,,{ 21 nwwwW =  
where each word iw  is obtained with a standard 
Chinese word segmentation tool, e.g. ICTCLAS. 
As ICTCLAS is a segmentation tool based on 
standard vocabulary, it means that some un-
known chat terms (e.g., ????) would be broken 
into several element Chinese words (i.e., ??? 
and ??? in the above case). This does not hurt 
the algorithm because we use trigram in this 
method. A chat term may produce some anoma-
lous word trigrams which are evidences for 
anomaly detection.  
We use non-zero probability for each trigram 
in this calculation. This is very simple but na?ve. 
The calculation seeks to produce a so-called con-
fidence, which reflects how much the given text 
fits into the training corpus in arranging its ele-
ment Chinese words. This is enlightened by the 
observation that the chat terms use element  
words in anomalous manners which can not be 
simulated by the training corpus.  
The confidence-based value is defined as  
( ) KK
i i
TCWC
1
1
)( ??
???
?= ?=                  (3) 
where K denotes the number of trigrams in chat 
text W  and iT  is the i-th order trigram. ( )iTC  is 
confidence of trigram iT . Generally ( )iTC  is as-
signed probability of the trigram iT  in training 
corpus, i.e. ( )iTp . When a trigram is missing, 
linear interpolation is applied to estimate its 
probability.  
We empirically setup a confidence threshold 
value to determine whether the input text con-
tains chat terms, namely, it is a piece of chat text. 
The input is concluded to be stand text if its con-
fidence is bigger than the confidence threshold 
value. Otherwise, the input is concluded to be 
chat text. The confidence threshold value can be 
estimated with a training chat text collection.  
3.3 The Entropy-based Approach 
The idea beneath this approach comes from en-
tropy based language modeling. Given a lan-
guage model, one can use the quantity of entropy 
to get an estimation of how good the language 
model (LM) might be. Denote by p the true dis-
tribution, which is unknown to us, of a segment 
of new text x of k words. Then the entropy on a 
per word basis is defined as 
??= ?>? xk xpxpkH )(ln)(1lim              (4) 
If every word in a vocabulary of size |V| is 
equally likely then the entropy would be 
||log 2 V ; ||ln VH ?  for other distributions of 
the words.  
Enlightened by the estimation method, we 
compute the entropy-based value on a per tri-
gram basis for the input chat text. Given a stan-
dard LM denoted by p~  which is modeled  by 
trigram, the entropy-value is calculate as 
50
?
=
?=
K
i
iiK TpTpK
H
1
)(~ln)(~1~             (5) 
where K denotes number of trigrams the input 
text contains. Our goal is to find how much dif-
ference the input text is compared against the 
LM. Obviously, bigger entropy discloses a piece 
of more anomalous chat text. An empirical en-
tropy threshold is again estimated on a training 
chat text collection. The input is concluded to be 
stand text if its entropy is smaller than the en-
tropy threshold value. Otherwise, the input is 
concluded to be chat text. 
4 Incorporating the Chat Text Corpus 
We argue performance of the approaches can be 
improved when an initial static chat text corpus 
is incorporated. The chat text corpus provides 
some basic forms of the anomalous chat text. 
These forms we observe provide valuable heuris-
tics in the trigram models. Within the chat text 
corpus, we only consider the word trigrams and 
POS tag trigrams in which anomalous chat text 
appears. We thus construct two trigram lists. 
Probabilities are produced for each trigram ac-
cording to its occurrence. One chat text example 
EXP1 is given below.  
EXP1: ?????????? 
SEG1: ? ? ?? ? ?? ? ? ? 
SEG1 presents the word segments produced 
by ICTCLAS. We generate chat text word tri-
grams based on SEG1 as follow. 
TRIGRAM1:   (1)/? ? ??/ 
            (2)/? ?? ?/ 
            (3)/?? ? ?/ 
            (4)/? ? ?/ 
For each input trigram iT , if it appears in the 
chat text corpus, we adjust the confidence and 
entropy values by incorporating its probability in 
chat text corpus.  
4.1 The Refined Confidence 
For each ( )iTC , we assign a weight i? , which is 
calculated as  
)()( icin TpTp
i e ?=?                      (6) 
where )( in Tp  is probability of the trigram iT  in 
standard corpus and )( ic Tp  probability in chat 
text corpus. Equation (3) therefore is re-written 
as  
( )
( ) KK
i in
TpTp
KK
i ii
Tpe
TCWC
icin
1
1
)()(
1
1
'
          
)(
??
???
?=
??
???
?=
?
?
=
?
= ?
         (7) 
The intention of inserting i?  into confidence 
calculation is to decrease confidence of input 
chat text when chat text trigrams are found. 
Normally, when a trigram iT  is found in chat text 
trigram lists, )( in Tp  will be much lower than 
)( ic Tp ; therefore i?  will be much lower than 1 . 
By multiplying such a weight, confidence of in-
put chat text can be decreased so that the text can 
be easily detected.  
4.2 The Refined Entropy 
Instead of assigning a weight, we introduce the 
entropy-based value of the input chat text on the 
chat text corpus, i.e. cKH
~ , to produce a new equa-
tion. We denote nKH
~  the entropy calculated with 
equation (5).  Similar to nKH
~ , cKH
~  is calculated 
with equation (8).  
?
=
?=
K
i
icic
c
K TpTpK
H
1
)(~ln)(~1~              (8) 
We therefore re-write the entropy-based value 
calculation as follows.  
( )?
=
+?=
+=
K
i
icicinin
c
K
n
KK
TpTpTpTp
K
HHH
1
)(~ln)(~)(~ln)(~1
~~~
  (9) 
The intention of introducing cKH
~  in entropy 
calculation is to increase the entropy of input 
chat text when chat text trigrams are found. It 
can be easily proved that KH
~  is never smaller 
than nKH . As bigger entropy discloses a piece of 
more anomalous chat text, we believe more 
anomalous chat texts can be correctly detected 
with equation (9).  
5 Evaluations 
Three experiments are conducted in this work. 
The first experiment aims to estimate threshold 
values from a real text collection. The remaining 
experiments seek to evaluate performance of the 
approaches with various configurations.   
5.1 Data Description 
We use two types of text corpora to train our ap-
proaches in the experiments. The first type is 
51
standard Chinese corpus which is used to con-
struct standard language models. We use Peo-
ple?s Daily corpus, also know as Peking Univer-
sity Corpus (PKU), the Chinese Gigaword 
(CNGIGA) and the Chinese Penn Treebank 
(CNTB) in this work. Considering coverage, 
CNGIGA is the most excellent one. However, 
PKU and CPT provide more syntactic informa-
tion in their annotations. Another type of training 
corpus is chat text corpus. We use NIL corpus 
described in (Xia et. al., 2005b). In NIL corpus 
each anomalous chat text is annotated with their 
attributes.  
We create four test sets in our experiments. 
We use the test set #1 to estimate the threshold 
values of confidence and entropy for our ap-
proaches. The values are estimated on two types 
of trigrams in three corpora. Test set #1 contains 
89 pieces of typical Chinese chat text selected 
from the NIL corpus and 49 pieces of standard 
Chinese sentences selected from online Chinese 
news by hand. There is no special consideration 
that we select different number of chat texts and 
standard sentences in this test set. 
The remaining three test sets are used to com-
pare performance of our approaches on test data 
created in different time periods. The test set #2 
is the earliest one and #4 the latest one according 
to their time stamp. There are 10K sentences in 
total in test set #2, #3 and #4. In this collection, 
chat texts are selected from YESKY BBS system 
(http://bbs.yesky.com/bbs/) which cover BBS 
text in March and April 2005 (later than the chat 
text in the NIL corpus), and standard texts are 
extracted from online Chinese news randomly. 
We describe the four test sets in Table 1. 
Test set # of standard sentences 
# of chat 
sentences 
#1 49 89 
#2 1013 2320 
#3 1013 2320 
#4 1014 2320 
Table 1: Number of sentences in the four test 
sets. 
5.2 Experiment I: Threshold Values Esti-
mation 
5.2.1 Experiment Description 
This experiment seeks to estimate the threshold 
values of confidence and entropy for two types 
of trigrams in three Chinese corpora.  
We first run the two approaches using only 
standard Chinese corpora on the 138 sentences in 
the first test set. We put the calculated values 
(confidence or entropy) into two arrays. Note 
that we already know type of each sentence in 
the first test set. So we are able to select in each 
array a value that produces the lowest error rate. 
In this way we obtain the first group of threshold 
values for our approaches.  
We incorporate the NIL corpus to the two ap-
proaches and run them again. We then produce 
the second group of threshold values in the same 
way to produce the first group of values.  
5.2.2 Results 
The selected threshold values and corresponding 
error rates are presented in Table 2~5.  
Trigram option Threshold Err rate 
word of CNGIGA 1.58E-07 0.092 
word of PKU 7.06E-07 0.098 
word of CNTB 2.09E-06 0.085 
POS tag of CNGIGA 0.0278 0.248 
POS tag of PKU 0.0143 0.263 
POS tag of CNTB 0.0235 0.255 
Table 2: Selected threshold values of confidence 
for the approach using standard Chinese corpora 
and error rates.  
Trigram option Threshold Err rate 
word of CNGIGA 3.762E-056 0.099 
word of PKU 5.683E-048 0.112 
word of CNTB 2.167E-037 0.169 
POS tag of CNGIGA 0.00295 0.234 
POS tag of PKU 0.00150 0.253 
POS tag of CNTB 0.00239 0.299 
Table 3: Selected threshold values of entropy for 
the approach using standard Chinese corpora and 
error rates. 
Trigram option Threshold Err rate 
word of CNGIGA 4.26E-05 0.089 
word of PKU 3.75E-05 0.102 
word of CNTB 6.85E-05 0.092 
POS tag of CNGIGA 0.0398 0.257 
POS tag of PKU 0.0354 0.266 
POS tag of CNTB 0.0451 0.249 
Table 4: Selected threshold values of confidence 
for the approach incorporating the NIL corpus 
and error rates.  
Trigram option Threshold Err rate 
word of CNGIGA 8.368E-027 0.102 
word of PKU 3.134E-019 0.096 
word of CNTB 5.528E-021 0.172 
POS tag of CNGIGA 0.00465 0.241 
POS tag of PKU 0.00341 0.251 
POS tag of CNTB 0.00532 0.282 
Table 5: Selected thresholds values of entropy 
for the approach incorporating the NIL corpus 
and error rates. 
52
We use the selected threshold values in ex-
periment II and III to detect anomalous chat text 
within test set #2, #3 and #4. 
5.3 Experiment II: Anomaly Detection with 
Three Standard Chinese Corpora 
5.3.1 Experiment Description 
In this experiment, we run the two approaches 
using the standard Chinese corpora on test set #2. 
The threshold values estimated in experiment I 
are applied to help make decisions.  
Input text can be detected as either standard 
text or chat text. But we are only interested in 
how correctly the anomalous chat text is de-
tected. Thus we calculate precision (p), recall (r) 
and 1F  measure (f) only for chat text.  
 2        
rp
rpf
ba
ar
ca
ap +
??=+=+=     (10) 
where a is the number of true positives, b the 
false negatives and c the false positives.  
5.3.2 Results 
The experiment results for the approaches using 
the standard Chinese corpora on test set #2 are 
presented in Table 6. 
5.3.3 Discussions 
Table 4 shows that, in most cases, the entropy-
based approach outperforms the confidence-
based approach slightly. It can thus be conclude 
that the entropy-based approach is more effective 
in anomaly detection.  
It is also revealed that both approaches per-
form better with word trigrams than that with 
POS tag trigrams. This is natural for class based 
trigram model when number of class is small. 
Thirty-nine classes are used in ICTCLAS in POS 
tagging Chinese words.  
When the three Chinese corpora are compared, 
the CNGIGA performs best in the confidence-
based approach with word trigram model. How-
ever, it is not the case with POS tag trigram 
model. Results of two approaches on CNTB are 
best amongst the three corpora. Although we are 
able to draw the conclusion that  bigger corpora 
yields better performance with word trigram, the 
same conclusion, however, does not work for 
POS tag trigram. This is very interesting. The 
reason we can address on this issue is that CNTB 
probably provides highest quality POS tag tri-
grams and other corpora contain more noisy POS 
tag trigrams, which eventually decreases the per-
formance. An observation on word/POS tag lists 
for three Chinese corpora verifies such a claim. 
Text in CNTB is best-edited amongst the three.  
5.4 Experiment III: Anomaly Detection 
with NIL Corpus Incorporated 
5.4.1 Experiment Description 
In this experiment, we incorporate one chat text 
corpus, i.e. NIL corpus, to the two approaches. 
We run them on test set #2, #3 and #4 with the 
estimated threshold values. We use precision, 
recall and 1F  measure again to evaluate perform-
ance of the two approaches. 
5.4.2 Results 
The experiment results are presented in Table 7~ 
Table 9 on test set #2,  #3 and #4 respectively. 
5.4.3 Discussions 
We first compare the two approaches with dif-
ferent running configurations. All conclusions 
made in experiment II still work for experiment 
III. They are, i) the entropy-based approach out-
performs the confidence-based approach slightly 
in most cases; ii) both approach perform better 
with word trigram than POS tag trigram; iii) both 
approaches perform best on CNGIGA with word 
trigram model. But with POS tag trigram model, 
CNTB produces the best results.  
An interesting comparison is conducted on 1F  
measure between the approaches in experiment II 
and experiment III on test set #2 in Figure 1 (the 
left two columns). Generally, 1F  measure of 
anomaly detection with both approaches with 
word trigram model is improved when the NIL 
corpus is incorporated. It is revealed in Table 
7~9 that same observation is found with POS tag 
trigram model.  
We compare 1F  measure of the approaches 
with word trigram model in experiment III on 
test set #2, #3 and #4 in Figure 1 (the right three 
columns). The graph in Figure 1 shows that 1F  
measure on three test sets are very close to each 
other. This is also true the approaches with POS 
tag trigram model as showed in Table 7~9. This 
provides evidences for the argument that the ap-
proaches can produce stable performance with 
the NIL corpus. Differently, as reported in (Xia 
et. al., 2005a), performance achieved in SVM 
classifier is rather unstable. It performs poorly 
with training set C#1 which contains BBS text 
posted several months ago, but much better with 
training set C#5 which contains the latest chat 
text.  
53
Word trigram POS tag trigram 
confidence entropy confidence entropy Corpus 
p r f p r f p r f p r f 
CNGIGA 0.685 0.737 0.710 0.722 0.761 0.741 0.614 0.654 0.633 0.637 0.664 0.650 
PKU 0.699 0.712 0.705 0.701 0.738 0.719 0.619 0.630 0.624 0.625 0.648 0.636 
CNTB 0.653 0.661 0.657 0.692 0.703 0.697 0.651 0.673 0.662 0.684 0.679 0.681 
Table 6: Results of anomaly detection using standard Chinese corpora on test set #2.  
Word trigram POS tag trigram 
confidence entropy confidence entropy Corpus 
p r f p r f p r f p r f 
CNGIGA 0.821 0.836 0.828 0.857 0.849 0.853 0.653 0.657 0.655 0.672 0.678 0.675 
PKU 0.818 0.821 0.819 0.838 0.839 0.838 0.672 0.672 0.672 0.688 0.679 0.683 
CNTB 0.791 0.787 0.789 0.821 0.811 0.816 0.691 0.679 0.685 0.712 0.688 0.700 
Table 7: Results of anomaly detection incorporating NIL corpus on test set #2 
Word trigram POS tag trigram 
confidence entropy confidence entropy Corpus 
p r f p r f p r f p r f 
CNGIGA 0.819 0.841 0.830 0.849 0.848 0.848 0.657 0.659 0.658 0.671 0.677 0.674 
PKU 0.812 0.822 0.817 0.835 0.835 0.835 0.663 0.671 0.667 0.687 0.681 0.684 
CNTB 0.801 0.783 0.792 0.822 0.803 0.812 0.689 0.677 0.683 0.717 0.689 0.703 
Table 8: Results of anomaly detection incorporating NIL corpus on test set #3 
Word trigram POS tag trigram 
confidence entropy confidence entropy Corpus 
p r f p r f p r f p r f 
CNGIGA 0.824 0.839 0.831 0.852 0.845 0.848 0.651 0.654 0.652 0.674 0.674 0.674 
PKU 0.815 0.825 0.820 0.836 0.84 0.838 0.668 0.668 0.668 0.692 0.682 0.687 
CNTB 0.796 0.785 0.790 0.817 0.807 0.812 0.694 0.681 0.687 0.713 0.686 0.699 
Table 9: Results of anomaly detection incorporating NIL corpus on test set #4 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
conf-CNGIGA-
word
ent-CNGIGA-
word
conf-PKU-word ent-PKU-word conf-CNTB-word ent-CNTB-word
Exp-II-#2
Exp-III-#2
Exp-III-#3
Exp-III-#4
 
Figure 1: Comparisons on 1F  measure of the approaches with word trigram on test set #2, #3 and #4 in 
experiment II and experiment III. 
 
We finally compare performance of our ap-
proaches against the one described in (Xia, et. 
al., 2005a). The best 1F  measure achieved in our 
work, i.e. 0. 853, is close to the best one in their 
work, i.e. 0.871 with training corpus C#5. This 
proves another argument that our approaches can 
produce equivalent performance to the best ones 
achieved by the approaches in existence. 
6 Conclusions  
The new approaches to detecting anomalous 
Chinese chat text are proposed in this paper. The 
approaches calculate confidence and entropy 
values with the language models constructed on 
negative training samples in three standard Chi-
54
nese corpora. To improve detection quality, we 
incorporate positive training samples in NIL cor-
pus in our approaches. Two conclusions can be 
made based on this work. Firstly, 1F  measure of 
anomaly detection can be improved by around 
0.10 when NIL corpus is incorporated into the 
approaches. Secondly, performance equivalent to 
the best ones produced by the approaches in exis-
tence can be achieved stably by incorporating the 
standard Chinese corpora and the NIL corpus.  
We believe some strong evidences for our 
claims can be obtained by training our ap-
proaches with more chat text corpora which con-
tain chat text created in different time periods. 
We are conducting this experiment seeks to find 
out whether and how our approaches are inde-
pendent of time. This work is still progressing. A 
report on this issue will be available shortly. We 
also plan to investigate how size of chat text cor-
pus influences performance of our approaches. 
The goal is to find the optimal size of chat text 
corpus which can achieve the best performance. 
The readers should also be noted that evaluation 
in this work is a within-domain test. Due to 
shortage of chat text resources, no cross-domain 
test is conducted. In the future cross-domain test, 
we will investigate how our approaches are inde-
pendent of domain.  
Eventual goal of chat text processing is to nor-
malize the anomalous chat text, namely, convert 
it to standard text holding the same meaning. So 
the work carried out in this paper is the first step 
leading to this goal. Approaches will be designed 
to locate the anomalous terms in chat text and 
map them to standard words.  
Acknowledgement 
Research described in this paper is partially sup-
ported by the Chinese University of Hong Kong 
under the Direct Grant Scheme (No: 2050330) 
and Strategic Grant Scheme project (No: 
4410001) respectively. 
Reference 
Brown, P. F., V. J. Della Pietra, P. V. de Souza, J. C. 
Lai, and R. L. Mercer. 1990. Class-based n-gram 
models of natural language. In Proceedings of the 
IBM Natural Language ITL, Paris, France. 
Finkelhor, D., K. J. Mitchell, and J. Wolak. 2000. 
Online Victimization: A Report on the Nation's 
Youth. Alexandria, Virginia: National Center for 
Missing & Ex-ploited Children, page ix. 
German News. 2004.  Germans are world SMS cham-
pions, 8 April 2004, http://www.expatica.com/ 
source/site_article.asp?subchannel_id=52&story_i
d=6469.  
Gianforte, G.. 2003. From Call Center to Contact 
Center: How to Successfully Blend Phone, Email, 
Web and Chat to Deliver Great Service and Slash 
Costs. RightNow Technologies.  
Heard-White, M., Gunter Saunders and Anita Pincas. 
2004. Report into the use of CHAT in education. 
Final report for project of Effective use of CHAT 
in Online Learning, Institute of Education, Univer-
sity of London. 
McCullagh, D.. 2004. Security officials to spy on chat 
rooms. News provided by CNET Networks. No-
vember 24, 2004.  
Xia, Y., K.-F. Wong and W. Gao. 2005a. NIL is not 
Nothing: Recognition of Chinese Network Infor-
mal Language Expressions, 4th SIGHAN Work-
shop on Chinese Language Processing at 
IJCNLP'05, pp95-102. 
Xia, Y., K.-F. Wong and R. Luk. 2005b. A Two-Stage 
Incremental Annotation Approach to Constructing 
A Network Informal Language Corpus. In Proc. of 
NTCIR-5 Meeting, pp. 529-536.  
Zhang, Z., H. Yu, D. Xiong and Q. Liu. 2003. HMM-
based Chinese Lexical Analyzer ICTCLAS. 
SIGHAN?03 within ACL?03, pp. 184-187. 
 
 
55
Proceedings of the Linguistic Annotation Workshop, pages 61?68,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating Chinese Collocations with Multi Information 
 
Ruifeng Xu1,     Qin Lu1,     Kam-Fai Wong2,    Wenjie Li1 
? 
?1 Department of Computing,                       2 Department of Systems Engineering and 
?                                                  Engineering Management 
 The Hong Kong Polytechnic University,        The Chinese University of Hong Kong,   
?Kowloon, Hong Kong                                      N.T., Hong Kong 
{csrfxu,csluqin,cswjli}@comp.polyu.edu.hk  kfwong@se.cuhk.edu.hk 
? 
 
Abstract 
This paper presents the design and construc-
tion of an annotated Chinese collocation bank 
as the resource to support systematic research 
on Chinese collocations. With the help of 
computational tools, the bi-gram and n-gram 
collocations corresponding to 3,643 head-
words are manually identified. Furthermore, 
annotations for bi-gram collocations include 
dependency relation, chunking relation and 
classification of collocation types. Currently, 
the collocation bank annotated 23,581 bi-
gram collocations and 2,752 n-gram colloca-
tions extracted from  a 5-million-word corpus. 
Through statistical analysis on the collocation 
bank, some characteristics of Chinese bi-
gram collocations are examined which is es-
sential to collocation research, especially for 
Chinese. 
1 Introduction 
Collocation is a lexical phenomenon in which two 
or more words are habitually combined and com-
monly used in a language to express certain seman-
tic meaning. For example, in Chinese, people will 
say ??-?? (historical baggage) rather than ?
? -?? (historical luggage) even though ??
(baggage) and ?? (luggage) are synonymous. 
However, no one can argue why ?? must collo-
cate with??. Briefly speaking, collocations are 
frequently used word combinations. The collocated 
words always have syntactic or semantic relations 
but they cannot be generated directly by syntactic 
or semantic rules. Collocation can bring out differ-
ent meanings a word can carry and it plays an in-
dispensable role in expressing the most appropriate 
meaning in a given context. Consequently, colloca-
tion knowledge is widely employed in natural lan-
guage processing tasks such as word sense disam-
biguation, machine translation, information re-
trieval and natural language generation (Manning 
et al 1999).  
Although the importance of collocation is well 
known, it is difficult to compile a complete collo-
cation dictionary. There are some existing corpus 
linguistic researches on automatic extraction of 
collocations from electronic text (Smadja 1993; 
Lin 1998; Xu and Lu 2006). These techniques are 
mainly based on statistical techniques and syntactic 
analysis. However, the performances of automatic 
collocation extraction systems are not satisfactory 
(Pecina 2005). A problem is that collocations are 
word combinations that co-occur within a short 
context, but not all such co-occurrences are true 
collocations. Further examinations is needed to 
filter out pseudo-collocations once co-occurred 
word pairs are identified.  A collocation bank with 
true collocations annotated is naturally an indis-
pensable resource for collocation research. (Kosho 
et al 2000) presented their works of collocation 
annotation on Japanese text. Also, the Turkish 
treebank, (Bedin 2003) included collocation anno-
tation as one step in its annotation. These two col-
location banks provided collocation identification 
and co-occurrence verification information. (Tutin 
2005) used shallow analysis based on finite state 
transducers and lexicon-grammar to identify and 
annotate collocations in a French corpus. This col-
location bank further provided the lexical functions 
of the collocations. However to this day, there is 
no reported Chinese collocation bank available. 
61
In this paper, we present the design and con-
struction of a Chinese collocation bank (acrony-
med CCB). This is the first attempt to build a 
large-scale Chinese collocation bank as a Chinese 
NLP resource with multiple linguistic information 
for each collocation including:  (1) annotating the 
collocated words for each given headword; (2) dis-
tinguishing n-gram and bi-gram collocations for 
the headword; (3) for bi-gram collocations, CCB 
provides their syntactic dependencies, chunking 
relation and classification of collocation types 
which is proposed by (Xu and Lu 2006). In addi-
tion, we introduce the quality assurance mecha-
nism used for CCB. CCB currently contains for 
3,643 common headwords taken from ?The Dic-
tionary of Modern Chinese Collocations? (Mei 
1999) with 23,581 unique bi-gram collocations and 
2,752 unique n-gram collocations extracted from a 
five-million-word segmented and chunked Chinese 
corpus (Xu and Lu, 2005). 
The rest of this paper is organized as follows. 
Section 2 presents some basic concepts. Section 3 
describes the annotation guideline. Section 4 de-
scribes the practical issues in the annotation proc-
ess including corpus preparation, headword prepa-
ration, annotation flow, and the quality assurance 
mechanism. Section 5 gives current status of CCB 
and characteristics analysis of the annotated collo-
cations. Section 6 concludes this paper. 
2 Basic Concepts 
Although collocations are habitual expressions in 
natural language use and they can be easily under-
stood by people, a precise definition of collocation 
is still far-reaching (Manning et al 1999). In this 
study, we define a collocation as a recurrent and 
conventional expression of two or more content 
words that holds syntactic and semantic relation. 
Content words in Chinese include noun, verb, ad-
jective, adverb, determiner, directional word, and 
gerund. Collocations with only two words are 
called bi-gram collocations and others are called n-
gram collocations. 
From a linguistic view point, collocations have a 
number of characteristics. Firstly, collocations are 
recurrent as they are of habitual use. Collocations 
occur frequently in similar contexts and they ap-
pear in certain fixed patterns. However, they can-
not be described by the same set of syntactic or 
semantic rules. Secondly, free word combinations 
which can be generated by linguistic rules are 
normally considered compositional.  In contrast, 
collocations should be limited compositional 
(Manning et al 1999) and they usually carry addi-
tional meanings when used as a collocation. 
Thirdly, collocations are also limited substitutable 
and limited modifiable. Limited substitutable here 
means that a word cannot be freely substituted by 
other words with similar linguistic functions in the 
same context such as synonyms. Also, many collo-
cations cannot be modified freely by adding modi-
fiers or through grammatical transformations. 
Lastly, collocations are domain-dependent (Smadja 
1993) and language-dependent.  
3 Annotation Guideline Design 
The guideline firstly determines the annotation 
strategy. 
(1) The annotation of CCB follows the head-
word-driven strategy. The annotation uses selected 
headwords as the starting point. In each circle, the 
collocations corresponding to one headword are 
annotated. Headword-driven strategy makes a 
more efficient annotation as it is helpful to estimate 
and compare the relevant collocations. 
(2) CCB is manually annotated with the help of 
automatic estimation of computational features, i.e. 
semi-automatic software tools are used to generate 
parsing and chunking candidates and to estimate 
the classification features. These data are present to 
the annotators for determination. The use of assis-
tive tools is helpful to produce accurate annota-
tions with efficiency. 
The guideline also specifies the information to 
be annotated and the labels used in the annotation. 
For a given headword, CCB annotates both bi-
gram collocations and n-gram collocations. Con-
sidering the fact that n-gram collocations consist-
ing of continuous significant bi-grams as a whole 
and, the n-gram annotation is based on the identifi-
cation and verification of bi-gram word combina-
tions and is prior to the annotation of bi-gram col-
locations.  
For bi-gram annotation, which is the major in-
terest  in collocation research, three kinds of in-
formation are annotated. The first one is the syn-
tactic dependency of the headword and its co-word 
in a bi-gram collocation . A syntactic dependency 
normally consists of one word as the governor (or 
head), a dependency type and another word serves 
62
as dependent (or modifier) (Lin 1998).Totally, 10 
types of dependencies are annotated in CCB. They 
are listed in Table 1 below. 
 
 Dependency Description Example 
ADA Adjective and its adverbial modifier ??/d ??/a  greatly painful 
ADV Predicate and its adverbial modifier in 
which the predicate serves as head 
??/ad ??/v heavily strike 
AN Noun and its adjective modifier ??/a ??/n lawful incoming 
CMP Predicate and its complement in which 
the predicate serves as head 
??/v??/v ineffectively treat
NJX Juxtaposition structure ??/a??/a fair and reasonable
NN Noun and its nominal modifier ??/n ??/n personal safety 
SBV Predicate and its subject ??/n ??/v property transfer 
VO Predicate and its object in which the 
predicate serves as head 
??/v ??/n change mechanism
VV Serial verb constructions which indi-
cates that there are serial actions  
??/v ??/v trace and report 
OT Others   
Table 1. The dependency categories 
The second one is the syntactic chunking informa-
tion (a chunk is defined as a minimum non-nesting 
or non-overlapping phrase) (Xu and Lu, 2005). 
Chunking information identifies all the words for a 
collocation within the context of an enclosed 
chunk. Thus, it is a way to identify its proper con-
text at the most immediate syntactic structure. 11 
types of syntactic chunking categories given in (Xu 
and 2006) are used as listed in Table 2.  
 
 Description Examples 
BNP Base noun phrase [??/n ??/n]NP     market economy 
BAP Base adjective phrase  [??/a??/a]BAP   fair and reasonable
BVP Base verb phrase [??/a??/v]BVP   successfully start 
BDP Base adverb phrase [?/d ??/d]BDP      no longer 
BQP Base quantifier phrase [?? /m ? /q]BQP ?? /n several thou-
sand soldiers 
BTP Base time phrase [??/t ??/t]BTP 8:00 in the morning 
BFP Base position phrase [??/ns ???/f]BFP Northeast of Mon-
golia 
BNT Name of an organization [??/ns ??/n]BNT Yantai University 
BNS Name of a place [??/ns ??/ns]BNS Tongshan, Jiangsu 
Province 
BNZ Other proper noun phrase [???/nr?/n]BNZ The Nobel Prize 
BSV S-V structure [??/n ??/a]BSV  territorial integrity 
Table 2. The chunking categories 
The third one is the classification of collocation 
types. Collocations cover a wide spectrum of ha-
bitual word combinations ranging from idioms to 
free word combinations. Some collocations are 
very rigid and some are more flexible. (Xu and Lu 
2006) proposed a scheme to classify collocations 
into four types according to the internal association 
of collocations including compositionality, non-
substitutability, non-modifiability, and statistical 
significance. They are,  
Type 0: Idiomatic Collocation 
Type 0 collocations are fully non-compositional 
as its meaning cannot be predicted from the mean-
ings of its components such as???? (climbing 
a tree to catch a fish, which is a metaphor for a 
fruitless endeavour). Some terminologies are also 
Type 0 collocations such as ?  ?(Blue-tooth ) 
which refers to a wireless communication protocol. 
Type 0 collocations must have fixed forms. Their 
components are non-substitutable and non-
modifiable allowing no syntactic transformation 
and no internal lexical variation. This type of col-
locations has very strong internal associations and 
co-occurrence statistics is not important.  
Type 1: Fixed Collocation 
Type 1 collocations are very limited composi-
tional with fixed forms which are non-substitutable 
and non-modifiable. However, this type can be 
compositional. None of the words in a Type 1 col-
location can be substituted by any other words to 
retain the same meaning such as in??/n ???
/n (diplomatic immunity). Finally, Type 1 colloca-
tions normally have strong co-occurrence statistics 
to support them. 
Type 2: Strong Collocation 
Type 2 collocations are limitedly compositional. 
They allow very limited substitutability. In other 
words, their components can only be substituted by 
few synonyms and the newly generated word com-
binations have similar meaning, e.g., ??/v ??
/n (alliance formation) and ??/v ??/n (alliance 
formation). Furthermore, Type 2 collocations al-
low limited modifier insertion and the order of 
components must be maintained. Type2 colloca-
tions normally have strong statistical support.  
Type 3: Loose Collocation 
Type 3 collocations have loose restrictions. 
They are nearly compositional. Their components 
may be substituted by some of their synonyms and 
the newly generated word combinations usually 
have very similar meanings. Type 3 collocations 
are modifiable meaning that they allow modifier 
insertions. Type 3 collocations have weak internal 
associations and they must have statistically sig-
nificant co-occurrence.  
The classification represents the strength of in-
ternal associations of collocated words. The anno-
tation of these three kinds of information is essen-
tial to all-rounded characteristic analysis of collo-
cations. 
63
4 Annotation of CCB 
4.1 Data Preparation 
CCB is based on the PolyU chunk bank (Xu and 
Lu, 2005) which contains chunking information on 
the People?s Daily corpus with both segmentation 
and part-of-speech tags. The accuracies of word 
segmentation and POS tagging are claimed to be 
higher than 99.9% and 99.5%, respectively (Yu et 
al. 2001). The use of this popular and accurate raw 
resource helped to reduce the cost of annotation 
significantly, and ensured maximal sharing of our 
output.  
The set of 3, 643 headwords are selected from 
?The Dictionary of Modern Chinese Collocation? 
(Mei 1999) among about 6,000 headwords in the 
dictionary. The selection  was based both on the 
judgment by linguistic experts as well as the statis-
tical information that they are commonly used. 
4.2 Corpus Preprocessing 
The CCB annotations are represented in XML. 
Since collocations are practical word combinations 
and word is the basic unit in collocation research, a 
preprocessing module is devised to transfer the 
chunked sentences in the PolyU chunk bank to 
word sequences with the appropriate labels to indi-
cate the corresponding chunking information. This 
preprocessing module indexes the words and 
chunks in the sentences and encodes the chunking 
information of each word in two steps. Consider 
the following sample sentence extracted from the 
PolyU chunk bank: 
??/v[??/n??/n]BNP?/u[??/n??/n??
/an ]BNP 
(ensure life and property safety of the people) 
The first step in preprocessing is to index each 
word and the chunk in the sentence by giving in-
cremental word ids and chunk ids from left to right. 
That is,, 
[W1]??/v [W2]??/n [W3]??/n [W4]?/u  
[W5]??/n [W6]??/n [W7]??/an [C1]BNP [C2]BNP 
where, [W1] to [W7] are the words and [C1] to [C2] 
are chunks although chunking positions are not 
included in this step. One Chinese word may occur 
in a sentence for more than one times, the unique 
word ids are helpful to avoid ambiguities in the 
collocation annotation on these words. 
The second step is to represent the chunking in-
formation of each word. Chunking boundary in-
formation is labeled by following initial/final rep-
resentation scheme. Four labels, O/B/I/E, are used 
to mark the isolated words outsides any chunks, 
chunk-initial words, words in the middle of chunks, 
and chunk-final words, respectively. Finally, a la-
bel H is used to mark the identified head of chunks 
and N to mark the non-head words. 
The above sample sentence is then transferred to 
a sequence of words with labels as shown below, 
<labeled> [W1][O_O_N][O]??/v [W2][B_BNP_N][C1]
??/n [W3][E_BNP_H][C1]??/n [W4][O_O_N][O]?/u 
[W5][B_BNP_N][C2]??/n [W6][I_BNP_N][C2]??/n 
[W7][E_BNP_N][C2]??/an </labeled> 
For each word, the first label is the word ID. The 
second one is a hybrid tag for describing its chunk-
ing status. The hybrid tags are ordinal with respect 
to the chunking status of boundary, syntactic cate-
gory and head, For example, B_BNP_N indicates 
that current word is the beginning or a BNP and 
this word is not the head of this chunk. The third 
one is the chunk ID if applicable. For the word out 
of any chunks, a fixed chunk ID O is given. 
4.3 Collocation Annotation 
Collocation annotation is conducted on one head-
word at a time. For a given headword, an annota-
tors examines its context to determine if its co-
occurred word(s) forms a collocation with it and if 
so, also annotate the collocation?s dependency, 
chunking and classification information. The anno-
tation procedure, requires three passes. We use a 
headword ??/an (safe), as an illustrative exam-
ple.  
Pass 1. Concordance and dependency identifica-
tion 
In the first pass, the concordance of the given 
headword is performed. Sentences containing the 
headwords are obtained, e.g.  
S1: ??/v [??/v  ??/an]BVP  ?/u  ??/n 
(follow the principles for ensuring the safety) 
S2: ??/v [??/n ??/n]BNP ?/u[??/n ??/n ?
?/an]BNP 
(ensure life and property safety of people) 
S3: ??/v  ??/ns  [??/an  ??/v]BVP 
(ensure the flood pass through Yangzi River safely) 
With the help of an automatic dependency pars-
er, the annotator determines all syntactically and 
semantically dependent words in the chunking con-
text of the observing headword. The annotation 
output of S1 is given below in which XML tags are 
used for the dependency annotation.  
S1:<sentence>??/v [??/v  ??/an]BVP  ?/u  ??/n 
64
<labeled> [W1][O_O_N][O]??/v [W2][B_BVP_H][C1]
??/v [W3][E_BNP_N][C1]??/an [W4][O_O_N][O]?
/u  [W5][O_O_N][O]??/n </labeled> 
<dependency no="1" observing="??/an" head="??
/v" head_wordid="W2" head_chunk ="B_BVP_H" 
head_chunkid="C1" modifier=" ? ? /an" modi-
fier_wordid="W3" modifier _chunk="E_BVP_N" 
modifer_chunkid="C1" relation="VO" > </dependency> 
</sentence> 
Dependency of word combination is annotated 
with the tag <dependency> which includes the fol-
lowing attributes: 
-<dependency> indicates an identified depend-
ency   
-no is the id of identified dependency within cur-
rent sentence according to ordinal sequence 
-observing indicates the current observing 
headword 
-head indicates the head of the identified word 
dependency 
-head_wordid is the word id of the head 
-head_chunk is the hybrid tags for labeling the 
chunking information of the head 
-head_chunkid is the chunk id of the head 
-modifier indicates the modifier of the identified 
dependency 
-modifier_wordid is the word id of the modifier 
-modifier_chunk is the hybrid tags for labeling 
chunking information of the modifier 
-modifier_chunkid is the chunk id of the modi-
fier 
-relation gives the syntactic dependency rela-
tions labeled according to the dependency labels 
listed in Table 1. 
In S1 and S2, the word combination ??/v??
/an has direct dependency, and in S3, such a de-
pendency does not exist as??/v only determines
??/v and ??/an depends on ??/v. The qual-
ity of CCB highly depends on the accuracy of de-
pendency annotation. This is very important for 
effective characteristics analysis of collocations 
and for the collocation extraction algorithms.  
Pass 2. N-gram collocations annotation 
It is relatively easy to identify n-gram colloca-
tions since an n-gram collocation is of habitual and 
recurrent use of a series of bi-grams. This means 
that n-gram collocations can be identified by find-
ing consecutive occurrence of significant bi-grams 
in certain position. In the second pass, the annota-
tors focus on the sentences where the headword 
has more than one dependency. The percentage of 
all appearances of each dependent word at each 
position around the headword is estimated with the 
help of a program (Xu and Lu, 2006). Finally, 
word dependencies frequently co-occurring in con-
secutive positions in a fixed order are extracted as 
n-gram collocations.   
For the headword, an n-gram collocation??/n 
? ? /n ? ? /an is identified since the co-
occurrence percentage of dependency??/-NN-?
?/an and dependency??/n-NN-??/an is 0.74 
is greater than a empirical threshold suggest in (Xu 
and Lu, 2006). This n-gram is annotated in S2 as 
follows: 
<ncolloc observing="??/an" w1="??/n" w2="??/n" 
w3="??/an" start_wordid="5"> </ncolloc> 
where, 
-<ncolloc> indicates an n-gram collocation  
-w1, w2,..wn give the components of the n-gram 
collocation according to the ordinal sequence.  
-start_wordid  indicates the word id of the first 
component of the n-gram collocation. 
Since n-gram collocation is regarded as a whole, 
its internal dependencies are ignored in the output 
file of pass 2. That is, if the dependencies of sev-
eral components are associated with an n-gram 
collocation in one sentence, the n-gram collocation 
is annotated and these dependencies are filtered out 
so as not to disturb the bi-gram dependencies.  
Pass 3. Bi-gram collocations annotation 
In this pass, all the word dependencies are ex-
amined to identify bi-gram collocations. Further-
more, if a dependent word combination is regarded 
as a collocation by the annotators, it will be further 
labeled based on the type determined. The identifi-
cation is based on expert knowledge combined 
with the use of several computational features as 
discussed in (Xu and Lu, 2006). 
An assistive tool is developed to estimate the 
computational features. We use the program to ob-
tain feature data based on two sets of data. The 
first data set is the annotated dependencies in the 
5-million-word corpus which is obtained through 
Pass 1 and Pass 2 annotations. Because the de-
pendent word combinations are manually identi-
fied and annotated in the first pass, the statistical 
significance is helpful to identify whether the word 
combination is a collocation and to determine its 
type. However, data sparseness problem must be 
considered since 5-million-word is not large 
enough. Thus, another set of statistical data are 
65
collected from a 100-million segmented and tagged 
corpus (Xu and Lu, 2006). With this large corpus, 
data sparseness is no longer a serious problem. But, 
the collected statistics are quite noisy since they 
are directly retrieved from text without any verifi-
cation. By analyzing the statistical features from 
both sets, the annotator can use his/her professional 
judgment to determine whether a bi-gram is a col-
location and its collocation type. 
In the example sentences, two collocations are 
identified. Firstly, ??/an ??/v is classified as a 
Type 1 collocation as they have only one peak co-
occurrence, very low substitution ratio and their 
co-occurrence order nearly never altered. Secondly, 
??/v ??/an is identified as a collocation. They 
have frequent co-occurrences and they are always 
co-occurred in fixed order among the verified de-
pendencies. However, their co-occurrences are dis-
tributed evenly and they have two peak co-
occurrences. Therefore, ??/v ??/an is classi-
fied as a Type 3 collocation. These bi-gram collo-
cations are annotated as illustrated below, 
<bcolloc observing="??/an" col="??/v" head="??
/v" type= "1" relation="ADV">  
<dependency no="1" observing="??/an" head="??/v" 
head_wordid="W4" head_chunk ="E_BVP_H" 
head_chunkid="C1" modifier=" ? ? /an" modi-
fier_wordid="W3" modifier _chunk="B_BVP_N" 
modifer_chunkid="C1" relation="ADV" 
></dependency></bcolloc> 
where, 
-<bcolloc> indicates a bi-gram collocation. 
-col is for  the collocated word. 
-head indicates the head of an identified colloca-
tion 
-type is the classified collocation type. 
-relation gives the syntactic dependency rela-
tions of this bi-gram collocation. 
Note that the dependency annotations within the 
bi-gram collocations are reserved. 
4.4 Quality Assurance 
The annotators of CCB are three post-graduate stu-
dents majoring in linguistics. In the first annotation 
stage, 20% headwords of the whole set was anno-
tated in duplicates by all three of them. Their out-
puts were checked by a program. Annotated collo-
cation including classified dependencies and types 
accepted by at least two annotators are reserved in 
the final data as the Golden Standard while the 
others are considered incorrect. The inconsisten-
cies between different annotators were discussed to 
clarify any misunderstanding in order to come up 
with the most appropriate annotations. In the sec-
ond annotation stage, 80% of the whole annota-
tions were then divided into three parts and sepa-
rately distributed to the annotators with 5% dupli-
cate headwords were distributed blindly. The du-
plicate annotation data were used to estimate the 
annotation consistency between annotators.  
5 Collocation Characteristic Analysis 
5.1 Progress and Quality of CCB 
Up to now, the first version of CCB is completed. 
We have obtained 23,581 unique bi-gram colloca-
tions and 2,752 unique n-gram collocations corre-
sponding to the 3,643 observing headwords. 
Meanwhile, their occurrences in the corpus are an-
notated and verified. With the help of a computer 
program, the annotators manually classified bi-
gram collocations into three types. The numbers of 
Type 0/1, Type 2 and Type 3 collocations are 152, 
3,982 and 19,447, respectively.  
For the 3,643 headwords in The Dictionary of 
Modern Chinese Collocations (Mei 1999) with 
35,742 bi-gram collocations,  20,035 collocations 
appear in the corpus. We call this collection as 
Mei?s Collocation Collection (MCC). There are 
19,967 common entries in MCC and CCB, which 
means 99.7% collocations in MCC appear in CCB 
indicating a good linguistic consistency. Further-
more, 3,614 additional collocations are found in 
CCB which enriches the static collocation diction-
ary.
 
5.2 Dependencies Numbers Statistics of Col-
locations 
Firstly, we study the statistics of how many types 
of dependencies a bi-gram collocation may have. 
The numbers of dependency types with respect to 
different collocation types are listed in Table 3.  
 
Collocations 1 type 2 types >2 types Total 
Type 0/1 152 0 0 152 
Type 2 3970 12 0 3982 
Type 3 17282 2130 35 19447 
Total 21404 2142 35 23581 
Table 3. Collocation classification versus number 
of dependency types 
66
It is observed that about 90% bi-gram collocations 
have only one dependency type. This indicates that 
a collocation normally has only one fixed syntactic 
dependency. It is also observed that about 10% bi-
gram collocations have more than one dependency 
type, especially Type 3 collocations. For example, 
two types of dependencies are identified in the bi-
gram collocation ??/an-??/n. They are ??
/an-AN-??/n (a safe nation) which indicates the 
dependency of a noun and its nominal modifier 
where ??/n serves as the head, and??/n-NN-
?? /an (national security) which indicates the 
dependency of a noun and its nominal modifier 
where ??/an serves as the head. It is attributed to 
the fact that the use of Chinese words is flexible. A 
Chinese word may support different part-of-speech. 
A collocation with different dependencies results 
in different distribution trends and most of these 
collocations are classified as Type 3. On the other 
hand, Type 0/1 and Type 2 collocations seldom 
have more than one dependency type. 
5.3 Syntactic Dependency Statistics of Collo-
cations 
The statistics of the 10 types of syntactic depend-
encies with respect to different types of bi-gram 
collocations are shown in Table 4. No. is the num-
ber of collocations with a given dependency type  
D and a given collocation type T. The percentage 
of No. among all collocations with the same collo-
cation type T is labeled as P_T, and the percentage 
of No. among all of the collocations with the same 
dependency D is labeled as P_D. 
 
 Type 0/1  Type 2  Type 3  Total 
 No. P_T P_D No. P_T P_D No. P_T P_D No. P_T
ADA 1 0.7 0.1 212 5.3 11.5 1637 7.6 88.5 1850 7.2
ADV 9 5.9 0.3 322 8.1 11.2 2555 11.8 88.5 2886 11.2
AN 20 13.2 0.4 871 21.8 15.4 4771 22.0 84.3 5662 22.0
CMP 12 7.9 2.2 144 3.6 26.9 379 1.8 70.8 535 2.1
NJX 8 5.3 3.2 42 1.1 16.9 198 0.9 79.8 248 1.0
NN 44 28.9 0.9 1036 25.9 21.6 3722 17.2 77.5 4802 18.6
SBV 4 2.6 0.2 285 7.1 11.1 2279 10.5 88.7 2568 10.0
VO 26 17.1 0.5 652 16.3 12.5 4545 21.0 87.0 5223 20.2
VV 3 2.0 0.2 227 5.7 13.4 1464 6.8 86.4 1694 6.6
OT 25 16.4 7.7 203 5.1 62.5 97 0.4 29.8 325 1.3
Total 152 100.0 0.6 3994 100.0 15.5 21647 100.0 83.9 25793 100.0
Table 4. The statistics of collocations with dif-
ferent collocation type and dependency 
 
Corresponding to 23,581 bi-gram collocations, 
25,793 types of dependencies are identified (some 
collocations have more than one types of depend-
ency). In which, about 82% belongs to five major 
dependency types. They are AN, VO, NN, ADV and 
SBV. It is note-worthy that the percentage of NN 
collocation is much higher than that in English. 
This is because nouns are more often used in paral-
lel to serve as one syntactic component in Chinese 
sentences than in English. 
The percentages of Type 0/1, Type 2 and Type 3 
collocations in CCB are 0.6%, 16.9% and 82.5%, 
respectively. However, the collocations with dif-
ferent types of dependencies have shown their own 
characteristics with respect to different collocation 
types. The collocations with CMP, NJX and NN 
dependencies on average have higher percentage to 
be classified into Type 0/1 and Type 2 collocations. 
This indicates that CMP, NJX and NN collocations 
in Chinese are always used in fixed patterns and 
these kinds of collocations are not freely modifi-
able and substitutable. In the contrary, many ADV 
and AN collocations are classified as Type 3. This 
is partially due to the special usage of auxiliary 
words in Chinese.  Many AN Chinese collocations 
can be inserted by a meaningless auxiliary word?
/u and many ADV Chinese collocations can be in-
serted by an auxiliary word?/u. This means that 
many AN and ADV collocations can be modified 
and thus, they always have two peak co-
occurrences. Therefore, they are classified as Type 
3 collocations. 7.7% and 62.5% of the collocations 
with dependency OT are classified as Type 0/1 and 
Type2 collocations, respectively. Such percentages 
are much higher than the average. This is attributed 
by the fact that some Type 0/1 and Type 2 colloca-
tions have strong semantic relations rather than 
syntactic relations and thus their dependencies are 
difficult to label. 
5.4 Chunking Statistics of Collocations 
The chunking characteristic for the collocations 
with different types and different dependencies are 
examined. In most cases, Type 0/1/2 collocations 
co-occur within one chunk or between neighboring 
chunks. Therefore, their chunking characteristics 
are not discussed in detail. The percentage of the 
occurrences of Type 3 collocations with different 
chunking distances are given in Table 5. If a collo-
cation co-occurs within one chunk, the chunking 
distance is 0. If a collocation co-occurs between 
neighboring chunks, or between neighboring words, 
or between a word and a neighboring chunk, the 
chunking distance is 1, and so on. 
67
 
 ADA ADV AN CMP NJX NN SBV VO VV OT
0 chunk 56.8 53.1 65.7 48.5 70.2 62.4 46.5 41.1 47.2 86.4
1 chunk 38.2 43.7 28.5 37.2 15.4 27.9 41.2 35.7 41.1 13.5
2 chunks 5.0 3.2 3.7 14.2 14.4 9.7 11.0 17.6 9.6 0.1
>2chunks 0.0 0.0 2.1 0.1 0.0 0.0 1.3 5.6 2.1 0.0
Table 5. Chunking distances of Type 3 collocations  
 
It is shown that the co-occurrence of collocations 
decreases with increased chunking distance. Yet, 
the behavior for decrease is different for colloca-
tions with different dependencies. Generally speak-
ing, the ADA, ADV, CMP, NJX, NN and OT collo-
cations seldom co-occur cross two words or two 
chunks. Furthermore, the occurrences of AN, NJX 
and OT collocations quickly drops when the 
chunking distance is greater than 0, i.e. these col-
locations tends to co-occur within the same chunk. 
In the contrary, the co-occurrences of ADA, ADV, 
CMP, SBV and VV collocations corresponding to 
chunking distance equals 0 and 1 decrease steadily. 
It means that these four kinds of collocations are 
more evenly distributed within the same chunk or 
between neighboring words or chunks. The occur-
rences of VO collocations corresponding to chunk-
ing distance from 0 to 3 with a much flatter reduc-
tion. This indicates that a verb may govern its ob-
ject in a long range. 
6 Conclusions 
This paper describes the design and construction of 
a manually annotated Chinese collocation bank. 
Following a set of well-designed annotation guide-
line, the collocations corresponding to 3,643 
headwords are identified from a chunked five-
million word corpus. 2,752 unique n-gram colloca-
tions and 23,581 unique bi-gram collocations are 
annotated. Furthermore, each bi-gram collocation 
is annotated with its syntactic dependency informa-
tion, classification information and chunking in-
formation. Based on CCB, characteristics of collo-
cations with different types and different depend-
encies are examined. The obtained result is essen-
tial for improving research related to Chinese col-
location. Also, CCB may be used as a standard an-
swer set for evaluating the performance of differ-
ent collocation extraction algorithms. In the future, 
collocations of all unvisited headwords will be an-
notated to produce a complete 5-million-word Chi-
nese collocation bank. 
Acknowledgement 
This research is supported by The Hong Kong 
Polytechnic University (A-P203), CERG Grant 
(5087/01E) and Chinese University of Hong Kong 
under the Direct Grant Scheme project (2050330) 
and Strategic Grant Scheme project (4410001). 
References 
Bedin N. et al 2003. The Annotation Process in the 
Turkish Treebank. In Proc. 11th Conference of the 
EACL-4th Linguistically Interpreted Corpora Work-
shop- LINC. 
Kosho S. et al 2000. Collocations as Word Co-
occurrence Restriction Data - An Application to 
Japanese Word Processor. In Proc. Second Interna-
tional Conference on Language Resources and 
Evaluation 
Lin D.K. 1998. Extracting collocations from text cor-
pora. In Proc. First Workshop on Computational 
Terminology, Montreal  
Manning, C.D., Sch?tze, H. 1999: Foundations of Sta-
tistical Natural Language Processing, MIT Press  
Mei J.J. 1999. Dictionary of Modern Chinese Colloca-
tions, Hanyu Dictionary Press  
Pecina P. 2005. An Extensive Empirical Study of Collo-
cation Extraction Methods. In Proc. 2005 ACL Stu-
dent Research Workshop. 13-18 
Smadja. F. 1993. Retrieving collocations from text: 
Xtract, Computational Linguistics. 19. 1.  143-177 
Tutin A. 2005. Annotating Lexical Functions in Corpora: 
Showing Collocations in Context. In Proc. 2nd Inter-
national Conference on the Meaning ? Text Theory 
Xu R. F. and Lu Q. 2005. Improving Collocation Ex-
traction by Using Syntactic Patterns, In Proc. IEEE 
International Conference on Natural Language Proc-
essing and Knowledge Engineering. 52-57 
Xu, R.F. and Lu, Q. 2006. A Multi-stage Chinese Col-
location Extraction System. Lecture Notes in Com-
puter Science, Vol. 3930, Springer-Verlag. 740-749 
Yu S.W. et al 2001. Guideline of People?s Daily Cor-
pus Annotation, Technical Report, Peking University  
 
 
68
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 162?171,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Discourse Relations for Eliminating
Intra-sentence Polarity Ambiguities
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, Kam-Fai Wong
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong
Shatin, NT, Hong Kong, China
Key Laboratory of High Confidence Software Technologies
Ministry of Education, China
{ljzhou, byli, wgao, zywei, kfwong}@se.cuhk.edu.hk
Abstract
Polarity classification of opinionated sen-
tences with both positive and negative senti-
ments1 is a key challenge in sentiment anal-
ysis. This paper presents a novel unsuper-
vised method for discovering intra-sentence
level discourse relations for eliminating polar-
ity ambiguities. Firstly, a discourse scheme
with discourse constraints on polarity was de-
fined empirically based on Rhetorical Struc-
ture Theory (RST). Then, a small set of cue-
phrase-based patterns were utilized to collect
a large number of discourse instances which
were later converted to semantic sequential
representations (SSRs). Finally, an unsuper-
vised method was adopted to generate, weigh
and filter new SSRs without cue phrases for
recognizing discourse relations. Experimen-
tal results showed that the proposed methods
not only effectively recognized the defined
discourse relations but also achieved signifi-
cant improvement by integrating discourse in-
formation in sentence-level polarity classifica-
tion.
1 Introduction
As an important task of sentiment analysis, polar-
ity classification is critically affected by discourse
structure (Polanyi and Zaenen, 2006). Previous re-
search developed discourse schema (Asher et al,
2008) (Somasundaran et al, 2008) and proved that
the utilization of discourse relations could improve
the performance of polarity classification on dia-
logues (Somasundaran et al, 2009). However, cur-
1Defined as ambiguous sentences in this paper
rent state-of-the-art methods for sentence-level po-
larity classification are facing difficulties in ascer-
taining the polarity of some sentences. For example:
(a) [Although Fujimori was criticized by the international
community]?[he was loved by the domestic population]?
[because people hated the corrupted ruling class]. (??
??????????????????????
??????????????????????)
Example (a) is a positive sentence holding a Con-
trast relation between first two segments and a
Cause relation between last two segments. The po-
larity of "criticized", "hated" and "corrupted" are rec-
ognized as negative expressions while "loved" is rec-
ognized as a positive expression. Example (a) is dif-
ficult for existing polarity classification methods for
two reasons: (1) the number of positive expressions
is less than negative expressions; (2) the importance
of each sentiment expression is unknown. However,
consider Figure 1, if we know that the polarity of
the first two segments holding a Contrast relation
is determined by the nucleus (Mann and Thompson,
1988) segment and the polarity of the last two seg-
ments holding aCause relation is also determined by
the nucleus segment, the polarity of the sentence will
be determined by the polarity of "[he...population]".
Thus, the polarity of Example (a) is positive.
Statistics showed that 43% of the opinionated
sentences in NTCIR2 MOAT (Multilingual Opinion
Analysis Task) Chinese corpus3 are ambiguous. Ex-
isting sentence-level polarity classification methods
ignoring discourse structure often give wrong results
for these sentences. We implemented state-of-the-
2http://research.nii.ac.jp/ntcir/
3Including simplified Chinese and traditional Chinese cor-
pus from NTCIR-6 MOAT and NTCIR-7 MOAT
162
Figure 1: Discourse relations for Example (a). (n and s
denote nucleus and satellite segment, respectively)
art method (Xu and Kit, 2010) in NTCIR-8 Chinese
MOAT as the baseline polarity classifier (BPC) in
this paper. Error analysis of BPC showed that 49%
errors came from ambiguous sentences.
In this paper, we focused on the automation of
recognizing intra-sentence level discourse relations
for polarity classification. Based on the previous
work of Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988), a discourse scheme with dis-
course constraints on polarity was defined empiri-
cally (see Section 3). The scheme contains 5 rela-
tions: Contrast, Condition, Continuation, Cause and
Purpose. From a raw corpus, a small set of cue-
phrase-based patterns were used to collect discourse
instances. These instances were then converted to
semantic sequential representations (SSRs). Finally,
an unsupervised SSR learner was adopted to gener-
ate, weigh and filter high quality new SSRs with-
out cue phrases. Experimental results showed that
the proposed methods could effectively recognize
the defined discourse relations and achieve signifi-
cant improvement in sentence-level polarity classi-
fication comparing to BPC.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work. Sec-
tion 3 presents the discourse scheme with discourse
constraints on polarity. Section 4 gives the detail of
proposed method. Experimental results are reported
and discussed in Section 5 and Section 6 concludes
this paper.
2 Related Work
Research on polarity classification were generally
conducted on 4 levels: document-level (Pang et al,
2002), sentence-level (Riloff et al, 2003), phrase-
level (Wilson et al, 2009) and feature-level (Hu and
Liu, 2004; Xia et al, 2007).
There was little research focusing on the auto-
matic recognition of intra-sentence level discourse
relations for sentiment analysis in the literature.
Polanyi and Zaenen (2006) argued that valence cal-
culation is critically affected by discourse struc-
ture. Asher et al (2008) proposed a shallow se-
mantic representation using a feature structure and
use five types of rhetorical relations to build a fine-
grained corpus for deep contextual sentiment anal-
ysis. Nevertheless, they did not propose a com-
putational model for their discourse scheme. Sny-
der and Barzilay (2007) combined an agreement
model based on contrastive RST relations with a lo-
cal aspect model to make a more informed over-
all decision for sentiment classification. Nonethe-
less, contrastive relations were only one type of dis-
course relations which may help polarity classifica-
tion. Sadamitsu et al (2008) modeled polarity re-
versal using HCRFs integrated with inter-sentence
discourse structures. However, our work is on intra-
sentence level and our purpose is not to find polar-
ity reversals but trying to adapt general discourse
schemes (e.g., RST) to help determine the overall
polarity of ambiguous sentences.
The most closely related works were (Somasun-
daran et al, 2008) and (Somasundaran et al, 2009),
which proposed opinion frames as a representation
of discourse-level associations on dialogue andmod-
eled the scheme to improve opinion polarity clas-
sification. However, opinion frames was difficult
to be implemented because the recognition of opin-
ion target was very challenging in general text. Our
work differs from their approaches in two key as-
pects: (1) we distinguished nucleus and satellite in
discourse but opinion frames did not; (2) our method
for discourse discovery was unsupervised while their
method needed annotated data.
Most research works about discourse classifica-
tion were not related to sentiment analysis. Su-
pervised discourse classification methods (Soricut
and Marcu, 2003; Duverle and Prendinger, 2009)
needed manually annotated data. Marcu and Echi-
habi (2002) presented an unsupervised method to
recognize discourse relations held between arbitrary
spans of text. They showed that lexical pairs ex-
tracted from massive amount of data can have a
major impact on discourse classification. Blair-
Goldensohn et al (2007) extended Marcu's work by
using parameter opitimization, topic segmentation
and syntactic parsing. However, syntactic parsers
163
were usually costly and impractical when dealing
with large scale of text. Thus, in additional to lex-
ical features, we incorporated sequential and seman-
tic information in proposed method for discourse re-
lation classification. Moreover, our method kept the
characteristic of language independent, so it could be
applied to other languages.
3 Discourse Scheme for Eliminating
Polarity Ambiguities
Since not all of the discourse relations in RST
would help eliminate polarity ambiguities, the dis-
course scheme defined in this paper was on a much
coarser level. In order to ascertain which relations
should be included in our scheme, 500 ambigu-
ous sentences were randomly chosen from NTCIR
MOAT Chinese corpus and the most common dis-
course relations for connecting independent clauses
in compound sentences were annotated. We found
that 13 relations from RST occupied about 70% of
the annotated discourse relations which may help
eliminate polarity ambiguities. Inspired by Marcu
and Echihabi (2002), to construct relatively low-
noise discourse instances for unsupervised methods
using cue phrases, we grouped the 13 relations into
the following 5 relations:
Contrast is a union of Antithesis, Concession, Oth-
erwise and Contrast from RST.
Condition is selected from RST.
Continuation is a union of Continuation, Parallel
from RST.
Cause is a union of Evidence, Volitional-Cause,
Nonvolitional-Cause, Volitional-result and
Nonvolitional-result from RST.
Purpose is selected from RST.
The discourse constraints on polarity presented
here were based on the observation of annotated dis-
course instances: (1) discourse instances holding
Contrast relation should contain two segments with
opposite polarities; (2) discourse instances hold-
ing Continuation relation should contain two seg-
ments with the same polarity; (3) the polarity of dis-
course instances holdingContrast,Condition,Cause
or Purpose was determined by the nucleus segment;
(4) the polarity of discourse instances holding Con-
tinuation was determined by either segment.
Relation Cue Phrases(English Translation)
Contrast although1, but2, however2
Condition if1, (if1?then2)
Continuation and, further more,(not only, but also)
Cause because
1, thus2, accordingly2,
as a result2
Purpose in order to
2, in order that2,
so that2
1 means CUE1 and 2 means CUE2
Table 1: Examples of cue phrases
4 Methods
The proposed methods were based on two as-
sumptions: (1) Cue-phrase-based patterns could be
used to find limited number of high quality discourse
instances; (2) discourse relations were determined
by lexical, structural and semantic information be-
tween two segments.
Cue-phrase-based patterns could find only lim-
ited number of discourse instances with high pre-
cision (Marcu and Echihabi, 2002). Therefore, we
could not rely on cue-phrase-based patterns alone.
Moreover, there was no annotated corpus similar to
Penn Discourse TreeBank (Miltsakaki et al, 2004)
in other languages such as Chinese. Thus, we pro-
posed a language independent unsupervised method
to identify discourse relations without cue phrases
while maintaining relatively high precision. For
each discourse relation, we started with several cue-
phrase-based patterns and collected a large number
of discourse instances from raw corpus. Then, dis-
course instances were converted to semantic sequen-
tial representations (SSRs). Finally, an unsupervised
method was adopted to generate, weigh and filter
common SSRswithout cue phrases. Themined com-
mon SSRs could be directly used in our SSR-based
classifier in unsupervised manner or be employed as
effective features for supervised methods.
4.1 Gathering and representing discourse
instances
A discourse instance, denoted by Di, consists of
two successive segments (Di[1], Di[2]) within a sen-
tence. For example:
D1: [Although Boris is very brilliant at math]s, [he
164
BOS... ?[CUE2]...EOS
BOS [CUE1]... ?...EOS
BOS... ?[CUE1]...EOS
BOS [CUE1]... ?[CUE2]...EOS
Table 2: Cue-phrase-based patterns. BOS and EOS de-
noted the beginning and end of two segments.
is a horrible teacher]n
D2: [John is good at basketball]s, [but he lacks team
spirit]n
In D1, "although" indicated the satellite section
while inD2, "but" indicated the nucleus section. Ac-
cordingly, different cue phrases may indicate differ-
ent segment type. Table 1 listed some examples of
cue phrases for each discourse relation. Some cue
phrases were singleton (e.g. "although" and "as a re-
sult") and some were used as a pair (e.g. "not only,
but also"). "CUE1" indicated satellite segments and
"CUE2" indicated nucleus segments. Note that we
did not distinguish satellite from nucleus for Con-
tinuation in this paper because the polarity could be
determined by either segment.
Table 2 listed cue-phrase-based patterns for all re-
lations. To simplify the problem of discourse seg-
mentation, we split compound sentences into dis-
course segments using commas and semicolons. Al-
though we collected discourse instances from com-
pound sentences only, the number of instances for
each discourse relation was large enough for the pro-
posed unsupervised method. Note that we only col-
lected instances containing at least one sentiment
word in each segment.
In order to incorporate lexical and semantic infor-
mation in our method, we represented each word in
a discourse instance using a part-of-speech tag, a se-
mantic label and a sentiment tag. Then, all discourse
instances were converted to SSRs. The rules for con-
verting were as follows:
(1) Cue phrases and punctuations were ingored.
But the information of nucleus(n) and satellite(s)
was preserved.
(2) Adverbs(RB) appearing in sentiment lexicon,
verbs(V ), adjectives(JJ ) and nouns(NN) were repre-
sented by their part-of-speech (pos) tag with seman-
tic label (semlabel) if available.
(3) Named entities (NE; PER: person name;ORG:
organization), pronouns (PRP), and function words
were represented by their corresponding named en-
tity tags and part-of-speech tags, respectively.
(4) Added sentiment tag (P : Positive; N : Nega-
tive) to all sentiment words.
By applying above rules, the SSRs forD1 andD2
would be:
d1: [PERV|Ja01 RB|Ka01 JJ|Ee14|P IN NN|Dk03]s
, [PRP V|Ja01 DT JJ|Ga16|N NN|Ae13 ]n
d2: [PER V|Ja01 JJ|Ee14|P IN NN|Bp12]s, [PRP
V|He15|N NN|Di10 NN|Dd08 ]n
Refer to d1 and d2, "Boris" could match "John"
in SSRs because they were converted to "PER" and
they all appeared at the beginning of discourse in-
stances. "Ja01", "Ee14" etc. were semantic labels
from Chinese synonym list extended version (Che et
al., 2010). There were similar resources in other lan-
guages such asWordnet(Fellbaum, 1998) in English.
The next problem became how to start from current
SSRs and generate new SSRs for recognizing dis-
course relations without cue phrases.
4.2 Mining common SSRs
Recall assumption (2), in order to incorporate lex-
ical, structural and semantic information for the sim-
ilarity calculation of two SSRs holding the same
discourse relation, three types of matches were de-
fined for {(u, v)|u ? di[k], v ? dj[k], k = 1, 2}:
(1)Full match: (i) u = v or (ii) u.pos = v.pos and
u.semlabel=v.semlabel or (iii) u.pos=v.pos and
u had a sentiment tag and v had a sentiment tag or
(iv) u.pos and v.pos?{PRP, PER, ORG} (2) Partial
match: u.pos = v.pos but not Full match; (3) Mis-
match: u.pos ?= v.pos.
Generating common SSRs
Intuitively, a simple way of estimating the simi-
larity between two SSRs was using the number of
mismatches. Therefore, we utilized match(di, dj)
where i ?= j, which integrated the three types of
matches defined above to calculate the number of
mismatches and generate common SSRs. Consider
Table 3, in common SSRs, full matches were pre-
served, partial matches were replaced by part of
speech tags and mismatches were replaced by '*'s.
The common SSRs generated during the calculation
of match(di, dj) consisted of two parts. The first
part was generated by di[1] and dj[1] and the second
part was generated by di[2] and dj[2]. We stipulated
165
d1 d2 mis conf ssr
PER PER 0 0 PER
V|Ja01 V|Ja01 0 0 V|Ja01
RB|Ka01 +1 ?0.298 *
JJ|Ee14|P JJ|Ee14|P 0 0 JJ|Ee14|P
IN IN 0 0 IN
NN|Dk03 NN|Bp12 0 ?0.50 NN
conf(ssr[1]) = ?0.798
PRP PRP 0 0 PRP
V|Ja01 V|He15|N 0 ?0.50 V
DT +1 ?0.184 *
JJ|Ga16|N +1 ?1.0 *
NN|Ae13 NN|Di10 0 ?0.50 NN
NN|Dd08 +1 ?1.0 *
conf(ssr[2]) = ?3.184
Table 3: Calculation of match(d1, d2). ssr denoted
the common SSR between d1 and d2 , conf(ssr[1]) and
conf(ssr[2]) denoted the confidence of ssr.
that di and dj could generate a common SSR if and
only if the orders of nucleus segment and satellite
segment were the same.
In order to guarantee relatively high quality com-
mon SSRs, we empirically set the upper threshold
of the number of mismatches as 0.5 (i.e., ? 1/2 of
the number of words in the generated SSR). It's not
difficult to figure out that the number of mismatches
generated in Table 3 satisfied this requirement. As a
result, for each discourse relation rn, a correspond-
ing common SSR set Sn could be obtained by adopt-
ing match(di, dj) where i ?= j for all discourse in-
stances. An advantage of match(d1, d2) was that
the generated common SSRs preserved the sequen-
tial structure of original discourse instances. And
common SSRs allows us to build high precision dis-
course classifiers (See Section 5).
Weighing and filtering common SSRs
A problem of match(di, dj) was that it ignored
some important information by treating different
mismatches equally. For example, the adverb "very"
in "very brilliant" of D1 was not important for dis-
course recognition. In other words, the number of
mismatches inmatch(di, dj) could not precisely re-
flect the confidence of the generated common SSRs.
Therefore, it was needed to weigh different mis-
matches for the confidence calculation of common
SSRs.
Intuitively, if a partial match or a mismatch (de-
noted by um) occurred very frequently in the gener-
ation of common SSRs, the importance of um tends
to diminish. Inspired by the tf-idf model, given
ssri?Sn, we utilized the following equation to esti-
mate the weight (denoted by wm) of um.
wm = ?ufm ? log (|Sn|/ssrfm )
where ufm denoted the frequency of um during the
generation of ssri, |Sn| denoted the size of Sn and
ssrfm denoted the number of common SSRs in Sn
containing um . All weights were normalized to
[?1, 0).
Nouns (except for named entities) and verbs were
most representative words in discourse recognition
(Marcu and Echihabi, 2002). In addition, adjectives
and adverbs appearing in sentiment lexicons were
important for polarity classification. Therefore, for
these 4 kinds of words, we utilized ?1.0 for a mis-
match and ?0.50 for a partial match.
As we had got the weights for all partial matches
and mismatches, the confidence of ssri?Sn could be
calculated using the cumulation of weights of par-
tial matches and mismatches in ssri[1] and ssri[2].
Recall Table 3, conf(ssr[1]) and conf(ssr[2]) rep-
resented the confidence scores ofmatch(di[1], dj[1])
and match(di[2], dj[2]), respectively. In order to
control the quantity and quality of mined SSRs, a
threshold minconf was introduced. ssri will be
preserved if and only if conf(ssri[1]) ?minconf
and conf(ssri[2]) ? minconf . The value of
minconf was tuned using the development data.
Finally, we combined adjacent '*'s and preserved
SSRs containing at least one notional word and at
least two words in each segment to meet the de-
mand of maintaining high precision (e.g., "[* DT
*]", "[PER *]" will be dropped). Moreover, since
many of the SSRs were duplicated, we ranked all
the generated SSRs according to their occurrences
and dropped those appearing only once in order to
preserve common SSRs. At last, SSRs appearing in
more than one common SSR set were removed for
maintaining the uniqueness of each set. The com-
mon SSR set Sn for each discourse relation rn could
be directly used in SSR-based unsupervised classi-
fiers or be employed as effective features in super-
vised methods.
166
Relation Occurrence
Contrast 86 (8.2%)
Condition 27 (2.6%)
Continuation 445 (42.2%)
Cause 123 (11.7%)
Purpose 55 (5.2%)
Others 318 (30.2%)
Table 4: Distribution of discourse relations on NTC-7.
Others represents discourse relations not included in our
discourse scheme.
5 Experiments
5.1 Annotation work and Data
We extracted all compound sentences which may
contain the defined discourse relations from opinion-
ated sentences (neutral ones were dropped) of NT-
CIR7MOAT simplified Chinese training data. 1,225
discourse instances were extracted and two annota-
tors were trained to annotate discourse relations ac-
cording to the discourse scheme defined in Section 3.
Note that we annotate both explicit and implicit dis-
course relations. The overall inter annotator agree-
ment was 86.05% and the Kappa-value was 0.8031.
Table 4 showed the distribution of annotated dis-
course relations based on the inter-annotator agree-
ment. The proportion of occurrences of each dis-
course relations varied greatly. For example, Con-
tinuation was the most common relation in anno-
tated corpus, but the occurrences of Condition rela-
tion were rare.
The experiments of this paper were performed us-
ing the following data sets:
NTC-7 contained manually annotated discourse
instances (shown in Table 4). The experiments of
discourse identification were performed on this data
set.
NTC-8 contained all opinionated sentences (neu-
tral ones were dropped) extracted from NTCIR8
MOAT simplified Chinese test data. The experi-
ments of polarity ambiguity elimination using the
identified discourse relations were performed on this
data set.
XINHUA contained simplified Chinese raw news
text from Xinhua.com (2002-2005). A word seg-
mentation tool, a part-of-speech tagging tool, a
named entity recognizer and a word sense disam-
biguation tool (Che et al, 2010) were adopted to all
sentences. The common SSRs were mined from this
data set.
5.2 Experimental Settings
Discourse relation identification
In order to systematically justify the effectiveness
of proposed unsupervised method, following exper-
iments were performed on NTC-7:
Baseline used only cue-phrase-based patterns.
M&E proposed by Marcu and Echihabi (2002).
Given a discourse instance Di, the probabilities:
P (rk|(Di[1], Di[2])) for each relation rk were esti-
mated on all text from XINHUA. Then, the most
likely discourse relation was determined by taking
the maximum over argmaxk{P (rk|(Di[1], Di[2])}.
cSSR used both cue-phrase-based patterns to-
gether with common SSRs for recognizing discourse
relations. Common SSRs were mined from dis-
course instances extracted fromXINHUAusing cue-
phrase-based patterns. Development data were ran-
domly selected for tuning minconf .
SVM was trained utilizing cue phrases, probabil-
ities from M&E, topic similarity, structure overlap,
polarity of segments and mined common SSRs (Op-
tional). The parameters of the SVM classifier were
set by a grid search on the training set. We performed
4-fold cross validation on NTC-7 to get an average
performance.
The purposes of introducing SVM in our experi-
ment were: (1) to compare the performance of cSSR
to supervised method; (2) to examine the effective-
ness of integrating common SSRs as features for su-
pervised methods.
Polarity ambiguity elimination
BPC was trained mainly utilizing punctuation,
uni-gram, bi-gram features with confidence score
output. Discourse classifiers such as Baseline, cSSR
or SVM were adopted individually for the post-
processing of BPC. Given an ambiguous sentence
which contained more than one segment, an intuitive
three-step method was adopted to integrated a dis-
course classifier and discourse constraints on polar-
ity for the post-processing of BPC:
(1) Recognize all discourse relations together with
nucleus and satellite information using a discourse
classifier. The nucleus and satellite information is
167
Figure 2: Influences of different values of minconf to
the performance of cSSR
acquired by cSSR if a segment pair could match a
cSSR. Otherwise, we use the annotated nucleus and
satellite information.
(2) Apply discourse constraints on polarity to
ascertain the polarity for each discourse instance.
There may be conflicts between polarities acquired
by BPC and discourse constraints on polarity (e.g.,
Two segments with the same polarity holding a Con-
trast relation). To handle this problem, we chose
the segment with higher polarity confidence and ad-
justed the polarity of the other segment using dis-
course constraints on polarity.
(3) If there was more than one discourse instance
in a single sentence, the overall polarity of the sen-
tence was determined by voting of polarities from
each discourse instance under the majority rule.
5.3 Experimental Results
Refer to Figure 2, the performance of cSSR was
significantly affected by minconf . Note that we
performed the tuning process ofminconf on differ-
ent development data (1/4 instances randomly se-
lected from NTC-7) and Figure 2 showed the av-
erage performance. cSSR became Baseline when
minconf =0. A significant drop of precision was
observed when minconf was less than ?2.5. The
recall remained around 0.495 when minconf ?
?4.0. The best performance was observed when
minconf=?3.5. As a result, ?3.5 was utilized as
the threshold value for cSSR in the following exper-
iments.
Table 5 presented the experimental results for dis-
course relation classification. it showed that:
(1) Cue-phrase-based patterns could find only lim-
ited number of discourse relations (34.1% of average
BPC Baseline cSSR SVM+SSRs
Precision 0.7661 0.7982 0.8059 0.8113
Recall 0.7634 0.7957 0.8038 0.8091
F-score 0.7648 0.7970 0.8048 0.8102
Table 6: Performance of integrating discourse classifiers
and constraints to polarity classification. Note that the
experiments were performed on NTC-8 which contained
only opinionated sentences.
recall) with a very high precision (96.17% of average
precision). This is a proof of assumption (1) given
in Section 4. On the other side, M&E which only
considered word pairs between two segments of dis-
course instances got a higher recall with a large drop
of precision. The drop of precision may be caused
by the neglect of structural and semantic information
of discourse instances. However, M&E still outper-
formed Baseline in average F -score.
(2) cSSR enhanced Baseline by increasing the av-
erage recall by about 15% with only a small drop of
precision. The performance of cSSR demonstrated
that our method could effectively discover high qual-
ity common SSRs. The most remarkable improve-
ment was observed on Continuation in which the re-
call increased by almost 20% with only a minor drop
of precision. Actually, cSSR outperformed Baseline
in all discourse relations except forContrast. In Dis-
course Tree Bank (Carlson et al, 2001) only 26%
of Contrast relations were indicated by cue phrases
while in NTC-7 about 70% of Contrast were indi-
cated by cue phrases. A possible reason was that
we were dealing with Chinese news text which were
usually well written. Another important observation
was that the performance of cSSR was very close to
the result of SVM.
(3) SVM+SSRs achieved the best F -score on
Continuation and average performance. The integra-
tion of SSRs to the feature set of SVM contributed to
a remarkable increase in average F -score. The re-
sults of cSSR and SVM+SSRs demonstrated the ef-
fectiveness of common SSRs mined by the proposed
unsupervised method.
Table 6 presented the performance of integrat-
ing discourse classifiers to polarity classification.
For Baseline and cSSR, the information of nucleus
and satellite could be obtained directly from cue-
168
Relation Baseline M&E cSSR SVM SVM+SSRs
Contrast
P 0.9375 0.4527 0.7531 0.9375 0.9375
R 0.6977 0.7791 0.7093 0.6977 0.6977
F 0.8000 0.5726 0.7305 0.8000 0.8000
Condition
P 1.0000 0.4444 0.6774 1.0000 0.7083
R 0.5556 0.8889 0.7778 0.5185 0.6296
F 0.7143 0.5926 0.7241 0.6829 0.6667
Continuation
P 0.9831 0.6028 0.9761 0.6507 0.7266
R 0.2607 0.5865 0.4584 0.6697 0.6629
F 0.4120 0.5945 0.6239 0.6600 0.6933
Cause
P 1.0000 0.5542 0.9429 1.0000 0.9412
R 0.2114 0.3740 0.2683 0.2114 0.2602
F 0.3489 0.4466 0.4177 0.3489 0.4076
Purpose
P 0.8947 0.3704 0.8163 0.9167 0.7193
R 0.6182 0.7273 0.7273 0.6000 0.7455
F 0.7312 0.4908 0.7692 0.7253 0.7321
Average
P 0.9617 0.5302 0.8864 0.7207 0.7607
R 0.3410 0.5951 0.4878 0.5856 0.6046
F 0.5035 0.5608 0.6293 0.6461 0.6737
Table 5: Performance of recognizing discourse relations. (The evaluation criteria are Precision, Recall and F-score)
phrase-based patterns and SSRs, respectively. For
SVM+cSSR, the nucleus and satellite information
was acquired by cSSR if a segment pair could match
a cSSR. Otherwise, we used manually annotated nu-
cleus and satellite information. It's clear that the
performance of polarity classification was enhanced
with the improvement of discourse relation recogni-
tion. M&E was not included in this experiment be-
cause the performance of polarity classification was
decreased by the mis-classified discourse relations.
SVM+SSRs achieved significant (p<0.01) improve-
ment in polarity classification compared to BPC.
5.4 Discussion
Effect of weighing and filtering
To assess the contribution of weighing and filter-
ing in mining SSRs using a minimum confidence
threshold, i.e. minconf , we implemented cSSR?
without weighing and filtering on the same data set.
Consider Table 7, cSSR achieved obvious improve-
ment in Precision and F -score than cSSR?. More-
over, the total number of SSRs was greatly reduced
in cSSR with only a minor drop of recall. This was
because cSSR? was affected by thousands of low
quality common SSRs which would be filtered in
cSSR. The result in Table 7 proved that weighing and
cSSR? cSSR
Precision 0.6182 0.8864
Recall 0.5014 0.4878
F-score 0.5537 0.6293
NOS > 1 million ? 0.12 million
Table 7: Comparison of cSSR? and cSSR. "NOS" denoted
the number of mined common SSRs.
filtering were essential in our proposed method.
We further analyzed how the improvement was
achieved in cSSR. In our experiment, the most com-
mon mismatches were auxiliary words, named enti-
ties, adjectives or adverbs without sentiments (e.g.,
"green", "very", etc.), prepositions, numbers and
quantifiers. It's straightforward that these words
were insignificant in discourse relation classification
purpose. Moreover, these words did not belong to
the 4 kinds of most representative words. In other
words, the weights of most mismatches were calcu-
lated using the equation presented in Section 4.2 in-
stead of utilizing a unified value, i.e. ?1. Recall
Table 3, the weight of "RB|Ka01" (original: "very")
was ?0.298 and "DT" (original: 'a') was ?0.184.
Comparing to the weights of mismatches for most
representative words (?1.0), the proposed method
successfully down weighed the words which were
169
Figure 3: Improvement from individual discourse rela-
tions. N denoted the number of ambiguities eliminated.
not important for discourse identification. There-
fore, weighing and filtering were able to preserve
high quality SSRs while filter out low quality SSRs
by setting the confidence threshold, i.e. minconf .
Contribution of different discourse relations
We also analyzed the contribution of different dis-
course relations in eliminating polarity ambiguities.
Refer to Figure 3, the improvement of polarity classi-
fication mainly came from three discourse relations:
Contrast, Continuation and Cause. It was straight-
forward that Contrast relation could eliminate po-
larity ambiguities because it held between two seg-
ments with opposite polarities. The contribution of
Cause relation also result from two segments holding
different polarities such as example (a) in Section 1.
However, recall Table 4, although Cause occurred
more often than Contrast, only a part of discourse
instances holding Cause relation contained two seg-
ments with the opposite polarities. Another impor-
tant relation in eliminating ambiguity was Continu-
ation. We investigated sentences with polarities cor-
rected by Continuation relation. Most of them fell
into two categories: (1) sentences with mistakenly
classified sentiments by BPC; (2) sentences with im-
plicit sentiments. For example:
(b) [France and Germany have banned human cloning at
present]?[on 20th, U.S. President George W. Bush called
for regulations of the same content to Congress] (???
????????????????????? 20
???????????????????)
The first segment of example (b) was negative
("banned" expressed a negative sentiment) and a
Continuation relation held between these two seg-
ments. Consequently, the polarity of the second seg-
ment should be negative.
6 Conclusions and Future work
This paper focused on unsupervised discovery
of intra-sentence discourse relations for sentence
level polarity classification. We firstly presented a
discourse scheme based on empirical observations.
Then, an unsupervised method was proposed start-
ing from a small set of cue-phrase-based patterns to
mine high quality common SSRs for each discourse
relation. The performance of discourse classification
was further improved by employing SSRs as features
in supervisedmethods. Experimental results showed
that our methods not only effectively recognized dis-
course relations but also achieved significant im-
provement (p<0.01) in sentence level polarity clas-
sification. Although we were dealing with Chinese
text, the proposed unsupervised method could be
easily generalized to other languages.
The future work will be focused on (1) integrating
more semantic and syntactic information in proposed
unsupervised method; (2) extending our method to
inter-sentence level and then jointly modeling intra-
sentence level and inter-sentence level discourse
constraints on polarity to reach a global optimal in-
ference for polarity classification.
Acknowledgments
This work is partially supported by National 863
program of China (Grant No. 2009AA01Z150),
the Innovation and Technology Fund of Hong Kong
SAR (Project No. GHP/036/09SZ) and 2010/11
CUHK Direct Grants (Project No. EE09743).
References
N. Asher, F. Benamara, and Y.Y. Mathieu. 2008. Distill-
ing opinion in discourse: A preliminary study. Coling
2008: Companion volume: Posters and Demonstra-
tions, pages 5--8.
S. Blair-Goldensohn, K.R. McKeown, and O.C. Ram-
bow. 2007. Building and refining rhetorical-semantic
relationmodels. InProceedings of NAACLHLT, pages
428--435.
L. Carlson, D.Marcu, andM.E. Okurowski. 2001. Build-
ing a discourse-tagged corpus in the framework of
170
rhetorical structure theory. In Proceedings of the Sec-
ond SIGdial Workshop on Discourse and Dialogue-
Volume 16, pages 1--10. Association for Computa-
tional Linguistics.
W. Che, Z. Li, and T. Liu. 2010. Ltp: A chinese language
technology platform. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Demonstrations, pages 13--16. Association for Com-
putational Linguistics.
D.A. Duverle and H. Prendinger. 2009. A novel dis-
course parser based on support vector machine classi-
fication. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2, pages 665--673. Associ-
ation for Computational Linguistics.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168--177. ACM.
W.C. Mann and S.A. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text organi-
zation. Text-Interdisciplinary Journal for the Study of
Discourse, 8(3):243--281.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 368--375. Associa-
tion for Computational Linguistics.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004.
The penn discourse treebank. InProceedings of the 4th
International Conference on Language Resources and
Evaluation. Citeseer.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing-
Volume 10, pages 79--86. Association for Computa-
tional Linguistics.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: The-
ory and applications, pages 1--10.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning sub-
jective nouns using extraction pattern bootstrapping.
In Proceedings of the seventh conference on Natu-
ral language learning at HLT-NAACL 2003-Volume 4,
pages 25--32. Association for Computational Linguis-
tics.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008. Sen-
timent analysis based on probabilistic models using
inter-sentence information.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings of
NAACL HLT, pages 300--307.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 801--808. Association for
Computational Linguistics.
S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor.
2009. Supervised and unsupervised methods in em-
ploying discourse relations for improving opinion po-
larity classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 170--179. Association for Compu-
tational Linguistics.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 149--156. Association for Computational Lin-
guistics.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399--433.
Y.Q. Xia, R.F. Xu, K.F. Wong, and F. Zheng. 2007. The
unified collocation framework for opinion mining. In
International Conference on Machine Learning and
Cybernetics, volume 2, pages 844--850. IEEE.
R. Xu and C. Kit. 2010. Incorporating feature-based and
similarity-based opinion mining--ctl in ntcir-8 moat.
InProceedings of the 8th NTCIRWorkshop, pages 276-
-281.
171
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 897?902,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Is Twitter A Better Corpus for Measuring Sentiment Similarity?
Shi Feng1, Le Zhang1, Binyang Li2,3, Daling Wang1, Ge Yu1, Kam-Fai Wong3
1Northeastern University, Shenyang, China
2University of International Relations, Beijing, China
3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
{fengshi,wangdaling,yuge}@ise.neu.edu.cn, zhang777le@gmail.com
{byli,kfwong}@se.cuhk.edu.hk
Abstract
Extensive experiments have validated the ef-
fectiveness of the corpus-based method for
classifying the word?s sentiment polarity.
However, no work is done for comparing d-
ifferent corpora in the polarity classification
task. Nowadays, Twitter has aggregated huge
amount of data that are full of people?s senti-
ments. In this paper, we empirically evaluate
the performance of different corpora in sen-
timent similarity measurement, which is the
fundamental task for word polarity classifica-
tion. Experiment results show that the Twitter
data can achieve a much better performance
than the Google, Web1T and Wikipedia based
methods.
1 Introduction
Measuring semantic similarity for words and short
texts has long been a fundamental problem for many
applications such as word sense disambiguation,
query expansion, search advertising and so on.
Determining the word?s polarity plays a critical
role in opinion mining and sentiment analysis task.
Usually we can detect the word?s polarity by mea-
suring it?s semantic similarity with a positive seed
word sep and a negative seed word sen respectively,
as shown in Formula (1):
SO(w) = sim(w, sep)? sim(w, sen) (1)
where sim(wi, wj) is the semantic similarity mea-
surement method for the given word wi and wj . A
lot of papers have been published for designing ap-
propriate similarity measurements. One direction is
to learn similarity from the knowledge base or con-
cept taxonomy (Lin, 1998; Resnik, 1999). Anoth-
er direction is to learn semantic similarity with the
help of large corpus such as Web or Wikipedia da-
ta (Sahami and Heilman, 2006; Yih and Meek, 2007;
Bollegala et al, 2011; Gabrilovich and Markovitch,
2007). The basic assumption of this kind of methods
is that the word with similar semantic meanings of-
ten co-occur in the given corpus. Extensive experi-
ments have validated the effectiveness of the corpus-
based method in polarity classification task (Turney,
2002; Kaji and Kitsuregawa, 2007; Velikovich et al,
2010). For example, PMI is a well-known similari-
ty measurement (Turney, 2002), which makes use of
the whole Web as the corpus, and utilizes the search
engine hits number to estimate the co-occurrence
probability of the give word pairs. The PMI based
method has achieved promising results. However,
according to Kanayama?s investigation, only 60%
co-occurrences in the same window in Web pages
reflect the same sentiment orientation (Kanayama
and Nasukawa, 2006). Therefore, we may ask the
question whether the choosing of corpus can change
the performance of sim and is there any better cor-
pus than the Web page data for measuring the senti-
ment similarity?
Everyday, enormous numbers of tweets that con-
tain people?s rich sentiments are published in Twit-
ter. The Twitter may be a good source for measuring
the sentiment similarity. Compared with the Web
page data, the tweets have a higher rate of subjective
text posts. The length limitation can guarantee the
polarity consistency of each tweet. Moreover, the
tweets contain graphical emoticons, which can be
897
considered as natural sentiment labels for the corre-
sponding tweets in Twitter. In this paper, we attempt
to empirically evaluate the performance of differen-
t corpora in sentiment similarity measurement task.
As far as we know, no work is done on this topic.
2 The Characteristics of Twitter Data
As the world?s second largest SNS website, at the
end of 2012 Twitter had aggregated more than 500
million registered users, among which 200 million
were active users . More than 400 million tweets are
posted every day.
Several examples of typical posts from Twitter are
shown below.
(1) She had a headache and feeling light headed
with no energy. :(
(2) @username Nice work! Looks like you had a
fun day. I?m headed there Sat or Sun. :)
(3) I seen the movie on Direc Tv. I ordered it and
I really liked it. I can?t wait to get it for blu ray!
Excellent work Rob!
We observe that comparing with the other corpus,
the Twitter data has several advantages in measuring
the sentiment similarity.
Large. Users like to record their personal feelings
and talk about the trend topics in Twitter (Java et al,
2007; Kwak et al, 2010). So there are huge amount
of subjective texts with various topics generated in
the millions of tweets everyday. Further more, the
flexible Twitter API makes these data easy to access
and collect.
Length Limitation. Twitter has a length limita-
tion of 140 characters. Users have limited space to
express their feelings. So the sentiments in tweet-
s are usually concise, straightforward and polarity
consistent.
Emoticons. Users tend to utilize emoticons to
emphasize their sentiment feelings. According to
the statistics, about 8.1% tweets contain at least one
emoticon (Yang and Leskovec, 2011). Since the
tweets have the length limitation, the sentiments ex-
pressed in these short texts are usually consistent
with the embedded emoticons, such as the word fun
and headache in above examples.
In addition to the above advantages, there are al-
so some disadvantages for measuring sentiment sim-
ilarity using Twitter data. The spam tweets that
caused by advertisements may add noise and bias
during the similarity measurement. The short length
may also bring in lower co-occurrence probability
of words. Some words may not co-occur with each
other when the corpus is small. These disadvantages
set obstacles for measuring sentiment similarity by
using Twitter data as corpus. In the experiment sec-
tion, we will see if we can overcome these draw-
backs and get benefit from the advantages of Twitter
data.
3 The Corpus-based Sentiment Similarity
Measurements
The intuition behind the corpus-based semantic sim-
ilarity measuring method is that the words with sim-
ilar meanings tend to co-occur in the corpus. Given
the word wi, wj , we use the notation P (wi) to de-
note the occurrence counts of word wi in the corpus
C. P (wi, wj) denotes the co-occurrence counts of
word wi and wj in C. In this paper we employ the
corpus-based version of the three well-known simi-
larity measurements: Jaccard, Dice and PMI.
CorpusJaccard(wi, wj)
= P (wi,wj)P (wi)+P (wj)?P (wi,wj)
(2)
CorpusDice(wi, wj) =
2? P (wi, wj)
P (wi) + P (wj)
(3)
CorpusPMI(wi, wj) = log2(
P (wi,wj)
N
P (wi)
N
P (wj)
N
) (4)
In Formula (4), N is the number of documents in
the corpus C. The above similarity measurements
may have their own strengths and weaknesses. In
this paper, we utilize these classical measurements
to evaluate the quality of the corpus in polarity clas-
sification task.
Google is the world?s largest search engine, which
has indexed a huge number of Web pages. Us-
ing the extreme large indexed Web pages as cor-
pus, Cilibrasi and Vitanyi (2007) presented a method
for measuring similarity between words and phras-
es based on information distance and Kolmogorov
complexity. The search result page counts of Google
were utilized to estimate the occurrence frequencies
of the words in the corpus. Suppose wi, wj rep-
resent the candidate words, the Normalized Google
898
Distance is defined as:
NGD(wi, wj) =
max{logP (wi),logP (wj)}?logP (wi,wj)
logN?min{logP (wi),logP (wj)}
(5)
where P (wi) denotes page counts returned by
Google using wi as keyword; P (wi, wj) denotes the
page counts by using wi and wj as joint keywords;
N is the number of Web pages indexed by Google.
Cilibrasi and Vitanyi have validated the effective-
ness of Google distance in measuring the semantic
similarity between concept words.
Based on the above formulas, we compare the
Twitter data with the Web and Wikipedia data as the
similarity measurement corpus. Given a candidate
word w, we firstly measure its sentiment similar-
ity with a positive seed word and a negative seed
word respectively in Formula (1), and the difference
of sim is used to further detect the polarity of w.
The above four similarity measurements serve as
sim with Web, Wikipedia and Twitter data as cor-
pus. Turney (2002) chose excellent and poor as
seed words. However, using isolated seed word-
s may cause the bias problem. Therefore, we fur-
ther select two groups of seed words that are lack
of sensitivity to context and form a positive seed set
PS and a negative seed set NS (Turney, 2003). The
Formula (1) can be rewritten as:
SO(w) =
?
sep?PS
sim(w, sep)?
?
sen?NS
sim(w, sen) (6)
Based on the Formula(6) and the sentiment seed
words, we can measure the sentiment polarity of the
given candidate words.
4 Experiment
4.1 Experiment Setup
Corpus Preparing. The Twitter corpus corre-
sponds to the 476 million Twitter tweets (Yang and
Leskovec, 2011), which includes over 476 million
Twitter posts from 20 million users, covering a 7
month period from June 1, 2009 to December 31,
2009. We filter out the non-English tweets and the
spam tweets that have only few words with URLs.
The tweets that contain three or more trending topics
are also removed. Finally, we construct the Twitter
corpus that consists of 266.8 million English tweets.
For calculating page counts in Web data, the candi-
date words were launched to Google from February
2013 to April 2013. We also conduct the experi-
ments on the Google Web 1T data that consists of
Google n-gram counts (frequency of occurrence of
each n-gram) for 1 ? n ? 5 (Brants and Franz,
2006). The Web 1T data provides a nice approxi-
mation to the word co-occurrence statistics in Web
pages in a predefined window size (1 ? n ? 5).
For example, the 5 gram Web1T data means the co-
occurrence window size is 5. The English Wikipedia
dump 1 we used was extracted at the end of March
2013, which contained more than 13 million articles.
We extracted the plain texts of the Wikipedia data as
the training corpus for the Formula (6).
EvaluationMethod. Two well-know sentiment lex-
icons are utilized as gold standard for polarity clas-
sification task. The statistics of Liu?s sentiment lex-
icon (Liu et al, 2005) and MPQA subjectivity lexi-
con (Wilson et al, 2005) are shown in Table 1. For
each word w in the lexicons, we employ the Formu-
la (6) to calculate the word?s polarity using different
corpora. If SO(w) > 0, the word w is classified in-
to the positive category. Otherwise if SO(w) < 0, it
is classified into the negative category. The accura-
cy of the classification result is used to measure the
quality of the corpus.
Positive# Negative#
Liu 2,006 4,783
MPQA 2,304 4,153
Table 1: Lexicon size
4.2 Experiment Results
Firstly, we chose the seed words excellent and poor
as Turney?s (2002) settings. The polarity classifica-
tion accuracies are shown in Table 2.
In Table 2, Google, Web1T, Wikipedia, Twitter
represent the corpora that used in the experiment;
CJ, CD, CP, GD represent the Formula (2) to For-
mula (5) respectively. We can see from the Table 2
that the Twitter based method can achieve the best
performance. The rich sentiment information and
1http://en.wikipedia.org/
899
Lexicon Corpus CJ CD CP GD
Liu
Google 0.5116 0.5117 0.5064 0.5076
Web1T-5gram 0.3903 0.3903 0.3897 0.3864
Web1T-4gram 0.3771 0.3771 0.3772 0.3227
Wikipedia 0.5280 0.5280 0.5350 0.5412
Twitter 0.5567 0.5567 0.5635 0.5635
MPQA
Google 0.4897 0.4890 0.4891 0.4864
Web1T-5gram 0.3843 0.3843 0.3837 0.3783
Web1T-4gram 0.3729 0.3729 0.3714 0.3225
Wikipedia 0.5181 0.5181 0.5380 0.5344
Twitter 0.5421 0.5421 0.5493 0.5494
Table 2: Polarity classification accuracies using excellent
and poor as seed words
natural window size (140 characters) have a posi-
tive impact on determining the word?s polarity. The
Google based method gets a lower accuracy, this
may be due to the length of Web documents which
can not usually guarantee the semantic consistency
in the returned data. Even though two words appear
in one page (returned by Google), they might not be
semantically related. Furthermore, the Google based
method is time-consuming, because we have to peri-
odically send queries in order to avoid being blocked
by Google. The Web1T based method gets a much
worse accuracy. After detailed analysis, we find that
although the small window size (4 or 5) can guar-
antee the semantic consistency, the short length also
brings in lower co-occurrence probability. Statistics
show that about 38% SO values are zero when using
Web1T corpus. Due to the short length, the Twitter
data also suffers from the low co-occurrence prob-
lem.
To tackle the low co-occurrence problem, the seed
word sets are selected as Turney?s (2003) settings.
The positive word set PS={good, nice, excellent,
positive, fortunate, correct, superior} and negative
word set NS = {bad, nasty, poor, negative, unfortu-
nate, wrong, inferior} for the Formula (6). These
seed words have been verified to be effective in Tur-
ney?s paper for polarity classification. The experi-
ment results are shown in Table 3.
Table 3 shows that the performance of Twitter cor-
pus is much improved since the multiple seed words
alleviate the problem of low co-occurrence probabil-
ity in tweets. Generally, when using the seed word
groups the Twitter can achieve a much better per-
formance than all the other corpora. The improve-
ments are statistically significant (p-value < 0.05).
Lexicon Corpus CJ CD CP GD
Liu
Google 0.4859 0.4936 0.4884 0.5060
Web1T-5gram 0.5785 0.5785 0.3963 0.5782
Web1T-4gram 0.5766 0.5766 0.3872 0.5775
Wikipedia 0.6226 0.6225 0.5957 0.6145
Twitter 0.6678 0.6678 0.6917 0.6457
Twitter+ 0.6921 0.6921 0.7273 0.6599
MPQA
Google 0.5108 0.5225 0.5735 0.5763
Web1T-5gram 0.5737 0.5737 0.4225 0.5718
Web1T-4gram 0.5749 0.5749 0.3329 0.4797
Wikipedia 0.6086 0.6085 0.5773 0.5985
Twitter 0.6431 0.6431 0.6671 0.6253
Twitter+ 0.6665 0.6665 0.7001 0.6383
Table 3: Polarity classification accuracies using the seed
word groups
We further add the emoticons ?:)? and ?:(? into the
seed word groups, denoted by Twitter+ in Table 3.
The emoticons are natural sentiment labels. We can
see that the performances are further improved by
considering emoticons as seed words. The above
experiment results have validated the effectiveness
of Twitter data as a better corpus for measuring the
sentiment similarity. The results also reveal the po-
tential usefulness of Twitter corpus in semantic sim-
ilarity measurement.
5 Related Work
Detecting the polarity of words is the fundamental
problem for most of sentiment analysis tasks (Hatzi-
vassiloglou and McKeown, 1997; Pang and Lee,
2007; Feldman, 2013).
Many methods have been proposed to measure
the words? or short texts similarity based on large
corpus (Sahami and Heilman, 2006; Yih and Meek,
2007; Gabrilovich and Markovitch, 2007). Bolle-
gala et al (2011) submitted the word to the search
engine, and the related result pages were employed
to represent the meaning of the original word. Mi-
halcea et al (2006) proposed a method to measure
the semantic similarity of words or short texts, con-
sidering both corpus-based and knowledge-based in-
formation. Although the previous algorithms have
achieved promising results, there are no work done
on evaluating the quality of different corpora.
Mohtarami et al (2012; 2013a; 2013b) intro-
duced the concept of sentiment similarity, which
was considered as different from the traditional se-
mantic similarity, and more focused on revealing the
underlying sentiment relations between words. Mo-
900
htarami et al (2013b) proposed a hidden emotion-
al model to calculating the sentiment similarity of
word pairs. However, the impact of the different cor-
pora is not considered for this task.
Mohammad et al (2013) generated word-
sentiment association lexicons from Tweets with the
help of hashtags and emoticons. Pak and Paroubek
(2010) collected tweets with happy and sad emoti-
cons as training corpus, and built sentiment classi-
fier based on traditional machine learning methods.
Brody and Diakopoulos (2011) showed that length-
ening was strongly associated with subjectivity and
sentiment in tweets. Davidov et al (2010) treated 50
Twitter tags and 15 smileys as sentiment labels and
a supervised sentiment classification framework was
proposed to classify the tweets. The previous litera-
tures have showed that the emoticons can be treated
as natural sentiment labels of the tweets.
6 Conclusion and Future Work
The quality of corpus may affect the performance
of sentiment similarity measurement. In this pa-
per, we compare the Twitter data with the Google,
Web1T and Wikipedia data in polarity classification
task. The experiment results validate that when us-
ing the seed word groups the Twitter can achieve a
much better performance than the other corpora and
adding emoticons as seed words can further improve
the performance. It is observed that the twitter cor-
pus is a potential good source for measuring senti-
ment similarity between words. In future work, we
intend to design new similarity measurements that
can make best of the advantages of Twitter data.
Acknowledgments
This research is partially supported by Gener-
al Research Fund of Hong Kong (No. 417112)
and Shenzhen Fundamental Research Program (J-
CYJ20130401172046450). This research is al-
so supported by the State Key Development Pro-
gram for Basic Research of China (Grant No.
2011CB302200-G), State Key Program of Nation-
al Natural Science of China (Grant No. 61033007),
National Natural Science Foundation of China
(Grant No. 61370074, 61100026), and the Funda-
mental Research Funds for the Central Universities
(N120404007).
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using
Word Lengthening to Detect Sentiment in Microblogs.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
562?570, Edinburgh, UK, ACL.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizu-
ka. 2011. A Web Search Engine-Based Approach to
Measure Semantic Similarity between Words. IEEE
Transactions on Knowledge and Data Engineering,
23(7): 977?990.
Rudi Cilibrasi and Paul Vitanyi. 2007. The Google Simi-
larity Distance. IEEE Transactions on Knowledge and
Data Engineering, 19(3): 370?383.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hashtags
and Smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages 241?
249, Beijing, China, ACL.
Ronen Feldman. 2013. Techniques and Applications for
Sentiment Analysis. Communications of the ACM,
56(4):82?89.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611, Hyderabad, India.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics, pages
174?181, Madrid, Spain, ACL.
Akshay Java, Xiaodan Song, Tim Finin, and Belle L. T-
seng. 2007. Why We Twitter: An Analysis of a Mi-
croblogging Community. In Proceedings of the 9th
International Workshop on Knowledge Discovery on
the Web and 1st International Workshop on Social Net-
works Analysis, pages 118?138, San Jose, CA, USA,
Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. Building Lex-
icon for Sentiment Analysis from Massive Collec-
tion of HTML Documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pp. 1075?1083, Prague, Czech
Republic, ACL.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Ful-
ly Automatic Lexicon Expansion for Domain-oriented
901
Sentiment Analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 355?363, Sydney, Australia, ACL.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
B. Moon. 2010. What is Twitter, a Social Network or a
News Media? In Proceedings of the 19th Internation-
al Conference on World Wide Web, pages 591?600,
Raleigh, North Carolina, USA, ACM.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics,
pages 768?774, Montreal, Quebec, Canada, ACL.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: Analyzing and Comparing Opin-
ions on the Web. In Proceedings of the 14th interna-
tional conference on World Wide Web, pages 342?351,
Chiba, Japan, ACM.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence and the
18th Innovative Applications of Artificial Intelligence
Conference, pages 775?780, Boston, Massachusetts,
USA, AAAI Press.
Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh Phu
Tran, and Chew Lim Tan. 2012. Sense Sentimen-
t Similarity: An Analysis. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence, pages 1706?1712, Toronto, Ontario, Canada,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013a.
From Semantic to Emotional Space in Probabilistic
Sense Sentiment Analysis. In Proceedings of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 711?717, Bellevue, Washington, USA,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013b.
Probabilistic Sense Sentiment Similarity through Hid-
den Emotions. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics, pages 983?992, Sofia, Bulgaria, ACL.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-the-
Art in Sentiment Analysis of Tweets. In Proceedings
of the seventh international workshop on Semantic E-
valuation Exercises, Atlanta, Georgia, USA, ACL.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
Corpus for Sentiment Analysis and Opinion Mining.
In Proceedings of the 2010 International Conference
on Language Resources and Evaluation, pages 1320?
1326, Valletta, Malta, ELRA.
Philip Resnik. 1999. Semantic Similarity in a Taxonomy:
An Information based Measure and Its Application to
Problems of Ambiguity in Natural Language. Journal
of Artificial Intelligence Research, 11:95?130.
Mehran Sahami and Timothy D. Heilman. 2006. A Web-
based Kernel Function for Measuring the Similarity of
Short Text Snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, pages 377?
386, Edinburgh, Scotland, UK, ACM.
Bo Pang and Lillian Lee. 2007. Opinion Mining and Sen-
timent Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Peter D. Turney. 2002. Thumbs Up or Thumbs Down?
Semantic Orientation Applied to Unsupervised Classi-
fication of Reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 417?424, Philadelphia, PA, USA, ACL.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transaction Information Sys-
tem, 21(4): 315?346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan T. McDonald. The Viability of Web-
derived Polarity Lexicons. In Proceedings of the 2010
North American Chapter of the Association of Compu-
tational Linguistics, pp. 777?785, Los Angeles, Cali-
fornia, USA, ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman-
n. 2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 347?354, Vancouver, British Columbi-
a, Canada, ACL.
Jaewon Yang and Jure Leskovec. 2011. Patterns of Tem-
poral Variation in Online Media. In Proceedings of
the Forth International Conference on Web Search and
Web Data Mining, pages 177?186, Hong Kong, Chi-
na, ACM.
Wen-tau Yih and Christopher Meek. 2007. Improving
Similarity Measures for Short Segments of Text. In
Proceedings of the 22nd AAAI Conference on Artificial
Intelligence, pages 1489?1494, Vancouver, British
Columbia, Canada, AAAI Press.
902
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1159?1168,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploiting Community Emotion for Microblog Event Detection
Gaoyan Ou
1,2
, Wei Chen
1,2,
, Tengjiao Wang
1,2
, Zhongyu Wei
1,3
,
Binyang Li
4
, Dongqing Yang
1,2
and Kam-Fai Wong
1,3
1
Key Laboratory of High Confidence Software Technologies, Ministry of Education, China
2
School of Electronics Engineering and Computer Science, Peking University, China
3
Shenzhen Research Institute, The Chinese University of Hong Kong, China
4
Dept. of Information Science & Technology, University of International Relations, China
pekingchenwei@pku.edu.cn
Abstract
Microblog has become a major plat-
form for information about real-world
events. Automatically discovering real-
world events from microblog has attracted
the attention of many researchers. Howev-
er, most of existing work ignore the impor-
tance of emotion information for event de-
tection. We argue that people?s emotion-
al reactions immediately reflect the occur-
ring of real-world events and should be im-
portant for event detection. In this study,
we focus on the problem of community-
related event detection by community e-
motions. To address the problem, we pro-
pose a novel framework which include
the following three key components: mi-
croblog emotion classification, community
emotion aggregation and community emo-
tion burst detection. We evaluate our ap-
proach on real microblog data sets. Exper-
imental results demonstrate the effective-
ness of the proposed framework.
1 Introduction
Microblog has become a popular and convenient
platform for people to share information about so-
cial events in real time. When an external even-
t occurs, it will be quickly propagated between
microblog users. During propagation process of
an event, sufficient amount of users will express
their emotions. Taking Sina Weibo
1
as an exam-
ple, more than 12 percent of users use emoticons
2
when reposting an event-related microblog mes-
sage.
The emotion information can not only help us
better understand a given event, but also be u-
tilized to discover new events. Figure 1 shows
1
http://weibo.com/
2
An icon to indicate user?s emotion, as shown in Table 1.
 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1367?1375,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Unified Graph Model for Sentence-based Opinion Retrieval 
 
 
Binyang Li, Lanjun Zhou, Shi Feng, Kam-Fai Wong 
Department of Systems Engineering and Engineering Management 
The Chinese University of Hong Kong 
{byli, ljzhou, sfeng, kfwong}@se.cuhk.edu.hk 
 
 
Abstract 
There is a growing research interest in opinion 
retrieval as on-line users? opinions are becom-
ing more and more popular in business, social 
networks, etc. Practically speaking, the goal of 
opinion retrieval is to retrieve documents, 
which entail opinions or comments, relevant to 
a target subject specified by the user?s query. A 
fundamental challenge in opinion retrieval is 
information representation. Existing research 
focuses on document-based approaches and 
documents are represented by bag-of-word. 
However, due to loss of contextual information, 
this representation fails to capture the associa-
tive information between an opinion and its 
corresponding target. It cannot distinguish dif-
ferent degrees of a sentiment word when asso-
ciated with different targets. This in turn se-
riously affects opinion retrieval performance. 
In this paper, we propose a sentence-based ap-
proach based on a new information representa-
tion, namely topic-sentiment word pair, to cap-
ture intra-sentence contextual information be-
tween an opinion and its target. Additionally, 
we consider inter-sentence information to cap-
ture the relationships among the opinions on 
the same topic. Finally, the two types of infor-
mation are combined in a unified graph-based 
model, which can effectively rank the docu-
ments. Compared with existing approaches, 
experimental results on the COAE08 dataset 
showed that our graph-based model achieved 
significant improvement. 
1 Introduction 
In recent years, there is a growing interest in 
sharing personal opinions on the Web, such as 
product reviews, economic analysis, political 
polls, etc. These opinions cannot only help inde-
pendent users make decisions, but also obtain 
valuable feedbacks (Pang et al, 2008). Opinion 
oriented research, including sentiment classifica-
tion, opinion extraction, opinion question ans-
wering, and opinion summarization, etc. are re-
ceiving growing attention (Wilson, et al, 2005; 
Liu et al, 2005; Oard et al, 2006). However, 
most existing works concentrate on analyzing 
opinions expressed in the documents, and none 
on how to represent the information needs re-
quired to retrieve opinionated documents. In this 
paper, we focus on opinion retrieval, whose goal 
is to find a set of documents containing not only 
the query keyword(s) but also the relevant opi-
nions. This requirement brings about the chal-
lenge on how to represent information needs for 
effective opinion retrieval. 
In order to solve the above problem, previous 
work adopts a 2-stage approach. In the first stage, 
relevant documents are determined and ranked 
by a score, i.e. tf-idf value. In the second stage, 
an opinion score is generated for each relevant 
document (Macdonald and Ounis, 2007; Oard et 
al., 2006). The opinion score can be acquired by 
either machine learning-based sentiment classifi-
ers, such as SVM (Zhang and Yu, 2007), or a 
sentiment lexicons with weighted scores from 
training documents (Amati et al, 2007; Hannah 
et al, 2007; Na et al, 2009). Finally, an overall 
score combining the two is computed by using a 
score function, e.g. linear combination, to re-rank 
the retrieved documents. 
Retrieval in the 2-stage approach is based on 
document and document is represented by 
bag-of-word. This representation, however, can 
only ensure that there is at least one opinion in 
each relevant document, but it cannot determine 
the relevance pairing of individual opinion to its 
target. In general, by simply representing a 
document in bag-of-word, contextual informa-
tion i.e. the corresponding target of an opinion, is 
neglected. This may result in possible mismatch 
between an opinion and a target and in turn af-
fects opinion retrieval performance. By the same 
token, the effect to documents consisting of mul-
1367
tiple topics, which is common in blogs and 
on-line reviews, is also significant. In this setting, 
even if a document is regarded opinionated, it 
cannot ensure that all opinions in the document 
are indeed relevant to the target concerned. 
Therefore, we argue that existing information 
representation i.e. bag-of-word, cannot satisfy 
the information needs for opinion retrieval. 
In this paper, we propose to handle opinion re-
trieval in the granularity of sentence. It is ob-
served that a complete opinion is always ex-
pressed in one sentence, and the relevant target 
of the opinion is mostly the one found in it. 
Therefore, it is crucial to maintain the associative 
information between an opinion and its target 
within a sentence. We define the notion of a top-
ic-sentiment word pair, which is composed of a 
topic term (i.e. the target) and a sentiment word 
(i.e. opinion) of a sentence. Word pairs can 
maintain intra-sentence contextual information to 
express the potential relevant opinions. In addi-
tion, inter-sentence contextual information is also 
captured by word pairs to represent the relation-
ship among opinions on the same topic. In prac-
tice, the inter-sentence information reflects the 
degree of a word pair. Finally, we combine both 
intra-sentence and inter-sentence contextual in-
formation to construct a unified undirected graph 
to achieve effective opinion retrieval. 
The rest of the paper is organized as follows. 
In Section 2, we describe the motivation of our 
approach. Section 3 presents a novel unified 
graph-based model for opinion retrieval. We 
evaluated our model and the results are presented 
in Section 4. We review related works on opi-
nion retrieval in Section 5. Finally, in Section 6, 
the paper is concluded and future work is sug-
gested.  
2 Motivation 
In this section, we start from briefly describing 
the objective of opinion retrieval. We then illu-
strate the limitations of current opinion retrieval 
approaches, and analyze the motivation of our 
method.  
2.1 Formal Description of Problem 
Opinion retrieval was first presented in the 
TREC 2006 Blog track, and the objective is to 
retrieve documents that express an opinion about 
a given target. The opinion target can be a ?tradi-
tional? named entity (e.g. a name of person, lo-
cation, or organization, etc.), a concept (e.g. a 
type of technology), or an event (e.g. presidential 
election). The topic of the document is not re-
quired to be the same as the target, but an opi-
nion about the target has to be presented in the 
document or one of the comments to the docu-
ment (Macdonald and Ounis, 2006). Therefore, 
in this paper we regard the information needs for 
opinion retrieval as relevant opinion. 
2.2 Motivation of Our Approach 
In traditional information retrieval (IR) 
bag-of-word representation is the most common 
way to express information needs. However, in 
opinion retrieval, information need target at re-
levant opinion, and this renders bag-of-word re-
presentation ineffective. 
Consider the example in Figure 1. There are 
three sentences A, B, and C in a document di. 
Now given an opinion-oriented query Q related 
to ?Avatar?. According to the conventional 
2-stage opinion retrieval approach, di is 
represented by a bag-of-word. Among the words, 
there is a topic term Avatar (t1) occurring twice, 
i.e. Avatar in A and Avatar in C, and two senti-
ment words comfortable (o1) and favorite (o2) 
(refer to Figure 2 (a)). In order to rank this doc-
ument, an overall score of the document di is 
computed by a simple combination of the rele-
vant score ( ???????? ) and the opinion score 
(???????), e.g. equal weighted linear combination, 
as follows. 
???????? ? ???????? ?     ??????? 
For simplicity, we let ???????? ? ? ?? ? ?? ?? , and 
???????  be computed by using lexicon-based 
method: ??????? ? ????????????????? ? ??????????????. 
 
 
 
 
 
Figure 1: A retrieved document di on the target 
?Avatar?. 
Although bag-of-word representation achieves 
good performance in retrieving relevant docu-
ments, our study shows that it cannot satisfy the 
information needs for retrieval of relevant opi-
nion. It suffers from the following limitations: 
(1) It cannot maintain contextual information; 
thus, an opinion may not be related to the target 
of the retrieved document is neglected. In this 
example, only the opinion favorite (o2) on Avatar 
in C is the relevant opinion. But due to loss of 
contextual information between the opinion and 
its corresponding target, Avatar in A and com-
A. ???????????? 
Tomorrow, Avatar will be shown in China. 
B. ????? IMAX??????????
I?ve reserved a comfortable seat in IMAX. 
C. ??????????? 3D??? 
Avatar is my favorite 3D movie.  
1368
fortable (o1) are also regarded as relevant opi-
nion mistakenly, creating a false positive. In re-
ality comfortable (o1) describes ?the seats in 
IMAX?, which is an irrelevant opinion, and sen-
tence A is a factual statement rather than an opi-
nion statement. 
    
     
  (a)                (b)        
Figure 2: Two kinds of information representa-
tion of opinion retrieval. (t1=?Avatar? o1= ?com-
fortable?, o2=?favorite?) 
(1) Current approaches cannot capture the re-
lationship among opinions about the same topic. 
Suppose there is another document including 
sentence C which expresses the same opinion on 
Avatar. Existing information representation 
simply does not cater for the two identical opi-
nions from different documents. In addition, if 
many documents contain opinions on Avatar, the 
relationship among them is not clearly 
represented by existing approaches.  
In this paper, we process opinion retrieval in 
the granularity of sentence as we observe that a 
complete opinion always exists within a sentence 
(refer to Figure 2 (b)). To represent a relevant 
opinion, we define the notion of topic-sentiment 
word pair, which consists of a topic term and a 
sentiment word. A word pair maintains the asso-
ciative information between the two words, and 
enables systems to draw up the relationship 
among all the sentences with the same opinion 
on an identical target. This relationship informa-
tion can identify all documents with sentences 
including the sentiment words and to determine 
the contributions of such words to the target 
(topic term). Furthermore, based on word pairs, 
we designed a unified graph-based method for 
opinion retrieval (see later in Section 3). 
3 Graph-based model 
3.1 Basic Idea 
Different from existing approaches which simply 
make use of document relevance to reflect the 
relevance of opinions embedded in them, our 
approach concerns more on identifying the re-
levance of individual opinions. Intuitively, we 
believed that the more relevant opinions appear 
in a document, the more relevant is that docu-
ment for subsequent opinion analysis operations. 
Further, since the lexical scope of an opinion 
does not usually go beyond a sentence, we pro-
pose to handle opinion retrieval in the granularity 
of sentence. 
Without loss of generality, we assume that 
there is a document set ? ? ???, ??, ??,? , ???, and 
a specific query  ? ? ???, ??, ??,? , ??? , where 
??, ??, ??,? , ?? are query keywords. Opinion re-
trieval aims at retrieving documents from ? 
with relevant opinion about the query ?. In ad-
dition, we construct a sentiment word lexicon ?? 
and a topic term lexicon ?? (see Section 4). To 
maintain the associative information between the 
target and the opinion, we consider the document 
set as a bag of sentences, and define a sentence 
set as ? ? ???, ??, ??,? , ???. For each sentence, we 
capture the intra-sentence information through 
the topic-sentiment word pair.  
Definition 1. topic-sentiment word pair ???  con-
sists of two elements, one is from ??, and the 
other one is from ??.  
??? ? ?? ??, ?? ? |?? ? ?? , ?? ? ????. 
The topic term from ?? determines relevance 
by the query term matching, and the sentiment 
word from ?? is used to express an opinion. We 
use the word pair to maintain the associative in-
formation between the topic term and the opinion 
word (also referred to as sentiment word). The 
word pair is used to identify a relevant opinion in 
a sentence. In Figure 2 (b), t1, i.e. Avatar in C, is 
a topic term relevant to the query, and o2 (?favo-
rite?) is supposed to be an opinion; and the word 
pair < t1, o2> indicates sentence C contains a re-
levant opinion. Similarly, we map each sentence 
in word pairs by the following rule, and express 
the intra-sentence information using word pairs. 
For each sentiment word of a sentence, we 
choose the topic term with minimum distance as 
the other element of the word pair: 
?? ? ?? ??, ?? ? |?? ? min???????, ??? for each ??? 
According to the mapping rule, although a 
sentence may give rise to a number of word pairs, 
only the pair with the minimum word distance is 
selected. We do not take into consideration of the 
other words in a sentence as relevant opinions 
are generally formed in close proximity. A sen-
tence is regarded non-opinionated unless it con-
tains at least one word pair. 
In practice, not all word pairs carry equal 
weights to express a relevant opinion as the con-
tribution of an opinion word differs from differ-
ent target topics, and vice versa. For example, 
the word pair < t1, o2> should be more probable 
as a relevant opinion than < t1, o1>. To consider 
1369
that, inter-sentence contextual information is ex-
plored. This is achieved by assigning a weight to 
each word pair to measure their associative de-
grees to different queries. We believe that the 
more a word pair appears the higher should be 
the weight between the opinion and the target in 
the context. 
We will describe how to utilize intra-sentence 
contextual information to express relevant opi-
nion, and inter-sentence information to measure 
the degree of each word pair through a 
graph-based model in the following section. 
3.2 HITS Model 
We propose an opinion retrieval model based on 
HITS, a popular graph ranking algorithm 
(Kleinberg, 1999). By considering both in-
tra-sentence information and inter-sentence in-
formation, we can determine the weight of a 
word pair and rank the documents.  
HITS algorithm distinguishes hubs and au-
thorities in objects. A hub object has links to 
many authorities. An authority object, which has 
high-quality content, would have many hubs 
linking to it. The hub scores and authority scores 
are computed in an iterative way. Our proposed 
opinion retrieval model contains two layers. The 
upper level contains all the topic-sentiment word 
pairs ??? ? ?? ??, ?? ? |?? ? ??, ?? ? ???? . The lower 
level contains all the documents to be retrieved. 
Figure 3 gives the bipartite graph representation 
of the HITS model.  
 
Figure 3: Bipartite link graph. 
For our purpose, the word pairs layer is consi-
dered as hubs and the documents layer authori-
ties. If a word pair occurs in one sentence of a 
document, there will be an edge between them. 
In Figure 3, we can see that the word pair that 
has links to many documents can be assigned a 
high weight to denote a strong associative degree 
between the topic term and a sentiment word, 
and it likely expresses a relevant opinion. On the 
other hand, if a document has links to many word 
pairs, the document is with many relevant opi-
nions, and it will result in high ranking. 
Formally, the representation for the bipartite 
graph is denoted as ? ?? ??, ??, ??? ? , where 
?? ? ????? is the set of all pairs of topic words 
and sentiment words, which appear in one sen-
tence. ?? ? ??? ?  is the set of documents. 
??? ? ????
? |??? ? ??, ?? ? ???  corresponds to the 
connection between documents and top-
ic-sentiment word pairs. Each edge ????  is asso-
ciated with a weight ???? ? ?0,1?  denoting the 
contribution of ???  to the document ?? . The 
weight ????  is computed by the contribution of 
word pair ??? in all sentences of ?? as follows: 
???
? ? ?
|??|
? ?? ? ??????, ??? ? ?1 ? ????????, ?????????????  ?1? 
? |??| is the number of sentences in ??; 
? ? is introduced as the trade-off parameter to 
balance the ??????, ??? and ??????, ???; 
? ??????, ??? is computed to judge the relevance 
of ??  in ?? which belongs to ??; 
??????, ??? ? ? ???,?? ? ?? ???              (2) 
where ? ???,??  is the number of ??  appears in ?? , 
and 
?? ????log?
???
?.??????
?                   (3) 
where ? ??? is the number of sentences that the 
word ?? appears in. 
? ??????, ???  is the contribution of ??  in ?? 
which belongs to ??. 
??????, ??? ?
??,????
??,????
?0.5??1.5?
???????
??? ?
      (4) 
where ??? is the average number of sentences in 
??; ? ???,?? is the number of ?? appears in ?? (Al-
lan et al, 2003; Otterbacher et al, 2005). 
It is found that the contribution of a sentiment 
word ??  will not decrease even if it appears in 
all the sentences. Therefore in Equation 4, we 
just use the length of a sentence instead of ?? ??? 
to normalize long sentences which would likely 
contain more sentiment words. 
The authority score ??????????????????  of 
document ?? and a hub score ?????????????????? 
of ???  at the ?? ? 1???  iteration are computed 
based on the hub scores and authority scores in 
the ??? iteration as follows. 
?????????????????? ? ? ???
? ? ????????????????????  (5) 
?????????????????? ? ? ???
? ? ???????????????????  (6) 
We let ? ? ???,??|??|?|??| denote the adjacency 
matrix.  
???????? ???????                 (7) 
????????? ?? ?????                (8) 
where ????? ? ????????????????|??|??  is the vector 
of authority scores for documents at the ??? ite-
ration and ?????? ? ????????????????|??|??  is the 
vector of hub scores for the word pairs at ??? 
iteration. In order to ensure convergence of the 
iterative form, ?? and ??? are normalized in each 
iteration cycle.  
1370
For computation of the final scores, the initial 
scores of all documents are set to 
?
??, and top-
ic-sentiment word pairs are set to 
?
???? . The 
above iterative steps are then used to compute 
the new scores until convergence. Usually the 
convergence of the iteration algorithm is 
achieved when the difference between the scores 
computed at two successive iterations for any 
nodes falls below a given threshold (Wan et al, 
2008; Li et al, 2009; Erkan and Radev, 2004). In 
our model, we use the hub scores to denote the 
associative degree of each word pair and the au-
thority scores as the total scores. The documents 
are then ranked based on the total scores. 
4 Experiment 
We performed the experiments on the Chinese 
benchmark dataset to verify our proposed ap-
proach for opinion retrieval. We first tested the 
effect of the parameter ?  of our model. To 
demonstrate the effectiveness of our opinion re-
trieval model, we compared its performance with 
the same of other approaches. In addition, we 
studied each individual query to investigate the 
influence of query to our model. Furthermore, 
we showed the top-5 highest weight word pairs 
of 5 queries to further demonstrate the effect of 
word pair. 
4.1 Experiment Setup  
4.1.1 Benchmark Datasets 
Our experiments are based on the Chinese 
benchmark dataset, COAE08 (Zhao et al, 2008). 
COAE dataset is the benchmark data set for the 
opinion retrieval track in the Chinese Opinion 
Analysis Evaluation (COAE) workshop, consist-
ing of blogs and reviews. 20 queries are provided 
in COAE08. In our experiment, we created re-
levance judgments through pooling method, 
where documents are ranked at different levels: 
irrelevant, relevant but without opinion, and re-
levant with opinion. Since polarity is not consi-
dered, all relevant documents with opinion are 
classified into the same level. 
4.1.2 Sentiment Lexicon  
In our experiment, the sentiment lexicon is 
composed by the following resources (Xu et al, 
2007):  
(1) The Lexicon of Chinese Positive Words, 
which consists of 5,054 positive words and 
the Lexicon of Chinese Negative Words, 
which consists of 3,493 negative words; 
(2) The opinion word lexicon provided by Na-
tional Taiwan University which consists of 
2,812 positive words and 8,276 negative 
words; 
(3) Sentiment word lexicon and comment word 
lexicon from Hownet. It contains 1836 posi-
tive sentiment words, 3,730 positive com-
ments, 1,254 negative sentiment words and 
3,116 negative comment words. 
The different graphemes corresponding to 
Traditional Chinese and Simplified Chinese are 
both considered so that the sentiment lexicons 
from different sources are applicable to process 
Simplified Chinese text. The lexicon was ma-
nually verified.  
4.1.3 Topic Term Collection 
In order to acquire the collection of topic terms, 
we adopt two expansion methods, dictio-
nary-based method and pseudo relevance feed-
back method.  
The dictionary-based method utilizes Wikipe-
dia (Popescu and Etzioni, 2005) to find an entry 
page for a phrase or a single term in a query. If 
such an entry exists, all titles of the entry page 
are extracted as synonyms of the query concept. 
For example, if we search ???? (Green Tsu-
nami, a firewall) in Wikipedia, it is re-directed to 
an entry page titled ?????? (Youth Escort). 
This term is then added as a synonym of ???? 
(Green Tsunami) in the query. Synonyms are 
treated the same as the original query terms in a 
retrieval process. The content words in the entry 
page are ranked by their frequencies in the page. 
The top-k terms are returned as potential ex-
panded topic terms. 
The second query expansion method is a 
web-based method. It is similar to the pseudo 
relevance feedback expansion but using web 
documents as the document collection. The 
query is submitted to a web search engine, such 
as Google, which returns a ranked list of docu-
ments. In the top-n documents, the top-m topic 
terms which are highly correlated to the query 
terms are returned. 
4.2 Performance Evaluation 
4.2.1 Parameter Tuning 
We first studied how the parameter ? (see Equ-
ation 1) influenced the mean average precision 
(MAP) in our model. The result is given in Fig-
ure 4. 
1371
 
Figure 4: Performance of MAP with varying ?. 
Best MAP performance was achieved in 
COAE08 evaluation, when ? was set between 
0.4 and 0.6. Therefore, in the following experi-
ments, we set ? ? 0.4. 
4.2.2 Opinion Retrieval Model Comparison 
To demonstrate the effectiveness of our proposed 
model, we compared it with the following mod-
els using different evaluation metrics: 
(1) IR: We adopted a classical information re-
trieval model, and further assumed that all re-
trieved documents contained relevant opinions. 
(2) Doc: The 2-stage document-based opinion 
retrieval model was adopted. The model used 
sentiment lexicon-based method for opinion 
identification and a conventional information 
retrieval method for relevance detection.  
(3) ROSC: This was the model which achieved 
the best run in TREC Blog 07. It employed ma-
chine learning method to identify opinions for 
each sentence, and to determine the target topic 
by a NEAR operator. 
(4) ROCC: This model was similar to ROSC, 
but it considered the factor of sentence and re-
garded the count of relevant opinionated sen-
tence to be the opinion score (Zhang and Yu, 
2007). In our experiment, we treated this model 
as the evaluation baseline. 
(5) GORM: our proposed graph-based opinion 
retrieval model. 
Approach COAE08 Evaluation metrics 
Run id MAP R-pre bPref P@10
IR 0.2797 0.3545 0.2474 0.4868
Doc 0.3316 0.3690 0.3030 0.6696
ROSC 0.3762 0.4321 0.4162 0.7089
Baseline 0.3774 0.4411 0.4198 0.6931
GORM 0.3978 0.4835 0.4265 0.7309
Table 1: Comparison of different approaches on 
COAE08 dataset, and the best is highlighted. 
Most of the above models were originally de-
signed for opinion retrieval in English, and 
re-designed them to handle Chinese opinionated 
documents. We incorporated our own Chinese 
sentiment lexicon for this purpose. In our expe-
riments, in addition to MAP, other metrics such 
as R-precision (R-prec), binary Preference (bPref) 
and Precision at 10 documents (P@10) were also 
used. The evaluation results based on these me-
trics are shown in Table 1. 
Table 1 summarized the results obtained. We 
found that GORM achieved the best performance 
in all the evaluation metrics. Our baseline, ROSC 
and GORM which were sentence-based ap-
proaches achieved better performance than the 
document-based approaches by 20% in average. 
Moreover, our GORM approach did not use ma-
chine learning techniques, but it could still 
achieve outstanding performance. 
To study GORM influenced by different que-
ries, the MAP from median average precision on 
individual topic was shown in Figure 5. 
Figure 5: Difference of MAP from Median on 
COAE08 dataset. (MAP of Median is 0.3724) 
As shown in Figure 5, the MAP performance 
was very low on topic 8 and topic 11. Topic 8, i.e. 
???? (Jackie Chan), it was influenced by topic 
7, i.e. ????? (Jet Lee) as there were a number 
of similar relevant targets for the two topics, and 
therefore many word pairs ended up the same. 
As a result, documents belonging to topic 7 and 
topic 8 could not be differentiated, and they both 
performed badly. In order to solve this problem, 
we extracted the topic term with highest relevant 
weight in the sentence to form word pairs so that 
it reduce the impact on the topic terms in com-
mon. 24% and 30% improvement were achieved, 
respectively. 
As to topic 11, i.e. ????? (Lord of King), 
there were only 8 relevant documents without 
any opinion and 14 documents with relevant 
opinions. As a result, the graph constructed by 
insufficient documents worked ineffectively.  
Except for the above queries, GORM per-
formed well in most of the others. To further in-
vestigate the effect of word pair, we summarized 
the top-5 word pairs with highest weight of 5 
queries in Table 2. 
 
0.2
0.25
0.3
0.35
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
A
P
?
COAE08
?0.4
?0.3
?0.2
?0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
1 2 3 4 5 6 7 8 9 1011121314151617181920
D
if
fe
re
nc
e
Topic
Difference from Median Average Precision per 
Topic
1372
Table 2: Top-5 highest weight word pairs for 5 queries in COAE08 dataset. 
Table 2 showed that most word pairs could 
represent the relevant opinions about the corres-
ponding queries. This showed that inter-sentence 
information was very helpful to identify the as-
sociative degree of a word pair. Furthermore, 
since word pairs can indicate relevant opinions 
effectively, it is worth further study on how they 
could be applied to other opinion oriented appli-
cations, e.g. opinion summarization, opinion 
prediction, etc. 
5 Related Work 
Our research focuses on relevant opinion rather 
than on relevant document retrieval. We, there-
fore, review related works in opinion identifica-
tion research. Furthermore, we do not support the 
conventional 2-stage opinion retrieval approach. 
We conducted literature review on unified opi-
nion retrieval models and related work in this 
area is presented in the section. 
5.1 Lexicon-based Opinion Identification 
Different from traditional IR, opinion retrieval 
focuses on the opinion nature of documents. 
During the last three years, NTICR and TREC 
evaluations have shown that sentiment lex-
icon-based methods led to good performance in 
opinion identification.  
A lightweight lexicon-based statistical ap-
proach was proposed by Hannah et al (2007). In 
this method, the distribution of terms in relevant 
opinionated documents was compared to their 
distribution in relevant fact-based documents to 
calculate an opinion weight. These weights were 
used to compute opinion scores for each re-
trieved document. A weighted dictionary was 
generated from previous TREC relevance data 
(Amati et al, 2007). This dictionary was submit-
ted as a query to a search engine to get an initial 
query-independent opinion score of all retrieved 
documents. Similarly, a pseudo opinionated 
word composed of all opinion words was first 
created, and then used to estimate the opinion 
score of a document (Na et al, 2009). This me-
thod was shown to be very effective in TREC 
evaluations (Lee et al, 2008). More recently, 
Huang and Croft (2009) proposed an effective 
relevance model, which integrated both 
query-independent and query-dependent senti-
ment words into a mixture model. 
In our approach, we also adopt sentiment lex-
icon-based method for opinion identification. 
Unlike the above methods, we generate a weight 
to a sentiment word for each target (associated 
topic term) rather than assign a unified weight or 
an equal weight to the sentiment word for the 
whole topics. Besides, in our model no training 
data is required. We just utilize the structure of 
our graph to generate a weight to reflect the as-
sociative degree between the two elements of a 
word pair in different context. 
5.2 Unified Opinion Retrieval Model 
In addition to conventional 2-stage approach, 
there has been some research on unified opinion 
retrieval models.  
Eguchi and Lavrenko proposed an opinion re-
trieval model in the framework of generative 
language modeling (Eguchi and Lavrenko, 2006). 
They modeled a collection of natural language 
documents or statements, each of which con-
sisted of some topic-bearing and some senti-
ment-bearing words. The sentiment was either 
represented by a group of predefined seed words, 
or extracted from a training sentiment corpus. 
This model was shown to be effective on the 
MPQA corpus.  
Mei et al tried to build a fine-grained opinion 
retrieval system for consumer products (Mei et 
al., 2007). The opinion score for a product was a 
mixture of several facets. Due to the difficulty in 
Top-5 MAP 
??? 
Chen Kaige 
??? 
Six States 
???? 
Macro-regulation 
??? 
Stephen Chow 
Vista 
Vista 
<??? ??> 
Chen Kaige Support 
<??? ??> 
Chen Kaige Best 
<???? ?> 
Limitless Revile 
<?? ??> 
Movie Excellent 
<?? ???> 
Cast Strong 
<?? ??> 
Room rate Rise 
<?? ??> 
Regulate Strengthen
<?? ??> 
CCP Strengthen 
<?? ??> 
Room rate Steady 
<?? ??> 
Housing Security 
<?? ??> 
Economics Steady 
<?? ??> 
Price Rise 
<?? ??> 
Development Steady
<?? ??> 
Consume Rise 
<?? ??> 
Social Security 
<?? ??> 
Movie Like 
<??? ??> 
Stephen Chow Like 
<?? ??> 
Protagonist Best 
<?? ?> 
Comedy Good 
<?? ??> 
Works Splendid 
<?? ?> 
Price Expensive 
<?? ??> 
Microsoft Like 
 <Vista ??> 
Vista Recommend
<?? ??> 
Problem Vital 
<?? ?> 
Performance No 
1373
associating sentiment with products and facets, 
the system was only tested using small scale text 
collections.  
Zhang and Ye proposed a generative model to 
unify topic relevance and opinion generation 
(Zhang and Ye, 2008). This model led to satis-
factory performance, but an intensive computa-
tion load was inevitable during retrieval, since 
for each possible candidate document, an opinion 
score was summed up from the generative prob-
ability of thousands of sentiment words. 
Huang and Croft proposed a unified opinion 
retrieval model according to the Kullback-Leib- 
ler divergence between the two probability dis-
tributions of opinion relevance model and docu-
ment model (Huang and Croft, 2009). They di-
vided the sentiment words into query-dependent 
and query-independent by utilizing several sen-
timent expansion techniques, and integrated them 
into a mixed model. However, in this model, the 
contribution of a sentiment word was its corres-
ponding incremental mean average precision 
value. This method required that large amount of 
training data and manual labeling. 
Different from the above opinion retrieval ap-
proaches, our proposed graph-based model 
processes opinion retrieval in the granularity of 
sentence. Instead of bag-of-word, the sentence is 
split into word pairs which can maintain the 
contextual information. On the one hand, word 
pair can identify the relevant opinion according 
to intra-sentence contextual information. On the 
other hand, it can measure the degree of a rele-
vant opinion by considering the inter-sentence 
contextual information. 
6 Conclusion and Future Work 
In this work we focus on the problem of opinion 
retrieval. Different from existing approaches, 
which regard document relevance as the key in-
dicator of opinion relevance, we propose to ex-
plore the relevance of individual opinion. To do 
that, opinion retrieval is performed in the granu-
larity of sentence. We define the notion of word 
pair, which can not only maintain the association 
between the opinion and the corresponding target 
in the sentence, but it can also build up the rela-
tionship among sentences through the same word 
pair. Furthermore, we convert the relationships 
between word pairs and sentences into a unified 
graph, and use the HITS algorithm to achieve 
document ranking for opinion retrieval. Finally, 
we compare our approach with existing methods. 
Experimental results show that our proposed 
model performs well on COAE08 dataset.  
The novelty of our work lies in using word 
pairs to represent the information needs for opi-
nion retrieval. On the one hand, word pairs can 
identify the relevant opinion according to in-
tra-sentence contextual information. On the other 
hand, word pairs can measure the degree of a 
relevant opinion by taking inter-sentence con-
textual information into consideration. With the 
help of word pairs, the information needs for 
opinion retrieval can be represented appropriate-
ly. 
In the future, more research is required in the 
following directions: 
(1) Since word pairs can indicate relevant opi-
nions effectively, it is worth further study on 
how they could be applied to other opinion 
oriented applications, e.g. opinion summa-
rization, opinion prediction, etc. 
(2) The characteristics of blogs will be taken 
into consideration, i.e., the post time, which 
could be helpful to create a more time sensi-
tivity graph to filter out fake opinions. 
(3) Opinion holder is another important role of 
an opinion, and the identification of opinion 
holder is a main task in NTCIR. It would be 
interesting to study opinion holders, e.g. its 
seniority, for opinion retrieval. 
Acknowledgements: This work is partially 
supported by the Innovation and Technology 
Fund of Hong Kong SAR (No. ITS/182/08) and 
National 863 program (No. 2009AA01Z150). 
Special thanks to Xu Hongbo for providing the 
Chinese sentiment resources. We also thank Bo 
Chen, Wei Gao, Xu Han and anonymous re-
viewers for their helpful comments. 
References 
James Allan, Courtney Wade, and Alvaro Bolivar. 
2003. Retrieval and novelty detection at the sen-
tence level. In SIGIR ?03: Proceedings of the 26th 
annual international ACM SIGIR conference on 
Research and development in information retrieval, 
pages 314-321. ACM. 
Giambattista Amati, Edgardo Ambrosi, Marco Bianc-
hi, Carlo Gaibisso, and Giorgio Gambosi. 2007. 
FUB, IASI-CNR and University of Tor Vergata at 
TREC 2007 Blog Track. In Proceedings of the 15th 
Text Retrieval Conference. 
Koji Eguchi and Victor Lavrenko. Sentiment retrieval 
using generative models. 2006. In EMNLP ?06, 
Proceedings of 2006 Conference on Empirical Me-
thods in Natural Language Processing, page 
345-354. 
1374
Gunes Erkan and Dragomir R. Radev. 2004. Lexpa-
gerank: Prestige in multi-document text summariza-
tion. In EMNLP ?04, Proceedings of 2004 Confe-
rence on Empirical Methods in Natural Language 
Processing. 
David Hannah, Craig Macdonald, Jie Peng, Ben He, 
and Iadh Ounis. 2007. University of Glasgow at 
TREC 2007: Experiments in Blog and Enterprise 
Tracks with Terrier. In Proceedings of the 15th Text 
Retrieval Conference. 
Xuanjing Huang, William Bruce Croft. 2009. A Uni-
fied Relevance Model for Opinion Retrieval. In 
Proceedings of CIKM. 
Jon M. Kleinberg. 1999. Authoritative sources in a 
hyperlinked environment. J. ACM, 46(5): 604-632. 
Yeha Lee, Seung-Hoon Na, Jungi Kim, Sang-Hyob 
Nam, Hun-young Jung, Jong-Hyeok Lee. 2008. 
KLE at TREC 2008 Blog Track: Blog Post and Feed 
Retrieval. In Proceedings of the 15th Text Retrieval 
Conference. 
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan 
Zhu. 2009. Answering Opinion Questions with 
Random Walks on Graphs. In ACL ?09, Proceedings 
of the 48th Annual Meeting of the Association for 
Computational Linguistics. 
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. 
Opinion observer: Analyzing and comparing opi-
nion s on the web. In WWW ?05: Proceedings of the 
14th International Conference on World Wide Web. 
Craig Macdonald and Iadh Ounis. 2007. Overview of 
the TREC-2007 Blog Track. In Proceedings of the 
15th Text Retrieval Conference. 
Craig Macdonald and Iadh Ounis. 2006. Overview of 
the TREC-2006 Blog Track. In Proceedings of the 
14th Text Retrieval Conference. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and Chengxiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In 
WWW ?07: Proceedings of the 16 International 
Conference on World Wide Web. 
Seung-Hoon Na, Yeha Lee, Sang-Hyob Nam, and 
Jong-Hyeok Lee. 2009. Improving opinion retrieval 
based on query-specific sentiment lexicon. In 
ECIR ?09: Proceedings of the 31st annual European 
Conference on Information Retrieval, pages 
734-738. 
Douglas Oard, Tamer Elsayed, Jianqiang Wang, Ye-
jun Wu, Pengyi Zhang, Eileen Abels, Jimmy Lin, 
and Dagbert Soergel. 2006. TREC-2006 at Mary-
land: Blog, Enterprise, Legal and QA Tracks. In 
Proceedings of the 15th Text Retrieval Conference. 
Jahna Otterbacher, Gunes Erkan, and Dragomir R. 
Radev. 2005. Using random walks for ques-
tion-focused sentence retrieval. In EMNLP ?05, 
Proceedings of 2005 Conference on Empirical Me-
thods in Natural Language Processing. 
Larry Page, Sergey Brin, Rajeev Motwani, and Terry 
Winograd. 1998. The pagerank citation ranking: 
Bringing order to the web. Technical report, Stan-
ford University. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2): 1-135.  
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinion s from reviews. In 
EMNLP ?05, Proceedings of 2005 Conference on 
Empirical Methods in Natural Language 
Processing. 
Xiaojun Wan and Jianwu Yang. 2008. Mul-
ti-document summarization using cluster-based link 
analysis. In SIGIR ?08: Proceedings of the 31th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval, 
pages 299-306. ACM. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing contextual polarity in 
phrase-level sentiment analysis. In EMNLP ?05, 
Proceedings of 2005 Conference on Empirical Me-
thods in Natural Language Processing.  
Ruifeng Xu, Kam-Fai Wong and Yunqing Xia. 2007. 
Opinmine - Opinion Analysis System by CUHK for 
NTCIR-6 Pilot Task. In Proceedings of NTCIR-6.  
Min Zhang and Xingyao Ye. 2008. A generation 
model to unify topic relevance and lexicon-based 
sentiment for opinion retrieval. In SIGIR ?08: Pro-
ceedings of the 31st Annual International ACM SI-
GIR conference on Research and Development in 
Information Retrieval, pages 411-418. ACM. 
Wei Zhang and Clement Yu. 2007. UIC at TREC 
2007 Blog Track. In Proceedings of the 15th Text 
Retrieval Conference. 
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, 
Kang Liu, and Qi Zhang. 2008. Overview of Chi-
nese Opinion Analysis Evaluation 2008. In Pro-
ceedings of the First Chinese Opinion Analysis 
Evaluation. 
 
1375
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 112?122,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Query Weighting for Ranking Model Adaptation
Peng Cai1, Wei Gao2, Aoying Zhou1, and Kam-Fai Wong2,3
1East China Normal University, Shanghai, China
pengcai2010@gmail.com, ayzhou@sei.ecnu.edu.cn
2The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
{wgao, kfwong}@se.cuhk.edu.hk
3Key Laboratory of High Confidence Software Technologies, Ministry of Education, China
Abstract
We propose to directly measure the impor-
tance of queries in the source domain to the
target domain where no rank labels of doc-
uments are available, which is referred to
as query weighting. Query weighting is a
key step in ranking model adaptation. As
the learning object of ranking algorithms is
divided by query instances, we argue that
it?s more reasonable to conduct importance
weighting at query level than document level.
We present two query weighting schemes.
The first compresses the query into a query
feature vector, which aggregates all document
instances in the same query, and then con-
ducts query weighting based on the query fea-
ture vector. This method can efficiently esti-
mate query importance by compressing query
data, but the potential risk is information loss
resulted from the compression. The second
measures the similarity between the source
query and each target query, and then com-
bines these fine-grained similarity values for
its importance estimation. Adaptation exper-
iments on LETOR3.0 data set demonstrate
that query weighting significantly outperforms
document instance weighting methods.
1 Introduction
Learning to rank, which aims at ranking documents
in terms of their relevance to user?s query, has been
widely studied in machine learning and information
retrieval communities (Herbrich et al, 2000; Fre-
und et al, 2004; Burges et al, 2005; Yue et al,
2007; Cao et al, 2007; Liu, 2009). In general,
large amount of training data need to be annotated
by domain experts for achieving better ranking per-
formance. In real applications, however, it is time
consuming and expensive to annotate training data
for each search domain. To alleviate the lack of
training data in the target domain, many researchers
have proposed to transfer ranking knowledge from
the source domain with plenty of labeled data to the
target domain where only a few or no labeled data is
available, which is known as ranking model adapta-
tion (Chen et al, 2008a; Chen et al, 2010; Chen et
al., 2008b; Geng et al, 2009; Gao et al, 2009).
Intuitively, the more similar an source instance
is to the target instances, it is expected to be more
useful for cross-domain knowledge transfer. This
motivated the popular domain adaptation solution
based on instance weighting, which assigns larger
weights to those transferable instances so that the
model trained on the source domain can adapt more
effectively to the target domain (Jiang and Zhai,
2007). Existing instance weighting schemes mainly
focus on the adaptation problem for classification
(Zadrozny, 2004; Huang et al, 2007; Jiang and Zhai,
2007; Sugiyama et al, 2008).
Although instance weighting scheme may be ap-
plied to documents for ranking model adaptation,
the difference between classification and learning to
rank should be highlighted to take careful consider-
ation. Compared to classification, the learning ob-
ject for ranking is essentially a query, which con-
tains a list of document instances each with a rel-
evance judgement. Recently, researchers proposed
listwise ranking algorithms (Yue et al, 2007; Cao
et al, 2007) to take the whole query as a learning
object. The benchmark evaluation showed that list-
112
Target domainSource Domain
d1(s1) d2(s1)
d3(s1)
d1(s2)
d2(s2)
d3(s2)
d2(t1)
d1(t2)d2(t2) d3(t2)
d3(t1)
d1(t1)
(a) Instance based weighting
d2(s1)
d1(s1)
d3(s1)
d1(s2)d2(s2)d3(s2)
qs2
qs1
d3(t1)
d2(t1)
d1(t1)
d1(t2)d2(t2)d3(t2)
qt1
qt2
Target domainSource Domain
(b) Query based weighting
Figure 1: The information about which document instances belong to the same query is lost in document instance
weighting scheme. To avoid losing this information, query weighting takes the query as a whole and directly measures
its importance.
wise approach significantly outperformed pointwise
approach, which takes each document instance as in-
dependent learning object, as well as pairwise ap-
proach, which concentrates learning on the order of
a pair of documents (Liu, 2009). Inspired by the
principle of listwise approach, we hypothesize that
the importance weighting for ranking model adapta-
tion could be done better at query level rather than
document level.
Figure 1 demonstrates the difference between in-
stance weighting and query weighting, where there
are two queries qs1 and qs2 in the source domain
and qt1 and qt2 in the target domain, respectively,
and each query has three retrieved documents. In
Figure 1(a), source and target domains are repre-
sented as a bag of document instances. It is worth
noting that the information about which document
instances belong to the same query is lost. To
avoid this information loss, query weighting scheme
shown as Figure 1(b) directly measures importance
weight at query level.
Instance weighting makes the importance estima-
tion of document instances inaccurate when docu-
ments of the same source query are similar to the
documents from different target queries. Take Fig-
ure 2 as a toy example, where the document in-
stance is represented as a feature vector with four
features. No matter what weighting schemes are
used, it makes sense to assign high weights to source
queries qs1 and qs2 because they are similar to tar-
get queries qt1 and qt2, respectively. Meanwhile, the
source query qs3 should be weighted lower because
<d1
s1
>=( 5, 1, 0 ,0 )
<d2
s1
>=( 6, 2, 0 ,0 )
<d1
s2
>=( 0, 0, 5, 1)
<d2
s2
>=( 0, 0, 6, 2)
<d1
s3
>=( 5, 1, 0, 0)
<d2
s3
>=( 0, 0, 6, 2)
<d1
t1
>=(5, 1, 0 ,0 )
<d2
t1
>=(6, 2, 0 ,0 )
<d1
t2
>=( 0, 0, 5, 1)
<d2
t2
>=( 0, 0, 6, 2)
q
s1
q
s2
q
s3
qt1
qt2
Figure 2: A toy example showing the problem of docu-
ment instance weighting scheme.
it?s not quite similar to any of qt1 and qt2 at query
level, meaning that the ranking knowledge from qs3
is different from that of qt1 and qt2 and thus less
useful for the transfer to the target domain. Unfor-
tunately, the three source queries qs1, qs2 and qs3
would be weighted equally by document instance
weighting scheme. The reason is that all of their
documents are similar to the two document instances
in target domain despite the fact that the documents
of qs3 correspond to their counterparts from different
target queries.
Therefore, we should consider the source query
as a whole and directly measure the query impor-
tance. However, it?s not trivial to directly estimate
113
a query?s weight because a query is essentially pro-
vided as a matrix where each row represents a vector
of document features. In this work, we present two
simple but very effective approaches attempting to
resolve the problem from distinct perspectives: (1)
we compress each query into a query feature vec-
tor by aggregating all of its document instances, and
then conduct query weighting on these query feature
vectors; (2) we measure the similarity between the
source query and each target query one by one, and
then combine these fine-grained similarity values to
calculate its importance to the target domain.
2 Instance Weighting Scheme Review
The basic idea of instance weighting is to put larger
weights on source instances which are more simi-
lar to target domain. As a result, the key problem
is how to accurately estimate the instance?s weight
indicating its importance to target domain. (Jiang
and Zhai, 2007) used a small number of labeled data
from target domain to weight source instances. Re-
cently, some researchers proposed to weight source
instance only using unlabeled target instances (Shi-
modaira, 2000; Sugiyama et al, 2008; Huang et al,
2007; Zadrozny, 2004; Gao et al, 2010). In this
work, we also focus on weighting source queries
only using unlabeled target queries.
(Gao et al, 2010; Ben-David et al, 2010) pro-
posed to use a classification hyperplane to separate
source instances from target instances. With the do-
main separator, the probability that a source instance
is classified to target domain can be used as the im-
portance weight. Other instance weighting methods
were proposed for the sample selection bias or co-
variate shift in the more general setting of classifier
learning (Shimodaira, 2000; Sugiyama et al, 2008;
Huang et al, 2007; Zadrozny, 2004). (Sugiyama et
al., 2008) used a natural model selection procedure,
referred to as Kullback-Leibler divergence Impor-
tance Estimation Procedure (KLIEP), for automat-
ically tuning parameters, and showed that its impor-
tance estimation was more accurate. The main idea
is to directly estimate the density function ratio of
target distribution pt(x) to source distribution ps(x),
i.e. w(x) = pt(x)ps(x) . Then model w(x) can be used to
estimate the importance of source instances. Model
parameters were computed with a linear model by
minimizing the KL-divergence from pt(x) to its esti-
mator p?t(x). Since p?t(x) = w?(x)ps(x), the ultimate
objective only contains model w?(x).
For using instance weighting in pairwise rank-
ing algorithms, the weights of document instances
should be transformed into those of document
pairs (Gao et al, 2010). Given a pair of documents
?xi, xj? and their weights wi and wj , the pairwise
weight wij could be estimated probabilistically as
wi ?wj . To consider query factor, query weight was
further estimated as the average value of the weights
over all the pairs, i.e., wq = 1M
?
i,j wij , where M
is the number of pairs in query q. Additionally, to
take the advantage of both query and document in-
formation, a probabilistic weighting for ?xi, xj? was
modeled by wq ? wij . Through the transformation,
instance weighting schemes for classification can be
applied to ranking model adaptation.
3 Query Weighting
In this section, we extend instance weighting to di-
rectly estimate query importance for more effec-
tive ranking model adaptation. We present two
query weighting methods from different perspec-
tives. Note that although our methods are based on
domain separator scheme, other instance weighting
schemes such as KLIEP (Sugiyama et al, 2008) can
also be extended similarly.
3.1 Query Weighting by Document Feature
Aggregation
Our first query weighting method is inspired by the
recent work on local learning for ranking (Geng et
al., 2008; Banerjee et al, 2009). The query can be
compressed into a query feature vector, where each
feature value is obtained by the aggregate of its cor-
responding features of all documents in the query.
We concatenate two types of aggregates to construct
the query feature vector: the mean ?? = 1|q|
?|q|
i=1 f?i
and the variance ?? = 1|q|
?|q|
i=1(f?i ? ??)2, where f?i
is the feature vector of document i and |q| denotes
the number of documents in q . Based on the ag-
gregation of documents within each query, we can
use a domain separator to directly weight the source
queries with the set of queries from both domains.
Given query data sets Ds = {qis}mi=1 and Dt =
{qjt }nj=1 respectively from the source and target do-
114
Algorithm 1 Query Weighting Based on Document Feature Aggregation in the Query
Input:
Queries in the source domain, Ds = {qis}mi=1;
Queries in the target domain, Dt = {qjt }nj=1;
Output:
Importance weights of queries in the source domain, IWs = {Wi}mi=1;
1: ys = ?1, yt = +1;
2: for i = 1; i ? m; i + + do
3: Calculate the mean vector ??i and variance vector ??i for qis;
4: Add query feature vector q?is = (??i, ??i, ys) to D?s ;
5: end for
6: for j = 1; j ? n; j + + do
7: Calculate the mean vector ??j and variance vector ??j for qjt ;
8: Add query feature vector q?jt = (??j , ??j , yt) to D?t;
9: end for
10: Find classification hyperplane Hst which separates D?s from D?t;
11: for i = 1; i ? m; i + + do
12: Calculate the distance of q?is to Hst, denoted as L(q?is);
13: Wi = P (qis ? Dt) = 11+exp(??L(q?is)+?)
14: Add Wi to IWs;
15: end for
16: return IWs;
mains, we use algorithm 1 to estimate the proba-
bility that the query qis can be classified to Dt, i.e.
P (qis ? Dt), which can be used as the importance of
qis relative to the target domain. From step 1 to 9,D?s
and D?t are constructed using query feature vectors
from source and target domains. Then, a classifi-
cation hyperplane Hst is used to separate D?s from
D?t in step 10. The distance of the query feature
vector q?is from Hst are transformed to the probabil-
ity P (qis ? Dt) using a sigmoid function (Platt and
Platt, 1999).
3.2 Query Weighting by Comparing Queries
across Domains
Although the query feature vector in algorithm 1 can
approximate a query by aggregating its documents?
features, it potentially fails to capture important fea-
ture information due to the averaging effect during
the aggregation. For example, the merit of features
in some influential documents may be canceled out
in the mean-variance calculation, resulting in many
distorted feature values in the query feature vector
that hurts the accuracy of query classification hy-
perplane. This urges us to propose another query
weighting method from a different perspective of
query similarity.
Intuitively, the importance of a source query to
the target domain is determined by its overall sim-
ilarity to every target query. Based on this intu-
ition, we leverage domain separator to measure the
similarity between a source query and each one of
the target queries, where an individual domain sep-
arator is created for each pair of queries. We esti-
mate the weight of a source query using algorithm 2.
Note that we assume document instances in the same
query are conditionally independent and all queries
are independent of each other. In step 3, D?qis is con-
structed by all the document instances {x?k} in query
qis with the domain label ys. For each target query
qjt , we use the classification hyperplane Hij to es-
timate P (x?k ? D?qjt
), i.e. the probability that each
document x?k of qis is classified into the document set
of qjt (step 8). Then the similarity between qis and q
j
t
is measured by the probability P (qis ? q
j
t ) at step 9.
Finally, the probability of qis belonging to the target
domain P (qis ? Dt) is calculated at step 11.
It can be expected that algorithm 2 will generate
115
Algorithm 2 Query Weighting by Comparing Source and Target Queries
Input:
Queries in source domain, Ds = {qis}mi=1;
Queries in target domain, Dt = {qjt }nj=1;
Output:
Importance weights of queries in source domain, IWs = {Wi}mi=1;
1: ys = ?1, yt = +1;
2: for i = 1; i ? m; i + + do
3: Set D?qis={x?k, ys)}
|qis|
k=1;
4: for j = 1; j ? n; j + + do
5: Set D?
qjt
={x?k? , yt)}
|qjt |
k?=1;
6: Find a classification hyperplane Hij which separates D?qis from D
?
qjt
;
7: For each k, calculate the distance of x?k to Hij , denoted as L(x?k);
8: For each k, calculate P (x?k ? D?qjt
) = 11+exp(??L(x?k)+?) ;
9: Calculate P (qis ? q
j
t ) = 1|qis|
?|qis|
k=1 P (x?k ? D
?
qjt
);
10: end for
11: Add Wi = P (qis ? Dt) = 1n
?n
j=1 P (qis ? q
j
t ) to IWs;
12: end for
13: return IWs;
more precise measures of query similarity by utiliz-
ing the more fine-grained classification hyperplane
for separating the queries of two domains.
4 Ranking Model Adaptation via Query
Weighting
To adapt the source ranking model to the target do-
main, we need to incorporate query weights into ex-
isting ranking algorithms. Note that query weights
can be integrated with either pairwise or listwise al-
gorithms. For pairwise algorithms, a straightforward
way is to assign the query weight to all the document
pairs associated with this query. However, document
instance weighting cannot be appropriately utilized
in listwise approach. In order to compare query
weighting with document instance weighting, we
need to fairly apply them for the same approach of
ranking. Therefore, we choose pairwise approach to
incorporate query weighting. In this section, we ex-
tend Ranking SVM (RSVM) (Herbrich et al, 2000;
Joachims, 2002) ? one of the typical pairwise algo-
rithms for this.
Let?s assume there are m queries in the data set
of source domain, and for each query qi there are
?(qi) number of meaningful document pairs that can
be constructed based on the ground truth rank labels.
Given ranking function f , the objective of RSVM is
presented as follows:
min
1
2
||w?||2 + C
m
?
i=1
?(qi)
?
j=1
?ij (1)
subject to zij ? f(w?, x?j(1)qi ? x?
j(2)
qi ) ? 1 ? ?ij
?ij ? 0, i = 1, . . . ,m; j = 1, . . . , ?(qi)
where x?j(1)qi and x?
j(2)
qi are two documents with dif-
ferent rank label, and zij = +1 if x?j(1)qi is labeled
more relevant than x?j(2)qi ; or zij = ?1 otherwise.
Let ? = 12C and replace ?ij with Hinge Loss func-
tion (.)+, Equation 1 can be turned to the following
form:
min ?||w?||2+
m
?
i=1
?(qi)
?
j=1
(
1 ? zij ? f(w?, x?j(1)qi ? x?
j(2)
qi )
)+
(2)
Let IW (qi) represent the importance weight of
source query qi. Equation 2 is extended for inte-
grating the query weight into the loss function in a
116
straightforward way:
min ?||w?||2+
m
?
i=1
IW (qi) ?
?(qi)
?
j=1
(
1 ? zij ? f(w?, x?j(1)qi ? x?
j(2)
qi )
)+
where IW (.) takes any one of the weighting
schemes given by algorithm 1 and algorithm 2.
5 Evaluation
We evaluated the proposed two query weighting
methods on TREC-2003 and TREC-2004 web track
datasets, which were released through LETOR3.0 as
a benchmark collection for learning to rank by (Qin
et al, 2010). Originally, different query tasks were
defined on different parts of data in the collection,
which can be considered as different domains for us.
Adaptation takes place when ranking tasks are per-
formed by using the models trained on the domains
in which they were originally defined to rank the
documents in other domains. Our goal is to demon-
strate that query weighting can be more effective
than the state-of-the-art document instance weight-
ing.
5.1 Datasets and Setup
Three query tasks were defined in TREC-2003 and
TREC-2004 web track, which are home page finding
(HP), named page finding (NP) and topic distilla-
tion (TD) (Voorhees, 2003; Voorhees, 2004). In this
dataset, each document instance is represented by 64
features, including low-level features such as term
frequency, inverse document frequency and docu-
ment length, and high-level features such as BM25,
language-modeling, PageRank and HITS. The num-
ber of queries of each task is given in Table 1.
The baseline ranking model is an RSVM directly
trained on the source domain without using any
weighting methods, denoted as no-weight. We im-
plemented two weighting measures based on do-
main separator and Kullback-Leibler divergence, re-
ferred to DS and KL, respectively. In DS measure,
three document instance weighting methods based
on probability principle (Gao et al, 2010) were
implemented for comparison, denoted as doc-pair,
doc-avg and doc-comb (see Section 2). In KL mea-
sure, there is no probabilistic meaning for KLweight
Query Task TREC 2003 TREC 2004
Topic Distillation 50 75
Home Page finding 150 75
Named Page finding 150 75
Table 1: The number of queries in TREC-2003 and
TREC-2004 web track
and the doc-comb based on KL is not interpretable,
and we only present the results of doc-pair and doc-
avg for KL measure. Our proposed query weight-
ing methods are denoted by query-aggr and query-
comp, corresponding to document feature aggrega-
tion in query and query comparison across domains,
respectively. All ranking models above were trained
only on source domain training data and the labeled
data of target domain was just used for testing.
For training the models efficiently, we imple-
mented RSVM with Stochastic Gradient Descent
(SGD) optimizer (Shalev-Shwartz et al, 2007). The
reported performance is obtained by five-fold cross
validation.
5.2 Experimental Results
The task of HP and NP are more similar to
each other whereas HP/NP is rather different from
TD (Voorhees, 2003; Voorhees, 2004). Thus,
we carried out HP/NP to TD and TD to HP/NP
ranking adaptation tasks. Mean Average Precision
(MAP) (Baeza-Yates and Ribeiro-Neto, 1999) is
used as the ranking performance measure.
5.2.1 Adaptation from HP/NP to TD
The first set of experiments performed adaptation
from HP to TD and NP to TD. The results of MAP
are shown in Table 2.
For the DS-based measure, as shown in the table,
query-aggr works mostly better than no-weight,doc-
pair, doc-avg and doc-comb, and query-comp per-
forms the best among the five weighting methods.
T-test on MAP indicates that the improvement of
query-aggr over no-weight is statistically significant
on two adaptation tasks while the improvement of
document instance weighting over no-weight is sta-
tistically significant only on one task. All of the
improvement of query-comp over no-weight, doc-
pair,doc-avg and doc-comb are statistically signifi-
cant. This demonstrates the effectiveness of query
117
Model Weighting method HP03 to TD03 HP04 to TD04 NP03 to TD03 NP04 to TD04
no-weight 0.2508 0.2086 0.1936 0.1756
DS
doc-pair 0.2505 0.2042 0.1982? 0.1708
doc-avg 0.2514 0.2019 0.2122?? 0.1716
doc-comb 0.2562 0.2051 0.2224??? 0.1793
query-aggr 0.2573 0.2106??? 0.2088 0.1808???
query-comp 0.2816??? 0.2147??? 0.2392??? 0.1861???
KL
doc-pair 0.2521 0.2048 0.1901 0.1761
doc-avg 0.2534 0.2127? 0.1904 0.1777
doc-comb - - - -
query-aggr 0.1890 0.1901 0.1870 0.1643
query-comp 0.2548? 0.2142? 0.2313??? 0.1807?
Table 2: Results of MAP for HP/NP to TD adaptation. ?, ?, ? and boldface indicate significantly better than no-weight,
doc-pair, doc-avg and doc-comb, respectively. Confidence level is set at 95%
weighting compared to document instance weight-
ing.
Furthermore, query-comp can perform better than
query-aggr. The reason is that although document
feature aggregation might be a reasonable represen-
tation for a set of document instances, it is possible
that some information could be lost or distorted in
the process of compression. By contrast, more ac-
curate query weights can be achieved by the more
fine-grained similarity measure between the source
query and all target queries in algorithm 2.
For the KL-based measure, similar observation
can be obtained. However, it?s obvious that DS-
based models can work better than the KL-based.
The reason is that KL conducts weighting by density
function ratio which is sensitive to the data scale.
Specifically, after document feature aggregation, the
number of query feature vectors in all adaptation
tasks is no more than 150 in source and target do-
mains. It renders the density estimation in query-
aggr is very inaccurate since the set of samples is
too small. As each query contains 1000 documents,
they seemed to provide query-comp enough samples
for achieving reasonable estimation of the density
functions in both domains.
5.2.2 Adaptation from TD to HP/NP
To further validate the effectiveness of query
weighting, we also conducted adaptation from TD
to HP and TD to NP . MAP results with significant
test are shown in Table 3.
We can see that document instance weighting
schemes including doc-pair, doc-avg and doc-comb
can not outperform no-weight based on MAP mea-
sure. The reason is that each query in TD has 1000
retrieved documents in which 10-15 documents are
relevant whereas each query in HP or NP only con-
sists 1-2 relevant documents. Thus, when TD serves
as the source domain, it leads to the problem that
too many document pairs were generated for train-
ing the RSVM model. In this case, a small number
of documents that were weighted inaccurately can
make significant impact on many number of docu-
ment pairs. Since query weighting method directly
estimates the query importance instead of document
instance importance, both query-aggr and query-
comp can avoid such kind of negative influence that
is inevitable in the three document instance weight-
ing methods.
5.2.3 The Analysis on Source Query Weights
An interesting problem is which queries in the
source domain are assigned high weights and why
it?s the case. Query weighting assigns each source
query with a weight value. Note that it?s not mean-
ingful to directly compare absolute weight values
between query-aggr and query-comp because source
query weights from distinct weighting methods have
different range and scale. However, it is feasible
to compare the weights with the same weighting
method. Intuitively, if the ranking model learned
from a source query can work well in target do-
main, it should get high weight. According to this
intuition, if ranking models fq1s and fq2s are learned
118
model weighting scheme TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04
no-weight 0.6986 0.6158 0.5053 0.5427
DS
doc-pair 0.6588 0.6235? 0.4878 0.5212
doc-avg 0.6654 0.6200 0.4736 0.5035
doc-comb 0.6932 0.6214? 0.4974 0.5077
query-aggr 0.7179??? 0.6292??? 0.5198??? 0.5551???
query-comp 0.7297??? 0.6499??? 0.5203??? 0.6541???
KL
doc-pair 0.6480 0.6107 0.4633 0.5413
doc-avg 0.6472 0.6132 0.4626 0.5406
doc-comb ? ? ? ?
query-aggr 0.6263 0.5929 0.4597 0.4673
query-comp 0.6530?? 0.6358??? 0.4726 0.5559???
Table 3: Results of MAP for TD to HP/NP adaptation. ?, ?, ? and boldface indicate significantly better than no-weight,
doc-pair, doc-avg and doc-comb, respectively. Confidence level is set as 95%.
from queries q1s and q2s respectively, and fq1s per-
forms better than fq2s , then the source query weight
of q1s should be higher than that of q2s .
For further analysis, we compare the weight val-
ues between each source query pair, for which we
trained RSVM on each source query and evaluated
the learned model on test data from target domain.
Then, the source queries are ranked according to the
MAP values obtained by their corresponding rank-
ing models. The order is denoted as Rmap. Mean-
while, the source queries are also ranked with re-
spect to their weights estimated by DS-based mea-
sure, and the order is denoted as Rweight. We hope
Rweight is correlated as positively as possible with
Rmap. For comparison, we also ranked these queries
according to randomly generated query weights,
which is denoted as query-rand in addition to query-
aggr and query-comp. The Kendall?s ? = P?QP+Q
is used to measure the correlation (Kendall, 1970),
where P is the number of concordant query pairs
and Q is the number of discordant pairs. It?s
noted that ? ?s range is from -1 to 1, and the larger
value means the two ranking is better correlated.
The Kendall?s ? by different weighting methods are
given in Table 4 and 5.
We find that Rweight produced by query-aggr and
query-comp are all positively correlated with Rmap
and clearly the orders generated by query-comp are
more positive than those by query-aggr. This is
another explanation why query-comp outperforms
query-aggr. Furthermore, both are far better than
weighting TD03 to HP03 TD04 to HP04
doc-pair 28,835 secs 21,640 secs
query-aggr 182 secs 123 secs
query-comp 15,056 secs 10,081 secs
Table 6: The efficiency of weighting in seconds.
query-rand because theRweight by query-rand is ac-
tually independent of Rmap.
5.2.4 Efficiency
In the situation where there are large scale data in
source and target domains, how to efficiently weight
a source query is another interesting problem. With-
out the loss of generality, we reported the weighting
time of doc-pair, query-aggr and query-comp from
adaptation from TD to HP using DS measure. As
doc-avg and doc-comb are derived from doc-pair,
their efficiency is equivalent to doc-pair.
As shown in table 6, query-aggr can efficiently
weight query using query feature vector. The reason
is two-fold: one is the operation of query document
aggregation can be done very fast, and the other is
there are 1000 documents in each query of TD or HP,
which means that the compression ratio is 1000:1.
Thus, the domain separator can be found quickly. In
addition, query-comp is more efficient than doc-pair
because doc-pair needs too much time to find the
separator using all instances from source and target
domain. And query-comp uses a divide-and-conquer
method to measure the similarity of source query to
each target query, and then efficiently combine these
119
Weighting method HP03 to TD03 HP04 to TD04 NP03 to TD03 NP04 to TD04
query-aggr 0.0906 0.0280 0.0247 0.0525
query-comp 0.1001 0.0804 0.0711 0.1737
query-rand 0.0041 0.0008 -0.0127 0.0163
Table 4: The Kendall?s ? of Rweight and Rmap in HP/NP to TD adaptation.
Weighting method TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04
query-aggr 0.1172 0.0121 0.0574 0.0464
query-comp 0.1304 0.1393 0.1586 0.0545
query-rand ?0.0291 0.0022 0.0161 -0.0262
Table 5: The Kendall?s ? of Rweight and Rmap in TD to HP/NP adaptation.
fine-grained similarity values.
6 Related Work
Cross-domain knowledge transfer has became an
important topic in machine learning and natural lan-
guage processing (Ben-David et al, 2010; Jiang
and Zhai, 2007; Blitzer et al, 2006; Daume? III
and Marcu, 2006). (Blitzer et al, 2006) pro-
posed model adaptation using pivot features to build
structural feature correspondence in two domains.
(Pan et al, 2009) proposed to seek a common fea-
tures space to reduce the distribution difference be-
tween the source and target domain. (Daume? III and
Marcu, 2006) assumed training instances were gen-
erated from source domain, target domain and cross-
domain distributions, and estimated the parameter
for the mixture distribution.
Recently, domain adaptation in learning to rank
received more and more attentions due to the lack
of training data in new search domains. Existing
ranking adaptation approaches can be grouped into
feature-based (Geng et al, 2009; Chen et al, 2008b;
Wang et al, 2009; Gao et al, 2009) and instance-
based (Chen et al, 2010; Chen et al, 2008a; Gao et
al., 2010) approaches. In (Geng et al, 2009; Chen et
al., 2008b), the parameters of ranking model trained
on the source domain was adjusted with the small
set of labeled data in the target domain. (Wang et al,
2009) aimed at ranking adaptation in heterogeneous
domains. (Gao et al, 2009) learned ranking mod-
els on the source and target domains independently,
and then constructed a stronger model by interpo-
lating the two models. (Chen et al, 2010; Chen et
al., 2008a) weighted source instances by using small
amount of labeled data in the target domain. (Gao et
al., 2010) studied instance weighting based on do-
main separator for learning to rank by only using
training data from source domain. In this work, we
propose to directly measure the query importance in-
stead of document instance importance by consider-
ing information at both levels.
7 Conclusion
We introduced two simple yet effective query
weighting methods for ranking model adaptation.
The first represents a set of document instances
within the same query as a query feature vector,
and then directly measure the source query impor-
tance to the target domain. The second measures
the similarity between a source query and each tar-
get query, and then combine the fine-grained simi-
larity values to estimate its importance to target do-
main. We evaluated our approaches on LETOR3.0
dataset for ranking adaptation and found that: (1)
the first method efficiently estimate query weights,
and can outperform the document instance weight-
ing but some information is lost during the aggrega-
tion; (2) the second method consistently and signifi-
cantly outperforms document instance weighting.
8 Acknowledgement
P. Cai and A. Zhou are supported by NSFC (No.
60925008) and 973 program (No. 2010CB731402).
W. Gao and K.-F. Wong are supported by national
863 program (No. 2009AA01Z150). We also thank
anonymous reviewers for their helpful comments.
120
References
Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval.
Somnath Banerjee, Avinava Dubey, Jinesh Machchhar,
and Soumen Chakrabarti. 2009. Efficient and accu-
rate local learning for ranking. In SIGIR workshop :
Learning to rank for information retrieval, pages 1?8.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79(1-2):151?175.
John Blitzer, Ryan Mcdonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proceedings of ICML,
pages 89?96.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise ap-
proach to listwise approach. In Proceedings of ICML,
pages 129 ? 136.
Depin Chen, Jun Yan, Gang Wang, Yan Xiong, Weiguo
Fan, and Zheng Chen. 2008a. Transrank: A novel
algorithm for transfer of rank learning. In Proceedings
of ICDM Workshops, pages 106?115.
Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun,
Larry Heck, and Belle Tseng. 2008b. Trada: Tree
based ranking function adaptation. In Proceedings of
CIKM.
Depin Chen, Yan Xiong, Jun Yan, Gui-Rong Xue, Gang
Wang, and Zheng Chen. 2010. Knowledge transfer
for cross domain learning to rank. Information Re-
trieval, 13(3):236?253.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2004.
An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4:933?
969.
Jianfeng Gao, Qiang Wu, Chris Burges, Krysta Svore,
Yi Su, Nazan Khan, Shalin Shah, and Hongyan Zhou.
2009. Model adaptation via model interpolation and
boosting for web search ranking. In Proceedings of
EMNLP.
Wei Gao, Peng Cai, Kam Fai Wong, and Aoying Zhou.
2010. Learning to rank only using training data from
related domain. In Proceedings of SIGIR, pages 162?
169.
Xiubo Geng, Tie-Yan Liu, Tao Qin, Andrew Arnold,
Hang Li, and Heung-Yeung Shum. 2008. Query de-
pendent ranking using k-nearest neighbor. In Proceed-
ings of SIGIR, pages 115?122.
Bo Geng, Linjun Yang, Chao Xu, and Xian-Sheng Hua.
2009. Ranking model adaptation for domain-specific
search. In Proceedings of CIKM.
R. Herbrich, T. Graepel, and K. Obermayer. 2000.
Large Margin Rank Boundaries for Ordinal Regres-
sion. MIT Press, Cambridge.
Jiayuan Huang, Alexander J. Smola, Arthur Gretton,
Karsten M. Borgwardt, and Bernhard Scho?lkopf.
2007. Correcting sample selection bias by unlabeled
data. In Proceedings of NIPS, pages 601?608.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
ACL.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of SIGKDD,
pages 133?142.
Maurice Kendall. 1970. Rank Correlation Methods.
Griffin.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information Re-
trieval, 3(3):225?331.
Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and
Qiang Yang. 2009. Domain adaptation via transfer
component analysis. In Proceedings of IJCAI, pages
1187?1192.
John C. Platt and John C. Platt. 1999. Probabilistic out-
puts for support vector machines and comparisons to
regularized likelihood methods. In Advances in Large
Margin Classifiers, pages 61?74. MIT Press.
Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. Letor:
A benchmark collection for research on learning to
rank for information retrieval. Information Retrieval,
13(4):346?374.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: Primal estimated sub-gradient solver for svm.
In Proceedings of the 24th International Conference
on Machine Learning, pages 807?814.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
Masashi Sugiyama, Shinichi Nakajima, Hisashi
Kashima, Paul von Bu?nau, and Motoaki Kawan-
abe. 2008. Direct importance estimation with
model selection and its application to covariate
shift adaptation. In Proceedings of NIPS, pages
1433?1440.
Ellen M. Voorhees. 2003. Overview of trec 2003. In
Proceedings of TREC-2003, pages 1?13.
Ellen M. Voorhees. 2004. Overview of trec 2004. In
Proceedings of TREC-2004, pages 1?12.
Bo Wang, Jie Tang, Wei Fan, Songcan Chen, Zi Yang,
and Yanzhu Liu. 2009. Heterogeneous cross domain
ranking in latent space. In Proceedings of CIKM.
121
Y. Yue, T. Finley, F. Radlinski, and T. Joachims. 2007.
A support vector method for optimizing average preci-
sion. In Proceedings of SIGIR, pages 271?278.
Bianca Zadrozny Zadrozny. 2004. Learning and evalu-
ating classifiers under sample selection bias. In Pro-
ceedings of ICML, pages 325?332.
122
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270?274,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Information-theoretic Multi-view Domain Adaptation
Pei Yang1,3, Wei Gao2, Qi Tan1, Kam-Fai Wong3
1South China University of Technology, Guangzhou, China
{yangpei,tanqi}@scut.edu.cn
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
wgao@qf.org.qa
3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
kfwong@se.cuhk.edu.hk
Abstract
We use multiple views for cross-domain doc-
ument classification. The main idea is to
strengthen the views? consistency for target
data with source training data by identify-
ing the correlations of domain-specific fea-
tures from different domains. We present
an Information-theoretic Multi-view Adapta-
tion Model (IMAM) based on a multi-way
clustering scheme, where word and link clus-
ters can draw together seemingly unrelated
domain-specific features from both sides and
iteratively boost the consistency between doc-
ument clusterings based on word and link
views. Experiments show that IMAM signifi-
cantly outperforms state-of-the-art baselines.
1 Introduction
Domain adaptation has been shown useful to many
natural language processing applications including
document classification (Sarinnapakorn and Kubat,
2007), sentiment classification (Blitzer et al, 2007),
part-of-speech tagging (Jiang and Zhai, 2007) and
entity mention detection (Daume? III and Marcu,
2006).
Documents can be represented by multiple inde-
pendent sets of features such as words and link struc-
tures of the documents. Multi-view learning aims
to improve classifiers by leveraging the redundancy
and consistency among these multiple views (Blum
and Mitchell, 1998; Ru?ping and Scheffer, 2005; Ab-
ney, 2002). Existing methods were designed for
data from single domain, assuming that either view
alone is sufficient to predict the target class accu-
rately. However, this view-consistency assumption
is largely violated in the setting of domain adapta-
tion where training and test data are drawn from dif-
ferent distributions.
Little research was done for multi-view domain
adaptation. In this work, we present an Information-
theoretical Multi-view Adaptation Model (IMAM)
based on co-clustering framework (Dhillon et al,
2003) that combines the two learning paradigms to
transfer class information across domains in multi-
ple transformed feature spaces. IMAM exploits a
multi-way-clustering-based classification scheme to
simultaneously cluster documents, words and links
into their respective clusters. In particular, the word
and link clusterings can automatically associate the
correlated features from different domains. Such
correlations bridge the domain gap and enhance the
consistency of views for clustering (i.e., classifying)
the target data. Results show that IMAM signifi-
cantly outperforms the state-of-the-art baselines.
2 Related Work
The work closely related to ours was done by Dai
et al (2007), where they proposed co-clustering-
based classification (CoCC) for adaptation learning.
CoCC was extended from information-theoretic co-
clustering (Dhillon et al, 2003), where in-domain
constraints were added to word clusters to provide
the class structure and partial categorization knowl-
edge. However, CoCC is a single-view algorithm.
Although multi-view learning (Blum and
Mitchell, 1998; Dasgupta et al, 2001; Abney,
2002; Sridharan and Kakade, 2008) is common
within a single domain, it is not well studied under
cross-domain settings. Chen et al (2011) proposed
270
CODA for adaptation based on co-training (Blum
and Mitchell, 1998), which is however a pseudo
multi-view algorithm where original data has only
one view. Therefore, it is not suitable for the
true multi-view case as ours. Zhang et al (2011)
proposed an instance-level multi-view transfer
algorithm that integrates classification loss and view
consistency terms based on large margin framework.
However, instance-based approach is generally poor
since new target features lack support from source
data (Blitzer et al, 2011). We focus on feature-level
multi-view adaptation.
3 Our Model
Intuitively, source-specific and target-specific fea-
tures can be drawn together by mining their
co-occurrence with domain-independent (common)
features, which helps bridge the distribution gap.
Meanwhile, the view consistency on target data can
be strengthened if target-specific features are appro-
priately bundled with source-specific features. Our
model leverages the complementary cooperation be-
tween different views to yield better adaptation per-
formance.
3.1 Representation
Let DS be the source training documents and DT
be the unlabeled target documents. Let C be the set
of class labels. Each source document ds ? DS is
labeled with a unique class label c ? C. Our goal
is to assign each target document dt ? DT to an
appropriate class as accurately as possible.
Let W be the vocabulary of the entire document
collectionD = DS?DT . LetL be the set of all links
(hyperlinks or citations) among documents. Each
d ? D can be represented by two views, i.e., a bag-
of-words set {w} and a bag-of-links set {l}.
Our model explores multi-way clustering that si-
multaneously clusters documents, words and links.
Let D?, W? and L? be the respective clustering of doc-
uments, words and links. The clustering functions
are defined as CD(d) = d? for document, CW (w) =
w? for word and CL(l) = l? for link, where d?, w? and l?
represent the corresponding clusters.
3.2 Objectives
We extend the information-theoretic co-clustering
framework (Dhillon et al, 2003) to incorporate the
loss frommultiple views. Let I(X,Y ) be mutual in-
formation (MI) of variables X and Y , our objective
is to minimize the MI loss of two different views:
? = ? ??W + (1? ?) ??L (1)
where
?W = I(DT ,W )? I(D?T , W? ) + ? ?
[
I(C,W )? I(C, W? )
]
?L = I(DT , L)? I(D?T , L?) + ? ?
[
I(C,L)? I(C, L?)
]
?W and ?L are the loss terms based on word view
and link view, respectively, traded off by ?. ? bal-
ances the effect of word or link clusters from co-
clustering. When ? = 1, the function relies on text
only that reduces to CoCC (Dai et al, 2007).
For any x ? x?, we define conditional distribution
q(x|y?) = p(x|x?)p(x?|y?) under co-clustering (X?, Y? )
based on Dhillon et al (2003). Therefore, for any
w ? w?, l ? l?, d ? d? and c ? C, we can calculate
a set of conditional distributions: q(w|d?), q(d|w?),
q(l|d?), q(d|l?), q(c|w?), q(c|l?).
Eq. 1 is hard to optimize due to its combinatorial
nature. We transform it to the equivalent form based
on Kullback-Leibler (KL) divergence between two
conditional distributions p(x|y) and q(x|y?), where
D(p(x|y)||q(x|y?)) =
?
x p(x|y)log
p(x|y)
q(x|y?) .
Lemma 1 (Objective functions) Equation 1 can
be turned into the form of alternate minimization:
(i) For document clustering, we minimize
? =
?
d
p(d)?D(d, d?) + ?C(W? , L?),
where ?C(W? , L?) is a constant1 and
?D(d, d?) =? ? D(p(w|d)||q(w|d?))
+ (1? ?) ? D(p(l|d)||q(l|d?)).
(ii) For word and link clustering, we minimize
? = ?
?
w
p(w)?W (w, w?)+(1??)
?
l
p(l)?L(l, l?),
where for any feature v (e.g., w or l) in feature set
V (e.g., W or L), we have
?V (v, v?) =D(p(d|v)||q(d|v?))
+ ? ? D(p(c|v)||q(c|v?)).
1We can obtain that ?C(W? , L?) =
?
[
?(I(C,W )? I(C, W? )) + (1? ?)(I(C,L)? I(C, L?))
]
,
which is constant since word/link clusters keep fixed during the
document clustering step.
271
Lemma 12 allows us to alternately reorder either
documents or both words and links by fixing the
other in such a way that the MI loss in Eq. 1 de-
creases monotonically.
4 Consistency of Multiple Views
In this section, we present how the consistency of
document clustering on target data could be en-
hanced among multiple views, which is the key issue
of our multi-view adaptation method.
According to Lemma 1, minimizing ?D(d, d?) for
each d can reduce the objective function value itera-
tively (t denotes round id):
C(t+1)D (d) = argmin
d?
[
? ? D(p(w|d)||q(t)(w|d?))
+(1? ?) ? D(p(l|d)||q(t)(l|d?))
]
(2)
In each iteration, the optimal document cluster-
ing function C(t+1)D is to minimize the weighted sum
of KL-divergences used in word-view and link-view
document clustering functions as shown above. The
optimal word-view and link-view clustering func-
tions can be denoted as follows:
C(t+1)DW (d) = argmin
d?
D(p(w|d)||q(t)(w|d?)) (3)
C(t+1)DL (d) = argmin
d?
D(p(l|d)||q(t)(l|d?)) (4)
Our central idea is that the document clusterings
C(t+1)DW and C
(t+1)
DL based on the two views are drawn
closer in each iteration due to the word and link
clusterings that bring together seemingly unrelated
source-specific and target-specific features. Mean-
while, C(t+1)D combines the two views and reallo-
cates the documents so that it remains consistent
with the view-based clusterings as much as possi-
ble. The more consistent the views, the better the
document clustering, and then the better the word
and link clustering, which creates a positive cycle.
4.1 Disagreement Rate of Views
For any document, a consistency indicator function
with respect to the two view-based clusterings can
be defined as follows (t is omitted for simplicity):
2Due to space limit, the proof of all lemmas will be given in
a long version of the paper.
Definition 1 (Indicator function) For any d ? D,
?CDW ,CDL (d) =
{
1, if CDW (d) = CDL(d);
0, otherwise
Then we define the disagreement rate between two
view-based clustering functions:
Definition 2 (Disagreement rate)
?(CDW , CDL) = 1?
?
d?D ?CDW ,CDL (d)
|D| (5)
Abney (2002) suggests that the disagreement rate
of two independent hypotheses upper-bounds the er-
ror rate of either hypothesis. By minimizing the dis-
agreement rate on unlabeled data, the error rate of
each view can be minimized (so does the overall er-
ror). However, Eq. 5 is not continuous nor convex,
which is difficult to optimize directly. By using the
optimization based on Lemma 1, we can show em-
pirically that disagreement rate is monotonically de-
creased (see Section 5).
4.2 View Combination
In practice, view-based document clusterings in
Eq. 3 and 4 are not computed explicitly. Instead,
Eq. 2 directly optimizes view combination and pro-
duces the document clustering. Therefore, it is nec-
essary to disclose how consistent it could be with the
view-based clusterings.
Suppose ? = {FD|FD(d) = d?, d? ? D?} is
the set of all document clustering functions. For
any FD ? ?, we obtain the disagreement rate
?(FD, CDW ? CDL), where CDW ? CDL denotes the
clustering resulting from the overlap of the view-
based clusterings.
Lemma 2 CD always minimizes the disagreement
rate for any FD ? ? such that
?(CD, CDW ? CDL) = minFD??
?(FD, CDW ? CDL)
Meanwhile, ?(CD, CDW ? CDL) = ?(CDW , CDL).
Lemma 2 suggests that IMAM always finds the
document clustering with the minimal disagreement
rate to the overlap of view-based clusterings, and the
minimal value of disagreement rate equals to the dis-
agreement rate of the view-based clusterings.
272
Table 1: View disagreement rate ? and error rate ? that
decrease with iterations and their Pearson?s correlation ?.
Round 1 2 3 4 5 ?
DA-EC ? 0.194 0.153 0.149 0.144 0.144 0.998? 0.340 0.132 0.111 0.101 0.095
DA-NT ? 0.147 0.083 0.071 0.065 0.064 0.996? 0.295 0.100 0.076 0.069 0.064
DA-OS ? 0.129 0.064 0.052 0.047 0.041 0.998? 0.252 0.092 0.068 0.060 0.052
DA-ML ? 0.166 0.102 0.071 0.065 0.064 0.984? 0.306 0.107 0.076 0.062 0.054
EC-NT ? 0.311 0.250 0.228 0.219 0.217 0.988? 0.321 0.137 0.112 0.096 0.089
5 Experiments and Results
Data and Setup
Cora (McCallum et al, 2000) is an online archive
of computer science articles. The documents in the
archive are categorized into a hierarchical structure.
We selected a subset of Cora, which contains 5 top
categories and 10 sub-categories. We used a similar
way as Dai et al (2007) to construct our training and
test sets. For each set, we chose two top categories,
one as positive class and the other as the negative.
Different sub-categories were deemed as different
domains. The task is defined as top category classifi-
cation. For example, the dataset denoted as DA-EC
consists of source domain: DA 1(+), EC 1(-); and
target domain: DA 2(+), EC 2(-).
The classification error rate ? is measured as the
proportion of misclassified target documents. In or-
der to avoid the infinity values, we applied Laplacian
smoothing when computing the KL-divergence. We
tuned ?, ? and the number of word/link clusters by
cross-validation on the training data.
Results and Discussions
Table 1 shows the monotonic decrease of view dis-
agreement rate ? and error rate ? with the iterations
and their Pearson?s correlation ? is nearly perfectly
positive. This indicates that IMAM gradually im-
proves adaptation by strengthening the view consis-
tency. This is achieved by the reinforcement of word
and link clusterings that draw together target- and
source-specific features that are originally unrelated
but co-occur with the common features.
We compared IMAM with (1) Transductive SVM
(TSVM) (Joachims, 1999) using both words and
links features; (2) Co-Training (Blum and Mitchell,
Table 2: Comparison of error rate with baselines.
Data TSVM Co-Train CoCC MVTL-LM IMAM
DA-EC 0.214 0.230 0.149 0.192 0.138
DA-NT 0.114 0.163 0.106 0.108 0.069
DA-OS 0.262 0.175 0.075 0.068 0.039
DA-ML 0.107 0.171 0.109 0.183 0.047
EC-NT 0.177 0.296 0.225 0.261 0.192
EC-OS 0.245 0.175 0.137 0.176 0.074
EC-ML 0.168 0.206 0.203 0.264 0.173
NT-OS 0.396 0.220 0.107 0.288 0.070
NT-ML 0.101 0.132 0.054 0.071 0.032
OS-ML 0.179 0.128 0.051 0.126 0.021
Average 0.196 0.190 0.122 0.174 0.085
1998); (3) CoCC (Dai et al, 2007): Co-clustering-
based single-view transfer learner (with text view
only); and (4) MVTL-LM (Zhang et al, 2011):
Large-margin-based multi-view transfer learner.
Table 2 shows the results. Co-Training performed
a little better than TSVM by boosting the confidence
of classifiers built on the distinct views in a comple-
mentary way. But since Co-Training doesn?t con-
sider the distribution gap, it performed clearly worse
than CoCC even though CoCC has only one view.
IMAM significantly outperformed CoCC on all
the datasets. In average, the error rate of IMAM
is 30.3% lower than that of CoCC. This is because
IMAM effectively leverages distinct and comple-
mentary views. Compared to CoCC, using source
training data to improve the view consistency on tar-
get data is the key competency of IMAM.
MVTL-LM performed worse than CoCC. It sug-
gests that instance-based approach is not effective
when the data of different domains are drawn from
different feature spaces. Although MVTL-LM regu-
lates view consistency, it cannot identify the associ-
ations between target- and source-specific features
that is the key to the success of adaptation espe-
cially when domain gap is large and less common-
ality could be found. In contrast, CoCC and IMAM
uses multi-way clustering to find such correlations.
6 Conclusion
We presented a novel feature-level multi-view do-
main adaptation approach. The thrust is to incor-
porate distinct views of document features into the
information-theoretic co-clustering framework and
strengthen the consistency of views on clustering
(i.e., classifying) target documents. The improve-
ments over the state-of-the-arts are significant.
273
References
Steven Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 360-367.
John Blitzer, Mark Dredze and Fernado Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 440-447.
John Blitzer, Sham Kakade and Dean P. Foster. 2011.
Domain Adaptation with Coupled Subspaces. In Pro-
ceedings of the 14th International Conference on Arti-
ficial Intelligence and Statistics (AISTATS), pages 173-
181.
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the 11th Annual Conference on Computa-
tional Learning Theory, pages 92-100.
Minmin Chen, Killian Q. Weinberger and John Blitzer.
2011. Co-Training for Domain Adaptation. In Pro-
ceedings of NIPS, pages 1-9.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang and Yong
Yu. 2007. Co-clustering Based Classification for Out-
of-domain Documents. In Proceedings of the 13th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 210-219.
Sanjoy Dasgupta, Michael L. Littman and David
McAllester. 2001. PAC Generalization Bounds for
Co-Training. In Proceeding of NIPS, pages 375-382.
Hal Daume? III and Daniel Marcu. 2006. Domain Adap-
tation for Statistical Classifiers. Journal of Artificial
Intelligence Research, 26(2006):101-126.
Inderjit S. Dhillon, Subramanyam Mallela and Dharmen-
dra S. Modha. 2003. Information-Theoretic Co-
clustering. In Proceedings of the ninth ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 210-219.
Thorsten Joachims. 1999. Transductive Inference for
Text Classification using Support Vector Machines. In
Proceedings of Sixteenth International Conference on
Machine Learning, pages 200-209.
Jing Jiang and Chengxiang Zhai. 2007. Instance Weight-
ing for Domain Adaptation in NLP. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 264-271.
Andrew K. McCallum, Kamal Nigam, Jason Rennie and
Kristie Seymore. 2000. Automating the Construction
of Internet Portals with Machine Learning. Informa-
tion Retrieval, 3(2):127-163.
Stephan Ru?ping and Tobias Scheffer. 2005. Learning
with Multiple Views. In Proceedings of ICML Work-
shop on Learning with Multiple Views.
Kanoksri Sarinnapakorn and Miroslav Kubat. 2007.
Combining Sub-classifiers in Text Categorization: A
DST-Based Solution and a Case Study. IEEE Transac-
tions Knowledge and Data Engineering, 19(12):1638-
1651.
Karthik Sridharan and Sham M. Kakade. 2008. An In-
formation Theoretic Framework for Multi-view Learn-
ing. In Proceedings of the 21st Annual Conference on
Computational Learning Theory, pages 403-414.
Dan Zhang, Jingrui He, Yan Liu, Luo Si and Richard D.
Lawrence. 2011. Multi-view Transfer Learning with
a Large Margin Approach. In Proceedings of the 17th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 1208-1216.
274
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 58?62,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Empirical Study on Uncertainty Identification in Social Media Context
Zhongyu Wei1, Junwen Chen1, Wei Gao2,
Binyang Li1, Lanjun Zhou1, Yulan He3, Kam-Fai Wong1
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
3School of Engineering & Applied Science, Aston University, Birmingham, UK
{zywei,jwchen,byli,ljzhou,kfwong}@se.cuhk.edu.hk
wgao@qf.org.qa, y.he@cantab.net
Abstract
Uncertainty text detection is important
to many social-media-based applications
since more and more users utilize social
media platforms (e.g., Twitter, Facebook,
etc.) as information source to produce
or derive interpretations based on them.
However, existing uncertainty cues are in-
effective in social media context because
of its specific characteristics. In this pa-
per, we propose a variant of annotation
scheme for uncertainty identification and
construct the first uncertainty corpus based
on tweets. We then conduct experiments
on the generated tweets corpus to study the
effectiveness of different types of features
for uncertainty text identification.
1 Introduction
Social media is not only a social network tool for
people to communicate but also plays an important
role as information source with more and more
users searching and browsing news on it. People
also utilize information from social media for de-
veloping various applications, such as earthquake
warning systems (Sakaki et al, 2010) and fresh
webpage discovery (Dong et al, 2010). How-
ever, due to its casual and word-of-mouth pecu-
liarities, the quality of information in social me-
dia in terms of factuality becomes a premier con-
cern. Chances are there for uncertain information
or even rumors flooding in such a context of free
form. We analyzed a tweet dataset which includes
326,747 posts (Details are given in Section 3) col-
lected during 2011 London Riots, and result re-
veals that at least 18.91% of these tweets bear un-
certainty characteristics1. Therefore, distinguish-
ing uncertain statements from factual ones is cru-
cial for users to synthesize social media informa-
tion to produce or derive reliable interpretations,
1The preliminary study was done based on a manually de-
fined uncertainty cue-phrase list. Tweets containing at least
one hedge cue were treated as uncertain.
and this is expected helpful for applications like
credibility analysis (Castillo et al, 2011) and ru-
mor detection (Qazvinian et al, 2011) based on
social media.
Although uncertainty has been studied theoret-
ically for a long time as a grammatical phenom-
ena (Seifert and Welte, 1987), the computational
treatment of uncertainty is a newly emerging area
of research. Szarvas et al (2012) pointed out that
?Uncertainty - in its most general sense - can be
interpreted as lack of information: the receiver of
the information (i.e., the hearer or the reader) can-
not be certain about some pieces of information?.
In recent years, the identification of uncertainty
in formal text, e.g., biomedical text, reviews or
newswire, has attracted lots of attention (Kilicoglu
and Bergler, 2008; Medlock and Briscoe, 2007;
Szarvas, 2008; Light et al, 2004). However, un-
certainty identification in social media context is
rarely explored.
Previous research shows that uncertainty identi-
fication is domain dependent as the usage of hedge
cues varies widely in different domains (Morante
and Sporleder, 2012). Therefore, the employment
of existing out-of-domain corpus to social media
context is ineffective. Furthermore, compared to
the existing uncertainty corpus, the expression of
uncertainty in social media is fairly different from
that in formal text in a sense that people usu-
ally raise questions or refer to external informa-
tion when making uncertain statements. But, nei-
ther of the uncertainty expressions can be repre-
sented based on the existing types of uncertainty
defined in the literature. Therefore, a different un-
certainty classification scheme is needed in social
media context.
In this paper, we propose a novel uncertainty
classification scheme and construct the first uncer-
tainty corpus based on social media data ? tweets
in specific here. And then we conduct experi-
ments for uncertainty post identification and study
the effectiveness of different categories of features
based on the generated corpus.58
2 Related work
We introduce some popular uncertainty corpora
and methods for uncertainty identification.
2.1 Uncertainty corpus
Several text corpora from various domains have
been annotated over the past few years at different
levels (e.g., expression, event, relation, sentence)
with information related to uncertainty.
Sauri and Pustejovsky (2009) presented a cor-
pus annotated with information about the factu-
ality of events, namely Factbank, which is con-
structed based on TimeBank2 containing 3,123 an-
notated sentences from 208 news documents with
8 different levels of uncertainty defined.
Vincze et al (2008) constructed the BioSocpe
corpus, which consists of medical and biological
texts annotated for negation, uncertainty and their
linguistic scope. This corpus contains 20,924 sen-
tences.
Ganter et al (2009) generated Wikipedia
Weasels Corpus, where Weasel tags in Wikipedia
articles is adopted readily as labels for uncertainty
annotation. It contains 168,923 unique sentences
with 437 weasel tags in total.
Although several uncertainty corpora exist,
there is not a uniform set of standard for uncer-
tainty annotation. Szarvas et al (2012) normal-
ized the annotation of the three corpora aforemen-
tioned. However, the context of these corpora
is different from that of social media. Typically,
these documents annotated are grammatically cor-
rect, carefully punctuated, formally structured and
logically expressed.
2.2 Uncertainty identification
Previous work on uncertainty identification fo-
cused on classifying sentences into uncertain
or definite categories. Existing approaches are
mainly based on supervised methods (Light et
al., 2004; Medlock and Briscoe, 2007; Medlock,
2008; Szarvas, 2008) using the annotated corpus
with different types of features including Part-Of-
Speech (POS) tags, stems, n-grams, etc..
Classification of uncertain sentences was con-
solidated as a task in the 2010 edition of CoNLL
shared task on learning to detect hedge cues
and their scope in natural language text (Farkas
et al, 2010). The best system for Wikipedia
data (Georgescul, 2010) employed Support Vector
Machine (SVM), and the best system for biolog-
ical data (Tang et al, 2010) adopted Conditional
2http://www.timeml.org/site/timebank/
timebank.html
Random Fields (CRF).
In our work, we conduct an empirical study of
uncertainty identification on tweets dataset and ex-
plore the effectiveness of different types of fea-
tures (i.e., content-based, user-based and Twitter-
specific) from social media context.
3 Uncertainty corpus for microblogs
3.1 Types of uncertainty in microblogs
Traditionally, uncertainty can be divided into
two categories, namely Epistemic and Hypothet-
ical (Kiefer, 2005). For Epistemic, there are two
sub-classes Possible and Probable. For Hypotheti-
cal, there are four sub-classes including Investiga-
tion, Condition, Doxastic andDynamic. The detail
of the classification is described as below (Kiefer,
2005):
Epistemic: On the basis of our world knowledge
we cannot decide at the moment whether the
statement is true or false.
Hypothetical: This type of uncertainty includes
four sub-classes:
? Doxastic: Expresses the speaker?s be-
liefs and hypotheses.
? Investigation: Proposition under inves-
tigation.
? Condition: Proposition under condi-
tion.
? Dynamic: Contains deontic, disposi-
tional, circumstantial and buletic modal-
ity.
Compared to the existing uncertainty corpora,
social media authors enjoy free form of writing.
In order to study the difference, we annotated a
small set of 827 randomly sampled tweets accord-
ing to the scheme of uncertainty types above, in
which we found 65 uncertain tweets. And then,
we manually identified all the possible uncertain
tweets, and found 246 really uncertain ones out of
these 827 tweets, which means that 181 uncertain
tweets are missing based on this scheme. We have
the following three salient observations:
? Firstly, there is no tweet found with the type of
Investigation. We find people seldom use words
like ?examine? or ?test? (indicative words of In-
vestigation category) when posting tweets. Once
they do this, the statement should be considered
as highly certain. For example, @dobibid I have
tested the link, it is fake!
? Secondly, people frequently raise questions
about some specific topics for confirmation which
expresses uncertainty. For example, @ITVCentral59
Can you confirm that Birmingham children?s hos-
pital has/hasn?t been attacked by rioters?
? Thirdly, people tend to post message with exter-
nal information (e.g., story from friends) which re-
veals uncertainty. For example, Friend who works
at the children?s hospital in Birmingham says the
riot police are protecting it.
Based on these observations, we propose a vari-
ant of uncertainty types in social media context
by eliminating the category of Investigation and
adding the category of Question and External un-
der Hypothetical, as shown in Table 3.1. Note
that our proposed scheme is based on Kiefer?s
work (2005) which was previously extended to
normalize uncertainty corpora in different genres
by Szarvas et al (2012). But we did not try these
extended schema for specific genres since even the
most general one (Kiefer, 2005) was proved un-
suitable for social media context.
3.2 Annotation result
The dataset we annotated was collected from Twit-
ter using Streaming API during summer riots
in London during August 6-13 2011, including
326,747 tweets in total. Search criteria include
hashtags like #ukriots, #londonriots, #prayforlon-
don, and so on. We further extracted the tweets
relating to seven significant events during the riot
identified by UK newspaper The Guardian from
this set of tweets. We annotated all the 4,743 ex-
tracted tweets for the seven events3.
Two annotators were trained to annotate the
dataset independently. Given a collection of
tweets T = {t1, t2, t3...tn}, the annotation task is
to label each tweet ti as either uncertain or cer-
tain. Uncertainty assertions are to be identified
in terms of the judgements about the author?s in-
tended meaning rather than the presence of uncer-
tain cue-phrase. For those tweets annotated as un-
certain, sub-class labels are also required accord-
ing to the classification indicated in Table 3.1 (i.e.,
multi-label is allowed).
The Kappa coefficient (Carletta, 1996) indi-
cating inter-annotator agreement was 0.9073 for
the certain/uncertain binary classification and was
0.8271 for fine-grained annotation. The conflict
labels from the two annotators were resolved by a
third annotator. Annotation result is displayed in
Table 3.2, where 926 out of 4,743 tweets are la-
beled as uncertain accounting for 19.52%. Ques-
tion is the uncertainty category with most tweets,
followed by External. Only 21 tweets are labeled
3http://www.guardian.co.uk/
uk/interactive/2011/dec/07/
london-riots-twitter
Tweet# 4743
Uncertainty# 926
Epistemic Possible# 16Probable# 129
Hypothetical
Condition# 71
Doxastic# 48
Dynamic# 21
External# 208
Question# 488
Table 2: Statistics of annotation result
as Dynamic and all of them are buletic modal-
ity4 which shares similarity with Doxastic. There-
fore, we consider Dynamic together with Domes-
tic in the error analysis for simplicity. During
the preliminary annotation, we found that uncer-
tainty cue-phrase is a good indicator for uncer-
tainty tweets since tweets labeled as uncertain al-
ways contain at least one cue-phrase. Therefore,
annotators are also required identify cue-phrases
which trigger the sense of uncertainty in the tweet.
All cue-phrases appearing more than twice are col-
lected to form a uncertainty cue-phrase list.
4 Experiment and evaluation
We aim to identify those uncertainty tweets from
tweet collection automatically based on machine
learning approaches. In addition to n-gram fea-
tures, we also explore the effectiveness of three
categories of social media specific features includ-
ing content-based, user-based and Twitter-specific
ones. The description of the three categories of
features is shown in Table 4. Since the length of
tweet is relatively short, we therefore did not carry
out stopwords removal or stemming.
Our preliminary experiments showed that com-
bining unigrams with bigrams and trigrams gave
better performance than using any one or two of
these three features. Therefore, we just report the
result based on the combination of them as n-gram
features. Five-fold cross validation is used for
evaluation. Precision, recall and F-1 score of un-
certainty category are used as the metrics.
4.1 Overall performance
The overall performance of different approaches
is shown in Table 4.1. We used uncertainty cue-
phrase matching approach as baseline, denoted
by CP. For CP, we labeled tweets containing at
least one entry in uncertainty cue-phrase list (de-
scribed in Section 3) as uncertain. All the other
approaches are supervised methods using SVM
based on different feature sets. n-gram stands for
n-gram feature set, C means content-based feature
set, U denotes user-based feature set, T represents
4Proposition expresses plans, intentions or desires.60
Category Subtype Cue Phrase Example
Epistemic Possible, etc. may, etc. It may be raining.Probable likely, etc. It is probably raining.
Hypothetical
Condition if, etc. If it rains, we?ll stay in.
Doxastic believe, etc. He believes that the Earth is flat.
Dynamic hope, etc. fake picture of the london eye on fire... i hope
External someone said, etc. Someone said that London zoo was attacked.
Question seriously?, etc. Birmingham riots are moving to the children hospital?! seriously?
Table 1: Classification of uncertainty in social media context
Category Name Description
Content-based
Length Length of the tweet
Cue Phrase Whether the tweet contains a uncertainty cue
OOV Ratio Ratio of words out of vocabulary
Twitter-specific
URL Whether the tweet contains a URL
URL Count Frequency of URLs in corpus
Retweet Count How many times has this tweet been retweeted
Hashtag Whether the tweet contains a hashtag
Hashtag Count Number of Hashtag in tweets
Reply Is the current tweet a reply tweet
Rtweet Is the current tweet a retweet tweet
User-based
Follower Count Number of follower the user owns
List Count Number of list the users owns
Friend Count Number of friends the user owns
Favorites Count Number of favorites the user owns
Tweet Count Number of tweets the user published
Verified Whether the user is verified
Table 3: Feature list for uncertainty classification
Approach Precision Recall F-1
CP 0.3732 0.9589 0.5373
SVMn?gram 0.7278 0.8259 0.7737
SVMn?gram+C 0.8010 0.8260 0.8133
SVMn?gram+U 0.7708 0.8271 0.7979
SVMn?gram+T 0.7578 0.8266 0.7907
SVMn?gram+ALL 0.8162 0.8269 0.8215
SVMn?gram+Cue Phrase 0.7989 0.8266 0.8125SVMn?gram+Length 0.7372 0.8216 0.7715SVMn?gram+OOV Ratio 0.7414 0.8233 0.7802
Table 4: Result of uncertainty tweets identification
Twitter-specific feature set and ALL is the combi-
nation of C, U and T.
Table 4.1 shows that CP achieves the best recall
but its precision is the lowest. The learning based
methods with different feature sets give some sim-
ilar recalls. Compared to CP, SVMn?gram in-
creases the F-1 score by 43.9% due to the salient
improvement on precision and small drop of re-
call. The performance improves in terms of pre-
cision and F-1 score when the feature set is ex-
panded by adding C, U or T onto n-gram, where
+C brings the highest gain, and SVMn?gram+ALL
performs best in terms of precision and F-1 score.
We then study the effectiveness of the three
content-based features, and result shows that the
presence of uncertain cue-phrase is most indica-
tive for uncertainty tweet identification.
4.2 Error analysis
We analyze the prediction errors based on
SVMn?gram+ALL. The distribution of errors in
terms of different types of uncertainty is shown
Type Poss. Prob. D.&D. Cond. Que. Ext.
Total# 16 129 69 71 488 208
Error# 11 20 18 11 84 40
% 0.69 0.16 0.26 0.15 0.17 0.23
Table 5: Error distributions
in Table 4.2. Our method performs worst on the
type of Possible and on the combination of Dy-
namic and Doxastic because these two types have
the least number of samples in the corpus and the
classifier tends to be undertrained without enough
samples.
5 Conclusion and future work
In this paper, we propose a variant of classification
scheme for uncertainty identification in social me-
dia and construct the first uncertainty corpus based
on tweets. We perform uncertainty identification
experiments on the generated dataset to explore
the effectiveness of different types of features. Re-
sult shows that the three categories of social media
specific features can improve uncertainty identifi-
cation. Furthermore, content-based features bring
the highest improvement among the three and the
presence of uncertain cue-phrase contributes most
for content-based features.
In future, we will explore to use uncertainty
identification for social media applications.
6 Acknowledgement
This work is partially supported by General Re-
search Fund of Hong Kong (No. 417112).61
References
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
linguistics, 22(2):249?254.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter.
In Proceedings of the 20th International Conference
on World Wide Web, pages 675?684.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 331?340. ACM.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The conll-
2010 shared task: learning to detect hedges and their
scope in natural language text. In Proceedings of
the 14th Conference on Computational Natural Lan-
guage Learning?Shared Task, pages 1?12. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009, pages 173?
176. Association for Computational Linguistics.
Maria Georgescul. 2010. A hedgehop over a max-
margin framework using hedge cues. In Proceed-
ings of the 14th Conference on Computational Natu-
ral Language Learning?Shared Task, pages 26?31.
Association for Computational Linguistics.
Ferenc Kiefer. 2005. Lehetoseg es szuk-
segszeruseg[Possibility and necessity]. Tinta Kiado,
Budapest.
H. Kilicoglu and S. Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a
linguistically motivated perspective. BMC bioinfor-
matics, 9(Suppl 11):S10.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings
of BioLink 2004 workshop on linking biological lit-
erature, ontologies and databases: tools for users,
pages 17?24.
B. Medlock and T. Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific litera-
ture. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
992?999.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636?654.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational Linguistics, 38(2):223?260.
Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it:
Identifying misinformation in microblogs. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1589?1599.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, pages 851?860. ACM.
R. Saur?? and J. Pustejovsky. 2009. Factbank: A cor-
pus annotated with event factuality. Language Re-
sources and Evaluation, 43(3):227?268.
Stephan Seifert and Werner Welte. 1987. A basic bib-
liography on negation in natural language, volume
313. Gunter Narr Verlag.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335?367.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text.
In Proceedings of the 14th Conference on Compu-
tational Natural Language Learning?Shared Task,
pages 13?17. Association for Computational Lin-
guistics.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and
J. Csirik. 2008. The bioscope corpus: biomedical
texts annotated for uncertainty, negation and their
scopes. BMC bioinformatics, 9(Suppl 11):S9.
62
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 97?102,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Web Information Mining and Decision Support Platform for the  
Modern Service Industry 
 
Binyang Li1,2, Lanjun Zhou2,3, Zhongyu Wei2,3, Kam-fai Wong2,3,4,  
Ruifeng Xu5, Yunqing Xia6 
 
1 Dept. of Information Science & Technology, University of International Relations, China 
2Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong 
Kong, Shatin, N.T., Hong Kong  
3MoE Key Laboratory of High Confidence Software Technologies, China 
4 Shenzhen Research Institute, The Chinese University of Hong Kong 
5Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China 
6Department of Computer Science & Technology, TNList, Tsinghua University, China 
{byli,ljzhou,zywei,kfwong}@se.cuhk.edu.hk  
 
 
Abstract 
This demonstration presents an intelligent infor-
mation platform MODEST. MODEST will pro-
vide enterprises with the services of retrieving 
news from websites, extracting commercial in-
formation, exploring customers? opinions, and 
analyzing collaborative/competitive social net-
works. In this way, enterprises can improve the 
competitive abilities and facilitate potential col-
laboration activities. At the meanwhile, MOD-
EST can also help governments to acquire in-
formation about one single company or the entire 
board timely, and make prompt strategies for 
better support. Currently, MODEST is applied to 
the pillar industries of Hong Kong, including 
innovative finance, modem logistics, information 
technology, etc. 
1 Introduction 
With the rapid development of Web 2.0, the 
amount of information is exploding. There are 
millions of events towards companies and bil-
lions of opinions on products generated every 
day (Liu, 2012). Such enormous information 
cannot only facilitate companies to improve their 
competitive abilities, but also help government to 
make prompt decisions for better support or 
timely monitor, e.g. effective risk management. 
For this reason, there is a growing demand of 
Web information mining and intelligent decision 
support services for the industries. Such services 
are collectively referred as modern service, 
which includes the following requirements: 
(1) To efficiently retrieve relevant information 
from the websites; 
(2) To accurately determine the latest business 
news and trends of the company; 
(3) To identify and analyze customers? opinions 
towards the company; 
(4) To explore the collaborative and competitive 
relationship with other companies; 
(5) To leverage the knowledge mined from the 
business news and company social network 
for decision support. 
In this demonstration, we will present a Web 
information mining and decision support plat-
form, MODEST1. The objective of MODEST is 
to provide modern services for both enterprises 
and government, including collecting Web in-
formation, making deep analysis, and providing 
supporting decision. The innovation of MOD-
EST is focusing on deep analysis which incor-
porates the following functions: 
? Topic detection and tracking function is to 
cluster the hot events and capture the rela-
tionship between the relevant events based on 
the collected data from websites (event also 
referred as topic in this paper). In order to re-
alize this function, Web mining techniques 
are adopted, e.g. topic clustering, heuristics 
algorithms, etc. 
? The second function is to identify and analyze 
customers? opinions about the company. 
Opinion mining technology (Zhou et al., 2010) 
is adopted to determine the polarity of those 
news, which can help the company timely and 
appropriately adjust the policy to strengthen 
the dominant position or avoid risks. 
                                                          
1 This work is supported by the Innovation and Technology 
Fund of Hong Kong SAR. 
97
? The third function is to explore and analyze 
social network based on the company centric. 
We utilize social network analysis (SNA) 
technology (Xia et al., 2010) to discover the 
relationships, and we further analyze the con-
tent in fine-grained granularity to identify its 
potential partners or competitors.  
With the help of MODEST, the companies can 
acquire modern service-related information, and 
timely adjust corporate policies and marketing 
plan ahead. Hence, the ability of information ac-
quisition and the competitiveness of the enter-
prises can be improved accordingly. 
In this paper, we will use a practical example 
to illustrate our platform and evaluate the per-
formance of main functions.  
The rest of this paper is organized as follows. 
Section 2 will introduce the system description 
as well as the main functions implementation. 
The practical case study will be illustrated in 
Section 3. The performance of MODEST will be 
evaluated in Section 4.Finally, this paper will be 
concluded in Section 5. 
2 System Description 
In this section, we first outline the system archi-
tecture of MODEST, and then describe the im-
plementation of the main functionality in detail.  
2.1 Architecture and Workflow 
The MODEST system consists of three modules: 
data acquisition, data analysis, and result display. 
The system architecture is shown in Figure 1. 
 
Figure 1: System architecture. (The module in 
blue is data acquisition, the module in orange is 
data analysis, and the module in light green is 
result display) 
(1) The core technique in the data acquisition 
module is the crawler, which is developed to 
collect raw data from websites, e.g. news portals, 
blogosphere. Then the system parse the raw web 
pages and extract information to store in the local 
database for further processing. 
(2) The data analysis module can be divided into 
two parts:  
? NLP pre-processor: utilizes NLP (natural 
language processing) techniques and some 
toolkits to perform the pre-processing on the 
raw data in (1), including word segmenta-
tion, part-of-speech (POS) tagging1, stop-
word removal, and named entity recognition 
(NER)2. We then create knowledgebase for 
individual industry, such as domain-specific 
sentiment word lexicon, name entity collec-
tion, and so on. 
? Miner?makes use of data mining techniques 
to realize four functions, topic detection and 
tracking (TDT), multi-document summari-
zation 3  (MDS), social network analysis 
(SNA), and opinion mining (OM). The re-
sults of data analysis are also stored in the 
database.  
(3) The result display module read out the analy-
sis results from the database and display them to 
users in the form of plain text, charts, figures, as 
well as video. 
2.2 Function Implementation 
Since the innovation of MODEST is focusing on 
the module of data analysis, we will describe its 
main functions in detail, including topic detec-
tion and tracking, opinion mining, and social 
networks analysis. 
2.2.1 Topic Detection and Tracking 
The TDT function targets on detecting and 
tracking the hot topics for each individual com-
pany. Given a period of data collected from web-
sites, there are various discussions about the 
company. In order to extract these topics, clus-
tering methods (Viermetz et al., 2007 and Yoon 
et al., 2009) are implemented to explore the top-
ics. Note that during the period of data collection, 
different topics with respect to the same compa-
ny may have relations. We, therefore, utilize hi-
erarchical clustering methods4to capture the po-
tential relations.  
Due to the large amount of data, it is impossi-
ble to view all the topics at a snapshot. MODEST 
utilizes topic tracking technique (Wang et al., 
2008) to identify related stories with a stream of 
                                                          
1 www.ictclas.org 
2http://ir.hit.edu.cn/demo/ltp 
3http://libots.sourceforge.net/ 
4http://dragon.ischool.drexel.edu/ 
Raw
files
Pre-processed
files
Database
Crawler UI
Word
segmentation
Web
NER
Stopword
removal
POS
tagging
NLP pre-p ocessor
TDT OM
MDS SNS
Miner
Data Layer
98
media. It is convenient for the users to see the 
latest information about the company.  
In summary, TDT function provides the ser-
vices of detecting and tracking the latest and 
emergent topics, analyzing the relationships of 
topics on the dynamics of the company. It meets 
the aforementioned demand, ?to accurately grasp 
the latest business news and trends of the com-
pany?. 
2.2.2 Opinion Mining 
The objective of OM function is to discover 
opinions towards a company and classify the 
opinions into positive, negative, or neutral. 
The opinion mining function is redesigned 
based on our own opinion mining engine (Zhou 
et al., 2010). It separates opinion identification 
and polarity classification into two stages.  
Given a set of documents that are relevant to 
the company, we first split the documents into 
sentences, and then identify whether the sentence 
is opinionated or not. We extract the features 
shown in Table 1 for opinion identification. 
(Zhou et al., 2010) 
Table 1: Features adopted in the opinionated 
sentence classifier 
Punctuation level features 
The presence of direct quote punctuation "?" and "?"  
The presence of other punctuations: "?" and "!" 
Word-Level and entity-level features 
The presence of known opinion operators 
The percentage of known opinion word in sentence 
Presence of a named entity 
Presence of pronoun 
Presence of known opinion indicators 
Presence of known degree adverbs 
Presence of known conjunctions 
Bi-gram features 
Named entities + opinion operators 
Pronouns + opinion operators 
Nouns or named entities + opinion words 
Pronouns + opinion words 
Opinion words (adjective) + opinion words(noun) 
Degree adverbs + opinion words 
Degree adverbs + opinion operators 
These features are then combined using a ra-
dial basis function (RBF) kernel and a support 
vector machine (SVM) classifier (Drucker et al., 
1997) is trained based on the NTCIR 8training 
data for opinion identification (Kando, 2010). 
For those opinionated sentences, we then clas-
sify them into positive, negative, or neutral. In 
addition to the features shown in Table 1, we 
incorporate features of s-VSM (Sentiment Vector 
Space Model) (Xia et al., 2008) to enhance the 
performance. The principles of the s-VSM are 
listed as follows: (1) Only sentiment-related 
words are used to produce sentiment features for 
the s-VSM. (2) The sentiment words are appro-
priately disambiguated with the neighboring ne-
gations and modifiers. (3) Negations and modifi-
ers are included in the s-VSM to reflect the func-
tions of inversing, strengthening and weakening. 
Sentiment unit is the appropriate element com-
plying with the above principles. (Zhou et al., 
2010) 
In addition to polarity classification, opinion 
holder and target are also recognized in OM 
function for further identifying the relationship 
that two companies have, e.g. collaborative or 
competitive. Both of the dependency parser and 
the semantic role labeling1 (SRL) tool are in-
corporated to identify the semantic roles of each 
chunk based on verbs in the sentence. 
The OM function provides the company with 
services of analyzing the social sentimental 
feedback on the dynamics of the company. It 
meets the aforementioned demand, ?to identify 
and analyze customers? opinions towards the 
company?. 
2.2.3 Social Network Analysis  
SNA function aims at producing the commercial 
network of companies that are hidden within the 
articles.  
To achieve this goal, we maintain two lexicons, 
the commercial named entity lexicon and com-
mercial relation lexicon. Commercial named en-
tity are firstly located within the text and then 
recorded in the commercial entity lexicon in the 
pre-processor NER. Commercial relation lexicon 
record the articles/documents that involve the 
commercial relations. Note that the commercial 
relation lexicon (Table 2) is manually compiled. 
In this work, we consider only two general 
commercial relations, namely cooperation and 
competition.  
 
Table 2: Statistics on relation lexicon. 
Type Amount Examples 
Competition 20 ??(challenge), ??
(compete), ? ?
(opponent) 
Collaboration 18 ??(collaborate),??
(coordinate), ? ?
(cooperate) 
SNA function produces the social network of a 
centric company, which can provide the compa-
                                                          
1http://ir.hit.edu.cn/demo/ltp 
99
ny with the impact analysis and decision-making 
chain tracking. It meets the aforementioned de-
mand, ?to explore the collaborative and competi-
tive relationship between companies?. 
3 Practical Example 
In this section, we use a case study to illustrate 
our system and further evaluate the performance 
of the main functions with respect to those com-
panies. Due to the limited space, we just illus-
trate the main functions of topic detection, opin-
ion mining and social network analysis. 
3.1 Topic Detection and Opinion Mining 
Figure 2(a) showed the results of topic detection 
and opinion mining functions for a Hong Kong 
local financial company Sun Hung Kai Proper-
ties (?????). On top of the figure are the 
results of topic detection and tracking function. 
Multi-document summary of the latest news is 
provided for the company and more news with 
the similar topics can be found by pressing the 
button ???? (more). Since there are a lot of 
duplicates of a piece of news on the websites, the 
summary is a direct way to acquire the recent 
news, which can improve the effectiveness of the 
company.  
The results of opinion mining function are 
shown at the bottom of Figure 2(a), where the 
green line indicates negative while the red line 
indicates positive. In order to give a dynamic 
insight of public opinions, we provide the 
amount changes of positive and negative articles 
with time variant. This is very helpful for the 
company to capture the feedback of their mar-
keting policies. As shown in Figure 2(a), there 
were 14 negative articles (????) on Oct. 29, 
2012, which achieved negative peak within the 6 
months. The users would probably read those 14 
articles and adjust the company strategy accord-
ingly.  
3.2 Social Network Analysis 
Figure 2(b) shows the social network based on 
the centric company in yellow, Sun Hung Kai 
Properties (?????). We only list the half 
of the connected companies with collaborative 
relationship from Sun Hung Kai Properties, and 
remove the competitive ones due to limited space. 
The thickness of the line indicates the strength of 
the collaboration between the two companies. 
The social network can explore the potential 
partners/competitors of a company. Furthermore, 
users are allowed to adjust the depth and set the 
nodes count of the network. The above analysis 
can provide a richer insight in to a company.  
In the following section, we will make exper-
iments to investigate the performance of the 
above functions.
 
(a) Topic detection and opinion mining of Sun Hung Kai Properties (?????). (For convenience, 
we translate the texts on the button in English) 
Opinion Mining 
Topic Detection 
100
 (b)Social network of Sun Hung Kai Properties (?????). (The rectangle in yellow is the centric) 
 
Figure 2: Screenshot of the MODEST system. 
4 Experiment and Result 
In our evaluation, the experiments were made 
based on 17692 articles collected from 52 Hong 
Kong websites during 6 months (1/7/2012~ 
31/12/2012). We investigate the performance of 
MODEST based on the standard metrics pro-
posed by NIST1, including precision, recall, and 
F-score. 
Precision (P) is the fraction of detected articles 
(U) that are relevantto the topic (N). 
  
 
 
      
Recall (R) is the fraction of the articles (T) that 
are relevant to the topic that are successfully de-
tected (N). 
  
 
 
      
Usually, there is an inverse relationship be-
tween precision and recall, where it is possible to 
increase one at the cost of reducing the other. 
Therefore, precision and recall scores are not 
discussed in isolation. Instead, F-Score (F) is 
proposed to combine precision and recall, which 
is the harmonic meanof precision and recall. 
  
 
 
 
 
 
 
      
     
   
      
4.1 Topic Detection and Tracking 
We first assess the performance of the topic de-
tection function. The data is divided into 6 parts 
                                                          
1http://trec.nist.gov/ 
according to the time. For different companies, 
the amount of articles vary a lot. Therefore, we 
calculate the metrics for each individual dataset, 
and then compute the weighted mean value. The 
experimental results are shown in Table 3.  
Table 3: Experimental results on topic detection. 
Dataset Recall Precision F-Score 
1/7/12-31/7/12 85.71% 89.52% 85.38% 
1/8/12-31/8/12 93.10% 93.68% 92.49% 
1/9/12-30/9/12 76.50% 83.13% 76.56% 
1/10/12-31/10/12 83.32% 88.53% 85.84% 
1/11/12-30/11/12 86.11% 89.94% 87.98% 
1/12/12-31/12/12 84.26% 87.65% 85.92% 
Average 85.13% 88.78% 85.69% 
From the experimental results, we can find 
that the average F-Score is about 85.69%.The 
dataset in the second row achieves the best per-
formance while the dataset in the third only get 
76.56% in F-Score. It is because that the amount 
of articles is smaller than the others and the re-
call value is very low. As far as we know, the 
best run of topic detection in (Allan et al., 2007) 
achieved 84%. The performance of topic detec-
tion in MODEST is comparable. 
4.2 Opinion Mining 
We then evaluate the performance of opinion 
mining function. We manually annotated 1568 
articles, which is further divided into 8 datasets 
randomly. Precision, recall, and F-score are also 
used as the metrics for the evaluation. The ex-
perimental results are shown in Table 4. 
101
  From Table 4, we can find that the average 
F-Score can reach 74.09%. Note that the opinion 
mining engine of MODEST is the implementa-
tion of (Zhou et al., 2010), which achieved the 
best run in NTCIR. However, the engine is 
trained on NTCIR corpus, which consists of arti-
cles of general domain, while the test set focuses 
on the financial domain. We further train our 
engine on the data from the financial domain and 
the average F-Score improves to over 80%. 
5 Conclusions 
This demonstration presents an intelligent infor-
mation platform designed to mine Web infor-
mation and provide decisions for modern service, 
MODEST. MODEST can provide the services of 
retrieving news from websites, extracting com-
mercial information, exploring customers? opin-
ions about a given company, and analyzing its 
collaborative/competitive social networks. Both 
enterprises and government are the target cus-
tomers. For enterprise, MODEST can improve 
the competitive abilities and facilitate potential 
collaboration. For government, MODEST can 
collect information about the entire industry, and 
make prompt strategies for better support. 
In this paper, we first introduce the system ar-
chitecture design and the main functions imple-
mentation, including topic detection and tracking, 
opinion mining, and social network analysis. 
Then a case study is given to illustrate the func-
tions of MODEST. In order to evaluate the per-
formance of MODEST, we also conduct the ex-
periments based on the data from 52 Hong Kong 
websites, and the results show the effectiveness 
of the above functions. 
In the future, MODEST will be improved in 
two directions: 
? Extend to other languages, e.g. English, 
Simplified Chinese, etc. 
? Enhance the compatibility to implement 
on mobile device.  
The demo of MODEST and the related 
toolkits can be found on the homepage: 
http://sepc111.se.cuhk.edu.hk:8080/adcom_hk/ 
Acknowledgements 
This research is partially supported by General Re-
search Fund of Hong Kong (417112), Shenzhen Fun-
damental Research Program (JCYJ201304011720464 
50, JCYJ20120613152557576), KTO(TBF1ENG007), 
National Natural Science Foundation of China 
(61203378, 61370165), and Shenzhen International 
Cooperation Funding (GJHZ20120613110641217). 
References: 
James Allan, Jaime Carbonell, George Doddington, 
Jonathan Yamron, and Yiming Yang. 1998. Topic 
Detection and Tracking Pilot Study: Final Report. 
Proceedings of the DARPA Broadcast News Tran-
scription and Understanding Workshop. 
Harris Drucker, Chris J.C. Burges, Linda Kaufman, 
Alex Smola, and Vladimir Vpnik. 1997. Support 
Vector Regression Machines. Proceedings of Ad-
vances in Neural Information Processing Systems, 
pp. 155-161. 
Noriko Kando.2010. Overview of the Eighth NTCIR 
Workshop. Proceedings of NTCIR-8 Workshop. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Proceedings of Synthesis Lectures on Hu-
man Language Technologies, pp. 1-167. 
Maximilian Viermetz, and Michal Skubacz. 2007. 
Using Topic Discovery to Segment Large Commu-
nication Graphs for Social Network Analysis. Pro-
ceedings of the IEEE/WIC/ACM International 
Conference on Web Intelligence, pp. 95-99. 
Canhui Wang, Min Zhang, Liyun Ru, and Shaoping 
Ma. 2008. Automatic Online News Topic Ranking 
Using Media Focus and User Attention based on 
Aging Theory. Proceedings of the Conference on 
Information and Knowledge Management. 
Yunqing Xia, Nianxing Ji, Weifeng Su, and Yi Liu. 
2010. Mining Commercial Networks from Online 
Financial News. Proceedings of the IEEE Interna-
tional Conference on E-Business Engineering, pp. 
17-23. 
Ruifeng Xu, Kam-fai Wong, and Yunqing Xia. 2008. 
Coarse-Fine Opinion Mining-WIA in NTCIR-7 
MOAT Task. In NTCIR-7 Workshop, pp. 307-313. 
Seok-Ho Yoon, Jung-Hwan Shin, Sang-Wook Kim, 
and Sunju Park. 2009. Extraction of a Latent Blog 
Community based on Subject. Proceeding of the 18th 
ACM Conference on Information and Knowledge 
Management, pp. 1529-1532. 
Lanjun Zhou, Yunqing Xia, Binyang Li, and Kam-fai 
Wong. 2010. WIA-Opinmine System in NTCIR-8 
MOAT Evaluation. Proceedings of NTCIR-8 
Workshop Meeting, pp. 286-292. 
Table 4: Experimental results on opinion mining. 
Dataset Size Precision Recall F-Score 
dataset-1 200 76.57% 78.26% 76.57% 
dataset-2 200 83.55% 89.64% 86.07% 
dataset-3 200 69.12% 69.80% 69.44% 
dataset-4 200 77.13% 75.40% 75.67% 
dataset-5 200 76.21% 77.65% 76.74% 
dataset-6 200 63.76% 66.22% 64.49% 
dataset-7 200 78.56% 78.41% 78.43% 
dataset-8 168 65.72% 65.15% 65.32% 
Average 196 73.83% 75.07% 74.09% 
102
