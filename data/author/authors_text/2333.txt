171
172
173
174
Using Machine Translation Evaluation Techniques to Determine
Sentence-level Semantic Equivalence
Andrew Finch
ATR Research Institute
2-2-2 Hikaridai
?Keihanna Science City?
Kyoto 619-0288
JAPAN
andrew.finch@atr.jp
Young-Sook Hwang
ATR Research Institute
2-2-2 Hikaridai
?Keihanna Science City?
Kyoto 619-0288
JAPAN
youngsook.hwang@atr.jp
Eiichiro Sumita
ATR Research Institute
2-2-2 Hikaridai
?Keihanna Science City?
Kyoto 619-0288
JAPAN
eiichiro.sumita@atr.jp
Abstract
The task of machine translation (MT)
evaluation is closely related to the
task of sentence-level semantic equiv-
alence classification. This paper in-
vestigates the utility of applying stan-
dard MT evaluation methods (BLEU,
NIST, WER and PER) to building clas-
sifiers to predict semantic equivalence
and entailment. We also introduce a
novel classification method based on
PER which leverages part of speech
information of the words contributing
to the word matches and non-matches
in the sentence. Our results show
that MT evaluation techniques are able
to produce useful features for para-
phrase classification and to a lesser ex-
tent entailment. Our technique gives a
substantial improvement in paraphrase
classification accuracy over all of the
other models used in the experiments.
1 Introduction
Automatic machine translation evaluation is a
means of scoring the output from a machine trans-
lation system with respect to a small corpus of
reference translations. The basic principle being
that an output is a good translation if it is ?close?
in some way to a member of a set of perfect trans-
lations for the input sentence. The closeness that
these techniques are trying to capture is in essence
the notion of semantic equivalence. Two sen-
tences being semantically equivalent if they con-
vey the same meaning.
MT evaluation techniques have found appli-
cation in the field of entailment recognition, a
close relative of semantic equivalence determina-
tion that seeks methods for deciding whether the
information provided by one sentence is included
in an another. (Perez and Alfonseca, 2005) di-
rectly applied the BLEU score to this task and
(Kouylekov and Magnini, 2005) applied both a
word and tree edit distance algorithm. In this pa-
per we evaluate these techniques or variants of
them and other MT evaluation techniques on both
entailment and semantic equivalence determina-
tion, to allow direct comparison to our results.
When using a single reference sentence for
each candidate the task of deciding whether a
pair of sentences are paraphrases and the task of
MT evaluation are very similar. Differences arise
from the nature of the sentences being compared,
that is MT output might not consist of grammat-
ically correct sentences. Moreover, MT evalu-
ation scoring need not necessarily be computed
on a sentence-by-sentence basis, but can be based
on statistics derived at the corpus level. Finally,
the process of MT evaluation is asymmetrical.
That is, there is a distinction between the ref-
erences and the candidate machine translations.
Fortunately, the automatic MT evaluation tech-
niques commonly in use do not make any ex-
plicit attempt to score grammaticality, and (ex-
cept BLEU) decompose naturally into their com-
ponent scores at the sentence level. (Blatz et al,
2004) used a variant of the WER score and the
NIST score at the sentence level to assign correct-
17
ness to translation candidates, by scoring them
with respect to a reference set. These correctness
labels were used as the ?ground truth? for classi-
fiers for the correctness of translation candidates
for candidate sentence confidence estimation. We
too adopt sentence level versions of these scores
and use them to classify paraphrase candidates.
The motivation for these experiments is two-
fold: firstly to determine how useful the features
used by these MT evaluation techniques to se-
mantic equivalence classifiers. One would ex-
pect that systems that perform well in one domain
should also perform well in the other. After all,
determining sentence level semantic equivalence
is ?part of the job? of an MT evaluator. Our sec-
ond motivation is the conjecture that successful
techniques and strategies will be transferable be-
tween the two tasks.
2 MT Evaluation Methods
MT evaluation schemes score a set of MT sys-
tem output segments (sentences in our case) S =
{s1, s2, ..., sI} with respect to a set of references
R corresponding to correct translations for their
respective segments. Since we classify sentence
pairs, we only consider the case of using a single
reference for evaluation. Thus the set of refer-
ences is given by: R = {r1, r2, ..., rI}.
2.1 WER
Word error rate (WER) (Su et al, 1992) is a mea-
sure of the number of edit operations required to
transform one sentence into another, defined as:
WER(si, ri) =
I(si, ri) + D(si, ri) + S(si, ri)
|ri|
where I(si, ri), D(si, ri) and S(si, ri) are the
number of insertions, deletions and substitutions
respectively.
2.2 PER
Position-independent word error rate (PER) (Till-
mann et al, 1997) is similar to WER except that
word order is not taken into account, both sen-
tences are treated as bags of words:
PER(si, ri) =
max[diff(si, ri), diff(ri, si)]
|ri|
where diff(si, ri) is the number of words ob-
served only in si.
2.3 BLEU
The BLEU score (Papineni et al, 2001) is based
on the geometric mean of n-gram precision. The
score is given by:
BLEU = BP ? exp
[ N
?
n=1
1
N ? log(pn)
]
where N is the maximum n-gram size.
The n-gram precision pn is given by:
pn =
? ? count(ngram)
i=1..I ngram?si
? ? countsys(ngram)
i=1..I ngram?si
where count(ngram) is the count of ngram
found in both si and ri and countsys(ngram) is
the count of ngram in si.
The brevity penalty BP penalizes MT output
for being shorter than the corresponding refer-
ences and is given by:
BP = exp
[
min
[
1? LrefLsys
, 1
]]
where Lsys is the number of words in the MT
output sentences and Lref is the number of words
in the corresponding references.
The BLEU brevity penalty is a single value
computed over the whole corpus rather than an
average of sentence level penalties which would
have made its effect too severe. For this reason,
in our experiments we omit the brevity penalty
from the BLEU score. Its effect is small since the
reference sentences and system outputs are drawn
from the same sample and have approximately the
same average length.
We ran experiments for N = 1...4, these are
referred to as BLEU1 to BLEU4 respectively.
2.4 NIST
The NIST score (Doddington, 2002) also uses
n-gram precision, differing in that an arithmetic
mean is used, weights are used to emphasize in-
formative word sequences and a different brevity
penalty is used:
NIST =
N
?
n=1
BP ?
? info(ngram)
all ngram
that co?occur
? 1
ngram?si
18
Sentence pair 1 (semantically equivalent):
1. Amrozi accused his brother, whom he called ?the witness?, of deliberately distorting his evidence.
2. Referring to him as only ?the witness?, Amrozi accused his brother of deliberately distorting his evidence.
Sentence pair 2 (not semantically equivalent):
1. Yucaipa owned Dominick?s before selling the chain to Safeway in 1998 for $2.5 billion.
2. Yucaipa bought Dominick?s in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.
Sentence pair 3 (semantically equivalent):
1. The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange.
2. PG&E Corp. shares jumped $1.63 or 8 percent to $21.03 on the New York Stock Exchange on Friday.
Figure 1: Example sentences from the Microsoft Research Paraphrase Corpus (MSRP)
info is defined to be:
info(ngram) = log2
[count((n? 1)gram)
count(ngram)
]
where count(ngram) is the count of ngram =
w1w2 . . . wn in all the reference translations, and
(n? 1)gram is w1w2 . . . wn?1.
For NIST the brevity penalty is computed on a
segment-by-segment basis and is given by:
BP = exp
[
? log2min
[
Lsys
Lref
, 1
]]
where Lsys is the length of the MT system
output, Lref is the average number of words in
a reference translation and ? is chosen to make
BP = 0.5 when LsysLref =
2
3 .
We ran experiments for N = 1...5, these are
referred to as NIST1 to NIST5 respectively. We
include the brevity penalty in the scores used for
our experiments.
2.5 Introducing Part of Speech Information
Early experiments based on the PER score re-
vealed that removing certain classes of function
words from the edit distance calculation had a
positive impact on classification performance. In-
stead of simply removing these words, we cre-
ated a mechanism that would allow the classifier
to learn for itself the usefulness of various classes
of word. For example, one would expect edits in-
volving nouns or verbs to cost more than edits in-
volving interjections or punctuation. We used a
POS tagger for the UPENN tag set (Marcus et al,
1994) to label all the data. We then divided the
total edit distance, into components, one for each
POS tag which hold the amount of edit distance
that words bearing this POS tag contributed to the
total edit distance. The feature vector therefore
having one element for each UPENN POS tag.
Let W? be the bag of words from si that have
no matches in ri and let W+ be the bag of words
from si that have matches in ri. The value of the
feature vector ~f? corresponding to the contribu-
tion to the PER from POS tag t is given by:
f?t =
?
w?W? count?t (w)
|si|
where count?t (w) is the number of times word
w occurs in W? with tag t.
The feature vector defined above characterizes
the nature of the words in the sentences that do
not match. However it might also be important to
include information on the words in the sentence
that match. To investigate this, we augment the
feature vector ~f? with an analogous set of fea-
tures ~f+ (again one for each UPENN POS tag)
that represent the distribution over the tag set of
word unigram precision, given by:
f+t =
?
w?W+ count+t (w)
|si|
where count+t (w) is the number of times word
w occurs in W+ with tag t.
This technique is analogous to the NIST score
in that it allows the classifier to weight the impor-
tance of matches, but differs in that this weight is
learned rather than defined, and is with respect to
the word?s grammatical/semantic role rather than
as a function of rarity. When both ~f+ and ~f? are
19
MSRP PASCAL CD IE MT QA RC PP IR
Sentence1 length 21.6 27.8 24.0 27.4 36.7 31.5 27.9 24.0 24.6
Sentence2 length 21.6 11.6 16.1 8.4 19.2 8.7 10.2 11.2 7.2
Length difference ratio 0.14 0.54 0.32 0.66 0.46 0.68 0.60 0.46 0.66
Edit distance 11.3 22.0 18.2 22.2 28.1 26.8 21.8 17.3 21.0
Table 1: Corpus statistics (columns CD-IR are sub-tasks of PASCAL), ?length difference ratio? is
explained in Section 3, ?edit distance? is the average Levenstein distance between the sentences of the
pairs
used in combination the method differs again by
utilizing information about the nature of both the
matching words and the non-matching words.
We will refer to the system based only on the
feature vector ~f? as POS- , that based only on
~f+ as POS+ and that based on both as POS.
2.6 Dealing with Synonyms
Often in paraphrases the semantic information
carried by a word in one sentence is conveyed by
a synonymous word in its paraphrase. To cover
these cases we investigated the effect of allow-
ing words to match with synonyms in the edit
distance calculations. Another pilot experiment
was run with a modified edit distance that al-
lowed words in the sentences to match if their
semantic distance was less than a specific thresh-
old (chosen by visual inspection of the output of
the system). The semantic distance measure we
used was that of (Jiang and Conrath, 1997) de-
fined using the relationships between words in the
WordNet database (Fellbaum, 1998). A perfor-
mance improvement of approximately 0.6% was
achieved on the semantic equivalence task using
the strategy.
3 Experimental Data
Two corpora were used for the experiments in this
paper: the Microsoft Research Paraphrase Corpus
(MSRP) and the PASCAL Challenge?s entailment
recognition corpus (PASCAL). Corpus statistics
for these corpora (after pre-processing) are pre-
sented in Table 1.
The MSRP corpus consists of 5801 sentence
pairs drawn from a corpus of news articles from
the internet. The sentences were annotated by hu-
man annotators with labels indicating whether or
not the two sentences are close enough in mean-
ing to be close paraphrases. Multiple annotators
were used to annotate each sentence: two anno-
tators labeled the data and a third resolved the
cases where they disagreed. The average inter-
annotator agreement on this task was 83%, indi-
cating the difficulty in defining the task and the
ambiguity of the labeling. Approximately 67% of
the sentences were judged to be paraphrases. The
data was divided randomly into 4076 training sen-
tences and 1725 test sentences. For full details of
how the corpus was collected we refer the reader
to the corpus documentation. To give an idea of
the nature of the data and the difficulty of the task,
three sentences from the corpus are shown in Fig-
ure 1. The example sentences show the ambigu-
ity inherent in this task. The first sentence pair
is clearly a pair of paraphrases. The second pair
of sentences share semantic information, but were
judged to be not semantically equivalent. The
third pair are not paraphrases, they are clearly de-
scribing the movements of totally different stocks,
but the sentences share sufficient semantic con-
tent to be labeled equivalent.
For the MSRP corpus we present results using
the provided training and test sets to allow com-
parison with our results. To obtain more accurate
figures and to get an estimate of the confidence
intervals we also conducted experiments by 10-
fold jackknifing over all the data. The results from
each fold were then averaged and 95% confidence
intervals were estimated for the means.
The PASCAL data consists of 567 development
sentences and 800 test sentences drawn from 7
domains: comparable document (CD), informa-
tion extraction (IE), machine translation (MT),
question answering (QA), reading comprehension
(RC), paraphrasing (PP) and information retrieval
(IR). A full description of this corpus is given in
20
the/DT cat/NN sat/VBD on/IN the/DT mat/NN
the/DT dog/NN sat/VBD on/IN the/DT mat/NN
DT
2/6
NN
1/6
VBD
1/6
IN
1/6
DT
0/6
NN
1/6
VBD
0/6
IN
0/6
Matches Non-matches
Sentence 1:
Sentence 2:
Feature Vector = (0.33, 0.16, 0.16, 0.16, 0, 0.16, 0, 0):
Figure 2: Example of a POS feature vector. The sentences are presented in word/TAG format, and the
feature vector is labeled with these POS tags (in the upper part of the squares)
the corpus documentation 1. The data differs from
the MSRP corpus in that it is annotated for en-
tailment rather than semantic equivalence. This
explains the asymmetry in the sentence lengths,
which is apparent even in the PP component of
the corpus. We do not present results for 10-fold
jackknifing on the PASCAL data since the data
were too small in number for this type of analy-
sis.
In Table 1 ?Sentence 1? refers to the first sen-
tence of a sentence pair in the corpus, and ?Sen-
tence 2? the second. The length distance ratio
(LDR) is defined to be the average over the corpus
of:
LDR(si, ri) =
||si| ? |ri||
max(|si|, |ri|)
This measures the similarity of the lengths of
the sentences in the pairs, it has the property of
being 0 when all sentence pairs have sentences of
the same length and 1 when all sentence pairs dif-
fer maximally in length. For the PASCAL corpus
the LDR is around 0.5 for the corpus as a whole,
corresponding to a large difference in the sentence
lengths. The CD component of the corpus being
considerably more consistent in terms of sentence
length. The differences among the tasks in terms
of edit distance are less clear-cut, with the PP task
having the lowest average edit distance despite its
higher LDR. The MSRP corpus has an LDR of
only 0.14. The sentences pairs are more similar in
terms of their length and edit distance than those
in the PASCAL corpus. We will argue later that
this length similarity has a significant effect on
the performance and applicability of these tech-
niques.
1http://www.pascal-network.org/Challenges/RTE/
4 Experimental Methodology
4.1 Tokenization
In order that the sentences could be tagged with
UPENN tags (Marcus et al, 1994), they were pre-
processed by a tokenizer. After tokenization the
average MSRP sentence length was 21 words.
4.2 Stemming
Stemming conflates morphologically related
words to the same root and has been shown to
have a beneficial effect on IR tasks (Krovetz,
1993). A pilot experiment showed that the
performance of a PER-based system degraded if
the stemmed form of the word was used in place
of the surface form. However, if the stemmer was
applied only to words labeled by a POS tagger
as verbs and nouns, a performance improvement
of around 0.8% was observed on the semantic
equivalence task. Therefore, for the purposes
of the experiments, the nouns and verbs in the
sentences were all pre-processed by a stemmer.
4.3 Classification
We used a support vector machine (SVM) clas-
sifier (Vapnik, 1995) with radial basis function
kernels to classify the data. The training sets for
the respective corpora were used for training, ex-
cept in the jackknifing experiments. Feature vec-
tors (an example is given in Figure 2) were con-
structed directly from the output of the MT evalu-
ation systems, when used. The vector has 2 parts,
one due to matches and one due to non-matches.
The sum of the elements corresponding to non-
matches is equal to the PER. We calculated the
vectors for each sentence in the pair as both ref-
erence and system output and averaged to get the
vector for the pair.
21
5 Results
5.1 MSRP Corpus
The results for the jackknifing experiments are
shown in Table 2 and the results using the pro-
vided training and test sets are shown in Table 3.
In the tables the rows labeled ?PER POS+?, re-
fer to models built using feature vectors made by
combining both the PER and POS+ feature vec-
tors. The rows labeled POS refer to models built
from the combination of features from the POS+
and POS- models. The rows labeled ALL refer
to models built from combining all of the features
used in these experiments.
The results show that decomposing the PER
edit distance score into components for each POS
tag is not able to better the classification perfor-
mance of PER. The accuracy (jackknifing) for
PER alone was 71.25% and the accuracy for the
analogous technique which divides this informa-
tion in contributions for each POS tag (POS-) was
70.99%. However, when the features from PER
and POS- are combined there is an improvement
in performance (to 72.71%) indicating that the
components for each POS tag are useful, but only
in addition to the more primitive feature encod-
ing the total edit distance. Moreover, comparing
the results from POS-, POS+ and POS it is clear
that there lot to be gained by considering the con-
tributions from both the matching words and the
non-matching words. Using both together gives a
classification performance of 74.2% whereas us-
ing either component in isolation can give a per-
formance no better than 71.5%.
The one of the worst performing systems was
that based on the WER score. However, it is
possible that the way the sentences were selected
handicapped this system, since only sentences
pairs with a word-based Levenshtein distance of 8
or higher were included in the corpus. Choosing
sentence pairs with larger edit distances makes
large structural differences more likely, and the
editing effort needed to correct such structural dif-
ferences may obscure the lexical comparison that
this score relies upon.
The results for the BLEU score were unex-
pected because the performance degrades as the
order of n-gram considered increases. This effect
is much less apparent in the NIST scores where
the performance degrades but to a lesser extent.
Paraphrases exhibit variety in their grammatical
structure and perhaps changes in word ordering
can explain this effect. If so, the geometric mean
employed in the BLEU score would make the ef-
fect of higher order n-grams considerably more
detrimental than with the arithmetic mean used in
the NIST score.
5.2 PASCAL Challenge Corpus
The results for the PASCAL corpus are given in
Table 4. As expected our results are consistent
with those of (Perez and Alfonseca, 2005). The
5% overall gain in accuracy may be accounted
for by the stemming and synonym extensions to
our technique and the fact that we used BLEU1.
Our approach also differs by being symmetrical
over source and reference sentences, however it
is not clear whether this would improve perfor-
mance. The number of test examples for the
sub-experiments for each task is low (50 to 150),
therefore the results here are likely to be noisy,
but it is apparent from our results that the CD
task is the most suitable for approaches based on
word/n-gram matching. Our POS technique per-
formed well on overall and particularly well on
the CD andMT tasks, but the overall performance
improvement relative to the other techniques is
not as clear-cut. We believe this is due to difficul-
ties arising from the asymmetrical nature of the
data, and we explore this in the next section.
5.3 Sentence length similarity
In this experiment we investigate whether there is
any advantage to be gained by using these tech-
niques on corpora consisting of sentence pairs of
similar length. Both the BLEU and NIST scores
use some form of count of the total number of
n-grams in the denominator of their n-gram pre-
cision formulae. When the sentences differ in
length, the total number of n-grams is likely to
be large in relation to the number of matching n-
grams since this is bounded by the number of n-
grams in the shorter sentence. This may result in
an increase in the ?noise? in the score due to vari-
ations in sentence length similarity, degrading its
effectiveness. To address the more general issue
of whether sentence length similarity has an im-
pact on the effectiveness of these techniques we
22
Accuracy Precision Recall F-measure
?95% conf. ?95% conf. ?95% conf. ?95% conf.
WER 68.80?0.90 69.89?1.08 94.20?0.99 80.22?0.69
PER 71.25?1.03 72.05?1.23 93.58?0.59 81.39?0.72
POS- 70.99?1.16 72.07?1.43 92.99?1.52 81.15?0.79
PER POS- 72.71?1.34 73.99?1.47 91.67?0.53 81.86?0.97
POS+ 71.56?0.99 72.51?1.20 93.02?1.50 81.46?0.74
POS 74.18?0.94 75.52?1.16 91.13?0.59 82.58?0.76
BLEU1 72.30?1.10 73.71?1.30 91.41?0.70 81.59?0.83
BLEU2 70.26?1.37 71.55?1.46 92.65?0.66 80.72?0.95
BLEU3 68.30?1.42 69.40?1.25 94.54?0.87 80.03?0.97
BLEU4 67.64?1.22 68.46?1.13 96.18?0.67 79.97?0.86
NIST1 71.78?1.44 73.95?1.55 89.65?1.06 81.02?1.04
NIST2 71.64?1.12 73.64?1.43 90.13?0.25 81.03?0.81
NIST3 71.59?1.17 72.94?1.36 91.82?0.39 81.28?0.87
NIST4 71.56?1.17 72.82?1.35 92.08?0.38 81.30?0.87
NIST5 71.52?1.14 72.75?1.33 92.18?0.45 81.30?0.85
ALL 75.35?1.13 77.35?1.10 89.54?0.90 82.99?0.89
Table 2: Experimental Results (10-fold Jackknifing)
Accuracy Precision Recall F-measure
WER 68.29 69.35 93.72 79.71
PER 71.88 72.30 93.55 81.56
POS- 70.96 72.09 91.89 80.79
PER POS- 73.33 74.14 91.98 82.10
POS+ 70.96 72.09 91.89 80.79
POS 74.20 75.29 91.11 82.45
BLEU1 73.22 74.17 91.63 81.98
BLEU2 70.96 71.62 93.29 81.03
BLEU3 68.93 69.45 95.12 80.28
BLEU4 67.88 68.13 97.12 80.08
NIST1 72.35 73.83 90.50 81.32
NIST2 71.59 73.09 90.67 80.94
NIST3 71.01 72.17 91.80 80.81
NIST4 70.96 72.09 91.89 80.79
NIST5 70.75 71.89 91.67 80.58
ALL 74.96 76.58 89.80 82.66
Table 3: Experimental Results (Microsoft?s Provided Train and Test Set)
sorted the sentences pairs of the MSRP corpus
according to the length difference ratio (LDR) de-
fined in Section 3, and partitioned the sorted cor-
pus into two: low and high LDR.We then selected
as many sentences as possible from the corpus
such that the training and test sets for each data
set (high and low LDR) contained the same num-
ber positive and negative examples. This gave two
sets (high and low LDR) of 1008 training exam-
ples and 438 test examples, all training and test
data consisiting of 50% positive and 50% nega-
tive examples. The results are shown in Table 5.
The experimental results validate our concerns. In
all of the cases the performance was higher on
the data with low LDR. Moreover, the effect was
most for the BLEU and NIST scores for which we
have an explanation of the cause.
6 Conclusion
We have shown that it is possible to derive fea-
tures that can be used to determine whether sim-
ilar sentences are paraphrases of each other from
methods currently being used to automatically
evaluate machine translation systems. The ex-
periments also show that using features that en-
code the distribution over the POS tag set of both
matching words and non-matching words can sig-
nificantly enhance the performance of a PER-
based system on this task.
23
Task BLEU1 NIST1 PER POS ALL
CD 74.67 76.67 73.33 79.33 82.00
IE 49.17 50.00 48.33 42.50 44.17
IR 47.78 45.56 41.11 37.78 40.00
MT 39.17 52.50 69.17 65.83 61.67
PP 56.00 44.00 58.00 44.00 38.00
QA 56.15 53.08 56.92 53.08 55.38
RC 52.86 53.57 48.57 57.14 55.00
ALL 54.50 55.63 57.37 56.75 56.75
Table 4: Accurracy Results (PASCAL Train and PASCAL Test Set)
BLEU1 NIST1 PER POS ALL
Low LDR 76.71 77.85 72.15 75.80 76.48
High LDR 68.49 70.09 69.63 72.83 73.52
Table 5: Accuracy Results Length Similarity (MSRP)
This research begs the important question ?Is
there any correlation between performance on the
semantic equivalence classification task and per-
formance of the underlying evaluation technique
on the task of MT evaluation??. Intuitively at
least, there certainly should be. If there is, it may
be possible to use the task of classifying sentences
for semantic equivalence as a proxy for the com-
plex and time-consuming task of evaluating eval-
uation schemes by correlating automatic scores
with human scores during the development pro-
cess of MT evaluation techniques. In future work
we look forward to addressing this question, as
well as incorporating new features into the mod-
els to increase their potency.
7 Acknowledgments
The research reported here was supported in part
by a contract with the National Institute of Infor-
mation and Communications Technology entitled
?A study of speech dialogue translation technol-
ogy based on a large corpus?.
References
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence estimation for machine transla-
tion. Technical report, Final report JHU / CLSP
2003 Summer Workshop, Baltimore.
G. Doddington. 2002. Automatic Evaluation of
Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the HLT
Conference, San Diego, California.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
taxonomy. CoRR, 9709008.
Milen Kouylekov and Bernardo Magnini. 2005.
Recognizing textual entailment with tree edit dis-
tance algorithms. In Proceedings PASCAL Chal-
lenges Worshop on Recognising Textual Entailment,
Southampton, UK.
Robert Krovetz. 1993. Viewing morphology as an
inference process. Technical Report UM-CS-1993-
036, University of Mass-Amherst, April.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. IBM Research Report rc22176
(w0109022), Thomas J. Watson Research Center.
Diana Perez and Enrique Alfonseca. 2005. Appli-
cation of the bleu algroithm for recognising tex-
tual entailments. In Proceedings PASCAL Chal-
lenges Worshop on Recognising Textual Entailment,
Southampton, UK.
K.Y. Su, M.W. Wu, and J.S. Chang. 1992. A new
quantitative quality measure for machine transla-
tion systems. In Proceedings of COLING-92, pages
433?439, Nantes, France.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated dp based search
for statistical translation. In Proceedings of
Eurospeech-97, pages 2667?2670, Rhodes, Greece.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
24
Abstract
This paper presents a technique for 
transliteration based directly on techniques 
developed for phrase-based statistical 
machine translation. The focus of our work 
is in providing a transliteration system that 
could be used to translate unknown words 
in a speech-to-speech machine translation 
system. Therefore the system must  be able 
to generate arbitrary sequence of characters 
in the target language, rather than words 
chosen from a pre-determined vocabulary. 
We evalauted our method automatically 
relative to a set of human-annotated 
reference transliterations as well as by 
assessing it  for correctness using human 
evaluators. Our experimental results 
demonstrate that for both transliteration 
and back-transliteration the system is able 
to produce correct, or phonetically 
e q u i v a l e n t t o c o r r e c t o u t p u t  i n 
approximately 80% of cases. 
1 Introduction
Dictionaries and corpora are only able to cover a 
certain proportion of language. Those words and 
phrases that are unknown to a translator/machine 
translation system present  a problem. Examples of 
such words include people?s names, place names, 
and technical terms. One solution to the problem is 
to transcribe the source language and use the 
transcription directly in the target language. 
Usually these transcrptions will be phonetically 
similar. This process of transcription is known as 
transliteration and in this paper we will present  a 
technique for automatically transliterating between 
English and Japanese, although the technique is 
general and is able to be appied directly to other 
language pairs. Of particular interest  to us is the 
appl icat ion of such a sys tem within a 
speech-to-speech machine translation (MT) 
system. Typically words not seen by the MT 
system, known as out of vocabulary words 
(OOVs), are either left  untranslated or simply 
removed from the output. Common examples of 
OOVs are named entities such as personal names, 
place names and technical terms, unknown 
occurences of which could benefit from being 
transliterated into the MT  system?s output during 
translation between Japanese and English. 
Moreover, in the case of a transation system that 
translates directly to speech, the transliteration 
system does not  necessarily need to produce the 
correct transliteration as any one of a set of 
phonetically equivalent alternatives would be 
equally acceptable.
1.1 English-Japanese Transliteration
In Japanese there are three separate alphabets, 
kanji (the Chinese character set), hiragana  (used as 
an alternative to the kanji, and to express 
functional elements such as particles etc.) and 
katakana (used to express foreign loan words, and 
relatively new words in the language, for example 
?karaoke?). Figure 1 shows some examples, the 
first  line is the English source, the second line is 
the Japanense and the last line is a direct 
transcription of the Japanese katakana into the 
roman alphabet with spaces delimiting the 
character boundaries. As can be seen from the 
examples, transliteration is not a straghtforward 
process. Example 1 of Figure 1 shows an example 
of a transliteration which is a reasonably direct 
phonetic transfer. The word ?manga? in English is 
a loan word from Japanese and has more-or-less 
the same pronunciation in both languages. In 
Example 2 we have an ambiguity, the ?aa? at the 
end of the word kompyutaa, corresponds to the 
Phrase-based Machine Transliteration
Andrew Finch
NiCT-ATR
?Keihanna Science City?
Kyoto, JAPAN
andrew.finch@atr.jp
Eiichiro Sumita
NiCT-ATR
?Keihanna Science City?
Kyoto, JAPAN
eiichiro.sumita@atr.jp
?er? of ?computer?. However, although incorrect 
the sequences kompyuta or kompyuuta are also 
plausible transliterations for the word. Example 4 
shows a contraction. The English word has been 
transfered over into Japanese, and then shortened. 
In this case ?personal? has been shortened to paso 
and ?computer? has been contracted into con. In 
Example 4 the Japanese loan word has come from 
a language other than English, in this case French, 
and these words are usually transliterated 
according to the pronunciation in their native 
language. In Example 5, the etymology is quite 
complex. The word has entered the language from 
the Portugese for ?English?: inglese, but has come 
to mean ?Great Britain?. Example 6 is a creative 
modern mixture of an imported loan word ero  a 
contraction of the transliteration erochikku of the 
English word ?erotic?, concatenated with a 
contraction of the Japanese word kawaii (usually 
written in kanji/kana) meaning ?cute?. Not  only is 
the English phrase phonetically unrelated in this 
case, but  the expression is difficult  to translate 
without  using a number of English words since it 
represents quite a lot of information.  
2 Related Work
2.1 Machine Transliteration
This paper is directly related to an important paper 
by Knight  and Graehl (1996).  Their transliteration 
system was also evaluated by English-Japanese 
(back-)transliteration performance. Our system 
differs from theirs in a number of aspects. The 
most important  of which is that their system 
outputs word sequences whereas our system 
outputs character sequences in the target language. 
The difference reflects the intended application of 
the transliteration system. Their system was 
intended to transliterate from the output  of an OCR 
system, and must  therefore be robust  to errors in 
the input, whereas our system has been developed 
with machine translation in mind, and the input  to 
our system is likely to consist of out-of-vocabulary 
words. This flexibility is a double-edged sword in 
that: on the one hand our system is able to handle 
OOVs; whereas on the other hand our system is 
free to generate non-words. A second difference 
between the approaches is that, Knight and 
Graehl?s model models the pronunciation of the 
source word sequences using a pronunciation 
dictionary in an intermediate model. Our system 
transforms the character sequence from one 
language into another in a subword-level character 
sequence-based manner. Our systems relies on the 
the system being able to implicitly learn the correct 
character-sequence mappings through the process 
of character alignment. Our system is also able to 
re-order the translated character sequences in the 
output. The system can be easily constrained to 
generate the target  in the same order as the source 
if necessary, however, often in Japanese names 
(including foreign names) are written with the 
family name first, therefore for the purposes of our 
experiments we allow the system to perform 
reordering.
2.2 Phrase-based Stat ist ical  Machine 
Translation (SMT)
Our approach couches the problem of machine 
transliteraion in terms of a character-level 
translation process. Character-based machine 
translation has been proposed as a method to 
overcome segmention issues in natural language 
computer
??????
ko n pi yu taa
2
personal computer
????
pa so ko n
3
bread
??
pa n
4
Figure 1: Example English-Japanese Transliterations
manga
???
ma n ga
1
Great Britain
????
i gi ri su
5
cute but still sexy
????
e ro ka wa
6
processing (Denoual and Lepage, 2006) and 
character-based machine translation systems have 
already been developed on these principles 
(Lepage and Denoual, 2006). Our system also 
takes a character-based approach but  restricts itself 
to the translation of short phrases. This is to our 
advantage because machine translation systems 
struggle in the translation of longer sequences. 
Moreover, the process of transliteration tends to be 
a monotone process, and this assists us further. We 
will give only a brief overview of the process of 
phrase-based machine translation, for a fuller 
account of statistical machine translation we refer 
the reader to (Brown et  al., 1991) and (Koehn, 
Och, and Marcu, 2003). 
During the process of phrase-based SMT the 
source sequence is segmented into sub-sequences , 
each sub-sequence being translated using bilingual 
sequence pairs (called phrase pairs when the 
translation proceeds at the word-level). The target 
generation process (for English-to-Japanese) at  the 
character level is illustrated in Figure 3. The 
example is a real system output  from an unseen 
phrase. The source sequence is segmented by the 
system into three segments. The translations of 
each of these segments have been gleaned from 
alignments of these segments where they occur in 
the training corpus. For example ?machine???
?? may have come from the pair ?Turing 
machine?????????? (chi yuu ri n gu 
ma chi n)? that is present in the Wikipedia 
component  of the training corpus. The  ?slation? in 
this example certainly came from the film title 
?Lost in Translation? since the Japanese translation 
of the English word ?translation? is usually written 
in kanji. 
3 Experimental Methodology
3.1 Experimental Data
The data for these experiments was taken from the 
publicly available EDICT  dictionary (Breen, 1995) 
together with a set  of katakana-English phrase 
pairs extracted from inter-language links in the 
Wikipedia
1
. These phrase pairs were extracted in a 
similar fashion to (Erdmann, et  al., 2007) who used 
them in the construction of a bilingual dictionary. 
An inter-language link is a direct link from an 
article in one language to an article in another. 
Phrase-pairs are extracted from these links by 
pairing the titles of the two articles. We collected 
only phrase pairs in which the Japanese side 
consisted of only katakana and the English side 
consisted of only ASCII characters (thus 
deliberately eliminating some foreign language 
?English? names that  would be hard to transliterate 
correctly). Data from both sources was combined 
to make a single corpus. Thus corpus was then 
randomly sub-divided into training (33479 phrase 
pairs), development  (2000 phrase pairs) and 
evaluation (2000 phrase pairs) sub-corpora. For the 
human evaluation a sample of 200 phrase-pairs 
was chosen randomly from the test corpus. In 
addition a small corpus of 73 US politicians? 
names was collected from a list of US presidents 
and vice presidents in the Wikipedia. Duplictate 
entries were removed from this list and the training 
set was also filtered to exclude these entries.  
3.2 Back-transliteration Accuracy
Following Knight and Graeh (1996), we evaluated 
our system with respect to back-transliteration 
performance. That is, word sequences in katakana 
were used to generate English sequences. As a 
point  of reference to the results in this paper, we 
back-transliterated a list  of American polititians? 
names. The results are shown in Table 1.  The 
number of exacty correct  results is lower than the 
system of Knight and Graehl, but the total number 
of correct  + phonetically equivalent  results is about 
the same. This can be explained by the fact that our 
system is able to generate character sequences 
more freely in order to be able to handle unknown 
words . A l toge the r a round 78% o f t he 
back-transliterations were judged either correct or 
phonetically equivalent to a correct  result. We 
included a class to respesent those results that were 
not equivalent in terms of English phonology but 
1
 http://www.wikipedia.org
Figure 3: The phrase-translation process
machine
???
ma shi n
tran
???
to ra n
slation
??????
su ree shi yo n
machine translation
were ?reasonable errors? in terms of Japanese 
phonology, for example ?James Polk? was 
back-transliterated as ?James Pork?, the ?r? and ?l? 
sound being hard to discrimitate in Japanese 
becasue the two sounds are combined into a single 
sound. The reason for making this distinction was 
to identify the proportion (around 10%) of more 
pathological errors caused by errors such as 
incorrect phrase pairs extracted due to erroneaous 
word alignments. 
3.3 Human Assessment
Figure 2 shows the results of the human 
evaluation. Transliterated text  from English to 
Japanese was graded by a professional translator 
who was fluent  in both languages but native in 
Japanese. Conversely the back-transliterated 
phrases were judged by a native English-speaking 
translator who was also fluent in Japanese. The 
evaluation data was graded into 4 categories:
(1) The transliteration or back-transliteration 
was correct.
(2) The transliteraton was not correct  however 
the result  was phonetically equivalent to a 
correct result.
(3) The transliteration or back-transliteration 
was incorrect.
(4) The annnotator was unsure of the correct 
grade for that example.
Transliteration examples:
Grade 1:  worm gear ? u oo mu gi ya
Grade 2:  worm gear ? waa mu gi a
Grade 3: marcel desailly ? ma ru se ru de sa i 
ri
Grade 4: agnieszka holland ? ?
Figure 2: Human Judgement of Quality Transliteration Performance and Wikipedia Data
0
25
50
75
100
EN?JA JA?EN EN?JA JA?EN
Machine
Wikipedia
P
r
o
p
o
r
t
i
o
n
 
o
f
 
D
a
t
a
 
(
%
)
Incorrect Don?t know
Phonetically correct Correct
58
22
18.5
87
5.5
6
77
15.5
5.5
57
17
10.5
15.5
Correct 57.53%
Phonetically equivalent (EN) 20.54%
Phonetically equivalent (JA) 10.96%
Incorrect 10.96%
Table 1: Back-transliteration performance on 
politicians? names
The example of Grade 1 is the Wikipedia entry and 
is the normal way of expressing this phrase in 
Japanese. The Grade 2 example is output from our 
system, the pronunciation of the string is almost 
the same as the Grade 1 version, however the form 
of expression is unusual. The Grade 3 example is 
also a system output. Here the system has made a 
reasonable attempt  at  generating the katakana, but 
has transliterated it in terms of the English 
pronunciation rather than the French from which 
the name dervies. The correct  transliteration from 
this name would be: ma ru se ru de sa ii. This 
problem has been caused by the nature of the 
training data which contains mainly English 
expressions. The word ?desailly? had not  occurred 
in the training data. 
The results reveal several things about the data, the 
task and the system performance. Looking at  the 
scoring of the Wikipedia data, there is a reasonable 
level of disagreement  between the two annotators, 
but the overall number of pairs judged as correct 
(back-)transliterations is nonetheless reasonably 
high; in the 80-90% range. Secondly, the 
annotators judged the quality of the transliteration 
and back- t rans l i t e ra t ion sys tems to be 
approximately the same. We found this result 
surprising since the English generation, intuitively 
at  least, appears to be harder than Japanese 
generation because there are fewer constraints on 
graphemic structure. The most significant result  is 
that the number of cases labelled ?correct? or 
?phonetcially equivalent to a correct  result? was 
around 80% for both systems, which should be 
high enough to allow the system to be used in a 
speech translation system, especially since by 
visual inspection of the data, many of the the 
?incorrect? results were near misses that  would be 
easy for a user of the system to understand. For 
example the transliteraton ko-roo-ra for ?Corolla? 
was judged correct, however ko-ro-ra was judged 
incorrect and not phonetically equivalent.
3.4 Assement using automatic machine 
translation evaluation methods
Table 2 shows the results from evaluating the 
output of our t ransl i terat ion and back-
transliteration systems according to a range of 
commonly-used automatic machine translation 
scoring schemes. We believe these techniques are 
an effective way to evaluate transliteration quality, 
and are therefore provided here as a reference. The 
difference between the WER and PER scores is 
interesting here as the WER score takes sequence 
order into account when comparing to a reference 
whereas  PER does not. There is a larger difference 
when the target  is English indicating that this 
process has more issues related to character order. 
4 Conclusion and Future Directions
This paper has demonstrated that transliteration 
can be done effectively by a machine translation 
system, and has quantified this empirically. It is 
clear that by leaving the system ?open? and free to 
generate any sequence of characters in the target 
language there is a price to pay since the system is 
able to generate non-words. On the other hand, 
restricting the system so that  it is only able to gen-
erate words is for many applications unrealistic, 
and in particular it  is necessary for the speech 
translation application this system has been devel-
oped for. Our results show that  our system gener-
ates correct  or phonetically correct transliterations 
around 80% of the time. This figure serves as a 
lower bound estimate for the proportion of practi-
cally useful transliterations it will produce. Perhaps 
a compromise between these two approaches can 
be achieved by introducing a lexically-based lan-
guage model into the system in addition to the ex-
isting high-order character-based language model. 
Furthermore, we are also interested in investigating 
the use of the models generated by training our 
system in the process of word alignment for statis-
tical machine translation, and as a precursor to this 
the models might be used in filtering the training 
data in a pre-processing stage. Lastly it  is impor-
BLEU NIST WER PER GTM METEOR TER
EN?JA
JA?EN
0.627 9.17 0.31 0.29 0.8 0.81 30.67
0.682 10.023 0.277 0.237 0.83 0.81 27.14
Table 2: System performance according to automatic machine translation scoring schemes
tant to mention that Wikipedia (which provided us 
with most  of our corpus), is growing very rapidly, 
and considerably more training data for statistical 
transliteration systems should be available in the 
near future.
References
J.W. Breen. 1995.  Building an electronic Japanese-
English dictionary. Japanese Studies Association of 
Australia Conference. Queensland, Australia.
Peter Brown, S. Della Pietra, V. Della Pietra, and R. 
Mercer (1991).  The mathematics of statistical ma-
chine translation: parameter estimation. Computa-
tional Linguistics, 19(2), 263-311. 
Etienne Denoual and  Yves Lepage.  2006. The character 
as an appropriate unit of processing for non-
segmenting languages, Proceedings of the 12th An-
nual Meeting of The Association of NLP, pp. 731-
734. 
Maike Erdmann, Kotaro Nakayama, Takahiro Hara,and 
Shojiro Nishio. 2007. Wikipedia Link Structure 
Analysis for Extracting Bilingual Terminology. 
IEICE Technical Committee on Data Engineering. 
Miyagi, Japan.
Kevin Knight and Jonathan Graehl. 1997. Machine 
Transliteration.  Proceedings of the Thirty-Fifth An-
nual Meeting of the Association for Computational 
Linguistics and Eighth Conference of the European 
Chapter of the Association for Computational Lin-
guistics, pp. 128-135, Somerset, New Jersey.
Yves Lepage and Etienne Denoual. 2006. Objective 
evaluation of the analogy-based machine translation 
system ALEPH. Proceedings of the 12th Annual 
Meeting of The Association of NLP, pp. 873-876.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation.  In Pro-
ceedings of the Human Language Technology Con-
ference 2003 (HLT-NAACL 2003), Edmonton, Can-
ada.
Proceedings of the Third Workshop on Statistical Machine Translation, pages 208?215,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Abstract
This paper presents a technique for class-
dependent decoding for statistical machine 
translation (SMT). The approach differs from 
previous methods of class-dependent transla-
tion in that the class-dependent forms of all 
models are integrated directly into the decod-
ing process. We employ probabilistic mixture 
weights between models that can change dy-
namically on a segment-by-segment basis 
depending on the characteristics of the source 
segment. The effectiveness of this approach is 
demonstrated by evaluating its performance 
on travel conversation data. We used the ap-
proach to tackle the translation of questions 
and declarative sentences using class-
dependent models.  To achieve this, our system 
integrated two sets of models specifically built 
to deal with sentences that fall into one of two 
classes of dialog sentence: questions and dec-
larations, with a third set of models built to 
handle the general class. The technique was 
thoroughly evaluated on data from 17 lan-
guage pairs using 6 machine translation 
evaluation metrics. We found the results were 
corpus-dependent, but in most cases our sys-
tem was able to improve translation perform-
ance, and for some languages the improve-
ments were substantial.
1 Introduction
Topic-dependent  modeling has proven to be an 
effective way to improve quality the quality of 
models in speech recognition (Iyer and Osendorf, 
1994; Carter, 1994). Recently, experiments in the 
field of machine translation (Hasan and Ney, 2005; 
Yamamoto and Sumita, 2007; Finch et al 2007, 
Foster and Kuhn, 2007) have shown that class-
specific models are also useful for translation.
In the method proposed by Yamamoto and Su-
mita (2007), topic dependency was implemented 
by partitioning the data into sets before the decod-
ing process commenced, and subsequently decod-
ing these sets independently using different models 
that were specific to the class predicted for the 
source sentence by a classifier that  was run over 
the source sentences in a pre-processing pass. Our 
approach is in many ways a generalization of this 
work. Our technique allows the use of multiple-
model sets within the decoding process itself. The 
contributions of each model set  can be controlled 
dynamically during the decoding through a set of 
interpolation weights. These weights can be 
changed on a sentence-by-sentence basis. The pre-
vious approach is, in essence, the case where the 
interpolation weights are either 1 (indicating that 
the source sentence is the same topic as the model) 
or 0 (the source sentence is a different  topic). One 
advantage of our proposed technique is that it is a 
soft approach. That is, the source sentence can be-
long to multiple classes to varying degrees. In this 
respect our approach is similar to that  of Foster and 
Kuhn (2007), however we used a probabilistic 
classifier to determine a vector of probabilities rep-
resenting class-membership, rather than distance-
based weights. These probabilities were used di-
rectly as the mixture weights for the respective 
models in an interpolated model-set. A second dif-
ference between our approach and that of Foster 
and Kuhn, is that  we include a general model built 
from all of the data along with the set  of class-
specific models.
Our approach differs from all previous ap-
proaches in the models that are class-dependent. 
Hasan and Ney (2005) used only a class-dependent 
language model. Both Yamamoto and Sumita 
(2007) and Foster and Kuhn (2007), extended this 
to include the translation model. In our approach 
we combine all of the models, including the distor-
tion and target length models, in the SMT system 
within  a single framework.
The contribution of this paper is two-fold. The 
first  is the proposal of a technique for combining 
Dynamic Model Interpolation for Statistical Machine Translation
Andrew FINCH
NICT
?
-ATR
?
Kyoto, Japan
andrew.finch@atr.jp
Eiichiro SUMITA
NICT
?
-ATR
?
 
Kyoto, Japan
eiichiro.sumita@atr.jp
?  National Institute for Science and Technology
?  Advanced Telecommunications Research Laboratories
208
multiple SMT systems in a weighted manner to 
allow probabilistic soft  weighting between topic-
dependent models for all models in the system. 
The second is the application of this technique to 
improve the quality of dialog systems by building 
and combing class-based models for interrogative 
and declarative sentences.
For the purposes of this paper, we wish to make 
the distinction between interrogative sentences and 
those which are not. For the sake of simplicity of 
expression we will call those sentences which are 
interrogative, questions and those which are not, 
declarations for the remainder of this article. 
The techniques proposed here were evaluated on 
a variety of different languages. We enumerate 
them below as a key: Arabic (ar), Danish (da), 
German (de), English (en), Spanish (es), French 
(fr), Indonesian (Malay) (id), Italian (it), Japanese 
(ja), Korean (ko), Malaysian (Malay) (ms), Dutch 
(nl), Portugese (pt), Russian (ru), Thai (th), Viet-
namese (vi) and Chinese (zh).
2 System Overview
2.1 Experimental Data
To evaluate the proposed technique, we conducted 
experiments on a travel conversation corpus. The 
experimental corpus was the travel arrangement 
task of the BTEC corpus (Kikui et al, 2003) and 
used English as the target  and each of the other 
languages as source languages. The training, de-
velopment, and evaluation corpus statistics are 
shown in Table 1. The evaluation corpus had six-
teen reference translations per sentence. This train-
ing corpus was also used in the IWSLT06 Evalua-
tion Campaign on Spoken Language Translation 
(Paul 2006) J-E open track, and the evaluation cor-
pus was used as the IWSLT05 evaluation set. 
2.2 System Architecture
Figure 1 shows the overall structure of our system. 
We used punctuation (a sentence-final ??? charac-
ter) on the target-side as the ground truth as to the 
class of the target sentence. Neither punctuation 
nor case information was used for any other pur-
pose in the experiments.  The data were partitioned 
into classes, and further sub-divided into training 
and development sets for each class. 1000 sen-
tences were set  aside as development data, and the 
remainder was used for training. Three complete 
SMT  systems were built: one for each class, and 
one on the data from both classes. A probabilistic 
classifier (described in the next section) was also 
trained from the full set of training data. 
The machine translation decoder used is able to 
linearly interpolate all of the models models from 
Figure 1. The architecture of the class-based SMT system used in our experiments
Model interpolating decoder
Labeled bilingual corpus
All data
bilingual declarations
Probabilistic
Classifier
Question-specific 
SMT System
General SMT System
Declaration-specific 
SMT System
DEVTRAIN
bilingual questions
DEVTRAIN
DEV TRAIN
Unlabeled test 
corpus
General weight
(fixed during decoding)
Question weight
(dynamic)
Declaration weight
(dynamic)
sentence
sentence
sentence
209
all of the sub-systems according to a vector of in-
terpolation weights supplied for each source word 
sequence to be decoded. To do this, prior to the 
search, the decoder must  first merge the phrase-
tables from each sub-system. Every phrase from all 
of the phrase-tables is used during the decoding. 
Phrases that  occur in one sub-system?s table, but 
do not  occur in another sub-system?s table will be 
used, but will receive no support  (zero probability) 
from those sub-systems that did not acquire this 
phrase during training. The search process pro-
ceeds as in a typical multi-stack phrase-based de-
coder. The weight for the general model was set  by 
tuning the parameter on the general development 
set in order to maximize performance in terms of 
BLEU score. This weight determines the amount 
of probability mass to be assigned to the general 
model, and it  remains fixed during the decoding of 
all sentences. The remainder of the probability 
mass is divided among the class-specific models 
dynamically sentence-by-sentence at  run-time. The 
proportion that  is assigned to each class is simply 
the class membership probability of the source se-
quence assigned by the classifier.
3 Question Prediction 
3.1 Outline of the Problem
Given a source sentence of a particular class (inter-
rogative or declarative in our case), we wish to 
ensure that  the target  sentence generated is of an 
appropriate class. Note that this does not necessar-
ily mean that given a question in the source, a 
question should be generated in the target. How-
ever, it seems reasonable to assume that, intuitively 
at  least, one should be able to generate a target 
question from a source question, and a target decla-
ration from a source declaration.  This is reason-
able because the role of a machine translation en-
gine is not  to be able to generate every possible 
translation from the source, but to be able to gener-
ate one acceptable translation. This assumption 
leads us to two plausible ways to proceed.
1. To predict the class of the source sentence, and 
use this to constrain the decoding process used 
to generate the target
2. To predict the class of the target  
In our experiments, we chose the second 
method, as it  seemed the most correct, but  feel 
there is some merit in both strategies.
3.2 The Maximum Entropy Classifier
We used a Maximum Entropy (ME) classifier to 
determine which class to which the input source 
sentence belongs using a set  of lexical features. 
That is, we use the classifier to set  the mixture 
weights of the class-specific models. In recent 
years such classifiers have produced powerful 
models utilizing large numbers of lexical features 
in a variety of natural language processing tasks, 
for example Rosenfeld (1996).  An ME model is an 
exponential model with the following form:
where: 
t is the class being predicted; 
c is the context of t; 
? is a normalization coefficient; 
K is the number of features in the model; 
?
k
 is the weight of feature f
k
; 
f
k
 are binary feature functions;
    p
0 
is the default model
p(t, c) = ?
K
?
k=0
?
fk(c,t)
k p0
Questions + Decls. Questions Declarations Test
Train Dev Train Dev Train Dev
Sentences 161317 1000 69684 1000 90633 1000 510
Words
1001671 6112 445676 6547 549375 6185 3169
Table 1. The corpus statistics of the target language corpus (en). The number of sentences is the same as 
these values for all source languaes. The number of words in the source language differs, and depends 
on the segmentation granularity.
210
We used the set of all n-grams (n?3) occurring 
in the source sentences as features to predict  the 
sentence?s class. Additionally we introduced be-
ginning of sentence tokens (<s>) and end of sen-
tence tokens into the word sequence to distinguish 
n-grams occurring at the start and end of sentences 
from those occurring within the sentence. This was 
based on the observation that ?question words? or 
words that indicate that the sentence is a question 
will frequently be found either at  the start of the 
sentence (as in the wh- <what, where, when> 
words in English or the -kah words in Malay <a-
pakah, dimanakah, kapankah>), or at the end of the 
sentence (for example the Japanese ?ka? or the 
Chinese ?ma?). In fact, in earlier models we used 
features consisting of n-grams occurring only at 
the start  and end of the source sentence. These 
classifiers performed quite well (approximately 4% 
lower than the classifiers that used features from 
all of the n-grams in the source), but  an error 
analysis showed that  n-grams from the interior of 
the sentence were necessary to handle sentences 
such as ?excuse me please where is ...?. A simple 
example sentence and the set of features generated 
from the sentence is shown in Figure 2.
We used the ME modeling toolkit of (Zhang, 
2004) to implement our ME models. The models 
were trained by using L-BFGS parameter estima-
tion, and a Gaussian prior was used for smoothing 
during training.
3.3 Forcing the target to conform
Before adopting the mixture-based approach set 
out in this paper, we first pursued an obvious and 
intuitively appealing way of using this classifier. 
We applied it as a filter to the output of the de-
coder, to force source sentences that the classifier 
predicts should generate questions in the target to 
actually generate questions in the target. This ap-
proach was unsuccessful due to a number of issues. 
We took the n-best  output  from the decoder and 
selected the highest translation hypothesis on the 
list that  had agreement  on class according to source 
and target  classifiers. The issues we encountered 
included, too much similarity in the n-best hy-
potheses, errors of the MT system were correlated 
with errors of the classifier, and the number of 
cases that were corrected by the system was small 
<2%. As a consequence, the method proposed in 
this paper was preferred.
4 Experiments
4.1 Experimental Conditions
Decoder
The decoder used to in the experiments, CleopA-
TRa is an in-house phrase-based statistical decoder 
that can operate on the same principles as the 
PHARAOH (Koehn, 2004) and MOSES (Koehn et 
Source
Language
English 
Punctuation
Own 
Punctuation
ar 98.0 N/A
da 97.3 98.0
de 98.1 98.6
en 98.9 98.9
es 96.3 96.7
fr 97.7 98.7
id 97.9 98.5
it 94.9 95.4
ja 94.1 N/A
ko 94.2 99.4
ms 98.1 99.0
nl 98.1 99.0
pt 96.2 96.0
ru 95.9 96.6
th 98.2 N/A
vi 97.7 98.0
zh 93.2 98.8
Table 2. The classifcation accuracy (%) of the 
classifier used to predict whether or not an input 
sentence either is or should give rise to a question in 
the target.
<s> where is the
<s> where is
<s> where is the is the station </s>
is the station </s>
the station </s>
Figure 2. The set of n-gram (n?3) features extracted 
from the sentence <s> where is the station </s> for 
use as predicates in the ME model to predict target 
sentence class.
211
al, 2007) decoders. The decoder was configured to 
produce near-identical output to MOSES for these 
experiments. The decoder was modified in order to 
handle multiple-sets of models, accept  weighted 
input, and to incorporate the dynamic interpolation 
process during the decoding. 
Practical Issues
Perhaps the largest concerns about the proposed 
approach come from the heavy resource require-
ments that could potentially occur when dealing 
with large numbers of models. However, one im-
portant characteristic of the decoder used in our 
experiments is its ability to leave its models on 
disk, loading only the parts of the models neces-
Source BLEU NIST WER PER GTM METEOR
ar
0.4457 
(0.00)
8.9386 
(0.00)
0.4458 
(0.00)
0.3742 
(0.00)
0.7469 
(0.00)
0.6766 
(0.00)
da
0.6640 
(0.64)
11.4500 
(1.64)
0.2560 
(0.08)
0.2174 
(2.42)
0.8338 
(0.68)
0.8154 
(1.23)
de
0.6642 
(0.79)
11.4107 
(0.44)
0.2606 
(2.18)
0.2105 
(0.14)
0.8348 
(-0.13)
0.8132 
(-0.07)
es
0.7345 
(0.00)
12.1384 
(0.00)
0.2117 
(0.00)
0.1668 
(0.00)
0.8519 
(0.00)
0.8541 
(0.00)
fr
0.6666 
(0.95)
11.7443 
(0.63)
0.2548 
(4.82)
0.2172 
(6.50)
0.8408 
(0.48)
0.8293 
(1.29)
id
0.5295 
(9.56)
10.3459 
(4.11)
0.3899 
(21.17)
0.3239 
(4.65)
0.7960 
(1.35)
0.7521 
(2.35)
it
0.6702 
(1.01)
11.5604 
(0.41)
0.2590 
(3.25)
0.2090 
(0.62)
0.8351 
(0.36)
0.8171 
(0.05)
ja
0.5971 
(3.47)
10.6346 
(2.56)
0.3779 
(5.53)
0.2842 
(2.80)
0.8125 
(0.74)
0.7669 
(0.67)
ko
0.5898 
(1.78)
10.2151 
(1.31)
0.3891 
(0.74)
0.3138 
(-0.10)
0.7880 
(0.36)
0.7397 
(0.35)
ms
0.5102 
(10.19)
9.9775 
(2.75)
0.4058 
(18.53)
0.3355 
(3.59)
0.7815 
(0.18)
0.7247 
(2.49)
nl
0.6906 
(2.55)
11.9092 
(1.47)
0.2415 
(3.21)
0.1872 
(1.73)
0.8548 
(0.39)
0.8399 
(0.36)
pt
0.6623 
(0.35)
11.6913 
(0.26)
0.2549 
(2.52)
0.2110 
(2.68)
0.8396 
(0.02)
0.8265 
(-0.07)
ru
0.5877 
(0.34)
10.1233 
(-1.10)
0.3447 
(1.99)
0.2928 
(1.71)
0.7900 
(0.15)
0.7537 
(-0.40)
th
0.4857 
(1.50)
9.5901 
(1.17)
0.4883 
(-0.23)
0.3579 
(2.03)
0.7608 
(0.45)
0.7104 
(1.23)
vi
0.5118 
(0.67)
9.8588 
(1.85)
0.4274 
(-0.05)
0.3301 
(0.12)
0.7806 
(1.05)
0.7254 
(0.43)
zh
0.5742 
(0.00)
10.1263 
(0.00)
0.3937 
(0.00)
0.3172 
(0.00)
0.7936 
(0.00)
0.7343 
(0.00)
Table 3. Performance results translating from a number of source languages into English. Figures in parentheses are 
the percentage improvement in the score relative to the original score. Bold-bordered cells indicate those conditions 
where performance degraded. White cells indicate the proposed system?s performance is significanly different from 
the baseline (using 2000-sample bootstrap resampling with a 95% confidence level). TER scores were not tested for 
significance due to technical difficulties. ar, es and zh were also omitted since the systems were identical.
212
sary to decode the sentence in hand. This reduced 
the memory overhead considerably when loading 
multiple models, without noticeably affecting de-
coding time. Moreover, it is also possible to pre-
compute the interpolated probabilities for most of 
the models for each sentence before the search 
commences, reducing both search memory and 
processing time. 
Decoding Conditions
For tuning of the decoder's parameters, minimum 
error training (Och 2003) with respect  to the BLEU 
score using was conducted using the respective 
development  corpus. A 5-gram language model, 
built using the SRI language modeling toolkit 
(Stolcke, 1999) with Witten-Bell smoothing was 
used. The model included a length model, and also 
the simple distance-based distortion model used by 
the PHARAOH decoder (Koehn, 2004).
Source Baseline No Classifier Hard Proposed
ar 0.4457 (0.00) 0.4457 (0.00) 0.4457 (0.00) 0.4457
da 0.6598 (0.64)
0.6647 (-0.11)
0.6591 (0.74) 0.664
de 0.6590 (0.79)
0.6651 (-0.14)
0.6634 (0.12) 0.6642
es 0.7345 (0.00)
0.7345 (0.00)
0.7345 (0.00) 0.7345
fr 0.6603 (0.95) 0.6594 (1.09) 0.6605 (0.92) 0.6666
id 0.4833 (9.56) 0.5029 (5.29) 0.5276 (0.36) 0.5295
it 0.6635 (1.01) 0.6660 (0.63) 0.6644 (0.87) 0.6702
ja 0.5771 (3.47) 0.5796 (3.02) 0.5667 (5.36) 0.5971
ko 0.5795 (1.78) 0.5837 (1.05) 0.5922 (-0.41) 0.5898
ms 0.4630 (10.19) 0.5015 (1.73) 0.5057 (0.89) 0.5102
nl 0.6734 (2.55) 0.6902 (0.06) 0.6879 (0.39) 0.6906
pt 0.6600 (0.35)
0.6643 (-0.30)
0.6598 (0.38) 0.6623
ru 0.5857 (0.34)
0.5885 (-0.14)
0.5844 (0.56) 0.5877
th 0.4785 (1.50)
0.4815 (0.87)
0.4831 (0.54) 0.4857
vi 0.5084 (0.67) 0.5095 (0.45) 0.5041 (1.53) 0.5118
zh 0.5742 (0.00) 0.5742 (0.00) 0.5742 (0.00) 0.5742
Table 4. Performance results comaparing our proposes method with other techniques. The column labeled ?Baseline? 
is the same as in Table 3, for reference. The column lableled ?No Classifier?, is the same system as our proposed 
method, except that the classifier was replaced with a default model that assigned a class membership probability of 
0.5 in every case. The column labeled ?Hard? corresponds to a system that used hard weights (either 1 or 0) for the 
class-dependent models. The column labeled ?Proposed? are the results from our proposed method. Figures in 
parentheses represent the percentage improvement of the proposed method?s score relative to the alternative method. 
Cells with bold borders indicate those conditions where performance was degraded.
213
Tuning the interpolation weights
The interpolation weights were tuned by maximiz-
ing the BLEU score on the development set  over a 
set of weights ranging from 0 to 1 in increments of 
0.1. Figure 1 shows the behavior of two of our 
models with respect to their weight parameter. 
Evaluation schemes
To obtain a balanced view of the merits of our pro-
posed approach, in our experiments we used 6 
evaluation techniques to evaluate our systems. 
These were: BLEU (Papineni, 2001), NIST (Dod-
dington, 2002), WER (Word Error Rate), PER 
(Position-independent WER), GTM (General Text 
Matcher), and METEOR (Banerjee and Lavie, 
2005).
4.2 Classification Accuracy
The performance of the classifier (from 10-fold 
cross-validation on the training set) is shown in 
Table 2. We give classification accuracy figures for 
predicting both source (same language) and target 
(English) punctuation. Unsurprisingly, all systems 
were better at  predicting their own punctuation. 
The poorer scores in the table might reflect linguis-
tic characteristics (perhaps questions in the source 
language are often expressed as statements in the 
target), or characteristics of the corpus itself. For 
all languages the accuracy of the classifier seemed 
satisfactory, especially considering the possibility 
of inconsistencies in the corpus itself (and there-
fore our test data for this experiment).
4.3 Translation Quality
The performance of the SMT systems are shown in 
Table 3. It  is clear from the table that  for most  of 
the experimental conditions evaluated the system 
outperformed a baseline system that consisted of 
an SMT system trained on all of the data. For those 
metrics in which performance degraded, in all-but-
one the results were statistically insignificant, and 
in all cases most  of the other MT evaluation met-
rics showed an improvement. Some of the lan-
guage pairs showed striking improvements, in par-
ticular both of the Malay languages id and ms im-
proved by over 3.5 BLEU points each using our 
technique. Interestingly Dutch, a relative of Malay, 
also improved substantially. This evidence points 
to a linguistic explanation for the gains. Malay has 
very simple and regular question structure, the 
question words appear at the front of question sen-
tences (in the same way as the target  language) and 
do not take any other function in the language (un-
like the English ?do? for example). Perhaps this 
simplicity of expression allowed our class-specific 
models to model the data well in spite of the re-
duced data caused by dividing the data. Another 
factor might be the performance of the classifier 
which was high for all these languages (around 
98%). Unfortunately, it is hard to know the reasons 
behind the variety of scores in the table. One large 
factor is likely to be differences in corpus quality, 
and also the relationship between the source and 
target  corpus. Some corpora are direct translations 
of each other, whereas others are translated 
through another language. Chinese was one such 
language, and this may explain why we were un-
able to improve on the baseline for this language 
even though we were very successful for both 
Japanese and Thai, which are relatives of Chinese.
4.4  Comparison to Previous Methods
We ran an experiment to compare our proposed 
method to an instance of our system that  used hard 
weights. The aim was to come as close as possible 
within our framework to the system proposed by 
Yamamoto and Sumita (2007). We used weights of 
1 and 0, instead of the classification probabilities 
to weight the class-specific models. To achieve 
this, we thresholded the probabilities from the clas-
sifier such that probabilities >0.5 gave a weight of 
1, otherwise a weight  of 0 was used. The perform-
ance of this system is shown in Table 4 under the 
column heading ?Hard?. In all-but-one of the con-
Figure 3. Graph showing the BLEU score on the 
developmment set plotted against the general 
model?s interpolation weight (a weight of 0 
meaning no contribution from the general model) 
for two systems in our experiments.
0 0.2 0.4 0.6 0.8 1Model interpolation weight
0.38
0.4
0.42
0.44
0.46
BLE
U sc
ore
zhid
214
ditions this system was outperformed by or equal 
to the proposed approach. 
The column labeled ?No Classifier? in Table 4 
illustrates the effectiveness of the classifier in our 
system. These results show the effect of using 
equal weights (0.5) to interpolate between the 
Question and Declaration models. This system, 
although not  as effective as the system with the 
classifier, gave a respectable performance.
5 Conclusion
In this paper we have presented a technique for 
combining all models from multiple SMT  engines 
into a single decoding process. This technique al-
lows for topic-dependent decoding with probabilis-
tic soft weighting between the component  models. 
We demonstrated the effectiveness of our approach 
on conversational data by building class-specific 
models for interrogative and declarative sentence 
classes. We carried out an extensive evaluation of 
the technique using a large number of language 
pairs and MT evaluation metrics. In most cases we 
were able to show significant improvements over a 
system without model interpolation, and for some 
language pairs the approach excelled. The best im-
provement of all the language pairs was for Malay-
sian (Malay)-English which outperformed the 
baseline system by 4.7 BLEU points (from 0.463 
to 0.510). In future research we would like to try 
the approach with larger sets of models, and also 
(possibly overlapping) subsets of the data produced 
using automatic clustering methods. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
an automatic metric for MT  evaluation with im-
proved correlation with human judgments.  ACL-
2005: Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization, pp. 65-72.
David Carter, 1994. Improving Language Models by 
Clustering Training Sentences, Proc. ACL, pp. 59-64. 
J. Civera and A. Juan. Domain adaptation in statistical 
machine translation with mixture modelling. In Pro-
ceedings of the Second Workshop on ACL Statistical 
Machine Translation, pp. 177-180, Prague,
Czech Republic, June 2007.Andrew Finch, Etienne De-
noual, Hideo Okuma, Michael Paul, Hirofumi Ya-
mamoto, Keiji Yasuda, Ruiqiang Zhang, and Eiichiro 
Sumita. 2007. The NICT/ATR speech translation 
system for IWSLT 2007.  IWSLT 2007, Trento, Italy.
George Doddington. 2002 Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence 
statistics. Proc. of Human Language Technology 
Conference, San Diego, California, pp. 138-145.
George Foster and Roland Kuhn. 2007. Mixture-model 
adaptation for SMT. In Proceedings of the Second 
Workshop on Statistical Machine Translation, ACL, 
pp. 128-135, Prague, Czech Republic,
Sasa Hasan and Hermann Ney. 2005. Clustered Lan-
guage Models Based on Regular Expressions for 
SMT, Proc. EAMT, Budapest, Hungary. 
Rukmini Iyer and Mari Ostendorf. 1994. Modeling 
Long Distance Dependence in Language: Topic mix-
ture versus dynamic cache models, IEEE Transac-
tions on Speech and Audio Processing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Human Language Technology Con-
ference 2003, Edmonton, Canada. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. Machine translation: from real users to research: 
6th conference of AMTA,  Washington, DC, Springer 
Verlag, pp. 115-124.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Rich-
ard Zens,  Chris Dyer, Ond?ej Bojar, Alexandra Con-
stantin, Evan Herbst.  2007. Moses: open source 
toolkit for statistical machine translation,  ACL 2007: 
proceedings of demo and poster sessions, Prague, 
Czech Republic, pp. 177-180. 
Franz J. Och, Hermann Ney, 2003. A Systematic Com-
parison of Various Statistical Alignment Models, 
Computational Linguistics, No. 1, Vol. 29, pp. 19-51.
Franz J. Och, 2003. Minimum error rate training for 
statistical machine trainslation, Proc. ACL.
Michael Paul,  2006. Overview of the IWSLT 2006 
Evaluation Campaign, IWSLT 2006.
Kishore Papineni, Salim Roukos, Todd Ward, & Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. IBM Research Report, 
RC22176, September 17.
Ronald Rosenfeld.  1996. A maximum entropy approach 
to adaptive statistical language modeling. Computer 
Speech and Language, 10:187?228. 
Matthew Snover, Bonnie Dorr,  Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of 
Translation Edit Rate with Targeted Human Annota-
tion,  Proceedings of Association for Machine Trans-
lation in the Americas.
Andreas Stolcke. 1999. SRILM - An Extensible Lan-
guage Model Toolkit.
     http://www.speech.sri.com/projects/srilm/
Hirofumi Yamamoto and Eiichiro Sumita. 2007. Bilin-
gual cluster based models for statistical machine 
translation. EMNLP-CoNLL-2007, Prague, Czech 
Republic; pp. 514-523.
Le Zhang. 2004. Maximum Entropy Modeling Toolkit 
for Python and C++, [On-line].
215
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1124?1132,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Bidirectional Phrase-based Statistical Machine Translation
Andrew Finch
NICT, Keihanna Science City,
Kyoto, 619-0288, Japan
andrew.finch@nict.go.jp
Eiichiro Sumita
NICT, Keihanna Science City,
Kyoto, 619-0288, Japan
eiichiro.sumita@nict.go.jp
Abstract
This paper investigates the effect of di-
rection in phrase-based statistial machine
translation decoding. We compare a typ-
ical phrase-based machine translation de-
coder using a left-to-right decoding strat-
egy to a right-to-left decoder. We also
investigate the effectiveness of a bidirec-
tional decoding strategy that integrates
both mono-directional approaches, with
the aim of reducing the effects due to lan-
guage specificity. Our experimental eval-
uation was extensive, based on 272 differ-
ent language pairs, and gave the surprising
result that for most of the language pairs,
it was better decode from right-to-left than
from left-to-right. As expected the rela-
tive performance of left-to-right and right-
to-left strategies proved to be highly lan-
guage dependent. The bidirectional ap-
proach outperformed the both the left-to-
right strategy and the right-to-left strategy,
showing consistent improvements that ap-
peared to be unrelated to the specific lan-
guages used for translation. Bidirectional
decoding gave rise to an improvement in
performance over a left-to-right decoding
strategy in terms of the BLEU score in
99% of our experiments.
1 Introduction
Human language production by its very nature is
an ordered process. That is to say, words are writ-
ten/uttered in a sequence. The current genera-
tion of phrase-based statistical machine translation
(SMT) systems also generate their target word se-
quences according to an order. Since the gener-
ation process is symmetrical, there are two pos-
sible strategies that could be used to generate the
target: from beginning to end; or from end to be-
ginning. Generating the target in the ?wrong? di-
rection (the opposite direction to the way in which
humans do) is counter intuitive, and possibly as a
result of this, SMT systems typically generate the
target word sequence in the same order as human
language production. However it is not necessar-
ily the case that this is most effective strategy for
all language pairs. In this paper we investigate the
effect of direction in phrase-based SMT decoding.
For the purposes of this paper, we will refer
to target word sequence generation that follows
the same order as human language production as
forward generation, and generation in the oppo-
site direction to human language production as re-
verse generation. These are often referred ?left-to-
right? and ?right-to-left? respectively in the litera-
ture, but we avoid this notation as many languages
are naturally written from right-to-left.
In earlier work (Watanabe and Sumita, 2002),
it was hypothesized that the optimal direction for
decoding was dependent on the characteristics of
the target language. Their results show that for
Japanese to English translation a reverse decod-
ing strategy was the most effective, whereas for
English to Japanese translation, a forward decod-
ing strategy proved superior. In addition they im-
plemented a bidirectional decoder, but their re-
sults were mixed. For English to Japanese transla-
tion, decoding bidirectionally gives higher perfor-
mance, but for Japanese to English translation they
were unable to improve performance by decod-
ing bidirectionally. Their experiments were per-
formed using a decoder based on IBM Model 4
using the translation techniques developed at IBM
(Brown et al, 1993).
This work is closely related to the techniques
proposed in (Watanabe and Sumita, 2002), but in
our case we decode within the framework of a
phrase-based SMT system, rather than the IBM
model. Our intention was to explore the effect of
direction in decoding within the context of a more
1124
contemporary machine translation paradigm, and
to experiment with a broader range of languages.
The underlying motivation for our studies however
remains the same. Languages have considerably
different structure, and certain grammatical con-
structs tend to occupy particular positions within
sentences of the same language, but different po-
sitions across languages. These differences may
make it easier to tackle the automatic translation
of a sentence in a given language from a partic-
ular direction. Our approach differs in that the
decoding process of a phrased-based decoder is
quite different from that used by (Watanabe and
Sumita, 2002) since decoding is done using larger
units making the re-ordering process much sim-
pler. In (Watanabe and Sumita, 2002) only one
language pair is considered, for our experiments
we extended this to include translation among 17
different languages including the Japanese and En-
glish pair used in (Watanabe and Sumita, 2002).
We felt that it was important to consider as many
languages as possible in this study, as intuition
and evidence from the original study suggests that
the effect of direction in decoding is likely to be
strongly language dependent.
The next section briefly describes the mecha-
nisms underlying phrase-based decoding. Then
we explain the principles behind the forward, re-
verse and bidirectional decoding strategies used in
our experiments. Section 3 presents the experi-
ments we performed. Section 4 gives the results
and some analysis. Finally in Section 5, we con-
clude and offer possible directions for future re-
search.
2 Phrase-based Translation
For our experiments we use the phrase-based ma-
chine translation techniques described in (Koehn,
2004) and (Koehn et al, 2007), integrating our
models within a log-linear framework (Och and
Ney, 2002).
One of the advantages of a log-linear model is
that it is possible to integrate a diverse set of fea-
tures into the model. For the decoders used in the
experiments in this paper, we included the follow-
ing feature functions:
? An n-gram language model over the target
word sequence
- Ensures the target word sequence is a
likely sequence of words in the target
language
? A phrase translation model
- Effects the segmentation of the source
word sequence, and is also responsible
for the transformation of source phrases
into target phrases.
? A target word sequence length model
- Controls the length of the target word
sequence. This is usually a constant
term added for each word in the trans-
lation hypothesis.
? A lexicalized distortion model
- Influences the reordering of the trans-
lated source phrases in the target word
sequence using lexical context on the
boundaries of the phrases being re-
ordered.
2.1 Decoding
In a phrase-based SMT decoder, the word se-
quence of the target language is typically gener-
ated in order in a forward manner. The words
at the start of the translation are generated first,
then the subsequent words, in order until the fi-
nal word of the target word sequence is gener-
ated. As the process is phrase-based, the trans-
lation is generated in a phrase-by-phrase manner,
rather word-by-word. The basic idea is to seg-
ment the source word sequence into subsequences
(phrases), then translate each phrase individually,
and finally compose the target word sequence by
reordering the translations of the source phrases.
This composition must occur in a particular order,
such that target words are generated sequentially
from the start (or end in the case of reverse de-
coding) of the sentence. The reason that the target
needs to be generated sequentially is to allow an
n-gram language model to be applied to the partial
target word sequence at each step of the decoding
process.
This process is illustrated in Figure 1. In the
decoding for both forward and reverse decoders
the source sentence is segmented into 2 phrases:
?where is? and ?the station? (although in this ex-
ample the segmentation is the same for both de-
coding strategies, it is not necessarily the case
since the search processes are different). In the
forward decoding process, first the English phrase
?the station? is translated into the Japanese phrase
?eki wa?. Initially the target sequence consists
1125
Left to right Right to left
where is the station
<s>
<s> eki wa
<s> eki wa doko </s>
</s>
doko </s>
<s> eki wa doko </s>
P(eki | <s> )
P(wa | eki, <s>)
P(doko | wa, eki, <s>)
P(</s> | doko, wa, eki, <s>)
P
LM
 =
G
e
n
e
r
a
t
i
o
n
P(doko | </s> )
P(wa | doko, </s>)
P(eki | wa, doko, </s>)
P(<s> | eki, wa, doko, </s>)
P
LM
 =
where is the station
Figure 1: The phrase-based decoding process for an English to Japanese translation, in both forward
and reverse directions. The n-gram language model probability calculation for the completed translation
hypotheses are also shown on the bottom of the figure. See Section 2.1 for a description of the decoding
process.
of only the start of sentence marker ??s??. This
marker only serves as context to indicate the start
of the sequence for the benefit of the language
model. The first target phrase is separated into its
component words and each word is added in order
to the target word sequence. Each addition causes
an application of the language model, hence in
Figure 1 the first term of P
LM
is P (eki|?s?), the
second is P (wa|?s?) and so on. For reverse de-
coding, the target sentence is generated starting
from the end of sentence marker ?/s? with the lan-
guage model context being to the right of the cur-
rent word. For the case of bidirectional decoding,
the model probability for the hypothesis is a linear
interpolation of the scores for both forward and re-
verse hypotheses.
2.2 Direction in Decoding
Direction in decoding influences both the models
used by the decoder and the search process itself.
The direction of decoding determines the order
in which target words are generated, the source
phrases being translated in any order, therefore it
is likely to be features of the target language rather
than those of the the source language that deter-
mine the effect that the decoding direction has on
decoder performance.
2.2.1 The Language Model
The fundamental difference between the language
models of a forward decoder and that of a reverse
decoder is the direction in which the model looks
for its context. The forward model looks back
to the start of the sentence, whereas the reverse
model looks forward to the end of the sentence.
2.2.2 The Search
Assuming a full search, a unigram language model
and no limitations on reordering, the forward and
reverse decoding processes are equivalent. When
these constraints are lifted, as is the case in the
experiments in this paper, the two search processes
diverge and can give rise to hypotheses that are
different in character.
The partial hypotheses from early in the search
process for forward decoding represent hypothe-
ses for the first few words of the target word se-
quence, whereas the early partial hypotheses of
a reverse decoder hold the last few words. This
has two consequences for the search. The first is
that (assuming a beam search as used in our ex-
periments), certain candidate word sequences in
the early stages of the search might be outside the
beam and be pruned. The consequence of this
is that sentences that start with (or end with in
the case of reverse decoding) the pruned word se-
quence will not be considered during the remain-
der of the search. The second is that word se-
1126
quences in the partial hypotheses are used in the
context of the models used in the subsequent de-
coding. Thus, correctly decoding the start (or end
for reverse decoding) of the sentence will benefit
the subsequent decoding process.
3 Experiments
3.1 Experimental Data
The experiments were conducted on all possi-
ble pairings among 17 languages. A key to the
acronyms used for languages together with in-
formation about their respective characteristics is
given in Table 1.
We used all of the first ATR Basic Travel Ex-
pression Corpus (BTEC1) (Kikui et al, 2003) for
these experiments. This corpus contains the kind
of expressions that one might expect to find in a
phrase-book for travelers. The corpus is similar in
character to the IWSLT06 Evaluation Campaign
on Spoken Language Translation (Paul, 2006) J-E
open track. The sentences are relatively short (see
Table 1) with a simple structure and a fairly narrow
range of vocabulary due to the limited domain.
The experiments were conducted on data that
contained no case information, and also no punc-
tuation (this was an arbitrary decision that we be-
lieve had no impact on the results).
We used a 1000 sentence development corpus
for all experiments, and the corpus used for eval-
uation consisted of 5000 sentences with a single
reference for each sentence.
3.2 Training
Each instance of the decoder is a standard phrase-
based machine translation decoder that operates
according to the same principles as the publicly
available PHARAOH (Koehn, 2004) and MOSES
(Koehn et al, 2007) SMT decoders. In these
experiments 5-gram language models built with
Witten-Bell smoothing were used along with a lex-
icalized distortion model. The system was trained
in a standard manner, using a minimum error-rate
training (MERT) procedure (Och, 2003) with re-
spect to the BLEU score (Papineni et al, 2001)
on held-out development data to optimize the log-
linear model weights. For simplicity, the MERT
procedure was performed on independently on the
forward and reverse decoders for the bidirectional
system, rather them attempting to tune the param-
eters for the full system.
3.3 Translation Engines
3.3.1 Forward
The forward decoding translation systems used in
these experiments represent the baseline of our ex-
periments. They consist of phrase-based, multi-
stack, beam search decoders commonly used in
the field.
3.3.2 Reverse
The reverse decoding translation systems used in
these experiments were exactly the same as the
forward decoding systems. The difference being
the that word sequences in the training, develop-
ment, and source side of the test corpora were re-
versed prior to training the systems. The final out-
put of the reverse decoders was reordered in a post
processing step before evaluation.
3.3.3 Bidirectional
The decoder used for the bidirectional decoding
experiments was modified in order to be able to
decode both forward and reverse in separate in-
stances of the decoder. Models for decoding in
forward and reverse directions are loaded, and two
decoding instances created. Scores for hypotheses
that share the same target word sequence from the
two decoders were combined at the end of the de-
coding process linearly using equal interpolation
weights. Hypotheses that were generated by only
one of the component decoders were not pruned.
The scores from these hypotheses only had a con-
tribution from the decoder that was able to gener-
ate them, the contribution from the other decoder
being zero.
3.4 Decoding Constraints
The experiments reported in this paper were con-
ducted with loose constraints on the decoding as
overconstraining the decoding process could lead
to differences between unidirectional and bidirec-
tional strategies. More specificially, the decod-
ing was done with a beam width of 100, no beam
thresholding and no constraints on the reordering
process. Figure 2 shows the effect of varying the
beam width (stack size) in the search for forward
decoder of the English to Japanese translation ex-
periment. At the beam width of 100 used in our
experiments, the gains from doubling the beam
with are small (0.07 BLEU percentage points).
It is also important to note that a future cost
identical to that used in the MOSES decoder
1127
Abbreviation Language #Words Avg. sent length Vocabulary Order
ar Arabic 806853 5.16 47093 SVO
da Danish 806853 5.16 47093 SVO
de German 907354 5.80 23443 SVO
en English 970252 6.21 12900 SVO
es Spanish 881709 5.64 18128 SVO
fr French 983402 6.29 17311 SVO
id Indonesian (Malay) 865572 5.54 15527 SVO
it Italian 865572 5.54 15527 SVO
ja Japanese 1149065 7.35 15405 SOV
ko Korean 1091874 6.98 17015 SOV
ms Malaysian (Malay) 873959 5.59 16182 SVO
nl Dutch 927861 5.94 19775 SVO
pt Portuguese 881428 5.64 18217 SVO
ru Russian 781848 5.00 32199 SVO
th Thai 1211690 7.75 6921 SVO
vi Vietnamese 1223341 7.83 8055 SVO
zh Chinese 873375 5.59 14854 SVO
Table 1: Key to the languages, corpus statistics and word order. SVO denotes a language that predomi-
nantly has subject-verb-object order, and SOV denotes a language that predominantly has subject-object-
verb order
Stack size BLEU Score
1 0.3954
2 0.4032
4 0.4075
8 0.4115
16 0.4149
32 0.4161
64 0.4181
128 0.4188
256 0.4197
512 0.4197
1024 0.4197
0.39
0.398
0.406
0.414
0.422
0 256 512 768 1024
B
L
E
U
 
S
c
o
r
e
Stack size
Figure 2: The performance of a forward decoder
(En-Ja) with increasing stack size.
(Koehn et al, 2007) was also included in the
scores for partial hypothesis during the decoding.
3.5 Computational Overhead
In the current implementation, bidirectional de-
coding takes twice as long as a mono-directional
system. However, in a multi-threaded environ-
ment, each instance of the decoder is able to run
on its own thread in parallel, and so this slowdown
can be mitigated in some circumstances. Future
generations of the bidirectional decoder will more
tightly couple the two decoders, and we believe
this will lead to faster and more effective search.
3.6 Evaluation
The results presented in this paper are given in
terms of the BLEU score (Papineni et al, 2001).
This metric measures the geometric mean of n-
gram precision of n-grams drawn from the output
translation and a set of reference translations for
that translation.
There are large number of proposed methods
for carrying out machine translation evaluation.
Methods differ in their focus of characteristics of
the translation (for example fluency or adequacy),
and moreover anomolous results can occur if a
single metric is relied on. Therefore, we also
carried out evaluations using the NIST (Dodding-
ton, 2002), METEOR (Banerjee and Lavie, 2005),
WER (Hunt, 1989), PER (Tillmann et al, 1997)
and TER (Snover et al, 2005) machine translation
evaluation techniques.
4 Results
The results of the experiments in terms of the
BLEU score are given in Tables ??, 5, 3 and
3. These results show the performance of the re-
verse and bidirectional decoding strategies relative
to the usual forward decoding strategy. The cells
in the tables that represent experiments in which
1128
ar da de en es fr id it ja ko ms nl pt ru th vi zh
ar - 47.8 48.8 51.7 48.8 47.3 46.5 49.2 29.8 27.8 46.9 49.0 49.0 47.8 39.7 43.0 27.8
da 58.3 - 58.7 63.0 58.6 55.7 53.5 58.5 37.5 35.1 54.4 59.6 59.0 55.4 48.1 51.7 35.2
de 53.8 55.5 - 59.4 55.9 51.9 50.3 55.3 34.2 32.0 50.8 57.0 55.9 51.2 45.7 48.9 32.7
en 63.6 65.8 64.8 - 67.0 61.0 58.4 65.8 41.1 38.7 59.1 67.6 66.7 58.7 52.8 57.7 38.6
es 57.6 58.2 58.0 65.6 - 56.6 54.2 61.1 38.3 36.4 54.3 59.6 62.6 55.1 47.6 51.3 36.0
fr 57.8 58.3 58.0 62.3 58.9 - 52.7 57.4 39.1 37.7 53.8 58.3 57.9 54.8 47.7 50.4 37.6
id 54.7 52.8 52.8 56.6 53.7 51.0 - 53.1 37.2 35.6 86.4 53.8 53.0 51.3 46.4 48.4 34.9
it 54.1 53.4 54.4 59.4 56.4 51.8 49.2 - 34.4 32.8 49.9 55.1 56.2 50.5 44.0 47.0 33.6
ja 38.2 39.2 38.6 41.9 39.9 40.2 40.7 39.5 - 69.4 40.4 39.5 39.7 37.8 37.3 37.2 52.1
ko 34.4 35.3 34.6 38.2 36.3 36.2 36.8 35.6 66.4 - 36.6 35.6 36.3 34.5 34.2 34.1 46.4
ms 54.5 52.7 52.6 56.2 53.4 50.6 82.5 53.2 36.8 34.9 - 53.6 53.4 51.3 46.7 49.2 34.8
nl 55.1 57.3 58.8 63.2 58.5 54.5 52.4 57.1 36.7 34.1 53.4 - 58.3 53.5 48.7 50.7 35.2
pt 56.8 57.7 57.6 63.8 62.0 55.5 52.7 59.7 37.8 36.4 53.4 58.7 - 54.2 47.1 50.6 35.8
ru 51.4 49.1 50.2 53.3 52.0 48.7 48.6 51.6 31.9 29.5 49.1 50.9 50.5 - 41.8 43.7 30.0
th 53.8 55.0 54.8 58.2 55.8 53.3 55.0 54.8 41.4 39.2 55.4 55.9 55.5 53.0 - 56.0 40.4
vi 53.6 53.6 54.2 57.4 54.2 51.4 52.3 53.3 37.6 35.8 53.3 54.6 54.4 51.7 50.3 - 36.2
zh 32.0 33.0 32.6 34.6 33.2 33.7 34.2 33.2 47.8 43.5 33.9 33.4 32.6 32.2 31.1 29.7 -
Table 2: Baseline BLEU scores for all systems. The figures represent the scores in BLEU percentage
points of the baseline left-to-right decoding systems. Source languages are indicated by the column
headers, the row headers denoting the target languages.
the forward strategy outperformed the contrasting
strategy are shaded in gray. The numbers in the
cells represent the difference in BLEU percentage
points for the systems being compared in that cell.
It is clear from Table 3 that for most of the lan-
guage pairs (67% of them for BLEU, and a simi-
lar percentage for all the other metrics except ME-
TEOR), better evaluation scores were achieved by
using a reverse decoding strategy than a forward
strategy. This is a surprising result because lan-
guage is produced naturally in a forward manner
(by definition), and therefore one might expect this
to also be the optimal direction for word sequence
generation in decoding.
4.1 Word Order Typography
Following (Watanabe and Sumita, 2002), to ex-
plain the effects we observe in our results we look
to the word order typography of the target lan-
guage (Comrie and Vogel, 2000). The word or-
der of a language is defined in terms of the order
in which you would expect to encounter the finite
verb (V) and its arguments, subject (S) and ob-
ject (O). In most languages S precedes O and V.
Whether or not O precedes or follows V defines
the two most prevalent word order types SOV and
SVO (Comrie and Vogel, 2000).
Two of the target languages in this study
(Japanese and Korean) have the SOV word type,
the remainder having the SVO word order type.
In Table 3 looking at the rows for ja and ko we
can see that for both of these languages reverse
decoding outperformed forward decoding in only
4 out of 12 experiments. Furthermore these two
languages were the two languages that benefited
the most (in terms of the number of experimental
cases) from forward decoding. The two languages
also agree on the best decoding direction for 12 of
the 16 language pairs. This apparent correlation
may reflect similarities between the two languages
(word order type, or other common features of the
languages).
Given this evidence, it seems plausible that
word order does account in part for the differences
in performance when decoding in differing direc-
tions, but this can only be part of the explanation
since there are 4 source languages for which re-
verse decoding yielded higher performance.
It should be noted that our results differ from
those of (Watanabe and Sumita, 2002) for En-
glish to Japanese translation, who observed gains
when decoding in the reverse direction for this lan-
guage pair. It is hard to compare our results di-
rectly with theirs however, due to the differences
in the decoders used in the experiments (ours be-
ing phrase-based, and theirs based on the IBM ap-
1129
ar da de en es fr id it ja ko ms nl pt ru th vi zh
ar - 0.87 0.34 1.30 0.93 1.63 0.66 0.58 0.12 0.36 0.85 0.33 0.88 0.22 1.33 1.04 0.88
da 0.25 - 0.41 0.71 0.56 0.70 1.10 0.31 0.46 0.07 0.96 0.13 0.62 0.17 1.28 0.71 0.29
de 0.41 0.04 - 0.38 0.52 0.15 0.80 0.01 0.47 0.72 0.60 0.25 0.21 0.05 0.47 0.68 0.20
en 0.04 0.05 0.21 - 0.05 0.13 0.58 0.02 0.73 0.35 0.39 0.07 0.52 0.05 0.67 0.63 0.29
es 0.14 0.19 0.05 0.35 - 0.68 0.01 0.08 0.25 0.31 0.25 0.25 0.17 0.07 0.43 0.44 0.78
fr 0.37 0.57 0.38 0.66 0.21 - 0.36 0.28 0.15 0.45 0.22 0.46 0.64 0.10 0.25 0.58 0.31
id 0.16 0.02 0.31 1.45 0.58 0.50 - 0.34 0.03 0.27 0.00 0.42 0.57 0.36 0.53 1.04 0.59
it 0.28 0.72 0.36 0.27 0.08 0.30 0.11 - 0.07 0.12 0.37 0.23 0.05 0.37 0.04 0.63 0.37
ja 0.36 0.22 0.03 0.03 0.22 0.13 0.64 0.36 - 0.21 0.57 0.46 0.08 0.33 0.08 0.83 0.70
ko 0.35 0.01 0.31 0.03 0.12 0.07 0.13 0.21 0.42 - 0.29 0.07 0.42 0.40 0.44 0.62 0.05
ms 0.06 0.49 0.53 1.38 0.99 0.71 0.47 0.34 0.11 0.32 - 0.62 0.27 0.10 0.83 0.99 0.11
nl 0.26 0.03 0.26 0.30 0.20 0.19 0.47 0.23 0.13 0.06 0.06 - 0.08 0.09 0.06 1.00 0.15
pt 0.03 0.34 0.06 0.51 0.07 0.17 0.06 0.18 0.13 0.65 0.08 0.10 - 0.06 0.09 0.85 0.35
ru 0.25 0.58 0.67 0.74 0.01 0.48 0.50 0.27 0.41 0.38 0.13 0.38 0.46 - 0.88 0.56 0.49
th 0.19 0.28 0.21 0.41 0.05 0.23 0.30 0.00 0.34 0.04 0.25 0.07 0.21 0.08 - 0.46 0.25
vi 0.21 0.34 0.24 0.65 0.72 0.34 0.06 0.59 0.24 0.22 0.19 0.12 0.11 0.18 0.63 - 0.15
zh 0.43 0.26 0.42 0.05 0.15 0.31 0.16 0.28 0.00 0.31 0.40 0.14 0.67 0.18 0.39 0.21 -
Table 3: Gains in BLEU score from reverse decoding over a forward decoding strategy The numbers
in the cells are the differences in BLEU percentage points between the systems. Shaded cells indicate
the cases where forward decoding give a higher score. Source languages are indicated by the column
headers, the row headers denoting the target languages.
Metric Bi>For Bi>Rev Rev>For
BLEU 98.90 84.93 67.65
NIST 98.53 78.31 75.00
METEOR 99.63 95.96 50.74
WER 99.26 92.85 66.18
PER 98.53 84.97 70.59
TER 99.63 91.18 68.75
Table 4: Summary of the results using several au-
tomatic metrics for evaluation. Numbers in the ta-
ble correspond to the percentage of experiments
in which the condition at the head of the column
was true (for example figure in the first row and
first column means that for 98.9 percent of the lan-
guage pairs the BLEU score for the bidirectional
decoder was better than that of the forward de-
coder)
proach (Brown et al, 1993)).
The results were the similar in character when
other MT evaluation methods were used. These
results are summarized in Table 3.
4.2 Bidirectional Decoding
Table 5 shows the performance of the bidirectional
decoder relative to a forward decoder. As can be
seen from the table, in 269 out of the 272 experi-
ments the bidirectional decoder outperformed the
unidirectional decoder. The gains ranged from a
maximum of 1.81 BLEU (translating from Thai
to Arabic) points, to a minimum of -0.04 BLEU
points (translating from Indonesian to Japanese)
with the average gain over all experiments being
0.56 BLEU points. It is clear from our experi-
ments that there is much to be gained from decod-
ing bidirectionally. Our results were almost unani-
mously positive, and in all three negative cases the
drop in performance was small.
5 Conclusion
In this paper we have investigated the effects on
phrase-based machine translation performance of
three different decoding strategies: forward, re-
verse and bidirectional. The experiments were
conducted on a large set of source and target lan-
guages consisting of 272 experiments representing
all possible pairings from a set of 17 languages.
These languages were very diverse in character
and included a broad selection of European and
Asian languages. The experimental results re-
vealed that for SVO word order languages it is
usually better to decode in a reverse manner, and in
contrast, for SOV word order languages it is usu-
1130
ar da de en es fr id it ja ko ms nl pt ru th vi zh
ar - 0.66 0.51 1.03 0.65 0.75 0.59 0.47 0.46 0.85 0.59 0.69 0.39 0.30 1.81 1.30 0.85
da 0.27 - 0.61 0.63 0.38 0.60 0.59 0.29 1.04 0.79 0.69 0.45 0.89 0.27 1.28 0.87 0.47
de 0.52 0.51 - 0.54 0.44 0.42 0.70 0.40 0.74 0.45 0.83 0.37 0.28 0.34 0.77 0.90 0.84
en 0.53 0.01 0.32 - 0.23 0.25 0.56 0.19 1.11 0.59 0.28 0.27 0.45 0.60 0.89 0.61 0.58
es 0.28 0.48 0.45 0.56 - 0.43 0.12 0.26 0.57 0.64 0.56 0.06 0.04 0.24 1.16 1.23 0.68
fr 0.70 0.33 0.54 0.66 0.46 - 0.49 0.57 0.24 0.13 0.11 0.43 0.33 0.55 0.91 1.09 0.57
id 0.24 0.32 0.36 0.93 0.70 0.65 - 0.35 0.75 0.77 0.11 0.46 0.69 0.57 0.99 0.85 0.47
it 0.13 0.55 0.32 0.43 0.47 0.51 0.64 - 0.65 0.42 0.77 0.51 0.51 0.69 0.85 0.98 0.58
ja 0.38 0.62 0.60 0.61 0.38 0.73 0.04 0.43 - 0.35 0.05 0.70 0.30 0.38 0.53 0.17 0.02
ko 0.49 0.62 0.90 0.40 0.34 0.57 0.47 0.47 0.02 - 0.23 0.52 0.20 0.83 0.70 0.44 0.83
ms 0.37 0.57 0.63 0.92 0.81 0.75 0.36 0.54 0.70 1.31 - 0.76 0.35 0.51 1.14 0.70 0.35
nl 0.35 0.14 0.54 0.33 0.30 0.46 0.68 0.69 0.77 0.63 0.44 - 0.42 0.67 0.71 1.13 0.55
pt 0.46 0.21 0.37 0.21 0.17 0.49 0.47 0.24 0.88 0.45 0.54 0.39 - 0.41 0.94 1.15 0.90
ru 0.69 0.63 0.69 0.77 0.26 0.50 0.79 0.52 0.69 0.90 0.66 0.69 0.40 - 1.19 1.23 0.47
th 0.90 0.49 0.53 0.77 0.64 0.38 0.21 0.60 0.37 0.96 0.38 0.63 0.68 0.72 - 0.33 0.45
vi 0.64 0.61 0.42 1.09 0.84 0.63 0.34 0.70 0.59 0.39 0.16 0.56 0.36 0.50 0.77 - 0.53
zh 0.23 0.48 0.96 0.33 0.49 0.32 0.27 0.43 0.43 0.69 0.31 0.97 0.85 0.23 0.40 0.50 -
Table 5: Gains in BLEU score from decoding bidirectionally over a forward decoding strategy. The
numbers in the cells are the differences in BLEU percentage points between the systems. Shaded cells
indicate the cases where forward decoding gave a higher score. Source languages are indicated by the
column headers, the row headers denoting the target languages.
ally better to decode in a forward direction. Our
main contribution has been to show that a bidirec-
tional decoding strategy is superior to both mono-
directional decoding strategies. It might be argued
that the gains arise simply from system combina-
tion. However, our systems are combined in a sim-
ple linear fashion, and gains will only arise when
the second system contributes novel and useful in-
formation to into the combination. Furthermore,
our systems are trained on two copies of the same
data, no additional data is required. The gains
from decoding bidirectionally were obtained very
consistently, with only loose constraints on the de-
coding. This can be seen clearly in Table 5 where
the results are almost unanimously positive. More-
over, these gains appear to be independent of the
linguistic characteristics of the source and target
languages.
In the future we would like to explore the pos-
sibilities created by more tightly coupling the for-
ward and reverse components of the bidirectional
decoder. Scores from partial hypotheses of both
processes could be combined and used at each
step of the decoding, making the search more in-
formed. Furthermore, forward partial hypotheses
and reverse hypotheses would ?meet? during de-
coding (when one decoding direction has covered
words in the source that the other has yet to cover),
and provide paths for each other to a final state in
the search.
Acknowledgment
This work is partly supported by the Grant-in-
Aid for Scientific Research (C) Number 19500137
and ?Construction of speech translation founda-
tion aiming to overcome the barrier between Asian
languages?, the Special Coordination Funds for
Promoting Science and Technology of the Min-
istry of Education, Culture, Sports, Science and
Technology, Japan.
References
Satanjeev Banerjee and Alon Lavie. 2005. Me-
teor: an automatic metric for mt evaluation with im-
proved correlation with human judgments. In ACL-
2005: Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization, pages 65?72.
P. Brown, S. Della Pietra, V. Della Pietra, and R.J. Mer-
cer. 1993. The Mathematics of Statistical Machine
Translation: Parameter Estimation. Computational
Linguistics, 19(2):263?311.
Bernard Comrie and Petra M Vogel, editors. 2000. Ap-
proaches to the Typography of Word Classes. Mou-
ton de Gruyter, Berlin.
1131
G. Doddington. 2002. Automatic Evaluation of
Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the HLT
Conference, San Diego, California.
Melvyn J. Hunt. 1989. Figures of merit for assess-
ing connected-word recognisers. In In Proceed-
ings of the ESCA Tutorial and Research Workshop
on Speech Input/Output Assessment and Speech
Databases, pages 127?131.
G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.
2003. Creating corpora for speech-to-speech trans-
lation. In Proceedings of EUROSPEECH-03, pages
381?384.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowa, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
open source toolkit for statistical machine transla-
tion. In ACL 2007: proceedings of demo and poster
sessions, pages 177?180, Prague, Czeck Republic,
June.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Machine translation: from real
users to research: 6th conference of AMTA, pages
115?124, Washington, DC.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2002), pages 295?302.
Franz J. Och. 2003. Minimum error rate training for
statistical machine trainslation. In Proceedings of
the ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Michael Paul. 2006. Overview of the iwslt 2006 eval-
uation campaign. In Proceedings of the IWLST.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciula, and Ralph Weischedel.
2005. A study of translation error rate with tar-
geted human annotation. Technical report, Univer-
sity of Maryland, College Park and BBN Technolo-
gies, July.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated dp based search
for statistical translation. In In European Conf.
on Speech Communication and Technology, pages
2667?2670.
Taro Watanabe and Eiichiro Sumita. 2002. Bidirec-
tional decoding for statistical machine translation.
In Proceedings of the 19th international conference
on Computational linguistics, pages 1?7, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
1132
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 105?109,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
NICT@WMT09:
Model Adaptation and Transliteration for Spanish-English SMT
Michael Paul, Andrew Finch and Eiichiro Sumita
Language Translation Group
MASTAR Project
National Institute of Information and Communications Technology
Michael.Paul@nict.go.jp
Abstract
This paper describes the NICT statis-
tical machine translation (SMT) system
used for the WMT 2009 Shared Task
(WMT09) evaluation. We participated in
the Spanish-English translation task. The
focus of this year?s participation was to in-
vestigate model adaptation and transliter-
ation techniques in order to improve the
translation quality of the baseline phrase-
based SMT system.
1 Introduction
This paper describes the NICT statistical machine
translation (SMT) system used for the shared
task of the Fourth Workshop on Statistical Ma-
chine Translation. We participated in the Spanish-
English translation task under the Constrained
Condition. For the training of the SMT engines,
we used two parallel Spanish-English corpora pro-
vided by the organizers: the Europarl (EP) cor-
pus (Koehn, 2005), which consists of 1.4M paral-
lel sentences extracted from the proceedings of the
European Parliament, and the News Commentary
(NC) corpus (Callison-Burch et al, 2008), which
consists of 74K parallel sentences taken from ma-
jor news outlets like BBC, Der Spiegel, and Le
Monde.
In order to adapt SMT systems to a specific do-
main, recent research focuses on model adapta-
tion techniques that adjust their parameters based
on information about the evaluation domain (Fos-
ter and Kuhn, 2007; Finch and Sumita, 2008a).
Statistical models can be trained on in-domain
and out-of-domain data sets and combined at
run-time using probabilistic weighting between
domain-specific statistical models. As the official
WMT09 evaluation testset consists of documents
taken from the news domain, we applied statistical
model adaptation techniques to combine transla-
tion models (tm), language models (lm) and dis-
tortion models (dm) trained on (a) the in-domain
NC corpus and (b) the out-of-domain EP corpus
(cf. Section 2).
One major problem in the given translation task
was the large amount of out-of-vocabulary (OOV)
words, i.e., source language words that do not oc-
cur in the training corpus. For unknown words, no
translation entry is available in the statistical trans-
lation model (phrase-table). As a result, these
OOV words cannot be translated. Dealing with
languages with a rich morphology like Spanish
and having a limited amount of bilingual resources
make this problem even more severe.
There have been several efforts in dealing with
OOV words to improve translation quality. In ad-
dition to parallel text corpora, external bilingual
dictionaries can be exploited to reduce the OOV
problem (Okuma et al, 2007). However, these ap-
proaches depend on the coverage of the utilized
external dictionaries.
Data sparseness problems due to inflectional
variations were previously addressed by applying
word transformations using stemming or lemmati-
zation (Popovic and Ney, 2005; Gupta and Fed-
erico, 2006). A tight integration of morpho-
syntactic information into the translation model
was proposed by (Koehn and Hoang, 2007) where
lemma and morphological information are trans-
lated separately, and this information is combined
on the output side to generate the translation.
However, these approaches still suffer from the
data sparseness problem, since lemmata and in-
flectional forms never seen in the training corpus
cannot be translated.
In order to generate translations for unknown
words, previous approaches focused on translit-
eration methods, where a sequence of charac-
ters is mapped from one writing system into an-
other. For example, in order to translate names and
technical terms, (Knight and Graehl, 1997) intro-
duced a probabilistic model that replaces Japanese
105
katakana1 words with phonetically equivalent En-
glish words. More recently, (Finch and Sumita,
2008b) proposed a transliteration method that is
based directly on techniques developed for phrase-
based SMT, and transforms a character sequence
from one language into another in a subword-
level, character-based manner. We extend this ap-
proach by exploiting the phrase-table of the base-
line SMT system to train a phrase-based translit-
eration model that generates English translations
of Spanish OOV words as described in Section 3.
The effects of the proposed techniques are investi-
gated in detail in Section 4.
2 Model Adaptation
Phrase-based statistical machine translation en-
gines use multiple statistical models to generate a
translation hypothesis in which (1) the translation
model ensures that the source phrases and the se-
lected target phrases are appropriate translations of
each other, (2) the language model ensures that the
target language is fluent, (3) the distortion model
controls the reordering of the input sentence, and
(4) the word penalty ensures that the translations
do not become too long or too short. During de-
coding, all model scores are weighted and com-
bined to find the most likely translation hypothesis
for a given input sentence (Koehn et al, 2007).
In order to adapt SMT systems to a specific do-
main, separate statistical models can be trained
on parallel text corpora taken from the respec-
tive domain (in-domain) and additional out-of-
domain language resources. The models are then
combined using mixture modeling (Hastie et al,
2001), i.e., each model is weighted according to
its fit with in-domain development data sets and
the linear combination of the respective scores is
used to find the best translation hypothesis during
the decoding of unseen input sentences.
In this paper, the above model adaptation tech-
nique is applied to combine the NC and the EP
language resources provided by the organizers
for the Spanish-English translation task. As the
WMT09 evaluation testset consists of documents
taken from the news domain, we used the NC cor-
pus to train the in-domain models and the EP cor-
pus to train the out-of-domain component models.
Using mixture modeling, the above mentioned sta-
tistical models are combined where each compo-
nent model is optimized separately. Weight opti-
1A special syllabary alphabet used to write down foreign
names or loan words.
mization is carried out using a simple grid-search
method. At each point on the grid of weight pa-
rameter values, the translation quality of the com-
bined weighted component models is evaluated for
development data sets taken from (a) the NC cor-
pus and (b) from the EP corpus.
3 Transliteration
Source language input words that cannot be trans-
lated by the standard phrase-based SMT mod-
els are either left untranslated or simply removed
from the translation output. Common examples
are named entities such as personal names or tech-
nical terms, but also include content words like
common nouns or verbs that are not covered by the
training data. Such unknown occurrences could
benefit from being transliterated into the MT sys-
tem?s output during translation of orthographically
related languages like Spanish and English.
In this paper, we apply a phrase-based translit-
eration approach similar to the one proposed in
(Finch and Sumita, 2008b). The transliteration
method is based directly on techniques developed
for phrase-based SMT and treats the task of trans-
forming a character sequence from one language
into another as a character-level translation pro-
cess. In contrast to (Finch and Sumita, 2008b)
where external dictionaries and inter-language
links in Wikipedia2 are utilized, the translitera-
tion training examples used for the experiments in
Section 4 are extracted directly from the phrase-
table of the baseline SMT systems trained on the
provided data sets. For each phrase-table entry,
corresponding word pairs are identified according
to a string similarity measure based on the edit-
distance (Wagner, 1974) that is defined as the sum
of the costs of insertion, deletion, and substitution
operations required to map one character sequence
into the other and can be calculated by a dynamic
programming technique (Cormen et al, 1989). In
order to reduce noise in the training data, only
word pairs whose word length and similarity are
above a pre-defined threshold are utilized for the
training of the transliteration model.
The obtained transliteration model is applied as
a post-process filter to the SMT decoding process,
i.e.. all source language words that could not be
translated using the SMT engine are replaced with
the corresponding transliterated word forms in or-
der to obtain the final translation output.
2http://www.wikipedia.org
106
4 Experiments
The effects of model adaptation and translitera-
tion techniques were evaluated using the Spanish-
English language resources summarized in Ta-
ble 1. In addition, the characteristics of this year?s
testset are given in Table 2. The sentence length
is given as the average number of words per sen-
tence. The OOV word figures give the percentage
of words in the evaluation data set that do not ap-
pear in the NC/EP training data. In order to get an
idea how difficult the translation task may be, we
also calculated the language perplexity of the re-
spective evaluation data sets according to 5-gram
target language models trained on the NC/EP data
sets.
Concerning the development sets, the news-
dev2009 data taken from the same news sources
as the evaluation set of the shared task was used
for the tuning of the SMT engines, and the de-
vtest2006 data taken from the EP corpus was used
for system parameter optimization. For the evalua-
tion of the proposed methods, we used the testsets
of the Second Workshop on SMT (nc-test2007 for
NC and test2007 for EP). All data sets were case-
sensitive with punctuation marks tokenized.
The numbers in Table 1 indicate that the char-
acteristics of this year?s testset differ largely from
testsets of previous evaluation campaigns. The
NC devset (2,438/1,378 OOVs) contains twice
as many untranslatable Spanish words as the
NC evalset (1,168/73 OOVs) and the EP devset
(912/63 OOVs). In addition, the high language
perplexity figures for this year?s testset show that
the translation quality output for both baseline sys-
tems is expected to be much lower than those for
the EP evaluation data sets. In this paper, transla-
tion quality is evaluated according to (1) the BLEU
metrics which calculates the geometric mean of n-
gram precision by the system output with respect
to reference translations (Papineni et al, 2002),
and (2) the METEOR metrics that calculates uni-
gram overlaps between translations (Banerjee and
Lavie, 2005). Scores of both metrics range be-
tween 0 (worst) and 1 (best) and are displayed in
percent figures.
4.1 Baseline
Our baseline system is a fairly typical phrase-
based machine translation system (Finch and
Sumita, 2008a) built within the framework of a
feature-based exponential model containing the
following features:
Table 1: Language Resources
Corpus Train Dev Eval
NC Spanish sentences 74K 2,001 2,007
words 2,048K 49,116 56,081
vocab 61K 9,047 8,638
length 27.6 24.5 27.9
OOV (%) ? 5.2 / 2.9 1.4 / 0.9
English sentences 74K 2,001 2,007
words 1,795K 46,524 49,693
vocab 47K 8,110 7,541
length 24.2 23.2 24.8
OOV (%) ? 5.2 / 2.9 1.2 / 0.9
perplexity ? 349 / 381 348 / 458
EP Spanish sentences 1,404K 1,861 2,000
words 41,003K 50,216 61,293
vocab 170K 7,422 8,251
length 29.2 27.0 30.6
OOV (%) ? 2.4 / 0.1 2.4 / 0.2
English sentences 1,404K 1,861 2,000
words 39,354K 48,663 59,145
vocab 121K 5,869 6,428
length 28.0 26.1 29.6
OOV (%) ? 1.8 / 0.1 1.9 / 0.1
perplexity ? 210 / 72 305 / 125
Table 2: Testset 2009
Corpus Test
NC Spanish sentences 3,027
words 80,591
vocab 12,616
length 26.6
? Source-target phrase translation probability
? Inverse phrase translation probability
? Source-target lexical weighting probability
? Inverse lexical weighting probability
? Phrase penalty
? Language model probability
? Lexical reordering probability
? Simple distance-based distortion model
? Word penalty
For the training of the statistical models, stan-
dard word alignment (GIZA++ (Och and Ney,
2003)) and language modeling (SRILM (Stolcke,
2002)) tools were used. We used 5-gram lan-
guage models trained with modified Knesser-Ney
smoothing. The language models were trained on
the target side of the provided training corpora.
Minimum error rate training (MERT) with respect
to BLEU score was used to tune the decoder?s pa-
rameters, and performed using the technique pro-
posed in (Och, 2003). For the translation, the in-
house multi-stack phrase-based decoder CleopA-
TRa was used.
The automatic evaluation scores of the baseline
systems trained on (a) only the NC corpus and (b)
only on the EP corpus are summarized in Table 3.
107
Table 3: Baseline Performance
NC Eval EP Eval
BLEU METEOR BLEU METEOR
baseline 17.56 40.52 33.00 56.50
4.2 Effects of Model Adaptation
In order to investigate the effect of model adapta-
tion, each model component was optimized sep-
arately using the method described in Section 2.
Table 4 summarizes the automatic evaluation re-
sults for various model combinations. The combi-
nation of NC and EP models using equal weights
achieves only a slight improvement for the NC
task (BLEU: +0.4%, METEOR: +0.4%), but a
large improvement for the EP task (BLEU: +1.0%,
METEOR: +1.7%). Weight optimization further
improves all translation tasks where the highest
evaluation scores are achieved when the optimized
weights for all statistical models are used. In total,
model adaptation gains 1.1% and 1.3% in BLEU
and 0.8% and 1.8% in METEOR for the NC and
EP translation tasks, respectively.
Table 4: Effects of Model Adaptation
weight NC Eval EP Eval
optimization BLEU METEOR BLEU METEOR
? 17.92 40.72 34.00 58.20
tm 18.13 40.95 34.05 58.23
tm+lm 18.25 41.23 34.12 58.22
tm+dm 18.36 41.06 34.24 58.34
tm+lm+dm 18.65 41.35 34.35 58.36
4.3 Effects of Transliteration
In order to investigate the effects of translitera-
tion, we trained three different transliteration us-
ing the phrase-table of the baseline systems trained
on (a) only the NC corpus, (b) only the EP cor-
pus, and (c) on the merged corpus (NC+EP). The
performance of these phrase-based transliteration
models is evaluated for 2000 randomly selected
transliteration examples. Table 5 summarizes the
haracter-based automatic evaluation scores for the
word error rate (WER) metrics, i.e., the edit dis-
tance between the system output and the closest
reference translation (Niessen et al, 2000), as well
as the BLEU and METEOR metrics. The best
performance is achieved when training examples
from both domains are exploit to transliterate un-
known Spanish words into English. Therefore, the
NC+EP transliteration model was applied to the
translation outputs of all mixture models described
in Section 4.2.
The effects of the transliteration post-process
are summarized in Table 6. Transliteration consis-
Table 5: Transliteration Performance
Training character-based
Data WER BLEU METEOR
NC 13.10 83.62 86.74
EP 11.76 85.93 87.89
NC+EP 11.72 86.08 87.89
tently improves the translation quality of all mix-
ture models, although the gains obtained for the
NC task (BLEU: +1.3%, METEOR: +1.3%) are
much larger than those for the EP task (BLEU:
+0.1%, METEOR: +0.2%) which is due to the
larger amount of untranslatable words in the NC
evaluation data set.
Table 6: Effects of Transliteration
weight NC Eval EP Eval
optimization BLEU METEOR BLEU METEOR
tm 19.14 42.39 34.11 58.46
tm+lm 19.46 42.65 34.16 58.44
tm+dm 19.77 42.35 34.38 58.57
tm+lm+dm 19.95 42.64 34.48 58.60
4.4 WMT09 Testset Results
Based on the automatic evaluation results pre-
sented in the previous sections, we selected the
SMT engine based on the tm+lm+dm weights op-
timized on the NC devset as the primary run for
our testset run submission. All other model weight
combinations were submitted as contrastive runs.
The BLEU scores of these runs are listed in Ta-
ble 7 and confirm the results obtained for the
above experiments, i.e., the best performing sys-
tem is the one based on the mixture models us-
ing separately optimized weights in combination
with the transliteration of untranslatable Span-
ish words using the phrase-based transliteration
model trained on all available language resources.
Table 7: Testset 2009 Performance
weight NC Eval EP Eval
optimization BLEU BLEU
tm 21.07 20.81
tm+lm 20.95 20.59
tm+dm 21.45 21.32
tm+lm+dm 21.67? 21.27
5 Conclusion
The work for this year?s shared task focused on
the task of effectively utilizing out-of-domain lan-
guage resources and handling OOV words to im-
prove translation quality. Overall our experi-
ments show that the incorporation of mixture mod-
els and phrase-based transliteration techniques
largely out-performed standard phrase-based SMT
engines gaining a total of 2.4% in BLEU and 2.1%
in METEOR for the news domain.
108
References
S. Banerjee and A. Lavie. 2005. METEOR: An Auto-
matic Metric for MT Evaluation with Improved Cor-
relation with Human Judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT, pages 65?72, Ann Arbor,
Michigan.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2008. Further Meta-Evaluation of Ma-
chine Translation. In Proceedings of the 3rd Work-
shop on SMT, pages 70?106, Columbus, Ohio.
H. Cormen, C. Leiserson, and L. Rivest. 1989. Intro-
duction to Algorithms. MIT Press.
A. Finch and E. Sumita. 2008a. Dynamic Model Inter-
polation for Statistical Machine Translation. In Pro-
ceedings of the 3rd Workshop on SMT, pages 208?
215, Columbus, Ohio.
A. Finch and E. Sumita. 2008b. Phrase-based Ma-
chine Transliteration. In Proceedings of the IJC-
NLP, pages 13?18, Hyderabad, India.
G. Foster and R. Kuhn. 2007. Mixture-Model Adapta-
tion for SMT. In Proceedings of the 2nd Workshop
on SMT, pages 128?135, Prague, Czech Republic.
D. Gupta and M. Federico. 2006. Exploiting Word
Transformation in SMT from Spanish to English. In
Proceedings of the EAMT, pages 75?80, Oslo, Nor-
way.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer, New
York.
K. Knight and J. Graehl. 1997. Machine Translitera-
tion. In Proceedings of the 35th ACL, pages 128?
135, Madrid, Spain.
P. Koehn and H. Hoang. 2007. Factored Transla-
tion Models. In Proceedings of the EMNLP-CoNLL,
pages 868?876, Prague, Czech Republic.
P. Koehn, F.J. Och, and D. Marcu. 2007. Statisti-
cal Phrase-Based Translation. In Proceedings of the
HLT-NAACL, pages 127?133, Edmonton, Canada.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Proceedings of the
MT Summit X, pages 79?86, Phuket, Thailand.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000.
An Evaluation Tool for Machine Translation: Fast
Evaluation for MT Research. In Proc. of the 2nd
LREC, pages 39?45, Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings of the
41st ACL, pages 160?167, Sapporo, Japan.
H. Okuma, H. Yamamoto, and E. Sumita. 2007. In-
troducing Translation Dictionary into phrase-based
SMT. In Proceedings of MT Summit XI, pages 361?
368, Copenhagen, Denmark.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th ACL,
pages 311?318, Philadelphia, USA.
M. Popovic and H. Ney. 2005. Exploiting Phrasal Lex-
ica and Additional Morpho-synatctic Language Re-
sources for SMT with Scarce Training Data. In Pro-
ceedings of the EAMT, pages 212?218, Budapest,
Hungary.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901?904, Denver.
R.W. Wagner. 1974. The string-to-string correction
problem. Journal of the ACM, 21(1):169?173.
109
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 52?56,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Abstract 
The system presented in this paper uses 
phrase-based statistical machine translation 
(SMT) techniques to directly transliterate be-
tween all language pairs in this shared task. 
The technique makes no language specific as-
sumptions, uses no dictionaries or explicit 
phonetic information. The translation process 
transforms sequences of tokens in the source 
language directly into to sequences of tokens 
in the target. All language pairs were transli-
terated by applying this technique in a single 
unified manner. The machine translation sys-
tem used was a system comprised of two 
phrase-based SMT decoders. The first gener-
ated from the first token of the target to the 
last. The second system generated the target 
from last to first. Our results show that if only 
one of these decoding strategies is to be cho-
sen, the optimal choice depends on the lan-
guages involved, and that in general a combi-
nation of the two approaches is able to outper-
form either approach.  
1 Introduction 
It is possible to couch the task of machine trans-
literation as a task of machine translation. Both 
processes involve the transformation of se-
quences of tokens in one language into se-
quences of tokens in another language. The 
principle differences between the machine trans-
lation and language translation are: 
? Transliteration does not normally re-
quire the re-ordering of tokens that are 
generated  in the target 
? The number of types (the vocabulary 
size) in both source and target languages 
is considerably less for the translitera-
tion task 
We take a statistical machine translation pa-
radigm (Brown at al., 1991) as the basis for our 
systems. The work in this paper is related to the 
work of (Finch and Sumita, 2008) who also use 
SMT directly to transliterate.  
We view the task of machine transliteration 
as a process of machine translation at the cha-
racter level (Donoual and LePage, 2006). We 
use state of the art phrase-based statistical ma-
chine translation systems (Koehn et al, 2003) to 
perform the transliteration. By adopting this ap-
proach we were able to build systems for all of 
the language pairs in the shared task using pre-
cisely the same procedures. No modeling of the 
phonetics of either source or target language 
(Knight and Graehl, 1997) was necessary, since 
the approach is simply a direct transformation of 
sequences of tokens in the source language into 
sequences of tokens in the target. 
2 Overview  
Our approach differs from the approach of 
(Finch and Sumita, 2008) in that we decode bi-
directional. In a typical statistical machine trans-
lation system the sequence of target tokens is 
generated in a left-to-right manner, by left-to-
right here we mean the target sequence is gener-
ated from the first token to its last. During the 
generation process the models (in particular the 
target language model) are able to refer to only 
the target tokens that have already been generat-
ed. In our approach, by using decoders that de-
code in both directions we are able to exploit 
context to the left and to the right of target to-
kens being generated. Furthermore, we expect 
our system to gain because it is a combination of 
two different MT systems that are performing 
the same task. 
3 Experimental Conditions 
In our experiments we used an in-house phrase-
based statistical machine translation decoder 
called CleopATRa. This decoder operates on 
exactly the same principles as the publicly 
available MOSES decoder (Koehn et al, 2003). 
Like MOSES we utilize a future cost in our cal-
culations. Our decoder was modified to be able 
to run two instances of the decoder at the same 
Transliteration by Bidirectional Statistical Machine Translation 
Andrew Finch 
NICT 
2-2-2 Hikaridai 
Keihanna Science City 
619-0288 JAPAN 
andrew.finch@nict.go.jp 
Eiichiro Sumita 
NICT 
2-2-2 Hikaridai 
Keihanna Science City 
619-0288 JAPAN 
eiichiro.sumita@nict.go.jp 
52
time. One instance decoding from left-to-right 
the other decoding from right-to-left. The hypo-
theses being combined by linearly interpolating 
the scores from both decoders at the end of the 
decoding process. In addition, the decoders were 
constrained decode in a monotone manner. That 
is, they were not allowed to re-order the phrases 
during decoding. The decoders were also confi-
gured to produce a list of unique sequences of 
tokens in their n-best lists. During SMT decod-
ing it is possible to derive the same sequence of 
tokens in multiple ways. Multiply occurring se-
quences of this form were combined into a sin-
gle hypothesis in the n-best list by summing 
their scores. 
3.1 Pre-processing 
In order to reduce data sparseness issues we 
took the decision to work with data in only its 
lowercase form. The only target language with 
case information was Russian. During the para-
meter tuning phase (where output translations 
are compared against a set of references) we 
restored the case for Russian by simply capita-
lizing the first character of each word.  
We chose not to perform any tokenization for 
any of the language pairs in the shared task. We 
chose this approach for several reasons: 
? It allowed us to have a single unified 
approach for all language pairs 
? It was in the spirit of the evaluation, as 
it did not require specialist knowledge 
outside of the supplied corpora 
? It enabled us to handle the Chinese 
 names that occurred in the Japanese 
 Romaji-Japanese Kanji task 
However we believe that a more effective 
approach for Japanese-Kanji task may have been 
to re-tokenize the alphabetic characters into ka-
na (for example transforming ?k a? into the kana 
consonant vowel pair ?ka?) since these are the 
basic building blocks of the Japanese language. 
3.2 Training 
For the final submission, all systems were 
trained on the union of the training data and de-
velopment data. It was felt that the training set 
was sufficiently small that the inclusion of the 
development data into the training set would 
yield a reasonable boost in performance by in-
creasing the coverage of the language model and 
phrase table. The language models and transla-
tion models were therefore built from all the 
data, and the log-linear weights used to combine 
the models of the systems were tuned using sys-
tems trained only on the training data. The de-
velopment data in this case being held-out. It 
was assumed that these parameters would per-
form well in the systems trained on the com-
bined development/training corpora. 
3.3 Parameter Tuning 
The SMT systems were tuned using the mini-
mum error rate training procedure introduced in 
(Och, 2003). For convenience, we used BLEU 
as a proxy for the various metrics used in the 
shared task evaluation. The BLEU score is 
Figure 1: The decoding process for multi-word sequences 
Word 1 Word 2 Word m 
Segment into individual words and decode each word independently
D
ecode 
D
ecode 
D
ecode 
n-best 
hypothesis 1 
hypothesis 2 
... 
hypothesis n 
 
n-best 
hypothesis 1 
hypothesis 2 
... 
hypothesis n 
 
n-best 
hypothesis 1 
hypothesis 2 
... 
hypothesis n 
 
 
Search for the best path 
53
commonly used to evaluate the performance of 
machine translation systems and is a function of 
the geometric mean of n-gram precision. Table 1 
shows the effect of tuning for BLEU on the 
ACC (1-best accuracy) scores for several lan-
guages. Improvements in the BLEU score also 
gave improvements in ACC. Tuning to maxim-
ize the BLEU score gave improvements for all 
language pairs and in all of the evaluation me-
trics used in this shared task. Nonetheless, it is 
reasonable to assume that one would be able to 
improve the performance in a particular evalua-
tion metric by doing minimum error rate train-
ing specifically for that metric. 
3.3.1 Multi-word sequences 
The data for some languages (for example Hin-
di) contained some multi-word sequences. These 
posed a challenge for our approach, and gave us 
the following alternatives: 
? Introduce a <space> token into the se-
quence, and treat it as one long charac-
ter sequence to transliterate; or 
? Segment the word sequences into indi-
vidual words and transliterate these in-
dependently, combining the n-best hy-
pothesis lists for all the individual words 
in the sequence into a single output se-
quence. 
 We adopted both approaches for the training 
of our systems. For those multi-word sequences 
where the number of words in the source and 
target matched, the latter approach was taken. 
For those where the numbers of source and tar-
get words differed, the former approach was 
taken. The decoding process for multi-word se-
quences is shown in Figure 1. This approach 
was only used during the parameter tuning on 
the development set, and in experiments to eva-
luate the system performance on development 
data since no multi-word sequences occurred in 
the test data. 
During recombination, the score for the target 
word sequence was calculated as the product of 
the scores of each hypothesis for each word. 
Therefore a search over all combinations of hy-
potheses was required. In almost all cases we 
were able to perform a full search. For the rare 
long word sequences in the data, a beam search 
strategy was adopted. 
3.3.2 Bidirectional Decoding 
In SMT it is usual to decode generating the tar-
get sequence in order from the first token to the 
last token (we refer to this as left-to-right decod-
ing, as this is the usual term for this, even 
though it may be confusing as some languages 
are naturally written from right-to-left). Since 
the decoding process is symmetrical, it is also 
possible to reverse the decoding process, gene-
rating from the end of the target sequence to the 
start (we will refer to this as right-to-left decod-
ing). This reverse decoding is counter-intuitive 
since language is generated in a left-to-right 
manner by humans (by definition), however, in 
pilot experiments on language translation, we 
found that the best decoding strategy varies de-
pending on the languages involved. The analo-
gue of this observation was observed in our 
transliteration results (Table 1). For some lan-
guage pairs, a left-to-right decoding strategy 
performed better, and for other language pairs 
the right-to-left strategy was preferable. 
Our pilot experiments also showed that com-
bining the hypotheses from both decoding 
processes almost always gave better results that 
the best of either left-to-right or right-to-left de-
coding. We observe a similar effect in the expe-
riments presented here, although our results here 
are less consistent. This is possibly due to the 
differences in the size of the data sets used for 
the experiments. The data used in the experi-
ments here being an order of magnitude smaller. 
4 Results 
The results of our experiments are shown in Ta-
ble 1. These results are from a closed evaluation 
on development data. Only the training data 
were used to build the system?s models, the de-
velopment data being used to tune the log-linear 
weights for the translation engines? models and 
for evaluation. We show results for the case of 
equal interpolation weights of the left-to-right 
and right-to-left decoders. For the final submis-
 En-Ch En-Ja En-Ko En-Ru Jn-Jk 
After tuning 0.908 0.772 0.622 0.914 0.769 
Before tuning 0.871 0.635 0.543 0.832 0.737 
Table 1: The effect on 1-best accuracy by tuning with respect to BLEU score 
54
sion these weights were tuned on the develop-
ment data. The bidirectional performance was 
the best strategy for all but En-Ja and En-Ka in 
terms of ACC. This varies for other metrics but 
in general the bidirectional system most often 
gave the highest performance. 
5 Conclusion 
Our results show the performance of state of the 
art phrase-based machine translation techniques 
on the task of transliteration. We show that it is 
reasonable to use the BLEU score to tune the 
system, and that bidirectional decoding can im-
prove performance. In future work we would 
like to consider more tightly coupling the de-
coders, introducing monotonicity into the 
alignment process, and adding contextual fea-
tures into the translation models. 
Acknowledgements 
The results presented in this paper draw on the 
following data sets. For Chinese-English, Li et 
al., 2004. For Japanese-English, Korean-
English, and Japanese(romaji)-Japanese(kanji), 
the reader is referred to the CJK website: 
http://www.cjk.org. For Hindi-English, Tamil-
English, Kannada-English and Russian-English 
the data sets originated from the work of Kura-
man and Kellner, 2007. 
References 
Peter Brown, S. Della Pietra, V. Della Pietra, and R. 
Mercer (1991). The mathematics of statistical ma-
chine translation: parameter estimation. Computa-
tional Linguistics, 19(2), 263-311.  
Etienne Denoual and  Yves Lepage. 2006. The cha-
racter as an appropriate unit of processing for non-
Language DecodingStrategy ACC 
Mean
F-score MRR MAP_ref MAP_10 MAP_sys 
En-Ch 
? 0.908 0.972 0.908 0.266 0.266 0.908 
? 0.914 0.974 0.914 0.268 0.268 0.914 
? 0.915 0.974 0.915 0.268 0.268 0.915 
En-Hi 
? 0.788 0.969 0.788 0.231 0.231 0.788 
? 0.785 0.968 0.785 0.230 0.230 0.785 
? 0.790 0.970 0.790 0.231 0.231 0.790 
En-Ja 
? 0.773 0.950 0.793 0.251 0.251 0.776 
? 0.767 0.948 0.785 0.249 0.249 0.768 
? 0.769 0.949 0.789 0.250 0.250 0.771 
En-Ka 
? 0.682 0.954 0.684 0.202 0.202 0.683 
? 0.660 0.953 0.661 0.195 0.195 0.660 
? 0.674 0.955 0.675 0.199 0.199 0.674 
En-Ko 
? 0.622 0.850 0.623 0.183 0.183 0.622 
? 0.620 0.851 0.621 0.182 0.182 0.619 
? 0.627 0.853 0.628 0.184 0.184 0.626 
En-Ru 
? 0.915 0.982 0.915 0.268 0.268 0.915 
? 0.921 0.983 0.921 0.270 0.270 0.921 
? 0.922 0.983 0.922 0.270 0.270 0.922 
En-Ta 
? 0.731 0.963 0.732 0.216 0.216 0.731 
? 0.734 0.962 0.735 0.217 0.217 0.735 
? 0.748 0.965 0.749 0.221 0.221 0.749 
Jn-Jk 
? 0.769 0.869 0.797 0.301 0.301 0.766 
? 0.766 0.862 0.792 0.299 0.299 0.761 
? 0.772 0.867 0.799 0.300 0.300 0.767 
Table 2: Results showing the peformance of three decoding strategies with respect to the evaluation 
metrics used for the shared task. Here ??denotes left-to-right decoding, ? denotes right-to-left de-
coding and ? denotes bidirectional decoding. 
Key to Language Acronyms: En = English, Ch = Chinese, Hi = Hindi, Ja = Japanese Katakana, Ka = 
Kannada, Ko = Korean, Ru = Russian, Ta = Tamil, Jn = Japanese Romaji, Jk = Japanese Kanji. 
55
segmenting languages, Proceedings of the 12th 
Annual Meeting of The Association of NLP, pp. 
731-734.  
Kevin Knight and Jonathan Graehl. 1997. Machine 
Transliteration. Proceedings of the Thirty-Fifth 
Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the 
European Chapter of the Association for Compu-
tational Linguistics, pp. 128-135, Somerset, New 
Jersey.  
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
Proceedings of the Human Language Technology 
Conference 2003 (HLT-NAACL 2003), Edmonton, 
Canada. 
Franz Josef Och, ?Minimum error rate training for 
statistical machine translation,? Proceedings of the 
ACL, 2003. 
Kumaran A., Kellner T., "A generic framework for 
machine transliteration", Proc. of the 30th SIGIR, 
2007 
Haizhou Li, Min Zhang, Jian Su, English-Chinese 
(EnCh): "A joint source channel model for ma-
chine transliteration", Proc. of the 42nd ACL, 
2004. 
 
56
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215?222,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Lexical Dependency and Ontological Knowledge to Improve a
Detailed Syntactic and Semantic Tagger of English
Andrew Finch
NiCT?-ATR?
Kyoto, Japan
andrew.finch
@atr.jp
Ezra Black
Epimenides Corp.
New York, USA
ezra.black
@epimenides.com
Young-Sook Hwang
ETRI
Seoul, Korea
yshwang7
@etri.re.kr
Eiichiro Sumita
NiCT-ATR
Kyoto, Japan
eiichiro.sumita
@atr.jp
Abstract
This paper presents a detailed study of
the integration of knowledge from both
dependency parses and hierarchical word
ontologies into a maximum-entropy-based
tagging model that simultaneously labels
words with both syntax and semantics.
Our findings show that information from
both these sources can lead to strong im-
provements in overall system accuracy:
dependency knowledge improved perfor-
mance over all classes of word, and knowl-
edge of the position of a word in an on-
tological hierarchy increased accuracy for
words not seen in the training data. The
resulting tagger offers the highest reported
tagging accuracy on this tagset to date.
1 Introduction
Part-of-speech (POS) tagging has been one of the
fundamental areas of research in natural language
processing for many years. Most of the prior re-
search has focussed on the task of labeling text
with tags that reflect the words? syntactic role in
the sentence. In parallel to this, the task of word
sense disambiguation (WSD), the process of de-
ciding in which semantic sense the word is being
used, has been actively researched. This paper ad-
dresses a combination of these two fields, that is:
labeling running words with tags that comprise, in
addition to their syntactic function, a broad seman-
tic class that signifies the semantics of the word in
the context of the sentence, but does not neces-
sarily provide information that is sufficiently fine-
grained as to disambiguate its sense. This differs
?National Institute of Information and Communications
Technology
?ATR Spoken Language Communication Research Labs
from what is commonly meant by WSD in that al-
though each word may have many ?senses? (by
senses here, we mean the set of semantic labels
the word may take), these senses are not specific
to the word itself but are drawn from a vocabulary
applicable to the subset of all types in the corpus
that may have the same semantics.
In order to perform this task, we draw on re-
search from several related fields, and exploit pub-
licly available linguistic resources, namely the
WordNet database (Fellbaum, 1998). Our aim is
to simultaneously disambiguate the semantics of
the words being tagged while tagging their POS
syntax. We treat the task as fundamentally a POS
tagging task, with a larger, more ambiguous tag
set. However, as we will show later, the ?n-gram?
feature set traditionally employed to perform POS
tagging, while basically competent, is not up to
this challenge, and needs to be augmented by fea-
tures specifically targeted at semantic disambigua-
tion.
2 Related Work
Our work is a synthesis of POS tagging and WSD,
and as such, research from both these fields is di-
rectly relevant here.
The basic engine used to perform the tagging
in these experiments is a direct descendent of the
maximum entropy (ME) tagger of (Ratnaparkhi,
1996) which in turn is related to the taggers of
(Kupiec, 1992) and (Merialdo, 1994). The ME
approach is well-suited to this kind of labeling be-
cause it allows the use of a wide variety of features
without the necessity to explicitly model the inter-
actions between them.
The literature on WSD is extensive. For a good
overview we direct the reader to (Nancy and Jean,
1998). Typically, the local context around the
215
word to be sense-tagged is used to disambiguate
the sense (Yarowsky, 1993), and it is common for
linguistic resources such as WordNet (Li et al,
1995; Mihalcea and Moldovan, 1998; Ramakrish-
nan and Prithviraj, 2004), or bilingual data (Li and
Li, 2002) to be employed as well as more long-
range context. An ME-system for WSD that op-
erates on similar principles to our system (Suarez,
2002) was based on an array of local features that
included the words/POS tags/lemmas occurring in
a window of +/-3 words of the word being dis-
ambiguated. (Lamjiri et al, 2004) also developed
an ME-based system that used a very simple set
of features: the article before; the POS before
and after; the preposition before and after, and the
syntactic category before and after the word be-
ing labeled. The features used in both of these
approaches resemble those present in the feature
set of a standard n-gram tagger, such as the one
used as the baseline for the experiments in this pa-
per. The semantic tags we use can be seen as a
form of semantic categorization acting in a similar
manner to the semantic class of a word in the sys-
tem of (Lamjiri et al, 2004). The major difference
is that with a left-to-right beam-search tagger, la-
beled context to the right of the word being labeled
is not available for use in the feature set.
Although POS tag information has been utilized
in WSD techniques (e.g. (Suarez, 2002)), there
has been relatively little work addressing the prob-
lem of assigning a part-of-speech tag to a word
together with its semantics, despite the fact that
the tasks involve a similar process of label disam-
biguation for a word in running text.
3 Experimental Data
The primary corpus used for the experiments pre-
sented in this paper is the ATR General English
Treebank. This consists of 518,080 words (ap-
proximately 20 words per sentence, on average) of
text annotated with a detailed semantic and syntac-
tic tagset.
To understand the nature of the task involved
in the experiments presented in this paper, one
needs some familiarity with the ATR General
English Tagset. For detailed presentations,
see (Black et al, 1996b; Black et al, 1996a;
Black and Finch, 2001). An apercu can be
gained, however, from Figure 1, which shows
two sample sentences from the ATR Treebank
(and originally from a Chinese take?out food
flier), tagged with respect to the ATR General
English Tagset. Each verb, noun, adjective and
adverb in the ATR tagset includes a semantic
label, chosen from 42 noun/adjective/adverb
categories and 29 verb/verbal categories, some
overlap existing between these category sets.
Proper nouns, plus certain adjectives and
certain numerical expressions, are further cat-
egorized via an additional 35 ?proper?noun?
categories. These semantic categories are in-
tended for any ?Standard?American?English?
text, in any domain. Sample categories include:
?physical.attribute? (nouns/adjectives/adverbs),
?alter? (verbs/verbals), ?interpersonal.act?
(nouns/adjectives/adverbs/verbs/verbals),
?orgname? (proper nouns), and ?zipcode?
(numericals). They were developed by the ATR
grammarian and then proven and refined via
day?in?day?out tagging for six months at ATR by
two human ?treebankers?, then via four months of
tagset?testing?only work at Lancaster University
(UK) by five treebankers, with daily interactions
among treebankers, and between the treebankers
and the ATR grammarian. The semantic catego-
rization is, of course, in addition to an extensive
syntactic classification, involving some 165 basic
syntactic tags.
The test corpus has been designed specifically
to cope with the ambiguity of the tagset. It is pos-
sible to correctly assign any one of a number of
?allowable? tags to a word in context. For exam-
ple, the tag of the word battle in the phrase ?a
legal battle? could be either NN1PROBLEM or
NN1INTER-ACT, indicating that the semantics is
either a problem, or an inter-personal action. The
test corpus consists of 53,367 words sampled from
the same domains as, and in approximately the
same proportions as the training data, and labeled
with a set of up to 6 allowable tags for each word.
During testing, only if the predicted tag fails to
match any of the allowed tags is it considered an
error.
4 Tagging Model
4.1 ME Model
Our tagging framework is based on a maximum
entropy model of the following form:
p(t, c) = ?
K?
k=0
?fk(c,t)k p0 (1)
where:
216
(_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DD1 coupon_NN1DOCUMENT
when_CSWHEN ordering_VVGINTER-ACT
OR_CCOR ONE_MC1WORD FREE_JJMONEY FANTAIL_NN1ANIMAL SHRIMPS_NN1FOOD
Figure 1: Two ATR Treebank Sentences from a Take?Out Food Flier
- t is tag being predicted;
- c is the context of t;
- ? is a normalization coefficient that ensures:
?Lt=0?
?K
k=0 ?
fk(c,t)
k p0 = 1;
- K is the number of features in the model;
- L is the number of tags in our tag set;
- ?k is the weight of feature fk;
- fk are feature functions and fk{0, 1};
- p0 is the default tagging model (in our case,
the uniform distribution, since all of the in-
formation in the model is specified using ME
constraints).
Our baseline model contains the following fea-
ture predecate set:
w0 t?1 pos0 pref1(w0)
w?1 t?2 pos?1 pref2(w0)
w?2 pos?2 pref3(w0)
w+1 pos+1 suff1(w0)
w+2 pos+2 suff2(w0)
suff3(w0)
where:
- wn is the word at offset n relative to the word
whose tag is being predicted;
- tn is the tag at offset n;
- posn is the syntax-only tag at offset n as-
signed by a syntax-only tagger;
- prefn(w0) is the first n characters of w0;
- suffn(w0) is the last n characters of w0;
This feature set contains a typical selection of
n-gram and basic morphological features. When
the tagger is trained in tested on the UPENN tree-
bank (Marcus et al, 1994), its accuracy (excluding
the posn features) is over 96%, close to the state of
the art on this task. (Black et al, 1996b) adopted
a two-stage approach to prediction, first predicting
syntax, then semantics given the syntax, whereas
in (Black et al, 1998) both syntax and semantics
were predicted together in one step. In using syn-
tactic tags as features, we take a softer approach
to the two-stage process. The tagger has access
to accurate syntactic information; however, it is
not necessarily constrained to accept this choice
of syntax. Rather, it is able to decide both syn-
tax and semantics while taking semantic context
into account. In order to find the most probable
sequence of tags, we tag in a left-to-right manner
using a beam-search algorithm.
4.2 Feature selection
For reasons of practicability, it is not always pos-
sible to use the full set of features in a model: of-
ten it is necessary to control the number of fea-
tures to reduce resource requirements during train-
ing. We use mutual information (MI) to select
the most useful feature predicates (for more de-
tails, see (Rosenfeld, 1996)). It can be viewed as
a means of determining how much information a
given predicate provides when used to predict an
outcome.
That is, we use the following formula to gauge
a feature?s usefulness to the model:
I(f ;T ) =
?
f?{0,1}
?
t?T
p(f, t)log
p(f, t)
p(f)p(t)
(2)
where:
- t ? T is a tag in the tagset;
- f ? {0, 1} is the value of any kind of predi-
cate feature.
Using mutual information is not without its
shortcomings. It does not take into account any
of the interactions between features. It is possi-
ble for a feature to be pronounced useful by this
procedure, whereas in fact it is merely giving the
same information as another feature but in differ-
ent form. Nonetheless this technique is invaluable
in practice. It is possible to eliminate features
217
which provide little or no benefit to the model,
thus speeding up the training. In some cases it
even allows a model to be trained where it would
not otherwise be possible to train one. For the pur-
poses of our experiments, we use the top 50,000
predicates for each model to form the feature set.
5 External Knowledge Sources
5.1 Lexical Dependencies
Features derived from n-grams of words and tags
in the immediate vicinity of the word being tagged
have underpinned the world of POS tagging for
many years (Kupiec, 1992; Merialdo, 1994; Rat-
naparkhi, 1996), and have proven to be useful fea-
tures in WSD (Yarowsky, 1993). Lower-order
n-grams which are closer to word being tagged
offer the greatest predictive power (Black et al,
1998). However, in the field of WSD, relational
information extracted from grammatical analysis
of the sentence has been employed to good effect,
and in particular, subject-object relationships be-
tween verbs and nouns have been shown be effec-
tive in disambiguating semantics (Nancy and Jean,
1998). We take the broader view that dependency
relationships in general between any classes of
words may help, and use the ME training process
to weed out the irrelevant relationships. The prin-
ciple is exactly the same as when using a word in
the local context as a feature, except that the word
in this case has a grammatical relationship with the
word being tagged, and can be outside the local
neighborhood of the word being tagged. For both
types of dependency, we encoded the model con-
straints fstl(d) as boolean functions of the form:
fstl(d) =
{
1 if d.s = s ? d.t = t ? d.l = l
0 otherwise
(3)
where:
- d is a lexical dependency, consisting of a
source word (the word being tagged) d.s, a
target word d.t and a label d.l
- s and t (words), and l (link label) are specific
to the feature
We generated two distinct features for each de-
pendency. The source and target were exchanged
to create these features. This was to allow the
models to capture the bidirectional nature of the
dependencies. For example, when tagging a verb,
the model should be aware of the dependent ob-
ject, and conversely when tagging that object, the
model should have a feature imposing a constraint
arising from the identity of the dependent verb.
5.1.1 Dependencies from the CMU Link
Grammar
We parsed our corpus using the parser detailed
in (Grinberg et al, 1995). The dependencies out-
put by this parser are labeled with the type of de-
pendency (connector) involved. For example, sub-
jects (connector type S) and direct objects of verbs
(O) are explicitly marked by the process (a full list
of connectors is provided in the paper). We used
all of the dependencies output by the parser as fea-
tures in the models.
5.1.2 Dependencies from Phrasal Structure
It is possible to extract lexical dependencies
from a phrase-structure parse. The procedure is
explained in detail in (Collins, 1996). In essence,
each non-terminal node in the parse tree is as-
signed a head word, which is the head of one of
its children denoted the ?head child?. Dependen-
cies are established between this headword and
the heads of each of the children (except for the
head child). In these experiments we used the
MXPOST tagger (Ratnaparkhi, 1996) combined
with Collins? parser (Collins, 1996) to assign parse
trees to the corpus. The parser had a 98.9% cover-
age of the sentences in our corpora. Again, all of
the dependencies output by the parser were used
as features in the models.
5.2 Hierarchical Word Ontologies
In this section we consider the effect of features
derived from hierarchical sets of words. The pri-
mary advantage is that we are able to construct
these hierarchies using knowledge from outside
the training corpus of the tagger itself, and thereby
glean knowledge about rare words. In these exper-
iments we use the human annotated word taxon-
omy of hypernyms (IS-A relations) in the Word-
Net database, and an automatically acquired on-
tology made by clustering words in a large corpus
of unannotated text.
We have chosen to use hierarchical schemes for
both the automatic and manually acquired ontolo-
gies because this offers the opportunity to com-
bat data-sparseness issues by allowing features de-
rived from all levels of the hierarchy to be used.
The process of training the model is able to de-
218
Top-level category
appleedible fruit apple treefruit
reproductivestructure fruit treeplant organ
plant partnatural objectobject angiospermoustreetreewoody plant
vascular plantplant
peargrape crab applewild appleHierarchy 
for sense 1 Hierarchy for sense 2
Figure 2: The WordNet taxonomy for both (WordNet) senses of the word apple
cide the levels of granularity that are most useful
for disambiguation. For the purposes of generat-
ing features for the ME tagger we treat both types
of hierarchy in the same fashion. One of these fea-
tures is illustrated in Figure 5.3. Each predicate
is effectively a question which asks whether the
word (or word being used in a particular sense in
the case of the WordNet hierarchy) is a descendent
of the node to which the predicate applies. These
predicates become more and more general as one
moves up the hierarchy. For example in the hierar-
chy shown in Figure 5.2, looking at the nodes on
the right hand branch, the lowest node represents
the class of apple trees whereas the top node rep-
resents the class of all plants.
We expect these hierarchies to be particularly
useful when tagging out of vocabulary words
(OOV?s). The identity of the word being tagged
is by far the most important feature in our baseline
model. When tagging an OOV this information is
not available to the tagger. The automatic cluster-
ing has been trained on 100 times as much data
as our tagger, and therefore will have information
about words that tagger has not seen during train-
ing. To illustrate this point, suppose that we are
tagging the OOV pomegranate. This word is in the
WordNet database, and is in the same synset as the
?fruit? sense of the word apple. It is reasonable to
assume that the model will have learned (from the
many examples of all fruit words) that the predi-
cate representing membership of this fruit synset
should, if true, favor the selection of the correct tag
for fruit words: NN1FOOD. The predicate will be
true for the word pomegranate which will thereby
benefit from the model?s knowledge of how to tag
the other words in its class. Even if this is not so
at this level in the hierarchy, it is likely to be so at
some level of granularity. Precisely which levels
of detail are useful will be learned by the model
during training.
5.2.1 Automatic Clustering of Text
We used the automatic agglomerative mutual-
information-based clustering method of (Ushioda,
1996) to form hierarchical clusters from approx-
imately 50 million words of tokenized, unanno-
tated text drawn from similar domains as the tree-
bank used to train the tagger. Figure 5.2 shows
the position of the word apple within the hierar-
chy of clusters. This example highlights both the
strengths and weaknesses of this approach. One
strength is that the process of clustering proceeds
in a purely objective fashion and associations be-
tween words that may not have been considered
by a human annotator are present. Moreover, the
clustering process considers all types that actually
occur in the corpus, and not just those words that
might appear in a dictionary (we will return to this
later). A major problem with this approach is that
219
eggapplecoca PREDICATE:Is the word in thesubtree below thisnode?coffee chicken diamond tin newsstandwellhead calf after-market palm-oilwinter-wheat meat milk timber ?
Figure 3: The dendrogram for the automatically acquired ontology, showing the word apple
the clusters tend to contain a lot of noise. Rare
words can easily find themselves members of clus-
ters to which they do not seem to belong, by virtue
of the fact that there are too few examples of the
word to allow the clustering to work well for these
words. This problem can be mitigated somewhat
by simply increasing the size of the text that is
clustered. However the clustering process is com-
putationally expensive. Another problem is that a
word may only be a member of a single cluster;
thus typically the cluster set assigned to a word
will only be appropriate for that word when used
in its most common sense.
Approximately 93% of running words in the test
corpus, and 95% in the training corpus were cov-
ered by the words in the clusters (when restricted
to verbs, nouns, adjectives and adverbs, these fig-
ures were 94.5% and 95.2% respectively). Ap-
proximately 81% of the words in the vocabulary
from the test corpus were covered, and 71% of the
training corpus vocabulary was covered.
5.2.2 WordNet Taxonomy
For this class of features, we used the hypernym
taxonomy of WordNet (Fellbaum, 1998). Fig-
ure 5.2 shows the WordNet hypernym taxonomy
for the two senses of the word apple that are in
the database. The set of predicates query member-
ship of all levels of the taxonomy for all WordNet
senses of the word being tagged. An example of
one such predicate is shown in the figure.
Only 63% of running words in both the train-
ing and the test corpus were covered by the words
in the clusters. Although this figure appears low,
it can be explained by the fact that WordNet only
contains entries for words that have senses in cer-
tain parts of speech. Some very frequent classes of
words, for example determiners, are not in Word-
Net. The coverage of only nouns, verbs, adjectives
and adverbs in running text is 94.5% for both train-
ing and test sets. Moreover, approximately 84%
of the words in the vocabulary from the test cor-
pus were covered, and 79% on the training cor-
pus. Thus, the effective coverage of WordNet on
the important classes of words is similar to that of
the automatic clustering method.
6 Experimental Results
The results of our experiments are shown in Ta-
ble 1. The task of assigning semantic and syntac-
tic tags is considerably more difficult than simply
assigning syntactic tags due to the inherent ambi-
guity of the tagset. To gauge the level of human
performance on this task, experiments were con-
ducted to determine inter-annotator consistency;
in addition, annotator accuracy was measured on
5,000 words of data. Both the agreement and ac-
curacy were found to be approximately 97%, with
all of the inconsistencies and tagging errors aris-
ing from the semantic component of the tags. 97%
accuracy is therefore an approximate upper bound
for the performance one would expect from an au-
tomatic tagger. As a point of reference for a lower
bound, the overall accuracy of a tagger which uses
only a single feature representing the identity of
the word being tagged is approximately 73%.
The overall baseline accuracy was 82.58% with
only 30.58% of OOV?s being tagged correctly.
Of the two lexical dependency-based approaches,
220
the features derived from Collins? parser were the
most effective, improving accuracy by 0.8% over-
all. To put the magnitude of this gain into perspec-
tive, dropping the features for the identity of the
previous word from the baseline model, only de-
graded performance by 0.2%. The features from
the link grammar parser were handicapped due to
the fact that only 31% of the sentences were able
to be parsed. When the model (Model 3 in Ta-
ble 1) was evaluated on only the parsable portion
on the test set, the accuracy obtained was roughly
comparable to that using the dependencies from
Collins? parses. To control for the differences be-
tween these parseable sentences and the full test
set, Model 4 was tested on the same 31% of sen-
tence that parsed. Its accuracy was within 0.2% of
the accuracy on the whole test set in all cases. Nei-
ther of the lexical dependency-based approaches
had a particularly strong effect on the performance
on OOV?s. This is in line with our intuition, since
these features rely on the identity of the word be-
ing tagged, and the performance gain we see is
due to the improvement in labeling accuracy of the
context around the OOV.
In contrast to this, for the word-ontology-based
feature sets, one would hope to see a marked im-
provement on OOV?s, since these features were
designed specifically to address this issue. We do
see a strong response to these features in the ac-
curacy of the models. The overall accuracy when
using the automatically acquired ontology is only
0.1% higher than the accuracy using dependencies
from Collins? parser. However the accuracy on
OOV?s jumps 3.5% to 35.08% compared to just
0.7% for Model 4. Performance for both cluster-
ing techniques was quite similar, with the Word-
Net taxonomical features being slightly more use-
ful, especially for OOV?s. One possible explana-
tion for this is that overall, the coverage of both
techniques is similar, but for rarer words, the MI
clustering can be inconsistent due to lack of data
(for an example, see Figure 5.2: the word news-
stand is a member of a cluster of words that appear
to be commodities), whereas the WordNet clus-
tering remains consistent even for rare words. It
seems reasonable to expect, however, that the au-
tomatic method would do better if trained on more
data. Furthermore, all uses of words can be cov-
ered by automatic clustering, whereas for exam-
ple, the common use of the word apple as a com-
pany name is beyond the scope of WordNet.
In Model 7 we combined the best lexical depen-
dency feature set (Model 4) with the best cluster-
ing feature set (Model 6) to investigate the amount
of information overlap existing between the fea-
ture sets. Models 4 and 6 improved the base-
line performance by 0.8% and 1.3% respectively.
In combination, accuracy was increased by 2.3%,
0.2% more than the sum of the component mod-
els? gains. This is very encouraging and indicates
that these models provide independent informa-
tion, with virtually all of the benefit from both
models manifesting itself in the combined model.
7 Conclusion
We have described a method for simultaneously
labeling the syntax and semantics of words in run-
ning text. We develop this method starting from
a state-of-the-art maximum entropy POS tagger
which itself outperforms previous attempts to tag
this data (Black et al, 1996b). We augment this
tagging model with two distinct types of knowl-
edge: the identity of dependent words in the sen-
tence, and word class membership information of
the word being tagged. We define the features in
such a manner that the useful lexical dependen-
cies are selected by the model, as is the granu-
larity of the word classes used. Our experimental
results show that large gains in performance are
obtained using each of the techniques. The de-
pendent words boosted overall performance, es-
pecially when tagging verbs. The hierarchical
ontology-based approaches also increased over-
all performance, but with particular emphasis on
OOV?s, the intended target for this feature set.
Moreover, when features from both knowledge
sources were applied in combination, the gains
were cumulative, indicating little overlap.
Visual inspection the output of the tagger on
held-out data suggests there are many remaining
errors arising from special cases that might be bet-
ter handled by models separate from the main tag-
ging model. In particular, numerical expressions
and named entities cause OOV errors that the tech-
niques presented in this paper are unable to handle.
In future work we would like to address these is-
sues, and also evaluate our system when used as a
component of a WSD system, and when integrated
within a machine translation system.
221
# Model Accuracy (? c.i.) OOV?s Nouns Verbs Adj/Adv
1 Baseline 82.58? 0.32 30.58 68.47 74.32 70.99
2 + Dependencies (link grammar) 82.74? 0.32 30.92 68.18 74.96 73.02
3 As above (only parsed sentences) 83.59? 0.53 30.92 69.16 77.21 73.52
4 + Dependencies (Collins? parser) 83.37? 0.31 31.24 69.36 75.78 72.62
5 + Automatically acquired ontology 83.71? 0.31 35.08 71.89 75.83 75.34
6 + WordNet ontology 83.90? 0.31 36.18 72.28 76.29 74.47
7 + Model 4 + Model 6 84.90? 0.31 37.02 72.80 78.36 76.16
Table 1: Tagging accuracy (%), ?+? being shorthand for ?Baseline +?, ?c.i.? denotes the confidence
interval of the mean at a 95% significance level, calculated using bootstrap resampling.
References
E. Black and A. Finch. 2001. Developing and prov-
ing effective broad-coverage semantic-and-syntactic
tagsets for natural language: The atr approach. In
Proceedings of ICCPOL-2001.
E. Black, S. Eubank, H. Kashioka, R. Garside,
G. Leech, and D. Magerman. 1996a. Beyond
skeleton parsing: producing a comprehensive large?
scale general?english treebank with full grammati-
cal analysis. In Proceedings of the 16th Annual Con-
ference on Computational Linguistics, pages 107?
112, Copenhagen.
E. Black, S. Eubank, H. Kashioka, and J. Saia. 1996b.
Reinventing part-of-speech tagging. Journal of Nat-
ural Language Processing (Japan), 5:1.
Ezra Black, Andrew Finch, and Hideki Kashioka.
1998. Trigger-pair predictors in parsing and tag-
ging. In Proceedings, 36th Annual Meeting of
the Association for Computational Linguistics, 17th
Annual Conference on Computational Linguistics,
Montreal, Canada.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Arivind
Joshi and Martha Palmer, editors, Proceedings of
the Thirty-Fourth Annual Meeting of the Association
for Computational Linguistics, pages 184?191, San
Francisco. Morgan Kaufmann Publishers.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for LINK
grammars. Technical Report CMU-CS-TR-95-125,
CMU, Pittsburgh, PA.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden markov model. Computer Speech and Lan-
guage, 6:225?242.
A. K. Lamjiri, O. El Demerdash, and L.Kosseim. 2004.
Simple features for statistical word sense disam-
biguation. In Proc. ACL 2004 ? Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text (Senseval-3), Barcelona,
Spain, July. ACL-2004.
C. Li and H. Li. 2002. Word translation disambigua-
tion using bilingual bootstrapping.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995.
A wordnet-based algorithm for word sense disam-
biguation. In IJCAI, pages 1368?1374.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
B. Merialdo. 1994. Tagging english text with a
probabilistic model. Computational Linguistics,
20(2):155?172.
Rada Mihalcea and Dan I. Moldovan. 1998. Word
sense disambiguation based on semantic density. In
Sanda Harabagiu, editor, Use of WordNet in Natural
Language Processing Systems: Proceedings of the
Conference, pages 16?22. Association for Compu-
tational Linguistics, Somerset, New Jersey.
I. Nancy and V. Jean. 1998. Word sense disambigua-
tion: The state of the art. Computational Linguis-
tics, 24:1:1?40.
G. Ramakrishnan and B. Prithviraj. 2004. Soft word
sense disambiguation. In International Conference
on Global Wordnet (GWC 04), Brno, Czeck Repub-
lic.
A. Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Confer-
ence.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modelling. Computer
Speech and Language, 10:187?228.
A. Suarez. 2002. A maximum entropy-based word
sense disambiguation system. In Proc. International
Conference on Computational Linguistics.
A. Ushioda. 1996. Hierarchical clustering of words.
In In Proceedings of COLING 96, pages 1159?1162.
D. Yarowsky. 1993. One sense per collocation. In
In the Proceedings of ARPA Human Language Tech-
nology Workshop.
222
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1654?1664,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Refining Word Segmentation Using a Manually Aligned Corpus
for Statistical Machine Translation
Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita
National Institute of Information and Communications Technology
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
Abstract
Languages that have no explicit word de-
limiters often have to be segmented for sta-
tistical machine translation (SMT). This is
commonly performed by automated seg-
menters trained on manually annotated
corpora. However, the word segmentation
(WS) schemes of these annotated corpora
are handcrafted for general usage, and
may not be suitable for SMT. An analysis
was performed to test this hypothesis us-
ing a manually annotated word alignment
(WA) corpus for Chinese-English SMT.
An analysis revealed that 74.60% of the
sentences in the WA corpus if segmented
using an automated segmenter trained on
the Penn Chinese Treebank (CTB) will
contain conflicts with the gold WA an-
notations. We formulated an approach
based on word splitting with reference to
the annotated WA to alleviate these con-
flicts. Experimental results show that the
refined WS reduced word alignment error
rate by 6.82% and achieved the highest
BLEU improvement (0.63 on average) on
the Chinese-English open machine trans-
lation (OpenMT) corpora compared to re-
lated work.
1 Introduction
Word segmentation is a prerequisite for many
natural language processing (NLP) applications
on those languages that have no explicit space
between words, such as Arabic, Chinese and
Japanese. As the first processing step, WS affects
all successive steps, thus it has a large potential
impact on the final performance. For SMT, the
unsupervised WA, building translation models and
reordering models, and decoding are all based on
segmented words.
Automated word segmenters built through
supervised-learning methods, after decades of in-
tensive research, have emerged as effective so-
lutions to WS tasks and become widely used in
many NLP applications. For example, the Stan-
ford word segmenter (Xue et al., 2002)1 which is
based on conditional random field (CRF) is em-
ployed to prepare the official corpus for NTCIR-
9 Chinese-English patent translation task (Goto et
al., 2011).
However, one problem with applying these
supervised-learning word segmenters to SMT is
that the WS scheme of annotating the training cor-
pus may not be optimal for SMT. (Chang et al.,
2008) noticed that the words in CTB are often too
long for SMT. For example, a full Chinese per-
sonal name which consists of a family name and a
given name is always taken as a single word, but
its counterpart in English is usually two words.
Manually WA corpora are precious resources
for SMT research, but they used to be only avail-
able in small volumes due to the production cost.
For example, (Och and Ney, 2000) initially an-
notated 447 English-French sentence pairs, which
later became the test data set in ACL 2003 shared
task on word alignment (Mihalcea and Pedersen,
2003), and was used frequently thereafter (Liang
et al., 2006; DeNero and Klein, 2007; Haghighi et
al., 2009)
For Chinese and English, the shortage of man-
ually WA corpora has recently been relieved
by the linguistic data consortium (LDC) 2
GALE Chinese-English word alignment and tag-
ging training corpus (the GALE WA corpus)3.
The corpus is considerably large, containing 4,735
documents, 18,507 sentence pairs, 620,189 Chi-
nese tokens, 518,137 English words, and 421,763
1http://nlp.stanford.edu/software/
segmenter.shtml
2http://catalog.ldc.upenn.edu
3Catalog numbers: LDC2012T16, LDC2012T20,
LDC2012T24 and LDC2013T05.
1654
alignment annotations. The corpus carries no Chi-
nese WS annotation, and the WA annotation was
performed between Chinese characters and En-
glish words. The alignment identifies minimum
translation units and relations 4, referred as atomic
blocks and atomic edges, respectively, in this pa-
per. Figure 1 shows an example that contains six
atomic edges.
Visual inspection of the segmentation of an au-
tomatic segmenter with reference to a WA cor-
pus revealed a number of inconsistencies. For ex-
ample, consider the word ?bao fa? in Figure 1.
Empirically we observed that this word is seg-
mented as a single token by an automatic seg-
menter trained on the CTB, however, this segmen-
tation differs with the alignment in the WA cor-
pus, since its two components are aligned to two
different English words. Our hypothesis was that
the removal of these inconsistencies would benefit
machine translation performance (this is explained
further in Section 2.3), and we explored this idea
in this work.
This paper focuses on optimizing Chinese WS
for Chinese-English SMT, but both the research
method and the proposed solution are language-
independent. They can be applied to other lan-
guage pairs.
The major contributions of this paper include,
? analyze the CTB WS scheme for Chinese-
English SMT;
? propose a lexical word splitter to refine the
WS;
? achieve a BLEU improvement over a baseline
Stanford word segmenter, and a state-of-the-
art extension, on Chinese-English OpenMT
corpora.
The rest of this paper is organized as follows:
first, Section 2 analyzes WS using a WA corpus;
next, Section 3 proposes a lexical word splitter
to refine WS; then, Section 4 evaluates the pro-
posed method on end-to-end SMT as well as word
segmentation and alignment; after that, Section 5
compares this work to related work; finally, Sec-
tion 6 concludes this paper.
4Guidelines for Chinese-English Word Align-
ment(Version 4.0)
2 Analysis of a General-purpose
Automatic Word Segmenter
This section first briefly describes the GALE WA
corpus, then presents an analysis of the WS arising
from a CTB-standard word segmenter with refer-
ence to the segmentation of the atomic blocks in
the GALE WA corpus, finally the impact of the
findings on SMT is discussed.
2.1 GALE WA corpus
The GALE WA corpus was developed by the
LDC, and was used as training data in the DARPA
GALE global autonomous language exploitation
program 5. The corpus incorporates linguistic
knowledge into word aligned text to help improve
automatic WA and translation quality. It em-
ploys two annotation schemes: alignment and tag-
ging (Li et al., 2010). Alignment identifies min-
imum translation units and translation relations;
tagging adds contextual, syntactic and language-
specific features to the alignment annotation. For
example, the sample shown in Figure 1 carries tags
on both alignment edges and tokens.
The GALE WA corpus contains 18,057 man-
ually word aligned Chinese and English parallel
sentences which are extracted from newswire and
web blogs. Table 1 presents the statistics on the
corpus. One third of the sentences are approxi-
mately newswire text, and the remainder consists
of web blogs.
2.2 Analysis of WS
In order to produce a Chinese word segmenta-
tion consistent with the CTB standard we used the
Stanford Chinese word segmenter with a model
trained on the CTB corpus. We will refer to this
as the ?CTB segmenter? in the rest of this paper.
The Chinese sentences in the GALE WA cor-
pus were first segmented by the CTB segmenter,
and the predicted words were compared against
the atomic blocks with respect to the granularity of
segmentation. The analysis falls into the following
three categories, two of which may be potentially
harmful to SMT:
? Fully consistent: the word locates within the
block of one atomic alignment edge. For ex-
ample, in Figure 2(a), the Chinese text has
5https://catalog.ldc.upenn.edu/
LDC2012T16
1655
 	
 
 
        	 
  
     
	
 	
 
       	 
	  
Figure 1: Example from the GALE WA corpus. Each line arrow represents an atomic edge, and each box
represents an atomic block. SEM (semantic), GIS (grammatically inferred semantic) and FUN (function)
are tags of edges. INC (not translated), TOI (to-infinitive) and DET (determiner) are tags of tokens.
Genre # Files # Sentences? # CN tokens # EN tokens # Alignment edges
Newswire 2,175 6,218 246,371 205,281 164,033
Web blog 2,560 11,839 373,818 312,856 257,730
Total 4,735 18,057 620,189 518,137 421,763
Table 1: GALE WA corpus. ? Sentences rejected by the annotators are excluded.
four atomic blocks; the CTB segmenter pro-
duces five words which all locate within the
blocks, so they are all small enough.
? Alignment inconsistent: the word aligns to
more than one atomic block, but the target
expression is contiguous, allowing for cor-
rect phrase pair extraction (Zens et al., 2002).
For example, in Figure 2(b), the characters in
the word ?shuang fang?, which is produced
by the CTB segmenter, contains two atomic
blocks, but the span of the target ?to both
side? is continuous, therefore the phrase pair
?shuang fang ||| to both sides? can be ex-
tracted.
? Alignment inconsistent and extraction hin-
dered: the word aligned to more than one
atomic block, and the target expression is not
contiguous, which hinders correct phrase pair
extractions. For example, in Figure 2(c), the
word ?zeng chan? has to be split in order to
match the target language.
Table 2 shows the statistics of the three cat-
egories of CTB WS on the GALE WA corpus.
90.74% of the words are fully consistent, while the
remaining 9.26% of the words have inconsistent
alignments. 74.60% of the sentences contain this
problem. The category with inconsistent align-
ment and extraction hindered only accounts for
0.46% of the words, affecting 9.06% of the sen-
tences.
2.3 Impact of WS on SMT
The word alignment has a direct impact on the na-
ture of both the translation model, and lexical re-
ordering model in a phrase-base SMT system. The
words in last two categories are all longer than an
atomic block, which might lead to problems in the
word alignment in two ways:
? First, longer words tend to be more sparse in
the training corpus, thus the estimated distri-
bution of their target phrases are less accu-
rate.
? Second, the alignment from them to target
sides are one-to-many, which is much more
complicated and requires fertilized alignment
models such as IBM model 4 ? 6 (Och and
Ney, 2000).
The words in the category of ?fully consistent?
can be aligned using simple models, because the
alignment from them to the target side are one-to-
one or many-to-one, and simple alignment models
such as IBM model 1, IBM model 2 and HMM
model are sufficient (Och and Ney, 2000).
3 Refining the Word Segmentation
In the last subsection, it was shown that 74.60% of
parallel sentences were affected by issues related
to under-segmentation of the corpus. Our hypoth-
esis is that if these words are split into pieces that
match English words, the accuracy of the unsuper-
vised WA as well as the translation quality will be
improved. To achieve this, we adopt a splitting
1656
	
						
   	
 	  
(a)
	



	






    

  	 	    
(b)
	
 
 	 	
	
     	

	 	 	  
(c)
Figure 2: Examples of automated WS on manually WA corpus: (a) Fully consistent; (b) Alignment
inconsistent; (c) Alignment inconsistent and extraction hindered. The Chinese words separated by white
space are the output of the CTB segmenter. Arrows represent the alignment of atomic blocks. Note that
?shuang fang? and ?zeng chan? are words produced by the CTB segmenter, but consist of two atomic
blocks.
Category Count Word Ratio Sentence Ratio
Fully consistent 355,702 90.74% 25.40%?
Alignment inconsistent 34,464 8.81% 65.54%
Alignment inconsistent & extraction hindered 1,830 0.46% 9.06%
Sum of conflict ? 36,294 9.26% 74.60%
Table 2: CTB WS on GALE WA corpus: ? All words are fully consistent; ? Alignment inconsistent plus
alignment inconsistent & extraction hindered
strategy, based on a supervised learning approach,
to re-segment the corpus. This subsection first for-
malizes the task, and then presents the approach.
3.1 Word splitting task
The word splitting task is formalized as a sequence
labeling task as follows: each word (represented
by a sequence of characters x = x
1
. . . x
T
where
T is the length of sample) produced by the CTB
segmenter is a sample, and a corresponding se-
quence of binary boundary labels y = y
1
. . . y
T
is the learning target,
y
t
=
?
?
?
1 if there is a split point
between c
t
and c
t?1
;
0 otherwise.
(1)
The sequence of boundary labels is derived
from the gold WA annotation as follows: for a
sequence of two atomic blocks, where the first
character of the second block is x
t
, then the la-



	



	




 	
 
Figure 3: Samples of word splitting task
bel y
t
= 1. Figure 3 presents several samples ex-
tracted from the examples in Figure 2.
Each word sample may have no split point, one
split point or multiple split points, depending on
the gold WA annotation. Table 3 shows the statis-
tics of the word splitting data set which is built
from the GALE manual WA corpus and the CTB
segmenter?s output, where 2000 randomly sam-
pled sentences are taken as a held-out test set.
1657
Set # Sentences # Samples # Split points # Split points per sample
Train. 16,057 348,086 32,337 0.0929
Test 2,000 43,910 3,929 0.0895
Table 3: Data set for learning the word splitting
3.2 CRF approach
This paper employs a condition random field
(CRF) to solve this sequence labeling task (Laf-
ferty et al., 2001). A linear-chain CRF defines the
conditional probability of y given x as,
P
?
(y|x) =
1
Z
x
(
T
?
t=1
?
k
?
k
f
k
(y
t?1
, y
t
,x, t)),
(2)
where ? = {?
1
, . . .} are parameters, Z
x
is a per-
input normalization that makes the probability of
all state sequences sum to one; f
k
(y
t?1
, y
t
,x, t) is
a feature function which is often a binary-valued
sparse feature. The training of CRF model is to
maximize the likelihood of training data together
with a regularization penalty to avoid over-fitting
as (Peng et al., 2004; Peng and McCallum, 2006),
?
?
= argmax
?
(
?
i
logP
?
(y
i
|x
i
) ?
?
k
?
2
k
2?
2
k
),
(3)
where (x,y) are training samples; the hyperparam-
eter ?
k
can be understood as the variance of the
prior distribution of ?
k
. When predicting the la-
bels of test samples, the CRF decoder searches for
the optimal label sequence y? that maximizes the
conditional probability,
y
?
= argmax
y
P
?
(y|x). (4)
In (Chang et al., 2008) a method is proposed to
select an appropriate level of segmentation gran-
ularity (in practical terms, to encourage smaller
segments). We call their method ?length tuner?.
The following artificial feature is introduced into
the learned CRF model:
f
0
(x, y
t?1
, y
t
, 1) =
{
1 if y
t
= +1
0 otherwise
(5)
The weight ?
0
of this feature is set by hand to
bias the output of CRF model. By way of expla-
nation, a very large positive ?
0
will cause every
character to be segmented, or conversely a very
large negative ?
0
will inhibit the output of segmen-
tation boundaries. In their experiments, ?
0
= 2
was used to force a CRF segmenter to adopt an in-
termediate granularity between character and the
CTB WS scheme. Compared to the length tuner,
our proposed method exploits lexical knowledge
about word splitting, and we will therefore refer to
it as the ?lexical word splitter? or ?lexical splitter?
for short.
3.3 Feature Set
The features f
k
(y
t?1
, y
t
,x, t) we used include the
WS features from the Chinese Stanford word seg-
menter and a set of extended features described
below. The WS features are included because the
target split points may share some common char-
acteristics with the boundaries in the CTB WS
scheme.
The extended features consists of four types ?
named entities, word frequency, word length and
character-level unsupervised WA. For each type of
the feature, the value and value concatenated with
previous or current character are taken as sparse
features (see Table 4 for details). The real val-
ues of word frequency, word length and character-
level unsupervised WA are converted into sparse
features due to the routine of CRF model.
The character-level unsupervised alignment
feature is inspired by the related works of unsu-
pervised bilingual WS (Xu et al., 2008; Chung and
Gildea, 2009; Nguyen et al., 2010; Michael et al.,
2011). The idea is that the character-level WA can
approximately capture the counterpart English ex-
pression of each Chinese token, and source tokens
aligned to different target expressions should be
split into different words (see Figure 4 for an illus-
tration).
The values of the character-level alignment fea-
tures are obtained through building a dictionary.
First, unsupervised WA is performed on the SMT
training corpus where the Chinese sentences are
treated as sequences of characters; then, the Chi-
nese sentences are segmented by CTB segmenter
and a dictionary of segmented words are built; fi-
nally, for each word in the dictionary, the relative
frequency of being split at a certain position is cal-
1658
Feature Definition Example
NE NE tag of current word Geography:NE
NE-C
?1
NE concatenated with previous character Geo.-ding:NE-C
?1
NE-C
0
NE concatenated with current character Geo.-mei:NE-C
0
Frequency Nearest integer of negative logarithm of word frequency 5?:Freq
Freq.-C
?1
Frequency concatenated with previous character 5-ding:Freq-C
?1
Freq.-C
0
Frequency concatenated with current character 5-mei:Freq-C
0
Length Length of current word (1,2,3,4,5,6,7 or >7) 4:Len
Len.-Position Length concatenated with the position 4-2:Len-Pos
Len.-C
?1
Length concatenated with previous character 4-ding:Len-C
?1
Len.-C
0
Length concatenated with current character 4-mei:Len-C
0
Char. Align. Five-level relative frequency of being split 0.4?:CA
C.A.-C
?1
C.A. concatenated with previous character 0.4-ding:CA-C
?1
C.A.-C
0
C.A. concatenated with current character 0.4-mei:CA-C
0
Table 4: Extended features used in the CRF model for word splitting. The example shows the features
used in the decision whether to split the Chinese word ?la ding mei zhou? (Latin America, the first
four Chinese characters in Figure 4) after the second Chinese character. ? Round(-log
10
(0.00019)); ?
Round(0.43 ? 5 ) / 5
	

	
        	 

	      
Figure 4: Illustration of character-level unsuper-
vised alignment features. The dotted lines are
word boundaries suggested by the alignment.
culated as,
f
CA
(w, i) =
n
i
n
w
(6)
where w is a word, i is a splitting position (from
1 to the length of w minus 1); n
i
is the number of
times the words as split at position i according to
the character-level alignment, that is, the character
before and after i are aligned to different English
expressions; n
w
is occurrence count of word w in
the training corpus.
4 Experiments
In the last section we found that 9.26% of words
produced by the CTB segmenter have the poten-
tial to cause problems for SMT, and propose a
lexical word splitter to address this issue through
segmentation refinement. This section contains
experiments designed to empirically evaluate the
proposed lexical word splitter in three aspects:
first, whether the WS accuracy is improved; sec-
ond, whether the accuracy of the unsupervised WA
during training SMT systems is improved; third,
whether the end-to-end translation quality is im-
proved.
This section first describes the experimental
methodology, then presents the experimental re-
sults, and finally illustrates the operation of our
proposed method using a real example.
4.1 Experimental Methodology
4.1.1 Experimental Corpora
The GALE manual WA corpus and the Chinese to
English corpus from the shared task of the NIST
open machine translation (OpenMT) 2006 evalua-
tion 6 were employed as the experimental corpus
(Table 5).
The experimental corpus for WS was con-
structed by first segmenting 2000 held out sen-
tences from the GALE manual WA corpus with
the Stanford segmenter, and then refining the seg-
mentation with the gold alignment annotation. For
example, the gold segmentation for the examples
in Figure 2 is presented in Figure 5. Note that
this test corpus is intended to represent an oracle
segmentation for our proposed method, and serves
primarily to gauge the improvement of our method
over the baseline Stanford segmenter, relative to
an upper bound.
6http://www.itl.nist.gov/iad/mig/
tests/mt/2006/
1659
    	

     
   
 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 393?398,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Tightly-coupled Unsupervised Clustering and
Bilingual Alignment Model for Transliteration
Tingting Li1, Tiejun Zhao1, Andrew Finch2, Chunyue Zhang1
1Harbin Institute of Technology, Harbin, China
2NICT, Japan
1{ttli, tjzhao, cyzhang}@mtlab.hit.edu.cn
2andrew.finch@nict.go.jp
Abstract
Machine Transliteration is an essential
task for many NLP applications. Howev-
er, names and loan words typically orig-
inate from various languages, obey dif-
ferent transliteration rules, and therefore
may benefit from being modeled inde-
pendently. Recently, transliteration mod-
els based on Bayesian learning have over-
come issues with over-fitting allowing for
many-to-many alignment in the training of
transliteration models. We propose a nov-
el coupled Dirichlet process mixture mod-
el (cDPMM) that simultaneously clusters
and bilingually aligns transliteration data
within a single unified model. The un-
ified model decomposes into two class-
es of non-parametric Bayesian component
models: a Dirichlet process mixture mod-
el for clustering, and a set of multino-
mial Dirichlet process models that perf-
orm bilingual alignment independently for
each cluster. The experimental results
show that our method considerably outper-
forms conventional alignment models.
1 Introduction
Machine transliteration methods can be catego-
rized into phonetic-based models (Knight et al,
1998), spelling-based models (Brill et al, 2000),
and hybrid models which utilize both phonetic
and spelling information (Oh et al, 2005; Oh et
al., 2006). Among them, statistical spelling-based
models which directly align characters in the train-
ing corpus have become popular because they
are language-independent, do not require phonet-
ic knowledge, and are capable of achieving state-
of-the-art performance (Zhang et al, 2012b). A
major problem with real-word transliteration cor-
pora is that they are usually not clean, may con-
tain name pairs with various linguistic origins and
this can hinder the performance of spelling-based
models because names from different origins obey
different pronunciation rules, for example:
?Kim Jong-il/???? (Korea),
?Kana Gaski/??? (Japan),
?Haw King/??? (England),
?Jin yong/??? (China).
The same Chinese character ??? should be
aligned to different romanized character se-
quences: ?Kim?, ?Kana?, ?King?, ?Jin?. To ad-
dress this issue, many name classification metho-
ds have been proposed, such as the supervised lan-
guage model-based approach of (Li et al, 2007),
and the unsupervised approach of (Huang et al,
2005) that used a bottom-up clustering algorithm.
(Li et al, 2007) proposed a supervised translitera-
tion model which classifies names based on their
origins and genders using a language model; it
switches between transliteration models based on
the input. (Hagiwara et al, 2011) tackled the is-
sue by using an unsupervised method based on the
EM algorithm to perform a soft classification.
Recently, non-parametric Bayesian
models (Finch et al, 2010; Huang et al,
2011; Hagiwara et al, 2012) have attracted
much attention in the transliteration field. In
comparison to many of the previous alignment
models (Li et al, 2004; Jiampojamarn et al,
2007; Berg-Kirkpatrick et al, 2011), the non-
parametric Bayesian models allow unconstrained
monotonic many-to-many alignment and are able
to overcome the inherent over-fitting problem.
Until now most of the previous work (Li et al,
2007; Hagiwara et al, 2011) is either affected by
the multi-origins factor, or has issues with over-
fitting. (Hagiwara et al, 2012) took these two fac-
tors into consideration, but their approach still op-
erates within an EM framework and model order
selection by hand is necessary prior to training.
393
We propose a simple, elegant, fully-
unsupervised solution based on a single generative
model able to both cluster and align simultaneous-
ly. The coupled Dirichlet Process Mixture Model
(cDPMM) integrates a Dirichlet process mixture
model (DPMM) (Antoniak, 1974) and a Bayesian
Bilingual Alignment Model (BBAM) (Finch et
al., 2010). The two component models work
synergistically to support one another: the clus-
tering model sorts the data into classes so that
self-consistent alignment models can be built
using data of the same type, and at the same time
the alignment probabilities from the alignment
models drive the clustering process.
In summary, the key advantages of our model
are as follows:
? it is based on a single, unified generative
model;
? it is fully unsupervised;
? it is an infinite mixture model, and does not
require model order selection ? it is effec-
tively capable of discovering an appropriate
number of clusters from the data;
? it is able to handle data from multiple origins;
? it can perform many-to-many alignment
without over-fitting.
2 Model Description
In this section we describe the methodology and
realization of the proposed cDPMM in detail.
2.1 Terminology
In this paper, we concentrate on the alignment
process for transliteration. The proposed cDP-
MM segments a bilingual corpus of transliteration
pairs into bilingual character sequence-pairs. We
will call these sequence-pairs Transliteration U-
nits (TUs). We denote the source and target of
a TU as sm1 = ?s1, ..., sm? and tn1 = ?t1, ..., tn?
respectively, where si (ti) is a single character in
source (target) language. We use the same no-
tation (s, t) = (?s1, ..., sm?, ?t1, ..., tn?) to de-
note a transliteration pair, which we can write as
x = (sm1 , tn1 ) for simplicity. Finally, we express
the training set itself as a set of sequence pairs:
D = {xi}Ii=1. Our aim is to obtain a bilingual
alignment ?(s1, t1), ..., (sl, tl)? for each transliter-
ation pair xi, where each (sj , tj) is a segment of
the whole pair (a TU) and l is the number of seg-
ments used to segment xi.
2.2 Methodology
Our cDPMM integrates two Dirichlet process
models: the DPMM clustering model, and the
BBAM alignment model which is a multinomial
Dirichlet process.
A Dirichlet process mixture model, models the
data as a mixture of distributions ? one for each
cluster. It is an infinite mixture model, and the
number of components is not fixed prior to train-
ing. Equation 1 expresses the DPMM hierarchi-
cally.
Gc|?c, G0c ? DP (?c, G0c)
?k|Gc ? Gc
xi|?k ? f(xi|?k) (1)
where G0c is the base measure and ?c > 0 is the
concentration parameter for the distribution Gc.
xi is a name pair in training data, and ?k repre-
sents the parameters of a candidate cluster k for
xi. Specifically ?k contains the probabilities of all
the TUs in cluster k. f(xi|?k) (defined in Equa-
tion 7) is the probability that mixture component
k parameterized by ?k will generate xi.
The alignment component of our cDPMM is
a multinomial Dirichlet process and is defined as
follows:
Ga|?a, G0a ? DP (?a, G0a)
(sj , tj)|Ga ? Ga (2)
The subscripts ?c? and ?a? in Equations 1 and 2
indicate whether the terms belong to the clustering
or alignment model respectively.
The generative story for the cDPMM is sim-
ple: first generate an infinite number of clusters,
choose one, then generate a transliteration pair us-
ing the parameters that describe the cluster. The
basic sampling unit of the cDPMM for the cluster-
ing process is a transliteration pair, but the basic
sampling unit for BBAM is a TU. In order to inte-
grate the two processes in a single model we treat
a transliteration pair as a sequence of TUs gener-
ated by a BBAM model. The BBAM generates a
sequence (a transliteration pair) based on the joint
source-channel model (Li et al, 2004). We use a
blocked version of a Gibbs sampler to train each
BBAM (see (Mochihashi et al, 2009) for details
of this process).
2.3 The Alignment Model
This model is a multinomial DP model. Under the
Chinese restaurant process (CRP) (Aldous, 1985)
394
interpretation, each unique TU corresponds to a
dish served at a table, and the number of customers
in each table represents the count of a particular
TU in the model.
The probability of generating the jth TU (sj , tj)
is,
P
(
(sj , tj)|(s?j , t?j)
)
=
N
(
(sj , tj)
)
+ ?aG0a
(
(sj , tj)
)
N + ?a (3)
where N is the total number of TUs generated
so far, and N
(
(sj , tj)
)
is the count of (sj , tj).
(s?j , t?j) are all the TUs generated so far except
(sj , tj). The base measure G0a is a joint spelling
model:
G0a
(
(s, t)
)
= P (|s|)P (s||s|)P (|t|)P (t||t|)
= ?
|s|
s
|s|! e
??sv?|s|s ?
?|t|t
|t|! e
??tv?|t|t
(4)
where |s| (|t|) is the length of the source (target)
sequence, vs (vt) is the vocabulary (alphabet) size
of the source (target) language, and ?s (?t) is the
expected length of source (target) side.
2.4 The Clustering Model
This model is a DPMM. Under the CRP interpre-
tation, a transliteration pair corresponds to a cus-
tomer, the dish served on each table corresponds
to an origin of names.
We use z = (z1, ..., zI), zi ? {1, ...,K} to in-
dicate the cluster of each transliteration pair xi in
the training set and ? = (?1, ..., ?K) to represent
the parameters of the component associated with
each cluster.
In our model, each mixture component is a
multinomial DP model, and since ?k contains the
probabilities of all the TUs in cluster k, the num-
ber of parameters in each ?k is uncertain and
changes with the transliteration pairs that belong
to the cluster. For a new cluster (the K + 1th clus-
ter), we use Equation 4 to calculate the probability
of each TU. The cluster membership probability
of a transliteration pair xi is calculated as follows,
P (zi = k|D, ?, z?i) ? nkn? 1 + ?c
P (xi|z, ?k) (5)
P (zi = K + 1|D, ?, z?i) ? ?cn? 1 + ?c
P (xi|z, ?K+1)
(6)
where nk is the number of transliteration pairs in
the existing cluster k ? {1, ...,K} (cluster K + 1
is a newly created cluster), zi is the cluster indi-
cator for xi, and z?i is the sequence of observed
clusters up to xi. As mentioned earlier, basic sam-
pling units are inconsistent for the clustering and
alignment model, therefore to couple the models
the BBAM generates transliteration pairs as a se-
quence of TUs, these pairs are then used directly
in the DPMM.
Let ? = ?(s1, t1), ..., (sl, tl)? be a derivation of
a transliteration pair xi. To make the model inte-
gration process explicit, we use function f to cal-
culate the probability P (xi|z, ?k), where f is de-
fined as follows,
f(xi|?k) =
{ ?
??R
?
(s,t)?? P (s, t|?k) k ? {1, ...,K}?
??R
?
(s,t)?? G0c(s, t) k = K + 1
(7)
where R denotes the set of all derivations of xi,
G0c is the same as Equation 4.
The cluster membership zi is sampled together
with the derivation ? in a single step according to
P (zi = k|D, ?, z?i) and f(xi|?k). Following the
method of (Mochihashi et al, 2009), first f(xi|?k)
is calculated by forward filtering, and then a sam-
ple ? is taken by backward sampling.
3 Experiments
3.1 Corpora
To empirically validate our approach, we investi-
gate the effectiveness of our model by conduct-
ing English-Chinese name transliteration genera-
tion on three corpora containing name pairs of
varying degrees of mixed origin. The first two cor-
pora were drawn from the ?Names of The World?s
Peoples? dictionary published by Xin Hua Pub-
lishing House. The first corpus was construct-
ed with names only originating from English lan-
guage (EO), and the second with names originat-
ing from English, Chinese, Japanese evenly (ECJ-
O). The third corpus was created by extracting
name pairs from LDC (Linguistic Data Consor-
tium) Named Entity List, which contains names
from all over the world (Multi-O). We divided the
datasets into training, development and test sets
for each corpus with a ratio of 10:1:1. The details
of the division are displayed in Table 2.
395
cDPMM Alignment BBAM Alignment
mun|? din|? ger|?(0, English) mun|? din|? ger|?
ding|? guo|?(2, Chinese) din|? g| guo|?
tei|? be|?(3, Japanese) t| |? e| ibe|?
fan|? chun|? yi|?(2, Chinese) fan|? chun|? y| i|?
hong|? il|? sik|?(5, Korea) hong|? i|? l| si|? k|
sei|? ichi|? ro|?(4, Japanese) seii|? ch| i|? ro|?
dom|? b|? ro|? w|? s|? ki|?(0, Russian) do|? mb|? ro|? w|? s|? ki|?
he|? dong|? chang|?(2, Chinese) he|? don|? gchang|?
b|? ran|? don|?(0, English) b|? ran|? don|?
Table 1: Typical alignments from the BBAM and cDPMM.
3.2 Baselines
We compare our alignment model with
GIZA++ (Och et al, 2003) and the Bayesian
bilingual alignment model (BBAM). We employ
two decoding models: a phrase-based machine
translation decoder (specifically Moses (Koehn
et al, 2007)), and the DirecTL decoder (Jiampo-
jamarn et al, 2009). They are based on different
decoding strategies and optimization targets, and
therefore make the comparison more compre-
hensive. For the Moses decoder, we applied the
grow-diag-final-and heuristic algorithm to extract
the phrase table, and tuned the parameters using
the BLEU metric.
Corpora Corpus ScaleTraining Development Testing
EO 32,681 3,267 3,267
ECJ-O 32,500 3,250 3,250
Multi-O 33,291 3,328 3,328
Table 2: Statistics of the experimental corpora.
To evaluate the experimental results, we uti-
lized 3 metrics from the Named Entities Workshop
(NEWS) (Zhang et al, 2012a): word accuracy in
top-1 (ACC), fuzziness in top-1 (Mean F-score)
and mean reciprocal rank (MRR).
3.3 Parameter Setting
In our model, there are several important parame-
ters: 1) max s, the maximum length of the source
sequences of the alignment tokens; 2) max t, the
maximum length of the target sequences of the
alignment tokens; and 3) nc, the initial number of
classes for the training data. We set max s = 6,
max t = 1 and nc = 5 empirically based on a
small pilot experiment. The Moses decoder was
used with default settings except for the distortion-
limit which was set to 0 to ensure monotonic de-
coding. For the DirecTL decoder the following
settings were used: cs = 4, ng = 9 and nBest =
5. cs denotes the size of context window for fea-
tures, ng indicates the size of n-gram features and
nBest is the size of transliteration candidate list
for updating the model in each iteration. The con-
centration parameter ?c, ?a of the clustering mod-
el and the BBAM was learned by sampling its val-
ue. Following (Blunsom et al, 2009) we used
a vague gamma prior ?(10?4, 104), and sampled
new values from a log-normal distribution whose
mean was the value of the parameter, and variance
was 0.3. We used the Metropolis-Hastings algo-
rithm to determine whether this new sample would
be accepted. The parameters ?s and ?t in Equa-
tion 4 were set to ?s = 4 and ?t = 1.
Model EO ECJ-O Multi-O
#(Clusters) cDPMM 5.8 9.5 14.3
#(Targets)
GIZA++ 14.43 5.35 6.62
BBAM 6.06 2.45 2.91
cDPMM 9.32 3.45 4.28
Table 3: Alignment statistics.
3.4 Experimental Results
Table 3 shows some details of the alignment re-
sults. The #(Clusters) represents the average num-
ber of clusters from the cDPMM. It is averaged
over the final 50 iterations, and the classes which
contain less than 10 name pairs are excluded. The
#(Targets) represents the average number of En-
glish character sequences that are aligned to each
Chinese sequence. From the results we can see
that in terms of the number of alignment targe-
ts: GIZA++ > cDPMM > BBAM. GIZA++ has
considerably more targets than the other approach-
es, and this is likely to be a symptom of it over-
fitting the data. cDPMM can alleviate the over-
fitting through its BBAM component, and at the
same time effectively model the diversity in Chi-
nese character sequences caused by multi-origin.
Table 1 shows some typical TUs from the align-
ments produced by BBAM and cDPMM on cor-
pus Multi-O. The information in brackets in Ta-
ble 1, represents the ID of the class and origin of
396
Corpora Model EvaluationACC M-Fscore MRR
EO
GIZA 0.7241 0.8881 0.8061
BBAM 0.7286 0.8920 0.8043
cDPMM 0.7398 0.8983 0.8126
ECJ-O
GIZA 0.5471 0.7278 0.6268
BBAM 0.5522 0.7370 0.6344
cDPMM 0.5643 0.7420 0.6446
Multi-O
GIZA 0.4993 0.7587 0.5986
BBAM 0.5163 0.7769 0.6123
cDPMM 0.5237 0.7796 0.6188
Table 4: Comparison of different methods using
the Moses phrase-based decoder.
the name pair; the symbol ? ? indicates a ?NUL-
L? alignment. We can see the Chinese characters
??(ding) ?(yi) ?(dong)? have different align-
ments in different origins, and that the cDPMM
has provided the correct alignments for them.
We used the sampled alignment from running
the BBAM and cDPMMmodels for 100 iterations,
and combined the alignment tables of each class
together. The experiments are therefore investigat-
ing whether the alignment has been meaningfully
improved by the clustering process. We would ex-
pect further gains from exploiting the class infor-
mation in the decoding process (as in (Li et al,
2007)), but this remains future research. The top-
10 transliteration candidates were used for testing.
The detailed experimental results are shown in Ta-
bles 4 and 5.
Our proposed model obtained the highest per-
formance on all three datasets for all evaluation
metrics by a considerable margin. Surprisingly,
for dataset EO although there is no multi-origin
factor, we still observed a respectable improve-
ment in every metric. This shows that although
names may have monolingual origin, there are hid-
den factors which can allow our model to succeed,
possibly related to gender or convention. Other
models based on supervised classification or clus-
tering with fixed classes may fail to capture these
characteristics.
To guarantee the reliability of the compara-
tive results, we performed significance testing
based on paired bootstrap resampling (Efron et al,
1993). We found all differences to be significant
(p < 0.05).
4 Conclusion
In this paper we propose an elegant unsupervised
technique for monotonic sequence alignment
based on a single generative model. The key ben-
Corpora Model EvaluationACC M-Fscore MRR
EO
GIZA 0.6950 0.8812 0.7632
BBAM 0.7152 0.8899 0.7839
cDPMM 0.7231 0.8933 0.7941
ECJ-O
GIZA 0.3325 0.6208 0.4064
BBAM 0.3427 0.6259 0.4192
cDPMM 0.3521 0.6302 0.4316
Multi-O
GIZA 0.3815 0.7053 0.4592
BBAM 0.3934 0.7146 0.4799
cDPMM 0.3970 0.7179 0.4833
Table 5: Comparison of different methods using
the DirecTL decoder.
efits of our model are that it can handle data from
multiple origins, and model using many-to-many
alignment without over-fitting. The model oper-
ates by clustering the data into classes while si-
multaneously aligning it, and is able to discover
an appropriate number of classes from the data.
Our results show that our alignment model can im-
prove the performance of a transliteration gener-
ation system relative to two other state-of-the-art
aligners. Furthermore, the system produced gains
even on data of monolingual origin, where no ob-
vious clusters in the data were expected.
Acknowledgments
We thank the anonymous reviewers for their valu-
able comments and helpful suggestions.We also
thank Chonghui Zhu, Mo Yu, and Wenwen Zhang
for insightful discussions. This work was support-
ed by National Natural Science Foundation of Chi-
na (61173073), and the Key Project of the Nation-
al High Technology Research and Development
Program of China (2011AA01A207).
References
D.J. Aldous. 1985. Exchangeability and Related Top-
ics. E?cole d?E?te? St Flour 1983. Springer, 1985,
1117:1?198.
C.E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. Annals of Statistics. 2:1152, 174.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proc. of EMNLP, pages 313?321.
P. Blunsom, T. Cohn, C. Dyer, and Osborne, M. 2009.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proc. of ACL, pages 782?790.
Eric Brill and Robert C. Moore. 2000. An Improved
Error Model for Noisy Channel Spelling Correction.
In Proc. of ACL, pages 286?293.
397
B. Efron and R. J. Tibshirani 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York, NY.
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Translitera-
tion. In Proc. of the 7th International Workshop on
Spoken Language Translation, pages 259?266.
Masato Hagiwara and Satoshi Sekine. 2011. Latent
Class Transliteration based on Source Language O-
rigin. In Proc. of ACL (Short Papers), pages 53-57.
Masato Hagiwara and Satoshi Sekine. 2012. Latent
semantic transliteration using dirichlet mixture. In
Proc. of the 4th Named Entity Workshop, pages 30?
37.
Fei Huang, Stephan Vogel, and Alex Waibel. 2005.
Clustering and Classifying Person Names by Origin.
In Proc. of AAAI, pages 1056?1061.
Yun Huang, Min Zhang and Chew Lim Tan. 2011.
Nonparametric Bayesian Machine Transliteration
with Synchronous Adaptor Grammars. In Proc. of
ACL, pages 534?539.
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek
Sherif. 2007. Applying Many-to-Many Alignments
and Hidden Markov Models to Letter-to-Phoneme
Conversion. In Proc. of NAACL, pages 372?379.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer and Grzegorz Kondrak. 2009.
DirecTL: a Language Independent Approach to
Transliteration. In Proc. of the 2009 Named Entities
Workshop: Shared Task on Transliteration (NEWS
2009), pages 1056?1061.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Journal of Computational Linguis-
tics, pages 28?31.
Philipp Koehn and Hieu Hoang and Alexandra Birch
and Chris Callison-Burch and Marcello Federico
and Nicola Bertoldi and Brooke Cowan and Wade
Shen and Christine Moran and Richard Zens and
Chris Dyer and Ondrej Bojar and Alexandra Con-
stantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL.
Haizou Li, Min Zhang, and Jian Su 2004. A join-
t source-channel model for machine transliteration.
In ACL ?04: Proceedings of the 42nd Annual Meet-
ing on Association for Computational Linguistics.
Association for Computational Linguistics, Morris-
town, NJ, USA, 159.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic Transliteration of Personal
Names. In Proc. of ACL, pages 120?127.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ue-
da. 2009. Bayesian Unsupervised Word Segmen-
tation with Nested Pitman-Yor Language Modeling.
In Proc. of ACL/IJCNLP, pages 100?108.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Journal of Comput. Linguist., 29(1):19-51.
Jong-Hoon Oh, and Key-Sun Choi. 2005. Machine
Learning Based English-to-Korean Transliteration
Using Grapheme and Phoneme Information. Jour-
nal of IEICE Transactions, 88-D(7):1737-1748.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A machine transliteration model based on
correspondence between graphemes and phonemes.
Journal of ACM Trans. Asian Lang. Inf. Process.,
5(3):185-208.
Min Zhang, Haizhou Li, Ming Liu and A Kumaran.
2012a. Whitepaper of NEWS 2012 shared task on
machine transliteration. In Proc. of the 4th Named
Entity Workshop (NEWS 2012), pages 1?9.
Min Zhang, Haizhou Li, A Kumaran and Ming Liu.
2012b. Report of NEWS 2012 Machine Translitera-
tion Shared Task. In Proc. of the 4th Named Entity
Workshop (NEWS 2012), pages 10?20.
398
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752?758,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Empirical Study of Unsupervised Chinese Word Segmentation Methods
for SMT on Large-scale Corpora
Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita
National Institute of Information and Communications Technology
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
Abstract
Unsupervised word segmentation (UWS)
can provide domain-adaptive segmenta-
tion for statistical machine translation
(SMT) without annotated data, and bilin-
gual UWS can even optimize segmenta-
tion for alignment. Monolingual UWS ap-
proaches of explicitly modeling the proba-
bilities of words through Dirichlet process
(DP) models or Pitman-Yor process (PYP)
models have achieved high accuracy, but
their bilingual counterparts have only been
carried out on small corpora such as ba-
sic travel expression corpus (BTEC) due to
the computational complexity. This paper
proposes an efficient unified PYP-based
monolingual and bilingual UWS method.
Experimental results show that the pro-
posed method is comparable to super-
vised segmenters on the in-domain NIST
OpenMT corpus, and yields a 0.96 BLEU
relative increase on NTCIR PatentMT cor-
pus which is out-of-domain.
1 Introduction
Many languages, especially Asian languages such
as Chinese, Japanese and Myanmar, have no ex-
plicit word boundaries, thus word segmentation
(WS), that is, segmenting the continuous texts of
these languages into isolated words, is a prerequi-
site for many natural language processing applica-
tions including SMT.
Though supervised-learning approaches which
involve training segmenters on manually seg-
mented corpora are widely used (Chang et al,
2008), yet the criteria for manually annotat-
ing words are arbitrary, and the available anno-
tated corpora are limited in both quantity and
genre variety. For example, in machine transla-
tion, there are various parallel corpora such as
BTEC for tourism-related dialogues (Paul, 2008)
and PatentMT in the patent domain (Goto et
al., 2011)1, but researchers working on Chinese-
related tasks often use the Stanford Chinese seg-
menter (Tseng et al, 2005) which is trained on a
small amount of annotated news text.
In contrast, UWS, spurred by the findings that
infants are able to use statistical cues to determine
word boundaries (Saffran et al, 1996), relies on
statistical criteria instead of manually crafted stan-
dards. UWS learns from unsegmented raw text,
which are available in large quantities, and thus
it has the potential to provide more accurate and
adaptive segmentation than supervised approaches
with less development effort being required.
The approaches of explicitly modeling the
probability of words(Brent, 1999; Venkataraman,
2001; Goldwater et al, 2006; Goldwater et al,
2009; Mochihashi et al, 2009) significantly out-
performed a heuristic approach (Zhao and Kit,
2008) on the monolingual Chinese SIGHAN-MSR
corpus (Emerson, 2005), which inspired the work
of this paper.
However, bilingual approaches that model word
probabilities suffer from computational complex-
ity. Xu et al (2008) proposed a bilingual method
by adding alignment into the generative model, but
was only able to test it on small-scale BTEC data.
Nguyen et al (2010) used the local best alignment
to increase the speed of the Gibbs sampling in
training but the impact on accuracy was not ex-
plored.
This paper is dedicated to bilingual UWS on
large-scale corpora to support SMT. To this end,
we model bilingual UWS under a similar frame-
work with monolingual UWS in order to improve
efficiency, and replace Gibbs sampling with ex-
pectation maximization (EM) in training.
We aware that variational bayes (VB) may be
used for speeding up the training of DP-based
1http://ntcir.nii.ac.jp/PatentMT
752
or PYP-based bilingual UWS. However, VB re-
quires formulating the m expectations of (m?1)-
dimensional marginal distributions, where m is
the number of hidden variables. For UWS, the
hidden variables are indicators that identify sub-
strings of sentences in the corpus as words. These
variables are large in number and it is not clear
how to apply VB to UWS, and as far the authors
aware there is no previous work related to the ap-
plication of VB to monolingual UWS. Therefore,
we have not explored VB methods in this paper,
but we do show that our method is superior to the
existing methods.
The contributions of this paper include,
? state-of-the-art accuracy in monolingual
UWS;
? the first bilingual UWS method practical for
large corpora;
? improvement of BLEU scores compared
to supervised Stanford Chinese word seg-
menter.
2 Methods
This section describes our unified monolingual
and bilingual UWS scheme. Table 1 lists the main
notation. The set F is chosen to represent an un-
segmented foreign language sentence (a sequence
of characters), because an unsegmented sentence
can be seen as the set of all possible segmentations
of the sentence denoted F , i.e. F ? F .
Notation Meaning
F an unsegmented foreign sentence
F
k
?
k
unsegmented substring of the un-
derlying string of F from k to k?
F a segmented foreign sentence
f
j
the j-th foreign word
M monolingual segmentation model
P
M
(x) probability of x being a word ac-
cording to M
E a tokenized English sentence
e
i
the i-th English word
(F ,E) a bilingual sentence pair
B bilingual segmentation model
P
B
(x|e
i
) probability of x being a word ac-
cording to B given e
i
Table 1: Main Notation.
Monolingual and bilingual WS can be formu-
lated as follows, respectively,
?
F (F) = argmax
F?F
P (F |F ,M), (1)
?
F (F , E) = argmax
F?F
?
a
P (F, a|F , E,B), (2)
where a is an alignment between F and E. The
English sentence E is used in the generation of a
segmented sentence F .
UWS learns models by maximizing the likeli-
hood of the unsegmented corpus, formulated as,
?
M = argmax
M
?
F?F
(
?
F?F
P (F |M)
)
, (3)
?
B = argmax
B
?
(F ,E)?B
(
?
F?F
?
a
P (F, a|F , E,B)
)
.
(4)
Our method of learning M and B proceeds in a
similar manner to the EM algorithm. The follow-
ing two operations are performed iteratively for
each sentence (pair).
? Exclude the previous expected counts of the
current sentence (pair) from the model, and
then derive the current sentence in all pos-
sible ways, calculating the new expected
counts for the words (see Section 2.1), that
is, we calculate the expected probabilities of
the Fk?
k
being words given the data excluding
F , i.e. E
F/{F}
(P (F
k
?
k
|F)) = P (F
k
?
k
|F ,M)
in a similar manner to the marginalization in
the Gibbs sampling process which we are re-
placing;
? Update the respective model M or B accord-
ing to these expectations (see Section2.2).
2.1 Expectation
2.1.1 Monolingual Expectation
P (F
k
?
k
|F ,M) is the marginal probability of all
the possible F ? F that contain Fk?
k
as a word,
which can be calculated efficiently through dy-
namic programming (the process is similar to the
foreward-backward algorithm in training a hidden
Markov model (HMM) (Rabiner, 1989)):
P
a
(k) =
U
?
u=1
P
a
(k ? u)P
M
(F
k
k?u
)
P
b
(k
?
) =
U
?
u=1
P
b
(k
?
+ u)P
M
(F
k
?
+u
k
?
)
P (F
k
?
k
|F ,M) = P
a
(k)P
M
(F
k
?
k
)P
b
(k
?
), (5)
753
where U is the predefined maximum length of for-
eign language words, P
a
(k) and P
b
(k
?
) are the
forward and backward probabilities, respectively.
This section uses a unigram model for description
convenience, but the method can be extended to
n-gram models.
2.1.2 Bilingual Expectation
P (F
k
?
k
|F , E,B) is the marginal probability of all
the possible F ? F that contain Fk?
k
as a word and
are aligned with E, formulated as:
P (F
k
?
k
|F , E,B) =
?
F?F
F
k
?
k
?F
?
a
P (F, a|E,B)
?
?
F?F
F
j
k
=F
k
?
k
?
a
J
?
j=1
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
)
=
?
F?F
f
j
k
=F
k
?
k
J
?
j=1
?
a
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
),
(6)
where J and I are the number of foreign and En-
glish words, respectively, and a
j
is the position of
the English word that is aligned to f
j
in the align-
ment a. For the alignment we employ an approx-
imation to IBM model 2 (Brown et al, 1993; Och
and Ney, 2003) described below.
We define the conditional probability of f
j
given the corresponding English sentence E and
the model B as:
P
B
(f
j
|E) =
?
a
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
) (7)
Then, the previous dynamic programming
method can be extended to the bilingual expecta-
tion
P
a
(k|E) =
U
?
u=1
P
a
(k ? u|E)P
B
(F
k
k?u
|E)
P
b
(k
?
|E) =
U
?
u=1
P
b
(k
?
+ u|E)P
B
(F
k
?
+u
k
?
|E)
P (F
k
?
k
|F , E,B) = P
a
(k|E)P
B
(F
k
?
k
|E)P
b
(k
?
|E).
(8)
Eq. 7 can be rewritten (as in IBM model 2):
P
B
(f
j
|E) =
I
?
i=1
P
?
(i|j, I, J)P
B
(f
j
|e
i
) (9)
P
?
(i|j, I, J) =
?
a:a
j
=i
P (a
j
|, j, I, J)
In order to maintain both speed and accuracy, the
following window function is adopted
P
?
(i|j, I, J) ? P
?
(i|k, I,K) =
?
?
?
e
?|i?kI/K|
/? |i? kI/K| 6 ?
b
/2
?
?
e
i
is empty word
0 otherwise
(10)
where K is the number of characters in F , and
the k-th character is the start of the word f
j
, since
j and J are unknown during the computation of
dynamic programming. ?
b
is the window size, ?
?
is the prior probability of an empty English word,
and ? ensures all the items sum to 1.
2.2 Maximization
Inspired by (Teh, 2006; Mochihashi et al, 2009;
Neubig et al, 2010; Teh and Jordan, 2010), we
employ a Pitman-Yor process model to build the
segmentation model M or B. The monolingual
model M is
P
M
(f
j
) =
max
(
n(f
j
)? d, 0
)
+ (? + d ? n
M
)G
0
(f
j
)
?
f
?
j
n(f
?
j
) + ?
n
M
=
?
?
{f
j
|n(f
j
) > d}
?
?
, (11)
where f
j
is a foreign language word, and n(f
j
) is
the observed counts of f
j
, ? is named the strength
parameter, G
0
(f
j
) is named the base distribution
of f
j
, and d is the discount.
The bilingual model is
P
B
(f
j
|e
i
) =
max
(
n(f
j
, e
i
)? d, 0
)
+ (? + d ? n
e
i
)G
0
(f
j
|e
i
)
?
f
?
j
n(f
?
j
, e
i
) + ?
n
e
i
=
?
?
{x |n(x, e
i
) > d}
?
?
. (12)
In Eqs. 11 and 12,
n(f
j
) =
?
F?F
P (f
j
|F ,M) (13)
n(f
j
, e
i
) =
?
(F ,E)?B
P (f
j
|F , E,B)
P
?
(i|j, I, J)P
B
(f
j
|e
i
)
?
I
i
?
=1
P
?
(i
?
|j, I, J)P
B
(f
j
|e
i
?
)
.
(14)
754
3 Complexity Analysis
The computational complexity of our method is
linear in the number of iterations, the size of the
corpus, and the complexity of calculating the ex-
pectations on each sentence or sentence pair. In
practical applications, the size of the corpus is
fixed, and we found empirically that the number
of iterations required by the proposed method for
convergence is usually small (less than five itera-
tions). We now look in more detail at the complex-
ity of the expectation calculation in monolingual
and bilingual models.
The monolingual expectation is calculated ac-
cording to Eq. 5; the complexity is linear in the
length of sentences and the square of the prede-
fined maximum length of words. Thus its overall
complexity is
O
unigram
monoling = O(Ni|F|KU
2
), (15)
where Ni is the number of iterations, K is the av-
erage number of characters per sentence, and U is
the predefined maximum length of words.
For the monolingual bigram model, the number
of states in the HMM is U times more than that
of the monolingual unigram model, as the states at
specific position of F are not only related to the
length of the current word, but also related to the
length of the word before it. Thus its complexity
is U2 times the unigram model?s complexity:
O
bigram
monoling = O(Ni|F|KU
4
). (16)
The bilingual expectation is given by Eq. 8,
whose complexity is the same as the monolingual
case. However, the complexity of calculating the
transition probability, in Eqs. 9 and 10, is O(?
b
).
Thus its overall complexity is:
O
unigram
biling = O(Ni|F|KU
2
?
b
). (17)
4 Experiments
In this section, the proposed method is first val-
idated on monolingual segmentation tasks, and
then evaluated in the context of SMT to study
whether the translation quality, measured by
BLEU, can be improved.
4.1 Experimental Settings
4.1.1 Experimental Corpora
Two monolingual corpora and two bilingual cor-
pora are used (Table 2). CHILDES (MacWhin-
ney and Snow, 1985) is the most common test
Corpus Type # Sentences # Characters
CHILDES Mono. 9,790 95,809
SIGHAN-MSR Mono. 90,903 4,234,824
OpenMT06 Biling. 437,004 19,692,605
PatentMT9 Biling. 1,004,000 63,130,757
Table 2: Experimental Corpora
corpus for UWS methods. The SIGHAN-MSR
corpus (Emerson, 2005) consists of manually seg-
mented simplified Chinese news text, released in
the SIGHAN bakeoff 2005 shared tasks.
The first bilingual corpus: OpenMT06 was used
in the NIST open machine translation 2006 Eval-
uation 2. We removed the United Nations cor-
pus and the traditional Chinese data sets from the
constraint training resources. The data sets of
NIST Eval 2002 to 2005 were used as the develop-
ment for MERT tuning (Och, 2003). This data set
mainly consists of news text 3. PatentMT9 is from
the shared task of NTCIR-9 patent machine trans-
lation . The training set consists of 1 million par-
allel sentences extracted from patent documents,
and the development set and test set both consist
of 2000 sentences.
4.1.2 Performance Measurement and
Baseline Methods
For the monolingual tasks, the F
1
score against
the gold annotation is adopted to measure the ac-
curacy. The results reported in related papers are
listed for comparison.
For the bilingual tasks, the publicly available
system of Moses (Koehn et al, 2007) with default
settings is employed to perform machine transla-
tion, and BLEU (Papineni et al, 2002) was used
to evaluate the quality. Character-based segmen-
tation, LDC segmenter and Stanford Chinese seg-
menters were used as the baseline methods.
4.1.3 Parameter settings
The parameters are tuned on held-out data sets.
The maximum length of foreign language words
is set to 4. For the PYP model, the base distri-
bution adopts the formula in (Chung and Gildea,
2009), and the strength parameter is set to 1.0, and
the discount is set to 1.0? 10?6.
For bilingual segmentation,the size of the align-
ment window is set to 6; the probability ?
?
of for-
eign language words being generated by an empty
2http://www.itl.nist.gov/iad/mig/
/tests/mt/2006/
3It also contains a small number of web blogs
755
Method Accuracy Time
CHILD. MSR CHILD. MSR
NPY(bigram)a 0.750 0.802 17 m ?
NPY(trigram)a 0.757 0.807 ? ?
HDP(bigram)b 0.723 ? 10 h ?
Fitnessc ? 0.667 ? ?
Prop.(unigram) 0.729 0.804 3 s 50 s
Prop.(bigram) 0.774 0.806 15 s 2530 s
a by (Mochihashi et al,2009);
b by (Goldwater et al,2009);
c by (Zhao and Kit, 2008).
Table 3: Results on Monolingual Corpora.
English word, was set to 0.3.
The training was started from assuming that
there was no previous segmentations on each sen-
tence (pair), and the number of iterations was
fixed. It was set to 3 for the monolingual unigram
model, and 2 for the bilingual unigram model,
which provided slightly higher BLEU scores on
the development set than the other settings. The
monolingual bigram model, however, was slower
to converge, so we started it from the segmenta-
tions of the unigram model, and using 10 itera-
tions.
4.2 Monolingual Segmentation Results
In monolingual segmentation, the proposed meth-
ods with both unigram and bigram models were
tested. Experimental results show that they are
competitive to state-of-the-art baselines in both ac-
curacy and speed (Table 3). Note that the com-
parison of speed is only for reference because the
times are obtained from their respective papers.
4.3 Bilingual Segmentation Results
Table 4 presents the BLEU scores for Moses using
different segmentation methods. Each experiment
was performed three times. The proposed method
with monolingual bigram model performed poorly
on the Chinese monolingual segmentation task;
thus, it was not tested. We intended to test (Mochi-
hashi et al, 2009), but found it impracticable on
large-scale corpora.
The experimental results show that the proposed
UWS methods are comparable to the Stanford seg-
menters on the OpenMT06 corpus, while achieves
a 0.96 BLEU increase on the PatentMT9 corpus.
This is because this corpus is out-of-domain for
the supervised segmenters. The CTB and PKU
Stanford segmenter were both trained on anno-
tated news text, which was the major domain of
OpenMT06.
Method BLEU
OpenMT06 PatentMT9
Character 29.50 ? 0.03 28.36 ? 0.09
LDC 31.33 ? 0.10 30.22 ? 0.14
Stanford(CTB) 31.68 ? 0.25 30.77 ? 0.13
Stanford(PKU) 31.54 ? 0.13 30.86 ? 0.04
Prop.(mono.) 31.47 ? 0.18 31.62 ? 0.06
Prop.(biling.) 31.61 ? 0.14 31.73 ? 0.05
Table 4: Results on Bilingual Corpora.
Method Time
OpenMT06 PatentMT9
Prop.(mono.) 28 m 1 h 01 m
Prop.(biling.) 2 h 25 m 5 h 02 m
Table 5: Time Costs on Bilingual Corpora.
Table 5 presents the run times of the proposed
methods on the bilingual corpora. The program
is single threaded and implemented in C++. The
time cost of the bilingual models is about 5 times
that of the monolingual model, which is consistent
with the complexity analysis in Section 3.
5 Conclusion
This paper is devoted to large-scale Chinese UWS
for SMT. An efficient unified monolingual and
bilingual UWS method is proposed and applied to
large-scale bilingual corpora.
Complexity analysis shows that our method is
capable of scaling to large-scale corpora. This was
verified by experiments on a corpus of 1-million
sentence pairs on which traditional MCMC ap-
proaches would struggle (Xu et al, 2008).
The proposed method does not require any
annotated data, but the SMT system with it
can achieve comparable performance compared
to state-of-the-art supervised word segmenters
trained on precious annotated data. Moreover,
the proposed method yields 0.96 BLEU improve-
ment relative to supervised word segmenters on
an out-of-domain corpus. Thus, we believe that
the proposed method would benefit SMT related to
low-resource languages where annotated data are
scare, and would also find application in domains
that differ too greatly from the domains on which
supervised word segmenters were trained.
In future research, we plan to improve the bilin-
gual UWS through applying VB and integrating
more accurate alignment models such as HMM
models and IBM model 4.
756
References
Michael R Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1-3):71?105.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational linguistics, 19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the 3rd Workshop on Statistical Machine
Translation, pages 224?232. Association for Com-
putational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 718?726. Association for Com-
putational Linguistics.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the 4th SIGHAN Workshop on Chinese Language
Processing, volume 133.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsu-
pervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 673?
680. Association for Computational Linguistics.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A Bayesian framework for word seg-
mentation: exploring the effects of context. Cogni-
tion, 112(1):21?54.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR, volume 9, pages 559?578.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Journal of
child language, 12(2):271?296.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100?108.
Association for Computational Linguistics.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language
model from continuous speech. In InterSpeech,
pages 1053?1056.
ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith.
2010. Nonparametric word segmentation for ma-
chine translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 815?823. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318. Association
for Computational Linguistics.
Michael Paul. 2008. Overview of the IWSLT 2008
evaluation campaign. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 1?17.
Lawrence R Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
286.
Jenny R Saffran, Richard N Aslin, and Elissa L New-
port. 1996. Statistical learning by 8-month-old in-
fants. Science, 274(5294):1926?1928.
Yee Whye Teh and Michael I Jordan. 2010. Hierar-
chical Bayesian nonparametric models with appli-
cations. Bayesian Nonparametrics: Principles and
Practice, pages 158?207.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting on Association for Computational Linguis-
tics, pages 985?992. Association for Computational
Linguistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter for SIGHAN
Bakeoff 2005. In Proceedings of the 4th SIGHAN
Workshop on Chinese Language Processing, volume
171. Jeju Island, Korea.
757
Anand Venkataraman. 2001. A statistical model for
word discovery in transcribed speech. Computa-
tional Linguistics, 27(3):351?372.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
Chinese word segmentation for statistical machine
translation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics-
Volume 1, pages 1017?1024. Association for Com-
putational Linguistics.
Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In Proceedings of the 3rd International Joint Con-
ference on Natural Language Processing, pages 9?
16.
758
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400?408,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Integration of Multiple Bilingually-Learned Segmentation Schemes
into Statistical Machine Translation
Michael Paul and Andrew Finch and Eiichiro Sumita
MASTAR Project
National Institute of Information and Communications Technology
Hikaridai 2-2-2, Keihanna Science City
619-0288 Kyoto, Japan
michael.paul@nict.go.jp
Abstract
This paper proposes an unsupervised
word segmentation algorithm that identi-
fies word boundaries in continuous source
language text in order to improve the
translation quality of statistical machine
translation (SMT) approaches. The
method can be applied to any language
pair where the source language is unseg-
mented and the target language segmen-
tation is known. First, an iterative boot-
strap method is applied to learn multi-
ple segmentation schemes that are consis-
tent with the phrasal segmentations of an
SMT system trained on the resegmented
bitext. In the second step, multiple seg-
mentation schemes are integrated into a
single SMT system by characterizing the
source language side and merging iden-
tical translation pairs of differently seg-
mented SMT models. Experimental re-
sults translating five Asian languages into
English revealed that the method of in-
tegrating multiple segmentation schemes
outperforms SMT models trained on any
of the learned word segmentations and
performs comparably to available state-of-
the-art monolingually-built segmentation
tools.
1 Introduction
The task of word segmentation, i.e., identifying
word boundaries in continuous text, is one of the
fundamental preprocessing steps of data-driven
NLP applications like Machine Translation (MT).
In contrast to Indo-European languages like En-
glish, many Asian languages like Chinese do not
use a whitespace character to separate meaningful
word units. The problems of word segmentation
are:
(1) ambiguity, e.g., for Chinese, a single charac-
ter can be a word component in one context,
but a word by itself in another context.
(2) unknown words, i.e., existing words can be
combined into new words such as proper
nouns, e.g. ?White House?.
Purely dictionary-based approaches like (Cheng
et al, 1999) addressed these problems by max-
imum matching heuristics. Recent research on
unsupervised word segmentation focuses on ap-
proaches based on probabilistic methods. For ex-
ample, (Brent, 1999) proposes a probabilistic seg-
mentation model based on unigram word distri-
butions, whereas (Venkataraman, 2001) uses stan-
dard n-gram language models. An alternative non-
parametric Bayesian inference approach based on
the Dirichlet process incorporating unigram and
bigram word dependencies is introduced in (Gold-
water et al, 2006).
The focus of this paper, however, is to
learn word segmentations that are consistent with
phrasal segmentations of SMT translation mod-
els. In case of small translation units, e.g. sin-
gle Chinese or Japanese characters, it is likely
that such tokens have been seen in the training
corpus, thus these tokens can be translated by
an SMT engine. However, the contextual infor-
mation provided by these tokens might not be
enough to obtain a good translation. For exam-
ple, a Japanese-English SMT engine might trans-
late the two successive characters ?   ? (?white?)
and ?  ? (?bird?) as ?white bird?, while a human
would translate ?    ? as ?swan?. Therefore, the
longer the translation unit, the more context can be
exploited to find a meaningful translation. On the
other hand, the longer the translation unit, the less
likely it is that such a token will occur in the train-
ing data due to data sparseness of the language
resources utilized to train the statistical translation
models. Therefore, a word segmentation that is
400
?consistent with SMT models? is one that identi-
fies translation units that are small enough to be
translatable, but large enough to be meaningful in
the context of the given input sentence, achieving a
trade-off between the coverage and the translation
task complexity of the statistical models in order to
improve translation quality.
The use of monolingual probabilistic models
does not necessarily yield a better MT perfor-
mance (Chang et al, 2008). However, improve-
ments have been reported for approaches taking
into account not only monolingual, but also bilin-
gual information, to derive a word segmentation
suitable for SMT. Due to the availability of lan-
guage resources, most recent research has focused
on optimizing Chinese word segmentation (CWS)
for Chinese-to-English SMT. For example, (Xu et
al., 2008) proposes a Bayesian Semi-Supervised
approach for CWS that builds on (Goldwater et al,
2006). The generative model first segments Chi-
nese text using an off-the-shelf segmenter and then
learns new word types and word distributions suit-
able for SMT. Similarly, a dynamic programming-
based variational Bayes approach using bilingual
information to improve MT is proposed in (Chung
and Gildea, 2009). Concerning other languages,
for example, (Kikui and Yamamoto, 2002) ex-
tended Hidden-Markov-Models, where hidden n-
gram probabilities were affected by co-occurring
words in the target language part for Japanese
word segmentation.
Recent research on SMT is also focusing on the
usage of multiple word segmentation schemes for
the source language to improve translation qual-
ity. For example, (Zhang et al, 2008) combines
dictionary-based and CRF-based approaches for
Chinese word segmentation in order to avoid out-
of-vocabulary (OOV) words. Moreover, the com-
bination of different morphological decomposi-
tion of highly inflected languages like Arabic or
Finnish is proposed in (de Gispert et al, 2009) to
reduce the data sparseness problem of SMT ap-
proaches. Similarly, (Nakov et al, 2009) utilizes
SMT engines trained on different word segmenta-
tion schemes and combines the translation outputs
using system combination techniques as a post-
process to SMT decoding.
In order to integrate multiple word segmenta-
tion schemes into the SMT decoder, (Dyer et al,
2008) proposed to generate word lattices covering
all possible segmentations of the input sentence
and to decode the lattice input. An extended ver-
sion of the lattice approach that does not require
the use (and existence) of monolingual segmenta-
tion tools was proposed in (Dyer, 2009) where a
maximum entropy model is used to assign prob-
abilities to the segmentations of an input word to
generate diverse segmentation lattices from a sin-
gle automatically learned model.
The method of (Ma and Way, 2009) also uses
a word lattice decoding approach, but they itera-
tively extract multiple word segmentation schemes
from the training bitext. This dictionary-based
approach uses heuristics based on the maximum
matching algorithm to obtain an agglomeration of
segments that are covered by the dictionary. It uses
all possible source segmentations that are consis-
tent with the extracted dictionary to create a word
lattice for decoding.
The method proposed in this papers differs from
previous approaches in the following points:
? it works for any language pair where the
source language is unsegmented and the tar-
get language segmentation is known.
? it can be applied for the translation of a
source language where no linguistically mo-
tivated word segmentation tools are available.
? it applies machine learning techniques to
identify segmentation schemes that improve
translation quality for a given language pair.
? it decodes directly from unsegmented text us-
ing segmentation information implicit in the
phrase-table to generate the target and thus
avoids issues of consistency between phrase-
table and input representation.
? it uses segmentations at all iterative levels of
the bootstrap process, rather than only those
from the final iteration allowing the consid-
eration of segmentations from many levels of
granularity.
Word segmentations are learned using a parallel
corpus by aligning character-wise source language
sentences to word units separated by a white-
space in the target language. Successive characters
aligned to the same target words are merged into a
larger source language unit. Therefore, the granu-
larity of the translation unit is defined in the given
bitext context. In order to minimize the side ef-
fects of alignment errors and to achieve segmenta-
tion consistency, a Maximum-Entropy (ME) algo-
rithm is applied to learn the source language word
401
segmentation that is consistent with the transla-
tion model of an SMT system trained on the re-
segmented bitext. The process is iterated until
no further improvement in translation quality is
achieved. In order to integrate multiple word seg-
mentation into a single SMT system, the statisti-
cal translation models trained on differently seg-
mented source language corpora are merged by
characterizing the source side of each translation
model, summing up the probabilities of identical
phrase translation pairs, and rescoring the merged
translation model (see Section 2).
The proposed segmentation method is applied
to the translation of five Asian languages, i.e.,
Japanese, Korean, Thai, and two Chinese dialects
(Standard Mandarin and Taiwanese Mandarin),
into English. The utilized language resources
and the outline of the experiments are summa-
rized in Section 3. The experimental results re-
vealed that the proposed method outperforms not
only a baseline system that translates character-
ized source language sentences, but also all SMT
models trained on any of the learned word seg-
mentations. In addition, the proposed method
achieves translation results comparable to SMT
models trained on linguistically segmented bitext.
2 Word Segmentation
The word segmentation method proposed in this
paper is an unsupervised, language-independent
approach that treats the task of word segmentation
as a phrase-boundary tagging task. This method
uses a parallel text corpus consisting of initially
unigram segmented source language character se-
quences and whitespace-separated target language
words. The initial bitext is used to train a stan-
dard phrase-based SMT system (SMTchr). Thecharacter-to-word alignment results of the SMT
training procedure1 are exploited to identify suc-
cessive source language characters aligned to the
same target language word in the respective bitext
and to merge these characters into larger transla-
tion units, defining its granularity in the given bi-
text context.
The obtained translation units are then used to
learn the word segmentation that is most consis-
tent with the phrase alignments of the given SMT
system. First, each character of the source lan-
guage text is annotated with a word-boundary in-
1For the experiments presented in Section 3, the GIZA++
toolkit was used.
dicator where only two tags are used, i.e, ?E?
(end-of-word character tag) and ?I? (in-word
character tag). The annotations are derived from
the SMT training corpus as described in Figure 1.
(1) proc annotate-phrase-boundaries( Bitext ) ;
(2) begin
(3) for each (Src, Trg) in {Bitext} do
(4) A? align(Src, Trg) ;
(5) for each i in {1, . . . , len(Src)-1} do
(6) Trgi ? get-target(Src[i], A) ;(7) Trgi+1 ? get-target(Src[i+1], A) ;(8) if null(Trgi) or Trgi 6= Trgi+1 then(9) (? aligned to none or different target ?)
(10) SrcME ? assign-tag(Src[i],? E?) ;(11) else
(12) (? aligned to the same target ?)
(13) SrcME ? assign-tag(Src[i],? I ?) ;(14) fi ;
(15) CorpusME ? add(SrcME) ;(16) od ;
(17) (? last source token ?)
(18) LastSrcME ? assign-tag(Src[len(Src)],? E?) ;(19) CorpusME ? add(LastSrcME) ;(20) od ;
(21) return( CorpusME ) ;(22) end ;
Figure 1: ME Training Data Annotation
Using these alignment-based word boundary
annotations, a Maximum-Entropy (ME) method is
applied to learn the word segmentation consistent
with the SMT translation model (see Section 2.1),
to resegment the original source language corpus,
and to retrain a phrase-based SMT engine that will
hopefully achieve a better translation performance
than the initial SMT engine. This process should
be repeated as long as an improvement in transla-
tion quality is achieved. Eventually, the concate-
nation of succeeding translation units will result in
overfitting, i.e., the newly created token can only
be translated in the context of rare training data ex-
amples. Therefore, a lower translation quality due
to an increase of untranslatable source language
phrases is to be expected (see Section 2.2).
However, in order to increase the coverage and
to reduce the translation task complexity of the
statistical models, the proposed method integrates
multiple segmentation schemes into the statistical
translation models of a single SMT engine so that
longer translation units are preferred for transla-
tion, if available, and smaller translation units can
be used otherwise (see Section 2.3).
2.1 Maximum-Entropy Tagging Model
ME models provide a general purpose machine
learning technique for classification and predic-
402
Lexical Context Features < t0, w?2 > < t0, w?1 >
< t0, w0 >
< t0, w+1 > < t0, w+2 >
Tag Context Features < t0, t?1 > < t0, t?1, t?2 >
Table 1: Feature Set of ME Tagging Model
tion. They are versatile tools that can handle
large numbers of features, and have shown them-
selves to be highly effective in a broad range of
NLP tasks including sentence boundary detection
or part-of-speech tagging (Berger et al, 1996).
A maximum entropy classifier is an exponential
model consisting of a number of binary feature
functions and their weights (Pietra et al, 1997).
The model is trained by adjusting the weights to
maximize the entropy of the probabilistic model
given constraints imposed by the training data. In
our experiments, we use a conditional maximum
entropy model, where the conditional probability
of the outcome given the set of features is modeled
(Ratnaparkhi, 1996). The model has the form:
p(t, c) = ?
K
?
k=0
?fk(c,t)k ? p0
where:
t is the tag being predicted;
c is the context of t;
? is a normalization coefficient;
K is the number of features in the model;
fk are binary feature functions;
ak is the weight of feature function fk;
p0 is the default model.
The feature set is given in Table 1. The lexical
context features consist of target words annotated
with a tag t. w0 denotes the word being tagged and
w?2, . . . , w+2 the surrounding words. t0 denotesthe current tag, t?1 the previous tag, etc. The tag
context features supply information about the con-
text of previous tag sequences. This conditional
model can be used as a classifier. The model is
trained iteratively, and we used the improved iter-
ative scaling algorithm (IIS) (Berger et al, 1996)
for the experiments presented in Section 3.
2.2 Iterative Bootstrap Method
The proposed iterative bootstrap method to learn
the word segmentation that is consistent with an
SMT engine is summarized in Figure 2. After
the ME tagging model is learned from the ini-
tial character-to-word alignments of the respec-
tive bitext ((1)-(4)), the obtained ME tagger is
SRC text
TRG text
unigram
segmented SRC
(1)   characterize
evalchrdecodeSMTchr
SRCtokenTRGwordalignment
(3)   extract
ME1classifierIteration 1segmented SRC
(4)
annotate
(5)
resegment
SMT1  eval1  
decode
SMTJ-1
SRCtokenTRGwordalignment
ME2classifier
(7)   extract
(8)
annotate
Iteration 2
segmented SRC
(9)
resegment
?
evalJ-1decode
SRCtokenTRGwordalignment
MEJ-1classifier
extract
Iteration J-1
segmented SRC
SMTJ evalJ
(6) train
(2) train
(J-1) train
(J) train
?
?
Selected Word
Segmenter
?
better
better
worse
Figure 2: Iterative Bootstrap Method
applied to resegment the source language side of
the unsegmented parallel text corpus ((5)). This
results in a resegmented bitext that can be used
to retrain and reevaluate another engine SMT1((6)), achieving what is hoped to be a better trans-
lation performance than the initial SMT engine
(SMTchr).
The unsupervised ME tagging method can also
be applied to the token-to-word alignments ex-
tracted during the training of the SMT1 engineto obtain an ME tagging model ME1 capable ofhandling longer translation units ((7)-(8)). Such
a bootstrap method iteratively creates a sequence
of SMT engines SMTi ((9)-(J)), each of whichreduces the translation complexity, because larger
chunks can be translated in a single step leading
to fewer word order or word disambiguation er-
rors. However, at some point, the increased length
of translation units learned from the training cor-
pus will lead to overfitting, resulting in reduced
translation performance when translating unseen
sentences. Therefore, the bootstrap method stops
when the J th resegmentation of the training cor-
pus results in a lower automatic evaluation score
for the unseen sentences than the one for the previ-
ous iteration. The ME tagging model MEJ?1 thatachieved the highest automatic translation scores
is then selected as the best single-iteration word
segmenter.
2.3 Integration of Multiple Segmentations
The integration of multiple word segmentation
schemes is carried out by merging the transla-
tion models of the SMT engines trained on the
characterized and iteratively learned segmentation
schemes. This process is performed by linearly in-
terpolating the model probabilities of each of the
403
models. In our experiments, equal weights were
used; however, it might be interesting to inves-
tigate varying the weights according to iteration
number, as the latter iterations may contain more
useful segmentations.
In addition, we also remove the internal seg-
mentation of the source phrases. The advantages
are twofold. Primarily it allows decoding directly
from unsegmented text. Moreover, the segmenta-
tion of the source phrase can differ between mod-
els at differing iterations; removing the source seg-
mentation at this stage makes the phrase pairs in
the translations models at various stages in the it-
erative process consistent with one another. Con-
sequently, duplicate bilingual phrase pairs appear
in the phrase table. These duplicates are combined
by normalizing their model probabilities prior to
model interpolation.
The rescored translation model covers all trans-
lation pairs that were learned by any of the
iterative models. Therefore, the selection of
longer translation units during decoding can re-
duce the complexity of the translation task. On the
other hand, overfitting problems of single-iteration
models can be avoided because multiple smaller
source language translation units can be exploited
to cover the given input parts and to generate trans-
lation hypotheses based on the concatenation of
associated target phrase expressions. Moreover,
the merging process increases the translation prob-
abilities of the source/target translation parts that
cover the same surface string but differ only in
the segmentation of the source language phrase.
Therefore, the more often such a translation pair is
learned by different iterative models, the more of-
ten the respective target language expression will
be exploited by the SMT decoder.
The translation of unseen data using the merged
translation models is carried out by (1) character-
izing the input text and (2) applying the SMT de-
coding in a standard way.
3 Experiments
The effects of using different word segmentations
and integrating them into an SMT engine are in-
vestigated using the multilingual Basic Travel Ex-
pressions Corpus (BTEC), which is a collection
of sentences that bilingual travel experts consider
useful for people going to or coming from other
countries (Kikui et al, 2006). For the word seg-
mentation experiments, we selected five Asian
languages that do not naturally separate word
BTEC train set dev set test set
# of sen 160,000 1,000 1,000
en voc 15,390 1,262 1,292
len 7.5 7.1 7.2
ja voc 17,168 1,407 1,408
len 8.5 8.2 8.2
ko voc 17,246 1,366 1,365
len 8.0 7.7 7.8
th voc 7,354 1,081 1,053
len 7.8 7.3 7.4
zh voc 11,084 1,312 1,301
len 7.1 6.4 6.5
Table 2: Language Resources
units, i.e., Japanese (ja), Korean (ko), Thai (th),
and two dialects of Chinese (Standard Mandarin
(zh) and Taiwanese Mandarin (tw)).
Table 2 summarizes the characteristics of the
BTEC corpus used for the training (train) of the
SMT models, the tuning of model weights and
stop conditions of the iterative bootstrap method
(dev), and the evaluation of translation quality
(test). Besides the number of sentences (sen)
and the vocabulary (voc), the sentence length
(len) is also given as the average number of
words per sentence. The given statistics are ob-
tained using commonly-used linguistic segmenta-
tion tools available for the respective language,
i.e., CHASEN (ja), WORDCUT (th), ICTCLAS
(zh), HanTagger (ko). No segmentation was avail-
able for Taiwanese Mandarin and therefore no
meaningful statistics could be obtained.
For the training of the SMT models, standard
word alignment (Och and Ney, 2003) and lan-
guage modeling (Stolcke, 2002) tools were used.
Minimum error rate training (MERT) was used to
tune the decoder?s parameters and performed on
the dev set using the technique proposed in (Och
and Ney, 2003). For the translation, a multi-stack
phrase-based decoder was used.
For the evaluation of translation quality, we ap-
plied standard automatic metrics, i.e., BLEU (Pap-
ineni et al, 2002) and METEOR (Lavie and Agar-
wal, 2007). We have tested the statistical signif-
cance of our results2 using the bootstrap method
reported in (Zhang et al, 2004) that (1) performs a
random sampling with replacement from the eval-
uation data set, (2) calculates the evaluation metric
score of each engine for the sampled test sentences
and the difference between the two MT system
scores, (3) repeats the sampling/scoring step itera-
22000 iterations were used for the analysis of the auto-
matic evaluation results in this paper. All reported differences
in evaluation scores are statistically significant.
404
tively, and (4) applies the Student?s t-test at a sig-
nificance level of 95% confidence to test whether
the score differences are significant.
In addition, human assessment of translation
quality was carried out using the Ranking metrics.
For the Ranking evaluation, a human grader was
asked to ?rank each whole sentence translation
from Best to Worst relative to the other choices
(ties are allowed)? (Callison-Burch et al, 2007).
The Ranking scores were obtained as the average
number of times that a system was judged better
than any other system and the normalized ranks
(NormRank) were calculated on a per-judge ba-
sis for each translation task using the method of
(Blatz et al, 2003).
Section 3.1 compares the proposed method to
the baseline system that translates characterized
source language sentences and to the SMT en-
gines that are trained on iteratively learned as well
as language-dependent linguistic word segmenta-
tions. The effects of the iterative learning method
are summarized in Section 3.2.
3.1 Effects of Word Segmentation
The automatic evaluation scores of the SMT en-
gines trained on the differently segmented source
language resources are given in Table 3, where
?character? refers to the baseline system of using
character-segmented source text; ?single-best?3 is
the SMT engine that is trained on the corpus seg-
mented by the best-performing iteration of the
bootstrap approach; ?proposed? is the SMT engine
whose models integrate multiple word segmen-
tation schemes; and ?linguistic? uses language-
dependent linguistically motivated word segmen-
tation tools. The reported scores are calculated as
the mean score of all metric scores obtained for the
iterative sampling method used for statistical sig-
nificance testing and listed as percentage figures.
The results show that the proposed method out-
performs the character (single-best) system for
each of the involved languages achieving gains
of 2.0 to 9.1 (0.4 to 1.6) BLEU points and 2.0
to 5.9 (0.7 to 4.6) METEOR points, respectively.
However, the improvements depend on the source
language. For example, the smallest gains were
obtained for Standard Mandarin, because single
characters frequently form words of their own,
thus resulting in more ambiguity than Japanese,
3This approximates the approach of (Ma and Way, 2009)
and is given as a way of showing the effect of segmentation
at multiple levels of granularity.
where consecutive hiragana or katakana charac-
ters can form larger meaningful units.
Comparing the proposed method towards lin-
guistically motivated segmenters, the results show
that the proposed method outperforms the SMT
engines using linguistic segmentation tools for
tasks such as translating Korean and Standard
Mandarin into English. Slightly lower evaluation
scores were achieved for the automatically learned
word segmentation for Japanese, although the re-
sults of the proposed method are quite similar.
This is a suprisingly strong result, given the ma-
turity of the linguistically motivated segmenters,
and given that our segmenters use only the bilin-
gual corpus used to train the SMT systems.
The Thai-English experiments expose some is-
sues that are related to the definition of what
a ?character? is. Our segmentation schemes
are learned directly from the bitext without any
language-specific information, and can cope well
with most languages. However, Thai seems to be
an exceptional case in our experiments, because
(1) the Thai script is a segmental writing system
which is based on consonants but in which vowel
notation is obligatory, so that the characterization
of the baseline system affects vowel dependen-
cies, (2) it uses tone markers that are placed above
the consonant, but are treated as a single charac-
ter in our approach, and (3) vowels sounding after
a consonant are non-sequential and can occur be-
fore, after, above, or below a consonant increasing
the number of word form variations in the training
corpus and reducing the accuracy of the learned
ME tagging models. This is an interesting result
that motivates further study on how to incorpo-
rate features on language scripts into our machine
learning framework. For example, Japanese is
written in three different scripts (kanji, hiragana,
katakana). Therefore, the script class of each char-
acter could be used as an additional feature to ob-
tain the initial segmentation of the training corpus.
Finally, the results for Taiwanese Mandarin,
where no linguistic tool was available to segment
the source language text, shows that the proposed
method can be applied successfully for the trans-
lation of any language where no linguistically-
motivated segmentation tools are available.
Table 4 summarizes the subjective evaluation
results which were carried out by a paid evalua-
tion expert who is a native speaker of English. The
NormRank results confirm the findings of the au-
405
BLEU
source word segmentation
language character single-best proposed linguistic
ja 36.93 39.65 41.25 41.46
ko 34.72 37.32 38.51 37.19
th 41.42 50.16 50.53 56.68
zh 36.59 37.02 38.61 38.13
tw 45.71 50.95 52.21 ?
METEOR
source word segmentation
language character single-best proposed linguistic
ja 59.78 60.95 65.45 66.03
ko 58.45 60.06 64.31 63.04
th 67.22 71.22 72.58 79.02
zh 61.77 62.38 63.80 62.72
tw 70.14 73.64 74.38 ?
Table 3: Automatic Evaluation
NormRank
source word segmentation
language character single-best proposed linguistic
ja 2.76 2.85 3.18 3.12
ko 2.68 2.90 3.17 3.09
th 2.65 2.95 3.05 3.43
zh 2.87 3.01 3.07 3.04
tw 2.83 2.86 3.24 ?
Table 4: Subjective Evaluation
tomatic evaluation. In addition, for Japanese, the
translation outputs of the proposed method were
judged better than those of the linguistically seg-
mented SMT model.
3.2 Effects of Bootstrap Iteration
In order to get an idea of the robustness of the pro-
posed method, the changes in system performance
for each source language during the iterative boot-
strap method is given in Figure 3. The results for
BLEU and METEOR show that all languages reach
their best performance after the first or second it-
eration and then slightly, but consistently decrease
with the increased number of iterations. The rea-
son for this is the effect of overfitting caused by
the concatenation of source tokens that are aligned
to longer target phrases, resulting in the segmenta-
tion of longer translation units.
The changes in the vocabulary size and the word
length are summarized in Figure 4. The amount of
words extracted by the proposed method is much
larger than the one of the baseline system, increas-
ing the vocabulary size by a factor of 10 for Stan-
dard Mandarin and Taiwanese Mandarin, 30 for
Japanese and Korean, and 100 for Thai. It is also
larger than the vocabulary obtained for the linguis-
tic tools by a factor of 1.5 to 2.5 for all investigated
Change in BLEU
20.00
30.00
40.00
50.00
60.00
iteration
BL
EU
 (%
)  

 
 


 
Change in METEOR
40.00
50.00
60.00
70.00
80.00
iteration
ME
TE
OR
 (%
)  	

 
 


 
chr 1        2       3      4         5      6      7        8 9       10
chr 1       2       3       4        5      6      7        8  9       10
Figure 3: Change in System Performance
Change in Vocabulary Size
0
20000
40000
60000
80000
100000
120000
iteration
Vo
cab
ula
ry 
Siz
e  
 
 


 
chr 1        2       3      4         5      6      7        8 9       10
Change in Average Vocabulary Length
0.0
5.0
10.0
15.0
20.0
25.0
30.0
iteration
Vo
cab
ula
ry 
Len
gth 

Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48?52,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Abstract
The system presented in this paper uses a 
combination of two techniques to directly 
transliterate from grapheme to grapheme. The 
technique makes no language specific as-
sumptions, uses no dictionaries or explicit 
phonetic information; the process transforms 
sequences of tokens in the source language 
directly into to sequences of tokens in the 
target.  All the language pairs in our experi-
ments were transliterated by applying this 
technique in a single unified manner. The 
approach we take is that of hypothesis re-
scoring to integrate the models of two state-
of-the-art techniques: phrase-based statistical 
machine translation (SMT), and a joint multi-
gram model. The joint multigram model was 
used to generate an n-best list of translitera-
tion hypotheses that were re-scored using the 
models of the phrase-based SMT system. The 
both of the models? scores for each hypothesis 
were linearly interpolated to produce a final 
hypothesis score that was used to re-rank the 
hypotheses. In our experiments on develop-
ment data,  the combined system was able to 
outperform both of its component systems 
substantially.  
1 Introduction
In statistical machine translation the re-scoring 
of hypotheses produced by a system with addi-
tional models that  incorporate information not 
available to the original system has been shown 
to be an effective technique to improve system 
performance (Paul et al, 2006). Our approach 
uses a re-scoring technique to integrate the 
models of two transliteration systems that are 
each capable in their own right: a phrase-based 
statistical machine translation system (Koehn et 
al., 2003), and a joint  multigram model (Deligne 
and Bimbot, 1995; Bisani and Ney, 2008). 
In this work we treat the process of translit-
eration as a process of direct  transduction from 
sequences of tokens in the source language to 
sequences of tokens in the target language with 
no modeling of the phonetics of either source or 
target  language (Knight and Graehl, 1997). Tak-
ing this approach allows for a very general 
transliteration system to be built  that does not 
require any language specific knowledge to be 
incorporated into the system (for some language 
pairs this may not be the best strategy since lin-
guistic information can be used to overcome 
issues of data sparseness on smaller datasets).  
2 Component Systems
For this shared task we chose to combine two 
systems through a process of re-scoring. The 
systems were selected because of their expected 
strong level of performance (SMT systems have 
been used successfully in the field, and joint 
multigram models have performed well both in 
grapheme to phoneme conversion and Arabic-
English transliteration). Secondly, the joint mul-
tigram model relies on key features not present 
in the SMT system, that is the history of bilin-
gual phrase pairs used to derive the target. For 
this reason we felt the systems would comple-
ment each other well. We now briefly describe 
the component systems.
2.1 Joint Multigram Model
The joint  multigram approach proposed by (De-
ligne and Bimbot, 1995) has arisen as an exten-
sion of the use of variable-length n-grams (mul-
tigrams) in language modeling. In a joint  multi-
gram, the units in the model consist of multiple 
input  and output symbols. (Bisani and Ney, 
2008) refined the approach and applied to it 
grapheme-to-phoneme conversion, where its 
performance was shown to be comparable to 
state-of-the-art systems. The approach was later 
applied to Arabic-English transliteration (Dese-
laers et al, 2009) again with promising results.
Joint multigram models have the following 
characteristics:
?
The symbols in the source and target are 
co-segmented
Transliteration using a Phrase-based Statistical Machine Translation 
System to Re-score the Output of a Joint Multigram Model 
Andrew Finch
NICT
3-5 Hikaridai
Keihanna Science City
619-0289 JAPAN
andrew.finch@nict.go.jp
Eiichiro Sumita
NICT
3-5 Hikaridai
Keihanna Science City
619-0289 JAPAN
eiichiro.sumita@nict.go.jp
48
-Maximum likelihood training using an 
EM algorithm (Deligne and Bimbot, 
1995)
?
The probability of sequences of joint mul-
tigrams is modeled using an n-gram 
model
In these respects the model can be viewed as 
a close relative of the joint source channel 
model proposed by (Li et  al., 2004) for translit-
eration.
2.2 Phrase-based SMT
It  is possible to view the process of translitera-
tion as a process of translation at the character 
level, without  re-ordering. From this perspective 
it is possible to directly employ a phrase-based 
SMT  system in the task of transliteration (Finch 
and Sumita, 2008; Rama and Gali, 2009). A 
phrase-based SMT system has the following 
characteristics:
?
The symbols in the source and target are 
aligned one to many in both directions. 
Joint sequences of source and target sym-
bols are heuristically extracted given 
these alignments
?
Transliteration is performed using a log-
linear model with weights tuned on de-
velopment data
?
The models include: a translation model 
(with 5 sub-models), and a target lan-
guage model
The bilingual phrase-pairs are analogous to 
the joint  multigrams, however the translation 
model of the SMT system doesn?t use the con-
text of previously translated phrase-pairs, in-
stead relying on a target language model.
3 Experimental Conditions
3.1 SMT Decoder
In our experiments we used an in-house phrase-
based statistical machine translation decoder 
called CleopATRa. This decoder operates on 
exactly the same principles as the publicly 
available MOSES decoder (Koehn et al, 2003). 
Our decoder was modified to be able to decode 
source sequences with reference to a target se-
quence; the decoding process being forced to 
generate the target. The decoder was also con-
figured to combine scores of multiple deriva-
tions yielding the same target  sequence. In this 
way the models in the decoder were used to de-
rive scores used to re-score the n-best (we used 
n=20 for our experiments) hypotheses generated 
by the joint  multigram model. The phrase-
extraction process was symmetrized with re-
spect  to token order using the technique pro-
posed in (Finch and Sumita, 2010). In order to 
adapt  the SMT system to the task of translitera-
tion, the decoder was constrained decode in a 
monotone manner, and furthermore during train-
ing, the phrase extraction process was con-
strained such that  only phrases with monotone 
order were extracted in order to minimize the 
effects of errors in the word alignment process.
In a final step the scores from both systems 
were linearly interpolated to produce a single 
integrated hypothesis score. The hypotheses 
were then re-ranked according to this integrated 
score for the final submission.
3.2 Joint Multigram model
For the joint  multigram system we used the pub-
licly available Sequitur G2P  grapheme-to-
phoneme converter (Bisani and Ney, 2008). The 
system was used with its default settings, and 
pilot experiments were run on development  data 
to determine appropriate settings for the maxi-
mum size of the multigrams. The results for the 
English-to-Japanese task are shown in Figure 1. 
As can be seen in the figure, the system rapidly 
improves to a near-optimal value with a maxi-
mum multigram size of 4. No improvement  at 
all was observed for sizes over 7. We therefore 
chose a maximum multigram size of 8 for the 
experiments presented in this paper, and for the 
systems entered in the shared task.
3.3 Pre-processing
In order to reduce data sparseness we took the 
decision to work with data in only its lowercase 
form.
We  chose not  to perform any tokenization or 
phonetic mapping for any of the language pairs 
Figure 1: The effect on F-score by tuning with 
respect to joint multigram size
0.3
0.4
0.6
0.7
0.9
1 2 3 4 5 6 7 8 9 10
F
-
S
c
o
r
e
Joint Multigram Size
49
in the shared task. We adopted this approach 
because:
?
It  allowed us to have a single unified 
approach for all language pairs
?
It  was in the spirit  of the shared task, as 
it did not  require extra knowledge out-
side of the supplied corpora
3.4 Handling Multi-Word Sequences
The data for some languages contained some 
multi-word sequences. To handle these we had 
to consider the following strategies:
?
Introduce a <space> token into the se-
quence, and treat  it  as one long charac-
ter sequence to transliterate; or
?
Segment the word sequences into indi-
vidual words and transliterate these in-
dependently, combining the n-best hy-
pothesis lists for all the individual words 
in the sequence into a single output se-
quence.
 We adopted both approaches: for those multi-
word sequences where the number of words in 
the source and target matched, the latter ap-
proach was taken; for those where the numbers 
of source and target words differed, the former 
approach was taken. The decoding process for 
multi-word sequences is shown in Figure 2. 
During recombination, the score for the target 
word sequence was calculated as the product of 
the scores of each hypothesis for each word. 
Therefore a search over all combinations of hy-
potheses is required. In almost all cases we were 
able to perform a full search. For the rare long 
word sequences in the data, a beam search strat-
egy was adopted.
3.5 Building the Models
For the final submissions, all systems were 
trained on the union of the training data and de-
velopment data. It was felt that the training set 
was sufficiently small that  the inclusion of the 
development  data into the training set  would 
yield a reasonable boost  in performance by in-
creasing the coverage of the systems. All tunable 
parameters were tuned on development data us-
ing systems built  using only the training data. 
Under the assumption that  these parameters 
would perform well in the systems trained on 
the combined development/training corpora, 
these tuned parameters were transferred directly 
to the systems trained on all available data.
3.6 Parameter Tuning
The SMT  systems were tuned using the mini-
mum error rate training procedure introduced in 
(Och, 2003). For convenience, we used BLEU 
as a proxy for the various metrics used in the 
shared task evaluation. The BLEU score is 
commonly used to evaluate the performance of 
Figure 2: The transliteration process for multi-word sequences
Word 1 Word 2 Word m
Segment into individual words and transliterate each word independently
T
r
a
n
s
l
i
t
e
r
a
t
e
T
r
a
n
s
l
i
t
e
r
a
t
e
T
r
a
n
s
l
i
t
e
r
a
t
e
n-best
hypothesis 1
hypothesis 2
...
hypothesis n
n-best
hypothesis 1
hypothesis 2
...
hypothesis n
n-best
hypothesis 1
hypothesis 2
...
hypothesis n
Search for the best path
Figure 3: The effect on the F-score of the integrated 
system by tuning with respect to the SMT system?s 
interpolation weight
0.83
0.84
0.85
0 0.2 0.4 0.6 0.8 1.0
F
-
S
c
o
r
e
SMT System Interpolation Weight
50
machine translation systems and is a function of 
the geometric mean of n-gram precision. The 
use of BLEU score as a proxy has been shown 
to be a reasonable strategy for the metrics used 
in these experiments (Finch and Sumita, 2009). 
Nonetheless, it is reasonable to assume that  one 
would be able to improve the performance in a 
particular evaluation metric by doing minimum 
error rate training specifically for that metric. 
The interpolation weight  was tuned by a grid 
search to find the value that gave the maximal f-
score (according to the official f-score evalua-
tion metric for the shared task) on the develop-
ment data, the process for English-Japanese is 
shown in Figure 3.
4 Results
The results of our experiments are shown in Ta-
ble 1. These results are the official shared task 
evaluation results on the test  data, and the scores 
for all of the evaluation metrics are shown in the 
table. The reader is referred to the workshop 
white paper (Li et al, 2010) for details of the 
evaluation metrics. The system achieved a high 
level of performance on most of the language 
pairs. Comparing the individual systems to each 
other, and to the integrated system, the joint 
multigram system outperformed the phrase-
based SMT  system. In experiments run on the 
English-to-Japanese katakana task, the joint 
multigram system in isolation achieved an F-
score of 0.837 on development data, whereas the 
SMT  system in isolation achieved an F-score of 
0.824. When integrated the models of the sys-
tems complemented each other well, and on the 
same English-Japanese task the integrated sys-
tem achieved an F-score of 0.843.
We feel that for some language pairs, most 
notably Arabic-English where a large difference 
existed between our system and the top-ranked 
system, there is much room for improvement. 
One of the strengths in terms of the utility of our 
approach is that it  is free from dependence on 
the linguistic characteristics of the languages 
being processed. This property makes it  gener-
ally applicable, but due to the limited amounts 
of data available for the shared task, we believe 
that in order to progress, a language-dependent 
approach will be required.
5 Conclusion
We applied a system that  integrated two state-of-
the-art  systems through a process of re-scoring, 
to the NEWS 2010 Workshop shared task on 
transliteration generation. Our systems gave a 
strong performance on the shared task test  set, 
and our experiments show the integrated system 
was able to outperform both of its component 
systems. In future work we would like to depart 
from the direct grapheme-to-grapheme approach 
taken here and address the problem of how best 
to represent  the source and target  sequences by 
either analyzing their symbols further, or ag-
glomerating them. We would also like to inves-
tigate the use of co-segmentation schemes that 
do not rely on maximum likelihood training to 
overcome the issues inherent in this technique. 
Acknowledgements
The results presented in this paper draw on the 
following data sets. For English-Japanese and 
Arabic-English, the reader is referred to the CJK 
website: http://www.cjk.org. For English-Hindi, 
English-Tamil, and English-Kannada, and 
English-Bangla the data sets originated from the 
work of Kumaran and Kellner, 2007.
Language Pair
Accuracy in 
top-1
Mean 
F-score
MRR MAP
ref
English ? Thai 0.412 0.883 0.550 0.412
Thai ? English 0.397 0.873 0.525 0.397
English ? Hindi 0.445 0.884 0.574 0.445
English ? Tamil 0.390 0.887 0.522 0.390
English ? Kannada 0.371 0.871 0.506 0.371
English ? Japanese 0.378 0.783 0.510 0.378
Arabic ? English 0.403 0.891 0.512 0.327
English ? Bangla 0.412 0.883 0.550 0.412
Table 1: The results of our system in the official evaluation on the test data on all performance metrics. 
51
References
Peter Brown, Stephen Della Pietra, Vincent Della 
Pietra, and Robert Mercer, 1991. The mathematics 
of statistical machine translation: parameter esti-
mation. Computational Linguistics,  19(2), 263-
311. 
Sabine Deligne, and Fr?d?ric Bimbot, 1995.  Lan-
guage modeling by variable length sequences: 
theoretical formulation and evaluation of multi-
grams. In: Proc. IEEE Internat. Conf. on Acous-
tics, Speech and Signal Processing, Vol. 1, Detroit, 
MI, USA, pp. 169?172.
Maximilian Bisani and Hermann Ney, 2008. Joint-
Sequence Models for Grapheme-to-Phoneme 
Conversion. Speech Communication, Volume 50, 
Issue 5, Pages 434-451.
Thomas Deselaers, Sasa Hasan, Oliver Bender, and 
Hermann Ney, 2009.  A Deep Learning Approach 
to Machine Transliteration.  In Proceedings of the 
EACL 2009 Workshop on Statistical Machine 
Translation (WMT09), Athens, Greece.
Andrew Finch and Eiichiro Sumita, 2008. Phrase-
based machine transliteration.  In Proceedings of 
WTCAST'08, pages 13-18.
Andrew Finch and Eiichiro Sumita,  2009. Translit-
eration by Bidirectional Statistical Machine Trans-
lation, Proceedings of the 2009 Named Entities 
Workshop: Shared Task on Transliteration, Singa-
pore.
Andrew Finch and Eiichiro Sumita, 2010.  Exploiting 
Directional Asymmetry in Phrase-table Generation 
for Statistical Machine Translation,  In Proceed-
ings of NLP2010, Tokyo, Japan.
Kevin Knight and Jonathan Graehl, 1997. Machine 
Transliteration.  Proceedings of the Thirty-Fifth 
Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the 
European Chapter of the Association for Compu-
tational Linguistics, pp. 128-135, Somerset,  New 
Jersey. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu, 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Human Language Technology 
Conference 2003 (HLT-NAACL 2003), Edmonton, 
Canada.
Franz Josef Och, 2003. Minimum error rate training 
for statistical machine translation, Proceedings of 
the ACL.
A Kumaran and Tobias Kellner, 2007. A generic 
framework for machine transliteration, Proc. of 
the 30th SIGIR.
Haizhou Li, Min Zhang, Jian Su, 2004.  A joint source 
channel model for machine transliteration, Proc. 
of the 42nd ACL.
Haizhou Li, A Kumaran, Min Zhang and Vladimir 
Pervouchine, 2010. Whitepaper of NEWS 2010 
Shared Task on Transliteration Generation. In 
Proc. of ACL2010 Named Entities Workshop.
Michael Paul, Eiichiro Sumita and Seiichi Yama-
moto,  2006, Multiple Translation-Engine-based 
Hypotheses and Edit-Distance-based Rescoring 
for a Greedy Decoder for Statistical Machine 
Translation, Information and Media Technologies, 
Vol. 1, No. 1, pp.446-460 .
Taraka Rama and Karthik Gali, 2009. Modeling ma-
chine transliteration as a phrase based statistical 
machine translation problem, Proceedings of the 
2009 Named Entities Workshop: Shared Task on 
Transliteration, Singapore.
52
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 28?33,
COLING 2010, Beijing, August 2010.
Syntactic Constraints on Phrase Extraction for Phrase-Based 
Machine Translation 
Hailong Cao, Andrew Finch and Eiichiro Sumita 
Language Translation Group, MASTAR Project 
National Institute of Information and Communications Technology 
{hlcao,andrew.finch,eiichiro.sumita }@nict.go.jp
 
Abstract 
A typical phrase-based machine transla-
tion (PBMT) system uses phrase pairs 
extracted from word-aligned parallel 
corpora. All phrase pairs that are consis-
tent with word alignments are collected. 
The resulting phrase table is very large 
and includes many non-syntactic phrases 
which may not be necessary. We propose 
to filter the phrase table based on source 
language syntactic constraints. Rather 
than filter out all non-syntactic phrases, 
we only apply syntactic constraints when 
there is phrase segmentation ambiguity 
arising from unaligned words. Our 
method is very simple and yields a 
24.38% phrase pair reduction and a 0.52 
BLEU point improvement when com-
pared to a baseline PBMT system with 
full-size tables. 
1 Introduction 
Both PBMT models (Koehn et al, 2003; Chiang, 
2005) and syntax-based machine translation 
models (Yamada et al, 2000; Quirk et al, 2005; 
Galley et al, 2006; Liu et al, 2006; Marcu et al, 
2006; and numerous others) are the state-of-the- 
art statistical machine translation (SMT) meth-
ods. Over the last several years, an increasing 
amount of work has been done to combine the 
advantages of the two approaches. DeNeefe et al 
(2007) made a quantitative comparison of the 
phrase pairs that each model has to work with 
and found it is useful to improve the phrasal 
coverage of their string-to-tree model. Liu et al 
(2007) proposed forest-to-string rules to capture 
the non-syntactic phrases in their tree-to-string 
model. Zhang et al (2008) proposed a tree se-
quence based tree-to-tree model which can de-
scribe non-syntactic phrases with syntactic struc-
ture information. 
The converse of the above methods is to in-
corporate syntactic information into the PBMT 
model. Zollmann and Venugopal (2006) started 
with a complete set of phrases as extracted by 
traditional PBMT heuristics, and then annotated 
the target side of each phrasal entry with the la-
bel of the constituent node in the target-side 
parse tree that subsumes the span. Marton and 
Resnik (2008) and Cherry (2008) imposed syn-
tactic constraints on the PBMT system by mak-
ing use of prior linguistic knowledge in the form 
of syntax analysis. In their PBMT decoders, a 
candidate translation gets an extra credit if it re-
spects the source side syntactic parse tree but 
may incur a cost if it violates a constituent 
boundary. Xiong et al (2009) proposed a syn-
tax-driven bracketing model to predict whether a 
phrase (a sequence of contiguous words) is 
bracketable or not using rich syntactic con-
straints. 
In this paper, we try to utilize syntactic 
knowledge to constrain the phrase extraction 
from word-based alignments for PBMT system. 
Rather than filter out all non-syntactic phrases, 
we only apply syntactic constraints when there is 
phrase segmentation ambiguity arising from un-
aligned words. Our method is very simple and 
yields a 24.38% phrase pair reduction and a 0.52 
BLEU point improvement when compared to the 
baseline PBMT system with full-size tables. 
2 Extracting Phrase Pairs from Word-
based Alignments 
In this section, we briefly review a simple and 
effective phrase pair extraction algorithm upon 
which this work builds. 
28
The basic translation unit of a PBMT model is 
the phrase pair, which consists of a sequence of 
source words, a sequence of target words and a 
vector of feature values which represents this 
pair?s contribution to the translation model. In 
typical PBMT systems such as MOSES (Koehn, 
2007), phrase pairs are extracted from word-
aligned parallel corpora. Figure 1 shows the 
form of training example. 
 
 
 
 
 
Figure 1: An example parallel sentence pair 
and word alignment 
 
Since there is no phrase segmentation infor-
mation in the word-aligned sentence pair, in 
practice all pairs of ?source word sequence ||| 
target word sequence? that are consistent with 
word alignments are collected. The words in a 
legal phrase pair are only aligned to each other, 
and not to words outside (Och et al, 1999). For 
example, given a sentence pair and its word 
alignments shown in Figure1, the following nine 
phrase pairs will be extracted: 
 
Source phrase ||| Target phrase 
f1 ||| e1 
f2 ||| e2 
f4 ||| e3 
f1 f2 ||| e1 e2 
f2 f3 ||| e2 
f3 f4 ||| e3 
f1 f2  f3 ||| e1 e2 
f2 f3 f4 ||| e2 e3 
f1 f2 f3 f4 ||| e1 e2 e3 
 
Table 1: Phrase pairs extracted from the example 
in Figure 1 
 
Note that neither the source phrase nor the 
target phrase can be empty. So ?f3 ||| EMPTY? is 
not a legal phrase pair. 
Phrase pairs are extracted over the entire 
training corpus. Given all the collected phrase 
pairs, we can estimate the phrase translation 
probability distribution by relative frequency. 
The collected phrase pairs will also be used to 
build the lexicalized reordering model. For more 
details of the lexicalized reordering model, 
please refer to Tillmann and Zhang (2005) and 
section 2.7.2 of the MOSES?s manual1. 
The main problem of such a phrase pair ex-
traction procedure is the resulting phrase transla-
tion table is very large, especially when a large 
quantity of parallel data is available. This is not 
desirable in real application where speed and 
memory consumption are often critical concerns. 
In addition, some phrase translation pairs are 
generated from training data errors and word 
alignment noise. Therefore, we need to filter the 
phrase table in an appropriate way for both effi-
ciency and translation quality (Johnson et al, 
2007; Yang and Zheng, 2009).  
  f1        f2      f3   f4 
        |           |               | 
  e1        e2            e3 
 
3 Syntactic Constraints on Phrase Pair 
Extraction 
We can divide all the possible phrases into two 
types: syntactic phrases and non-syntactic 
phrases. A ?syntactic phrase? is defined as a 
word sequence that is covered by a single sub-
tree in a syntactic parse tree (Imamura, 2002). 
Intuitively, we would think syntactic phrases are 
much more reliable while the non-syntactic 
phrases are useless. However, (Koehn et al, 
2003) showed that restricting phrasal translation 
to only syntactic phrases yields poor translation 
performance ? the ability to translate non-
syntactic phrases (such as ?there are?, ?note 
that?, and ?according to?) turns out to be critical 
and pervasive. 
 (Koehn et al, 2003) uses syntactic constraints 
from both the source and target languages, and 
over 80% of all phrase pairs are eliminated. In 
this section, we try to use syntactic knowledge in 
a less restrictive way.  
Firstly, instead of using syntactic restriction 
on both source phrases and target phrases, we 
only apply syntactic restriction to the source 
language side. 
Secondly, we only apply syntactic restriction 
to the source phrase whose first or last word is 
unaligned. 
For example, given a parse tree illustrated in 
Figure 2, we will filter out the phrase pair ?f2 f3 
||| e2? since the source phrase ?f2 f3? is a non-
syntactic phrase and its last word ?f3? is not 
                                                 
1 http://www.statmt.org/moses/ 
29
aligned to any target word. The phrase pair ?f1 
f2  f3 ||| e1 e2? will  also be eliminated for the 
same reason. But we do keep phrase pairs such 
as ?f1 f2 ||| e1 e2? even if its source phrase ?f1 
f2? is a non-syntactic phrase. Also, we keep ?f3 
f4 ||| e3? since ?f3 f4? is a syntactic phrase. Ta-
ble 2 shows the completed set of phrase pairs 
that are extracted with our constraint-based 
method. 
 
Source phrase ||| Target phrase 
f1 ||| e1 
f2 ||| e2 
f4 ||| e3 
f1 f2 ||| e1 e2 
f3 f4 ||| e3 
f2 f3 f4 ||| e2 e3 
f1 f2 f3 f4 ||| e1 e2 e3 
 
Table 2: Phrase pairs extracted from the example 
in Figure 2 
 
 
 
Figure 2: An example parse tree and word-
based alignments 
 
The state-of-the-art alignment tool such as 
GIZA++ 2  can not always find alignments for 
every word in the sentence pair. The possible 
reasons could be: its frequency is too low, noisy 
data, auxiliary words or function words which 
have no obvious correspondence in the opposite 
language. 
In the automatically aligned parallel corpus, 
unaligned words are frequent enough to be no-
ticeable (see section 4.1 in this paper). How to 
decide the translation of unaligned word is left to 
the phrase extraction algorithm. An unaligned 
                                                 
2 http://fjoch.com/GIZA++.html 
source word should be translated together with 
the words on the right of it or the words on the 
left of it. The existing algorithm considers both 
of the two directions. So both ?f2 f3 ||| e2? and 
?f3 f4 ||| e3? are extracted. However, it is 
unlikely that ?f3? can be translated into both 
?e2? and ?e3?.  So our algorithm uses prior syn-
tactic knowledge to keep ?f3 f4 ||| e3? and ex-
clude ?f2 f3 ||| e2?. 
4 Experiments 
Our SMT system is based on a fairly typical 
phrase-based model (Finch and Sumita, 2008). 
For the training of our SMT model, we use a 
modified training toolkit adapted from the 
MOSES decoder. Our decoder can operate on 
the same principles as the MOSES decoder. 
Minimum error rate training (MERT) with re-
spect to BLEU score is used to tune the de-
coder?s parameters, and it is performed using the 
standard technique of Och (2003). A lexicalized 
reordering model was built by using the ?msd-
bidirectional-fe? configuration in our experi-
ments. 
The translation model was created from the 
FBIS parallel corpus. We used a 5-gram lan-
guage model trained with modified Kneser-Ney 
smoothing. The language model was trained on 
the target side of the FBIS corpus and the Xin-
hua news in the GIGAWORD corpus. The de-
velopment and test sets are from the NIST MT08 
evaluation campaign. Table 3 shows the statis-
tics of the corpora used in our experiments. 
N3 
N2
N1 
  f1        f2      f3   f4 
 
  e1       e2             e3 
 
 
Data Sentences Chinese words 
English 
words 
Training set 221,994 6,251,554 8,065,629 
Development set 1,664 38,779 46,387 
Test set 1,357 32,377 42,444 
GIGAWORD 19,049,757 - 306,221,306
 
Table 3: Corpora statistics 
 
The Chinese sentences are segmented, POS 
tagged and parsed by the tools described in Kru-
engkrai et al (2009) and Cao et al (2007), both 
of which are trained on the Penn Chinese Tree-
bank 6.0.  
30
4.1 Experiments on Word Alignments 
We use GIZA++ to align the sentences in both 
the Chinese-English and English-Chinese direc-
tions. Then we combine the alignments using the 
standard ?grow-diag-final-and? procedure pro-
vided with MOSES. 
In the combined word alignments, 614,369 or 
9.82% of the Chinese words are unaligned. Ta-
ble 4 shows the top 10 most frequently un-
aligned words. Basically, these words are auxil-
iary words or function words whose usage is 
very flexible. So it would be difficult to auto-
matically align them to the target words.  
 
Unaligned word Frequency 
? 77776 
, 29051 
? 9414 
? 8768 
? 8543 
? 7471 
? 7365 
? 6155 
? 5945 
? 5450 
 
Table 4: Frequently unaligned words from the 
training corpus 
4.2 Experiments on Chinese-English SMT 
In order to confirm that it is advantageous to 
apply appropriate syntactic constraints on phrase 
extraction, we performed three translation ex-
periments by using different ways of phrase ex-
traction.  
In the first experiment, we used the method 
introduced in Section 2 to extract all possible 
phrase translation pairs without using any con-
straints arising from knowledge of syntax.  
The second experiment used source language 
syntactic constraints to filter out all non-
syntactic phrases during phrase pair extraction. 
The third experiment used source language 
syntactic constraints to filter out only non-
syntactic phrases whose first or last source word 
was unaligned.  
With the exception of the above differences in 
phrase translation pair extraction, all the other 
settings were the identical in the three 
experiments. Table 5 summarizes the SMT per-
formance. The evaluation metric is case-
sensitive BLEU-4 (Papineni et al, 2002) which 
estimates the accuracy of translation output with 
respect to a set of reference translations. 
 
Syntactic Con-
straints 
Number of 
distinct phrase pairs BLEU
None 14,195,686 17.26
Full constraint 4,855,108 16.51
Selectively 
constraint 10,733,731 17.78
 
Table 5: Comparison of different constraints on 
phrase pair extraction by translation quality 
 
As shown in the table, it is harmful to fully 
apply syntactic constraints on phrase extraction, 
even just on the source language side. This is 
consistent with the observation of (Koehn et al, 
2003) who applied both source and target con-
straints in German to English translation ex-
periments. 
Clearly, we obtained the best performance if 
we use source language syntactic constraints 
only on phrases whose first or last source word 
is unaligned. In addition, we reduced the number 
of distinct phrase pairs by 24.38% over the base-
line full-size phrase table. 
The results in table 5 show that while some 
non-syntactic phrases are very important to 
maintain the performance of a PBMT system, 
not all of them are necessary. We can achieve 
better performance and a smaller phrase table by 
applying syntactic constraints when there is 
phrase segmentation ambiguity arising from un-
aligned words. 
5 Related Work 
To some extent, our idea is similar to Ma et al 
(2008), who used an anchor word alignment 
model to find a set of high-precision anchor 
links and then aligned the remaining words rely-
ing on dependency information invoked by the 
acquired anchor links. The similarity is that both 
Ma et al (2008) and this work utilize structure 
information to find appropriate translations for 
words which are difficult to align. The differ-
31
ence is that they used dependency information in 
the word alignment stage while our method uses 
syntactic information during the phrase pair ex-
traction stage. There are also many works which 
leverage syntax information to improve word 
alignments (e.g., Cherry and Lin, 2006; DeNero 
and Klein, 2007; Fossum et al, 2008; Hermja-
kob, 2009). 
Johnson et al, (2007) presented a technique 
for pruning the phrase table in a PBMT system 
using Fisher?s exact test. They compute the sig-
nificance value of each phrase pair and prune the 
table by deleting phrase pairs with significance 
values smaller than a certain threshold. Yang 
and Zheng (2008) extended the work in Johnson 
et al, (2007) to a hierarchical PBMT model, 
which is built on synchronous context free 
grammars (SCFG). Tomeh et al, (2009) de-
scribed an approach for filtering phrase tables in 
a statistical machine translation system, which 
relies on a statistical independence measure 
called Noise, first introduced in (Moore, 2004). 
The difference between the above research and 
this work is they took advantage of some statis-
tical measures while we use syntactic knowledge 
to filter phrase tables. 
6 Conclusion and Future Work 
Phrase pair extraction plays a very important 
role on the performance of PBMT systems. We 
utilize syntactic knowledge to constrain the 
phrase extraction from word-based alignments 
for a PBMT system. Rather than filter out all 
non-syntactic phrases, we only filter out non-
syntactic phrases whose first or last source word 
is unaligned. Our method is very simple and 
yields a 24.38% phrase pair reduction and a 0.52 
BLEU point improvement when compared to the 
baseline PBMT system with full-size tables. 
In the future work, we will use other language 
pairs to test our phrase extraction method so that 
we can discover whether or not it is language 
independent. 
References 
Robert C. Moore. 2004. On log-likelihood-ratios and 
the significance of rare events. In EMNLP. 
Hailong Cao, Yujie Zhang and Hitoshi Isahara. Em-
pirical study on parsing Chinese based on Collins' 
model. 2007. In PACLING. 
Colin Cherry and Dekang Lin. 2006. Soft syntactic 
constraints for word alignment through discrimina-
tive training. In ACL. 
Colin Cherry. 2008. Cohesive phrase-Based decoding 
for statistical machine translation. In ACL-HLT. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In ACL. 
Steve DeNeefe, Kevin Knight, Wei Wang, and 
Daniel Marcu. 2007. What can syntax-based MT 
learn from phrase-based MT? In EMNLP-CoNLL. 
John DeNero and Dan Klein. 2007. Tailoring word 
alignments to syntactic machine translation. In 
ACL. 
Andrew Finch and Eiichiro Sumita. 2008. Dynamic 
model interpolation for statistical machine transla-
tion. In SMT Workshop. 
Victoria Fossum, Kevin Knight and Steven Abney. 
2008. Using syntax to improve word alignment 
precision for syntax-based machine translation. In 
SMT Workshop, ACL. 
Michel Galley, Jonathan Graehl, Kevin Knight, 
Daniel Marcu, Steve Deneefe, Wei Wang and 
Ignacio Thayer. 2006. Scalable inference and 
training of context-rich syntactic translation mod-
els. In ACL. 
Ulf Hermjakob. 2009. Improved word alignment with 
statistics and linguistic heuristics. In EMNLP. 
Kenji Imamura. 2002. Application of translation 
knowledge acquired by hierarchical phrase align-
ment for pattern-based MT. In TMI. 
Howard Johnson, Joel Martin, George Foster and 
Roland Kuhn. 2007. Improving translation quality 
by discarding most of the phrase table. In EMNLP-
CoNLL. 
Franz Josef Och, Christoph Tillmann and Hermann 
Ney. 1999. Improved alignment models for statis-
tical machine translation. In EMNLP-VLC. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In ACL. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In HLT-
NAACL. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine 
Moran, Richard Zens, Chris Dyer, Ondrej Bojar, 
Alexandra Constantin, Evan Herbst. 2007. Moses: 
Open Source Toolkit for Statistical Machine 
Translation. In ACL demo and poster sessions. 
32
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun'ichi 
Kazama, Yiou Wang, Kentaro Torisawa and Hito-
shi Isahara. 2009. An error-driven word-character 
hybrid model for joint Chinese word segmentation 
and POS tagging. In ACL-IJCNLP. 
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In ACL-COLING. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-string statistical translation rules. 
In ACL. 
Yanjun Ma, Sylwia Ozdowska, Yanli Sun and Andy 
Way. 2008. Improving word alignment using syn-
tactic dependencies. In SSST. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi, 
and Kevin Knight. 2006. SPMT: Statistical ma-
chine translation with syntactified target language 
phrases. In EMNLP. 
Yuval Marton and Philip Resnik. 2008. Soft syntactic 
constraints for hierarchical phrased-based transla-
tion. In ACL-HLT. 
Kishore Papineni, Salim Roukos, Todd Ward and 
WeiJing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In ACL. 
Chris Quirk and Arul Menezes and Colin Cherry. 
2005. Dependency treelet translation: Syntactically 
informed phrasal SMT. In ACL. 
Christoph Tillmann and Tong Zhang. 2005. A local-
ized prediction model for statistical machine trans-
lation. In ACL. 
Nadi Tomeh, Nicola Cancedda and Marc Dymetman. 
2009. Complexity-based phrase-table filtering for 
statistical machine translation. In MT Summit. 
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li. 
2009. A syntax-driven bracketing model for 
phrase-based translation. In ACL-IJCNLP. 
Kenji Yamada and Kevin Knight. 2000. A syntax-
based statistical translation model. In ACL. 
Mei Yang and Jing Zheng. 2009. Toward smaller, 
faster, and better hierarchical phrase-based SMT. 
In ACL. 
Min Zhang, Hongfei Jiang, Aiti Aw, Chew Lim Tan 
and Sheng Li. 2008. A tree sequence alignment-
based tree-to-tree translation model. In ACL- HLT. 
Andreas Zollmann and Ashish Venugopal. 2006. 
Syntax augmented machine translation via chart 
parsing. In SMT Workshop, HLT-NAACL. 
33
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1?9,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Dialect Translation:
Integrating Bayesian Co-segmentation Models with Pivot-based SMT
Michael Paul and Andrew Finch and Paul R. Dixon and Eiichiro Sumita
National Institute of Information and Communications Technology
MASTAR Project
Kyoto, Japan
michael.paul@nict.go.jp
Abstract
Recent research on multilingual statistical ma-
chine translation (SMT) focuses on the usage
of pivot languages in order to overcome re-
source limitations for certain language pairs.
This paper proposes a new method to translate
a dialect language into a foreign language by
integrating transliteration approaches based
on Bayesian co-segmentation (BCS) models
with pivot-based SMT approaches. The ad-
vantages of the proposed method with respect
to standard SMT approaches are three fold:
(1) it uses a standard language as the pivot lan-
guage and acquires knowledge about the re-
lation between dialects and the standard lan-
guage automatically, (2) it reduces the transla-
tion task complexity by using monotone de-
coding techniques, (3) it reduces the num-
ber of features in the log-linear model that
have to be estimated from bilingual data. Ex-
perimental results translating four Japanese
dialects (Kumamoto, Kyoto, Okinawa, Os-
aka) into four Indo-European languages (En-
glish, German, Russian, Hindi) and two Asian
languages (Chinese, Korean) revealed that
the proposed method improves the translation
quality of dialect translation tasks and outper-
forms standard pivot translation approaches
concatenating SMT engines for the majority
of the investigated language pairs.
1 Introduction
The translation quality of SMT approaches heavily
depends on the amount and coverage of the bilin-
gual language resources available to train the statis-
tical models. There are several data collection ini-
tiatives1 amassing and distributing large amounts of
textual data. For frequently used language pairs like
French-English, large-sized text data sets are read-
ily available. However, for less frequently used lan-
guage pairs, only a limited amount of bilingual re-
sources are available, if any at all.
In order to overcome language resource limi-
tations, recent research on multilingual SMT fo-
cuses on the use of pivot languages (de Gispert and
Marino, 2006; Utiyama and Isahara, 2007; Wu and
Wang, 2007; Bertoldi et al, 2008; Koehn et al,
2009). Instead of a direct translation between two
languages where only a limited amount of bilingual
resources is available, the pivot translation approach
makes use of a third language that is more appropri-
ate due to the availability of more bilingual corpora
and/or its relatedness to the source/target language.
In most of the previous research, English has been
the pivot language of choice due to the richness of
available language resources. However, recent re-
search on pivot translation has shown that the usage
of non-English pivot languages can improve trans-
lation quality of certain language pairs, especially
when translating from or into Asian languages (Paul
et al, 2009).
This paper focuses on the translation of dialects,
i.e., a variety of a language that is characteristic of
a particular group of the language?s speakers, into
a foreign language. A standard dialect (or stan-
dard language) is a dialect that is recognized as
the ?correct? spoken and written form of the lan-
guage. Dialects typically differ in terms of mor-
phology, vocabulary and pronunciation. Various
1LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info
1
methods have been proposed to measure relatedness
between dialects using phonetic distance measures
(Nerbonne and Heeringa, 1997), string distance al-
gorithms (Heeringa et al, 2006; Scherrer, 2007), or
statistical models (Chitturi and Hansen, 2008).
Concerning data-driven natural language process-
ing (NLP) applications like machine translation
(MT), however, linguistic resources and tools usu-
ally are available for the standard language, but not
for dialects. In order to create dialect language re-
sources, previous research utilized explicit knowl-
edge about the relation between the standard lan-
guage and the dialect using rule-based and statistical
models (Habash et al, 2005; Sawaf, 2010). In addi-
tion, applying the linguistic tools for the standard
language to dialect resources is often insufficient.
For example, the task of word segmentation, i.e.,
the identification of word boundaries in continuous
text, is one of the fundamental preprocessing steps
of MT applications. In contrast to Indo-European
languages like English, many Asian languages like
Japanese do not use a whitespace character to sep-
arate meaningful word units. However, the applica-
tion of a linguistically motivated standard language
word segmentation tool to a dialect corpus results
in a poor segmentation quality due to morphological
differences in verbs and adjectives, thus resulting in
a lower translation quality for SMT systems that ac-
quire the translation knowledge automatically from
a parallel text corpus (Paul et al, 2011).
This paper differs from previous research in the
following aspects:
? it reduces the data sparseness problem of di-
rect translation approaches by translating a
resource-limited dialect language into a foreign
language by using the resource-rich standard
language as the pivot language.
? it is language independent and acquires knowl-
edge about the relation between the standard
language and the dialect automatically.
? it avoids segmentation mismatches between the
input and the translation model by mapping the
characterized dialect language, i.e., each char-
acter is treated as a single token, to the word
segmentation of the standard language using a
Bayesian co-segmentation model.
? it reduces the translation task complexity by us-
ing monotone decoding techniques.
? it reduces the number of features in the log-
linear model that have to be estimated from
bilingual data.
The details of the proposed dialect translation
method are described in Section 2. Experiments
were carried out for the translation of four Japanese
dialects (Kumamoto, Kyoto, Okinawa, Osaka) into
four Indo-European languages (English, German,
Russian, Hindi) and two Asian languages (Chinese,
Korean). The utilized language resources and the
outline of the experiments are summarized in Sec-
tion 3. The results reveal that the integration of
Bayesian co-segmentation models with pivot-based
SMT improves the translation quality of dialect to
foreign language translation tasks and that the pro-
posed system outperforms standard pivot translation
approaches concatenating SMT engines that trans-
late the dialect into the standard language and the
standard language MT output into the foreign lan-
guage for the majority of the investigated language
pairs.
2 Dialect Translation
Spoken language translation technologies attempt to
bridge the language barriers between people with
different native languages who each want to engage
in conversation by using their mother-tongue. For
standard languages, multilingual speech translation
services like the VoiceTra2 system for travel conver-
sations are readily available. However, such tech-
nologies are not capable of dealing with dialect lan-
guages due to the lack of language resources and the
high development costs of building speech transla-
tion components for a large number of dialect varia-
tions.
In order to reduce such problems, the dialect
translation method proposed in this paper integrates
two different methods of transducing a given dialect
input sentence into a foreign language. In the first
step, the close relationship between the local and
standard language is exploited to directly map char-
acter sequences in the dialect input to word seg-
ments in the standard language using a Bayesian co-
2http://mastar.jp/translation/voicetra-en.html
2
segmentation approach, details of which are given in
Section 2.1. The proposed transliteration method is
described in Section 2.2. The advantages of the pro-
posed Bayesian co-segmentation approach are two
fold: it reduces the translation complexity and it
avoids segmentation inconsistencies between the in-
put and the translation models. In the second step,
a state-of-the-art phrase-based SMT system trained
on a large amount of bilingual data is applied to ob-
tain high-quality foreign language translations as de-
scribed in Section 2.3.
2.1 Bayesian Co-segmentation
The method for mapping the dialect sentences into
the standard language word segments is a direct
character-to-character mapping between the lan-
guages. This process is known as translitera-
tion. Many transliteration methods have previously
been proposed, including methods based on string-
similarity measures between character sequences
(Noeman and Madkour, 2010) or generation-based
models (Lee and Chang, 2003; Tsuji and Kageura,
2006; Jiampojamarn et al, 2010).
In this paper, we use a generative Bayesian model
similar to the one from (DeNero et al, 2008) which
offers several benefits over standard transliteration
techniques: (1) the technique has the ability to train
models whilst avoiding over-fitting the data, (2)
compact models that have only a small number of
well-chosen parameters are constructed, (3) the un-
derlying generative transliteration model is based on
the joint source-channel model (Li et al, 2004), and
(4) the model is symmetric with respect to source
and target language. Intuitively, the model has two
basic components: a model for generating an out-
come that has already been generated at least once
before, and a second model that assigns a probabil-
ity to an outcome that has not yet been produced.
Ideally, to encourage the re-use of model parame-
ters, the probability of generating a novel bilingual
sequence pair should be considerably lower then the
probability of generating a previously observed se-
quence pair. The probability distribution over these
bilingual sequence pairs (including an infinite num-
ber of unseen pairs) can be learned directly from un-
labeled data by Bayesian inference of the hidden co-
segmentation of the corpus.
The co-segmentation process is driven by a
Dirichlet process, which is a stochastic process de-
fined over a set S (in our case, the set of all pos-
sible bilingual sequence pairs) whose sample path
is a probability distribution on S. The underlying
stochastic process for the generation of a corpus
composed of bilingual phrase pairs (sk,tk) can be
written in the following form:
G|?,G0 ? DP (?,G0)
(sk, tk)|G ? G (1)
G is a discrete probability distribution over all
the bilingual sequence pairs according to a Dirichlet
process prior with a base measure G0 and concen-
tration parameter ?. The concentration parameter
? > 0 controls the variance of G; intuitively, the
larger ? is, the more similar G0 will be to G.
For the base measure that controls the genera-
tion of novel sequence pairs, we use a joint spelling
model that assigns probability to new sequence pairs
according to the following joint distribution:
G0((s, t)) = p(|s|)p(s||s|)? p(|t|)p(t||t|)
= ?
|s|
s
|s|! e
??sv?|s|s ?
?|t|t
|t|! e
??tv?|t|t (2)
where |s| and |t| are the length in characters of
the source and target sides of the bilingual sequence
pair; vs and vt are the vocabulary sizes of the source
and target languages respectively; and ?s and ?t are
the expected lengths3 of the source and target.
According to this model, source and target se-
quences are generated independently: in each case
the sequence length is chosen from a Poisson dis-
tribution, and then the sequence itself is generated
given the length. Note that this model is able to
assign a probability to arbitrary bilingual sequence
pairs of any length in the source and target sequence,
but favors shorter sequences in both.
The generative model is given in Equation 3. The
equation assigns a probability to the kth bilingual
sequence pair (sk, tk) in a derivation of the corpus,
given all of the other sequence pairs in the history so
far (s?k, t?k). Here ?k is read as: ?up to but not
including k?.
p((sk, tk))|(s?k, t?k))
= N((sk, tk)) + ?G0((sk, tk))N + ? (3)
3Following (Xu et al, 2008), we assign the parameters ?s,
?t and ?, the values 2, 2 and 0.3 respectively.
3
Input: Random initial corpus segmentation
Output: Unsupervised co-segmentation of the corpus
according to the model
foreach iter=1 to NumIterations do
foreach bilingual word-pair w ? randperm(W) do
foreach co-segmentation ?i of w do
Compute probability p(?i|h)
where h is the set of data (excluding w) and
its hidden co-segmentation
end
Sample a co-segmentation ?i from the
distribution p(?i|h)
Update counts
end
end
Algorithm 1: Blocked Gibbs Sampling
In this equation, N is the total number of bilingual
sequence pairs generated so far and N((sk, tk)) is
the number of times the sequence pair (sk, tk) has
occurred in the history. G0 and ? are the base mea-
sure and concentration parameter as before.
We used a blocked version of a Gibbs sampler
for training, which is similar to that of (Mochihashi
et al, 2009). We extended their forward filtering
/ backward sampling (FFBS) dynamic programing
algorithm in order to deal with bilingual segmenta-
tions (see Algorithm 1). We found our sampler con-
verged rapidly without annealing. The number of
iterations was set by hand after observing the con-
vergence behavior of the algorithm in pilot experi-
ments. We used a value of 75 iterations through the
corpus in all experiments reported in this paper. For
more details on the Bayesian co-segmentation pro-
cess, please refer to (Finch and Sumita, 2010).
2.2 Dialect to Standard Language
Transduction
A Bayesian segmentation model is utilized to trans-
form unseen dialect sentences into the word seg-
mentation of the standard language by using the
joint-source channel framework proposed by (Li et
al., 2004). The joint-source channel model, also
called the n-gram transliteration model, is a joint
probability model that captures information on how
the source and target sentences can be generated
simultaneously using transliteration pairs, i.e., the
most likely sequence of source characters and tar-
get words according to a joint language model built
from the co-segmentation from the Bayesian model.
Suppose that we have a dialect sentence ? =
l1l2 . . . lL and a standard language sentence ? =
s1s2 . . . sS where li are dialect characters, sj are
word tokens of the standard language, and there
exists an alignment ? =< l1 . . . lq, s1 >, . . . , <
lr . . . lL, sS >, 1 ? q < r ? L of K translitera-
tion units. Then, an n-gram transliteration model is
defined as the transliteration probability of a translit-
eration pair < l, s >k depending on its immediate n
preceding transliteration pairs:
P (?, ?, ?) =
K
?
k=1
P (< l, s >k|< l, s >k?1k?n+1) (4)
For the experiments reported in this paper, we im-
plemented the joint-source channel model approach
as a weighted finite state transducer (FST) using
the OpenFst toolkit (Allauzen et al, 2007). The
FST takes the sequence of dialect characters as its
input and outputs the co-segmented bilingual seg-
ments from which the standard language segments
are extracted.
2.3 Pivot-based SMT
Recent research on speech translation focuses on
corpus-based approaches, and in particular on statis-
tical machine translation (SMT), which is a machine
translation paradigm where translations are gener-
ated on the basis of statistical models whose param-
eters are derived from the analysis of bilingual text
corpora. SMT formulates the problem of translat-
ing a source language sentence src into a target lan-
guage sentence trg as a maximization problem of
the conditional probability:
argmaxtrg p(src|trg) ? p(trg) (5)
where p(src|trg) is called a translation model
(TM ) and represents the generation probability
from trg into src, and p(trg) is called a language
model (LM ) and represents the likelihood of the tar-
get language (Brown et al, 1993). During the trans-
lation process (decoding), a score based on the sta-
tistical model probabilities is assigned to each trans-
lation hypothesis and the one that gives the highest
probability is selected as the best translation.
The translation quality of SMT approaches heav-
ily depends on the amount and coverage of the bilin-
gual language resources available to train the statis-
tical models. In the context of dialect translation,
4
where only few bilingual language resources (if any
at all) are available for the dialect and the foreign
language, only a relatively low translation quality
can be obtained. In order to obtain better transla-
tions, we apply a pivot translation approach. Pivot
translation is the translation from a source language
(SRC) to a target language (TRG) through an inter-
mediate pivot (or bridging) language (PVT). In this
paper, we select the standard language as the pivot
language.
Within the SMT framework, various coupling
strategies like cascading, phrase-table composition,
or pseudo-corpus generation have been proposed.
For the experiments reported in this paper, we uti-
lized the cascading approach because it is compu-
tational less expensive, but still performs compara-
bly well compared to the other pivot translation ap-
proaches. In the first step, the dialect input is tran-
scribed into the standard language as described in
Section 2.1. Next, the obtained standard language
MT output is translated into the target language us-
ing SMT models trained on the much larger lan-
guage resources.
3 Experiments
The effects of integrating Bayesian co-segmentation
models with pivot-based SMT are investigated using
the Basic Travel Expressions Corpus (BTEC), which
is a collection of sentences that bilingual travel ex-
perts consider useful for people traveling abroad
(Kikui et al, 2006). For the dialect translation ex-
periments, we selected Japanese (ja), a language that
does not naturally separate word units, and the di-
alects from the Kumamoto (jaku), Kyoto (jaky), Ok-
inawa (jaok), and Osaka (jaos) areas. All dialects
share the same Japanese writing system that com-
bines logographic Chinese characters and two syl-
labic scripts, i.e., hiragana (used for native Japanese
words) and katakana (used for foreign loanwords
or onomatopoeia). For the target language, we in-
vestigated four Indo-European languages, i.e., En-
glish (en), German (de), Russian (ru), and Hindi
(hi) and two Asian languages, i.e., Chinese (zh)
and Korean (ko). The corpus statistics are summa-
rized in Table 1, where Voc specifies the vocabulary
size and Len the average sentence length of the re-
spective data sets. These languages differ largely
Table 1: Language Resources
Language Voc Len Order Unit Infl
Japanese ja 17,168 8.5 SOV none moderate
English en 15,390 7.5 SVO word moderate
German de 25,716 7.1 SVO word high
Russian ru 36,199 6.4 SVO word high
Hindi hi 33,629 7.8 SOV word high
Chinese zh 13,343 6.8 SVO none light
Korean ko 17,246 8.1 SOV phrase moderate
in word order (Order: subject-object-verb (SOV),
subject-verb-object (SVO)), segmentation unit (Unit:
phrase, word, none), and degree of inflection (Infl:
high, moderate, light). Concerning word segmenta-
tion, the corpora were preprocessed using language-
specific word segmentation tools that are widely-
accepted within the MT community for languages
that do not use white spaces to separate word/phrase
tokens, i.e., CHASEN4 for Japanese and ICTCLAS5
for Chinese. For all other languages, simple to-
kenization tools were applied. All data sets were
case-sensitive with punctuation marks preserved.
The language resources were randomly split into
three subsets for the evaluation of translation quality
(eval, 1k sentences), the tuning of the SMT model
weights (dev, 1k sentences) and the training of the
statistical models (train, 160k sentences). For the
dialect languages, a subset of 20k sentences was
used for the training of translation models for all
of the resource-limited language pairs. In order to
avoid word segmentation errors from the standard
language segmentation tool beeing applied to dialect
resources, these models are trained on bitext, where
the local dialect source sentence is characterized and
the target language is segmented using language-
specific segmentation tools.
For the training of the SMT models, standard word
alignment (Och and Ney, 2003) and language mod-
eling (Stolcke, 2002) tools were used. Minimum
error rate training (MERT) was used to tune the de-
coder?s parameters on the dev set using the technique
proposed in (Och and Ney, 2003). For the trans-
lation, an inhouse multi-stack phrase-based decoder
was used. For the evaluation of translation quality,
we applied the standard automatic evaluation metric
4http://chasen-legacy.sourceforge.jp
5http://www.nlp.org.cn
5
Table 2: SMT-based Direct Translation Quality
BLEU (%)
SRC ja jaku jaky jaok jaos
TRG (160k) (20k) (20k)
en 56.51 32.84 32.27 31.81 30.99 31.97
de 51.73 26.24 25.06 25.71 24.37 25.18
ru 50.34 23.67 23.12 23.19 22.30 22.07
hi 49.99 21.10 20.46 20.40 19.72 20.96
zh 48.59 33.80 32.72 33.15 32.66 32.96
ko 64.52 53.31 52.93 51.24 49.40 51.57
BLEU, which calculates the geometric mean of n-
gram precision by the system output with respect to
reference translations with the addition of a brevity
penalty to punish short sentences. Scores range be-
tween 0 (worst) and 1 (best) (Papineni et al, 2002).
For the experiments reported here, single translation
references were used.
3.1 Direct Translation
Table 2 summarizes the translation performance of
the SMT engines used to directly translate the source
language dialects into the foreign language. For
the large training data condition (160k), the high-
est BLEU scores are obtained for the translation of
Japanese into Korean followed by English, German,
Russian, and Hindi with Chinese seeming to be the
most difficult translation task out of the investigated
target languages. For the standard language (ja), the
translation quality for the small data condition (20k)
that corresponds to the language resources used for
the translation of the dialect languages is also given.
For the Asian target languages, gains of 11%?14%
BLEU points are obtained when increasing the train-
ing data size from 20k to 160k. However, an even
larger increase (24%?27% BLEU points) in trans-
lation quality can be seen for all Indo-European tar-
get languages. Therefore, larger gains are to be
expected when the pivot translation framework is
applied to the translation of dialect languages into
Indo-European languages compared to Asian target
languages. Comparing the evaluation results for the
small training data condition, the highest scores are
achieved for the standard language for all target lan-
guages, indicating the difficulty in translating the di-
alects. Moreover, the Kumamoto dialect seems to be
the easiest task, followed by the Kyoto dialect and
the Osaka dialect. The lowest BLEU scores were
Table 3: SMT-based Pivot Translation Quality
BLEU (%)
SRC jaku jaky jaok jaos
TRG (SMTSRC?ja+SMTja?TRG)
en 52.10 50.66 45.54 49.50
de 47.51 46.33 39.42 44.82
ru 44.59 43.83 38.25 42.87
hi 45.89 44.01 36.87 42.95
zh 45.14 44.26 40.96 44.20
ko 60.76 59.67 55.59 58.62
obtained for the translation of the Okinawa dialect.
3.2 SMT-based Pivot Translation
The SMT engines of Table 2 are then utilized within
the framework of the SMT-based pivot translation
by (1) translating the dialect input into the stan-
dard language using the SMT engines trained on the
20k data sets and (2) translating the standard lan-
guage MT output into the foreign language using
the SMT engines trained on the 160k data sets. The
translation quality of the SMT-based pivot transla-
tion experiments are summarized in Table 3. Large
gains of 6.2%?25.4% BLEU points compared to
the direct translation results are obtained for all in-
vestigated language pairs, showing the effectiveness
of pivot translation approaches for resource-limited
language pairs. The largest gains are obtained for
jaku, followed by jaos, jaky, and jaok. Therefore, the
easier the translation task, the larger the improve-
ments of the pivot translation approach.
3.3 Bayesian Co-segmentation Model
The proposed method differs from the standard pivot
translation approach in that a joint-source channel
transducer trained from a Bayesian co-segmentation
of the training corpus is used to transliterate the di-
alect input into the standard language, as described
in Section 2.2. This process generates the co-
segmented bilingual segments simultaneously in a
monotone way, i.e., the order of consecutive seg-
ments on the source side as well as on the target side
are the same. Similarly, the decoding process of the
SMT approaches can also be carried out monotoni-
cally. In order to investigate the effect of word order
differences for the given dialect to standard language
transduction task, Table 4 compares the transla-
tion performance of SMT approaches with (reorder-
6
Table 4: Dialect to Standard Language Transduction
BLEU (%)
SRC jaku jaky jaok jaos
Engine (decoding) (SRC?ja)
BCS (monotone) 91.55 86.74 80.36 85.04
SMT (monotone) 88.39 84.87 74.27 82.86
(reordering) 88.39 84.73 74.26 82.66
ing) and without (monotone) distortion models to
the monotone Bayesian co-segmentation approach
(BCS). Only minor differences between SMT decod-
ing with and without reordering are obtained. This
shows that the grammatical structure of the dialect
sentences and the standard language sentences are
very similar, thus justifying the usage of monotone
decoding strategies for the given task. The compari-
son of the SMT-based and the BCS-based transduc-
tion of the dialect sentences into the standard lan-
guage shows that the Bayesian co-segmentation ap-
proach outperforms the SMT approach significantly,
gaining 1.9% / 2.2% / 3.2% / 6.1% BLEU points for
jaky / jaos / jaku / jaok, respectively.
3.4 BCS-based Pivot Translation
The translation quality of the proposed method,
i.e. the integration of the Bayesian co-segmentation
models into the pivot translation framework, are
given in Table 5. The overall gains of the proposed
method compared to (a) the direct translation ap-
proach (see Table 2) and (b) the SMT-based pivot
translation approach (see Table 3) are summarized in
Table 6. The results show that the BCS-based pivot
translation approach also largely outperforms the
direct translation approach, gaining 5.9%?25.3%
BLEU points. Comparing the two pivot translation
approaches, the proposed BCS-based pivot transla-
tion method gains up to 0.8% BLEU points over
the concatenation of SMT engines for the Indo-
European target languages, but is not able to im-
prove the translation quality for translating into Ko-
rean and Chinese. Interestingly, the SMT-based
pivot translation approach seems to be better for lan-
guage pairs where only small relative gains from the
pivot translation approach are achieved when trans-
lating the dialect into a foreign language. For exam-
ple, Korean is a language closely related to Japanese
and the SMT models from the small data condition
already seem to cover enough information to suc-
Table 5: BCS-based Pivot Translation Quality
BLEU (%)
SRC jaku jaky jaok jaos
TRG (BCSSRC?ja+SMTja?TRG)
en 52.42 50.68 45.58 50.22
de 47.52 46.74 39.93 45.60
ru 45.29 44.08 38.39 43.53
hi 45.72 44.71 37.60 43.56
zh 45.15 43.92 40.15 44.06
ko 60.26 59.14 55.33 58.13
Table 6: Gains of BCS-based Pivot Translation
BLEU (%)
SRC jaku jaky jaok jaos
TRG on SMT-based Pivot (Direct) Translation
en +0.32 +0.02 +0.04 +0.72
(+20.15) (+18.87) (+14.59) (+18.25)
de +0.01 +0.41 +0.51 +0.78
(+22.46) (+21.03) (+15.56) (+20.50)
ru +0.70 +0.25 +0.14 +0.66
(+22.17) (+20.89) (+16.09) (+21.46)
hi -0.17 +0.70 +0.73 +0.61
(+25.26) (+24.31) (+17.88) (+22.60)
zh +0.01 -0.34 -0.81 -0.14
(+12.43) (+10.77) (+7.49) (+11.10)
ko -0.50 -0.53 -0.26 -0.49
(+7.33) (+7.90) (+5.93) (+6.56)
cessfully translate the dialect languages into Korean.
In the case of Chinese, the translation quality for
even the large data condition SMT engines is rela-
tively low. Therefore, improving the quality of the
standard language input might have only a small im-
pact on the overall pivot translation performance, if
any at all. On the other hand, the proposed method
can be successfully applied for the translation of lan-
guage pairs where structural differences have a large
impact on the translation quality. In such a transla-
tion task, the more accurate transduction of the di-
alect structure into the standard language can affect
the overall translation performance positively.
4 Conclusion
In this paper, we proposed a new dialect transla-
tion method for resource-limited dialect languages
within the framework of pivot translation. In the first
step, a Bayesian co-segmentation model is learned
to transduce character sequences in the dialect sen-
tences into the word segmentation of the standard
7
language. Next, an FST-based joint-source channel
model is applied to unseen dialect input sentences to
monotonically generate co-segmented bilingual seg-
ments from which the standard language segments
are extracted. The obtained pivot sentence is then
translated into the foreign language using a state-of-
the-art phrase-based SMT engine trained on a large
corpus.
Experiments were carried out for the translation
of four Japanese dialects into four Indo-European
as well as into two Asian languages. The re-
sults revealed that the Bayesian co-segmentation
method largely improves the quality of the stan-
dard language sentence generated from a dialect in-
put compared to SMT-based translation approaches.
Although significant improvements of up to 0.8%
in BLEU points are achieved for certain target
languages, such as all of the investigated Indo-
European languages, it is difficult to transfer the
gains obtained by the Bayesian co-segmentation
model to the outcomes for the pivot translation
method.
Further research will have to investigate features
like language relatedness, structural differences,
and translation model complexity to identify indica-
tors of translation quality that could enable the selec-
tion of BCS-based vs. SMT-based pivot translation
approaches for specific language pairs to improve
the overall system performance further.
In addition we would like to investigate the ef-
fects of using the proposed method for translating
foreign languages into dialect languages. As the
Bayesian co-segmentation model is symmetric with
respect to source and target language, we plan to
reuse the models learned for the experiments pre-
sented in this paper and hope to obtain new insights
into the robustness of the Bayesian co-segmentation
method when dealing with noisy data sets like ma-
chine translation outputs.
Acknowledgments
This work is partly supported by the Grant-in-Aid
for Scientific Research (C) Number 19500137.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Open-
Fst: A General and Efficient Weighted Finite-State
Transducer Library. In Proc. of the 9th Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Nicola Bertoldi, Madalina Barbaiani, Marcello Federico,
and Roldano Cattoni. 2008. Phrase-Based statistical
machine translation with Pivot Languages. In Proc. of
the 5th International Workshop on Spoken Language
Translation (IWSLT), pages 143?149, Hawaii, USA.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
tical machine translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?311.
Ragul Chitturi and John Hansen. 2008. Dialect Clas-
sification for online podcasts fusing Acoustic and
Language-based Structural and Semantic Information.
In Proc. of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics - Human Language
Technologies (ACL-HLT), Companion Volume, pages
21?24, Columbus, USA.
Adria de Gispert and Jose B. Marino. 2006. Catalan-
English statistical machine translation without paral-
lel corpus: bridging through Spanish. In Proc. of 5th
International Conference on Language Resources and
Evaluation (LREC), pages 65?68, Genoa, Italy.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling Alignment Structure under a
Bayesian Translation Model. In Proc. of Conference
on Empirical Methods on Natural Language Process-
ing (EMNLP), Hawaii, USA.
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Transliteration.
In Proc. of the 7th International Workshop on Spoken
Language Translation (IWSLT), pages 259?266, Paris,
France.
Nizar Habash, Owen Rambow, and George Kiraz. 2005.
Morphological Analysis and Generation for Arabic
Dialects. In Proc. of the ACL Workshop on Computa-
tional Approaches to Semitic Languages, pages 17?24,
Ann Arbor, USA.
Wilbert Heeringa, Peter Kleiweg, Charlotte Gosskens,
and John Nerbonne. 2006. Evaluation of String Dis-
tance Algorithms for Dialectology. In Proc. of the
Workshop on Linguistic Distances, pages 51?62, Syd-
ney, Australia.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,
Aditya Bhargava, Qing Dou, Mi-Young Kim, and
Grzegorz Kondrak. 2010. Transliteration Generation
and Mining with Limited Training Resources. In Proc.
of the 2010 Named Entities Workshop (NEWS), pages
39?47, Uppsala, Sweden.
8
Genichiro Kikui, Seiichi Yamamoto, Toshiyuki
Takezawa, and Eiichiro Sumita. 2006. Compar-
ative study on corpora for speech translation. IEEE
Transactions on Audio, Speech and Language,
14(5):1674?1682.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 Machine Translation Systems for Europe.
In Proc. of the MT Summit XII, Ottawa, Canada.
Chun-Jen Lee and Jason S. Chang. 2003. Acqui-
sition of English-Chinese transliterated word pairs
from parallel-aligned texts using a statistical machine
transliteration model. In Proc. of the HLT-NAACL
2003 Workshop on Building and using parallel texts,
Volume 3, pages 96?103, Edmonton, Canada.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration. In
Proc. of the 42nd ACL, pages 159?166, Barcelona,
Spain.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proc of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP), pages 100?108, Suntec, Singapore.
John Nerbonne and Wilbert Heeringa. 1997. Measur-
ing Dialect Distance Phonetically. In Proc. of the ACL
Special Interest Group in Computational Phonology,
pages 11?18, Madrid, Spain.
Sara Noeman and Amgad Madkour. 2010. Language
Independent Transliteration Mining System Using Fi-
nite State Automata Framework. In Proc. of the 2010
Named Entities Workshop (NEWS), pages 57?61, Up-
psala, Sweden.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th An-
nual Meeting on Association for Computational Lin-
guistics (ACL), pages 311?318, Philadelphia, USA.
Michael Paul, Hirofumi Yamamoto, Eiichiro Sumita, and
Satoshi Nakamura. 2009. On the Importance of Pivot
Language Selection for Statistical Machine Transla-
tion. In Proc. of the North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies (NAACL HLT), pages 221?
224, Boulder, USA.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2011. Word Segmentation for Dialect Translation.
LNCS Lectures Note in Computer Science, Springer,
6609:55?67.
Hassan Sawaf. 2010. Arabic Dialect Handling in Hybrid
Machine Translation. In Proc. of the 9th Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA), Denver, USA.
Yves Scherrer. 2007. Adaptive String Distance Mea-
sures for Bilingual Dialect Lexicon Induction. In Proc.
of the ACL Student Research Workshop, pages 55?60,
Prague, Czech Republic.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proc. of the International Con-
ference on Spoken Language Processing (ICSLP), Vol-
ume 2, pages 901?904, Denver, USA.
Keita Tsuji and Kyo Kageura. 2006. Automatic gen-
eration of JapaneseEnglish bilingual thesauri based
on bilingual corpora. J. Am. Soc. Inf. Sci. Technol.,
57:891?906.
Masao Utiyama and Hitoshi Isahara. 2007. A Compari-
son of Pivot Methods for Phrase-Based Statistical Ma-
chine Translation. In Proc. of Human Language Tech-
nologies (HLT), pages 484?491, New York, USA.
Hua Wu and Haifeng Wang. 2007. Pivot Language Ap-
proach for Phrase-Based Statistical Machine Transla-
tion. In Proc. of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
856?863, Prague, Czech Republic.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann
Ney. 2008. Bayesian semi-supervised Chinese word
segmentation for Statistical Machine Translation. In
Proc. of the 22nd International Conference on Com-
putational Linguistics (COLING), pages 1017?1024,
Manchester, United Kingdom.
9
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 47?51,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
5HVFRULQJ D 3KUDVHEDVHG 0DFKLQH 7UDQVOLWHUDWLRQ 6\VWHP ZLWK 5HFXUUHQW
1HXUDO 1HWZRUN /DQJXDJH 0RGHOV
$QGUHZ )LQFK
1,&7
 +LNDULGDL
.HLKDQQD 6FLHQFH &LW\
 -$3$1
M/`2rX7BM+?!MB+iX;QXDT
3DXO 'L[RQ
1,&7
 +LNDULGDL
.HLKDQQD 6FLHQFH &LW\
 -$3$1
TmHX/BtQM!MB+iX;QXDT
(LLFKLUR 6XPLWD
1,&7
 +LNDULGDL
.HLKDQQD 6FLHQFH &LW\
 -$3$1
2BB+?B`QXbmKBi!MB+iX;QXDT
$EVWUDFW
7KH V\VWHP HQWHUHG LQWR WKLV \HDU?V VKDUHG
WUDQVOLWHUDWLRQ HYDOXDWLRQ LV LPSOHPHQWHG
ZLWKLQ D SKUDVHEDVHG VWDWLVWLFDO PDFKLQH
WUDQVOLWHUDWLRQ 607 IUDPHZRUN 7KH V\VWHP
LV EDVHG RQ D MRLQW VRXUFHFKDQQHO PRGHO LQ
FRPELQDWLRQ ZLWK D WDUJHW ODQJXDJH PRGHO DQG
PRGHOV WR FRQWURO WKH OHQJWK RI WKH VHTXHQFHV
JHQHUDWHG 7KH MRLQW VRXUFHFKDQQHO PRGHO
ZDV WUDLQHG XVLQJ D PDQ\WRPDQ\ %D\HVLDQ
ELOLQJXDO DOLJQPHQW 7KH IRFXV RI WKLV \HDU?V
V\VWHP LV RQ LQSXW UHSUHVHQWDWLRQ ,Q RUGHU DW
WHPSW WR PLWLJDWH GDWD VSDUVHQHVV LVVXHV LQ WKH
MRLQW VRXUFHFKDQQHO PRGHO ZH DXJPHQWHG WKH
V\VWHP ZLWK UHFXUUHQW QHXUDO QHWZRUN 511
PRGHOV WKDW FDQ OHDUQ WR SURMHFW WKH JUDSKHPH
VHW RQWR D VPDOOHU KLGGHQ UHSUHVHQWDWLRQ :H
SHUIRUPHG H[SHULPHQWV RQ GHYHORSPHQW GDWD
WR HYDOXDWH WKH HIIHFWLYHQHVV RI RXU DSSURDFK
2XU UHVXOWV VKRZ WKDW XVLQJ DQ 511 ODQJXDJH
PRGHO FDQ LPSURYH SHUIRUPDQFH IRU ODQJXDJH
SDLUV ZLWK ODUJH JUDSKHPH VHWV RQ WKH WDUJHW
VLGH
 ,QWURGXFWLRQ
2XU V\VWHP IRU WKH 1(:6 VKDUHG HYDOXDWLRQ RQ
WUDQVOLWHUDWLRQ JHQHUDWLRQ LV EDVHG RQ WKH V\VWHP HQ
WHUHG LQWR ODVW \HDUV HYDOXDWLRQ )LQFK HW DO 
6RPH PLQRU LPSURYHPHQWV KDYH EHHQ PDGH WR VRPH
RI WKH FRPSRQHQWV EXW WKH PDMRU GLIIHUHQFH LV WKH
DGGLWLRQ RI D UHVFRULQJ VWHS ZLWK WKUHH UHVFRULQJ
PRGHOV DQ 511 WDUJHW ODQJXDJH PRGHO DQ 511
MRLQW VRXUFHFKDQQHO PRGHO DQG DPD[LPXP HQWURS\
PRGHO WKLV PRGHO ZDV SDUW RI ODVW \HDU?V V\VWHP
EXW KDV EHHQ PRYHG IURP WKH GHFRGLQJ VWHS LQWR WKH
UHVFRULQJ VWHS IRU HIILFLHQF\ ,Q DOO RXU H[SHUL
PHQWV ZH KDYH WDNHQ D VWULFWO\ ODQJXDJH LQGHSHQ
GHQW DSSURDFK (DFK RI WKH ODQJXDJH SDLUV ZHUH SUR
FHVVHG DXWRPDWLFDOO\ IURP WKH JUDSKHPLF UHSUHVHQWD
WLRQ VXSSOLHG IRU WKH VKDUHG WDVNV ZLWK QR ODQJXDJH
VSHFLILF WUHDWPHQW IRU DQ\ RI WKH ODQJXDJH SDLUV
5HFHQW UHVHDUFK UHVXOWV RQ WKH DSSOLFDWLRQ RI UH
FXUUHQW QHXUDO QHWZRUN PRGHOV WR ODQJXDJH PRGHO
LQJ KDYH VKRZQ WKDW YHU\ SURPLVLQJ UHGXFWLRQV LQ
WH[W GDWD SHUSOH[LW\ UHODWLYH WR WUDGLWLRQDO QJUDP ODQ
JXDJHPRGHO DSSURDFKHV DUH SRVVLEOH 0LNRORY HW DO
 0LNRORY HW DO  7KH 511 DSSURDFK
GLIIHUV IURP WKH VWDQGDUG QJUDP DSSURDFK LQ WKDW
511V DUH DEOH WR VPRRWK E\ SURMHFWLQJ WKH JUDSKHPH
VHW RQWR D VHW RI KLGGHQ XQLWV D SURFHVV WKDW HI
IHFWLYHO\ FOXVWHUV VLPLODU JUDSKHPHV )XUWKHUPRUH
511V KDYH EHHQ UHSRUWHG WR EH HIIHFWLYH ZKHUH GDWD
UHVRXUFHV DUH OLPLWHG .RPEULQN HW DO 
7KHVH FKDUDFWHULVWLFV PRWLYDWH XV WR LQYHVWLJDWH
WKH HIIHFW RI DSSO\LQJ WKLV DSSURDFK LQ PRGHOLQJ DW
WKH JUDSKHPH RU JUDSKHPH VHTXHQFH SDLU OHYHO
SDUWLFXODUO\ DV WZR RI WKH PRVW LPSRUWDQW PRGHOV LQ
RXU V\VWHP DUH ERWK ODQJXDJH PRGHOV 7KH PDLQ
GUDZEDFN RI 511 EDVHG PRGHOV WKHLU H[FHSWLRQDOO\
KLJK WUDLQLQJ FRPSXWDWLRQDO FRPSOH[LW\ 0LNRORY HW
DO  LV QRW DQ REVWDFOH IRU WUDLQLQJ PRGHOV IRU
WKLV VKDUHG WDVN WKRXJK LW PD\ EH DQ LVVXH LI ODUJH
DPRXQWV RI PRQROLQJXDO GDWD DUH XVHG WR EXLOG WKH
ODQJXDJH PRGHOV :H UXQ H[SHULPHQWV XVLQJ WKLV
WHFKQLTXH WR LQYHVWLJDWH LWV HIIHFW RQ ERWK FRUSXV SHU
SOH[LW\ DQG HQGWRHQG V\VWHP SHUIRUPDQFH VLQFH
LW LV QRW QHFHVVDULO\ WKH FDVH WKDW JDLQV LQ ODQJXDJH
PRGHO SHUSOH[LW\ UHVXOW LQ EHWWHU V\VWHPV &KHQ HW DO

7KURXJKRXW WKLV SDSHU ZH ZLOO UHIHU WR JUDSKHPHV
JUDSKHPH VHTXHQFHV DQG JUDSKHPH VHTXHQFH SDLUV
%\ JUDSKHPH ZH PHDQ D VLQJOH XQLFRGH FKDUDFWHU
IRU H[DPSOH ?D? LQ (QJOLVK ??? LQ -DSDQHVH RU ???
LQ &KLQHVH *UDSKHPH VHTXHQFHV DUH DUELWUDU\ VH
TXHQFHV RI WKHVH JUDSKHPHV DQG JUDSKHPH VHTXHQFH
SDLUV DUH WXSOHV RI JUDSKHPH VHTXHQFHV HDFK HOH
PHQW LQ WKH WXSOH EHLQJ D JUDSKHPH VHTXHQFH LQ D
JLYHQ ODQJXDJH IRU H[DPSOH ?KHOOR??????
47
 6\VWHP 'HVFULSWLRQ
 %LOLQJXDO %D\HVLDQ *UDSKHPH $OLJQPHQW
7R WUDLQ WKH MRLQWVRXUFHFKDQQHO PRGHOV LQ RXU
V\VWHP ZH SHUIRUP D PDQ\WRPDQ\ JUDSKHPHWR
JUDSKHPH DOLJQPHQW 7R GLVFRYHU WKLV DOLJQPHQW
ZH XVH WKH %D\HVLDQ QRQSDUDPHWULF WHFKQLTXH GH
VFULEHG LQ )LQFK DQG 6XPLWD  ZKLFK LV D UHO
DWLYH RI WKH WHFKQLTXH SURSRVHG E\ +XDQJ HW DO
 %D\HVLDQ WHFKQLTXHV W\SLFDOO\ EXLOG FRPSDFW
PRGHOV ZLWK IHZ SDUDPHWHUV WKDW GR QRW RYHUILW WKH
GDWD DQG KDYH EHHQ VKRZQ WR EH HIIHFWLYH IRU WUDQVOLW
HUDWLRQ )LQFK DQG 6XPLWD  )LQFK HW DO 
 3KUDVHEDVHG 607 0RGHOV
7KH GHFRGLQJ ZDV SHUIRUPHG XVLQJ D VSHFLDOO\ PRGL
ILHG YHUVLRQ RI WKH 2&7$9,$1 GHFRGHU )LQFK HW DO
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20?27,
Dublin, Ireland, August 23-29 2014.
Integrating Dictionaries into an Unsupervised Model for
Myanmar Word Segmentation
Ye Kyaw Thu
NICT
Keihanna Science City
Kyoto, Japan
yekyawthu@nict.go.jp
Andrew Finch
NICT
Keihanna Science City
Kyoto, Japan
andrew.finch@nict.go.jp
Eiichiro Sumita
NICT
Keihanna Science City
Kyoto, Japan
eiichiro.sumita@nict.go.jp
Yoshinori Sagisaka
GITI/Speech Science Research Lab.
Waseda Univerity
Tokyo, Japan
ysagisaka@gmail.com
Abstract
This paper addresses the problem of word segmentation for low resource languages, with the
main focus being on Myanmar language. In our proposed method, we focus on exploiting lim-
ited amounts of dictionary resource, in an attempt to improve the segmentation quality of an
unsupervised word segmenter. Three models are proposed. In the first, a set of dictionaries
(separate dictionaries for different classes of words) are directly introduced into the generative
model. In the second, a language model was built from the dictionaries, and the n-gram model
was inserted into the generative model. This model was expected to model words that did not
occur in the training data. The third model was a combination of the previous two models. We
evaluated our approach on a corpus of manually annotated data. Our results show that the pro-
posed methods are able to improve over a fully unsupervised baseline system. The best of our
systems improved the F-score from 0.48 to 0.66. In addition to segmenting the data, one pro-
posed method is also able to partially label the segmented corpus with POS tags. We found that
these labels were approximately 66% accurate.
1 Introduction
In many natural language processing applications, for example machine translation, parsing and tagging,
it is essential to have text that is segmented into sequences of tokens (these tokens usually represent
?words?). In many languages, including the Myanmar language (alternatively called the Burmese lan-
guage), Japanese, and Chinese, words are not necessarily delimited by white space in running text. How-
ever, in some low-resource languages (Myanmar being one) broad-coverage word segmentation tools are
scarce, and there are two common approaches to dealing with this issue. The first is to apply unsuper-
vised word segmentation tools to a body of monolingual text in order to induce a segmentation. The
second is to use a dictionary of words in the language together with a set of heuristics to identify word
boundaries in text.
Myanmar language can be accurately segmented into a sequence of syllables using finite state au-
tomata (examples being (Berment, 2004; Thu et al., 2013a)). However, words composed of single or
multiple syllables are not usually separated by white space. Although spaces are sometimes used for
separating phrases for easier reading, it is not strictly necessary, and these spaces are rarely used in short
sentences. There are no clear rules for using spaces in Myanmar language, and thus spaces may (or
may not) be inserted between words, phrases, and even between a root word and its affixes. Myanmar
language is a resource-poor language and large corpora, lexical resources, and grammatical dictionaries
are not yet widely available. For this reason, using corpus-based machine learning techniques to develop
word segmentation tools is a challenging task.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
20
2 Related Work
In this section, we will briefly introduce some proposed word segmentation methods with an emphasis
on the schemes that have been applied to Myanmar.
Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chi-
nese and Japanese languages. These methods can be roughly classified into dictionary-based (Sorn-
lertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993;
Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In
dictionary-based methods, only words that are stored in the dictionary can be identified and the perfor-
mance depends to a large degree upon the coverage of the dictionary. New words appear constantly and
thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem.
On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic
or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they
require large amounts of data; the processing time required; and the difficulty in incorporating linguistic
knowledge effectively into the segmentation process (Teahan et al., 2000). For low-resource languages
such as Myanmar, there is no freely available corpus and dictionary based or rule based methods are
being used as a temporary solution.
If we only focus on Myanmar language word segmentation, as far as the authors are aware there have
been only two published methodologies, and one study. Both of the proposed methodologies operate
according using a process of syllable breaking followed by Maximum Matching; the differences in the
approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al.,
2008) statistical information is used (based on bigram information), whereas (Htay and Murthy, 2008)
utilize a word list extracted from a monolingual Myanmar corpus.
In a related study (Thu et al., 2013a), various Myanmar word segmentation approaches including
character segmentation, syllable segmentation, human lexical/phrasal segmentation, unsupervised and
semi-supervised word segmentation, were investigated. They reported that the highest quality machine
translation was attained either without word segmentation using simply sequences of syllables, or by
a process of Maximum Matching with a monolingual dictionary. In this study the effectiveness of ap-
proaches unsupervised word segmentation using latticelm (with 3-gram to 7-gram language models) and
supervised word segmentation using KyTea was evaluated, however, none of the approaches was able to
match the performance of the simpler syllable/Maximum Matching techniques.
In (Pei et al., 2013) an unsupervised Bayesian word segmentation scheme was augmented by using a
dictionary of words. These words were obtained from segmenting the data using another unsupervised
word segmenter. The probability distribution over these words was calculated from occurrence counts,
and this distribution was interpolated into the base measure.
3 Methodology
3.1 Baseline Non-parametric Bayesian Segmentation Model
The baseline system, and the model that forms the basis for all of the models is a non-parametric Bayesian
unsupervised word segmenter similar to that proposed in (Goldwater et al., 2009). The major differences
being the sampling strategy and the base measure. The principles behind this segmenter are described
below.
Intuitively, the model has two basic components: a model for generating an outcome that has already
been generated at least once before, and a second model that assigns a probability to an outcome that
has not yet been produced. Ideally, to encourage the re-use of model parameters, the probability of
generating a novel segment should be considerably lower then the probability of generating a previously
observed segment. This is a characteristic of the Dirichlet process model we use and furthermore, the
model has a preference to generate new segments early on in the process, but is much less likely to do so
later on. In this way, as the cache becomes more and more reliable and complete, so the model prefers to
use it rather than generate novel segments. The probability distribution over these segments (including an
infinite number of unseen segments) can be learned directly from unlabeled data by Bayesian inference
of the hidden segmentation of the corpus.
The underlying stochastic process for the generation of a corpus composed of segments s
k
is usually
written in the following from:
21
G|
?,G
0
? DP (?,G
0
)
s
k
|G ? G (1)
G is a discrete probability distribution over the all segments according to a Dirichlet process prior
with base measure G
0
and concentration parameter ?. The concentration parameter ? > 0 controls the
variance of G; intuitively, the larger ? is, the more similar G
0
will be to G.
3.1.1 The Base Measure
For the base measure G
0
that controls the generation of novel sequence-pairs, we use a spelling model
that assigns probability to new segments according to the following distribution:
G
0
(s) = p(|s|)p(s||s|)
=
?
|s|
|s|!
e
??|V |
?|s|
(2)
where |s| is the number of tokens in the segment; |V | and is the token set size; and ? is the expected
length of the segments.
According to this model, the segment length is chosen from a Poisson distribution, and then the el-
ements of the segment itself is generated given the length. Note that this model is able to assign a
probability to arbitrary sequences of tokens drawn from the set of tokens V (in this paper V is the set
of all Myanmar syllables). The motivation for using a base measure of this form, is to overcome issues
with overfitting when training the model; other base measures are possible for example the enhancement
proposed in Section 3.4.
3.1.2 The Generative Model
The generative model is given in Equation 3 below. The equation assignes a probability to the k
th
segment s
k
in a derivation of the corpus, given all of the other segments in the history so far s
?k
. Here
?k is read as: ?up to but not including k?.
p(s
k
|s
?k
) =
N(s
k
) + ?G
0
(s
k
)
N + ?
(3)
In this equation, N is the total number of segments generated so far, N(s
k
) is the number of times the
segment s
k
has occurred in the history. G
0
and ? are the base measure and concentration parameter as
before.
3.1.3 Bayesian Inference
We used a blocked version of a Gibbs sampler for training. In (Goldwater et al., 2006) they report
issues with mixing in the sampler that were overcome using annealing. In (Mochihashi et al., 2009)
this issue was overcome by using a blocked sampler together with a dynamic programming approach.
Our algorithm is an extension of application the forward filtering backward sampling (FFBS) algorithm
(Scott, 2002) to the problem of word segmentation presented in (Mochihashi et al., 2009). We extend
their approach to handle the joint segmentation and alignment of character sequences. We refer the reader
to (Mochihashi et al., 2009) for a complete description of the FFBS process. In essence the process uses
a forward variable at each node in the segmentation graph to store the probability of reaching the node
from the source node of the graph. These forward variables are calculated efficiently in a single forward
pass through the graph, from source node to sink node (forward filtering). During backward sampling, a
single path through the segmentation graph is sampled in accordance with its probability. This sampling
process uses the forward variables calculated in the forward filtering step.
In each iteration of the training process, each entry in the training corpus was sampled without re-
placement; its segmentation was removed and the models were updated to reflect this. Then a new
segmentation for the sequence was chosen using the FFBS process, and the models were updated with
22
the counts from this new segmentation. The two hyperparameters, the Dirichlet concentration parameter
?, and the Poisson rate parameter ? were set by slice sampling using vague priors (a Gamma prior in the
case of ? and the Jeffreys prior was used for ?). The token set size V used in the base measure was set
to the number of types in the training corpus, and V = 3363.
3.2 Dictionary Augmented Model
The dictionary augmented model is in essence the same model as proposed by (Thu et al., 2013b), but
a different dictionary was used. Their method integrates dictionary-based word segmentation (similar to
the maximum matching approaches used successfully in (Thet et al., 2008; Htay and Murthy, 2008; Thu
et al., 2013a) ) into a fully unsupervised Bayesian word segmentation scheme.
Dictionary-based word segmentation has the advantage of being able to exploit human knowledge
about the sequences of characters in the language that are used to form words. This approach is simple
and has proven to be a very effective technique in previous studies. Problems arise due to the coverage
of the dictionary. The dictionary may not be able to cover the running text well, for example in the case
of low-resource languages the dictionary might be small, or in the case of named entities, even though a
comprehensive dictionary of common words may exist, it is likely to fall far short of covering all of the
words that can occur in the language.
Unsupervised word segmentation techniques, have high coverage. They are able to learn how to
segment by discovering patterns in the text that recur. The weakness of these approaches is that they
have no explicit knowledge of how words are formed in the language, and the sequences they discover
from text may simply be sequences in text that frequently occur and may bear no relationship to actual
words in the language. As such these units, although they are useful in the context of the generative
model used to discover them, may not be appropriate for use in an application that might benefit from
these segments being words in the language. We believe that machine translation is one such application.
This method gives the unsupervised method a means of exploiting a dictionary of words in its training
process, by allowing the integrated method to use the dictionary to segment text when appropriate, and
otherwise use its unsupervised models to handle the segmentation. To do this a separate dictionary
generation process is integrated into the generative model of the unsupervised segmenter to create a
semi-supervised segmenter that segments using a single unified generative model.
3.3 Dictionary Set Augmented Model
In this model, the a set of subsets of the dictionary were extracted based on the part-of-speech labels
contained in the dictionary (Lwin, 1993). This set of subsets was not a partition of the original dictionary
since some of the types in the dictionary were ambiguous causing some overlap of the subsets. In the
previous model, during the generative process a decision was made, with a certain probability learned
from the data, as to whether the segment would be generated from the unsupervised sub-model or the
dictionary sub-model. In this model, the decision to generate from the dictionary model is refined into a
number of decisions to generate from a number of subsets of the dictionary, each with its own probability.
These probabilities were re-estimated from the sampled segmentation of the corpus at the end of each
iteration of the training (in a similar manner to the dictionary augmented model). A diagram showing
the generative process is shown in Figure 1.
3.4 Language Model Augmented Model
In (Theeramunkong and Usanavasin, 2001) dictionary approaches were deliberately avoided in order
to address issues with unknown words. Instead a decision tree model for segmentation was proposed.
Our approach although different in character (since a generative model is used), shares the insight that
knowledge of how words are constructed is key to segmentation when dictionary information is absent.
In this model we used the dictionary resource, but in a more indirect manner. We use a language model
to capture the notion of exactly what constitutes a segment. To do this words in the dictionary were first
segmented into syllables. Then, a language model was trained on this segmented dictionary. This model
will assign high probabilities to the words it has been trained on, and therefore in some sense is able
to capture the spirit of the dictionary-based methods described previously. However, it will also have
learned something about the way Myanmar words are composed from syllables, and can be expected to
assign a higher probability to unknown words that resemble the words it has been trained on, than to
sequences of syllables that are not consistent with its training data.
23
DP ModelP(unsupervised)
P(D1)
P(D2)
P(Dn)
D1 Model
D2 Model
Dn Model
Figure 1: Generative process with multiple dictionaries competing to generate the data alongside an
unsupervised Dirichlet process model.
This model can be naturally introduced directly into the Dirichlet process model as a component of
the base measure. Equation 2 decomposes into two terms:
1. A Poisson probability mass function:
?
|s|
|s|!
e
??
2. A uniform distribution over the vocabulary:
1
|V |
|s|
.
The first term above models the choice of length for the segment. The second term models the choice
of syllables that comprise the segment is in essence a unigram language model that provides little infor-
mation about segments are constructed, and serves simply to discourage the formation of long segments
that would lead to overfitting. We directly replace the part of the base measure with our more informative
language model built from the dictionary.
4 Experiments
4.1 Overview
In the experimental section we aim to analyze two aspects of the performance of the proposed segmen-
tation approaches. Firstly their segmentation quality and secondly for those approaches that are capable
of partially labeling the corpus, the accuracy of the labeling.
4.2 Corpora
For all our experiments we used a 160K-sentence subset of the Basic Travel Expression (BTEC) corpus
(Kikui et al., 2003), for the Myanmar language. This corpus was segmented using an accurate rule-based
approach into individual syllables, and this segmentation was used as the base segmentation in all our
experiments.
In addition a test corpus was made by sampling 150-sentences randomly from the corpus. This corpus
was then segmented by hand and the segments were annotated manually using the set of POS tags that
our system was capable of annotating, together with an ?UNK? tag to annotate segments that fell out of
this scope. The test sentences were included in the data used for the training of the Bayesian models.
These sentences were treated in the same manner the rest of the data in that they were initially syllable
segmented. At the end of the training, the test sentences together with the segmentation assigned by the
training, were extracted from the corpus, and their segmentation/labeling was evaluated with reference
to the human annotation.
4.3 Segmentation Performance
We used the Edit Distance of the Word Separator (EDWS) to evaluate the segmentation performance of
the models. This technique yields precision, recall and F-score statistics from the edit operations required
to transform one segmented sequence into another by means of the insertion, deletion and substitution of
segmentation boundaries. In this measure the substitution operator corresponds to an identity substitution
and therefore indicates a correct segment boundary. Insertions correspond to segmentation boundaries in
24
Method Precision Recall F-score Sub Ins Del
Unsupervised 82.27 33.82 0.48 348 681 75
Maximum Matching 78.39 99.42 0.88 1023 6 282
Dictionary 89.67 57.34 0.70 590 439 68
Dictionary Set 89.46 51.99 0.66 535 494 63
Language Model (LM) 88.15 31.10 0.46 320 709 43
Dictionary Set + LM 91.27 49.76 0.64 512 517 49
Table 1: Segmentation Performance.
the segmented output that do not correspond to any segmentation boundary in the reference. Deletions
correspond to segmentation boundaries in the reference that do not correspond to any segmentation
boundary in the segmented output. Precision recall and F-score are calculated as follows using the
Chinese Word Segmentation Evaluation Tookit (Joy, 2004):
precision =
#substitutions
#segment boundaries in output
recall =
#substitutions
#segment boundaries in reference
F-score =
2 ? precision ? recall
precision + recall
Table 1 shows the segmentation performance all of the systems. In terms of precision we see an im-
provement when the additional models are added to the baseline unsupervised model. The maximum
matching strategy has the lowest precision but the highest recall, this is due to the over-generation of
segmentation boundaries in regions where no dictionary matches are possible. In these regions the ref-
erence segmentation boundaries are always annotated (since the model defaults to syllable segmentation
in these regions), but at the expense of precision. This is reflected in the relatively low numbers of inser-
tions (6), and the relatively high number of deletions (282). As expected the dictionary set approach gave
similar performance to the Dictionary approach. The language model approach produced a respectable
level of precision, but a low value for recall. When integrated into the dictionary-based models however,
it was able to increase precision.
4.4 Labeling Accuracy
We evaluated the accuracy of the labeling on two methods: the first method used only a set of dictio-
naries (as described in Section 3.3). This method was able to label with an accuracy of 64.53%. The
second method consisted of the same technique, but with the addition of the language model trained on
the syllable segmented dictionaries (as described in Section 3.4). We found that the dictionary-based
language model was able to improved the labeling accuracy 1.2% to 65.73%.
5 Examples and Analysis
Figure 2 shows an example an unsupervised segmentation with a typical error. Frequent words are often
attached to neighboring words to form erroneous compound segments. In this example, taken from real
output of the segmenter, the word ?De? (this) has been attached to the word ?SarOuk? (book), and similar
the words in the phrase ?KaBeMharLe? (where is it) have all been segmented as a single segment (which
occurs frequently in the corpus).
In Figure 3, a typical segmentation from the maximum matcher is shown. In this example the word
?BarThar? (language) occurs in the dictionary but the word ?JaPan? (Japan) does not. The maximum
matcher defaults to segmenting the word for Japan into its component syllables, whereas the unsuper-
vised segmenter with dictionary has attempted an unsupervised segmentation on this part of the string.
The word ?Japan? occurs sufficiently frequently in the BTEC corpus that the segmenter has been able to
25
this book ?"?$?u?( DeSarOuk where is it. ???(?-$?/? KaBeMharLe
Figure 2: An unsupervised segmentation.
learn the word during training and has thereby managed to successfully segment the word in the output.
The word for language was segmented by means of the embedded dictionary model.
Figure 4 shows an example of the partial labeling produced from the model that used a set of dictio-
naries in combination with a dictionary-based language model in the base measure. Due to the small
amount of resources available, substantial parts of the sequence are unable to be labeled (and are anno-
tated with the ?U? tag in the figure, indicating that they were segmented by the unsupervised component
of the model). The remainder of the words are annotated with POS tags corresponding to the dictionary
they were generated from.
Japan ?"??% Ja Pan language ?'?' BarThar
N/A ?" Ja N/A ??% Pan language ?'?' BarThar
Maximum Matching
Unsupervised with dictionary
Figure 3: A segmentation from maximum matching.
I ?"?$e?'$/PRO KyunDaw
manager ??$e??*'/NS ManNayJar
phonecall ?u?$-?iue?0/U PhoneGoKhaw
doing  e??'?2/U NayDarBar
. ?/P 
Figure 4: A partially labeled segmentation.
6 Conclusion
In this paper we have proposed and investigated the effectiveness of several methods intended to exploit
limited quantities of dictionary resources available for low resource languages. Our results show that
by integrating a dictionary directly into an unsupervised word segmenter we were able to improve both
precision and recall. We found that attempting to model word formation using a language model on
its own was ineffective compared with the approaches that directly used a dictionary. However, this
language model proved useful when used in conjunction with the direct dictionary-based models, where
it served to assist the modeling of words that were not in the dictionary. In future work we intend to
develop the dictionary set approach by extending it to introduce basic knowledge of the morphological
structure of the language directly into the model.
References
Vincent Berment. 2004. Sylla and gmsword: applications to myanmar languages computerization. In Burma
Studies Conference.
26
Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word
segmentation. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and
the 44th annual meeting of the Association for Computational Linguistics, pages 673?680, Morristown, NJ,
USA. Association for Computational Linguistics.
Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation:
Exploring the effects of context. Cognition, 112(1):21?54.
Hla Hla Htay and Kavi Narayana Murthy. 2008. Myanmar word segmentation using syllable level longest match-
ing. In IJCNLP, pages 41?48.
Joy, 2004. Chinese Word Segmentation Evaluation Toolkit.
Chang Jyun-Shen, Chi-Dah Chen, and Shun-Der Chen. 1991. Chinese word segmentation through constraint
satisfaction and statistical optimization. In Proceedings of ROC Computational Linguistics Conference, pages
147?165.
G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto. 2003. Creating corpora for speech-to-speech translation. In
Proceedings of EUROSPEECH-03, pages 381?384.
San Lwin. 1993. Myanmar - English Dictionary. Department of the Myanmar Language Commission, Ministry
of Education, Union of Myanmar.
Sun Maosong, Shen Dayang, and Benjamin K. Tsou. 1998. Chinese word segmentation without using lexicon and
hand-crafted training data. In Proceedings of the 17th international conference on Computational linguistics -
Volume 2, COLING ?98, pages 1265?1271, Stroudsburg, PA, USA. Association for Computational Linguistics.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In ACL-IJCNLP ?09: Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the
AFNLP: Volume 1, pages 100?108, Morristown, NJ, USA. Association for Computational Linguistics.
Papageorgiou and Constantine P. 1994. Japanese word segmentation by hidden markov model. In Proceedings of
the workshop on Human Language Technology, HLT ?94, pages 283?288, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Wenzhe Pei, Dongxu Han, and Baobao Chang. 2013. A refined hdp-based model for unsupervised chinese word
segmentation. In Maosong Sun, Min Zhang, Dekang Lin, and Haifeng Wang, editors, Chinese Computational
Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, volume 8202 of Lecture
Notes in Computer Science, pages 44?51. Springer Berlin Heidelberg.
Steven L Scott. 2002. Bayesian methods for hidden markov models : Recursive computing in the 21st century.
Journal of the American Statistical Association, 97(457):337?351.
Virach Sornlertlamvanich. 1993. Word segmentation for thai in machine translation system. Machine Translation,
National Electronics and Computer Technology Center, Bangkok, pages 50?56.
A Srithirath and P. Seresangtakul. 2013. A hybrid approach to lao word segmentation using longest syllable level
matching with named entities recognition. In Electrical Engineering/Electronics, Computer, Telecommunica-
tions and Information Technology (ECTI-CON), 2013 10th International Conference on, pages 1?5, May.
W. J. Teahan, Rodger McNab, Yingying Wen, and Ian H. Witten. 2000. A compression-based algorithm for
chinese word segmentation. Comput. Linguist., 26(3):375?393, September.
Thanaruk Theeramunkong and Sasiporn Usanavasin. 2001. Non-dictionary-based thai word segmentation us-
ing decision trees. In In Proceedings of the First International Conference on Human Language Technology
Research.
Tun Thura Thet, Jin-Cheon Na, and Wunna Ko Ko. 2008. Word segmentation for the myanmar language. J.
Information Science, 34(5):688?704.
Ye Kyaw Thu, Andrew Finch, Yoshinori Sagisaka, and Eiichiro Sumita. 2013a. A study of myanmar word
segmentation schemes for statistical machine translation. Proceeding of the 11th International Conference on
Computer Applications, pages 167?179.
Ye Kyaw Thu, Andrew Finch, Eiichiro Sumita, and Yoshinori Sagisaka. 2013b. Unsupervised and semi-supervised
myanmar word segmentation approaches for statistical machine translation.
Zinmin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems.
Journal of the American Society for Information Science, 44(5):532?542.
27
