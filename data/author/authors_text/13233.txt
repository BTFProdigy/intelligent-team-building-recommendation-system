Using Masks, Suffix Array-based Data Structures and Multidimensional 
Arrays to Compute Positional Ngram Statistics from Corpora 
Alexandre Gil* 
Computer Science Department 
New University of Lisbon 
Caparica, Portugal 
agil@pt.ibm.com 
Ga?l Dias 
Centre of Mathematics 
Beira Interior University 
Covilh?, Portugal 
ddg@di.ubi.pt 
                                                          
* The authors want to thank Professor Jos? Gabriel Pereira Lopes from the New University of Lisbon for his advices. 
 
 
 
 
 
 
 
 
 
 
Abstract 
This paper describes an implementation to 
compute positional ngram statistics (i.e. Fre-
quency and Mutual Expectation) based on 
masks, suffix array-based data structures and 
multidimensional arrays. Positional ngrams 
are ordered sequences of words that represent 
continuous or discontinuous substrings of a 
corpus. In particular, the positional ngram 
model has shown successful results for the ex-
traction of discontinuous collocations from 
large corpora. However, its computation is 
heavy. For instance, 4.299.742 positional 
ngrams (n=1..7) can be generated from a 
100.000-word size corpus in a seven-word 
size window context. In comparison, only 
700.000 ngrams would be computed for the 
classical ngram model. It is clear that huge ef-
forts need to be made to process positional 
ngram statistics in reasonable time and space. 
Our solution shows O(h(F) N log N) time 
complexity where N is the corpus size and 
h(F) a function of the window context. 
1 Introduction 
Many models have been proposed to evaluate word de-
pendencies. One of the most successful statistical mod-
els is certainly the ngram model (Jelinek, 1990). 
However, in order to overcome its conceptual rigidity, 
T. Kuhn et al (1994) have defined the polygram model 
that estimates the probability of an ngram by interpolat-
ing the relative frequencies of all its kgrams (k ? n). 
Another way to account for variable length dependen-
cies is the n-multigram model designed by Deligne and 
Bimbot (1995).  
 
All these models have in common the fact that they 
need to compute continuous string frequencies. This 
task can be colossal when gigabytes of data need to be 
processed. Indeed, Yamamoto and Church (2000) show 
that there exist N(N+1)/2 substrings in a N-size corpus. 
That is the reason why low order ngrams have been 
commonly used in Natural Language Processing appli-
cations.  
 
In the specific field of multiword unit extraction, Dias 
(2002) has introduced the positional ngram model that 
has evidenced successful results for the extraction of 
discontinuous collocations from large corpora. Unlikely 
previous models, positional ngrams are ordered se-
quences of tokens that represent continuous or discon-
tinuous substrings of a corpus computed in a (2.F+1)-
word size window context (F represents the context in 
terms of words on the right and on the left of any word 
in the corpus). As a consequence, the number of gener-
ated substrings rapidly explodes and reaches astronomic 
figures. Dias (2002) shows that ? (Equation 1) posi-
tional ngrams can be computed for an N-size corpus in a 
(2.F+1)-size window context.  
  
( ) ???
?
???
? ++??=? ? ??+
= = =
???
?
1.2
3 1 1
11
11.2
F
k
F
i
F
j
ik
j
i
j CCFFN  
 
Equation 1: Number of positional ngrams 
 
In order to illustrate this equation, 4.299.742 positional 
ngrams (n=1..7) would be generated from a 100.000-
word size corpus in a seven-word size window context. 
In comparison, only 700.000 ngrams would be com-
puted for the classical ngram model. It is clear that huge 
efforts need to be made to process positional ngram 
statistics in reasonable time and space.  
 
In this paper, we describe an implementation that com-
putes the Frequency and the Mutual Expectation (Dias 
et al 1999) of any positional ngram with time complex-
ity O(h(F) N log N). The global architecture is based on 
the definition of masks that allow virtually representing 
any positional ngram in the corpus. Thus, we follow the 
Virtual Corpus approach introduced by Kit and Wilks 
(1998) and apply a suffix-array-like method, coupled to 
the Multikey Quicksort algorithm (Bentley and Sedge-
wick, 1997), to compute positional ngram frequencies. 
Finally, a multidimensional array is built to easily 
process the Mutual Expectation, an association measure 
for collocation extraction.  
 
The evaluation of our C++ implementation has been 
realized over the CETEMP?blico2 corpus and shows 
satisfactory results. For example, it takes 8.59 minutes 
to compute both frequency and Mutual Expectation for 
a 1.092.7233-word corpus on an Intel Pentium III 900 
MHz Personal Computer for a seven-word size window 
context. 
 
This article is divided into four sections: (1) we explain 
the basic principles of positional ngrams and the mask 
representation to build the Virtual Corpus; (2) we pre-
sent the suffix-array-based data structure that allows 
counting occurrences of positional ngrams; (3) we show 
how a multidimensional array eases the efficient com-
putation of the Mutual Expectation; (4) we present re-
sults over different size sub-corpora of the 
CETEMP?blico corpus. 
2 Positional Ngrams 
In the specific field of multiword unit extraction, Dias 
(2002) has introduced the positional ngram model that 
has evidenced successful results for the extraction of 
discontinuous collocations from large corpora. 
2.1 Principles 
 
The original idea of the positional ngram model comes 
from the lexicographic evidence that most lexical rela-
tions associate words separated by at most five other 
words (Sinclair, 1974). As a consequence, lexical rela-
tions such as collocations can be continuous or discon-
tinuous sequences of words in a context of at most 
eleven words (i.e. 5 words to the left of a pivot word, 5 
                                                          
2 The CETEMP?blico is a 180 million-word corpus of Portuguese. It 
can be obtained at http://www.ldc.upenn.edu/. 
3 This represents 46.986.831 positional ngrams. 
words to the right of the same pivot word and the pivot 
word itself). In general terms, a collocation can be de-
fined as a specific4 continuous or discontinuous se-
quence of words in a (2.F+1)-word size window context 
(i.e. F words to the left of a pivot word, F words to the 
right of the same pivot word and the pivot word itself). 
This situation is illustrated in Figure 1 for the colloca-
tion Ngram Statistics that fits in the window context. 
 
 
Figure 1: 2.F-word size window context 
 
Thus, as computation is involved, we need to process all 
possible substrings (continuous or discontinuous) that fit 
inside the window context and contain the pivot word. 
Any of these substrings is called a positional ngram. For 
instance, [Ngram Statistics] is a positional ngram as is the 
discontinuous sequence [Ngram ___ from] where the gap 
represented by the underline stands for any word occur-
ring between Ngram and from (in this case, Statistics). 
More examples are given in Table 1. 
 
Positional 2grams Positional 3grams 
[Ngram Statistics] [Ngram Statistics from] 
[Ngram ___ from] [Ngram Statistics ___ Large] 
[Ngram ___ ___ Large] [Ngram ___ from Large] 
[to ___ Ngram] [to ___ Ngram ___ from] 
 
Table 1: Possible positional ngrams 
 
In order to compute all the positional ngrams of a cor-
pus, we need to take into account all the words as possi-
ble pivot words.  
 
A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....
A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....
A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....
A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....
A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....
....
....  
Figure 2: One-window context for F=3 
 
A simple way would be to shift the two-window context 
to the right so that each word would sequentially be 
processed. However, this would inevitably lead to du-
plications of positional ngrams. Instead, we propose a 
                                                          
4 As specific, we intend a sequence that fits the definition of colloca-
tion given by Dias (2002): ?A collocation is a recurrent sequence of 
words that co-occur together more than expected by chance in a given 
domain?. 
Virtual   Approach to Deriving   Ngram Statistics from Large   Scale 
pivot 
F=3 F=3
one-window context that shifts to the right along the 
corpus as illustrated in Figure 2. It is clear that the size 
of the new window should be 2.F+1. 
 
This new representation implies new restrictions. While 
all combinations of words were valid positional ngrams 
in the two-window context, this is not true for a one-
window context. Indeed, two restrictions must be ob-
served. 
 
Restriction 1: Any substring, in order to be valid, must 
contain the first word of the window context.  
 
Restriction 2: For any continuous or discontinuous sub-
string in the window context, by shifting the substring 
from left to right, excluding gaps and words on the right 
and inserting gaps on the left, so that there always exists 
a word in the central position cpos (Equation 2) of the 
window, there should be at least one shift that contains 
all the words of the substring in the context window. 
 
1
2
1.2 +??
???
? += Fcpos  
 
Equation 2: Central position of the window 
 
For example, from the first case of Figure 2, the discon-
tinuous sequence [A B _ _ E _ G] is not a positional 
ngram although it is a possible substring as it does not 
follow the second restriction. Indeed, whenever we try 
to align the sequence to the central position, at least one 
word is lost as shown in Table 2: 
 
Possible 
shift 
Central 
word 
Disappearing 
words 
[_ _ A B _ _ E] B G 
[_ _ _ A B _ _] A E, G 
 
Table 2: Shifting Substrings 
 
In contrast, the sequence [A _ C _ E F _] is a positional 
ngram as the shift [_ A _ C _ E F], with C in the central 
position, includes all the words of the substring.  
 
Basically, the first restriction aims at avoiding duplica-
tions and the second restriction simply guarantees that 
no substring that would not be computed in a two-
window context is processed. 
2.2 Virtual Representation 
 
The representation of positional ngrams is an essential 
step towards efficient computation. For that, purpose, 
we propose a reference representation rather than an 
explicit structure of each positional ngram. The idea is 
to adapt the suffix representation (Manber and Myers, 
1990) to the positional ngram case. 
 
Following the suffix representation, any continuous 
corpus substring is virtually represented by a single po-
sition of the corpus as illustrated in Figure 3. In fact, the 
substring is the sequence of words that goes from the 
word referred by the position till the end of the corpus. 
 
 
 
Figure 3: Suffix Representation5 
 
Unfortunately, the suffix representation can not directly 
be extended to the specific case of positional ngrams. 
One main reason aims at this situation: a positional 
ngram may represent a discontinuous sequence of 
words. In order to overcome this situation, we propose a 
representation of positional ngrams based on masks. 
 
As we saw in the previous section, the computation of 
all the positional ngrams is a repetitive process. For 
each word in the corpus, there exists an algorithmic 
pattern that identifies all the possible positional ngrams 
in a 2.F+1-word size window context. So, what we need 
is a way to represent this pattern in an elegant and effi-
cient way.  
 
One way is to use a set of masks that identify all the 
valid sequences of words in a given window context. 
Thus, each mask is nothing more than a sequence of 1 
and 0 (where 1 stands for a word and 0 for a gap) that 
represents a specific positional ngram in the window 
context. An example is illustrated in Figure 4. 
 
 
 
 
Figure 4: Masks 
 
                                                          
5 The $ symbol stands for the end of the corpus.  
1
A
2
B
3
C
4
A
5
B
6
B
7
C
8
A
9 
$ 
corpus 
A B B C A $ 
A B C A B B C A $
A $ 
B B C A $ 
B C A B B C A $
B C A $ 
C A B B C A $
C A $ 
$ 9
8
7
6
5
4
3
2
1
substrings 
F = 3
1 2 3 4 5 6 7 8 9 10 
corpus A B C D E F G H I J 
mask 1 0 0 1 1 1 0 
ngram A _ _ D E F _ 
X X X 
? 
Computing all the masks is an easy and quick process. 
In our implementation, the generation of masks is done 
recursively and is negligible in terms of space and time. 
In table 3, we give the number of masks h(F) for differ-
ent values of F. 
 
F h(F) 
1 4 
2 11 
3 43 
4 171 
5 683 
 
Table 3: Number of masks 
 
In order to identify each mask and to prepare the refer-
ence representation of positional ngrams, an array of 
masks is finally built as in Figure 5. 
 
 
Figure 5: Masks Array 
 
From these structures, the virtual representation of any 
positional ngram is straightforward. Indeed, any posi-
tional ngram can be identified by a position in the cor-
pus and a given mask. Taking into account that a corpus 
is a set of documents, any positional ngram can be rep-
resented by the tuple {{iddoc, posdoc}, idmask} where iddoc 
stands for the document id of the corpus, posdoc for a 
given position in the document and idmask for a specific 
mask. An example is illustrated in Figure 6. 
 
 
 
Figure 6: Virtual Representation 
 
As we will see in the following section, this reference 
representation will allow us to follow the Virtual Cor-
pus approach introduced by Kit and Wilks (1998) to 
compute ngram frequencies. 
3 Computing Frequency 
With the Virtual Corpus approach, counting continuous 
substrings can easily and efficiently be achieved. After 
sorting the suffix-array data structure presented in Fig-
ure 3, the count of an ngram consisting of any n words 
in the corpus is simply the count of the number of adja-
cent indices that take the n words as prefix. We illus-
trate the Virtual Corpus approach in Figure 6. 
 
 
 
2gram Freq 3gram Freq 
[A B] 2 [A B B] 1 
[B B] 1 [B C A] 2 
 
Figure 6: Virtual Corpus Approach 
 
Counting positional ngrams can be computed exactly in 
the same way. The suffix-array structure is sorted using 
lexicographic ordering for each mask in the array of 
masks. After sorting, the count of a positional ngram in 
the corpus is simply the count of adjacent indices that 
stand for the same sequence. We illustrate the Virtual 
Corpus approach for positional ngrams in Figure 7. 
 
 
 
Figure 7: Virtual Corpus for positional ngrams 
mask
..
4 1 0 0 1 0 1 1 
5 1 0 0 1 1 0 0 
6 1 0 0 1 1 0 1 
7 1 0 0 1 1 1 0 
8 1 0 0 1 1 1 1 
9 1 0 1 0 0 0 0 
10 1 0 1 0 0 1 0 
..
F=3 
pos 
doc 0 1 2 3 4 5 6 7 8 9 10 11 12
corpus 0 A B C D  E F G H I  J K L M
... 
masks 
..
4 1 0 0 1 0 1 1 
5 1 0 0 1 1 0 0 
6 1 0 0 1 1 0 1 
7 1 0 0 1 1 1 0 
8 1 0 0 1 1 1 1 
9 1 0 1 0 0 0 0 
10 1 0 1 0 0 1 0 
..
{ {0,2} , 7 } = [ C _ _ F G H _ ] 
9
7
3
6
2
5
8
1
4
1 2
B
3
C
4
A
5
B
6
B
7
C
8
A
9
$
corpus
B B C A $ 
A B C A B B C A $
A $ 
B B C A $ 
B C A B B C A $
B C A $ 
C A B B C A $
C A $ 
$ 
A
A 
3
7
5
6
2
1
9
4
8
corpus
1 2
B
3
C
4
A
5
B
6
B
7
C
8
A
9 
A A
masks 
...
4 1 0 0 1 0 1 1 
5 1 0 0 1 1 0 0 
6 1 0 0 1 1 0 1 
7 1 0 1 1 1 0 
8 1 0 0 1 1 1 1 
9 1 0 1 0 0 0 0 
10 1 0 1 0 0 1 0 
...
0
10 11 
B 12 C 13 A 14 B 15 B 16 C 17A 18$A 
A _ A _ _ _ _
A _ B _ _ _ _
A _ B _ _ _ _
A _ C _ _ _ _
B _ A _ _ _ _
B _ A _ _ _ _
B _ C _ _ _ _
C _ A _ _ _ _
?
C _ B _ _ _ _
? _ ? _ _ _ _
The efficiency of the counting mainly resides in the use 
of an adapted sort algorithm. Kit and Wilks (1998) pro-
pose to use a bucket-radixsort although they acknowl-
edge that the classical quicksort performs faster for 
large-vocabulary corpora. Around the same perspective, 
Yamamoto and Church (2000) use the Manber and 
Myers?s algorithm (1990), an elegant radixsort-based 
algorithm that takes at most O(N log N) time and shows 
improved results when long repeated substrings are 
common in the corpus. 
 
For the specific case of positional ngrams, we have cho-
sen to implement the Multikey Quicksort algorithm 
(Bentley and Sedgewick, 1997) that can be seen as a 
mixture of the Ternary-Split Quicksort (Bentley and 
McIlroy, 1993) and the MSD6 radixsort (Anderson and 
Nilsson, 1998).  
 
The algorithm processes as follows: (1) the array of 
string is partitioned into three parts based on the first 
symbol of each string. In order to process the split a 
pivot element is chosen just as in the classical quicksort 
giving rise to: one part with elements smaller than the 
pivot, one part with elements equal to the pivot and one 
part with elements larger than the pivot; (2) the smaller 
and the larger parts are recursively processed in exactly 
the same manner as the whole array; (3) the equal part is 
also sorted recursively but with partitioning starting 
from the second symbol of each string; (4) the process 
goes on recursively: each time an equal part is being 
processed, the considered position in each string is 
moved forward by one symbol.  
 
In Figure 8, we propose an illustration of the Multikey 
Quicksort taken from the paper (Bentley and Sedge-
wick, 1997). The pivot is chosen using the median 
method. 
 
 
 
Figure 8: Sorting 12 two-letter words. 
 
                                                          
6 MSD stands for Most Significant Digit. 
Different reasons have lead to use the Multikey Quick-
sort algorithm. First, it performs independently from the 
vocabulary size. Second, it shows O(N log N) time 
complexity in our specific case. Third, Anderson and 
Nilsson (1998) show that it performs better than the 
MSD radixsort and proves comparable results to the 
newly introduced Forward radixsort. 
 
Counting frequencies is just a preliminary step towards 
collocation extraction. The following step attaches an 
association measure to each positional ngram that 
evaluates the interdependency between words inside a 
given sequence. In the positional ngram model, Dias et 
al. (1999) propose the Mutual Expectation measure.   
4 Computing Mutual Expectation 
4.1 Principles 
 
The Mutual Expectation evaluates the degree of rigidity 
that links together all the words contained in a posi-
tional ngram (?n, n ? 2) based on the concept of Nor-
malized Expectation and relative frequency.  
Normalized Expectation 
The basic idea of the Normalized Expectation is to 
evaluate the cost, in terms of cohesiveness, of the loss of 
one word in a positional ngram. Thus, the Normalized 
Expectation measure is defined in Equation 3 where the 
function k(.) returns the frequency of any positional 
ngram7.  
 [ ]( )
[ ]( )
[ ]( ) ???
?
???
? ??
???
?
??
???
?+
=
?
=
n
i
kk
n
k
NE
2
n1n
^
i
^
1i1 11n2n i 2i2 22
n1ni1i1 11
n1ni1i1 11
u p ... u  p ... wpup ... up ... up1
u p ... u ...p up
u p ... u ...p up
 
 
Equation 3: Normalized Expectation 
 
For that purpose, any positional ngram is defined alge-
braically as a vector of words [p11 u1 p12 u2 ? p1n un] 
where ui stands for any word in the positional ngram 
and p1i represents the distance that separates words u1 
and ui8. Thus, the positional ngram [A _ C D E _ _] would 
be rewritten as [0 A +2 C +3 D +4 E] and its Normalized 
Expectation would be given by Equation 4. 
 
                                                          
7 The "^" corresponds to a convention used in Algebra that consists in 
writing a "^" on the top of the omitted term of a given succession 
indexed from 1 to n. 
8 By statement, any pii is equal to zero.   
i
b o
s
n t 
e 
y 
a h 
e s 
t 
t
o
n
f r
Unsorted array 
Sorted array as  at   be    by   he       in    is       it           of          on       or          to
as   is  be   by   on       in    at       it          of        he       or         to
[ ]( ) [ ]( )[ ]( )
[ ]( )
[ ]( )
[ ]( ) ??
??
?
?
?
??
??
?
?
?
++ 
+++ 
+++ 
+++ 
+++ =+++ 
E 2 D 1 C0
E 4 D 3A 0
E 4 C 2A 0
D 3 C 2A 0
4
1
E 4 D 3 C 2A 0E 4 D 3 C 2A 0
k
k
k
k
kNE
 
 
which is equivalent to 
 
[ ]( ) [ ]( )[ ]( )
[ ]( )
[ ]( )
[ ]( ) ??
??
?
?
?
??
??
?
?
?
+
+
+=
_ _ _ _ E D C
_ _ E D _ _A 
_ _ E _ C _A 
_ _ _ D C _A 
4
1
_ _ E D C _A _ _ E D C _A 
k
k
k
k
kNE
 
 
Equation 4: Normalized Expectation example 
 
Mutual Expectation 
One effective criterion for multiword lexical unit identi-
fication is frequency. From this assumption, Dias et al 
(1999) pose that between two positional ngrams with 
the same Normalized Expectation, the most frequent 
positional ngram is more likely to be a collocation. So, 
the Mutual Expectation of any positional ngram is de-
fined in Equation 5 based on its Normalized Expectation 
and its relative frequency. 
 [ ]( )
[ ]( ) [ ]( )n1ni1i1 11n1ni1i1 11
n1ni1i1 11
u p ... u ...p upu p ... u ...p up
u p ... u ...p up
NEp
ME
?
=  
 
Equation 5: Mutual Expectation 
 
In order to compute the Mutual Expectation of any posi-
tional ngram, it is necessary to build a data structure that 
allows rapid and efficient search over the space of all 
positional ngrams. For that purpose, we propose a mul-
tidimensional array structure called Matrix9. 
4.2 Matrix 
The attentive reader will have noticed that the denomi-
nator of the Normalized Expectation formula is the av-
erage frequency of all the positional (n-1)grams 
included in a given positional ngram. These specific 
positional ngrams are called positional sub-ngrams of 
order n-110. So, in order to compute the Normalized 
Expectation and a fortiori the Mutual Expectation, it is 
necessary to access efficiently to the sub-ngrams fre-
quencies. This operation is done through the Matrix. 
                                                          
9 The Matrix also speeds up the extraction process that applies the 
GenLocalMaxs algorithm (Ga?l Dias, 2002). We do not present this 
algorithm due to lack of space. 
10 In order to ease the reading, we will use the term sub-ngrams to 
denote positional sub-ngrams of order n-1. 
However, to understand the Matrix itself, we first need 
to show how the sub-ngrams of any positional ngram 
can be represented.  
Representing sub-ngrams 
A sub-ngram is obtained by extracting one word at a 
time from its related positional ngram as shown in Fig-
ure 9. 
 
 
 
Figure 9: Sub-ngrams 
 
By representing a sub-ngram, we mean calculating its 
virtual representation that identifies its related substring. 
The previous figure shows that representing the first 
three sub-ngrams of the positional ngram {{0,0},14} is 
straightforward as they all contain the first word of the 
window context. The only difficulty is to know the 
mask they are associated to. Knowing this, the first three 
sub-ngrams would respectively be represented as: 
{{0,0},15}, {{0,0},16}, {{0,0},13}. 
 
For the last sub-ngram, the situation is different. The 
first word of the window context is omitted. As a con-
sequence, in order to calculate its virtual representation, 
we need to know the position of the first word of the 
substring as well as its corresponding mask. In this case, 
the position in the document of the positional sub-ngram 
is simply the position of its related positional ngram 
plus the distance that separates the first word of the 
window context from the first word of the substring. We 
call delta this distance. The obvious representation of 
the fourth sub-ngram is then {{0,2},18} where the position 
is calculated as 0+(delta=2)=2. 
 
In order to represent the sub-ngrams of any positional 
ngram, all we need is to keep track of the masks related 
0 1 2 3 4 5 6 7 8 9 
corpus
A B C D E F G H I J ...
 ngram   {{0,0},14} A _ C D E _ 
 sub-ngram 1 A _ C D _ _ _ 
sub-ngram 2 A _ C _ E _ _ 
sub-ngram 3 A _ _ D E _ _ 
delta=2 
_ 
13 1 0 0 1 1 0 0 
14 1 0 1 1 1 0 0 
sub-ngram 4 _ _ C D E _ _ 
masks 
doc
pos ...
...
0
1 0 1 1 0 0 0 
1 0 1 0 1 0 0 
1 0 1 0 0 0 0 
1 1 1 0 0 0 0 
15
16
17
18
to the mask of the positional ngram and the respective 
deltas. Thus, it is clear that for each mask, there exists a 
set of pairs {idmask, delta} that allows identifying all the 
sub-ngrams of any given positional ngram. Each pair is 
called a submask and is associated to its upper mask11 as 
illustrated in Figure 10. 
  
 
 
 
Figure 10: Submasks 
 
Now that all necessary virtual representations are well-
established, in order to calculate the Mutual Expecta-
tion, we need to build a structure that allows efficiently 
accessing any positional ngram frequency. This is the 
objective of the Matrix, a 2-dimension array structure. 
2-dimension Array Structure 
Searching for specific positional ngrams in a huge sam-
ple space can be overwhelming. To overcome this com-
putation problem, two solutions are possible: (1) keep 
the suffix array-based data structure and design opti-
mized search algorithms or (2) design a new data struc-
ture to ease the searching process. We chose the second 
solution as our complete system heavily depends on 
searching through the entire space of positional 
ngrams12 and, as a consequence, we hardly believe that 
improved results may be reached following the second 
solution.  
 
This new structure is a 2-dimension array where lines 
stand for the masks ids and the columns for the posi-
tions in the corpus. Thus, each cell of the 2-dimension 
array represents a given positional ngram as shown in 
Figure 11. This structure is called the Matrix. 
 
The frequency of each positional ngram can easily be 
represented by all its positions in the corpus. Indeed, a 
given positional ngram is a substring that can appear in 
different positions of the corpus being the count of these 
positions its frequency. From the previous suffix array-
                                                          
11 The upper mask is the mask from which the submasks are calcu-
lated. While upper masks represent positional ngrams, submasks 
represent sub-ngrams. 
12 In fact, this choice mainly has to do with the extraction process and 
the application of the GenLocalMaxs algorithm.  
based data structure, calculating all these positions is 
straightforward.  
 
Calculating the Mutual Expectation is also straightfor-
ward and fast as accessing to any positional ngram can 
be done in O(1) time complexity. We will illustrate this 
reality in the next section. 
 
 
 
Figure 11: The Matrix 
 
The illustration of our architecture is now complete. We 
now need to test our assumptions. For that purpose, we 
present results of our implementation over the 
CETEMP?blico corpus. 
5 Experiments 
We have conducted a number of experiments of our 
C++ implementation on the CETEMP?blico Portuguese 
corpus to derive positional ngram statistics (Frequency 
and Mutual Expectation). The experiments have been 
realized on an Intel Pentium 900 MHz PC with 390MB 
of RAM. From the original corpus, we have randomly 
defined 5 different size sub-corpora that we present in 
Table 4. 
 
corpus 01 02 03 04 05 
Size in 
Mb 0.7 3.1 5.3 6.7 8.8 
# of 
words 114.373 506.259 864.790 1.092.723 1.435.930 
# of 
ngrams13 4.917.781 21.768.879 37.185.712 46.986.831 61.744.732 
 
Table 4: Sub-corpora 
 
For each sub-corpus we have calculated the execution 
time of different stages of the process: (1) the tokeniza-
tion that transforms the corpus into a set of integers; (2) 
the preparation of the mask structure and the construc-
tion of the suffix-array data structure; (3) the sorting of 
the suffix-array data structure and the creation of the 
Matrix; (4) the calculation of the ME. The results are 
given in Table 5. 
 
 
                                                          
13 The window context of the experiment is F=3. 
2.F+1 
mask 
mask 
mask 
submasks 
idmask delta 
ME 
mask
Pos 
N
... 
... 
... 
... 
... 
...
pos 
M?
corpus 01 02 03 04 05 
Tokeniz. 0:00:01 0:00:04 0:00:08 0:00:09 0:00:17 
Masks/Suffix 0:00:04 0:00:14 0:00:25 0:00:31 0:00:40 
Matrix 0:00:35 0:03:23 0:06:16 0:08:11 0:11:12 
ME 0:00:00 0:00:03 0:00:06 0:00:08 0:00:10 
total 0:00:40 0:03:44 0:06:55 0:08:59 0:12:19 
 
Table 5: Execution Time in (hh:mm:ss) 
 
The results clearly show that the construction of the 
Matrix and the sort operation over the suffix-array data 
structure are the most time consuming procedures. On 
the contrary, the computation of the Mutual Expectation 
is quick due to the direct access to sub-ngrams frequen-
cies enabled by the Matrix. In order to understand the 
evolution of the results, we present, in Figure 12, a 
graphical representation of the results. 
 
Pentium III, 900 MHz, 390 MB
0:00:00
0:01:26
0:02:53
0:04:19
0:05:46
0:07:12
0:08:38
0:10:05
0:11:31
0:12:58
114373 506259 864790 1092723 1435930
# of words in the corpus
E
xe
cu
tio
n 
Ti
m
e 
(h
h:
m
m
:s
s)
 
 
Figure 12: Evolution of execution time 
 
The graphical representation illustrates a linear time 
complexity. In fact, Alexandre Gil (2002) has proved 
that, mainly due to the implementation of the Multikey 
Quicksort algorithm, our implementation evidences a 
time complexity of O(h(F) N log N) where N is the size 
of the corpus and h(F) a function of the window con-
text. 
6 Conclusion 
In this paper, we have described an implementation to 
compute positional ngram statistics based on masks, 
suffix array-based data structure and multidimensional 
arrays. Our C++ solution shows that it takes 8.59 min-
utes to compute both frequency and Mutual Expectation 
for a 1.092.723-word corpus on an Intel Pentium III 900 
MHz for a seven-word size window context. In fact, our 
architecture evidences O(h(F) N log N) time complex-
ity. To some extent, this work proposes a response to 
the conclusion of (Kit and Wilks, 1998) that claims that 
?[?] a utility for extracting discontinuous co-
occurrences of corpus tokens, of any distance from each 
other, can be implemented based on this program [The 
Virtual Corpus Approach]?. 
References 
Alexandre Gil. 2002. Extrac??o eficiente de padr?es textuais 
utilizando algoritmos e estruturas de dados avan?adas. 
Master Thesis, New University of Lisbon, Portugal. 
Arne Anderson and Stefan Nilsson. 1998. Implementing 
Radixsort. ACM Journal of Experimental Algorithmics, 
Vol. 3. citeseer.nj.nec.com/79696.html 
Chunyu Kit and Yorick Wilks. 1998. The Virtual Approach to 
Deriving Ngram Statistics from Large Scale Corpora. In-
ternational Conference on Chinese Information Processing 
Conference, Beijing, China, 223-229. cite-
seer.nj.nec.com/kit98virtual.html. 
Ga?l Dias, Sylvie Guillor?, and Jos? Lopes. 1999. Language 
Independent Automatic Acquisition of Rigid Multiword 
Units from Unrestricted Text corpora. Traitement Automa-
tique des Langues Naturelles, Institut d?Etudes Scientifi-
ques, Carg?se, France, 333-339. 
www.di.ubi.pt/~ddg/publications/taln1999.ps.gz 
Ga?l Dias 2002. Extraction Automatique d?Associations Lexi-
cales ? partir de Corpora. PhD Thesis. New University of 
Lisbon (Portugal) and University of Orl?ans (France). 
www.di.ubi.pt/~ddg/publications/thesis.pdf.gz 
John Sinclair. 1974. English Lexical Collocations: A study in 
computational linguistics. Singapore, reprinted as chapter 2 
of Foley, J. A. (ed). 1996, John Sinclair on Lexis and Lexi-
cography, Uni Press. 
Jon Bentley and Robert Sedgewick. 1997. Fast Algorithms for 
Sorting and Searching Strings. 8th Annual ACM-SIAM 
Symposium on Discrete Algorithms, New Orl?ans. cite-
seer.nj.nec.com/bentley97fast.html. 
Jon Bentley and Douglas McIlroy. 1993. Engineering a sort 
function. Software - Practice and Experience, 23(11):1249-
1265. 
Mikio Yamamoto and Kenneth Church. 2000. Using Suffix 
Arrays to Compute Term Frequency and Document Fre-
quency for All Substrings in a corpus. Association for 
Computational Linguistics, 27(1):1-30. 
www.research.att.com/~kwc/CL_suffix_array.pdf 
Sabine Deligne and Fr?d?ric Bimbot. 1995. Language Model-
ling by Variable Length Sequences: Theoretical Formula-
tion and Evaluation of Multigrams. ICASSP-95. Detroit, 
Michigan, 1:169-172. cite-
seer.nj.nec.com/deligne95language.html 
T. Kuhn, H. Nieman, E.G. Schukat-Talamazzini. 1994. Er-
godic Hidden Markov Models and Polygrams for Language 
Modelling. ICASSP-94, 1:357-360. cite-
seer.nj.nec.com/kuhn94ergodic.html 
Udi Manber and Gene Myers. 1990. Suffix-arrays: A new 
method for on-line string searches. First Annual ACM-
SIAM Symposium on Discrete Algorithms. 319-327. 
www.cs.arizona.edu/people/udi/suffix.ps 
Multiword Unit Hybrid Extraction   
Ga?l Dias 
Centre of Mathematics 
Beira Interior University 
Covilh?, Portugal 
ddg@di.ubi.pt 
 
 
Abstract 
This paper describes an original hybrid system 
that extracts multiword unit candidates from 
part-of-speech tagged corpora. While classical 
hybrid systems manually define local part-of-
speech patterns that lead to the identification 
of well-known multiword units (mainly com-
pound nouns), our solution automatically 
identifies relevant syntactical patterns from 
the corpus. Word statistics are then combined 
with the endogenously acquired linguistic in-
formation in order to extract the most relevant 
sequences of words. As a result, (1) human in-
tervention is avoided providing total flexibil-
ity of use of the system and (2) different 
multiword units like phrasal verbs, adverbial 
locutions and prepositional locutions may be 
identified. The system has been tested on the 
Brown Corpus leading to encouraging results.     
1 Introduction 
Multiword units (MWUs) include a large range of lin-
guistic phenomena, such as compound nouns (e.g. inte-
rior designer), phrasal verbs (e.g. run through), 
adverbial locutions (e.g. on purpose), compound deter-
minants (e.g. an amount of), prepositional locutions 
(e.g. in front of) and institutionalized phrases (e.g. con 
carne). MWUs are frequently used in everyday lan-
guage, usually to precisely express ideas and concepts 
that cannot be compressed into a single word. As a con-
sequence, their identification is a crucial issue for appli-
cations that require some degree of semantic processing 
(e.g. machine translation, summarization, information 
retrieval). 
 
In recent years, there has been a growing awareness in 
the Natural Language Processing (NLP) community of 
the problems that MWUs pose and the need for their 
robust handling. For that purpose, syntactical (Didier 
Bourigault, 1993), statistical (Frank Smadja, 1993; Ted 
Dunning, 1993; Ga?l Dias, 2002) and hybrid syntaxico-
statistical methodologies (B?atrice Daille, 1996; Jean-
Philippe Goldman et al 2001) have been proposed. 
 
In this paper, we propose an original hybrid system 
called HELAS1 that extracts MWU candidates from 
part-of-speech tagged corpora. Unlike classical hybrid 
systems that manually pre-define local part-of-speech 
patterns of interest (B?atrice Daille, 1996; Jean-Philippe 
Goldman et al 2001), our solution automatically identi-
fies relevant syntactical patterns from the corpus. Word 
statistics are then combined with the endogenously ac-
quired linguistic information in order to extract the most 
relevant sequences of words i.e. MWU candidates. 
Technically, we conjugate the Mutual Expectation (ME) 
association measure with the acquisition process called 
GenLocalMaxs (Ga?l Dias, 2002) in a five step process. 
First, the part-of-speech tagged corpus is divided into 
two sub-corpora: one containing words and one contain-
ing part-of-speech tags. Each sub-corpus is then seg-
mented into a set of positional ngrams i.e. ordered 
vectors of textual units. Third, the ME independently 
evaluates the degree of cohesiveness of each positional 
ngram i.e. any positional ngram of words and any posi-
tional ngram of part-of-speech tags. A combination of 
both MEs is then used to evaluate the global degree of 
cohesiveness of any sequence of words associated with 
its respective part-of-speech tag sequence. Finally, the 
GenLocalMaxs retrieves all the MWU candidates by 
evidencing local maxima of association measure values 
thus avoiding the definition of global thresholds. The 
overall architecture can be seen in Figure 1. 
 
Compared to existing hybrid systems, the benefits of 
HELAS are clear. By avoiding human intervention in 
the definition of syntactical patterns, it provides total 
                                                          
1 HELAS stands for Hybrid Extraction of Lexical ASsocia-
tions. 
flexibility of use. Indeed, the system can be used for any 
language without any specific tuning. HELAS also al-
lows the identification of various MWUs like phrasal 
verbs, adverbial locutions, compound determinants, 
prepositional locutions and institutionalized phrases. 
Finally, it responds to some extent to the affirmation of 
Beno?t Habert and Christian Jacquemin (1993) that 
claim that ?existing hybrid systems do not sufficiently 
tackle the problem of the interdependency between the 
filtering stage [the definition of syntactical patterns] 
and the acquisition process [the scoring and the election 
of relevant sequences of words] as they propose that 
these two steps should be independent?.  
 
 
 
Figure 1: Global architecture of HELAS 
 
The article is divided into five main sections: (1) we 
introduce the related work; (2) we present the text cor-
pus segmentation into positional ngrams; (3) we define 
the Mutual Expectation and a new combined association 
measure; (4) we propose the GenLocalMaxs algorithm 
as the acquisition process; Finally, in (5), we present 
some results over the Brown Corpus. 
2 Related Work 
For the purpose of MWU extraction, syntactical, statis-
tical and hybrid syntaxico-statistical methodologies 
have been proposed. On one hand, purely linguistic sys-
tems (Didier Bourigault, 1993) propose to extract rele-
vant MWUs by using techniques that analyse specific 
syntactical structures in the texts. However, these meth-
odologies suffer from their monolingual basis as the 
systems require highly specialised linguistic techniques 
to identify clues that isolate possible MWU candidates. 
  
On the other hand, purely statistical systems (Frank 
Smadja, 1993; Ted Dunning, 1993; Ga?l Dias, 2002) 
extract discriminating MWUs from text corpora by 
means of association measure regularities. As they use 
plain text corpora and only require the information ap-
pearing in texts, such systems are highly flexible and 
extract relevant units independently from the domain 
and the language of the input text. However, these 
methodologies can only identify textual associations in 
the context of their usage. As a consequence, many 
relevant structures can not be introduced directly into 
lexical databases as they do not guarantee adequate lin-
guistic structures for that purpose. 
 
Finally, hybrid syntactico-statistical systems (B?atrice 
Daille, 1996; Jean-Philippe Goldman et al 2001) define 
co-occurrences of interest in terms of syntactical pat-
terns and statistical regularities. Thus, such systems 
reduce the searching space to groups of words that cor-
respond to a priori defined syntactical patterns (e.g. 
Adj+Noun, Noun+Prep+Noun) and apply statistical 
scores to identify the most relevant sequences of words. 
One major drawback of such systems is that they do not 
deal with a great proportion of interesting MWUs (e.g. 
phrasal verbs, prepositional locutions). Moreover, they 
lack flexibility as the syntactical patterns have to be 
revised whenever the targeted language changes.  
 
In order to overcome these difficulties, we propose an 
original architecture that combines word statistics with 
endogenously acquired linguistic information. We base 
our study on two assumptions. On one hand, a great deal 
of studies in lexicography and terminology assess that 
most of the MWUs evidence well-known morpho-
syntactic structures (Gaston Gross, 1996). On the other 
hand, MWUs are recurrent combinations of words. In-
deed, according to Beno?t Habert and Christian Jacque-
min (1993), the MWUs may represent a fifth of the 
overall surface of a text. Consequently, it is reasonable 
to think that the syntactical patterns embodied by the 
MWUs may be endogenously identified by using statis-
tical scores over texts of part-of-speech tags exactly in 
the same manner as word dependencies are identified in 
corpora of words. So, the global degree of cohesiveness 
of any sequence of words may be evaluated by a combi-
nation of its degree of cohesiveness of words and the 
degree of cohesiveness of its associated part-of-speech 
tag sequence (See Figure 1).  
 
Compared to existing systems, the benefits of our archi-
tecture are clear. By avoiding human intervention in the 
definition of syntactical patterns, (1) HELAS provides 
total flexibility of use being independent of the targeted 
Input Tagged  
Text 
Text of Words Text of Tags 
Word ngrams Tag ngrams 
ME (word ngram) 
ME (word ngram)? 
x 
ME (tag ngram)1- ? 
ME (tag ngram) 
GenLocalMaxs 
MWU candidates 
language and (2) it allows the identification of various 
MWUs like phrasal verbs, adverbial locutions, com-
pound determinants, prepositional locutions and institu-
tionalized phrases. 
3 Text Segmentation 
Positional ngrams are nothing more than ordered vec-
tors of textual units which principles are introduced in 
the next subsection. 
3.1 Positional Ngrams 
The original idea of the positional ngram model (Ga?l 
Dias, 2002) comes from the lexicographic evidence that 
most lexical relations associate words separated by at 
most five other words (John Sinclair, 1974). As a con-
sequence, lexical relations such as MWUs can be con-
tinuous or discontinuous sequences of words in a 
context of at most eleven words (i.e. 5 words to the left 
of a pivot word, 5 words to the right of the same pivot 
word and the pivot word itself). In general terms, a 
MWU can be defined as a specific continuous or dis-
continuous sequence of words in a (2.F+1)-word size 
window context (i.e. F words to the left of a pivot word, 
F words to the right of the same pivot word and the 
pivot word itself). This situation is illustrated in Figure 
2 for the multiword unit Ngram Statistics that fits in the 
window context of size 2.3+1=7. 
 
 
 
Figure 2: 7-word size window context 
 
Thus, any substring (continuous or discontinuous) that 
fits inside the window context and contains the pivot 
word is called a positional word ngram. For instance, 
[Ngram Statistics] is a positional word ngram as is the 
discontinuous sequence [Ngram ___ from] where the gap 
represented by the underline stands for any word occur-
ring between Ngram and from (in this case, Statistics). 
More examples are given in Table 1. 
 
Positional word 2grams Positional word 3grams 
[Ngram Statistics] [Ngram Statistics from] 
[Ngram ___ from] [Ngram Statistics ___ Large] 
[Ngram ___ ___ Large] [Ngram ___ from Large] 
[to ___ Ngram] [to ___ Ngram ___ from] 
 
Table 1: Possible positional ngrams 
 
Generically, any positional word ngram may be defined 
as a vector of words [p11 u1 p12 u2 ? p1n un] where ui 
stands for any word in the positional ngram and p1i 
represents the distance that separates words u1 and ui2. 
Thus, the positional word ngram [Ngram Statisitcs] would 
be rewritten as [0 Ngram +1 Statistics]. More examples are 
given in Table 2. 
 
Positional word ngrams Algebraic notation 
[Ngram ___ from] [0 Ngram +2 from] 
[Ngram ___ ___ Large] [0 Ngram +3 Large] 
[to ___ Ngram] [0 to +2 Ngram] 
[Ngram Statistics ___ Large] [0 Ngram +1 Statisitcs +3 Large] 
 
Table 2: Algebraic Notation 
 
However, in a part-of-speech tagged corpus, each word 
is associated to a unique part-of-speech tag. As a conse-
quence, each positional word ngram is linked to a corre-
sponding positional tag ngram. A positional tag ngram 
is nothing more than an ordered vector of part-of-speech 
tags exactly in the same way a positional word ngram is 
an ordered vector of words. Let?s exemplify this situa-
tion. Let?s consider the following portion of a part-of-
speech tagged sentence following the Brown tag set:  
 
Virtual /JJ Approach /NN to /IN Deriving /VBG Ngram /NN Statistics /NN 
from /IN Large /JJ Scale /NN Corpus /NN 
 
It is clear that the corresponding positional tag ngram of 
the positional word ngram [0 Ngram +1 Statisitcs] is the 
vector [0 /NN +1 /NN]. More examples are in Table 3. 
Generically, any positional tag ngram may be defined as 
a vector of part-of-speech tags [p11 t1 p12 t2 ? p1n tn] 
where ti stands for any part-of-speech tag in the posi-
tional tag ngram and p1i represents the distance that 
separates the part-of-speech tags t1 and ti. 
 
Positional word ngrams Positional tag ngrams 
[0 Ngram +2 from] [0 /NN +2 /IN] 
[0 Ngram +3 Large] [0 /NN +3 /JJ] 
[0 to +2 Ngram] [0 /IN +2 /NN] 
[0 Ngram +1 Statisitcs +3 Large] [0 /NN +1 /NN +3 /JJ] 
 
Table 3: Positional tag ngrams 
 
So, any sequence of words, in a part-of-speech tagged 
corpus, is associated to a positional word ngram and a 
corresponding positional tag ngram. In order to intro-
duce the part-of-speech tag factor in any sequence of 
words of part-of-speech tagged corpus, we present an 
alternative notation of positional ngrams called posi-
tional word-tag ngrams.  
 
In order to represent a sequence of words with its asso-
ciated part-of-speech tags, a positional ngram may be 
represented by the following vector of words and part-
                                                          
2 By statement, any pii is equal to zero.   
Virtual   Approach to Deriving   Ngram  Statistics from Large   Scale 
pivot 
F=3 F=3 
of-speech tags [p11 u1 t1 p12 u2 t2? p1n un tn] where ui 
stands for any word in the positional ngram, ti stands for 
the part-of-speech tag of the word ui and p1i represents 
the distance that separates words u1 and ui. Thus, the 
positional ngram [Ngram Statistics] can be represented by 
the vector [0 Ngram /NN +1 Statistics /NN] given the text 
corpus in section (3.1). More examples are given in Ta-
ble 4. 
 
Positional ngrams Alternative notation 
[Ngram ___ from] [0 Ngram /NN +2 from /IN] 
[Ngram ___ ___ Large] [0 Ngram /NN +3 Large /JJ] 
[to ___ Ngram] [0 to /IN +2 Ngram /NN] 
 
Table 4: Alternative Notation 
 
This alternative notation will allow us to defining, with 
elegance, our combined association measure, introduced 
in the next section. 
3.2 Data Preparation 
So, the first step of our architecture deals with segment-
ing the input text corpus into positional ngrams. First, 
the part-of-speech tagged corpus is divided into two 
sub-corpora: one sub-corpus of words and one sub-
corpus of part-of-speech tags. The word sub-corpus is 
then segmented into its set of positional word ngrams 
exactly in the same way the tagged sub-corpus is seg-
mented into its set of positional tag ngrams.  
 
In parallel, each positional word ngram is associated to 
its corresponding positional tag ngram in order to fur-
ther evaluate the global degree of cohesiveness of any 
sequence of words in a part-of-speech tagged corpus. 
Our basic idea is to evaluate the degree of cohesiveness 
of each positional ngram independently (i.e. the posi-
tional word ngrams on one side and the positional tag 
ngrams on the other side) in order to calculate the global 
degree of cohesiveness of any sequence in the part-of-
speech tagged corpus by combining its respective de-
grees of cohesiveness i.e. the degree of cohesiveness of 
its sequence of words and the degree of cohesiveness of 
its sequence of part-of-speech tags.  
 
In order to evaluate the degree of cohesiveness of any 
sequence of textual units, we use the association meas-
ure called Mutual Expectation. 
4 Cohesiveness Evaluation 
The Mutual Expectation (ME) has been introduced by 
Ga?l Dias (2002) and evaluates the degree of cohesive-
ness that links together all the textual units contained in 
a positional ngram (?n, n ? 2) based on the concept of 
Normalized Expectation and relative frequency.  
4.1 Normalized Expectation 
The basic idea of the Normalized Expectation (NE) is to 
evaluate the cost, in terms of cohesiveness, of the loss of 
one element in a positional ngram. Thus, the NE is de-
fined in Equation 1 where the function k(.) returns the 
frequency of any positional ngram3.  
 [ ]( )
[ ]( )
[ ]( ) ???
?
???
?
???
?
???
?
??
???
?+
=
?
=
n
i
kk
n
k
NE
2
n1n
^
i
^
1i1 11n2n i 2i2 22
n1ni1i1 11
n1ni1i1 11
u p ... u  p ... upup ... up ... up1
u p ... u ...p up
u p ... u ...p up
 
 
Equation 1: Normalized Expectation 
 
In order to exemplify the NE formula, we present in 
Equation 2 its development for the given positional 
ngram [0 A +2 C +3 D +4 E] where each letter may repre-
sent a word or a part-of-speech tag.  
 
[ ]( ) [ ]( )[ ]( )
[ ]( )
[ ]( )
[ ]( ) ??
??
?
?
?
??
??
?
?
?
 
+ 
+ 
+ 
 = 
E 2 ,D 1 ,C0
E 4 ,D 3 ,A0
E 4 ,C 2 ,A0
D 3 ,C 2 ,A0
4
1
E 4 ,D 3 ,C 2 ,A0E 4 D, 3 ,C 2 ,A0
k
k
k
k
kNE
 
 
Equation 2: Normalized Expectation example 
 
However, evaluating the average cost of the loss of an 
element is not enough to characterize the degree of co-
hesiveness of a sequence of textual units. The Mutual 
Expectation is introduced to solve this insufficiency. 
4.2 Mutual Expectation 
Many applied works in Natural Language Processing 
have shown that frequency is one of the most relevant 
statistics to identify relevant textual associations. For 
instance, in the context of multiword unit extraction, 
(John Justeson and Slava Katz, 1995; B?atrice Daille, 
1996) assess that the comprehension of a multiword unit 
is an iterative process being necessary that a unit should 
be pronounced more than one time to make its compre-
hension possible. G?el Dias (2002) believes that this 
phenomenon can be enlarged to part-of-speech tags. 
From this assumption, they pose that between two posi-
tional ngrams with the same NE, the most frequent posi-
tional ngram is more likely to be a relevant sequence.  
 
So, the Mutual Expectation of any positional ngram is 
defined in Equation 3 based on its NE and its relative 
frequency embodied by the function p(.). 
 
                                                          
3 The "^" corresponds to a convention used in Algebra that 
consists in writing a "^" on the top of the omitted term of a 
given succession indexed from 1 to n. 
[ ]( )
[ ]( ) [ ]( )n1ni1i1 11n1ni1i1 11
n1ni1i1 11
u p ... u ...p upu p ... u ...p up
u p ... u ...p up
NEp
ME
?
=  
 
Equation 3: Mutual Expectation 
 
We will note that the ME shows interesting properties. 
One of them is the fact that it does not sub-evaluate in-
terdependencies when frequent individual textual units 
are present. In particular, this allows us to avoid the use 
of lists of stop words. Thus, when calculating all the 
positional ngrams, all the words and part-of-speech tags 
are used. This fundamentally participates to the flexibil-
ity of use of our system. 
 
As we said earlier, the ME is going to be used to calcu-
late the degree cohesiveness of any positional word 
ngram and any positional tag ngram. The way we calcu-
late the global degree of cohesiveness of any sequence 
of words associated to its part-of-speech tag sequence, 
based on its two MEs, is discussed in the next subsec-
tion.  
4.3 Combined Association Measure 
The drawbacks shown by the statistical methodologies 
evidence the lack of linguistic information. Indeed, 
these methodologies can only identify textual associa-
tions in the context of their usage. As a consequence, 
many relevant structures can not be introduced directly 
into lexical databases as they do not guarantee adequate 
linguistic structures for that purpose. 
 
In this paper, we propose a first attempt to solve this 
problem without pre-defining syntactical patterns of 
interest that bias the extraction process. Our idea is sim-
ply to combine the strength existing between words in a 
sequence and the evidenced interdependencies between 
its part-of-speech tags. We could summarize this idea as 
follows: the more cohesive the words of a sequence and 
the more cohesive its part-of-speech tags, the more 
likely the sequence may embody a multiword unit. 
 
This idea can only be supported due to two assumptions. 
On one hand, a great deal of studies in lexicography and 
terminology assess that most of the MWUs evidence 
well-known morpho-syntactic structures (Gaston Gross, 
1996). On the other hand, MWUs are recurrent combi-
nations of words capable of representing a fifth of the 
overall surface of a text (Beno?t Habert and Christian 
Jacquemin, 1993). Consequently, it is reasonable to 
think that the syntactical patterns embodied by the 
MWUs may endogenously be identified by using statis-
tical scores over texts of part-of-speech tags exactly in 
the same manner as word dependencies are identified in 
corpora of words. So, the global degree of cohesiveness 
of any sequence of words may be evaluated by a combi-
nation of its own ME and the ME of its associated part-
of-speech tag sequence. The degree of cohesiveness of 
any positional ngram based on a part-of-speech tagged 
corpus can then be evaluated by the combined associa-
tion measure (CAM) defined in Equation 4 where ? 
stands as a parameter that tunes the focus whether on 
words or on part-of-speech tags. 
 [ ]( )
[ ]( ) [ ]( ) ?? ??
=
1
n1ni1i1 11n1ni1i1 11
nn 1nii1i11 11
 tp ...  t...p tpu p ... u ...p up
tu p ... tu ...p tup
MEME
CAM  
 
Equation 4: Combined Association Measure 
 
We will see in the final section of this paper that differ-
ent values of ? lead to fundamentally different sets of 
multiword unit candidates. Indeed, ? can go from a total 
focus on part-of-speech tags (i.e. the relevance of a 
word sequence is based only on the relevance of its part-
of-speech sequence) to a total focus on words (i.e. the 
relevance of a word sequence is defined only by its 
word dependencies). Before going to experimentation, 
we need to introduce the used acquisition process which 
objective is to extract the MWUs candidates. 
5 The Acquisition Process 
The GenLocalMaxs (Ga?l Dias, 2002) proposes a flexi-
ble and fine-tuned approach for the selection process as 
it concentrates on the identification of local maxima of 
association measure values. Specifically, the GenLo-
calMaxs elects MWUs from the set of all the valued 
positional ngrams based on two assumptions. First, the 
association measures show that the more cohesive a 
group of words is, the higher its score will be. Second, 
MWUs are localized associated groups of words. So, we 
may deduce that a positional word-tag ngram is a MWU 
if its combined association measure value is higher or 
equal than the combined association measure values of 
all its sub-groups of (n-1) words and if it is strictly 
higher than the combined association measure values of 
all its super-groups of (n+1) words. Let cam be the 
combined association measure, W a positional word-tag 
ngram, ?n-1 the set of all the positional word-tag (n-1)-
grams contained in W, ?n+1 the set of all the positional 
word-tag (n+1)-grams containing W and sizeof(.) a func-
tion that returns the number of words of a positional 
word-tag ngram. The GenLocalMaxs is defined as: 
 
?x ??n-1 , ?y ??n+1 ,   W  is a MWU if 
(sizeof(W)=2  ?  cam(W) > cam(y) ) 
? 
(sizeof(W)?2  ?  cam(W) ? cam(x)  ?  cam(W) > cam(y)) 
 
Definition 1: GenLocalMaxs Algorithm 
 
Among others, the GenLocalMaxs shows one important 
property: it does not depend on global thresholds. A 
direct implication of this characteristic is the fact that, as 
no tuning needs to be made in order to acquire the set of 
all the MWU candidates, the use of the system remains 
as flexible as possible. Finally, we show the results ob-
tained by applying HELAS over the Brown Corpus. 
6 The Experiments 
In order to test our architecture, we have conducted a 
number of experiments with 11 different values of ? for 
a portion of the Brown Corpus containing 249 578 
words i.e. 249 578 words plus its 249 578 part-of-
speech tags. The limited size of our corpus is mainly 
due to the space complexity of our system. Indeed, the 
number of computed positional ngrams is huge even for 
a small corpus. For instance, 21 463 192 positional 
ngrams are computed for this particular corpus for a 7-
word size window context. As a consequence, computa-
tion is hard. For this experiment, HELAS has been 
tested on a personal computer with 128 Mb of RAM, 20 
Gb of Hard Disk and an AMD 1.4 Ghz processor under 
Linux Mandrake 7.2. On average, each experiment (i.e. 
for a given ?) took 4 hours and 20 minutes. Knowing 
that our system increases proportionally with the size of 
the corpus, it was unmanageable, for this particular ex-
periment, to test our architecture over a bigger corpus. 
Even though, the whole processing stage lasted almost 
48 hours4. 
 
We will divide our experiment into two main parts. 
First, we will do a quantitative analysis and then we will 
lead a qualitative analysis. All results will only tackle 
contiguous multiword units although non-contiguous 
sequences may be extracted. This decision is due to the 
lack of space. 
6.1 Quantitative Analysis 
In order to understand, as deeply as possible, the inter-
action between word cohesiveness and part-of-speech 
tag cohesiveness, we chose eleven different values for 
?, i.e. ? ? {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}, going 
from total focus on words (? = 1) to total focus on part-
of-speech tags (? = 0). 
 
First, we show the number of extracted contiguous 
MWU candidates by ? in table 5. The total results are 
not surprising. Indeed, with ? = 0, the focus is exclu-
sively on part-of-speech tags. It means that any word 
sequence, with an identified relevant part-of-speech 
sequence, is extracted independently of the words it 
contains. For instance, all the word sequences with the 
pattern [/JJ /NN] (i.e. Adjective + Noun) may be ex-
                                                          
4 We are already working on an efficient implementation of 
HELAS using suffix-arrays and the concept of masks.   
tracted independently of their word dependencies! This 
obviously leads to an important number of extracted 
sequences. The inclusion of the word factor, by increas-
ing the value of ?, progressively leads to a decreasing 
number of extracted positional ngrams. In fact, the word 
sequences with relevant syntactical structures are being 
filtered out depending on their word statistics. Finally, 
with ? = 1, the focus is exclusively on words. The im-
pact of the syntactical structure is null and the positional 
ngrams are extracted based on their word associations. 
In this case, the word sequences do not form classes of 
morpho-syntactic structures being the reason why less 
positional ngrams are extracted. 
 
alpha 0 0.1 0.2 0.3 0.4 0.5 
2gram 23146 21890 20074 17689 15450 13461 
3gram 297 467 567 351 1188 1693 
4gram 86 108 127 163 225 326 
5gram 79 81 81 82 77 82 
6gram 62 57 56 57 56 58 
TOTAL 23670 22603 20905 18342 16996 15620 
alpha 0.6 0.7 0.8 0.9 1.0  
2gram 11531 9950 9114 8650 8465  
3gram 2147 2501 2728 2828 2651  
4gram 428 557 679 740 484  
5gram 93 112 128 161 145  
6gram 58 58 60 64 60  
TOTAL 14257 13178 12709 12443 11805  
 
Table 5: Number of extracted MWU candidates 
 
A deeper analysis of table 5 reveals interesting results. 
The smaller the values of ?, the more positional 2grams 
are extracted. This situation is illustrated in Figure 3.  
 
#  of  e x t r a c t e d ngr a ms by  a l pha
0
5 0 0 0
10 0 0 0
15 0 0 0
2 0 0 0 0
2 5 0 0 0
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1
al pha
2gr am 3gr am 4gr am 5gr am 6gr am
 
Figure 3: Number of extracted MWU candidates 
 
Once again these results are not surprising. The Mutual 
Expectation tends to give more importance to frequent 
sequences of textual units. While it performs reasonably 
well on word sequences, it tends to over-evaluate the 
part-of-speech tag sequences. Indeed, sequences of two 
part-of-speech tags are much more frequent than other 
types of sequences and, as a consequence, tend to be 
over-evaluated in terms of cohesiveness. As small val-
ues of ? focus on syntactical structures, it is clear that in 
this case, small sequences of words are preferred over 
longer sequences.  
 
By looking at Figure 3 and Table 5, we may think that a 
great number of extracted sequences are common to 
each experiment. However, this is not true. In order to 
assess this affirmation, we propose, in Table 6, the 
summary of the identical ratio. 
  
alphas 0 0.1 0.2 0.3 0.4 0.5 
0  14.64 5.74 2.99 1.73 1.17 
0.1   9.99 3.77 2.08 1.35 
0.2    6.2 2.83 1.69 
0.3     4.89 2.36 
0.4      5.31 
0.5       
alphas 0.6 0.7 0.8 0.9 1.0  
0 0.83 0.63 0.54 0.49 0.47  
0.1 0.93 0.70 0.59 0.54 0.52  
0.2 1.11 0.81 0.68 0.61 0.59  
0.3 1.42 0.98 0.81 0.72 0.69  
0.4 2.34 1.44 1.13 0.97 0.90  
0.5 4.77 2.26 1.62 1.33 1.17  
0.6  5.06 2.82 2.10 1.73  
0.7   7.21 3.99 2.81  
0.8    9.45 4.50  
0.9     7.71  
1.0       
 
Table 6: Identical Ratio 
 
The identical ratio calculates, for two values of ?, the 
quotient between the number of identical extracted se-
quences and the number of different extracted se-
quences. Thus, the first value of the first row of table 6, 
represents the identical ratio for ?=0 and ?=0.1, and 
means that there are 14.64 times more identical ex-
tracted sequences than different sequences between both 
experiments.  
 
Taking ?=0 and ?=1, it is interesting to see that there are 
much more different sequences than identical sequences 
between both experiments (identical ratio = 0.47). In 
fact, this phenomenon progressively increases as the 
word factor is being introduced in the combined asso-
ciation measure to reach ?=1. This was somewhat unex-
pected. Nevertheless, this situation can be partly 
decrypted from Figure 3.  Indeed, figure 3 shows that 
longer sequences are being preferred as ? increases. In 
fact, what happens is that short syntactically well-
founded sequences are being replaced by longer word 
sequences that may lack linguistic information. For in-
stance, the sequence [Blue Mosque] was extracted with 
?=0, although the longer sequence [the Blue Mosque] was 
preferred with ?=1 as whenever [Blue Mosque] appears in 
the text, the determinant [the] precedes it. 
 
Finally, a last important result concerns the frequency of 
the extracted sequences. Table 7 gives an overview of 
the situation. The figures are clear. Most of the ex-
tracted sequences occur only twice in the input text cor-
pus. This result is rather encouraging as most known 
extractors need high frequencies in order to decide 
whether a sequence is a MWU or not. This situation is 
mainly due to the GenLocalMaxs algorithm. 
 
alpha 0 0.1 0.2 0.3 0.4 0.5 
Freq=2 13555 13093 12235 11061 10803 10458 
Freq=3 4203 3953 3616 3118 2753 2384 
Freq=4 1952 1839 1649 1350 1166 960 
Freq=5 1091 1019 917 743 608 511 
Freq>2 2869 2699 2488 2070 1666 1307 
TOTAL 23670 22603 20905 18342 16996 15620 
alpha 0.6 0.7 0.8 0.9 1.0  
Freq=2 10011 9631 9596 9554 9031  
Freq=3 2088 1858 1730 1685 1678  
Freq=4 766 617 524 485 468  
Freq=5 392 276 232 202 189  
Freq>2 1000 796 627 517 439  
TOTAL 14257 13178 12709 12443 11805  
 
Table 7: Number of extracted MWUs by frequency 
6.2 Qualitative Analysis 
As many authors assess (Frank Smadja, 1993; John 
Justeson and Slava Katz, 1995), deciding whether a se-
quence of words is a multiword unit or not is a tricky 
problem. For that purpose, different definitions of mul-
tiword unit have been proposed. One of the most suc-
cessful attempts can be attributed to Gaston Gross 
(1996) that classifies multiword units into six groups 
and provides techniques to determine their belonging. 
As a consequence, we intend as multiword unit any 
compound noun (e.g. interior designer), compound deter-
minant (e.g. an amount of), verbal locution (e.g. run 
through), adverbial locution (e.g. on purpose), adjectival 
locution (e.g. dark blue) or prepositional locution (e.g. in 
front of).  
 
The analysis of the results has been done intramuros 
although we are aware that an external independent 
cross validation would have been more suited. How-
ever, it was not logistically possible to do so and by 
using Gaston Gross?s classification and methodology, 
we narrow the human error evaluation as much as pos-
sible. Technically, we have randomly extracted and ana-
lysed 200 positional 2grams, 200 positional 3grams and 
100 positional 4grams for each value of ?. For the spe-
cific case of positional 5grams and 6grams, all the se-
quences have been analysed.  
 
Precision results of this analysis are given in table 8 and 
show that word dependencies and part-of-speech tag 
dependencies may both play an important role in the 
identification of relevant sequences. Indeed, values of ? 
between 0.4 and 0.5 seem to lead to optimum results. 
Knowing that most extracted sequences are positional 
2grams or positional 3grams, the global precision results 
approximate the results given by 2grams and 3grams. In 
these conditions, the best results are for ?=0.5 reaching 
an average precision of 62 %. This would mean that 
word dependencies and part-of-speech tags contribute 
equally to multiword unit identification.  
 
alpha 0 0.1 0.2 0.3 0.4 0.5 
2gram 29 % 22 % 30 % 44 % 53 % 60 % 
3gram 52 % 77 % 74 % 73 % 80 % 85 % 
4gram 38 % 32 % 32 % 46 % 47 % 41 % 
5gram 34 % 28 % 29 % 31 % 33 % 34 % 
6gram 29 % 22 % 18 % 24 % 31 % 38 % 
alpha 0.6 0.7 0.8 0.9 1.0  
2gram 45 % 23 % 25 % 18 % 30 %  
3gram 43 % 35 % 46 % 51 % 36 %  
4gram 41 % 45 % 39 % 44 % 37 %  
5gram 27 % 27 % 29 % 38 % 38 %  
6gram 32 % 37 % 26 % 29 % 29 %  
 
Table 8: Precision in % by alpha 
 
A deeper look at the results evidences interesting regu-
larities as shown in figure 4. Indeed, the curves for 
4grams, 5grams and 6grams are reasonably steady along 
the X axis evidencing low results. This means, to some 
extent, that that our system does not seem to be able to 
tackle successfully multiword units with more than 
three words. In fact, neither a total focus on words or on 
part-of-speech tags seems to change the extraction re-
sults. However, the importance of these results must be 
weakened as they represent a small proportion of the 
extracted structures.   
 
Precision by alpha and ngram
0%
20%
40%
60%
80%
100%
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
alpha
p
re
c
is
io
n
 (
%
)
2gram 3gram 4gram 5gram 6gram
  
Figure 4: Precision by alpha and ngram 
 
On the other hand, the curves for 2grams and 3grams 
show different behaviours.  For the 3gram case, it seems 
that the syntactical structure plays an important role in 
the identification process. Indeed, precision falls down 
drastically when the focus passes to word dependencies. 
This is mainly due to the extraction of recurrent se-
quences of words that do not embody multiword unit 
syntactical structures like [been able to] or [can still be]. 
As 2grams are concerned, the situation is different. In 
fact, it seems that too much focus on either words or 
part-of-speech tags leads to unsatisfactory results. In-
deed, optimum results are obtained for a balance be-
tween both criteria. This result can be explained by the 
fact that there exist many recurrent sequences of two 
words in a corpus. However, most of them are not mul-
tiword units like [of the] or [can be]. For that reason, only 
a balanced weight on part-of-speech tag and word de-
pendencies may identify relevant two word sequences. 
However, not-so-high precision results show that two-
word sequences still remain a tricky problem for our 
extractor as it is difficult to filter out very frequent pat-
terns that embody meaningless syntactical structures. 
7 Conclusion 
This paper describes an original hybrid system that ex-
tracts multiword unit candidates by endogenously iden-
tifying relevant syntactical patterns from the corpus and 
by combining word statistics with the acquired linguis-
tic information. As a result, by avoiding human inter-
vention in the definition of syntactical patterns, (1) 
HELAS provides total flexibility of use being independ-
ent of the targeted language and (2) it allows the identi-
fication of various MWUs like compound nouns, 
compound determinants, verbal locutions, adverbial 
locutions, prepositional locutions and adjectival locu-
tions without defining any threshold or using lists of 
stop words. The system has been tested on the Brown 
Corpus leading to encouraging results evidenced by a 
precision score of 62 % for the best configuration. The 
system will soon be available on http://helas.di.ubi.pt. 
References 
B?atrice Daille. 1996. Study and Implementation of Combined Tech-
niques for Automatic Extraction of Terminology. The balancing 
act combining symbolic and statistical approaches to language, 
MIT Press, 49-66. 
Beno?t Habert and Chistian Jacquemin. 1993. Noms compos?s, termes, 
d?nominations complexes: probl?matiques linguistiques et traite-
ments automatiques. Traitement Automatique des Langues, vol. 
34(2), 5-41. 
Didier Bourigault. 1993. Analyse syntaxique locale pour le rep?rage 
de termes complexes dans un texte. Traitement Automatique des 
Langues, vol. 34 (2), 105-117. 
Frank Smadja. 1993. Retrieving collocations from text: XTRACT. 
Computational Linguistics, vol. 19(1), 143-177. 
Ga?l Dias. 2002. Extraction Automatique d?Associations Lexicales ? 
partir de Corpora. PhD Thesis. DI/FCT New University of Lisbon 
(Portugal) and LIFO University of Orl?ans (France). 
Gaston Gross. 1996. Les expressions fig?es en fran?ais. Paris, Ophrys. 
Jean-Philippe Goldman, Luka Nerima and Eric Wehrli. 2001. Collo-
cation Extraction using a Syntactic Parser. Workshop of the 39th 
Annual Meeting of the Association for Computational Linguistics 
on Collocation: Computational Extraction, Analysis and Exploita-
tion, Toulouse, France, 61-66. 
John Justeson and Slava Katz. 1995. Technical Terminology: some 
linguistic properties and an algorithm for identification in text. 
Natural Language Engineering, vol. 1, 9-27. 
John Sinclair. 1974. English Lexical Collocations: A study in compu-
tational linguistics. Singapore, reprinted as chapter 2 of Foley, J. 
A. (ed). 1996, John Sinclair on Lexis and Lexicography, Uni Press. 
Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise 
and Coincidence. Computational Linguistics, vol. 19(1), 61-74. 
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 36?47,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Knowledge Representation using a Graph-based Algorithm for
Language-Independent Lexical Chaining
Gae?l Dias
HULTIG
University of Beira Interior
Covilha?, Portugal
ddg@di.ubi.pt
Cla?udia Santos
HULTIG
University of Beira Interior
Covilha?, Portugal
claudia@dmnet.ubi.pt
Guillaume Cleuziou
LIFO
University of Orle?ans
Orle?ans, France
cleuziou@univ-orleans.fr
Abstract
Lexical Chains are powerful representa-
tions of documents. In particular, they
have successfully been used in the field
of Automatic Text Summarization. How-
ever, until now, Lexical Chaining algo-
rithms have only been proposed for Eng-
lish. In this paper, we propose a greedy
Language-Independent algorithm that au-
tomatically extracts Lexical Chains from
texts. For that purpose, we build a hier-
archical lexico-semantic knowledge base
from a collection of texts by using the
Pole-Based Overlapping Clustering Algo-
rithm. As a consequence, our method-
ology can be applied to any language
and proposes a solution to language-
dependent Lexical Chainers.
1 Introduction
Lexical Chains are powerful representations of doc-
uments compared to broadly used bag-of-words rep-
resentations. In particular, they have successfully
been used in the field of Automatic Text Summa-
rization (Barzilay and Elhadad, 1997). However, un-
til now, Lexical Chaining algorithms have only been
proposed for English as they rely on linguistic re-
sources such as Thesauri (Morris and Hirst, 1991) or
Ontologies (Barzilay and Elhadad, 1997; Hirst and
St-Onge, 1997; Silber and McCoy, 2002; Galley and
McKeown, 2003).
Morris and Hirst (1991) were the first to propose
the concept of Lexical Chains to explore the dis-
course structure of a text. However, at the time of
writing their paper, no machine-readable thesaurus
was available so they manually generated Lexical
Chains using Roget?s Thesaurus (Roget, 1852).
A first computational model of Lexical Chains
is introduced by Hirst and St-Onge (1997). Their
biggest contribution to the study of Lexical Chains
is the mapping of WordNet (Miller, 1995) relations
and paths (transitive relationships) to (Morris and
Hirst, 1991) word relationship types. However, their
greedy algorithm does not use a part-of-speech tag-
ger. Instead, the algorithm only selects those words
that contain noun entries in WordNet to compute
Lexical Chains. But, as Barzilay and Elhadad (1997)
point at, the use of a part-of-speech tagger could
eliminate wrong inclusions of words such as read,
which has both noun and verb entries in WordNet.
So, Barzilay and Elhadad (1997) propose the first
dynamic method to compute Lexical Chains. They
argue that the most appropriate sense of a word can
only be chosen after examining all possible Lexi-
cal Chain combinations that can be generated from
a text. Because all possible senses of the word are
not taken into account, except at the time of inser-
tion, potentially pertinent context information that
is likely to appear after the word is lost. However,
this method of retaining all possible interpretations
until the end of the process, causes the exponential
growth of the time and space complexity.
As a consequence, Silber and McCoy (2002) pro-
pose a linear time version of (Barzilay and Elhadad,
1997) lexical chaining algorithm. In particular, (Sil-
ber and McCoy, 2002)?s implementation creates a
structure, called meta-chains, that implicitly stores
36
all chain interpretations without actually creating
them, thus keeping both the space and time usage
of the program linear.
Finally, Galley and McKeown (2003) propose a
chaining method that disambiguates nouns prior to
the processing of Lexical Chains. Their evaluation
shows that their algorithm is more accurate than
(Barzilay and Elhadad, 1997) and (Silber and Mc-
Coy, 2002) ones.
One common point of all these works is that Lex-
ical Chains are built using WordNet as the standard
linguistic resource. Unfortunately, systems based on
static linguistic knowledge bases are limited. First,
such resources are difficult to find. Second, they
are largely obsolete by the time they are available.
Third, linguistic resources capture a particular form
of lexical knowledge which is often very different
from the sort needed to specifically relate words or
sentences. In particular, WordNet is missing a lot
of explicit links between intuitively related words.
Fellbaum (1998) refers to such obvious omissions
in WordNet as the ?tennis problem? where nouns
such as nets, rackets and umpires are all present,
but WordNet provides no links between these related
tennis concepts.
In order to solve these problems, we propose to
automatically construct from a collection of docu-
ments a lexico-semantic knowledge base with the
purpose to identify cohesive lexical relationships be-
tween words based on corpus evidence. This hi-
erarchical lexico-semantic knowledge base is built
by using the Pole-Based Overlapping Clustering Al-
gorithm (Cleuziou et al, 2004) that clusters words
with similar meanings and allows words with mul-
tiple meanings to belong to different clusters. The
second step of the process aims at automatically
extracting Lexical Chains from texts based on our
knowledge base. For that purpose, we propose a
new greedy algorithm which can be seen as an ex-
tension of (Hirst and St-Onge, 1997) and (Barzilay
and Elhadad, 1997) algorithms which allows polyse-
mous words to belong to different chains thus break-
ing the ?one-word/one-concept per document? par-
adigm (Gale et al, 1992)1. In particular, it imple-
1This characteristic can be interesting for multi-topic docu-
ments like web news stories. Indeed, in this case, there may be
different topics in the same document as different news stories
may appear. In some way, it follows the idea of (Krovetz, 1998).
ments (Lin, 1998) information-theoretic definition
of similarity as the relatedness criterion for the at-
tribution of words to Lexical Chains2.
2 Building a Similarity Matrix
In order to build the lexico-semantic knowledge
base, the Pole-Based Overlapping Clustering Algo-
rithm needs as input a similarity matrix that gathers
the similarities between all the words in the corpus.
For that purpose, we propose a contextual analysis
of each nominal unit (nouns and compound nouns)
in the corpus. In particular, each nominal unit is as-
sociated to a word context vector and the similar-
ity between nominal units is calculated by the in-
formative similarity measure proposed by (Dias and
Alves, 2005).
2.1 Data Preparation
The context corpus is first pre-processed in order
to extract nominal units from it. The TnT tagger
(Brants, 2000) is first applied to our context cor-
pus to morpho-syntactically mark all the words in
it. Once all words have been morpho-syntactically
tagged, we apply the statistically-based multiword
unit extractor SENTA (Dias et al, 1999) that ex-
tracts multiword units based on any input text3. For
example, multiword units are compound nouns (free
kick), compound determinants (an amount of), ver-
bal locutions (to put forward), adjectival locutions
(dark blue) or institutionalized phrases (con carne).
Finally, we use a set of well-known heuristics
(Daille, 1995) to retrieve compound nouns using the
idea that groups of words that correspond to a pri-
ori defined syntactical patterns such as Adj+Noun,
Noun+Noun, Noun+Prep+Noun can be identified
as compound nouns. Indeed, nouns usually con-
vey most of the information in a written text. They
are the main contributors to the ?aboutness? of a
text. For example, free kick, city hall, operating sys-
tem are compound nouns which sense is not com-
positional i.e. the sense of the multiword unit can
2Of course, other similarity measures (Resnik, 1995; Jiang
and Conrath, 1997; Leacock and Chodorow, 1998) could be
implemented and should be evaluated in further work. How-
ever, we used (Lin, 1998) similarity measure as it has shown
improved results for Lexical Chains construction.
3By choosing both the TnT tagger and the multiword unit
extractor SENTA, we guarantee that our architecture remains as
language-independent as possible.
37
not be expressed by the sum of its constituents
senses. So, identifying lexico-semantic connections
between nouns is an adequate means of determining
cohesive ties between textual units4.
2.2 Word Context Vectors
The similarity matrix is a matrix where each cell cor-
responds to a similarity value between two nominal
units5. In this paper, we propose a contextual analy-
sis of nominal units based on similarity between
word context vectors.
Word context vectors are an automated method
for representing information based on the local con-
text of words in texts. So, for each nominal unit in
the corpus, we associate an N-dimension vector con-
sisting of its N most related words6.
In order to find the most relevant co-occurrent
nominal units, we implement the Symmetric Con-
ditional Probability (Silva et al, 1999) which is
defined in Equation 1 where p(w1, w2), p(w1)
and p(w2) are respectively the probability of co-
occurrence of the nominal units w1 and w2 and the
marginal probabilities of w1 and w2.
SCP (w1, w2) =
p(w1, w2)2
p(w1)? p(w2)
(1)
In particular, the window context for the calcula-
tion of co-occurrence probabilities is settled to F=20
words. In fact, we count, in all the texts of the
corpus, the number of occurrences of w1 and w2
appearing together in a window context of F ? 2
words. So, p(w1, w2) represents the density func-
tion computed as follows: the number of times w1
and w2 co-occur divided by the number of words in
the corpus7. In the present work, the values of the
SCP (., .) are not used as a factor of importance be-
tween words in the word context vector i.e. no dif-
ferentiation is made in terms of relevance between
the words within the word context vector. This issue
will be tackled in future work8.
4However, we acknowledge that verbs and adjectives should
also be tackled in future work.
5Many works have been proposed on word similarity (Lin,
1998).
6In our experiments, N=10.
7We note that multiword units are counted as single words
as when they are identified (e.g. President of the United States),
they are re-written in the corpus by linking all single words with
an underscore (e.g. President of the United States)
8We may point at the fact that satisfying results were
2.3 Similarity between Context Vectors
The closeness of vectors in the space is equivalent to
the closeness of the subject content. Thus, nominal
units that are used in a similar local context will have
vectors that are relatively close to each other. How-
ever, in order to define similarities between vectors,
we must transform each word context vector into
a high dimensional vector consisting of real-valued
components. As a consequence, each co-occurring
word of the word context vector is associated to a
weight which evaluates its importance in the corpus.
2.3.1 Weighting score
The weighting score of any word in a document
can be directly derived from an adaptation of the
score proposed in (Dias and Alves, 2005). In par-
ticular, we consider the combination of two main
heuristics: the well-known tf.idf measure proposed
by (Salton et al, 1975) and a new density measure
(Dias and Alves, 2005).
tf.idf: Given a word w and a document d, the
tf.idf(w, d) is defined in Equation 2 where tf(w, d)
is the number of occurrences of w in d, |d| corre-
sponds to the number of words in d, N is the num-
ber of documents in the corpus and df(w) stands for
the number of documents in the corpus in which the
word w occurs.
tf.idf(w, d) =
tf(w, d)
|d|
? log2

N
df(w)

(2)
density: The basic idea of the word density mea-
sure is to evaluate the dispersion of a word within
a document. So, very disperse words will not be
as relevant as dense words. This density measure
dens(., .) is defined in Equation 3.
dens(w, d) =
tf(w,d)?1
X
k=1
1
ln(dist(o(w,k), o(w,k+1)) + e)
(3)
For any given word w, its density dens(w, d)
is calculated from all the distances between all
its occurrences in document d, tf(w, d). So,
dist(o(w,k), o(w,k+1)) calculates the distance that
separates two consecutive occurrences of w in terms
of words within the document. In particular, e is the
obtained by the Symmetric Conditional Probability measure
compared to the Pointwise Mutual Information for instance
(Cleuziou et al, 2003)
38
base of the natural logarithm so that ln(e) = 1. This
argument is included into Equation 3 as it will give
a density value of 1 for any word that only occurs
once in the document. In fact, we give this word a
high density value.
final weight: The weighting score weight(w) of
any word w in the corpus can be directly derived
from the previous two heuristics. This score is de-
fined in Equation 4 where tf and dens are respec-
tively the average of tf(., .) and dens(., .) over all
the documents in which the word w occurs i.e. Nw.
weight(w) = tf .idf(w)? dens(w) (4)
where tf =
P
d tf(w,d)
Nw
and dens(w) =
P
d dens(w,d)
Nw
2.3.2 Informative Similarity Measure
The next step aims at determining the similarity
between all nominal units. Theoretically, a similar-
ity measure can be defined as follows. Suppose that
Xi = (Xi1, Xi2, Xi3, , Xip) is a row vector of ob-
servations on p variables associated with a label i.
The similarity between two words i and j is defined
as Sij = f(Xi, Xj) where f is some function of the
observed values. In the context of our work, Xi and
Xj are 10-dimension word context vectors.
In order to avoid the lexical repetition problem of
similarity measures, (Dias and Alves, 2005) have
proposed an informative similarity measure called
infoSimBA, which basic idea is to integrate into
the Cosine measure, the word co-occurrence fac-
tor inferred from a collection of documents with
the Symmetric Conditional Probability (Silva et al,
1999). See Equation 5.
InfoSimBA(Xi, Xj) =
Aij
Bi ?Bj + Aij
(5)
where
Aij =
p
X
k=1
p
X
l=1
Xik ?Xjl ? SCP (wik, wjl)
?i, Bi =
v
u
u
t
p
X
k=1
p
X
l=1
Xik ?Xil ? SCP (wik, wil)
and any Xzv corresponds to the word weighting fac-
tor weight(wzv), SCP (wik, wjl) is the Symmetric
Conditional Probability value betweenwik, the word
that indexes the word context vector i at position k
and wjl, the word that indexes the word context vec-
tor j at position l.
In particular, this similarity measure has proved to
lead to better results compared to the classical simi-
larity measure (Cosine) and shares the same idea as
the Latent Semantic Analysis (LSA) but in a differ-
ent manner. Let?s consider the following two sen-
tences.
(1) Ronaldo defeated the goalkeeper once more.
(2) Real_Madrid_striker scored again.
It is clear that both sentences (1) and (2) are simi-
lar although they do not share any word in common.
Such a situation would result in a null Cosine value
so evidencing no relationship between (1) and (2).
To solve this problem, the InfoSimBA(., .) func-
tion would calculate for each word in sentence (2),
the product of its weight with each weight of all the
words in sentence (1), and would then multiply this
product by the degree of cohesiveness existing be-
tween those two words calculated by the Symmet-
ric Conditional Probability measure. For example,
Real Madrid striker would give rise to the sum of
6 products i.e. Real Madrid striker with Ronaldo,
Real Madrid striker with defeated and so on and
so forth. As a consequence, sentence (1) and (2)
would show a high similarity as Real Madrid striker
is highly related to Ronaldo.
Once the similarity matrix is built based on the
infoSimBA between all word context vectors of
all nominal units in the corpus, we give it as in-
put to the Pole-Based Overlapping Clustering Algo-
rithm (Cleuziou et al, 2004) to build a hierarchy of
concepts i.e. our lexico-semantic knowledge base.
3 Hierarchy of Concepts
Clustering is the task that structures units in such
a way it reflects the semantic relations existing be-
tween them. In our framework nominal units are first
grouped into overlapping clusters (or soft-clusters)
such that final clusters correspond to conceptual
classes (called ?concepts? in the following). Then,
concepts are hierarchically structured in order to
capture semantic links between them.
Many clustering methods have been proposed in
the data analysis research fields. Few of them
propose overlapping clusters as output, in spite of
the interest it represents for domains of application
39
such as Natural Language Processing or Bioinfor-
matics. PoBOC (Pole-Based Overlapping Cluster-
ing) (Cleuziou et al, 2004) and CBC (Clustering By
Committees) (Pantel and Lin, 2002) are two clus-
tering algorithms suitable for the word clustering
task. They both proceed by first constructing tight
clusters9 and then assigning residual objects to their
most similar tight clusters.
A recent comparative study (Cicurel et al, 2006)
shows that CBC and PoBOC both lead to relevant
results for the task of word clustering. Neverthe-
less CBC requires parameters hard to tune whereas
PoBOC is free of any parametrization. The last ar-
gument encouraged us to use the PoBOC algorithm.
Unlike most of commonly used clustering algo-
rithms, the Pole-Based Overlapping Clustering Al-
gorithm shows the following advantages among oth-
ers : (1) it requires no parameters i.e. input is re-
stricted to a single similarity matrix, (2) the num-
ber of final clusters is automatically found and (3) it
provides overlapping clusters allowing to take into
account the different possible meanings of lexical
units.
3.1 A Graph-based Approach
The Pole-Based Overlapping Clustering Algorithm
is based on a graph-theoretical framework. Graph
formalism is often used in the context of cluster-
ing (graph-clustering). It first consists in defining
a graph structure which illustrates the data (vertices)
with links (edges) between them and then in propos-
ing a graph-partitioning process.
Numerous graph structures have been proposed
(Estivill-Castro et al, 2001). They all consider the
data set as set of vertices but differ on the way to de-
cide that two vertices are connected. Some method-
ologies are listed below where V is the set of ver-
tices, E the set of edges, G(V,E) a graph and d a
distance measure:
? Nearest Neighbor Graph (NNG) : each vertex
is connected to its nearest neighbor,
? Minimum Spanning Tree (MST) : ?(xi, xj) ?
V ?V a path exists between xi and xj in G with
P
(xi,xj)?E
d(xi, xj) minimized,
9The tight clusters are called ?committees? in CBC and
?poles? in PoBOC.
? Relative Neighborhood Graph (RNG) : xi and
xj are connected iff ?xk ? V \ {xi, xj},
d(xi, xj) ? max{d(xi, xk), d(xj , xk)}
? Gabriel Graph (GG) : xi and xj are connected
iff the circle with diameter xixj is empty,
? Delaunay Triangulation (DT) : xi and xj are
connected iff the associated Voronoi cells are
adjacent.
In particular, an inclusion order exists on these
graphs. One can show that NNG ? MST ? RNG ?
GG ? DT .
The choice of the suitable graph structure depends
on the expressiveness we want an edge to capture
and the partitioning process we plan to perform. The
Pole-Based Overlapping Clustering Algorithm aims
at retrieving dense subsets in a graph where two
similar data are connected and two dissimilar ones
are disconnected. Noticing that previous structures
do not match with this definition of a proximity-
graph10, a new variant is proposed with the Pole-
Based Overlapping Clustering Algorithm in defini-
tion 3.1.
Definition 3.1 Given a similarity measure s on a
data set X , the graph (denoted Gs(V,E)) is defined
by the set of vertices V = X and the set of edges E
such that (xi, xj) ? E ? xi ? N (xj) ? xj ? N (xi).
In particular,N (xi) corresponds to the local neigh-
borhood of xi built as in equation 6.
N (xi) = {xj ? X|s(xi, xj) > s(xi, X)} (6)
where the notation s(xi, I) denotes the average sim-
ilarity of xi with the set of objects I i.e.
X
xk?I
s(xi, xk)
|I|
(7)
This definition of neighborhood is a way to avoid
requiring to a parameter that would be too dependent
of the similarity used. Furthermore, the use of lo-
cal neighborhoods avoids the use of arbitrary thresh-
olds which mask the variations of densities. Indeed,
clusters are extracted from a similarity graph which
differs from traditional proximity graphs (Jarom-
czyk and Toussaint, 1992) in the definition of local
10Indeed, for instance, all of these graphs connect an outlier
with at least one other vertex. This is not the case with PoBOC.
40
neighborhoods which condition edges in the graph.
Neighborhood is different for each object and is
computed on the basis of similarities with all other
objects. Finally, an edge connects two vertices if
they are both contained in the neighborhood of the
other one. Figure 1 illustrates the neighborhood con-
straint above. In this case, as xi and xj are not both
in the intersection, they would not be connected.
Figure 1: To be connected, both xi and xj must be
in the intersection.
3.2 Discovery of Poles
The graph representation helps to discover a set
of fully-connected subgraphs (cliques) highly sep-
arated, denoted as Poles. Because Gs(V,E) is built
such that two vertices xi and xj are connected if and
only if they are similar11, a clique has the required
properties to be a good cluster. Indeed, such a clus-
ter guarantees that all its constituents are similar.
The search of maximal cliques in a graph is an
NP-complete problem. As a consequence, heuristics
are used in order to (1) build a great clique around a
starting vertex (Bomze et al, 1999) and (2) choose
the starting vertices in such a way cliques are as dis-
tant as possible.
Given a starting vertex x, the first heuristic con-
sists in adding iteratively the vertex xi which satis-
fies the following conditions:
? xi is connected to each vertex in P (with P the
clique/Pole in construction),
? among the connected vertices, xi is the nearest
one in average (s(xi, P )).
11In the sense that xi (resp. xj) is more similar to xj (resp.
xi) than to other data on average.
As a consequence, initialized with P = {x}, the
clique then grows until no vertex can be added.
The second heuristic guides the selection of the
starting vertices in a simple manner. Given a set
of Poles P1, . . . , Pm already extracted, we select the
vertex x as in Equation 8.
s(x, P1 ? ? ? ? ? Pm) = min
xi
s(xi, P1 ? ? ? ? ? Pm) (8)
A new Pole is then built from x if and only if x
satisfies the following conditions:
? ?k ? {1, . . . ,m} , x /? Pk ,
? s(x, P1 ? ? ? ? ? Pm) < s(X,X) =
1
|X|2
X
xi
X
xj
s(xi, xj)
Poles are thus extracted while P1 ? ? ? ? ? Pm 6=
X and the next starting vertex x is far enough from
the previous Poles. In particular, as Poles represent
the seeds of the further final clusters, this heuristic
gives no restriction on the number of clusters. The
first Pole is obtained from the starting point x? that
checks Equation 9.
x? = argmin
xk?X
s(xk, X) (9)
3.3 Multi-Assignment
Once the Poles are built, the Pole-Based Overlap-
ping Clustering algorithm uses them as clusters rep-
resentatives. Membership functions m(., .) are de-
fined in order to assign each object to its nearest
Poles as shown in Equation 10.
?xi ? X, Pj ? {P1, . . . , Pm} : m(xi, Pj) = s(xi, Pj) (10)
For each object xi to assign, the set of poles is
ordered (P1(xi), . . . , Pm(xi)) such that P1(xi) de-
notes the nearest pole12 for xi, P2(xi) the second
nearest pole for xi and so on. We first assign xi to its
closest Pole (P1(xi)). Then, for each pole Pk(xi)(in
the order previously defined) we decide to assign xi
to Pk(xi) if it satisfies to the following two condi-
tions :
? ?k? < k, xi is assigned to Pk? (xi),
? if k < m,
s(xi, Pk(xi)) ?
s(xi, Pk?1(xi)) + s(xi, Pk+1(xi))
2
This methodology results into a coverage of the
starting data set with overlapping clusters (extended
Poles).
12P1(xi) = argmaxPj s(xi, Pj)
41
3.4 Hierarchical Organization
A final step consists in organizing the obtained clus-
ters into a hierarchical tree. This structure is use-
ful to catch the topology of a set of a priori discon-
nected groups. The Pole-Based Overlapping Clus-
tering algorithm integrates this stage and proceeds
by successive merging of the two nearest clusters
like for usual agglomerative approaches (Sneath and
Sokal, 1973). In this process, the similarity be-
tween two clusters is obtained by the average-link
(or complete-link) method:
s(Ip, Iq) =
1
|Ip|.|Iq|
X
xi?Ip
X
xj?Iq
s(xi, xj) (11)
To deal with overlapping clusters we considere in
Equation 11 the similarity between an object and it-
self to be equal to 1 : s(xi, xi) = 1.
4 Lexical Chaining Algorithm
Once the lexico-semantic knowledge base has been
built, it is possible to use it for Lexical Chaining.
In this section, we propose a new greedy algorithm
which can be seen as an extension of (Hirst and St-
Onge, 1997) and (Barzilay and Elhadad, 1997) al-
gorithms as it allows polysemous words to belong
to different chains thus breaking the ?one-word/one-
concept per document? paradigm (Gale et al, 1992).
Indeed, multi-topic documents like web news sto-
ries may introduce different topics in the same doc-
ument/url and do not respect the ?one sense per dis-
course? paradigm. As we want to deal with real-
world applications, this characteristic may show in-
teresting results for the specific task of Text Summa-
rization for Web documents. Indeed, comparatively
to the experiments made by (Gale et al, 1992) that
deal with ?well written discourse?, web documents
show unusual discourse structures. In some way,
our algorithm follows the idea of (Krovetz, 1998).
Finally, it implements (Lin, 1998)?s information-
theoretic definition of similarity as the relatedness
criterion for the attribution of words to Lexical
Chains.
4.1 Algorithm
Our chaining algorithm is based on both approaches
of (Barzilay and Elhadad, 1997) and (Hirst and St-
Onge, 1997). So, our chaining model is developed
according to all possible alternatives of word senses.
In fact, all senses of a word are defined by the clus-
ters the word appears in13. We present our algorithm
below.
Begin with no chain.
For all distinct nominal units in text order do
For all its senses do
a) - among present chains find the sense
which satisfies the relatedness
criterion and link the new word to
this chain.
- Remove unappropriate senses of the
new word and the chain members.
b)if no sense is close enough, start a new chain.
End For
End For
End
4.2 Assignment of a word to a Lexical Chain
In order to assign a word to a given Lexical Chain,
we need to evaluate the degree of relatedness of the
given word to the words in the chain. This is done
by evaluating the relatedness between all the clusters
present in the Lexical Chain and all the clusters in
which the word appears.
4.2.1 Scoring Function
In order to determine if two clusters are semanti-
cally related, we use our lexico-semantic knowledge
base and apply (Lin, 1998)?s measure of semantic
similarity defined in Equation 12.
simLin(C1, C2) =
2? logP (C0)
logP (C1) + logP (C2)
(12)
The computation of Equation 12 is illustrated be-
low using the fragment of WordNet in Figure 2.
Figure 2: Fragment of WordNet (Lin, 1998).
13From now on, for presentation purposes, we will take as
synonymous the words clusters and senses
42
In this case, it would be easy to compute the sim-
ilarity between the concepts of hill and coast where
the number attached to each node C is P (C). It is
shown in Equation 13.
simLin(hill, coast) =
2 logP (geological ? formation)
logP (hill) + logP (coast)
= 0.59 (13)
However, in our taxonomy, as in any knowl-
edge base computed by hierarchical clustering algo-
rithms, only leaves contain words. So, upper clusters
(i.e. nodes) in the taxonomy gather all distinct words
that appear in the clusters they subsume. We present
this situation in Figure 3.
Figure 3: Fragment of our taxonomy.
In particular, clusters C305 and C306 of our
hierarchical tree, for the domain of Economy,
are represented by the following sets of words
C305 ={life, effort, stability, steps, negotiations}
and C306 ={steps, restructure, corporations, abuse,
interests, ministers} and the number attached to each
node C is P (C) calculated as in Equation 1414.
P (Ci) =
# of words in the cluster
# of distinct words in all clusters
(14)
4.2.2 Relatedness criterion
The relatedness criterion is the threshold that
needs to be respected in order to assign a word to
a Lexical Chain. In fact, it works like a threshold.
In this case, it is based on the average semantic sim-
ilarity between all the clusters present in the taxon-
omy. So, if all semantic similarities between a candi-
date word cluster Ck and all the clusters in the chain
?l, Cl respect the relatedness criterion, the word is
14The value 2843 in Figure 3 is the total number of distinct
words in our concept hierarchy.
assigned to the Lexical Chain. This situation is de-
fined in Equation 15 where c is a constant to be tuned
and n is the number of words in the taxonomy. So,
if Equation 15 is satisfied, the word w with cluster
Ck is agglomerated to the Lexical Chain.
?l, simLin(Ck, Cl) > c?
n
X
i=0
n
X
j=i+1
simLin(Ci, Cj)
n2
2
? n
(15)
In the following section, we present an example
of our algorithm.
4.2.3 Example of the Lexical Chain algorithm
The example below illustrates our Lexical Chain
algorithm. Let?s consider that a node is created
for the first nominal unit encountered in the text
i.e. crisis with its sense (C31). The next ap-
pearing candidate word is recession which has two
senses (C29 and C34). Considering a relatedness cri-
terion equal to 0.81 and the following similarities,
simLin(C31, C29) = 0.87, simLin(C31, C34) = 0.82 , the
choice of the sense for recession splits the Lexical
Chain into two different interpretations as shown
in Figure 4, as both similarities overtake the given
threshold 0.81.
Figure 4: Interpretations 1 and 2.
The next candidate word trouble has also two
senses (C29 and C32). As all the words in a Lexi-
cal Chain influence each other in the selection of the
respective senses of the new word considered, we
have the following situation in Figure 5.
So, three cases can happen: (1) all similarities
overtake the threshold and we must consider both
representations, (2) only the similarities related to
one representation overtake the threshold and we
43
Figure 5: Selection of senses.
only consider this representation or (3) none of the
similarities overtake the threshold and we create a
new Lexical Chain. So, we proceed with our algo-
rithm for both interpretations.
Interpretation 1 shows the following similari-
ties simLin(C31, C29) = 0.87, simLin(C31, C32) =
0.75, simLin(C29, C29) = 1.0, simLin(C29, C32) =
0.78 and interpretation 2 the following ones,
simLin(C31, C29) = 0.87, simLin(C31, C32) = 0.75,
simLin(C34, C29) = 0.54, simLin(C34, C32) = 0.55 .
By computing the average similarities for in-
terpretations 1 and 2, we reach the following re-
sults: average(Interpretation1) = 0.85 > 0.81 and
average(Interpretation2) = 0.68 ? 0.81 .
As a consequence, the word trouble is inserted in
the Lexical Chain with the appropriate sense (C29)
as it maximizes the overall similarity of the chain
and the chain members senses are updated. In this
example, the interpretation with (C32) is discarded
as is the cluster (C34) for recession. This processing
is described in Figure 6.
Figure 6: Selection of appropriate senses.
4.2.4 Score of a chain
Once all chains have been computed, only the
high-scoring ones must be picked up as represent-
ing the important concepts of the original docu-
ment. Therefore, one must first identify the strongest
chains. Like in (Barzilay and Elhadad, 1997), we
define a chain score which is defined in Equation 16
where |chain| is the number of words in the chain.
score(chain) =
|chain|?1
X
i=0
|chain|
X
j=i+1
simLin(Ci, Cj)
(|chain| ? 1)|chain|
2
(16)
As all chains will be scored, the ones with higher
scores will be extracted. Of course, a threshold will
have to be defined by the user. In the next section,
we will show some qualitative and quantitative re-
sults of our architecture.
5 Evaluation
The evaluation of Lexical Chains is generally diffi-
cult. Even if they can be effectively used in many
practical applications, Lexical Chains are seldom
desirable outputs in a real-world application, and
it is unclear how to assess their quality indepen-
dently of the underlying application in which they
are used (Budanitsky and Hirst, 2006). For example,
in Summarization, it is hard to determine whether a
good or bad performance comes from the efficiency
of the lexical chaining algorithm or from the appro-
priateness of using Lexical Chains in that kind of
application. It is also true that some work has been
done in this direction (Budanitsky and Hirst, 2006)
by collecting Human Lexical Chains to compare
against automatically built Lexical Chains. How-
ever, this type of evaluation is logistically impos-
sible to perform as we aim at developing a system
that does not depend on any language or topic. So,
in this section, we will only present some results
generated by our architecture (like (Barzilay and El-
hadad, 1997; Teich and Fankhauser, 2004) do), al-
though we acknowledge that other comparative eval-
uations (with WordNet, with Human Lexical Chains
or within independent applications like Text Sum-
marization) must be done in order to draw definitive
conclusions.
We have generated four taxonomies from four dif-
ferent domains (Sport, Economy, Politics and War)
from a set of documents of the DUC 200415. More-
over, we have extracted Lexical Chains for all four
15http://duc.nist.gov/duc2004/
44
domains to show the ability of our system to switch
from domain to domain without any problem.
5.1 Quantitative Function
Four texts from each domain of the DUC 2004 cor-
pus have been used to extract Lexical Chains based
on the four knowledge bases built from all texts of
DUC 2004 for each one of the four following do-
mains: Sport, Economy, Politics and War. However,
in this section, we will only present the results from
the Sport Domain as results show similar behaviors
for the other domains. In particular, we present in
Table 1 the characteristics of each document.
# Words #Distinct Words #Distinct Nouns
Doc 1 8133 1956 672
Doc 2 3823 1630 708
Doc 3 4594 953 324
Doc 4 4530 1265 431
Table 1: Characteristics of Documents for Sport
The first interesting conclusion shown in Table 2
is that the number of Lexical Chains does not de-
pend on the document size but rather on the nominal
units distribution. Indeed, for example, the number
of words in Document 1 is twice as big as in Doc-
ument 2. Although, we have more Lexical Chains
in Document 2 than in Document 1, as Document 2
has more distinct nominal units.
c=5 c=6 c=7 c=8
Doc 1 27 43 73 73
Doc 2 31 52 81 83
Doc 3 28 40 51 51
Doc 4 29 53 83 87
Table 2: # Lexical Chains per Document
The second interesting conclusion is that our algo-
rithm does not gather words that belong to only one
cluster and take advantage of the automatically built
lexico-semantic knowledge base. This is illustrated
in Table 3. However, it is obvious that by increasing
the constant c the words in a chain tend to belong to
only one cluster as it is the case for most of the best
Lexical Chains with c = 8.
5.2 Qualitative Evaluation
In this section, as it is done in (Barzilay and Elhadad,
1997; Teich and Fankhauser, 2004), we present the
c=5 c=6 c=7 c=8
Doc 1 19 13 7 7
Doc 2 13 6 3 3
Doc 3 3 4 4 4
Doc 4 6 4 3 3
Table 3: # Clusters per Lexical Chain
five highest-scoring chains for the best threshold that
we experimentally evaluated to be c = 7 for each
domain (See Tables 4, 5, 6, 7). It is clear that the
obtained Lexical Chains show a desirable degree of
representativeness of the text in analysis.
Domain=Sport, Document=3, c=7
- #0, 1 cluster and score=1.0: {United States, couple, competition}
- #6, 3 clusters and score=1.0: {boats, Sunday night, sailor, Sword, Orion,
veteran, cutter, Winston Churchill, Solo Globe, Challenger, navy, Race, sup-
position, instructions, responsibility, skipper, east, Melbourne, deck, kilo-
meter, masts, bodies, races, GMT, Admiral?s, Cups, Britain, Star, Class,
Atlanta, Seattle, arms, fatality, sea, waves, dark, yacht?s, Dad, Guy?s, son,
Mark, beer, talk, life, Richard, Winning, affair, canopy, death}
- #9, 1 cluster and score=1.0: {record, days, hours, minutes, rescue}
- #16, 3 clusters and score=1.0: {Snow, shape, north, easters, thunder,
storm, change, knots, west, level, maxi?s, search, Authority, seas, helicopter,
night vision, equipment, feet, rescues, Campbell, suffering, hypothermia,
safety, foot, sailors, colleagues, Hospital, deaths, bodies, fatality}
- #19, 2 clusters and score=1.0: {challenge, crew, Monday, VC, Offshore,
Stand, Newcastle, mid morning, Eden, Rescuers, aircraft, unsure, where-
abouts, killing, contact}
Table 4: 5 best Lexical Chains for Sport
Domain=Economy, Document=5, c=7
- #88, 4 clusters and score=1.0: {sign, chance, Rio, Janeiro, Grande, Sul,
uphill, promise, hospitals, powerhouse, success, inhabitants, victory, pad,
presidency, contingent, exit, legislature}
- #50, 1 cluster and score=1.0: {transactions, taxes, Stabilization, spate,
fuel, income, fortunes, means}
- #77, 1 cluster and score=1.0: {proposal, factory, owners, Fund, Rubin?s}
- #126, 1 cluster and score=1.0: {disaster, control, investment, review}
- #12, 2 clusters and score=0.99: {issue, order, University, population, ques-
tion, timing, currencies}
Table 5: 5 best Lexical Chains for Economy
For instance, the Lexical Chain #16 in the domain
of Sport clearly exemplifies the tragedy of climbers
that were killed in a sudden change of weather in
the mountains and who could not be rescued by the
authorities.
However, some Lexical Chains are less expres-
sive. For instance, it is not clear what the Lexical
Chain #40 expresses in the domain of Politics. In-
deed, none of the words present in the chain seem
45
Domain=Politics, Document=3, c=7
- #5, 1 cluster and score=1.0: {report, leaders, lives, information}
- #33, 1 cluster and score=1.0: {past, attention, defenders, investigations}
- #28, 2 clusters and score=0.95: {investigators, hospital, ward, wounds,
neck, description, fashion, suspects, raids, assault, rifles, door, further de-
tails, surgery, service, detective, Igor, Kozhevnikov, Ministry}
- #40, 2 clusters and score=0.92: {security, times, weeks, fire}
- #24, 3 clusters and score=0.85: {enemies, Choice, stairwell, assailants,
woman, attackers, entrance, car, guns, Friends, relatives, Mrs. Staravoitova,
founder, movement, well thought, Sergei, Kozyrev, Association, Societies,
supporter, Stalin?s, council, criminals, Yegor, Gaidar, minister, ally, sugges-
tions, measures, smile, commitment}
Table 6: 5 best Lexical Chains for Politics
Domain=War, Document=1, c=7
- #25, 2 clusters and score=1.0: {lightning, advance, Africa?s, nation,
outskirts, capital Kinshasa, troops, Angola, Zimbabwe, Namibia, chunk,
routes, Katanga, Eastern, Kasai, provinces, copper}
- #53, 1 cluster and score=1.0: {Back, years, Ngeyo, farm, farmers, organi-
zation, breadbasket, quarter, century, businessman, hotels, tourist, memory,
rivalry, rebellions}
- #56, 1 cluster and score=1.0: {political, freedoms, Hutus, Mai-Mai, war-
riors, Hunde, Nande, militiamen, Rwanda, ideology, weapons, persecu-
tion, landowners, ranchers, anarchy, Safari, Ngezayo, farmer, hotel, owner,
camps}
- #24, 2 clusters and score=0.87: {fighting, people, leaders, diplomats,
cause, president, Washington, U.S, units, weeks}
- #51, 2 clusters and score=0.82: {West, buildings, sight, point, tourists,
mountain, gorillas, shops, guest, disputes}
Table 7: 5 best Lexical Chains for War
to express any idea about Politics. Moreover, due
to the small number of inter-related nominal units
within the Lexical Chain, this one can not be under-
stood as it is without context. In fact, it was related
to problems of car firing that have been occurring in
the past few weeks and provoked security problems
in the town.
Although some Lexical Chains are understand-
able as they are, most of them must be replaced in
their context to fully understand their representative-
ness of the topics or subtopics of the text being an-
alyzed. As a consequence, we deeply believe that
Lexical Chains must be evaluated in the context of
Natural Language Processing applications (such as
Text Summarization (Doran et al, 2004)), as com-
paring Lexical Chains as they are is a very difficult
task to tackle which may even lead to inconclusive
results.
6 Conclusions and Future Work
In this paper, we implemented a greedy Language-
Independent algorithm for building Lexical Chains.
For that purpose, we first constructed a lexico-
semantic knowledge base by applying the Pole-
Based Overlapping Clustering algorithm (Cleuziou
et al, 2004) to word-context vectors obtained by the
application of the SCP (., .) measure (Silva et al,
1999) and the InfoSimBA(., .) (Dias and Alves,
2005) similarity measure. In a second step, we im-
plemented (Lin, 1998)?s similarity measure and used
it to define the relatedness criterion in order to as-
sign a given word to a given chain in the lexical
chaining process. Finally, our experimental eval-
uation shows that relevant Lexical Chains can be
constructed with our lexical chaining algorithm, al-
though we acknowledge that more comparative eval-
uations must be done in order to draw definitive con-
clusions. In particular, in future work, we want to
compare our methodology using WordNet as the ba-
sic knowledge base, implement different similarity
measures (Resnik, 1995; Jiang and Conrath, 1997;
Leacock and Chodorow, 1998), experiment differ-
ent Lexical Chains algorithms (Hirst and St-Onge,
1997; Barzilay and Elhadad, 1997; Galley andMcK-
eown, 2003), scale our greedy algorithm for real-
world applications following (Silber and McCoy,
2002) ideas and finally evaluate our system in inde-
pendent Natural Language Processing applications
such as Text Summarization (Doran et al, 2004).
References
R. Barzilay andM. Elhadad. 1997. Using Lexical Chains
for Text Summarization. Proceedings of the Intelli-
gent Scalable Text Summarization Workshop (ISTS-
97), ACL, Madrid, Spain, pages 10-18.
I. Bomze, M. Budinich, P. Pardalos, andM. Pelillo. 1999.
The Maximum Clique Problem. Handbook of Com-
binatorial Optimization, volume 4. Kluwer Academic
publishers, Boston, MA.
T. Brants. 2000. TnT - a Statistical Part-of-Speech Tag-
ger. In Proceedings of the 6th Applied NLP Confer-
ence, ANLP-2000. Seattle, WA.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness. In
Computational Linguistics, 32(1). pages: 13-47.
L. Cicurel, S. Bloehdorn and P. Cimiano. 2006. Cluster-
ing of Polysemic Words. In Advances in Data Analysis
- 30th Annual Conference of the German Classifica-
tion Society (GfKl). Berlin, Germany, March 8-10.
46
G. Cleuziou, L. Martin, and C. Vrain. 2004. PoBOC:
an Overlapping Clustering Algorithm. Application to
Rule-Based Classication and Textual Data. In Pro-
ceedings of the 16th European Conference on Artifi-
cial Intelligence, pages 440-444, Spain, August 22-27.
G. Cleuziou, V. Clavier, L. Martin. 2003. Une Me?thode
de Regroupement de Mots Fonde?e sur la Recherche de
Cliques dans un Graphe de Cooccurrences. In Pro-
ceedings of Rencontres Terminologie et Intelligence
Artificielle, France. pages 179-182.
B. Daille. 1995. Study and Implementation of Combined
Techniques for Automatic Extraction of Terminology.
In The balancing act combining symbolic and statisti-
cal approaches to language. MIT Press.
G. Dias and E. Alves. 2005. Unsupervised Topic Seg-
mentation Based on Word Co-occurrence and Multi-
Word Units for Text Summarization. In Proceedings
of the ELECTRA Workshop associated to 28th ACM
SIGIR Conference, Salvador, Brazil, pages 41-48.
G. Dias, S. Guillore? and J.G.P. Lopes. 1999. Language
Independent Automatic Acquisition of Rigid Multi-
word Units from Unrestricted Text Corpora. In Pro-
ceedings of 6th Annual Conference on Natural Lan-
guage Processing, Carge`se, France, pages 333-339.
W. Doran, N. Stokes, J. Carthy and J. Dunnion. 2004.
Assessing the Impact of Lexical Chain Scoring Meth-
ods and Sentence Extraction Schemes on Summariza-
tion. In Proc. of the 5th Conference on Intelligent Text
Processing and Computational Linguistics.
V. Estivill-Castro, I. Lee, and A. T. Murray. 2001. Crite-
ria on Proximity Graphs for Boundary Extraction and
Spatial Clustering. In Proceedings of the 5th Pacific-
Asia Conference on Knowledge Discovery and Data
Mining, Springer-Verlag. pages 348-357.
C.D. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press, New York.
W. Gale, K. Church, and D. Yarowsky. 1992. One Sense
per Discourse. In Proceedings of the DARPA Speech
and Natural Language Workshop.
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In Pro-
ceedings of 18th International Joint Conference on Ar-
tificial Intelligence (IJCAI?03), Acapulco, Mexico.
G. Hirst and D. St-Onge. 1997. Lexical Chains as Repre-
sentation of Context for the Detection and Correction
of Malapropisms. In WordNet: An electronic lexical
database and some of its applications. MIT Press.
J.W. Jaromczyk and G.T. Toussaint. 1992. Relative
Neighborhood Graphs and Their Relatives. P-IEEE,
80, pages 1502-1517.
J.J. Jiang and D.W. Conrath. 1997. Semantic Similarity
Based on Corpus Statistics and Lexical Taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, Taiwan.
R. Krovetz. 1998. More than One Sense per Discourse.
NEC Princeton NJ Labs., Research Memorandum.
C. Leacock and M. Chodorow. 1998. Combining Local
Context and WordNet Similarity for Word Sense Iden-
tification. In C. Fellbaum, editor, WordNet: An elec-
tronic lexical database. MIT Press. pages 265-283.
D. Lin. 1998. An Information-theoretic Definition of
Similarity. In 15th International Conference on Ma-
chine Learning. Morgan Kaufmann, San Francisco.
G. Miller. 1995. WordNet: An Lexical Database for Eng-
lish. Communications of the Association for Comput-
ing Machinery (CACM), 38(11), pages 39-41.
J. Morris and G. Hirst. 1991. Lexical Cohesion Com-
puted by Thesaural Relations as an Indicator of the
Structure of Text. Computational Linguistics, 17(1).
P. Pantel and D. Lin. 2002. Discovering Word
Senses from Text. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. pages 613-619.
P. Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal. pages 448-453.
P.M. Roget. 1852. Roget?s Thesaurus of English Words
and Phrases. Harlow, Essex, England: Longman.
G. Salton, C.S. Yang and C.T. Yu. 1975. A Theory
of Term Importance in Automatic Text Analysis. In
American Society of Information Science, 26(1).
G. Silber and K. McCoy. 2002. Efficiently Computed
Lexical Chains as an Intermediate Representation for
Automatic Text Summarization. Computational Lin-
guistics, 28(4), pages 487-496.
J. Silva, G. Dias, S. Guillore? and J.G.P. Lopes. 1999. Us-
ing LocalMaxs Algorithm for the Extraction of Con-
tiguous and Non-contiguous Multiword Lexical Units.
In Proceedings of 9th Portuguese Conference in Arti-
ficial Intelligence. Springer-Verlag.
P. H. A. Sneath and R. R. Sokal. 1973. Numerical Taxon-
omy - The Principles and Practice of Numerical Clas-
sification. San Francisco, Freeman and Co.
E. Teich and P. Fankhauser. 2004. Exploring Lexical Pat-
terns in Text: Lexical Cohesion Analysis with Word-
Net. In Proceedings of the 2nd International Wordnet
Conference, Brno, Czech Republic. pages 326-331.
47
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177?184,
Prague, June 2007. c?2007 Association for Computational Linguistics
Biology Based Alignments of Paraphrases for Sentence Compression
Joa?o Cordeiro
CLT and Bioinformatics
University of Beira Interior
Covilha?, Portugal
jpaulo@di.ubi.pt
Ga?el Dias
CLT and Bioinformatics
University of Beira Interior
Covilha?, Portugal
ddg@di.ubi.pt
Guillaume Cleuziou
LIFO - University of Orle?ans
Orle?ans, France
guillaume.cleuziou@
univ-orleans.fr
Abstract
1 In this paper, we present a study for ex-
tracting and aligning paraphrases in the con-
text of Sentence Compression. First, we jus-
tify the application of a new measure for the
automatic extraction of paraphrase corpora.
Second, we discuss the work done by (Barzi-
lay & Lee, 2003) who use clustering of para-
phrases to induce rewriting rules. We will
see, through classical visualization method-
ologies (Kruskal & Wish, 1977) and exhaus-
tive experiments, that clustering may not be
the best approach for automatic pattern iden-
tification. Finally, we will provide some re-
sults of different biology based methodolo-
gies for pairwise paraphrase alignment.
1 Introduction
Sentence Compression can be seen as the removal
of redundant words or phrases from an input sen-
tence by creating a new sentence in which the gist
of the original meaning of the sentence remains un-
changed. Sentence Compression takes an impor-
tant place for Natural Language Processing (NLP)
tasks where specific constraints must be satisfied,
such as length in summarization (Barzilay & Lee,
2002; Knight & Marcu, 2002; Shinyama et al, 2002;
Barzilay & Lee, 2003; Le Nguyen & Ho, 2004;
Unno et al, 2006), style in text simplification (Marsi
& Krahmer, 2005) or sentence simplification for
subtitling (Daelemans et al, 2004).
1Project partially funded by Portuguese FCT (Reference:
POSC/PLP/57438/2004)
Generally, Sentence Compression involves per-
forming the following three steps: (1) Extraction
of paraphrases from comparable corpora, (2) Align-
ment of paraphrases and (3) Induction of rewriting
rules. Obviously, each of these steps can be per-
formed in many different ways going from totally
unsupervised to totally supervised.
In this paper, we will focus on the first two steps.
In particular, we will first justify the application of
a new measure for the automatic extraction of para-
phrase corpora. Second, we will discuss the work
done by (Barzilay & Lee, 2003) who use cluster-
ing of paraphrases to induce rewriting rules. We
will see, through classical visualization methodolo-
gies (Kruskal & Wish, 1977) and exhaustive ex-
periments, that clustering may not be the best ap-
proach for automatic pattern identification. Finally,
we will provide some results of different biology
based methodologies for pairwise paraphrase align-
ment.
2 Related Work
Two different approaches have been proposed for
Sentence Compression: purely statistical method-
ologies (Barzilay & Lee, 2003; Le Nguyen & Ho,
2004) and hybrid linguistic/statistic methodologies
(Knight & Marcu, 2002; Shinyama et al, 2002;
Daelemans et al, 2004; Marsi & Krahmer, 2005;
Unno et al, 2006).
As our work is based on the first paradigm, we
will focus on the works proposed by (Barzilay &
Lee, 2003) and (Le Nguyen & Ho, 2004).
(Barzilay & Lee, 2003) present a knowledge-lean
algorithm that uses multiple-sequence alignment to
177
learn generate sentence-level paraphrases essentially
from unannotated corpus data alone. In contrast to
(Barzilay & Lee, 2002), they need neither paral-
lel data nor explicit information about sentence se-
mantics. Rather, they use two comparable corpora.
Their approach has three main steps. First, work-
ing on each of the comparable corpora separately,
they compute lattices compact graph-based repre-
sentations to find commonalities within groups of
structurally similar sentences. Next, they identify
pairs of lattices from the two different corpora that
are paraphrases of each other. Finally, given an input
sentence to be paraphrased, they match it to a lattice
and use a paraphrase from the matched lattices mate
to generate an output sentence.
(Le Nguyen & Ho, 2004) propose a new sentence-
reduction algorithm that do not use syntactic pars-
ing for the input sentence. The algorithm is an ex-
tension of the template-translation algorithm (one of
example-based machine-translation methods) via in-
novative employment of the Hidden Markov model,
which uses the set of template rules learned from ex-
amples.
In particular, (Le Nguyen & Ho, 2004) do not
propose any methodology to automatically extract
paraphrases. Instead, they collect a corpus by per-
forming the decomposition program using news and
their summaries. After correcting them manually,
they obtain more than 1,500 pairs of long and re-
duced sentences. Comparatively, (Barzilay & Lee,
2003) propose to use the N-gram Overlap metric
to capture similarities between sentences and auto-
matically create paraphrase corpora. However, this
choice is arbitrary and mainly leads to the extraction
of quasi-exact or exact matching pairs. For that pur-
pose, we introduce a new metric, the Sumo-Metric.
Unlike (Le Nguyen & Ho, 2004), one interesting
idea proposed by (Barzilay & Lee, 2003) is to clus-
ter similar pairs of paraphrases to apply multiple-
sequence alignment. However, once again, this
choice is not justified and we will see by classi-
cal visualization methodologies (Kruskal & Wish,
1977) and exhaustive experiments by applying dif-
ferent clustering algorithms, that clustering may not
be the best approach for automatic pattern identifi-
cation. As a consequence, we will study global and
local biology based sequence alignments compared
to multi-sequence alignment that may lead to better
results for the induction of rewriting rules.
3 Paraphrase Corpus Construction
Paraphrase corpora are golden resources for learning
monolingual text-to-text rewritten patterns. How-
ever, such corpora are expensive to construct manu-
ally and will always be an imperfect and biased rep-
resentation of the language paraphrase phenomena.
Therefore, reliable automatic methodologies able to
extract paraphrases from text and subsequently cor-
pus construction are crucial, enabling better pattern
identification. In fact, text-to-text generation is a
particularly promising research direction given that
there are naturally occurring examples of compara-
ble texts that convey the same information but are
written in different styles. Web news stories are an
obvious example. Thus, presented with such texts,
one can pair sentences that convey the same infor-
mation, thereby building a training set of rewriting
examples i.e. a paraphrase corpus.
3.1 Paraphrase Identification
A few unsupervised metrics have been applied to
automatic paraphrase identification and extraction
(Barzilay & Lee, 2003; Dolan & Brockett, 2004).
However, these unsupervised methodologies show a
major drawback by extracting quasi-exact2 or even
exact match pairs of sentences as they rely on clas-
sical string similarity measures such as the Edit Dis-
tance in the case of (Dolan & Brockett, 2004) and
word N-gram overlap for (Barzilay & Lee, 2003).
Such pairs are clearly useless.
More recently, (Anonymous, 2007) proposed a
new metric, the Sumo-Metric specially designed
for asymmetrical entailed pairs identification, and
proved better performance over previous established
metrics, even in the specific case when tested with
the Microsoft Paraphrase Research Corpus (Dolan
& Brockett, 2004). For a given sentence pair, hav-
ing each sentence x and y words, and with ? exclu-
sive links between the sentences, the Sumo-Metric is
defined in Equation 1 and 2.
2Almost equal strings, for example: Bush said America is
addicted to oil. and Mr. Bush said America is addicted to oil.
178
S(Sa, Sb) =
8
>><
>>:
S(x, y, ?) if S(x, y, ?) < 1.0
0 if ? = 0
e?k?S(x,y,?) otherwise
(1)
where
S(x, y, ?) = ? log2(
x
? ) + ? log2(
y
? ) (2)
with ?, ? ? [0, 1] and ?+ ? = 1.
(Anonymous, 2007) show that the Sumo-Metric
outperforms all state-of-the-art metrics over all
tested corpora. In particular, it shows systematically
better F-Measure and Accuracy measures over all
other metrics showing an improvement of (1) at least
2.86% in terms of F-Measure and 3.96% in terms
of Accuracy and (2) at most 6.61% in terms of F-
Measure and 6.74% in terms of Accuracy compared
to the second best metric which is also systemati-
cally the word N-gram overlap similarity measure
used by (Barzilay & Lee, 2003).
3.2 Clustering
Literature shows that there are two main reasons to
apply clustering for paraphrase extraction. On one
hand, as (Barzilay & Lee, 2003) evidence, clusters
of paraphrases can lead to better learning of text-to-
text rewriting rules compared to just pairs of para-
phrases. On the other hand, clustering algorithms
may lead to better performance than stand-alone
similarity measures as they may take advantage of
the different structures of sentences in the cluster to
detect a new similar sentence.
However, as (Barzilay & Lee, 2003) do not pro-
pose any evaluation of which clustering algorithm
should be used, we experiment a set of clustering al-
gorithms and present the comparative results. Con-
trarily to what expected, we will see that clustering
is not a worthy effort.
Instead of extracting only sentence pairs from cor-
pora3, one may consider the extraction of paraphrase
sentence clusters. There are many well-known clus-
tering algorithms, which may be applied to a cor-
pus sentence set S = {s1, ..., sn}. Clustering im-
plies the definition of a similarity or (distance) ma-
trix An?n, where each each element aij is the simi-
larity (distance) between sentences si and sj .
3A pair may be seen as a cluster with only two elements.
3.2.1 Experimental Results
We experimented four clustering algorithms on a
corpus of web news stories and then three human
judges manually cross-classified a random sample
of the generated clusters. They were asked to clas-
sify a cluster as a ?wrong cluster? if it contained at
least two sentences without any entailment relation
between them. Results are shown in the next table 1.
Table 1: Precision of clustering algorithms
BASE S-HAC C-HAC QT EM
0.618 0.577 0.569 0.640 0.489
The ?BASE? column is the baseline, where the
Sumo-Metric was applied rather than clustering.
Columns ?S-HAC? and ?C-HAC? express the re-
sults for Single-link and Complete-link Hierarchi-
cal Agglomerative Clustering (Jain et al, 1999).
The ?QT? column shows the Quality Threshold al-
gorithm (Heyer et al, 1999) and the last column
?EM? is the Expectation Maximization clustering al-
gorithm (Hogg et al, 2005).
One main conclusion, from table 1 is that cluster-
ing tends to achieve worst results than simple para-
phrase pair extraction. Only the QT achieves better
results, but if we take the average of the four cluster-
ing algorithms it is equal to 0.568, smaller than the
0.618 baseline. Moreover, these results with the QT
algorithm were applied with a very restrictive value
for cluster attribution as it is shown in table 2 with
an average of almost two sentences per cluster.
Table 2: Figures about clustering algorithms
Algorithm # Sentences/# Clusters
S-HAC 6,23
C-HAC 2,17
QT 2,32
EM 4,16
In fact, table 2 shows that most of the clusters
have less than 6 sentences which leads to question
the results presented by (Barzilay & Lee, 2003) who
only keep the clusters that contain more than 10 sen-
tences. In fact, the first conclusion is that the num-
ber of experimented clusters is very low, and more
important, all clusters with more than 10 sentences
showed to be of very bad quality.
The next subsection will reinforce the sight that
179
clustering is a worthless effort for automatic para-
phrase corpora construction.
3.2.2 Visualization
In this subsection, we propose a visual analy-
sis of the different similarity measures tested pre-
viously: the Edit Distance (Levenshtein, 1966), the
BLEU metric (Papineni et al, 2001), the word N-
gram overlap and the Sumo-Metric. The goal of this
study is mainly to give the reader a visual interpre-
tation about the organization each measure induces
on the data.
To perform this study, we use a Multidimensional
Scaling (MDS) process which is a traditional data
analysis technique. MDS (Kruskal & Wish, 1977)
allows to display the structure of distance-like data
into an Euclidean space.
Since the only available information is a similar-
ity in our case, we transform similarity values into
distance values as in Equation 3.
dij = (sii ? 2sij + sjj)1/2 (3)
This transformation enables to obtain a (pseudo)
distance measure satisfying properties like minimal-
ity, identity and symmetry. On a theoretical point
of view, the measure we obtain is a pseudo-distance
only, since triangular inequality is not necessary sat-
isfied. In practice, the projection space we build with
the MDS from such a pseudo-distance is sufficient to
have an idea about whether data are organized into
classes.
We perform the MDS process on 500 sentences4
randomly selected from the Microsoft Research
Paraphrase Corpus. In particular, the projection over
the three first eigenvectors (or proper vectors) pro-
vides the best visualization where data are clearly
organized into several classes (at least two classes).
The obtained visualizations (Figure 1) show dis-
tinctly that no particular data organization can be
drawn from the used similarity measures. Indeed,
we observe only one central class with some ?satel-
lite? data randomly placed around the class.
The last observation allows us to anticipate on the
results we could obtain with a clustering step. First,
clustering seems not to be a natural way to manage
4The limitation to 500 data is due to computation costs since
MDS requires the diagonalization of the square similarity or
distance matrix.
such data. Then, according to the clustering method
used, several types of clusters can be expected: very
small clusters which contain ?satellite? data (pretty
relevant) or large clusters with part of the main cen-
tral class (pretty irrelevant). These results confirm
the observed figures in the previous subsection and
reinforce the sight that clustering is a worthless ef-
fort for automatic paraphrase corpora construction,
contrarily to what (Barzilay & Lee, 2003) suggest.
4 Biology Based Alignments
Sequence alignments have been extensively ex-
plored in bioinformatics since the beginning of the
Human Genome Project. In general, one wants to
align two sequences of symbols (genes in Biology)
to find structural similarities, differences or transfor-
mations between them.
In NLP, alignment is relevant in sub-domains
like Text Generation (Barzilay & Lee, 2002). In
our work, we employ alignment methods for align-
ing words between two sentences, which are para-
phrases. The words are the base blocks of our se-
quences (sentences).
There are two main classes of pairwise align-
ments: the global and local classes. In the first
one, the algorithms try to fully align both sequences,
admitting gap insertions at a certain cost, while in
the local methods the goal is to find pairwise sub-
alignments. How suitable each algorithm may be
applied to a certain problem is discussed in the next
two subsections.
4.1 Global Alignment
The well established and widely used Needleman-
Wunsch algorithm for pairwise global sequence
alignment, uses dynamic programming to find the
best possible alignment between two sequences. It is
an optimal algorithm. However, it reveals space and
time inefficiency as sequence length increases, since
an m ? n matrix must be maintained and processed
during computations. This is the case with DNA se-
quence alignments, composed by many thousands of
nucleotides. Therefore, a huge optimization effort
were engaged and new algorithms appeared like k-
tuple, not guaranteeing to find optimal alignments
but able to tackle the complexity problem.
In our alignment tasks, we do not have these com-
180
plexity obstacles, because in our corpora the mean
length of a sentence is equal to 20.9 words, which
is considerably smaller than in a DNA sequence.
Therefore an implementation of the Needleman-
Wunsch algorithm has been used to generate optimal
global alignments.
The figure 2 exemplifies a global word alignment
on a paraphrase pair.
4.2 Local Alignment
The Smith-Waterman (SW) algorithm is similar to
the Needleman Wunsch (NW) one, since dynamic
programming is also followed hence denoting the
similar complexity issues, to which our alignment
task is immune. The main difference is that SW
seeks optimal sub-alignments instead of a global
alignment and, as described in the literature, it
is well tailored for pairs with considerable differ-
ences5, in length and type. In table 3 we exemplify
this by showing two character sequences6 where one
may clearly see that SW is preferable:
N Char. Sequences Alignments
1 ABBAXYTRVRVTTRVTR XYTRVFWHWWHGWGFXYTVWGF XYT-V
2 ABCDXYDRQR AB-CDDQZZSTABZCD ABZCD
Table 3: Preferable local alignment cases.
Remark that in the second pair, only the maximal
local sub-alignment is shown. However, there ex-
ists another sub-alignment: (DRQ, D-Q). This means
that local alignment may be tuned to generate not
only the maximum sub-alignment but a set of sub-
alignments that satisfy some criterium, like having
alignment value greater than some minimum thresh-
old. In fact, this is useful in our word alignment
problem and were experimented by adapting the
Smith Waterman algorithm.
4.3 Dynamic Alignment
According to the previous two subsections, where
two alignment strategies were presented, a natu-
ral question rises: which alignment algorithm to
use for our problem of inter-sentence word align-
ment? Initially, we thought to use only the global
5With sufficient similar sequences there is no difference be-
tween NW and SW.
6As in DNA subsequences and is same for word sequences.
alignment Needleman Wunsch algorithm, since a
complete inter-sentence word alignment is obtained.
However, we noticed that this strategy is unappro-
priate for certain pairs, specially when there are syn-
tactical alternations, like in the next example:
During his magnificent speech, :::the ::::::::president
:::::::::remarkably::::::praised::::IBM::::::::research.
:::The::::::::president:::::::praised::::IBM::::::::research, during hisspeech.
If a global alignment is applied for such a pair, then
weird alignments will be generated, like the one that
is shown in the next representation (we use character
sequences for space convenience and try to preserve
the word first letter, from the previous example):
D H M S T P R Q I S _ _ _
_ _ _ _ T P _ Q I S D H S
Here it would be more adequate to apply local align-
ment and extract all relevant sub-alignments. In this
case, two sub-alignments would be generated:
|D H M S| |T P R P I R|
|D H _ S| |T P _ P I R|
Therefore, for inter-paraphrase word alignments,
we propose a dynamic algorithm which chooses the
best alignment to perform: global or local. To com-
pute this pre-scan, we regard the notion of link-
crossing between sequences as illustrated in the fig-
ure 3, where the 4 crossings are signalized with the
small squares.
It is easily verifiable that the maximum number
of crossings, among two sequences with n exclusive
links in between is equal to ? = 12 ? n ? (n ? 1).
We suggest that if a fraction of these crossings holds,
for example 0.4 ? ? or 0.5 ? ?, then a local align-
ment should be used. Remark that the more this frac-
tion tends to 1.0 the more unlikely it is to use global
alignment.
Crossings may be calculated by taking index pairs
?xi, yi? to represent links between sequences, where
xi and yi are respectively the first and second se-
quence indexes, for instance in figure 3 the ?U?
link has pair ?5, 1?. It is easily verifiable that two
links ?xi, yi? and ?xj , yj? have a crossing point if:
(xi ? xj) ? (yi ? yj) < 0.
4.4 Alignment with Similarity Matrix
In bioinformatics, DNA sequence alignment algo-
rithms are usually guided by a scoring function, re-
lated to the field of expertise, that defines what is
181
the mutation probability between nucleotides. These
scoring functions are defined by PAM7 or BLO-
SUM8 matrices and encode evolutionary approx-
imations regarding the rates and probabilities of
amino acid mutations. Different matrices might pro-
duce different alignments.
Subsequently, this motivated the idea of model-
ing word mutation. It seems intuitive to allow such
a word mutation, considering the possible relation-
ships that exit between words: lexical, syntactical
or semantic. For example, it seems evident that be-
tween spirit and spiritual there exists a stronger rela-
tion (higher mutation probability) than between spir-
itual and hamburger.
A natural possibility to choose a word muta-
tion representation function is the Edit-distance
(Levenshtein, 1966) (edist(.,.)) as a negative re-
ward for word alignment. For a given word pair
?wi, wj?, the greater the Edit-distance value, the
more unlikely the word wi will be aligned with
word wj . However, after some early experiments
with this function, it revealed to lead to some prob-
lems by enabling alignments between very differ-
ent words, like ?total, israel?, ?fire,made? or
?troops,members?, despite many good alignments
also achieved. This happens because the Edit-
distance returns relatively small values, unable to
sufficiently penalize different words, like the ones
listed before, to inhibit the alignment. In bioinfor-
matics language, it means that even for such pairs
the mutation probability is still high. Another prob-
lem of the Edit-distance is that it does not distin-
guish between long and small words, for instance
the pairs ?in, by? and ?governor, governed? have
both the Edit-distance equals to 2.
As a consequence, we propose a new func-
tion (Equation 4) for word mutation penaliza-
tion, able to give better answers for the men-
tioned problems. The idea is to divide the Edit-
distance value by the length of the normalized9
maximum common subsequence maxseq(., .) be-
tween both words. For example, the longest
common subsequence for the pair ?w1, w2? =
?reinterpretation, interpreted? is ?interpret?,
7Point Access Mutation.
8Blocks Substitution Matrices.
9The length of the longest common subsequence divided by
the word with maximum length value.
with length equal to 9 and maxseq(w1, w2) =
9
max{16,11} = 0.5625
costAlign(wi, wj) = ? edist(wi, wj)?+maxseq(wi, wj) (4)
where ? is a small value10 that acts like a
?safety hook? against divisions by zero, when
maxseq(wi, wj) = 0.
word 1 word 2 -edist costAlign
rule ruler -1 -1.235
governor governed -2 -2.632
pay paying -3 -5.882
reinterpretation interpreted -7 -12.227
hamburger spiritual -9 -74.312
in by -2 -200.000
Table 4: Word mutation functions comparision.
Remark that with the costAlign(., .) scoring
function the problems with pairs like ?in, by? simply
vanish. The smaller the words, the more constrained
the mutation will be.
5 Experiments and Results
5.1 Corpus of Paraphrases
To test our alignment method, we used two types
of corpora. The first is the ?DUC 2002? corpus
(DUC2002) and the second is automatically ex-
tracted from related web news stories (WNS) auto-
matically extracted. For both original corpora, para-
phrase extraction has been performed by using the
Sumo-Metric and two corpora of paraphrases were
obtained. Afterwards the alignment algorithm was
applied over both corpora.
5.2 Quality of Dynamic Alignment
We tested the proposed alignment methods by giving
a sample of 201 aligned paraphrase sentence pairs
to a human judge and ask to classify each pair as
correct, acorrect11, error 12, and merror13. We also
asked to classify the local alignment choice14 as ad-
equate or inadequate. The results are shown in the
next table:
10We take ? = 0.01.
11Almost correct - minor errors exist
12With some errors.
13With many errors
14Global or local alignment.
182
Global Local
not para correct acorrect error merror adequate
31 108 28 12 8 12/14
15.5% 63.5% 16.5% 7.1% 4.7% 85.7%
Table 5: Precision of alignments.
For global alignments15 we have 11.8% pairs with
relevant errors and 85.7% (12 from 14) of all lo-
cal alignment decisions were classified as adequate.
The not para column shows the number of false
paraphrases identified, revealing a precision value of
84.5% for the Sumo-Metric.
6 Conclusion and Future Work
A set of important steps toward automatic construc-
tion of aligned paraphrase corpora are presented and
inherent relevant issues discussed, like clustering
and alignment. Experiments, by using 4 algorithms
and through visualization techniques, revealed that
clustering is a worthless effort for paraphrase cor-
pora construction, contrary to the literature claims
(Barzilay & Lee, 2003). Therefore simple para-
phrase pair extraction is suggested and by using
a recent and more reliable metric (Sumo-Metric)
(Anonymous, 2007) designed for asymmetrical en-
tailed pairs. We also propose a dynamic choosing of
the alignment algorithm and a word scoring function
for the alignment algorithms.
In the future we intend to clean the automatic
constructed corpus by introducing syntactical con-
straints to filter the wrong alignments. Our next step
will be to employ Machine Learning techniques for
rewriting rule induction, by using this automatically
constructed aligned paraphrase corpus.
References
Barzilay R. and Lee L. 2002. Bootstrapping Lexical
Choice via Multiple-Sequence Alignment. Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, (EMNLP), 164-171.
Barzilay, R., and Lee, L. 2003. Learning to para-
phrase: An unsupervised approach using multiple-
sequence alignment. Proceedings of HLT-NAACL.
15Percentage are calculated by dividing by 170 (201 ? 31)
the number of true paraphrases that exists.
Dolan W.B. and Brockett C. 2004. Unsupervised con-
struction of large paraphrase corpora: Exploiting
massively parallel news sources. Proceedings of 20th
International Conference on Computational Linguis-
tics (COLING 2004).
Anonymous 2007. Learning Paraphrases from WNS
Corpora. Proceedings of 20th International FLAIRS
Conference. AAAI Press. Key West, Florida.
Daelemans W., Hothker A., and Tjong E. 2004. Auto-
matic Sentence Simplification for Subtitling in Dutch
and English. In Proceedings of LREC 2004, Lisbon,
Portugal.
Heyer L.J., Kruglyak S. and Yooseph S. 1999. Exploring
Expression Data: Identification and Analysis of Coex-
pressed Genes. Genome Research, 9:1106-1115.
Hogg R., McKean J., and Craig A. 2005 Introduction
to Mathematical Statistics. Upper Saddle River, NJ:
Pearson Prentice Hall, 359-364.
Jain A., Murty M. and Flynn P. Data clustering: a review.
ACM Computing Surveys, 31:264-323
Knight K. and Marcu D. 2002. Summarization beyond
sentence extraction: A probabilistic approach to sen-
tence compression. Artificial Intelligence, 139(1):91-
107.
Kruskal J. B. and Wish M. 1977. Multidimensional Scal-
ing. Sage Publications. Beverly Hills. CA.
Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004.
Example-based sentence reduction using the hidden
markov model. ACM Transactions on Asian Language
Information Processing (TALIP), 3(2):146-158.
Levenshtein V. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Soviet
Physice-Doklady, 10:707-710.
Marsi E. and Krahmer E. 2005. Explorations in sentence
fusion. In Proceedings of the 10th European Work-
shop on Natural Language Generation.
Papineni K., Roukos S., Ward T., Zhu W.-J. 2001.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. IBM Research Report RC22176.
Shinyama Y., Sekine S., and Sudo K. 2002. Auto-
matic Paraphrase Acquisition from News Articles. Sao
Diego, USA.
Unno Y., Ninomiya T., Miyao Y. and Tsujii J. 2006.
Trimming CFG Parse Trees for Sentence Compres-
sion Using Machine Learning Approaches. In the Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions.
183
-0.2
-0.15
-0.1
-0.05
 0
 0.05
 0.1
 0.15
 0.2-0.25-0.2-0.15
-0.1-0.05 0
 0.05 0.1 0.15
 0.2-0.3
-0.2-0.1
 0 0.1
 0.2 0.3
Edit Distance
-0.35
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
 0
 0.05
 0.1
 0.15
 0.2-0.2-0.15
-0.1-0.05
 0 0.05
 0.1 0.15
-0.2-0.15
-0.1-0.05
 0 0.05
 0.1 0.15
 0.2
Word N-Gram Family
-0.25
-0.2
-0.15
-0.1
-0.05
 0
 0.05
 0.1
 0.15
 0.2
 0.25-0.2-0.15
-0.1-0.05 0
 0.05 0.1 0.15
 0.2-0.2
-0.15-0.1
-0.05 0
 0.05 0.1
 0.15 0.2
BLEU Metric
-0.04
-0.02
 0
 0.02
 0.04 -0.04
-0.02  0
 0.02 0.04
-0.04-0.02
 0 0.02
 0.04
Sumo-Metric
Figure 1: MDS on 500 sentences with the Edit Distance (top left), the BLEU Metric (top right), the Word
N-Gram Family (bottom left) and the Sumo-Metric (bottom right).
To the horror of their television fans , Miss Ball and Arnaz were divorced in 1960.
__ ___ ______ __ _____ __________ ____ _ ____ Ball and Arnaz ____ divorced in 1960.
Figure 2: Global aligned words in a paraphrase pair.
Figure 3: Crossings between a sequence pair.
184
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 15?22,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Unsupervised Induction of Sentence Compression Rules
Jo
?
ao Cordeiro
CLT and Bioinformatics
University of Beira Interior
Covilh?a, Portugal
jpaulo@di.ubi.pt
Ga
?
el Dias
CLT and Bioinformatics
University of Beira Interior
Covilh?a, Portugal
ddg@di.ubi.pt
Pavel Brazdil
LIAAD
University of Porto
Porto, Portugal
pbrazdil@liaad.up.pt
Abstract
In this paper, we propose a new unsu-
pervised approach to sentence compres-
sion based on shallow linguistic process-
ing. For that purpose, paraphrase extrac-
tion and alignment is performed over web
news stories extracted automatically from
the web on a daily basis to provide struc-
tured data examples to the learning pro-
cess. Compression rules are then learned
through the application of Inductive Logic
Programming techniques. Qualitative and
quantitative evaluations suggests that this
is a worth following approach, which
might be even improved in the future.
1 Introduction
Sentence compression, simplification or summa-
rization has been an active research subject dur-
ing this decade. A set of approaches involving
machine learning algorithms and statistical mod-
els have been experimented and documented in the
literature and several of these are described next.
1.1 Related Work
In (Knight & Marcu, 2002) two methods were
proposed, one is a probabilistic model - the
noisy channel model - where the probabili-
ties for sentence reduction (P{S
compress
|S)}
1
) are estimated from a training set of 1035
(Sentence, Sentence
compress
) pairs, manually
crafted, while considering lexical and syntacti-
cal features. The other approach learns syntac-
tic tree rewriting rules, defined through four op-
erators: SHIFT, REDUCE DROP and ASSIGN.
Sequences of these operators are learned from the
training set, and each sequence defines a complete
1
In the original paper the P (t|s) notation is used, where t
is the sentence in the target language and s the original sen-
tence in the source language.
transformation from an original sentence to the
compressed version.
In the work of (Le Nguyen & Ho, 2004)
two sentence reduction algorithms were also pro-
posed. The first one is based on template-
translation learning, a method inherited from the
machine translation field, which learns lexical
transformation rules
2
, by observing a set of 1500
(Sentence, Sentence
reduced
) pair, selected from
a news agency and manually tuned to obtain the
training data. Due to complexity difficulties found
for the application of this big lexical ruleset, they
proposed an improvement where a stochastic Hid-
den Markov Model is trained to help in the deci-
sion of which sequence of possible lexical reduc-
tion rules should be applied to a specific case.
An unsupervised approach was included in the
work of (Turner & Charniak, 2005), where train-
ing data are automatically extracted from the Penn
Treebank corpus, to fit a noisy channel model,
similar to the one used by (Knight & Marcu,
2002). Although it seems an interesting approach
to provide new training instances, it still be depen-
dent upon data manually labeled.
More recently, the work of (Clarke & Lapata,
2006) devise a different and quite curious ap-
proach, where the sentence compression task is
defined as an optimization goal, from an Integer
Programming problem. Several constraints are de-
fined, according to language models, linguistic,
and syntactical features. Although this is an unsu-
pervised approach, without using any paralel cor-
pus, it is completely knowledge driven, like a set
of crafted rules and heuristics incorporated into a
system to solve a certain problem.
1.2 Our Proposal
In this paper, we propose a new approach to
this research field, which follows an unsupervised
methodology to learn sentence compression rules
2
Those rules are named there as template-reduction rules.
15
based on shallow linguistic processing. We de-
signed a system composed of four main steps
working in pipeline, where the first three are re-
sponsible for data extraction and preparation and
in the last one the induction process takes place.
The first step gathers web news stories from re-
lated news events collected on a daily basis from
which paraphrases are extracted. In the second
step, word alignment between two sentences of
a paraphrase is processed. In the third step, spe-
cial regions from these aligned paraphrases, called
bubbles, are extracted and conveniently prepro-
cessed to feed the induction process. The whole
sequence is schematized in figure 1.
Figure 1: The Pipeline Architecture.
The induction process generates sentence re-
duction rules which have the following general
structure: L
cond
?X
cond
?R
cond
? suppress(X).
This means that the sentence segment X will
be eliminated if certain conditions hold over left
(L), middle (X) and right (R) segments
3
. In
Figure 2, we present seven different rules which
have been automatically induced from our archi-
tecture. These rules are formed by the conjunc-
tion of several literals, and they define constraints
under which certain sentence subparts may be
deleted, therefore compressing or simplifying the
sentence. The X symbol stands for the segment
3
For the sake of simplicity and compact representation,
we will omit the rule consequent, which is always the same
(?? suppress(X)?), whenever a rule is presented.
Z
(X)
= 1 ? L
c
= NP ?X
1
= JJ ?R
1
= IN (1)
Z
(X)
= 1 ? L
c
= NP ?X
1
= RB ?R
1
= IN (2)
Z
(X)
= 2 ? L
1
= and ?X
1
= the ?R
1
= JJ (3)
Z
(X)
= 2 ? L
1
= the ?X
2
= of ?R
1
= NN (4)
Z
(X)
= 2 ? L
1
= the ?X
c
= NP ?R
1
= NN (5)
Z
(X)
= 3 ? L
c
= PP ?X
1
= the ?R
c
= NP (6)
Z
(X)
= 3 ? L
c
= NP ?X
1
= and ?R
2
= V B (7)
Figure 2: Learned Sentence Compression Rules.
to be dropped, L
(?)
and R
(?)
are conditions over
the left and right contexts respectively. The nu-
meric subscripts indicate the positions
4
where a
segment constraint holds and the c subscript stands
for a syntactic chunk type. The Z
(?)
function com-
putes the length of a given segment, by counting
the number of words it contains. For instance, the
first rule means that a word
5
will be eliminated if
we have a NP (Noun Phrase) chunk in the left
context, and a preposition or subordinating con-
junction, in the right context (R
1
= IN ). The rule
also requires that the elimination word must be an
adjective, as we have X
1
= JJ .
This rule would be applied to the following seg-
ment
6
[NP mutual/jj funds/nns information/nn]
[ADJP available/jj] [PP on/in] [NP
reuters.com/nn]
and would delete the word available giving rise
to the simplified segment:
[NP mutual/jj funds/nns information/nn]
[PP on/in] [NP reuters.com/nn].
Comparatively to all existing works, we propose
in this paper a framework capable to extract com-
pression rules in a real world environment. More-
over, it is fully unsupervised as, at any step of the
process, examples do not need to be labeled.
In the remaining of the paper, we will present
the overall architecture which achieves precision
4
The position starts with 1 and is counted from left to
right, on the word segments, except for the left context, where
it is counted reversely.
5
As we have Z
(X)
= 1, the candidate segment size to
eliminate is equal to one.
6
The segment is marked with part-of-speech tags (POS)
and chunked with a shallow parser. Both transformations
were made with the OpenNLP toolkit.
16
values up to 85.72%, correctness up to 4.03 in 5
and utility up to 85.72%.
2 Data Preparation
Creating relevant training sets, with some thou-
sands examples is a difficult task, as well as is the
migration of such a system to process other lan-
guages. Therefore, we propose an unsupervised
methodology to automatically create a training set
of aligned paraphrases, from electronically avail-
able texts on the web. This step is done through
step one and step two of Figure 1, and the details
are described in the next two subsections.
2.1 Paraphrase Extraction
Our system collects web news stories on a daily
basis, and organized them into clusters, which
are exclusively related to different and unique
events, happening each day: ?a company acqui-
sition?, ?a presidential speech?, ?a bomb attack?,
etc. Usually, such clusters contain near 30 small
or medium news articles, collected from differ-
ent media sources. This environment proves to be
very fruitful for paraphrase extraction, since we
have many sentences conveying similar informa-
tion yet written in a different form.
A few unsupervised metrics have been applied
to automatic paraphrase identification and extrac-
tion (Barzilay & Lee, 2003; Dolan et al, 2004).
However, these unsupervised methodologies show
a major drawback by extracting quasi-exact or
even exact match pairs of sentences as they rely
on classical string similarity measures such as the
Edit Distance in the case of (Dolan et al, 2004)
and Word N-gram Overlap for (Barzilay & Lee,
2003). Such pairs are useless for our purpose,
since we aim to identify asymmetrical paraphrase
pairs to be used for sentence compression rule
induction, as explained in (Cordeiro et al, Oct
2007). There we proposed a new metric, the
Sumo-Metric, specially designed for asymmetrical
entailed pairs identification, and proved better per-
formance over previous established metrics, even
in the specific case when tested with the Microsoft
Paraphrase Research Corpus (Dolan et al, 2004),
which contains mainly symmetrical cases. For a
given sentence pair, having each sentence x and
y words, and with ? exclusive links between the
sentences, the Sumo-Metric is defined in Equation
8 and 9.
S(S
a
, S
b
) =
8
>
>
<
>
>
:
S(x, y, ?) if S(x, y, ?) < 1.0
0 if ? = 0
e
?k?S(x,y,?)
otherwise
(8)
where
S(x, y, ?) = ? log
2
(
x
?
) + ? log
2
(
y
?
) (9)
with ?, ? ? [0, 1] and ?+ ? = 1.
We have shown (Cordeiro et al, Oct 2007) that
Sumo-Metric outperforms all state-of-the-art met-
rics over all tested corpora and allows to identify-
ing similar sentences with high probability to be
paraphrases. In Figure 3, we provide the reader
with an example of an extracted paraphrase.
(1) To the horror of their fans, Miss Ball
and Arnaz were divorced in 1960.
(2) Ball and Arnaz divorced in 1960.
Figure 3: An Assymetrical Paraphrase
2.2 Paraphrase Alignment
From a corpus of asymmetrical paraphrases, we
then use biology-based gene alignment algorithms
to align the words contained in each of the two
sentences within each paraphrase. For that pur-
pose, we implemented two well established algo-
rithms, one identifying local alignments (Smith
& Waterman, 1981) and the other one computing
global alignments (Needleman & Wunsch, 1970).
We also proposed a convenient dynamic strategy
(Cordeiro et al, 2007), which chooses the best
alignment algorithm to be applied to a specific
case at runtime.
The difference between local and global se-
quence alignments is illustrated below, where we
use letters, instead of words, to better fit our paper
space constraints. Suppose that we have the fol-
lowing two sequences: [D,H,M,S,T,P,R,Q,I,S]
and [T,P,Q,I,S,D,H,S] a global alignment
would produce the following pair.
D H M S T P R Q I S _ _ _
_ _ _ _ T P _ Q I S D H S
For the same two sequences, a local alignment
strategy could generate two or more aligned sub-
sequences as follows.
17
|D H M S| |T P R Q I S|
|D H _ S| |T P _ Q I S|
Hence, at this stage of the process, we end with a
corpus of aligned
7
asymmetrical paraphrases. In
Figure 4, we present the alignment of the para-
phrase of Figure 3.
(1) To the horror of their fans ,
(2) __ ___ ______ __ _____ ____ _
(1) Miss Ball and Arnaz were divorced in 1960.
(2) ____ Ball and Arnaz ____ divorced in 1960.
Figure 4: An Aligned Paraphrase
The next section describes how we use this
structured data to extract instances which are go-
ing to feed a learning system.
3 Bubble Extraction
In order to learn rewriting rules, we have focus
our experiences on a special kind of data, se-
lected from the corpus of aligned sentences, and
we named this data as Bubbles
8
. Given two word
aligned sentences, a bubble is a non-empty seg-
ment aligned with an empty segment of the other
sentence of the paraphrase, sharing a ?strong? con-
text. In Figure 5, we show different examples of
bubbles.
the situation here in chicago with the workers
the situation ____ in chicago with the workers
obama talks exclusively with tom brokaw on meet
obama talks ___________ with tom brokaw on meet
Ball and Arnaz were divorced in 1960
Ball and Arnaz ____ divorced in 1960
america is in the exact same seat as sweigert and
america is in ___ _____ same seat as sweigert and
after a while at the regents park gym, the president
after a while at ___ _______ ____ gym, the president
Figure 5: Examples of Bubbles
To extract a bubble, left and right contexts of
equally aligned words must occur, and the proba-
bility of such extraction depends on the contexts
size as well as the size of the region aligned with
the empty space. The main idea is to eliminate
cases where the bubble middle sequence is too
large when compared to the size of left and right
contexts. More precisely, we use the condition in
7
By ?aligned? we mean, from now on, word alignment
between paraphrase sentence pairs.
8
There are other possible regions to explore, but due to
the complexity of this task, we decided to initially work only
with bubbles
Equation 10 to decide whether a bubble should be
extracted or not.
Z
(L)
? Z
(X)
+ Z
(R)
? 0 (10)
whereL andR stand for the left and right contexts,
respectively, and X is the middle region. The Z
(?)
function computes the length of a given segment,
in terms of number of words. For example, in the
first and last examples of Figure 5, we have: 2 ?
1+5 = 6 ? 0 and 4?3+4 = 5 ? 0. In this case,
both bubbles will be extracted. This condition is
defined to prevent from extracting eccentric cases,
as the ones shown in the examples shown in Figure
6, where the conditions respectively fail: 0 ? 8 +
3 = ?5 < 0 and 1? 7 + 2 = ?4 < 0.
To the horror of their fans , Miss Ball and Arnaz
__ ___ ______ __ _____ ____ _ ____ Ball and Arnaz
will vote __ ___ _______ ____ __ _____ __ friday .
____ vote on the amended bill as early as friday .
Figure 6: Examples of Rejected Bubbles
Indeed, we favor examples with high common
contexts and few deleted words to enhance the in-
duction process.
So far, we only consider bubbles where the
middle region is aligned with a void segment
(X
transf
?? ?). However, more general transforma-
tions will be investigated in the future. Indeed, any
transformation X
transf
?? Y , where Y 6= ?, having
Z
(X)
> Z
(Y )
, may be a relevant compression ex-
ample.
Following this methodology, we obtain a huge
set of examples, where relevant sentence transfor-
mations occur. To have an idea about the amount
of data we are working with, from a set of 30 days
web news stories (133.5 MB of raw text), we iden-
tified and extracted 596678 aligned paraphrases,
from which 143761 bubbles were obtained.
In the next section, we show how we explore
Inductive Logic Programming (ILP) techniques to
generalize regularities and find conditions to com-
press sentence segments.
4 The Induction of Compression Rules
Many different algorithms exist to induce knowl-
edge from data. In this paper, we use Inductive
Logic Programming (ILP) (Muggleton, 1991) and
it was a choice based on a set of relevant fea-
tures like: the capacity to generate symbolic and
18
relational knowledge; the possibility to securely
avoid negative instances; the ability to mix differ-
ent types of attribute and to have more control over
the theory search process.
Unlike (Clarke & Lapata, 2006), we aim at
inducing human understandable knowledge, also
known as symbolic knowledge. For that pur-
pose, ILP satisfies perfectly this goal by produc-
ing clauses based on first order logic. Moreover,
most of the learning algorithms require a com-
plete definition and characterization of the feature
set, prior to the learning process, where any at-
tribute must be specified. This is a conceptual bot-
tleneck to many learning problems such as ours,
since we need to combine different types of at-
tributes i.e. lexical, morpho-syntactic and syntac-
tical. With ILP, we only need to define a set of pos-
sible features and the induction process will search
throughout this set.
4.1 The Aleph System
The Aleph system(Srinivasan, 2000) is an empir-
ical ILP system, initially designed to be a pro-
totype for exploring ILP ideas. It has become a
quite mature ILP implementation, used in many
research projects, ranging form Biology to NLP. In
fact, Aleph is the successor of several and ?more
primitive? ILP systems, like: Progol (Muggleton,
1999), FOIL (Quinlan, 1990), and Indlog (Cama-
cho, 1994), among others, and may be appropri-
ately parametrized to emulate any of those older
systems.
One interesting advantage in Aleph is the possi-
bility to learn exclusively from positive instances,
contrarily to what is required by most learning sys-
tems. Moreover, there is theoretical research work
(Muggleton, 1996) demonstrating that the increase
in the learning error tend to be negligible with the
absence of negative examples, as the number of
learning instances increases. This is a relevant
issue, for many learning domains, and specially
ours, where negative examples are not available.
4.2 Learning Instances
In our problem, we define predicates that charac-
terize possible features to be considered during the
induction process. Regarding the structure of our
learning instances (bubbles), we define predicates
which restrict left and right context sequences as
well as the aligned middle sequence. In particu-
lar, we limit the size of our context sequences to
a maximum of three words and, so far, only use
bubbles in which the middle sequence has a max-
imum length of three
9
words. The notion of con-
texts from bubbles is clarified with the next exam-
ple.
L2 L1 X1 X2 X3 R1 R2 R3 R4
L2 L1 __ __ __ R1 R2 R3 R4
For such a case, we consider [L1, L2] as the left
context, [R1, R2, R3] as the right context, and
[X1, X2, X3] as the aligned middle sequence.
Such an example is represented with a Prolog term
with arity 5 (bub/5) in the following manner:
bub(ID, t(3,0), [L1,L2],
[X1,X2,X3]--->[],
[R1,R2,R3]).
The ID is the identifier of the sequence instance,
t/2 defines the ?transformation dimension?, in
this case from 3 words to 0. The third and fifth
arguments are lists with the left and right con-
texts, respectively, and the fourth argument con-
tains the list with the elements deleted from the
middle sequence. It is important to point out that
every L
i
, X
i
andR
i
are structures with 3 elements
such as word/POS/Chunk. For example, the
word president would be represented by the
expanded structure president/nn/np.
4.3 Feature Space
As mentioned previously, with an ILP system, and
in particular with Aleph, the set of attributes is
defined through a set of conditions, expressed in
the form of predicates. These predicates are the
building blocks that will be employed to construct
rules, during the induction process. Hence, our at-
tribute search space is defined using Prolog pred-
icates, which define the complete set of possibil-
ities for rule body construction. In our problem,
we let the induction engine seek generalization
conditions for the bubble main regions (left, mid-
dle, and right). Each condition may be from one
of the four types: dimensional, lexical, POS, and
chunk. Dimensional conditions simply express
the aligned sequence transformation dimensional-
ity. Lexical conditions impose a fixed position to
match a given word. The POS condition is similar
to the lexical one, but more general, as the position
must match a specific part-of-speech tag. Likely,
chunk conditions bind a region to be equal to a
particular chunk type. For example, by looking
9
They represent 83.47% from the total number of ex-
tracted bubbles.
19
at Figure 2, the attentive reader may have noticed
that these three conditions are present in rule 7. In
terms of Aleph declaration mode, these conditions
are defined as follows.
:- modeh(1,rule(+bub)).
:- modeb(1,transfdim(+bub,n(#nat,#nat))).
:- modeb(3,chunk(+bub,#side,#chk)).
:- modeb(
*
,inx(+bub,#side,#k,#tword)).
:- determination(rule/1,transfdim/2).
:- determination(rule/1,chunk/3).
:- determination(rule/1,inx/4).
The inx/4 predicate defines lexical and POS
type conditions, the chunk/3 predicate de-
fines chunking conditions and the transfdim/2
predicate defines the transformation dimension-
ality, which is in the form transfdim(N,0)
with N>0, according to the kind of bubbles we are
working with.
4.4 The Rule Value Function
The Aleph system implements many different
evaluation
10
functions which guide the theory
search process, allowing the basic procedure for
theory construction to be altered. In order to bet-
ter fit to our problem, we define a new evaluation
function calculated as the geometrical mean be-
tween the coverage percentage and the rule size
value, as shown in Equation 11 whereR is the can-
didate rule and Cov(R) is the proportion of posi-
tive instances covered by R and the LV (?) func-
tion defines the rule value in terms of its length,
returning a value in the [0, 1] interval.
V alue(R) =
p
Cov(R)? LV (R) (11)
The V alue(?) function guides the induction
process, by preferring not too general rules having
maximum possible coverage value. As shown in
Figure 7, the V alue(?) function gives preferences
to rules with 3, 4 and 5 literals.
5 Results
The automatic evaluation of a system is always the
best way to do it, due to its objectivity and scal-
ability. However, in many cases it is unfeasible
for several practical reasons, like the unavailability
of data or the difficulty to prepare an appropriate
10
In the Aleph terminology, this function is named as the
?cost? function, despite the fact that it really computes the
value in the sense that the grater the value, the more likely it
is to be chosen.
0
10
20
30
40
50
60
70
80
90
100
10
25
50
90
60
40
20
1 2 3 4 5 6 7
n
o
clauses
value
Figure 7: Rule length value function
dataset. Some supervised learning approach use
manually labeled test sets to evaluated their sys-
tems. However, these are small test sets, for exam-
ple, (Knight & Marcu, 2002) use a set of 1035 sen-
tences to train the system and only 32 sentences
to test it, which is a quite small test set. As a
consequence, it is also important to propose more
through evaluation. In order to assess as clearly
as possible the performance of our methodology
on large datasets, we propose a set of qualitative
and quantitative evaluations based on three differ-
ent measures: Utility, Ngram simplification and
Correctness.
5.1 Evaluation
A relevant issue, not very commonly discussed, is
the Utility of a learned theory. In real life prob-
lems, people may be more interested in the vol-
ume of data processed than the quality of the re-
sults. Maybe, between a system which is 90%
precise and processes only 10% of data, and a sys-
tem with 70% precision, processing 50% of data,
the user would prefer the last one. The Utility
may be a stronger than the Recall measure, used
for the evaluation of supervised learning systems,
because the later measures how many instances
were well identified or processed from the test set
only, and the former takes into account the whole
universe. For example, in a sentence compres-
sion system, it is important to know how many
sentences would be compressed, from the whole
possible set of sentences encountered in electronic
news papers, or in classical literature books, or
both. This is what we mean here by Utility.
The Ngram-Simplification methodology is an
automatic extrinsic test, performed to perceive
how much a given sentence reduction ruleset
would simplify sentences in terms of syntactical
complexity. The answer is not obvious at first
sight, because even smaller sentences can contain
20
more improbable syntactical subsequences than
their uncompressed versions. To evaluate the syn-
tactical complexity of a sentence, we use a 4 ?
gram model and compute a relative
11
sequence
probability as defined in Equation 12 where
~
W =
[t
1
, t
2
, ..., t
m
] is the sequence of part-of-speech
tags for a given sentence with size m.
P{
~
W} =
?
m?n
Y
k=n
P{t
k
| t
k?1
, ..., t
k?n
}
?
1
m
(12)
The third evaluation is qualitative. We measure
the quality of the learned rules when applied to
sentence reduction. The objective is to assess how
correct is the application of the reduction rules.
This evaluation was made through manual annota-
tion for a statistically representative random sam-
ple of compressed sentences. A human judged
the adequacy and Correctness of each compres-
sion rule to a given sentence segment, in a scale
from 1 to 5, where 1 means that it is absolutely in-
correct and inadequate, and 5 that the compression
rule fits perfectly to the situation (sentence) being
analyzed.
To perform our evaluation, a sample of 300 sen-
tences were randomly extracted, where at least one
compression rule had been applied. This eval-
uation set may be subdivided into three subsets,
where 100 instances came from rules with Z
(X)
=
1 (BD1), 100 from rules with Z
(X)
= 2 (BD2),
and the other 100 from rules with Z
(X)
= 3
(BD3). Another random sample, also with 100
cases has been extracted to evaluate our base-line
(BL) which consists in the direct application of
the bubble set to make compressions. This means
that no learning process is performed. Instead, we
store the complete bubble set as if they were rules
by themselves (in the same manner as (Le Nguyen
& Ho, 2004) do).
Table 1 compiles the comparative results
for Correctness, Precision, Utility and Ngram-
simplification for all datasets. In particular,
Ngram-simplification in percentage is the pro-
portion of test cases where P{reduced(
~
W )} ?
P{
~
W}.
Table 1 provides evidence of the improvement
achieved with the induction rules, in comparison
with the base line, on each test parameter: Cor-
rectness, Utility and Ngram-simplification. Con-
11
Because it is raised to the inverse power of m, which is
the number of words in the sentence.
Parameter BL BD1 BD2 BD3
Correctness: 2.93 3.56 4.03 4.01
Precision: 58.60% 71.20% 80.60% 80.20%
Utility: 8.65% 32.67% 85.72% 26.86%
NG-Simpl: 47.39% 89.33% 90.03% 89.23%
Table 1: Results with Four Evaluation Parameters.
sidering the three experiences, BD1, BD2, and
BD3, as a unique evaluation run, we obtained a
mean Correctness quality of 3.867 (i.e. 77.33%
Precision), a mean Utility of 48.45%, and a mean
Ngram-simplification equal to 89.53%, which are
significantly better than the base line.
Moreover, best results overall are obtained for
BD2 with 80.6% Precision, 85.72% Utility and
90.03% Ngram-simplification which means that
we can expect a reduction of two words with high
quality for a great number of sentences. In partic-
ular, Figure 2 shows examples of learned rules.
5.2 Time Complexity
In the earlier
12
days of ILP, the computation time
spent by their systems was a serious difficult ob-
stacle, disabling its implementation for real life
problems. However, nowadays these time ef-
ficiency issues have been overcome, opening a
wide range of application possibilities, for many
problems, from Biology to Natural Language Pro-
cessing. The graph in figure 8, shows that even
with considerable big datasets, our learning sys-
tem (based on Aleph) evidences acceptable feasi-
ble computation time.
0
20
40
60
80
100
120
140
12
27
0
53
0
120
0
10 20 30 40 50 60
10
3
bubbles
seconds
Figure 8: Time spent during the induction process,
for datasets with size expressed in thousands of
bubbles.
To give an idea about the size of an induced
rule set, and taking as an example the learned rules
12
In the 1990-2000 decade.
21
with Z
(X)
= 2, these were learned from a dataset
containing 37271 t(2, 0) bubbles, and in the final
5806 sentence reduction rules were produced.
6 Conclusion and Future Directions
Sentence Compression is an active research topic,
where several relevant contributions have recently
been proposed. However, we believe that many
milestones still need to be reached. In this pa-
per, we propose a new framework in the form of
a pipeline, which processes huge sets of web news
articles and retrieves compression rules in an un-
supervised way. For that purpose, we extract and
align paraphrases, explore and select specific text
characteristics called bubbles and finally induce a
set of logical rules for sentence reduction in a real-
world environment. Although we have only con-
sidered bubbles having Z
(X)
? 3, a sentence may
have a compression length greater than this value,
since several compression rules may be applied to
a single sentence.
Our results evidence good practical applicabil-
ity, both in terms of Utility, Precision and Ngram-
simplification. In particular, we assess results up
to 80.6% Precision, 85.72% Utility and 90.03%
Ngram-simplification for reduction rules of two
word length. Moreover, results were compared to
a base line set of rules produced without learning
and the difference reaches a maximum improve-
ment using Inductive Logic Programming of 22%.
Acknowledgments
This work was supported by the VIPACCESS
project - Ubiquitous Web Access for Visually Im-
paired People. Funding Agency: Fundac??ao para
a Ci?encia e a Tecnologia (Portugal). Reference:
PTDC/PLP/72142/2006.
References
Barzilay R. and Lee L.. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In HLT-NAACL 2003: Main Proceed-
ings, pages 16?23.
Camacho R. 1994. Learning stage transition rules with
Indlog. Gesellschaft f?ur Mathematik und Datenver-
arbeitung MBH., Volume 237 of GMD- Studien, pp.
273-290.
Clarke J., and Lapata M. 2006. Constraint-based Sen-
tence Compression: An Integer Programming Ap-
proach. 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics.
Cordeiro J. and Dias G. and Cleuziou G. 2007. Bi-
ology Based Alignments of Paraphrases for Sen-
tence Compression. In Proceedings of the Workshop
on Textual Entailment and Paraphrasing (ACL-
PASCAL / ACL2007), Prague, Czech Republic.
Cordeiro J. and Dias G. and Brazdir P. October
2007. New Functions for Unsupervised Asymmet-
rical Paraphrase Detection. In Journal of Software.,
Volume:2, Issue:4, Page(s): 12-23. Academy Pub-
lisher. Finland. ISSN: 1796-217X.
Dolan W.B. and Quirck C. and Brockett C. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of 20th International Conference on Com-
putational Linguistics (COLING 2004).
Knight K. and Marcu D. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91-107.
Muggleton S. 1991. Inductive Logic Programming.
New Generation Computing, 8 (4):295-318.
Muggleton S. 1996. Learning from positive data. Pro-
ceedings of the Sixth International Workshop on In-
ductive Logic Programming (ILP-96), LNAI 1314,
Berlin, 1996. Springer-Verlag.
Muggleton S. 1999. Inductive logic programming: Is-
sues, results and the challenge of learning language
in logic. Artificial Intelligence, 114 (1-2), 283?296.
Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004.
Example-based sentence reduction using the hidden
markov model. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 3(2):146-
158.
Needleman SB, Wunsch CD. 1970. A general method
applicable to the search for similarities in the amino
acid sequence of two proteins. Journal of Molecular
Biology, 48 (3): 443?53.
Quinlan J. R. 1990. Learning Logical Deinitions from
Relations. Machine Learning., 5 (3), 239-266. 33,
39, 41.
Smith TF, Waterman MS. 1981. Identification of Com-
mon Molecular Subsequences. Journal of Molecu-
lar Biology, 147: 195?197.
Srinivasan A. 2000. The Aleph Manual, Technical
Report. Computing Laboratory, Oxford University,
UK.
Turner J, Charniak E. 2005. Supervised and Unsuper-
vised Learning for Sentence Compression. Proceed-
ings of the 43rd Annual Meeting of the ACL, pages
290-297.
22
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 403?411,
Beijing, August 2010
Paraphrase Alignment for Synonym Evidence Discovery
Gintare? Grigonyte?
Saarland University
g.grigonyte@hmf.vdu.lt
Joa?o Cordeiro, Gae?l Dias,
Rumen Moraliyski
HULTIG
University of Beira Interior
{jpaulo,ddg,rumen}@di.ubi.pt
Pavel Brazdil
LIAAD/FEP
University of Porto
pbrazdil@liaad.up.pt
Abstract
We describe a new unsupervised approach
for synonymy discovery by aligning para-
phrases in monolingual domain corpora.
For that purpose, we identify phrasal
terms that convey most of the concepts
within domains and adapt a methodol-
ogy for the automatic extraction and align-
ment of paraphrases to identify para-
phrase casts from which valid synonyms
are discovered. Results performed on two
different domain corpora show that gen-
eral synonyms as well as synonymic ex-
pressions can be identified with a 67.27%
precision.
1 Introduction
Synonymy is a specific type of a semantic re-
lationship. According to (Sowa and Siekmann,
1994), a synonym is a word (or concept) that
means the same or nearly the same as another
word (or concept). It has been observed that
words are similar if their contexts are similar (Fre-
itag et al, 2005) and so synonymy detection has
received a lot of attention during the last decades.
However, words used in the same context are
not necessarily synonyms and can embody dif-
ferent semantic relationships such as hyponyms,
meronyms or co-hyponyms (Heylen et al, 2008).
In this paper, we introduce a new unsupervised
methodology for synonym detection by extract-
ing and aligning paraphrases on normalized do-
main corpora1. In particular, we study a specific
structure within aligned paraphrases, paraphrase
1By normalized, we intend that phrasal terms have been
previously identified.
casts, from which valid synonyms are discovered.
In fact, we propose a new approach based on the
idea that synonyms are substitutable words within
a given domain corpus. Results performed on two
different domain corpora, the Corpus of Computer
Security (COCS) and the Corpus of Cancer Re-
search (COCR), show that general synonyms as
well as synonymic expressions can be identified
with a 67.27% precision performance.
2 Related Work
Automatic synonymy detection has been tackled
in a variety of ways which we explain as follows.
2.1 Pattern-based Approaches
This approach to information extraction is based
on a technique called selective concept extraction
as defined by (Riloff, 1993). Selective concept
extraction is a form of text skimming that selec-
tively processes relevant text while effectively ig-
noring surrounding text that is thought to be ir-
relevant to the domain. The pioneer of pattern-
based approaches (Hearst, 1992) has introduced
lexico-syntactic patterns to automatically acquire
given word semantic relationships. Specific pat-
terns like ?X and other Y? or ?X such as Y? were
used for hypernym-hyponym detection. Later, the
idea was extended and adapted for synonymy by
other researchers such as (Roark and Charniak,
1998), (Caraballo, 1999) and (Maynard and Pe-
ters, 2009). In general, manual pattern definition
is time consuming and requires linguistic skills.
Usually, systems based on lexico-syntactic pat-
terns perform with very high precision, but low
recall due to the fact that these patterns are rare.
However, recent work by (Ohshima and Tanaka,
403
2009) on Web data reported high recall figures.
To avoid manual encoding of patterns, many su-
pervised approaches have been proposed as sum-
marized in (Stevenson and Greenwood, 2006).
2.2 Distributional Similarity
Distributional similarity for capturing semantic
relatedness is relying on the hypothesis that se-
mantically similar words share similar contexts.
These methods vary in the level of supervision
from unsupervised to semi-supervised or to su-
pervised. The first type of methods includes the
work of (Hindle, 1990), (Lin, 1998) and (Heylen
et al, 2008) who used unsupervised methods
for detecting word similarities based on shallow-
parsed corpora. Others have proposed unsuper-
vised methodologies to solve TOEFL-like tests,
instead of discovering synonyms (Turney, 2001),
(Terra and Clarke, 2003) and (Freitag et al, 2005).
Other researchers, such as (Girju et al, 2004),
(Muller et al, 2006), (Wu and Zhou, 2003) and
(Wei et al, 2009), have used language or knowl-
edge resources to enhance the representation of
the vector space model. Unlike the pattern-based
approach, the distributional similarity-based ap-
proach shows low precision compared to high re-
call.
One obvious way to verify all the possible con-
nections between words of the vocabulary em-
ploys an exhaustive search. However, compari-
son based on word usage can only highlight those
terms that are highly similar in meaning. This
method of representation is usually unable to dis-
tinguish between middle strength and weak se-
mantic relations, or detect the relationship be-
tween hapax-legomena.
2.3 Hybrid Approaches
More recently, approaches combining patterns
and distributional similarity appeared to bring the
best of the two methodologies. (Hagiwara et
al., 2009) describe experiments that involve train-
ing various synonym classifiers. (Giovannetti et
al., 2008) use syntactically parsed text and man-
ually composed patterns together with distribu-
tional similarity for detecting semantically related
words. Finally, (Turney, 2008) proposes a super-
vised machine learning approach for discovering
synonyms, antonyms, analogies and associations.
For that purpose, feature vectors are based on fre-
quencies of patterns and classified by a SVM.
2.4 Our Approach
(Van der Plas and Tiedemann, 2006) state that
?People use multiple ways to express the same
idea. These alternative ways of conveying the
same information in different ways are referred
to by the term paraphrase and in the case of
single words or phrasal terms sharing the same
meaning, we speak of synonyms?. Based on this,
we propose that in order to discover pairs of se-
mantically related words (in the best case syn-
onyms) that may be used in figurative or rare
sense, and as consequence impossible to be iden-
tified by the distributional similarity approach,
we need to have them highlighted by their lo-
cal specific environment. Here we differ from
the pattern-based approach that use local general
environment. We propose to align paraphrases
from domain corpora and discover words that are
possibly substitutable for one another in a given
context (paraphrase casts), and as such are syn-
onyms or near-synonyms. Comparatively to exist-
ing approaches, we propose an unsupervised and
language-independent methodology which does
not depend on linguistic processing2, nor manual
definition of patterns or training sets and leads to
higher precision when compared to distributional
similarity-based approaches.
3 Normalization of the Corpora
The main goal of our research is to build knowl-
edge resources in different domains that can ef-
fectively be used in different NLP applications.
As such, precision takes an important part in the
overall process of our methodology. For that pur-
pose, we first identify the phrasal terms (or multi-
word units) present in the corpora. Indeed, it has
been shown in many works that phrasal terms con-
vey most of the specific contents of a given do-
main. Our approach to term extraction is based
on linguistic pattern matching and Inverse Doc-
ument Frequency (IDF) measurements for term
2We will see in the next section that we will use linguistic
resources to normalize our corpora, but the methodology can
be applied to any raw text.
404
quality assurance as explained in (Avizienis et al,
2009). For that purpose, we present a domain in-
dependent hybrid term extraction framework that
includes the following steps. First, the text is
morphologically annotated with the MPRO sys-
tem (Maas et al, 2009). Then grammar rules for
morphological disambiguation, syntactic parsing
and noun phrase detection are applied based on
finite-state automata technology, KURD (Carl and
Schmidt-Wigger, 1998). Following this, a vari-
ant and non-basic term form detection is applied,
as well as stop words removal. Then, combining
rich morphological and shallow syntactical analy-
sis with pattern matching techniques allows us to
extract a wide span of candidate terms which are
finally filtered with the well-known IDF measure.
4 Paraphrase Identification
A few unsupervised metrics have been applied to
automatic paraphrase identification and extraction
(Barzilay and McKeown, 2001) and (Dolan et al,
2004). However, these unsupervised methodolo-
gies show a major drawback by extracting quasi-
exact or even exact match pairs of sentences as
they rely on classical string similarity measures.
Such pairs are useless for our research purpose.
More recently, (Cordeiro et al, 2007a) proposed
the sumo metric specially designed for asymmet-
rical entailed pair identification in corpora which
obtained better performance than previously es-
tablished metrics, even in corpora with exclu-
sively symmetrical entailed paraphrases as in the
Microsoft Paraphrase Research Corpus (Dolan et
al., 2004). This function states that for a given
sentence pair ?Sa, Sb?, having m and n words in
each sentence and ? lexical exclusive links (word
overlaps, see figure 1) between them, its lexi-
cal connection strength is computed as defined in
Equations 1 and 2.
Sumo(Sa, Sb) =
?
?
?
S(m,n, ?) if S(m,n, ?) < 1
0 if ? = 0
e?kS(m,n,?) otherwise
(1)
where
S(m,n, ?) = ? log2(m? ) + ? log2(
n
? )
?, ? ? [0, 1], ? + ? = 1
(2)
Figure 1: 4 exclusive links between Sa and Sb.
To obtain a paraphrase corpus, we compute all
sentence pairs similarities Sumo(Sa, Sb) and se-
lect only those pairs exceeding a given threshold,
in our case threshold = 0.85, which is quite re-
strictive, ensuring the selection of pairs strongly
connected3.
However, to take into account the normalization
of the corpus, little adjustments had to be inte-
grated in the methodology proposed in (Cordeiro
et al, 2007a). Indeed, the original Sumo(., .)
function was under-weighting links that occurred
between phrasal terms such as ?molecular labo-
ratory? or ?renal cancer?. So, instead of counting
the number of lexical links among sentences, each
link weights differently according to the word
length in the connection, hence connections of
longer words will result in a larger value. For ex-
ample, in figure 1, instead of having ? = 4, we
count ? = 3 + 8 + 7 + 4 = 22. This adjust-
ment is important as multi-word units are treated
as longer words in the corpus. This modification
has also, as a side effect, under-evaluation of func-
tional words which usually follow the Zipf?s Law
and give more importance to meaningful words in
the paraphrase extraction process.
5 Paraphrase Alignment
In order to usefully explore the evidence syn-
onymy from paraphrases, sentence alignment
techniques must be applied to paraphrases in or-
der to identify paraphrase casts, i.e., substitutable
word pairs as shown in figure 2. As we can see,
the paraphrase cast includes three parts: the left
segment (context), a middle segment and the right
segment (context). In our figure the left and right
segments (contexts) are identical.
In the context of DNA sequence alignment,
two main algorithms have been proposed: (1) the
Needleman-Wunsch algorithm (Needleman and
3Further details about the sumo metric are available in
(Cordeiro et al, 2007a).
405
Figure 2: A paraphrase cast.
Wunsch, 1970) based on dynamic programming
which outputs a unique global alignment and (2)
the Smith-Waterman (SW) algorithm (Smith and
Waterman, 1981) which is an adaptation of the
previous algorithm and outputs local alignments.
In the context of NLP, (Cordeiro et al, 2007a)
proposed a combination of both algorithms de-
pending on the structure of paraphrase. How-
ever, since any local alignment is a candidate for
paraphrase casts, the SW algorithm revealed it-
self more appropriate and was always chosen. The
SW alignment algorithm uses dynamic program-
ming to compute the optimal local alignments be-
tween two sequences4. This process requires first
the definition of an alignment matrix (function),
which governs the likelihood of alignment of two
symbols. Thus we first build a matrix H such that
H(i, 0) = 0 and H(0, j) = 0, for 0 ? i ? m,
and 0 ? j ? n, where m and n are the number of
words in the paraphrase sentences. The rest of the
H elements are recursively calculated as in Equa-
tion 3 where gs(., .) is the gap-scoring function
and Sai (resp. Sbj ) represents the ith (resp. jth)
word of sentence Sa (resp. Sb).
H(i, j) = max
?
????
????
0
H(i? 1, j ? 1) + gs(Sai , Sbj ) MMisatch
H(i? 1, j) + gs(Sai , ) Deletion
H(i, j ? 1) + gs( , Sbj ) Insertion
(3)
Obviously, this algorithm is based on an align-
ment function which exploits the alignment like-
lihood between two alphabet symbols. For DNA
sequence alignments, this function is defined as
a mutation matrix, scoring gene mutation and gap
alignments. In our case, we define the gap-scoring
4In our case, the two sequences are the two sentences of
a paraphrase
function gs(., .) in Equations 4 and 5 which prior-
itize the alignment of specific domain key terms
i.e., single match, or key expressions i.e., phrasal
match, (reward 50), as well as lexically similar5
words such as ?programme? and ?programming?
for example. In particular, for these similar words
an adaptation of the well known Edit Distance is
used, the c(., .) function (5), which is explained in
more detail in (Cordeiro et al, 2007b).
gs(Sai , Sbj ) =
?
???????
???????
?1 if (Sai = ?) and (Sbj 6= ?)
?1 if (Sbj = ?) and (Sai 6= ?)
10 Single Match
50 Phrasal Match
c(Sai , Sbj ) Mismatch
(4)
where
c(Sai , Sbj ) = ?
edist(Sai , Sbj )
+maxseq(Sai , Sbj )
(5)
To obtain local alignments, the SW algorithm is
applied, using the alignment function defined with
H(., .) in 3. The alignment of the paraphrase in
figure 2 would give the result in figure 3.
Figure 3: An alignment.
6 Paraphrase Casts
In order to discover synonyms, we are looking for
special patterns from the aligned paraphrase sen-
tences, which naturally give us more evidence for
the existence of equivalent terms or expressions.
Due to the topological aspect of such patterns, we
decided to name them paraphrase casts, or just
casts as shown in figure 2. As we have mentioned
earlier, the paraphrase cast includes three parts:
the left segment (contextL), a middle segment and
the right segment (contextR). In the following ex-
ample the left and right segments (contexts) are
identical, but the middle segment includes differ-
ent misaligned sequences of words, represented
by wordSa and wordSb.
contextL wordSa ----- contextR
contextL ----- wordSb contextR
5This is why we have in equation 3 the label ?Mismatch?,
where ?mismatch? means different yet lexically near words.
406
We can attribute different levels of confidence
to different paraphrase casts. Indeed, the larger
the contexts and the smaller the misaligned se-
quences are, the more likely it is for single or
phrasal terms to be synonyms or near-synonyms.
Note that in the cast shown in figure 3, each con-
text has a significant size, with four words on
each side, and the misaligned segments are in fact
equivalent expressions i.e. ?paper? is a synonym
of ?research article?. In the analyzed domain
these expressions are equivalent and interchange-
able and appear to be interchangeable in other do-
mains. For the purpose of this paper, we only
take into account the casts where the misaligned
sequences of words contain only one word or one
multi-word unit in each sentence. That is, we have
a one-to-one match. However, no constraints are
imposed on the contexts6. So, all casts are com-
puted and analyzed for synonym discovery and re-
sults are provided in the next section.
7 Experiments
To evaluate our methodology we have used
two different corpora, both from scientific do-
mains built from abstracts of publications (see
Table 1). The corpus of computer secu-
rity (COCS) is a collection of 4854 abstracts
on computer security extracted from the IEEE
(http://ieee.rkbexplorer.com/) repository7. The
corpus of cancer research (COCR) contains 3334
domain specific abstracts of scientific publica-
tions extracted from the PubMed8 on three types
of cancer: (1) the mammary carcinoma register
(COCR1) consisting of 1500 abstracts, (2) the
nephroblastoma register (COCR2) consisting of
1500 abstracts, and (3) the rhabdoid tumor regis-
ter (COCR3) consisting of 334 abstracts. From
the paraphrase casts, we were able to automat-
ically extract, without further processing, single
synonymous word pairs, as well as synonymic
multi-word units, as can be seen in Table 2. For
that purpose we have used specific paraphrase
casts, whose aim was to privilege precision to
6This issue will be discussed in the next section.
7An example of an abstract can be viewed at:
http://ieee.rkbexplorer.com/description/publication-
00534618
8http://www.ncbi.nlm.nih.gov/pubmed
Corpus COCS COCR1 COCR2 COCR3
Tokens 412.265 336.745 227.477 46.215
Sentences 18.974 15.195 10.575 2.321
Aligned Pairs 589 994 511 125
Casts without filter 320 10.217 2.520 48
Casts with filter 202 361 292 16
Table 1: Corpora
recall. In particular, we have removed all casts
which contained numbers or special characters i.e.
casts with filter in Table 1. However, no con-
straints were imposed on the frequency of the
casts. Indeed, all casts were included even if
their overall frequency was just one. Although
Synonym (COCS) Complementary
frequency tuning frequency control
attack consequences attack impact
error-free operation error free operation
pseudo code pseudo algorithm
tolerance resilience
package loss message loss
adjustable algorithm context-aware algorithm
helpful comment valuable comment
Synonym (COCR) Complementary
childhood renal tumor childhood kidney tumor
hypertrophy growth
doxorubicin vincristine
carcinomas of the kidney sarcoma of the kidney
metastasis neoplasm
renal tumor renal malignancy
neoplastic thrombus tumor thrombus
vincristine adriamycin
Table 2: Synonyms for COCS
most of the word relationships were concerned
with synonymy, the other ones were not just er-
rors, but lexically related words, namely examples
of antonymy, hyperonymy/hyponymy and associ-
ations as shown in Table 3. In the evaluation, we
Antonym Complementary
positive sentinel nodes negative sentinel nodes
higher bits lower bits
older version newer version
Hypernym Hyponym
Multi-Tasking Virtual Machine Java Virtual Machine
therapy chemotherapy
hormone breast cancer estrogen breast cancer
Association Complementary
performance reliability
statistical difference significant difference
relationship correlation
Table 3: Other Word Semantic Relationships.
have focused on the precision of the method. The
evaluation of the extracted pairs was performed
manually by two domain experts. In fact, four
407
different evaluations were carried out depending
on whether the adapted S(., .) measure was used
(or not) and whether the normalization of the cor-
pora was used (or not). The best results were ob-
tained in all cases for the adapted S(., .) measure
with the multi-word units. Table 4 shows also the
worst results for the COCS as a baseline (COCS
(1)), i.e. non-adapted S(., .) and non-normalized
corpus. For the rest of the experiments we always
present the results with the adapted S(., .) mea-
sure and normalized corpus.
Corpus COCS (1) COCS (2)
Precision 54.62% 71.29%
Extracted Synonyms 130 144
Errors 108 58
Corpus COCR1 COCR2 COCR3
Precision 69.80% 61.30% 75.00%
Extracted Synonyms 252 178 12
Errors 109 111 4
Table 4: Overall Results
7.1 Discussion
Many results have been published in the literature,
especially tackling the TOEFL synonym detection
problem which aims at identifying the correct syn-
onym among a small set of alternatives (usually
four). For that purpose, the best precision rate has
been reached by (Turney et al, 2003) with 97.50%
who have exploited many resources, both statis-
tical and linguistic. However, our methodology
tackles a different problem. Indeed, our goal
is to automatically extract synonyms from texts.
The published works toward this direction have
not reached so good results. One of the latest stud-
ies was conducted by (Heylen et al, 2008) who
used distributional similarity measures to extract
synonyms from shallow parsed corpora with the
help of unsupervised methods. They report that
?the dependency-based model finds a tightly re-
lated neighbor for 50% of the target words and a
true synonym for 14%?. So, it means that by com-
paring all words in a corpus with all other words,
one can expect to find a correct semantic relation-
ship in 50% of the cases and a correct synonym
in just 14%. In that perspective, our approach
reaches higher results. Moreover, (Heylen et al,
2008) use a frequency cut-off which only selects
features that occur at least five times together with
the target word. In our case, no frequency thresh-
old is imposed to enable extraction of synonyms
with low frequency, such as hapax legomena. This
situation is illustrated in figure 4. We note that
most of the candidate pairs appear only once in
the corpus.
1 2 3 4 5
SynonymousNon?synonymous
Frequency of candidate pairs
Log nu
mber 
of can
didate
 pairs
01
23
45
6
Figure 4: Synonyms Frequency Distribution.
In order to assess the quality of our results,
we calculated the similarity between all extracted
pairs of synonyms following the distributional
analysis paradigm as in (Moraliyski and Dias,
2007) who build context9 feature vectors for noun
synonyms. In particular, we used the cosine sim-
ilarity measure and the Loglike association mea-
sure (Dunning, 1993) as the weighting scheme of
the context features. The distribution of the simi-
larity measure for all noun synonyms (62 pairs) is
shown in figure 5.
Similarity
Frequ
ency
0.0 0.2 0.4 0.6 0.8 1.00
510
1520
2530
Figure 5: Synonym Pairs Similarity Distribution.
The results clearly show that all extracted syn-
onyms are highly correlated in terms of context.
9In this case, the contexts are the surrounding nouns,
verbs and adjectives in the closest chunks of a shallow parsed
corpus.
408
Nearly half of the cases have similarities higher
than 0.5. It is important to notice that a spe-
cific corpus10 was built to calculate as sharply as
possible the similarity measures as it is done in
(Moraliyski and Dias, 2007). Indeed, based on
the COCS and the COCR most statistics were in-
significant leading to zero-valued features. This
situation is well-known as it is one of the major
drawbacks of the distributional analysis approach
which needs huge quantities of texts to make se-
cure decisions. So we note that applying the distri-
butional analysis approach to such small corpora
would have led to rather poor results. Even with
an adapted corpus, figure 5 (left-most bar) shows
that there are no sufficient statistics for 30 pairs of
synonyms. Although the quality of the extracted
pairs of synonyms is high, the precision remains
relatively low with 67.27% precision on average.
As we pointed out in the previous section, we did
not make any restrictions to the left and right con-
texts of the casts. However, the longer these con-
texts are, compared to the misaligned sequence of
words, the higher the chance is that we find a cor-
rect synonym. Table 5 shows the average lengths
of both cast contexts for synonyms and erroneous
pairings, in terms of words (WCL) and charac-
ters (CCL). We also provide the ratio (R) between
the character lengths of the middle segment (i.e.
misaligned character sequences) and the charac-
ter lengths of the cast contexts (i.e. right and left
sizes of equally aligned character sequences). It is
Type WCL CCL R
Synonyms 2.70 11.67 0.70
Errors 2.45 8.05 0.55
Table 5: Average Casts Contexts Lengths
clear that a more thorough study of the effects of
the left and right contexts should be carried out to
improve precision of our approach, although this
may be to the detriment of recall. Based on the
results of the ratio R11, we note that the larger the
cast context is compared to the cast content, the
more likely it is that the misaligned words are syn-
onyms.
10This corpus contains 125.888.439 words.
11These results are statistically relevant with p? value <
0.001 using the Wilcoxon Rank-Sum Test.
8 Conclusions
In this paper we have introduced a new unsu-
pervised methodology for synonym detection that
involves extracting and aligning paraphrases on
normalized domain corpora. In particular, we
have studied a specific structure within aligned
paraphrases, paraphrase casts, from which valid
synonyms were discovered. The overall preci-
sion was 71.29% for the computer security do-
main and 66.06% for the cancer research domain.
This approach proved to be promising for ex-
tracting synonymous words and synonymic multi-
word units. Its strength is the ability to effectively
work with small domain corpora, without super-
vised training, nor definition of specific language-
dependent patterns. Moreover, it is capable to
extract synonymous pairs with figurative or rare
senses which would be impossible to identify us-
ing the distributional similarity approach. Finally,
our approach is completely language-independent
as it can be applied to any raw text, not obli-
gatorily normalized corpora, although the results
for domain terminology may be improved by the
identification of phrasal terms.
However, further improvements of the method
should be considered. A measure of quality of the
paraphrase casts is necessary to provide a mea-
sure of confidence to the kind of extracted word
semantic relationship. Indeed, the larger the con-
texts and the smaller the misaligned sequences
are, the more likely it is for single or phrasal terms
to be synonyms or near-synonyms. Further work
should also be carried out to differentiate the ac-
quired types of semantically related pairs. As it
is shown in Table 3, some of the extracted pairs
were not synonymic, but lexically related words
such as antonyms, hypernyms/hyponyms and as-
sociations. A natural follow-up solution for dis-
criminating between semantic types of extracted
pairs could involve context-based classification of
acquired casts pairs. In particular, (Turney, 2008)
tackled the problem of classifying different lexi-
cal information such as synonymy, antonymy, hy-
pernymy and association by using context words.
In order to propose a completely unsupervised
methodology, we could also follow the idea of
(Dias et al, 2010) to automatically construct small
409
TOEFL-like tests based on sets of casts which
could be handled with the help of different dis-
tributional similarities.
Acknowledgments
We thank anonymous reviewers whose comments
helped to improve this paper. We also thank
IFOMIS institute (Saarbrucken) and the ReSIST
project for allowing us to use the COCR and
COCS corpora.
References
Avizienis, A., Grigonyte, G., Haller, J., von Henke,
F., Liebig, T., and Noppens, O. 2009. Organiz-
ing Knowledge as an Ontology of the Domain of
Resilient Computing by Means of NLP - An Experi-
ence Report. In Proc. of the 22th Int. FLAIRS Conf.
AAAI Press, pp. 474-479.
Barzilay, R. and McKeown, K. R. 2001. Extracting
Paraphrases from a Parallel Corpus. In Proc. of the
39th meeting of ACL, pp. 50-57.
Caraballo, S. A. 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. In
Proc. of 37th meeting of ACL 1999, pp 120-126.
Carl, M., and Schmidt-Wigger, A. 1998. Shallow Post
Morphological Processing with KURD. In Proc. of
the Conf. on New Methods in Language Processing.
Cordeiro, J.P., Dias, G. and Brazdil, P. 2007. Learning
Paraphrases from WNS Corpora. In Proc. of the
20th Int. FLAIRS Conf. AAAI Press, pp. 193-198.
Cordeiro, J.P., Dias, G. and Cleuziou, G. 2007. Biol-
ogy Based Alignments of Paraphrases for Sentence
Compression. In Proc. of the 20th meeting of ACL,
workshop PASCAL, pp. 177-184.
Dias, G., Moraliyski, R., Cordeiro, J.P., Doucet, A. and
Ahonen-Myka, H. 2010. Automatic Discovery of
Word Semantic Relations using Paraphrase Align-
ment and Distributional Lexical Semantics Analy-
sis. In Journal of Natural Language Engineering,
Cambridge University Press. ISSN 1351-3249, pp.
1-26.
Dolan, B., Quirk, C. and Brockett, C. 2004. Un-
supervised Construction of Large Paraphrase Cor-
pora: Exploiting Massively Parallel News Sources.
In Proc. of the 20th int. Conf. on Computational
Linguistics.
Dunning T. D 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. In Computational
Linguistics, 19(1), pp. 61-74.
Freitag, D., Blume, M., Byrnes, J., Chow, E., Kapa-
dia, S., Rohwer, R. and Wang, Z. 2005. New Ex-
periments in Distributional Representations of Syn-
onymy. In Proc. of 9th conf. on Computational Nat-
ural Language Learning, pp. 25-32.
Giovannetti, E., Marchi, S. and Montemagni, S.
2008. Combining Statistical Techniques and
Lexico-Syntactic Patterns for Semantic Relations
Extraction from Text. In Proc. of the 5th Workshop
on Semantic Web Applications and Perspectives.
Girju, R., Giuglea, A. M., Olteanu, M., Fortu, O.,
Bolohan, O. and Moldovan, D. 2004. Support Vec-
tor Machines Applied to the Classification of Se-
mantic Relations in Nominalized Noun Phrases. In
Proc. of the HLT-NAACL Workshop on Computa-
tional Lexical Semantics, pp. 68-75.
Hagiwara, M. O. Y. and Katsuhiko, T. 2009. Su-
pervised Synonym Acquisition using Distributional
Features and Syntactic Patterns. In Information and
Media Technologies 4(2), pp. 558-582.
Hearst, M. A. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proc. of the
14th conf. on Computational Linguistics, pp. 539-
545.
Heylen, K., Peirsman, Y., Geeraerts, D. and Speelman,
D. 2008. Modelling Word Similarity: an Evalua-
tion of Automatic Synonymy Extraction Algorithms.
In Proc. of the 6th LREC.
Hindle, D. 1990. Noun Classification from Predicate-
Argument Structures. In Proc. of the 28th meeting
of ACL, pp. 268-275.
Inkpen, D. 2007. A Statistical Model for Near-
Synonym choice. In ACM Trans. Speech Lang. Pro-
cess. 4(1), 1-17.
Kiefer, J. 1953. Sequential Minimax Search for a
Maximum. In Proc. of the American Mathematical
Society 4, pp. 502-506.
Lin, D. 1998. Automatic Retrieval and Clustering of
Similar Words. In Proc. of the 17th Int. Conf. on
Computational Linguistics, pp. 768-774.
Maas, D., Rosener, Ch., and Theofilidis, A. 2009.
Morphosyntactic and Semantic Analysis of Text:
The MPRO Tagging Procedure. Workshop on sys-
tems and frameworks for computational morphol-
ogy. Springer., pp. 76-87.
Maynard, D. F. A. and Peters, W. 2009. Using lexico-
syntactic Ontology Design Patterns for Ontology
Creation and Population. In Proc. of the Workshop
on Ontology Patterns.
410
Moraliyski, R., and Dias, G. 2007. One Sense per
Discourse for Synonym Detection. In Proc. of the
Int. Conf. On Recent Advances in NLP, Bulgaria,
pp. 383-387.
Muller, P., Hathout, N. and Gaume, B. 2006. Synonym
Extraction Using a Semantic Distance on a Dictio-
nary. In Proc. of the 1st Workshop on Graph-Based
Methods for NLP, pp. 65-72.
Needleman, S. B. and Wunsch, C. D. 1970. A General
Method Applicable to the Search for Similarities in
the Amino Acid Sequence of two Proteins. In Jour-
nal of Molecular Biology 48(3), pp. 443-453.
Ohshima, H. and Tanaka, K. 2009. Real Time Ex-
traction of Related Terms by Bi-directional lexico-
syntactic Patterns from the Web. In Proc. of the 3rd
Int. Conf. on Ubiquitous Information Management
and Communication, pp. 441-449.
Riloff, E. 1993. Automatically Constructing a Dic-
tionary for Information Extraction Tasks. In Proc.
of the 11th Nat. Conf. on Artificial Intelligence, pp.
811-816.
Roark, B. and Charniak, E. 1998. Noun-phrase
Co-occurrence Statistics for Semiautomatic Seman-
tic Lexicon Construction. In Proc. of the 17th
Int. Conf. on Computational Linguistics, pp. 1110-
1116.
Smith, T. and Waterman, M. 1981. Identification of
Common Molecular Subsequences. In Journal of
Molecular Biology 147, pp. 195-197.
Sowa, J. F. and Siekmann, J. H. 1994. Concep-
tual Structures: Current Practices. Springer-Verlag
New York, Inc., Secaucus, NJ, USA.
Stevenson, M. and Greenwood, M. 2006. Compar-
ing Information Extraction Pattern Models. In Proc.
of the Workshop on Information Extraction Beyond
the Document, ACL, pp. 29-35.
Terra, E. and Clarke, C. 2003. Frequency Estimates
for Statistical Word Similarity Measures. In Proc.
of HTL/NAACL 2003, pp. 165-172.
Turney, P. D. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167, pp. 491-502.
Turney, P. D., Littman, M. L., Bigham, J. and Shnay-
der, V. 2003. Combining Independent Modules in
Lexical Multiple-choice Problems. In Recent Ad-
vances in NLP III: Selected Papers, pp. 101-110.
Turney, P. D. 2008. A Uniform Approach to Analogies,
Synonyms, Antonyms and Associations. In Proc .of
the 22nd Int. Conf. on Computational Linguistics,
pp. 905-912.
Van der Plas, L. and Tiedemann, J. 2006. Finding Syn-
onyms Using Automatic Word Alignment and Mea-
sures of Distributional Similarity. In Proc. of the
21st Int. Conf. on Computational Linguistics, pp.
866-873.
Wei, X., Peng, F., Tseng, H., Lu, Y. and Dumoulin,
B. 2009. Context Sensitive Synonym Discovery for
Web Search Queries. In Proc. of the 18th ACM
conference on Information and Knowledge Man-
agement, pp. 1585-1588.
Wu, H. and Zhou, M. 2003. Optimizing Synonym
Extraction Using Monolingual and Bilingual Re-
sources. In Proc. of the 2nd Int. Workshop on Para-
phrasing, pp. 72-79.
411
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 153?158,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Post-Retrieval Clustering Using Third-Order Similarity Measures
Jose? G. Moreno
Normandie University
UNICAEN, GREYC CNRS
F-14032 Caen, France
jose.moreno@unicaen.fr
Gae?l Dias
Normandie University
UNICAEN, GREYC CNRS
F-14032 Caen, France
gael.dias@unicaen.fr
Guillaume Cleuziou
University of Orle?ans
LIFO
F-45067 Orle?ans, France
cleuziou@univ-orleans.fr
Abstract
Post-retrieval clustering is the task of clus-
tering Web search results. Within this
context, we propose a new methodology
that adapts the classical K-means algo-
rithm to a third-order similarity measure
initially developed for NLP tasks. Results
obtained with the definition of a new stop-
ping criterion over the ODP-239 and the
MORESQUE gold standard datasets evi-
dence that our proposal outperforms all re-
ported text-based approaches.
1 Introduction
Post-retrieval clustering (PRC), also known as
search results clustering or ephemeral clustering,
is the task of clustering Web search results. For
a given query, the retrieved Web snippets are au-
tomatically clustered and presented to the user
with meaningful labels in order to minimize the
information search process. This technique can
be particularly useful for polysemous queries but
it is hard to implement efficiently and effectively
(Carpineto et al, 2009). Indeed, as opposed to
classical text clustering, PRC must deal with small
collections of short text fragments (Web snippets)
and be processed in run-time.
As a consequence, most of the successful
methodologies follow a monothetic approach (Za-
mir and Etzioni, 1998; Ferragina and Gulli, 2008;
Carpineto and Romano, 2010; Navigli and Crisa-
fulli, 2010; Scaiella et al, 2012). The underlying
idea is to discover the most discriminant topical
words in the collection and group together Web
snippets containing these relevant terms. On the
other hand, the polythetic approach which main
idea is to represent Web snippets as word feature
vectors has received less attention, the only rele-
vant work being (Osinski and Weiss, 2005). The
main reasons for this situation are that (1) word
feature vectors are hard to define in small collec-
tions of short text fragments (Timonen, 2013), (2)
existing second-order similarity measures such as
the cosine are unadapted to capture the seman-
tic similarity between small texts, (3) Latent Se-
mantic Analysis has evidenced inconclusive re-
sults (Osinski and Weiss, 2005) and (4) the la-
beling process is a surprisingly hard extra task
(Carpineto et al, 2009).
This paper is motivated by the fact that the poly-
thetic approach should lead to improved results if
correctly applied to small collections of short text
fragments. For that purpose, we propose a new
methodology that adapts the classical K-means
algorithm to a third-order similarity measure ini-
tially developed for Topic Segmentation (Dias et
al., 2007). Moreover, the adapted K-means algo-
rithm allows to label each cluster directly from its
centroids thus avoiding the abovementioned extra
task. Finally, the evolution of the objective func-
tion of the adapted K-means is modeled to auto-
matically define the ?best? number of clusters.
Finally, we propose different experiments over
the ODP-239 (Carpineto and Romano, 2010)
and MORESQUE (Navigli and Crisafulli, 2010)
datasets against the most competitive text-based
PRC algorithms: STC (Zamir and Etzioni, 1998),
LINGO (Osinski and Weiss, 2005), OPTIMSRC
(Carpineto and Romano, 2010) and the classical
bisecting incremental K-means (which may be
seen as a baseline for the polythetic paradigm)1.
A new evaluation measure called the b-cubed F -
measure (Fb3) and defined in (Amigo? et al, 2009)
is then calculated to evaluate both cluster homo-
geneity and completeness. Results evidence that
our proposal outperforms all state-of-the-art ap-
proaches with a maximum Fb3 = 0.452 for ODP-
239 and Fb3 = 0.490 for MORESQUE.
1The TOPICAL algorithm proposed by (Scaiella et
al., 2012) is a knowledge-driven methodology based on
Wikipedia.
153
2 Polythetic Post-Retrieval Clustering
The K-means is a geometric clustering algorithm
(Lloyd, 1982). Given a set of n data points, the
algorithm uses a local search approach to partition
the points into K clusters. A set of K initial clus-
ter centers is chosen. Each point is then assigned
to the center closest to it and the centers are recom-
puted as centers of mass of their assigned points.
The process is repeated until convergence. To as-
sure convergence, an objective function Q is de-
fined which decreases at each processing step. The
classical objective function is defined in Equation
(1) where pik is a cluster labeled k, xi ? pik is
an object in the cluster, mpik is the centroid of the
cluster pik and E(., .) is the Euclidean distance.
Q =
K?
k=1
?
xi?pik
E(xi,mpik )2. (1)
Within the context of PRC, the K-means algo-
rithm needs to be adapted to integrate third-order
similarity measures (Mihalcea et al, 2006; Dias
et al, 2007). Third-order similarity measures,
also called weighted second-order similarity mea-
sures, do not rely on exact matches of word fea-
tures as classical second-order similarity measures
(e.g. the cosine metric), but rather evaluate simi-
larity based on related matches. In this paper, we
propose to use the third-order similarity measure
called InfoSimba introduced in (Dias et al, 2007)
for Topic Segmentation and implement its simpli-
fied version S3s in Equation 2.
S3s (Xi, Xj) =
1
p2
p?
k=1
p?
l=1
Xik ?Xjl ? S(Wik,Wjl). (2)
Given two Web snippets Xi and Xj , their sim-
ilarity is evaluated by the similarity of its con-
stituents based on any symmetric similarity mea-
sure S(., .) where Wik (resp. Wjl) corresponds to
the word at the kth (resp. lth) position in the vector
Xi (resp. Xj) and Xik (resp. Xjl) is the weight of
word Wik (resp. Wjl) in the set of retrieved Web
snippets. A direct consequence of the change in
similarity measure is the definition of a new ob-
jective function QS3s to ensure convergence. Thisfunction is defined in Equation (3) and must be
maximized2.
2A maximization process can easily be transformed into a
minimization one
QS3s =
K?
k=1
?
xi?pik
S3s (xi,mpik ). (3)
A cluster centroid mpik is defined by a vector of
p words (wpik1 , . . . , wpikp ). As a consequence, each
cluster centroid must be instantiated in such a way
that QS3s increases at each step of the clusteringprocess. The choice of the best p words repre-
senting each cluster is a way of assuring conver-
gence. For that purpose, we define a procedure
which consists in selecting the best p words from
the global vocabulary V in such a way that QS3sincreases. The global vocabulary is the set of all
words which appear in any context vector.
So, for each word w ? V and any symmet-
ric similarity measure S(., .), its interestingness
?k(w) is computed as regards to cluster pik. This
operation is defined in Equation (4) where si ? pik
is any Web snippet from cluster pik. Finally, the p
words with higher ?k(w) are selected to construct
the cluster centroid. In such a way, we can easily
prove that QS3s is maximized. Note that a wordwhich is not part of cluster pik may be part of the
centroid mpik .
?k(w) = 1p
?
si?pik
?
wiq?si
S(wiq, w). (4)
Finally, we propose to rely on a modified ver-
sion of the K-means algorithm called Global K-
means (Likasa et al, 2003), which has proved to
lead to improved results. To solve a clustering
problem with M clusters, all intermediate prob-
lems with 1, 2, ...,M ? 1 clusters are sequentially
solved. The underlying idea is that an optimal so-
lution for a clustering problem with M clusters
can be obtained using a series of local searches us-
ing the K-means algorithm. At each local search,
the M ? 1 cluster centers are always initially
placed at their optimal positions corresponding to
the clustering problem with M ? 1 clusters. The
remaining M th cluster center is initially placed at
several positions within the data space. In addi-
tion to effectiveness, the method is deterministic
and does not depend on any initial conditions or
empirically adjustable parameters. Moreover, its
adaptation to PRC is straightforward.
3 Stopping Criterion
Once clustering has been processed, selecting the
best number of clusters still remains to be decided.
154
For that purpose, numerous procedures have been
proposed (Milligan and Cooper, 1985). However,
none of the listed methods were effective or adapt-
able to our specific problem. So, we proposed
a procedure based on the definition of a ratio-
nal function which models the quality criterion
QS3s . To better understand the behaviour of QS3sat each step of the adapted GK-means algorithm,
we present its values for K = 10 in Figure 1.
Figure 1: QS3s and its modelisation.
QS3s can be modelled as in Equation (5) whichconverges to a limit ? whenK increases and starts
from Q1S3s (i.e. QS3s at K = 1). The underlyingidea is that the best number of clusters is given by
the ? value which maximizes the difference with
the average ?mean. So, ?, ? and ? need to be
expressed independently of unknown variables.
?K, f(K) = ?? ?K? . (5)
As ? can theoretically or operationally be de-
fined and it can easily be proved that ? = ??Q1S3s ,? needs to be defined based on ? or ?. This can
also be easily proved and the given result is ex-
pressed in Equation (6).
? =
log(??Q1S3s )? log(??Q
K
S3s
)
log(K) . (6)
Now, the value of ? which best approximates
the limit of the rational function must be defined.
For that purpose, we computed its maximum theo-
retical and experimental values as well as its ap-
proximated maximum experimental value based
on the ?2-Aitken (Aitken, 1926) procedure to ac-
celerate convergence as explained in (Kuroda et
al., 2008). Best results were obtained with the
maximum experimental value which is defined as
building the cluster centroid mpik for each Web
snippet individually. Finally, the best number of
clusters is defined as in Algorithm (1) and each
one receives its label based on the p words with
greater interestingness of its centroid mpik .
Algorithm 1 The best K selection procedure.
1. Calculate ?K for each K
2. Evaluate the mean of all ?K i.e. ?mean
3. Select ?K which maximizes ?K ? ?mean
4. Return K as the best number of partitions
This situation is illustrated in Figure (1) where
the red line corresponds to the rational functional
for ?mean and the blue line models the best ?
value (i.e. the one which maximizes the difference
with ?mean). In this case, the best number would
correspond to ?6 and as a consequence, the best
number of clusters would be 6. In order to illus-
trate the soundness of the procedure, we present
the different values for ? at each K iteration and
the differences between consecutive values of ? at
each iteration in Figure 2. We clearly see that the
highest inclination of the curve is between clus-
ter 5 and 6 which also corresponds to the highest
difference between two consecutive values of ?.
Figure 2: Values of ? (on the left) and differences
between consecutive values of ? (on the right).
4 Evaluation
Evaluating PRC systems is a difficult task as stated
in (Carpineto et al, 2009). Indeed, a successful
PRC system must evidence high quality level clus-
tering. Ideally, each query subtopic should be rep-
resented by a unique cluster containing all the rel-
evant Web pages inside. However, this task is far
from being achievable. As such, this constraint
is reformulated as follows: the task of PRC sys-
tems is to provide complete topical cluster cov-
erage of a given query, while avoiding excessive
155
Fb3 K Stop Criterion2 3 4 5 6 7 8 9 10 Fb3 Avg. K
SCP p
2 0.387 0.396 0.398 0.396 0.391 0.386 0.382 0.378 0.374 0.395 4.799
3 0.400 0.411 0.412 0.409 0.406 0.400 0.397 0.391 0.388 0.411 4.690
4 0.405 0.416 0.423 0.425 0.423 0.420 0.416 0.414 0.411 0.441 4.766
5 0.408 0.422 0.431 0.431 0.429 0.429 0.423 0.422 0.421 0.452 4.778
PMI p
2 0.391 0.399 0.397 0.393 0.388 0.383 0.377 0.373 0.366 0.393 4.778
3 0.408 0.418 0.422 0.418 0.414 0.410 0.405 0.398 0.392 0.416 4.879
4 0.420 0.434 0.439 0.439 0.435 0.430 0.425 0.420 0.412 0.436 4.874
5 0.423 0.444 0.451 0.451 0.451 0.445 0.441 0.434 0.429 0.450 4.778
Table 1: Fb3 for SCP and PMI for the global search and the stopping criterion for the ODP-239 dataset.
Adapated GK-means
STC LINGO BIK OPTIMSRCSCP PMI
ODP-239
p p
2 3 4 5 2 3 4 5
F1 0.312 0.341 0.352 0.366 0.332 0.358 0.378 0.390 0.324 0.273 0.200 0.313
F2 0.363 0.393 0.404 0.416 0.363 0.395 0.421 0.435 0.319 0.167 0.173 0.341
F5 0.411 0.441 0.453 0.462 0.390 0.430 0.459 0.476 0.322 0.153 0.165 0.380
Fb3 0.395 0.411 0.441 0.452 0.393 0.416 0.436 0,450 0.403 0.346 0.307 N/A
MORESQUE
F1 0.627 0.649 0.665 0.664 0.615 0.551 0.543 0.571 0.455 0.326 0.317 N/A
F2 0.685 0.733 0.767 0.770 0.644 0.548 0.521 0.551 0.392 0.260 0.269 N/A
F5 0.747 0.817 0.865 0.872 0.679 0.563 0.519 0.553 0.370 0.237 0.255 N/A
Fb3 0.482 0.482 0.473 0.464 0.490 0.465 0.462 0.485 0.460 0.399 0.315 N/A
Table 2: PRC comparative results for F? and Fb3 over the ODP-239 and MORESQUE datasets.
redundancy of the subtopics in the result list of
clusters. So, in order to evaluate our methodol-
ogy, we propose two different evaluations. First,
we want to evidence the quality of the stopping
criterion when compared to an exhaustive search
over all tunable parameters. Second, we propose a
comparative evaluation with existing state-of-the-
art algorithms over gold standard datasets and re-
cent clustering evaluation metrics.
4.1 Text Processing
Before the clustering process takes place, Web
snippets are represented as word feature vectors.
In order to define the set of word features, the
Web service proposed in (Machado et al, 2009) is
used3. In particular, it assigns a relevance score to
any token present in the set of retrieved Web snip-
pets based on the analysis of left and right token
contexts. A specific threshold is then applied to
withdraw irrelevant tokens and the remaining ones
form the vocabulary V . Then, each Web snippet is
represented by the set of its p most relevant to-
kens in the sense of the W (.) value proposed in
(Machado et al, 2009). Note that within the pro-
posed Web service, multiword units are also iden-
tified. They are exclusively composed of relevant
individual tokens and their weight is given by the
arithmetic mean of their constituents scores.
3Access to this Web service is available upon request.
4.2 Intrinsic Evaluation
The first set of experiments focuses on understand-
ing the behaviour of our methodology within a
greedy search strategy for different tunable param-
eters defined as a tuple < p,K, S(Wik,Wjl) >.
In particular, p is the size of the word feature vec-
tors representing both Web snippets and centroids
(p = 2..5), K is the number of clusters to be
found (K = 2..10) and S(Wik,Wjl) is the col-
location measure integrated in the InfoSimba sim-
ilarity measure. In these experiments, two asso-
ciation measures which are known to have dif-
ferent behaviours (Pecina and Schlesinger, 2006)
are tested. We implement the Symmetric Condi-
tional Probability (Silva et al, 1999) in Equation
(7) which tends to give more credits to frequent as-
sociations and the Pointwise Mutual Information
(Church and Hanks, 1990) in Equation (8) which
over-estimates infrequent associations. Then, best
< p,K, S(Wik,Wjl) > configurations are com-
pared to our stopping criterion.
SCP (Wik,Wjl) =
P (Wik,Wjl)2
P (Wik)? P (Wjl)
. (7)
PMI(Wik,Wjl) = log2
P (Wik,Wjl)
P (Wik)? P (Wjl)
. (8)
In order to perform this task, we evaluate per-
formance based on the Fb3 measure defined in
(Amigo? et al, 2009) over the ODP-239 gold stan-
dard dataset proposed in (Carpineto and Romano,
156
2010). In particular, (Amigo? et al, 2009) indi-
cate that common metrics such as the F?-measure
are good to assign higher scores to clusters with
high homogeneity, but fail to evaluate cluster com-
pleteness. First results are provided in Table 1 and
evidence that the best configurations for different
< p,K, S(Wik,Wjl) > tuples are obtained for
high values of p, K ranging from 4 to 6 clusters
and PMI steadily improving over SCP . How-
ever, such a fuzzy configuration is not satisfac-
tory. As such, we proposed a new stopping cri-
terion which evidences coherent results as it (1)
does not depend on the used association measure
(FSCPb3 = 0.452 and FPMIb3 = 0.450), (2) discov-ers similar numbers of clusters independently of
the length of the p-context vector and (3) increases
performance with high values of p.
4.3 Comparative Evaluation
The second evaluation aims to compare our
methodology to current state-of-the-art text-based
PRC algorithms. We propose comparative exper-
iments over two gold standard datasets (ODP-239
(Carpineto and Romano, 2010) and MORESQUE
(Di Marco and Navigli, 2013)) for STC (Za-
mir and Etzioni, 1998), LINGO (Osinski and
Weiss, 2005), OPTIMSRC (Carpineto and Ro-
mano, 2010) and the Bisecting Incremental K-
means (BIK) which may be seen as a baseline for
the polythetic paradigm. A brief description of
each PRC algorithm is given as follows.
STC: (Zamir and Etzioni, 1998) defined the
Suffix Tree Clustering algorithm which is still a
difficult standard to beat in the field. In partic-
ular, they propose a monothetic clustering tech-
nique which merges base clusters with high string
overlap. Indeed, instead of using the classical Vec-
tor Space Model (VSM) representation, they pro-
pose to represent Web snippets as compact tries.
LINGO: (Osinski and Weiss, 2005) proposed a
polythetic solution called LINGO which takes into
account the string representation proposed by (Za-
mir and Etzioni, 1998). They first extract frequent
phrases based on suffix-arrays. Then, they reduce
the term-document matrix (defined as a VSM) us-
ing Single Value Decomposition to discover latent
structures. Finally, they match group descriptions
with the extracted topics and assign relevant doc-
uments to them.
OPTIMSRC: (Carpineto and Romano, 2010)
showed that the characteristics of the outputs re-
turned by PRC algorithms suggest the adoption of
a meta clustering approach. As such, they intro-
duce a novel criterion to measure the concordance
of two partitions of objects into different clusters
based on the information content associated to the
series of decisions made by the partitions on single
pairs of objects. Then, the meta clustering phase
is casted to an optimization problem of the concor-
dance between the clustering combination and the
given set of clusterings.
With respect to implementation, we used the
Carrot2 APIs4 which are freely available for STC,
LINGO and the classical BIK. It is worth notic-
ing that all implementations in Carrot2 are tuned
to extract exactly 10 clusters. For OPTIMSRC,
we reproduced the results presented in the paper
of (Carpineto and Romano, 2010) as no imple-
mentation is freely available. The results are il-
lustrated in Table 2 including both F?-measure
and Fb3 . They evidence clear improvements of
our methodology when compared to state-of-the-
art text-based PRC algorithms, over both datasets
and all evaluation metrics. But more important,
even when the p-context vector is small (p = 3),
the adapted GK-means outperforms all other ex-
isting text-based PRC which is particularly impor-
tant as they need to perform in real-time.
5 Conclusions
In this paper, we proposed a new PRC ap-
proach which (1) is based on the adaptation of
the K-means algorithm to third-order similar-
ity measures and (2) proposes a coherent stop-
ping criterion. Results evidenced clear improve-
ments over the evaluated state-of-the-art text-
based approaches for two gold standard datasets.
Moreover, our best F1-measure over ODP-239
(0.390) approximates the highest ever-reached F1-
measure (0.413) by the TOPICAL knowledge-
driven algorithm proposed in (Scaiella et al,
2012)5. These results are promising and in future
works, we propose to define new knowledge-based
third-order similarity measures based on studies in
entity-linking (Ferragina and Scaiella, 2010).
4http://search.carrot2.org/stable/search [Last access:
15/05/2013].
5Notice that the authors only propose the F1-measure al-
though different results can be obtained for different F?-
measures and Fb3 as evidenced in Table 2.
157
References
A.C. Aitken. 1926. On bernoulli?s numerical solution
of algebraic equations. Research Society Edinburgh,
46:289?305.
E. Amigo?, J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Re-
trieval, 12(4):461?486.
C. Carpineto and G. Romano. 2010. Optimal meta
search results clustering. In 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 170?177.
C. Carpineto, S. Osinski, G. Romano, and D. Weiss.
2009. A survey of web clustering engines. ACM
Computer Survey, 41(3):1?38.
K. Church and P. Hanks. 1990. Word association
norms mutual information and lexicography. Com-
putational Linguistics, 16(1):23?29.
A. Di Marco and R. Navigli. 2013. Clustering and
diversifying web search results with graph-based
word sense induction. Computational Linguistics,
39(4):1?43.
G. Dias, E. Alves, and J.G.P. Lopes. 2007. Topic
segmentation algorithms for text summarization and
passage retrieval: An exhaustive evaluation. In Pro-
ceedings of 22nd Conference on Artificial Intelli-
gence (AAAI), pages 1334?1339.
P. Ferragina and A. Gulli. 2008. A personalized search
engine based on web-snippet hierarchical clustering.
Software: Practice and Experience, 38(2):189?225.
P. Ferragina and U. Scaiella. 2010. Tagme: On-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 1625?1628.
M. Kuroda, M. Sakakihara, and Z. Geng. 2008. Ac-
celeration of the em and ecm algorithms using the
aitken ?2 method for log-linear models with par-
tially classified data. Statistics & Probability Let-
ters, 78(15):2332?2338.
A. Likasa, Vlassis. N., and J. Verbeek. 2003.
The global k-means clustering algorithm. Pattern
Recognition, 36:451?461.
S.P. Lloyd. 1982. Least squares quantization in
pcm. IEEE Transactions on Information Theory,
28(2):129?137.
D. Machado, T. Barbosa, S. Pais, B. Martins, and
G. Dias. 2009. Universal mobile information re-
trieval. In Proceedings of the 5th International Con-
ference on Universal Access in Human-Computer
Interaction (HCI), pages 345?354.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of
text semantic similarity. In Proceedings of the
21st National Conference on Artificial Intelligence
(AAAI), pages 775?780.
G.W. Milligan and M.C. Cooper. 1985. An exami-
nation of procedures for determining the number of
clusters in a data set. Psychometrika, 50(2):159?
179.
R. Navigli and G. Crisafulli. 2010. Inducing word
senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 116?126.
S. Osinski and D. Weiss. 2005. A concept-driven algo-
rithm for clustering search results. IEEE Intelligent
Systems, 20(3):48?54.
P. Pecina and P. Schlesinger. 2006. Combining as-
sociation measures for collocation extraction. In
Proceedings of the Joint Conference of the Inter-
national Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING/ACL), pages 651?658.
U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita.
2012. Topical clustering of search results. In Pro-
ceedings of the 5th ACM International Conference
on Web Search and Data Mining (WSDM), pages
223?232.
J. Silva, G. Dias, S. Guillore?, and J.G.P. Lopes. 1999.
Using localmaxs algorithm for the extraction of con-
tiguous and non-contiguous multiword lexical units.
In Proceedings of 9th Portuguese Conference in Ar-
tificial Intelligence (EPIA), pages 113?132.
M. Timonen. 2013. Term Weighting in Short Docu-
ments for Document Categorization, Keyword Ex-
traction and Query Expansion. Ph.D. thesis, Uni-
versity of Helsinki, Finland.
O. Zamir and O. Etzioni. 1998. Web document clus-
tering: A feasibility demonstration. In 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 46?54.
158
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 97?104
Manchester, August 2008
Fully Unsupervised Graph-Based Discovery of General-Specific Noun 
Relationships from Web Corpora Frequency Counts 
Ga?l Dias 
HULTIG 
University of  
Beira Interior 
ddg@di.ubi.pt 
Raycho Mukelov 
HULTIG 
University of  
Beira Interior 
raicho@hultig.di.ubi.pt 
Guillaume Cleuziou 
LIFO 
University of  
Orl?ans 
cleuziou@univ-orleans.pt 
 
Abstract.  
In this paper, we propose a new metho-
dology based on directed graphs and the 
TextRank algorithm to automatically in-
duce general-specific noun relations from 
web corpora frequency counts. Different 
asymmetric association measures are im-
plemented to build the graphs upon 
which the TextRank algorithm is applied 
and produces an ordered list of nouns 
from the most general to the most specif-
ic. Experiments are conducted based on 
the WordNet noun hierarchy and assess 
65.69% of correct word ordering.   
1 Introduction 
Taxonomies are crucial for any knowledge-
based system. They are in fact important because 
they allow to structure information, thus foster-
ing their search and reuse. However, it is well 
known that any knowledge-based system suffers 
from the so-called knowledge acquisition bottle-
neck, i.e. the difficulty to actually model the do-
main in question. As stated in (Caraballo, 1999), 
WordNet has been an important lexical know-
ledge base, but it is insufficient for domain spe-
cific texts. So, many attempts have been made to 
automatically produce taxonomies (Grefenstette, 
1994), but (Caraballo, 1999) is certainly the first 
work which proposes a complete overview of the 
problem by (1) automatically building a hierar-
chical structure of nouns based on bottom-up 
clustering methods and (2) labeling the internal 
nodes of the resulting tree with hypernyms from 
the nouns clustered underneath by using patterns 
such as ?B is a kind of A?. 
                                                 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
In this paper, we are interested in dealing with 
the second problem of the construction of an or-
ganized lexical resource i.e. discovering general-
specific noun relationships, so that correct nouns 
are chosen to label internal nodes of any hierar-
chical knowledge base, such as the one proposed 
in (Dias et al, 2006). Most of the works pro-
posed so far have (1) used predefined patterns or 
(2) automatically learned these patterns to identi-
fy hypernym/hyponym relationships. From the 
first paradigm, (Hearst, 1992) first identifies a set 
of lexico-syntactic patterns that are easily recog-
nizable i.e. occur frequently and across text genre 
boundaries. These can be called seed patterns. 
Based on these seeds, she proposes a bootstrap-
ping algorithm to semi-automatically acquire 
new more specific patterns. Similarly, (Carabal-
lo, 1999) uses predefined patterns such as ?X is a 
kind of Y? or ?X, Y, and other Zs? to identify 
hypernym/hyponym relationships. This approach 
to information extraction is based on a technique 
called selective concept extraction as defined by 
(Riloff, 1993). Selective concept extraction is a 
form of text skimming that selectively processes 
relevant text while effectively ignoring surround-
ing text that is thought to be irrelevant to the do-
main. 
A more challenging task is to automatically learn 
the relevant patterns for the hypernym/hyponym 
relationships. In the context of pattern extraction, 
there exist many approaches as summarized in 
(Stevenson and Greenwood, 2006). The most 
well-known work in this area is certainly the one 
proposed by (Snow et al, 2005) who use ma-
chine learning techniques to automatically re-
place hand-built knowledge. By using depend-
ency path features extracted from parse trees, 
they introduce a general-purpose formalization 
and generalization of these patterns. Given a 
training set of text containing known hypernym 
pairs, their algorithm automatically extracts use-
ful dependency paths and applies them to new 
corpora to identify novel pairs. (Sang and Hof-
97
mann, 2007) use a similar way as (Snow et al, 
2006) to derive extraction patterns for hy-
pernym/hyponym relationships by using web 
search engine counts from pairs of words en-
countered in WordNet. However, the most inter-
esting work is certainly proposed by (Bollegala 
et al, 2007) who extract patterns in two steps. 
First, they find lexical relationships between 
synonym pairs based on snippets counts and ap-
ply wildcards to generalize the acquired knowl-
edge. Then, they apply a SVM classifier to de-
termine whether a new pair shows a relation of 
synonymy or not, based on a feature vector of 
lexical relationships. This technique could be 
applied to hypernym/hyponym relationships al-
though the authors do not mention it. 
On the one hand, links between words that result 
from manual or semi-automatic acquisition of 
relevant predicative or discursive patterns 
(Hearst, 1992; Carballo, 1999) are fine and accu-
rate, but the acquisition of these patterns is a te-
dious task that requires substantial manual work. 
On the other hand, works done by (Snow et al, 
2005; Snow et al, 2006; Sang and Hofmann, 
2007; Bollegala et al, 2007) have proposed me-
thodologies to automatically acquire these pat-
terns mostly based on supervised learning to le-
verage manual work. However, training sets still 
need to be built.  
Unlike other approaches, we propose an unsu-
pervised methodology which aims at discovering 
general-specific noun relationships which can be 
assimilated to hypernym/hyponym relationships 
detection2. The advantages of this approach are 
clear as it can be applied to any language or any 
domain without any previous knowledge, based 
on a simple assumption: specific words tend to 
attract general words with more strength than the 
opposite. As (Michelbacher et al, 2007) state: 
?there is a tendency for a strong forward associa-
tion from a specific term like adenocarcinoma to 
the more general term cancer, whereas the asso-
ciation from cancer to adenocarcinoma is weak?.  
Based on this assumption, we propose a metho-
dology based on directed graphs and the Tex-
tRank algorithm (Mihalcea and Tarau, 2004) to 
automatically induce general-specific noun rela-
tionships from web corpora frequency counts. 
Indeed, asymmetry in Natural Language 
Processing can be seen as a possible reason for 
                                                 
2
 We must admit that other kinds of relationships may be 
covered. For that reason, we will speak about general-
specific relationships instead of hypernym/hyponym rela-
tionships. 
the degree of generality of terms (Michelbacher 
et al, 2007). So, different asymmetric associa-
tion measures are implemented to build the 
graphs upon which the TextRank algorithm is 
applied and produces an ordered list of nouns, 
from the most general to the most specific. Expe-
riments have been conducted based on the 
WordNet noun hierarchy and assessed that 65% 
of the words are ordered correctly. 
2 Asymmetric Association Measures 
In (Michelbacher et al, 2007), the authors 
clearly point at the importance of asymmetry in 
Natural Language Processing. In particular, we 
deeply believe that asymmetry is a key factor for 
discovering the degree of generality of terms. It 
is cognitively sensible to state that when some-
one hears about mango, he may induce the prop-
erties of a fruit. But, when hearing fruit, more 
common fruits will be likely to come into mind 
such as apple or banana. In this case, there exists 
an oriented association between fruit and mango 
(mango ? fruit) which indicates that mango at-
tracts more fruit than fruit attracts mango. As a 
consequence, fruit is more likely to be a more 
general term than mango. 
Based on this assumption, asymmetric associa-
tion measures are necessary to induce these asso-
ciations. (Pecina and Schlesinger, 2006) and 
(Tan et al, 2004) propose exhaustive lists of as-
sociation measures from which we present the 
asymmetric ones that will be used to measure the 
degree of attractiveness between two nouns, x 
and y, where f(.,.), P(.), P(.,.) and N are respec-
tively the frequency function, the marginal prob-
ability function, the joint probability function and 
the total of digrams. 
( )
)),(),(),,(),(max(
,
Blanquet-Braun
yxfyxfyxfyxf
yxf
++
=
 (1) 
??
??
?
?
?
??
??
?
?
?
+
+
=
)(
)|(log),(
)(
)|(log),(
,
)(
)|(log),(
)(
)|(log),(
maxmeasure J
xP
yxPyxP
xP
yxPyxP
yP
xyPyxP
yP
xyPyxP
 
(2) 
[ ])|(),|(maxConfidence xyPyxP=
 (3) 
??
?
??
?
+
+
+
+
=
2)(.
1),(.
,
2)(.
1),(.
maxLaplace
yPN
yxPN
xPN
yxPN
 (4) 
??
?
??
?
=
),(
)().(
,
),(
)().(
maxConviction
yxP
yPxP
yxP
yPxP
 
(5) 
98
??
?
??
?
?
?
?
?
=
)(1
)()|(
,
)(1
)()|(
maxFactorCertainty 
xP
xPyxP
yP
yPxyP
 
(6) 
[ ])()|(),()|(maxValue Added xPyxPyPxyP ??=
 (7) 
All seven definitions show their asymmetry by 
evaluating the maximum value between two hy-
potheses i.e. by evaluating the attraction of x 
upon y but also the attraction of y upon x. As a 
consequence, the maximum value will decide the 
direction of the general-specific association i.e. 
(x ? y) or (y ? x). 
3 TextRank Algorithm 
Graph-based ranking algorithms are essential-
ly a way of deciding the importance of a vertex 
within a graph, based on global information re-
cursively drawn from the entire graph. The basic 
idea implemented by a graph-based ranking 
model is that of voting or recommendation. 
When one vertex links to another one, it is basi-
cally casting a vote for that other vertex. The 
higher the number of votes that are cast for a ver-
tex, the higher the importance of the vertex. 
Moreover, the importance of the vertex casting 
the vote determines how important the vote itself 
is, and this information is also taken into account 
by the ranking model. Hence, the score asso-
ciated with a vertex is determined based on the 
votes that are cast for it, and the score of the ver-
tices casting these votes. 
Our intuition of using graph-based ranking algo-
rithms is that more general words will be more 
likely to have incoming associations as they will 
be associated to many specific words. On the 
opposite, specific words will have few incoming 
associations as they will not attract general words 
(see Figure 1). As a consequence, the voting pa-
radigm of graph-based ranking algorithms should 
give more strength to general words than specific 
ones, i.e. a higher voting score. 
For that purpose, we first need to build a directed 
graph. Informally, if x attracts more y than y at-
tracts x, we will draw an edge between x and y as 
follows (x ? y) as we want to give more credits 
to general words. Formally, we can define a di-
rected graph G = (V, E) with the set of vertices V 
(in our case, a set of words) and a set of edges E 
where E is a subset of V?V (in our case, defined 
by the asymmetric association measure value 
between two words). In Figure 1, we show the 
directed graph obtained by using the set of words 
V = {isometry, rate of growth, growth rate, rate} 
randomly extracted from WordNet where rate of 
growth and growth rate are synonyms, isometry 
an hyponym of the previous set and rate an 
hypernym of the same set. The weights asso-
ciated to the edges have been evaluated by the 
confidence association measure (Equation 3) 
based on web search engine counts3. 
  
 
 
Fig. 1. Directed Graph based on synset #13153496 (rate of 
growth, growth rate) and its direct hypernym (rate) and 
hyponym (isometry). 
Figure 1 clearly shows our assumption of gene-
rality of terms as the hypernym rate only has 
incoming edges whereas the hyponym isometry 
only has outgoing edges. As a consequence, by 
applying a graph-based ranking algorithm, we 
aim at producing an ordered list of words from 
the most general (with the highest value) to the 
most specific (with the lowest value). For that 
purpose, we present the TextRank algorithm pro-
posed by (Mihalcea and Tarau, 2004) both for 
unweighted and weighted directed graphs. 
3.1 Unweighted Directed Graph 
For a given vertex Vi let In(Vi) be the set of 
vertices that point to it, and let Out(Vi) be the set 
of vertices that vertex Vi points to. The score of a 
vertex Vi is defined in Equation 8 where d is a 
damping factor that can be set between 0 and 1, 
which has the role of integrating into the model 
the probability of jumping from a given vertex to 
another random vertex in the graph4. 
 
 
 
(8) 
3.2 Weighted Directed Graph 
In order to take into account the edge weights, 
a new formula is introduced in Equation 9. 
 
                                                 
3
 We used counts returned by http://www.yahoo.com. 
4
 d is usually set to 0.85.  
)(
)|(|
1)1(
)( jiVInVj j
i VS
VOut
dd)S(V ??+?= ?
?
99
 (9) 
After running the algorithm in both cases, a score 
is associated to each vertex, which represents the 
?importance? of the vertex within the graph. No-
tice that the final values obtained after TextRank 
runs to completion are not affected by the choice 
of the initial values randomly assigned to the ver-
tices. Only the number of iterations needed for 
convergence may be different. As a consequence, 
after running the TextRank algorithm, in both its 
configurations, the output is an ordered list of 
words from the most general one to the most 
specific one. In table 1, we show both the lists 
with the weighted and unweighted versions of 
the TextRank based on the directed graph shown 
in Figure 1. 
 
Unweighted Weighted WordNet 
S(Vi) Word WS(Vi) Word Categ. Word 
0.50 rate 0.81 rate Hyper. rate 
0.27 growth 
rate 
0.44 growth 
rate 
Synset growth 
rate 
0.19 rate of growth 0.26 
rate of 
growth Synset 
rate of 
growth 
0.15 isometry 0.15 isometry Hypo. isometry 
Table 1. TextRank ordered lists. 
 
The results show that asymmetric measures 
combined with directed graphs and graph-based 
ranking algorithms such as the TextRank are 
likely to give a positive answer to our hypothesis 
about the degree of generality of terms. More-
over, we propose an unsupervised methodology 
for acquiring general-specific noun relationships. 
However, it is clear that deep evaluation is 
needed. 
4 Experiments and Results 
Evaluation is classically a difficult task in 
Natural Language Processing. In fact, as human 
evaluation is time-consuming and generally sub-
jective even when strict guidelines are provided, 
measures to automatically evaluate experiments 
must be proposed. In this section, we propose 
three evaluation measures and discuss the respec-
tive results. 
4.1 Constraints 
WordNet can be defined as applying a set of 
constraints to words. Indeed, if word w is the 
hypernym of word x, we may represent this rela-
tion by the following constraint y ? x, where ? is 
the order operator stating that y is more general 
than x. As a consequence, for each set of three 
synsets (the hypernym synset, the seed synset 
and the hyponym synset), a list of constraints can 
be established i.e. all words of the hypernym 
synset must be more general than all the words of 
the seed synset and the hyponym synset, and all 
the words of the seed synset must be more gener-
al than all the words in the hyponym synset. So, 
if we take the synsets presented in Table 1, we 
can define the following set of constraints: {rate 
? growth rate, rate ? rate of growth, growth rate ? 
isometry, rate of growth ? isometry}. 
In order to evaluate our list of words ranked by 
the level of generality against the WordNet cate-
gorization, we just need to measure the propor-
tion of constraints which are respected as shown 
in Equation (10). We call, correctness this meas-
ure. 
 
(10) 
For example, in Table 1, all the constraints are 
respected for both weighted and unweighted 
graphs, giving 100% correctness for the ordered 
lists compared to WordNet categorization. 
4.2 Clustering 
Another way to evaluate the quality of the or-
dering of words is to apply hard clustering to the 
words weighted by their level of generality. By 
evidencing the quality of the mapping between 
three hard clusters generated automatically and 
the hypernym synset, the seed synset and the hy-
ponym synset, we are able to measure the quality 
of our ranking. As a consequence, we propose to 
(1) perform 3-means clustering over the list of 
ranked words, (2) classify the clusters by level of 
generality and (3) measure the precision, recall 
and f-measure of each cluster sorted by level of 
generality with the hypernym synset, the seed 
synset and the hyponym synset. 
For the first task, we use the implementation of 
the k-means algorithm of the NLTK toolkit5. In 
particular, we bootstrap the k-means by choosing 
the initial means as follows. For the first mean, 
we choose the weight (the score) of the first word 
in the TextRank generated list of words. For the 
second mean, we take the weight of the middle 
word in the list and for the third mean, the weight 
of the last word in the list.  
For the second task the level of generality of 
each cluster is evaluated by the average level of 
                                                 
5
 http://nltk.sourceforge.net/ 
)()1(
)(
)(
j
iVInVj
jVOutVk
jk
ji
i VWS
w
w
dd)WS(V ??+?= ? ??
?
constraint of #
constraintcommon  of #
=scorrectnes
100
generality of words inside the cluster (or said 
with other words by its mean).  
For the third task, the most general cluster and 
the hypernym synset are compared in terms of-
precision, recall and f-measure as shown in Equ-
ation (11), (12) and (13)6. The same process is 
applied to the second most general cluster and 
the seed synset, and the third cluster and the hy-
ponym synset. 
 
(11) 
  
(12) 
 
(13) 
4.3 Rank Coefficient Test 
The evaluation can be seen as a rank test be-
tween two ordered lists. Indeed, one way to eva-
luate the results is to compare the list of general-
specific relationships encountered by the Tex-
tRank algorithm and the original list given by 
WordNet. However, we face one problem. 
WordNet does not give an order of generality 
inside synsets. In order to avoid this problem, we 
can order words in each synset by their estimated 
frequency given by WordNet7  as well as their 
frequency calculated by web search hits. An ex-
ample of both ordered lists is given in Table 2 for 
the synset #6655336 and its immediate hyper-
nyms and hyponyms. 
 
WordNet Estimated Frequency  Web Estimated Frequency 
Category Word Category Word 
Hypernym statement Hypernym statement 
Synset answer Synset reply 
Synset reply Synset response 
Synset response Synset answer 
Hyponym rescript Hyponym feedback 
Hyponym feedback Hyponym rescript 
Table 2. Estimated Frequency ordered lists for synset 
#6655336. 
 
For that purpose, we propose to use the Spear-
man?s rank correlation coefficient (Rho). The 
Spearman?s Rho is a statistical coefficient that 
shows how much two random variables are cor-
                                                 
6
 Where Cluster ? Synset means the number of words 
common to both Synset and Cluster, and |Synset| and 
|Cluster| respectively measure the number of words in the 
Synset and the Cluster. 
7
 We use WordNet 2.1. 
related. It is defined in Equation (14) where d is 
the distance between every pair of words in the 
list ordered with TextRank and the reference list 
which is ordered according to WordNet or the 
Web and n is the number of pairs of ranked 
words. 
 
  
(14) 
 
In particular, the Spearman?s rank correlation 
coefficient is a number between -1 (no correla-
tion at all) and 1 (very strong correlation). 
4.4 Experiments 
In order to evaluate our methodology, we ran-
domly8 extracted 800 seed synsets for which we 
retrieved their hypernym and hyponym synsets. 
For each seed synset, we then built the associated 
directed weighted and unweighted graphs based 
on the asymmetric association measures referred 
to in section 29 and ran the TextRank algorithm 
to produce a general-specific ordered lists of 
terms. 
4.4.1 Results by Constraints 
In Table 3, we present the results of the cor-
rectness for all seven asymmetric measures, both 
for the unweighted and weighted graphs. 
 
Equation Type of Graph Correctness  
Braun-Blanquet 
Unweighted 65.68% 
Weighted 65.52% 
J measure 
Unweighted 60.00% 
Weighted 60.34% 
Confidence 
Unweighted 65.69% 
Weighted 65.40% 
Laplace 
Unweighted 65.69% 
Weighted 65.69% 
Conviction 
Unweighted 61.81% 
Weighted 63.39% 
Certainty Factor 
Unweighted 65.59% 
Weighted 63.76% 
Added Value 
Unweighted 65.61% 
Weighted 64.90% 
Baseline10 None 55.68% 
Table 3. Results for the Evaluation by Constraints. 
 
The best results are obtained by the Confidence 
and the Laplace measures reaching 65.69% cor-
                                                 
8
 We guarantee 98% significance level for an error of 0.05 
following the normal distribution. 
9
 The probability functions are estimated by the Maximum 
Likelihood Estimation (MLE). 
10
 The baseline is the list of words ordered by web hits fre-
quency (without TextRank). 
)1(
6
1
2
2
?
?
?=
?
nn
di
?
recallprecision
precisionrecall
measuref
+
??
=?
2
|Cluster|
Synset Cluster ?
=precision
|Synset|
Synset Cluster ?
=recall
101
rectness. However, the Braun-Blanquet, the Cer-
tainty Factor and the Added Value give results 
near the best ones. Only the J measure and the 
Conviction metric seem to perform worst.  
It is also important to note that the difference 
between unweighted and weighted graphs is 
marginal which clearly points at the fact that the 
topology of the graph is more important than its 
weighting. This is also confirmed by the fact that 
most of the asymmetric measures perform alike. 
4.4.2 Results by Clustering 
In Table 4, we present the results of precision, 
recall and f-measure for both weighted and un-
weighted graphs for all the seven asymmetric 
measures. The best precision is obtained for the 
weighted graph with the Confidence measure 
evidencing 47.62% and the best recall is also 
obtained by the Confidence measure also for the 
weighted graph reaching 47.68%. Once again, 
the J measure and the Conviction metric perform 
worst showing worst f-measures. Contrarily, the 
Confidence measure shows the best performance 
in terms of f-measure for the weighted graph, i.e. 
47.65% while the best result for the unweighted 
graphs is obtained by the Certainty factor with 
46.50%.  
These results also show that the weighting of the 
graph plays an important issue in our methodolo-
gy. Indeed, most metrics perform better with 
weighted graphs in terms of f-measure. 
 
Equation Graph Precision Recall F-measure 
Braun-
Blanquet 
Unweighted 46.61 46.06 46.33 
Weighted 47.60 47.67 47.64 
J measure 
Unweighted 40.92 40.86 40.89 
Weighted 42.61 43.71 43.15 
Confidence 
Unweighted 46.54 46.02 46.28 
Weighted 47.62 47.68 47.65 
Laplace 
Unweighted 46.67 46.11 46.39 
Weighted 46.67 46.11 46.39 
Conviction 
Unweighted 42.13 41.67 41.90 
Weighted 43.62 43.99 43.80 
Certainty 
Factor 
Unweighted 46.49 46.52 46.50 
Weighted 44.84 45.85 45.34 
Added 
Value 
Unweighted 46.61 46.59 46.60 
Weighted 47.13 47.27 47.19 
Table 4. Results for the Evaluation by Clustering. 
 
In Table 5, 6 and 7, we present the same results 
as in Table 4 but at different levels of analysis 
i.e. precision, recall and f-measure at hypernym, 
seed and hyponym levels. Indeed, it is important 
to understand how the methodology performs at 
different levels of generality as we verified that 
our approach performs better at higher levels of 
generality. 
 
Equation Graph Precision Recall F-measure 
Braun-
Blanquet 
Unweighted 59.38 37.38 45.88 
Weighted 58.75 39.35 47.14 
J measure 
Unweighted 46.49 37.00 41.20 
Weighted 47.19 41.90 44.38 
Confidence 
Unweighted 59.20 37.30 45.77 
Weighted 58.71 39.22 47.03 
Laplace 
Unweighted 59.50 37.78 45.96 
Weighted 59.50 37.78 45.96 
Conviction 
Unweighted 50.07 35.88 41.80 
Weighted 52.72 40.74 45.96 
Certainty 
Factor 
Unweighted 55.90 38.29 45.45 
Weighted 51.64 42.93 46.88 
Added 
Value 
Unweighted 56.26 37.90 45.29 
Weighted 58.21 40.09 47.48 
Table 5. Results at the hypernym level. 
 
Equation Graph Precision Recall F-measure 
Braun-
Blanquet 
Unweighted 43.05 37.86 40.29 
Weighted 46.38 33.14 38.66 
J measure 
Unweighted 40.82 43.72 42.22 
Weighted 43.98 33.89 38.28 
Confidence 
Unweighted 43.03 37.67 40.17 
Weighted 46.36 33.02 38.57 
Laplace 
Unweighted 43.10 37.78 40.27 
Weighted 43.10 37.78 40.27 
Conviction 
Unweighted 40.36 38.02 39.16 
Weighted 42.60 26.39 32.59 
Certainty 
Factor 
Unweighted 44.28 40.87 42.51 
Weighted 44.14 40.70 42.35 
Added 
Value 
Unweighted 44.21 40.74 42.40 
Weighted 45.78 32.90 38.29 
Table 6. Results at the seed level. 
 
Equation Graph Precision Recall F-measure 
Braun-
Blanquet 
Unweighted 37.39 62.96 46.92 
Weighted 37.68 70.50 49.12 
J measure 
Unweighted 35.43 41.87 38.38 
Weighted 36.69 55.33 44.12 
Confidence 
Unweighted 37.38 63.09 46.95 
Weighted 37.79 70.80 49.27 
Laplace 
Unweighted 37.40 63.11 46.97 
Weighted 37.40 63.11 46.97 
Conviction 
Unweighted 35.97 50.94 42.16 
Weighted 35.54 64.85 45.92 
Certainty 
Factor 
Unweighted 39.28 60.40 47.60 
Weighted 38.74 53.92 45.09 
Added 
Value 
Unweighted 39.36 61.15 47.89 
Weighted 37.39 68.81 48.45 
Table 7. Results at the hyponym level. 
 
Indeed, the precision scores go down from 
59.50% at the hypernym level to 39.36% at the 
hyponym level with 46.38% at the seed level. 
The same phenomenon is inversely true for the 
recall with 42.93% at the hypernym level, 
102
43.72% at the seed level and 70.80% at the hy-
ponym level.  
This situation can easily be understood as most 
of the clusters created by the k-means present the 
same characteristics i.e. the upper level cluster 
usually has fewer words than the middle level 
cluster which in turn has fewer words than the 
last level cluster. As a consequence, the recall is 
artificially high for the hyponym level. But on 
the opposite, the precision is high for higher le-
vels of generality which is promising for the au-
tomatic construction of hierarchical thesauri. In-
deed, our approach can be computed recursively 
so that each level of analysis is evaluated as if it 
was at the hypernym level, thus taking advantage 
of the good performance of our approach at up-
per levels of generality11. 
4.4.3 Results by Rank Test 
For each produced list, we calculated the 
Spearman?s Rho both with WordNet and Web 
Estimated Lists for weighted and unweighted 
graphs. Table 8 presents the average results for 
the 800 randomly selected synsets. 
 
Equation 
Type of 
Graph 
Rho with 
WNet Est. 
list 
Rho with 
Web Est. 
list 
Braun-
Blanquet 
Unweighted 0.38 0.30 
Weighted 0.39 0.39 
J measure 
Unweighted 0.23 0.19 
Weighted 0.27 0.27 
Confidence 
Unweighted 0.38 0.30 
Weighted 0.39 0.39 
Laplace 
Unweighted 0.38 0.30 
Weighted 0.38 0.38 
Conviction 
Unweighted 0.30 0.22 
Weighted 0.33 0.33 
Certainty 
Factor 
Unweighted 0.38 0.29 
Weighted 0.35 0.35 
Added Value 
Unweighted 0.37 0.29 
Weighted 0.38 0.38 
Baseline12 None 0.14 0.14 
Table 8. Results for the Spearman?s rank correlation 
coefficient. 
 
Similarly to what we evidenced in section 4.4.1., 
the J measure and the Conviction metric are the 
measures which less seem to map the correct or-
der by evidencing low correlation scores. On the 
other hand, the Confidence metric still gives the 
best results equally with the Laplace and Braun-
Blanquet metrics.  
                                                 
11
 This will be studied as future work. 
12
 The baseline is the list of words ordered by web hits fre-
quency. 
It is interesting to note that in the case of the web 
estimated list, the weighted graphs evidence 
much better results than the unweighted ones, 
although they do not show improved results 
compared to the WordNet list. On the one hand, 
these results show that our methodology is capa-
ble to map to WordNet lists as easily as to Web 
lists even that it is based on web frequency 
counts. On the other hand, the fact that weighted 
graphs perform best, shows that the topology of 
the graph lacks in accuracy and needs the appli-
cation of weights to counterpoint this lack.    
4.5 Discussion 
An important remark needs to be made at this 
point of our explanation. There is a large ambi-
guity introduced in the methodology by just 
looking at web counts. Indeed, when counting 
the occurrences of a word like answer, we count 
all its occurrences for all its meanings and forms. 
For example, based on WordNet, the word an-
swer can be a verb with ten meanings and a noun 
with five meanings. Moreover, words are more 
frequent than others although they are not so 
general, unconfirming our original hypothesis. 
Looking at Table 2, feedback is a clear example 
of this statement. As we are not dealing with a 
single domain within which one can expect to 
see the ?one sense per discourse? paradigm, it is 
clear that the Rho coefficient would not be as 
good as expected as it is clearly biased by ?incor-
rect? counts. One direct implication of this com-
ment is the use of web estimated lists to evaluate 
the methodology. 
Also, there has been a great discussion over the 
last few months in the corpora list13 whether one 
should use web counts instead of corpus counts 
to estimate word frequencies. In our study, we 
clearly see that web counts show evident prob-
lems, like the ones mentioned by (Kilgarriff, 
2007). However, they cannot be discarded so 
easily. In particular, we aim at looking at web 
counts in web directories that would act as spe-
cific domains and would reduce the space for 
ambiguity. Of course, experiments with well-
known corpora will also have to be made to un-
derstand better this phenomenon. 
5 Conclusions and Future Work 
In this paper, we proposed a new methodology 
based on directed weighted/unweighted graphs 
and the TextRank algorithm to automatically in-
                                                 
13
 Finalized by (Kilgarriff, 2007). 
103
duce general-specific noun relationships from 
web corpora frequency counts. To our know-
ledge, such an unsupervised experiment has nev-
er been attempted so far. In order to evaluate our 
results, we proposed three different evaluation 
metrics. The results obtained by using seven 
asymmetric association measures based on web 
frequency counts showed promising results 
reaching levels of (1) constraint coherence of 
65.69%, (2) clustering mapping of 59.50% in 
terms of precision for the hypernym level and 
42.72% on average in terms of f-measure and (3) 
ranking similarity of 0.39 for the Spearman?s 
rank correlation coefficient. 
As future work, we intend to take advantage of 
the good performance of our approach at the 
hypernym level to propose a recursive process to 
improve precision results over all levels of gene-
rality.  
Finally, it is important to notice that the evalua-
tion by clustering evidences more than a simple 
evaluation of the word order, but shows how this 
approach is capable to automatically map clus-
ters to WordNet classification.   
References 
Bollegala, D., Matsuo, Y. and Ishizuka, M. 2007. 
Measuring Semantic Similarity between Words Us-
ing WebSearch Engines. In Proceedings of Interna-
tional World Wide Web Conference (WWW 
2007). 
Caraballo, S.A. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text. In 
Proceedings of the Conference of the Association 
for Computational Linguistics (ACL 1999). 
Dias, G., Santos, C., and Cleuziou, G. 2006. Automat-
ic Knowledge Representation using a Graph-based 
Algorithm for Language-Independent Lexical 
Chaining. In Proceedings of the Workshop on In-
formation Extraction Beyond the Document asso-
ciated to the Joint Conference of the International 
Committee of Computational Linguistics and the 
Association for Computational Linguistics (COL-
ING/ACL), pages. 36-47. 
Grefenstette, G. 1994. Explorations in Automatic 
Thesaurus Discovery. Kluwer Academic Publish-
ers, USA. 
Hearst, M.H. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proceedings of 
the Fourteenth International Conference on Com-
putational Linguistics (COLING 1992), pages 539-
545. 
Kilgarriff, A. 2007. Googleology is Bad Science. 
Computational Linguistics 33 (1), pages: 147-151. 
Michelbacher, L., Evert, S. and Sch?tze, H. 2007. 
Asymmetric Association Measures. In Proceedings 
of the Recent Advances in Natural Language 
Processing (RANLP 2007). 
Mihalcea, R. and Tarau, P. 2004. TextRank: Bringing 
Order into Texts. In Proceedings of the Conference 
on Empirical Methods in Natural Language 
Processing (EMNLP 2004), pages 404-411. 
Pecina, P. and Schlesinger, P. 2006. Combining Asso-
ciation Measures for Collocation Extraction. In 
Proceedings of the International Committee of 
Computational Linguistics and the Association for 
Computational Linguistics (COLING/ACL 2006).  
Riloff, E. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceed-
ings of the Eleventh National Conference on Ar-
tificial Intelligence (AAAI 1993), pages 811-816. 
Sang, E.J.K. and Hofmann, K. 2007. Automatic Ex-
traction of Dutch Hypernym-Hyponym Pairs. In 
Proceedings of Computational Linguistics in the 
Netherlands Conference (CLIN 2007). 
Snow, R., Jurafsky, D. and Ng, A. Y. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Dis-
covery. In Proceedings of the International Com-
mittee of Computational Linguistics and the Asso-
ciation for Computational Linguistics (COL-
ING/ACL 2006). 
 
Snow, R., Jurafsky, D. and Ng, A. Y. 2005. Semantic 
Taxonomy Induction from Heterogenous Evidence. 
In Proceedings of the Neural Information 
Processing Systems Conference (NIPS 2005). 
Stevenson, M., and Greenwood, M. 2006. Comparing 
Information Extraction Pattern Models. In Proceed-
ings of the Workshop on Information Extraction 
Beyond the Document associated to the Joint Con-
ference of the International Committee of Compu-
tational Linguistics and the Association for Com-
putational Linguistics (COLING/ACL 2006), pag-
es. 29-35. 
Tan, P.-N., Kumar, V. and Srivastava, J. 2004. Select-
ing the Right Objective Measure for Association 
Analysis. Information Systems, 29(4). pages 293-
313. 
 
104
JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 41?48,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
D?tection de mots-cl?s par approches au grain caract?re et au
grain mot
Ga?lle Doualan, Mathieu Boucher, Romain Brixtel, Ga?l Lejeune, Ga?l Dias
?quipe HULTECH (GREYC, Universit? de Caen), Bd Mar?chal juin, 14032 Caen Cedex
prenom.nom@unicaen.fr
R?SUM?
Nous pr?sentons dans cet article les m?thodes utilis?es par l??quipe HULTECH pour sa partici-
pation au D?fi Fouille de Textes 2012 (Deft 2012). La t?che de cette ?dition du d?fi consiste ?
retrouver dans des articles scientifiques, les mots-cl?s choisis par les auteurs. Nous nous appuyons
sur la d?tection de cha?nes r?p?t?es maximales (rst rmax), au grain caract?re et au grain mot. Lam?thode d?velopp?e est simple et non supervis?e. Elle a permis ? notre syst?me d?atteindre la 3e
place (sur 10 ?quipes) sur la premi?re piste du d?fi.
ABSTRACT
Keywords extraction by repeated string analysis
We present here the HULTECH(Human Language Technology) team approach for the Deft 2012
(french text mining challenge). The aim of the challenge is to retrieve the keywords given by
the authors of scientific articles. Our method relies on a text algorithmics technic : detection of
maximal repeated strings. This technic is applied at character level and word level. We achieved
the third rank (over 10) of the first track.
MOTS-CL?S : Recherche d?information, extraction de mots-cl?s, algorithmique du texte.
KEYWORDS: Information retrieval, keywords extraction, string algorithmics.
1 Introduction
La t?che propos?e dans le cadre du D?fi Fouille de Textes 2012 consiste ? retrouver dans des
articles de sciences humaines les mots-cl?s propos?s par les auteurs. Le corpus de travail est
scind? en deux pistes, la premi?re comportant 140 articles et la seconde 141. Une terminologie
qui regroupe tous les mots-cl?s des articles est propos?e avec la premi?re piste. Dans cet article
nous proposerons deux approches : une bas?e sur la connaissance de la terminologie, une autre
adapt?e ? l?absence de cette terminologie. Ce sera pour nous l?occasion de comparer les deux
approches et leurs r?sultats. Nos deux approches s?appuient sur un algorithme de recherche
de cha?nes r?p?t?es maximales, ci-apr?s rst rmax 1. Dans la premi?re approche, bas?e sur laterminologie, nous prenons comme grain d?analyse le caract?re. Dans la seconde approche nous
prenons comme grain d?analyse le mot graphique, sans appui sur la terminologie ni pour la
piste 1 ni pour la piste 2. Dans la section 2, nous proc?dons ? une analyse du corpus qui nous
permet d?appr?hender le mat?riau sur lequel nous travaillons. Dans la section 3, nous d?taillons
1. L?implantation en python utilis?e est disponible ? l?url suivante : code.google.com/p/py-rstr-max
41
nos deux approches. Ensuite, nous pr?senterons les r?sultats dans la section 4et proposons une
confrontation de ces deux approches dans la section 5.
2 Description du corpus
Le corpus utilis? comporte des articles de sciences humaines provenant de quatre revues diffus?es
sur le site Erudit 2. Nous pr?senterons ici plus pr?cis?ment les articles2.1 ? traiter et les mots-cl?s
qui leur sont associ?s2.2.
2.1 Les articles du corpus DEFT 2012
Le corpus DEFT 2012 est constitu? de 300 articles r?partis sur 4 revues de sciences humaines :
? Anthropologie et Soci?t? (AS)
? Revue des Sciences de l?Education (RSE)
? Traduction, Terminologie et R?daction (TTR)
? M?ta : journal des traducteurs (META)
2.1.1 Configuration des articles
Les articles sont au format xml. Ils sont constitu?s d?un identifiant, de la liste des mots-cl?s
fournis par l?auteur, d?un r?sum? et du corps de l?article lui-m?me. Le nom de la revue n?appara?t
pas dans le fichier xml mais dans le nom du fichier. De m?me, le nom de l?auteur et le titre de
l?article ne figurent pas dans le fichier xml. Ceci a rendu plus complexe la recherche des mots-cl?s
notamment du fait que le nom de l?auteur figurait syst?matiquement parmi les mots-cl?s des
articles de la revue Anthropologie et Soci?t?.
Nous pr?sentons dans la figure 1 un exemple d?article du corpus afin de montrer sa configuration
et sa structure, notons que les titres et sous-titres des articles n??taient pas disponibles.
2.1.2 Statistiques sur les articles
Nous avons effectu? des statistiques sur les articles afin de pouvoir mieux les appr?hender
(Tableau 2).
Nombre de documents Taille moyenne en paragraphes Taille moyenne en caract?res
Piste 1 94 67,8 41235
Piste 2 93 80,2 39153
Tableau 1 ? Statistiques sur les documents du corpus d??valuation
Le nombre moyen de paragraphes ne varie pas particuli?rement en fonction de la revue, ?
l?exception de certains articles de META pour lequel le d?coupage en paragraphes ?tait mauvais.
2. http://www.erudit.org
42
< ?xml version="1.0" encoding="UTF-8" ?>
-<doc id="0001">
-<motscles>
<nombre>4</nombre>
<mots>Labrecque ;?conomie politique ;f?minisme ;ethnographie</mots>
</motscles>
-<article>
-<resume>
<p>Tout en poursuivant l?objectif de la pr?sentation du num?ro,
. . . . . . . . . . . . . . . . . .
la consolidation de la th?orie.</p>
</resume>
-<corps>
<p>Qui sape l?ethnographie ?branle la th?orie
. . . . . . . . . . . . . . . . . . . . . . . .
d?une anthropologie engag?e, d?autre part.</p>
</corps>
</article>
</doc>
FIGURE 1 ? Un exemple d?article du jeu d?entra?nement
43
2.2 Les mots-cl?s
Nous avons remarqu? que les articles ne comportent pas le m?me nombre de mots-cl?s : en
moyenne 5,4 95,2 sur la piste 2 et 5,7 sur la piste 1). Mais une grande disparit? peut exister
d?un texte ? l?autre, l??tendue ?tant de 9 (1 ? 10 mots cl?s par article). Nous avons not? que le
premier mot-cl? est syst?matiquement le nom de l?auteur de l?article pour la revue Anthropologie
et Soci?t?. C?est dans un tel cas que la mention du nom de l?auteur dans le fichier nous aurait ?t?
utile.
2.2.1 Nature des mots-cl?s
? Noms propres : nom de l?auteur (ex : Labrecque), auteur faisant l?objet de l?article (ex : Jack
Kerouac), lieu g?ographique (ex : Japon)
? Noms communs : des noms communs seuls ou parfois accompagn?s d?adjectifs, mais jamais de
verbes ni d?adverbes (ex : f?minisme, ?conomie politique)
? Parfois les noms sont compl?t?s par des compl?ments du noms, formant des motifs tels que
celui-ci : N de art N (ex : traitement de l?information sociale)
? Cas particuliers : des noms coordonn?s (ex : traduction scientifique et technique)
Nous avons remarqu? que plus les mots-cl?s ?taient longs, moins on avait de chances de les
retrouver tels quels dans le texte. Lorsque l?on a la chance de les rencontrer dans le texte, ils y
sont peu fr?quents. Globalement 79% des mots-cl?s sont pr?sents tels quels dans le corps du
texte, 44,5% dans le r?sum? et 42% et dans le corps et dans le r?sum?.
3 Description des approches
Notre premi?re approche bas?e sur le grain caract?re utilise la terminologie afin de s?attaquer ? la
piste 1. Notre seconde approche n?utilise pas la terminologie et a ?t? utilis?e sur les deux pistes.
3.1 Approche au grain caract?re
Nous reprenons ici les principes de la m?thode utilis?e pour le Deft 2011 (Lejeune et al, 2011).
On suppose que les segments communs entre le r?sum? et le reste du texte constituent de bons
descripteurs. Pour s?lectionner les descripteurs pertinents nous nous fondons sur leur proximit?
avec des ?l?ments terminologie, technique utilis?e dans le domaine de l?Extraction d?Information
multilingue (Lejeune et al, 2010).
La m?thode rst rmax L?analyse au grain caract?re est effectu? en recherchant des motifs sanstrous (ci-apr?s motifs) tels que d?finis par (Ukkonen, 2009). Ces motifs sont des sous-cha?nes du
texte ayant les caract?ristiques suivantes 3 :
r?p?t?s : les motifs apparaissent au moins deux fois ;
maximaux : les motifs ne sont pas inclus dans des motifs plus grands et de m?me effectif
3. Pour une description plus formelle voir code.google.com/p/py-rstr-max
44
Nous comparons les deux segments textuels(r?sum? et corps) et l?ensemble de la terminologie
en une seule op?ration. Nous conservons les rst r ?max apparaissant dans ces deux segments et
dans un ?l?ment de la terminologie. Seuls les motifs respectant un crit?re de longueur donn?
sont consid?r?s comme pertinents. Pour tenir compte des variations morphologiques du fran?ais,
nous avons fix? la proximit? minimale entre un motif trouv? et un ?l?ment de la terminologie ?
0.9. Autrement dit, un ?l?ment t de la terminologie est consid?r? comme mot-cl? du texte s?il
existe une cha?ne c telle que :
? c est pr?sent dans le r?sum? et dans le corps de l?article
? c est une sous cha?ne de t
? len(c)len(t) ? 910 avec len le nombre de caract?res dans c et t
Nous n?avons pas appliqu? cette m?thode ? la seconde piste car la s?lection de cha?nes de
caract?res adapt?es ? l??valuation ?tait malais?e. Il aurait fallu un grand nombre d?heuristiques
pour retrouver des mots-cl?s comparables ? la r?f?rence. Nous avons pr?f?r? garder la "puret?"
de cette m?thode. En effet le seul pr?-traitement effectu? est le d?coupage en deux segments
textuels (r?sum? et corps). Aucun outillage linguistique (lemmatisation, ?tiquetage. . .) n?est
n?cessaire. Par ailleurs, aucun post-traitement n?est effectu?.
3.2 Approche au grain mot
Pour notre seconde approche, nous proc?dons ? un d?coupage plus classique en mots. Cette
m?thode est con?ue pour fonctionner en l?absence de terminologie de r?f?rence. Nous appliquons
l?algorithme de d?tection des rst rmax (section : 3.1) mais en l?appliquant cette fois sur des mots.
L?algorithme rst rmax est appliqu? ? tout ce qui est compris entre les balises <article> ce quicorrespond au r?sum? et au corps de l?article. Nous consid?rons le tout comme une cha?ne. Nous
obtenons ainsi un ensemble de cha?nes de mots r?p?t?es et maximales. Un grand nombre de
motifs sont d?tect?s dont certains sont partiellement redondants. Par exemple, on a les motif
ABC D et BC DF et on souhaite souvent ne garder que la partie centrale BC D. Pour am?liorer la
pr?cision, nous appliquons donc une seconde fois rst rmax sur la liste des cha?nes obtenues.
3.2.1 IDF
Pour am?liorer la pr?cision de nos r?sultats, nous voulons r?duire encore le nombre de cha?nes
obtenues. Cependant, il nous faut conserver un rappel correct. Pour ce faire nous avons choisi
de calculer l?IDF (Inverse Document Frequency) de chaque cha?ne. Cette mesure fait ressortir
les cha?nes sp?cifiques ? un texte par rapport au corpus. L?IDF est l?inverse de la fr?quence de
la cha?ne dans un ensemble de documents. Cette mesure est g?n?ralement coupl?e avec le TF
(term frequency ou effectif du mot dans un document) en Voici comment se calcule le T F ? I DF
d?une cha?ne C dans un document D 4 :
T F ? I DF = f req(C ,D)t(D) ?? log2 nd(C)N
45
Avec :
? freq(C,D) le nombre de fois que la cha?ne C appara?t dans le document D
? t(D) le nombre de mots du document D
? nd(C) le nombre de documents contenant C dans le corpus
? N la taille du corpus en documents
Cependant, nous ne conservons que l?IDF. Dans notre cas, il n??tait pas n?cessaire l?appliquer le
TF. En effet, gr?ce ? la m?thode rst rmax , nous obtenons les cha?nes maximales r?p?t?es, ce quisignifie qu?elles ont d?j? une certaine fr?quence dans le document. Par ailleurs, le TF a tendance
? privil?gier les cha?nes tr?s fr?quentes d?un texte, autrement dit des mots vides peu susceptibles
d??tre des mots-cl?s.
Pour calculer l?IDF, nous consid?rons l?ensemble des articles d?une piste. Cela nous permet de
caract?riser un article par rapport ? une piste. Cela se justifie si nous nous repla?ons dans
le s?mantique textuelle de Fran?ois Rastier : " le texte pour une linguistique ?volu?e l?unit?
minimale, et le corpus l?ensemble dans lequel cette unit? prend son sens "(Rastier, 2002). Ainsi,
un article ne prend son sens que dans le corpus de travail si bien que nous devons caract?riser
ces cha?nes et ces mots-cl?s par rapport ? l?ensemble du corpus. Lorsque nous calculons l?IDF des
cha?nes nous obtenons des r?sultats compris entre 0 et 5. Nous classons les cha?nes en ordre
d?croissant de leur IDF. Le but ?tant de r?duire le nombre de cha?nes, nous ne conservons que
celles dans l?IDF est sup?rieure ? 2.
3.2.2 Pond?ration des cha?nes
L?IDF constitue un premier filtrage par pond?ration mais ce n?est pas suffisant. Nous proc?dons
donc ? un second filtrage par pond?ration en attribuant un poids aux cha?nes restantes en
fonction des crit?res suivants :
? IDF
? fr?quence de la cha?ne dans l?article
? fr?quence de la cha?ne dans le r?sum?
? longueur de la cha?ne
? pr?sence de la cha?ne dans le premier paragraphe (a priori : introduction
? pr?sence de la cha?ne dans la dernier paragraphe (a priori : conclusion)
A chacune de ces mesures est attribu? un coefficient qui pond?re leur importance. Nous avons
effectu? des statistiques sur le corpus afin d?anticiper les places occup?es par les mots-cl?s dans
les articles. Ainsi, si une cha?ne est fr?quente dans le r?sum?, elle a davantage de chance d??tre
un mot-cl? qu?une autre cha?ne. Nous attribuons donc un certain poids ? ces mesures en fonction
de leur capacit? ? traduire le comportement des mots-cl?s. Notons que l?absence des titres dans
les documents analys?s rend difficile la d?tection des segments introductifs et conclusifs . Les
cha?nes sont rang?es en ordre d?croissant de poids et nous s?lectionnons les 7 premi?res cha?nes
en guise de mots-cl?s. Ce seuil a ?t? fix? ? partir des meilleurs r?sultats obtenus sur le corpus
d?entra?nement.
46
4 R?sultats
R?sultat piste 1 R?sultat piste 2
Approche 1 : rst rmax au grain caract?re 0.44, 3e/10 Approche 2 : rst rmax au grain mot 0,12 0,13, 7e/9Baseline : tf-idf simple 0,08 0,07
Tableau 2 ? R?sultats et rangs pour nos 2 approches et notre baseline
La premi?re approche donne de bons r?sultats en raison de l?appui de la terminologie, bien
meilleurs qu?avec l?approche par poids. Sans doute ces r?sultats auraient pu ?tre am?lior?s avec
quelques heuristiques, par exemple : chercher ? affecter chaque mot-cl? de la terminologie ? au
moins un document. Mais nous n?avons pas souhait? complexifier la proc?dure utilis?e.
Concernant la seconde approche, elle aurait sans doute eu de meilleurs r?sultats sur la piste 1 en
s?appuyant sur la terminologie mais nous avons souhait? pour les deux pistes conserver l?aspect
?sans ressources externes".
5 Discussion
Nous avons opt? pour des m?thodes simples ? mettre en place et peu co?teuses en temps,
peut ?tre au d?triment de la qualit? des r?sultats. La premi?re approche se voulait avant tout
ind?pendante de la langue consid?r?e. Travailler sur le grain caract?re permet de d?passer
les probl?mes de d?coupage des textes en mots. Toutefois pour se conformer aux modalit?s
d??valuation, le soutien de la terminologie s?est av?r? n?cessaire. La seconde approche se voulait
ind?pendante de tout support ext?rieur. En effet, ne pas utiliser la terminologie permet d?extraire
des informations nouvelles ? partir d?un document brut.
Nos deux approches ont en commun l?utilisation d?une m?thode d?algorithmique du texte :
rst rmax . L?algorithme recherche des cha?nes r?p?t?es maximales, suppos?es caract?ristiques d?untexte. Nos approches diff?rent par le grain d?analyse : caract?re pour l?une, mot pour l?autre.
La premi?re m?thode pr?sente l?avantage de la simplicit?, elle ne n?cessite aucun param?tre mais
e base sur la terminologie. La seconde m?thode ne n?cessite pas de terminologie mais impose
des traitements suppl?mentaires.
Nos deux m?thodes pr?sentent par ailleurs l?avantage de d?tecter facilement des unit?s multi-
mots, souvent plus pertinentes pour des t?ches d?indexation documentaire et de recherche
d?information.
Enfin, nos deux approches sont ind?pendantes de tout module d?analyse linguistique (lemma-
tisation, ?tiquetage. . .) ce qui les rend a priori moins sensibles ? une utilisation sur d?autres
langues que le fran?ais. Il serait donc int?ressant d?exp?rimenter ces techniques sur des corpus
multilingues.
47
R?f?rences
LEJEUNE, G., BRIXTEL, R., GIGUET, E. et LUCAS, N. (2011). Deft2011 : appariement de r?sum?s et
d?articles scientifiques fond? sur les cha?nes de caract?res. In D?fi Fouille de Textes/TALN 2011,
pages 53?64.
LEJEUNE, G., DOUCET, A., YANGARBER, R. et LUCAS, N. (2010). Filtering news for epidemic
surveillance : towards processing more languages with fewer resources. In 4th Workshop on
Cross Lingual Information Access, pages 3?10.
RASTIER, F. (2002). Enjeux ?pist?mologiques de la linguistique de corpus. In 2?me journ?es de la
linguistique de corpus.
UKKONEN, E. (2009). Maximal and minimal representations of gapped and non-gapped motifs
of a string. Theor. Comput. Sci., 410(43):4341?4349.
48
