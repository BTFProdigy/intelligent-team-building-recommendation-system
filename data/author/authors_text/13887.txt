Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 118?124,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Question Detection in Spoken Conversations Using Textual Conversations
Anna Margolis and Mari Ostendorf
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
{amargoli,mo}@ee.washington.edu
Abstract
We investigate the use of textual Internet con-
versations for detecting questions in spoken
conversations. We compare the text-trained
model with models trained on manually-
labeled, domain-matched spoken utterances
with and without prosodic features. Over-
all, the text-trained model achieves over 90%
of the performance (measured in Area Under
the Curve) of the domain-matched model in-
cluding prosodic features, but does especially
poorly on declarative questions. We describe
efforts to utilize unlabeled spoken utterances
and prosodic features via domain adaptation.
1 Introduction
Automatic speech recognition systems, which tran-
scribe words, are often augmented by subsequent
processing for inserting punctuation or labeling
speech acts. Both prosodic features (extracted from
the acoustic signal) and lexical features (extracted
from the word sequence) have been shown to be
useful for these tasks (Shriberg et al, 1998; Kim
and Woodland, 2003; Ang et al, 2005). However,
access to labeled speech training data is generally
required in order to use prosodic features. On the
other hand, the Internet contains large quantities of
textual data that is already labeled with punctua-
tion, and which can be used to train a system us-
ing lexical features. In this work, we focus on ques-
tion detection in the Meeting Recorder Dialog Act
corpus (MRDA) (Shriberg et al, 2004), using text
sentences with question marks in Wikipedia ?talk?
pages. We compare the performance of a ques-
tion detector trained on the text domain using lex-
ical features with one trained on MRDA using lex-
ical features and/or prosodic features. In addition,
we experiment with two unsupervised domain adap-
tation methods to incorporate unlabeled MRDA ut-
terances into the text-based question detector. The
goal is to use the unlabeled domain-matched data to
bridge stylistic differences as well as to incorporate
the prosodic features, which are unavailable in the
labeled text data.
2 Related Work
Question detection can be viewed as a subtask of
speech act or dialogue act tagging, which aims
to label functions of utterances in conversations,
with categories as question/statement/backchannel,
or more specific categories such as request or com-
mand (e.g., Core and Allen (1997)). Previous work
has investigated the utility of various feature types;
Boakye et al (2009), Shriberg et al (1998) and Stol-
cke et al (2000) showed that prosodic features were
useful for question detection in English conversa-
tional speech, but (at least in the absence of recog-
nition errors) most of the performance was achieved
with words alone. There has been some previous
investigation of domain adaptation for dialogue act
classification, including adaptation between: differ-
ent speech corpora (MRDA and Switchboard) (Guz
et al, 2010), speech corpora in different languages
(Margolis et al, 2010), and from a speech domain
(MRDA/Switchboard) to text domains (emails and
forums) (Jeong et al, 2009). These works did
not use prosodic features, although Venkataraman
118
et al (2003) included prosodic features in a semi-
supervised learning approach for dialogue act la-
beling within a single spoken domain. Also rele-
vant is the work of Moniz et al (2011), who com-
pared question types in different Portuguese cor-
pora, including text and speech. For question de-
tection on speech, they compared performance of a
lexical model trained with newspaper text to models
trained with speech including acoustic and prosodic
features, where the speech-trained model also uti-
lized the text-based model predictions as a feature.
They reported that the lexical model mainly iden-
tified wh questions, while the speech data helped
identify yes-no and tag questions, although results
for specific categories were not included.
Question detection is related to the task of auto-
matic punctuation annotation, for which the contri-
butions of lexical and prosodic features have been
explored in other works, e.g. Christensen et al
(2001) and Huang and Zweig (2002). Kim and
Woodland (2003) and Liu et al (2006) used auxil-
iary text corpora to train lexical models for punc-
tuation annotation or sentence segmentation, which
were used along with speech-trained prosodic mod-
els; the text corpora consisted of broadcast news or
telephone conversation transcripts. More recently,
Gravano et al (2009) used lexical models built from
web news articles on broadcast news speech, and
compared their performance on written news; Shen
et al (2009) trained models on an online encyclo-
pedia, for punctuation annotation of news podcasts.
Web text was also used in a domain adaptation
strategy for prosodic phrase prediction in news text
(Chen et al, 2010).
In our work, we focus on spontaneous conversa-
tional speech, and utilize a web text source that is
somewhat matched in style: both domains consist of
goal-directed multi-party conversations. We focus
specifically on question detection in pre-segmented
utterances. This differs from punctuation annota-
tion or segmentation, which is usually seen as a se-
quence tagging or classification task at word bound-
aries, and uses mostly local features. Our focus also
allows us to clearly analyze the performance on dif-
ferent question types, in isolation from segmenta-
tion issues. We compare performance of textual-
and speech-trained lexical models, and examine the
detection accuracy of each question type. Finally,
we compare two domain adaptation approaches to
utilize unlabeled speech data: bootstrapping, and
Blitzer et al?s Structural Correspondence Learning
(SCL) (Blitzer et al, 2006). SCL is a feature-
learning method that uses unlabeled data from both
domains. Although it has been applied to several
NLP tasks, to our knowledge we are the first to apply
SCL to both lexical and prosodic features in order to
adapt from text to speech.
3 Experiments
3.1 Data
The Wiki talk pages consist of threaded posts by
different authors about a particular Wikipedia entry.
While these lack certain properties of spontaneous
speech (such as backchannels, disfluencies, and in-
terruptions), they are more conversational than news
articles, containing utterances such as: ?Are you se-
rious?? or ?Hey, that?s a really good point.? We
first cleaned the posts (to remove URLs, images,
signatures, Wiki markup, and duplicate posts) and
then performed automatic segmentation of the posts
into sentences using MXTERMINATOR (Reynar
and Ratnaparkhi, 1997). We labeled each sentence
ending in a question mark (followed optionally by
other punctuation) as a question; we also included
parentheticals ending in question marks. All other
sentences were labeled as non-questions. We then
removed all punctuation and capitalization from the
resulting sentences and performed some additional
text normalization to match the MRDA transcripts,
such as number and date expansion.
For the MRDA corpus, we use the manually-
transcribed sentences with utterance time align-
ments. The corpus has been hand-annotated with
detailed dialogue act tags, using a hierarchical la-
beling scheme in which each utterance receives one
?general? label plus a variable number of ?specific?
labels (Dhillon et al, 2004). In this work we are
only looking at the problem of discriminating ques-
tions from non-questions; we consider as questions
all complete utterances labeled with one of the gen-
eral labels wh, yes-no, open-ended, or, or-after-yes-
no, or rhetorical question. (To derive the question
categories below, we also consider the specific la-
bels tag and declarative, which are appended to one
of the general labels.) All remaining utterances, in-
119
cluding backchannels and incomplete questions, are
considered as non-questions, although we removed
utterances that are very short (less than 200ms), have
no transcribed words, or are missing segmentation
times or dialogue act label. We performed minor text
normalization on the transcriptions, such as mapping
all word fragments to a single token.
The Wiki training set consists of close to 46k
utterances, with 8.0% questions. We derived an
MRDA training set of the same size from the train-
ing division of the original corpus; it consists of
6.6% questions. For the adaptation experiments, we
used the full MRDA training set of 72k utterances
as unlabeled adaptation data. We used two meet-
ings (3k utterances) from the original MRDA devel-
opment set for model selection and parameter tun-
ing. The remaining meetings (in the original devel-
opment and test divisions; 26k utterances) were used
as our test set.
3.2 Features and Classifier
Lexical features consisted of unigrams through tri-
grams including start- and end-utterance tags, repre-
sented as binary features (presence/absence), plus a
total-number-of-words feature. All ngram features
were required to occur at least twice in the training
set. The MRDA training set contained on the order
of 65k ngram features while the Wiki training set
contained over 205k. Although some previous work
has used part-of-speech or parse features in related
tasks, Boakye et al (2009) showed no clear benefit
of these features for question detection on MRDA
beyond the ngram features.
We extracted 16 prosody features from the speech
waveforms defined by the given utterance times, us-
ing stylized F0 contours computed based on So?nmez
et al (1998) and Lei (2006). The features are de-
signed to be useful for detecting questions and are
similar or identical to some of those in Boakye et
al. (2009) or Shriberg et al (1998). They include:
F0 statistics (mean, stdev, max, min) computed over
the whole utterance and over the last 200ms; slopes
computed from a linear regression to the F0 contour
(over the whole utterance and last 200ms); initial
and final slope values output from the stylizer; ini-
tial intercept value from the whole utterance linear
regression; ratio of mean F0 in the last 400-200ms
to that in the last 200ms; number of voiced frames;
and number of words per frame. All 16 features
were z-normalized using speaker-level parameters,
or gender-level parameters if the speaker had less
than 10 utterances.
For all experiments we used logistic regression
models trained with the LIBLINEAR package (Fan
et al, 2008). Prosodic and lexical features were
combined by concatenation into a single feature vec-
tor; prosodic features and the number-of-words were
z-normalized to place them roughly on the same
scale as the binary ngram features. (We substituted 0
for missing prosody features due to, e.g., no voiced
frames detected, segmentation errors, utterance too
short.) Our setup is similar to (Surendran and
Levow, 2006), who combined ngram and prosodic
features for dialogue act classification using a lin-
ear SVM. Since ours is a detection problem, with
questions much less frequent than non-questions,
we present results in terms of ROC curves, which
were computed from the probability scores of the
classifier. The cost parameter C was tuned to opti-
mize Area Under the Curve (AUC) on the develop-
ment set (C = 0.01 for prosodic features only and
C = 0.1 in all other cases.)
3.3 Baseline Results
Figure 1 shows the ROC curves for the baseline
Wiki-trained lexical system and the MRDA-trained
systems with different feature sets. Table 2 com-
pares performance across different question cate-
gories at a fixed false positive rate (16.7%) near the
equal error rate of the MRDA (lex) case. For analy-
sis purposes we defined the categories in Table 2 as
follows: tag includes any yes-no question given the
additional tag label; declarative includes any ques-
tion category given the declarative label that is not
a tag question; the remaining categories (yes-no, or,
etc.) include utterances in those categories but not
included in declarative or tag. Table 1 gives exam-
ple sentences for each category.
As expected, the Wiki-trained system does worst
on declarative, which have the syntactic form of
statements. For the MRDA-trained system, prosody
alone does best on yes-no and declarative. Along
with lexical features, prosody is more useful for
declarative, while it appears to be somewhat re-
dundant with lexical features for yes-no. Ideally,
such redundancy can be used together with unla-
120
yes-no did did you do that?
declarative you?re not going to be around
this afternoon?
wh what do you mean um reference
frames?
tag you know?
rhetorical why why don?t we do that?
open-ended do we have anything else to say
about transcription?
or and @frag@ did they use sig-
moid or a softmax type thing?
or-after-YN or should i collect it all?
Table 1: Examples for each MRDA question category as
defined in this paper, based on Dhillon et al (2004).
beled spoken utterances to incorporate prosodic fea-
tures into the Wiki system, which may improve de-
tection of some kinds of questions.
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
0.925
0.912
0.696
0.833
false pos rate
de
te
ct
io
n 
ra
te
 
 
train meetings (lex+pros)
train meetings (lex only)
train meetings (pros only)
train wiki (lex only)
Figure 1: ROC curves with AUC values for question de-
tection on MRDA; comparison between systems trained
on MRDA using lexical and/or prosodic features, and
Wiki talk pages using lexical features.
3.4 Adaptation Results
For bootstrapping, we first train an initial baseline
classifier using the Wiki training data, then use it to
label MRDA data from the unlabeled adaptation set.
We select the k most confident examples for each
of the two classes and add them to the training set
using the guessed labels, then retrain the classifier
using the new training set. This is repeated for r
rounds. In order to use prosodic features, which are
type (count) MRDA
(L+P)
MRDA
(L)
MRDA
(P)
Wiki
(L)
yes-no (526) 89.4 86.1 59.3 77.2
declar. (417) 69.8 59.2 49.4 25.9
wh (415) 95.4 93.0 42.2 92.8
tag (358) 89.7 90.5 26.0 79.1
rhetorical (75) 88.0 90.7 25.3 93.3
open-ended (50) 88.0 92.0 16.0 80.0
or (38) 97.4 100 29.0 89.5
or-after-YN (32) 96.9 96.9 25.0 90.6
Table 2: Question detection rates (%) by question type for
each system (L=lexical features, P=prosodic features.)
Detection rates are given at a false positive rate of 16.7%
(starred points in Figure 1), which is the equal error rate
point for the MRDA (L) system. Boldface gives best re-
sult for each type.
type (count) baseline bootstrap SCL
yes-no (526) 77.2 81.4 83.5
declar. (417) 25.9 30.5 32.1
wh (415) 92.8 92.8 93.5
tag (358) 79.1 79.3 80.7
rhetorical (75) 93.3 88.0 92.0
open-ended (50) 80.0 76.0 80.0
or (38) 89.5 89.5 89.5
or-after-YN (32) 90.6 90.6 90.6
Table 3: Adaptation performance by question type, at
false positive rate of 16.7% (starred points in Figure 2.)
Boldface indicates adaptation results better than baseline;
italics indicate worse than baseline.
available only in the bootstrapped MRDA data, we
simply add 16 zeros onto the Wiki examples in place
of the missing prosodic features. The values k = 20
and r = 6 were selected on the dev set.
In contrast with bootstrapping, SCL (Blitzer et al,
2006) uses the unlabeled target data to learn domain-
independent features. SCL has generated much in-
terest lately because of the ability to incorporate fea-
tures not seen in the training data. The main idea is
to use unlabeled data in both domains to learn linear
predictors for many ?auxiliary? tasks, which should
be somewhat related to the task of interest. In par-
ticular, if x is a row vector representing the original
feature vector and yi represents the label for auxil-
iary task i, the linear predictor wi is learned to pre-
dict y?i = wi ? x? (where x? is a modified version of
121
x that excludes any features completely predictive
of yi.) The learned predictors for all tasks {wi} are
then collected into the columns of a matrix W, on
which singular value decomposition USVT = W
is performed. Ideally, features that behave simi-
larly across many yi will be represented in the same
singular vector; thus, the auxiliary tasks can tie to-
gether features which may never occur together in
the same example. Projection of the original feature
vector onto the top h left singular vectors gives an
h?dimensional feature vector z ? UT1:h ? x?. The
model is then trained on the concatenated feature
representation [x, z] using the labeled source data.
As auxiliary tasks yi, we identify all initial words
that begin an utterance at least 5 times in each do-
main?s training set, and predict the presence of each
initial word (yi = 0 or 1). The idea of using the
initial words is that they may be related to the inter-
rogative status of an utterance? utterances starting
with ?do? or ?what? are more often questions, while
those starting with ?i? are usually not. There were
about 250 auxiliary tasks. The prediction features x?
used in SCL include all ngrams occuring at least 5
times in the unlabeled Wiki or MRDA data, except
those over the first word, as well as prosody features
(which are zero in the Wiki data.) We tuned h = 100
and the scale factor of z (to 1) on the dev set.
Figure 2 compares the results using the boot-
strapping and SCL approaches, and the baseline un-
adapted Wiki system. Table 3 shows results by ques-
tion type at the fixed false positive point chosen
for analysis. At this point, both adaptation meth-
ods improved detection of declarative and yes-no
questions, although they decreased detection of sev-
eral other types. Note that we also experimented
with other adaptation approaches on the dev set:
bootstrapping without the prosodic features did not
lead to an improvement, nor did training on Wiki
using ?fake? prosody features predicted based on
MRDA examples. We also tried a co-training ap-
proach using separate prosodic and lexical classi-
fiers, inspired by the work of Guz et al (2007) on
semi-supervised sentence segmentation; this led to
a smaller improvement than bootstrapping. Since
we tuned and selected adaptation methods on the
MRDA dev set, we compare to training with the la-
beled MRDA dev (with prosodic features) and Wiki
data together. This gives superior results compared
to adaptation; but note that the adaptation process
did not use labeled MRDA data to train, but merely
for model selection. Analysis of the adapted sys-
tems suggests prosody features are being utilized to
improve performance in both methods, but clearly
the effect is small, and the need to tune parame-
ters would present a challenge if no labeled speech
data were available. Finally, while the benefit from
3k labeled MRDA utterances added to the Wiki ut-
terances is encouraging, we found that most of the
MRDA training utterances (with prosodic features)
had to be added to match the MRDA-only result in
Figure 1, although perhaps training separate lexical
and prosodic models would be useful in this respect.
4 Conclusion
This work explored the use of conversational web
text to detect questions in conversational speech.
We found that the web text does especially poorly
on declarative questions, which can potentially be
improved using prosodic features. Unsupervised
adaptation methods utilizing unlabeled speech and
a small labeled development set are shown to im-
prove performance slightly, although training with
the small development set leads to bigger gains.
Our work suggests approaches for combining large
amounts of ?naturally? annotated web text with
unannotated speech data, which could be useful in
other spoken language processing tasks, e.g. sen-
tence segmentation or emphasis detection.
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
0.859
0.850
0.833
0.884
false pos rate
de
te
ct
io
n 
ra
te
 
 
SCL
bootstrap
baseline (no adapt)
include MRDA dev
Figure 2: ROC curves and AUC values for adaptation,
baseline Wiki, and Wiki + MRDA dev.
122
References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classification
in multiparty meetings. In Proc. Int. Conference on
Acoustics, Speech, and Signal Processing.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Kofi Boakye, Benoit Favre, and Dilek Hakkini-tu?r. 2009.
Any questions? Automatic question detection in meet-
ings. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding.
Zhigang Chen, Guoping Hu, and Wei Jiang. 2010. Im-
proving prosodic phrase prediction by unsupervised
adaptation and syntactic features extraction. In Proc.
Interspeech.
Heidi Christensen, Yoshihiko Gotoh, and Steve Renals.
2001. Punctuation annotation using statistical prosody
models. In in Proc. ISCA Workshop on Prosody in
Speech Recognition and Understanding, pages 35?40.
Mark G. Core and James F. Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proc. of the
Working Notes of the AAAI Fall Symposium on Com-
municative Action in Humans and Machines, Cam-
bridge, MA, November.
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Eliz-
abeth Shriberg. 2004. Meeting recorder project: Di-
alog act labeling guide. Technical report, ICSI Tech.
Report.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874, August.
Agustin Gravano, Martin Jansche, and Michiel Bacchi-
ani. 2009. Restoring punctuation and capitalization in
transcribed speech. In Proc. Int. Conference on Acous-
tics, Speech, and Signal Processing.
Umit Guz, Se?bastien Cuendet, Dilek Hakkani-Tu?r, and
Gokhan Tur. 2007. Co-training using prosodic and
lexical information for sentence segmentation. In
Proc. Interspeech.
Umit Guz, Gokhan Tur, Dilek Hakkani-Tu?r, and
Se?bastien Cuendet. 2010. Cascaded model adaptation
for dialog act segmentation and tagging. Computer
Speech & Language, 24(2):289?306, April.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proc. Int. Conference on Spoken Language Process-
ing, pages 917?920.
Minwoo Jeong, Chin-Yew Lin, and Gary G. Lee. 2009.
Semi-supervised speech act recognition in emails and
forums. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1250?1259, Singapore, August. Association for
Computational Linguistics.
Ji-Hwan Kim and Philip C. Woodland. 2003. A
combined punctuation generation and speech recog-
nition system and its performance enhancement us-
ing prosody. Speech Communication, 41(4):563?577,
November.
Xin Lei. 2006. Modeling lexical tones for Man-
darin large vocabulary continuous speech recognition.
Ph.D. thesis, Department of Electrical Engineering,
University of Washington.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Trans. Audio, Speech, and Language Processing,
14(5):1526?1540, September.
Anna Margolis, Karen Livescu, and Mari Ostendorf.
2010. Domain adaptation with unlabeled data for dia-
log act tagging. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Process-
ing, pages 45?52, Uppsala, Sweden, July. Association
for Computational Linguistics.
Helena Moniz, Fernando Batista, Isabel Trancoso, and
Ana Mata. 2011. Analysis of interrogatives in dif-
ferent domains. In Toward Autonomous, Adaptive,
and Context-Aware Multimodal Interfaces. Theoret-
ical and Practical Issues, volume 6456 of Lecture
Notes in Computer Science, chapter 12, pages 134?
146. Springer Berlin / Heidelberg.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. 5th Conf. on Applied Natural
Language Processing, April.
Wenzhu Shen, Roger P. Yu, Frank Seide, and Ji Wu.
2009. Automatic punctuation generation for speech.
In Proc. IEEE Workshop on Automatic Speech Recog-
nition and Understanding, pages 586?589, December.
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke,
Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coc-
caro, Rachel Martin, Marie Meteer, and Carol Van Ess-
Dykema. 1998. Can prosody aid the automatic classi-
fication of dialog acts in conversational speech? Lan-
guage and Speech (Special Double Issue on Prosody
and Conversation), 41(3-4):439?487.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meet-
ing recorder dialog act (MRDA) corpus. In Proc. of
the 5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100.
123
Kemal So?nmez, Elizabeth Shriberg, Larry Heck, and
Mitchel Weintraub. 1998. Modeling dynamic
prosodic variation for speaker verification. In Proc.
Int. Conference on Spoken Language Processing,
pages 3189?3192.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339?373.
Dinoj Surendran and Gina-Anne Levow. 2006. Dialog
act tagging with support vector machines and hidden
Markov models. In Proc. Interspeech, pages 1950?
1953.
Anand Venkataraman, Luciana Ferrer, Andreas Stolcke,
and Elizabeth Shriberg. 2003. Training a prosody-
based dialog act tagger from unlabeled data. In Proc.
Int. Conference on Acoustics, Speech, and Signal Pro-
cessing, volume 1, pages 272?275, April.
124
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 45?52,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Domain Adaptation with Unlabeled Data for Dialog Act Tagging
Anna Margolis1,2 Karen Livescu2
1Department of Electrical Engineering, University of Washington, Seattle, WA, USA.
2TTI-Chicago, Chicago, IL, USA.
amargoli@ee.washington.edu, klivescu@ttic.edu, mo@ee.washington.edu
Mari Ostendorf1
Abstract
We investigate the classification of utter-
ances into high-level dialog act categories
using word-based features, under condi-
tions where the train and test data dif-
fer by genre and/or language. We han-
dle the cross-language cases with ma-
chine translation of the test utterances.
We analyze and compare two feature-
based approaches to using unlabeled data
in adaptation: restriction to a shared fea-
ture set, and an implementation of Blitzer
et al?s Structural Correspondence Learn-
ing. Both methods lead to increased detec-
tion of backchannels in the cross-language
cases by utilizing correlations between
backchannel words and utterance length.
1 Introduction
Dialog act (or speech act) tagging aims to label
abstract functions of utterances in conversations,
such as Request, Floorgrab, or Statement; poten-
tial applications include automatic conversation
analysis, punctuation transcription, and human-
computer dialog systems. Although some appli-
cations require domain-specific tag sets, it is often
useful to label utterances based on generic tags,
and several tag sets have been developed for this
purpose, e.g. DAMSL (Core and Allen, 1997).
Many approaches to automatic dialog act (DA)
tagging assume hand-labeled training data. How-
ever, when building a new system it may be diffi-
cult to find a labeled corpus that matches the tar-
get domain, or even the language. Even within the
same language, speech from different domains can
differ linguistically, and the same DA categories
might be characterized by different cues. The do-
main characteristics (face-to-face vs. telephone,
two-party vs. multi-party, informal vs. agenda-
driven, familiar vs. stranger) can influence both
the distribution of tags and word choice.
This work attempts to use unlabeled target do-
main data in order to improve cross-domain train-
ing performance, an approach referred to as both
unsupervised and semi-supervised domain adapta-
tion in the literature. We refer to the labeled train-
ing domain as the source domain. We compare
two adaptation approaches: a simple one based
on forcing the classifier to learn only on ?shared?
features that appear in both domains, and a more
complex one based on Structural Correspondence
Learning (SCL) from Blitzer et al (2007). The
shared feature approach has been investigated for
adaptation in other tasks, e.g. Aue and Gamon
(2005) for sentiment classification and Dredze et
al. (2007) for parsing. SCL has been used suc-
cessfully for sentiment classification and part-of-
speech tagging (Blitzer et al, 2006); here we in-
vestigate its applicability to the DA classification
task, using a multi-view learning implementation
as suggested by Blitzer et al (2009). In addition to
analyzing these two methods on a novel task, we
show an interesting comparison between them: in
this setting, both methods turn out to have a simi-
lar effect caused by correlating cues for a particu-
lar DA class (Backchannel) with length.
We classify pre-segmented utterances based on
their transcripts, and we consider only four high-
level classes: Statement, Question, Backchannel,
and Incomplete. Experiments are performed us-
ing all train/test pairs among three conversational
speech corpora : the Meeting Recorder Dialog Act
corpus (MRDA) (Shriberg et al, 2004), Switch-
board DAMSL (Swbd) (Jurafsky et al, 1997), and
the Spanish Callhome dialog act corpus (SpCH)
(Levin et al, 1998). The first is multi-party,
face-to-face meeting speech; the second is topic-
prompted telephone speech between strangers;
and the third is informal telephone speech between
friends and family members. The first two are in
English, while the third is in Spanish. When the
source and target domains differ in language, we
45
apply machine translation to the target domain to
convert it to the language of the source domain.
2 Related Work
Automatic DA tagging across domain has been
investigated by a handful of researchers. Webb
and Liu (2008) investigated cross-corpus train-
ing between Swbd and another corpus consist-
ing of task-oriented calls, although no adaptation
was attempted. Similarly, Rosset et al (2008)
reported on recognition of task-oriented DA tags
across domain and language (French to English)
by using utterances that had been pre-processed
to extract entities. Tur (2005) applied supervised
model adaptation to intent classification across
customer dialog systems, and Guz et al (2010)
applied supervised model adaptation methods for
DA segmentation and classification on MRDA us-
ing labeled data from both MRDA and Swbd.
Most similar to our work is that of Jeong et al
(2009), who compared two methods for semi-
supervised adaptation, using Swbd/MRDA as the
source training set and email or forums corpora as
the target domains. Both methods were based on
incorporating unlabeled target domain examples
into training. Success has also been reported for
self-training approaches on same-domain semi-
supervised learning (Venkataraman et al, 2003;
Tur et al, 2005). We are not aware of prior work
on cross-lingual DA tagging via machine transla-
tion, although a translation approach has been em-
ployed for cross-lingual text classification and in-
formation retrieval, e.g. Bel et al (2003).
In recent years there has been increasing in-
terest in domain adaptation methods based on
unlabeled target domain data. Several kinds of
approaches have been proposed, including self-
training (Roark and Bacchiani, 2003), instance
weighting (Huang et al, 2007), change of feature
representation (Pan et al, 2008), and clustering
methods (Xing et al, 2007). SCL (Blitzer et al,
2006) is one feature representation approach that
has been effective on certain high-dimensional
NLP problems, including part-of-speech tagging
and sentiment classification. SCL uses unlabeled
data to learn feature projections that tie together
source and target features via their correlations
with features shared between domains. It first se-
lects ?pivot features? that are common in both do-
mains; next, linear predictors for those features are
learned on all the other features. Finally, singular
value decomposition (SVD) is performed on the
collection of learned linear predictors correspond-
ing to different pivot features. Features that tend
to get similar weights in predicting pivot features
will be tied together in the SVD. By learning on
the SVD dimensions, the source-trained classifier
can put weight on target-only features.
3 Methods
Our four-class DA problem is similar to problems
studied in other work, such as Tur et al (2007)
who used five classes (ours plus Floorgrab/hold).
When defining a mapping from each corpus? tag
set to the four high-level classes, our goal was to
try to make the classes similarly defined across
corpora. Note that the Incomplete category is de-
fined in Swbd-DAMSL to include only utterances
too short to determine their DA label (e.g., just a
filler word). Thus, for our work the MRDA In-
complete category excludes utterances also tagged
as Statement or Question; it includes those con-
sisting of just a floor-grab, hold or filler word.
For classification we used an SVM with linear
kernel, with L2 regularization and L1 loss, as im-
plemented in the Liblinear package (Fan et al,
2008) which uses the one-vs.-rest configuration
for multiclass classification. SVMs have been suc-
cessful for supervised learning of DAs based on
words and other features (Surendran and Levow,
2006; Liu, 2006). Features are derived from the
hand transcripts, which are hand-segmented into
DA units. Punctuation and capitalization are re-
moved so that our setting corresponds to classifi-
cation based on (perfect) speech recognition out-
put. The features are counts of unigrams, bi-
grams, and trigrams that occur at least twice in
the train set, including beginning/end-of-utterance
tags (?s?, ?/s?), and a length feature (total num-
ber of words, z-normalized across the training
set). Note that some previous work on DA tag-
ging has used contextual features from surround-
ing utterances, or Markov models for the DA se-
quence. In addition, some work has used prosodic
or other acoustic features. The work of Stolcke
et al (2000) found benefits to using Markov se-
quence models and prosodic features in addition
to word features, but those benefits were relatively
small, so for simplicity our experiments here use
only word features and classify utterances in iso-
lation.
We used Google Translate to derive English
46
translations of the Spanish SpCH utterances, and
to derive Spanish translations of the English Swbd
and MRDA utterances. Of course, translations are
far from perfect; DA classification performance
could likely be improved by using a translation
system trained on spoken dialog. For instance,
Google Translate often failed on certain words like
?i? that are usually capitalized in text. Even so,
when training and testing on translated utterances,
the results with the generic system are surprisingly
good.
The results reported below used the standard
train/test splits provided with the corpora: MRDA
had 51 train meetings/11 test; Swbd had 1115 train
conversations/19 test; SpCH had 80 train conver-
sations/20 test. The SpCH train set is the smallest
at 29k utterances. To avoid issues of differing train
set size when comparing performance of different
models, we reduced the Swbd and MRDA train
sets to the same size as SpCH using randomly se-
lected examples from the full train sets. For each
adaptation experiment, we used the target domain
training set as the unlabeled data, and report per-
formance on the target domain test set. The test
sets contain 4525, 15180, and 3715 utterances for
Swbd, MRDA, and SpCH respectively.
4 Results
Table 1 shows the class proportions in the training
sets for each domain. MRDA has fewer Backchan-
nels than the the others, which is expected since
the meetings are face-to-face. SpCH has fewer In-
completes and more Questions than the others; the
reasons for this are unclear. Backchannels have
the shortest mean length (less than 2 words) in all
domains. Incompletes are also short, while State-
ments have the longest mean length. The mean
lengths of Statements and Questions are similar
in the English corpora, but are shorter in SpCH.
(This may point to differences in how the utter-
ances were segmented; for instance Swbd utter-
ances can span multiple turns, although 90% are
only one turn long.)
Because of the high class skew, we consider two
different schemes for training the classifiers, and
report different performance measures for each.
To optimize overall accuracy, we use basic un-
weighted training. To optimize average per-class
recall (weighted equally across all classes), we use
weighted training, where each training example is
weighted inversely to its class proportion. We op-
timize the regularization parameter using a source
domain development set corresponding to each
training set. Since the optimum values are close
for all three domains, we choose a single value for
all the accuracy classifiers and a single value for
all the per-class recall classifiers. (Different values
are chosen for different feature types correspond-
ing to the different adaptation methods.)
Inc. Stat. Quest. Back.
Swbd 8.1% 67.1% 5.8% 19.1%
MRDA 10.7% 67.9% 7.5% 14.0%
SpCH 5.7% 60.6% 12.1% 21.7%
Table 1: Proportion of utterances in each
DA category (Incomplete, Statement, Question,
Backchannel) in each domain?s training set.
Table 2 gives baseline performance for all train-
test pairs, using translated versions of the test set
when the train set differs in language. It also lists
the in-domain results using translated (train and
test) data, and results using the adaptation methods
(which we discuss below). Figure 1 shows details
of the contribution of each class to the average per-
class recall; bar height corresponds to the second
column in Table 2.
4.1 Baseline performance and analysis
We observe first that translation does not have a
large effect on in-domain performance; degrada-
tion occurs primarily in Incompletes and Ques-
tions, which depend most on word order and there-
fore might be most sensitive to ordering differ-
ences in the translations. We conclude that it is
possible to perform well on the translated test sets
when the training data is well matched. However,
cross-domain performance degradation is much
worse between pairs that differ in language than
between the two English corpora.
We now describe three kinds of issues contribut-
ing to cross-domain domain degradation, which
we observed anecdotally. First, some highly im-
portant words in one domain are sometimes miss-
ing entirely from another domain. This issue ap-
pears to have a dramatic effect on Backchannel
detection across languages: when optimizing for
average per-class recall, the English-trained clas-
sifiers detect about 20% of the Spanish translated
Backchannels and the Spanish classifier detects
a little over half of the English ones, while they
each detect more than 80% in their own domain.
47
train set Acc (%) Avg. Rec. (%)
Test on Swbd
Swbd 89.2 84.9
Swbd translated 86.7 80.4
MRDA baseline 86.4 78.0
MRDA shared only 85.7* 77.7
MRDA SCL 81.8* 69.6
MRDA length only 78.3* 51.4
SpCH baseline 74.5 57.2
SpCH shared only 77.4* 64.2
SpCH SCL 76.8* 64.8
SpCH length only 77.7* 48.2
majority 67.7 25.0
Test on MRDA
MRDA 83.8 80.5
MRDA translated 80.5 74.7
Swbd baseline 81.0 71.6
Swbd shared only 80.1* 72.1
Swbd SCL 75.6* 68.1
Swbd length only 68.6* 44.9
SpCH baseline 66.9 50.5
SpCH shared only 66.8 52.1
SpCH SCL 66.1* 58.4
SpCH length only 68.3* 44.6
majority 65.2 25.0
Test on SpCH
SpCH 83.1 72.8
SpCH translated 82.4 71.3
Swbd baseline 63.8 41.1
Swbd shared only 66.2* 50.9
Swbd SCL 68.2* 47.2
Swbd length only 72.6* 43.6
MRDA baseline 65.1 42.9
MRDA shared only 65.5 51.2
MRDA SCL 67.6* 50.9
MRDA length only 72.6* 44.7
majority 65.3 25.0
Table 2: Overall accuracy and average per-class
recall on each test set, using in-domain, in-domain
translated, and cross-domain training. Starred re-
sults under the accuracy column are significantly
different from the corresponding cross-domain
baseline under McNemar?s test (p < 0.05). (Sig-
nificance is not calculated for the average per-class
recall column.) ?Majority? classifies everything as
Statement.
The reason for the cross-domain drop is that many
backchannel words in the English corpora (uhhuh,
right, yeah) do not overlap with those in the Span-
train Swbd train MRDA train SpCH
0
20
40
60
80
Test on Swbd
a
v
e
. 
pe
r?
cl
as
s 
re
ca
ll 
(%
)
 
 in?dom
ain
in?dom
ain trans.
baseln
shared
SC
L
length
baseln
shared
SC
L
length
I
S
Q
B
train MRDA train Swbd train SpCH
0
20
40
60
80
Test on MRDA
in?dom
ain
in?dom
ain trans.
baseln
shared
SC
L
length
baseln
shared
SC
L
length
train SpCH train Swbd train MRDA
0
20
40
60
80
Test on SpCH
in?dom
ain
in?dom
ain trans.
baseln
shared
SC
L
length
baseln
shared
SC
L
length
Figure 1: Per-class recall of weighted classifiers
in column 2 of Table 2. Bar height represents
average per-class recall; colors indicate contribu-
tion of each class: I=incomplete, S=statement,
Q=question, B=backchannel. (Maximum possible
bar height is 100%, each color 25%).
ish corpora (mmm, s??, ya) even after translation?
for example, ?ya? becomes ?already?, ?s??? be-
comes ?yes?, ?right? becomes ?derecho?, and ?uh-
huh?, ?mmm? are unchanged.
A second issue has to do with different kinds
of utterances found in each domain, which some-
times lead to different relationships between fea-
tures and class label. This is sometimes caused
by the translation system; for example, utterances
starting with ?es que . . .? are usually statements
in SpCH, but without capitalization the translator
often gives ?is that . . .?. Since ??s??is?that? is
a cue feature for Question in English, these utter-
ances are usually labeled as Question by the En-
glish domain classifiers. The existence of differ-
ent types of utterances can result in sets of features
that are more highly correlated in one domain than
the other. In both Swbd and translated SpCH, ut-
terances containing the trigram ??s??but??/s?? are
most likely to be in the Incomplete class. In Swbd,
the bigram ?but??/s?? rarely occurs outside of that
trigram, but in SpCH it sometimes occurs at the
48
end of long (syntactically-incomplete) Statements,
so it corresponds to much lower likelihood for the
Incomplete class.
The last issue concerns utterances whose true
label probabilities given the word sequence are
not the same across domains. We distinguish two
such kinds utterances. The first are due to class
definition differences across domains and anno-
tators, e.g., long statements or questions that are
also incomplete are more often labeled Incomplete
in SpCH and Swbd than in MRDA. The second
kind are utterances whose class labels are not com-
pletely determined by their word sequence. To
minimize error rate the classifier should label an
utterance with its most frequent class, but that may
differ across domains. For example, ?yes? can be
either a Statement of Backchannel; in the English
corpora, it is most likely to be a Statement (?yeah?
is more commonly used for Backchannels). How-
ever, ?s??? is most likely to be a Backchannel in
SpCH. To measure the effect of differing label
probabilities across domains, we trained ?domain-
general? classifiers using concatenated training
sets for each pair of domains. We found that they
performed about the same or only slightly worse
than domain-specific models, so we conclude that
this issue is likely only a minor effect.
4.2 Adaptation using shared features only
In the cross-language domain pairs, some dis-
criminative features in one domain are missing
in the other. By removing all features from the
source domain training utterances that are not ob-
served (twice) in the target domain training data,
we force the classifier to learn only on features
that are present in both domains. As seen in
Figure 1, this had the effect of improving re-
call of Backchannels in the four cross-language
cases. Backchannels are the second-most frequent
class after Statements, and are typically short in
all domains. Many typical Backchannel words
are domain-specific; by removing them from the
source data, we force the classifier to attempt to
detect Backchannels based on length alone. The
resulting classifier has a better chance of recog-
nizing target domain Backchannels that lack the
source-only Backchannel words. At the same
time, it mistakes many other short utterances for
Backchannels, and does particularly worse on In-
completes, for which length is also strong cue.
Although average per-class recall improved in all
four cross-language cases, total accuracy only im-
proved significantly in two of those cases, and
for the Swbd/MRDA pair, accuracy got signifi-
cantly worse. The effect on the one-vs.-rest com-
ponent classifiers was mixed: for some (State-
ment and some Backchannel classifiers in the
cross-language cases), accuracy improved, while
in other cases it decreased.
As noted above, the shared feature approach
was investigated by Aue and Gamon (2005), who
argued that its success depends on the assump-
tion that class/feature relationships be the same
across domains. However, we argue here that the
success of this method requires stronger assump-
tions about both the relationship between domains
and the correlations between domain-specific and
shared features. Consider learning a linear model
on either the full source domain feature set or the
reduced shared feature set. In general, the co-
efficients for a given feature will be different in
each model?in the reduced case, the coefficients
incorporate correlation information and label pre-
dictive information for the removed (source-only)
features. This is potentially useful on the tar-
get domain, provided that there exist analogous,
target-only features that have similar correlations
with the shared features, and similar predictive co-
efficients.
For example, consider the discriminative source
and target features ?uhhuh? and ?mmm,? which
are both are correlated with a shared, noisier, fea-
ture (length). Forcing the model to learn only on
the shared, noisy feature incorporates correlation
information about ?uhhuh?, which is similar to
that of ?mmm?. Thus, the reduced model is poten-
tially more useful on the target domain, compared
to the full source domain model which might not
put weight on the noisy feature. On the other hand,
the approach is inappropriate in several other sce-
narios. For one, if the target domain utterances
actually represent samples from a subspace of the
source domain, the absence of features is informa-
tive: the fact that an utterance does not contain
??s??verdad??/s??, for instance, might mean that
it is less likely to be a Question, even if none of
the target domain utterances contain this feature.
4.3 Adaptation using SCL
The original formulation of SCL proposed predict-
ing pivot features using the entire feature set, ex-
cept for those features perfectly correlated with
49
the pivots (e.g., the pivots themselves). Our ex-
periments with this approach found it unsuitable
for our task, since even after removing the pivots
there are many features which remain highly cor-
related with the pivots due to overlapping n-grams
(i-love vs. love). The number of features that over-
lap with pivots is large, so removing these would
lead to few features being included in the projec-
tions. Therefore, we adopted the multi-view learn-
ing approach suggested by Blitzer et al (2009).
We split the utterances into two parts; pivot fea-
tures in the first part were predicted with all the
features in the second, and vice versa. We experi-
mented with splitting the utterances in the middle,
but found that since the number of words in the
first part (nearly) predicts the number in the sec-
ond part, all of the features in the first part were
positively predictive of pivots in the second part
so the main dimension learned was length. In the
results presented here, the first part consists of the
first word only, and the second part is the rest of
the utterance. (All utterances in our experiments
have at least one word.) Pivot features are selected
in each part and predicted using a least-squares
linear regression on all features in the other part.
We used the SCL-MI method of Blitzer et al
(2007) to select pivot features, which requires that
they be common in both domains and have high
mutual information (MI) with the class (according
to the source labels.) We selected features that oc-
curred at least 10 times in each domain and were
in the top 500 ranked MI features for any of the
four classes; this resulted in 78-99 first-part piv-
ots and 787-910 second-part pivots (depending on
the source-target pair). We performed SVD on
the learned prediction weights for each part sep-
arately, and the top (at most) 100 dimensions were
used to project utterances on each side.
In all train-test pairs, the first dimension of the
first part appeared to distinguish short utterance
words from long ones. Such short-utterance words
included backchannels from both domains, in ad-
dition to acknowledgments, exclamations, swear
words and greetings. An analogous dimension ex-
isted in the second part, which captured words cor-
related with short utterances greater than one word
(right, really, interesting). The other dimensions of
both domains were difficult to interpret.
We experimented with using the SCL fea-
tures together with the raw features (n-grams and
length), as suggested by (Blitzer et al, 2006). As
in (Blitzer et al, 2006), we found it necessary to
scale up the SCL features to increase their utiliza-
tion in the presence of the raw features; however,
it was difficult to guess the optimal scaling factor
without having access to labeled target data. The
results here use SCL features only, which also al-
lows us to more clearly investigate the utility of
those features and to compare them with the other
feature sets.
The most notable effect was an improvement
in Backchannel recall, which occurred under both
weighted and unweighted training. In addition,
there was high confusability between Statements
and the other classes, and more false detections
of Backchannels. When optimizing for accuracy,
SCL led to an improvement in accuracy in three
of the four cross-language cases. When optimiz-
ing for average per-class recall, it led to improve-
ment in all cross-language cases; however, re-
call of Statements went down dramatically in all
cases. In addition, while there was no clear ben-
efit of the SCL vs. the shared-feature method on
the cross-language cases, the SCL approach did
much worse than the shared-feature approach on
the Swbd/MRDA pair, causing large degradation
from the baseline.
As we have noted, utterance length appears
to underlie the improvement seen in the cross-
language performance for both the SCL and
shared-feature approaches. Therefore, we include
results for a classifier based only on the length
feature. Optimizing for accuracy, this method
achieves the highest accuracy of all methods in
the cross-language pairs. (It does so by classifying
everything as Statement or Backchannel, although
with weighted training, as shown in Figure 1, it
gets some Incompletes.) However, under weighted
class training, the average per-class recall of this
method is much worse than the shared-feature and
SCL approaches.
Comparison with other SCL tasks Although
we basically take a text classification approach to
the problem of dialog act tagging, our problem dif-
fers in several ways from the sentiment classifi-
cation task in Blitzer et al (2007). In particular,
utterances are much shorter than documents, and
we use position information via the start/end-of-
sentence tags. Some important DA cue features
(such as the value of the first word) are mutually
exclusive rather than correlated. In this way our
problem resembles the part-of-speech tagging task
50
(Blitzer et al, 2006), where the category of each
word is predicted using values of the left, right,
and current word token. In fact, that work used
a kind of multi-view learning for the SCL projec-
tion, with three views corresponding to the three
word categories. However, our problem essen-
tially uses a mix of bag-of-words and position-
based features, which poses a greater challenge
since there is no natural multi-view split. The ap-
proach described here suffers from the fact that it
cannot use all the features available to the base-
line classifier?bigrams and trigrams spanning the
first and second words are left out. It also suffers
from the fact that the first-word pivot feature set is
extremely small?a consequence of the small set
of first words that occur at least 10 times in the
29k-utterance corpora.
5 Conclusions
We have considered two approaches for domain
adaptation for DA tagging, and analyzed their
performance for source/target pairs drawn from
three different domains. For the English domains,
the baseline cross-domain performance was quite
good, and both adaptation methods generally led
to degradation over the baseline. For the cross-
language cases, both methods were effective at im-
proving average per-class recall, and particularly
Backchannel recall. SCL led to significant accu-
racy improvement in three cases, while the shared
feature approach did so in two cases. On the
other hand, SCL showed poor discrimination be-
tween Statements and other classes, and did worse
on the same-language pair that had little cross-
domain degradation. Both methods work by tak-
ing advantage of correlations between shared and
domain-specific class-discriminative features. Un-
fortunately in our task, membership in the rare
classes is often cued by features that are mutually
exclusive, e.g., the starting n-gram for Questions.
Both methods might therefore benefit from addi-
tional shared features that are correlated with these
n-grams, e.g., sentence-final intonation for Ques-
tions. (Indeed, other work on semi-supervised
DA tagging has used a richer feature set: Jeong
et al (2009) included parse, part-of-speech, and
speaker sequence information, and Venkataraman
et al (2003) used prosodic information, plus a
sequence-modeling framework.) From the task
perspective, an interesting result is that machine
translation appears to preserve most of the dialog-
act information, in that in-domain performance is
similar on original and translated text.
Acknowledgments
We thank Sham Kakade for suggesting the multi-
view SCL method based on utterance splits and
for many other helpful discussions, as well as John
Blitzer for helpful discussions. We thank the three
reviewers for their useful comments.
This research was funded by the Office of
the Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA). All statements of fact, opinion or con-
clusions contained herein are those of the authors
and should not be construed as representing the of-
ficial views or policies of IARPA, the ODNI or the
U.S. Government.
References
Anthony Aue and Michael Gamon. 2005. Customiz-ing sentiment classifiers to new domains: a casestudy. In Proc. International Conference on Recent
Advances in NLP.
Nuria Bel, Cornelis H. A. Koster, and Marta Ville-
gas. 2003. Cross-lingual text categorization. In
Research and Advanced Technology for Digital Li-
braries, pages 126?139. Springer Berlin / Heidel-
berg.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-dence learning. In Proc. of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 120?128.
John Blitzer, Mark Dredze, and Fernando Pereira.2007. Biographies, Bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,pages 440?447.
John Blitzer, Dean P. Foster, and Sham M. Kakade.2009. Zero-shot domain adaptation: A multi-view
approach. Technical report, Toyota TechnologicalInstitute TTI-TR-2009-1.
Mark G. Core and James F. Allen. 1997. Coding di-alogs with the DAMSL annotation scheme. In Proc.
of the Working Notes of the AAAI Fall Symposium on
Communicative Action in Humans and Machines.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Joa?o Graca, and FernandoPereira. 2007. Frustratingly hard domain adapta-tion for dependency parsing. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages1051?1055.
51
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: Alibrary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Umit Guz, Gokhan Tur, Dilek Hakkani-Tu?r, andSe?bastien Cuendet. 2010. Cascaded model adapta-tion for dialog act segmentation and tagging. Com-
puter Speech & Language, 24(2):289?306.
Jiayuan Huang, Alexander J. Smola, Arthur Gretton,
Karsten M. Borgwardt, and Bernhard Scho?lkopf.2007. Correcting sample selection bias by unlabeleddata. In Advances in Neural Information Processing
Systems 19, pages 601?608.
Minwoo Jeong, Chin Y. Lin, and Gary G. Lee. 2009.
Semi-supervised speech act recognition in emailsand forums. In Proc. of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1250?1259.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft 13. Tech-
nical report, University of Colorado at BoulderTechnical Report 97-02.
Lori Levin, Ann Thyme?-Gobbel, Alon Lavie, KlausRies, and Klaus Zechner. 1998. A discourse cod-ing scheme for conversational Spanish. In Proc. The
5th International Conference on Spoken Language
Processing, pages 2335?2338.
Yang Liu. 2006. Using SVM and error-correctingcodes for multiclass dialog act classification in meet-ing corpus. In Proc. Interspeech, pages 1938?1941.
Sinno J. Pan, James T. Kwok, and Qiang Yang. 2008.
Transfer learning via dimensionality reduction. In
Proc. of the Twenty-Third AAAI Conference on Arti-
ficial Intelligence.
Brian Roark and Michiel Bacchiani. 2003. Supervisedand unsupervised PCFG adaptation to novel do-
mains. In Proc. of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 126?133.
Sophie Rosset, Delphine Tribout, and Lori Lamel.2008. Multi-level information and automatic dia-log act detection in human?human spoken dialogs.
Speech Communication, 50(1):1?13.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meetingrecorder dialog act (MRDA) corpus. In Proc. of the
5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversationalspeech. Computational Linguistics, 26:339?373.
Dinoj Surendran and Gina-Anne Levow. 2006. Dialogact tagging with support vector machines and hiddenMarkov models. In Proc. Interspeech, pages 1950?
1953.
Gokhan Tur, Dilek Hakkani-Tu?r, and Robert E.Schapire. 2005. Combining active and semi-supervised learning for spoken language under-
standing. Speech Communication, 45(2):171?186.
Gokhan Tur, Umit Guz, and Dilek Hakkani-Tu?r.2007. Model adaptation for dialog act tagging.In Proc. IEEE Spoken Language Technology Work-
shop, pages 94?97.
Gokhan Tur. 2005. Model adaptation for spoken lan-guage understanding. In Proc. IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 41?44.
Anand Venkataraman, Luciana Ferrer, Andreas Stol-cke, and Elizabeth Shriberg. 2003. Traininga prosody-based dialog act tagger from unlabeled
data. In Proc. IEEE International Conference on
Acoustics, Speech, and Signal Processing, volume I,pages 272?275.
Nick Webb and Ting Liu. 2008. Investigating the
portability of corpus-derived cue phrases for dia-logue act classification. In Proc. of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 977?984.
Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and YongYu. 2007. Bridged refinement for transfer learn-ing. In Knowledge Discovery in Databases: PKDD
2007, pages 324?335. Springer Berlin / Heidelberg.
52
