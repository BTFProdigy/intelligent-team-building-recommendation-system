Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 107?115,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Modeling the Use of Graffiti Style Features to Signal Social Relations 
within a Multi-Domain Learning Paradigm 
Mario Piergallini1, A. Seza Do?ru?z2, Phani Gadde1, David Adamson1, Carolyn P. Ros?1,3  
1Language Technologies 
Institute 
Carnegie Mellon University 
5000 Forbes Avenue, 
Pittsburgh PA, 15213 
{mpiergal,pgadde, 
dadamson}@cs.cmu.edu 
2Tilburg University, TSH, 
5000 LE Tilburg, The 
Netherlands/ 
Language Technologies 
Institute, Carnegie Mellon 
University, 5000 Forbes 
Ave.,Pittsburgh PA 15213 
a.s.dogruoz@gmail.com 
3Human-Computer 
Interaction Institute 
Carnegie Mellon University 
5000 Forbes Avenue, 
Pittsburgh PA, 15213 
cprose@cs.cmu.edu 
 
Abstract 
In this paper, we present a series of 
experiments in which we analyze the usage of 
graffiti style features for signaling personal 
gang identification in a large, online street 
gangs forum, with an accuracy as high as 83% 
at the gang alliance level and 72% for the 
specific gang.  We then build on that result in 
predicting how members of different gangs 
signal the relationship between their gangs 
within threads where they are interacting with 
one another, with a predictive accuracy as high 
as 66% at this thread composition prediction 
task.  Our work demonstrates how graffiti 
style features signal social identity both in 
terms of personal group affiliation and 
between group alliances and oppositions.  
When we predict thread composition by 
modeling identity and relationship 
simultaneously using a multi-domain learning 
framework paired with a rich feature 
representation, we achieve significantly higher 
predictive accuracy than state-of-the-art 
baselines using one or the other in isolation. 
1 Introduction 
Analysis of linguistic style in social media has 
grown in popularity over the past decade.  
Popular prediction problems within this space 
include gender classification (Argamon et al., 
2003), age classification (Argamon et al., 2007), 
political affiliation classification (Jiang & 
Argamon, 2008), and sentiment analysis (Wiebe 
et al., 2004).  From a sociolinguistic perspective, 
this work can be thought of as fitting within the 
area of machine learning approaches to the 
analysis of style (Biber & Conrad, 2009), 
perhaps as a counterpart to work by variationist 
sociolinguists in their effort to map out the space 
of language variation and its accompanying 
social interpretation (Labov, 2010; Eckert & 
Rickford, 2001).  One aspiration of work in 
social media analysis is to contribute to this 
literature, but that requires that our models are 
interpretable.  The contribution of this paper is an 
investigation into the ways in which stylistic 
features behave in the language of participants of 
a large online community for street gang 
members.  We present a series of experiments 
that reveal new challenges in modeling stylistic 
variation with machine learning approaches.  As 
we will argue, the challenge is achieving high 
predictive accuracy without sacrificing 
interpretability. 
 Gang language is a type of sociolect that has 
so far not been the focus of modeling in the area 
of social media analysis.  Nevertheless, we argue 
that the gangs forum we have selected as our 
data source provides a strategic source of data for 
exploring how social context influences stylistic 
language choices, in part because it is an area 
where the dual goals of predictive accuracy and 
interpretability are equally important. In 
particular, evidence that gang related crime may 
account for up to 80% of crime in the United 
States attests to the importance of understanding 
the social practices of this important segment of 
society (Johnsons, 2009).  Expert testimony 
attributing meaning to observed, allegedly gang-
related social practices is frequently used as 
evidence of malice in criminal investigations 
(Greenlee, 2010).  Frequently, it is police officers 
who are given the authority to serve as expert 
witnesses on this interpretation because of their 
routine interaction with gang members.  
107
Nevertheless, one must consider their lack of 
formal training in forensic linguistics (Coulthard 
& Johnson, 2007) and the extent to which the 
nature of their interaction with gang members 
may subject them to a variety of cognitive biases 
that may threaten the validity of their 
interpretation (Kahneman, 2011).   
 Gang-related social identities are known to be 
displayed through clothing, tattoos, and language 
practices including speech, writing, and gesture 
(Valentine, 1995), and even dance (Philips, 
2009).  Forensic linguists have claimed that these 
observed social practices have been over-
interpreted and inaccurately interpreted where 
they have been used as evidence in criminal trials 
and that they may have even resulted in 
sentences that are not justified by sufficient 
evidence (Greenlee, 2010).  Sociolinguistic 
analysis of language varieties associated with 
gangs and other counter-cultural groups attests to 
the challenges in reliable interpretation of such 
practices (Bullock, 1996; Lefkowitz, 1989).  If 
we as a community can understand better how 
stylistic features behave due to the choices 
speakers make in social contexts, we will be in a 
better position to achieve high predictive 
accuracy with models that are nevertheless 
interpretable.  And ultimately, our models may 
offer insights into usage patterns of these social 
practices that may then offer a more solid 
empirical foundation for interpretation and use of 
language as evidence in criminal trials. 
 In the remainder of the paper we describe our 
annotated corpus.  We then motivate the 
technical approach we have taken to modeling 
linguistic practices within the gangs forum.  
Next, we present a series of experiments 
evaluating our approach and conclude with a 
discussion of remaining challenges. 
2 The Gangs Forum Corpus 
The forum that provides data for our experiments 
is an online forum for members of street gangs. 
The site was founded in November, 2006. It was 
originally intended to be an educational resource 
compiling knowledge about the various gang 
organizations and the street gang lifestyle. Over 
time, it became a social outlet for gang members. 
There are still traces of this earlier focus in that 
there are links at the top of each page to websites 
dedicated to information about particular gangs. 
At the time of scraping its contents, it had over a 
million posts and over twelve thousand active 
users.   Our work focuses on analysis of stylistic 
choices that are influenced by social context, so 
it is important to consider some details about the 
social context of this forum.  Specifically, we 
discuss which gangs are present in the data and 
how the gangs are organized into alliances and 
rivalries.  Users are annotated with their gang 
identity at two levels of granularity, and threads 
are annotated with labels that indicate which 
gang dominates and how the participating gangs 
relate to one another.   
2.1 User-Level Annotations 
At the fine-grained level, we annotated users 
with the gang that they indicated being affiliated 
with,  including Bloods, Crips, Hoovers, 
Gangster Disciples, other Folk Nation, Latin 
Kings, Vice Lords, Black P. Stones, other People 
Nation, Trinitarios, Norte?os, and Sure?os.  
There was also an Other category for the smaller 
gangs.  For a coarser grained annotation of gang 
affiliation, we also noted the nation, otherwise 
known as gang alliance, each gang was 
associated with.   
For our experiments, a sociolinguist with 
significant domain expertise annotated the gang 
identity of 3384 users.  Information used in our 
annotation included the user?s screen name, their 
profile, which included a slot for gang affiliation, 
and the content of their posts.  We used regular 
expressions to find gang names or other 
identifiers occurring within the gang affiliation 
field and the screen names and annotated the 
users that matched.  If the value extracted for the 
two fields conflicted, we marked them as 
claiming multiple gangs.  For users whose 
affiliation could not be identified automatically, 
we manually checked their profile to see if their 
avatar (an image that accompanies their posts) or 
other fields there contained any explicit 
information.  Otherwise, we skimmed their posts 
for explicit statements of gang affiliation.   
Affiliation was unambiguously identified 
automatically for 56% of the 3384 users from 
their affiliation field.  Another 36% were 
identified automatically based on their screen 
name.  Manual inspection was only necessary in 
9% of the cases.  Users that remained ambiguous, 
were clearly fake or joke accounts, or who 
claimed multiple gangs were grouped together in 
an ?Other? category, which accounts for 6.2% of 
the total.  Thus, 94% of the users were classified 
into the 12 specific gangs mentioned above. 
108
At a coarse-grained level, users were also 
associated with a nation.  The nation category 
was inspired by the well-known gang alliances 
known as the People Nation and Folks Nation, 
which are city-wide alliances of gangs in 
Chicago. We labeled the Crips and Hoovers as a 
nation since they are closely allied gangs.  
Historically, the Hoovers began breaking away 
from the Crips and are rivals with certain subsets 
of Crips, but allies with the majority of other 
Crips gangs.  The complex inner structure of the 
Crips alliance will be discussed in Section 5 
where we interpret our quantitative results. 
There are a large number of gangs that 
comprise the People and Folks Nations. The 
major gangs within the People Nation are the 
Latin Kings, Vice Lords and Black P. Stones. 
The Folks Nation is dominated by the Gangster 
Disciples with other Folks Nation gangs being 
significantly smaller. The People Nation, Blood 
and Norte?os gangs are in a loose, national 
alliance against the opposing national alliance of 
the Folks Nation, Crips and Sure?os. Remaining 
gangs were annotated as other, such as the 
Trinitarios, that don't fit into this national 
alliance system nor even smaller alliances.   
2.2 Thread-Level Annotations 
In addition to person-level annotations of gang 
and nation, we also annotated 949 threads with 
dominant gang as well as thread composition, by 
which we mean whether the users who 
participated on the thread were only from allied 
gangs, included opposing gangs, or contained a 
mix of gangs that were neither opposing nor 
allied.  These 949 threads were ones where a 
majority of the users who posted were in the set 
of 3384 users annotated with a gang identity. 
For the dominant gang annotation at the 
gang level, we consider only participants on the 
thread for whom there was an annotated gang 
affiliation. If members of a single gang produced 
the majority of the posts in the thread, then that 
was annotated as the dominant gang of the thread. 
If no gang had a majority in the thread, it was 
instead labeled as Mixed. For dominant gang at 
the nation level, the same procedure was used, 
but instead of looking for which gang accounted 
for more of the members, we looked for which 
gang alliance accounted for the majority of users. 
For the thread composition annotation, we 
treated the Bloods, People Nation, and Norte?os 
as allied with each other as the ?Red set?.  We 
treated Crips, Hoovers, Folks Nation, and 
Sure?os as allies with each other as the ?Blue 
set?.  The Red and Blue sets oppose one another.  
The Latin Kings and Trinitarios also oppose one 
another.  Thread composition was labeled as 
Allied, Mixed or Opposing depending on the 
gangs that appeared in the thread. As with the 
dominant gang annotation, only annotated users 
were considered. If all of the posts were by users 
of the same gang or allied gangs, the thread was 
labeled as Allied.  If there were any posts from 
rival gangs, it was labeled as Opposing. 
Otherwise, it was labeled as Mixed. If the users 
were all labeled with Other as their gang it was 
also labeled as Mixed.  
3 Modeling Language Practices at the 
Feature Level 
In this section, we first describe the rich feature 
representation we developed for this work.  
Finally, we discuss the motivation for employing 
a multi-domain learning framework in our 
machine-learning experiments. 
3.1 Feature Space Design: Graffiti Style 
Features 
While computational work modeling gang-
related language practices is scant, we can learn 
lessons from computational work on other types 
of sociolects that may motivate a reasonable 
approach.  Gender prediction, for example, is a 
problem where there have been numerous 
publications in the past decade (Corney et al., 
2002; Argamon et al., 2003; Schler et al., 2005; 
Schler, 2006; Yan & Yan, 2006; Zhang et al., 
2009).  Because of the complex and subtle way 
gender influences language choices, it is a 
strategic example to motivate our work. 
 Gender-based language variation arises from 
multiple sources. Among these, it has been noted 
that within a single corpus comprised of samples 
of male and female language that the two 
genders do not speak or write about the same 
topics. This is problematic because word-based 
features such as unigrams and bigrams, which 
are very frequently used, are highly likely to pick 
up on differences in topic (Schler, 2006) and 
possibly perspective. Thus, in cases where 
linguistic style variation is specifically of 
interest, these features do not offer good 
generalizability (Gianfortoni et al., 2011). 
Similarly, in our work, members of different 
109
gangs are located in different areas associated 
with different concerns and levels of 
socioeconomic status.  Thus, in working to 
model the stylistic choices of gang forum 
members, it is important to consider how to 
avoid overfitting to content-level distinctions. 
 Typical kinds of features that have been used 
in gender prediction apart from unigram features 
include part-of-speech (POS) ngrams (Argamon 
et al., 2003), word-structure features that cluster 
words according to endings that indicate part of 
speech (Zhang et al., 2009), features that indicate 
the distribution of word lengths within a corpus 
(Corney et al., 2002), usage of punctuation, and 
features related to usage of jargon (Schler et al., 
2005). In Internet-based communication, 
additional features have been investigated such 
as usage of internet specific features including 
?internet speak? (e.g., lol, wtf, etc.), emoticons, 
and URLs (Yan & Yan, 2006).   
Transformation Origin or meaning 
b^, c^, h^, p^ ?Bloods up? Positive towards 
Bloods, Crips, Hoovers, 
Pirus, respectively 
b ? bk, c ? ck 
h ? hk, p ? pk 
Blood killer, Crip killer 
Hoover killer, Piru killer 
ck ? cc, kc Avoid use of ?ck? since it 
represents Crip killer 
o ? x, o ? ? Represents crosshairs, 
crossing out the ?0?s in a 
name like Rollin? 60s Crips 
b ? 6 Represents the six-pointed 
star. Symbol of Folk Nation 
and the affiliated Crips. 
e ? 3 Various. One is the trinity in 
Trinitario. 
s ? 5 Represents the five-pointed 
star. Symbol of People 
Nation and the affiliated 
Bloods. 
Table 1: Orthographical substitutions from gang 
graffiti symbolism 
 
 In order to place ourselves in the best position 
to build an interpretable model, our space of 
graffiti style features was designed based on a 
combination of qualitative observations of the 
gangs forum data and reading about gang 
communication using web accessible resources 
such as informational web pages linked to the 
forum and other resources related to gang 
communication (Adams & Winter, 1997; Garot, 
2007).  Specifically, in our corpus we observed 
gang members using what we refer to as graffiti 
style features to mark their identity.  Gang 
graffiti employs shorthand references to convey 
affiliation or threats (Adams & Winter, 
1997).  For example, the addition of a <k> after a 
letter representing a rival gang stands for ?killer.? 
So, writing <ck> would represent ?crip killer.? A 
summary of these substitutions can be seen in 
Table 1.  Unfortunately, only about 25% of the 
users among the 12,000 active users employ 
these features in their posts, which limits their 
ability to achieve a high accuracy, but 
nevertheless offers the opportunity to model a 
frequent social practice observed in the corpus.  
 The graffiti style features were extracted 
using a rule-based algorithm that compares 
words against a standard dictionary as well as 
using some phonotactic constraints on the 
position of certain letters.  The dictionary was 
constructed using all of the unique words found 
in the AQUAINT corpus (Graff, 2002).  If a 
word in a post did not match any word from the 
AQUAINT corpus, we tested it against each of 
the possible transformations in Table 1.  
Transformations were applied to words using 
finite state transducers.  If some combination 
transformations from that table applied to the 
observed word could produce some term from 
the AQUAINT corpus, then we counted that 
observed word as containing the features 
associated with the applied transformations. 
 The transformations were applied in the order 
of least likely to occur in normal text to the most 
likely. Since ?bk? only occurs in a handful of 
obscure words, for example, almost any 
occurrence of it can be assumed to be a 
substitution and the ?k? can safely be removed 
before the next step. By contrast, ?cc? and ?ck? 
occur in many common words so they must be 
saved for last to ensure that the final dictionary 
checks have any simultaneous substitutions 
already removed. 
 When computing values for the graffiti style 
features for a text, the value for each feature was 
computed as the number of words (tokens) that 
contained the feature divided by the total number 
of words (tokens) in the document.  We used a 
set of 13 of these features, chosen on the basis of 
how frequently they occurred and how strongly 
they distinguished gangs from one another (for 
example, substituting ?$? for ?s? was a 
transformation that was common across gangs in 
110
our qualitative analysis, and thus did not seem 
beneficial to include).  
Transformation Freq. False 
Positive 
rate 
False 
Negative 
rate 
b^, c^, h^, p^ 15103 0% 0% 
b ? bk 26923 1% 0% 
c ? ck 16144 25% 8% 
h ? hk 10053 1% 0% 
p ? pk 5669 3% 0% 
ck ? cc, kc 72086 2% 0% 
o ? x, o ? ? 13646 15% 5% 
b ? 6 2470 16% 0% 
e ? 3 8628 28% 1% 
s ? 5 13754 6% 0% 
Table 2: Evaluation of extraction of graffiti style 
features over the million post corpus 
 
 The feature-extraction approach was 
developed iteratively. After extracting the 
features over the corpus of 12,000 active users, 
we created lists of words where the features were 
detected, sorted by frequency. We then manually 
examined the words to determine where we 
observed errors occurring and then made some 
minor adjustments to the extractors.  Table 2 
displays a quantitative evaluation of the accuracy 
of the graffiti style feature extraction. 
 Performance of the style features was 
estimated for each style-feature rule.  For each 
rule, we compute a false positive and false 
negative rate.  For false positive rate, we begin 
by retrieving the list of words marked by the 
feature extraction rule containing the associated 
style marking. From the full set of words that 
matched a style feature rule, we selected the 200 
most frequently occurring word types.  We 
manually checked that complete set of word 
tokens and counted the number of misfires.  The 
false positive rate was then calculated for each 
feature by dividing the number of tokens that 
were misfires over the total number of tokens in 
the set. In all cases, we ensured that at least 55% 
of the total word tokens were covered, so 
additional words may have been examined.  
 In the case of false negatives, we started with 
the set of word types that did not match any word 
in the dictionary and also did not trigger the style 
feature rule.  Again we sorted word types in this 
list by frequency and selected the top 200 most 
frequent.  We then manually checked for missed 
instances where the associated style feature was 
used but not detected.  The false negative rate 
was then the total number of word tokens within 
this word type set divided by the total number of 
word tokens in the complete set of word types. 
 Another type of feature we used referenced 
the nicknames gangs used for themselves and 
other gangs, which we refer to as Names features.  
The intuition behind this is simple: someone who 
is a member of the Crips gang will talk about the 
Crips more often. The measure is simply how 
often a reference to a gang occurs per document. 
Some of these nicknames we included were 
gang-specific insults, with the idea that if 
someone uses insults for Crips often, they are 
likely not a Crip. The last type of reference is 
words that refer to gang alliances like the People 
Nation and Folks Nation. Members of those 
Chicago-based gangs frequently refer to their 
gang as the ?Almighty [gang name] Nation?. 
Gang Positive/Neutral 
Mentions 
Insults 
Crips crip, loc crab, ckrip, ck 
Bloods blood, damu, 
piru, ubn 
slob, bklood, 
pkiru, bk, pk 
Hoovers hoover, groover, 
crim, hgc, hcg 
snoover, 
hkoover, hk 
Gangster 
Disciples 
GD, GDN, 
Gangster 
Disciple 
gk, dk, nigka 
Folks 
Nations 
folk, folknation, 
almighty, nation 
 
People 
Nation 
people, 
peoplenation, 
almighty, nation 
 
Latin 
Kings 
alkqn, king, 
queen 
 
Black P. 
Stones 
stone, abpsn, 
moe, black p. 
 
Vice 
Lords 
vice, lord, vl, 
avln, foe, 4ch 
 
Table 3: Patterns used for gang name features.  For all 
gangs listed in the table, there are slang terms used as 
positive mentions of the gang.  For some gangs there 
are also typical insult names. 
 
We used regular expressions to capture 
occurrences of these words and variations on 
them such as the use of the orthographic 
substitutions mentioned previously, plurals, 
feminine forms, etc. Additionally, in the Blood 
and Hoover features, they sometimes use 
numbers to replace the ?o?s representing the 
street that their gang is located on. So the Bloods 
from 34th Street, say, might write ?Bl34d?. 
111
3.2 Computational Paradigm: Multi-
domain learning 
The key to training an interpretable model in our 
work is to pair a rich feature representation with 
a model that enables accounting for the structure 
of the social context explicitly.  Recent work in 
the area of multi-domain learning offers such an 
opportunity (Arnold, 2009; Daum? III, 2007; 
Finkel & Manning, 2009).  In our work, we treat 
the dominant gang of a thread as a domain for 
the purpose of detecting thread composition.  
This decision is based on the observation that 
while it is a common practice across gangs to 
express their attitudes towards allied and 
opposing gangs using stylistic features like the 
Graffiti style features, the particular features that 
serve the purpose of showing affiliation or 
opposition differ by gang.  Thus, it is not the 
features themselves that carry significance, but 
rather a combination of who is saying it and how 
it is being said. 
 As a paradigm for multi-domain learning, we 
use Daume?s Frustratingly Easy Domain 
Adaptation approach (Daum? III, 2007) as 
implemented in LightSIDE (Mayfield & Ros?, 
2013). In this work, Daum? III proposes a very 
simple ?easy adapt? approach, which was 
originally proposed in the context of adapting to 
a specific target domain, but easily generalizes to 
multi-domain learning. The key idea is to create 
domain-specific versions of the original input 
features depending on which domain a data point 
belongs to. The original features represent a 
domain-general feature space. This allows any 
standard learner to appropriately optimize the 
weights of domain-specific and domain-general 
features simultaneously.  In our work, this allows 
us to model how different gangs signal within-
group identification and across-group animosity 
or alliance using different features.  The resulting 
model will enable us to identify how gangs differ 
in their usage of style features to display social 
identity and social relations. 
 It has been noted in prior work that style is 
often expressed in a topic-specific or even 
domain-specific way (Gianfortoni et al., 2011).  
What exacerbates these problems in text 
processing approaches is that texts are typically 
represented with features that are at the wrong 
level of granularity for what is being 
modeled.  Specifically, for practical reasons, the 
most common types of features used in text 
classification tasks are still unigrams, bigrams, 
and part-of-speech bigrams, which are highly 
prone to over-fitting. When text is represented 
with features that operate at too fine-grained of a 
level, features that truly model the target style are 
not present within the model.  Thus, the trained 
models are not able to capture the style itself and 
instead capture features that correlate with that 
style within the data (Gianfortoni et al., 2011). 
 This is particularly problematic in cases 
where the data is not independent and identically 
distributed (IID), and especially where instances 
that belong to different subpopulations within the 
non-IID data have different class value 
distributions.  In those cases, the model will tend 
to give weight to features that indicate the 
subpopulation rather than features that model the 
style.   Because of this insight from prior work, 
we contrast our stylistic features with unigram 
features and our multi-domain approach with a 
single-domain approach wherever appropriate in 
our experiments presented in Section 4. 
4 Prediction Experiments 
In this section we present a series of prediction 
experiments using the annotations described in 
Section 2.  We begin by evaluating our ability to 
identify gang affiliation for individual users.  
Because we will use dominant gang as a domain 
feature in our multi-domain learning approach to 
detect thread composition, we also present an 
evaluation of our ability to automatically predict 
dominant gang for a thread.  Finally, we evaluate 
our ability to predict thread composition.  All of 
our experiments use L1 regularized Logistic 
regression. 
4.1 Predicting Gang Affiliation per User 
The first set of prediction experiments we ran 
was to identify gang affiliation.  For this 
experiment, the full set of posts contributed by a 
user was concatenated together and used as a 
document from which to extract text features.  
We conducted this experiment using a 10-fold 
cross-validation over the full set of users 
annotated for gang affiliation. Results contrasting 
alternative feature spaces at the gang level and 
nation level are displayed in Table 4.  We begin 
with a unigram feature space as the baseline.  We 
contrast this with the Graffiti style features 
described above in Section 3.1.  Because all of 
the Graffiti features are encoded in words as 
pairs of characters, we contrast the carefully 
extracted Graffiti style features with character 
112
bigrams.  Next we test the nickname features 
also described in Section 3.1.  Finally, we test 
combinations of these features.   
 Gang Nation 
Unigrams 70% 81% 
Character Bigrams 64% 76% 
Graffiti Features 44% 68% 
Name Features 63% 78% 
Name + Graffiti 67% 81% 
Unigrams + Name 70% 82% 
Unigrams + Character 
Bigrams 
71% 82% 
Unigrams + Graffiti 71% 82% 
Unigrams + Name  + 
Graffiti 
72% 83% 
Unigrams + Name  + 
Character Bigrams 
72% 79% 
Table 4: Results (percent accuracy) for gang 
affiliation prediction at the gang and nation level. 
  
     We note that the unigram space is a 
challenging feature space to beat, possibly 
because only about 25% of the users employ the 
style features we identified with any regularity.  
The character bigram space actually significantly 
outperforms the Graffiti features, in part because 
it captures aspects of both the Graffiti features, 
the name features, and also some other gang 
specific jargon.  When we combine the stylistic 
features with unigrams, we start to see an 
advantage over unigrams alone.  The best 
combination is Unigrams, Graffiti style features, 
and Name features, at 72% accuracy (.65 Kappa) 
at the gang level and 83% accuracy (.69 Kappa) 
at the nation level.  Overall the accuracy is 
reasonable and offers us the opportunity to 
expand our analysis of social practices on the 
gangs forum to a much larger sample in our 
future work than we present in this first foray. 
4.2 Predicting Dominant Gang per Thread 
In Section 4.3 we present our multi-domain 
learning approach to predicting thread 
composition.  In that work, we use dominant 
gang on a thread as a domain.  In those 
experiments, we contrast results with hand-
annotated dominant gang and automatically-
predicted dominant gang.  In order to compute an 
automatically-identified dominant gang for the 
949 threads used in that experiment, we build a 
model for gang affiliation prediction using data 
from the 2689 users who did not participate on 
any of those threads as training data so there is 
no overlap in users between train and test. 
     The feature space for that classifier included 
unigrams, character bigrams, and the gang name 
features since this feature space tied for best 
performing at the gang level in Section 4.1 and 
presents a slightly lighter weight solution than 
Unigrams, graffiti style features, and gang name 
features. We applied that trained classifier to the 
users who participated on the 949 threads.  From 
the automatically-predicted gang affiliations, we 
computed a dominant gang using the gang and 
nation level for each thread using the same rules 
that we applied to the annotated user identities 
for the annotated dominant gang labels described 
in Section 2.2.  We then evaluated our 
performance by comparing the automatically-
identified dominant gang with the more carefully 
annotated one.  Our automatically identified 
dominant gang labels were 73.3% accurate (.63 
Kappa) at the gang level and 76.6% accurate (.72 
Kappa) at the nation level. This experiment is 
mainly important as preparation for the 
experiment presented in Section 4.3. 
4.3 Predicting Thread Composition 
Our final and arguably most important prediction 
experiments were for prediction of thread 
composition.  This is where we begin to 
investigate how stylistic choices reflect the 
relationships between participants in a 
discussion.  We conducted this experiment twice, 
specifically, once with the annotated dominant 
gang labels (Table 5) and once with the 
automatically predicted ones (Table 6).  In both 
cases, we evaluate gang and nation as alternative 
domain variables.  In both sets of experiments, 
the multi-domain versions significantly 
outperform the baseline across a variety of 
feature spaces, and the stylistic features provide 
benefit above the unigram baseline.  In both 
tables the domain and nation variables are hand-
annotated. * indicates the results are significantly 
better than the no domain unigram baseline.  
Underline indicates best result per column.  And 
bold indicates overall best result.  
     The best performing models in both cases 
used a multi-domain model paired with a stylistic 
feature space rather than a unigram space.  Both 
models performed significantly better than any of 
the unigram models, even the multi-domain 
versions with annotated domains. Where gang 
was used as the domain variable and Graffiti 
style features were the features used for 
prediction, we found that the high weight 
features associated with Allied threads were 
113
either positive about gang identity for a variety 
of gangs other than their own (like B^ in a Crips 
dominated thread) or protective (like CC in a 
Bloods dominated thread).   
 No 
Domain 
Dominant 
Gang 
Dominant 
Nation 
Unigrams 53% 58%* 60%* 
Character 
Bigrams 
49% 55% 56% 
Graffiti 
Features 
53% 54% 61%* 
Name 
Features 
54% 63%* 66%* 
Name + 
Graffiti 
54% 61%* 65%* 
Unigrams 
+ Name 
52% 58%* 61%* 
Unigrams 
+ Graffiti 
53% 57% 57% 
Unigrams 
+ Name  
+ Graffiti 
54% 61%* 65%* 
Table 5: Results (percent accuracy) for thread 
composition prediction, contrasting a single domain 
approach with two multi-domain approaches, one 
with dominant gang as the domain variables, and the 
other with dominant nation as the domain variable. In 
this case, the domain variables are annotated. 
 
 No 
Domain 
Dominant 
Gang 
Dominant 
Nation 
Unigrams 53% 57% 57% 
Character 
Bigrams 
49% 53% 55% 
Graffiti 
Features 
53% 65%* 58%* 
Name 
Features 
54% 61%* 59%* 
Name + 
Graffiti 
54% 60%* 59%* 
Unigrams 
+ Name 
52% 56% 56% 
Unigrams 
+ Graffiti 
53% 58%* 57% 
Unigrams 
+ Name  
+ Graffiti 
54% 60%* 59%* 
Table 6: Results (percent accuracy) for thread 
composition prediction, contrasting a single domain 
approach with two multi-domain approaches with 
predicted domain variables, one with dominant gang 
as the domain variables, and the other with dominant 
nation as the domain variable.  
 
Crips-related features were the most frequent 
within this set, perhaps because of the complex 
social structure within the Crips alliance, as 
discussed above.  We saw neither features 
associated with negative attitudes of the gang 
towards others nor other gangs towards them in 
these Allied threads, but in opposing threads, we 
see both, for example, PK in Crips threads or BK 
in Bloods threads.  Where unigrams are used as 
the feature space, the high weight features are 
almost exclusively in the general space rather 
than the domain space, and are generally 
associated with attitude directly rather than gang 
identity.  For example, ?lol,? and ?wtf.? 
5 Conclusions  
We have presented a series of experiments in 
which we have analyzed the usage of stylistic 
features for signaling personal gang 
identification and between gang relations in a 
large, online street gangs forum.  This first foray 
into modeling the language practices of gang 
members is one step towards providing an 
empirical foundation for interpretation of these 
practices.  In embarking upon such an endeavor, 
however, we must use caution.  In machine-
learning approaches to modeling stylistic 
variation, a preference is often given to 
accounting for variance over interpretability, 
with the result that interpretability of models is 
sacrificed in order to achieve a higher prediction 
accuracy.  Simple feature encodings such as 
unigrams are frequently chosen in a (possibly 
misguided) attempt to avoid bias.  As we have 
discussed above, however, rather than cognizant 
introduction of bias informed by prior linguistic 
work, unknown bias is frequently introduced 
because of variables we have not accounted for 
and confounding factors we are not aware of, 
especially in social data that is rarely IID. Our 
results suggest that a strategic combination of 
rich feature encodings and structured modeling 
approach leads to high accuracy and 
interpretability.  In our future work, we will use 
our models to investigate language practices in 
the forum at large rather than the subset of users 
and threads used in this paper1. 
                                                          
1 An appendix with additional analysis and the 
specifics of the feature extraction rules can be found 
at http://www.cs.cmu.edu/~cprose/Graffiti.html. This 
work was funded in part by ARL 
000665610000034354.   
114
References  
Adams, K. & Winter, A. (1997). Gang graffiti as a 
discourse genre, Journal of Sociolinguistics 1/3. Pp 
337-360. 
Argamon, S., Koppel, M., Fine, J., & Shimoni, A. 
(2003). Gender, genre, and writing style in formal 
written texts, Text, 23(3), pp 321-346. 
Argamon, S., Koppel, M., Pennebaker, J., & Schler, J. 
(2007). Mining the blogosphere: age, gender, and 
the varieties of self-expression. First Monday 
12(9). 
Arnold, A. (2009). Exploiting Domain And Task 
Regularities For Robust Named Entity 
Recognition. PhD thesis, Carnegie Mellon 
University, 2009. 
Biber, D. & Conrad, S. (2009). Register, Genre, and 
Style, Cambridge University Press 
Bullock, B. (1996). Derivation and Linguistic Inquiry: 
Les Javnais, The French Review 70(2), pp 180-191. 
Corney, M., de Vel, O., Anderson, A., Mohay, G. 
(2002). Gender-preferential text mining of e-mail 
discourse, in the Proceedings of the 18th Annual 
Computer Security Applications Conference. 
Coulthard, M. & Johnson, A. (2007). An Introduction 
to Forensic Linguistics: Language as Evidence, 
Routledge 
Daum? III, H. (2007). Frustratingly Easy Domain 
Adaptation. In Proceedings of the 45th Annual 
Meeting of the Association of Computational 
Linguistics, pages 256-263. 
Eckert, P. & Rickford, J. (2001). Style and 
Sociolinguistic Variation, Cambridge: University 
of Cambridge Press. 
Finkel, J. & Manning, C. (2009). Hierarchical 
Bayesian Domain Adaptation. In Proceedings of 
Human Language Technologies: The 2009 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics. 
Garot, R. (2007). ?Where You From!?: Gang Identity 
as Performance, Journal of Contemporary 
Ethnography, 36, pp 50-84. 
Gianfortoni, P., Adamson, D. & Ros?, C. P. (2011).  
Modeling Stylistic Variation in Social Media with 
Stretchy Patterns, in Proceedings of First 
Workshop on Algorithms and Resources for 
Modeling of Dialects and Language Varieties, 
Edinburgh, Scottland, UK, pp 49-59. 
Graff, D. (2002).  The AQUAINT Corpus of English 
News Text, Linguistic Data Consortium, 
Philadelphia 
Greenlee, M. (2010).  Youth and Gangs, in M. 
Coulthard and A. Johnson (Eds.). The Routledge 
Handbook of Forensic Linguistics, Routledge. 
Jiang, M. & Argamon, S. (2008). Political leaning 
categorization by exploring subjectivities in 
political blogs. In Proceedings of the 4th 
International Conference on Data Mining, pages 
647-653. 
Johnsons, K. (2009).  FBI: Burgeoning gangs behind 
up to 80% of U.S. Crime, in USA Today, January 
29, 2009. 
Kahneman,  D. (2011).  Thinking Fast and Slow, 
Farrar, Straus, and Giroux 
Krippendorff, K. (2013). Content Analysis: An 
Introduction to Its Methodology (Chapter 13), 
SAGE Publications 
Labov, W. (2010). Principles of Linguistic Change: 
Internal Factors (Volume 1), Wiley-Blackwell. 
Lefkowitz, N. (1989).  Talking Backwards in French, 
The French Review 63(2), pp 312-322. 
Mayfield, E. & Ros?, C. P. (2013). LightSIDE: Open 
Source Machine Learning for Text Accessible to 
Non-Experts, in The Handbook of Automated 
Essay Grading, Routledge Academic Press.        
http://lightsidelabs.com/research/ 
Philips, S. (2009).  Crip Walk, Villian Dance, Pueblo 
Stroll: The Embodiment of Writing in African 
American Gang Dance, Anthropological Quarterly 
82(1), pp69-97. 
Schler, J., Koppel, M., Argamon, S., Pennebaker, J. 
(2005). Effects of Age and Gender on Blogging, 
Proceedings of AAAI Spring Symposium on 
Computational Approaches for Analyzing Weblogs. 
Schler, J. (2006). Effects of Age and Gender on 
Blogging. Artificial Intelligence, 86, 82-84. 
Wiebe, J., Bruce, R., Martin, M., Wilson, T., & Ball, 
M. (2004). Learning Subjective Language, 
Computational Linguistics, 30(3). 
Yan, X., & Yan, L. (2006). Gender classification of 
weblog authors. AAAI Spring Symposium Series 
Computational Approaches to Analyzing Weblogs 
(p. 228?230). 
Zhang, Y., Dang, Y., Chen, H. (2009). Gender 
Difference Analysis of Political Web Forums : An 
Experiment on International Islamic Women?s 
Forum, Proceedings of the 2009 IEEE international 
conference on Intelligence and security 
informatics, pp 61-64. 
 
115
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 657?660,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Data Driven Dependency Parsing using Clausal Information 

Phani Gadde, Karan Jindal, Samar Husain, Dipti Misra Sharma, Rajeev Sangal 
Language Technologies Research Centre, IIIT-Hyderabad, India. 
phani.gadde@research.iiit.ac.in, karan_jindal@students.iiit.ac.in, 
{samar,dipti,sangal}@mail.iiit.ac.in 
 
 
 
 
 
Abstract 
The paper describes a data driven dependency 
parsing approach which uses clausal informa-
tion of a sentence to improve the parser per-
formance. The clausal information is added 
automatically during the parsing process. We 
demonstrate the experiments on Hindi, a lan-
guage with relatively rich case marking sys-
tem and free-word-order. All the experiments 
are done using a modified version of 
MSTParser. We did all the experiments on the 
ICON 2009 parsing contest data. We achieved 
an improvement of 0.87% and 0.77% in unla-
beled attachment and labeled attachment accu-
racies respectively over the baseline parsing 
accuracies. 
1 Introduction 
Linguistic analysis of morphologically rich free-
word-order languages (MoRFWO) using depen-
dency framework have been argued to be more 
effective (Shieber, 1985; Mel??uk, 1988, Bharati et 
al., 1993). Not surprisingly, most parsers for such 
languages are dependency based (Nivre et al, 
2007a; Bharati et al, 2008a; Hall et al, 2007). In 
spite of availability of annotated treebanks, state-
of-the-art parsers for MoRFWO have not reached 
the performance obtained for English. Some of the 
reasons stated for the low performance are small 
treebank size, complex linguistic phenomenon, 
long-distance dependencies, and non-projective 
structures (Nivre et al, 2007a, 2007b; Bharati et 
al., 2008a).   
Several approaches have been tried to handle these 
difficulties in such languages. For Hindi, Bharati et   
al. (2008a) and Ambati et al (2009) used semantic 
features in parsing to reduce the negative impact of 
unavailable syntactic features and showed that use 
of minimal semantics can help in identifying cer-
tain core dependency labels. Various attempts have 
proved to simplify the structure by dividing the 
sentence into suitable linguistic units (Attardi and 
Dell?Orletta 2008; Bharati et al, 1993, 2008b, 
2009; Husain et al, 2009). These approaches han-
dle complex structures by breaking the parsing 
process into several steps. Attardi and Dell'Orletta 
(2008) used chunk information as a feature to 
MaltParser (Nivre et al, 2007a) for parsing Eng-
lish. Bharati et al, 1993 used the notion of local 
word groups, while Bharati et al, 2009 and Husain 
et al, 2009 used clauses.   
In this paper, we describe a data driven depen-
dency parsing approach which uses clausal infor-
mation of a sentence to improve the parser 
performance. Previous attempts at data driven 
parsing for Hindi have failed to exploit this feature 
explicitly. The clausal information is added auto-
matically during the parsing process. We demon-
strate the experiments on Hindi1. All the 
experiments are done using a modified version of 
MSTParser (McDonald et al, 2005a and the refer-
ences therein) (henceforth MST) on the ICON 
2009 parsing contest2 (Husain, 2009) data. We 
achieved an improvement of 0.87% and 0.77% in 
unlabeled attachment and labeled attachment accu-
racies respectively over the baseline parsing accu-
racies. 
                                                        
1 Hindi is a verb final language with free word order and a rich 
case marking system. It is an official language of India and is 
spoken by ~800 million people. 
2 http://www.icon2009.in/contests.html 
657
2 Why Clausal Information?  
Traditionally, a clause is defined as a group of 
words having a subject and a predicate. Clause 
boundary identification is the process of dividing 
the given sentence into a set of clauses. It can be 
seen as a partial parsing step after chunking, in 
which one tries to divide the sentence into mea-
ningful units. It is evident that most of the depen-
dents of words in a clause appear inside the same 
clause; in other words the dependencies of the 
words in a clause are mostly localized within the 
clause boundary. 
In the dependency parsing task, a parser has to 
disambiguate between several words in the sen-
tence to find the parent/child of a particular word. 
This work is to see whether the clause boundary 
information can help the parser to reduce the 
search space when it is trying to find the correct 
parent/child for a word. The search space of the 
parser can be reduced by a large extent if we solve 
a relatively small problem of identifying the claus-
es. Interestingly, it has been shown recently that 
most of the non-projective cases in Hindi are inter-
clausal (Mannem et al, 2009). Identifying clausal 
boundaries, therefore, should prove to be helpful in 
parsing non-projective structures. The same holds 
true for many long-distance dependencies. 
3 Experimental Setup 
3.1 Dataset 
The experiments reported in this paper have been 
done on Hindi; the data was released as part of the 
ICON 2009 parsing contest (Husain, 2009). The 
sentences used for this contest are subset of the 
Hyderabad Dependency Treebank (HyDT) devel-
oped for Hindi (Begum et al, 2008). The depen-
dency relations in the treebank are syntactico-
semantic. The dependency tagset in the annotation 
scheme has around 28 relations. The dependency 
trees in the treebank show relations between chunk 
heads. Note, therefore, that the experiments and 
results described in this paper are based on parse 
trees that have chunk head as nodes. 
The data provided in the task contained morpho-
logical features along with the lemma, POS tag, 
and coarse POS tag, for each word. These are six 
morphological features namely category, gender, 
number, person, vibhakti3 or TAM4 markers of the 
node 
3.2 Clause Boundary Identifier 
We used the Stage15 parser of Husain et al (2009), 
to provide the clause boundary information that is 
then incorporated as features during the actual 
parsing process. The Stage1 parser uses MST to 
identify just the intra-clausal relations. To achieve 
this, Husain et al, introduce a special dummy node 
named _ROOT_ which becomes the head of the 
sentence. All the clauses are connected to this 
dummy node with a dummy relation. In effect the 
Stage1 parser gives only intra-clausal relations. In 
the current work, we used MaltParser6 (Nivre et al, 
2007b) (henceforth Malt) to do this task. This is 
because Malt performs better than MST in case of 
intra-clausal relations, which are mostly short dis-
tance dependencies. We use the same algorithm 
and feature setting of Bharati et al, (2008a) to train 
the Stage1 parser. 
Since the above tool parses clauses, therefore 
along with the clause boundary information we 
also know the root of the clausal sub-tree. Several 
experiments were done to identify the most optim-
al set of clausal features available from the partial 
parse. The best results are obtained when the 
clause boundary information, along with the head 
information i.e. head node of a clause, is given as a 
feature to each node. 
We trained the Stage1 parser by converting the 
treebank data into the stage1 format, following the 
steps that were given in Husain et al (2009). This 
conversion depends on the definition of the clause. 
We experimented with different definitions of 
clause in order to tune the tool to give the optimal 
clause boundary and head information required for 
parsing. For the results reported in this paper, a 
clause is a sequence of words, with a single verb, 
unless the verb is a child of another verb. 
 
 
                                                        
3 Vibhakti is a generic term for preposition, post-position and 
suffix. 
4TAM: Tense, Aspect and Modality. 
5Stage1 handles intra-clausal dependency relations. These 
relations generally correspond to the argument structure of the 
verb, noun-noun genitive relation, infinitive-noun relation, 
adjective-noun, adverb-verb relations, etc. 
6 Malt version 1.2 
658
 Precision Recall 
Clause Boundary 84.83% 91.23% 
Head Information 92.42% 99.40% 
Table 1. Accuracies of the features being used 
 
Table 1 gives the accuracy of the clausal informa-
tion being used as features in parsing. It is clear 
from Table1 that the tool being used doesn?t have 
very high clause boundary identification perfor-
mance; nevertheless, the performance is sufficient 
enough to make an improvement in parsing expe-
riments. On the other hand, the head of the clause 
(or, the root head in the clausal sub-tree) is identi-
fied efficiently. All the above experiments for pa-
rameter tuning were done on the development data 
of the ICON 2009 parsing contest. 
3.3 Parser  
We used MSTParser7 for the actual parsing step. 
MST uses Chu-Liu-Edmonds Maximum Spanning 
Tree Algorithm for non-projective parsing and 
Eisner's algorithm for projective parsing (Eisner, 
1996). It uses online large margin learning as the 
learning algorithm (McDonald et al, 2005b). 
We modified MST so that it uses the clause 
boundary. Unlike the normal features that MST 
uses, the clause boundary features span across 
many words. 
. 
4 Experiments and Results 
We experimented with different combinations of 
the information provided in the data (as mentioned 
in 3.1). Vibhakti and TAM fields gave better re-
sults than others. This is consistent with the best 
previous settings for Hindi parsing (Bharati et al, 
2008a, Ambati et al, 2009). We used the results 
obtained using this setting as our baseline (F1). 
We first experimented by giving only the clause 
inclusion (boundary) information to each node 
(F2). This feature should help the parser reduce its 
search space during parsing decisions. Then, we 
provided only the head and non-head information 
(whether that node is the head of the clause or not) 
(F3). The head or non-head information helps in 
handling complex sentences that have more than 
                                                        
7 MST version 0.4b 
one clause and each verb in the sentence has its 
own argument structure. We achieved the best per-
formance by using both as features (F4) during the 
parsing process. 
 
 LA (%) UA (%) L (%) 
F1 73.62 91.00 76.04 
F2 72.66 91.00 74.74 
F3 73.88 91.35 75.78 
F4 74.39 91.87 76.21 
Table 2. Parsing accuracies with different features 
 
Table 2 gives the results for all the settings. It is 
interesting to note that the boundary information 
(F1) alone does not cross the baseline; however 
this feature is reliable enough to give the best per-
formance when combined with F3. 
5 Observations  
We see from the above results (F4 in Table 2) that 
there is a rise of 0.87% in UA (unlabeled 
attachment) and 0.77% in LA (labeled attachment) 
over previous best (F1).  This shows the positive 
effect of using the clausal information during the 
parsing process. 
We analyzed the performance of both the pars-
ers in handling the long distance dependencies and 
non-projective dependencies. We found that the 
non-projective arcs handled by F4 have a precision 
and recall of 41.1% and 50% respectively for UA, 
compared to 30.5% and 39.2% for the same arcs 
during F1. 
 
 
Figure 1. Distance stats 
 
Figure 1 compares the accuracies of the depen-
dencies at various distances. It is clear that the ef-
fect of clausal information become more 
659
pronounced as the distance increases. This means 
F4 does help the parser in effectively handling long 
distance dependencies as well. 
6 Conclusion and Future Work  
The results show that there is a significant 
improvement in the parsing accuracy when the 
clausal information is being used.  
The clausal information is presently being used 
only as attachment features in MST. Experiments 
can be done in future, to find out if there is a label 
bias to the clause boundary, which also helps in 
reducing the search space for specific labels. Im-
proving the feature set for the labeled parse also 
improves the unlabeled attachment accuracy, as 
MST does attachments and labels in a single step, 
and the labels of processed nodes will also be tak-
en in features. 
We can see from Table1 that the precision of the 
clause boundary is 84.83%. Using a tool, targeted 
at getting just the clausal information, instead of 
using a parser can improve the accuracy of the 
clausal information, which helps improving pars-
ing. 
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009. Experi-
ments in Indian Language Dependency Parsing. In 
Proceedings of the ICON09 NLP Tools Contest: In-
dian Language Dependency Parsing, pp 32-37.  
B. R. Ambati, P. Gade and C. GSK. 2009. Effect ofMi-
nimal Semantics on Dependency Parsing. In the Pro-
ceedings of RANLP 2009 Student Research 
Workshop. 
G. Attardi and F. Dell?Orletta. Chunking and Depen-
dency Parsing. LREC Workshop on Partial Parsing: 
Between Chunking and Deep Parsing. Marrakech, 
Morocco. 2008. 
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. In Proceedings of IJCNLP-2008. 
A. Bharati and R. Sangal. 1993. Parsing Free Word Or-
der Languages in the Paninian Framework. Proceed-
ings of ACL:93.  
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma 
and R. Sangal. 2008a. Two Semantic features make 
all the difference in Parsing accuracy. In Proceed-
ings. of International Conference on Natural Lan-
guage Processing-2008. 
A. Bharati, S. Husain, D. Sharma, and R. Sangal. 2008b. 
A two stage constraint based dependency parser for 
free word order languages. In Proceedings. of 
COLIPS International Conference on Asian Lan-
guage Processing. Thailand. 2008. 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
Two stage constraint based hybrid approach to free 
word order language dependency parsing. In the Pro-
ceedings of the 11th International Conference on 
Parsing Technologies (IWPT09). Paris. 2009. 
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson,M. Saers.2007. Single Malt or Blended? A 
Study in Multilingual Parser Optimization. 
In Proceedings of the CoNLL Shared Task Session of 
EMNLP-CoNLL 2007. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proceedings of ICON09 NLP Tools Con-
test:Indian Language Dependency Parsing. 
Hyderabad, India. 2009. 
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and Ra-
jeev Sangal. 2009. A modular cascaded approach to 
complete parsing. In the Proceedings of COLIPS In-
ternational Conference on Asian Language 
Processing. Singapore. 2009. 
P. Mannem and H. Chaudhry.2009. Insights into Non-
projectivity in Hindi. In ACL-IJCNLP Student paper 
workshop. 2009. 
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 
2005a. Non-projective dependency parsing using 
spanning tree algorithms. In the Proceedings of 
HLT/EMNLP, pp. 523?530. 
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In 
the Proceedings of ACL 2005. pp. 91?98. 
I. A. Mel'Cuk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proceedings 
of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering, 
13(2), 95-135. 
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. In Linguistics and Phi-
losophy, p. 8, 334?343. 
660
