Proceedings of the 12th European Workshop on Natural Language Generation, pages 74?81,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Learning Lexical Alignment Policies for Generating
Referring Expressions in Spoken Dialogue Systems
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
s.janarthanam@ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
olemon@inf.ed.ac.uk
Abstract
We address the problem that different
users have different lexical knowledge
about problem domains, so that automated
dialogue systems need to adapt their gen-
eration choices online to the users? domain
knowledge as it encounters them. We ap-
proach this problem using policy learning
in Markov Decision Processes (MDP). In
contrast to related work we propose a new
statistical user model which incorporates
the lexical knowledge of different users.
We evaluate this user model by showing
that it allows us to learn dialogue poli-
cies that automatically adapt their choice
of referring expressions online to differ-
ent users, and that these policies are sig-
nificantly better than adaptive hand-coded
policies for this problem. The learned
policies are consistently between 2 and
8 turns shorter than a range of different
hand-coded but adaptive baseline lexical
alignment policies.
1 Introduction
In current ?troubleshooting? spoken dialogue sys-
tems (SDS)(Williams, 2007), the major part of the
conversation is directed by the system, while the
user follows the system?s instructions. Once the
system decides what instruction to give the user
(at the dialogue management level), it faces sev-
eral decisions to be made at the natural language
generation (NLG) level. These include, deciding
which concepts to include in the utterance, decid-
ing the referring expressions (RE) to use in the ut-
terance and so on. A little-studied problem is to
what extent a system could automatically align to
the user?s lexical knowledge by adapting its RE
choices, in particular based on his domain exper-
tise, and how this can be modelled and optimised
computationally.
(Issacs and Clark, 1987) show how two in-
terlocutors adapt their language in a conversa-
tion by assessing each other?s domain expertise
during dialogue, by observing how they react to
each other?s RE choices. This is called align-
ment through Audience Design (Clark, 1996; Bell,
1984). Using inappropriate REs in instructions has
been identified as a serious problem affecting a
system?s usability (Molich and Nielsen, 1990).
In this paper, we treat NLG within a com-
putational learning paradigm (Lemon, 2008;
Rieser and Lemon, 2009; Janarthanam and
Lemon, 2008). We examine whether a SDS can
automatically learn a lexical alignment policy
for audience design, which enables it to choose
appropriate REs by predicting the user?s lexical
knowledge dynamically during the course of the
dialogue. This can avoid clarification requests
from the users and keep the dialogues short. The
example given below describes the kind of lexical
alignment behaviour that we want the system to
learn. The system chooses ?small white box?
instead of ?ADSL filter? and ?monitor symbol?
instead of ?network icon?, because it learnt that
the user is a novice based on their clarification
requests. However, it switches to using technical
terms like ?browser?, when it learns that the
user is not a complete novice (as he verifies the
description for the network icon in Usr 4).
Example 1.
Sys 1>> Is your router connected
to the computer?
Usr 1>> Uh. What?s a router?
Sys 2>> It?s the big black box.
Usr 2>> Ok.. yes.
Sys 3>> Do you see a small white
box connected to the router?
Usr 3>> Yes.
Sys 4>> Ok. Is there a flashing
monitor symbol at the bottom
74
right of the screen?
Usr 4>> the network icon?
Sys 5>> Yes. Is it flashing?
Usr 5>> Yes. It is flashing.
Sys 6>> Ok. Please open your
browser.
We study lexical alignment in the context of
troubleshooting dialogues ? where users wish to
mend their broken internet connection. This task
demands a new type of User Simulation.
In this paper, after a review of related work
(section 2), we first present (section 3) a statisti-
cal User Simulation which supports different do-
main knowledge profiles and reacts accordingly to
questions or instructions from an SDS. We then
present a reinforcement learning model of lexical
alignment due to audience design (in sections 4 &
5). We then evaluate the User Simulation (section
6), testing whether a simulation that is sensitive to
a system?s RE choices can be used to learn good
lexical alignment policies. Finally, we compare
policies learned in interaction with the User Sim-
ulation with hand-coded policies, and present the
results in section 7.
2 Related work
Several statistical user simulation models that
model a user?s behaviour in a conversation have
been proposed (Georgila et al, 2005; Schatzmann
et al, 2006; Schatzmann et al, 2007). These mod-
els issue task specific dialogue acts like inform-
ing their search constraints, confirming values, re-
jecting misrecognised values, etc. However, they
do not model a user population with varying do-
main expertise. Also, none of these models seek
clarification at conceptual or lexical levels that oc-
cur naturally in conversations between real users.
(Komatani et al, 2003) proposed using user mod-
els with features like skills, domain knowledge
and hastiness as a part of the dialogue manager
to produce adaptive responses. (Janarthanam and
Lemon, 2008) presented a user simulation model
that simulates a variety of users with different do-
main knowledge profiles. Although this model
incorporated clarification acts at the conceptual
level, these users ignore the issues concerning the
user?s understanding of the REs used by the sys-
tem. In this work, in contrast to the above, we
present a User Simulation model which explicitly
encodes the user?s lexical knowledge of the do-
main, understands descriptive expressions, and is-
sues clarification requests at the lexical level.
3 User Simulation
Our User Simulation module simulates dialogue
behaviour of different users, and interacts with the
dialogue system by exchanging both dialogue acts
and REs. It produces users with different knowl-
edge profiles. The user population produced by
the simulation comprises a spectrum from com-
plete novices to experts in the domain. Simulated
users behave differently from one another because
of differences in their knowledge profiles. Simu-
lated users are also able to learn new REs during
interaction with the SDS. These new expressions
are held in the user simulation?s short term mem-
ory for later use in the conversation. Simulated
users interact with the environment using an in-
teractive mechanism that allows them to observe
and manipulate the states of various domain ob-
jects. The interaction between the user and the
other components is given in figure 1 (notations
explained in later sections).
Figure 1: Experimental setup
3.1 Domain knowledge model
Domain experts know most of the technical terms
that are used to refer to domain objects whereas
novice users can only reliably identify them when
descriptive expressions are used. While in the
model of (Janarthanam and Lemon, 2008) knowl-
edge profiles were presented only at conceptual
levels (e.g. does the user know what a modem is?),
we present them in a more granular fashion. In
this model, the user?s domain knowledge profile
is factored into lexical (LKu,t), factual (FKu,t)
and procedural knowledge (PKu,t) components.
75
Lexical knowledge LKu,t
vocab([modem, router], dobj1)
vocab([wireless, WiFi], dobj3)
vocab([modem power light], dobj7)
Factual knowledge FKu,t
location(dobj1)
location(dobj7)
Procedural knowledge PKu,t
procedure(replace filter)
procedure(refresh page)
Table 1: Knowledge profile - Intermediate user.
A user?s lexical knowledge is encoded in the for-
mat:
vocab(referring expressions, domain object)
where referring expressions can be a list of ex-
pressions that the user knows can be used to talk
about each domain object.
Whether the user knows facts like the location
of the domain objects (location(domain object)) is
encoded in the factual component. Similarly, the
procedural component encodes the user?s knowl-
edge of how to find or manipulate domain objects
(procedure(domain action)). Table 1 shows an ex-
ample user knowledge profile.
In order to create a knowledge spectrum, a
Bayesian knowledge model is used. The current
model incorporates patterns of only the lexical
knowledge among the users. For instance, peo-
ple who know the word ?router? most likely also
know ?DSL light? and ?modem? and so on. These
dependencies between REs are encoded as condi-
tional probabilities in the Bayesian model. Figure
2 shows the dependencies between knowledge of
REs.
Figure 2: Bayes Net for User Lexical Knowledge
Using this Bayesian model, we instantiate dif-
ferent knowledge profiles for different users. The
current conditional probabilities were set by hand
based on intuition. In future work, these values
will be populated based on simple knowledge sur-
veys performed on real users (Janarthanam and
Lemon, 2009). This method creates a spectrum of
users from ones who have no knowledge of tech-
nical terms to ones who know all the technical
jargon, though every profile will have a different
frequency of occurrence. This difference in fre-
quency reflects that expert users are less common
than novice users.
The user?s domain knowledge can be dynami-
cally updated. The new REs, both technical and
descriptive, presented by the system through clar-
ification moves are stored in the user?s short term
memory. Exactly how long (in terms of dialogue
turns) to retain the newly acquired knowledge is
given by a retention index RIu. At the end of RIu
turns, the lexical item is removed from user?s short
term memory.
3.2 User Dialogue Action set
Apart from environment-directed acts, simulated
users issue a number of dialogue acts. The list of
dialogue actions that the user can perform in this
model is given in Table 2. It consists of default
moves like provide info and acknowledge as well
as some clarification moves. Request description
is issued when the SDS uses technical terms that
the simulated user does not know, e.g. ?What is
a router??. Request verification is issued when
the SDS uses descriptive lexical items for do-
main objects that the user knows more techni-
cal terms for, e.g. System: ?Is the black box
plugged in?? User: ?Do you mean the router??.
Request disambiguation is issued when the user
faces an underspecified and ambiguous descrip-
tive expression, e.g.?User: I have two black boxes
here - one with lights and one without. Which
one is it??. These clarification strategies have
been modeled based on (Schlangen, 2004). The
user simulation also issues request location and
request procedure dialogue acts, when it does not
know the location of domain objects or how to ma-
nipulate them, respectively.
3.3 Environment simulation
The environment simulation includes both physi-
cal objects, such as the computer, modem, ADSL
filter, etc and virtual objects, such as the browser,
control panel, etc in the user?s environment. Phys-
ical and virtual connections between these objects
76
report problem
provide info(dobj, info)
acknowledge
request verification(x, y)
request description(x)
request disambiguation(x, [y1,y2])
request location(dobj)
request procedure(daction)
thank system
Table 2: User Dialogue Acts.
are also simulated. At the start of every dialogue,
the environment is initiated to a faulty condition.
Following a system instruction or question, the
user issues two kinds of environment acts. It is-
sues an observation act Ou,t to observe the status
of a domain object and a manipulation act Mu,t
to change the state of the environment (Se,t). The
simulation also includes task irrelevant objects in
order to confuse the users with underspecified de-
scriptive expressions. For instance, we simulate
two domain objects that are black in colour - an
external hard disk and a router. So, the users may
get confused when the system uses the expression,
?black box?.
3.4 User Action Selection
User Action selection has several steps. The user?s
dialogue behaviour is described in the action se-
lection algorithm (Table 3). Firstly, the user must
identify all the RE choices (RECs,t) that are used
to refer to different domain objects (dobj) and
domain actions (daction) in the system instruc-
tion (step 1). Secondly, the user?s knowledge of
the prerequisite factual (FKprereq) and procedural
(PKprereq) knowledge components connected to
the observation or manipulation action is checked.
If the user does not satisfy the knowledge re-
quirements, the user simulation issues an appro-
priate clarification request (steps 2 & 3). After
the knowledge requirements are satisfied, the user
issues environment directed actions and responds
to system instruction As,t (steps 4 & 5). When
the system provides the user specific information,
they are added to the user?s short term memory
(steps 6-8). Although, the action selection process
is deterministic at this level, it is dependent on
the users? diverse knowledge profiles, which en-
sures stochastic dialogue behaviour amongst dif-
ferent users created by the module.
greet the user
request status(x)
request action(x)
give description(x)
accept verification(x,y)
give location(dobj)
give procedure(daction)
close dialogue
Table 4: System Dialogue acts.
4 Dialogue System Model
The dialogue system is modeled as a reinforce-
ment learning agent in a Markov Decision Pro-
cess framework (Levin et al, 1997). At every
turn, it interacts with the Simulated User by issu-
ing a System Dialogue Act (As,t) along with a set
of REs, called the System RE Choices (RECs,t).
RECs,t contains the REs that refer to various do-
main objects in the dialogue act As,t. First, the
system decides the dialogue act to issue using a
hand-coded dialogue strategy. Troubleshooting in-
structions are coded in the troubleshooting deci-
sion tree1. Dialogue repair moves include select-
ing clarification moves in response to user?s re-
quest. The list of system dialogue acts is given
Table 4.
The system issues various repair moves when
the users are unable to carry out the system?s in-
structions due to ignorance, non-understanding or
the ambiguous nature of the instructions. The
give description act is used to give the user a de-
scription of the domain object previously referred
to using a technical term. It is also used when
the user requests disambiguation. Similarly, ac-
cept verification is given when the user wants to
verify whether the system is referring to a certain
domain object y using the expression x.
After selecting the dialogue act As,t, a set
of REs must be chosen to refer to each of
the domain objects/actions used in the dia-
logue act. For instance, the dialogue act re-
quest status(router dsl light) requires references
to be made to domain objects ?router? and ?DSL
light?. For each of these references, the system
chooses a RE, creating the System RE Choice
RECs,t. In this study, we have 7 domain objects
and they can either be referred to using technical
1The Troubleshooting decision tree was hand-built using
guidelines from www.orange.co.uk and is similar to the one
used by their Customer Support personnel
77
Input: System Dialogue Act As,t, System Referring Expressions Choice RECs,t
and User State Su,t: LKu,t, FKu,t, PKu,t
Step 1. ? x ? RECs,t
Step 1a. if (vocab(x, dobj)? LKu,t) then next x.
Step 1b. else if (description(x, dobj) & ? j ((is jargon(j) & vocab(j, dobj) /? LKu,t))) then next x.
Step 1c. else if (is jargon(x) & (vocab(x, dobj) /? LKu,t)) then return request description(x).
Step 1d. else if (is ambiguous(x)) then return request disambiguation(x).
Step 1e. else if (description(x, dobj) & ? j ((is jargon(j) & vocab(j, dobj) ? LKu,t)))
then return request verification(x, j).
Step 2. if (?dobj location(dobj) ? FKprereq & location(dobj) /? FKu,t)
then return request location(dobj).
Step 3. else if (?daction procedure(daction) ? PKprereq & procedure(daction) /? PKu,t)
then return request procedure(daction).
Step 4. else if (As,t = request status(dobj))
then observe env(dobj, status), return provide info(dobj, status)
Step 5. else if (As,t = request action(daction))
then manipulate env(daction), return acknowledge.
Step 6. else if (As,t = give description(j, d) & description(d, dobj))
then add to short term memory(vocab(j, dobj)), return acknowledge.
Step 7. else if (As,t = give location(dobj))
then add to short term memory(location(dobj)), return acknowledge.
Step 8. else if (As,t = give procedure(daction))
then add to short term memory(procedure(daction)), return acknowledge.
Table 3: Algorithm: Simulated User Action Selection
terms or descriptive expressions. For instance, the
DSL light on the router can be descriptively re-
ferred to as the ?second light on the panel? or us-
ing the technical term, ?DSL light?. Sometimes
the system has to choose between a lesser known
technical term and a well-known one. Some de-
scriptive expressions may be underspecified and
therefore can be ambiguous to the user (for ex-
ample, ?the black box?). Choosing inappropri-
ate expressions can make the conversation longer
with lots of clarification and repair episodes. This
can lead to long frustrating dialogues, affecting the
task success rate. Therefore, the dialogue system
must learn to use appropriate REs in its utterances.
The RE choices available to the system are given
in Table 5.
The system?s RE choices are based on a part
of the dialogue state that records which of the
technical terms the user knows. These variables
are initially set to unknown (u). During the di-
alogue, they are updated to user knows (y) or
user doesnot know (n) states. We therefore record
the user?s lexical knowledge during the course of
the dialogue and let the system learn the statistical
usage patterns by itself. Part of the dialogue state
1. router / black box / black box with lights
2. power light / first light on the panel
3. DSL light / second light on the panel
4. online light / third light on the panel
5. network icon / flashing computer symbol
6. network connections / earth with plug
7. WiFi / wireless
Table 5: System RE choices.
relevant to system?s RE choices is given in Table 6.
The state can be extended to include other rele-
vant information like the usage of various REs by
the user as well to enable alignment with the user
through priming (Pickering and Garrod, 2004) and
personal experience (Clark, 1996). However they
are not yet implemented in the present work.
5 Reward function
The reward function calculates the reward
awarded to the reinforcement learning agent at
the end of each dialogue session. Successful
task completion is rewarded with 1000 points.
Dialogues running beyond 50 turns are deemed
78
Feature Values
user knows router y/n/u
user knows power light y/n/u
user knows dsl light y/n/u
user knows online light y/n/u
user knows network icon y/n/u
user knows network connections y/n/u
user knows wifi y/n/u
Table 6: (Part of) Dialogue state for Lexical Align-
ment.
unsuccessful and are awarded 0 points. The
number of turns in each dialogue varies according
to the system?s RE choices and the simulated
user?s response moves. Each turn costs 10 points.
The final reward is calculated as follows:
TaskCompletionReward(TCR) = 1000
TurnCost(TC) = 10
TotalTurnCost(TTC) = #(Turns) ? TC
FinalReward = TCR? TTC
The reward function therefore gives high re-
wards when the system produces shorter dia-
logues, which is possible by adaptively using ap-
propriate REs for each user.
6 Training
The system was trained to produce an adaptive
lexical alignment policy, which can adapt to users
with different lexical knowledge profiles. Ideally,
the system must interact with a number of dif-
ferent users in order to learn to align with them.
However, with a large number of distinct Bayesian
user profiles (there are 90 possible user profiles),
the time taken for learning to converge is exorbi-
tantly high. Hence the system was trained with
selected profiles from the distribution. It was
initially trained using two user profiles from the
very extremes of the knowledge spectrum pro-
duced by the Bayesian model - complete experts
and complete novices. In this study, we cali-
brated all users to know all the factual and proce-
dural knowledge components, because the learn-
ing exercise was targeted only at the lexical level.
With respect to the lexical knowledge, complete
experts knew all the technical terms in the do-
main. Complete novices, on the other hand, knew
only one: power light. We set the RIu to 10,
so that the users do not forget newly learned lexi-
cal items for 10 subsequent turns. Ideally, we ex-
pected the system to learn to use technical terms
with experts and to use descriptive expressions
with novices and a mixture for intermediates. The
system was trained using SARSA reinforcement
learning algorithm (Sutton and Barto, 1998), with
linear function approximation, for 50000 cycles.
It produced around 1500 dialogues and produced
an alignment policy (RL1) that adapted to users
after the first turn which provides evidence about
the kind of user the system is dealing with.
The system learns to get high reward by pro-
ducing shorter dialogues. By learning to choose
REs by adapting to the lexical knowledge of the
user, it avoids unnecessary clarification and repair
episodes. It learns to choose descriptive expres-
sions for novice users and jargon for expert users.
It also learns to use technical terms when all users
know them (for instance, ?power light?). Due to
the user?s high retention (10 turns), the system
learned to use newly learned items later in the di-
alogue.
We also trained another alignment policy (RL2)
with two other intermediate high frequency user
lexical profiles. These profiles (Int1 and Int2)
were chosen from either ends of the knowledge
spectrum close to the extremes. Int1 is a knowl-
edge profile that is close to the novice end. It
only knows two technical terms: ?power light?
and ?WiFi?. On the other hand, Int2 is profile
that is close to the expert end and knows all tech-
nical terms except: ?dsl light? and ?online light?
(which are the least well-known technical terms
in the user population). With respect to the other
knowledge components - factual and procedural,
both users know every component equally. We
trained the system for 50000 cycles following the
same procedure as above. This produced an align-
ment policy (RL2) that learned to optimize the
moves, similar to RL1, but with respect to the
given distinct intermediate users.
Figure 3 shows the overall dialogue reward for
the 2 policies during training.
Both policies RL1 and RL2, apart from learn-
ing to adapt to the users, also learned not to use
ambiguous expressions. Ambiguous expressions
lead to confusion and the system has to spend ex-
tra turns for clarification. Therefore both policies
learnt to avoid using ambiguous expressions.
Figure 4 shows the dialogue length variation for
the 2 policies during training.
79
7 Evaluation and baselines
We evaluated both the learned policies using a test-
ing simulation and compared the results to other
baseline hand-coded policies. Unlike the train-
ing simulation, the testing simulation used the
Bayesian knowledge model to produce all differ-
ent kinds of user knowledge profiles. It produced
around 90 different profiles in varying distribution,
resembling a realistic user population. The tests
were run over 250 simulated dialogues each.
Several rule-based baseline policies were man-
ually created for the sake of comparison:
1. Random - Choose REs at random.
2. Descriptive only - Only choose descriptive
expressions. If there is more than one de-
scriptive expression it picks one randomly.
3. Jargon only - Chooses the technical terms.
4. Adaptive 1 - It starts with a descriptive ex-
pression. If the user asks for verification, it
Figure 3: Final reward for RL1 & RL2.
Figure 4: Dialogue length for RL1 & RL2.
Policy Avg. Reward Avg. Length
RL2 830.4 16.98
RL1 812.3 18.77
Adaptive 1 809.6 19.04
Adaptive 2 792.1 20.79
Adaptive 3 780.2 21.98
Random 749.8 25.02
Desc only 796.6 20.34
Jargon only 762.0 23.8
Table 7: Rewards and Dialogue Length.
switches to technical terms for the rest of the
dialogue.
5. Adaptive 2 - It starts with a technical term
and switches to descriptive expressions if the
user does not understand in the first turn.
6. Adaptive 3 - This rule-based policy adapts
continuously based on the previous expres-
sion. For instance, if the user did not un-
derstand the technical reference to the current
object, it uses a descriptive expression for the
next object in the dialogue.
The first three policies (random, descriptive
only and jargon only) are equivalent to policies
learned using user simulations that are not sensi-
tive to system?s RE choices. In such cases, the
learned policies will not have a well-defined strat-
egy to choose REs based on user?s lexical knowl-
edge. Table 7 shows the comparative results for
the different policies. RL (1 & 2) are significantly
better than all the hand-coded policies. Also, RL2
is significantly better than RL1 (p < 0.05).
Ideally the system with complete knowledge of
the user would be able to finish the dialogue in
13 turns. Similarly, if it got it wrong every time
it would take 28 turns. From table 7 we see that
RL2 performs better than other policies, with an
average dialogue length of around 17 turns. The
learned policies were able to discover the hid-
den dependencies between lexical items that were
encoded in the Bayesian knowledge model. Al-
though trained only on two knowledge profiles, the
learned policies adapt well to unseen users, due to
the generalisation properties of the linear function
approximation method. Many unseen states arise
when interacting with users with new profiles and
both the learned policies generalise very well in
such situations, whereas the baseline policies do
not.
80
8 Conclusion
In this paper, we have shown that by using a sta-
tistical User Simulation that is sensitive to RE
choices we are able to learn NLG policies that
adaptively decide which REs to use based on audi-
ence design. We have shown that the lexical align-
ment policies learned with this type of simulation
are better than a range of hand-coded policies.
Although lexical alignment policies could be
hand-coded, the designers would need to invest
significant resources every time the list of referring
expressions is revised or the conditions of the dia-
logue change. Using reinforcement learning, near-
optimal lexical alignment policies can be learned
quickly and automatically. This model can be used
in any task where interactions need to be tailored
to different users? lexical knowledge of the do-
main.
8.1 Future work
Lexical alignment in dialogue also happens due
to priming (Pickering and Garrod, 2004) and per-
sonal experience (Clark, 1996). We will examine
trade-offs in various conditions, like ?instruct? ver-
sus ?teach? and low versus high retention users.
Using Wizard-of-Oz studies and knowledge sur-
veys, we plan to make the model more data-driven
and realistic (Janarthanam and Lemon, 2009). We
will also evaluate the learned policies with real
users.
Acknowledgements
The research leading to these results has re-
ceived funding from the European Community?s
Seventh Framework (FP7) under grant agree-
ment no. 216594 (CLASSiC Project www.classic-
project.org), EPSRC project no. EP/E019501/1,
and the British Council (UKIERI PhD Scholar-
ships 2007-08).
References
A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145?204.
H. H. Clark. 1996. Using Language. Cambridge Uni-
versity Press, Cambridge.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proceedings of Eu-
rospeech/Interspeech.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26?37.
S. Janarthanam and O. Lemon. 2008. User simulations
for online adaptation and knowledge-alignment in
Troubleshooting dialogue systems. In Proc. SEM-
dial?08.
S. Janarthanam and O. Lemon. 2009. A Wizard-of-Oz
environment to study Referring Expression Genera-
tion in a Situated Spoken Dialogue Task. In Proc.
ENLG?09.
K. Komatani, S. Ueno, T. Kawahara, and H. G. Okuno.
2003. Flexible Guidance Generation using User
Model in Spoken Dialogue Systems. In Proc.
ACL?03.
O. Lemon. 2008. Adaptive Natural Language Genera-
tion in Dialogue using Reinforcement Learning. In
Proc. SEMdial?08.
E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proceedings of ASRU97.
R. Molich and J. Nielsen. 1990. Improving a Human-
Computer Dialogue. Communications of the ACM,
33-3:338?348.
M. J. Pickering and S. Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and
Brain Sciences, 27:169?225.
V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL?09.
J. Schatzmann, K. WeilHammer, M. N. Stuttle, and
S. J. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proceedings of HLT/NAACL 2007.
D. Schlangen. 2004. Causes and strategies for request-
ing clarification in dialogue. Proceedings of the 5th
SIGdial Workshop on Discourse and Dialogue (SIG-
DIAL 04), Boston.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
J. Williams. 2007. Applying POMDPs to Dialog
Systems in the Troubleshooting Domain. In Proc
HLT/NAACL Workshop on Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technol-
ogy.
81
Proceedings of the 12th European Workshop on Natural Language Generation, pages 94?97,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Wizard-of-Oz environment to study Referring Expression Generation
in a Situated Spoken Dialogue Task
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
s.janarthanam@ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
olemon@inf.ed.ac.uk
Abstract
We present a Wizard-of-Oz environment
for data collection on Referring Expres-
sion Generation (REG) in a real situated
spoken dialogue task. The collected data
will be used to build user simulation mod-
els for reinforcement learning of referring
expression generation strategies.
1 Introduction
In this paper, we present a Wizard-of-Oz (WoZ)
environment for data collection in a real situated
spoken dialogue task for referring expression gen-
eration (REG). Our primary objective is to study
how participants (hereafter called users) with dif-
ferent domain knowledge and expertise interpret
and resolve different types of referring expressions
(RE) in a situated dialogue context. We also study
the effect of the system?s lexical alignment due
to priming (Pickering and Garrod, 2004) by the
user?s choice of REs. The users follow instruc-
tions from an implemented dialogue manager and
realiser to perform a technical but realistic task ?
setting up a home Internet connection. The dia-
logue system?s utterances are manipulated to con-
tain different types of REs - descriptive, technical,
tutorial or lexically aligned REs, to refer to various
domain objects in the task. The users? responses
to different REs are then logged and studied.
(Janarthanam and Lemon, 2009) presented a
framework for reinforcement learning of optimal
natural language generation strategies to choose
appropriate REs to users with different domain
knowledge expertise. For this, we need user sim-
ulations with different domain knowledge profiles
that are sensitive to the system?s choice of REs. A
WoZ environment is an ideal tool for data collec-
tion to build data-driven user simulations. How-
ever, our study requires a novel WoZ environment.
In section 2, we present prior related work. Sec-
tion 3 describes the task performed by partici-
pants. In section 4, we describe the WoZ envi-
ronment in detail. Section 5 describes the data
collected in this experiment and section 6 presents
some preliminary results from pilot studies.
2 Related Work
(Whittaker et al, 2002) present a WoZ environ-
ment to collect data concerning dialogue strate-
gies for presenting restaurant information to users.
This study collects data on strategies used by users
and human expert wizards to obtain and present in-
formation respectively. (van Deemter et al, 2006)
present methods to collect data (the TUNA cor-
pus) for REG using artificially constructed pic-
tures of furniture and photographs of real people.
(Arts, 2004) presents a study choosing between
technical and descriptive expressions for instruc-
tion writing.
In contrast to the above studies, our study is
novel in that it collects data from users having dif-
ferent levels of expertise in a real situated task do-
main, and for spontaneous spoken dialogue. Our
focus is on choosing between technical, descrip-
tive, tutorial, and lexically aligned expressions
rather than selecting different attributes for gen-
erating descriptions.
3 The Domain Task
In this experiment, the task for each user is to lis-
ten to and follow the instructions from the WoZ
system and set up their home broadband Internet
connection. We provide the users with a home-
like environment with a desktop computer, phone
socket and a Livebox package from Orange con-
taining cables and components such as the mo-
dem, broadband filters and a power adaptor. Dur-
ing the experiment, they set up the Internet con-
nection by connecting these components to each
other. Prior to the task, the users are informed that
they are interacting with a spoken dialogue system
94
that will give them instructions to set up the con-
nection. However, their utterances are intercepted
by a human wizard. The users are requested to
have a conversation as if they were talking to a hu-
man operator, asking for clarifications if they are
confused or fail to understand the system?s utter-
ances. The system?s utterances are converted au-
tomatically to speech using the Cereproc Speech
Synthesiser and played back to the user. The user
follows the instructions and assembles the compo-
nents. The setup is examined by the wizard at the
end of the experiment to measure the percentage of
task success. The user also fills in questionnaires
prior to and after the task answering questions on
his background, quality of the system during the
task and the knowledge gained during the task.
4 The Wizard-of-Oz environment
The Wizard-of-Oz environment facilitates the en-
tire experiment as described in the section above.
The environment consists of the Wizard Interac-
tion Tool, the dialogue system and the wizard. The
users wear a headset with a microphone. Their ut-
terances are relayed to the wizard who then anno-
tates it using the Wizard Interaction Tool (shown
in figure 1) and sends it to the dialogue system.
The system responds with a natural language ut-
terance which is automatically converted to speech
and is played back to the user and the wizard.
4.1 Wizard Interaction Tool (WIT)
The Wizard Interaction Tool (WIT) (shown in fig-
ure 1) allows the wizard to interact with the dia-
logue system and the user. The GUI is divided in
to several panels.
a. System Response Panel - This panel displays
the dialogue system?s utterances and RE choices
for the domain objects in the utterance. It also dis-
plays the strategy adopted by the system currently
and a visual indicator of whether the system?s ut-
terance is being played to the user.
b. Confirmation Request Panel - This panel lets
the wizard handle issues in communication (for
e.g. noise). The wizard can ask the user to repeat,
speak louder, confirm his responses, etc using ap-
propriate pre-recorded messages or build his own
custom messages.
c. Confirmation Panel - This panel lets the wiz-
ard handle confirmation questions from the user.
The wizard can choose ?yes? or ?no? or build a cus-
tom message.
yes ?Yes it is on?
no ?No, its not flashing?
ok ?Ok. I did that?
req description ?Whats an ethernet cable??
req location ?Where is the filter??
req verify jargon ?Is it the ethernet cable??
req verify desc ?Is it the white cable??
req repeat ?Please repeat?
req rephrase ?What do you mean??
req wait ?Give me a minute??
Table 1: User Dialogue Acts.
d. Annotation panel - This panel lets the wizard
annotate the content of participant?s utterances.
User responses (dialogue acts and example utter-
ances) that can be annotated using this panel are
given in Table 1. In addition to these, other be-
haviours, like remaining silent or saying irrelevant
things are also accommodated.
e. User?s RE Choice panel - The user?s choice
of REs to refer to the domain objects are annotated
by the wizard using this panel.
4.2 The Instructional Dialogue Manager
The dialogue manager drives the conversation by
giving instructions to the users. It follows a deter-
ministic dialogue management policy so that we
only study variation in the decisions concerning
the choice of REs. It should be noted that typi-
cal WoZ environments (Whittaker et al, 2002) do
not have dialogue managers and the strategic de-
cisions will be taken by the wizard. Our dialogue
system has three main responsibilities - choosing
the RE strategy, giving instructions and handling
clarification requests.
The dialogue system, initially randomly
chooses the RE strategy at the start of the
dialogue. The list of strategies are as follows.
1. Jargon: Choose technical terms for every ref-
erence to the domain objects.
2. Descriptive: Choose descriptive terms for ev-
ery reference to the domain objects.
3. Tutorial: Use technical terms, but also aug-
ment the description for every reference.
The above three strategies are also augmented
with an alignment feature, so that the system can
either align or not align with the user?s prior choice
of REs. In aligned strategies, the system abandons
the existing strategy (jargon, descriptive or tuto-
rial) for a domain object reference when the user
95
Figure 1: Wizard Interaction Tool
uses a different expression from that of the system
to refer to the domain object. For instance, under
the descriptive strategy, the ethernet cable is re-
ferred to as ?the thick cable with red ends?. But
if the user refers to it as ?ethernet cable?, then the
system uses ?ethernet cable? in subsequent turns
instead of the descriptive expression. In case of
non-aligned strategies, the system simply ignores
user?s use of novel REs and continues to use its
own strategy.
The step-by-step instructions to set up the
broadband connection are hand-coded as a dia-
logue script. The script is a simple determinis-
tic finite state automaton, which contains execu-
tion instruction acts(e.g. Plug in the cable in to
the socket) and observation instruction acts(e.g. Is
the ethernet light flashing?) for the user. Based
on the user?s response, the system identifies the
next instruction. However, the script only con-
tains the dialogue acts. The dialogue acts are then
processed by a built-in realiser component to cre-
ate the system utterances. The realiser uses tem-
plates in which references to domain objects are
changed based on the selected strategy to create
final utterances. By using a fixed dialogue man-
agement policy and by changing the REs, we only
explore users? reactions to various RE strategies.
The utterances are finally converted to speech and
are played back to the user.
The dialogue system handles two kinds of clar-
ification requests - open requests and closed re-
quests. With open CRs, users request the sys-
tem for location of various domain objects (e.g.
?where is the ethernet cable??) or to describe
them. With closed CRs, users verify the intended
reference, in case of ambiguity (e.g. ?Do you
mean the thin white cable with grey ends??, ?Is
it the broadband filter??, etc.). The system han-
dles these requests using a knowledge base of the
domain objects.
4.3 Wizard Activities
The primary responsibility of the wizard is to un-
derstand the participant?s utterance and annotate
it as one of the dialogue acts in the Annotation
panel, and send the dialogue act to the dialogue
system for response. In addition to the primary
responsibility, the wizard also requests confirma-
tion from the user (if needed) and also responds to
confirmation requests from the user. The wizard
also observes the user?s usage of novel REs and
records them in the User?s RE Choice panel. As
mentioned earlier, our wizard neither decides on
which strategy to use to choose REs nor chooses
96
the next task instruction to give the user.
5 Data collected
Several different kinds of data are collected before,
during and after the experiment. This data will be
used to build user simulations and reward func-
tions for learning REG strategies and language
models for speech recognition.
1. WIT log - The WIT logs the whole conversa-
tion as an XML file. The log contains system and
user dialogue acts, time of system utterance, sys-
tem?s choice of REs and its utterance at every turn.
It also contains the dialogue start time, total time
elapsed, total number of turns, number of words
in system utterances, number of clarification re-
quests, number of technical, descriptive and tuto-
rial REs, number of confirmations etc.
2. Background of the user - The user is asked to fill
in a pre-task background questionnaire containing
queries on their experience with computers, Inter-
net and dialogue systems.
3. User satisfaction survey - The user is re-
quested to fill in a post-task questionnaire contain-
ing queries on the performance of the system dur-
ing the task. Each question is answered in a four
point Likert scale on how strongly the user agrees
or disagrees with the given statement. Statements
like, ?Conversation with the system was easy?,
?I would use such a system in future?, etc are
judged by the user which will be used to build re-
ward functions for reinforcement learning of REG
strategies.
4. Knowledge pre-test - Users? initial domain
knowledge is tested by asking them to match a list
of technical terms to their respective descriptive
expressions.
5. Knowledge gain post-test - Users? knowledge
gain during the dialogue task is measured by ask-
ing them to redo the matching task.
6. Percentage of task completion - The wizard
examines the final set up on the user?s table to
determine the percentage of task success using a
form containing declarative statements describing
the ideal broadband set up (for e.g. ?the broad-
band filter is plugged in to the phone socket on
the wall?). The wizard awards one point to every
statement that is true of the user?s set up.
7. User?s utterances WAV file - The user?s ut-
terances are recorded in WAV format for build-
ing language models for automatic speech recog-
nition.
6 Results from pilot studies
We are currently running pilot studies (with 6 par-
ticipants so far) and have collected around 60 min-
utes of spoken dialogue data. We found that in
the jargon strategy, some users take a lot longer to
finish the task than others (max 59 turns, min 26
turns). We found that besides requesting clarifi-
cations, sometimes novice users assume incorrect
references to some domain objects, affecting their
task completion rates.
7 Conclusion
We have presented a novel Wizard-of-Oz environ-
ment to collect spoken data in a real situated task
environment, and to study user reactions to a va-
riety of REG strategies, including system align-
ment. The data will be used for training user sim-
ulations for reinforcement learning of REG strate-
gies to choose between technical, descriptive, tu-
torial, and aligned REs based on a user?s expertise
in the task domain.
Acknowledgements
The research leading to these results has re-
ceived funding from the European Community?s
Seventh Framework (FP7) under grant agree-
ment no. 216594 (CLASSiC Project www.classic-
project.org), EPSRC project no. EP/E019501/1,
and the British Council (UKIERI PhD Scholar-
ships 2007-08).
References
A. Arts. 2004. Overspecification in Instructive Text.
Ph.D. thesis, Tilburg University, The Netherlands.
S. Janarthanam and O. Lemon. 2009. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG?09.
M. J. Pickering and S. Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and
Brain Sciences, 27:169?225.
K. van Deemter, I. van der Sluis, and A. Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In Proc.
INLG?06.
S. Whittaker, M. Walker, and J. Moore. 2002. Fish
or Fowl: A Wizard of Oz Evaluation of Dialogue
Strategies in the Restaurant Domain. In Language
Resources and Evaluation Conference.
97
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 120?123,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
A Two-tier User Simulation Model for Reinforcement Learning of
Adaptive Referring Expression Generation Policies
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
s.janarthanam@ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
olemon@inf.ed.ac.uk
Abstract
We present a new two-tier user simula-
tion model for learning adaptive referring
expression generation (REG) policies for
spoken dialogue systems using reinforce-
ment learning. Current user simulation
models that are used for dialogue pol-
icy learning do not simulate users with
different levels of domain expertise and
are not responsive to referring expres-
sions used by the system. The two-
tier model displays these features, that
are crucial to learning an adaptive REG
policy. We also show that the two-tier
model simulates real user behaviour more
closely than other baseline models, using
the dialogue similarity measure
based on Kullback-Leibler divergence.
1 Introduction
We present a new user simulation model for
learning adaptive referring expression generation
(REG) policies for spoken dialogue systems us-
ing reinforcement learning methods. An adap-
tive REG policy equips a dialogue system to dy-
namically modify its utterances in order to adapt
to user?s domain knowledge level. For instance,
to refer to domain objects, the system might use
simple descriptive expressions with novices and
technical jargon with experts. Such adaptations
help grounding between the dialogue partners (Is-
sacs and Clark, 1987). Since the user?s knowl-
edge level is unknown, the system must be able to
adapt dynamically during the conversation. Hand-
coding such a policy could be extremely difficult.
(Janarthanam and Lemon, 2009b) have shown
that such policies can be learned using simula-
tion based reinforcement learning (RL) methods.
The quality of such learned policies is directly de-
pendent on the performance of the user simula-
tions used to train them. So far, only hand-coded
user simulations have been employed. In contrast,
we now present a data driven two-tier user sim-
ulation model trained on dialogue data collected
from real users. We also show that the two-tier
model simulates real users more faithfully than
other data driven baseline n-gram models (Eckert
et al, 1997).
In section 2 we briefly discuss other work re-
lated to user simulations for dialogue policy learn-
ing using RL. In section 3 we describe the data
used to build the simulation. Section 4 describes
the simulation models in detail. In section 5 and
6 we present the evaluation metrics used and the
results.
2 Related work
Several user simulation models have been pro-
posed for dialogue management policy learning
(Schatzmann et al, 2006; Schatzmann et al,
2007). However, these models cannot be directly
used for REG policy learning because they inter-
act with the dialogue system only using high-level
dialogue acts. Also, they do not simulate differ-
ent user groups like experts, novices, etc. In order
to learn adaptive REG policies, user simulations
need to respond to the system?s choice of referring
expressions and simulate user groups with differ-
ent knowledge levels. We propose a two-tier simu-
lation which simulates users with different knowl-
edge levels and is sensitive to the system?s choice
of referring expressions.
120
3 Corpus
The ?Wizard-of-Oz? (WOZ) methodology is a
widely accepted way of collecting dialogue data
for user simulation modeling (Whittaker et al,
2002). In this setup, real users interact with a hu-
man wizard disguised as a dialogue system. The
wizard interprets the users responses and passes
them on to the dialogue system. The dialogue sys-
tem updates the dialogue state and decides the re-
sponses to user?s moves. The task of the partici-
pant is to interact with the dialogue system to get
instructions to setup a broadband Internet connec-
tion. The referring expression generation strategy
is chosen before the dialogue starts and stays the
same for the whole session. The strategies used
were ?jargon?, ?descriptive? and ?tutorial?. In the
jargon strategy the system instructs the user us-
ing technical terms (e.g. ?Plug the broadband
filter into the phone socket.?). In the de-
scriptive strategy, it uses descriptive terms (e.g.
?Plug the small white box into the square
white box on the wall.?). In the tutorial
strategy, the system uses both jargon and descrip-
tive terms together. The system provides clari-
fications on referring expressions when users re-
quest them. The participant?s domain knowledge
is also recorded during the task. Please refer to (Ja-
narthanam and Lemon, 2009a) for a more details
on our Wizard-of-Oz environment for data collec-
tion. The dialogues were collected from 17 par-
ticipants (one dialogue each) with around 24 to 35
turns per dialogue depending on the strategy and
user?s domain knowledge.
4 User Simulation models
The dialogue data and knowledge profiles were
used to build user simulation models. These mod-
els take as input the system?s dialogue act As,t (at
turn t) and choice of referring expressions RECs,t
and output the user?s dialogue Au,t and environ-
ment EAu,t acts. User?s observation and manipu-
lation of the domain objects is represented by the
environment act.
4.1 Advanced n-gram model
A simple approach to model real user behaviour
is to model user responses (dialogue act and
environment act) as advanced n-gram models
(Georgila et al, 2006) based on many context vari-
ables - all referring expressions used in the utter-
ance (RECs,t), the user?s knowledge of the REs
(DKu), history of clarification requests on the
REs (H), and the system?s dialogue act (As,t), as
defined below:
P (Au,t|As,t, RECs,t, DKu,H)
P (EAu,t|As,t, RECs,t, DKu,H)
Although this is an ideal model of the real user
data, it covers only a limited number of contexts
owing to the limited size of the corpus. Therefore,
it cannot be used for training as there may be a
large number of unseen contexts which the model
needs to respond to. For example, this model can-
not respond when the system uses a mix of jar-
gon and descriptive expressions in its utterance be-
cause such a context does not exist in our corpus.
4.2 A Two-tier model
Instead of using a complex context model, we di-
vide the large context in to several sub-contexts
and model the user?s response based on them. We
propose a two-tier model, in which the simulation
of a user?s response is divided into two steps. First,
all the referring expressions used in the system?s
utterance are processed as below:
P (CRu,t|REs,t, DKRE,u,HRE , As,t)
This step is repeated for each expression REs,t
separately. The above model returns a clarifi-
cation request based on the referring expression
REs,t used, the user?s knowledge of the expres-
sion DKRE,u, and previous clarification requests
on the expression HRE and the system dialogue
act As,t. A clarification request is highly likely in
case of the jargon strategy and less likely in other
strategies. Also, if a clarification has already been
issued, the user is less likely to issue another re-
quest for clarification. In such cases, the clarifica-
tion request model simply returns none.
In the next step, the model returns a user di-
alogue act Au,t and an environment act EAu,t
based on the system dialogue act As,t and the clar-
ification request CRu,t, as follows:
P (Au,t|As,t, CRu,t)
P (EAu,t|As,t, CRu,t)
By dividing the complex context into smaller
sub-contexts, the two-tier model simulates real
users in contexts that are not directly observed in
the dialogue data. The model will therefore re-
spond to system utterances containing a mix of
REG strategies (for example, one jargon and one
descriptive expression in the same utterance).
121
4.3 Baseline Bigram model
A bigram model was built using the dialogue data
by conditioning the user responses only on the sys-
tem?s dialogue act (Eckert et al, 1997).
P (Au,t|As,t)
P (EAu,t|As,t)
Since it ignores all the context variables except
the system dialogue act, it can be used in contexts
that are not observed in the dialogue data.
4.4 Trigram model
The trigram model is similar to the bigram model,
but with the previous system dialogue act As,t?1
as an additional context variable.
P (Au,t|As,t, As,t?1)
P (EAu,t|As,t, As,t?1)
4.5 Equal Probability model baseline
The equal probability model is similar to the bi-
gram model, except that it is not trained on the
dialogue data. Instead, it assigns equal probabil-
ity to all possible responses for the given system
dialogue act.
4.6 Smoothing
We used Witten-Bell discounting to smooth all
our models except the equal probability model,
in order to account for unobserved but possible
responses in dialogue contexts. Witten-Bell dis-
counting extracts a small percentage of probability
mass, i.e. number of distinct responses observed
for the first time (T ) in a context, out of the to-
tal number of instances (N ), and redistributes this
mass to unobserved responses in the given context
(V ? T ) (where V is the number of all possible
responses) . The discounted probabilities P ? of
observed responses (C(ei) > 0) and unobserved
responses (C(ei) = 0) are given below.
P ?(ei) = C(ei)N+T if(C(ei) > 0)
P ?(ei) = t(N+T )(V?T ) if(C(ei) = 0)
On analysis, we found that the Witten-Bell
discounting assigns greater probability to unob-
served responses than to observed responses, in
cases where the number of responses per con-
text is very low. For instance, in a partic-
ular context, the possible responses, their fre-
quencies and their original probabilities were -
provide info (3, 0.75), other (1, 0.25),
request clarification (0, 0). After dis-
counting, the revised probabilities P ? are 0.5,
0.167 and 0.33. request clarification
gets the whole share of extracted probability as
it is the only unobserved response in the context
and is more than the other responses actually
observed in the data. This is counter-intuitive for
our application. Therefore, we use a modified ver-
sion of Witten-Bell discounting (given below) to
smooth our models, where the extracted proba-
bility is equally divided amongst all possible re-
sponses. Using the modified version, the revised
probabilities for the illustrated example are 0.61,
0.28 and 0.11 respectively.
P ?(ei) = C(ei)N+T + T(N+T )V
5 Metrics for evaluation of simulations
While there are many proposed measures to rank
user simulation models with respect to real user
data (Schatzmann et al, 2005; Georgila et al,
2006; Rieser and Lemon, 2006a; Williams, 2008),
we use the Dialogue Similarity measure
based on Kullback-Leibler (KL) (Cuayahuitl et
al., 2005; Cuayahuitl, 2009) divergence to mea-
sure how similar the probability distributions of
the simulation models are to the original real hu-
man data.
5.1 Dialogue Similarity
Dialogue Similarity is a measure of divergence be-
tween real and simulated dialogues and can mea-
sure how similar a model is to real data. The mea-
sure is based on Kullback-Leibler (KL) divergence
and is defined as follows:
DS(P ||Q) = 1N
?N
i=1
DKL(P ||Q)+DKL(Q||P )
2
DKL(P ||Q) =
?M
i=1 pi ? log(piqi )
The metric measures the divergence between
distributions P and Q in N different contexts
with M responses per context. Ideally, the dia-
logue similarity between two similar distributions
is close to zero.
6 Evaluation results
We consider the Advanced N-gram model to be
a realistic model of the real human dialogue cor-
pus, as it takes into account all context variables
and is reasonably smoothed to account for unob-
served user responses. Therefore, we compare the
probability distributions of all the other models to
122
Model Au,t EAu,t
Two-tier 0.078 0.018
Bigram 0.150 0.139
Trigram 0.145 0.158
Equal Probability 0.445 0.047
Table 1: Dialogue Similarity with Modified
Witten-Bell discounting w.r.t Advanced N-gram
model
the advanced n-gram model using the dialogue
similarity measure. The results of the evalu-
ation are given in table 1.
The results show that the two-tier model is
much closer (0.078, 0.018) to the Advanced N-
gram model than the other models. This is due to
the fact that the bigram and trigram models don?t
take into account factors like the user?s knowl-
edge, the strategy used, and the dialogue history.
By effectively dividing the RE processing and the
environment interaction, the two-tier simulation
model is not only realistic in observed contexts but
also usable in unobserved contexts (unlike the Ad-
vanced N-gram model).
7 Conclusion
We have presented a data driven user simulation
model called the two-tier model for learning REG
policies using reinforcement learning. We have
also shown that the two-tier model is much closer
to real user data than the other baseline models.
We will now train REG policies using the two-tier
model and test them on real users in the future.
Acknowledgements
The research leading to these results has re-
ceived funding from the EPSRC (project no.
EP/E019501/1) and from the European Commu-
nity?s Seventh Framework Programme (FP7/2007-
2013) under grant agreement no. 216594 (CLAS-
SiC project www.classic-project.org),
and from the British Council?s UKERI pro-
gramme.
References
H. Cuayahuitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2005. Human-Computer Dialogue Sim-
ulation Using Hidden Markov Models. In Proc. of
ASRU 2005.
H. Cuayahuitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. the-
sis, University of Edinburgh, UK.
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
Modeling for Spoken Dialogue System Evaluation.
In Proc. of ASRU97.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
Simulation for Spoken Dialogue System: Learning
and Evaluation. In Proc of ICSLP 2006.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26?37.
S. Janarthanam and O. Lemon. 2009a. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG?09.
S. Janarthanam and O. Lemon. 2009b. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG?09.
V. Rieser and O. Lemon. 2006a. Cluster-based User
Simulations for Learning Dialogue Strategies. In
Proc. Interspeech/ICSLP.
J. Schatzmann, K. Georgila, and S. J. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. SIG-
dial workshop on Discourse and Dialogue ?05.
J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.
S. Whittaker, M. Walker, and J. Moore. 2002. Fish
or Fowl: A Wizard of Oz Evaluation of Dialogue
Strategies in the Restaurant Domain. In Language
Resources and Evaluation Conference.
J. Williams. 2008. Evaluating User Simulations with
the Cramer-von Mises Divergence. Speech Commu-
nication, 50:829?846.
123
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 69?78,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning to Adapt to Unknown Users:
Referring Expression Generation in Spoken Dialogue Systems
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
s.janarthanam@ed.ac.uk
Oliver Lemon
Interaction Lab
Mathematics and Computer Science (MACS)
Heriot-Watt University
o.lemon@hw.ac.uk
Abstract
We present a data-driven approach to learn
user-adaptive referring expression gener-
ation (REG) policies for spoken dialogue
systems. Referring expressions can be dif-
ficult to understand in technical domains
where users may not know the techni-
cal ?jargon? names of the domain entities.
In such cases, dialogue systems must be
able to model the user?s (lexical) domain
knowledge and use appropriate referring
expressions. We present a reinforcement
learning (RL) framework in which the sys-
tem learns REG policies which can adapt
to unknown users online. Furthermore,
unlike supervised learning methods which
require a large corpus of expert adaptive
behaviour to train on, we show that effec-
tive adaptive policies can be learned from
a small dialogue corpus of non-adaptive
human-machine interaction, by using a RL
framework and a statistical user simula-
tion. We show that in comparison to
adaptive hand-coded baseline policies, the
learned policy performs significantly bet-
ter, with an 18.6% average increase in
adaptation accuracy. The best learned pol-
icy also takes less dialogue time (average
1.07 min less) than the best hand-coded
policy. This is because the learned poli-
cies can adapt online to changing evidence
about the user?s domain expertise.
1 Introduction
We present a reinforcement learning (Sutton and
Barto, 1998) framework to learn user-adaptive re-
ferring expression generation policies from data-
driven user simulations. A user-adaptive REG pol-
icy allows the system to choose appropriate ex-
pressions to refer to domain entities in a dialogue
Jargon: Please plug one end of the broadband
cable into the broadband filter.
Descriptive: Please plug one end of the thin
white cable with grey ends into the
small white box.
Table 1: Referring expression examples for 2 enti-
ties (from the corpus)
setting. For instance, in a technical support con-
versation, the system could choose to use more
technical terms with an expert user, or to use more
descriptive and general expressions with novice
users, and a mix of the two with intermediate users
of various sorts (see examples in Table 1).
In natural human-human conversations, dia-
logue partners learn about each other and adapt
their language to suit their domain expertise (Is-
sacs and Clark, 1987). This kind of adaptation
is called Alignment through Audience
Design (Clark and Murphy, 1982; Bell, 1984).
We assume that users are mostly unknown to
the system and therefore that a spoken dialogue
system (SDS) must be capable of observing the
user?s dialogue behaviour, modelling his/her do-
main knowledge, and adapting accordingly, just
like human interlocutors. Rule-based and super-
vised learning approaches to user adaptation in
SDS have been proposed earlier (Cawsey, 1993;
Akiba and Tanaka, 1994). However, such methods
require expensive resources such as domain ex-
perts to hand-code the rules, or a corpus of expert-
layperson interactions to train on. In contrast, we
present a corpus-driven framework using which
a user-adaptive REG policy can be learned using
RL from a small corpus of non-adaptive human-
machine interaction.
We show that these learned policies perform
better than simple hand-coded adaptive policies
in terms of accuracy of adaptation and dialogue
69
time. We also compared the performance of poli-
cies learned using a hand-coded rule-based simu-
lation and a data-driven statistical simulation and
show that data-driven simulations produce better
policies than rule-based ones.
In section 2, we present some of the related
work. Section 3 presents the dialogue data that
we used to train the user simulation. Section 4 and
section 5 describe the dialogue system framework
and the user simulation models. In section 6, we
present the training and in section 7, we present
the evaluation for different REG policies.
2 Related work
There are several ways in which natural language
generation (NLG) systems adapt to users. Some
of them adapt to a user?s goals, preferences, en-
vironment and so on. Our focus in this study
is restricted to the user?s lexical domain exper-
tise. Several NLG systems adapt to the user?s do-
main expertise at different levels of generation -
text planning (Paris, 1987), complexity of instruc-
tions (Dale, 1989), referring expressions (Reiter,
1991), and so on. Some dialogue systems, such
as COMET, have also incorporated NLG modules
that present appropriate levels of instruction to the
user (McKeown et al, 1993). However, in all the
above systems, the user?s knowledge is assumed to
be accurately represented in an initial user model
using which the system adapts its language. In
contrast to all these systems, our adaptive REG
policy knows nothing about the user when the con-
versation starts.
Rule-based and supervised learning approaches
have been proposed to learn and adapt during the
conversation dynamically. Such systems learned
from the user at the start and later adapted to the
domain knowledge of the users. However, they ei-
ther require expensive expert knowledge resources
to hand-code the inference rules (Cawsey, 1993) or
large corpus of expert-layperson interaction from
which adaptive strategies can be learned and mod-
elled, using methods such as Bayesian networks
(Akiba and Tanaka, 1994). In contrast, we present
an approach that learns in the absence of these ex-
pensive resources. It is also not clear how super-
vised and rule-based approaches choose between
when to seek more information and when to adapt.
In this study, we show that using reinforcement
learning this decision is learned automatically.
Reinforcement Learning (RL) has been suc-
cessfully used for learning dialogue management
policies since (Levin et al, 1997). The learned
policies allow the dialogue manager to optimally
choose appropriate dialogue acts such as instruc-
tions, confirmation requests, and so on, under
uncertain noise or other environment conditions.
There have been recent efforts to learn information
presentation and recommendation strategies using
reinforcement learning (Rieser and Lemon, 2009;
Hernandez et al, 2003; Rieser and Lemon, 2010),
and joint optimisation of Dialogue Management
and NLG using hierarchical RL has been pro-
posed by (Lemon, 2010). In contrast, we present a
framework to learn to choose appropriate referring
expressions based on a user?s domain knowledge.
Earlier, we reported a proof-of-concept work us-
ing a hand-coded rule-based user simulation (Ja-
narthanam and Lemon, 2009c).
3 The Wizard-of-Oz Corpus
We use a corpus of technical support dialogues
collected from real human users using a Wizard-
of-Oz method (Janarthanam and Lemon, 2009b).
The corpus consists of 17 dialogues from users
who were instructed to physically set up a home
broadband connection using objects like a wire-
less modem, cables, filters, etc. They listened to
the instructions from the system and carried them
out using the domain objects laid in front of them.
The human ?wizard? played the role of only an in-
terpreter who would understand what the user said
and annotate it as a dialogue act. The set-up ex-
amined the effect of using three types of referring
expressions (jargon, descriptive, and tutorial), on
the users.
Out of the 17 dialogues, 6 used a jargon strat-
egy, 6 used a descriptive strategy, and 5 used a
tutorial strategy1. The task had reference to 13
domain entities, mentioned repeatedly in the di-
alogue. In total, there are 203 jargon, 202 descrip-
tive and 167 tutorial referring expressions. Inter-
estingly, users who weren?t acquainted with the
domain objects requested clarification on some of
the referring expressions used. The dialogue ex-
changes between the user and system were logged
in the form of dialogue acts and the system?s
choices of referring expressions. Each user?s
knowledge of domain entities was recorded both
before and after the task and each user?s interac-
1The tutorial strategy uses both jargon and descriptive ex-
pressions together.
70
tions with the environment were recorded. We use
the dialogue data, pre-task knowledge tests, and
the environment interaction data to train a user
simulation model. Pre and post-task test scores
were used to model the learning behaviour of the
users during the task (see section 5).
The corpus also recorded the time taken to com-
plete each dialogue task. We used these data to
build a regression model to calculate total dialogue
time for dialogue simulations. The strategies were
never mixed (with some jargon, some descriptive
and some tutorial expressions) within a single con-
versation. Therefore, please note that the strate-
gies used for data collection were not adaptive and
the human ?wizard? has no role in choosing which
referring expression to present to the user. Due to
this fact, no user score regarding adaptation was
collected. We therefore measure adaptation objec-
tively as explained in section 6.1.
4 The Dialogue System
In this section, we describe the different modules
of the dialogue system. The interaction between
the different modules is shown in figure 1 (in
learning mode). The dialogue system presents the
user with instructions to setup a broadband con-
nection at home. In the Wizard of Oz setup, the
system and the user interact using speech. How-
ever, in our machine learning setup, they interact at
the abstract level of dialogue actions and referring
expressions. Our objective is to learn to choose
the appropriate referring expressions to refer to the
domain entities in the instructions.
Figure 1: System User Interaction (learning)
4.1 Dialogue Manager
The dialogue manager identifies the next instruc-
tion (dialogue act) to give to the user based on the
dialogue management policy pidm. Since, in this
study, we focus only on learning the REG policy,
the dialogue management is coded in the form of
a finite state machine. In this dialogue task, the
system provides two kinds of instructions - ob-
servation and manipulation. For observation in-
structions, users observe the environment and re-
port back to the system, and for the manipulation
instructions (such as plugging in a cable in to a
socket), they manipulate the domain entities in the
environment. When the user carries out an instruc-
tion, the system state is updated and the next in-
struction is given. Sometimes, users do not under-
stand the referring expressions used by the system
and then ask for clarification. In such cases, the
system provides clarification on the referring ex-
pression (provide clar), which is information to
enable the user to associate the expression with
the intended referent. The system action As,t (t
denoting turn, s denoting system) is therefore to
either give the user the next instruction or a clarifi-
cation. When the user responds in any other way,
the instruction is simply repeated. The dialogue
manager is also responsible for updating and man-
aging the system state Ss,t (see section 4.2). The
system interacts with the user by passing both the
system action As,t and the referring expressions
RECs,t (see section 4.3).
4.2 The dialogue state
The dialogue state Ss,t is a set of variables that
represent the current state of the conversation. In
our study, in addition to maintaining an overall di-
alogue state, the system maintains a user model
UMs,t which records the initial domain knowl-
edge of the user. It is a dynamic model that starts
with a state where the system does not have any
idea about the user. As the conversation pro-
gresses, the dialogue manager records the evi-
dence presented to it by the user in terms of his
dialogue behaviour, such as asking for clarifica-
tion and interpreting jargon. Since the model is
updated according to the user?s behaviour, it may
be inaccurate if the user?s behaviour is itself uncer-
tain. So, when the user?s behaviour changes (for
instance, from novice to expert), this is reflected
in the user model during the conversation. Hence,
unlike previous studies mentioned in section 2, the
user model used in this system is not always an ac-
curate model of the user?s knowledge and reflects
a level of uncertainty about the user.
71
Each jargon referring expression x is repre-
sented by a three valued variable in the dialogue
state: user knows x. The three values that each
variable takes are yes, no, not sure. The vari-
ables are updated using a simple user model up-
date algorithm. Initially each variable is set to
not sure. If the user responds to an instruction
containing the referring expression x with a clari-
fication request, then user knows x is set to no.
Similarly, if the user responds with appropriate in-
formation to the system?s instruction, the dialogue
manager sets user knows x is set to yes.
The dialogue manager updates the variables
concerning the referring expressions used in the
current system utterance appropriately after the
user?s response each turn. The user may have the
capacity to learn jargon. However, only the user?s
initial knowledge is recorded. This is based on the
assumption that an estimate of the user?s knowl-
edge helps to predict the user?s knowledge of the
rest of the referring expressions. Another issue
concerning the state space is its size. Since, there
are 13 entities and we only model the jargon ex-
pressions, the state space size is 313.
4.3 REG module
The REG module is a part of the NLG module
whose task is to identify the list of domain enti-
ties to be referred to and to choose the appropriate
referring expression for each of the domain enti-
ties for each given dialogue act. In this study, we
focus only on the production of appropriate refer-
ring expressions to refer to domain entities men-
tioned in the dialogue act. It chooses between the
two types of referring expressions - jargon and de-
scriptive. For example, the domain entity broad-
band filter can be referred to using the jargon ex-
pression ?broadband filter? or using the descrip-
tive expression ?small white box?2. We call this
the act of choosing the REG action. The tutorial
strategy was not investigated here since the corpus
analysis showed tutorial utterances to be very time
consuming. In addition, they do not contribute to
the adaptive behaviour of the system.
The REG module operates in two modes - learn-
ing and evaluation. In the learning mode, the REG
module is the learning agent. The REG mod-
ule learns to associate dialogue states with opti-
mal REG actions. This is represented by a REG
2We will use italicised forms to represent the domain enti-
ties (e.g. broadband filter) and double quotes to represent the
referring expressions (e.g. ?broadband filter?).
policy pireg : UMs,t ? RECs,t, which maps
the states of the dialogue (user model) to optimal
REG actions. The referring expression choices
RECs,t is a set of pairs identifying the refer-
ent R and the type of expression T used in the
current system utterance. For instance, the pair
(broadband filter, desc) represents the descriptive
expression ?small white box?.
RECs,t = {(R1, T1), ..., (Rn, Tn)}
In the evaluation mode, a trained REG policy in-
teracts with unknown users. It consults the learned
policy pireg to choose the referring expressions
based on the current user model.
5 User Simulations
In this section, we present user simulation models
that simulate the dialogue behaviour of a real hu-
man user. These external simulation models are
different from internal user models used by the
dialogue system. In particular, our model is the
first to be sensitive to a system?s choices of refer-
ring expressions. The simulation has a statistical
distribution of in-built knowledge profiles that de-
termines the dialogue behaviour of the user being
simulated. If the user does not know a referring
expression, then he is more likely to request clar-
ification. If the user is able to interpret the refer-
ring expressions and identify the references then
he is more likely to follow the system?s instruc-
tion. This behaviour is simulated by the action se-
lection models described below.
Several user simulation models have been pro-
posed for use in reinforcement learning of dia-
logue policies (Georgila et al, 2005; Schatzmann
et al, 2006; Schatzmann et al, 2007; Ai and Lit-
man, 2007). However, they are suited only for
learning dialogue management policies, and not
natural language generation policies. Earlier, we
presented a two-tier simulation trained on data
precisely for REG policy learning (Janarthanam
and Lemon, 2009a). However, it is not suited for
training on small corpus like the one we have at
our disposal. In contrast to the earlier model, we
now condition the clarification requests on the ref-
erent class rather than the referent itself to handle
data sparsity problem.
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a
user action Au,t (u denoting user). This can ei-
ther be a clarification request (cr) or an instruction
72
response (ir). We used two kinds of action selec-
tion models: corpus-driven statistical model and
hand-coded rule-based model.
5.1 Corpus-driven action selection model
In the corpus-driven model, the US produces a
clarification request cr based on the class of the
referent C(Ri), type of the referring expression
Ti, and the current domain knowledge of the user
for the referring expression DKu,t(Ri, Ti). Do-
main entities whose jargon expressions raised clar-
ification requests in the corpus were listed and
those that had more than the mean number of clar-
ification requests were classified as difficult
and others as easy entities (for example, ?power
adaptor? is easy - all users understood this
expression, ?broadband filter? is difficult).
Clarification requests are produced using the fol-
lowing model.
P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) ? RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment action EAu,t
is generated using the following model.
P (EAu,t|As,t) if Au,t! = cr(Ri, Ti)
Finally, the user action is an instruction re-
sponse which is determined by the system action
As,t. Instruction responses can be different in dif-
ferent conditions. For an observe and report in-
struction, the user issues a provide info action
and for a manipulation instruction, the user re-
sponds with an acknowledgement action and so
on.
P (Au,t = ir|EAu,t, As,t)
All the above models were trained on our cor-
pus data using maximum likelihood estimation and
smoothed using a variant of Witten-Bell discount-
ing. According to the data, clarification requests
are much more likely when jargon expressions
are used to refer to the referents that belong to
the difficult class and which the user doesn?t
livebox = 1 power adaptor = 1
wall phone socket = 1 broadband filter = 0
broadband cable = 0 ethernet cable = 1
lb power light = 1 lb power socket = 1
lb broadband light = 0 lb ethernet light = 0
lb adsl socket = 0 lb ethernet socket = 0
pc ethernet socket = 1
Table 2: Domain knowledge: an Intermediate
User
know about. When the system uses expressions
that the user knows, the user generally responds
to the instruction given by the system. These user
simulation models have been evaluated and found
to produce behaviour that is very similar to the
original corpus data, using the Kullback-Leibler
divergence metric (Cuayahuitl, 2009).
5.2 Rule-based action selection model
We also built a rule-based simulation using the
above models but where some of the parameters
were set manually instead of estimated from the
data. The purpose of this simulation is to in-
vestigate how learning with a data-driven statisti-
cal simulation compares to learning with a simple
hand-coded rule-based simulation. In this simula-
tion, the user always asks for a clarification when
he does not know a jargon expression (regardless
of the class of the referent) and never does this
when he knows it. This enforces a stricter, more
consistent behaviour for the different knowledge
patterns, which we hypothesise should be easier to
learn to adapt to, but may lead to less robust REG
policies.
5.3 User Domain knowledge
The user domain knowledge is initially set to one
of several models at the start of every conver-
sation. The models range from novices to ex-
perts which were identified from the corpus using
k-means clustering. The initial knowledge base
(DKu,initial) for an intermediate user is shown in
table 2. A novice user knows only ?power adap-
tor?, and an expert knows all the jargon expres-
sions. We assume that users can interpret the de-
scriptive expressions and resolve their references.
Therefore, they are not explicitly represented. We
only code the user?s knowledge of jargon expres-
sions. This is represented by a boolean variable
for each domain entity.
73
Corpus data shows that users can learn jargon
expressions during the conversation. The user?s
domain knowledge DKu is modelled to be dy-
namic and is updated during the conversation.
Based on our data, we found that when presented
with clarification on a jargon expression, users al-
ways learned the jargon.
if As,t = provide clar(Ri, Ti)
DKu,t+1(Ri, Ti) ? 1
Users also learn when jargon expressions are re-
peatedly presented to them. Learning by repetition
follows the pattern of a learning curve - the greater
the number of repetitions #(Ri, Ti), the higher the
likelihood of learning. This is modelled stochas-
tically based on repetition using the parameter
#(Ri, Ti) as follows (where (Ri, Ti) ? RECs,t) .
P (DKu,t+1(Ri, Ti) ? 1|#(Ri, Ti))
The final state of the user?s domain knowl-
edge (DKu,final) may therefore be different from
the initial state (DKu,initial) due to the learn-
ing effect produced by the system?s use of jar-
gon expressions. In most studies done previously,
the user?s domain knowledge is considered to be
static. However in real conversation, we found that
the users nearly always learned jargon expressions
from the system?s utterances and clarifications.
6 Training
The REG module was trained (operated in learn-
ing mode) using the above simulations to learn
REG policies that select referring expressions
based on the user expertise in the domain. As
shown in figure 1, the learning agent (REG mod-
ule) is given a reward at the end of every dialogue.
During the training session, the learning agent ex-
plores different ways to maximize the reward. In
this section, we discuss how to code the learning
agent?s goals as reward. We then discuss how the
reward function is used to train the learning agent.
6.1 Reward function
A reward function generates a numeric reward for
the learning agent?s actions. It gives high rewards
to the agent when the actions are favourable and
low rewards when they are not. In short, the re-
ward function is a representation of the goal of the
agent. It translates the agent?s actions into a scalar
value that can be maximized by choosing the right
action sequences.
We designed a reward function for the goal of
adapting to each user?s domain knowledge. We
present the Adaptation Accuracy score AA that
calculates how accurately the agent chose the ex-
pressions for each referent r, with respect to the
user?s knowledge. Appropriateness of an expres-
sion is based on the user?s knowledge of the ex-
pression. So, when the user knows the jargon ex-
pression for r, the appropriate expression to use is
jargon, and if s/he doesn?t know the jargon, an de-
scriptive expression is appropriate. Although the
user?s domain knowledge is dynamically chang-
ing due to learning, we base appropriateness on
the initial state, because our objective is to adapt to
the initial state of the user DKu,initial. However,
in reality, designers might want their system to ac-
count for user?s changing knowledge as well. We
calculate accuracy per referent RAr as the ratio
of number of appropriate expressions to the total
number of instances of the referent in the dialogue.
We then calculate the overall mean accuracy over
all referents as shown below.
RAr = #(appropriate expressions(r))#(instances(r))
AdaptationAccuracyAA = 1#(r)?rRAr
Note that this reward is computed at the end of
the dialogue (it is a ?final? reward), and is then
back-propagated along the action sequence that
led to that final state. Thus the reward can be com-
puted for each system REG action, without the
system having access to the user?s initial domain
knowledge while it is learning a policy.
Since the agent starts the conversation with
no knowledge about the user, it may try to use
more exploratory moves to learn about the user,
although they may be inappropriate. However,
by measuring accuracy to the initial user state,
the agent is encouraged to restrict its exploratory
moves and start predicting the user?s domain
knowledge as soon as possible. The system should
therefore ideally explore less and adapt more to
increase accuracy. The above reward function re-
turns 1 when the agent is completely accurate in
adapting to the user?s domain knowledge and it
returns 0 if the agent?s REC choices were com-
pletely inappropriate. Usually during learning, the
reward value lies between these two extremes and
the agent tries to maximize it to 1.
74
6.2 Learning
The REG module was trained in learning mode us-
ing the above reward function using the SHAR-
SHA reinforcement learning algorithm (with lin-
ear function approximation) (Shapiro and Langley,
2002). This is a hierarchical variant of SARSA,
which is an on-policy learning algorithm that up-
dates the current behaviour policy (see (Sutton
and Barto, 1998)). The training produced approx.
5000 dialogues. Two types of simulations were
used as described above: Data-driven and Hand-
coded. Both user simulations were calibrated to
produce three types of users: Novice, Int2 (in-
termediate) and Expert, randomly but with equal
probability. Novice users knew just one jargon
expression, Int2 knew seven, and Expert users
knew all thirteen jargon expressions. There was
an underlying pattern in these knowledge profiles.
For example, Intermediate users were those who
knew the commonplace domain entities but not
those specific to broadband connection. For in-
stance, they knew ?ethernet cable? and ?pc ether-
net socket? but not ?broadband filter? and ?broad-
band cable?.
Initially, the REG policy chooses randomly be-
tween the referring expression types for each do-
main entity in the system utterance, irrespective
of the user model state. Once the referring expres-
sions are chosen, the system presents the user sim-
ulation with both the dialogue act and referring ex-
pression choices. The choice of referring expres-
sion affects the user?s dialogue behaviour which in
turn makes the dialogue manager update the user
model. For instance, choosing a jargon expres-
sion could evoke a clarification request from the
user, which in turn prompts the dialogue manager
to update the user model with the new information
that the user is ignorant of the particular expres-
sion. It should be noted that using a jargon expres-
sion is an information seeking move which enables
the REG module to estimate the user?s knowledge
level. The same process is repeated for every dia-
logue instruction. At the end of the dialogue, the
system is rewarded based on its choices of refer-
ring expressions. If the system chooses jargon ex-
pressions for novice users or descriptive expres-
sions for expert users, penalties are incurred and if
the system chooses REs appropriately, the reward
is high. On the one hand, those actions that fetch
more reward are reinforced, and on the other hand,
the agent tries out new state-action combinations
to explore the possibility of greater rewards. Over
time, it stops exploring new state-action combina-
tions and exploits those actions that contribute to
higher reward. The REG module learns to choose
the appropriate referring expressions based on the
user model in order to maximize the overall adap-
tation accuracy.
Figure 2 shows how the agent learns using the
data-driven (Learned DS) and hand-coded simu-
lations (Learned HS) during training. It can be
seen in the figure 2 that towards the end the curve
plateaus signifying that learning has converged.
Figure 2: Learning curves - Training
7 Evaluation
In this section, we present the evaluation metrics
used, the baseline policies that were hand-coded
for comparison, and the results of evaluation.
7.1 Metrics
In addition to the adaptation accuracy mentioned
in section 6.1, we also measure other parame-
ters from the conversation in order to show how
learned adaptive policies compare with other poli-
cies on other dimensions. We calculate the time
taken (Time) for the user to complete the dialogue
task. This is calculated using a regression model
from the corpus based on number of words, turns,
and mean user response time. We also measure
the (normalised) learning gain (LG) produced by
using unknown jargon expressions. This is calcu-
lated using the pre and post scores from the user
domain knowledge (DKu) as follows.
Learning Gain LG = Post?Pre1?Pre
75
7.2 Baseline REG policies
In order to compare the performance of the learned
policy with hand-coded REG policies, three sim-
ple rule-based policies were built. These were
built in the absence of expert domain knowledge
and a expert-layperson corpus.
? Jargon: Uses jargon for all referents by de-
fault. Provides clarifications when requested.
? Descriptive: Uses descriptive expressions for
all referents by default.
? Switching: This policy starts with jargon
expressions and continues using them until
the user requests for clarification. It then
switches to descriptive expressions and con-
tinues to use them until the user complains.
In short, it switches between the two strate-
gies based on the user?s responses.
All the policies exploit the user model in sub-
sequent references after the user?s knowledge of
the expression has been set to either yes or no.
Therefore, although these policies are simple, they
do adapt to a certain extent, and are reasonable
baselines for comparison in the absence of expert
knowledge for building more sophisticated base-
lines.
7.3 Results
The policies were run under a testing condition
(where there is no policy learning or exploration)
using a data-driven simulation calibrated to simu-
late 5 different user types. In addition to the three
users - Novice, Expert and Int2, from the train-
ing simulations, two other intermediate users (Int1
and Int3) were added to examine how well each
policy handles unseen user types. The REG mod-
ule was operated in evaluation mode to produce
around 200 dialogues per policy distributed over
the 5 user groups.
Overall performance of the different policies in
terms of Adaptation Accuracy (AA), Time and
Learning Gain (LG) are given in Table 3. Fig-
ure 3 shows how each policy performs in terms of
accuracy on the 5 types of users.
We found that the Learned DS policy (i.e.
learned with the data-driven user simulation) is
the most accurate (Mean = 79.70, SD = 10.46)
in terms of adaptation to each user?s initial state
of domain knowledge. Also, it is the only pol-
icy that has more or less the same accuracy scores
Figure 3: Evaluation - Adaptation Accuracy
Policies AA Time T LG
Descriptive 46.15 7.44 0
Jargon 74.54 9.15* 0.97*
Switching 62.47 7.48 0.30
Learned HS 69.67 7.52 0.33
Learned DS 79.70* 8.08* 0.63*
* Significantly different from all oth-
ers (p < 0.05).
Table 3: Evaluation on 5 user types
over all different user types (see figure 3). It
should also be noted that the it generalised well
over user types (Int1 and Int3) which were un-
seen in training. Learned DS policy outperforms
all other policies: Learned HS (Mean = 69.67, SD
= 14.18), Switching (Mean = 62.47, SD = 14.18),
Jargon (Mean = 74.54, SD = 17.9) and Descrip-
tive (Mean = 46.15, SD = 33.29). The differences
between the accuracy (AA) of the Learned DS pol-
icy and all other policies were statistically signif-
icant with p < 0.05 (using a two-tailed paired t-
test). Although Learned HS policy is similar to
the Learned DS policy, as shown in the learning
curves in figure 2, it does not perform as well
when confronted with users types that it did not
encounter during training. The Switching policy,
on the other hand, quickly switches its strategy
(sometimes erroneously) based on the user?s clar-
ification requests but does not adapt appropriately
to evidence presented later during the conversa-
tion. Sometimes, this policy switches erroneously
because of the uncertain user behaviours. In con-
trast, learned policies continuously adapt to new
evidence. The Jargon policy performs better than
76
the Learned HS and Switching policies. This be-
cause the system can learn more about the user
by using more jargon expressions and then use
that knowledge for adaptation for known referents.
However, it is not possible for this policy to pre-
dict the user?s knowledge of unseen referents. The
Learned DS policy performs better than the Jargon
policy, because it is able to accurately predict the
user?s knowledge of referents unseen in the dia-
logue so far.
The learned policies are a little more time-
consuming than the Switching and Descriptive
policies but compared to the Jargon policy,
Learned DS takes 1.07 minutes less time. This is
because learned policies use a few jargon expres-
sions (giving rise to clarification requests) to learn
about the user. On the other hand, the Jargon pol-
icy produces more user learning gain because of
the use of more jargon expressions. Learned poli-
cies compensate on time and learning gain in order
to predict and adapt well to the users? knowledge
patterns. This is because the training was opti-
mized for accuracy of adaptation and not for learn-
ing gain or time taken. The results show that using
our RL framework, REG policies can be learned
using data-driven simulations, and that such a pol-
icy can predict and adapt to a user?s knowledge
pattern more accurately than policies trained us-
ing hand-coded rule-based simulations and hand-
coded baseline policies.
7.4 Discussion
The learned policies explore the user?s expertise
and predict their knowledge patterns, in order to
better choose expressions for referents unseen in
the dialogue so far. The system learns to iden-
tify the patterns of knowledge in the users with
a little exploration (information seeking moves).
So, when it is provided with a piece of evidence
(e.g. the user knows ?broadband filter?), it is able
to accurately estimate unknown facts (e.g. the user
might know ?broadband cable?). Sometimes, its
choices are wrong due to incorrect estimation of
the user?s expertise (due to stochastic behaviour
of the users). In such cases, the incorrect adapta-
tion move can be considered to be an information
seeking move. This helps further adaptation us-
ing the new evidence. By continuously using this
?seek-predict-adapt? approach, the system adapts
dynamically to different users. Therefore, with
a little information seeking and better prediction,
the learned policies are able to better adapt to users
with different domain expertise.
In addition to adaptation, learned policies learn
to identify when to seek information from the user
to populate the user model (which is initially set
to not sure). It should be noted that the sys-
tem cannot adapt unless it has some information
about the user and therefore needs to decisively
seek information by using jargon expressions. If
it seeks information all the time, it is not adapting
to the user. The learned policies therefore learn to
trade-off between information seeking moves and
adaptive moves in order to maximize the overall
adaptation accuracy score.
8 Conclusion
In this study, we have shown that user-adaptive
REG policies can be learned from a small cor-
pus of non-adaptive dialogues between a dialogue
system and users with different domain knowl-
edge levels. We have shown that such adaptive
REG policies learned using a RL framework adapt
to unknown users better than simple hand-coded
policies built without much input from domain ex-
perts or from a corpus of expert-layperson adap-
tive dialogues. The learned, adaptive REG poli-
cies learn to trade off between adaptive moves and
information seeking moves automatically to max-
imize the overall adaptation accuracy. Learned
policies start the conversation with information
seeking moves, learn a little about the user, and
start adapting dynamically as the conversation
progresses. We have also shown that a data-driven
statistical user simulation produces better policies
than a simple hand-coded rule-based simulation,
and that the learned policies generalise well to un-
seen users.
In future work, we will evaluate the learned
policies with real users to examine how well
they adapt, and examine how real users evalu-
ate them (subjectively) in comparison to baselines.
Whether the learned policies perform better or as
well as a hand-coded policy painstakingly crafted
by a domain expert (or learned using supervised
methods from an expert-layperson corpus) is an
interesting question that needs further exploration.
Also, it would also be interesting to make the
learned policy account for the user?s learning be-
haviour and adapt accordingly.
77
Acknowledgements
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
H. Ai and D. Litman. 2007. Knowledge consistent
user simulations for dialog systems. In Proceedings
of Interspeech 2007, Antwerp, Belgium.
T. Akiba and H. Tanaka. 1994. A Bayesian approach
for User Modelling in Dialogue Systems. In Pro-
ceedings of the 15th conference on Computational
Linguistics - Volume 2, Kyoto.
A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145?204.
A. Cawsey. 1993. User Modelling in Interactive Ex-
planations. User Modeling and User-Adapted Inter-
action, 3(3):221?247.
H. H. Clark and G. L. Murphy. 1982. Audience de-
sign in meaning and reference. In J. F. LeNy and
W. Kintsch, editors, Language and comprehension.
Amsterdam: North-Holland.
H. Cuayahuitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. the-
sis, University of Edinburgh, UK.
R. Dale. 1989. Cooking up referring expressions. In
Proc. ACL-1989.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc of Eu-
rospeech/Interspeech.
F. Hernandez, E. Gaudioso, and J. G. Boticario. 2003.
A Multiagent Approach to Obtain Open and Flexible
User Models in Adaptive Learning Communities. In
User Modeling 2003, volume 2702/2003 of LNCS.
Springer, Berlin / Heidelberg.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26?37.
S. Janarthanam and O. Lemon. 2009a. A Two-tier
User Simulation Model for Reinforcement Learning
of Adaptive Referring Expression Generation Poli-
cies. In Proc. SigDial?09.
S. Janarthanam and O. Lemon. 2009b. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG?09.
S. Janarthanam and O. Lemon. 2009c. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG?09.
O. Lemon. 2010. Learning what to say and how to say
it: joint optimization of spoken dialogue manage-
ment and Natural Language Generation. Computer
Speech and Language. (to appear).
E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proc. of ASRU97.
K. McKeown, J. Robin, and M. Tanenblatt. 1993. Tai-
loring Lexical Choice to the User?s Vocabulary in
Multimedia Explanation Generation. In Proc. ACL
1993.
C. L. Paris. 1987. The Use of Explicit User Models
in Text Generations: Tailoring to a User?s Level of
Expertise. Ph.D. thesis, Columbia University.
E. Reiter. 1991. Generating Descriptions that Exploit a
User?s Domain Knowledge. In R. Dale, C. Mellish,
and M. Zock, editors, Current Research in Natural
Language Generation, pages 257?285. Academic
Press.
V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL?09.
V. Rieser and O. Lemon. 2010. Optimising informa-
tion presentation for spoken dialogue systems. In
Proc. ACL. (to appear).
J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.
D. Shapiro and P. Langley. 2002. Separating skills
from preference: Using learning to program by re-
ward. In Proc. ICML-02.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
78
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 49?54,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Web-based Evaluation Framework for Spatial Instruction-Giving Systems
Srinivasan Janarthanam, Oliver Lemon, and Xingkun Liu
Interaction Lab
School of Mathematical and Computer Sciences
Heriot Watt University, Edinburgh
sc445,o.lemon,x.liu@hw.ac.uk
Abstract
We demonstrate a web-based environment for
development and testing of different pedes-
trian route instruction-giving systems. The
environment contains a City Model, a TTS
interface, a game-world, and a user GUI in-
cluding a simulated street-view. We describe
the environment and components, the metrics
that can be used for the evaluation of pedes-
trian route instruction-giving systems, and the
shared challenge which is being organised us-
ing this environment.
1 Introduction
Generating navigation instructions in the real world
for pedestrians is an interesting research problem
for researchers in both computational linguistics
and geo-informatics (Dale et al, 2003; Richter and
Duckham, 2008). These systems generate verbal
route directions for users to go from A to B, and
techniques range from giving ?a priori? route direc-
tions (i.e. all route information in a single turn) and
incremental ?in-situ? instructions, to full interactive
dialogue systems (see section 4). One of the major
problems in developing such systems is in evaluat-
ing them with real users in the real world. Such eval-
uations are expensive, time consuming and painstak-
ing to organise, and are carried out not just at the end
of the project but also during the development cycle.
Consequently, there is a need for a common platform
to effectively compare the performances of verbal
navigation systems developed by different teams us-
ing a variety of techniques (e.g. a priori vs. in-situ
or rule-based vs. machine learning).
This demonstration system brings together exist-
ing online data resources and software toolkits to
create a low-cost framework for evaluation of pedes-
trian route instruction systems. We have built a
web-based environment containing a simulated real
world in which users can simulate walking on the
streets of real cities whilst interacting with differ-
ent navigation systems. This evaluation framework
will be used in the near future to evaluate a series of
instruction-giving dialogue systems.
2 Related work
The GIVE challenge developed a 3D virtual in-
door environment for development and evaluation
of indoor pedestrian navigation instruction systems
(Koller et al, 2007; Byron et al, 2007). In this
framework, users can walk through a building with
rooms and corridors, similar to a first-person shooter
game. The user is instructed by a navigation sys-
tem that generates route instructions. The basic idea
was to have several such navigation systems hosted
on the GIVE server and evaluate them in the same
game worlds, with a number of users over the in-
ternet. Conceptually our work is very similar to the
GIVE framework, but its objective is to evaluate sys-
tems that instruct pedestrian users in the real world.
The GIVE framework has been successfully used for
comparative evaluation of several systems generat-
ing instructions in virtual indoor environments.
Another system, ?Virtual Navigator?, is a simu-
lated 3D environment that simulates the real world
for training blind and visually impaired people to
learn often-used routes and develop basic naviga-
tion skills (McGookin et al, 2010). The framework
49
uses haptic force-feedback and spatialised auditory
feedback to simulate the interaction between users
and the environment they are in. The users simulate
walking by using arrow keys on a keyboard and by
using a device that works as a 3D mouse to simulate
a virtual white cane. Auditory clues are provided
to the cane user to indicate for example the differ-
ence between rush hour and a quiet evening in the
environment. While this simulated environment fo-
cusses on the providing the right kind of tactile and
auditory feedback to its users, we focus on provid-
ing a simulated environment where people can look
at landmarks and navigate based on spatial and vi-
sual instructions provided to them.
User simulation modules are usually developed
to train and test reinforcement learning based in-
teractive spoken dialogue systems (Janarthanam and
Lemon, 2009; Georgila et al, 2006; Schatzmann et
al., 2006). These agents replace real users in interac-
tion with dialogue systems. However, these models
simulate the users? behaviours in addition to the en-
vironment in which they operate. Users? dialogue
and physical behaviour are dependent on a number
of factors such as a user?s preferences, goals, knowl-
edge of the environment, environmental constraints,
etc. Simulating a user?s behaviour realistically based
on many such features requires large amounts of
data. In contrast to this approach, we propose a sys-
tem where only the spatial and visual environment is
simulated.
See section 4 for a discussion of different pedes-
trian navigation systems.
3 Architecture
The evaluation framework architecture is shown in
figure 1. The server side consists of a broker module,
navigation system, gameworld server, TTS engine,
and a city model. On the user?s side is a web-based
client that consists of the simulated real-world and
the interaction panel.
3.1 Game-world module
Walking aimlessly in the simulated real world can be
a boring task. Therefore, instead of giving web users
navigation tasks from A to B, we embed navigation
tasks in a game-world overlaid on top of the simu-
lated real world. We developed a ?treasure hunting?
game which consists of users solving several pieces
of a puzzle to discover the location of the treasure
chest. In order to solve the puzzle, they interact with
game characters (e.g. a pirate) to obtain clues as to
where the next clue is. This sets the user a number of
navigation tasks to acquire the next clues until they
find the treasure. In order to keep the game interest-
ing, the user?s energy depletes as time goes on and
they therefore have limited time to find the treasure.
Finally, the user?s performance is scored to encour-
age users to return. The game characters and enti-
ties like keys, chests, etc. are laid out on real streets
making it easy to develop a game without develop-
ing a game-world. New game-worlds can be easily
scripted using Javascript, where the location (lati-
tude and longitude) and behaviour of the game char-
acters are defined. The game-world module serves
game-world specifications to the web-based client.
3.2 Broker
The broker module is a web server that connects the
web clients to their corresponding different naviga-
tion systems. This module ensures that the frame-
work works for multiple users. Navigation systems
are instantiated and assigned to new users when they
first connect to the broker. Subsequent messages
from the users will be routed to the assigned navi-
gation system. The broker communicates with the
navigation systems via a communication platform
thereby ensuring that different navigation systems
developed using different languages (such as C++,
Java, Python, etc) are supported.
3.3 Navigation system
The navigation system is the central component of
this architecture, which provides the user instruc-
tions to reach their destinations. Each navigation
system is run as a server remotely. When a user?s
client connects to the server, it instantiates a navi-
gation system object and assigns it to the user ex-
clusively. Every user is identified using a unique id
(UUID), which is used to map the user to his/her re-
spective navigation system. The navigation system
is introduced in the game scenario as a buddy sys-
tem that will help the user in his objective: find the
treasure. The web client sends the user?s location to
the system periodically (every few seconds).
50
Figure 1: Evaluation framework architecture
3.4 TTS engine
Alongside the navigation system we use the Cere-
proc text-to-speech engine that converts the utter-
ances of the system into speech. The URL of the
audio file is then sent to the client?s browser which
then uses the audio plugin to play the synthesized
speech to the user. The TTS engine need not be used
if the output modality of the system is just text.
3.5 City Model
The navigation system is supported by a database
called the City Model. The City Model is a GIS
database containing a variety of data required to sup-
port navigation tasks. It has been derived from an
open-source data source called OpenStreetMaps1. It
consists of the following:
? Street network data: the street network data
consists of nodes and ways representing junc-
tions and streets.
? Amenities: such as ATMs, public toilets, etc.
? Landmarks: other structures that can serve as
landmarks. E.g. churches, restaurants, etc.
The amenities and landmarks are represented as
nodes (with latitude and longitude information). The
City Model interface API consists of a number of
1www.openstreetmaps.org
subroutines to access the required information such
as the nearest amenity, distance or route from A to B,
etc. These subroutines provide the interface between
the navigation systems and the database.
3.6 Web-based client
The web-based client is a JavaScript/HTML pro-
gram running on the user?s web browser software
(e.g. Google Chrome). A snapshot of the webclient
is shown in figure 2. It has two parts: the streetview
panel and the interaction panel.
Streetview panel: the streetview panel presents a
simulated real world visually to the user. When
the page loads, a Google Streetview client (Google
Maps API) is created with an initial user coordinate.
Google Streetview is a web service that renders a
panoramic view of real streets in major cities around
the world. This client allows the web user to get a
panoramic view of the streets around the user?s vir-
tual location. A gameworld received from the server
is overlaid on the simulated real world. The user can
walk around and interact with game characters using
the arrow keys on his keyboard or the mouse. As the
user walks around, his location (stored in the form
of latitude and longitude coordinates) gets updated
locally. Streetview also returns the user?s point of
view (0-360 degrees), which is also stored locally.
Interaction panel: the web-client also includes an
51
interaction panel that lets the user interact with his
buddy navigation system. In addition to user lo-
cation information, users can also interact with the
navigation system using textual utterances or their
equivalents. We provide users with two types of in-
teraction panel: a GUI panel and a text panel. In the
GUI panel, there are GUI objects such as buttons,
drop-down lists, etc. which can be used to construct
requests and responses to the system. By clicking
the buttons, users can send abstract semantic repre-
sentations (dialogue actions) that are equivalent to
their textual utterances. For example, the user can
request a route to a destination by selecting the street
name from a drop down list and click on the Send
button. Similarly, users can click on ?Yes?, ?No?,
?OK?, etc. buttons to respond to the system?s ques-
tions and instructions. In the text panel, on the other
hand, users are free to type any request or response
they want. Of course, both types of inputs are parsed
by the navigation system. We also plan to add an ad-
ditional input channel that can stream user speech to
the navigation system in the future.
4 Candidate Navigation Systems
This framework can be used to evaluate a variety
of navigation systems. Route navigation has been
an interesting research topic for researchers in both
geoinformatics and computational linguistics alike.
Several navigation prototype systems have been de-
veloped over the years. Although there are several
systems that do not use language as a means of com-
munication for navigation tasks (instead using geo-
tagged photographs (Beeharee and Steed, 2006; Hi-
ley et al, 2008), haptics (Bosman et al, 2003), mu-
sic (Holland et al, 2002; Jones et al, 2008), etc), we
focus on systems that generate instructions in natu-
ral language. Therefore, our framework does not in-
clude systems that generate routes on 2D/3D maps
as navigation aids.
Systems that generate text/speech can be further
classified as follows:
? ?A priori? systems: these systems generate
route instructions prior to the users touring the
route. These systems describe the entire route
before the user starts navigating. Several web
services exist that generate such lists of step-
by-step instructions (e.g. Google/Bing direc-
tions).
? ?In-situ? or incremental route instruction sys-
tems: these systems generate route instructions
incrementally along the route. e.g. CORAL
(Dale et al, 2003). They keep track of the
user?s location and issue the next instruction
when the user reaches the next node on the
planned route. The next instruction tells the
user how to reach the new next node. Some
systems do not keep track of the user, but re-
quire the user to request the next instruction
when they reach the next node.
? Interactive navigation systems: these systems
are both incremental and interactive. e.g.
DeepMap (Malaka and Zipf, 2000). These
systems keep track of the user?s location and
proactively generate instructions based on user
proximity to the next node. In addition, they
can interact with users by asking them ques-
tions about entities in their viewshed. For ex-
ample ?Can you see a tower at about 100 feet
away??. Questions like these will let the system
assess the user?s location and thereby adapt its
instruction to the situated context.
5 Evaluation metrics
Navigation systems can be evaluated using two
kinds of metrics using this framework. Objective
metrics such as time taken by the user to finish
each navigation task and the game, distance trav-
elled, number of wrong turns, etc. can be directly
measured from the environment. Subjective met-
rics based on each user?s ratings of different features
of the system can be obtained through user satisfac-
tion questionnaires. In our framework, users are re-
quested to fill in a questionnaire at the end of the
game. The questionnaire consists of questions about
the game, the buddy, and the user himself, for exam-
ple:
? Was the game engaging?
? Would you play it again (i.e. another similar
gameworld)?
? Did your buddy help you enough?
52
Figure 2: Snapshot of the web client
? Were the buddy instructions easy to under-
stand?
? Were the buddy instructions ever wrong or mis-
placed?
? If you had the chance, will you choose the same
buddy in the next game?
? How well did you know the neighbourhood of
the gameworld before the game?
6 Evaluation scenarios
We aim to evaluate navigation systems under a vari-
ety of scenarios.
? Uncertain GPS: GPS positioning available in
smartphones is erroneous (Zandbergen and
Barbeau, 2011). Therefore, one scenario for
evaluation would be to test how robustly nav-
igation systems handle erroneous GPS signals
from the user?s end.
? Output modalities: the output of navigation
systems can be presented in two modalities:
text and speech. While speech may enable a
hands-free eyes-free navigation, text displayed
on navigation aids like smartphones may in-
crease cognitive load. We therefore believe it
will be interesting to evaluate the systems in
both conditions and compare the results.
? Noise in user speech: for systems that take
as input user speech, it is important to handle
noise in such a channel. Noise due to wind and
traffic is most common in pedestrian scenarios.
Scenarios with different levels of noise settings
can be evaluated.
? Adaptation to users: returning users may have
learned the layout of the game world. An inter-
esting scenario is to examine how navigation
systems adapt to user?s increasing spatial and
visual knowledge.
Errors in GPS positioning of the user and noise
in user speech can be simulated at the server end,
thereby creating a range of challenging scenarios to
evaluate the robustness of the systems.
7 The Shared Challenge
We plan to organise a shared challenge for outdoor
pedestrian route instruction generation, in which a
variety of systems can be evaluated. Participating
research teams will be able to use our interfaces
and modules to develop navigation systems. Each
team will be provided with a development toolkit
53
and documentation to setup the framework in their
local premises for development purposes. Devel-
oped systems will be hosted on our challenge server
and a web based evaluation will be organised in con-
sultation with the research community (Janarthanam
and Lemon, 2011).
8 Demonstration system
At the demonstration, we will present the evaluation
framework along with a demo navigation dialogue
system. The web-based client will run on a laptop
using a high-speed broadband connection. The nav-
igation system and other server modules will run on
a remote server.
Acknowledgments
The research has received funding from the
European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant
agreement no. 216594 (SPACEBOOK project
www.spacebookproject.org).
References
Ashweeni K. Beeharee and Anthony Steed. 2006. A nat-
ural wayfinding exploiting photos in pedestrian navi-
gation systems. In Proceedings of the 8th conference
on Human-computer interaction with mobile devices
and services (2006).
S. Bosman, B. Groenendaal, J. W. Findlater, T. Visser,
M. de Graaf, and Panos Markopoulos. 2003. Gen-
tleGuide: An Exploration of Haptic Output for Indoors
Pedestrian Guidance. In Proceedings of 5th Interna-
tional Symposium, Mobile HCI 2003, Udine, Italy.
D. Byron, A. Koller, J. Oberlander, L. Stoia, and
K. Striegnitz. 2007. Generating Instructions in Vir-
tual Environments (GIVE): A challenge and evaluation
testbed for NLG. In Proceedings of the Workshop on
Shared Tasks and Comparative Evaluation in Natural
Language Generation.
Robert Dale, Sabine Geldof, and Jean-Philippe Prost.
2003. CORAL : Using Natural Language Generation
for Navigational Assistance. In Proceedings of the
Twenty-Sixth Australasian Computer Science Confer-
ence (ACSC2003), 4th7th February, Adelaide, South
Australia.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In Proceedings of Inter-
speech/ICSLP, pages 1065?1068.
Harlan Hiley, Ramakrishna Vedantham, Gregory Cuel-
lar, Alan Liuy, Natasha Gelfand, Radek Grzeszczuk,
and Gaetano Borriello. 2008. Landmark-based pedes-
trian navigation from collections of geotagged photos.
In Proceedings of the 7th International Conference on
Mobile and Ubiquitous Multimedia (MUM) 2008.
S. Holland, D. Morse, and H. Gedenryd. 2002. Audio-
gps: Spatial audio navigation with a minimal atten-
tion interface. Personal and Ubiquitous Computing,
6(4):253?259.
Srini Janarthanam and Oliver Lemon. 2009. A User Sim-
ulation Model for learning Lexical Alignment Policies
in Spoken Dialogue Systems. In European Workshop
on Natural Language Generation.
Srini Janarthanam and Oliver Lemon. 2011. The
GRUVE Challenge: Generating Routes under Uncer-
tainty in Virtual Environments. In Proceedings of
ENLG / Generation Challenges.
M. Jones, S. Jones, G. Bradley, N. Warren, D. Bainbridge,
and G. Holmes. 2008. Ontrack: Dynamically adapt-
ing music playback to support navigation. Personal
and Ubiquitous Computing, 12(7):513?525.
A. Koller, J. Moore, B. Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared Task Proposal: Instruction Giving in Virtual
Worlds. In Workshop on Shared Tasks and Compar-
ative Evaluation in Natural Language Generation.
Rainer Malaka and Er Zipf. 2000. Deep Map - chal-
lenging IT research in the framework of a tourist in-
formation system. In Information and Communication
Technologies in Tourism 2000, pages 15?27. Springer.
D. McGookin, R. Cole, and S. Brewster. 2010. Vir-
tual navigator: Developing a simulator for independent
route learning. In Proceedings of Workshop on Haptic
Audio Interaction Design 2010, Denmark.
Kai-Florian Richter and Matt Duckham. 2008. Simplest
instructions: Finding easy-to-describe routes for navi-
gation. In Proceedings of the 5th international confer-
ence on Geographic Information Science.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. The Knowledge Engi-
neering Review, 21:97?126.
P. A. Zandbergen and S. J. Barbeau. 2011. Positional
accuracy of assisted gps data from high-sensitivity
gps-enabled mobile phones. Journal of Navigation,
64(3):381?399.
54
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1660?1668,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evaluating a City Exploration Dialogue System Combining
Question-Answering and Pedestrian Navigation
Srinivasan Janarthanam1, Oliver Lemon1, Phil Bartie2, Tiphaine Dalmas2,
Anna Dickinson2, Xingkun Liu1, William Mackaness2, and Bonnie Webber2
1 The Interaction Lab, Heriot-Watt University
2 Edinburgh University
sc445@hw.ac.uk
Abstract
We present a city navigation and tourist
information mobile dialogue app with in-
tegrated question-answering (QA) and ge-
ographic information system (GIS) mod-
ules that helps pedestrian users to nav-
igate in and learn about urban environ-
ments. In contrast to existing mobile apps
which treat these problems independently,
our Android app addresses the prob-
lem of navigation and touristic question-
answering in an integrated fashion using
a shared dialogue context. We evaluated
our system in comparison with Samsung
S-Voice (which interfaces to Google nav-
igation and Google search) with 17 users
and found that users judged our system to
be significantly more interesting to inter-
act with and learn from. They also rated
our system above Google search (with the
Samsung S-Voice interface) for tourist in-
formation tasks.
1 Introduction
We present a mobile dialogue system (an Android
app) called Spacebook that addresses the problem
of pedestrian navigation and tourist information in
urban environments. There has been little prior
work that addresses these two problems - naviga-
tion and tourist information provision - in an inte-
grated way. By navigation, we refer to the prob-
lem of finding appropriate destinations to go to
and the task of wayfinding to reach them and by
tourist information provision we refer to the prob-
lem of meeting the informational needs of a user
about entities such as museums, statues and fa-
mous personalities. A dialogue system such as this
could serve as a personal tour guide to pedestrian
tourists as they walk around unknown cities. With
the proliferation of smartphones, there has been a
number of mobile apps developed to address these
problems. However these apps have the following
problems: first, they demand the user?s visual at-
tention because they predominantly present infor-
mation on a mobile screen. This can be dangerous
in urban environments, as well as being distract-
ing. Second, these apps address the problems of
navigation and tourist information independently
and therefore do not have a shared interaction con-
text. This means that users cannot switch between
information and navigation tasks in a natural and
fluid manner.
User1: Take me to the National Museum.
System2: The National Museum is about 300m away..
System3: At the KFC, turn left on to South Bridge
System4 : Near you is the statue of David Hume.
User2: Who is David Hume.
System5: David Hume was a Scottish philosopher....
User3: Tell me more about David Hume.
System6: He was one of the most important figures in..
System7: You should be able to see the museum ...
User4: Tell me more about the museum.
System8: The National Museum of Scotland is a....
Table 1: An example interaction with the evalu-
ated system
In contrast to many existing mobile apps,
Spacebook has a speech-only interface and ad-
dresses both problems in an integrated way. We
conjecture that with a speech-only interface, users
can immerse themselves in exploring the city,
and that because of the shared context they can
switch between navigation and tourist information
tasks more easily. Using the navigational context,
Spacebook pushes point-of-interest information
which can then initiate tourist information tasks
using the QA module. Table 1 presents an example
interaction with our system showing the integrated
use of navigation and question-answering capabil-
1660
ities. Utterances System4-8 show the system?s ca-
pability to push information about nearby points-
of-interest (PoI) during a navigation task and an-
swer followup questions using the QA system (in
utterances User2 and User3). The final 3 utter-
ances show a natural switch between navigation to
an entity and QA about that entity.
We investigate whether our system using a com-
bination of geographical information system (GIS)
and natural language processing (NLP) technolo-
gies would be a better companion to pedestrian
city explorers than the current state-of-the-art mo-
bile apps. We hypothesize that, (1) users will find
our speech-only interface to navigation efficient as
it allows them to navigate without having to re-
peatedly look at a map and (2), that users will
find a dialogue interface which integrates touris-
tic question-answering and navigation within a
shared context to be useful for finding information
about entities in the urban environment. We first
present some related work in section 2. We de-
scribe the architecture of the system in section 3.
We then present our experimental design, results
and analysis in sections 5, 6 and 7.
2 Related work
Mobile apps such as Siri, Google Maps Naviga-
tion, Sygic, etc. address the problem of naviga-
tion while apps like Triposo, Guidepal, Wikihood,
etc. address the problem of tourist information by
presenting the user with descriptive information
about various points of interest (PoI) in the city.
While some exploratory apps present snippets of
information about a precompiled list of PoIs, other
apps dynamically generate a list of PoIs arranged
based on their proximity to the users. Users can
also obtain specific information about PoIs using
Search apps. Also, since these navigation and ex-
ploratory/search apps do not address both prob-
lems in an integrated way, users need to switch
between them and therefore lose interaction con-
text.
While most apps address these two problems
independently, some like Google Now, Google
Field Trip, etc, mix navigation with exploration.
But such apps present information primarily vi-
sually on the screen for the user to read. Some
of these are available for download at the Google
Play Android app store1. Several dialogue and
natural language systems have addressed the issue
1https://play.google.com/store
of pedestrian navigation (Malaka and Zipf, 2000;
Raubal and Winter, 2002; Dale et al, 2003; Bar-
tie and Mackaness, 2006; Shroder et al, 2011;
Dethlefs and Cuaya?huitl, 2011). There has also
been recent interest in shared tasks for generat-
ing navigation instructions in indoor and urban en-
vironments (Byron et al, 2007; Janarthanam and
Lemon, 2011). Some dialogue systems deal with
presenting information concerning points of inter-
est (Ko et al, 2005; Kashioka et al, 2011) and in-
teractive question answering (Webb and Webber,
2009).
In contrast, Spacebook has the objective of
keeping the user?s cognitive load low and prevent-
ing users from being distracted (perhaps danger-
ously so) from walking in the city (Kray et al,
2003). Also, it allows users to interleave the two
sub-tasks seamlessly and can keep entities dis-
cussed in both tasks in shared context (as shown
in Table 1).
3 Architecture
The architecture of the Spacebook system is
shown in figure 1. Our architecture brings to-
gether Spoken Dialogue Systems (SDS), Geo-
graphic Information Systems (GIS) and Question-
Answering (QA) technologies (Janarthanam et al,
2012). Its essentially a spoken dialogue system
(SDS) consisting of an automatic speech recog-
niser (ASR), a semantic parser, an Interaction
Manager, an utterance generator and a text-to-
speech synthesizer (TTS). The GIS modules in
this architecture are the City Model, the Visibility
Engine, and the Pedestrian tracker. Users commu-
nicate with the system using a smartphone-based
client app (an Android app) that sends users? po-
sition, pace rate, and spoken utterances to the sys-
tem, and delivers synthesised system utterances to
the user.
Figure 1: System Architecture
1661
3.1 Dialogue interface
The dialogue interface consists of a speech recog-
nition module, an utterance parser, an interaction
manager, an utterance generator and a speech syn-
thesizer. The Nuance 9 speech recogniser with
a domain specific language model was used for
speech recognition. The recognised speech is cur-
rently parsed using a rule-based parser into dia-
logue acts and semantic content.
The Interaction Manager (IM) is the central
component of this architecture, which provides
the user with navigational instructions, pushes PoI
information and manages QA questions. It re-
ceives the user?s input in the form of a dialogue
act (DA), the user?s location (latitude and longi-
tude) and pace rate. Based on these inputs and the
dialogue context, it responds with system output
dialogue act, based on a dialogue policy. The IM
initiates the conversation with a calibration phase
where the user?s initial location and orientation are
obtained. The user can then initiate tasks that in-
terest him/her. These tasks include searching for
an entity (e.g. a museum or a restaurant), request-
ing navigation instructions to a destination, ask-
ing questions about the entities in the City Model,
and so on. When the user is mobile, the IM iden-
tifies points of interest2 on the route proximal to
the user. We call this ?PoI push?. The user is en-
couraged to ask for more information if he/she is
interested. The system also answers adhoc ques-
tions from the user (e.g. ?Who is David Hume??,
?What is the Old College??, etc) (see section 3.4).
Navigation instructions are given in-situ by ob-
serving user?s position continuously, in relation
to the next node (street junction) on the current
planned route, and they are given priority if in con-
flict with a PoI push at the same time. Navigation
instructions use landmarks near route nodes when-
ever possible (e.g. ?When you reach Clydesdale
Bank , keep walking forward?). The IM also in-
forms when users pass by recognisable landmarks,
just to reassure them that they are on track (e.g.
?You will pass by Tesco on the right?). In addition
to navigation instructions, the IM also answers
users? questions concerning the route, his/her lo-
cation, and location of and distance to the various
entities. Finally, the IM uses the city model?s Vis-
ibility Engine (VE) to determine whether the des-
tination is visible to the user (see section 3.3).
2Using high scoring ones when there are many, based on
tourist popularity ratings in the City Model.
The shared spatial and dialogue context em-
ploys a feature-based representation which is up-
dated every 1 second (for location), and after every
dialogue turn. Spatial context such as the user?s
coordinates, street names, PoIs and landmarks
proximal to the user, etc are used by PoI push-
ing and navigation. The dialogue context main-
tains the history of landmarks and PoIs pushed,
latest entities mentioned, etc to resolve anaphoric
references in navigation and QA requests, and to
deliver coherent dialogue responses. The IM re-
solves anaphoric references by keeping a record
of entities mentioned in the dialogue context. It
also engages in clarification sub-dialogues when
the speech recognition confidence scores are low.
The IM stores the name and type information for
each entity (such as landmark, building, etc) men-
tioned in navigation instructions and PoI pushes.
Subsequent references to these entities using ex-
pressions such as ?the museum?, ?the cafe? etc
are resolved by searching for the latest entity of
the given type. Pronouns are resolved to the last
mentioned entity.
The IM also switches between navigation, PoI
push, and QA tasks in an intelligent manner by
using the shared context to prioritise its utterances
from these different tasks. The utterance genera-
tor is a Natural Language Generation module that
translates the system DA into surface text which is
converted into speech using the Cereproc Text-to-
Speech Synthesizer using a Scottish female voice.
The only changes made were minor adjustments
to the pronunciation of certain place names.
3.2 Pedestrian tracker
Urban environments can be challenging with lim-
ited sky views, and hence limited line of sight
to satellites, in deep urban corridors. There is
therefore significant uncertainty about the user?s
true location reported by GNSS sensors on smart-
phones (Zandbergen and Barbeau, 2011). This
module improves on the reported user position
by combining smartphone sensor data (e.g. ac-
celerometer) with map matching techniques, to
determine the most likely location of the pedes-
trian (Bartie and Mackaness, 2012).
3.3 City Model
The City Model is a spatial database containing
information about thousands of entities in the city
of Edinburgh (Bartie and Mackaness, 2013). This
data has been collected from a variety of exist-
1662
ing resources such as Ordnance Survey, Open-
StreetMap, Google Places, and the Gazetteer for
Scotland. It includes the location, use class, name,
street address, and where relevant other properties
such as build date and tourist ratings. The model
also includes a pedestrian network (streets, pave-
ments, tracks, steps, open spaces) which is used
by an embedded route planner to calculate min-
imal cost routes, such as the shortest path. The
city model also consists of a Visibility Engine
that identifies the entities that are in the user?s
vista space (Montello, 1993). To do this it ac-
cesses a digital surface model, sourced from Li-
DAR, which is a 2.5D representation of the city
including buildings, vegetation, and land surface
elevation. The Visibility Engine uses this dataset
to offer a number of services, such as determining
the line of sight from the observer to nominated
points (e.g. which junctions are visible), and de-
termining which entities within the city model are
visible. Using these services, the IM determines if
the destination is visible or not.
3.4 Question-Answering server
The QA server currently answers a range of def-
inition and biographical questions such as, ?Tell
me more about the Scottish Parliament?, ?Who
was David Hume??, ?What is haggis??, and re-
quests to resume (eg. ?Tell me more?). QA
is also capable of recognizing out of scope re-
quests, that is, either navigation-related questions
that can be answered by computations from the
City Model and dealt with elsewhere in the sys-
tem (?How far away is the Scottish Parliament??,
?How do I get there??), or exploration queries
that cannot be handled yet (?When is the can-
non gun fired from the castle??). Question clas-
sification is entirely machine learning-based using
the SMO algorithm (Keerthi et al, 1999) trained
over 2013 annotated utterances. Once the question
has been typed, QA proceeds to focus detection
also using machine learning techniques (Mikhail-
sian et al, 2009). Detected foci include possi-
bly anaphoric expressions (?Who was he??, ?Tell
me more about the castle?). These expressions
are resolved against the dialogue history and ge-
ographical context. QA then proceeds to a tex-
tual search on texts from the Gazetteer of Scotland
(Gittings, 2012) and Wikipedia, and definitions
from WordNet glosses. The task is similar to TAC
KBP 2013 Entity Linking Track and named en-
tity disambiguation (Cucerzan, 2007). Candidate
answers are reranked using a trained confidence
score with the top candidate used as the final an-
swer. These are usually long, descriptive answers
and are provided as a flow of sentence chunks that
the user can interrupt (see table 2). The Interaction
Manager queries the QA model and pushes infor-
mation when a salient PoI is in the vicinity of the
user.
?Edinburgh?s most famous and historic thoroughfare,
which has formed the heart of the Old Town since
mediaeval times. The Royal Mile includes Castlehill,
the Lawnmarket, the Canongate and the Abbey Strand,
but, is officially known simply as the High Street.?
Table 2: QA output: query on ?Royal Mile?
3.5 Mobile client
The mobile client app, installed on an Android
smartphone (Samsung Galaxy S3), connects the
user to the dialogue system using a 3G data con-
nection. The client senses the user?s location us-
ing positioning technology using GNSS satellites
(GPS and GLONASS) which is sent to the dia-
logue system at the rate of one update every two
seconds. It also sends pace rate of the user from
the accelerometer sensor. In parallel, the client
also places a phone call using which the user com-
municates with the dialogue system.
4 Baseline system
The baseline system chosen for evaluation was
Samsung S-Voice, a state-of-the-art commercial
smartphone speech interface. S-Voice is a Sam-
sung Android mobile phone app that allows a user
to use the functionalities of device using a speech
interface. For example, the user can say ?Call
John? and it will dial John from the user?s con-
tacts. It launches the Google Navigation app when
users request directions and it activates Google
Search for open ended touristic information ques-
tions. The Navigation app is capable of providing
instructions in-situ using speech. We used the S-
Voice system for comparison because it provided
an integrated state-of-the-art interface to use both
a navigation app and also an information-seeking
app using the same speech interface. Users were
encouraged to use these apps using speech but
were allowed to use the GUI interface when us-
ing speech wasn?t working (e.g. misrecognition of
local names). Users obtained the same kind of in-
1663
formation (i.e. navigation directions, descriptions
about entities such as people, places, etc) from the
baseline system as they would from our system.
However, our system interacted with the user us-
ing the speech modality only.
5 Experimental design
Spacebook and the baseline were evaluated in the
summer of 2012. We evaluated both systems with
17 subjects in the streets of Edinburgh. There
were 11 young subjects (between 20 and 26 years,
mean=22 ? 2) and 6 older subjects (between 50
and 71 years, mean=61 ? 11). They were mostly
native English speakers (88%). 59% of the users
were regular smartphone users and their mean
overall time spent in the city was 76 months. The
test subjects had no previous experience with the
proposed system. They were recruited via email
adverts and mail shots. Subjects were given a task
sheet with 8 tasks in two legs (4 tasks per leg).
These tasks included both navigation and tourist
information tasks (see table 3). Subjects used our
system for one of the legs and the baseline system
for the other and the order was balanced. Each leg
took up to 30 mins to finish and the total duration
including questionnaires was about 1.5 hours. Fig-
ure 2 shows the route taken by the subjects. The
route is about 1.3 miles long. Subjects were fol-
lowed by the evaluator who made notes on their
behaviour (e.g. subject looks confused, subject
looks at or manipulates the phone, subject looks
around, etc).
Subjects filled in a demographic questionnaire
prior to the experiment. After each leg, they filled
in a system questionnaire (see appendix) rating
their experience. After the end of the experi-
ment, they filled out a comparative questionnaire
and were debriefed. They were optionally asked
to elaborate on their questionnaire ratings. Users
were paid ?20 after the experiment was over.
6 Results
Subjects were asked to identify tasks that they
thought were successfully completed. The per-
ceived task success rates of the two systems were
compared for each task using the Chi square test.
The results show that there is no statistically sig-
nificant difference between the two systems in
terms of perceived task success although the base-
line system had a better task completion rate in
tasks 1-3, 5 and 6. Our system performed better in
Figure 2: Task route
tourist information tasks (4, 7) (see table 4).
Task Our system Baseline p
T1 (N) 77.7 100 0.5058
T2 (TI) 88.8 100 0.9516
T3 (N) 100 100 NA
T4 (TI) 100 87.5 0.9516
T5 (N+TI) 62.5 100 0.1654
T6 (N+TI) 87.5 100 0.9516
T7 (TI) 100 55.5 0.2926
T8 (N) 75.0 88.8 0.9105
Table 4: % Perceived Task success - task wise
comparison (N - navigation task, TI - Tourist In-
formation task)
The system questionnaires that were filled out
by users after each leg were analysed. These
consisted of questions concerning each system to
be rated on a six point Likert scale (1-Strongly
Disagree, 2-Disagree, 3-Somewhat Disagree, 4-
Somewhat Agree, 5-Agree, 6-Strongly Agree).
The responses were paired and tested using a
Wilcoxon Sign Rank test. Median and Mode for
each system and significance in differences are
shown in table 5. Results show that although
our system is not performing significantly better
than the baseline system (SQ1-SQ10 except SQ7),
users seem to find it more understanding (SQ7)
and more interesting to interact with (SQ11) than
the baseline. We grouped the subjects by age
group and tested their responses. We found that
the young subjects (age group 20-26), also felt that
1664
Leg 1
(Task 1) Ask the system to guide you to the Red Fort restaurant.
(Task 2) You?ve heard that Mary Queen of Scots lived in Edinburgh. Find out about her.
(Task 3) Walk to the university gym.
(Task 4) Near the gym there is an ancient wall with a sign saying ?Flodden Wall?. Find out what that is.
Leg 2
(Task 5) Try to find John Knox House and learn about the man.
(Task 6) Ask the system to guide you to the Old College. What can you learn about this building?
(Task 7) Try to find out more about famous Edinburgh people and places, for example, David Hume,
John Napier, and Ian Rankin. Try to find information about people and places that you are personally
interested in or that are related to what you see around you.
(Task 8) Ask the system to guide you back to the Informatics Forum.
Table 3: Tasks for the user
they learned something new about the city using it
(SQ12) (p < 0.05) while the elderly (age group
50-71) didn?t. We also found statistically signifi-
cant differences in smartphone users rating for our
system on their learning compared to the baseline
(SQ12).
Subjects were also asked to choose between the
two systems given a number of requirements such
as ease of use, use for navigation, tourist infor-
mation, etc. There was an option to rank the sys-
tems equally (i.e. a tie). They were presented with
the same requirements as the system questionnaire
with one additional question - ?Overall which sys-
tem do you prefer?? (CQ0). Users? choice of sys-
tem based on a variety of requirements is shown
in table 6. Users? choice counts were tested us-
ing Chi-square test. Significant differences were
found in users? choice of system for navigation
and tourist information requirements. Users pre-
ferred the baseline system for navigation (CQ2)
and our system for touristic information (CQ3) on
the city. Although there was a clear choice of sys-
tems based on the two tasks, there was no signifi-
cant preference of one system over the other over-
all (CQ0). They chose our system as the most in-
teresting system to interact with (CQ11) and that
it was more informative than the baseline (CQ12).
Figure 3 shows the relative frequency between
user choices on comparative questions.
7 Analysis
Users found it somewhat difficult to navigate using
Spacebook (see comments in table 7). Although
the perceived task success shows that our system
was able to get the users to their destination and
there was no significant difference between the
two systems based on their questionnaire response
on navigation, they pointed out a number of issues
and suggested a number of modifications. Many
Figure 3: Responses to comparative questions
users noted that a visual map and the directional
arrow in the baseline system was helpful for nav-
igation. In addition, they noted that our system?s
navigation instructions were sometimes not satis-
factory. They observed that there weren?t enough
instructions coming from the system at street junc-
tions. They needed more confirmatory utterances
(that they are walking in the right direction) (5
users) and quicker recovery and notification when
walking the wrong way (5 users). They observed
that the use of street names was confusing some-
times. Some users also wanted a route summary
before the navigation instructions are given.
The problem with Spacebook?s navigation pol-
icy was that it did not, for example, direct the
user via easily visible landmarks (e.g. ?Head to-
wards the Castle?), and relies too much on street
names. Also, due to the latency in receiving GPS
information, the IM sometimes did not present in-
structions soon enough during evaluation. Some-
times it received erroneous GPS information and
therefore got the user?s orientation wrong. These
problems will be addressed in the future version.
Some users did find navigation instructions use-
ful because of the use of proximal landmarks such
1665
Question B Mode B Median S Mode S Median p
SQ1 - Ease of use 4 4 5 4 0.8207
SQ2 - Navigation 4 4 5 4 0.9039
SQ3 - Tourist Information 2 3 4 4 0.07323
SQ4 - Easy to understand 5 5 5 5 0.7201
SQ5 - Useful messages 5 4 5 4 1
SQ6 - Response time 5 5 2 2 0.2283
SQ7 - Understanding 3 3 5 4 0.02546
SQ8 - Repetitive 2 3 2 3 0.3205
SQ9 - Aware of user environment 5 5 4 4 0.9745
SQ10 - Cues for guidance 5 5 5 5 0.1371
SQ11 - Interesting to interact with 5 4 5 5 0.01799
SQ12 - Learned something new 5 4 5 5 0.08942
Table 5: System questionnaire responses (B=Baseline, S=our system)
Task Baseline Our system Tie p-
Preferred Preferred value
CQ0 23.52 35.29 41.17 0.66
CQ1 35.29 29.41 35.29 0.9429
CQ2 64.70 0 35.29 0.004
CQ3 17.64 64.70 17.64 0.0232
CQ4 35.29 29.41 23.52 0.8187
CQ5 23.52 52.94 23.52 0.2298
CQ6 23.52 29.41 35.29 0.8187
CQ7 17.64 47.05 35.29 0.327
CQ8 29.41 23.52 47.05 0.4655
CQ9 29.41 52.94 17.64 0.1926
CQ10 47.05 29.41 23.52 0.4655
CQ11 5.88 76.47 17.64 0.0006
CQ12 0 70.58 29.41 0.005
Table 6: User?s choice on comparative questions
(CQ are the same questions as SQ but requesting
a ranking of the 2 systems)
as KFC, Tesco, etc. (popular chain stores). Some
users also suggested that our system should have
a map and that routes taken should be plotted on
them for reference. Based on the ratings and ob-
servations made by the users, we conclude that our
first hypothesis that Spacebook would be more ef-
ficient for navigation than the baseline because of
its speech-only interface was inconclusive. We be-
lieve so because users? poor ratings for Spacebook
may be due to the current choice of dialogue pol-
icy for navigation. It may be possible to reassure
the user with a better dialogue policy with just the
speech interface. However, this needs further in-
vestigation.
Users found the information-search task inter-
esting and informative when they used Spacebook
(see sample user comments in table 8). They
also found push information on nearby PoIs un-
expected and interesting as they would not have
found them otherwise. Many users believed that
this could be an interesting feature that could help
tourists. They also found that asking questions and
finding answers was much easier with Spacebook
compared to the baseline system, where some-
times users needed to type search keywords in.
Another user observation was that they did not
have to stop to listen to information presented
by our system (as it was in speech) and could
carry on walking. However, with the baseline sys-
tem, they had to stop to read information off the
screen. Although users in general liked the QA
feature, many complained that Spacebook spoke
too quickly when it was presenting answers. Some
users felt that the system might lose context of the
navigation task if presented with a PoI question.
In contrast, some others noted Spacebook?s ability
to interleave the two tasks and found it to be an
advantage.
Users? enthusiasm for our system was observed
when (apart from the points of interest that were
in the experimental task list) they also asked spon-
taneous questions about James Watt, the Talbot
Rice gallery, the Scottish Parliament and Edin-
burgh Castle. Some of the PoIs that the system
pushed information about were the Royal College
of Surgeons, the Flodden Wall, the Museum of
Childhood, and the Scottish Storytelling Centre.
Our system answered a mean of 2.5 out of 6.55
questions asked by users in leg 1 and 4.88 out of
8.5 questions in leg 2. Please note that an utter-
ance is sent to QA if it is not parsed by the parser
and therefore some utterances may not be legit-
mate questions themselves. Users were pushed a
mean of 2.88 and 6.37 PoIs during legs 1 and 2.
There were a total of 17 ?tell me more? requests
requesting the system to present more information
(mean=1.35 ? 1.57).
Evaluators who followed the subjects noted that
the subjects felt difficulty using the baseline sys-
tem as they sometimes struggled to see the screen
1666
1. ?It?s useful when it says ?Keep walking? but it should say it more often.?
2. ?[Your system] not having a map, it was sometimes difficult to check how aware it was of my environment.?
3. ?[Google] seemed to be easier to follow as you have a map as well to help.?
4. ?It told me I had the bank and Kentucky Fried Chicken so I crossed the road because I knew it?d be somewhere over
beside them. I thought ?OK, great. I?m going the right way.? but then it didn?t say anything else. I like those kind of
directions because when it said to go down Nicolson Street I was looking around trying to find a street sign.?
5. ?The system keeps saying ?when we come to a junction, I will tell you where to go?, but I passed junctions and it
didn?t say anything. It should say ?when you need to change direction, I will tell you.??
6. ?I had to stop most of the times for the system to be aware of my position. If walking very slowly, its awareness of
both landmarks and streets is excellent.?
Table 7: Sample user comments on the navigation task
1. ?Google doesn?t *offer* any information. I would have to know what to ask for...?
2. ?Since many information is given without being asked for (by your system), one can discover new places and
landmarks even if he lives in the city. Great feature!!?
3. ?I didn?t feel confident to ask [your system] a question and still feel it would remember my directions?
4. ?Google could only do one thing at a time, you couldn?t find directions for a place whilst learning more.?
5. ?If she talked a little bit slower [I would use the system for touristic purposes]. She just throws masses of information
really, really quickly.?
Table 8: Sample user comments on the tourist information task
in bright sunlight. They sometimes had difficulty
identifying which way to go based on the route
plotted on the map. In comparison, subjects did
not have to look at the screen when they used
our system. Based on the ratings and observa-
tions made by the users about our system?s tourist
information features such as answering questions
and pushing PoI information, we have support for
our second hypothesis: that users find a dialogue
interface which integrates question-answering and
navigation within a shared context to be useful for
finding information about entities in the urban en-
vironment.
8 Future plans
We plan to extend Spacebook?s capabilities to ad-
dress other challenges in pedestrian navigation and
tourist information. Many studies have shown
that visible landmarks provide better cues for nav-
igation than street names (Ashweeni and Steed,
2006; Hiley et al, 2008). We will use visible
landmarks identified using the visibility engine to
make navigation instructions more effective, and
we plan to include entities in dialogue and visual
context as candidates for PoI push, and to imple-
ment an adaptive strategy that will estimate user
interests and push information that is of interest
to them. We are also taking advantage of user?s
local knowledge of the city to present navigation
instructions only for the part of the route that the
user does not have any knowledge of. These fea-
tures, we believe, will make users? experience of
the interface more pleasant, useful and informa-
tive.
9 Conclusion
We presented a mobile dialogue app called Space-
book to support pedestrian users in navigation
and tourist information gathering in urban envi-
ronments. The system is a speech-only interface
and addresses navigation and tourist information
in an integrated way, using a shared dialogue con-
text. For example, using the navigational context,
Spacebook can push point-of-interest information
which can then initiate touristic exploration tasks
using the QA module.
We evaluated the system against a state-of-the-
art baseline (Samsung S-Voice with Google Navi-
gation and Search) with a group of 17 users in the
streets of Edinburgh. We found that users found
Spacebook interesting to interact with, and that
it was their system of choice for touristic infor-
mation exploration tasks. These results were sta-
tistically significant. Based on observations and
user ratings, we conclude that our speech-only
system was less preferred for navigation and more
preferred for tourist information tasks due to fea-
tures such as PoI pushing and the integrated QA
module, when compared to the baseline system.
Younger users, who used Spacebook, even felt that
they learned new facts about the city.
Acknowledgments
The research leading to these results was funded by the Eu-
ropean Commission?s Framework 7 programme under grant
1667
agreement no. 270019 (SPACEBOOK project).
References
K. B. Ashweeni and A. Steed. 2006. A natural
wayfinding exploiting photos in pedestrian naviga-
tion systems. In Proceedings of the 8th conference
on Human-computer interaction with mobile devices
and services.
P. Bartie and W. Mackaness. 2006. Development
of a speech-based augmented reality system to sup-
port exploration of cityscape. Transactions in GIS,
10:63?86.
P. Bartie and W. Mackaness. 2012. D3.4 Pedestrian
Position Tracker. Technical report, The SPACE-
BOOK Project (FP7/2011-2014 grant agreement no.
270019).
P. Bartie and W. Mackaness. 2013. D3.1.2 The Space-
Book City Model. Technical report, The SPACE-
BOOK Project (FP7/2011-2014 grant agreement no.
270019).
D. Byron, A. Koller, J. Oberlander, L. Stoia, and
K. Striegnitz. 2007. Generating Instructions in Vir-
tual Environments (GIVE): A challenge and evalua-
tion testbed for NLG. In Proceedings of the Work-
shop on Shared Tasks and Comparative Evaluation
in Natural Language Generation.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings
of EMNLP-CoNLL.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL : Using
Natural Language Generation for Navigational As-
sistance. In Proceedings of ACSC2003, Australia.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011. Hierar-
chical Reinforcement Learning and Hidden Markov
Models for Task-Oriented Natural Language Gener-
ation. In Proc. of ACL.
B. Gittings. 2012. The Gazetteer for Scotland -
http://www.scottish-places.info.
H. Hiley, R. Vedantham, G. Cuellar, A. Liuy,
N. Gelfand, R. Grzeszczuk, and G. Borriello. 2008.
Landmark-based pedestrian navigation from collec-
tions of geotagged photos. In Proceedings of the
7th Int. Conf. on Mobile and Ubiquitous Multimedia
(MUM).
S. Janarthanam and O. Lemon. 2011. The GRUVE
Challenge: Generating Routes under Uncertainty in
Virtual Environments. In Proceedings of ENLG.
S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mack-
aness, T. Dalmas, and J. Goetze. 2012. Integrat-
ing location, visibility, and Question-Answering in
a spoken dialogue system for Pedestrian City Explo-
ration. In Proc. of SIGDIAL 2012, S. Korea.
H. Kashioka, T. Misu, E. Mizukami, Y. Shiga,
K. Kayama, C. Hori, and H. Kawai. 2011. Multi-
modal Dialog System for Kyoto Sightseeing Guide.
In Asia-Pacific Signal and Information Processing
Association Annual Summit and Conference.
S.S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 1999. Improvements to Platt?s
SMO Algorithm for SVM Classifier Design. Neural
Computation, 3:637?649.
J. Ko, F. Murase, T. Mitamura, E. Nyberg, M. Tateishi,
I. Akahori, and N. Hataoka. 2005. CAMMIA: A
Context-Aware Spoken Dialog System for Mobile
Environments. In IEEE ASRU Workshop.
C. Kray, K. Laakso, C. Elting, and V. Coors. 2003.
Presenting Route Instructions on Mobile Devices.
In Proceedings of IUI 03, Florida.
R. Malaka and A. Zipf. 2000. Deep Map - challenging
IT research in the framework of a tourist information
system. In Information and Communication Tech-
nologies in Tourism 2000, pages 15?27. Springer.
A. Mikhailsian, T. Dalmas, and R. Pinchuk. 2009.
Learning foci for question answering over topic
maps. In Proceedings of ACL 2009.
D. Montello. 1993. Scale and multiple psychologies
of space. In A. U. Frank and I. Campari, editors,
Spatial information theory: A theoretical basis for
GIS.
M. Raubal and S. Winter. 2002. Enriching wayfinding
instructions with local landmarks. In Second Inter-
national Conference GIScience. Springer, USA.
C.J. Shroder, W. Mackaness, and B. Gittings. 2011.
Giving the Right Route Directions: The Require-
ments for Pedestrian Navigation Systems. Transac-
tions in GIS, pages 419?438.
N. Webb and B. Webber. 2009. Special Issue on Inter-
active Question Answering: Introduction. Natural
Language Engineering, 15(1):1?8.
P. A. Zandbergen and S. J. Barbeau. 2011. Posi-
tional Accuracy of Assisted GPS Data from High-
Sensitivity GPS-enabled Mobile Phones. Journal of
Navigation, 64(3):381?399.
1668
Generation under Uncertainty
Oliver Lemon
Heriot-Watt University
Edinburgh, United Kingdom
o.lemon@hw.ac.uk
Srini Janarthanam
Edinburgh University
Edinburgh, United Kingdom
s.janarthanam@ed.ac.uk
Verena Rieser
Edinburgh University
Edinburgh, United Kingdom
vrieser@inf.ed.ac.uk
Abstract
We invite the research community to con-
sider challenges for NLG which arise from
uncertainty. NLG systems should be able
to adapt to their audience and the genera-
tion environment in general, but often the
important features for adaptation are not
known precisely. We explore generation
challenges which could employ simulated
environments to study NLGwhich is adap-
tive under uncertainty, and suggest possi-
ble metrics for such tasks. It would be par-
ticularly interesting to explore how differ-
ent planning approaches to NLG perform
in challenges involving uncertainty in the
generation environment.
1 Introduction
We would like to highlight the design of NLG sys-
tems for environments where there may be incom-
plete or faulty information, where actions may not
always have the same results, and where there may
be tradeoffs between the different possible out-
comes of actions and plans.
There are various sources of uncertainty in sys-
tems which employ NLG techniques, for example:
? the current state of the user / audience (e.g.
their knowledge, preferred vocabulary, goals,
preferences....),
? the likely user reaction to the generated out-
put,
? the behaviour of related components (e.g. a
surface realiser, or TTS module),
? noise in the environment (for spoken output),
? ambiguity of the generated output.
The problem here is to generate output that
takes these types of uncertainty into account ap-
propriately. For example, you may need to choose
a referring expression for a user, even though you
are not sure whether they are an expert or novice in
the domain. In addition, the next time you speak
to that user, you need to adapt to new informa-
tion you have gained about them (Janarthanam and
Lemon, 2010). The issue of uncertainty for refer-
ring expression generation has been discussed be-
fore by (Reiter, 1991; Horacek, 2005).
Another example is in planning an Information
Presentation for a user, when you cannot know
with certainty how they will respond to it (Rieser
and Lemon, 2009; Rieser et al, 2010). In the worst
case, you may even be uncertain about the user?s
goals or information needs (as in ?POMDP? ap-
proaches to dialogue management (Young et al,
2009; Henderson and Lemon, 2008a)), but you
still need to generate output for them in an appro-
priate way.
In particular, in interactive applications of NLG:
? each NLG action changes the environment
state or context,
? the effect of each NLG action is uncertain.
Several recent approaches describe NLG tasks
as different kinds of planning, e.g. (Koller and Pet-
rick, 2008; Rieser et al, 2010; Janarthanam and
Lemon, 2010), or as contextual decision making
according to a cost function (van Deemter, 2009).
It would be very interesting to explore how differ-
ent approaches perform in NLG problems where
different types of uncertainty are present in the
generation environment.
In the following we discuss possible genera-
tion challenges arising from such considerations,
which we hope will lead to work on an agreed
shared challenge in this research community. In
section 2 we briefly review recent work showing
that simulated environments can be used to evalu-
ate generation under uncertainty, and in section 3
we discuss some possible metrics for such tasks.
Section 4 concludes by considering how a useful
generation challenge could be constructed using
similar methods.
2 Generation in Uncertain Simulated
Environments
Finding the best (or ?optimal?) way to generate
under uncertainty requires exploring the possible
outcomes of actions in stochastic environments.
Therefore, related research on Dialogue Strategy
learning has used data-driven simulated environ-
ments as a cheap and efficient way to explore un-
certainty (Lemon and Pietquin, 2007). However,
building good simulated environments is a chal-
lenge in its own right, as we illustrate in the fol-
lowing using the examples of Information Presen-
tation and Referring Expression Generation. We
also point out the additional challenges these sim-
ulations have to face when being used for NLG.
2.1 User Simulations for Information
Presentation
User Simulations can provide a model of proba-
ble, but uncertain, user reactions to NLG actions,
and we propose that they are a useful potential
direction for exploring and evaluate different ap-
proaches to handling uncertainty in generation.
User Simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al, 2007). A user simulation for
Information Presentation is very similar, in that it
is a predictive model of the most likely next user
act. 1 However, this NLG predicted user act does
not actually change the overall dialogue state (e.g.
by filling slots) but it only changes the generator
state. In other words, this NLG user simulation
tells us what the user is most likely to do next, if
we were to stop generating now.
In addition to the challenges of building user
simulations for learning Dialogue policies, e.g.
modelling, evaluation, and available data sets
(Lemon and Pietquin, 2007), a crucial decision for
NLG is the level of detail needed to train sensible
1Similar to the internal user models applied in recent
work on POMDP (Partially Observable Markov Decision
Process) dialogue managers (Young et al, 2007; Henderson
and Lemon, 2008b; Gasic et al, 2008) for estimation of user
act probabilities.
policies. While high-level dialogue act descrip-
tions may be sufficient for dialogue policies, NLG
decisions may require a much finer level of detail.
The finer the required detail of user reactions, the
more data is needed to build data-driven simula-
tions.
For content selection in Information Presen-
tation tasks (choosing presentation strategy and
number of attributes), for example, the level of de-
scription can still be fairly abstract. We were most
interested in probability distributions over the fol-
lowing possible user reactions:
1. select: the user chooses one of the pre-
sented items, e.g. ?Yes, I?ll take that one.?.
This reply type indicates that the informa-
tion presentation was sufficient for the user
to make a choice.
2. addInfo: The user provides more at-
tributes, e.g. ?I want something cheap.?. This
reply type indicates that the user has more
specific requests, which s/he wants to specify
after being presented with the current infor-
mation.
3. requestMoreInfo: The user asks for
more information, e.g. ?Can you recommend
me one??, ?What is the price range of the
last item??. This reply type indicates that the
system failed to present the information the
user was looking for.
4. askRepeat: The user asks the system to
repeat the samemessage again, e.g. ?Can you
repeat??. This reply type indicates that the
utterance was either too long or confusing for
the user to remember, or the TTS quality was
not good enough, or both.
5. silence: The user does not say anything.
In this case it is up to the system to take ini-
tiative.
6. hangup: The user closes the interaction.
We have built user simulations using n-gram
models of system (s) and user (u) acts, as first
introduced by (Eckert et al, 1997). In order to
account for data sparsity, we apply different dis-
counting (?smoothing?) techniques including au-
tomatic back-off, using the CMU Statistical Lan-
guage Modelling toolkit (Clarkson and Rosenfeld,
1997). For example we have constructed a bi-
gram model2 for the users? reactions to the sys-
tem?s IP structure decisions (P (au,t|IPs,t)), and
a tri-gram (i.e. IP structure + attribute choice)
model for predicting user reactions to the system?s
combined IP structure and attribute selection deci-
sions: P (au,t|IPs,t, attributess,t).
We have evaluated the performance of these
models by measuring dialogue similarity to the
original data, based on the Kullback-Leibler (KL)
divergence, as also used by e.g. (Cuaya?huitl et al,
2005; Jung et al, 2009; Janarthanam and Lemon,
2009). We compared the raw probabilities as ob-
served in the data with the probabilities generated
by our n-gram models using different discounting
techniques for each context. All the models have a
small divergence from the original data (especially
the bi-gram model), suggesting that they are rea-
sonable simulations for training and testing NLG
policies (Rieser et al, 2010).
2.2 Other Simulated Components
In some systems, NLG decisions may also depend
on related components, such as the database, sub-
sequent generation steps, or the Text-to-Speech
module for spoken generation. Building simula-
tions for these components to capture their inher-
ent uncertainty, again, is an interesting challenge.
For example, one might want to adapt the gen-
erated output according to the predicted TTS qual-
ity. Therefore, one needs a model of the expected/
predicted TTS quality for a TTS engine (Boidin et
al., 2009).
Furthermore, NLG decisions might be inputs
to a stochastic sentence realiser, such as SPaRKy
(Stent et al, 2004). However, one might not have
a fully trained stochastic sentence realiser for this
domain (yet). In (Rieser et al, 2010) we therefore
modelled the variance as observed in the top rank-
ing SPaRKy examples.
2.3 Generating Referring Expressions under
uncertainty
In this section, we present an example user simu-
lation (US) model, that simulates the dialogue be-
haviour of users who react to referring expressions
depending on their domain knowledge. These ex-
ternal simulation models are different from inter-
nal user models used by dialogue systems. In
2Where au,t is the predicted next user action at time t,
IPs,t was the system?s Information Presentation action at t,
and attributess,t is the set of attributes selected by the sys-
tem at t.
particular, such models must be sensitive to a
system?s choices of referring expressions. The
simulation has a statistical distribution of in-built
knowledge profiles that determines the dialogue
behaviour of the user being simulated. Uncer-
tainty arises because if the user does not know a
referring expression, then he is more likely to re-
quest clarification. If the user is able to interpret
the referring expressions and identify the refer-
ences then he is more likely to follow the system?s
instruction. This behaviour is simulated by the ac-
tion selection models described below.
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). We used two kinds of action selection mod-
els: a corpus-driven statistical model and a hand-
coded rule-based model.
2.4 Corpus-driven action selection model
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). The US produces a clarification request cr
based on the class of the referent C(Ri), type of
the referring expression Ti, and the current domain
knowledge of the user for the referring expression
DKu,t(Ri, Ti). Domain entities whose jargon ex-
pressions raised clarification requests in the cor-
pus were listed and those that had more than the
mean number of clarification requests were clas-
sified as difficult and others as easy enti-
ties (for example, ?power adaptor? is easy - all
users understood this expression, ?broadband fil-
ter? is difficult). Clarification requests are
produced using the following model.
P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) ? RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment actionEAu,t
is generated using the following model.
P (EAu,t|As,t) if Au,t! = cr(Ri, Ti)
Finally, the user action is an instruction re-
sponse which is determined by the system ac-
tion As,t. Instruction responses can be ei-
ther provide info, acknowledgement or other
based on the system?s instruction.
P (Au,t = ir|EAu,t, As,t)
All the above models were trained on our cor-
pus data using maximum likelihood estimation
and smoothed using a variant of Witten-Bell dis-
counting. According to the data, clarification re-
quests are much more likely when jargon expres-
sions are used to refer to the referents that be-
long to the difficult class and which the user
doesn?t know about. When the system uses ex-
pressions that the user knows, the user gener-
ally responds to the instruction given by the sys-
tem. These user simulation models have been
evaluated and found to produce behaviour that is
very similar to the original corpus data, using the
Kullback-Leibler divergence metric (Janarthanam
and Lemon, 2010).
3 Metrics
Here we discuss some possible evaluation met-
rics that will allow different approaches to NLG
under uncertainty to be compared. We envisage
that other metrics should be explored, in particular
those measuring adaptivity of various types.
3.1 Adaptive Information Presentation
Given a suitable corpus, a data-driven evaluation
function can be constructed, using a stepwise lin-
ear regression, following the PARADISE frame-
work (Walker et al, 2000).
For example, in (Rieser et al, 2010) we
build a model which selects the features which
significantly influenced the users? ratings for
NLG strategies in a Wizard-of-Oz study. We
also assign a value to the user?s reactions
(valueUserReaction), similar to optimising task
success for DM (Young et al, 2007). This re-
flects the fact that good Information Presentation
strategies should help the user to select an item
(valueUserReaction = +100) or provide more
constraints addInfo (valueUserReaction =
?0), but the user should not do anything else
(valueUserReaction = ?100). The regression
in equation 1 (R2 = .26) indicates that users? rat-
ings are influenced by higher level and lower level
features: Users like to be focused on a small set
of database hits (where #DBhits ranges over [1-
100]), which will enable them to choose an item
(valueUserReaction), while keeping the IP ut-
terances short (where #sentence was in the range
[2-18]):
Reward = (?1.2)?#DBhits (1)
+(.121)? valueUserReaction
?(1.43)?#sentence
3.2 Measuring Adaptivity of Referring
Expressions
We have also designed a metric for the goal of
adapting referring expressions to each user?s do-
main knowledge. We present the Adaptation Ac-
curacy score AA that calculates how accurately
the agent chose the expressions for each referent
r, with respect to the user?s knowledge. Appro-
priateness of an expression is based on the user?s
knowledge of the expression. So, when the user
knows the jargon expression for r, the appropri-
ate expression to use is jargon, and if s/he doesn?t
know the jargon, an descriptive expression is ap-
propriate. Although the user?s domain knowledge
is dynamically changing due to learning, we base
appropriateness on the initial state, because our
objective is to adapt to the initial state of the user
DKu,initial. However, in reality, designers might
want their system to account for user?s changing
knowledge as well. We calculate accuracy per ref-
erent RAr as the ratio of number of appropriate
expressions to the total number of instances of the
referent in the dialogue. We then calculate the
overall mean accuracy over all referents as shown
below.
RAr =
#(appropriate expressions(r))
#(instances(r))
AdaptationAccuracyAA = 1#(r)?rRAr
4 Conclusion
We have invited the research community to con-
sider challenges for NLG which arise from uncer-
tainty. We argue that NLG systems, like dialogue
managers, should be able to adapt to their audi-
ence and the generation environment. However,
often the important features for adaptation are not
precisely known. We then summarised 2 potential
directions for such challenges ? example genera-
tion tasks which employ simulated uncertain en-
vironments to study adaptive NLG, and discussed
some possible metrics for such tasks. We hope
that this will lead to discussions on a shared chal-
lenge allowing comparison of different approaches
to NLG with respect to how well they handle un-
certainty.
Acknowledgments
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
Cedric Boidin, Verena Rieser, Lonneke van der Plas,
Oliver Lemon, and Jonathan Chevelu. 2009. Pre-
dicting how it sounds: Re-ranking alternative in-
puts to TTS using latent variables (forthcoming). In
Proc. of Interspeech/ICSLP, Special Session on Ma-
chine Learning for Adaptivity in Spoken Dialogue
Systems.
P.R. Clarkson and R. Rosenfeld. 1997. Statisti-
cal Language Modeling Using the CMU-Cambridge
Toolkit. In Proc. of ESCA Eurospeech.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and S. Young. 2008. Training and
Evaluation of the HIS POMDP Dialogue System in
Noise. In Proc. of SIGdial Workshop on Discourse
and Dialogue.
James Henderson and Oliver Lemon. 2008a. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proceedings of
ACL.
James Henderson and Oliver Lemon. 2008b. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proc. of ACL.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In ENLG.
Srinivasan Janarthanam and Oliver Lemon. 2009. A
Two-tier User Simulation Model for Reinforcement
Learning of Adaptive Referring Expression Genera-
tion Policies. In Proc. of SIGdial.
Srini Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: Referring expression
generation in spoken dialogue systems. In Proceed-
ings of ACL. (to appear).
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Computer, Speech & Lan-
guage, 23:479?509.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Oliver Lemon and Olivier Pietquin. 2007. Machine
learning for spoken dialogue systems. In Inter-
speech.
E. Reiter. 1991. Generating Descriptions that Exploit a
User?s Domain Knowledge. In R. Dale, C. Mellish,
and M. Zock, editors, Current Research in Natural
Language Generation, pages 257?285. Academic
Press.
Verena Rieser and Oliver Lemon. 2009. Natural lan-
guage generation as planning under uncertainty for
spoken dialogue systems. In EACL.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proceedings of ACL. (to appear).
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
Kees van Deemter. 2009. What game theory can do
for NLG: the case of vague language. In 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG).
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2009. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language. To appear.
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 124?131,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Adaptive Referring Expression Generation in Spoken Dialogue Systems:
Evaluation with Real Users
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
s.janarthanam@ed.ac.uk
Oliver Lemon
Interaction Lab
Mathematics and Computer Science
Heriot-Watt University
o.lemon@hw.ac.uk
Abstract
We present new results from a real-user
evaluation of a data-driven approach to
learning user-adaptive referring expres-
sion generation (REG) policies for spoken
dialogue systems. Referring expressions
can be difficult to understand in techni-
cal domains where users may not know
the technical ?jargon? names of the do-
main entities. In such cases, dialogue sys-
tems must be able to model the user?s (lex-
ical) domain knowledge and use appro-
priate referring expressions. We present
a reinforcement learning (RL) framework
in which the system learns REG policies
which can adapt to unknown users on-
line. For real users of such a system, we
show that in comparison to an adaptive
hand-coded baseline policy, the learned
policy performs significantly better, with
a 20.8% average increase in adaptation ac-
curacy, 12.6% decrease in time taken, and
a 15.1% increase in task completion rate.
The learned policy also has a significantly
better subjective rating from users. This is
because the learned policies adapt online
to changing evidence about the user?s do-
main expertise. We also discuss the issue
of evaluation in simulation versus evalua-
tion with real users.
1 Introduction
We present new results from an evaluation with
real users, for a reinforcement learning (Sutton
and Barto, 1998) framework to learn user-adaptive
referring expression generation policies from data-
driven user simulations. Such a policy allows the
system to choose appropriate expressions to re-
fer to domain entities in a dialogue setting. For
instance, in a technical support conversation, the
Jargon: Please plug one end of the broadband
cable into the broadband filter.
Descriptive: Please plug one end of the thin
white cable with grey ends into the
small white box.
Table 1: Referring expression examples for 2 enti-
ties (from the corpus)
system could choose to use more technical terms
with an expert user, or to use more descriptive and
general expressions with novice users, and a mix
of the two with intermediate users of various sorts
(see examples in Table 1).
In natural human-human conversations, dia-
logue partners learn about each other and adapt
their language to suit their domain expertise (Is-
sacs and Clark, 1987). This kind of adaptation
is called Alignment through Audience
Design (Clark and Murphy, 1982; Bell, 1984).
We assume that users are mostly unknown to
the system and therefore that a spoken dialogue
system (SDS) must be capable of observing the
user?s dialogue behaviour, modelling his/her do-
main knowledge, and adapting accordingly, just
like human interlocutors. Therefore unlike sys-
tems that use static user models, our system has to
dynamically model the user?s domain knowledge
in order to adapt during the conversation.
We present a corpus-driven framework for
learning a user-adaptive REG policy from a small
corpus of non-adaptive human-machine interac-
tion. We show that the learned policy performs
better than a simple hand-coded adaptive policy
in terms of accuracy of adaptation, dialogue time
and task completion rate when evaluated with real
users in a wizarded study.
In section 2, we present some of the related
work. Section 3 and section 4 describe the dia-
logue system framework and the user simulation
124
model. In section 5, we present the training and in
section 6, we present the evaluation for different
REG policies with real users.
2 Related work
Rule-based and supervised learning approaches
have been proposed to learn and adapt during
conversations dynamically. Such systems learn
from a user at the start and later adapt to the do-
main knowledge of the user. However, they either
require expensive expert knowledge resources to
hand-code the inference rules (Cawsey, 1993) or a
large corpus of expert-layperson interaction from
which adaptive strategies can be learned and mod-
elled, using methods such as Bayesian networks
(Akiba and Tanaka, 1994). In contrast, we present
an approach that learns in the absence of these
expensive resources. It is also not clear how su-
pervised approaches choose between when to seek
more information and when to adapt. In this study,
we show that using reinforcement learning this de-
cision is learned automatically.
Reinforcement Learning (RL) has been suc-
cessfully used for learning dialogue management
policies since (Levin et al, 1997). The learned
policies allow the dialogue manager to optimally
choose appropriate dialogue acts such as instruc-
tions, confirmation requests, and so on, under
uncertain noise or other environment conditions.
There have been recent efforts to learn infor-
mation presentation and recommendation strate-
gies using reinforcement learning (Hernandez et
al., 2003; Rieser and Lemon, 2009; Rieser and
Lemon, 2010), and joint optimisation of Dialogue
Management and NLG using hierarchical RL has
been proposed by (Lemon, 2010). In addition,
we present a framework to learn to choose appro-
priate referring expressions based on a user?s do-
main knowledge. Following a proof-of-concept
study using a hand-coded rule-based user simu-
lation (Janarthanam and Lemon, 2009c), we pre-
viously showed that adaptive REG policies can
be learned using an RL framework with data-
driven user simulations and that such policies per-
form better than simple hand-coded policies (Ja-
narthanam and Lemon, 2010).
3 The Dialogue System
In this section, we describe the different modules
of the dialogue system. The interaction between
the different modules is shown in figure 1 (in
learning mode). The dialogue system presents the
user with instructions to setup a broadband con-
nection at home. In the Wizard of Oz setup, the
system and the user interact using speech. How-
ever, in our machine learning setup, they interact at
the abstract level of dialogue actions and referring
expressions. Our objective is to learn to choose
the appropriate referring expressions to refer to the
domain entities in the instructions.
Figure 1: System User Interaction (learning)
3.1 Dialogue Manager
The dialogue manager identifies the next dialogue
act (As,t where t denotes turn, s denotes system)
to give to the user based on the dialogue man-
agement policy pidm. The dialogue management
is coded in the form of a finite state machine. In
this dialogue task, the system provides instructions
to either observe or manipulate the environment.
When users ask for clarifications on referring ex-
pressions, the system clarifies (provide clar) by
giving information to enable the user to associate
the expression with the intended referent. When
the user responds in any other way, the instruc-
tion is simply repeated. The dialogue manager
is also responsible for updating and managing the
system state Ss,t (see section 3.2). The system in-
teracts with the user by passing both the system
action As,t and the referring expressions RECs,t
(see section 3.3).
3.2 The dialogue state
The dialogue state Ss,t is a set of variables that
represent the current state of the conversation. In
our study, in addition to maintaining an overall di-
alogue state, the system maintains a user model
UMs,t which records the initial domain knowl-
edge of the user. It is a dynamic model that starts
125
with a state where the system does not have any
knowledge about the user. Since the model is up-
dated according to the user?s behaviour, it may be
inaccurate if the user?s behaviour is itself uncer-
tain. Hence, the user model used in this system is
not always an accurate model of the user?s knowl-
edge and reflects a level of uncertainty about the
user.
Each jargon referring expression x is repre-
sented by a three-valued variable in the dialogue
state: user knows x. The three values that each
variable takes are yes, no, not sure. The vari-
ables are updated using a simple user model up-
date algorithm after the user?s response each turn.
Initially each variable is set to not sure. If the
user responds to an instruction containing the re-
ferring expression x with a clarification request,
then user knows x is set to no. Similarly, if
the user responds with appropriate information to
the system?s instruction, the dialogue manager sets
user knows x is set to yes. Only the user?s ini-
tial knowledge is recorded. This is based on the
assumption that an estimate of the user?s initial
knowledge helps to predict the user?s knowledge
of the rest of the referring expressions.
3.3 REG module
The REG module is a part of the NLG module
whose task is to identify the list of domain enti-
ties to be referred to and to choose the appropriate
referring expression for each of the domain enti-
ties for each given dialogue act. In this study, we
focus only on the production of appropriate refer-
ring expressions to refer to domain entities men-
tioned in the dialogue act. It chooses between
the two types of referring expressions - jargon
and descriptive. For example, the domain entity
broadband filter can be referred to using the jar-
gon expression ?broadband filter? or using the de-
scriptive expression ?small white box?1. Although
adaptation is the primary goal, it should be noted
that in order to get an idea of the user the system
is dealing with, it needs to seek information using
jargon expressions.
The REG module operates in two modes - learn-
ing and evaluation. In the learning mode, the REG
module is the learning agent. The REG module
learns to associate dialogue states with optimal re-
ferring expressions. This is represented by a REG
1We will use italicised forms to represent the domain enti-
ties (e.g. broadband filter) and double quotes to represent the
referring expressions (e.g. ?broadband filter?).
policy pireg : UMs,t ? RECs,t, which maps
the states of the dialogue (user model) to opti-
mal referring expressions. The referring expres-
sion choices RECs,t is a set of pairs identifying
the referent R and the type of expression T used in
the current system utterance. For instance, the pair
(broadband filter, desc) represents the descriptive
expression ?small white box?.
RECs,t = {(R1, T1), ..., (Rn, Tn)}
In the evaluation mode, a trained REG policy in-
teracts with unknown users. It consults the learned
policy pireg to choose the referring expressions
based on the current user model.
4 User Simulations
In this section, we present user simulation mod-
els that simulate the dialogue behaviour of a real
human user. Several user simulation models have
been proposed for use in reinforcement learning
of dialogue policies (Georgila et al, 2005; Schatz-
mann et al, 2006; Schatzmann et al, 2007; Ai and
Litman, 2007). However, they are suited only for
learning dialogue management policies, and not
natural language generation policies. In particular,
our model is the first to be sensitive to a system?s
choices of referring expressions. Earlier, we pre-
sented a two-tier simulation trained on data pre-
cisely for REG policy learning (Janarthanam and
Lemon, 2009a). However, it is not suited for train-
ing on small corpus like the one we have at our
disposal. In contrast to the earlier model, we now
condition the clarification requests on the referent
class rather than the referent itself to handle the
data sparsity problem.
4.1 Corpus-driven action selection model
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). The US produces a clarification request cr
based on the class of the referent C(Ri), type of
the referring expression Ti, and the current domain
knowledge of the user for the referring expression
DKu,t(Ri, Ti). Domain entities whose jargon ex-
pressions raised clarification requests in the cor-
pus were listed and those that had more than the
mean number of clarification requests were clas-
sified as difficult and others as easy enti-
ties (for example, power adaptor is easy - all
126
users understood this expression, broadband filter
is difficult). Clarification requests are pro-
duced using the following model.
P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) ? RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment action EAu,t
is generated using the following model.
P (EAu,t|As,t) if Au,t! = cr(Ri, Ti)
Finally, the user action is an instruction re-
sponse which is determined by the system ac-
tion As,t. Instruction responses can be ei-
ther provide info, acknowledgement or other
based on the system?s instruction.
P (Au,t = ir|EAu,t, As,t)
All the above models were trained on our cor-
pus data using maximum likelihood estimation and
smoothed using a variant of Witten-Bell discount-
ing. The corpus contained dialogues between
a non-adaptive dialogue system and real users.
According to the data, clarification requests are
much more likely when jargon expressions are
used to refer to the referents that belong to the
difficult class and which the user doesn?t
know about. When the system uses expressions
that the user knows, the user generally responds to
the instruction given by the system.
4.2 User Domain knowledge
The user domain knowledge is initially set to one
of several models at the start of every conver-
sation. The models range from novices to ex-
perts which were identified from the corpus using
k-means clustering. A novice user knows only
?power adaptor?, an expert knows all the jargon
expressions and intermediate users know some.
We assume that users can interpret the descriptive
expressions and resolve their references. There-
fore, they are not explicitly represented. We only
code the user?s knowledge of jargon expressions
using boolean variables representing whether the
user knows the expression or not.
4.3 Corpus
We trained the action selection model on a small
corpus of 12 non-adaptive dialogues between real
users and a dialogue system. There were six
dialogues in which users interacted with a sys-
tem using just jargon expressions and six with a
system using descriptive expressions. For more
discussions on our user simulation models and
the corpus, please refer to (Janarthanam and
Lemon, 2009b; Janarthanam and Lemon, 2009a;
Janarthanam and Lemon, 2010).
5 Training
The REG module was trained (operated in learn-
ing mode) using the above simulations to learn
REG policies that select referring expressions
based on the user expertise in the domain. In
this section, we discuss how to code the learning
agent?s goals as reward. We then discuss how the
reward function is used to train the learning agent.
5.1 Reward function
We designed a reward function for the goal of
adapting to each user?s domain knowledge. We
present the Adaptation Accuracy score (AA) that
calculates how accurately the agent chose the ap-
propriate expressions for each referent r, with re-
spect to the user?s knowledge. So, when the user
knows the jargon expression for r, the appropri-
ate expression to use is jargon, and if s/he doesn?t
know the jargon, a descriptive expression is appro-
priate. Although the user?s domain knowledge is
dynamically changing due to learning, we base ap-
propriateness on the initial state, because our ob-
jective is to adapt to the initial state of the user
DKu,initial. However, in reality, designers might
want their system to account for user?s chang-
ing knowledge as well. We calculate accuracy
per referent RAr and then calculate the overall
mean adaptation accuracy (AA) over all referents
as shown below.
RAr = #(appropriate expressions(r))#(instances(r))
AdaptationAccuracyAA = 1#(r)?rRAr
5.2 Learning
The REG module was trained in learning mode us-
ing the above reward function using the SHAR-
SHA reinforcement learning algorithm (with lin-
ear function approximation) (Shapiro and Langley,
2002). This is a hierarchical variant of SARSA,
127
which is an on-policy learning algorithm that up-
dates the current behaviour policy (see (Sutton
and Barto, 1998)). The training produced approx.
5000 dialogues. The user simulation was cali-
brated to produce three types of users: Novice,
Intermediate and Expert, randomly but with equal
probability.
Initially, the REG policy chooses randomly be-
tween the referring expression types for each do-
main entity in the system utterance, irrespective
of the user model state. Once the referring ex-
pressions are chosen, the system presents the user
simulation with both the dialogue act and refer-
ring expression choices. The choice of referring
expression affects the user?s dialogue behaviour.
For instance, choosing a jargon expression could
evoke a clarification request from the user, based
on which, the dialogue manager updates the inter-
nal user model (UMs,t) with the new information
that the user is ignorant of the particular expres-
sion. It should be noted that using a jargon expres-
sion is an information seeking move which enables
the REG module to estimate the user?s knowledge
level. The same process is repeated for every dia-
logue instruction. At the end of the dialogue, the
system is rewarded based on its choices of refer-
ring expressions. If the system chooses jargon ex-
pressions for novice users or descriptive expres-
sions for expert users, penalties are incurred and if
the system chooses REs appropriately, the reward
is high. On the one hand, those actions that fetch
more reward are reinforced, and on the other hand,
the agent tries out new state-action combinations
to explore the possibility of greater rewards. Over
time, it stops exploring new state-action combina-
tions and exploits those actions that contribute to
higher reward. The REG module learns to choose
the appropriate referring expressions based on the
user model in order to maximize the overall adap-
tation accuracy. Figure 2 shows how the agent
learns using the data-driven (Learned DS) during
training. It can be seen in the figure 2 that towards
the end the curve plateaus, signifying that learning
has converged.
6 Evaluation
In this section, we present the details of the eval-
uation process, the baseline policy, the metrics
used, and the results. In a recent study, we eval-
uated the learned policy and several hand-coded
baselines with simulated users and found that
Figure 2: Learning curve - Training
the Learned-DS policy produced higher adapta-
tion accuracy than other policies (Janarthanam and
Lemon, 2010). An interesting issue for research
in this area is to what extent evaluation results ob-
tained in simulated environments transfer to eval-
uations with real users (Lemon et al, 2006).
6.1 Baseline system
In order to compare the performance of the learned
policy with a baseline, a simple rule-based policy
was built. This baseline was chosen because it per-
formed better in simulation, compared to a vari-
ety of other baselines (Janarthanam and Lemon,
2010). It uses jargon for all referents by default
and provides clarifications when requested. It ex-
ploits the user model in subsequent references af-
ter the user?s knowledge of the expression has
been set to either yes or no. Therefore, although
it is a simple policy, it adapts to a certain extent
(?locally?). We refer to this policy as the ?Jargon-
adapt? policy. It should be noted that this policy
was built in the absence of expert domain knowl-
edge and/or an expert-layperson corpus.
6.2 Process
We evaluated the two policies with real users.
36 university students from different backgrounds
(e.g. Arts, Humanities, Medicine and Engineer-
ing) participated in the evaluation. 17 users were
given a system with Jargon-adapt policy and 19
users interacted with a system with Learned-DS
policy. Each user was given a pre-task recognition
test to record his/her initial domain knowledge.
The experimenter read out a list of technical terms
and the user was asked to point out to the domain
entities laid out in front of them. They were then
128
given one of the two systems - learned or base-
line, to interact with. Following the system in-
structions, they then attempted to set up the broad-
band connection. When the dialogue had ended,
the user was given a post-task test where the recog-
nition test was repeated and their responses were
recorded. The user?s broadband connection setup
was manually examined for task completion (i.e.
the percentage of correct connections that they had
made in their final set-up). The user was given the
task completion results and was then given a user
satisfaction questionnaire to evaluate the features
of the system based on the conversation.
All users interacted with a wizarded system em-
ploying one of the two REG policies (see figure
3). The user?s responses were intercepted by a hu-
man interpreter (or ?wizard?) and were annotated
as dialogue acts, to which the automated dialogue
manager responded with a system dialogue action
(the dialogue policy was fixed). The wizards were
not aware of the policy used by the system. The
respective policies chose only the referring expres-
sions to generate the system utterance for the given
dialogue action. The system utterances were con-
verted to speech by a speech synthesizer (Cere-
proc) and were played to the user.
Figure 3: Wizarded Dialogue System
6.3 Metrics
In addition to the adaptation accuracy mentioned
in section 5.1, we also measure other parame-
ters from the conversation in order to show how
learned adaptive policies compare with other poli-
cies on other dimensions. We also measure the
learning effect on the users as (normalised) learn-
ing gain (LG) produced by using unknown jargon
expressions. This is calculated using the pre- and
post-test scores for the user domain knowledge
(DKu) as follows.
Metrics Jargon-adapt Learned-DS
AA 63.91 84.72 **
LG 0.59 0.61
DT 7.86 6.98 *
TC 84.7 99.8 **
* Statistical significance (p < 0.05).
** Statistical significance (p < 0.001).
Table 2: Evaluation with real users
Learning Gain LG = Post?Pre1?Pre
Dialogue time (DT) is the actual time taken for
the user to complete the task. We measured task
completion (TC) by examining the user?s broad-
band setup after the task was completed (i.e. the
percentage of correct connections that they had
made in their final set-up).
6.4 Results
We compare the performance of the two strategies
on real users using objective parameters and sub-
jective feedback scores. Tests for statistical sig-
nificance were done using Mann-Whitney test for
2 independent samples (due to non-parametric na-
ture of the data).
Table 2 presents the mean accuracy of adap-
tation (AA), learning gain (LG), dialogue time
(DT), and task completion (TC), produced by the
two strategies. The Learned-DS strategy pro-
duced more accurate adaptation than the Jargon-
adapt strategy (p<0.001, U=9.0, r=-0.81). Higher
accuracy of adaptation (AA) of the Learned-DS
strategy translates to less dialogue time (U=73.0,
p<0.05, r=-0.46) and higher task completion
(U=47.5, p<0.001, r=-0.72) than the Jargon-adapt
policy. However, there was no significant differ-
ence in learning gain (LG).
Table 3 presents how the users subjectively
scored on a agreement scale of 1 to 4 (with 1
meaning ?strongly disagree?), different features of
the system based on their conversations with the
two different strategies. Users? feedback on dif-
ferent features of the systems were not very differ-
ent from each other. However, users did feel that
it was easier to identify domain objects with the
Learned-DS strategy than the Jargon-adapt strat-
egy (U=104.0, p<0.05, r=-0.34). To our knowl-
edge, this is the first study to show a significant
improvement in real user ratings for a learned pol-
icy in spoken dialogue systems (normally, objec-
tive metrics show an improvement, but not subjec-
129
Feedback questions Jargon-adapt Learned-DS
Q1. Quality of voice 3.11 3.36
Q2. Had to ask too many questions 2.23 1.89
Q3. System adapted very well 3.41 3.58
Q4. Easy to identify objects 2.94 3.37 *
Q5. Right amount of dialogue time 3.23 3.26
Q6. Learned useful terms 2.94 3.05
Q7. Conversation was easy 3.17 3.42
Q8. Future use 3.22 3.47
* Statistical significance (p < 0.05).
Table 3: Real user feedback
tive scores (Lemon et al, 2006)).
6.5 Analysis
The results show that the Learned-DS strategy is
significantly better than the hand-coded Jargon-
Adapt policy in terms of adaptation accuracy, di-
alogue time, and task completion rate. The ini-
tial knowledge of the users (mean pre-task recog-
nition score) of the two groups were not signifi-
cantly different from each other (Jargon-adapt =
7.33, Learned-DS = 7.45). Hence there is no bias
on the user?s pre-task score towards any strategy.
While the Learned-DS system adapts well to its
users globally, the Jargon-adapt system adapted
only locally. This led to higher task completion
rate and lower dialogue time.
The Learned-DS strategy enabled the system to
adapt using the dependencies that it learned dur-
ing the training phase. For instance, when the user
asked for clarification on some referring expres-
sions (e.g. ?ethernet cable?), it used descriptive
expressions for domain objects like ethernet light
and ethernet socket. Such adaptation across ref-
erents enabled the Learned-DS strategy to score
better than the Jargon-adapt strategy. Since the
agent starts the conversation with no knowledge
about the user, it learned to use information seek-
ing moves (use jargon) at appropriate moments,
although they may be inappropriate. But since it
was trained to maximize the adaptation accuracy,
the agent also learned to restrict such moves and
start predicting the user?s domain knowledge as
soon as possible. By learning to trade-off between
information-seeking and adaptation, the Learned-
DS policy produced a higher adaptation with real
users with different domain knowledge levels.
The users however did not generally rate the
two policies differently. However, they did rate
it (significantly) easier to identify objects when
using the learned policy. For the other ratings,
users seemed to be not able to recognize the nu-
ances in the way the system adapted to them. They
could have been satisfied with the fact that the sys-
tem adapted better (Q3). This adaptation and the
fact that the system offered help when the users
were confused in interpreting the technical terms,
could have led the users to score the system well in
terms of future use (Q8), dialogue time (Q5), and
ease of conversation (Q7), but in common with ex-
periments in dialogue management (Lemon et al,
2006) it seems that users find it difficult to evaluate
these improvements subjectively. The users were
given only one of the two strategies and therefore
were not in a position to compare the two strate-
gies and judge which one is better. Results in table
3 lead us to conclude that perhaps users need to
compare two or more strategies in order to judge
the strategies better.
7 Conclusion
We presented new results from an evaluation with
real users. In this study, we have shown that user-
adaptive REG policies can be learned using an RL
framework and data-driven user simulations. It
learned to trade off between adaptive moves and
information seeking moves automatically to max-
imize the overall adaptation accuracy. The learned
policy started the conversation with information
seeking moves, learned a little about the user, and
started adapting dynamically as the conversation
progressed. We also showed that the learned pol-
icy performs better than a reasonable hand-coded
policy with real users in terms of accuracy of adap-
tation, dialogue time, task completion, and a sub-
jective evaluation. Finally, this paper provides
further evidence that evaluation results obtained
130
in simulated environments can transfer reliably to
evaluations with real users (Lemon et al, 2006).
Whether the learned policy would perform bet-
ter than a hand-coded policy which was painstak-
ingly crafted by a domain expert (or learned us-
ing supervised methods from an expert-layperson
corpus) is an interesting question that needs fur-
ther exploration. Also, it would also be interesting
to make the learned policy account for the user?s
learning behaviour and adapt accordingly. We also
believe that this framework can be extended to in-
clude other decisions in NLG besides REG (Deth-
lefs and Cuayahuitl, 2010).
Acknowledgements
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
H. Ai and D. Litman. 2007. Knowledge consistent
user simulations for dialog systems. In Proceedings
of Interspeech 2007, Antwerp, Belgium.
T. Akiba and H. Tanaka. 1994. A Bayesian approach
for User Modelling in Dialogue Systems. In Pro-
ceedings of the 15th conference on Computational
Linguistics - Volume 2, Kyoto.
A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145?204.
A. Cawsey. 1993. User Modelling in Interactive Ex-
planations. User Modeling and User-Adapted Inter-
action, 3(3):221?247.
H. H. Clark and G. L. Murphy. 1982. Audience de-
sign in meaning and reference. In J. F. LeNy and
W. Kintsch, editors, Language and comprehension.
Amsterdam: North-Holland.
N. Dethlefs and H. Cuayahuitl. 2010. Hierarchical Re-
inforcement Learning for Adaptive Text Generation.
In Proc. INLG 2010.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc of Eu-
rospeech/Interspeech.
F. Hernandez, E. Gaudioso, and J. G. Boticario. 2003.
A Multiagent Approach to Obtain Open and Flexible
User Models in Adaptive Learning Communities. In
User Modeling 2003, volume 2702/2003 of LNCS.
Springer, Berlin / Heidelberg.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26?37.
S. Janarthanam and O. Lemon. 2009a. A Two-tier
User Simulation Model for Reinforcement Learning
of Adaptive Referring Expression Generation Poli-
cies. In Proc. SigDial?09.
S. Janarthanam and O. Lemon. 2009b. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG?09.
S. Janarthanam and O. Lemon. 2009c. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG?09.
S. Janarthanam and O. Lemon. 2010. Learning to
Adapt to Unknown Users: Referring Expression
Generation in Spoken Dialogue Systems. In Proc.
ACL?10.
O. Lemon, Georgila. K., and J. Henderson. 2006.
Evaluating Effectiveness and Portability of Rein-
forcement Learned Dialogue Strategies with real
users: the TALK TownInfo Evaluation. In
IEEE/ACL Spoken Language Technology.
O. Lemon. 2010. Learning what to say and how to say
it: joint optimization of spoken dialogue manage-
ment and Natural Language Generation. Computer
Speech and Language. (to appear).
E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proc. of ASRU97.
V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL?09.
V. Rieser and O. Lemon. 2010. Optimising informa-
tion presentation for spoken dialogue systems. In
Proc. ACL. (to appear).
J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.
D. Shapiro and P. Langley. 2002. Separating skills
from preference: Using learning to program by re-
ward. In Proc. ICML-02.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
131
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 142?151,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
?The day after the day after tomorrow?? A machine learning approach to
adaptive temporal expression generation:
training and evaluation with real users
Srinivasan Janarthanam, Helen Hastie, Oliver Lemon, Xingkun Liu
Interaction Lab
School of Mathematical and Computer Sciences (MACS)
Heriot-Watt University
{sc445, h.hastie, o.lemon, x.liu}@hw.ac.uk
Abstract
Generating Temporal Expressions (TE) that
are easy to understand, unambiguous, and rea-
sonably short is a challenge for humans and
Spoken Dialogue Systems. Rather than devel-
oping hand-written decision rules, we adopt a
data-driven approach by collecting user feed-
back on a variety of possible TEs in terms
of task success, ambiguity, and user prefer-
ence. The data collected in this work is freely
available to the research community. These
data were then used to train a simulated user
and a reinforcement learning policy that learns
an adaptive Temporal Expression generation
strategy for a variety of contexts. We evalu-
ate our learned policy both in simulation and
with real users and show that this data-driven
adaptive policy is a significant improvement
over a rule-based adaptive policy, leading to
a 24% increase in perceived task completion,
while showing a small increase in actual task
completion, and a 16% decrease in call dura-
tion. This means that dialogues are more ef-
ficient and that users are also more confident
about the appointment that they have agreed
with the system.
1 Introduction
Temporal Expressions are linguistic expressions that
are used to refer to a date and are often a source of
confusion in human-human, human-computer and
text interactions such as emails and instant messag-
ing. For example, ?Let?s meet next Sunday?? ?do
you mean Sunday this week or a week on Sunday??.
(Mccoy and Strube, 1999) state that changes in tem-
poral structure in text are often indicated by either
cue words and phrases (e.g. ?next Thursday?, ?this
week?, ?tomorrow?), a change in grammatical time
of the verb (e.g. present tense versus future tense),
or changes in aspect (e.g. atomic versus extended
events versus states as defined by (Moens and Steed-
man, 1988)). In this study, we will concentrate on
the first of these phenomena, generating TEs with
the optimal content and lexical choice.
Much work in the field of Natural Language Pro-
cessing concerns understanding and resolving these
temporal expressions in text (Gerber et al, 2002;
Pustejovsky et al, 2003; Ahn et al, 2007; Mazur
and Dale, 2007; Han et al, 2006), however, little
work has looked at how best to plan and realise tem-
poral expressions in order to minimize ambiguity
and confusion in a Spoken Dialogue System (SDS).
(Reiter et al, 2005) presented a data driven ap-
proach to generating TEs to refer to time in weather
forecast information where appropriate expressions
were identified using contextual features using su-
pervised learning. We adopt an adaptive, data-driven
reinforcement learning approach instead. Similar
data-driven approaches have been applied to infor-
mation presentation (Rieser et al, 2010; Walker et
al., 2007) where each Natural Language Generation
(NLG) action is a sequential decision point, based on
the current dialogue context and expected long-term
reward of that action. A data-driven approach has
also been applied to the problem of referring expres-
sion generation in dialogue for expert and novice-
users of a SDS (Janarthanam and Lemon, 2010).
However, to date, there has been no previous work
on adaptive data-driven approaches for temporal re-
ferring expression generation, where uncertainty in
142
the stochastic environment is explicitly modelled.
The data-driven approach to temporal expression
generation presented here is in the context of ap-
pointment scheduling dialogues. The fact that there
are multiple ways that a time slot can be referred to
leads to an interesting NLG problem of how best to
realise a TE for a particular individual in a particular
context for certain domains. For example, the fol-
lowing expressions all vary in terms of length, ambi-
guity, redundant information and users? preference:
?next Friday afternoon? or ?Friday next week at the
same time?, or ?in the afternoon, a week on Friday?.
Temporal Expressions contain two types of refer-
ences: absolute references such as ?Tuesday? and
?12th January?, and relative references such as ?to-
morrow? and ?this Tuesday?. Generating TEs there-
fore, involves both in selecting appropriate pieces of
information (date, day, time, month, and week) to
present and deciding how to present them (absolute
or relative reference).
Our objective here is to convey a target appoint-
ment slot to users using an expression that is optimal
in terms of the trade-off between understandability,
length and user preference.
2 Methodology
We address the issue of generating TEs by adopting
a data-driven approach that has four stages. Firstly,
we define Temporal Expression Units (TEU) as de-
scribed in Section 2.1. Secondly, we design and im-
plement a web-based data collection, gathering met-
rics on the TEUs in various contexts for a variety
of date types (Section 3). Thirdly, we train a user
simulation and use it to learn a policy using rein-
forcement learning techniques that generates the op-
timal combination of TEUs for each context (Sec-
tion 4). Finally, we deploy and evaluate this pol-
icy in a Spoken Dialogue System for appointment
scheduling and show that our learned policy per-
forms better than a hand-written, adaptive one (re-
sults presented in Section 5).
2.1 Temporal Expression Units
For this study, TEs are broken down into 5 cate-
gories or units (TEUs) presented in a fixed order:
DAY, DATE, MONTH, WEEK and TIME. Each of
these units can be expressed relative to the current
TEU Choices
DAY abs, rel, rc, nn
DATE abs, nn
MONTH abs, nn
WEEK abs, rel, nn
TIME abs, rc
Table 1: TEU choices where abs is absolute, rel is rela-
tive, rc is relative to context and nn is none
day and to the current context (i.e. previously men-
tioned dates). Specifically, there are 3 unit attributes:
absolute (e.g. DAY=abs ?Tuesday?); relative to cur-
rent day (e.g. DAY=rel ?tomorrow?); and relative to
context (e.g. DAY=rc ?the following day?).
Certain restrictions on possible TEU combina-
tions were imposed, for example, DATE=rc and
DAY=rel were combined to be just DAY=rel, and
some combinations were omitted on the basis that
it is highly unlikely that they would be uttered
in natural speech, for example WEEK=rel and
MONTH=abs would result in ?this week in Septem-
ber?. Finally, every TE has to contain a time (am or
pm for this application). The possible combinations
are summarised in Table 1.
3 Data Collection
The data collection experiment was in two parts
(Task 1 and Task 2) and was designed using the We-
bexp experimental software1. Webexp is a client-
server set up where a server application hosts the ex-
periment and stores the experimental files, logs and
results. The client side runs an applet on the user?s
web-browser.
In Task 1, participants listened to an audio file
containing a TE generated from absolute and rela-
tive TEUs (see Figure 1). No relative-context (rc)
TEUs were used in Task 1 since the dialogue ex-
cerpt presented was in isolation and therefore had
no context. Each participant was asked to listen to
10 different audio files in a sequence corresponding
to a variety of dates randomly chosen from 8 pos-
sible dates. The participant then had to identify the
correct appointment slot that the system is referring
to. There is scope for the participant to add multi-
ple answers in order to capture potential ambiguity
1http://www.webexp.info
143
Figure 1: Screen shot of Task 1 in the on-line data collection experiment
of a TE, and we report on this below. The 8 dates
that were used to generate the TEs fell into a two
week period in a single month which is in-line with
the evaluation set-up of the appointment scheduling
SDS discussed in Section 5.3.
For each date, the TE was randomly picked from a
set of 30 possible combinations of TEUs. Each TEU
was generated by a rule-based realiser and synthe-
sized using the Baratinoo synthesizer (France Tele-
com, 2011). This realiser generates text from a can-
didate list for each TEU based on the given date.
For example, if the slot currently being discussed
is Tuesday 7th, the realiser would generate ?tomor-
row? for DAY=rel; if the date in discussion was
Wednesday 8th then DAY=rel would be realised as
?the day after tomorrow?. There was potential for
overlap of stimuli, as any given TE for any given
date may be assessed by more than one participant.
Task 2 of the experiment was in two stages. In the
first stage (Task 2A), the participants are given to-
day?s date and the following dialogue excerpt; Op-
erator: ?We need to send out an engineer to your
home. The first available appointment is . . .? (see
Figure 2). They are then asked to listen to 5 audio
files of the system saying different TEs for the same
date and asked to rate preference on a scale of 1-6
(where 1 is bad and 6 is great.) For the second stage
(Task 2B), the dialogue is as follows; Operator: ?so
you can?t do Wednesday 8th September in the morn-
ing.? and then the participants are asked to listen
to 5 more audio files that are generated TEs includ-
ing relative context such as ?how about Thursday at
the same time??. This two-stage process is then re-
peated 4 times for each participant.
Table 2 summarizes the metrics collected in the
different parts of the experiment. The metric Dis-
tance is calculated in terms of the number of slots
from the current date to the target date (TD). In-
stances were grouped into four distance groups: G1:
TD is 1-2 slots away; G2: TD is 3-6 slots away; G3:
TD is 7-11 slots away and G4: TD more than 11
slots away. P replay is calcuated by the total num-
ber of replays divided by the total number of plays
for that temporal expression, i.e. the probability that
the temporal expression played is requested to be re-
played. P ambiguous is calculated by the number of
times a given temporal expression is given more than
1 interpretation divided by the total number of times
that the same given referring expression is answered.
In total there were 73 participants for Task 1 and
144
Figure 2: Screen shot of Task 2 in the on-line data collection experiment
730 TE samples collected. Although Task 2 directly
followed on from Task 1, there was a significant
drop out rate as only 48 participants completed the
second task resulting in 1,920 TE samples. Partici-
pants who completed both tasks were rewarded by a
chance to win an Amazon voucher.
3.1 Data Analysis
Figure 3 shows various metrics with respect to TE
absoluteness and relativeness is the number of ab-
solute and relative TEUs respectively. These two
graphs represent the state space that the genera-
tion policy described in Section 4 is exploring, trad-
ing off between various features such as Length,
taskSuccess and userPref.
As we can see, there is a tendency for average
taskSuccess to increase as absoluteness increases
whereas, for relativeness the distribution is more
even. The TE with the greatest taskSuccess has an
absoluteness of 4 and zero relativeness: DATE=abs,
MONTH=abs, WEEK=abs, TIME=abs (e.g. ?11th
September, the week starting the 10th, between 8am
and 10am?) and the TE with the least taskSuccess
has an absoluteness of only 2, again with no rela-
tiveness: DATE=abs, TIME=abs, (e.g. ?8th between
8am and 10am?).
Average userPref stays level and then decreases
if absoluteness is 5. We infer from this that al-
though long utterances that are completely explicit
are more clear in terms of taskSuccess, they are not
necessarily preferred by users. This is likely due
to TE length increasing. On average, the inclusion
of one relative expression is preferred over none at
all or two. The most preferred TE has an abso-
luteness of 3 with a relativeness of 2: DAY=rel,
DATE=abs, MONTH=abs, WEEK=rel, TIME=abs
(e.g. ?Tomorrow the 7th of September, this week,
between 8am and 10am?).
145
Figure 3: Graph showing the trade-offs between various metrics with respect to absoluteness and relativeness (number
of absolute/relative TEUs) in terms of probabilities or normalised values.
Metric Description Task
P ambiguous Probability that the expres-
sion is ambiguous to the
user
1
taskSuccess Correct slot identified 1
P replay Probability of replay (mea-
sure of understandability)
1 & 2
Length Expression length in terms
of number of TEUs that
are non null divided by the
total number of possible
TEUs (5)
1 & 2
wordLength Expression length in words
normalised over max num
of words (15)
1 & 2
userPref Preference rating of audio
from 1-6
2
Distance Distance from target date
(TD) to current date in
terms of number of slots
1 & 2
Table 2: Metrics collected in various parts of the experi-
ment
The probability of ambiguity and replay does not
seem to be affected by absoluteness. The most am-
biguous TE has an absoluteness of 3 and zero rela-
tiveness: DAY=abs MONTH=abs TIME=abs, (e.g.
?Tuesday September between 8am and 10am?) in-
dicating that a date is needed for precision. The
TEs that the participants were most likely to replay
tended to be short e.g. ?Tomorrow at the same time?.
This may be due to the clarity of the speech synthe-
siser.
4 Learning a TE generation policy
Reinforcement learning is a machine learning ap-
proach based on trial and error learning, in which
a learning agent learns to map sequences of ?opti-
mal? actions to environment or task states (Sutton
and Barto, 1998). In this framework the problem
of generating temporal expressions is presented as
a Markov Decision Process. The goal of the learn-
ing agent is to learn to choose those actions that ob-
tain maximum expected reward in the long run. In
this section, we present the reinforcement learning
setup for learning temporal expression generation
policies.
4.1 Actions and States
In this learning setup, we focus only on generating
the formal specification and treat the set of TEU
choices as the sequential actions of the learning
agent. Table 1 presents the choices that are available
for each TEU.
The actions are taken based on two factors: the
146
distance (in terms of time slots: morning or after-
noon appointments) between (1) the current date
and the target slot and (2) the current date and the
slot in context. Based on the distance, the target
slot was classified to belong to one of the four dis-
tance groups (G1-G4). The slot in context repre-
sents whether there was any other slot already men-
tioned in the conversation so far, so that the system
has an option to use ?relative context? expressions
to present day and time information. Information
concerning the target slot?s group and the slot in con-
text make up the state space of the Markov Decision
Process (MDP).
4.2 User Simulation
We built a user simulation to simulate the dialogue
behaviour of a user in appointment scheduling con-
versations based on the data from real users de-
scribed in Section 3. It responds to the TE used
by the system to refer to an appointment slot. It
responds by either accepting, rejecting, or clarify-
ing the offered slot based on the user?s own calen-
dar of available slots. For instance, the simulated
user rejects an offered slot if the user is not avail-
able at that time. If they accept or reject an offered
slot, the user is assumed to understand the TE unam-
biguously. However, if the user is unable to resolve
the appointment slot from the TE, it responds with a
clarification request. The simulation responded with
a dialogue action (Au,t) to TEs based on the sys-
tem?s dialogue act (As,t), system?s TE (TEs,t). The
following probabilistic model was used to generate
user dialogue actions:
P (Au,t|As,t, TEs,t, G,C,Cal)
In addition to TEs,t and As,t, other factors such as
distance between the target slot and the current slot
(G), the previous slot in context (C), and the user?s
calendar (Cal) were also taken into account. G is ei-
ther G1, G2, G3 or G4 as explained in Section 3. The
User?s dialogue action (Au,t) is one of the three: Ac-
cept slot, Reject slot or Request Clarification. The
probability of clarification request was calculated as
the average of the ambiguity and replay probabilities
seen in real user data.
4.3 Reward function
The learning agent was rewarded for each TE that it
generated. The reward given to the agent was based
on trade-offs between three variables: User prefer-
ence (UP), Length of the temporal expression (L),
and Clarification request probability (CR). UP for
each TE is obtained from Task 2 of the data collec-
tion. In the following reward function, UP is nor-
malised to be between 0 and 1. L is based on number
of TEUs used. The maximum number of TEUs that
can be used is 5 (i.e. DAY, DATE, WEEK, MONTH,
TIME). L is calculated as follows:
Length of TE (L) = No. of used TEUsMax. no. of TEUs
The clarification request (CR) is set to be 1 if the
user responds to the TE with a Request Clarification
and 0 otherwise. Reward is therefore calculated on
a turn-by-turn basis using the following formula:
Reward = UP ? 10.0 ? L ? 10.0 ? CR ? 10.0
In short, we chose a reward function that penalises
TEs that are long and ambiguous, and which rewards
TEs that users prefer. It also indirectly rewards task
success by penalising ambiguous TEs resulting in
clarification requests. This trade-off structure is evi-
dent from the data collection where TEs that are too
long are dispreferred by the users (see Figure 3). The
maximum possible reward is 6 (i.e. UP=1, CR=0,
L=2/5) and the minimum is -20 (i.e. UP=0, CR=1,
L=1). Note that other reward functions could be ex-
plored in future work, for example maximising only
for user preference or length.
4.4 Training
We trained a TE generation policy using the above
user simulation model for 10,000 runs using the
SARSA reinforcement learning algorithm (Sutton
and Barto, 1998). During the training phase, the
learning agent generated and presented TEs to the
user simulation. When a dialogue begins, there is no
appointment slot in context (i.e. C = 0). However,
if the user rejects the first slot, the dialogue system
sets C to 1 and presents the next slot. This is again
reset at the beginning of the next dialogue. The
agent was rewarded at the end of every turn based
on the user?s response, length of the TE, and user
preference scores as shown above. It gradually ex-
plored all possible combinations of TEUs and identi-
fied those TEUs in different contexts that maximize
147
Figure 4: Learning curve
the long-term reward. Figure 4 shows the learning
curve of the agent.
Table 3 presents the TE generation policy learned
by the agent. As one can observe, it used a mini-
mum number of TEUs to avoid length penalties in
the reward. In all cases, MONTH and WEEK in-
formation have not been presented at all. For target
slots that were closest (in group G1) and the farthest
(in group G4), it used relative forms of day (e.g. ?to-
morrow?, ?next Tuesday?, etc.). This is probably
because users dispreferred day information for in-
between slots (e.g. ?the day after the day after to-
morrow?). Also, MONTH information may have
been considered to be irrelevant due to the fact that
the two week window over which the data has been
collected do not span over two different months.
5 Evaluation
In this section, we present the baseline policies that
were evaluated along with the learned policy. We
then present the results of evaluation.
Slots Specification learned
1-2 DAY=rel;DATE=abs;MONTH=nn;
> 11 WEEK=nn;TIME=abs
3-11 DAY=nn;DATE=abs;MONTH=nn;
WEEK=nn;TIME=abs
Table 3: Learned policy
5.1 Baseline policies
The following are the baseline TEG policies:
1. Absolute policy: always use absolute for-
mats for all TEUs (i.e. DAY=abs; DATE=abs;
MONTH=abs; WEEK=abs; TIME=abs)
2. Minimal policy: always use a minimal format
with only date, month and time information in
their absolute forms (i.e. DAY=nn; DATE=abs;
MONTH=abs; WEEK=nn; TIME=abs)
3. Random policy: select possible formats ran-
domly for each TEU.
148
TEG Policy Average reward
Learned -0.071* (?3.75)
Absolute -4.084 (?4.36)
Minimal -1.340 (?4.2)
Random -8.21 (?7.72)
Table 4: Evaluation with simulated users (* p < 0.05,
two-tailed independent samples t-test)
5.2 Results
We evaluated the learned policy and the three other
hand-coded baseline TE generation policies with our
user simulation model. Each policy generated 1,000
TEs in different states. Table 4 present the results
of evaluation with simulated users. On average, the
learned policy scores higher than all the baseline
policies and the differences between the average re-
ward of the learned policy and the other baselines
are statistically significant. This shows that target
slots can be presented using different TEs depending
on how far they are from the current date and such
adaptation can produce less ambiguous, shorter and
user preferred expressions.
5.3 Evaluation with real users
The policy was also integrated into an NLG com-
ponent of a deployed Appointment Scheduling spo-
ken dialogue system. Please note that this is differ-
ent from the web environment in which the training
data was collected. Our data-driven policy was acti-
vated when the system informs the user of an avail-
able time slot. This system was compared to the
exact same system but with a rule-based adaptive
baseline system. In the rule-based policy MONTH,
DATE and TIME were always absolute, DAY was
relative if the target date was less than three days
away (i.e. ?today, tomorrow, day after tomorrow?),
and WEEK was always relative (i.e. ?this week, next
week?). All 5 information units were included in the
realisation (e.g. ?Thursday the 15th July in the after-
noon, next week?) although the order was slightly
different (DAY-DATE-MONTH-TIME-WEEK).
In this domain, the user tries to make an appoint-
ment for an engineer to visit their home. Each user
is given a set of 2-week calendars which shows their
availability and the goal is to arrange an appoint-
ment when both they and the engineer are available.
There were 12 possible scenarios that were evenly
rotated across participants and systems. Each sce-
nario is categorised in terms of scheduling difficulty
(Hard/Medium/Easy). Scheduling difficulty is cal-
culated for User Difficulty (UD) and System Diffi-
culty (SD) separately to assess the system?s mixed
initiative ability. Scheduling difficulty is calculated
as the ordinal of the first session that is free for both
the User and the System. Hard scenarios are with an
ordinal of 3 or 4; Medium with an ordinal of 2, and
Easy with an ordinal of 1. There are 4 scenarios in
each of these difficulty categories for both the user
and system. To give an example, in Scenario 10,
the user can schedule an appointment on Wednes-
day afternoon but he/she also has one free session
on the previous Tuesday afternoon when the engi-
neer is busy therefore UD = 2. For the system, in
this scenario, the first free session it has is on the
Wednesday afternoon therefore SD=1. In this case,
the scenario is easier for the system than the user be-
cause the system could just offer the first session that
it has free.
605 dialogues were collected and analysed. The
system was evaluated by employees at France Tele-
com and students of partner universities who have
never used the appointment scheduling system be-
fore. After each scenario, participants were then
asked to fill out a questionnaire on perceived task
success and 5 user satisfaction questions on a 6-
point Likert Scale (Walker et al, 2000). Results
from the real user study are summarised in Table 5.
The data-driven policy showed significant improve-
ment in Perceived Task Success (+23.7%) although
no significant difference was observed between the
two systems in terms of Actual Task Success (Chi-
square test, df=1). Perceived Task Success is users?
perception of whether they completed the task suc-
cessfully or not. Overall user satisfaction (the aver-
age score of all the questions) was also significantly
higher (+5%)2. Dialogues with the learned policy
were significantly shorter with lower Call Duration
in terms of time (-15.7%)2 and fewer average words
per system turn (-23.93%)2. Figure 5 shows the
length results in time for systems of varying UD and
SD. We can see that the data-driven adaptive policy
consistently results in a shorter dialogue across all
levels of difficulty. In summary, these results show
that using a policy trained on the data collected here
149
Parameters Learned Baseline
TEG TEG
Actual Task Success 80.05% 78.57%
Perceived Task Success 74.86%* 60.50%
User satisfaction 4.51* 4.30
No. system turns 22.8 23.2
Words per system turn 13.16* 17.3
Call duration 88.60 sec * 105.11 sec
Table 5: Results with real users (* statistically significant
difference at p<0.05)
results in shorter dialogues and greater confidence
in the user that they have had a successful dialogue.
Although the learned policy was trained to generate
optimal TEs within a two week window and there-
fore is not general policy for all TE generation prob-
lems, we believe that the data-driven approach that
we have followed can generalise to other TE gener-
ation tasks.
Figure 5: Graph comparing length of dialogues for user
(UD) and system difficulty (SD)
6 Conclusion
We have presented a principled statistical learning
method for generating Temporal Expressions (TEs)
that refer to appointment slots in natural language
utterances. We presented a method for gathering
data on TEs with an on-line experiment and showed
how we can use these data to generate TEs us-
ing a Markov Decision Process which can be opti-
mised using reinforcement learning techniques. We
showed that a TEG policy learned using our frame-
2independent two-tailed t-test p < 0.05
work performs signifcantly better than hand-coded
adaptive policies with real users as well as with sim-
ulated users.
The data collected in this work has been freely
released to the research community in 20113.
Acknowledgements
The research leading to these results has received
funding from the EC?s 7th Framework Programme
(FP7/2007-2013) under grant agreement no. 216594
(CLASSiC project www.classic-project.
org), (FP7/2011-2014) under grant agreement no.
248765 (Help4Mood project), (FP7/2011-2014) un-
der grant agreement no. 270435 (JAMES project),
(FP7/2011-2014) under grant agreement no. 270019
(SpaceBook project), and from the EPSRC, project
no. EP/G069840/1. We would also like to thank our
CLASSiC project colleagues at Cambridge Univer-
sity and France Telecom / Orange Labs.
References
D. Ahn, J. van Rantwijk, and M. de Rijke. 2007. A
Cascaded Machine Learning Approach to Interpret-
ing Temporal Expressions. In Proceedings of NAACL-
HLT 2007.
France Telecom. 2011. Baratinoo expressive speech syn-
thesiser. http://tts.elibel.tm.fr.
L. Gerber, L. Ferro, I. Mani, B. Sundheim, G. Wilson,
and R. Kozierok. 2002. Annotating Temporal Infor-
mation: From Theory to Practice. In Proceedings of
HLT.
B. Han, D. Gates, and L. Levin. 2006. Understanding
temporal expressions in emails. In HLT-NAACL 2006.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In ACL ?10.
P. Mazur and R. Dale. 2007. The DANTE Temporal Ex-
pression Tagger. In Proceedings of the 3rd Language
and Technology Conference, Poznan, Poland.
Kathleen F. Mccoy and Michael Strube. 1999. Taking
time to structure discourse: Pronoun generation be-
yond accessibility. In Proc. of the 21th Annual Con-
ference of the Cognitive Science Society.
M. Moens and M. Steedman. 1988. Temporal ontology
and temporal reference. In Computational Linguistics,
volume 14(2), pages 15?28.
3Sec 2.6 at http://www.macs.hw.ac.uk/ilabarchive/classicproject/data/
150
J. Pustejovsky, J. Castano, R. Ingria, R. Sauri,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2003. TimeML: Robust specification of event and
temporal expressions in text. In AAAI Spring Sympo-
sium on New Directions in Question-Answering, Stan-
ford, CA.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005. Choos-
ing words in computer-generated weather forecasts.
Artificial Intelligence, 167:137169.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proc. ACL 2010.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards Developing General Models of
Usability with PARADISE. Natural Language Engi-
neering, 6(3).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413?456.
151
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 134?136,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Integrating Location, Visibility, and Question-Answering in a Spoken
Dialogue System for Pedestrian City Exploration
Srinivasan Janarthanam1, Oliver Lemon1, Xingkun Liu1, Phil Bartie2,
William Mackaness2, Tiphaine Dalmas3 and Jana Goetze4
1Interaction Lab, Heriot-Watt University, Edinburgh
2 School of GeoSciences, University of Edinburgh
3School of Informatics, University of Edinburgh
4KTH Royal Institute of Technology, Stockholm, Sweden
sc445,o.lemon,x.liu@hw.ac.uk, philbartie@gmail.com,
william.mackaness@ed.ac.uk,
tiphaine.dalmas@aethys.com, jagoetze@kth.se
Abstract
We demonstrate a spoken dialogue-based in-
formation system for pedestrians. The system
is novel in combining geographic information
system (GIS) modules such as a visibility en-
gine with a question-answering (QA) system,
integrated within a dialogue system architec-
ture. Users of the demonstration system can
use a web-based version (simulating pedes-
trian movement using StreetView) to engage
in a variety of interleaved conversations such
as navigating from A to B, using the QA func-
tionality to learn more about points of interest
(PoI) nearby, and searching for amenities and
tourist attractions. This system explores a va-
riety of research questions involving the inte-
gration of multiple information sources within
conversational interaction.
1 Motivation
Although navigation and local information are avail-
able to users through smartphone apps, there are still
important problems such as how such information is
delivered safely and proactively, and without cogni-
tively overloading the user. (Kray et al, 2003) sug-
gested that cognitive load of information presented
in textual and speech-based interfaces is medium
and low respectively when compared to more com-
plicated visual interfaces. Our objective, therefore,
is to build a hands-free and eyes-free system that en-
gages the pedestrian user by presenting all informa-
tion and receiving user requests through speech only.
In addition, and in contrast to other mobile ap-
plications, this system is conversational ? meaning
that it accumulates information over time, and plans
its utterances to achieve long-term goals. It inte-
grates with a city model and a visibility engine (Bar-
tie and Mackaness, 2012) to identify points of inter-
ests and visibile landmarks for presentation, a pedes-
trian tracker to improve the GPS positioning of the
user and a question-answering (QA) system to en-
able users to explore information about the city more
freely than with a graphical interface.
Table 1 presents an example dialogue interaction
with the system showing the use of visibility infor-
mation and Question-Answering.
User: Take me to Princes Street.
System: Turn left on to South Bridge and
walk towards the tower in front of you.
...
System: Near you is the famous statue of David Hume.
User: Tell me more about David Hume.
System: David Hume is a Scottish philosopher....
Table 1: An example interaction with the system
2 Related work
There are several mobile apps such as Triposo, Trip-
wolf, and Guidepal that provide point of interest
information, and apps such as Google Navigation
that provide navigation instructions to users. How-
ever, they demand the user?s visual attention because
they predominantly present information on a mobile
screen. In contrast, ours is a speech only interface
in order to keep the user?s cognitive load low and
avoid users from being distracted (perhaps danger-
134
ously so) from their primary task.
Generating navigation instructions in the real
world for pedestrians is an interesting research
problem in both computational linguistics and geo-
informatics (Dale et al, 2003; Richter and Duck-
ham, 2008). CORAL is an NLG system that gener-
ates navigation instructions incrementally upon user
requests based on the user?s location (Dale et al,
2003). DeepMap is a system that interacts with
the user to improve positioning using GUI controls
(Malaka and Zipf, 2000). SmartKom is a dialogue
system that presents navigation information multi-
modally (Reithinger et al, 2003). There are also
several mobile apps developed to help low-vision
users with navigation instructions (see (Stent et al,
2010) for example). In contrast to these earlier sys-
tems we present navigational, point-of-interest and
amenity information in an integrated way with users
interacting eyes-free and hands-free through a head-
set connected to a smartphone.
3 Architecture
The architecture of the current system is shown in
figure 1. The server side consists of a dialogue in-
terface (parser, interaction manager, and generator),
a City Model, a Visibility Engine, a QA server and a
Pedestrian tracker. On the user?s side is a web-based
client that consists of the simulated real-world and
the interaction panel.
Figure 1: System Architecture
3.1 Dialogue interface
The dialogue interface consists of an utterance
parser, an interaction manager and an utterance gen-
erator. The interaction manager is the central com-
ponent of this architecture, which provides the user
navigational instructions and interesting PoI infor-
mation. It receives the user?s input in the form of a
dialogue act and the user?s location in the form of
latitude and longitude information. Based on these
inputs and the dialogue context, it responds with sys-
tem output dialogue act (DA), based on a dialogue
policy. The utterance generator is a natural language
generation module that translates the system DA into
surface text, using the Open CCG toolkit (White et
al., 2007).
3.2 Pedestrian tracker
Global Navigation Satellite Systems (GNSS) (e.g.
GPS, GLONASS) provide a useful positioning so-
lution with minimal user side setup costs, for loca-
tion aware applications. However urban environ-
ments can be challenging with limited sky views,
and hence limited line of sight to the satellites, in
deep urban corridors. There is therefore signifi-
cant uncertainty about the user?s true location re-
ported by GNSS sensors on smartphones (Zandber-
gen and Barbeau, 2011). This module improves on
the reported user position by combining smartphone
sensor data (e.g. accelerometer) with map matching
techniques, to determine the most likely location of
the pedestrian (Bartie and Mackaness, 2012).
3.3 City Model
The city model is a spatial database containing in-
formation about thousands of entities in the city of
Edinburgh. These data have been collected from a
variety of existing resources such as Ordnance Sur-
vey, OpenStreetMap and the Gazetteer for Scotland.
It includes the location, use class, name, street ad-
dress, and where relevant other properties such as
build date. The model also includes a pedestrian net-
work (streets, pavements, tracks, steps, open spaces)
which can be used to calculate minimal cost routes,
such as the shortest path.
3.4 Visibility Engine
This module identifies the entities that are in the
user?s vista space (Montello, 1993). To do this it
accesses a digital surface model, sourced from Li-
DAR, which is a 2.5D representation of the city in-
cluding buildings, vegetation, and land surface ele-
vation. The visibility engine uses this dataset to offer
a number of services, such as determining the line
135
of sight from the observer to nominated points (e.g.
which junctions are visible), and determining which
entities within the city model are visible. These met-
rics can be then used by the interaction manager
to generate effective navigation instructions. E.g.
?Walk towards the castle?, ?Can you see the tower
in front of you??, ?Turn left after the large building
on your left after the junction? and so on.
3.5 Question-Answering server
The QA server currently answers a range of defini-
tion questions. E.g., ?Tell me more about the Scot-
tish Parliament?, ?Who was David Hume??, etc. QA
identifies the entity focused on in the question us-
ing machine-learning techniques (Mikhailian et al,
2009), and then proceeds to a textual search on texts
from the Gazetteer of Scotland and Wikipedia, and
definitions from WordNet glosses. Candidates are
reranked using a trained confidence score with the
top candidate used as the final answer. This answer
is provided as a flow of sentence chunks that the user
can interrupt. This information can also be pushed
by the system when a salient entity appears in the
user?s viewshed.
4 Web-based User interface
For the purposes of this (necessarily non-mobile)
demonstration, we present a web-based interface
that simulates users walking in a 3D city environ-
ment. Users will be able to provide speech or text
input (if the demonstration environment is too noisy
for usable speech recognition as is often the case at
conference demonstration sessions).
The web-based client is a JavaScript/HTML pro-
gram running on the user?s web browser. For a
detailed description of this component, please re-
fer to (Janarthanam et al, 2012). It consists of two
parts: the Streetview panel and the Interaction panel.
The Streetview panel presents a simulated real world
visually to the user. A Google Streetview client
(Google Maps API) is created with an initial user
coordinate which then allows the web user to get
a panoramic view of the streets around the user?s
virtual location. The user can walk around using
the arrow keys on his keyboard or the mouse. The
system?s utterances are synthesized using Cereproc
text-to-speech engine and presented to the user.
Acknowledgments
The research has received funding from the Eu-
ropean Community?s 7th Framework Programme
(FP7/2007-2013) under grant agreement no. 270019
(SPACEBOOK project http://www.spacebook-
project.eu/).
References
P. Bartie and W. Mackaness. 2012. D3.4 Pedestrian Po-
sition Tracker. Technical report, The SPACEBOOK
Project (FP7/2011-2014 grant agreement no. 270019).
R. Dale, S. Geldof, and J. Prost. 2003. CORAL : Using
Natural Language Generation for Navigational Assis-
tance. In Proceedings of ACSC2003, South Australia.
S. Janarthanam, O. Lemon, and X. Liu. 2012. A web-
based evaluation framework for spatial instruction-
giving systems. In Proc. of ACL 2012, South Korea.
C. Kray, K. Laakso, C. Elting, and V. Coors. 2003. Pre-
senting route instructions on mobile devices. In Pro-
ceedings of IUI 03, Florida.
R. Malaka and A. Zipf. 2000. Deep Map - challenging IT
research in the framework of a tourist information sys-
tem. In Information and Communication Technologies
in Tourism 2000, pages 15?27. Springer.
A. Mikhailian, T. Dalmas, and R. Pinchuk. 2009. Learn-
ing foci for question answering over topic maps. In
Proceedings of ACL 2009.
D. Montello. 1993. Scale and multiple psychologies of
space. In A. U. Frank and I. Campari, editors, Spatial
information theory: A theoretical basis for GIS.
N. Reithinger, J. Alexandersson, T. Becker, A. Blocher,
R. Engel, M. Lckelt, J. Mller, N. Pfleger, P. Poller,
M. Streit, and V. Tschernomas. 2003. SmartKom -
Adaptive and Flexible Multimodal Access to Multiple
Applications. In Proceedings of ICMI 2003, Vancou-
ver, B.C.
K. Richter and M. Duckham. 2008. Simplest instruc-
tions: Finding easy-to-describe routes for navigation.
In Proceedings of the 5th Intl. Conference on Geo-
graphic Information Science.
A. J. Stent, S. Azenkot, and B. Stern. 2010. Iwalk: a
lightweight navigation system for low-vision users. In
Proc. of the ASSETS 2010.
M. White, R. Rajkumar, and S. Martin. 2007. Towards
Broad Coverage Surface Realization with CCG. In
Proc. of the UCNLG+MT workshop.
P. A. Zandbergen and S. J. Barbeau. 2011. Positional
Accuracy of Assisted GPS Data from High-Sensitivity
GPS-enabled Mobile Phones. Journal of Navigation,
64(3):381?399.
136
Proceedings of the 14th European Workshop on Natural Language Generation, pages 115?124,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Generating student feedback from time-series data using Reinforcement
Learning
Dimitra Gkatzia, Helen Hastie, Srinivasan Janarthanam and Oliver Lemon
Department of Mathematical and Computer Sciences
Heriot-Watt University
Edinburgh, Scotland
{dg106, h.hastie, sc445, o.lemon} @hw.ac.uk
Abstract
We describe a statistical Natural Language
Generation (NLG) method for summarisa-
tion of time-series data in the context of
feedback generation for students. In this
paper, we initially present a method for
collecting time-series data from students
(e.g. marks, lectures attended) and use ex-
ample feedback from lecturers in a data-
driven approach to content selection. We
show a novel way of constructing a reward
function for our Reinforcement Learning
agent that is informed by the lecturers?
method of providing feedback. We eval-
uate our system with undergraduate stu-
dents by comparing it to three baseline
systems: a rule-based system, lecturer-
constructed summaries and a Brute Force
system. Our evaluation shows that the
feedback generated by our learning agent
is viewed by students to be as good as the
feedback from the lecturers. Our findings
suggest that the learning agent needs to
take into account both the student and lec-
turers? preferences.
1 Introduction
Data-to-text generation refers to the task of auto-
matically generating text from non-linguistic data
(Reiter and Dale, 2000). The goal of this work is
to develop a method for summarising time-series
data in order to provide continuous feedback to
students across the entire semester. As a case
study, we took a module in Artificial Intelligence
and asked students to fill out a very short diary-
type questionnaire on a weekly basis. Questions
included, for example, number of deadlines, num-
ber of classes attended, severity of personal issues.
These data were then combined with the marks
from the weekly lab reflecting the students? per-
formance. As data is gathered each week in the
lab, we now have a set of time-series data and our
goal is to automatically create feedback. The goal
is to present a holistic view through these diary en-
tries of how the student is doing and what factors
may be affecting performance.
Feedback is very important in the learning pro-
cess but very challenging for academic staff to
complete in a timely manner given the large num-
ber of students and the increasing pressures on
academics? time. This is where automatic feed-
back can play a part, providing a tool for teachers
that can give insight into factors that may not be
immediately obvious (Porayska-Pomsta and Mel-
lish, 2013). As reflected in NSS surveys1, stu-
dents are not completely satisfied with how feed-
back is currently delivered. The 2012 NSS survey,
for all disciplines reported an 83% satisfaction rate
with courses, with 70% satisfied with feedback.
This has improved from recent years (in 2006 this
was 60% for feedback) but shows that there is
still room for improvement in how teachers deliver
feedback and its content.
In the next section (Section 2) a discussion of
the related work is presented. In Section 3, a de-
scription of the methodology is given as well as
the process of the data collection from students,
the template construction and the data collection
with lecturers. In Section 4, the Reinforcement
Learning implementation is described. In Section
5, the evaluation results are presented, and finally,
in Sections 6 and 7, a conclusion and directions
for future work are discussed.
2 Related Work
Report generation from time-series data has been
researched widely and existing methods have been
used in several domains such as weather forecasts
(Belz and Kow, 2010; Angeli et al, 2010; Sripada
et al, 2004), clinical data summarisation (Hunter
1http://www.thestudentsurvey.com/
115
et al, 2011; Gatt et al, 2009), narrative to assist
children with communication needs (Black et al,
2010) and audiovisual debriefs from sensor data
from Autonomous Underwater Vehicles missions
(Johnson and Lane, 2011).
The two main challenges for time-series data
summarisation are what to say (Content Selec-
tion) and how to say it (Surface Realisation). In
this work we concentrate on the former. Previ-
ous methods for content selection include Gricean
Maxims (Sripada et al, 2003); collective con-
tent selection (Barzilay and Lapata, 2004); and
the Hidden Markov model approach for content
selection and ordering (Barzilay and Lee, 2004).
NLG systems tend to be very domain-specific
and data-driven systems that seek to simultane-
ously optimize both content selection and sur-
face realisation have the potential to be more
domain-independent, automatically optimized and
lend themselves to automatic generalization (An-
geli et al, 2010; Rieser et al, 2010; Dethlefs
and Cuayahuitl, 2011). Recent work on report
generation uses statistical techniques from Ma-
chine Translation (Belz and Kow, 2010), super-
vised learning (Angeli et al, 2010) and unsuper-
vised learning (Konstas and Lapata, 2012).
Here we apply Reinforcement Learning meth-
ods (see Section 4 for motivation) which have been
successfully applied to other NLG tasks, such as
Temporal Expressions Generation (Janarthanam
et al, 2011), Lexical Choice (Janarthanam and
Lemon, 2010), generation of adaptive restaurant
summaries in the context of a dialogue system
(Rieser et al, 2010) and generating instructions
(Dethlefs and Cuayahuitl, 2011).
3 Methodology
Figure 1: Methodology for data-driven feedback
report generation
Figure 1 shows graphically our approach to the de-
velopment of a generation system. Firstly, we col-
lected data from students including marks, demo-
graphic details and weekly study habits. Next, we
created templates for surface realisation with the
help of a Teaching and Learning expert. These
templates were used to generate summaries that
were rated by lecturers. We used these ratings to
train the learning agent. The output of the learning
agent (i.e. automatically generated feedback re-
ports) were finally evaluated by the students. Each
of these steps are discussed in turn.
3.1 Time-series Data Collection from
Students
The data were collected during the weekly lab ses-
sions of a Computer Science module which was
taught to third year Honours and MSc students
over the course of a 10 week semester. We re-
cruited 26 students who were asked to fill in a
web-based diary-like questionnaire. Initially, we
asked students to provide some demographic de-
tails (age, nationality, level of study). In addition,
students provided on a weekly basis, information
for nine factors that could influence their perfor-
mance. These nine factors were motivated from
the literature and are listed here in terms of effort
(Ames, 1992), frustration (Craig et al, 2004) , dif-
ficulty (Person et al, 1995; Fox, 1993) and per-
formance (Chi et al, 2001). Effort is measured
by three factors: (1) how many hours they studied;
(2) the level of revision they have done; (3) as well
as the number of lectures (of this module) they at-
tended. Frustration is measured by (4) the level
of understandability of the content; (5) whether
they have had other deadlines; and whether they
faced any (6) health and/or (7) personal issues and
at what severity. The difficulty of the lab exercises
is measured by (8) the students? perception of dif-
ficulty. Finally, (9) marks achieved by the students
in each weekly lab was used as a measure of their
performance.
3.2 Data Trends
Initially, the data were processed so as to iden-
tify the existing trend of each factor during the
semester, (e.g. number of lectures attending de-
creases). The tendencies of the data are estimated
using linear least-squares regression, with each
factor annotated as INCREASING, DECREAS-
ING or STABLE. In addition, for each student we
perform a comparison between the average of each
116
Type Description Examples
AVERAGE describes the factor data by either averaging the values given by
the student,
?You spent 2 hours studying the lecture material
on average?. (HOURS STUDIED)
or by comparing the student?s average with the class average
(e.g. if above the mean value for the class, we say that the ma-
terial is challenging).
?You found the lab exercises very challenging?.
(DIFFICULTY)
TREND discusses the trend of the data, e.g. increasing, decreasing or
stable.
?Your workload is increasing over the
semester?. (DEADLINES)
WEEKS talks about specific events that happened in one or more weeks. ?You have had other deadlines during weeks 5,
6 and 9?. (DEADLINES)
OTHER all other expressions that are not directly related to data. ?Revising material during the semester will im-
prove your performance?. (REVISION)
Table 1: The table explains the different template types.
factor and the class average of the same factor.
3.3 Template Generation
The wording and phrasing used in the templates to
describe the data were derived from working with
and following the advice of a Learning and Teach-
ing (L&T) expert. The expert provided consulta-
tion on how to summarise the data. We derived 4
different kinds of templates for each factor: AV-
ERAGE, TREND, WEEKS and OTHER based on
time-series data on plotted graphs. A description
of the template types is shown in Table 1.
In addition, the L&T expert consulted on how
to enhance the templates so that they are ap-
propriate for communicating feedback accord-
ing to the guidelines of the Higher Education
Academy (2009), for instance, by including moti-
vating phrases such as ?You may want to plan your
study and work ahead?.
3.4 Data Collection from Lecturers
The goal of the Reinforcement Learning agent is
to learn to generate feedback at least as well as
lecturers. In order to achieve this, a second data
collection was conducted with 12 lecturers partic-
ipating.
The data collection consisted of three stages
where lecturers were given plotted factor graphs
and were asked to:
1. write a free style text summary for 3 students
(Figure 2);
2. construct feedback summaries using the tem-
plates for 3 students (Figure 3);
3. rate random feedback summaries for 2 stu-
dents (Figure 4).
We developed the experiment using the Google
Web Toolkit for Web Applications, which facil-
itates the development of client-server applica-
tions. The server side hosts the designed tasks and
stores the results in a datastore. The client side is
responsible for displaying the tasks on the user?s
browser.
In Task 1, the lecturers were presented with the
factor graphs of a student (one graph per factor)
and were asked to provide a free-text feedback
summary for this student. The lecturers were en-
couraged to pick as many factors as they wanted
and to discuss the factors in any order they found
useful. Figure 2 shows an example free text sum-
mary for a high performing student where the lec-
turer decided to talk about lab marks and under-
standability. Each lecturer was asked to repeat this
task 3 times for 3 randomly picked students.
In Task 2, the lecturers were again asked to con-
struct a feedback summary but this time they were
given a range of sentences generated from the tem-
plates (as described in Section 2.3). They were
asked to use these to construct a feedback report.
The number of alternative utterances generated for
each factor varies depending on the factor and the
given data. In some cases, a factor can have 2 gen-
erated utterances and in other cases up to 5 (with
a mean of 3 for each factor) and they differenti-
ate in the style of trend description and wording.
Again the lecturer was free to choose which fac-
tors to talk about and in which order, as well as
to decide on the template style he/she prefers for
the realisation through the template options. Fig-
ure 3 shows an example of template selection for
the same student as in Figure 2.
In Task 3, the lecturers were presented with the
plotted factor graphs plus a corresponding feed-
back summary that was generated by randomly
choosing n factors and their templates, and were
asked to rate it in a scale between 0-100 (100 for
the best summary). Figure 4 shows an example of
117
Figure 2: The interface of the 1st task of the data collection: the lecturer consults the factor graphs and
provides feedback in a free text format.
Figure 3: The interface of the 2nd task of data collection: the lecturer consults the graphs and constructs
a feedback summary from the given templates (this graph refers to the same student as Figure 2).
a randomly generated summary for the same stu-
dent as in Figure 2.
4 Learning a Time-Series Generation
Policy
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns to
take optimal actions in a dynamic environment so
as to maximize a cumulative reward (Sutton and
Barto, 1998). In our framework, the task of con-
tent selection of time-series data is presented as a
Markov Decision problem. The goal of the agent
is to learn to choose a sequence of actions that
obtain the maximum expected reward in the long
run. In this section, we describe the Reinforce-
ment Learning setup for learning content selection
118
Figure 4: The interface of the 3rd task of data col-
lection: the lecturer consults the graphs and rates
the randomly generated feedback summary (this
graph refers to the same student as Figures 2 and
3).
from time-series data for feedback report gener-
ation. Summarisation from time-series data is an
open challenge and we aim to research other meth-
ods in the future, such as supervised learning, evo-
lutionary algorithms etc.
4.1 Actions and States
In this learning setup, we focused only on select-
ing the correct content, i.e. which factors to talk
about. The agent selects a factor and then decides
whether to talk about it or not. The state consists
of a description of the factor trends and the num-
ber of templates that have been selected so far. An
example of the initial state of a student can be:
<marks increased, lectures attended stable,
hours studied increased, understandability stable,
difficulty increased, health issues stable, per-
sonal issues stable, revision increased, 0>
The agent explores the state space by selecting a
factor and then by deciding whether to talk about
it or not. If the agent decides to talk about the
selected factor, it chooses the template in a greedy
way, i.e. it chooses for each factor the template
that results in a higher reward. After an action has
been selected, it is deleted from the action space.
4.1.1 Ordering
In order to find out in which order the lectur-
ers describe the factors, we transformed the feed-
back summaries into n-grams of factors. For in-
stance, a summary that talks about the student?s
performance, the number of lectures that he/she
attended, potential health problems and revision
done can be translated into the following ngram:
start, marks, lectures attended, health issues, re-
vision, end. We used the constructed n-grams to
compute the bigram frequency of the tokens in or-
der to identify which factor is most probable to be
referred to initially, which factors follow particu-
lar factors and which factor is usually talked about
in the end. It was found that the most frequent or-
dering is: start, marks, hours studied, understand-
ability, difficulty, deadlines, health issues, per-
sonal issues, lectures attended, revision, end.
4.2 Reward Function
The goal of the reward function is to optimise the
way lecturers generate and rate feedback. Given
the expert annotated summaries from Task 1, the
constructed summaries from Task 2 and the ratings
from Task 3, we derived the multivariate reward
function:
Reward = a +
n?
i=1
bi ? xi + c ? length
where X = {x1, x2, ..., xn} represents the
combinations between the data trends observed in
the time-series data and the corresponding lectur-
ers? feedback (i.e. whether they included a factor
to be realised or not and how). The value xi for
factor i is defined by the function:
xi =
?
?????
?????
1, the combination i of a factor trend
and a template type is included in
the feedback
0, if not.
For instance, the value of x1 is 1 if marks were
increased and this trend is realised in the feedback,
otherwise it is 0. In our domain n = 90 in order to
cover all the different combinations. The length
stands for the number of factors selected, a is the
intercept, bi and c are the coefficients for xi and
length respectively.
In order to model the reward function, we used
linear regression to compute the weights from the
data gathered from the lecturers. Therefore, the
reward function is fully informed by the data pro-
vided by the experts. Indeed, the intercept a, the
vector weights b and the weight c are learnt by
making use of the data collected by the lecturers
from the 3 tasks discussed in Section 3.4.
The reward function is maximized (Reward
= 861.85) for the scenario (i.e. each student?s
data), content selection and preferred template
style shown in Table 2 (please note that this sce-
nario was not observed in the data collection).
119
Factor Trend Template
difficulty stable NOT MENTIONED
hours studied stable TREND
understandability stable NOT MENTIONED
deadlines increase WEEKS
health issues stable WEEKS
personal issues stable WEEKS
lectures att. stable WEEKS
revision stable OTHER
marks increase TREND
Table 2: The table shows the scenario at which the
reward function is maximised.
The reward function is minimized (Reward =
-586.0359) for the scenario shown in Table 3
(please note that this scenario also was not ob-
served in the data collection).
Factor Trend Template
difficulty increase AVERAGE
hours studied stable NOT MENTIONED
understandability decrease AVERAGE
deadlines * *
health issues increase TREND
personal issues stable TREND
lectures att. stable NOT MENTIONED
revision stable AVERAGE
marks stable TREND
Table 3: The table shows the scenario at which the
reward function is minimised (* denotes multiple
options result in the same minimum reward).
4.3 Training
We trained a time-series generation policy
for 10,000 runs using the Tabular Temporal-
Difference Learning (Sutton and Barto, 1998).
During the training phase, the learning agent gen-
erated feedback summaries. When the construc-
tion of the summary begins, the length of the sum-
mary is 0. Each time that the agent adds a template
(by selecting a factor), the length is incremented,
thus changing the state. It repeats the process until
it decides for all factors whether to talk about them
or not. The agent is finally rewarded at the end of
the process using the Reward function described
in Section 3.2. Initially, the learning agent selects
factors randomly, but gradually learns to identify
factors that are highly rewarding for a given data
scenario. Figure 5 shows the learning curve of the
agent.
Figure 5: Learning curve for the learning agent.
The x-axis shows the number of summaries pro-
duced and y- axis the total reward received for
each summary.
5 Evaluation
We evaluated the system using the reward func-
tion and with students. In both these evaluations,
we compared feedback reports generated using
our Reinforcement Learning agent with four other
baseline systems. Here we present a brief descrip-
tion of the baseline systems.
Baseline 1: Rule-based system. This system
selects factors and templates for generation using a
set of rules. These hand-crafted rules were derived
from a combination of the L&T expert?s advice
and a student?s preferences and is therefore a chal-
lenging baseline and represents a middle ground
between the L&T expert?s advice and a student?s
preferences. An example rule is: if the mark aver-
age is less than 50% then refer to revision.
Baseline 2: Brute Force system. This system
performs a search of the state space, by exploring
randomly as many different feedback summaries
as possible. The Brute Force algorithm is shown
below:
Algorithm 1 Brute Force algorithm
I n p u t d a t a : D
f o r n = 0 . . . 1 0 , 0 0 0
c o n s t r u c t randomly f e e d b a c k [ n ]
a s s i g n getReward [ n ]
i f ge tReward [ n]>getReward [ n?1]
b e s t F e e d b a c k = f e e d b a c k [ n ]
e l s e
b e s t F e e d b a c k = f e e d b a c k [ n?1]
r e t u r n b e s t F e e d b a c k
In each run the algorithm constructs a feedback
summary, then it calculates its reward, using the
same reward function used for the Reinforcement
Learning approach, and if the reward of the new
feedback is better than the previous, it keeps the
120
new one as the best. It repeats this process for
10,000 times for each scenario. Finally, the algo-
rithm returns the summary that scored the highest
ranking.
Baseline 3: Lecturer-produced summaries.
These are the summaries produced by the lectur-
ers, as described in Section 2.4, for Task 2 using
template-generated utterances.
Baseline 4: Random system: The Random
system constructs feedback summaries by select-
ing factors and templates randomly as described in
Task 3 (in Section 3.4).
5.1 Evaluation with Reward Function
Table 4 presents the results of the evaluation per-
formed using the Reward Function, comparing
the learned policy with the four baseline systems.
Each system generated 26 feedback summaries.
On average the learned policy scores significantly
higher than any other baseline for the given sce-
narios (p <0.05 in a paired t-test).
Time-Series Summarisation Systems Reward
Learned 243.82
Baseline 1: Rule-based 107.77
Baseline 2: Brute Force 241.98
Baseline 3: Lecturers 124.62
Baseline 4: Random 43.29
Table 4: The table summarises the average re-
wards that are assigned to summaries produced
from the different systems.
5.2 Evaluation with Students
A subjective evaluation was conducted using 1st
year students of Computer Science as participants.
We recruited 17 students, who were all English na-
tive speakers. The participants were shown 4 feed-
back summaries in a random order, one generated
by the learned policy, one from the rule-based sys-
tem (Baseline 1), one from the Brute Force system
(Baseline 2) and one summary produced by a lec-
turer using the templates (Baseline 3). Given the
poor performance of the Random system in terms
of reward, Baseline 4 was omitted from this study.
Overall there were 26 different scenarios, as de-
scribed in Section 3.1. All summaries presented
to a participant were generated from the same sce-
nario. The participants then had to rank the sum-
maries in order of preference: 1 for the most pre-
ferred and 4 for the least preferred. Each partici-
pant repeated the process for 4.5 scenarios on aver-
age (the participant was allowed to opt out at any
stage). The mode values of the rankings of the
preferences of the students are shown in Table 5.
The web-based system used for the evaluation is
shown in Figure 6.
System Mode of Rankings
Learned 3rd
Baseline 3: Lecturers 3rd
Baseline 1: Rule-based 1st
Baseline 2: Brute Force 4th
Table 5: The table shows the mode value of the
rankings of the preference of the students.
We ran a Mann-Whitney?s U test to evaluate the
difference in the responses of our 4-point Likert
Scale question between the Learned system and
the other three baselines. It was found that, for
the given data, the preference of students for the
feedback generated by the Learned system is as
good as the feedback produced by the experts, i.e.
there is no significant difference between the mean
value of the rankings of the Learned system and
the lecturer-produced summaries (p = 0.8) (Base-
line 3).
The preference of the users for the Brute Force
system does not differ significantly from the sum-
maries generated by the Learned system (p =
0.1335). However, the computational cost of the
Brute Force is higher because each time that the
algorithm sees a new scenario it has to run ap-
proximately 3k times to reach a good summary (as
seen in Figure 7) and about 10k to reach an optimal
one, which corresponds to 46 seconds. This delay
would prohibit the use of such a system in time-
critical situations (such as defence) and in live sys-
tems such as tutoring systems. In addition, the
processing time would increase with more compli-
cated scenarios and if we want to take into account
the ordering of the content selection and/or if we
have more variables. In contrast, the RL method
needs only to be trained once.
Finally, the users significantly preferred the
summaries produced by the Rule-based system
(Baseline 1) to the summaries produced by the
Learned system. This is maybe because of the fact
that in the rule-based system some knowledge of
the end user?s preferences (i.e. students) was taken
into account in the rules which was not the case
in the other three systems. This fact suggests that
121
Figure 6: The interface for the evaluation: the students viewed the four feedback summaries and ranked
them in order of preference. From left to right, the summaries as generated by: an Expert (Baseline 3),
the Rule based system (Baseline 1), the Brute Force algorithm (Baseline 2), the Learned system.
Figure 7: The graphs shows the number of cycles
that the Brute Force algorithm needs to achieve
specific rewards.
students? preferences should be taken into account
as they are the receivers of the feedback. This can
also be generalised to other areas, where the ex-
perts and the end users are not the same group
of people. As the learned policy was not trained
to optimise for the evaluation criteria, in future,
we will explore reward functions that bear in mind
both the expert knowledge and the student?s pref-
erences.
6 Conclusion
We have presented a statistical learning approach
to summarisation from time-series data in the area
of feedback reports. In our reports, we took into
account the principles of good feedback provision
as instructed by the Higher Education Academy.
We also presented a method for data gathering
from students and lecturers and show how we can
use these data to generate feedback by presenting
the problem as a Markov Decision Process and
optimising it using Reinforcement Learning tech-
niques. We also showed a way of constructing a
data-driven reward function that can capture de-
pendencies between the time-series data and the
realisation phrases, in a similar way that the lec-
turers do when providing feedback. Finally, our
evaluation showed that the learned report genera-
tion policy generates reports as well as lecturers.
7 Future Work
We aim to conduct further qualitative research in
order to explore what factors and templates stu-
dents find useful to be included in the feedback
and inform our reward function with this informa-
tion as well as what we have observed in the lec-
turer data collection. This way, we hope, not only
to gain insights into what is important to students
and lecturers but also to develop a data-driven ap-
proach that, unlike the rule-based system, does not
require expensive and difficult-to-obtain expert in-
put from Learning and Teaching experts. In ad-
dition, we want to compare RL techniques with
supervised learning approaches and evolutionary
algorithms. Finally, we want to unify content se-
122
lection and surface realisation, therefore we will
extend the action space in order to include actions
for template selection.
8 Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
References
Carole Ames. 1992. Classrooms: Goals, Structures,
and Student Motivation. Journal of Educational Psy-
chology, 84(3):p261-71.
Gabor Angeli, Percy Liang and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. EMNLP ?10: Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing.
Regina Barzilay and Mirella Lapata. 2004. Collec-
tive content selection for concept-to-text generation.
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. HLT-NAACL
2004: Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics.
Anja Belz and Eric Kow. 2010. Extracting parallel
fragments from comparable corpora for data-to-text
generation. INLG ?10: Proceedings of the 6th Inter-
national Natural Language Generation Conference.
Rolf Black, Joe Reddington, Ehud Reiter, Nava
Tintarev, and Annalu Waller. 2010. Using NLG and
Sensors to Support Personal Narrative for Children
with Complex Communication Needs. SLPAT ?10:
Proceedings of the NAACL HLT 2010 Workshop on
Speech and Language Processing for Assistive Tech-
nologies.
Michelene T.H. Chi, Stephanie A. Siler, Heisawn
Jeong, Takashi Yamauchi, Robert G. Hausmann.
2001. Learning from human tutoring. Journal of
Cognitive Science, 25(4):471-533.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
Barry Gholson. 2004. Affect and learning: an ex-
ploratory look into the role of affect in learning with
AutoTutor. Journal of Educational Media, 29:241-
250.
Nina Dethlefs and Heriberto Cuayahuitl. 2011.
Combining hierarchical reinforcement learning and
bayesian networks for natural language generation
in situated dialogue. ENLG ?11: Proceedings of the
13th European Workshop on Natural Language Gen-
eration.
Barbara Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the Design of Instructional Sys-
tems. Lawrence Erlbaum Associates, Hillsdale,
New Jersey.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood,Wendy Moncur, and So-
mayajulu Sripada. 2009. From Data to Text in the
Neonatal Intensive Care Unit: Using NLG Technol-
ogy for Decision Support and Information Manage-
ment. Journal of AI Communications, 22:153-186.
Higher Education Academy. 2009. Providing individ-
ual written feedback on formative and summative
assessments. http://www.heacademy.
ac.uk/assets/documents/resources/
database/id353_senlef_guide.pdf.
Last modified September 16.
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. BT-Nurse:
Computer Generation of Natural Language Shift
Summaries from Complex Heterogeneous Medical
Data. Journal of the American Medical Informatics
Association,18:621-624.
Srinivasan Janarthenam, Helen Hastie, Oliver Lemon,
Xingkun Liu. 2011. ?The day after the day after to-
morrow?? A machine learning approach to adaptive
temporal expression generation: training and evalu-
ation with real users. SIGDIAL ?11: Proceedings of
the SIGDIAL 2011 Conference.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive Referring Expression Generation in Spo-
ken Dialogue Systems: Evaluation with Real Users.
SIGDIAL ?10: Proceedings of the 11th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue.
Nicholas A. R. Johnson and David M. Lane. 2011.
Narrative Monologue as a First Step Towards Ad-
vanced Mission Debrief for AUV Operator Situa-
tional Awareness. In the 15th International Confer-
ence on Advanced Robotics.
Ioannis Konstas and Mirella Lapata. 2012. Unsuper-
vised concept-to-text generation with hypergraphs.
NAACL HLT ?12: Proceedings of the 2012 Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan and
Arthur C. Graesser. 1995. Pragmatics and Peda-
gogy: Conversational Rules and Politeness Strate-
gies May Inhibit Effective Tutoring. Journal of Cog-
nition and Instruction, 13(2):161-188.
Kaska Porayska-Pomsta and Chris Mellish. 2013.
Modelling human tutors? feedback to inform natural
language interfaces for learning. International Jour-
nal of Human-Computer Studies,71(6):703724.
123
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation systems. Cambridge Univer-
sity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. ACL ?10: Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
Somayajulu Sripada, Ehud Reiter, I Davy, and K
Nilssen. 2004. Lessons from Deploying NLG Tech-
nology for Marine Weather Forecast Text Gener-
ation. In Proceedings of PAIS session of ECAI-
2004:760-764.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Generating English Summaries of Time
Series Data using the Gricean Maxims. KDD ?03:
Proceedings of the ninth ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment Learning. MIT Press.
124
Proceedings of the SIGDIAL 2013 Conference, pages 151?153,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A Multithreaded Conversational Interface for Pedestrian Navigation and
Question Answering
Srinivasan Janarthanam1, Oliver Lemon1, Xingkun Liu1, Phil Bartie2,
William Mackaness2, Tiphaine Dalmas3
1Interaction Lab, Heriot-Watt University, Edinburgh
2 School of GeoSciences, University of Edinburgh
3School of Informatics, University of Edinburgh
sc445,o.lemon,x.liu@hw.ac.uk, philbartie@gmail.com,
william.mackaness@ed.ac.uk, tiphaine.dalmas@aethys.com
Abstract
We demonstrate a conversational interface
that assists pedestrian users in navigat-
ing within urban environments and acquir-
ing tourist information by combining spo-
ken dialogue system, question-answering
(QA), and geographic information sys-
tem (GIS) technologies. In contrast to
existing mobile applications which treat
these problems independently, our An-
droid agent addresses the problem of navi-
gation and touristic question-answering in
an integrated fashion using a shared dia-
logue context with multiple interleaved di-
alogue threads. In this paper, we present
the architecture and features of our lat-
est system, extended from an earlier ver-
sion which was built and evaluated with
real users (Janarthanam et al, 2013). The
new features include navigation based on
visible landmarks, navigation adapted to
the user?s previous route knowledge, and
tourist information pushing based on vis-
ible and proximal points-of-interest. The
system also uses social media to infer
?popularity? of geographical entities.
1 Introduction
We demonstrate a conversational interface that ad-
dresses the problems of pedestrian navigation and
Question Answering (QA) in urban environments,
which is an extended version of the system eval-
uated in (Janarthanam et al, 2013). Our system
brings together Spoken Dialogue Systems (SDS)
and Question Answering (QA) with Geographic
Information Systems (GIS) for user positioning
and tracking, city modelling and urban visibility
modelling. There has been little prior work that
addresses these problems - navigation, visibility,
and tourist information provision - in an integrated
way. By navigation, we refer to the problem of
finding appropriate destinations to go to and the
task of wayfinding to reach them (Golledge, 1992;
Raubal and Winter, 2002) and by tourist informa-
tion provision we refer to the problem of meet-
ing the informational needs of a user about enti-
ties such as museums, statues, famous personal-
ities, etc. With the proliferation of smartphones,
there has been a number of mobile apps developed
to address these problems. However, these apps
have the following problems: first, they demand
the user?s visual attention because they predom-
inantly present information on a mobile screen.
This can be dangerous in urban environments, as
well as being distracting. Second, these apps ad-
dress the problems of navigation and tourist infor-
mation independently and do not employ a shared
interaction context. This means that users cannot
switch between tourist information and navigation
tasks in a natural and fluid manner.
User1: Take me to the National Museum.
System2: The National Museum is about 300 meters away ...
System3: At the KFC, turn left on to South Bridge ...
System4 : Near you is the statue of David Hume.
User2: Who is David Hume.
System5: David Hume was a Scottish philosopher....
User3: Tell me more about David Hume.
System6: He was one of the most important figures in.. ...
System7: You should be able to see the museum in front of you.
User4: Tell me more about the museum.
System8: The National Museum of Scotland is a....
Table 1: An example interaction with SpaceBook
In contrast to existing mobile apps, we present
a system that has a speech-only interface and ad-
dresses navigation and QA in an integrated way.
Table 1 presents an example interaction with
our system showing the integrated use of navi-
gation and question-answering capabilities. Ut-
151
terance System4-8 shows the system?s capability
to push information from the QA system about
nearby points-of-interest (PoI) during a navigation
task, which the user can then follow-up on (in ut-
terances User2 and User3). The final 3 utterances
show a natural switch between navigation to an en-
tity and QA about that entity.
2 Related work
Mobile applications such as Siri, Google Maps
Navigation, Sygic, etc. address the problem of
navigation while applications like Triposo, Guide-
pal, Wikihood, etc. address the problem of tourist
information by presenting the user with descrip-
tive information about various points of interest
(PoI) in the city. While some exploratory applica-
tions present snippets of information about a pre-
compiled list of PoI, others applications dynam-
ically generate a list of PoI arranged based on
their proximity to the users. Users can also ob-
tain specific information about PoI using Search
applications. Also, since these navigation and ex-
ploratory/search applications do not address both
problems in an integrated way, users need to
switch between them and therefore lose interac-
tion context.
While most applications address these two
problems independently, some like Google Now,
Google Field Trip, etc, mix navigation with ex-
ploration. However, such applications present in-
formation primarily visually on the screen for the
user to read. In contrast, our system has the objec-
tive of keeping the user?s cognitive load low and
preventing users from being distracted (perhaps
dangerously so) from walking in the city (Kray et
al., 2003). Also, our system allows users to inter-
leave the two sub-tasks seamlessly and can keep
entities discussed in both tasks in shared context
(as shown in Table 1).
Several systems have addressed the issue of
pedestrian navigation (Malaka and Zipf, 2000;
Dale et al, 2003; Heinroth and Buhler, 2008).
Some dialogue systems deal with presenting in-
formation concerning points of interest (Ko et al,
2005; Misu and Kawahara, 2007; Kashioka et al,
2011). In contrast to all these earlier work, we
demonstrate a system that deals with both naviga-
tion and tourist information issues in an integrated
fashion.
Figure 1: System Architecture
3 Multithreaded dialogue management
The architecture of the current system is shown
in figure 1. The Interaction Manager (IM) is
the central component of this architecture, which
provides the user with navigational instructions,
pushes PoI information and manages QA ques-
tions. It receives the user?s input in the form of
a dialogue act (DA) from the ASR module and
the user?s location (latitude and longitude), orien-
tation and speed from the Pedestrian Tracker mod-
ule. Based on these inputs and the dialogue con-
text, the IM responds with a system output dia-
logue act. The Interaction Manager manages the
conversation using five coversational threads: di-
alogue control, response, navigation, question an-
swering, and PoI pushing. These different threads
represent the state of different dimensions of the
user-system conversation that interleave with each
other. Each of these threads generates a dialogue
action based on a dialogue policy. A dialogue pol-
icy is a mapping between dialogue states and dia-
logue actions, which are semantic representations
of what the system wants to say next. Dialogue
actions from the five threads are stored in five sep-
arate queues.
The queues are assigned priorities that decide
the order in which items from the queues will
be popped. For instance, informing the user of
a PoI could be delayed if the user needs to be
given an instruction to turn at the junction he is
approaching. For this reason, priority is assigned
to dialogue threads as follows.
Priority 1. Dialogue control (calibration phase,
repeat request, clarifications etc)
Priority 2. Responding to user requests
Priority 3. System initiated navigation task actions
Priority 4. Responses to User initiated QA actions
Priority 5. PoI Push actions
152
Dialogue control The IM initiates the conversa-
tion with a calibration phase where the user?s ini-
tial location and orientation are obtained. In this
phase, the IM requests the user to walk a few yards
so that the pedestrian tracker can sense the user?s
location and orientation. During the course of the
coversation, the IM uses this thread to manage
repeat requests, issues with unparsed user utter-
ances, utterances that have low ASR confidence,
and so on. The dialogue control thread is used to
manage reference resolution in cases where refer-
ring expressions are underspecified.
Navigation The IM identifies the location of the
destination entity and queries the City Model for a
route plan. The plan provides information such as
numbers of exits at junctions, the exit number the
user should take, turn angle, popularity index of
the street, and the slope of the road. In an attempt
to adapt the route instructions to user route knowl-
edge, the IM first picks the most popular street in
the plan and asks the users if they can get to the
street on their own. Also, the IM queries the Visi-
bility Engine (VE) for highly salient visible land-
marks (computed using Flickr tags) that can used
to direct the user. Instructions based on visible
landmarks are given whenever possible.
Question Answering The system also answers
ad hoc questions from the user (e.g. ?Who is David
Hume??, ?What is the Old College??, etc). These
are sent to the QA server and answered based on
responses from the QA server. The dialogue pol-
icy here is to answer the user?s question with the
first snippet available and ask the user to request
for more if interested.
Pushing PoI Information When the user is mo-
bile, the IM identifies points of interest on the
route based on two factors: proximity and visibil-
ity. Proximity push is done by checking for PoIs
near the user using high-scoring ones when there
are many, based on tourist popularity ratings in the
City Model. Visibility push is done by querying
the VE for salient entities visible to the user that
may be worth pushing. The dialogue policy is to
introduce the PoI entity along with visual descrip-
tors if available. The IM queries the QA server for
snippets on entity and if available, pushes them the
first snippet to the user. The user is encouraged to
ask for more if interested.
4 Conclusion
We demonstrate a mobile conversational system
to support pedestrian users in navigation and
question-answering tasks in urban environments.
The system is a speech-only interface and inter-
leaves navigation and tourist information in an in-
tegrated way, using a shared dialogue context. For
example, using the navigational context, our sys-
tem can push point-of-interest information which
can then initiate touristic exploration tasks using
the QA module. An evaluation of an earlier ver-
sion was reported in (Janarthanam et al, 2013).
Acknowledgments
The research leading to these results was funded by the Eu-
ropean Commission?s Framework 7 programme under grant
agreement no. 270019 (SPACEBOOK project).
References
R. Dale, S. Geldof, and J. Prost. 2003. CORAL : Using Nat-
ural Language Generation for Navigational Assistance. In
Proceedings of ACSC2003, South Australia.
R. G. Golledge. 1992. Place recognition and wayfinding:
Making sense of space. Geoforum, 23.
T. Heinroth and D. Buhler. 2008. Arrigator: evaluation of
a speech-based pedestrian navigation system. In Proceed-
ings of 4th International Conference on Intelligent Envi-
ronments, 2008.
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dick-
inson, X. Liu, W. Mackaness, and B. Webber. 2013.
Evaluating a city exploration dialogue system combining
question-answering and pedestrian navigation. In Proc.
ACL 2013.
H. Kashioka, T. Misu, E. Mizukami, Y. Shiga, K. Kayama,
C. Hori, and H. Kawai. 2011. Multimodal Dialog System
for Kyoto Sightseeing Guide. In Asia-Pacific Signal and
Information Processing Association Conference.
J. Ko, F. Murase, T. Mitamura, E. Nyberg, M. Tateishi,
I. Akahori, and N. Hataoka. 2005. CAMMIA: A Context-
Aware Spoken Dialog System for Mobile Environments.
In IEEE Automatic Speech Recognition and Understand-
ing Workshop.
C. Kray, K. Laakso, C. Elting, and V. Coors. 2003. Present-
ing route instructions on mobile devices. In Proceedings
of IUI 03, Florida.
R. Malaka and A. Zipf. 2000. Deep Map - challenging IT
research in the framework of a tourist information sys-
tem. In Information and Communication Technologies in
Tourism 2000, pages 15?27. Springer.
T. Misu and T. Kawahara. 2007. An Interactive Framework
for Document Retrieval and Presentation with Question-
Answering Function in Restricted Domain. In Proc. of
the 26th IEA/AIE conference, pages 126?134.
M. Raubal and S. Winter. 2002. Enriching wayfinding in-
structions with local landmarks. In Second International
Conference GIScience. Springer, Boulder, USA.
153
Proceedings of the SIGDIAL 2013 Conference, pages 363?365,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Emote Wizard of Oz Interface for Empathic
Robotic Tutors
Shweta Bhargava1, Srinivasan Janarthanam1, Helen Hastie1, Amol Deshmukh1,
Ruth Aylett1, Lee Corrigan2, Ginevra Castellano 2
1School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
2School of Electronic, Electrical and Computer Engineering, University of Birmingham
sb426,sc445,h.hastie,a.deshmukh,r.s.aylett@hw.ac.uk,
ljc228,g.castellano@bham.ac.uk
Abstract
We present a Wizard of Oz (WoZ) envi-
ronment that was designed to build an arti-
ficial embodied intelligent tutoring system
(ITS) that is capable of empathic conver-
sations with school pupils aged between
10-13. We describe the components and
the data that we plan to collect using the
environment.
1 Introduction
We present a Wizard of Oz (WoZ) environment
that was built as a part of the EC FP7 EMOTE
project1. The objective of this work is to col-
lect multimodal interaction data to build an arti-
ficial embodied intelligent tutoring system (ITS)
that is capable of empathic conversations with
school pupils aged between 10-13. Specifically,
the EMOTE (EMbOdied-perceptive Tutors for
Empathy-based learning) project aims to design
and evaluate a new generation of robotic tutors
that have perceptive and expressive capabilities
to engage in empathic interactions with learners
in schools and home environments. The project
will carry out interdisciplinary research on affect
recognition, learner models, adaptive behaviour
and embodiment for human-robot interaction in
learning environments, grounded in psychologi-
cal theories of emotion in social interaction and
pedagogical models for learning facilitation. An
overview of the project can be found in (Desh-
mukh et al, 2013).
Wizard of Oz is an effective technique in Hu-
man Computer Interaction (HCI) studies where
an interactive agent, which is not yet fully au-
tonomous, is remotely controlled by a human wiz-
1http://emote-project.eu/
ard. However the participants who are interacting
with the agent are not told that the agent is being
remotely controlled. The wizard may be tasked
to control one or many parts of the agent such
as speech recognition and understanding, affect
recognition, dialogue management, utterance and
gesture generation and so on. Studies have shown
that users ?go easy? on computers during inter-
action and therefore interaction with ?wizarded?
system are at the level of complexity that can be
learned and emulated (Pearson et al, 2006).
The WoZ environment presented in this paper
will be used to collect data to inform the algo-
rithms for affect recognition and empathic dia-
logue management. The WoZ environment is de-
signed to collect data on how human tutors aided
with a robotic interface adapt to learners? emotions
and cognitive states in tutorial tasks. In this study,
the wizard plays the same role as that of affect
recognition and dialogue management modules in
the actual final system.
2 Previous work
Wizard-of-Oz (WoZ) frameworks have been used
in several studies since (Fraser and Gilbert, 1991)
in order to collect human-computer dialogue data
to help design dialogue systems. WoZ systems
have been used to collect data to learn (e.g.
(Strauss et al, 2007)) and evaluate dialogue man-
agement policies (e.g. (Cuaya?huitl and Kruijff-
Korbayova, 2012)).
3 The EMOTE Wizard of Oz
environment
The WoZ environment consists of the wizard?s
desk, the interactive touch table, sensors, and the
robotic embodiment as shown in Figure 1. The
363
wizard will be seated in a different room away
from the learner.
Figure 1: Wizard of Oz environment
3.1 Wizard?s desk
The wizard?s desk consists of two display screens.
The touch table display at the user end will be mir-
rored on to one of the displays at the wizard?s desk
using which the wizard can observe the learner?s
activities related to the educational application.
Another display will contain the Wizard Interface,
a software application that allows the wizard to in-
teract with the learner (see Figure 2). The Wiz-
ard Interface consists of four panels: task control,
information, learner response and operations. In
the task control panel, the wizard will be
able to choose a task plan for the learner and ac-
cess the tool and curriculum scripts (XML file).
The tool script contains information on how to use
the tools that are at the disposal of the learner. For
instance, to create a marker on the map, one has
to click on the appropriate tool and click on the
map and so on. The curriculum script contains
information on the skills that the learner needs
to exercise or develop during his interaction with
the system. For instance, in order to identify the
right direction, the system will present the mneu-
monic phrase ?Naughty Elephants Squirt Water?
in various forms such as a hint, question, pump-
ing move, etc. to provide support to the learner.
The information panel contains the video
feed from two cameras (see Section 3.4). This
will allow the wizard to determine the affective
state of the learner. The learner?s response to the
agent?s utterances (such as answering questions in
the curriculum scripts) will also be displayed in
the learner response panel. Finally, the
operations panel provides options for the
Wizard to respond to the learner based on the tools
and curriculum scripts. These responses are ei-
ther customised or predefined. The customised
responses facilitate the wizard to execute robot
movements on lower level (individual head, arm
movements) and predefined responses contain a
list for combined predefined speech, sound and be-
haviours.
Figure 2: Wizard?s Interface
3.2 Touch table
The interactive touch table is a 55 inch Multitac-
tion table capable of sensing multiple touch events
simultaneously. The educational application is
displayed on the table surface. A map based appli-
cation has been developed to teach learners basic
and advanced map reading skills (see Figure 3).
The touch interface allows the learner to use touch
to click, drag and zoom the map. The application
has two panels of GUI objects such as buttons and
text boxes namely, the tools panel and the interac-
tion panel. The tools panel consists of tools that
the learner can use to manipulate the map, while
using the interaction panel the learner can interact
with the tutor. Some of the tools that are currently
available are to get grid references for a position
on the map, dropping markers on the map, change
map types, etc. For instance, if the tutor asks a
yes/no question, the learner can respond by press-
ing the yes or the no button. The learner can an-
swer the tutor?s questions by typing into the text
box in the interaction panel.
364
Figure 3: Map reading skills application
3.3 Robotic embodiment
The robotic embodiment is a Nao robot (torso ver-
sion) that sits on the side of the touch table. It is
capable of head, arm and body gestures in addi-
tion to synthesised speech. The robot receives the
text and gestures selected by the wizard through
the Wizard Interface. Tutor?s utterances will be
synthesized into speech using the in-built text to
speech (TTS) engine while the gestures are re-
alised using appropriate head, arm and body mo-
tions. To increase naturalness, the robot will also
have idle movement in-between wizard selections.
3.4 Sensors
The environment has an array of sensors such as
two video cameras and a Kinect sensor. A Kinect
sensor and a video camera are placed in front the
learner. Another camera is placed in front of the
robot (as shown in Figure 1).
4 Data collection
In this section, we discuss the data that we aim
to collect using the WoZ environment. We intend
to collect these data during experiments where hu-
man tutors play the wizard?s role and the learners
from in the 10-13 year age-range will play the role
of learners. The task for the learner is to carry
out an expedition using the map application that
he or she is provided with. In order to solve the
steps of the expedition, the learner will have to
exercise his/her map reading skills. Map reading
skills such as compass directions, contour lines,
grid lines, etc. will have to be exercised using
appropriate map tools provided in the application.
The tutor?s role is to observe the learner responses
(both verbal and physical) and respond to them ap-
propriately using the interaction panel in the Wiz-
ard Interface application.
Simultaneous video feeds from two cameras
and the Kinect sensor will be recorded during the
tutor-learner interaction. These data will be fur-
ther used for affect recognition tasks based on
learner?s head, arm and body gestures. The inter-
action between the tutor and the learner in terms
of tutor dialogue actions, utterances and learner
responses in terms of button presses will also be
logged.
5 Demo
We propose to demonstrate the WoZ environment
set up using two laptops: learner desktop with the
map application and another with the wizard?s in-
terface. The learner desktop will also display a
simulated Nao robot. We will also exhibit the logs
that we collect from the pilot studies with a Geogr-
phy teacher acting as the wizard tutor and school
pupils as tutees.
Acknowledgements
This work was partially supported by the Euro-
pean Commission (EC) and was funded by the
EU FP7 ICT-317923 project EMOTE. The authors
are solely responsible for the content of this pub-
lication. It does not represent the opinion of the
EC, and the EC is not responsible for any use that
might be made of data appearing therein.
References
H. Cuaya?huitl and I Kruijff-Korbayova. 2012. An In-
teractive Humanoid Robot Exhibiting Flexible Sub-
Dialogues. In Proceedings of the NAACL-HTL,
Montreal, Canada.
A. Deshmukh, G. Castellano, A. Kappas, W. Baren-
dregt, F. Nabais, A. Paiva, T. Ribeiro, I. Leite, and
R. Aylett. 2013. Towards empathic artificial tutors.
In Proceedings of the 8th ACM/IEEE international
conference on Human-robot interaction.
N. Fraser and G. N. Gilbert. 1991. Simulating speech
systems. Computer Speech and Language, 5:81?99.
J. Pearson, J. Hu, H. P. Branigan, M. J. Pickering, and
C. Nass. 2006. Adaptive language behavior in HCI:
how expectations and beliefs about a system affect
users? word choice. In Proceedings of the SIGCHI
conference on Human Factors in computing systems,
Montral.
P. M. Strauss, H. Hoffmann, and S. Scherer. 2007.
Evaluation and user acceptance of a dialogue sys-
tem using Wizard-of-Oz recordings. In Proceedings
of 3rd IET International Conference on Intelligent
Environments.
365
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 19?27,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Click or Type: An Analysis of Wizard?s Interaction for Future Wizard
Interface Design
Srinivasan Janarthanam
1
, Robin Hill
2
, Anna Dickinson
2
, Morgan Fredriksson
3
1
School of Mathematical and Computer Sciences, Heriot-Watt University
2
School of Informatics, University of Edinburgh
3
Liquid Media AB, Stockholm
sc445@hw.ac.uk
Abstract
We present an analysis of a Pedestrian
Navigation and Information dialogue cor-
pus collected using a Wizard-of-Oz inter-
face. We analysed how wizards preferred
to communicate to users given three differ-
ent options: preset buttons that can gen-
erate an utterance, sequences of buttons
and dropdown lists to construct complex
utterances and free text utterances. We
present our findings and suggestions for
future WoZ design based on our findings.
1 Introduction
Wizard-of-Oz environments (WoZ) have been be-
come an essential tool for collecting and studying
dialogue between humans pretending to be ma-
chines and human users in various domains. It is
an effective way to collect dialogues between real
users and dialogue systems before actually imple-
menting the dialogue system. In this framework,
participants interact with an expert human oper-
ator (known as ?Wizard?) who is disguised as a
dialogue system. These Wizards replace one or
more parts of the dialogue system such as speech
recognition, natural language understanding, di-
alogue management, natural language generation
modules and so on. Real users interact differently
with humans and computers. While their expecta-
tions with human interlocutors are high and varied,
they are ready to adapt and ?go easy? on comput-
ers during interaction (Pearson et al., 2006). So, in
a WoZ framework, the conversation between real
users and the Wizards (pretending to be dialogue
systems) are of an appropriate type to be used for
dialogue system design and not as complex as in
human-human conversation.
In order to provide a speedy response, most
WoZ systems are designed in such a way that re-
sponses are hard wired to buttons so that they can
be sent to the synthesizer at the touch of a button.
However, in order to handle unexpected situations,
most WoZ interfaces also have a free text interface
that allows the Wizard to type any text to be syn-
thesised by the synthesizer. Are free text interfaces
used only under unexpected situations? In this pa-
per, we analyse how free text interfaces are used
by Wizards in a pedestrian tourist navigation and
information dialogue and discuss how the results
of our analysis be used to inform future WoZ de-
signs. These dialogues were collected as a part of
SpaceBook EU FP7 project.
In Section 2, we present previous work in WoZ
interfaces and the domain of pedestrian navigation
and information. We then present our WoZ setup
and data collection in Section 3 and 4. In Section
5, we present our analysis of the corpus, issues and
suggestions in Sections 6 and 7.
2 Related work
Wizard-of-Oz (WoZ) frameworks have been used
since early 90s in order to collect human-computer
dialogue data to help design dialogue systems
(Fraser and Gilbert, 1991). WoZ systems have
been used extensively to collect data to learn di-
alogue management policies (Rieser and Lemon,
2011) and information presentation strategies
(Demberg et al., 2011).
Pedestrian navigation and information systems
is a domain of interest to many mobile phone
applications. Applications such as Siri, Google
Maps Navigation, or Sygic deal with the task of
navigation while TripAdvisor, Triposo, etc . focus
on the tourist information problem. Additionally,
several research prototypes have been built to gen-
erate navigation instructions (Bartie and Mack-
aness, 2006; Shroder et al., 2011) and to have con-
versations with tourists (Janarthanam et al., 2013).
WoZ experiments enable the collection of realis-
tic data to assist in the development and testing of
these systems.
19
Figure 1: Wizard of Oz interface - Google Satellite Map and StreetView
3 WoZ setup
The wizard interface consisted a browser window
showing a Google Map and Street View with the
Tourists position. Google StreetView showed
the Tourist?s point of view (see Figure 1). The
Wizard was able to communicate information to
the Tourist in three different ways in the Wizard
Response Panel (see Figure 2):
Hot buttons: By clicking on one of several
buttons with commonly used phrases (e.g. ?OK.
I?ll suggest a route for you?, ?You want to cross
the road whenever you can?, ?Would you like
further information about that??). Buttons were
organised thematically in sections such as: con-
firmations, ways of asking the Tourist to repeat
what they had said, ways to indicate to the Tourist
that the Wizard was doing something and they
should wait (?Just a moment, please?, ?I?m just
finding that out for you now? and ?Apologies for
the delay?) and directions. The range of choices
available via the buttons (there were nine different
confirmations) was intended to allow the Wizard
to mimic the variability of human speech; they
were grouped to facilitate rapid identification and
selection.
Sequences: By generating text from a sequence
of drop-down menus, e.g. (where items in square
brackets are drop-down lists): ?You want to take
the [count] [pathway] on your [direction]).
Free text: By typing free text into a text editor.
Pre-entered phrases for Hot Buttons were se-
lected following two previous Wizard of Oz exper-
iments where the Tourist and the Wizard commu-
nicated by voice; common expressions used dur-
ing these sessions were summarised and presented
on an initial evaluation interface which was evalu-
ated with 15 dyads. Results from that experiment
fed into the WoZ interface above.
At the bottom right of the screen, there was a
scrollable record of the Wizard?s output in case
the participant needed to confirm what had been
sent to the Tourist. Finally, there was a selection
of system comments the Wizard could make, for
example to note system problems such as prob-
lems hearing the Tourist. This information was
recorded by the system but not sent to the Tourist.
Additionally, screen capture software was used to
record all the on-screen interaction. As a back-up,
the lab was videoed on DV cassette using a tripod-
mounted camcorder.
Instructions to participants were developed to
encourage participants (i.e. playing the role of
Tourists) to solve problems without directing them
too much. e.g. ?You?ve heard a story about a
statue of a dog that you think is nearby and would
like to take a photo of the dog and perhaps learn
a little more about the story.?, ?You have arranged
to have lunch with a friend in a nearby pub. You
20
Figure 2: Wizard of Oz interface - Wizard response panel
can?t remember the exact name but you are sure it
had the word ?Bell? in the title.?
The Tourist was equipped with an Android mo-
bile phone (Samsung Galaxy Note) and headset.
The phone ran a custom-built app that sent live
GPS, satellite and accelerometer data back to the
WoZ system while receiving the Wizards text mes-
sages and converting them to speech. As a back-
up, and to ensure the reliability of the position-
ing data, a GPS logging application (My Tracks)
also recorded the data every two seconds on the
phone. Time-stamping within the files permits off-
line synchronisation with the speech data.
4 Data collection
Participants were enrolled using an events organ-
ising website called EventBrite
1
. Two participants
attended each experimental session and were as-
signed to one of two roles: the Tourist or the Wiz-
ard. At the end of the experiment each received
?10. Ten dyads (twenty people) completed the
experiment. They were aged between 19 and 26
(mean 22), and had lived in Edinburgh between
0.7 and 10 years (mean 2.9). 8 were male, and 12
female.
After participants had arrived at the lab, they
signed a consent form and provided demographic
1
www.eventbrite.com
information (age, sex, and length of time in Ed-
inburgh). The task descriptions were handed out
and roles were assigned. The Wizard was given
supplementary information about some of the lo-
cations and Google Map print-outs, but was in-
structed to make up any answers to questions
asked by the Tourist if necessary.
After an initial equipment test and training,
the Tourist dialled a standard Edinburgh landline
number on the mobile phone which connected to a
Skype account and the experiment began. If the
call dropped, the Tourist would redial and con-
tinue. There was a basic set of tasks assigned to
the Tourist, but they were encouraged to expand
and adapt this and were free to ask any tourist or
navigation-based questions that they thought of on
the way.
The Tourist traversed Edinburgh on their own;
the Wizard and experimenter remained in a labo-
ratory. The Wizard used GPS information and dia-
logue with the Tourist to establish location. For the
Wizard, the Tourist?s view had to be reconstructed
using the display software available. These di-
alogue sessions ranged between 41:56 to 66:43
minutes. The average dialogue duration (accord-
ing to the transcriber) for the 10 dyads was 51min
46s.
Please note that for each run, a new pair of Wiz-
21
ard and Tourist were used. Wizards were not re-
tained to do more than one run because we wanted
to collect data from a variety of human wizards in
order to study variations in how each wizard dealt
with the navigation task.
5 Corpus analysis
We analysed the corpus collected based on the
three types of response generation mechanisms:
hot buttons, sequences and free text, to under-
stand their relative utility. We wanted to explore
whether pre-configured text was used when avail-
able, or whether the user?s interaction with the
pre-configured and free text sections of the inter-
face were influenced by other considerations than
availability.
Analysis showed that buttons corresponding
to preset utterances were used only 33% (+/- 14)
of the time. Although wizards had the option of
constructing complex utterances using a sequence
of drop down lists, they only used such sequences
9% (+/- 9) of the time. 58% (+/-19) of Wizard
utterances were generated using the free text inter-
face. This may imply that the buttons did not offer
what the Wizards wanted to say; in which case, we
would anticipate that their self-created utterances
would be very different from those pre-configured.
Individual differences: Use of the button inter-
face varied between Wizards, with some using it
very rarely and others depending on it when it pro-
vided a feature they required. The highest was
82.7% while the lowest use of free text was 31.7%.
Table 1 shows that 6 out of 10 Wizards used the
free text interface more than 60% of the time. It
is likely that these differences were due to individ-
ual variations such as speed of typing and comfort
with using an array of buttons.
Usage of free text interface Wizard count
Below 30% 0
30-40% 3
40-50% 1
50-60% 0
60-70% 3
70-80% 1
80-90% 2
Table 1: Usage of free text interface
As an example of these individual differences,
one Wizard used the button-press interface only
once during the first navigation task (to ask ?What
can you see at the moment??), choosing to direct
the Tourist almost exclusively through use of the
free text interface. By contrast, of the twelve Wiz-
ard utterances in another session?s initial naviga-
tion task, only two were free text. It is interesting
to note, however, that the Tourist commented ?I?ve
a feeling (the Wizard) is laughing at me right now.?
5.1 Hot button interface
We analysed how frequently each hot button in the
interface was used by Wizards. We also counted
how frequently the same text as the buttons was
generated using the free text interface. This will
show us if Wizards tend to type the same text that
can effectively be generated at the push of a hot
button. The following table shows the frequency
of each hot button used over the 10 dialogues that
we analysed.
There were forty buttons in total. Two initial
buttons intended to be used at the start of the ex-
periment or when the call was restarted after a
problem: ?Okay, we are ready to go. Please pre-
tend to have just dialed Space Book and say hello.?
and ?Hello, SpaceBook speaking.? (These were
used 29 times) and two intended for the end of
the call: ?Thank you for using SpaceBook? and
?Goodbye? (10 times). Table 2 shows the fre-
quency of usage for other hot buttons.
Utterance type Frequency
Confirmation (e.g. Yes, Okay, Certainly) 168
Navigation (e.g. ?Keep going straight ahead?) 114
Filler (e.g. ?Just a moment please?) 60
Repeat request (e.g. ?Sorry, could you repeat
that please??) 34
Visual checks (?Can you see it yet??/
?What can you see at the moment??) 32
Offer of further information/ help 30
References (e.g. ?According to Wikipedia?) 20
Negation (?No?, ?No, that?s wrong?) 18
Failure (?I?m afraid I don?t know the
answer to that?) 8
Table 2: Usage of Hot Buttons
The above table presents a Zipfian curve with
some utterances such as ?Okay?, ?Keep going
straight ahead? having high frequency and some
utterances such as ?I?m afraid I don?t know the an-
swer to that,? ?I couldn?t understand that, sorry?
with extremely low frequency. Even the highest
frequency utterance, ?Okay? was only used about
5 times per session on average. This does not
mean that the Wizard acknowledged the subject at
such low frequency but, as the analysis below in-
dicates, decided to acknowledge the user with free
22
text-generated utterances.
5.2 Free text utterances
We analysed the free text utterances generated by
the Wizards. This analysis, we believe, could
show us how to build better Wizard interfaces for
collecting dialogue data for pedestrian navigation.
First, we counted the number of free text utter-
ances that duplicated Hot Button text. Then, we
analysed the other utterances generated using the
free text interface.
Table 3 presents the frequency of utterances that
were generated using the free text interface but
were the same as hot button text. The table shows
that even though there are hot buttons for utter-
ances such as ?Yes?, ?Sorry?, Wizards tended to
type them into the free text interface. In some
cases these words were followed by a more com-
plex utterance which the Wizard had chosen to de-
liver as a single statement (e.g. ?Yes, that?s the
way to go.?, ?no, you should turn around?), and
second, these utterances are short and could easily
be typed rather than searching for the correspond-
ing hot button. Also, Wizards sometimes used
alternative spellings for words such as ?Okay?
which could be produced using a hot button. The
word ?Ok? was used 15 times in 10 sessions.
Text Frequency
Yes 45
Sorry 21
No 21
Take the next left 4
No problem 3
Certainly 2
Thank you 1
Table 3: Usage of Free Text for utterances same as
Hot Buttons
In addition, Wizards use free text to generate
utterances that are paraphrases of hot button utter-
ances, such as:
? ?Keep going?, ?Just keep walking?, etc
? ?Great?, ?Excellent?, etc
? ?One moment?, ?Wait a second please?, etc
? ?Of course?
? ?Okay cool?
These analyses imply that free text is not ac-
cessed only in the last resort because the user can-
not find the hot button that says what they?d like
to say. Clearly, the interaction is more complex
and concerns both speed (the contrast of typing a
short utterance such as ?Yes? compared with the
time needed to discover the correct button on a dis-
play and navigate to it with a mouse) and the user?s
imposition of their own identity on the conversa-
tion; where the hot button interface offered sev-
eral confirmatory utterances, users often used their
own (e.g. ?Great, ?Excellent?, ?Cool?), utterances
which were, presumably, part of the way these
Wizards more normally interacted with peers.
In this section, we present the other types of
utterances Wizards generated using the free text
interface.
1) Check user?s location:
Wizards asked several free text questions to check
where the user was, given that the positioning
system on smartphones was not entirely accurate.
They framed most questions as yes/no check
questions and enriched them with situational cues
(e.g.?Is the Pear Tree on your right??, ?Have
you reached the Royal Mile yet??, ?Can you see
Nicolson Square??, ?Have you passed the primary
school on your left??).
2) Informing user?s location:
Wizards sometimes informed users of their loca-
tion. e.g. ?This is West Nicolson Street?.
3) Complex navigation instructions:
Using the free text interface, Wizards generated
a variety of complex navigation instructions that
were not covered by the hot buttons. These include
instructions where the subject was asked to carry
out two instructions in sequence (e.g. ?Turn left,
and keep walking until you get to Chapel Street?),
orienting the user (e.g. ?You want the road on your
right?, ?Please go back in the direction you came
from?), signaling to the user that he/she was walk-
ing in the wrong direction (e.g. ?You?re going the
wrong way?), a priori instructions to destination
(e.g. ?To get there you will need to keep going
up the Royal Mile. Then turn left at the junction
between North and South Bridge. Walk up South
Bridge, and it will change to Nicolson Street. Sur-
geon?s Hall will be on the left hand side.?).
Some navigation instructions were complex be-
cause they were not general instructions but direct
responses to the Tourist?s question. One exam-
ple of this was by Dynamic Earth (dyad 07) when
23
the Wizard told the Tourist to follow a footpath.
Tourist: ?One of the footpaths banks to the right,
and the other goes straight forward. Which one??,
the Wizard answered: ?You want the one that is
straight forward.?
The navigation directions on hot buttons were
necessarily very general (e.g. Keep going straight
ahead/ Take the next left) and Wizards frequently
used the free text to enrich the directions and make
them more specific, e.g. (dyad 09) ?Walk down
Crichton Street towards the Mosque.? In the ini-
tial navigation task, this Wizard used the free text
interface 7 times, and the navigation hot buttons
only 4 times. Each segment of free text enriched
the interaction by providing specific navigational
information, so where the Wizard could have se-
lected the hot button for ?Keep going straight?,
instead she chose to add value to the interaction
through the use of place names and typed, ?Con-
tinue straight onto West Richmond Street?.
A similar pattern can be seen in the interaction
in dyad 10 where the Wizard used the free text op-
tion to navigate the Tourist according to objects
in his environment. e.g. ?Turn right at the traffic
lights? and ?Walk straight down past the Bingo on
your left.?. Of the 22 Wizard utterances in the first
navigation task in the dyad, only 5 were hot but-
tons. 14 were navigation instructions, of which 3
were button-presses and one (?Walk straight on?)
paraphrased an existing button. The Tourist got
lost in this task, so there was also some checking
on his location.
These are not isolated examples. In total, over
the ten dyads, 308 utterances from the total 927
free text utterances were Wizards ?enriching? their
navigation directions by adding contextual cues,
most commonly the name of the street or a land-
mark to help situate the Tourist. For example,
?You can reach it by turning right down Holyrood
Road at the junction.?, ?Please head towards the
Mosque?.
Although 33% of overall free text utterances
were enriched navigation instructions, this over-
all pattern varied depending on the dyad, ranging
from dyad 03 where 62.5% were enriched instruc-
tions, to dyad 08, where only 8% were enriched.
These value-added uses of the free text suggest
that the addition of contextual cues is regarded as
important by the individuals acting as Wizards.
An improved WoZ interface might seek to support
such utterances.
4) Reassuring user:
Wizards presented information such as landmarks
users can see as they walk along to reassure
them that they are on the right track (e.g. ?You
will pass Richmond Place on your left?, ?You
will walk past the Canongate Kirk on your right
beforehand?).
5) Informing time/distance to destination:
Wizards presented how long it will take to reach
the destination to set the right expectation in the
user?s mind (e.g. ?It will be about a two minute
walk?, ?the gym is 200 metres along this road on
your right?).
6) Providing destination information:
Wizards provided information about the location
of destination in reference to the user (e.g. ?And
Bonsai Bar Bistro will be on the left, just before
you reach The Pleasance?, ?The Museum of
Edinburgh will be on the left?) or other landmarks
(e.g. ?The Scottish Parliament is next to Our
Dynamic Earth?, ?The entrance is on the other
side?). Note that this interaction, too, is normally
enriched by situational cues.
7) Informing destinations that match search
criteria:
Some tasks presented to subjects did not specify
the actual name of the destination. Hence when
they asked the Wizard for a matching destination,
Wizards used free text to suggest destinations
that match the search criteria (e.g. ?There is
a restaurant called Bonsai Bistro?, ?There are
three museums to visit. They are Museum of
Edinburgh, People?s Story Museum, and Museum
of Childhood?).
8) Check if destination reached and identified:
Wizards checked whether users had reached their
destination by asking them to confirm if they had
(e.g. ?Have you reached it??, ?Have you found
the sports centre??). The hot button ?Can you see
it yet?? covered this functionality, but once more,
free text allowed the user to increase situational
specificity by identifying the target.
9) Additional information about landmarks:
Wizards presented additional information about
landmarks such as its name (?the hill besides par-
24
liament is in fact 2 hills, the rocky cliffs you can
see are called crags?, ?behind that is arthurs seat?),
the year it was built/opened (e.g. ?it was opened
in 1999?), what it functions as (e.g. ?offices for a
newspaper publisher?).
In some cases such free text utterances were
produced in response to questions asked by
Tourists. For example, when the Tourist of dyad
05 passed the Fringe office, they asked, ?Do you
know what dates the Fringe is on this year??.
The Wizard used free text to answer the question.
Later in the same experiment, the Tourist identi-
fied Vodka Rev as a landmark (?Down past Vodka
Rev??) and the Wizard responded with free text
about the landmark: ?Vodka Rev does half price
food on Mondays.?.
10) Signalling connection problems:
Wizards informed users when they lost the user?s
GPS signal (e.g. ?hold on 1 second, gps connec-
tion gone?) and to establish contact and check
user?s attention (e.g. ?hello??, ?I can?t hear you at
the moment?).
Further, some Wizards used the free text to hu-
manise the person-to-person element of the inter-
action. They would chat to Tourists, make jokes
(?I cannot answer rhetorical questions, as I am
both a computer and aware they are not meant to
be answered.?) and in one case, invite the Tourist
out for a drink.
6 Issues with free text
As one can imagine, there are issues with free text
utterances generated by Wizards.
Spelling:
Several words used in free text utterances were
misspelled. e.g. ?roundabaout?, ?entrace?, ?ple-
sae?, ?toewards?, ?You want ot cross the roD?)
etc. These ranged from 0 to 13 errors per session
with a mean of 3.6 (+/- 3.9) errors per session.
Adjacent words were sometimes joined together
(e.g. ?atyour?, ?upahead?, etc) and sometimes
incorrectly segmented with space (e.g. ?collection
sof?, ?hea ryou?, etc). Some entity names were
misspelled as well (e.g. ?Critchon?, ?Dyanmic
Earth?, ?artthurs seat?, etc). Spelling errors can
reflect poorly when the utterances are synthesized
and the misspelled words mispronounced.
Syntax:
We also found a few syntactic errors in utterance
construction (e.g. ?Continue going Chambers
street?). Similar to spelling errors, utterances with
improper syntax can sound weird to the Tourist
and could lead to confusion and misunderstanding
instructions.
Incorrect entity names:
Wizards did not always get street names correct,
e.g. in dyad 02, the Wizard directed the Tourist to
?Nicholas Square? and the Tourist needed to seek
clarification that he meant ?Nicolson Square?.
Time and effort:
It takes time and can slow the interaction with the
user, leading to issues like interruptions and the
flow of the conversation being upset.
7 Suggestions
Based on the above analysis, we propose a list
of suggestions to build a better Wizard of Oz in-
terface for collecting dialogues concerning pedes-
trian navigation and exploration. The objective of
the WoZ system is to provide an effective inter-
face to Wizards to interact with Tourists while pre-
tending to be dialogue systems. One of the impor-
tant requirements is that Wizards should be able
to generate context appropriate utterances quickly
to make the dialogue appear more natural with-
out unnecessary lag between a user?s requests and
the system?s responses. Hot buttons are designed
so that the utterance can generated at the push of
a button. However as our data shows, Wizards
tended to use the free text interface about 60% of
the time.
While there are situations in which free text is
necessary, in general it risks slowing the interac-
tion and potentially confusing the Tourist when
words are mis-spelled or omitted. In addition,
supporting the Wizard more effectively with an
improved WoZ interface is likely to permit them
to spend more time supporting and informing
the Tourist. Free text utterances can lead to slow
system response and there is therefore a need to
find a compromise between the two. We have the
following suggestions:
1. More hot buttons:
Some utterances generated using the free text in-
terface could not be generated using the hot but-
25
tons or the sequences. These include reassuring
users, informing them of the time/distance to des-
tination, informing them of search results, etc.
While free text is a useful interface to Wizards to
generate unforeseen utterances, more hot buttons
covering new functionality can be faster to use.
However, introducing additional hot buttons
would add complexity to the interface, which is
likely to have the undesireable effect of encourag-
ing users to avoid the cluttered display in favour if
the straightforward free text interface. One partial
solution is to ensure that buttons are organised and
grouped in ways that are intuitive for the Wizard.
This, and the optimum number of buttons for the
display, should be investigated experimentally.
2. Multi functional hot buttons:
Some free text utterances were complex versions
of simple utterances that were already covered by
hot buttons. For instance, utterances like ?Keep
going up Nicolson Street? or ?Keep walking until
you get to Chapel Street? can be seen as a version
of ?Keep going straight ahead? but with some ap-
pended information (i.e. street name, landmark).
The interface could be designed so that hot
button utterances could be modified or appended
with more information. For example, a single
click the hot button might send the utterance to
the free text editor, allowing the Wizard to add
more information, whereas a double click would
send the utterance directly to the TTS.
3. Spell check, grammar check and auto cor-
rection:
To ensure that the speech synthesizer works as ef-
fectively as possible, the utterances typed in the
free text editor must be correctly spelled. One so-
lution to the frequent mis-spelling made by Wiz-
ards typing at speed is to automatically spell check
and correct text typed in the free text interface.
Ensuring that text is correct would reduce the
risk of the speech synthesizer mispronouncing
misspelt names and words. Similarly, a grammar
check would mean that the synthesised utterances
felt more natural.
Since there is the danger of an automatic spell
checker making mistakes, the spell check and cor-
rection should happen when the Wizard finishes
typing a word or utterance and the auto corrected
word or utterance be shown to the Wizard before
it is sent to the TTS.
4. Autocomplete:
Autocomplete is a feature that predicts the next
words the user intends to type based on those
already typed. It is currently used by search
engines such as Google to complete users? search
queries based on their search history and profile.
A similar feature that can complete utterances
taking into account the user?s request, dialogue
history, and the spatial context could speed up the
response time of the Wizard.
5. Location aware WoZ interface:
The WoZ system could be ?aware? of the user?s
surroundings. Such a solution might enable the
interface to have dynamically changing buttons,
so when the user is headed up Nicolson Street,
the ?Keeping going? button could have Nicolson
Street on it. Information about entities around
the user can also be assigned to hot buttons dy-
namically. However, hot buttons with dynamically
changing labels and functionality could be cogni-
tively overloading to Wizards.
Of course, the addition of such functionality
to the WoZ interface must be carefully evaluated.
A dynamic interface may be harder to learn, and
increasing the number of buttons may, counter-
intuitively, mean that users are less likely to select
hot buttons because the effort to scan the array of
buttons is greater than the effort needed to type ut-
terances, particularly short ones, into a free text
box.
8 Conclusion
In this paper, we presented a Wizard of Oz system
that was used to collect dialogues in the domain of
pedestrian navigation and information. We anal-
ysed the corpus collected to identify how Wizards
preferred to interact with the pedestrian users and
why. We identified issues with free text interfaces
that was used by majority of Wizards and sug-
gested improvements towards future Wizard inter-
face design.
Acknowledgments
The research leading to these results was funded by the Eu-
ropean Commission?s Framework 7 programme under grant
agreement no. 270019 (SPACEBOOK project).
26
References
P. Bartie and W. Mackaness. 2006. Development of a
speech-based augmented reality system to support explo-
ration of cityscape. Transactions in GIS, 10:63?86.
Vera Demberg, Andi Winterboer, and Johanna D. Moore.
2011. A strategy for information presentation in spo-
ken dialog systems. Comput. Linguist., 37(3):489?539,
September.
N. Fraser and G. N. Gilbert. 1991. Simulating speech sys-
tems. Computer Speech and Language, 5:81?99.
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dick-
inson, X. Liu, W. Mackaness, and B. Webber. 2013.
Evaluating a city exploration dialogue system combining
question-answering and pedestrian navigation. In Proc.
ACL 2013.
J. Pearson, J. Hu, H. P. Branigan, M. J. Pickering, and
C. Nass. 2006. Adaptive language behavior in HCI: how
expectations and beliefs about a system affect users? word
choice. In Proceedings of the SIGCHI conference on Hu-
man Factors in computing systems, Montral.
V. Rieser and O. Lemon. 2011. Learning and Evaluation
of Dialogue Strategies for new Applications: Empirical
Methods for Optimization from Small Data Sets. Compu-
tational Linguistics, 37:1.
C.J. Shroder, W. Mackaness, and B. Gittings. 2011. Giving
the Right Route Directions: The Requirements for Pedes-
trian Navigation Systems. Transactions in GIS, pages
419?438.
27
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 48?52,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Multi-threaded Interaction Management for
Dynamic Spatial Applications
Srinivasan Janarthanam
Interaction Lab
Heriot-Watt University
Edinburgh
sc445@hw.ac.uk
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh
o.lemon@hw.ac.uk
Abstract
We present a multi-threaded Interaction
Manager (IM) that is used to track differ-
ent dimensions of user-system conversa-
tions that are required to interleave with
each other in a coherent and timely man-
ner. This is explained in the context of
a spoken dialogue system for pedestrian
navigation and city question-answering,
with information push about nearby or vis-
ible points-of-interest (PoI).
1 Introduction
We present a multi-threaded Interaction Manager
(IM) that is used to track different dimensions of
user-system conversations and interleave the dif-
ferent converational threads coherently. The IM
that we present interacts with the user in a spa-
tial domain and interleaves navigation informa-
tion along with historical and cultural information
about the entities that users can see around them.
In addition, it aims to answer questions that users
might have about those entities. This presents
a complex conversational situation where several
conversational threads have to be interleaved in
such a way that the system utterances are pre-
sented to the user at the right time but in a pri-
oritised order, and with bridging utterances when
threads are interrupted and resumed. For instance,
a navigation instruction may be important (since
the user is walking up to a junction at which they
need to turn) and therefore it needs to be spoken
before continuing information presentation about
an entity or answering other ongoing questions.
2 Related work
Previously, multi-threaded interaction was used
to handle multiple simultaneous tasks in human-
robot interaction (HRI) scenarios (Lemon and
Gruenstein, 2004). This idea also turns out to be
important for cases where humans are interacting
with a variety of different web-services in paral-
lel. Human multitasking in dialogue is discussed
in (Yang et al., 2008).
(Lemon and Gruenstein, 2004) presented a
multi-threaded dialogue management approach
for managing several concurrent tasks in an HRI
scenario. The robot could, for example be flying
to a location while simultaneously searching for
a vehicle, and utterances about both tasks could
be interleaved. Here, conversational threads were
managed using a representation called the ?Dia-
logue Move Tree?, which represented conversa-
tional threads as branches of the tree, linked to an
?Activity Tree? which represented the states of on-
going robot tasks (deliver medical supplies, fly to a
waypoint, search for a truck), which could be ac-
tive simultaneously. The situation for our pedes-
trian navigation and information system is simi-
lar - concurrent tasks need to be managed coher-
ently via conversation. The approach adopted in
this paper is similar to (Lemon and Gruenstein,
2004). However, in this work we separate out
a domain-general thread called ?dialogue control?
which handles generic issues like clarification of
reference across all tasks. This increasing modu-
larisation of the dialogue threads makes it possible
to learn individual dialogue policies for each one,
in future work.
(Nakano et al., 2008) presented an approach
where one of the several expert modules handling
different tasks is activated based on the user input,
but only one verbal expert is active at any one time.
In contrast to this, we present an approach where
several thread managers each handling a different
task can be activated in parallel and their outputs
stored and retrieved based on priority.
3 Multi-threaded IM
The Interaction Manager (IM) is the central com-
ponent of any spoken dialogue system architec-
48
Figure 1: Interaction Manager Architecture
ture. Generally, it takes as input the user?s utter-
ances in the form of dialogue acts from the parser
and identifies the next dialogue action to present to
the user. Dialogue about a domain task is managed
using a dialogue strategy or policy (e.g. (Young,
2000; Lemon and Pietquin, 2007)). A dialogue
policy is a mapping between dialogue states and
dialogue actions, which are semantic representa-
tions of what the system should say next.
In order to handle multiple tasks simul-
taneously, we present an architecture for a
multi-threaded interaction manager that treats
conversation about each domain task as a thread.
These conversational threads are interleaved and
managed using techniques such as multi-queuing,
priority based pushing, and queue revision. We
describe these techniques below. The architecture
of the Interaction Manager is shown in figure 1.
Multi-threading and queuing
In order to manage complex interactions involving
several conversational tasks/topics, we propose
that the each task be handled by a thread manager
within the interaction management framework.
Each such manager will handle a conversational
thread using a dialogue policy. Each thread
manager will be fed with the input from the user
and the dialogue actions generated will be stored
in separate queues. This approach allows the
interaction manager to produce several dialogue
actions at the same time although for different
conversational tasks.
Prioritised Queue Management
Dialogue actions from the several threads are
stored in separate queues. The queues can be
assigned priorities that decide the order in which
items from the queues will be popped. The
dialogue actions in the queues are pushed to the
user based on an order of priority (see below).
This priority can either be fixed or dynamic based
on context. The system and user engagement
should also be checked so that system utterances
are pushed only when the system and user are not
speaking already.
Queue Revision: resuming and bridging
The dialogue actions are generated and stored in
queues. Therefore, there is a difference between
the time they are generated and time that they are
pushed. Therefore dialogue actions in the queues
are revised periodically to reflect changes in con-
text. Obsolete dialogue actions will have to re-
moved for two reasons. Firstly, pushing them to
the user may make the conversation incoherent be-
cause the system may be speaking about an entity
that is no longer relevant and secondly, these obso-
lete dialogue actions may delay other other impor-
tant dialogue actions from being pushed on time.
In addition, it may also be useful to edit the dia-
logue actions to include discourse markers to sig-
nify topic change (Yang et al., 2008) and bridge
49
phrases to reintroduce a previous topic. We dis-
cuss some examples later in section 4.3.
4 SPACEBOOK Interaction Manager
As a part of the SpaceBook EU FP7 project,
we implemented the above design for a multi-
threaded interaction manager that presents the user
with navigational instructions, pushes PoI infor-
mation, and manages QA questions (Janarthanam
et al., 2013). It receives the user?s input in the
form of a dialogue act (DA) from the ASR mod-
ule and the user?s location (latitude and longitude),
orientation, and speed from the Pedestrian Tracker
module. Based on these inputs and the dialogue
context, the IM responds with a system output di-
alogue act. It should be noted that the location
coordinates of the user are sent to the IM every 2
seconds. This allows the IM to generate location
aware information at a high frequency. In addition,
the IM has to deal with incoming requests and re-
sponses from the user?s spoken inputs. With the
possibility of system utterances being generated
at a frequency of one every two seconds, there is
a need for an efficient mechanism to manage the
conversation and reduce the risk of overloading
the user with information. These tasks are treated
as separate conversational threads.
4.1 Conversational Threads
The SpaceBook IM manages the conversation
using five conversational threads using dedicated
task managers. Three threads: ?navigation?,
?question answering? and ?PoI pushing?, represent
the core tasks of our system. In addition, for
handling the issues in dialogue management,
we introduce two threads: ?dialogue control?
and ?request response?. These different threads
represent the state of different dimensions of the
user-system conversation that need to interleave
with each other coherently. Each of the threads
is managed by a thread manager using a dialogue
policy. Each thread can generate a dialogue ac-
tion depending on the context, as described below:
Dialogue Control
During the course of the conversation, the IM uses
this thread to manage user requests for repetition,
issues with unparsed (i.e. not understood) user
utterances, utterances that have low ASR confi-
dence, and so on. The dialogue control thread is
also used to manage reference resolution in cases
where referring expressions are underspecified.
The IM resolves anaphoric references by keeping
a record of entities mentioned in the dialogue
context. It stores the name and type information
for each entity (such as landmark, building, etc)
mentioned in previous utterances by either user
or system. Subsequent user references to these
entities using expressions such as ?the museum?,
?the cafe?, and so on, are resolved by searching
for the latest entity of the given type. In cases
where the IM cannot resolve the referent, it asks
the user to clarify.
Request Response
The user can also initiate tasks that interest
him/her at anytime during the conversation.
These tasks include searching for an entity (e.g.
a museum or a restaurant), requesting navigation
instructions to a destination, and asking questions
about the entities in the city database such as their
location (?Where is X??, ?How far is X??). Dur-
ing navigation, users might want to ask questions
about the destination, ask for next instructions,
etc. All these user requests are handled using
the request response thread. For instance, when
the user asks for directions, the IM resolves the
destination entity (perhaps using clarification)
in the city model and acknowledges the user
request. The task is then further handled using the
Navigation thread.
Navigation
The IM identifies the location of the destination
entity and queries a city database (Bartie and
Mackaness, 2013) for a route plan. Using the route
plan, the navigation thread presents step-by-step
instructions to the user based on the current loca-
tion and orientation of the user. The IM contin-
uously monitors users to determine if at any time
they are deviating from the planned route and pro-
vides corrective instructions. As users get near
to the next node on the route plan, the next in-
struction is given. The IM uses highly salient vis-
ible landmarks (Bartie et al., 2013) and popular
landmarks near the nodes to instruct the user (e.g.
?When you reach Clydesdale Bank, turn left on
to Nicolson Square?). The IM also informs users
when they pass by recognisable landmarks, just to
reassure them that they are on the right track (e.g.
?You will pass by Tesco on the right?). When the
user is close to his/her destination, the IM deter-
mines whether the destination is visible to the user,
informs the user, and closes the task.
50
Usr: I want to go to the National Gallery.
Sys: Ok. I am now looking for directions to
the National Gallery.
Sys: The National Gallery is on Queens Street
at about 300 meters away.
....
Sys: Can you see the Bank of Scotland about
300 meters away on your left? It has a big
green dome.
Usr: Yes. I can see it.
Sys: Walk towards it.
Sys: Can you see the National Gallery in front
of you at about 100 meters? It is a building
with Roman architecture.
Usr: Yes.
Sys: You have now reached your destination.
Question Answering
The system also answers ad hoc questions from
the user (e.g. ?Who is David Hume??, ?What is
the Old College??, ?Who was William Wallace?,
etc). These are sent to the QA server and answered
based on responses from the Question-Answering
(QA) server (Janarthanam et al., 2013). The
dialogue policy here is to answer the user?s
question with the first snippet available and ask
the user to request for more if more snippets are
available and he or she is interested.
Pushing PoI Information
When the user is mobile, the IM identifies pop-
ular points of interest (PoI) on the route based on
two factors: proximity and visibility. The dialogue
policy is to introduce the PoI, query the QA server
for snippets and push the first snippet to the user.
The user is encouraged to ask for more informa-
tion if he/she is interested.
Sys: In front of you, about 200 meters away is
Old College. It has a grey dome on top.
Sys: Situated on South Bridge, Old College is
. . .
Sys: Ask for more information if interested.
4.2 Priority assignment in SpaceBook
Priority is assigned to the above dialogue threads
as follows:
Priority 1. Dialogue control (repeat request, clari-
fications etc)
Priority 2. Responding to user requests
Priority 3. System initiated navigation task actions
Priority 4. Responses to User-initiated QA actions
Priority 5. PoI Push actions
For instance, informing the user of a PoI could be
delayed if the user needs to be given an instruction
to turn at the junction he is approaching.
4.3 Queue revision and bridging utterances
The queues need to be revised at regular intervals
in order to keep the information in them relevant
to context. For instance, the dialogue action of in-
forming the user of his/her location is deleted after
5 seconds, as this tends to become obsolete. Sim-
ilarly, dialogue actions corresponding to informa-
tion segments in PoI and QA queues are edited to
inform the utterance generator of other interven-
ing dialogue actions so that it can use appropri-
ate bridge phrases to reintroduce the focus of the
conversational thread. For instance, as shown in
the example below, the utterance generator inserts
a bridge phrase (i.e. ?More on Old College?) to
reintroduce the focus of the PoI push task because
of the intervening user request and the subsequent
system response.
Sys: In front of you, about 200 meters away is
the Old College. It has a grey dome on top.
User: Where am I?
Sys: You are on Chambers street.
Sys: More onOld College. Situated on South
Bridge, the Old College is......
5 Conclusion
We presented an architecture for a multi-threaded
Interaction Manager that can handle multiple con-
versational tasks. We also described an implemen-
tation of the architecture in a dynamic spatial en-
vironment. The SpaceBook IM is a multi-tasking
IM that aims to interleave navigation information
along with historical information about the enti-
ties users can see around them. In addition, it aims
to answer questions users might have about those
entities.
Acknowledgements
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 270019 (SPACEBOOK project
www.spacebook-project.org).
51
References
P. Bartie and W. Mackaness. 2013. D3.1.2: The SpaceBook
City Model. Technical report, The SPACEBOOK Project
(FP7/2011-2014 grant agreement no. 270019).
P. Bartie, W. Mackaness, M. Fredriksson, and J. Konigsmann.
2013. D2.1.2 Final Viewshed Component. Technical
report, The SPACEBOOK Project (FP7/2011-2014 grant
agreement no. 270019).
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dick-
inson, X. Liu, W. Mackaness, and B. Webber. 2013.
Evaluating a city exploration dialogue system combining
question-answering and pedestrian navigation. In Proc.
ACL 2013.
Oliver Lemon and Alexander Gruenstein. 2004. Mul-
tithreaded context for robust conversational interfaces:
context-sensitive speech recognition and interpretation of
corrective fragments. ACM Transactions on Computer-
Human Interaction (ACM TOCHI), 11(3):241? 267.
Oliver Lemon and Olivier Pietquin. 2007. Machine learning
for spoken dialogue systems. In Interspeech.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and Hi-
roshi Tsujino. 2008. A framework for building conversa-
tional agents based on a multi-expert model. In Proceed-
ings of the 9th SIGdial Workshop on Discourse and Dia-
logue, SIGdial ?08, pages 88?91, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fan Yang, Peter A. Heeman, and Andrew Kun. 2008.
Switching to real-time tasks in multi-tasking dialogue.
In Proceedings of the 22Nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 1025?1032, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Steve Young. 2000. Probabilistic methods in spoken dia-
logue systems. Philosophical Transactions of the Royal
Society (Series A), 358(1769):1389?1402.
52
