Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 478?488, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Dynamic Programming for Higher Order Parsing of Gap-Minding Trees
Emily Pitler, Sampath Kannan, Mitchell Marcus
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
epitler,kannan,mitch@seas.upenn.edu
Abstract
We introduce gap inheritance, a new struc-
tural property on trees, which provides a way
to quantify the degree to which intervals of de-
scendants can be nested. Based on this prop-
erty, two new classes of trees are derived that
provide a closer approximation to the set of
plausible natural language dependency trees
than some alternative classes of trees: unlike
projective trees, a word can have descendants
in more than one interval; unlike spanning
trees, these intervals cannot be nested in ar-
bitrary ways. The 1-Inherit class of trees has
exactly the same empirical coverage of natural
language sentences as the class of mildly non-
projective trees, yet the optimal scoring tree
can be found in an order of magnitude less
time. Gap-minding trees (the second class)
have the property that all edges into an interval
of descendants come from the same node, and
thus an algorithm which uses only single in-
tervals can produce trees in which a node has
descendants in multiple intervals.
1 Introduction
Dependency parsers vary in what space of possi-
ble tree structures they search over when parsing
a sentence. One commonly used space is the set
of projective trees, in which every node?s descen-
dants form a contiguous interval in the input sen-
tence. Finding the optimal tree in the set of projec-
tive trees can be done efficiently (Eisner, 2000), even
when the score of a tree depends on higher order fac-
tors (McDonald and Pereira, 2006; Carreras, 2007;
Koo and Collins, 2010). However, the projectivity
assumption is too strict for all natural language de-
pendency trees; for example, only 63.6% of Dutch
sentences from the CoNLL-X training set are pro-
jective (Table 1).
At the other end of the spectrum, some parsers
search over all spanning trees, a class of structures
much larger than the set of plausible linguistic struc-
tures. The maximum scoring directed spanning tree
can be found efficiently when the score of a tree de-
pends only on edge-based factors (McDonald et al
2005b). However, it is NP-hard to extend MST to in-
clude sibling or grandparent factors (McDonald and
Pereira, 2006; McDonald and Satta, 2007). MST-
based non-projective parsers that use higher order
factors (Martins et al2009; Koo et al2010), uti-
lize different techniques than the basic MST algo-
rithm. In addition, learning is done over a relaxation
of the problem, so the inference procedures at train-
ing and at test time are not identical.
We propose two new classes of trees between pro-
jective trees and the set of all spanning trees. These
two classes provide a closer approximation to the set
of plausible natural language dependency trees: un-
like projective trees, a word can have descendants in
more than one interval; unlike spanning trees, these
intervals cannot be nested in arbitrary ways. We in-
troduce gap inheritance, a new structural property
on trees, which provides a way to quantify the de-
gree to which these intervals can be nested. Differ-
ent levels of gap inheritance define each of these two
classes (Section 3).
The 1-Inherit class of trees (Section 4) has exactly
the same empirical coverage (Table 1) of natural lan-
guage sentences as the class of mildly non-projective
trees (Bodirsky et al2005), yet the optimal scoring
tree can be found in an order of magnitude less time
(Section 4.1).
Gap-minding trees (the second class) have the
478
property that all edges into an interval of descen-
dants come from the same node. Non-contiguous
intervals are therefore decoupled given this single
node, and thus an algorithm which uses only single
intervals (as in projective parsing) can produce trees
in which a node has descendants in multiple inter-
vals (as in mildly non-projective parsing (Go?mez-
Rodr??guez et al2011)). A procedure for finding
the optimal scoring tree in this space is given in Sec-
tion 5, which can be searched in yet another order of
magnitude faster than the 1-Inherit class.
Unlike the class of spanning trees, it is still
tractable to find the optimal tree in these new spaces
when higher order factors are included. An exten-
sion which finds the optimal scoring gap-minding
tree with scores over pairs of adjacent edges (grand-
parent scoring) is given in Section 6. These gap-
minding algorithms have been implemented in prac-
tice and empirical results are presented in Section 7.
2 Preliminaries
In this section, we review some relevant defini-
tions from previous work that characterize degrees
of non-projectivity. We also review how well
these definitions cover empirical data from six lan-
guages: Arabic, Czech, Danish, Dutch, Portuguese,
and Swedish. These are the six languages whose
CoNLL-X shared task data are either available open
source1 or from the LDC2.
A dependency tree is a rooted, directed spanning
tree that represents a set of dependencies between
words in a sentence.3 The tree has one artificial root
node and vertices that correspond to the words in an
input sentence w1, w2,...,wn. There is an edge from
h to m if m depends on (or modifies) h.
Definition 1. The projection of a node is the set of
words in the subtree rooted at it (including itself).
A tree is projective if, for every node in the tree,
that node?s projection forms a contiguous interval in
the input sentence order.
A tree is non-projective if the above does not hold,
i.e., there exists at least one word whose descendants
1http://ilk.uvt.nl/conll/free_data.html
2LDC catalogue numbers LDC2006E01 and LDC2006E02
3Trees are a reasonable assumption for most, but not all,
linguistic structures. Parasitic gaps are an example in which
a word perhaps should have multiple parents.
do not form a contiguous interval.
Definition 2. A gap of a node v is a non-empty, max-
imal interval that does not contain any words in the
projection of v but lies between words that are in
the projection of v. The gap degree of a node is
the number of gaps it has. The gap degree of a tree
is the maximum of the gap degrees of its vertices.
(Bodirsky et al2005)
Note that a projective tree will have gap degree 0.
Two subtrees interleave if there are vertices l1, r1
from one subtree and l2, r2 from the other such that
l1 < l2 < r1 < r2.
Definition 3. A tree is well-nested if no two disjoint
subtrees interleave (Bodirsky et al2005).
Definition 4. A mildly non-projective tree has gap
degree at most one and is well-nested.
Mildly non-projective trees are of both theoret-
ical and practical interest, as they correspond to
derivations in Lexicalized Tree Adjoining Grammar
(Bodirsky et al2005) and cover the overwhelming
majority of sentences found in treebanks for Czech
and Danish (Kuhlmann and Nivre, 2006).
Table 1 shows the proportion of mildly non-
projective sentences for Arabic, Czech, Danish,
Dutch, Portuguese, and Swedish, ranging from
95.4% of Portuguese sentences to 99.9% of Ara-
bic sentences.4 This definition covers a substan-
tially larger set of sentences than projectivity does
? an assumption of projectivity covers only 63.6%
(Dutch) to 90.2% (Swedish) of examples (Table 1).
3 Gap Inheritance
Empirically, natural language sentences seem to be
mostly mildly non-projective trees, but mildly non-
projective trees are quite expensive to parse (O(n7)
(Go?mez-Rodr??guez et al2011)). The parsing com-
plexity comes from the fact that the definition al-
lows two non-contiguous intervals of a projection to
be tightly coupled, with an unbounded number of
edges passing back and forth between the two inter-
vals; however, this type of structure seems unusual
4While some of the treebank structures are ill-nested or have
a larger gap degree because of annotation decisions, some lin-
guistic constructions in German and Czech are ill-nested or
require at least two gaps under any reasonable representation
(Chen-Main and Joshi, 2010; Chen-Main and Joshi, 2012).
479
Arabic Czech Danish Dutch Portuguese Swedish Parsing
Mildly non-proj 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n7)
Mild+1-Inherit 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n6)
Mild+0-Inherit 1394 (95.5) 70695 (97.2) 4985 (96.1) 12068 (90.4) 8481 (93.5) 10787 (97.7) O(n5)
Projective 1297 (88.8) 55872 (76.8) 4379 (84.4) 8484 (63.6) 7353 (81.1) 9963 (90.2) O(n3)
# Sentences 1460 72703 5190 13349 9071 11042
Table 1: The number of sentences from the CoNLL-X training sets whose parse trees fall into each of the above
classes. The two new classes of structures, Mild+0-Inherit and Mild+1-Inherit, have more coverage of empirical data
than projective structures, yet can be parsed faster than mildly non-projective structures. Parsing times assume an edge-
based factorization with no pruning of edges. The corresponding algorithms for Mild+1-Inherit and Mild+0-Inherit
are in Sections 4 and 5.
for natural language. We therefore investigate if we
can define further structural properties that are both
appropriate for describing natural language trees and
which admit more efficient parsing algorithms.
Let us first consider an example of a tree which
both has gap degree at most one and satisfies well-
nestedness, yet appears to be an unrealistic struc-
ture for a natural language syntactic tree. Consider
a tree which is rooted at node xn+2, which has one
child, node xn+1, whose projection is [x1, xn+1] ?
[xn+3, x2n+2], with n children (x1, ..., xn), and each
child xi has a child at x2n?i+3. This tree is well-
nested, has gap degree 1, but all n of xn+1?s children
have edges into the other projection interval.
We introduce a further structural restriction in this
section, and show that trees satisfying our new prop-
erty can be parsed more efficiently with no drop in
empirical coverage.
Definition 5. A child is gap inheriting if its parent
has gap degree 1 and it has descendants on both
sides of its parent?s gap. The inheritance degree of
a node is the number of its children which inherit its
gap. The inheritance degree of a tree is the maximum
inheritance degree over all its nodes.
Figure 1 gives examples of trees with varying de-
grees of gap inheritance. Each projection of a node
with a gap is shown with two matching rectangles. If
a child has a projection rectangle nested inside each
of the parent?s projection rectangles, then that child
inherits the parent?s gap. Figure 1(a) shows a mildly
projective tree (with inheritance degree 2), with both
node 2 and node 11 inheriting their parent (node 3)?s
gap (note that both the dashed and dotted rectangles
each show up inside both of the solid rectangles).
Figure 1(b) shows a tree with inheritance degree 1:
there is now only one pair of rectangles (the dot-
ted ones) which show up in both of the solid ones.
Figure 1(c) shows a tree with inheritance degree 0:
while there are gaps, each set of matching rectangles
is contained within a single rectangle (projection in-
terval) of its parent, i.e., the two dashed rectangles
of node 2?s projection are contained within the left
interval of node 3; the two dotted rectangles of node
12?s projection are contained within the right inter-
val of node 3, etc.
We now ask:
1. How often does gap inheritance occur in the
parses of natural language sentences found in
treebanks?
2. Furthermore, how often are there multiple gap
inheriting children of the same node (inheri-
tance degree at least two)?
Table 1 shows what proportion of mildly non-
projective trees have the added property of gap in-
heritance degree 0 (Mild+0-Inherit) or have gap in-
heritance degree 1 (Mild+1-Inherit). Over all six
languages, there are no examples of multiple gap
inheritance ? Mild+1-Inherit has exactly the same
empirical coverage as the unrestricted set of mildly
non-projective trees.
4 Mild+1-Inherit Trees
There are some reasons from syntactic theory why
we might expect at most one child to inherit its par-
ent?s gap. Traditional Government and Binding the-
ories of syntax (Chomsky, 1981) assume that there
is an underlying projective (phrase structure) tree,
and that gaps primarily arise through movement of
480
6
2 3 41 5 7 11 12 1398 10
(a) Mildly Non-Projective: The projec-
tions (set of descendants) of both node 2
(the dashed red rectangles) and node 11
(dotted magenta) appear in both of node
3?s intervals (the solid blue rectangles).
6
2 3 41 5 7 11 12 1398 10
(b) Mild+1-Inherit: Only node 2 inherits
node 3?s gap: the dashed red rectangles
appear in each of the two solid blue rect-
angles.
6
2 3 41 5 7 11 12 1398 10
(c) Mild+0-Inherit: Even though node 3
has children with gaps (node 2 and node
12), neither of them inherit node 3?s gap.
There are several nodes with gaps, but
every node with a gap is properly con-
tained within just one of its parent?s in-
tervals.
Figure 1: Rectangles that match in color and style indicate the two projection intervals of a node, separated by a gap.
In all three trees, node 3?s two projection intervals are shown in the two solid blue rectangles. The number of children
which inherit its gap vary, however; in 1(a), two children have descendants within both sides; in 1(b) only one child
has descendants on both sides; in 1(c), none of its children do.
subtrees (constituents). One of the fundamental as-
sumptions of syntactic theory is that movement is
upward in the phrase structure tree.5
Consider one movement operation and its effect
on the gap degree of all other nodes in the tree: (a) it
should have no effect on the gap degree of the nodes
in the subtree itself, (b) it can create a gap for an an-
cestor node if it moves out of its projection interval,
and (c) it can create a gap for a non-ancestor node
if it moves in to its projection interval. Now con-
sider which cases can lead to gap inheritance: in case
(b), there is a single path from the ancestor to the
root of the subtree, so the parent of the subtree will
have no gap inheritance and any higher ancestors
will have a single child inherit the gap created by this
movement. In case (c), it is possible for there to be
multiple children that inherit this newly created gap
if multiple children had descendents on both sides.
However, the assumption of upward movement in
the phrase structure tree should rule out movement
into the projection interval of a non-ancestor. There-
fore, under these syntactic assumptions, we would
expect at most one child to inherit a parent?s gap.
5The Proper Binding Condition (Fiengo, 1977) asserts that a
moved element leaves behind a trace (unpronounced element),
which must be c-commanded (Reinhart, 1976) by the corre-
sponding pronounced material in its final location. Informally,
c-commanded means that the first node is descended from the
lowest ancestor of the other that has more than one child.
4.1 Parsing Mild+1-Inherit Trees
Finding the optimal Mild+1-Inherit tree can be done
by bottom-up constructing the tree for each node and
its descendants. We can maintain subtrees with two
intervals (two endpoints each) and one root (O(n5)
space). Consider the most complicated possible
case: a parent that has a gap, a (single) child which
inherits the gap, and additional children. An exam-
ple of this is seen with the parent node 3 in Figure
1(b).
This subtree can be constructed by first starting
with the child spanning the gap, updating its root
index to be the parent, and then expanding the inter-
val indices to the left and right to include the other
children. In each case, only one index needs to be
updated at a time, so the optimal tree can be found
in O(n6) time. In the Figure 1(b) example, the sub-
tree rooted at 3 would be built by starting with the
intervals [1, 2] ? [12, 13] rooted at 2, first adding the
edge from 2 to 3 (so the root is updated to 3), then
adding an edge from 3 to 4 to extend the left inter-
val to [1, 5], and then adding an edge from 3 to 11 to
extend the right interval to [8, 13]. The subtree cor-
responds to the completed item [1, 5]? [8, 13] rooted
at 3.
This procedure corresponds to Go?mez-Rodr??guez
et al2011)?s O(n7) algorithm for parsing mildly
non-projective structures if the most expensive step
(Combine Shrinking Gap Centre) is dropped; this
step would only ever be needed if a parent node has
481
more than one child inheriting its gap.
This is also similar in spirit to the algorithm de-
scribed in Satta and Schuler (1998) for parsing a
restricted version of TAG, in which there are some
limitations on adjunction operations into the spines
of trees.6 That algorithm has similar steps and items,
with the root portion of the item replaced with a
node in a phrase structure tree (which may be a non-
terminal).
5 Gap-minding Trees
The algorithm in the previous section used O(n5)
space and O(n6) time. While more efficient than
parsing in the space of mildly projective trees, this
is still probably not practically implementable. Part
of the difficulty lies in the fact that gap inheritance
causes the two non-contiguous projection intervals
to be coupled.
Definition 6. A tree is called gap-minding7 if it has
gap degree at most one, is well-nested, and has gap
inheritance degree 0.
Gap-minding trees still have good empirical cov-
erage (between 90.4% for Dutch and 97.7% for
Swedish). We now turn to the parsing of gap-
minding trees and show how a few consequences of
its definition allow us to use items ranging over only
one interval.
In Figure 1(c), notice how each rectangle has
edges incoming from exactly one node. This is not
unique to this example; all projection intervals in a
gap-minding tree have incoming edges from exactly
one node outside the interval.
Claim 1. Within a gap-minding tree, consider any
node n with a gap (i.e., n?s projection forms two
non-contiguous intervals [xi, xj ] ? [xk, xl]). Let p
be the parent of n.
1. For each of the intervals of n?s projection:
(a) If the interval contains n, the only edge
incoming to that interval is from p to n.
6That algorithm has a running time of O(Gn5), where as
written G would likely add a factor of n2 with bilexical selec-
tional preferences; this can be lowered to n using the same tech-
nique as in Eisner and Satta (2000) for non-restricted TAG.
7The terminology is a nod to the London Underground but
imagines parents admonishing children to mind the gap.
(b) If the interval does not contain n, all edges
incoming to that interval come from n.
2. For the gap interval ([xj+1, xk?1]):
(a) If the interval contains p, then the only
edge incoming is from p?s parent to p
(b) If the interval does not contain p, then all
edges incoming to that interval come from
p.
As a consequence of the above, [xi, xj ] ? {n} forms
a gap-minding tree rooted at n, [xk, xl] ? {n}
also forms a gap-minding tree rooted at n, and
[xj+1, xk?1] ? {p} forms a gap-minding tree rooted
at p.
Proof. (Part 1): Assume there was a directed edge
(x, y) such that y is inside a projection interval of n
and x is not inside the same interval, and x 6= y 6= n.
y is a descendant of n since it is contained in n?s pro-
jection. Since there is a directed edge from x to y,
x is y?s parent, and thus x must also be a descen-
dant of n and therefore in another of n?s projection
intervals. Since x and y are in different intervals,
then whichever child of n that x and y are descended
from would have inherited n?s gap, leading to a con-
tradiction.
(Part 2): First, suppose there existed a set of nodes
in n?s gap which were not descended from p. Then
p has a gap over these nodes. (p clearly has descen-
dants on each side of the gap, because all descen-
dants of n are also descendants of p). n, p?s child,
would then have descendants on both sides of p?s
gap, which would violate the property of no gap in-
heritance. It is also not possible for there to be edges
incoming from other descendants of p outside the
gap, as that would imply another child of p being
ill-nested with respect to n.
From the above, we can build gap-minding trees
using only single intervals, potentially with a sin-
gle node outside of the interval. Our objective is
to find the maximum scoring gap-minding tree, in
which the score of a tree is the sum of the scores of
its edges. Let Score(p,x) indicate the score of the
directed edge from p to x.
Therefore, the main type of sub-problems we will
use are:
482
1. C[i, j,p]: The maximum score of any gap-
minding tree, rooted at p, with vertices [i, j] ?
{p} (p may or may not be within [i, j]).
This improves our space requirement, but not nec-
essarily the time requirement. For example, if we
built up the subtree in Figure 1(c) by concatenating
the three intervals [1, 5] rooted at 3, [6, 7] rooted at 6,
and [8, 13] rooted at 3, and add the edge 6 ? 3, we
would still need 6 indices to describe this operation
(the four interval endpoints and the two roots), and
so we have not yet improved the running time over
the Inherit-1 case.
By part 2, we can concatenate one interval of a
child with its gap, knowing that the gap is entirely
descended from the child?s parent, and forget the
concatenation split point between the parent?s other
descendants and this side of the child. This allows us
to substitute all operations involving 6 indices with
two operations involving just 5 indices. For exam-
ple, in Figure 1(c), we could first merge [6, 7] rooted
at 6 with [8, 13] rooted at 3 to create an interval
[6, 13] and say that it is descended from 6, with the
rightmost side descended from its child 3. That step
required 5 indices. The following step would merge
this concatenated interval ([6, 13] rooted at 6 and 3)
with [1, 5] rooted at 3. This step also requires only 5
indices.
Our helper subtype we make use of is then:
2. D[i, j,p,x,b]: The maximum score of any set
of two gap-minding trees, one rooted at p, one
rooted at x, with vertices [i, j] ? {p, x} (x /?
[i, j], p may or may not be in [i, j]), such that
for some k, vertices [i, k] are in the tree rooted
at p if b = true (and at x if b = false), and
vertices [k+1, j] are in the tree rooted at x (p).
Consider an optimum scoring gap-minding tree T
rooted at p with vertices V = [i, j] ? {p} and edges
E, where E 6= ?. The form of the dynamic program
may depend on whether:
? p is within (i, j) (I) or external to [i, j] (E)8
8In the discussion we will assume that p 6= i and p 6= j,
since any optimum solution with V = [i, j] ? {i} and a root
at i will be equivalent to V = [i + 1, j] ? {i} rooted at i (and
similarly for p = j).
We can exhaustively enumerate all possibilities for
T by considering all valid combinations of the fol-
lowing binary cases:
? p has a single child (S) or multiple children (M)
? i and j are descended from the same child of p
(C) or different children of p (D)
Note that case (S/D) is not possible: i and j cannot
be descended from different children of p if p has
only a single child. We therefore need to find the
maximum scoring tree over the three cases of S/C,
M/C, and M/D.
Claim 2. Let T be the optimum scoring gap-
minding tree rooted at p with vertices V = [i, j] ?
{p}. Then T and its score are derived from one of
the following:
S/C If p has a single child x in T , then if p ? (i, j)
(I), T ?s score is Score(p,x)+C[i,p?1,x]+
C[p+ 1, j,x]; if p /? [i, j] (E), T ?s score is
Score(p,x) +C[i, j,x].
M/C If p has multiple children in T and i and j
are descended from the same child x in T , then
there is a split point k such that T ?s score is:
Score(p,x)+C[i,k,x]+D[k+ 1, j,p,x,T]
if x is on the left side of its own gap, and
T ?s score is: Score(p,x) + C[k, j,x] +
D[i,k? 1,p,x,F] if x is on the right side.
M/D If p has multiple children in T and i and j
are descended from different children in T , then
there is a split point k such that T ?s score is
C[i,k,p] +C[k+ 1, j,p].
T has the maximum score over each of the above
cases, for all valid choices of x and k.
Proof. Case S/C: If p has exactly one child x,
then the tree can be decomposed into the edge
from p to x and the subtree rooted at x. If p
is outside the interval, then the maximum scor-
ing such tree is clearly Score(p,x) + C[i, j,x].
If p is inside, then x has a gap across p, and
so using Claim 1, the maximum scoring tree
rooted at p with a single child x has score of
Score(p,x) +C[i,p? 1,x] +C[p+ 1, j,x].
Case M/C: If there are multiple children and the
endpoints are descended from the same child x, then
483
the child x has to have gap degree 1. x itself is on
either the left or right side of its gap. For the mo-
ment, assume x is in the left interval. By Claim 1,
we can split up the score of the tree as the score of
the edge from p to x (Score(p,x)), the score of the
subtree corresponding to the projection of x to the
left of its gap (C[i,k,x]), and the score of the sub-
trees rooted at p with its remaining children and the
subtree rooted at x corresponding to the right side
of x?s projection (D[k+ 1, j,p,x,T]). The case in
which x is on the right side of its gap is symmetric.
Case M/D: If there are multiple children and the
endpoints are descended from different children of
p, then there must exist a split point k that parti-
tions the children of p into two non-empty sets, such
that each child?s projection is either entirely on the
left or entirely on the right of the split point. We
show one such split point to demonstrate that there
always exists at least one. Let x be the child of p
that i is descended from, and let xl and xr be x?s
leftmost and right descendants, respectively.9 Con-
sider all the children of p (whose projections taken
together partition [i, j] ? {p}). No child can have
descendants both to the left of xr and to the right
of xr, because otherwise that child and x would be
ill-nested. Therefore we can split up the interval at
xr to have two gap-minding trees, both rooted at p.
The score of T is then the sum of the scores of the
best subtree rooted at p over [i, k] (C[i,k,p]) and
the score of the best subtree rooted at p over [k+1, j]
(C[k+ 1, j,p]).
The above cases cover all non-empty gap-
minding trees, so the maximum will be found.
Using Claim 2 to Devise an Algorithm The above
claim showed that any problem of type C can be
decomposed into subproblems of types C and D.
From the definition of D, any problem of type D can
clearly be decomposed into two problems of type C
? simply split the interval at the split point known
to exist and assign p or x as the roots for each side
of the interval, as prescribed by the boolean b:
D(i, j,p,x,T) = maxkC[i,k,p] +C[k+ 1, j,x]
D(i, j,p,x,F) = maxkC[i,k,x] +C[k+ 1, j,p]
9Note that xl = i by construction, and xr 6= j (because the
endpoints are descended from different children).
Algorithm 1 makes direct use of the above claims.
Note that in every gap-minding tree referred to
in the cases above, all vertices that were not the
root formed a single interval. Algorithm 1 builds
up trees in increasing sizes of [i, j] ? {p}. The
tree in C[i, j,p] corresponds to the maximum of
four subroutines: SingleChild (S/C), EndpointsDiff
(M/D), EndsFromLeftChild (M/C), and EndsFrom-
RightChild (M/C). The D subproblems are filled in
with the subroutine Max2Subtrees, which uses the
above discussion. The maximum score of any gap-
minding tree is then found in C[1,n,0], and the tree
itself can be found using backpointers.
5.1 Runtime analysis
If the input is assumed to be the complete graph (any
word can have any other word as its parent), then
the above algorithm takes O(n5) time. The most
expensive steps are M/C, which take O(n2) time to
fill in each of the O(n3) C cells. and solving a D
subproblem, which takes O(n) time on each of the
O(n4) possible such problems.
Pruning: In practice, the set of edges considered
(m) is not necessarily O(n2). Many edges can be
ruled out beforehand, either based on the distance
in the sentence between the two words (Eisner and
Smith, 2010), the predictions of a local ranker (Mar-
tins et al2009), or the marginals computed from a
simpler parsing model (Carreras et al2008).
If we choose a pruning strategy such that each
word has at most k potential parents (incoming
edges), then the running time drops to O(kn4). The
five indices in an M/C step were: i, j, k, p, and x.
As there must be an edge from p to x, and x only has
k possible parents, there are now only O(kn4) valid
such combinations. Similarly, each D subproblem
(which ranges over i, j, k, p, x) may only come into
existence because of an edge from p to x, so again
the runtime of these such steps drops to O(kn4).
6 Extension to Grandparent
Factorizations
The ability to define slightly non-local features has
been shown to improve parsing performance. In this
section, we assume a grandparent-factored model,
where the score of a tree is now the sum over scores
of (g, p, c) triples, where (g, p) and (p, c) are both
484
directed edges in the tree. Let Score(g,p, c) indi-
cate the score of this grandparent-parent-child triple.
We now show how to extend the above algorithm
to find the maximum scoring gap-minding tree with
grandparent scoring.
Our two subproblems are now C[i, j,p,g] and
D[i, j,p,x,b,g]; each subproblem has been aug-
mented with an additional grandparent index g,
which has the meaning that g is p?s parent. Note that
g must be outside of the interval [i, j] (if it were not,
a cycle would be introduced). Edge scores are now
computed over (g, p, x) triples. In particular, claim
2 is modified:
Claim 3. Let T be the optimum scoring gap-
minding tree rooted at p with vertices V = [i, j] ?
{p}, where p ? (i, j) (I), with a grandparent index
g (g /? V ). Then T and its score are derived from
one of the following:
S/C If p has a single child x in T , then if
p ? (i, j) (I), T ?s score is Score(g,p,x) +
C[i,p?1,x,p]+C[p+ 1, j,x,p]; if p /? [i, j]
(E), T ?s score is Score(g,p,x)+C[i, j,x,p].
M/C If p has multiple children in T and i
and j are descended from the same child
x in T , then there is a split point k
such that T ?s score is: Score(g,p,x) +
C[i,k,x,p] + D[k+ 1, j,p,x,T,g] if x is
on the left side of its own gap, and T ?s
score is: Score(g,p,x) + C[k, j,x,p] +
D[i,k? 1,p,x,F,g] if x is on the right side.
M/D If p has multiple children in T and i and j
are descended from different children in T , then
there is a split point k such that T ?s score is
C[i,k,p,g] +C[k+ 1, j,p,g].
T has the maximum score over each of the above
cases, for all valid choices of x and k.
Note that for subproblems rooted at p, g is the
grandparent index, while for subproblems rooted at
x, p is the updated grandparent index. The D sub-
problems with the grandparent index are shown be-
low:
D(i, j,p,x,T,g) = maxkC[i,k,p,g] +C[k+ 1, j,x,p]
D(i, j,p,x,F,g) = maxkC[i,k,x,p] +C[k+ 1, j,p,g]
We have added another index which ranges over
n, so without pruning, we have now increased the
running time to O(n6). However, every step now in-
cludes both a g and a p (and often an x), so there is
at least one implied edge in every step. If pruning
is done in such a way that each word has at most k
parents, then each word?s set of grandparent and par-
ent possibilities is at most k2. To run all of the S/C
steps, we therefore need O(k2n3) time; for all of the
M/C steps, O(k2n4) time; for all of the M/D steps,
O(kn4); for all of the D subproblems, O(k2n4). The
overall running time is therefore O(k2n4), and we
have shown that when edges are sufficiently pruned,
grandparent factors add only an extra factor of k, and
not a full extra factor of n.
7 Experiments
The space of projective trees is strictly contained
within the space of gap-minding trees which is
strictly contained within spanning trees. Which
space is most appropriate for natural language pars-
ing may depend on the particular language and the
type and frequencies of non-projective structures
found in it. In this section we compare the parsing
accuracy across languages for a parser which uses
either the Eisner algorithm (projective), MST (span-
ning trees), or MaxGapMindingTree (gap-minding
trees) as its decoder for both training and inference.
We implemented both the basic gap-minding al-
gorithm and the gap-minding algorithm with grand-
parent scoring as extensions to MSTParser10. MST-
Parser (McDonald et al2005b; McDonald et al
2005a) uses the Margin Infused Relaxed Algo-
rithm (Crammer and Singer, 2003) for discrimina-
tive training. Training requires a decoder which
produces the highest scoring tree (in the space of
valid trees) under the current model weights. This
same decoder is then used to produce parses at test
time. MSTParser comes packaged with the Eis-
ner algorithm (for projective trees) and MST (for
spanning trees). MSTParser also includes two sec-
ond order models: one of which is a projective de-
coder that also scores siblings (Proj+Sib) and the
other of which produces non-projective trees by re-
arranging edges after producing a projective tree
(Proj+Sib+Rearr). We add a further decoder with
10http://sourceforge.net/projects/mstparser/
485
the algorithm presented here for gap minding trees,
and plan to make the extension publicly available.
The gap-minding decoder has both an edge-factored
implementation and a version which scores grand-
parents as well.11
The gap-minding algorithm is much more effi-
cient when edges have been pruned so that each
word has at most k potential parents. We use the
weights from the trained MST models combined
with the Matrix Tree Theorem (Smith and Smith,
2007; Koo et al2007; McDonald and Satta, 2007)
to produce marginal probabilities of each edge. We
wanted to be able to both achieve the running time
bound and yet take advantage of the fact that the
size of the set of reasonable parent choices is vari-
able. We therefore use a hybrid pruning strategy:
each word?s set of potential parents is the smaller of
a) the top k parents (we chose k = 10) or b) the set
of parents whose probabilities are above a thresh-
old (we chose th = .001). The running time for
the gap-minding algorithm is then O(kn4); with the
grandparent features the gap-minding running time
is O(k2n4).
The training and test sets for the six languages
come from the CoNLL-X shared task.12 We train
the gap-minding algorithm on sentences of length
at most 10013 (the vast majority of sentences). The
projective and MST models are trained on all sen-
tences and are run without any pruning. The Czech
training set is much larger than the others and so for
Czech only the first 10,000 training sentences were
used. Testing is on the full test set, with no length
restrictions.
The results are shown in Table 2. The first three
lines show the first order gap-minding decoder com-
pared with the first order projective and MST de-
11The grandparent features used were identical to the fea-
tures provided within MSTParser for the second-order sibling
parsers, with one exception ? many features are conjoined with
a direction indicator, which in the projective case has only two
possibilities. We replaced this two-way distinction with a six-
way distinction of the six possible orders of the grandparent,
parent, and child.
12MSTParser produces labeled dependencies on CoNLL for-
matted input. We replace all labels in the training set with a
single dummy label to produce unlabeled dependency trees.
13Because of long training times, the gap-minding with
grandparent models for Portuguese and Swedish were trained
on only sentences up to 50 words.
Ar Cz Da Du Pt Sw
Proj. 78.0 80.0 88.2 79.8 87.4 86.9
MST 78.0 80.4 88.1 84.6 86.7 86.2
Gap-Mind 77.6 80.8 88.6 83.9 86.8 86.0
Proj+Sib 78.2 80.0 88.9 81.1 87.5 88.1
+Rearr 78.5 81.3 89.3 85.4 88.2 87.7
GM+Grand 78.3 82.1 89.1 84.6 87.7 88.5
Table 2: Unlabeled Attachment Scores on the CoNLL-X
shared task test set.
coders. The gap-minding decoder does better than
the projective decoder on Czech, Danish, and Dutch,
the three languages with the most non-projectivity,
even though it was at a competitive disadvantage in
terms of both pruning and (on languages with very
long sentences) training data. The gap-minding de-
coder with grandparent features is better than the
projective decoder with sibling features on all six
of the languages. On some languages, the local
search decoder with siblings has the absolute high-
est accuracy in Table 2; on other languages (Czech
and Swedish) the gap-minding+grandparents has the
highest accuracy. While not directly comparable be-
cause of the difference in features, the promising
performance of the gap-minding+grandparents de-
coder shows that the space of gap-minding trees is
larger than the space of projective trees, yet unlike
spanning trees, it is tractable to find the best tree with
higher order features. It would be interesting to ex-
tend the gap-minding algorithm to include siblings
as well.
8 Conclusion
Gap inheritance, a structural property on trees, has
implications both for natural language syntax and
for natural language parsing. We have shown that
the mildly non-projective trees present in natural
language treebanks all have zero or one children in-
herit each parent?s gap. We also showed that the as-
sumption of 1 gap inheritance removes a factor of
n from parsing time, and the further assumption of
0 gap inheritance removes yet another factor of n.
The space of gap-minding trees provides a closer fit
to naturally occurring linguistic structures than the
space of projective trees, and unlike spanning trees,
the inclusion of higher order factors does not sub-
stantially increase the difficulty of finding the maxi-
mum scoring tree in that space.
486
Acknowledgments
We would like to thank Aravind Joshi for comments
on an earlier draft. This material is based upon
work supported under a National Science Founda-
tion Graduate Research Fellowship.
Algorithm 1: MaxGapMindingTree
Init: ?i?[1,n]C[i, i, i] = 0
for size = 0 to n? 1 do
for i = 1 to n? size do
j = i+ size
/* Endpoint parents */
if size > 0 then
C[i, j, i] = C[i+ 1, j, i]
C[i, j, j] = C[i, j ? 1, j]
/* Interior parents */
for p = i+ 1 to j ? 1 do
C[i, j, p] = max (SingleChild(i,j,p),
EndpointsDiff(i,j,p),
EndsFromLeftChild(i,j,p),
EndsFromRightChild(i,j,p))
/* Exterior parents */
forall the p ? [0, i? 1] ? [j + 1, n] do
C[i, j, p] = max (SingleChild(i,j,p),
EndpointsDiff(i,j,p),
EndsFromLeftChild(i,j,p),
EndsFromRightChild(i,j,p))
/* Helper subproblems */
for p ? [0, n] do
forall the x ? PosChild[p] ? x /? [i, j] do
if p 6= j then
D[i, j, p, x, T ] = Max2Subtrees(i, j, p, x, T )
if p 6= i then
D[i, j, p, x, F ] = Max2Subtrees(i, j, p, x, F )
Final answer: C[1, n, 0]
Function SingleChild(i,j,p)
X = PosChild[p] ? [i, j]
/* Interior p */
if p > i ? p < j then
return maxx?X C[i, p? 1, x]
+C[p+ 1, j, x] + Score(p, x)
/* Exterior p */
else
return maxx?X C[i, j, x] + Score(p, x)
Function EndpointsDiff(i,j,p)
return maxk?[i,j?1] C[i, k, p] + C[k + 1, j, p]
Function EndsFromLeftChild(i,j,p)
/* Interior p */
if p > i ? p < j then
X = PosChild[p] ? [i, p? 1]
forall the x ? X ? x < p do
K[x] = [x, p? 1]
/* Exterior p */
else
X = PosChild[p] ? [i, j]
forall the x ? X do
K = [x, j ? 2]
return maxx?X,k?K[x] C[i, k, x]
+Score(p, x) +D[k + 1, j, p, x, T ]
Function EndsFromRightChild(i,j,p)
/* Interior p */
if p > i ? p < j then
X = PosChild[p] ? [p+ 1, j]
forall the x ? X ? x > p do
K[x] = [p+ 1, x]
/* Exterior p */
else
X = PosChild[p] ? [i, j]
forall the x ? X do
K[x] = [i+ 2, x]
return maxx?X,k?K[x] C[k, j, x]
+Score(p, x) +D[i, k ? 1, p, x, F ]
Function Max2Subtrees(i,j,p,x,pOnLeft)
/* Interior p */
if p ? i ? p ? j then
if pOnLeft then
K = [p, j ? 1]
return maxk?K C[i, k, p] + C[k + 1, j, x]
else
K = [i, p? 1]
return maxk?K C[i, k, x] + C[k + 1, j, p]
/* Exterior p */
else
K = [i, j ? 1]}
if pOnLeft then
return maxk?K C[i, k, p] + C[k + 1, j, x]
else
return maxk?K C[i, k, x] + C[k + 1, j, p]
487
References
M. Bodirsky, M. Kuhlmann, and M. Mo?hl. 2005. Well-
nested drawings as models of syntactic structure. In
In Tenth Conference on Formal Grammar and Ninth
Meeting on Mathematics of Language, pages 88?1.
University Press.
X. Carreras, M. Collins, and T. Koo. 2008. Tag, dynamic
programming, and the perceptron for efficient, feature-
rich parsing. In Proceedings of CoNLL, pages 9?16.
Association for Computational Linguistics.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957?961.
J. Chen-Main and A. Joshi. 2010. Unavoidable ill-
nestedness in natural language and the adequacy of
tree local-mctag induced dependency structures. In
Proceedings of the Tenth International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+ 10).
J. Chen-Main and A.K. Joshi. 2012. A depen-
dency perspective on the adequacy of tree local multi-
component tree adjoining grammar. In Journal of
Logic and Computation. (to appear).
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951?991, March.
J. Eisner and G. Satta. 2000. A faster parsing algorithm
for lexicalized tree-adjoining grammars. In Proceed-
ings of the 5th Workshop on Tree-Adjoining Grammars
and Related Formalisms (TAG+5), pages 14?19.
J. Eisner and N.A. Smith. 2010. Favor short dependen-
cies: Parsing with soft and hard constraints on depen-
dency length. In Harry Bunt, Paola Merlo, and Joakim
Nivre, editors, Trends in Parsing Technology: Depen-
dency Parsing, Domain Adaptation, and Deep Parsing,
chapter 8, pages 121?150. Springer.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers, October.
R. Fiengo. 1977. On trace theory. Linguistic Inquiry,
8(1):35?61.
C. Go?mez-Rodr??guez, J. Carroll, and D. Weir. 2011. De-
pendency parsing schemata and mildly non-projective
dependency parsing. Computational Linguistics,
37(3):541?586.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of ACL, pages 1?11.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proceedings of EMNLP-CoNLL.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proceedings of EMNLP,
pages 1288?1298.
M. Kuhlmann and J. Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings of
COLING/ACL, pages 507?514.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL, pages 342?
350.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 121?132.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
T. Reinhart. 1976. The Syntactic Domain of Anaphora.
Ph.D. thesis, Massachusetts Institute of Technology.
G. Satta and W. Schuler. 1998. Restrictions on tree ad-
joining languages. In Proceedings of COLING-ACL,
pages 1176?1182.
D.A. Smith and N.A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In Proceedings of
EMNLP-CoNLL.
488
Transactions of the Association for Computational Linguistics, 1 (2013) 13?24. Action Editor: Giorgio Satta.
Submitted 11/2012; Published 3/2013. c?2013 Association for Computational Linguistics.
Finding Optimal 1-Endpoint-Crossing Trees
Emily Pitler, Sampath Kannan, Mitchell Marcus
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
epitler,kannan,mitch@seas.upenn.edu
Abstract
Dependency parsing algorithms capable of
producing the types of crossing dependencies
seen in natural language sentences have tra-
ditionally been orders of magnitude slower
than algorithms for projective trees. For 95.8-
99.8% of dependency parses in various nat-
ural language treebanks, whenever an edge
is crossed, the edges that cross it all have a
common vertex. The optimal dependency tree
that satisfies this 1-Endpoint-Crossing prop-
erty can be found with an O(n4) parsing al-
gorithm that recursively combines forests over
intervals with one exterior point. 1-Endpoint-
Crossing trees also have natural connections
to linguistics and another class of graphs that
has been studied in NLP.
1 Introduction
Dependency parsing is one of the fundamental prob-
lems in natural language processing today, with ap-
plications such as machine translation (Ding and
Palmer, 2005), information extraction (Culotta and
Sorensen, 2004), and question answering (Cui et
al., 2005). Most high-accuracy graph-based depen-
dency parsers (Koo and Collins, 2010; Rush and
Petrov, 2012; Zhang and McDonald, 2012) find the
highest-scoring projective trees (in which no edges
cross), despite the fact that a large proportion of nat-
ural language sentences are non-projective. Projec-
tive trees can be found in O(n3) time (Eisner, 2000),
but cover only 63.6% of sentences in some natural
language treebanks (Table 1).
The class of directed spanning trees covers all
treebank trees and can be parsed in O(n2) with
edge-based features (McDonald et al, 2005), but it
is NP-hard to find the maximum scoring such tree
with grandparent or sibling features (McDonald and
Pereira, 2006; McDonald and Satta, 2007).
There are various existing definitions of mildly
non-projective trees with better empirical coverage
than projective trees that do not have the hardness of
extensibility that spanning trees do. However, these
have had parsing algorithms that are orders of mag-
nitude slower than the projective case or the edge-
based spanning tree case. For example, well-nested
dependency trees with block degree 2 (Kuhlmann,
2013) cover at least 95.4% of natural language struc-
tures, but have a parsing time of O(n7) (G?mez-
Rodr?guez et al, 2011).
No previously defined class of trees simultane-
ously has high coverage and low-degree polynomial
algorithms for parsing, allowing grandparent or sib-
ling features.
We propose 1-Endpoint-Crossing trees, in which
for any edge that is crossed, all other edges that
cross that edge share an endpoint. While simple
to state, this property covers 95.8% or more of de-
pendency parses in natural language treebanks (Ta-
ble 1). The optimal 1-Endpoint-Crossing tree can
be found in faster asymptotic time than any previ-
ously proposed mildly non-projective dependency
parsing algorithm. We show how any 1-Endpoint-
Crossing tree can be decomposed into isolated sets
of intervals with one exterior point (Section 3). This
is the key insight that allows efficient parsing; the
O(n4) parsing algorithm is presented in Section 4.
1-Endpoint-Crossing trees are a subclass of 2-planar
graphs (Section 5.1), a class that has been studied
13
in NLP. 1-Endpoint-Crossing trees also have some
linguistic interpretation (pairs of cross serial verbs
produce 1-Endpoint-Crossing trees, Section 5.2).
2 Definitions of Non-Projectivity
Definition 1. Edges e and f cross if e and f have
distinct endpoints and exactly one of the endpoints
of f lies between the endpoints of e.
Definition 2. A dependency tree is 1-Endpoint-
Crossing if for any edge e, all edges that cross e
share an endpoint p.
Table 1 shows the percentage of dependency
parses in the CoNLL-X training sets that are 1-
Endpoint-Crossing trees. Across six languages with
varying amounts of non-projectivity, 95.8-99.8%
of dependency parses in treebanks are 1-Endpoint-
Crossing trees.1
We next review and compare other relevant def-
initions of non-projectivity from prior work: well-
nested with block degree 2, gap-minding, projective,
and 2-planar.
The definitions of block degree and well-
nestedness are given below:
Definition 3. For each node u in the tree, a block of
the node is ?a longest segment consisting of descen-
dants of u.? (Kuhlmann, 2013). The block-degree of
u is ?the number of distinct blocks of u?. The block
degree of a tree is the maximum block degree of any
of its nodes. The gap degree is the number of gaps
between these blocks, and so by definition is one less
than the block degree. (Kuhlmann, 2013)
Definition 4. Two trees ?T1 and T2 interleave iff
there are nodes l1,r1 ? T1 and l2,r2 ? T2 such that
l1 < l2 < r1 < r2.? A tree is well-nested if no two
disjoint subtrees interleave. (Bodirsky et al, 2005)
As can be seen in Table 1, 95.4%-99.9% of depen-
dency parses across treebanks are both well-nested
and have block degree 2. The optimal such tree can
be found in O(n7) time and O(n5) space (G?mez-
Rodr?guez et al, 2011).
1Conventional edges from the artificial root node to the
root(s) of the sentence reduce the empirical coverage of 1-
Endpoint-Crossing trees. Excluding these artificial root edges,
the empirical coverage for Dutch rises to 12949 (97.0%). These
edges have no effect on the coverage of well-nested trees with
block degree at most 2, gap-minding trees, or projective trees.
a c d e fb g
(a)
a b c d fe g h
(b)
Figure 1: 1a is 1-Endpoint-Crossing, but is neither
block degree 2 nor well-nested; 1b is gap-minding
but not 2-planar.
Definition 5. A tree is gap-minding if it is well-
nested, has gap degree at most 1, and has gap inher-
itance degree 0. Gap inheritance degree 0 requires
that there are no child nodes with descendants in
more than one of their parent?s blocks. (Pitler et
al., 2012)
Gap-minding trees can be parsed in O(n5) (Pitler
et al, 2012). They have slightly less empirical cov-
erage, however: 90.4-97.7% (Table 1).
Definition 6. A tree is projective if it has block de-
gree 1 (gap degree 0).
This definition has the least coverage (as low as
63.6% for Dutch), but can be parsed in O(n3) (Eis-
ner, 2000).
Definition 7. A tree is 2-planar if each edge can be
drawn either above or below the sentence such that
no edges cross (G?mez-Rodr?guez and Nivre, 2010).
G?mez-Rodr?guez and Nivre (2010) presented a
transition-based parser for 2-planar trees, but there
is no known globally optimal parsing algorithm for
2-planar trees.
Clearly projective ( gap-minding ( well-nested
with block degree at most 2. In Section 5.1, we
prove the somewhat surprising fact that 1-Endpoint-
Crossing ( 2-planar. These are two distinct hi-
erarchies capturing different dimensions of non-
projectivity: 1-Endpoint-Crossing 6? well-nested
with block degree 2 (Figure 1a), and gap-minding
6? 2-planar (Figure 1b).
3 Edges (and their Crossing Point) Define
Isolated Crossing Regions
We introduce notation to facilitate the discussion:
14
Arabic Czech Danish Dutch Portuguese Swedish Parsing
1-Endpoint-Crossing 1457 (99.8) 71810 (98.8) 5144 (99.1) 12785 (95.8) 9007 (99.3) 10902 (98.7) O(n4)
Well-nested, block degree 2 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n7)
Gap-Minding 1394 (95.5) 70695 (97.2) 4985 (96.1) 12068 (90.4) 8481 (93.5) 10787 (97.7) O(n5)
Projective 1297 (88.8) 55872 (76.8) 4379 (84.4) 8484 (63.6) 7353 (81.1) 9963 (90.2) O(n3)
Sentences 1460 72703 5190 13349 9071 11042
Table 1: Over 95% of the dependency parse trees in the CoNLL-X training sets are 1-Endpoint-Crossing
trees. Coverage statistics and parsing times of previously proposed properties are shown for comparison.
Definition 8. Within a 1-Endpoint-Crossing tree,
the (crossing) pencil2 of an edge e (P(e)) is defined
as the set of edges (sharing an endpoint) that cross e.
The (crossing pencil) point of an edge e (Pt(e)) is
defined as the endpoint that all edges in P(e) share.
We will use euv to indicate an edge in either direc-
tion between u and v, i.e., either u? v or u? v.
Before defining the parsing algorithm, we first
give some intuition by analogy to parsing for pro-
jective trees. (This argument mirrors that of Eisner
(2000, pps.38-39).) Projective trees can be produced
using dynamic programming over intervals. Inter-
vals are sufficient for projective trees: consider any
edge euv in a projective tree.
The vertices in (u, v) must only have edges to
vertices in [u, v]. If there were an edge between a
vertex in (u, v) and a vertex outside [u, v], such an
edge would cross euv, which would contradict the
assumption of projectivity. Thus every edge in a
projective tree creates one interior interval isolated
from the rest of the tree, allowing dynamic program-
ming over intervals. We can analyze the case of 1-
Endpoint-Crossing trees in a similar fashion:
Definition 9. An isolated interval [i, j] has no edges
between the vertices in (i, j) and the vertices out-
side of [i, j]. An interval and one exterior vertex
[i, j] ? {x} is called an isolated crossing region if
the following two conditions are satisfied:
1. There are no edges between the vertices? (i, j)
and vertices /? [i, j] ? {x}
2. None of the edges between x and vertices ?
(i, j) are crossed by any edges with both end-
points ? (i, j)
2This notation comes from an analogy to geometry: ?A set
of distinct, coplanar, concurrent lines is a pencil of lines? (Rin-
genberg, 1967, p. 221); concurrent lines all intersect at the same
single point.
u v p
(a) [u, v] ? {p}
u v p
(b) [v, p] ? {u}
u p v
(c) [u, p] ? {v}
u p v
(d) [p, v] ? {u}
Figure 2: An edge euv and Pt(euv) = p form two
sets of isolated crossing regions (Lemma 1). 2a and
2b show p /? (u, v); 2c and 2d show p ? (u, v).
Lemma 1. Consider any edge euv and Pt(euv) = p
in a 1-Endpoint-Crossing forest F . Let l, r, and m
denote the leftmost, rightmost, and middle point out
of {u, v, p}, respectively. Then the three points u,
v, and p define two isolated crossing regions: (1)
[l,m] ? {r}, and (2) [m, r] ? {l}.
Proof. First note that as p = Pt(euv), P(euv) is
non-empty: there must be at least one edge between
vertices ? (u, v) and vertices /? [u, v]. p is either
/? [u, v] (i.e., p = l?p = r) or? (u, v) (i.e., p = m):
Case 1: p = l ? p = r: Assume without loss of
generality that u < v < p (i.e., p = r).
(a) [u, v] ? {p} is an isolated crossing region
(Figure 2a): Condition 1: Assume for the sake of
contradiction that there were an edge between a ver-
tex? (u, v) and a vertex /? [u, v]?{p}. Then such an
edge would cross euv without having an endpoint at
p, which contradicts the 1-Endpoint-Crossing prop-
erty for euv.
Condition 2: Assume that for some epa such that
a ? (u, v), epa was crossed by an edge in the interior
of (u, v). The interior edge would not share an end-
point with euv; since euv also crosses epa, this con-
tradicts the 1-Endpoint-Crossing property for epa.
15
(b) [v, p] ? {u} is an isolated crossing region
(Figure 2b): Condition 1: Assume there were an
edge eab with a ? (v, p) and b /? [v, p] ? {u}. b
cannot be in (u, v) (by above). Thus, b /? [u, p],
which implies that eab crosses the edges in P(euv);
as euv does not share a vertex with eab, this contra-
dicts the 1-Endpoint-Crossing property for all edges
in P(euv).
Condition 2: Assume that for some eua such that
a ? (v, p), eua was crossed by an edge in the interior
of (v, p). eua would also be crossed by all the edges
in P(euv); as the interior edge would not share an
endpoint with any of the edges inP(euv), this would
contradict the 1-Endpoint-Crossing property for eua.
Case 2: p = m :
(a) [u, p] ? {v} is an isolated crossing region
(Figure 2c): Condition 1: Assume there were an
edge eab with a ? (u, p) and b /? [u, p] ? {v}
(b ? (p, v) ? b /? [u, v]). First assume b ? (p, v).
Then eab crosses all edges in P(euv); as eab does not
share an endpoint with euv, this contradicts the 1-
Endpoint-Crossing property for the edges inP(euv).
Next assume b /? [u, v]. Then eab crosses euv; since
a 6= p?b 6= p, this violates the 1-Endpoint-Crossing
property for euv.
Condition 2: Assume that for some eva with a ?
(u, p), eva was crossed by an edge in the interior of
(u, v). eva is also crossed by all the edges inP(euv);
as the interior edge will not share an endpoint with
the edges inP(euv), this contradicts the 1-Endpoint-
Crossing property for eva.
(b) [p, v] ? {u} is an isolated crossing region
(Figure 2d): Symmetric to the above.
4 Parsing Algorithm
The optimal 1-Endpoint-Crossing tree can be found
using a dynamic programming algorithm that ex-
ploits the fact that edges and their crossing points
define intervals and isolated crossing regions. This
section assumes an arc-factored model, in which the
score of a tree is defined as the sum of the scores of
its edges; scoring functions for edges are generally
learned from data.
(a) Only edges inci-
dent to the Left point
of the interval may
cross the edges from
the exterior point
(b) Only edges in-
cident to the Right
point of the inter-
val may cross the
edges from the exte-
rior point
(c) both (LR) (d) Neither
Figure 3: Isolated crossing region sub-problems.
The dynamic program uses five types of sub-
problems: interval sub-problems for each interval
[i, j], denoted Int[i, j], and four types of isolated
crossing region sub-problems for each interval and
exterior point [i, j] ? {x}, which differ in whether
edges from the exterior point may be crossed by
edges with an endpoint at the Left point of the inter-
val, the Right point, both LR, or Neither (Figure 3).
L[i, j, x], for example, refers to an isolated crossing
region over the interval [i, j] with an exterior point
of x, in which edges incident to i (the left boundary
point) can cross edges between x and (i, j).
These distinctions allow the 1-Endpoint-Crossing
property to be globally enforced; crossing edges in
one region may constrain edges in another. For ex-
ample, consider that Figure 2a allows edges with an
endpoint at v to cross the edges from p, while Figure
2b allows edges from u into (v, p). Both simultane-
ously would cause a 1-Endpoint-Crossing violation
for the edges in P(euv). Figures 4 and 5 show valid
combinations of the sub-problems in Figure 3.
The full dynamic program is shown in Appendix
A. The final answer must be a valid dependency tree,
which requires each word to have exactly one parent
and prohibits cycles. We use booleans (bi, bj , bx) for
each sub-problem, in which the boolean is set to true
if and only if the solution to the sub-problem must
contain the incoming (parent) edge for the corre-
sponding boundary point. We use the suffix AFromB
for a sub-problem to enforce that a boundary point A
must be descended from boundary point B (to avoid
cycles). We will occasionally mention these issues,
16
(a) If l ? (k, j]:
ki l j
(b) If l ? (i, k):
li k j
(i) If the dashed edge exists:
All the edges from l into (i, k) must choose k
as their Pt. The interval decomposes into
S[eik] +R[i, k, l] + Int[k, l] + L[l, j, k]:
ki l j
(ii) If no edges like the dashed edge exist:
All edges from l into (i, k) may choose either i
or k as their Pt. The interval decomposes into
S[eik] + LR[i, k, l] + Int[k, l] + Int[l, j]:
i k l j
(i) If dashed edge exists: All the edges from l into
(k, j] must choose i as their Pt. The interval decom-
poses into S[eik] + Int[i, l] + L[l, k, i] +N [k, j, l]:
li k j
(ii) If no edges like the dashed edge exist: All edges
from l may choose k as their Pt. The interval decom-
poses into S[eik] +R[i, l, k] + Int[l, k] + L[k, j, l]:
li k j
Figure 4: Decomposing an Int[i, j] sub-problem, with Pt(eik) = l
but for simplicity focus the discussion on the decom-
position into crossing regions and the maintenance
of the 1-Endpoint-Crossing property. Edge direction
does not affect these points of focus, and so we will
refer simply to S[euv] to mean the score of either the
edge from u to v or vice-versa.
In the following subsections, we show that the op-
timal parse for each type of sub-problem can be de-
composed into smaller valid sub-problems. If we
take the maximum over all these possible combina-
tions of smaller solutions, we can find the maximum
scoring parse for that sub-problem. Note that the
overall tree is a valid sub-problem (over the inter-
val [0, n]), so the argument will also hold for finding
the optimal overall tree. Each individual vertex and
each pair of adjacent vertices (with no edges) triv-
ially form isolated intervals (as there is no interior);
this forms the base case of the dynamic program.
The overall dynamic program takes O(n4) time:
there are O(n2) interval sub-problems, each of
which needs two free split points to find the max-
imum, and O(n3) region sub-problems, each of
which is a maximization over one free split point.
4.1 Decomposing an Int sub-problem
Consider an isolated interval sub-problem Int[i, j].
There are three cases: (1) there are no edges between
i and the rest of the interval, (2) the longest edge in-
cident to i is not crossed, (3) the longest edge inci-
dent to i is crossed. An Int sub-problem can be de-
composed into smaller valid sub-problems in each of
these three cases. Finding the optimal Int forest can
be done by taking the maximum over these cases:
No edges between i and [i + 1, j]: The same set
of edges is also a valid Int[i + 1, j] sub-problem.
bi must be true for the Int[i + 1, j] sub-problem to
ensure i+ 1 receives a parent.
Furthest edge from i is not crossed: If the furthest
edge is to j, the problem can be decomposed into
S[eij ] + Int[i, j], as that edge has no effect on the
interior of the interval. Clearly, this is only appli-
cable if the boundary point needed a parent (as in-
dicated by the booleans) and the boolean must then
be updated accordingly. If the furthest edge is to
some k in (i, j), the problem is decomposed into
S[eik] + Int[i, k] + Int[k, j].
Furthest edge from i is crossed: This is the most
17
interesting case, which uses two split points: the
other endpoint of the edge (k), and l = Pt(eik). The
dynamic program depends on the order of k and l.
l /? (i,k) (Figure 4a): By Lemma 1, [i, k]?{l} and
[k, l]?{i} form isolated regions. (l, j] is the remain-
der of the interval, and the only vertex from [i, l) that
can have edges into (l, j] is k: (i, k) and (k, l) are
part of isolated regions, and i is ruled out because k
was i?s furthest neighbor.
If at least one edge from k into (l, j] (the dashed
line in Figure 4a) exists, the decomposition is as in
Figure 4a, Case i; otherwise, it is as in Figure 4a,
Case ii. In Case i, eik and the edge(s) between k and
(l, j] force all of the edges between l and (i, k) to
have k as their Pt. Thus, the region [i, k]?{l}must
be a sub-problem of type R (Figure 3b), as these
edges from l can only be crossed by edges with an
endpoint at k (the right endpoint of [i, k]). All of the
edges between k and (l, j] have l as their Pt, as they
are crossed by all the edges in P(eik), and so the
sub-problem corresponding to the region [l, j]?{k}
is of type L (Figure 3a). In Case ii, each of the edges
in P(eik) may choose either i or k as their Pt, so the
sub-problem [i, k] ? {l} is of type LR (Figure 3c).
Note that l = j is a special case of Case ii in which
the rightmost interval Int[l, j] is empty.
l ? (i,k) (Figure 4b): [i, l] ? {k} and [l, k] ? {i}
form isolated crossing regions by Lemma 1. There
cannot both be edges between i and (l, k) and be-
tween k and (i, l), as this would violate 1-Endpoint-
Crossing for the edges in P(eik). If there are any
edges between i and (l, k) (i.e., Case i in Figure 4b),
then all of the edges in P(eik) must choose i as their
Pt, and so these edges cannot be crossed at all in
the region [k, j]?{l}, and there cannot be any edges
from k into (i, l). If there are no such edges (Case
ii in 4b), then k must be a valid Pt for all edges in
P(eik), and so there can both be edges from k into
(i, l) and [k, j] ? {l} may be of type L (allowing
crossings with an endpoint at k).
4.2 Decomposing an LR sub-problem
An LR sub-problem is over an isolated crossing re-
gion [i, j] ? {x}, such that edges from x into (i, j)
may be crossed by edges with an endpoint at either i
or j. This sub-problem is only defined when neither
i nor j get their parent from this sub-problem. From
a top-down perspective, this case is only used when
there will be an edge between i and j (as in one of
the sub-problems in Figure 4a, Case ii).
If none of the edges from x are crossed by any
edges with an endpoint at i, this can be considered
an R problem. Similarly, if none are crossed by any
edges with an endpoint at j, this may be considered
an L sub-problem. The only case which needs dis-
cussion is when both edges with an endpoint at i and
also at j cross edges from x; see Figure 3c for a
schematic. In that scenario, there must exist a split
point such that: (1) to the left of the point, all edges
crossing x-edges have an endpoint at i, and to the
right of the point, all such edges have an endpoint at
j, and (2) no edges in the region cross the split point.
Let ri be i?s rightmost child in (i, j); let lj be
j?s leftmost child in (i, j). Every edge from x into
(i, ri) is crossed by eiri ; every edge between x and
(lj , j) is crossed by eljj . eiri cannot cross eljj , as
that would either violate 1-Endpoint-Crossing (be-
cause of the x-interior edges) or create a cycle (if
both children are also connected by an edge to x). ri
and lj also cannot be equal: as neither i nor j may
be assigned a parent, they must both be in the direc-
tion of the child, and the child cannot have multiple
parents. Thus, ri is to the left of lj .
Any split point between ri and lj clearly satis-
fies (1). There is at least one point within [ri, lj ]
that satisfies (2) as long as there is not a chain
of crossing edges from eiri to eljj . The proof is
omitted for space reasons, but such a chain can be
ruled out using a counting argument similar to that
in the proof in Section 5.1. The decomposition is:
L[i, k, x] +R[k, j, x] for some k ? (i, j).
4.3 Decomposing an N sub-problem
Consider the maximum scoring forest of type N
over [i, j] ? {x} (Figure 3d; no edges from x are
crossed in this sub-problem). If there are no edges
from x, then it is also a valid Int[i, j] sub-problem.
If there are edges between x and the endpoints i or j,
then the forest with that edge removed is still a valid
N sub-problem (with the ancestor and parent book-
keeping updated). Otherwise, if there are edges be-
tween x and (i, j), choose the neighbor of x closest
to j (call it k). Since the edge exk is not crossed,
there are no edges from [i, k) into (k, j]; since k was
the neighbor of x closest to j, there are no edges
from x into (k, j]. Thus, the region decomposes into
18
x k ji
(i) If dashed edge exists: All the edges from i into
(k, j] must choose x as their Pt. The interval decom-
poses into S[exk] + L[i, k, x] +N [k, j, i]:
x k ji
(ii) If no edges like the dashed edge exist: Edges
from i into (k, j] may choose k as their Pt. The in-
terval decomposes into S[exk]+Int[i, k]+L[k, j, i]:
x k ji
Figure 5: An L sub-problem over [i, j] ? {x}, k is
the neighbor of x furthest from i in the interval.
S[eik] + Int[k, j] +N [i, k, x].
As an aside, if bx was true (x needed a parent
from this sub-problem), and k was a child of x,
then x?s parent must come from the [i, k]?{x} sub-
problem. However, it cannot be a descendant of k,
as that would cause a cycle. Thus in this case, we
call the sub-problem a N_XFromI problem, to in-
dicate that x needs a parent, i and k do not, and x
must be descended from i, not k.
4.4 Decomposing an L or R sub-problem
An L sub-problem over [i, j]?{x} requires that any
edges in this region that cross an edge with an end-
point at x have an endpoint at i (the left endpoint). If
there are no edges between x and [i, j] in an L sub-
problem, then it is also a valid Int sub-problem over
[i, j]. If there are edges between x and i or j, then
the sub-problem can be decomposed into that edge
plus the rest of the forest with that edge removed.
The interesting case is when there are edges be-
tween x and the interior (Figure 5). Let k be the
neighbor of x within (i, j) that is furthest from i. As
all edges that cross exk will have an endpoint at i,
there are no edges between (i, k) and (k, j]. Com-
bined with the fact that k was the neighbor of x clos-
est to j, we have that [i, k] ? {x} must form an iso-
a b c d e f
Figure 6: 2-planar but not 1-Endpoint-Crossing
lated crossing region, as must [k, j] ? {i}.
If there are additional edges between x and the in-
terior (Case i in 5), all of the edges from i into (k, j]
cross both the edge exk and the other edges from x
into (i, k). The Pt for all these edges must there-
fore be x, and as x is not in the region [k, j] ? {i},
those edges cannot be crossed at all in that region
(i.e., [k, j] ? {i} must be of type N ). If there are no
additional edges from x into (i, k) (Case ii in Fig-
ure 5), then all of the edges from i into (k, j) must
choose either x or k as their Pt. As there will be no
more edges from x, choosing k as their Pt allows
strictly more trees, and so [k, j]? {i} can be of type
L (allowing edges from i to be crossed in that region
by edges with an endpoint at k).
An R sub-problem is identical, with k instead
chosen to be the neighbor of x furthest from j.
5 Connections
5.1 Graph Theory: All 1-Endpoint-Crossing
Trees are 2-Planar
The 2-planar characterization of dependency struc-
tures in G?mez-Rodr?guez and Nivre (2010) exactly
correspond to 2-page book embeddings in graph the-
ory: an embedding of the vertices in a graph onto
a line (by analogy, along the spine of a book), and
the edges of the graph onto one of 2 (more gener-
ally, k) half-planes (pages of the book) such that no
edges on the same page cross (Bernhart and Kainen,
1979). The problem of finding an embedding that
minimizes the number of pages required is a natural
formulation of many problems arising in disparate
areas of computer science, for example, sorting a se-
quence using the minimum number of stacks (Even
and Itai, 1971), or constructing fault-tolerant layouts
in VLSI design (Chung et al, 1987).
In this section we prove 1-Endpoint-Crossing ?
2-planar. These classes are not equal (Figure 6).
We first prove some properties about the crossings
graphs (G?mez-Rodr?guez and Nivre, 2010) of 1-
Endpoint-Crossing trees. The crossings graph of a
19
(a,b) (a,c)
(b,d) (c,e)
(d,f)
(a)
(a,b) (a,c)
(b,e)
(g,d)(h,f)
(b,g)
(g,h)
(b)
Figure 7: The crossing graphs for Figures 1a and 1b.
graph has a vertex corresponding to each edge in
the original, and an edge between two vertices if the
two edges they correspond to cross. The crossings
graphs for the dependency trees in Figures 1a and
1b are shown in Figures 7a and 7b, respectively.
Lemma 2. No 1-Endpoint-Crossing tree has a cycle
of length 3 in its crossings graph.
Proof. Assume there existed a cycle e1, e2, e3. e1
and e3 must share an endpoint, as they both cross
e2. Since e1 and e3 share an endpoint, e1 and e3 do
not cross. Contradiction.
Lemma 3. Any odd cycle of size n (n ? 4) in a
crossings graph of a 1-Endpoint-Crossing tree uses
at most n distinct vertices in the original graph.
Proof. Let e1, e2, ..., en be an odd cycle in a cross-
ings graph of a 1-Endpoint-Crossing tree with n ?
4. Since n ? 4, e1, e2, en?1, and en are distinct
edges. Let a be the vertex that e1 and en?1 share
(because they both cross en) and let b be the vertex
that e2 and en share (both cross e1). Note that e1
and en?1 cannot contain b and that e2 and en cannot
contain a (otherwise they would not cross an edge
adjacent to them along the cycle).
We will now consider how many vertices each
edge can introduce that are distinct from all vertices
previously seen in the cycle. e1 and e2 necessarily
introduce two distinct vertices each.
Let eo be the first odd edge that contains b (we
know one exists since en contains b). (o is at least 3,
since e1 does not contain b.) eo?s other vertex must
be the one shared with eo?2 (eo?2 does not contain b,
since eo was the first odd edge to contain b). There-
fore, both of eo?s vertices have already been seen
along the cycle.
Similarly, let ee be the first even edge that con-
tains an a. By the same reasoning, ee must not in-
troduce any new vertices.
All other edges ei such that i > 2 and ei 6= eo and
ei 6= ee introduce at most one new vertex, since one
must be shared with the edge ei?2. There are n ? 4
such edges.
Counting up all possibilities, the maximum num-
ber of distinct vertices is 4 + (n? 4) = n.
Theorem 1. 1-Endpoint-Crossing trees ? 2-planar.
Proof. Assume there existed an odd cycle in the
crossings graph of a 1-Endpoint-Crossing tree. The
cycle has size at least 5 (by Lemma 2). There are
at least as many edges as vertices in the subgraph of
the forest induced by the vertices used in the cycle
(by Lemma 3). That implies the existence of a cycle
in the original graph, contradicting that the original
graph was a tree.
Since there are no odd cycles in the crossings
graph, the crossings graph of edges is bipartite. Each
side of the bipartite graph can be assigned to a page,
such that no two edges on the same page cross.
Therefore, the original graph was 2-planar.
5.2 Linguistics: Cross-serial Verb
Constructions and Successive Cyclicity
Cross-serial verb constructions were used to provide
evidence for the ?non-context-freeness? of natural
language (Shieber, 1985). Cross-serial verb con-
structions with two verbs form 1-Endpoint-Crossing
trees. Below is a cross-serial sentence from Swiss-
German, from (1) in Shieber (1985):
das mer em Hans es huus h?lfed aastriiche
that we HansDAT the houseACC helped paint
The edges (that , helped), (helped ,we), and
(helped ,Hans) are each only crossed by an edge
with an endpoint at paint; the edge (paint , house)
is only crossed by edges with an endpoint at helped.
More generally, with a set of two cross serial verbs
in a subordinate clause, each verb should suffice as
the crossing point for all edges incident to the other
verb that are crossed.
Cross-serial constructions with three or more
verbs would have dependency trees that violate 1-
20
What did say BA C ... Z ate t ?nsaid 1 said 2t1 t2
Figure 8: An example of wh-movement over a poten-
tially unbounded number of clauses. The edges be-
tween the heads of each clause cross the edges from
trace to trace, but all obey 1-Endpoint-Crossing.
Endpoint-Crossing. Psycholinguistically, between
two and three verbs is exactly where there is a large
change in the sentence processing abilities of human
listeners (based on both grammatical judgments and
scores on a comprehension task) (Bach et al, 1986).
More speculatively, there may be a connection
between the form of 1-Endpoint-Crossing trees and
phases (roughly, propositional units such as clauses)
in Minimalism (Chomsky et al, 1998). Figure 8
shows an example of wh-movement over a poten-
tially unbounded number of clauses. The phase-
impenetrability condition (PIC) states that only the
head of the phase and elements that have moved to
its edge are accessible to the rest of the sentence
(Chomsky et al, 1998, p.22). Movement is there-
fore required to be successive cyclic, with a moved
element leaving a chain of traces at the edge of
each clause on its way to its final pronounced loca-
tion (Chomsky, 1981). In Figure 8, notice that the
crossing edges form a repeated pattern that obeys
the 1-Endpoint-Crossing property. More generally,
we suspect that trees satisfying the PIC will tend to
also be 1-Endpoint-Crossing. Furthermore, if the
traces were not at the edge of each clause, and in-
stead were positioned between a head and one of
its arguments, 1-Endpoint-Crossing would be vio-
lated. For example, if t2 in Figure 8 were be-
tween C and said2, then the edge (t1, t2) would cross
(say, said1), (said1, said2), and (C, said2), which
do not all share an endpoint. An exploration of these
linguistic connections may be an interesting avenue
for further research.
6 Conclusions
1-Endpoint-Crossing trees characterize over 95% of
structures found in natural language treebank, and
can be parsed in only a factor of n more time than
projective trees. The dynamic programming algo-
rithm for projective trees (Eisner, 2000) has been
extended to handle higher order factors (McDonald
and Pereira, 2006; Carreras, 2007; Koo and Collins,
2010), adding at most a factor of n to the edge-
based running time; it would be interesting to ex-
tend the algorithm presented here to include higher
order factors. 1-Endpoint-Crossing is a condition
on edges, while properties such as well-nestedness
or block degree are framed in terms of subtrees.
Three edges will always suffice as a certificate of a
1-Endpoint-Crossing violation (two vertex-disjoint
edges that both cross a third). In contrast, for a
property like ill-nestedness, two nodes might have
a least common ancestor arbitrarily far away, and so
one might need the entire graph to verify whether
the sub-trees rooted at those nodes are disjoint and
ill-nested. We have discussed cross-serial depen-
dencies; a further exploration of which linguistic
phenomena would and would not have 1-Endpoint-
Crossing dependency trees may be revealing.
Acknowledgments
We would like to thank Julie Legate for an in-
teresting discussion. This material is based upon
work supported under a National Science Foun-
dation Graduate Research Fellowship, NSF Award
CCF 1137084, and Army Research Office MURI
grant W911NF-07-1-0216.
A Dynamic Program to find the maximum
scoring 1-Endpoint-Crossing Tree
Input: Matrix S: S[i, j] is the score of the directed edge (i, j)
Output: Maximum score of a 1-Endpoint-Crossing tree over
vertices [0, n], rooted at 0
Init: ?i Int[i, i, F, F ] = Int[i, i+ 1, F, F ] = 0
Int[i, i, T, F ] = Int[i, i, F, T ] = Int[i, i, T, T ] = ??
Final: Int[0, n, F, T ]
Shorthand for booleans: T F (x, S) :=
if x = T , exactly one of the set S is true
if x = F , all of the set S must be false
bi, bj , bx are true iff the corresponding boundary point has its
incoming edge (parent) in that sub-problem. For the LR sub-
problem, bi and bj are always false, and so omitted. For all
sub-problems with the suffix AFromB, the boundary point A
has its parent edge in the sub-problem solution; the other two
boundary points do not. For example, L_XFromI would cor-
respond to having booleans bi = bj = F and bx = T , with the
restriction that x must be a descendant of i.
21
Int[i, j, F, bj ]? max?
??????????
??????????
Int[i+ 1, j, T, F ] if bj = F
S[i, j] + Int[i, j, F, F ] if bj = T
max
k?(i,j)
S[i, k]+
?
???????
???????
Int[i, k, F, F ] + Int[k, j, F, bj ]
max
T F (bj ,{bl,br})
LR[i, k, j, bl] + Int[k, j, F, br]
maxl?(k,j),T F (T,{bl,bm,br}){
R[i, k, l, F, F, bl] + Int[k, l, F, bm] + L[l, j, k, br, bj , F ]
LR[i, k, l, bl] + Int[k, l, F, bm] + Int[l, j, br, bj ]
maxl?(i,k),T F (T,{bl,bm,br}){
Int[i, l, F, bl] + L[l, k, i, bm, F, F ] +N [k, j, l, F, bj , br]
R[i, l, k, F, bl, F ] + Int[l, k, bm, F ] + L[k, j, l, F, bj , br]
Int[i, j, T, F ]? symmetric to Int[i, j, F, T ]
Int[i, j, T, T ]? ??
LR[i, j, x, bx]? max?
??
??
L[i, j, x, F, F, bx]
R[i, j, x, F, F, bx]
maxk?(i,j),T F (bx,{bxl,bxr}),T F (T,{bkl,bkr})
L[i, k, x, F, bkl, bxl] +R[k, j, x, bkr, F, bxr]
N [i, j, x, bi, bj , F ]? max?
???
???
Int[i, j, bi, bj ]
S[x, i] +N [i, j, x, F, bj , F ] if bi = T
S[x, j] +N [i, j, x, bi, F, F ] if bj = T
max
k?(i,j)
S[x, k] +N [i, k, x, bi, F, F ] + Int[k, j, F, bj ]
N [i, j, x, F, bj , T ]? max?
???????
???????
S[i, x] +N [i, j, x, F, bj , F ]
S[x, j] +N_XFromI[i, j, x] if bj = T
S[j, x] +N [i, j, x, F, F, F ] if bj = F
S[j, x] + Int[i, j, F, T ] if bj = T
max
k?(i,j)
S[x, k] +N_XFromI[i, k, x] + Int[k, j, F, bj ]
max
k?(i,j)
S[k, x]+
{
Int[i, k, F, T ] + Int[k, j, F, bj ]
N [i, k, x, F, F, F ] + Int[k, j, T, bj ]
N [i, j, x, T, F, T ]? symmetric to N [i, j, x, F, T, T ]
N [i, j, x, T, T, T ]? ??
N_XFromI[i, j, x]? max?
??
??
S[i, x] +N [i, j, x, F, F, F ]
maxk?(i,j){
S[x, k] +N_XFromI[i, k, x] + Int[k, j, F, F ]
S[k, x] + Int[i, k, F, T ] + Int[k, j, F, F ]
N_IFromX[i, j, x]? max{
S[x, i] +N [i, j, x, F, F, F ]
max
k?(i,j)
S[x, k] +N [i, k, x, T, F, F ] + Int[k, j, F, F ]
N_XFromJ [i, j, x]? symmetric to N_XFromI[i, j, x]
N_JFromX[i, j, x]? symmetric to N_IFromX[i, j, x]
L[i, j, x, bi, bj , F ]? max?
?????
?????
Int[i, j, bi, bj ]
S[x, i] + L[i, j, x, F, bj , F ] if bi = T
S[x, j] + L[i, j, x, bi, F, F ] if bj = T
max
k?(i,j),T F (bi,{bl,br})
S[x, k]+
{
L[i, k, x, bl, F, F ] +N [k, j, i, F, bj , br]
Int[i, k, bl, F ] + L[k, j, i, F, bj , br]
L[i, j, x, F, bj , T ]? max?
?????????
?????????
S[i, x] + L[i, j, x, F, bj , F ]
S[x, j] + L_XFromI[i, j, x] if bj = T
S[j, x] + L[i, j, x, F, F, F ] if bj = F
S[j, x] + L_JFromI[i, j, x] if bj = T
max
k?(i,j)
S[x, k] + L_XFromI[i, k, x] +N [k, j, i, F, bj , F ]
max
k?(i,j)
S[k, x]+
?
??
??
L_JFromI[i, k, x] +N [k, j, i, F, bj , F ]
L[i, k, x, F, F, F ] +N [k, j, i, T, bj , F ]
max
T F (T,{bl,br})
Int[i, k, F, bl] + L[k, j, i, br, bj , F ]
L[i, j, x, T, bj , T ]? not reachable
L_XFromI[i, j, x]? max?
??????
??????
S[i, x] + L[i, j, x, F, F, F ]
max
k?(i,j)
S[x, k] + L_XFromI[i, k, x] +N [k, j, i, F, F, F ]
max
k?(i,j)
S[k, x]+
?
??
??
L_JFromI[i, k, x] +N [k, j, i, F, F, F ]
L[i, k, x, F, F, F ] +N_IFromX[k, j, i]
Int[i, k, F, T ] + L[k, j, i, F, F, F ]
Int[i, k, F, F ] + L_IFromX[k, j, i]
L_IFromX[i, j, x]? max?
?????
?????
S[x, i] + L[i, j, x, F, F, F ]
max
k?(i,j)
S[x, k]+
?
??
??
L[i, k, x, T, F, F ] +N [k, j, i, F, F, F ]
L[i, k, x, F, F, F ] +N_XFromI[k, j, i]
Int[i, k, T, F ] + L[k, j, i, F, F, F ]
Int[i, k, F, F ] + L_XFromI[k, j, i]
L_JFromX[i, j, x]? max?
???
???
S[x, j] + L[i, j, x, F, F, F ]
max
k?(i,j)
S[x, k]+
{
L[i, k, x, F, F, F ] + Int[k, j, F, T ]
Int[i, k, F, F ] + L_JFromI[k, j, i]
L_JFromI[i, j, x]? max?
???
???
Int[i, j, F, T ]
max
k?(i,j)
S[x, k]+
{
L[i, k, x, F, F, F ] +N_JFromX[k, j, i]
Int[i, k, F, F ] + L_JFromX[k, j, i]
22
R[i, j, x, bi, bj , F ]? symmetric to L[i, j, x, bi, bj , F ]
R[i, j, x, bi, F, T ]? symmetric to L[i, j, x, F, bj , T ]
R[i, j, x, bi, T, T ]? not reachable
R_XFromJ [i, j, x]? symmetric to L_XFromI[i, j, x]
R_JFromX[i, j, x]? symmetric to L_IFromX[i, j, x]
R_IFromX[i, j, x]? symmetric to L_JFromX[i, j, x]
R_IFromJ [i, j, x]? symmetric to L_JFromI[i, j, x]
References
E. Bach, C. Brown, and W. Marslen-Wilson. 1986.
Crossed and nested dependencies in german and dutch:
A psycholinguistic study. Language and Cognitive
Processes, 1(4):249?262.
F. Bernhart and P.C. Kainen. 1979. The book thickness
of a graph. Journal of Combinatorial Theory, Series
B, 27(3):320 ? 331.
M. Bodirsky, M. Kuhlmann, and M. M?hl. 2005. Well-
nested drawings as models of syntactic structure. In
In Tenth Conference on Formal Grammar and Ninth
Meeting on Mathematics of Language, pages 88?1.
University Press.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957?961.
N. Chomsky, Massachusetts Institute of Technology.
Dept. of Linguistics, and Philosophy. 1998. Minimal-
ist inquiries: the framework. MIT occasional papers
in linguistics. Distributed by MIT Working Papers in
Linguistics, MIT, Dept. of Linguistics.
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
F. Chung, F. Leighton, and A. Rosenberg. 1987. Em-
bedding graphs in books: A layout problem with ap-
plications to VLSI design. SIAM Journal on Algebraic
Discrete Methods, 8(1):33?58.
H. Cui, R. Sun, K. Li, M.Y. Kan, and T.S. Chua. 2005.
Question answering passage retrieval using depen-
dency relations. In Proceedings of the 28th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 400?407.
ACM.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 423. Association for Compu-
tational Linguistics.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
541?548. Association for Computational Linguistics.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers, October.
S. Even and A. Itai. 1971. Queues, stacks, and graphs.
In Proc. International Symp. on Theory of Machines
and Computations, pages 71?86.
C. G?mez-Rodr?guez and J. Nivre. 2010. A transition-
based parser for 2-planar dependency structures. In
Proceedings of ACL, pages 1492?1501.
C. G?mez-Rodr?guez, J. Carroll, and D. Weir. 2011. De-
pendency parsing schemata and mildly non-projective
dependency parsing. Computational Linguistics,
37(3):541?586.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of ACL, pages 1?11.
M. Kuhlmann. 2013. Mildly non-projective dependency
grammar. Computational Linguistics, 39(2).
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 121?132.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, pages 523?530. As-
sociation for Computational Linguistics.
E. Pitler, S. Kannan, and M. Marcus. 2012. Dynamic
programming for higher order parsing of gap-minding
trees. In Proceedings of EMNLP, pages 478?488.
L.A. Ringenberg. 1967. College geometry. Wiley.
A. Rush and S. Petrov. 2012. Vine pruning for effi-
cient multi-pass dependency parsing. In Proceedings
of NAACL, pages 498?507.
S.M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Pro-
ceedings of EMNLP, pages 320?331.
23
24
