Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 473?480
Manchester, August 2008
Reading the Markets:
Forecasting Public Opinion of Political Candidates by News Analysis
Kevin Lerman
Dept. of Computer Science
Columbia University
New York, NY USA
klerman@cs.columbia.edu
Ari Gilder and Mark Dredze
Dept. of CIS
University of Pennsylvania
Philadelphia, PA USA
agilder@alumni.upenn.edu
mdredze@cis.upenn.edu
Fernando Pereira
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA USA
pereira@google.com
Abstract
Media reporting shapes public opinion
which can in turn influence events, partic-
ularly in political elections, in which can-
didates both respond to and shape public
perception of their campaigns. We use
computational linguistics to automatically
predict the impact of news on public per-
ception of political candidates. Our sys-
tem uses daily newspaper articles to pre-
dict shifts in public opinion as reflected
in prediction markets. We discuss various
types of features designed for this problem.
The news system improves market predic-
tion over baseline market systems.
1 Introduction
The mass media can affect world events by sway-
ing public opinion, officials and decision makers.
Financial investors who evaluate the economic per-
formance of a company can be swayed by positive
and negative perceptions about the company in the
media, directly impacting its economic position.
The same is true of politics, where a candidate?s
performance is impacted by media influenced pub-
lic perception. Computational linguistics can dis-
cover such signals in the news. For example, De-
vitt and Ahmad (2007) gave a computable metric
of polarity in financial news text consistent with
human judgments. Koppel and Shtrimberg (2004)
used a daily news analysis to predict financial mar-
ket performance, though predictions could not be
used for future investment decisions. Recently,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
a study conducted of the 2007 French presiden-
tial election showed a correlation between the fre-
quency of a candidate?s name in the news and elec-
toral success (V?eronis, 2007).
This work forecasts day-to-day changes in pub-
lic perception of political candidates from daily
news. Measuring daily public perception with
polls is problematic since they are conducted by a
variety of organizations at different intervals and
are not easily comparable. Instead, we rely on
daily measurements from prediction markets.
We present a computational system that uses
both external linguistic information and internal
market indicators to forecast public opinion mea-
sured by prediction markets. We use features from
syntactic dependency parses of the news and a
user-defined set of market entities. Successive
news days are compared to determine the novel
component of each day?s news resulting in features
for a machine learning system. A combination sys-
tem uses this information as well as predictions
from internal market forces to model prediction
markets better than several baselines. Results show
that news articles can be mined to predict changes
in public opinion.
Opinion forecasting differs from that of opin-
ion analysis, such as extracting opinions, evaluat-
ing sentiment, and extracting predictions (Kim and
Hovy, 2007). Contrary to these tasks, our system
receives objective news, not subjective opinions,
and learns what events will impact public opinion.
For example, ?oil prices rose? is a fact but will
likely shape opinions. This work analyzes news
(cause) to predict future opinions (effect). This af-
fects the structure of our task: we consider a time-
series setting since we must use past data to predict
future opinions, rather than analyzing opinions in
batch across the whole dataset.
473
We begin with an introduction to prediction
markets. Several methods for new feature extrac-
tion are explored as well as market history base-
lines. Systems are evaluated on prediction markets
from the 2004 US Presidential election. We close
with a discussion of related and future work.
2 Prediction Markets
Prediction markets, such as TradeSports and the
Iowa Electronic Markets
1
, provide a setting sim-
ilar to financial markets wherein shares represent
not companies or commodities, but an outcome
of a sporting, financial or political event. For ex-
ample, during the 2004 US Presidential election,
one could purchase a share of ?George W. Bush
to win the 2004 US Presidential election? or ?John
Kerry to win the 2004 US Presidential election.?
A pay-out of $1 is awarded to winning sharehold-
ers at market?s end, e.g. Bush wins the election.
In the interim, price fluctuations driven by supply
and demand indicate the perception of the event?s
likelihood, which indicates public opinion of an
event. Several studies show the accuracy of predic-
tion markets in predicting future events (Wolfers
and Zitzewitz, 2004; Servan-Schreiber et al, 2004;
Pennock et al, 2000), such as the success of up-
coming movies (Jank and Foutz, 2007), political
stock markets (Forsythe et al, 1999) and sports
betting markets (Williams, 1999).
Market investors rely on daily news reports to
dictate investment actions. If something positive
happens for Bush (e.g. Saddam Hussein is cap-
tured), Bush will appear more likely to win, so
demand increases for ?Bush to win? shares, and
the price rises. Likewise, if something negative for
Bush occurs (e.g. casualties in Iraq increase), peo-
ple will think he is less likely to win, sell their
shares, and the price drops. Therefore, predic-
tion markets can be seen as rapid response indi-
cators of public mood concerning political candi-
dates. Market-internal factors, such as general in-
vestor mood and market history, also affect price.
For instance, a positive news story for a candidate
may have less impact if investors dislike the can-
didate. Explaining market behavior requires mod-
eling news information external to the market and
internal trends to the market.
This work uses the 2004 US Presidential elec-
tion markets from Iowa Electronic Markets. Each
market provides a daily average price, which indi-
1
www.tradesports.com, www.biz.uiowa.edu/iem/
cates the overall market sentiment for a candidate
on a given day. The goal of the prediction system
is to predict the price direction for the next day (up
or down) given all available information up to the
current day: previous days? market pricing/volume
information and the morning news. Market his-
tory represents information internal to the market:
if an investor has no knowledge of external events,
what is the most likely direction for the market?
This information can capture general trends and
volatility of the market. The daily news is the ex-
ternal information that influences the market. This
provides information, independent of any internal
market effects to which investors will respond. A
learning system for each information source is de-
veloped and combined to explain market behavior.
The following sections describe these systems.
3 External Information: News
Changes in market price are likely responses to
current events reported in the news. Investors read
the morning paper and act based on perceptions of
events. Can a system with access to this same in-
formation make good investment decisions?
Our system operates in an iterative (online) fash-
ion. On each day (round) the news for that day is
used to construct a new instance. A logistic re-
gression classifier is trained on all previous days
and the resulting classifier predicts the price move-
ment of the new instance. The system either prof-
its or loses money according to this prediction. It
then receives the actual price movement and labels
the instance accordingly (up or down). This set-
ting is straightforward; the difficulty is in choosing
a good feature representation for the classifier. We
now explore several representation techniques.
3.1 Bag-of-Words Features
The prediction task can be treated as a document
classification problem, where the document is the
day?s news and the label is the direction of the mar-
ket. Document classification systems typically rely
on bag-of-words features, where each feature indi-
cates the number of occurrences of a word in the
document. The news for a given day is represented
by a normalized unit length vector of counts, ex-
cluding common stop words and features that oc-
cur fewer than 20 times in our corpus.
474
3.2 News Focus Features
Simple bag-of-words features may not capture rel-
evant news information. Public opinion is influ-
enced by new events ? a change in focus. The day
after a debate, most papers may declare Bush the
winner, yielding a rise in the price of a ?Bush to
win? share. However, while the debate may be
discussed for several days after the event, public
opinion of Bush will probably not continue to rise
on old news. Changes in public opinion should
reflect changes in daily news coverage. Instead of
constructing features for a single day, they can rep-
resent differences between two days of news cov-
erage, i.e. the novelty of the coverage. Given the
counts of feature i on day t as c
t
i
, where feature i
may be the unigram ?scandal,? and the set of fea-
tures on day t as C
t
, the fraction of news focus for
each feature is f
t
i
=
c
t
i
|C
t
|
. The news focus change
(?) for feature i on day t is defined as,
?f
t
i
= log
(
f
t
i
1
3
(f
t?1
i
+ f
t?2
i
+ f
t?3
i
)
)
, (1)
where the numerator is the focus of news on fea-
ture i today and the denominator is the average
focus over the previous three days. The resulting
value captures the change in focus on day t, where
a value greater than 0 means increased focus and a
value less than 0 decreased focus. Feature counts
were smoothed by adding a constant (10).
3.3 Entity Features
As shown by Wiebe et al (2005), it is important to
know not only what is being said but about whom it
is said. The term ?victorious? by itself is meaning-
less when discussing an election ? meaning comes
from the subject. Similarly, the word ?scandal?
is bad for a candidate but good for the opponent.
Subjects can often be determined by proximity. If
the word ?scandal? and Bush are mentioned in the
same sentence, this is likely to be bad for Bush. A
small set of entities relevant to a market can be de-
fined a priori to give context to features. For exam-
ple, the entities ?Bush,? ?Kerry? and ?Iraq? were
known to be relevant before the general election.
Kim and Hovy (2007) make a similar assumption.
News is filtered for sentences that mention ex-
actly one of these entities. Such sentences are
likely about that entity, and the extracted features
are conjunctions of the word and the entity. For ex-
ample, the sentence ?Bush is facing another scan-
dal? produces the feature ?bush-scandal? instead
of just ?scandal.?
2
Context disambiguation comes
at a high cost: about 70% of all sentences do not
contain any predefined entities and about 7% con-
tain more than one entity. These likely relevant
sentences are unfortunately discarded, although
future work could reduce the number of discarded
sentences using coreference resolution.
3.4 Dependency Features
While entity features are helpful they cannot pro-
cess multiple entity sentences, nearly a quarter of
the entity sentences. These sentences may be the
most helpful since they indicate entity interactions.
Consider the following three example sentences:
? Bush defeated Kerry in the debate.
? Kerry defeated Bush in the debate.
? Kerry, a senator from Massachusetts, de-
feated President Bush in last night?s debate.
Obviously, the first two sentences have very dif-
ferent meanings for each candidate?s campaign.
However, representations considered so far do not
differentiate between these sentences, nor would
any heuristic using proximity to an entity.
3
Effec-
tive features rely on the proper identification of the
subject and object of ?defeated.? Longer n-grams,
which would be very sparse, would succeed for the
first two sentences but not the third.
To capture these interactions, features were ex-
tracted from dependency parses of the news ar-
ticles. Sentences were part of speech tagged
(Toutanova et al, 2003), parsed with a depen-
dency parser and labeled with grammatical func-
tion labels (McDonald et al, 2006). The result-
ing parses encode dependencies for each sentence,
where word relationships are expressed as parent-
child links. The parse for the third sentence above
indicates that ?Kerry? is the subject of ?defeated,?
and ?Bush? is the object. Features are extracted
from parse trees containing the pre-defined enti-
ties (section 3.3), using the parent, grandparent,
aunts, nieces, children, and siblings of any in-
stances of the pre-defined entities we observe. Fea-
tures are conjoined indicators of the node?s lexical
entry, part of speech tag and dependency relation
2
Other methods can identify the subject of sentiment ex-
pressions, but our text is objective news. Therefore, we em-
ploy this approximate method.
3
Several failed heuristics were tried, such as associating
each word to an entity within a fixed window in the sentence
or the closer entity if two were in the window.
475
Feature Good For
Kerry? plan? the Kerry
poll? showed? Bush Bush
won? Kerry
4
Kerry
agenda? ?s? Bush Kerry
Kerry? spokesperson? campaign Bush
Table 1: Simplified examples of features from the
general election market. Arrows point from parent
to child. Features also include the word?s depen-
dency relation labels and parts of speech.
label. For aunts, nieces, and children, the com-
mon ancestor is used, and in the case of grand-
parent, the intervening parent is included. Each
of these conjunctions includes the discovered en-
tity and back-off features are included by remov-
ing some of the other information. Note that be-
sides extracting more precise information from the
news text, this handles sentences with multiple en-
tities, since it associates parts of a sentence with
different entities. In practice, we use this in con-
junction with News Focus. Useful features from
the general election market are in table 1. Note
that they capture events and not opinions. For ex-
ample, the last feature indicates that a statement by
the Kerry campaign was good for Bush, possibly
because Kerry was reacting to criticism.
4 Internal Information: Market History
News cannot explain all market trends. Momen-
tum in the market, market inefficiencies, and slow
news days can affect share price. A candidate who
does well will likely continue to do well unless
new events occur. Learning general market behav-
ior can help explain these price movements.
For each day t, we create an instance using fea-
tures for the price and volume at day t ? 1 and
the price and volume change between days t ? 1
and t ? 2. We train using a ridge regression
5
on
all previous days (labeled with their actual price
movements) to forecast the movement for day t,
which we convert into a binary value: up or down.
4
This feature matches phrases like ?Kerry won [the de-
bate]? and ?[something] won Kerry [support]?
5
This outperformed more sophisticated algorithms, in-
cluding the logistic regression used earlier. This may be due
to the fact that many market history features (e.g. previous
price movements) are very similar in nature to the future price
movements being predicted.
5 Combined System
Since both news and internal market information
are important for modeling market behavior, each
one cannot be evaluated in isolation. For example,
a successful news system may learn to spot impor-
tant events for a candidate, but cannot explain the
price movements of a slow news day. A combina-
tion of the market history system and news features
is needed to model the markets.
Expert algorithms for combining prediction sys-
tems have been well studied. However, experi-
ments with the popular weighted majority algo-
rithm (Littlestone and Warmuth, 1989) yielded
poor performance since it attempts to learn the
optimal balance between systems while our set-
ting has rapidly shifting quality between few ex-
perts with little data for learning. Instead, a sim-
ple heuristic was used to select the best perform-
ing predictor on each day. We compare the 3-
day prediction accuracy (measured in total earn-
ings) for each system (news and market history)
to determine the current best system. The use of
a small window allows rapid change in systems.
When neither system has a better 3-day accuracy
the combined system will only predict if the two
systems agree and abstain otherwise. This strategy
measures how accurately a news system can ac-
count for price movements when non-news move-
ments are accounted for by market history. The
combined system improved over individual evalu-
ations of each system on every market
6
.
6 Evaluation
Daily pricing information was obtained from the
Iowa Electronic Markets for the 2004 US Presi-
dential election for six Democratic primary con-
tenders (Clark, Clinton, Dean, Gephardt, Kerry
and Lieberman) and two general election candi-
dates (Bush and Kerry). Market length varied as
some candidates entered the race later than others:
the DNC markets for Clinton, Gephardt, Kerry,
and Lieberman were each 332 days long, while
Dean?s was 130 days and Clark?s 106. The general
election market for Bush was 153 days long, while
Kerry?s was 142.
7
The price delta for each day
was taken as the difference between the average
6
This outperformed a single model built over all features,
perhaps due to the differing natures of the feature types we
used.
7
The first 11 days of the Kerry general election market
were removed due to strange price fluctuations in the data.
476
price between the previous and current day. Mar-
ket data also included the daily volume that was
used as a market history feature. Entities selected
for each market were the names of all candidates
involved in the election and ?Iraq.?
News articles covering the election were ob-
tained from Factiva
8
, an online news archive run
by Dow Jones. Since the system must make a pre-
diction at the beginning of each day, only articles
from daily newspapers released early in the morn-
ing were included. The corpus contained approxi-
mately 50 articles per day over a span of 3 months
to almost a year, depending on the market.
9
While most classification systems are evaluated
by measuring their accuracy on cross-validation
experiments, both the method and the metric are
unsuitable to our task. A decision for a given day
must be made with knowledge of only the previ-
ous days, ruling out cross validation. In fact, we
observed improved results when the system was
allowed access to future articles through cross-
validation. Further, raw prediction accuracy is not
a suitable metric for evaluation because it ignores
the magnitude in price shifts each day. A sys-
tem should be rewarded proportional to the signif-
icance of the day?s market change.
To address these issues we used a chronological
evaluation where systems were rewarded for cor-
rect predictions in proportion to the magnitude of
that day?s shift, i.e. the ability to profit from the
market. This metric is analogous to weighted accu-
racy. On each day, the system is provided with all
available morning news and market history from
which an instance is created using one of the fea-
ture schemes described above. We then predict
whether the market price will rise or fall and the
system either earns or loses the price change for
that day if it was right or wrong respectively. The
system then learns the correct price movement and
the process is repeated for the next day.
10
Sys-
tems that correctly forecast public opinions from
the news will make more money. In economic
terms, this is equivalent to buying or short-selling a
single share of the market and then selling or cov-
ering the short at the end of the day.
11
Scores were
8
http://www.factiva.com/
9
While 50 articles may not seem like much, humans read
far less text before making investment decisions.
10
This scheme is called ?online learning? for which a
whole class of algorithms apply. We used batch algorithms
since training happens only once per day.
11
More complex investment schemes are possible than
what has been described here. We choose a simple scheme
Market History Baseline
DNC Clark 20 13
Clinton 38 -8
Dean 23 24
Gephardt 8 1
Kerry -6 6
Lieberman 3 2
General Kerry 2 15
Bush 21 20
Average (% omniscience) 13.6 9.1
Table 2: Results using history features for predic-
tion compared with a baseline system that invests
according to the previous day?s result.
normalized for comparison across markets using
the maximum profit obtainable by an omniscient
system that always predicts correctly.
Baseline systems for both news and market his-
tory are included. The news baseline follows the
spirit of a study of the French presidential elec-
tion (V?eronis, 2007), which showed that candidate
mentions correlate to electoral success. Attempts
to follow this method directly ? predicting mar-
ket movement based on raw candidate mentions ?
did very poorly. Instead, we trained our learning
system with features representing daily mention
counts of each entity. For a market history base-
line, we make a simple assumption about market
behavior: the current market trend will continue,
predict today?s behavior for tomorrow.
There were too many features to learn in the
short duration of the markets so only features that
appeared at least 20 times were included, reduc-
ing bag-of-words features from 88.8k to 28.3k and
parsing features from 1150k to 15.9k. A real world
system could use online feature selection.
6.1 Results
First, we establish performance without news in-
formation by testing the market history system
alone. Table 2 shows the profit of the history pre-
diction and baseline systems. While learning beats
the rule based system on average, both earn im-
pressive profits considering that random trading
would break even. These results corroborate the
inefficient market observation of Pennock et al
(2000). Additionally, the general election markets
sometimes both increased or decreased, an impos-
sible result in an efficient zero-sum market.
to make the evaluation more transparent.
477
Figure 1: Results for the different news features and combined system across five markets. Bottom
bars can be compared to evaluate news components and combined with the stacked black bars (history
system) give combined performance. The average performance (far right) shows improved performance
from each news system over the market history system.
During initial news evaluations with the com-
bined system, the primary election markets did ei-
ther very poorly or quite well. The news predic-
tion component lost money for Clinton, Gephardt,
and Lieberman while Clark, Dean and Kerry all
made money. Readers familiar with the 2004 elec-
tion will immediately see the difference between
the groups. The first three candidates were minor
contenders for the nomination and were not news-
makers. Hillary Clinton never even declared her
candidacy. The average number of mentions per
day for these candidates in our data was 20. In con-
trast, the second group were all major contenders
for the nomination and an average mention of 94 in
our data. Clearly, the news system can only do well
when it observes news that effects the market. The
system does well on both general election markets
where the average candidate mention per day was
503. Since the Clinton, Gephardt and Lieberman
campaigns were not newsworthy, they are omitted
from the results.
Results for news based prediction systems are
shown in figure 1. The figure shows the profit
made from both news features (bottom bars) and
market history (top black bars) when evaluated as
a combined system. Bottom bars can be compared
to evaluate news systems and each is combined
with its top bar to indicate total performance. Neg-
ative bars indicate negative earnings (i.e. weighted
accuracy below 50%). Averages across all mar-
kets for the news systems and the market history
system are shown on the right. In each market,
the baseline news system makes a small profit, but
the overall performance of the combined system is
worse than the market history system alone, show-
ing that the news baseline is ineffective. However,
all news features improve over the market history
system; news information helps to explain market
behaviors. Additionally, each more advanced set
of news features improves, with dependency fea-
tures yielding the best system in a majority of mar-
kets. The dependency system was able to learn
more complex interactions between words in news
articles. As an example, the system learns that
when Kerry is the subject of ?accused? his price in-
creases but decreased when he is the object. Sim-
ilarly, when ?Bush? is the subject of ?plans? (i.e.
Bush is making plans), his price increased. But
when he appears as a modifier of the plural noun
?plans? (comments about Bush policies), his price
falls. Earning profit indicates that our systems
were able to correctly forecast changes in public
opinion from objective news text.
The combined system proved an effective way
of modeling the market with both information
sources. Figure 2 shows the profits of the depen-
dency news system, the market history system, and
the combined system?s profits and decision on two
segments from the Kerry DNC market. In the first
segment, the history system predicts a downward
trend in the market (increasing profit) and the sec-
ond segment shows the final days of the market,
where Kerry was winning primaries and the news
system correctly predicted a market increase.
V?eronis (2007) observed a connection between
electoral success and candidate mentions in news
media. The average daily mentions in the general
election was 520 for Bush (election winner) and
478
485 for Kerry. However, for the three major DNC
candidates, Dean had 183, Clark 56 and Kerry
(election winner) had the least at 43. Most Kerry
articles occurred towards the end of the race when
it was clear he would win, while early articles fo-
cused on the early leader Dean. Also, news activity
did not indicate market movement direction; me-
dian candidate mentions for a positive market day
was 210 and 192 for a negative day.
Dependency news system accuracy was corre-
lated with news activity. On days when the news
component was correct ? although not always cho-
sen ? there were 226 median candidate mentions
compared to 156 for incorrect days. Additionally,
the system was more successful at predicting neg-
ative days. While days for which it was incorrect
the market moved up or down equally, when it was
correct and selected it predicted buy 42% of the
time and sell 58%, indicating that the system bet-
ter tracked negative news impacts.
7 Related Work
Many studies have examined the effects of news on
financial markets. Koppel and Shtrimberg (2004)
found a low correlation between news and the
stock market, likely because of the extreme effi-
ciency of the stock market (Gid?ofalvi, 2001). Two
studies reported success but worked with a very
small time granularity (10 minutes) (Lavrenko et
al., 2000; Mittermayer and Knolmayer, 2006). It
appears that neither system accounts for the time-
series nature of news during learning, instead us-
ing cross-validation experiments which is unsuit-
able for evaluation of time-series data. Our own
preliminary cross-validation experiments yielded
much better results than chronological evaluation
since the system trains using future information,
and with much more training data than is actu-
ally available for most days. Recent work has ex-
amined prediction market behavior and underlying
principles (Serrano-Padial, 2007).
12
Pennock et
al. (2000) found that prediction markets are some-
what efficient and some have theorized that news
could predict these markets, which we have con-
firmed (Debnath et al, 2003; Pennock et al, 2001;
Servan-Schreiber et al, 2004).
Others have explored the concurrent modeling
of text corpora and time series, such as using stock
market data and language modeling to identify
12
For a sample of the literature on prediction markets, see
the proceedings of the recent Prediction Market workshops
(http://betforgood.com/events/pm2007/index.html).
Figure 2: Two selections from the Kerry DNC mar-
ket showing profits over time (days) for depen-
dency news, history and combined systems. Each
day?s chosen system is indicated by the bottom
stripe as red (upper) for news, blue (lower) for his-
tory, and black for ties.
influential news stories (Lavrenko et al, 2000).
Hurst and Nigam (2004) combined syntactic and
semantic information for text polarity extraction.
Our task is related to but distinct from sentiment
analysis, which focuses on judgments in opin-
ions and, recently, predictions given by opinions.
Specifically, Kim and Hovy (2007) identify which
political candidate is predicted to win by an opin-
ion posted on a message board and aggregate opin-
ions to correctly predict an election result. While
the domain and some techniques are similar to our
own, we deal with fundamentally different prob-
lems. We do not consider opinions but instead ana-
lyze objective news to learn events that will impact
opinions. Opinions express subjective statements
about elections whereas news reports events. We
use public opinion as a measure of an events im-
pact. Additionally, they use generalized features
similar to our own identification of entities by re-
placing (a larger set of) known entities with gen-
eralized terms. In contrast, we use syntactic struc-
tures to create generalized ngram features. Note
that our features (table 1) do not indicate opinions
in contrast to the Kim and Hovy features. Finally,
Kim and Hovy had a batch setting to predict elec-
tion winners while we have a time-series setting
that tracked daily public opinion of candidates.
8 Conclusion and Future Work
We have presented a system for forecasting public
opinion about political candidates using news me-
479
dia. Our results indicate that computational sys-
tems can process media reports and learn which
events impact political candidates. Additionally,
the system does better when the candidate appears
more frequently and for negative events. A news
source analysis could reveal which outlets most in-
fluence public opinion. A feature analysis could
reveal which events trigger public reactions. While
these results and analyses have significance for po-
litical analysis they could extend to other genres,
such as financial markets. We have shown that fea-
ture extraction using syntactic parses can general-
ize typical bag-of-word features and improve per-
formance, a non-trivial result as dependency parses
contain significant errors and can limit the selec-
tion of words. Also, combining the internal mar-
ket baseline with a news system improved perfor-
mance, suggesting that forecasting future public
opinions requires a combination of new informa-
tion and continuing trends, neither of which can be
captured by the other.
References
Debnath, S., D. M. Pennock, C. L. Giles, and
S. Lawrence. 2003. Information incorporation in
online in-game sports betting markets. In Electronic
Commerce.
Devitt, Ann and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Association for Computational
Linguistics (ACL).
Forsythe, R., T.A. Rietz, , and T.W. Ross. 1999.
Wishes, expectations, and actions: A survey on price
formation in election stock markets. Journal of Eco-
nomic Behavior and Organization, 39:83?110.
Gid?ofalvi, G. 2001. Using news articles to predict
stock price movements. Technical report, Univ. of
California San Diego, San Diego.
Hurst, Matthew and Kamal Nigam. 2004. Retrieving
topical sentiments from online document collections.
In Document Recognition and Retrieval XI.
Jank, Wolfgang and Natasha Foutz. 2007. Using vir-
tual stock exchanges to forecast box-office revenue
via functional shape analysis. In The Prediction
Markets Workshop at Electronic Commerce.
Kim, Soo-Min and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In Empirical
Methods in Natural Language Processing (EMNLP).
Koppel, M. and I. Shtrimberg. 2004. Good news or
bad news? let the market decide. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Lavrenko, V., M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000. Mining of concur-
rent text and time series. In KDD.
Littlestone, Nick and Manfred K. Warmuth. 1989. The
weighted majority algorithm. In IEEE Symposium
on Foundations of Computer Science.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency parsing with a two-stage dis-
criminative parser. In Conference on Natural Lan-
guage Learning (CoNLL).
Mittermayer, M. and G. Knolmayer. 2006. News-
CATS: A news categorization and trading system. In
International Conference in Data Mining.
Pennock, D. M., S. Lawrence, C. L. Giles, and F. A.
Nielsen. 2000. The power of play: Efficiency and
forecast accuracy in web market games. Technical
Report 2000-168, NEC Research Institute.
Pennock, D. M., S. Lawrence, F. A. Nielsen, and C. L.
Giles. 2001. Extracting collective probabilistic fore-
casts from web games. In KDD.
Serrano-Padial, Ricardo. 2007. Strategic foundations
of prediction markets and the efficient markets hy-
pothesis. In The Prediction Markets Workshop at
Electronic Commerce.
Servan-Schreiber, E., J. Wolfers, D. M. Pennock, and
B. Galebach. 2004. Prediction markets: Does
money matter? Electronic Markets, 14.
Toutanova, K., D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL.
V?eronis, Jean. 2007. La presse a fait mieux que les
sondeurs. http://aixtal.blogspot.com/2007/04/2007-
la-presse-fait-mieux-que-les.html.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. LREC, 39:165?210.
Williams, L.V. 1999. Information efficiency in betting
markets: A survey. Bulletin of Economic Research,
51:1?30.
Wolfers, J. and E. Zitzewitz. 2004. Prediction markets.
Journal of Economic Perspectives, 18(2):107?126.
480
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1051?1055,
Prague, June 2007. c?2007 Association for Computational Linguistics
Frustratingly Hard Domain Adaptation for Dependency Parsing
Mark Dredze1 and John Blitzer1 and Partha Pratim Talukdar1 and
Kuzman Ganchev1 and Joa?o V. Grac?a2 and Fernando Pereira1
1CIS Dept., University of Pennsylvania, Philadelphia, PA 19104
{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu
2L2F ? INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugal
javg@l2f.inesc-id.pt
Abstract
We describe some challenges of adaptation
in the 2007 CoNLL Shared Task on Domain
Adaptation. Our error analysis for this task
suggests that a primary source of error is
differences in annotation guidelines between
treebanks. Our suspicions are supported by
the observation that no team was able to im-
prove target domain performance substan-
tially over a state of the art baseline.
1 Introduction
Dependency parsing, an important NLP task, can be
done with high levels of accuracy. However, adapt-
ing parsers to new domains without target domain
labeled training data remains an open problem. This
paper outlines our participation in the 2007 CoNLL
Shared Task on Domain Adaptation (Nivre et al,
2007). The goal was to adapt a parser trained on
a single source domain to a new target domain us-
ing only unlabeled data. We were given around
15K sentences of labeled text from the Wall Street
Journal (WSJ) (Marcus et al, 1993; Johansson and
Nugues, 2007) as well as 200K unlabeled sentences.
The development data was 200 sentences of labeled
biomedical oncology text (BIO, the ONCO portion
of the Penn Biomedical Treebank), as well as 200K
unlabeled sentences (Kulick et al, 2004). The two
test domains were a collection of medline chem-
istry abstracts (pchem, the CYP portion of the Penn
Biomedical Treebank) and the Child Language Data
Exchange System corpus (CHILDES) (MacWhin-
ney, 2000; Brown, 1973). We used the second or-
der two stage parser and edge labeler of McDonald
et al (2006), which achieved top results in the 2006
CoNLL-X shared task. Preliminary experiments in-
dicated that the edge labeler was fairly robust to do-
main adaptation, lowering accuracy by 3% in the de-
velopment domain as opposed to 2% in the source,
so we focused on unlabeled dependency parsing.
Our system did well, officially coming in 3rd
place out of 12 teams and within 1% of the top sys-
tem (Table 1). 1 In unlabeled parsing, we scored
1st and 2nd on CHILDES and pchem respectively.
However, our results were obtained without adap-
tation. Given our position in the ranking, this sug-
gests that no team was able to significantly improve
performance on either test domain beyond that of a
state-of-the-art parser.
After much effort in developing adaptation meth-
ods, it is critical to understand the causes of these
negative results. In what follows, we provide an er-
ror analysis that attributes domain loss for this task
to a difference in annotation guidelines between do-
mains. We then overview our attempts to improve
adaptation. While we were able to show limited
adaptation on reduced training data or with first-
order features, no modifications improved parsing
with all the training data and second-order features.
2 Parsing Challenges
We begin with an error analysis for adaptation be-
tween WSJ and BIO. We divided the available WSJ
data into a train and test set, trained a parser on
the train set and compared errors on the test set
and BIO. Accuracy dropped from 90% on WSJ to
84% on BIO. We then computed the fraction of er-
rors involving each POS tag. For the most common
1While only 8 teams participated in the closed track with us,
our score beat all of the teams in the open track.
1051
pchem l pchem ul childes ul bio ul
Ours 80.22 83.38 61.37 83.93
Best 81.06 83.42 61.37 -
Mean 73.03 76.42 57.89 -
Rank 3rd 2nd 1st -
Table 1: Official labeled (l) and other unlabeled (ul)
submitted results for the two test domains (pchem
and childes) and development data accuracy (bio).
The parser was trained on the provided WSJ data.
POS types, the loss (difference in source and tar-
get error) was: verbs (2%), conjunctions (5%), dig-
its (23%), prepositions (4%), adjectives (3%), de-
terminers (4%) and nouns (9%). 2 Two POS types
stand out: digits and nouns. Digits are less than
4% of the tokens in BIO. Errors result from the BIO
annotations for long sequences of digits which do
not appear in WSJ. Since these annotations are new
with respect to the WSJ guidelines, it is impossi-
ble to parse these without injecting knowledge of
the annotation guidelines. 3 Nouns are far more
common, comprising 33% of BIO and 30% of WSJ
tokens, the most popular POS tag by far. Addi-
tionally, other POS types listed above (adjectives,
prepositions, determiners, conjunctions) often attach
to nouns. To confirm that nouns were problem-
atic, we modified a first-order parser (no second or-
der features) by adding a feature indicating correct
noun-noun edges, forcing the parser to predict these
edges correctly. Adaptation performance rose on
BIO from 78% without the feature to 87% with the
feature. This indicates that most of the loss comes
from missing these edges.
The primary problem for nouns is the difference
between structures in each domain. The annota-
tion guidelines for the Penn Treebank flattened noun
phrases to simplify annotation (Marcus et al, 1993),
so there is no complex structure to NPs. Ku?bler
(2006) showed that it is difficult to compare the
Penn Treebank to other treebanks with more com-
plex noun structures, such as BIO. Consider theWSJ
phrase ?the New York State Insurance Department?.
The annotation indicates a flat structure, where ev-
2We measured these drops on several other dependency
parsers and found similar results.
3For example, the phrase ?(R = 28% (10/26); K=10% (3/29);
chi2 test: p=0.014).?
ery token is headed by ?Department?. In contrast,
a similar BIO phrase has a very different structure,
pursuant to the BIO guidelines. For ?the detoxi-
cation enzyme glutathione transferase P1-1?, ?en-
zyme? is the head of the NP, ?P1-1? is the head of
?transferase?, and ?transferase? is the head of ?glu-
tathione?. Since the guidelines differ, we observe no
corresponding structure in the WSJ. It is telling that
the parser labels this BIO example by attaching ev-
ery token to the final proper noun ?P1-1?, exactly as
the WSJ guidelines indicate. Unlabeled data cannot
indicate that BIO uses a different standard.
Another problem concerns appositives. For ex-
ample, the phrase ?Howard Mosher, president and
chief executive officer,? has ?Mosher? as the head
of ?Howard? and of the appositive NP delimited by
commas. While similar constructions occur in BIO,
there are no commas to indicate this. An example is
the above BIO NP, in which the phrase ?glutathione
transferase P1-1? is an appositive indicating which
?enzyme? is meant. However, since there are no
commas, the parser thinks ?P1-1? is the head. How-
ever, there are not many right to left attaching nouns.
In addition to a change in the annotation guide-
lines for NPs, we observed an important difference
in the distribution of POS tags. NN tags were almost
twice as likely in the BIO domain (14% in WSJ and
25% in BIO). NNP tags, which are close to 10% of
the tags in WSJ, are nonexistent in BIO (.24%). The
cause for this is clear when the annotation guide-
lines are considered. The proper nouns in WSJ are
names of companies, people and places, while in
BIO they are names of genes, proteins and chemi-
cals. However, for BIO these nouns are labeled NN
instead of NNP. This decision effectively removes
NNP from the BIO domain and renders all features
that depend on the NNP tag ineffective. In our above
BIO NP example, all nouns are labeled NN, whereas
the WSJ example contains NNP tags. The largest
tri-gram differences involve nouns, such as NN-NN-
NN, NNP-NNP-NNP, NN-IN-NN, and IN-NN-NN.
However, when we examine the coarse POS tags,
which do not distinguish between nouns, these dif-
ferences disappear. This indicates that while the
overall distribution of POS tags is similar between
the domains, the fine grained tags differ. These fine
grained tags provide more information than coarse
tags; experiments that removed fine grained tags
1052
hurt WSJ performance but did not affect BIO.
Finally, we examined the effect of unknown
words. Not surprisingly, the most significant dif-
ferences in error rates concerned dependencies be-
tween words of which one or both were unknown
to the parser. For two words that were seen in the
training data loss was 4%, for a single unknown
word loss was 15%, and 26% when both words were
unknown. Both words were unknown only 5% of
the time in BIO, while one of the words being un-
known was more common, reflecting 27% of deci-
sions. Upon further investigation, the majority of
unknown words were nouns, which indicates that
unknown word errors were caused by the problems
discussed above.
Recent theoretical work on domain adapta-
tion (Ben-David et al, 2006) attributes adaptation
loss to two sources: the difference in the distribu-
tion between domains and the difference in label-
ing functions. Adaptation techniques focus on the
former since it is impossible to determine the lat-
ter without knowledge of the labeling function. In
parsing adaptation, the former corresponds to a dif-
ference between the features seen in each domain,
such as new words in the target domain. The de-
cision function corresponds to differences between
annotation guidelines between two domains. Our er-
ror analysis suggests that the primary cause of loss
from adaptation is from differences in the annotation
guidelines themselves. Therefore, significant im-
provements cannot be made without specific knowl-
edge of the target domain?s annotation standards. No
amount of source training data can help if no rele-
vant structure exists in the data. Given the results
for the domain adaptation track, it appears no team
successfully adapted a state-of-the-art parser.
3 Adaptation Approaches
We survey the main approaches we explored for this
task. While some of these approaches provided a
modest performance boost to a simple parser (lim-
ited data and first-order features), no method added
any performance to our best parser (all data and
second-order features).
3.1 Features
A natural approach to improving parsing is to mod-
ify the feature set, both by removing features less
likely to transfer and by adding features that are
more likely to transfer. We began with the first ap-
proach and removed a large number of features that
we believed transfered poorly, such as most features
for noun-noun edges. We obtained a small improve-
ment in BIO performance on limited data only. We
then added several different types of features, specif-
ically designed to improve noun phrase construc-
tions, such as features based on the lexical position
of nouns (common position in NPs), frequency of
occurrence, and NP chunking information. For ex-
ample, trained on in-domain data, nouns that occur
more often tend to be heads. However, none of these
features transfered between domains.
A final type of feature we added was based on
the behavior of nouns, adjectives and verbs in each
domain. We constructed a feature representation
of words based on adjacent POS and words and
clustered words using an algorithm similar to that
of Saul and Pereira (1997). For example, our clus-
tering algorithm grouped first names in one group
and measurements in another. We then added the
cluster membership as a lexical feature to the parser.
None of the resulting features helped adaptation.
3.2 Diversity
Training diversity may be an effective source for
adaptation. We began by adding information from
multiple different parsers, which has been shown
to improve in-domain parsing. We added features
indicating when an edge was predicted by another
parser and if an edge crossed a predicted edge, as
well as conjunctions with edge types. This failed
to improve BIO accuracy since these features were
less reliable at test time. Next, we tried instance
bagging (Breiman, 1996) to generate some diversity
among parsers. We selected with replacement 2000
training examples from the training data and trained
three parsers. Each parser then tagged the remain-
ing 13K sentences, yielding 39K parsed sentences.
We then shuffled these sentences and trained a final
parser. This failed to improve performance, possibly
because of conflicting annotations or because of lack
of sufficient diversity. To address conflicting annota-
1053
tions, we added slack variables to the MIRA learn-
ing algorithm (Crammer et al, 2006) used to train
the parsers, without success. We measured diversity
by comparing the parses of each model. The dif-
ference in annotation agreement between the three
instance bagging parsers was about half the differ-
ence between these parsers and the gold annotations.
While we believe this is not enough diversity, it was
not feasible to repeat our experiment with a large
number of parsers.
3.3 Target Focused Learning
Another approach to adaptation is to favor training
examples that are similar to the target. We first mod-
ified the weight given by the parser to each training
sentence based on the similarity of the sentence to
target domain sentences. This can be done by mod-
ifying the loss to limit updates in cases where the
sentence does not reflect the target domain. We tried
a number of criteria to weigh sentences without suc-
cess, including sentence length and number of verbs.
Next, we trained a discriminative model on the pro-
vided unlabeled data to predict the domain of each
sentence based on POS n-grams in the sentence.
Training sentences with a higher probability of be-
ing in the target domain received higher weights,
also without success. Further experiments showed
that any decrease in training data hurt parser perfor-
mance. It would seem that the parser has no dif-
ficulty learning important training sentences in the
presence of unimportant training examples.
A related idea focused on words, weighing highly
tokens that appeared frequently in the target domain.
We scaled the loss associated with a token by a fac-
tor proportional to its frequency in the target do-
main. We found certain scaling techniques obtained
tiny improvements on the target domain that, while
significant compared to competition results, are not
statistically significant. We also attempted a sim-
ilar approach on the feature level. A very predic-
tive source domain feature is not useful if it does
not appear in the target domain. However, limiting
the feature space to target domain features had no
effect. Instead, we scaled each feature?s value by a
factor proportional to its frequency in the target do-
main and trained the parser on these scaled feature
values. We obtained small improvements on small
amounts of training data.
4 Future Directions
Given our pessimistic analysis and the long list of
failed methods, one may wonder if parser adapta-
tion is possible at all. We believe that it is. First,
there may be room for adaptation with our domains
if a common annotation scheme is used. Second,
we have stressed that typical adaptation, modifying
a model trained on the source domain, will fail but
there may be unsupervised parsing techniques that
improve performance after adaptation, such as a rule
based NP parser for BIO based on knowledge of the
annotations. However, this approach is unsatisfying
as it does not allow general purpose adaptation.
5 Acknowledgments
We thank Joel Wallenberg and Nikhil Dinesh for
their informative and helpful linguistic expertise,
Kevin Lerman for his edge labeler code, and Koby
Crammer for helpful conversations. Dredze is sup-
ported by a NDSEG fellowship; Ganchev and Taluk-
dar by NSF ITR EIA-0205448; and Blitzer by
DARPA under Contract No. NBCHD03001. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views of
the DARPA or the Department of Interior-National
Business Center (DOI-NBC).
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, Mar.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Sandra Ku?bler. 2006. How do treebank annotation
schemes influence parsing results? or how not to com-
pare apples and oranges. In RANLP.
1054
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency parsing with a two-
stage discriminative parser. In Conference on Natural
Language Learning (CoNLL).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language modeling. In EMNLP.
1055
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 689?697,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Methods for Multi-Domain Learning and Adaptation
Mark Dredze and Koby Crammer
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104 USA
{mdredze,crammer}@cis.upenn.edu
Abstract
NLP tasks are often domain specific, yet sys-
tems can learn behaviors across multiple do-
mains. We develop a new multi-domain online
learning framework based on parameter com-
bination from multiple classifiers. Our algo-
rithms draw from multi-task learning and do-
main adaptation to adapt multiple source do-
main classifiers to a new target domain, learn
across multiple similar domains, and learn
across a large number of disparate domains.
We evaluate our algorithms on two popular
NLP domain adaptation tasks: sentiment clas-
sification and spam filtering.
1 Introduction
Statistical classifiers routinely process millions of
websites, emails, blogs and other text every day.
Variability across different data sources means that
training a single classifier obscures differences and
separate classifiers ignore similarities. Similarly,
adding new domains to existing systems requires
adapting existing classifiers.
We present new online algorithms for three multi-
domain learning scenarios: adapting existing classi-
fiers to new domains, learning across multiple simi-
lar domains and scaling systems to many disparate
domains. Multi-domain learning combines char-
acteristics of both multi-task learning and domain
adaptation and drawing from both areas, we de-
velop a multi-classifier parameter combination tech-
nique for confidence-weighted (CW) linear classi-
fiers (Dredze et al, 2008). We focus on online algo-
rithms that scale to large amounts of data.
Next, we describe multi-domain learning and re-
view the CW algorithm. We then consider our three
settings using multi-classifier parameter combina-
tion. We conclude with related work.
2 Multi-Domain Learning
In online multi-domain learning, each instance x is
drawn from a domain d specific distribution x ? Dd
over a vectors space RN and labeled with a domain
specific function fd with label y ? {?1,+1} (for
binary classification.) On round i the classifier re-
ceives instance xi and domain identifier di and pre-
dicts label y?i ? {?1,+1}. It then receives the true
label yi ? {?1,+1} and updates its prediction rule.
As an example, consider a multi-user spam fil-
ter, which must give high quality predictions for
new users (without new user data), learn on multi-
ple users simultaneously and scale to thousands of
accounts. While a single classifier trained on all
users would generalize across users and extend to
new users, it would fail to learn user-specific prefer-
ences. Alternatively, separate classifiers would cap-
ture user-specific behaviors but would not general-
ize across users. The approach we take to solv-
ing multi-domain problems is to combine domain-
specific classifiers. In the adaptation setting, we
combine source domain classifiers for a new tar-
get domain. For learning across domains, we com-
bine domain-specific classifiers and a shared classi-
fier learned across all domains. For learning across
disparate domains we learn which domain-specific
and shared classifiers to combine.
Multi-domain learning combines properties of
both multi-task learning and domain adaptation. As
689
in multi-task learning, we consider domains that are
labeled with different classification functions. For
example, one user may enjoy some emails that an-
other user considers spam: differing in their classifi-
cation function. The goal of multi-task learning is to
generalize across tasks/domains (Dekel et al, 2006;
Evgeniou and Pontil, 2004). Furthermore, as in do-
main adaptation, some examples are draw from dif-
ferent distributions. For example, one user may re-
ceive emails about engineering while another about
art, differing in their distribution over features. Do-
main adaptation deals with these feature distribution
changes (Blitzer et al, 2007; Jiang and Zhai, 2007).
Our work combines these two areas by learning both
across distributions and behaviors or functions.
3 Confidence-Weighted Linear Classifiers
Confidence-weighted (CW) linear classification
(Dredze et al, 2008), a new online algorithm, main-
tains a probabilistic measure of parameter confi-
dence, which may be useful in combining parame-
ters from different domain distributions. We sum-
marize CW learning to familiarize the reader.
Parameter confidence is formalized by a Gaussian
distribution over weight vectors with mean ? ? RN
and diagonal covariance ? ? RN?N . The values
?j and ?j,j represent knowledge of and confidence
in the parameter for feature j. The smaller ?j,j ,
the more confidence we have in the mean parameter
value ?j . In this work we consider diagonal covari-
ance matrices to scale to NLP data.
A model predicts the highest probability label,
arg max
y?{?1}
Prw?N (?,?) [yi(w ? xi) ? 0] .
The Gaussian distribution over parameter vectors w
induces a univariate Gaussian distribution over the
score Si = w ? xi parameterized by ?, ? and the
instance xi: Si ? N
(
?i, ?2i
)
, with mean ?i = ??xi
and variance ?2i = x
>
i ?xi.
The CW algorithm is inspired by the Passive Ag-
gressive (PA) update (Crammer et al, 2006) ?
which ensures a positive margin while minimizing
parameter change. CW replaces the Euclidean dis-
tance used in the PA update with the Kullback-
Leibler (KL) divergence over Gaussian distribu-
tions. It also replaces the minimal margin constraint
with a minimal probability constraint: with some
given probability ? ? (0.5, 1] a drawn classifier will
assign the correct label. This strategy yields the fol-
lowing objective solved on each round of learning:
min DKL (N (?,?) ?N (?i,?i))
s.t. Pr [yi (w ? xi) ? 0] ? ? ,
where (?i,?i) are the parameters on round i and
w ? N (?,?). The constraint ensures that the re-
sulting parameters
(
?i+1,?i+1
)
will correctly clas-
sify xi with probability at least ?. For convenience
we write ? = ??1 (?), where ? is the cumula-
tive function of the normal distribution. The opti-
mization problem above is not convex, but a closed
form approximation of its solution has the follow-
ing additive form: ?i+1 = ?i + ?iyi?ixi and
??1i+1 = ?
?1
i + 2?i?xix
>
i for,
?i=
?(1+2??i)+
?
(1+2??i)
2?8?
(
?i???2i
)
4??2i
.
Each update changes the feature weights ?, and in-
creases confidence (variance ? always decreases).
We employ CW classifiers since they provide con-
fidence estimates, which are useful for classifier
combination. Additionally, since we require per-
parameter confidence estimates, other confidence
based classifiers are not suitable for this setting.
4 Multi-Classifier Parameter Combination
The basis of our approach to multi-domain learning
is to combine the parameters of CW classifiers from
separate domains while respecting parameter confi-
dence. A combination method takes M CW classi-
fiers each parameterized by its own mean and vari-
ance parameters {(?m,?m)}Mm=1 and produces a
single combined classifier (?c,?c). A simple tech-
nique would be to average the parameters of classi-
fiers into a new classifier. However, this ignores the
difference in feature distributions. Consider for ex-
ample that the weight associated with some word in
a source classifier has a value of 0. This could either
mean that the word is very rare or that it is neutral
for prediction (like the work ?the?). The informa-
tion captured by the variance parameter allow us to
distinguish between the two cases: an high-variance
indicates a lack of confidence in the value of the
690
weight vectors because of small number of exam-
ples (first case), and vise-versa, small-variance indi-
cates that the value of the weight is based on plenty
of evidence. We favor combinations sensitive to this
distinction.
Since CW classifiers are Gaussian distributions,
we formalize classifier parameter combination as
finding a new distribution that minimizes the
weighted-divergence to a set of given distributions:
(?c,?c) = arg min
M?
m
D((?c,?c)||(?m,?m) ; bm) ,
where (since ? is diagonal),
D((?c,?c)||(?,?) ; b) =
?N
f bfD((?
c
f ,?
c
f,f )||(?f ,?f,f )) .
The (classifier specific) importance-weights bm ?
RN+ are used to weigh certain parameters of some
domains differently in the combination. When D is
the Euclidean distance (L2), we have,
D((?cf ,?
c
f,f )||(?f ,?f,f )) =
(?cf ? ?f )
2 + (?cf,f ? ?f,f )
2 .
and we obtain:
?cf =
1
?M
m b
m
f
M?
m
bmf ?
m
f ,
?cf,f =
1
?
m?M b
m
f
M?
m
bmf ?
m
f,f . (1)
Note that this is a (weighted) average of parameters.
The other case we consider is when D is a weighted
KL divergence we obtain a weighting of ? by ??1:
?cf =
(
M?
m
(?mf,f )
?1bmf
)?1 M?
m
(?mf,f )
?1?mf b
m
f
(?c)?1 =
(
M
M?
m
bmf
)?1 M?
m
(?mf )
?1bf
m . (2)
While each parameter is weighed by its variance in
the KL, we can also explicitly encode this behavior
as bmf = a ? ?
m
f,f ? 0, where a is the initializa-
tion value for ?mf,f . We call this weighting ?vari-
ance? as opposed to a uniform weighting of param-
eters (bmf = 1). We therefore have two combination
methods (L2 and KL) and two weighting methods
(uniform and variance).
5 Datasets
For evaluation we selected two domain adaptation
datasets: spam (Jiang and Zhai, 2007) and sentiment
(Blitzer et al, 2007). The spam data contains two
tasks, one with three users (task A) and one with 15
(task B). The goal is to classify an email (bag-of-
words) as either spam or ham (not-spam) and each
user may have slightly different preferences and fea-
tures. We used 700 and 100 training messages for
each user for task A and B respectively and 300 test
emails for each user.
The sentiment data contains product reviews from
Amazon for four product types: books, dvds, elec-
tronics and kitchen appliances and we extended this
with three additional domains: apparel, music and
videos. We follow Blitzer et. al. for feature ex-
traction. We created different datasets by modify-
ing the decision boundary using the ordinal rating
of each instance (1-5 stars) and excluding boundary
instances. We use four versions of this data:
? All - 7 domains, one per product type
? Books - 3 domains of books with the binary
decision boundary set to 2, 3 and 4 stars
? DVDs - Same as Books but with DVD reviews
? Books+DVDs - Combined Books and DVDs
The All dataset captures the typical domain adap-
tation scenario, where each domain has the same
decision function but different features. Books
and DVDs have the opposite problem: the same
features but different classification boundaries.
Books+DVDs combines both issues. Experiments
use 1500 training and 100 test instances per domain.
6 Multi-Domain Adaptation
We begin by examining the typical domain adapta-
tion scenario, but from an online perspective since
learning systems often must adapt to new users or
domains quickly and with no training data. For ex-
ample, a spam filter with separate classifiers trained
on each user must also classify mail for a new
user. Since other user?s training data may have been
deleted or be private, the existing classifiers must be
combined for the new user.
691
Train L2 KL
Target Domain All Src Target Best Src Avg Src Uniform Variance Uniform Variance
S
pa
m
user0 3.85 1.80 4.80 8.26 5.25 4.63 4.53 4.32
user1 3.57 3.17 4.28 6.91 4.53 3.80 4.23 3.83
user2 3.30 2.40 3.77 5.75 4.75 4.60 4.93 4.67
S
en
ti
m
en
t
apparel 12.32 12.02 14.12 21.15 14.03 13.18 13.50 13.48
books 16.85 18.95 22.95 25.76 19.58 18.63 19.53 19.05
dvd 13.65 17.40 17.30 21.89 15.53 13.73 14.48 14.15
kitchen 13.65 14.40 15.52 22.88 16.68 15.10 14.78 14.02
electronics 15.00 14.93 15.52 23.84 18.75 17.37 17.45 16.82
music 18.20 18.30 20.75 24.19 18.38 17.83 18.10 18.22
video 17.00 19.27 19.43 25.78 17.13 16.25 16.33 16.42
Table 1: Test error for multi-source adaptation on sentiment and spam data. Combining classifiers improves over
selecting a single classifier a priori (Avg Src).
We combine the existing user-specific classifiers
into a single new classifier for a new user. Since
nothing is known about the new user (their deci-
sion function), each source classifier may be useful.
However, feature similarity ? possibly measured us-
ing unlabeled data ? could be used to weigh source
domains. Specifically, we combine the parameters
of each classifier according to their confidence us-
ing the combination methods described above.
We evaluated the four combination strategies ? L2
vs. KL, uniform vs. variance ? on spam and sen-
timent data. For each evaluation, a single domain
was held out for testing while separate classifiers
were trained on each source domain, i.e. no target
training. Source classifiers are then combined and
the combined classifier is evaluated on the test data
(400 instances) of the target domain. Each classi-
fier was trained for 5 iterations over the training data
(to ensure convergence) and each experiment was
repeated using 10-fold cross validation. The CW
parameter ? was tuned on a single randomized run
for each experiment. We include several baselines:
training on target data to obtain an upper bound
on performance (Target), training on all source do-
mains together, a useful strategy if all source data is
maintained (All Src), selecting (with omniscience)
the best performing source classifier on target data
(Best Src), and the expected real world performance
of randomly selecting a source classifier (Avg Src).
While at least one source classifier achieved high
performance on the target domain (Best Src), the
correct source classifier cannot be selected without
target data and selecting a random source classifier
yields high error. In contrast, a combined classifier
almost always improved over the best source domain
classifier (table 1). That some of our results improve
over the best training scenario is likely caused by in-
creased training data from using multiple domains.
Increases over all available training data are very in-
teresting and may be due to a regularization effect of
training separate models.
The L2 methods performed best and KL improved
7 out of 10 combinations. Classifier parameter com-
bination can clearly yield good classifiers without
prior knowledge of the target domain.
7 Learning Across Domains
In addition to adapting to new domains, multi-
domain systems should learn common behaviors
across domains. Naively, we can assume that the
domains are either sufficiently similar to warrant
one classifier or different enough for separate clas-
sifiers. The reality is often more complex. Instead,
we maintain shared and domain-specific parameters
and combine them for learning and prediction.
Multi-task learning aims to learn common behav-
iors across related problems, a similar goal to multi-
domain learning. The primary difference is the na-
ture of the domains/tasks: in our setting each domain
is the same task but differs in the types of features in
addition to the decision function. A multi-task ap-
proach can be adapted to our setting by using our
classifier combination techniques.
692
Spam Sentiment
Method Task A Task B Books DVD Books+DVD All
Single 3.88 8.75 23.7 25.11 23.26 16.57
Separate 5.46 14.53 22.22 21.64 21.23 21.89
Feature Splitting 4.16 8.93 15.65 16.20 14.60 17.45
MDR 4.09 9.18 15.65 15.12 13.76 17.45
MDR+L2 4.27 8.61 12.70 14.95 12.73 17.16
MDR+L2-Var 3.75 7.52 12.90 14.21 12.52 17.37
MDR+KL 4.32 9.22 13.51 13.81 13.32 17.20
MDR+KL-Var 4.02 8.70 14.93 14.03 14.22 18.40
Table 2: Online training error for learning across domains.
Spam Sentiment
Method Task A Task B Books DVD Books+DVD All
Single 2.11 5.60 18.43 18.67 19.08 14.09
Separate 2.43 8.5 18.87 15.97 16.45 17.23
Feature Splitting 1.94 5.51 9.97 9.70 9.05 14.73
MDR 1.94 5.69 9.97 8.33 8.20 14.73
MDR+L2 1.87 5.16 6.63 7.97 7.62 14.20
MDR+L2-Var 1.90 4.78 6.40 7.83 7.30 14.33
MDR+KL 1.94 5.61 8.37 7.07 8.43 14.60
MDR+KL-Var 1.97 5.46 9.40 7.50 8.05 15.50
Table 3: Test data error: learning across domains (MDR) improves over the baselines and Daume? (2007).
We seek to learn domain specific parameters
guided by shared parameters. Dekel et al (2006)
followed this approach for an online multi-task algo-
rithm, although they did not have shared parameters
and assumed that a training round comprised an ex-
ample from each task. Evgeniou and Pontil (2004)
achieved a similar goal by using shared parameters
for multi-task regularization. Specifically, they as-
sumed that the weight vector for problem d could be
represented aswc = wd+ws, wherewd are task spe-
cific parameters and ws are shared across all tasks.
In this framework, all tasks are close to some under-
lying meanws and each one deviates from this mean
by wd. Their SVM style multi-task objective mini-
mizes the loss ofwc and the norm ofwd andws, with
a tradeoff parameter allowing for domain deviance
from the mean. The simple domain adaptation al-
gorithm of feature splitting used by Daume? (2007)
is a special case of this model where the norms are
equally weighted. An analogous CW objective is:
min
1
?1
DKL
(
N
(
?d,?d
)
?N
(
?di ,?
d
i
))
+
1
?2
DKL (N (?s,?s) ?N (?si ,?
s
i ))
s.t. Prw?N (?c,?c) [yi (w ? xi) ? 0] ? ? . (3)
(
?d,?d
)
are the parameters for domain d, (?s,?s)
for the shared classifier and (?c,?c) for the com-
bination of the domain and shared classifiers. The
parameters are combined via (2) with only two ele-
ments summed - one for the shared parameters s and
the other for the domain parameters d . This captures
the intuition of Evgeniou and Pontil: updates en-
force the learning condition on the combined param-
eters and minimize parameter change. For conve-
nience, we rewrite ?2 = 2? 2?1, where ?1 ? [0, 1].
If classifiers are combined using the sum of the indi-
vidual weight vectors and ?1 = 0.5, this is identical
to feature splitting (Daume?) for CW classifiers.
The domain specific and shared classifiers can be
693
updated using the closed form solution to (3) as:
?s = ?si + ?2?yi?
cxi
(?s)?1 = (?si )
?1 + 2?2??xixTi
?d = ?di + ?1?yi?
c
ixi
(?d)?1 = (?di )
?1 + 2?1??xixTi
(4)
We call this objective Multi-Domain Regulariza-
tion (MDR). As before, the combined parameters
are produced by one of the combination methods.
On each round, the algorithm receives instance xi
and domain di for which it creates a combined clas-
sifier (?c,?c) using the shared (?s,?s) and domain
specific parameters
(
?d,?d
)
. A prediction is is-
sued using the standard linear classifier prediction
rule sign(?c ? x) and updates follow (4). The ef-
fect is that features similar across domains quickly
converge in the shared classifier, sharing informa-
tion across domains. The combined classifier re-
flects shared and domain specific parameter confi-
dences: weights with low variance (i.e. greater con-
fidence) will contribute more.
We evaluate MDR on a single pass over a stream
of instances from multiple domains, simulating a
real world setting. Parameters ?1 and ? are iter-
atively optimized on a single randomized run for
each dataset. All experiments use 10-fold CV. In ad-
dition to evaluating the four combination methods
with MDR, we evaluate the performance of a sin-
gle classifier trained on all domains (Single), a sep-
arate classifier trained on each domain (Separate),
Feature Splitting (Daume?) and feature splitting with
optimized ?1 (MDR). Table 3 shows results on test
data and table 2 shows online training error.
In this setting, L2 combinations prove best on 5
of 6 datasets, with the variance weighted combina-
tion doing the best. MDR (optimizing ?1) slightly
improves over feature splitting, and the combination
methods improve in every case. Our best result is
statistically significant compared to Feature Split-
ting using McNemar?s test (p = .001) for Task B,
Books, DVD, Books+DVD. While a single or sepa-
rate classifiers have a different effect on each dataset,
MDR gives the best performance overall.
8 Learning in Many Domains
So far we have considered settings with a small
number of similar domains. While this is typical
of multi-task problems, real world settings present
many domains which do not all share the same be-
haviors. Online algorithms scale to numerous ex-
amples and we desire the same behavior for numer-
ous domains. Consider a spam filter used by a large
email provider, which filters billions of emails for
millions of users. Suppose that spammers control
many accounts and maliciously label spam as legiti-
mate. Alternatively, subsets of users may share pref-
erences. Since behaviors are not consistent across
domains, shared parameters cannot be learned. We
seek algorithms robust to this behavior.
Since subsets of users share behaviors, these can
be learned using our MDR framework. For example,
discovering spammer and legitimate mail accounts
would enable intra-group learning. The challenge is
the online discovery of these subsets while learning
model parameters. We augment the MDR frame-
work to additionally learn this mapping.
We begin by generalizing MDR to include k
shared classifiers instead of a single set of shared pa-
rameters. Each set of shared parameters represents
a different subset of domains. If the corresponding
shared parameters are known for a domain, we could
use the same objective (3) and update (4) as before.
If there are many fewer shared parameters than do-
mains (k  D), we can benefit from multi-domain
learning. Next, we augment the learning algorithm
to learn a mapping between the domains and shared
classifiers. Intuitively, a domain should be mapped
to shared parameters that correctly classify that do-
main. A common technique for learning such ex-
perts in the Weighted Majority algorithm (Little-
stone and Warmuth, 1994), which weighs a mixture
of experts (classifiers). However, since we require a
hard assignment ? pick a single shared parameter
set s ? rather than a mixture, the algorithm reduces
to picking the classifier s with the fewest mistakes
in predicting domain d. This requires tracking the
number of mistakes made by each shared classifier
on each domain once a label is revealed. For learn-
ing, the shared classifier with the fewest mistakes
for a domain is selected for an MDR update. Clas-
sifier ties are broken randomly. While we experi-
694
Figure 1: Learning across many domains - spam (left) and sentiment (right) - with MDR using k shared classifiers.
Figure 2: Learning across many domains - spam (left) and sentiment (right) - with no domain specific parameters.
mented with more complex techniques, this simple
method worked well in practice. When a new do-
main is added to the system, it takes fewer exam-
ples to learn which shared classifier to use instead of
learning a new model from scratch.
While this approach adds another free parameter
(k) that can be set using development data, we ob-
serve that k can instead be fixed to a large constant.
Since only a single shared classifier is updated each
round, the algorithm will favor selecting a previ-
ously used classifier as opposed to a new one, using
as many classifiers as needed but not scaling up to k.
This may not be optimal, but it is a simple.
To evaluate a larger number of domains, we cre-
ated many varying domains using spam and senti-
ment data. For spam, 6 email users were created by
splitting the 3 task A users into 2 users, and flipping
the label of one of these users (a malicious user),
yielding 400 train and 100 test emails per user. For
sentiment, the book domain was split into 3 groups
with binary boundaries at a rating of 2, 3 or 4. Each
of these groups was split into 8 groups of which half
had their labels flipped, creating 24 domains. The
same procedure was repeated for DVD reviews but
for a decision boundary of 3, 6 groups were created,
and for a boundary of 2 and 4, 3 groups were created
with 1 and 2 domains flipped respectively, resulting
in 12 DVD domains and 36 total domains with var-
ious decision boundaries, features, and inverted de-
cision functions. Each domain used 300 train and
100 test instances. 10-fold cross validation with one
training iteration was used to train models on these
695
two datasets. Parameters were optimized as before.
Experiments were repeated for various settings of
k. Since L2 performed well before, we evaluated
MDR+L2 and MDR+L2-Var.
The results are shown in figure 1. For both spam
and sentiment adding additional shared parameters
beyond the single shared classifier significantly re-
duces error, with further reductions as k increases.
This yields a 45% error reduction for spam and a
38% reduction for sentiment over the best baseline.
While each task has an optimal k (about 5 for spam,
2 for sentiment), larger values still achieve low error,
indicating the flexibility of using large k values.
While adding parameters clearly helps for many
domains, it may be impractical to keep domain-
specific classifiers for thousands or millions of do-
mains. In this case, we could eliminate the domain-
specific classifiers and rely on the k shared clas-
sifiers only, learning the domain to classifier map-
ping. We compare this approach using the best result
from MDR above, again varying k. Figure 2 shows
that losing domain-specific parameters hurts perfor-
mance, but is still an improvement over baseline
methods. Additionally, we can expect better perfor-
mance as the number of similar domains increases.
This may be an attractive alternative to keeping a
very large number of parameters.
9 Related Work
Multi-domain learning intersects two areas of re-
search: domain adaptation and multi-task learning.
In domain adaptation, a classifier trained for a source
domain is transfered to a target domain using either
unlabeled or a small amount of labeled target data.
Blitzer et al (2007) used structural correspondence
learning to train a classifier on source data with
new features induced from target unlabeled data. In
a complimentary approach, Jiang and Zhai (2007)
weighed training instances based on their similarity
to unlabeled target domain data. Several approaches
utilize source data for training on a limited number
of target labels, including feature splitting (Daume?,
2007) and adding the source classifier?s prediction
as a feature (Chelba and Acero, 2004). Others have
considered transfer learning, in which an existing
domain is used to improve learning in a new do-
main, such as constructing priors (Raina et al, 2006;
Marx et al, 2008) and learning parameter functions
for text classification from related data (Do and Ng,
2006). These methods largely require batch learn-
ing, unlabeled target data, or available source data
at adaptation. In contrast, our algorithms operate
purely online and can be applied when no target data
is available.
Multi-task algorithms, also known as inductive
transfer, learn a set of related problems simultane-
ously (Caruana, 1997). The most relevant approach
is that of Regularized Multi-Task Learning (Evge-
niou and Pontil, 2004), which we use to motivate
our online algorithm. Dekel et al (2006) gave a sim-
ilar online approach but did not use shared parame-
ters and assumed multiple instances for each round.
We generalize this work to both include an arbi-
trary classifier combination and many shared classi-
fiers. Some multi-task work has also considered the
grouping of tasks similar to our learning of domain
subgroups (Thrun and O?Sullivan, 1998; Bakker and
Heskes, 2003).
There are many techniques for combining the out-
put of multiple classifiers for ensemble learning or
mixture of experts. Kittler et al (Mar 1998) provide
a theoretical framework for combining classifiers.
Some empirical work has considered adding versus
multiplying classifier output (Tax et al, 2000), using
local accuracy estimates for combination (Woods et
al., 1997), and applications to NLP tasks (Florian et
al., 2003). However, these papers consider combin-
ing classifier output for prediction. In contrast, we
consider parameter combination for both prediction
and learning.
10 Conclusion
We have explored several multi-domain learning
settings using CW classifiers and a combination
method. Our approach creates a better classifier for
a new target domain than selecting a random source
classifier a prior, reduces learning error on multiple
domains compared to baseline approaches, can han-
dle many disparate domains by using many shared
classifiers, and scales to a very large number of do-
mains with a small performance reduction. These
scenarios are realistic for NLP systems in the wild.
This work also raises some questions about learning
on large numbers of disparate domains: can a hi-
696
erarchical online clustering yield a better represen-
tation than just selecting between k shared parame-
ters? Additionally, how can prior knowledge about
domain similarity be included into the combination
methods? We plan to explore these questions in fu-
ture work.
Acknowledgements This material is based upon
work supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. FA8750-07-D-0185.
References
B. Bakker and T. Heskes. 2003. Task clustering and gat-
ing for bayesian multi?task learning. Journal of Ma-
chine Learning Research, 4:83?99.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41?75.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
max- imum entropy classifier: Little data can help a
lot. In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Hal Daume?. 2007. Frustratingly easy domain adaptation.
In Association for Computational Linguistics (ACL).
Ofer Dekel, Philip M. Long, and Yoram Singer. 2006.
Online multitask learning. In Conference on Learning
Theory (COLT).
Chuong B. Do and Andrew Ng. 2006. Transfer learning
for text classification. In Advances in Neural Informa-
tion Processing Systems (NIPS).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Conference on
Knowledge Discovery and Data Mining (KDD).
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Conference on Computational
Natural Language Learning (CONLL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Association for
Computational Linguistics (ACL).
J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas. Mar
1998. On combining classifiers. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
20(3):226?239.
N. Littlestone and M. K. Warmuth. 1994. The weighted
majority algorithm. Information and Computation,
108:212?261.
Zvika Marx, Michael T. Rosenstein, Thomas G. Diet-
terich, and Leslie Pack Kaelbling. 2008. Two algo-
rithms for transfer learning. In Inductive Transfer: 10
years later.
Rajat Raina, Andrew Ng, and Daphne Koller. 2006.
Constructing informative priors using transfer learn-
ing. In International Conference on Machine Learn-
ing (ICML).
David M. J. Tax, Martijn van Breukelen, Robert P. W.
Duina, and Josef Kittler. 2000. Combining multiple
classifiers by averaging or by multiplying? Pattern
Recognition, 33(9):1475?1485, September.
S. Thrun and J. O?Sullivan. 1998. Clustering learning
tasks and the selective cross?task transfer of knowl-
edge. In S. Thrun and L.Y. Pratt, editors, Learning To
Learn. Kluwer Academic Publishers.
Kevin Woods, W. Philip Kegelmeyer Jr., and Kevin
Bowyer. 1997. Combination of multiple classifiers
using local accuracy estimates. IEEE Transactions on
Pattern Analysis andMachine Intelligence, 19(4):405?
410.
697
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440?447,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification
John Blitzer Mark Dredze
Department of Computer and Information Science
University of Pennsylvania
{blitzer|mdredze|pereria@cis.upenn.edu}
Fernando Pereira
Abstract
Automatic sentiment classification has been
extensively studied and applied in recent
years. However, sentiment is expressed dif-
ferently in different domains, and annotating
corpora for every possible domain of interest
is impractical. We investigate domain adap-
tation for sentiment classifiers, focusing on
online reviews for different types of prod-
ucts. First, we extend to sentiment classifi-
cation the recently-proposed structural cor-
respondence learning (SCL) algorithm, re-
ducing the relative error due to adaptation
between domains by an average of 30% over
the original SCL algorithm and 46% over
a supervised baseline. Second, we identify
a measure of domain similarity that corre-
lates well with the potential for adaptation
of a classifier from one domain to another.
This measure could for instance be used to
select a small set of domains to annotate
whose trained classifiers would transfer well
to many other domains.
1 Introduction
Sentiment detection and classification has received
considerable attention recently (Pang et al, 2002;
Turney, 2002; Goldberg and Zhu, 2004). While
movie reviews have been the most studied domain,
sentiment analysis has extended to a number of
new domains, ranging from stock message boards
to congressional floor debates (Das and Chen, 2001;
Thomas et al, 2006). Research results have been
deployed industrially in systems that gauge market
reaction and summarize opinion from Web pages,
discussion boards, and blogs.
With such widely-varying domains, researchers
and engineers who build sentiment classification
systems need to collect and curate data for each new
domain they encounter. Even in the case of market
analysis, if automatic sentiment classification were
to be used across a wide range of domains, the ef-
fort to annotate corpora for each domain may be-
come prohibitive, especially since product features
change over time. We envision a scenario in which
developers annotate corpora for a small number of
domains, train classifiers on those corpora, and then
apply them to other similar corpora. However, this
approach raises two important questions. First, it
is well known that trained classifiers lose accuracy
when the test data distribution is significantly differ-
ent from the training data distribution 1. Second, it is
not clear which notion of domain similarity should
be used to select domains to annotate that would be
good proxies for many other domains.
We propose solutions to these two questions and
evaluate them on a corpus of reviews for four differ-
ent types of products from Amazon: books, DVDs,
electronics, and kitchen appliances2. First, we show
how to extend the recently proposed structural cor-
1For surveys of recent research on domain adaptation, see
the ICML 2006 Workshop on Structural Knowledge Transfer
for Machine Learning (http://gameairesearch.uta.
edu/) and the NIPS 2006 Workshop on Learning when test
and training inputs have different distribution (http://ida.
first.fraunhofer.de/projects/different06/)
2The dataset will be made available by the authors at publi-
cation time.
440
respondence learning (SCL) domain adaptation al-
gorithm (Blitzer et al, 2006) for use in sentiment
classification. A key step in SCL is the selection of
pivot features that are used to link the source and tar-
get domains. We suggest selecting pivots based not
only on their common frequency but also according
to their mutual information with the source labels.
For data as diverse as product reviews, SCL can
sometimes misalign features, resulting in degrada-
tion when we adapt between domains. In our second
extension we show how to correct misalignments us-
ing a very small number of labeled instances.
Second, we evaluate the A-distance (Ben-David
et al, 2006) between domains as measure of the loss
due to adaptation from one to the other. The A-
distance can be measured from unlabeled data, and it
was designed to take into account only divergences
which affect classification accuracy. We show that it
correlates well with adaptation loss, indicating that
we can use the A-distance to select a subset of do-
mains to label as sources.
In the next section we briefly review SCL and in-
troduce our new pivot selection method. Section 3
describes datasets and experimental method. Sec-
tion 4 gives results for SCL and the mutual informa-
tion method for selecting pivot features. Section 5
shows how to correct feature misalignments using a
small amount of labeled target domain data. Sec-
tion 6 motivates the A-distance and shows that it
correlates well with adaptability. We discuss related
work in Section 7 and conclude in Section 8.
2 Structural Correspondence Learning
Before reviewing SCL, we give a brief illustrative
example. Suppose that we are adapting from re-
views of computers to reviews of cell phones. While
many of the features of a good cell phone review are
the same as a computer review ? the words ?excel-
lent? and ?awful? for example ? many words are to-
tally new, like ?reception?. At the same time, many
features which were useful for computers, such as
?dual-core? are no longer useful for cell phones.
Our key intuition is that even when ?good-quality
reception? and ?fast dual-core? are completely dis-
tinct for each domain, if they both have high correla-
tion with ?excellent? and low correlation with ?aw-
ful? on unlabeled data, then we can tentatively align
them. After learning a classifier for computer re-
views, when we see a cell-phone feature like ?good-
quality reception?, we know it should behave in a
roughly similar manner to ?fast dual-core?.
2.1 Algorithm Overview
Given labeled data from a source domain and un-
labeled data from both source and target domains,
SCL first chooses a set ofm pivot features which oc-
cur frequently in both domains. Then, it models the
correlations between the pivot features and all other
features by training linear pivot predictors to predict
occurrences of each pivot in the unlabeled data from
both domains (Ando and Zhang, 2005; Blitzer et al,
2006). The `th pivot predictor is characterized by
its weight vector w`; positive entries in that weight
vector mean that a non-pivot feature (like ?fast dual-
core?) is highly correlated with the corresponding
pivot (like ?excellent?).
The pivot predictor column weight vectors can be
arranged into a matrix W = [w`]n`=1. Let ? ? R
k?d
be the top k left singular vectors of W (here d indi-
cates the total number of features). These vectors are
the principal predictors for our weight space. If we
chose our pivot features well, then we expect these
principal predictors to discriminate among positive
and negative words in both domains.
At training and test time, suppose we observe a
feature vector x. We apply the projection ?x to ob-
tain k new real-valued features. Now we learn a
predictor for the augmented instance ?x, ?x?. If ?
contains meaningful correspondences, then the pre-
dictor which uses ? will perform well in both source
and target domains.
2.2 Selecting Pivots with Mutual Information
The efficacy of SCL depends on the choice of pivot
features. For the part of speech tagging problem
studied by Blitzer et al (2006), frequently-occurring
words in both domains were good choices, since
they often correspond to function words such as
prepositions and determiners, which are good indi-
cators of parts of speech. This is not the case for
sentiment classification, however. Therefore, we re-
quire that pivot features also be good predictors of
the source label. Among those features, we then
choose the ones with highest mutual information to
the source label. Table 1 shows the set-symmetric
441
SCL, not SCL-MI SCL-MI, not SCL
book one <num> so all a must a wonderful loved it
very about they like weak don?t waste awful
good when highly recommended and easy
Table 1: Top pivots selected by SCL, but not SCL-
MI (left) and vice-versa (right)
differences between the two methods for pivot selec-
tion when adapting a classifier from books to kitchen
appliances. We refer throughout the rest of this work
to our method for selecting pivots as SCL-MI.
3 Dataset and Baseline
We constructed a new dataset for sentiment domain
adaptation by selecting Amazon product reviews for
four different product types: books, DVDs, electron-
ics and kitchen appliances. Each review consists of
a rating (0-5 stars), a reviewer name and location,
a product name, a review title and date, and the re-
view text. Reviews with rating > 3 were labeled
positive, those with rating < 3 were labeled neg-
ative, and the rest discarded because their polarity
was ambiguous. After this conversion, we had 1000
positive and 1000 negative examples for each do-
main, the same balanced composition as the polarity
dataset (Pang et al, 2002). In addition to the labeled
data, we included between 3685 (DVDs) and 5945
(kitchen) instances of unlabeled data. The size of the
unlabeled data was limited primarily by the number
of reviews we could crawl and download from the
Amazon website. Since we were able to obtain la-
bels for all of the reviews, we also ensured that they
were balanced between positive and negative exam-
ples, as well.
While the polarity dataset is a popular choice in
the literature, we were unable to use it for our task.
Our method requires many unlabeled reviews and
despite a large number of IMDB reviews available
online, the extensive curation requirements made
preparing a large amount of data difficult 3.
For classification, we use linear predictors on un-
igram and bigram features, trained to minimize the
Huber loss with stochastic gradient descent (Zhang,
3For a description of the construction of the polarity
dataset, see http://www.cs.cornell.edu/people/
pabo/movie-review-data/.
2004). On the polarity dataset, this model matches
the results reported by Pang et al (2002). When we
report results with SCL and SCL-MI, we require that
pivots occur in more than five documents in each do-
main. We set k, the number of singular vectors of the
weight matrix, to 50.
4 Experiments with SCL and SCL-MI
Each labeled dataset was split into a training set of
1600 instances and a test set of 400 instances. All
the experiments use a classifier trained on the train-
ing set of one domain and tested on the test set of
a possibly different domain. The baseline is a lin-
ear classifier trained without adaptation, while the
gold standard is an in-domain classifier trained on
the same domain as it is tested.
Figure 1 gives accuracies for all pairs of domain
adaptation. The domains are ordered clockwise
from the top left: books, DVDs, electronics, and
kitchen. For each set of bars, the first letter is the
source domain and the second letter is the target
domain. The thick horizontal bars are the accura-
cies of the in-domain classifiers for these domains.
Thus the first set of bars shows that the baseline
achieves 72.8% accuracy adapting from DVDs to
books. SCL-MI achieves 79.7% and the in-domain
gold standard is 80.4%. We say that the adaptation
loss for the baseline model is 7.6% and the adapta-
tion loss for the SCL-MImodel is 0.7%. The relative
reduction in error due to adaptation of SCL-MI for
this test is 90.8%.
We can observe from these results that there is a
rough grouping of our domains. Books and DVDs
are similar, as are kitchen appliances and electron-
ics, but the two groups are different from one an-
other. Adapting classifiers from books to DVDs, for
instance, is easier than adapting them from books
to kitchen appliances. We note that when transfer-
ring from kitchen to electronics, SCL-MI actually
outperforms the in-domain classifier. This is possi-
ble since the unlabeled data may contain information
that the in-domain classifier does not have access to.
At the beginning of Section 2 we gave exam-
ples of how features can change behavior across do-
mains. The first type of behavior is when predictive
features from the source domain are not predictive
or do not appear in the target domain. The second is
442
657075
808590
D->B E->B K->B B->D E->D K->D
baseline SCL SCL-MIbooks
72.8 76.8
79.7
70.7 75.4 75.4 70.9 66.1 68.6
80.4 82.477.2 74.0 75.8 70.6 74.3 76.2 72.7 75.4 76.9
dvd
6570
7580
8590
B->E D->E K->E B->K D->K E->K
electronics kitchen
70.8 77.5 75.9 73.0 74.1 74.1
82.7 83.7 86.884.4
87.7
74.5 78.7 78.9 74.079.4
81.4 84.0 84.4 85.9
Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI. Thick black
lines are the accuracies of in-domain classifiers.
domain\polarity negative positive
books plot <num> pages predictable reader grisham engaging
reading this page <num> must read fascinating
kitchen the plastic poorly designed excellent product espresso
leaking awkward to defective are perfect years now a breeze
Table 2: Correspondences discovered by SCL for books and kitchen appliances. The top row shows features
that only appear in books and the bottom features that only appear in kitchen appliances. The left and right
columns show negative and positive features in correspondence, respectively.
when predictive features from the target domain do
not appear in the source domain. To show how SCL
deals with those domain mismatches, we look at the
adaptation from book reviews to reviews of kitchen
appliances. We selected the top 1000 most infor-
mative features in both domains. In both cases, be-
tween 85 and 90% of the informative features from
one domain were not among the most informative
of the other domain4. SCL addresses both of these
issues simultaneously by aligning features from the
two domains.
4There is a third type, features which are positive in one do-
main but negative in another, but they appear very infrequently
in our datasets.
Table 2 illustrates one row of the projection ma-
trix ? for adapting from books to kitchen appliances;
the features on each row appear only in the corre-
sponding domain. A supervised classifier trained on
book reviews cannot assign weight to the kitchen
features in the second row of table 2. In con-
trast, SCL assigns weight to these features indirectly
through the projection matrix. When we observe
the feature ?predictable? with a negative book re-
view, we update parameters corresponding to the
entire projection, including the kitchen-specific fea-
tures ?poorly designed? and ?awkward to?.
While some rows of the projection matrix ? are
443
useful for classification, SCL can also misalign fea-
tures. This causes problems when a projection is
discriminative in the source domain but not in the
target. This is the case for adapting from kitchen
appliances to books. Since the book domain is
quite broad, many projections in books model topic
distinctions such as between religious and political
books. These projections, which are uninforma-
tive as to the target label, are put into correspon-
dence with the fewer discriminating projections in
the much narrower kitchen domain. When we adapt
from kitchen to books, we assign weight to these un-
informative projections, degrading target classifica-
tion accuracy.
5 Correcting Misalignments
We now show how to use a small amount of target
domain labeled data to learn to ignore misaligned
projections from SCL-MI. Using the notation of
Ando and Zhang (2005), we can write the supervised
training objective of SCL on the source domain as
min
w,v
?
i
L
(
w?xi + v??xi, yi
)
+ ?||w||2 + ?||v||2 ,
where y is the label. The weight vector w ? Rd
weighs the original features, while v ? Rk weighs
the projected features. Ando and Zhang (2005) and
Blitzer et al (2006) suggest ? = 10?4, ? = 0, which
we have used in our results so far.
Suppose now that we have trained source model
weight vectors ws and vs. A small amount of tar-
get domain data is probably insufficient to signif-
icantly change w, but we can correct v, which is
much smaller. We augment each labeled target in-
stance xj with the label assigned by the source do-
main classifier (Florian et al, 2004; Blitzer et al,
2006). Then we solve
minw,v
?
j L (w
?xj + v??xj , yj) + ?||w||2
+?||v ? vs||2 .
Since we don?t want to deviate significantly from the
source parameters, we set ? = ? = 10?1.
Figure 2 shows the corrected SCL-MI model us-
ing 50 target domain labeled instances. We chose
this number since we believe it to be a reasonable
amount for a single engineer to label with minimal
effort. For reasons of space, for each target domain
dom \ model base base scl scl-mi scl-mi
+targ +targ
books 8.9 9.0 7.4 5.8 4.4
dvd 8.9 8.9 7.8 6.1 5.3
electron 8.3 8.5 6.0 5.5 4.8
kitchen 10.2 9.9 7.0 5.6 5.1
average 9.1 9.1 7.1 5.8 4.9
Table 3: For each domain, we show the loss due to transfer
for each method, averaged over all domains. The bottom row
shows the average loss over all runs.
we show adaptation from only the two domains on
which SCL-MI performed the worst relative to the
supervised baseline. For example, the book domain
shows only results from electronics and kitchen, but
not DVDs. As a baseline, we used the label of the
source domain classifier as a feature in the target, but
did not use any SCL features. We note that the base-
line is very close to just using the source domain
classifier, because with only 50 target domain in-
stances we do not have enough data to relearn all of
the parameters inw. As we can see, though, relearn-
ing the 50 parameters in v is quite helpful. The cor-
rected model always improves over the baseline for
every possible transfer, including those not shown in
the figure.
The idea of using the regularizer of a linear model
to encourage the target parameters to be close to the
source parameters has been used previously in do-
main adaptation. In particular, Chelba and Acero
(2004) showed how this technique can be effective
for capitalization adaptation. The major difference
between our approach and theirs is that we only pe-
nalize deviation from the source parameters for the
weights v of projected features, while they work
with the weights of the original features only. For
our small amount of labeled target data, attempting
to penalize w using ws performed no better than
our baseline. Because we only need to learn to ig-
nore projections that misalign features, we can make
much better use of our labeled data by adapting only
50 parameters, rather than 200,000.
Table 3 summarizes the results of sections 4 and
5. Structural correspondence learning reduces the
error due to transfer by 21%. Choosing pivots by
mutual information allows us to further reduce the
error to 36%. Finally, by adding 50 instances of tar-
get domain data and using this to correct the mis-
aligned projections, we achieve an average relative
444
657075
808590
E->B K->B B->D K->D B->E D->E B->K E->K
base+50-targ SCL-MI+50-targbooks kitchen
70.9 76.0 70.7 76.8
78.5 72.7
80.4 87.776.6 70.8 76.6 73.0 77.9 74.3
80.7 84.3
dvd electronics82.4 84.4
73.2
85.9
Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances.
reduction in error of 46%.
6 Measuring Adaptability
Sections 2-5 focused on how to adapt to a target do-
main when you had a labeled source dataset. We
now take a step back to look at the problem of se-
lecting source domain data to label. We study a set-
ting where an engineer knows roughly her domains
of interest but does not have any labeled data yet. In
that case, she can ask the question ?Which sources
should I label to obtain the best performance over
all my domains?? On our product domains, for ex-
ample, if we are interested in classifying reviews
of kitchen appliances, we know from sections 4-5
that it would be foolish to label reviews of books or
DVDs rather than electronics. Here we show how to
select source domains using only unlabeled data and
the SCL representation.
6.1 The A-distance
We propose to measure domain adaptability by us-
ing the divergence of two domains after the SCL
projection. We can characterize domains by their
induced distributions on instance space: the more
different the domains, the more divergent the distri-
butions. Here we make use of the A-distance (Ben-
David et al, 2006). The key intuition behind the
A-distance is that while two domains can differ in
arbitrary ways, we are only interested in the differ-
ences that affect classification accuracy.
Let A be the family of subsets of Rk correspond-
ing to characteristic functions of linear classifiers
(sets on which a linear classifier returns positive
value). Then theA distance between two probability
distributions is
dA(D,D
?) = 2 sup
A?A
|PrD [A] ? PrD? [A]| .
That is, we find the subset in A on which the distri-
butions differ the most in the L1 sense. Ben-David
et al (2006) show that computing the A-distance for
a finite sample is exactly the problem of minimiz-
ing the empirical risk of a classifier that discrimi-
nates between instances drawn fromD and instances
drawn from D?. This is convenient for us, since it al-
lows us to use classification machinery to compute
the A-distance.
6.2 Unlabeled Adaptability Measurements
We follow Ben-David et al (2006) and use the Hu-
ber loss as a proxy for the A-distance. Our proce-
dure is as follows: Given two domains, we compute
the SCL representation. Then we create a data set
where each instance ?x is labeled with the identity
of the domain from which it came and train a linear
classifier. For each pair of domains we compute the
empirical average per-instance Huber loss, subtract
it from 1, and multiply the result by 100. We refer
to this quantity as the proxy A-distance. When it is
100, the two domains are completely distinct. When
it is 0, the two domains are indistinguishable using a
linear classifier.
Figure 3 is a correlation plot between the proxy
A-distance and the adaptation error. Suppose we
wanted to label two domains out of the four in such a
445
024
6810
1214
60 65 70 75 80 85 90 95 100Proxy A-distanceAd
aptation Loss EK BD DE
DK BE, BK
Figure 3: The proxy A-distance between each do-
main pair plotted against the average adaptation loss
of as measured by our baseline system. Each pair of
domains is labeled by their first letters: EK indicates
the pair electronics and kitchen.
way as to minimize our error on all the domains. Us-
ing the proxy A-distance as a criterion, we observe
that we would choose one domain from either books
or DVDs, but not both, since then we would not be
able to adequately cover electronics or kitchen appli-
ances. Similarly we would also choose one domain
from either electronics or kitchen appliances, but not
both.
7 Related Work
Sentiment classification has advanced considerably
since the work of Pang et al (2002), which we use
as our baseline. Thomas et al (2006) use discourse
structure present in congressional records to perform
more accurate sentiment classification. Pang and
Lee (2005) treat sentiment analysis as an ordinal
ranking problem. In our work we only show im-
provement for the basic model, but all of these new
techniques also make use of lexical features. Thus
we believe that our adaptation methods could be also
applied to those more refined models.
While work on domain adaptation for senti-
ment classifiers is sparse, it is worth noting that
other researchers have investigated unsupervised
and semisupervised methods for domain adaptation.
The work most similar in spirit to ours that of Tur-
ney (2002). He used the difference in mutual in-
formation with two human-selected features (the
words ?excellent? and ?poor?) to score features in
a completely unsupervised manner. Then he clas-
sified documents according to various functions of
these mutual information scores. We stress that our
method improves a supervised baseline. While we
do not have a direct comparison, we note that Tur-
ney (2002) performs worse on movie reviews than
on his other datasets, the same type of data as the
polarity dataset.
We also note the work of Aue and Gamon (2005),
who performed a number of empirical tests on do-
main adaptation of sentiment classifiers. Most of
these tests were unsuccessful. We briefly note their
results on combining a number of source domains.
They observed that source domains closer to the tar-
get helped more. In preliminary experiments we
confirmed these results. Adding more labeled data
always helps, but diversifying training data does not.
When classifying kitchen appliances, for any fixed
amount of labeled data, it is always better to draw
from electronics as a source than use some combi-
nation of all three other domains.
Domain adaptation alone is a generally well-
studied area, and we cannot possibly hope to cover
all of it here. As we noted in Section 5, we are
able to significantly outperform basic structural cor-
respondence learning (Blitzer et al, 2006). We also
note that while Florian et al (2004) and Blitzer et al
(2006) observe that including the label of a source
classifier as a feature on small amounts of target data
tends to improve over using either the source alone
or the target alne, we did not observe that for our
data. We believe the most important reason for this
is that they explore structured prediction problems,
where labels of surrounding words from the source
classifier may be very informative, even if the cur-
rent label is not. In contrast our simple binary pre-
diction problem does not exhibit such behavior. This
may also be the reason that the model of Chelba and
Acero (2004) did not aid in adaptation.
Finally we note that while Blitzer et al (2006) did
combine SCL with labeled target domain data, they
only compared using the label of SCL or non-SCL
source classifiers as features, following the work of
Florian et al (2004). By only adapting the SCL-
related part of the weight vector v, we are able to
make better use of our small amount of unlabeled
data than these previous techniques.
446
8 Conclusion
Sentiment classification has seen a great deal of at-
tention. Its application to many different domains
of discourse makes it an ideal candidate for domain
adaptation. This work addressed two important
questions of domain adaptation. First, we showed
that for a given source and target domain, we can
significantly improve for sentiment classification the
structural correspondence learning model of Blitzer
et al (2006). We chose pivot features using not only
common frequency among domains but also mutual
information with the source labels. We also showed
how to correct structural correspondence misalign-
ments by using a small amount of labeled target do-
main data.
Second, we provided a method for selecting those
source domains most likely to adapt well to given
target domains. The unsupervised A-distance mea-
sure of divergence between domains correlates well
with loss due to adaptation. Thus we can use the A-
distance to select source domains to label which will
give low target domain error.
In the future, we wish to include some of the more
recent advances in sentiment classification, as well
as addressing the more realistic problem of rank-
ing. We are also actively searching for a larger and
more varied set of domains on which to test our tech-
niques.
Acknowledgements
We thank Nikhil Dinesh for helpful advice through-
out the course of this work. This material is based
upon work partially supported by the Defense Ad-
vanced Research Projects Agency (DARPA) un-
der Contract No. NBCHD03001. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA or
the Department of Interior-National BusinessCenter
(DOI-NBC).
References
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. JMLR, 6:1817?1853.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
http://research.microsoft.com/ anthaue/.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In Neural Information Processing
Systems (NIPS).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In EMNLP.
Sanjiv Das and Mike Chen. 2001. Yahoo! for ama-
zon: Extracting market sentiment from stock message
boards. In Proceedings of Athe Asia Pacific Finance
Association Annual Conference.
R. Florian, H. Hassan, A.Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In of HLT-NAACL.
Andrew Goldberg and Xiaojin Zhu. 2004. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of Association
for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of Empiri-
cal Methods in Natural Language Processing.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of Association for
Computational Linguistics.
Tong Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In International Conference on Machine
Learning (ICML).
447
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 33?36,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Icelandic Data Driven Part of Speech Tagging
Mark Dredze
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
mdredze@cis.upenn.edu
Joel Wallenberg
Department of Linguistics
University of Pennsylvania
Philadelphia, PA 19104
joelcw@babel.ling.upenn.edu
Abstract
Data driven POS tagging has achieved good
performance for English, but can still lag be-
hind linguistic rule based taggers for mor-
phologically complex languages, such as Ice-
landic. We extend a statistical tagger to han-
dle fine grained tagsets and improve over the
best Icelandic POS tagger. Additionally, we
develop a case tagger for non-local case and
gender decisions. An error analysis of our sys-
tem suggests future directions.
1 Introduction
While part of speech (POS) tagging for English is
very accurate, languages with richer morphology de-
mand complex tagsets that pose problems for data
driven taggers. In this work we consider Icelandic,
a language for which a linguistic rule-based method
is the current state of the art, indicating the difficulty
this language poses to learning systems. Like Ara-
bic and Czech, other morphologically complex lan-
guages with large tagsets, Icelandic can overwhelm
a statistical tagger with ambiguity and data sparsity.
Shen et al (2007) presented a new framework for
bidirectional sequence classification that achieved
the best POS score for English. In this work, we
evaluate their tagger on Icelandic and improve re-
sults with extensions for fine grained annotations.
Additionally, we show that good performance can
be achieved using a strictly data-driven learning ap-
proach without external linguistic resources (mor-
phological analyzer, lexicons, etc.). Our system
achieves the best performance to date on Icelandic,
with insights that may help improve other morpho-
logically rich languages.
After some related work, we describe Icelandic
morphology followed by a review of previous ap-
proaches. We then apply a bidirectional tagger and
extend it for fine grained languages. A tagger for
case further improves results. We conclude with an
analysis of remaining errors and challenges.
2 Related Work
Previous approaches to tagging morphologically
complex languages with fine grained tagsets have
considered Czech and Arabic. Khoja (2001) first in-
troduced a tagger for Arabic, which has 131 tags,
but subsequent work has collapsed the tagset to sim-
plify tagging (Diab et al, 2004). Like previous Ice-
landic work (Loftsson, 2007), morphological ana-
lyzers disambiguate words before statistical tagging
in Arabic (Habash and Rambow, 2005) and Czech
(Hajic? and Hladk?, 1998). This general approach
has led to the serial combination of rule based and
statistical taggers for efficiency and accuracy (Hajic?
et al, 2001). While our tagger could be combined
with these linguistic resources as well, as in Loftsson
(2007), we show state of the art performance without
these resources. Another approach to fine-grained
tagging captures grammatical structures with tree-
based tags, such as ?supertags? in the tree-adjoining
grammar of Bangalore and Joshi (1999).
3 Icelandic Morphology
Icelandic is notable for its morphological richness.
Verbs potentially show as many as 54 different
forms depending on tense, mood, voice, person and
33
number. A highly productive class of verbs also
show stem vowel alternations reminiscent of Semitic
verb morphology (Arabic). Noun morphology ex-
hibits a robust case system; nouns may appear in
as many as 16 different forms. The four-case sys-
tem of Icelandic is similar to that of the Slavic lan-
guages (Czech), with case morphology also appear-
ing on elements which agree in case with nouns.
However, unlike Czech, case frequently does not
convey distinct meaning in Icelandic as it is of-
ten determined by elements such as the governing
verb in a clause (non-local information). There-
fore, while Icelandic case looks formally like Slavic
and presents similar challenges for POS tagging, it
also may be syntactically-determined, as in Standard
Arabic. Icelandic word-order allows a very limited
form of scrambling, but does not produce the variety
of permutations allowed in Slavic languages. This
combination of morphological complexity and syn-
tactic constraint makes Icelandic a good case study
for statistical POS tagging techniques.
The morphology necessitates the large extended
tagset developed for the Icelandic Frequency Dictio-
nary (?slensk or?t??nib?k/IFD), a corpus of roughly
590,000 tokens (Pind et al, 1991). We use the
10 IFD data splits produced by Helgad?ttir (2004),
where the first nine splits are used for evaluation
and the tenth for model development. Tags are com-
prised of up to six elements, such as word class, gen-
der, number, and case, yielding a total of 639 tags,
not all of which occur in the training data.
4 Previous Approaches
Helgad?ttir (2004) evaluated several data-driven
models for Icelandic, including MXPost, a maxi-
mum entropy tagger, and TnT, a trigram HMM; both
did considerably worse than on English. Icelandic
poses significant challenges: data sparseness, non-
local tag dependencies, and 136,264 observed tri-
gram sequences make discriminative sequence mod-
els, such as CRFs, prohibitively expensive. Given
these challenges, the most successful tagger is Ic-
eTagger (Loftsson, 2007), a linguistic rule based
system with several linguistic resources: a morpho-
logical analyzer, a series of local rules and heuris-
tics for handling PPs, verbs, and forcing agreement.
Loftsson also improves TnT by integrating a mor-
phological analyzer (TnT*).
Despite these challenges, data driven taggers have
several advantages. Learning systems can be eas-
ily applied to new corpora, tagsets, or languages and
can accommodate integration of other systems (in-
cluding rule based) or new linguistic resources, such
as those used by Loftsson. Therefore, we seek a
learning system that can handle these challenges.
5 Bidirectional Sequence Classification
Bidirectional POS tagging (Shen et al, 2007), the
current state of the art for English, has some prop-
erties that make it appropriate for Icelandic. For ex-
ample, it can be trained quickly with online learning
and does not use tag trigrams, which reduces data
sparsity and the cost of learning. It can also allow
long range dependencies, which we consider below.
Bidirectional classification uses a perceptron style
classifier to assign potential POS tags (hypotheses)
to each word using standard POS features and some
additional local context features. On each round, the
algorithm selects the highest scoring hypothesis and
assigns the guessed tag. Unassigned words in the
context are reevaluated with this new information.
If an incorrect hypothesis is selected during train-
ing, the algorithm promotes the score of the correct
hypothesis and demotes the selected one. See Shen
et al for a detailed explanation.
We begin with a direct application of the bidirec-
tional tagger to Icelandic using a beam of one and
the same parameters and features as Shen et al On
the development split the tagger achieved an accu-
racy of 91.61%, which is competitive with the best
Icelandic systems. However, test evaluation is not
possible due to the prohibitive cost of training the
tagger on nine splits; training took almost 4 days on
an AMD Opteron 2.8 GHz machine.
Tagset size poses a problem since the tagger must
evaluate over 600 options to select the top tag for
a word. The tagger rescores the local context af-
ter a tag is committed or all untagged words if the
classifier is updated. This also highlights a problem
with the learning model itself. The tagger uses a one
vs. all multi-class strategy, requiring a correct tag to
have higher score than every other tag to be selected.
While this is plausible for a small number of labels,
it overly constrains an Icelandic tagger.
34
Accuracy Train
Tagger All Known Unkn. Time
Bidir 91.61 93.21 69.76 90:27
Bidir+WC 91.98 93.58 70.10 12:20
Bidir+WC+CT 92.36 93.93 70.95 14:02
Table 1: Results on development data. Accuracy is mea-
sured by exact match with the gold tag. About 7% of
tokens are unknown at test time.
As with most languages, it is relatively simple to
assign word class (noun, verb, etc.) and we use this
property to divide the tagset into separate learning
problems. First, the tagger classifies a word accord-
ing to one of the eleven word classes. Next, it se-
lects and evaluates all tags consistent with that class.
When an incorrect selection is updated, the word
class classifier is updated only if it was mistaken
as well. The result is a dramatic reduction in the
number of tags considered at each step. For some
languages, it may make sense to consider further re-
ductions, but not for Icelandic since case, gender,
and number decisions are interdependent. Addition-
ally, by learning word class and tag separately, a cor-
rect tag need only score higher than other tags of
the same word class, not all 639. Furthermore, col-
lapsing tags into word class groups increases train-
ing data, allowing the model to generalize features
over all tags in a class instead of learning each tag
separately (a form of parameter tying).
Training time dropped to 12 hours with the bidi-
rectional word class (WC) tagger and learning per-
formance increased to 91.98% (table 1). Word class
accuracy, already quite high at 97.98%, increased to
98.34%, indicating that the tagger can quickly fil-
ter out most inappropriate tags. The reduced train-
ing cost allowed for test data evaluation, yielding
91.68%, which is a 12.97% relative reduction in er-
ror over the best pure data driven model (TnT) and a
1.65% reduction over the best model (IceTagger).
6 Case Tagger
Examining tagger error reveals that most mis-
takes are caused by case confusion on nouns
(84.61% accuracy), adjectives (76.03%), and pro-
nouns (90.67%); these account for 40% of the cor-
pus. While there are 16 case-number-definiteness
combinations in the noun morphology, a noun might
realize several combinations with a single phonolog-
ical/orthographic form (case-syncretism). Mistakes
in noun case lead to further mistakes for categories
which agree with nouns, e.g. adjectives. Assigning
appropriate case for nouns is important for a num-
ber of other tagging decisions, but often the noun?s
case provides little or no information about the iden-
tity of other tags. It is in this situation that the tag-
ger makes most case-assignment errors. Therefore,
while accuracy depends on correct case assignment
for these nouns, other tags are mostly unaffected.
One approach to correcting these errors is to intro-
duce long range dependencies, such as those used by
IceTagger. While normally hard to add to a learn-
ing system, bidirectional learning provides a natu-
ral framework since non-local features can be added
once a tag has been committed. To allow dependen-
cies on all other tag assignments, and because cor-
recting the remaining case assignments is unlikely to
improve other tags, we constructed a separate bidi-
rectional case tagger (CT) that retags case on nouns,
adjectives and pronouns. 1 Since gender is important
as it relates to case, it is retagged as well. The CT
takes a fully tagged sentence from the POS tagger
and retags case and gender to nouns, adjectives and
pronouns. The CT uses the same features as the POS
tagger, but it now has access to all predicted tags.
Additionally, we develop several non-local features.
Many case decisions are entirely idiosyncratic,
even from the point of view of human language-
learners. Some simple transitive verbs in Icelandic
arbitrarily require their objects to appear in dative
or genitive case, rather than the usual accusative.
This arbitrary case-assignment adds no additional
meaning, and this set of idiosyncratic verbs is mem-
orized by speakers. A statistical tagger likewise
must memorize these verbs based on examples in
the training data. To aid generalization, verb-forms
were augmented by verb-stems features as described
in Dredze and Wallenberg (2008): e.g., the verb
forms dveldi, dvaldi, dvelst, dvelur
all mapped to the stem dv*l (dvelja ?dwell?). The
tagger used non-local features, such as the preced-
ing verb?s (predicted) tag, gender, case, stem, and
nouns within the clause boundary as indicated by
1We considered adding case tagging features to and remov-
ing case decisions from the tagger; both hurt performance.
35
Tagger All Known Unknown
MXPost 89.08 91.04 62.50
TnT 90.44 91.82 71.68
TnT* 91.18 92.53 72.75
IceTagger 91.54 92.74 75.09
Bidir+WC 91.68 93.32 69.25
Bidir+WC+CT 92.06 93.70 69.74
Table 2: Results on test data.
the tags cn (complementizer) or ct (relativizer)
(Dredze and Wallenberg, 2008).
The CT was used to correct the output of the tag-
ger after training on the corresponding train split.
The CT improved results yielding a new best ac-
curacy of 92.06%, a 16.95% and 12.53% reduction
over the best data driven and rule systems.
7 Remaining Challenges
We have shown that a data driven approach can
achieve state of the art performance on highly in-
flected languages by extending bidirectional learn-
ing to fine grained tagsets and designing a bidirec-
tional non-local case tagger. We conclude with an
error analysis to provide future direction.
The tagger is particularly weak on unknown
words, a problem caused by case-syncretism and
idiosyncratic case-assignment. Data driven taggers
can only learn which verbs assign special object
cases by observation in the training data. Some
verbs and prepositions also assign case based on the
meaning of the whole phrase. These are both serious
challenges for data-driven methods and could be ad-
dressed with the integration of linguistic resources.
However, there is more work to be done on data
driven methods. Mistakes in case-assignment due
to case syncretism, especially in conjunction with
idiosyncratic-case-assigning verbs, account for a
large proportion of remaining errors. Verbs that take
dative rather than accusative objects are a particu-
lar problem, such as mistaking accusative for dative
feminine objects (10.6% of occurrences) or dative
for accusative feminine objects (11.9%). A possi-
ble learning solution lies in combining POS tagging
with syntactic parsing, allowing for the identifica-
tion of clause boundaries, which may help disam-
biguate noun cases by deducing their grammatical
function from that of other clausal constituents.
Additionally, idiosyncratic case-assignment could
be learned from unlabeled data by finding un-
ambiguous dative objects to identify idiosyncratic
verbs. Furthermore, our tagger learns which prepo-
sitions idiosyncratically assign a single odd case
(e.g. genitive) since prepositions are a smaller class
and appear frequently in the corpus. This indicates
that further work on data driven methods may still
improve the state of the art.
8 Acknowledgments
We thank Hrafn Loftsson for sharing IceTagger and
the datasplits, Libin Shen for his tagger, and the ?rni
Magn?sson Institute for Icelandic Studies for access
to the corpus.
References
Srinivas Bangalore and Arivand K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Compu-
tational Linguistics, 25(2).
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of arabic text: From raw text to
base phrase chunks. In NAACL/HLT.
Mark Dredze and Joel Wallenberg. 2008. Further results
and analysis of icelandic part of speech tagging. Tech-
nical Report MS-CIS-08-13, CIS Dept, University of
Pennsylvania.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In ACL.
Jan Hajic? and Barbora Hladk?. 1998. Tagging inflective
languages: prediction of morphological categories for
a rich, structured tagset. In COLING.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva, and
Vladim?r Petkevic?. 2001. Serial combination of rules
and statistics: a case study in czech tagging. In ACL.
Sigrun Helgad?ttir. 2004. Testing data-driven learning
algorithms for pos tagging of icelandic.
Shereen Khoja. 2001. Apt: Arabic part-of-speech tagger.
In NAACL Student Workshop.
Hrafn Loftsson. 2007. Tagging icelandic text using a
linguistic and a statistical tagger. In NAACL/HLT.
J Pind, F Magn?sson, and S Briem. 1991. The icelandic
frequency dictionary. Technical report, The Institute
of Lexicography, University of Iceland.
Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In ACL.
36
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 233?236,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Active Learning with Confidence
Mark Dredze and Koby Crammer
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{mdredze,crammer}@cis.upenn.edu
Abstract
Active learning is a machine learning ap-
proach to achieving high-accuracy with a
small amount of labels by letting the learn-
ing algorithm choose instances to be labeled.
Most of previous approaches based on dis-
criminative learning use the margin for choos-
ing instances. We present a method for in-
corporating confidence into the margin by us-
ing a newly introduced online learning algo-
rithm and show empirically that confidence
improves active learning.
1 Introduction
Successful applications of supervised machine
learning to natural language rely on quality labeled
training data, but annotation can be costly, slow and
difficult. One popular solution is Active Learning,
which maximizes learning accuracy while minimiz-
ing labeling efforts. In active learning, the learning
algorithm itself selects unlabeled examples for anno-
tation. A variety of techniques have been proposed
for selecting examples that maximize system perfor-
mance as compared to selecting instances randomly.
Two learning methodologies dominate NLP ap-
plications: probabilistic methods ? naive Bayes,
logistic regression ? and margin methods ? sup-
port vector machines and passive-aggressive. Active
learning for probabilistic methods often uses uncer-
tainty sampling: label the example with the lowest
probability prediction (the most ?uncertain?) (Lewis
and Gale, 1994). The equivalent technique for mar-
gin learning associates the margin with prediction
certainty: label the example with the lowest margin
(Tong and Koller, 2001). Common intuition equates
large margins with high prediction confidence.
However, confidence and margin are two distinct
properties. For example, an instance may receive
a large margin based on a single feature which has
been updated only a small number of times. Another
example may receive a small margin, but its features
have been learned from a large number of examples.
While the first example has a larger margin it has
low confidence compared to the second. Both the
margin value and confidence should be considered
in choosing which example to label.
We present active learning with confidence us-
ing a recently introduced online learning algo-
rithm called Confidence-Weighted linear classifica-
tion. The classifier assigns labels according to a
Gaussian distribution over margin values instead of
a single value, which arises from parameter confi-
dence (variance). The variance of this distribution
represents the confidence in the mean (margin). We
then employ this distribution for a new active learn-
ing criteria, which in turn could improve other mar-
gin based active learning techniques. Additionally,
we favor the use of an online method since online
methods have achieved good NLP performance and
are fast to train ? an important property for inter-
active learning. Experimental validation on a num-
ber of datasets shows that active learning with con-
fidence can improve standard methods.
2 Confidence-Weighted Linear Classifiers
Common online learning algorithms, popular in
many NLP tasks, are not designed to deal with
the particularities of natural language data. Fea-
233
ture representations have very high dimension and
most features are observed on a small fraction of in-
stances. Confidence-weighted (CW) linear classifi-
cation (Dredze et al, 2008), a new online algorithm,
maintains a probabilistic measure of parameter con-
fidence leading to a measure of prediction confi-
dence, potentially useful for active learning. We
summarize CW learning to familiarize the reader.
Parameter confidence is formalized with a distri-
bution over weight vectors, specifically a Gaussian
distribution with mean ? ? RN and diagonal co-
variance ? ? RN?N . The values ?j and ?j,j repre-
sent knowledge of and confidence in the parameter
for feature j. The smaller ?j,j , the more confidence
we have in the mean parameter value ?j .
A model predicts the label with the highest prob-
ability, maxy?{?1} Prw?N (?,?) [y(w ? x) ? 0] .
The Gaussian distribution over parameter vectors w
induces a univariate Gaussian distribution over the
unsigned-margin M = w ? x parameterized by ?,
? and the instance x: M ? N (M,V ), where the
mean is M = ? ? x and the variance V = x>?x.
CW is an online algorithm inspired by the Passive
Aggressive (PA) update (Crammer et al, 2006) ?
which ensures a positive margin while minimizing
parameter change. CW replaces the Euclidean dis-
tance used in the PA update with the KL divergence
over Gaussian distributions. It also replaces the min-
imal margin constraint with a minimal probability
constraint: with some given probability ? ? (0.5, 1]
a drawn classifier will assign the correct label. This
strategy yields the following objective solved on
each round of learning:
(?i+1,?i+1) = min DKL (N (?,?) ?N (?i,?i))
s.t. Pr [yi (? ? xi) ? 0] ? ? ,
where (?i,?i) are the parameters on round i and(
?i+1,?i+1
)
are the new parameters after update.
The constraint ensures that the resulting parameters
will correctly classify xi with probability at least ?.
For convenience we write ? = ??1 (?), where ? is
the cumulative function of the normal distribution.
The optimization problem above is not convex, but
a closed form approximation of its solution has the
following additive form: ?i+1 = ?i+?iyi?ixi and
??1i+1 = ?
?1
i + 2?i?xix
>
i for,
?i=
?(1+2?Mi)+
?
(1+2?Mi)
2?8? (Mi??Vi)
4?Vi
.
Each update changes the feature weights ?, and in-
creases confidence (variance ? always decreases).
3 Active Learning with Confidence
We consider pool based active learning. An active
learning algorithm is given a pool of unlabeled in-
stances U = {xi}ni=1, a learning algorithm A and a
set of labeled examples initially set to be L = ? . On
each round the active learner uses its selection crite-
ria to return a single instance xi to be labeled by an
annotator with yi ? {?1,+1} (for binary classifica-
tion). The instance and label are added to the labeled
set L ? L ? {(xi, yi)} and passed to the learning
algorithm A, which in turn generates a new model.
At the end of labeling the algorithm returns a classi-
fier trained on the final labeled set. Effective active
learning minimizes prediction error and the number
of labeled examples.
Most active learners for margin based algorithms
rely on the magnitude of the margin. Tong and
Koller (2001) motivate this approach by consider-
ing the half-space representation of the hypothesis
space for learning. They suggest three margin based
active learning methods: Simple margin, MaxMin
margin, and Ratio margin. In Simple margin, the al-
gorithm predicts an unsigned margin M for each in-
stance in U and returns for labeling the instance with
the smallest margin. The intuition is that instances
for which the classifier is uncertain (small margin)
provide the most information for learning. Active
learning based on PA algorithms runs in a similar
fashion but full SVM retraining on every round is
replaced with a single PA update using the new la-
beled example, greatly increasing learning speed.
Maintaining a distribution over prediction func-
tions makes the CW algorithm attractive for ac-
tive learning. Instead of using a geometrical
quantity (?margin?), it use a probabilistic quan-
tity and picks the example whose label is pre-
dicted with the lowest probability. Formally,
the margin criteria, x = argminz?U (w ? z),
is replaced with a probabilistic criteria x =
argminz?U |
(
Prw?N (?i,?i) [sign(w ? z) = 1]
)
? 12 | .
234
The selection criteria naturally captures the notion
that we should label the example with the highest
uncertainty. Interestingly, we can show (omitted due
to lack of space) that the probabilistic criteria can be
translated into a corrected geometrical criteria. In
practice, we can compute this normalized margin as
M? = M/
?
V . We call this selection criteria Active
Confident Learning (ACL).
4 Evaluation
To evaluate our active learning methods we used
a similar experimental setup to Tong and Koller
(2001). Each active learning algorithm was given
two labeled examples, one from each class, for ini-
tial training of a classifier, and remaining data as un-
labeled examples. On each round the algorithm se-
lected a single instance for which it was then given
the correct label. The algorithm updated the online
classifier and evaluated it on held out test data to
measure learning progress.
We selected four binary NLP datasets for evalu-
ation: 20 Newsgroups1 and Reuters (Lewis et al,
2004) (used by Tong and Koller) and sentiment clas-
sification (Blitzer et al, 2007) and spam (Bickel,
2006). For each dataset we extracted binary uni-
gram features and sentiment was prepared accord-
ing to Blitzer et al (2007). From 20 Newsgroups
we created 3 binary decision tasks to differentiate
between two similar labels from computers, sci-
ence and talk. We created 3 similar problems from
Reuters from insurance, business services and re-
tail distribution. Sentiment used 4 Amazon domains
(book, dvd, electronics, kitchen). Spam used the
three users from task A data. Each problem had
2000 instances except for 20 Newsgroups, which
used between 1850 and 1971 instances. This created
13 classification problems across four tasks.
Each active learning algorithm was evaluated us-
ing a PA (with slack variable c = 1) or CW classifier
(? = 1) using 10-fold cross validation. We eval-
uated several methods in the Simple margin frame-
work: PA Margin and CW Margin, which select ex-
amples with the smallest margin, and ACL. As a
baseline we included selecting a random instance.
We also evaluated CW and a PA classifier trained on
all training instances. Each method was evaluated by
1
http://people.csail.mit.edu/jrennie/20Newsgroups/
labeling up to 500 labels, about 25% of the training
data. The 10 runs on each dataset for each problem
appear in the left and middle panel of Fig. 1, which
show the test accuracy after each round of active
learning. Horizontal lines indicate CW (solid) and
PA (dashed) training on all instances. Legend num-
bers are accuracy after 500 labels. The left panel av-
erages results over 20 Newsgroups, and the middle
panel averages results over all 13 datasets.
To achieve 80% of the accuracy of training on all
data, a realistic goal for less than 100 labels, PA
Margin required 93% the number of labels of PA
Random, while CW Margin needed only 73% of
the labels of CW Random. By using fewer labels
compared to random selection baselines, CW Mar-
gin learns faster in the active learning setting as com-
pared with PA. Furthermore, adding confidence re-
duced labeling cost compared to margin alone. ACL
improved over CW Margin on every task and after
almost every round; it required 63% of the labels of
CW Random to reach the 80% mark.
We computed the fraction of labels CW Margin
and ACL required (compared to CW Random) to
achieve the 80% accuracy mark of training with all
data. The results are summarized in the right panel
of Fig. 1, where we plot one point per dataset. Points
above the diagonal-line demonstrate the superiority
of ACL over CW Margin. ACL required fewer la-
bels than CW margin twice as often as the opposite
occurred (8 vs 4). Note that CW Margin used more
labels than CW Random in three cases, while ACL
only once, and this one time only about a dozen la-
bels were needed. To conclude, not only does CW
Margin outperforms PA Margin for active-learning,
CW maintains additional valuable information (con-
fidence), which further improves performance.
5 Related Work
Active learning has been widely used for NLP tasks
such as part of speech tagging (Ringger et al, 2007),
parsing (Tang et al, 2002) and word sense disam-
biguation (Chan and Ng, 2007). Many methods rely
on entropy-based scores such as uncertainty sam-
pling (Lewis and Gale, 1994). Others use margin
based methods, such as Kim et al (2006), who com-
bined margin scores with corpus diversity, and Sas-
sano (2002), who considered SVM active learning
235
100 150 200 250 300 350 400 450 500Labels0.65
0.70
0.75
0.80
0.85
0.90
0.95
Test 
Accu
racy
20 Newsgroups
PA Random (82.53)CW Random (92.92)PA Margin (88.06)CW Margin (95.39)ACL (95.51) 100 150 200 250 300 350 400 450 500Labels
0.75
0.80
0.85
0.90
Test 
Accu
racy
All
PA Random (81.30)CW Random (86.67)PA Margin (83.99)CW Margin (88.61)ACL (88.79) 0.2 0.4 0.6 0.8 1.0 1.2 1.4ACL Labels0.2
0.4
0.6
0.8
1.0
1.2
1.4
CW M
argin
 Labe
ls
Reuters20 NewsgroupsSentimentSpam
Figure 1: Results averaged over 20 Newsgroups (left) and all datasets (center) showing test accuracy over active
learning rounds. The right panel shows the amount of labels needed by CW Margin and ACL to achieve 80% of the
accuracy of training on all data - each points refers to a different dataset.
for Japanese word segmentation. Our confidence
based approach can be used to improve these tasks.
Furthermore, margin methods can outperform prob-
abilistic methods; CW beats maximum entropy on
many NLP tasks (Dredze et al, 2008).
A theoretical analysis of margin based methods
selected labels that maximize the reduction of the
version space, the hypothesis set consistent with the
training data (Tong and Koller, 2001). Another ap-
proach selects instances that minimize the future er-
ror in probabilistic algorithms (Roy and McCallum,
2001). Since we consider an online learning algo-
rithm our techniques can be easily extended to on-
line active learning (Cesa-Bianchi et al, 2005; Das-
gupta et al, 2005; Sculley, 2007).
6 Conclusion
We have presented techniques for incorporating con-
fidence into the margin for active learning and have
shown that CW selects better examples than PA, a
popular online algorithm. This approach creates op-
portunities for new active learning frameworks that
depend on margin confidence.
References
S. Bickel. 2006. Ecml-pkdd discovery challenge
overview. In The Discovery Challenge Workshop.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL.
Nicolo` Cesa-Bianchi, Ga?bor Lugosi, and Gilles Stolt.
2005. Minimizing regret with label efficient predic-
tion. IEEE Tran. on Inf. Theory, 51(6), June.
Y. S. Chan and H. T. Ng. 2007. Domain adaptation with
active learning for word sense disambiguation. In As-
sociation for Computational Linguistics (ACL).
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
S. Dasgupta, A.T. Kalai, and C. Monteleoni. 2005. Anal-
ysis of perceptron-based active learning. In COLT.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.
S. Kim, Yu S., K. Kim, J-W Cha, and G.G. Lee. 2006.
Mmr-based active machine learning for bio named en-
tity recognition. In NAACL/HLT.
D. D. Lewis and W. A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In SIGIR.
D. D. Lewis, Y. Yand, T. Rose, and F. Li. 2004. Rcv1:
A new benchmark collection for text categorization re-
search. JMLR, 5:361?397.
E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In ACL Linguistic Anno-
tation Workshop.
N. Roy and A. McCallum. 2001. Toward optimal active
learning through sampling estimation of error reduc-
tion. In ICML.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for japanese
word segmentation. In ACL.
D. Sculley. 2007. Online active learning methods for fast
label-efficient spam filtering. In CEAS.
M. Tang, X. Luo, and S. Roukos. 2002. Active learning
for statistical natural language parsing. In ACL.
S. Tong and D. Koller. 2001. Supprt vector machine
active learning with applications to text classification.
JMLR.
236
BioNLP 2007: Biological, translational, and clinical language processing, pages 129?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Code Assignment to Medical Text
Koby Crammer and Mark Dredze and Kuzman Ganchev and Partha Pratim Talukdar
Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA
{crammer|mdredze|kuzman|partha}@seas.upenn.edu
Steven Carroll
Division of Oncology, The Children?s Hospital of Philadelphia, Philadelphia, PA
carroll@genome.chop.edu
Abstract
Code assignment is important for handling
large amounts of electronic medical data in
the modern hospital. However, only expert
annotators with extensive training can as-
sign codes. We present a system for the
assignment of ICD-9-CM clinical codes to
free text radiology reports. Our system as-
signs a code configuration, predicting one or
more codes for each document. We com-
bine three coding systems into a single learn-
ing system for higher accuracy. We compare
our system on a real world medical dataset
with both human annotators and other auto-
mated systems, achieving nearly the maxi-
mum score on the Computational Medicine
Center?s challenge.
1 Introduction
The modern hospital generates tremendous amounts
of data: medical records, lab reports, doctor notes,
and numerous other sources of information. As hos-
pitals move towards fully electronic record keeping,
the volume of this data only increases. While many
medical systems encourage the use of structured in-
formation, including assigning standardized codes,
most medical data, and often times the most impor-
tant information, is stored as unstructured text.
This daunting amount of medical text creates
exciting opportunities for applications of learning
methods, such as search, document classification,
data mining, information extraction, and relation ex-
traction (Shortliffe and Cimino, 2006). These ap-
plications have the potential for considerable bene-
fit to the medical community as they can leverage
information collected by hospitals and provide in-
centives for electronic record storage. Much of the
data generated by medical personnel is unused past
the clinical visit, often times because there is no way
to simply and quickly apply the wealth of informa-
tion. Medical NLP holds the promise of both greater
care for individual patients and enhanced knowledge
about health care.
In this work we explore the assignment of ICD-9-
CM codes to clinical reports. We focus on this prac-
tical problem since it is representative of the type
of task faced by medical personnel on a daily ba-
sis. Many hospitals organize and code documents
for later retrieval using different coding standards.
Often times, these standards are extremely complex
and only trained expert coders can properly perform
the task, making the process of coding documents
both expensive and unreliable since a coder must se-
lect from thousands of codes a small number for a
given report. An accurate automated system would
reduce costs, simplify the task for coders, and create
a greater consensus and standardization of hospital
data.
This paper addresses some of the challenges asso-
ciated with ICD-9-CM code assignment to clinical
free text, as well as general issues facing applica-
tions of NLP to medical text. We present our auto-
mated system for code assignment developed for the
Computational Medicine Center?s challenge. Our
approach uses several classification systems, each
with the goal of predicting the exact code configu-
ration for a medical report. We then use a learning
129
system to combine our predictions for superior per-
formance.
This paper is organized as follows. First, we ex-
plain our task and difficulties in detail. Next we de-
scribe our three automated systems and features. We
combine the three approaches to create a single su-
perior system. We evaluate our system on clinical
reports and show accuracy approaching human per-
formance and the challenge?s best score.
2 Task Overview
The health care system employs a large number of
categorization and classification systems to assist
data management for a variety of tasks, including
patient care, record storage and retrieval, statistical
analysis, insurance, and billing. One of these sys-
tems is the International Classification of Diseases,
Ninth Revision, Clinical Modification (ICD-9-CM)
which is the official system of assigning codes to di-
agnoses and procedures associated with hospital uti-
lization in the United States. 1 The coding system
is based on World Health Organization guidelines.
An ICD-9-CM code indicates a classification of a
disease, symptom, procedure, injury, or information
from the personal history. Codes are organized hier-
archically, where top level entries are general group-
ings (e.g. ?diseases of the respiratory system?) and
bottom level codes indicate specific symptoms or
diseases and their location (e.g. ?pneumonia in as-
pergillosis?). Each specific, low-level code consists
of 4 or 5 digits, with a decimal after the third. Higher
level codes typically include only 3 digits. Overall,
there are thousands of codes that cover a broad range
of medical conditions.
Codes are assigned to medical reports by doc-
tors, nurses and other trained experts based on com-
plex coding guidelines (National Center for Health
Statistics, 2006). A particular medical report can be
assigned any number of relevant codes. For exam-
ple, if a patient exhibits a cough, fever and wheez-
ing, all three codes should be assigned. In addi-
tion to finding appropriate codes for each condition,
complex rules guide code assignment. For exam-
ple, a diagnosis code should always be assigned if a
diagnosis is reached, a diagnosis code should never
1http://www.cdc.gov/nchs/about/otheract/
icd9/abticd9.htm
be assigned when the diagnosis is unclear, a symp-
tom should never be assigned when a diagnosis is
present, and the most specific code is preferred. This
means that codes that seem appropriate to a report
should be omitted in specific cases. For example,
a patient with hallucinations should be coded 780.1
(hallucinations) but for visual hallucinations, the
correct code is 368.16. The large number of codes
and complexity of assignment rules make this a diffi-
cult problem for humans (inter-annotator agreement
is low). Therefore, an automated system that sug-
gested or assigned codes could make medical data
more consistent.
These complexities make the problem difficult
for NLP systems. Consider the task as multi-class,
multi-label. For a given document, many codes may
seem appropriate but it may not be clear to the algo-
rithm how many to assign. Furthermore, the codes
are not independent and different labels can inter-
act to either increase or decrease the likelihood of
the other. Consider a report that says, ?patient re-
ports cough and fever.? The presence of the words
cough and fever indicate codes 786.2 (cough) and
780.6 (fever). However, if the report continues to
state that ?patient has pneumonia? then these codes
are dropped in favor of 486 (pneumonia). Further-
more, if the report then says ?verify clinically?, then
the diagnosis is uncertain and only codes 786.2 and
780.6 apply. Clearly, this is a challenging problem,
especially for an automated system.
2.1 Corpus
We built and evaluated our system in accordance
with the Computational Medicine Center?s (CMC)
2007 Medical Natural Language Processing Chal-
lenge.2 Since release of medical data must strictly
follow HIPAA standards, the challenge corpus un-
derwent extensive treatment for disambiguation,
anonymization, and careful scrubbing. A detailed
description of data preparation is found in Compu-
tational Medicine Center (2007). We describe the
corpus here to provide context for our task.
The training corpus is comprised of 978 radiolog-
ical reports taken from real medical records. A test
corpus contains 976 unlabeled documents. Radiol-
ogy reports have two text fields, clinical history and
2www.computationalmedicine.org/challenge
130
impression. The physician ordering the x-ray writes
the clinical history, which contains patient informa-
tion for the radiologist, including history and current
symptoms. Sometimes a guess as to the diagnosis
appears (?evaluate for asthma?). The descriptions
are sometimes whole sentences and other times sin-
gle words (?cough?). The radiologist writes the im-
pression to summarize his or her findings. It con-
tains a short analysis and often times a best guess as
to the diagnosis. At times this field is terse, (?pneu-
monia? or ?normal kidneys?) and at others it con-
tains an entire paragraph of text. Together, these two
fields are used to assign ICD-9-CM codes, which
justify a certain procedure, possibly for reimburse-
ment by the insurance company.
Only a small percentage of ICD-9-CM codes ap-
pear in the challenge. In total, the reports include 45
different codes arranged in 94 configurations (com-
binations). Some of these codes appear frequently,
while others are rare, appearing only a single time.
The test set is restricted so that each configuration
appears at least once in the training set, although
there is no further guarantee as to the test set?s distri-
bution over codes. Therefore, in addition to a large
number of codes, there is variability in the amount
of data for each code. Four codes have over 100
examples each and 24 codes have 10 or fewer doc-
uments, with 10 of these codes having only a single
document.
Since code annotation is a difficult task, each doc-
ument in the corpus was evaluated by three expert
annotators. A gold annotation was created by tak-
ing the majority of the annotators; if two of the three
annotators provided a code, that code is used in the
gold configuration. This approach means that a doc-
ument?s configuration may be a construction of mul-
tiple annotators and may not match any of the three
annotators exactly. Both the individual and the ma-
jority annotations are included with the training cor-
pus.
While others have attempted ICD-9 code classi-
fication, our task differs in two respects (Section 7
provides an overview of previous work). First, pre-
vious work has used discharge reports, which are
typically longer with more text fields. Second, while
most systems are evaluated as a recommendation
system, offering the top k codes and then scoring
recall at k, our task is to provide the exact configu-
ration. The CMC challenge evaluated systems using
an F1 score, so we are penalized if we suggest any
label that does not appear in the majority annotation.
To estimate task difficulty we measured the inter-
annotator score for the training set using the three
annotations provided. We scored two annotations
with the micro average F1, which weighs each code
assignment equally (see Section 5 for details on
evaluation metrics). If an annotator omitted a code
and included an extra code, he or she is penalized
with a false positive (omitting a code) and a false
negative (adding an extra code). We measured anno-
tators against each other; the average f-measure was
74.85 (standard deviation of .06). These scores were
low since annotators chose from an unrestricted set
of codes, many of which were not included in the fi-
nal majority annotation. However, these scores still
indicate the human accuracy for this task using an
unrestricted label set. 3
3 Code Assignment System
We developed three automated systems guided by
our above analysis. First, we designed a learning
system that used natural language features from the
official code descriptions and the text of each re-
port. It is general purpose and labels all 45 codes
and 94 configurations (labels). Second, we built a
rule based system that assigned codes based on the
overlap between the reports and code descriptions,
similar to how an annotator may search code de-
scriptions for appropriate labels. Finally, a special-
ized system aimed at the most common codes imple-
mented a policy that mimics the guidelines a medical
staffer would use to assign these codes.
3.1 Learning System
We begin with some notational definitions. In what
follows, x denotes the generic input document (ra-
diology report), Y denotes the set of possible label-
ings (code configurations) of x, and y?(x) the cor-
rect labeling of x. For each pair of document x
and labeling y ? Y , we compute a vector-valued
feature representation f(x, y). A linear model is
3We also measured each annotator with the majority codes,
taking the average score (87.48), and the best annotator with
the majority label (92.8). However, these numbers are highly
biased since the annotator influences the majority labeling. We
observe that our final system still exceeds the average score.
131
given by a weight vector w. Given this weight vec-
tor w, the score w ? f(x, y) ranks possible labelings
of x, and we denote by Yk,w(x) the set of k top
scoring labelings for x. For some structured prob-
lems, a factorization of f(x, y) is required to enable
a dynamic program for inference. For our problem,
we know all the possible configurations in advance
(there are 94 of them) so we can pick the highest
scoring y ? Y by trying them all. For each docu-
ment x and possible labeling y, we compute a score
using w and the feature representation f(x, y). The
top scoring y is output as the correct label. Section
3.1.1 describes our feature function f(x, y) while
Section 3.1.2 describes how we find a good weight
vector w.
3.1.1 Features
Problem representation is one of the most impor-
tant aspects of a learning system. In our case, this
is defined by the set of features f(x, y). Ideally we
would like a linear combination of our features to ex-
actly specify the true labeling of all the instances, but
we want to have a small total number of features so
that we can accurately estimate their values. We sep-
arate our features into two classes: label specific fea-
tures and transfer features. For simplicity, we index
features by their name. Label specific features are
only present for a single label. For example, a simple
class of label specific features is the conjunction of a
word in the document with an ICD-9-CM code in the
label. Thus, for each word we create 94 features, i.e.
the word conjoined with every label. These features
tend to be very powerful, since weights for them can
encode very specific information about the way doc-
tors talk about a disease, such as the feature ?con-
tains word pneumonia and label contains code 486?.
Unfortunately, the cost of this power is that there are
a large number of these features, making parameter
estimation difficult for rare labels. In contrast, trans-
fer features can be present in multiple labels. An
example of a transfer feature might be ?the impres-
sion contains all the words in the code descriptions
of the codes in this label?. Transfer features allow us
to generalize from one label to another by learning
things like ?if all the words of the label description
occur in the impression, then this label is likely? but
have the drawback that we cannot learn specific de-
tails about common labels. For example, we cannot
learn that the word ?pneumonia? in the impression
is negatively correlated with the code cough. The
inclusion of both label specific and transfer features
allows us to learn specificity where we have a large
number of examples and generality for rare codes.
Before feature extraction we normalized the re-
ports? text by converting it to lower case and by
replacing all numbers (and digit sequences) with a
single token ?NUM?. We also prepared a synonym
dictionary for a subset of the tokens and n-grams
present in the training data. The synonym dictionary
was based onMeSH4, the Medical Subject Headings
vocabulary, in which synonyms are listed as terms
under the same concept. All ngrams and tokens
in the training data which had mappings defined in
the synonym dictionary were then replaced by their
normalized token; e.g. all mentions of ?nocturnal
enuresis? or ?nighttime urinary incontinence? were
replaced by the token ?bedwetting?. Additionally,
we constructed descriptions for each code automati-
cally from the official ICD-9-CM code descriptions
in National Center for Health Statistics (2006). We
also created a mapping between code and code type
(diagnosis or symptom) using the guidelines.
Our system used the following features. The de-
scriptions of particular features are in quotes, while
schemes for constructing features are not.
? ?this configuration contains a disease code?,
?this configuration contains a symptom code?,
?this configuration contains an ambiguous
code? and ?this configuration contains both dis-
ease and symptom codes?.5
? With the exception of stop-words, all words of
the impression and history conjoined with each
label in the configuration; pairs of words con-
joined with each label; words conjoined with
pairs of labels. For example, ?the impression
contains ?pneumonia? and the label contains
codes 786.2 and 780.6?.
? A feature indicating when the history or im-
pression contains a complete code description
4www.nlm.nih.gov/mesh
5We included a feature for configurations that had both dis-
ease and symptom codes because they appeared in the training
data, even though coding guidelines prohibit these configura-
tions.
132
for the label; one for a word in common with
the code description for one of the codes in the
label; a common word conjoined with the pres-
ence of a negation word nearby (?no?, ?not?,
etc.); a word in common with a code descrip-
tion not present in the label. We applied similar
features using negative words associated with
each code.
? A feature indicating when a soft negation word
appears in the text (?probable?, ?possible?,
?suspected?, etc.) conjoined with words that
follow; the token length of a text field (?im-
pression length=3?); a conjunction of a feature
indicating a short text field with the words in
the field (?impression length=1 and ?pneumo-
nia? ?)
? A feature indicating each n-gram sequence that
appears in both the impression and clinical his-
tory; the conjunction of certain terms where
one appears in the history and the other in the
impression (e.g. ?cough in history and pneu-
monia in impression?).
3.1.2 Learning Technique
Using these feature representations, we now learn
a weight vector w that scores the correct labelings
of the data higher than incorrect labelings. We used
a k-best version of the MIRA algorithm (Crammer,
2004; McDonald et al, 2005). MIRA is an online
learning algorithm that for each training document
x updates the weight vector w according to the rule:
wnew = argmin
w
?w ? wold?
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y)
where L(y?(x), y) is a measure of the loss of label-
ing y with respect to the correct labeling y?(x). For
our experiments, we set k to 30 and iterated over the
training data 10 times. Two standard modifications
to this approach also helped. First, rather than using
just the final weight vector, we average all weight
vectors. This has a smoothing effect that improves
performance on most problems. The second modifi-
cation is the introduction of slack variables:
wnew = argmin
w
?w ? wold? + ?
?
i
?i
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y) ? ?i
?i ? {1 . . . k} : ?i ? 0.
We used a ? of 10?3 in our experiments.
The most straightforward loss function is the 0/1
loss, which is one if y does not equal y?(x) and zero
otherwise. Since we are evaluated based on the num-
ber of false negative and false positive ICD-9-CM
codes assigned to all the documents, we used a loss
that is the sum of the number of false positive and the
number of false negative labels that y assigns with
respect to y?(x).
Finally, we only used features that were possi-
ble for some labeling of the test data by using only
the test data to construct our feature alphabet. This
forced the learner to focus on hypotheses that could
be used at test time and resulted in a 1% increase in
F-measure in our final system on the test data.
3.2 Rule Based System
Since some of the configurations appear a small
number of times in our corpus (some only once),
we built a rule based system that requires no train-
ing. The system uses a description of the ICD-9-CM
codes and their types, similar to the list used by our
learning system (Section 3.1.1). The code descrip-
tions include between one and four short descrip-
tions, such as ?reactive airway disease?, ?asthma?,
and ?chronic obstructive pulmonary disease?. We
treat each of these descriptions as a bag of words.
For a given report, the system parses both the clini-
cal history and impression into sentences, using ?.?
as a sentence divider. Each sentence is the checked
to see if all of the words in a code description appear
in the sentence. If a match is found, we set a flag
corresponding to the code. However, if the code is
a disease, we search for a negation word in the sen-
tence, removing the flag if a negation word is found.
Once all code descriptions have been evaluated, we
check if there are any flags set for disease codes. If
so, we remove all symptom code flags. We then emit
a code corresponding to each set flag. This simple
system does not enforce configuration restrictions;
133
we may predict a code configuration that does not
appear in our training data. Adding this restriction
improved precision but hurt recall, leading to a slight
decrease in F1 score. We therefore omitted the re-
striction from our system.
3.3 Automatic Coding Policies
As we described in Section 2, enforcing coding
guidelines can be a complex task. While a learning
system may have trouble coding a document, a hu-
man may be able to define a simple policy for cod-
ing. Since some of the most frequent codes in our
dataset have this property, we decided to implement
such an automatic coding policy. We selected two
related sets of codes to target with a rule based sys-
tem, a set of codes found in pneumonia reports and
a set for urinary tract infection/reflux reports.
Reports related to pneumonia are the most com-
mon in our dataset and include codes for pneumo-
nia, asthma, fever, cough and wheezing; we handle
them with a single policy. Our policy is as follows:
? Search for a small set of keywords (e.g.
?cough?, ?fever?) to determine if a code should
be applied.
? If ?pneumonia? appears unnegated in the im-
pression and the impression is short, or if it oc-
curs in the clinical history and is not preceded
by phrases such as ?evaluate for? or ?history
of?, apply pneumonia code and stop.
? Use the same rule to code asthma by looking
for ?asthma? or ?reactive airway disease?.
? If no diagnosis is found, code all non-negated
symptoms (cough, fever, wheezing).
We selected 80% of the training set to evaluate in the
construction of our rules. We then ran the finished
system on both this training set and the held out 20%
of the data. The system achieved F1 scores of 87%
on the training set and 84% on the held out data for
these five codes. The comparable scores indicates
that we did not over-fit the training data.
We designed a similar policy for two other related
codes, urinary tract infection and vesicoureteral re-
flux. We found these codes to be more complex as
they included a wide range of kidney disorders. On
these two codes, our system achieved 78% on the
train set and 76% on the held out data. Overall, au-
tomatically applying our two policies yielded high
confidence predictions for a significant subset of the
corpus.
4 Combined System
Since our three systems take complimentary ap-
proaches to the problem, we combined them to im-
prove performance. First, we took our automatic
policy and rule based systems and cascaded them; if
the automatic policy system does not apply a code,
the rule based system classifies the report. We used
a cascaded approach since the automatic policy sys-
tem was very accurate when it was able to assign
a code. Therefore, the rule based system defers to
the policy system when it is triggered. Next, we in-
cluded the prediction of the cascaded system as a
feature for our learning system. We used two fea-
ture rules: ?cascaded-system predicted exactly this
label? and ?cascaded-system predicted one of the
codes in this label?. As we show, this yielded our
most accurate system. While we could have used a
meta-classifier to combine the three systems, includ-
ing the rule based systems as features to the learning
system allowed it to learn the appropriate weights
for the rule based predictions.
5 Evaluation Metric
Evaluation metrics for this task are often based on
recommendation systems, where the system returns
a list of the top k codes for selection by the user. As
a result, typical metrics are ?recall at k? and aver-
age precision (Larkey and Croft, 1995). Instead, our
goal was to predict the exact configuration, returning
exactly the number of codes predicted to be on the
report. The competition used a micro-averaged F1
score to evaluate predictions. A contingency table
(confusion matrix) is computed by summing over
each predicted code for each document by predic-
tion type (true positive, false positive, false negative)
weighing each code assignment equally. F1 score
is computed based on the resultant table. If specific
codes or under-coding is favored, we can modify our
learning loss function as described in Section 3.1.2.
A detailed treatment of this evaluation metric can be
found in Computational Medicine Center (2007).
134
System Precision Recall F1
BL 61.86 72.58 66.79
RULE 81.9 82.0 82.0
CASCADE 86.04 84.56 85.3
LEARN 85.5 83.6 84.6
CASCADE+LEARN 87.1 85.9 86.5
Table 1: Performance of our systems on the provided
labeled training data (F1 score). The learning sys-
tems (CASCADE+LEARN and LEARN ) were eval-
uated on ten random split of the data while RULE
was evaluated on all of the training data. We include
a simple rule based system (BL ) as a baseline.
6 Results
We evaluated our systems on the labeled training
data of 978 radiology reports. For each report, each
system predicted an exact configuration of codes
(i.e. one of 94 possible labels). We score each sys-
tem using a micro-averaged F1 score. Since we only
had labels for the training data, we divided the data
using an 80/20 training test split and averaged results
over 10 runs for our learning systems. We evaluated
the following systems:
? RULE : The rule based system based on ICD-
9-CM code descriptions (Section 3.2).
? CASCADE : The automatic code policy system
(Section 3.3) cascaded with RULE (Section 4).
? LEARN : The learning system with both label
specific and transfer features (Section 3.1).
? CASCADE+LEARN : Our combined system
that incorporates CASCADE predictions as a
feature to LEARN (Section 4).
For a baseline, we built a simple system that ap-
plies the official ICD-9-CM code descriptions to find
the correct labels (BL ). For each code in the train-
ing set, the system generates text-segments related to
it. During testing, for each new document, the sys-
tem checks if any text-segment (as discovered dur-
ing training) appears in the document. If so, the cor-
responding code is predicted. The results from our
four systems and baseline are shown in Table 1.
System Train Test
CASCADE 85.3 84
CASCADE+LEARN 86.5 87.60
Average - 76.6
Best - 89.08
Table 2: Performance of two systems on the train
and test data. Results obtained from the web sub-
mission interface were rounded. Average and Best
are the average and best f-measures of the 44 sub-
mitted systems (standard deviation 13.40).
Each of our systems easily beats the baseline, and
the average inter-annotator score for this task. Ad-
ditionally, we were able to evaluate two of our sys-
tems on the test data using a web interface as pro-
vided by the competition. The test set contains 976
documents (about the same as the training set) and
is drawn the from same distribution as the training
data. Our test results were comparable to perfor-
mance on the training data, showing that we did
not over-fit to the training data (Table 2). Addi-
tionally, our combined system (CASCADE+LEARN
) achieved a score of 87.60%, beating our training
data performance and exceeding the average inter-
annotator score. Out of 44 submitted systems, the
average score on test data was 76.7% (standard devi-
ation of 13.40) and the maximum score was 89.08%.
Our system scored 4th overall and was less than
1.5% behind the best system. Overall, in comparison
with our baselines and over 40 systems, we perform
very well on this task.
7 Related Work
There have been several attempts at ICD-9-CM
code classification and related problems for med-
ical records. The specific problem of ICD-9-CM
code assignment was studied by Lussier et al (2000)
through an exploratory study. Larkey and Croft
(1995) designed classifiers for the automatic assign-
ment of ICD-9 codes to discharge summaries. Dis-
charge summaries tend to be considerably longer
than our data and contain multiple text fields. Ad-
ditionally, the number of codes per document has
a larger range, varying between 1 and 15 codes.
Larkey and Croft use three classifiers: K-nearest
neighbors, relevance feedback, and bayesian inde-
135
pendence. Similar to our approach, they tag items
as negated and try to identify diagnosis and symp-
tom terms. Additionally, their final system combines
all three models. A direct comparison is not possi-
ble due to the difference in data and evaluation met-
rics; they use average precision and recall at k. On
a comparable metric, ?principal code is top candi-
date?, their best system achieves 59.9% accuracy. de
Lima et al (1998) rely on the hierarchical nature of
medical codes to design a hierarchical classification
scheme. This approach is likely to help on our task
as well but we were unable to test this since the lim-
ited number of codes removes any hierarchy. Other
approaches have used a variety of NLP techniques
(Satomura and Amaral, 1992).
Others have used natural language systems for the
analysis of medical records (Zweigenbaum, 1994).
Chapman and Haug (1999) studied radiology re-
ports looking for cases of pneumonia, a goal sim-
ilar to that of our automatic coding policy system.
Meystre and Haug (2005) processed medical records
to harvest potential entries for a medical problem
list, an important part of electronic medical records.
Chuang et al (2002) studied Charlson comorbidi-
ties derived from processing discharge reports and
chest x-ray reports and compared them with admin-
istrative data. Additionally, Friedman et al (1994)
applies NLP techniques to radiology reports.
8 Conclusion
We have presented a learning system that processes
radiology reports and assigns ICD-9-CM codes.
Each of our systems achieves results comparable
with an inter-annotator baseline for our training data.
A combined system improves over each individ-
ual system. Finally, we show that on test data un-
available during system development, our final sys-
tem continues to perform well, exceeding the inter-
annotator baseline and achieving the 4th best score
out of 44 systems entered in the CMC challenge.
9 Acknowledgements
We thank Andrew Lippa for his extensive medical
wisdom. Dredze is supported by an NDSEG fel-
lowship; Ganchev and Talukdar by NSF ITR EIA-
0205448; and Crammer by DARPA under Contract
No. NBCHD03001. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the DARPA or the De-
partment of Interior-National Business Center (DOI-
NBC).
References
W.W. Chapman and P.J. Haug. 1999. Comparing expert sys-
tems for identifying chest x-ray reports that support pneu-
monia. In AMIA Symposium, pages 216?20.
JH Chuang, C Friedman, and G Hripcsak. 2002. A com-
parison of the charlson comorbidities derived from medical
language processing and administrative data. AMIA Sympo-
sium, pages 160?4.
Computational Medicine Center. 2007. The
computational medicine center?s 2007 med-
ical natural language processing challenge.
http://computationalmedicine.org/challenge/index.php.
Koby Crammer. 2004. Online Learning of Complex Categorial
Problems. Ph.D. thesis, Hebrew Univeristy of Jerusalem.
Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A.
Ribeiro-Neto. 1998. A hierarchical approach to the auto-
matic categorization of medical documents. In CIKM.
C Friedman, PO Alderson, JH Austin, JJ Cimino, and SB John-
son. 1994. A general natural-language text processor for
clinical radiology. Journal of the American Medical Infor-
matics Association, 1:161?74.
Leah S. Larkey and W. Bruce Croft. 1995. Automatic assign-
ment of icd9 codes to discharge summaries. Technical re-
port, University of Massachusetts at Amherst, Amherst, MA.
YA Lussier, C Friedman, L Shagina, and P Eng. 2000. Au-
tomating icd-9-cm encoding using medical language pro-
cessing: A feasibility study.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Flexible text segmentation with structured multilabel classi-
fication. In HLT/EMNLP.
Stephane Meystre and Peter J Haug. 2005. Automation of a
problem list using natural language processing. BMC Medi-
cal Informatics and Decision Making.
National Center for Health Statistics. 2006. Icd-
9-cm official guidelines for coding and reporting.
http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.
Y Satomura and MB Amaral. 1992. Automated diagnostic in-
dexing by natural language processing. Medical Informat-
ics, 17:149?163.
Edward H. Shortliffe and James J. Cimino, editors. 2006.
Biomedical Informatics: Computer Applications in Health
Care and Biomedicine. Springer.
P. Zweigenbaum. 1994. Menelas: an access system for medical
records using natural language. Comput Methods Programs
Biomed, 45:117?20.
136
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Small Statistical Models by Random Feature Mixing
Kuzman Ganchev and Mark Dredze
Department of Computer and Information Science
University of Pennsylvania, Philadelphia, PA
{kuzman,mdredze}@cis.upenn.edu
Abstract
The application of statistical NLP systems to
resource constrained devices is limited by the
need to maintain parameters for a large num-
ber of features and an alphabet mapping fea-
tures to parameters. We introduce random
feature mixing to eliminate alphabet storage
and reduce the number of parameters without
severely impacting model performance.
1 Introduction
Statistical NLP learning systems are used for many
applications but have large memory requirements, a
serious problem for mobile platforms. Since NLP
applications use high dimensional models, a large
alphabet is required to map between features and
model parameters. Practically, this means storing
every observed feature string in memory, a pro-
hibitive cost for systems with constrained resources.
Offline feature selection is a possible solution, but
still requires an alphabet and eliminates the poten-
tial for learning new features after deployment, an
important property for adaptive e-mail or SMS pre-
diction and personalization tasks.
We propose a simple and effective approach to
eliminate the alphabet and reduce the problem of di-
mensionality through random feature mixing. We
explore this method on a variety of popular datasets
and classification algorithms. In addition to alpha-
bet elimination, this reduces model size by a factor
of 5?10 without a significant loss in performance.
2 Method
Linear models learn a weight vector over features
constructed from the input. Features are constructed
as strings (e.g. ?w=apple? interpreted as ?contains
the word apple?) and converted to feature indices
maintained by an alphabet, a map from strings to
integers. Instances are efficiently represented as a
sparse vector and the model as a dense weight vec-
tor. Since the alphabet stores a string for each fea-
ture, potentially each unigram or bigram it encoun-
ters, it is much larger than the weight vector.
Our idea is to replace the alphabet with a random
function from strings to integers between 0 and an
intended size. This size controls the number of pa-
rameters in our model. While features are now eas-
ily mapped to model parameters, multiple features
can collide and confuse learning. The collision rate
is controlled by the intended size. Excessive colli-
sions can make the learning problem more difficult,
but we show significant reductions are still possible
without harming learning. We emphasize that even
when using an extremely large feature space to avoid
collisions, alphabet storage is eliminated. For the
experiments in this paper we use Java?s hashCode
function modulo the intended size rather than a ran-
dom function.
3 Experiments
We evaluated the effect of random feature mix-
ing on four popular learning methods: Perceptron,
MIRA (Crammer et al, 2006), SVM and Maximum
entropy; with 4 NLP datasets: 20 Newsgroups1,
Reuters (Lewis et al, 2004), Sentiment (Blitzer
et al, 2007) and Spam (Bickel, 2006). For each
dataset we extracted binary unigram features and
sentiment was prepared according to Blitzer et al
(2007). From 20 Newsgroups we created 3 binary
decision tasks to differentiate between two similar
1
http://people.csail.mit.edu/jrennie/20Newsgroups/
19
 70 75 80 85 90  0 10 2
0 30 40 50
 60 70 80 9
0
thousands of
 featuresfeature mixi
ng
no feature m
ixing
 70 75 80 85 90  0 10 2
0 30 40 50
 60 70 80 9
0
thousands of
 featuresfeature mixi
ng
no feature m
ixing
Figure 1: Kitchen appliance reviews. Left: Maximum en-
tropy. Right: Perceptron. Shaded area and vertical lines
extend one standard deviation from the mean.
labels from computers, science and talk. We cre-
ated 3 similar problems from Reuters from insur-
ance, business services and retail distribution. Senti-
ment used 4 Amazon domains (book, dvd, electron-
ics, kitchen). Spam used the three users from task
A data. Each problem had 2000 instances except for
20 Newsgroups, which used between 1850 and 1971
instances. This created 13 binary classification prob-
lems across four tasks. Each model was evaluated
on all problems using 10-fold cross validation and
parameter optimization. Experiments varied model
size to observe the effect of feature collisions on per-
formance.
Results for sentiment classification of kitchen ap-
pliance reviews (figure 1) are typical. The original
model has roughly 93.6k features and its alphabet
requires 1.3MB of storage. Assuming 4-byte float-
ing point numbers the weight vector needs under
0.37MB. Consequently our method reduces storage
by over 78% when we keep the number of param-
eters constant. A further reduction by a factor of 2
decreases accuracy by only 2%.
Figure 2 shows the results of all experiments
for SVM and MIRA. Each curve shows normalized
dataset performance relative to the full model as the
percentage of original features decrease. The shaded
rectangle extends one standard deviation above and
 1.02  1  0.98  0.96  0.94  0
 0.5 1
 1.5 2
Relative # fe
atures
 1.02  1  0.98  0.96  0.94  0
 0.5 1
 1.5 2
Relative # fe
atures
Figure 2: Relative performance on all datasets for SVM
(left) and MIRA (right).
 76 78 80 82 84 86 88  0 2 
4 6 8 1
0 12 14 16
thousands of
 featuresfeature mixi
ng
no feature m
ixing
 76 78 80 82 84 86 88  0 2 
4 6 8 1
0 12 14 16
thousands of
 featuresfeature mixi
ng
no feature m
ixing
Figure 3: The anomalous Reuters dataset from figure 2
for Perceptron (left) and MIRA (right).
below full model performance. Almost all datasets
perform within one standard deviation of the full
model when using feature mixing set to the total
number of features for the problem, indicating that
alphabet elimination is possible without hurting per-
formance. One dataset (Reuters retail distribution) is
a notable exception and is illustrated in detail in fig-
ure 3. We believe the small total number of features
used for this problem is the source of this behavior.
On the vast majority of datasets, our method can re-
duce the size of the weight vector and eliminate the
alphabet without any feature selection or changes to
the learning algorithm. When reducing weight vec-
tor size by a factor of 10, we still obtain between
96.7% and 97.4% of the performance of the original
model, depending on the learning algorithm. If we
eliminate the alphabet but keep the same size weight
vector, model the performance is between 99.3%
of the original for MIRA and a slight improvement
for Perceptron. The batch learning methods are be-
tween those two extremes at 99.4 and 99.5 for max-
imum entropy and SVM respectively. Feature mix-
ing yields substantial reductions in memory require-
ments with a minimal performance loss, a promising
result for resource constrained devices.
References
S. Bickel. 2006. Ecml-pkdd discovery challenge
overview. In The Discovery Challenge Workshop.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Ressearch, 7.
D. D. Lewis, Y. Yand, T. Rose, and F. Li. 2004. Rcv1:
A new benchmark collection for text categorization re-
search. JMLR, 5:361?397.
20
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 496?504,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Multi-Class Confidence Weighted Algorithms
Koby Crammer
?
?
Department of Computer
and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{crammer,kulesza}@cis.upenn.edu
Mark Dredze
?
Alex Kulesza
?
?
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
mdredze@cs.jhu.edu
Abstract
The recently introduced online
confidence-weighted (CW) learning
algorithm for binary classification per-
forms well on many binary NLP tasks.
However, for multi-class problems CW
learning updates and inference cannot
be computed analytically or solved as
convex optimization problems as they are
in the binary case. We derive learning
algorithms for the multi-class CW setting
and provide extensive evaluation using
nine NLP datasets, including three derived
from the recently released New York
Times corpus. Our best algorithm out-
performs state-of-the-art online and batch
methods on eight of the nine tasks. We
also show that the confidence information
maintained during learning yields useful
probabilistic information at test time.
1 Introduction
Online learning algorithms such as the Perceptron
process one example at a time, yielding simple and
fast updates. They generally make few statisti-
cal assumptions about the data and are often used
for natural language problems, where high dimen-
sional feature representations, e.g., bags-of-words,
demand efficiency. Most online algorithms, how-
ever, do not take into account the unique properties
of such data, where many features are extremely
rare and a few are very frequent.
Dredze, Crammer and Pereira (Dredze et al,
2008; Crammer et al, 2008) recently introduced
confidence weighted (CW) online learning for bi-
nary prediction problems. CW learning explicitly
models classifier weight uncertainty using a multi-
variate Gaussian distribution over weight vectors.
The learner makes online updates based on its con-
fidence in the current parameters, making larger
changes in the weights of infrequently observed
features. Empirical evaluation has demonstrated
the advantages of this approach for a number of bi-
nary natural language processing (NLP) problems.
In this work, we develop and test multi-class
confidence weighted online learning algorithms.
For binary problems, the update rule is a sim-
ple convex optimization problem and inference
is analytically computable. However, neither is
true in the multi-class setting. We discuss sev-
eral efficient online learning updates. These up-
date rules can involve one, some, or all of the
competing (incorrect) labels. We then perform an
extensive evaluation of our algorithms using nine
multi-class NLP classification problems, includ-
ing three derived from the recently released New
York Times corpus (Sandhaus, 2008). To the best
of our knowledge, this is the first learning evalua-
tion on these data. Our best algorithm outperforms
state-of-the-art online algorithms and batch algo-
rithms on eight of the nine datasets.
Surprisingly, we find that a simple algorithm in
which updates consider only a single competing
label often performs as well as or better than multi-
constraint variants if it makes multiple passes over
the data. This is especially promising for large
datasets, where the efficiency of the update can
be important. In the true online setting, where
only one iteration is possible, multi-constraint al-
gorithms yield better performance.
Finally, we demonstrate that the label distribu-
tions induced by the Gaussian parameter distribu-
tions resulting from our methods have interesting
properties, such as higher entropy, compared to
those from maximum entropy models. Improved
label distributions may be useful in a variety of
learning settings.
2 Problem Setting
In the multi-class setting, instances from an input
space X take labels from a finite set Y , |Y| = K.
496
We use a standard approach (Collins, 2002) for
generalizing binary classification and assume a
feature function f(x, y) ? R
d
mapping instances
x ? X and labels y ? Y into a common space.
We work in the online framework, where learn-
ing is performed in rounds. On each round the
learner receives an input x
i
, makes a prediction y?
i
according to its current rule, and then learns the
true label y
i
. The learner uses the new example
(x
i
, y
i
) to modify its prediction rule. Its goal is to
minimize the total number of rounds with incor-
rect predictions, |{i : y
i
6= y?
i
}|.
In this work we focus on linear models parame-
terized by weightsw and utilizing prediction func-
tions of the form h
w
(x) = arg max
z
w ? f(x, z).
Note that since we can choose f(x, y) to be the
vectorized Cartesian product of an input feature
function g(x) and y, this setup generalizes the use
of unique weight vectors for each element of Y .
3 Confidence Weighted Learning
Dredze, Crammer, and Pereira (2008) introduced
online confidence weighted (CW) learning for bi-
nary classification, where X = R
d
and Y =
{?1}. Rather than using a single parameter vec-
tor w, CW maintains a distribution over param-
eters N (?,?), where N (?,?) the multivariate
normal distribution with mean ? ? R
d
and co-
variance matrix ? ? R
d?d
. Given an input in-
stance x, a Gibbs classifier draws a weight vector
w from the distribution and then makes a predic-
tion according to the sign of w ? x.
This prediction rule is robust if the example
is classified correctly with high-probability, that
is, for some confidence parameter .5 ? ? < 1,
Pr
w
[y (w ? x) ? 0] ? ?. To learn a binary CW
classifier in the online framework, the robustness
property is enforced at each iteration while mak-
ing a minimal update to the parameter distribution
in the KL sense:
(?
i+1
,?
i+1
) =
arg min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr
w
[y
i
(w ? x
i
) ? 0] ? ? (1)
Dredze et al (2008) showed that this optimization
can be solved in closed form, yielding the updates
?
i+1
= ?
i
+ ?
i
?
i
x
i
(2)
?
i+1
=
(
?
?1
i
+ ?
i
x
i
x
T
i
)
?1
(3)
for appropriate ?
i
and ?
i
.
For prediction, they use the Bayesian rule
y? = arg max
z?{?1}
Pr
w?N (?,?)
[z (x ?w) ? 0] ,
which for binary labels is equivalent to using the
mean parameters directly, y? = sign (? ? x).
4 Multi-Class Confidence Weighted
Learning
As in the binary case, we maintain a distribution
over weight vectors w ? N (?,?). Given an in-
put instance x, a Gibbs classifier draws a weight
vector w ? N (?,?) and then predicts the label
with the maximal score, arg max
z
(w ? f(x, z)).
As in the binary case, we use this prediction rule
to define a robustness condition and corresponding
learning updates.
We generalize the robustness condition used in
Crammer et al (2008). Following the update on
round i, we require that the ith instance is correctly
labeled with probability at least ? < 1. Among the
distributions that satisfy this condition, we choose
the one that has the minimal KL distance from the
current distribution. This yields the update
(?
i+1
,?
i+1
) = (4)
arg min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr [y
i
|x
i
,?,?] ? ? ,
where
Pr [y |x,?,?] =
Pr
w?N (?,?)
[
y = arg max
z?Y
(w ? f(x, z))
]
.
Due to the max operator in the constraint, this op-
timization is not convex when K > 2, and it does
not permit a closed form solution. We therefore
develop approximations that can be solved effi-
ciently. We define the following set of events for a
general input x:
A
r,s
(x)
def
= {w : w ? f(x, r) ? w ? f(x, s)}
B
r
(x)
def
= {w : w ? f(x, r) ? w ? f(x, s) ?s}
=
?
s 6=r
A
r,s
(x)
We assume the probability that w ? f(x, r) =
w ? f(x, s) for some s 6= r is zero, which
497
holds for non-trivial distribution parameters and
feature vectors. We rewrite the prediction y? =
arg max
r
Pr [B
r
(x)], and the constraint from
Eq. (4) becomes
Pr [B
y
i
(x)] ? ? . (5)
We focus now on approximating the event B
y
i
(x)
in terms of events A
y
i
,r
. We rely on the fact that
the level sets of Pr [A
y
i
,r
] are convex in ? and
?. This leads to convex constraints of the form
Pr [A
y
i
,r
] ? ?.
Outer Bound: Since B
r
(x) ? A
r,s
(x), it holds
trivially that Pr [B
y
i
(x)] ? ? ? Pr [A
y
i
,r
] ?
?,?r 6= y
i
. Thus we can replace the constraint
Pr [B
y
i
(x)] ? ? with Pr [A
y
i
,r
] ? ? to achieve an
outer bound. We can simultaneously apply all of
the pairwise constraints to achieve a tighter bound:
Pr [A
y
i
,r
] ? ? ?r 6= y
i
This yields a convex approximation to Eq. (4) that
may improve the objective value at the cost of
violating the constraint. In the context of learn-
ing, this means that the new parameter distribu-
tion will be close to the previous one, but may not
achieve the desired confidence on the current ex-
ample. This makes the updates more conservative.
Inner Bound: We can also consider an inner
bound. Note that B
y
i
(x)
c
= (?
r
A
y
i
,r
(x))
c
=
?
r
A
y
i
,r
(x)
c
, thus the constraint Pr [B
y
i
(x)] ? ?
is equivalent to
Pr [?
r
A
y
i
,r
(x)
c
] ? 1? ? ,
and by the union bound, this follows whenever
?
r
Pr [A
y
i
,r
(x)
c
] ? 1? ? .
We can achieve this by choosing non-negative
?
r
? 0,
?
r
?
r
= 1, and constraining
Pr [A
y
i
,r
(x)] ? 1? (1? ?) ?
r
for r 6= y
i
.
This formulation yields an inner bound on the
original constraint, guaranteeing its satisfaction
while possibly increasing the objective. In the
context of learning, this is a more aggressive up-
date, ensuring that the current example is robustly
classified even if doing so requires a larger change
to the parameter distribution.
Algorithm 1 Multi-Class CW Online Algorithm
Input: Confidence parameter ?
Feature function f(x, y) ? R
d
Initialize: ?
1
= 0 , ?
1
= I
for i = 1, 2 . . . do
Receive x
i
? X
Predict ranking of labels y?
1
, y?
2
, . . .
Receive y
i
? Y
Set ?
i+1
,?
i+1
by approximately solving
Eq. (4) using one of the following:
Single-constraint update (Sec. 5.1)
Exact many-constraint update (Sec. 5.2)
Seq. many-constraint approx. (Sec. 5.2)
Parallel many-constraint approx. (Sec. 5.2)
end for
Output: Final ? and ?
Discussion: The two approximations are quite
similar in form. Both replace the constraint
Pr [B
y
i
(x)] ? ? with one or more constraints of
the form
Pr [A
y
i
,r
(x)] ? ?
r
. (6)
To achieve an outer bound we choose ?
r
= ? for
any set of r 6= y
i
. To achieve an inner bound we
use all K ? 1 possible constraints, setting ?
r
=
1 ? (1? ?) ?
r
for suitable ?
r
. A simple choice is
?
r
= 1/(K ? 1).
In practice, ? is a learning parameter whose
value will be optimized for each task. In this case,
the outer bound (when all constraints are included)
and inner bound (when ?
r
= 1/(K ? 1)) can be
seen as equivalent, since for any fixed value of
?
(in)
for the inner bound we can choose
?
(out)
= 1?
1? ?
(in)
K ? 1
,
for the outer bound and the resulting ?
r
will be
equal. By optimizing ? we automatically tune the
approximation to achieve the best compromise be-
tween the inner and outer bounds. In the follow-
ing, we will therefore assume ?
r
= ?.
5 Online Updates
Our algorithms are online and process examples
one at a time. Pseudo-code for our approach is
given in algorithm 1. We approximate the pre-
diction step by ranking each label y according
to the score given by the mean weight vector,
? ? f(x
i
, y). Although this approach is Bayes op-
timal for binary problems (Dredze et al, 2008),
498
it is an approximation in general. We note that
more accurate inference can be performed in the
multi-class case by sampling weight vectors from
the distribution N (?,?) or selecting labels sen-
sitive to the variance of prediction; however, in
our experiments this did not improve performance
and required significantly more computation. We
therefore proceed with this simple and effective
approximation.
The update rule is given by an approximation
of the type described in Sec. 4. All that remains
is to choose the constraint set and solve the opti-
mization efficiently. We discuss several schemes
for minimizing KL divergence subject to one or
more constraints of the form Pr [A
y
i
,r
(x)] ? ?.
We start with a single constraint.
5.1 Single-Constraint Updates
The simplest approach is to select the single con-
straint Pr [A
y
i
,r
(x)] ? ? corresponding to the
highest-ranking label r 6= y
i
. This ensures that,
following the update, the true label is more likely
to be predicted than the label that was its closest
competitor. We refer to this as the k = 1 update.
Whenever we have only a single constraint, we
can reduce the optimization to one of the closed-
form CW updates used for binary classification.
Several have been proposed, based on linear ap-
proximations (Dredze et al, 2008) and exact for-
mulations (Crammer et al, 2008). For simplicity,
we use the Variance method from Dredze et al
(2008), which did well in our initial evaluations.
This method leads to the following update rules.
Note that in practice ? is projected to a diagonal
matrix as part of the update; this is necessary due
to the large number of features that we use.
?
i+1
= ?
i
+ ?
i
?
i
g
i,y
i
,r
(7)
?
i+1
=
(
?
?1
i
+ 2?
i
?g
i,y
i
,r
g
>
i,y
i
,r
)
?1
(8)
g
i,y
i
,r
= f(x
i
, y
i
)? f (x
i
, r) ? = ?
?1
(?)
The scale ?
i
is given by max(?
i
, 0), where ?
i
is
equal to
?(1 + 2?m
i
) +
?
(1 + 2?m
i
)
2
? 8?(m
i
? ?v
i
)
4?v
i
and
m
i
= ?
i
? g
i,y
i
,r
v
i
= g
>
i,y
i
,r
?
i
g
i,y
i
,r
.
These rules derive directly from Dredze et al
(2008) or Figure 1 in Crammer et al (2008); we
simply substitute y
i
= 1 and x
i
= g
i,y
i
,r
.
5.2 Many-Constraints Updates
A more accurate approximation can be obtained
by selecting multiple constraints. Analogously, we
choose the k ? K?1 constraints corresponding to
the labels r
1
, . . . , r
k
6= y
i
that achieve the highest
predicted ranks. The resulting optimization is con-
vex and can be solved by a standard Hildreth-like
algorithm (Censor & Zenios, 1997). We refer to
this update as Exact. However, Exact is expen-
sive to compute, and tends to over-fit in practice
(Sec. 6.2). We propose several approximate alter-
natives.
Sequential Update: The Hildreth algorithm it-
erates over the constraints, updating with respect
to each until convergence is reached. We approxi-
mate this solution by making only a single pass:
? Set ?
i,0
= ?
i
and ?
i,0
= ?
i
.
? For j = 1, . . . , k, set (?
i,j
,?
i,j
) to the solu-
tion of the following optimization:
min
?,?
D
KL
(
N (?,?) ?N
(
?
i,j?1
,?
i,j?1
))
s.t. Pr
[
A
y
i
,r
j
(x)
]
? ?
? Set ?
i+1
= ?
i,k
and ?
i+1
= ?
i,k
.
Parallel Update: As an alternative to the Hil-
dreth algorithm, we consider the simultaneous al-
gorithm of Iusem and Pierro (1987), which finds
an exact solution by iterating over the constraints
in parallel. As above, we approximate the exact
solution by performing only one iteration. The
process is as follows.
? For j = 1, . . . , k, set (?
i,j
,?
i,j
) to the solu-
tion of the following optimization:
min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr
[
A
y
i
,r
j
(x)
]
? ?
? Let ? be a vector, ?
j
?0 ,
?
j
?
j
=1.
? Set ?
i+1
=
?
j
?
j
?
i,j
, ?
?1
i+1
=
?
j
?
j
?
?1
i,j
.
In practice we set ?
j
= 1/k for all j.
6 Experiments
6.1 Datasets
Following the approach of Dredze et al (2008),
we evaluate using five natural language classifica-
tion tasks over nine datasets that vary in difficulty,
size, and label/feature counts. See Table 1 for an
overview. Brief descriptions follow.
499
Task Instances Features Labels Bal.
20 News 18,828 252,115 20 Y
Amazon 7 13,580 686,724 7 Y
Amazon 3 7,000 494,481 3 Y
Enron A 3,000 13,559 10 N
Enron B 3,000 18,065 10 N
NYTD 10,000 108,671 26 N
NYTO 10,000 108,671 34 N
NYTS 10,000 114,316 20 N
Reuters 4,000 23,699 4 N
Table 1: A summary of the nine datasets, includ-
ing the number of instances, features, and labels,
and whether the numbers of examples in each class
are balanced.
Amazon Amazon product reviews. Using the
data of Dredze et al (2008), we created two do-
main classification datasets from seven product
types (apparel, books, dvds, electronics, kitchen,
music, video). Amazon 7 includes all seven prod-
uct types and Amazon 3 includes books, dvds, and
music. Feature extraction follows Blitzer et al
(2007) (bigram features and counts).
20 Newsgroups Approximately 20,000 news-
group messages, partitioned across 20 different
newsgroups.
1
This dataset is a popular choice for
binary and multi-class text classification as well as
unsupervised clustering. We represent each mes-
sage as a binary bag-of-words.
Enron Automatic sorting of emails into fold-
ers.
2
We selected two users with many email
folders and messages: farmer-d (Enron A) and
kaminski-v (Enron B). We used the ten largest
folders for each user, excluding non-archival email
folders such as ?inbox,? ?deleted items,? and ?dis-
cussion threads.? Emails were represented as bi-
nary bags-of-words with stop-words removed.
NY Times To the best of our knowledge we are
the first to evaluate machine learning methods on
the New York Times corpus. The corpus con-
tains 1.8 million articles that appeared from 1987
to 2007 (Sandhaus, 2008). In addition to being
one of the largest collections of raw news text,
it is possibly the largest collection of publicly re-
leased annotated news text, and therefore an ideal
corpus for large scale NLP tasks. Among other
annotations, each article is labeled with the desk
that produced the story (Financial, Sports, etc.)
(NYTD), the online section to which the article was
1
http://people.csail.mit.edu/jrennie/20Newsgroups/
2
http://www.cs.cmu.edu/?enron/
Task Sequential Parallel Exact
20 News 92.16 91.41 88.08
Amazon 7 77.98 78.35 77.92
Amazon 3 93.54 93.81 93.00
Enron A 82.40 81.30 77.07
Enron B 71.80 72.13 68.00
NYTD 83.43 81.43 80.92
NYTO 82.02 78.67 80.60
NYTS 52.96 54.78 51.62
Reuters 93.60 93.97 93.47
Table 2: A comparison of k = ? updates. While
the two approximations (sequential and parallel)
are roughly the same, the exact solution over-fits.
posted (NYTO), and the section in which the arti-
cle was printed (NYTS). Articles were represented
as bags-of-words with feature counts (stop-words
removed).
Reuters Over 800,000 manually categorized
newswire stories (RCV1-v2/ LYRL2004). Each
article contains one or more labels describing its
general topic, industry, and region. We performed
topic classification with the four general topics:
corporate, economic, government, and markets.
Details on document preparation and feature ex-
traction are given by Lewis et al (2004).
6.2 Evaluations
We first set out to compare the three update ap-
proaches proposed in Sec. 5.2: an exact solution
and two approximations (sequential and parallel).
Results (Table 2) show that the two approxima-
tions perform similarly. For every experiment the
CW parameter ? and the number of iterations (up
to 10) were optimized using a single randomized
iteration. However, sequential converges faster,
needing an average of 4.33 iterations compared to
7.56 for parallel across all datasets. Therefore, we
select sequential for our subsequent experiments.
The exact method performs poorly, displaying
the lowest performance on almost every dataset.
This is unsurprising given similar results for bi-
nary CW learning Dredze et al (2008), where ex-
act updates were shown to over-fit but converged
after a single iteration of training. Similarly, our
exact implementation converges after an average
of 1.25 iterations, much faster than either of the
approximations. However, this rapid convergence
appears to come at the expense of accuracy. Fig. 1
shows the accuracy on Amazon 7 test data after
each training iteration. While both sequential and
parallel improve with several iterations, exact de-
500
1 2 3 4 5Training Iterations
77.0
77.5
78.0
78.5
Tes
t Ac
cura
cy
K=1Sequential K=5Sequential K=AllParallel K=AllExact K=All
Figure 1: Accuracy on test data after each iteration
on the Amazon 7 dataset.
grades after the first iteration, suggesting that it
may over-fit to the training data. The approxima-
tions appear to smooth learning and produce better
performance in the long run.
6.3 Relaxing Many-Constraints
While enforcing many constraints may seem op-
timal, there are advantages to pruning the con-
straints as well. It may be time consuming to en-
force dozens or hundreds of constraints for tasks
with many labels. Structured prediction tasks of-
ten involve exponentially many constraints, mak-
ing pruning mandatory. Furthermore, many real
world datasets, especially in NLP, are noisy, and
enforcing too many constraints can lead to over-
fitting. Therefore, we consider the impact of re-
ducing the constraint set in terms of both reducing
run-time and improving accuracy.
We compared using all constraints (k = ?)
with using 5 constraints (k = 5) for the sequential
update method (Table 3). First, we observe that
k = 5 performs better than k =? on nearly every
dataset: fewer constraints help avoid over-fitting
and once again, simpler is better. Additionally,
k = 5 converges faster than k = ? in an average
of 2.22 iterations compared with 4.33 iterations.
Therefore, reducing the number of constraints im-
proves both speed and accuracy. In comparing
k = 5 with the further reduced k = 1 results, we
observe the latter improves on seven of the nine
methods. This surprising result suggests that CW
learning can perform well even without consid-
ering more than a single constraint per example.
However, k = 1 exceeds the performance of mul-
tiple constraints only through repeated training it-
erations. k = 5 CW learning converges faster ?
2.22 iterations compared with 6.67 for k = 1 ? a
desirable property in many resource restricted set-
tings. (In the true online setting, only a single it-
eration may be possible.) Fig. 1 plots the perfor-
mance of k = 1 and k = 5 CW on test data after
each training iteration. While k = 1 does better
in the long run, it lags behind k = 5 for several
iterations. In fact, after a single training iteration,
k = 5 outperforms k = 1 on eight out of nine
datasets. Thus, there is again a tradeoff between
faster convergence (k = 5) and increased accuracy
(k = 1). While the k = 5 update takes longer per
iteration, the time required for the approximate so-
lutions grows only linearly in the number of con-
straints. The evaluation in Fig. 1 required 3 sec-
onds for the first iteration of k = 1, 10 seconds
for k = 5 and 11 seconds for one iteration of all
7 constraints. These differences are insignificant
compared to the cost of performing multiple itera-
tions over a large dataset. We note that, while both
approximate methods took about the same amount
of time, the exact solution took over 4 minutes for
its first iteration.
Finally, we compare CW methods with sev-
eral baselines in Table 3. Online baselines in-
clude Top-1 Perceptron (Collins, 2002), Top-1
Passive-Aggressive (PA), and k-best PA (Cram-
mer & Singer, 2003; McDonald et al, 2004).
Batch algorithms include Maximum Entropy (de-
fault configuration in McCallum (2002)) and sup-
port vector machines (LibSVM (Chang & Lin,
2001) for one-against-one classification and multi-
class (MC) (Crammer & Singer, 2001)). Classifier
parameters (C for PA/SVM and maxent?s Gaus-
sian prior) and number of iterations (up to 10) for
the online methods were optimized using a sin-
gle randomized iteration. On eight of the nine
datasets, CW improves over all baselines. In gen-
eral, CW provides faster and more accurate multi-
class predictions.
7 Error and Probabilistic Output
Our focus so far has been on accuracy and speed.
However, there are other important considerations
for selecting learning algorithms. Maximum en-
tropy and other probabilistic classification algo-
rithms are sometimes favored for their probabil-
ity scores, which can be useful for integration
with other learning systems. However, practition-
501
PA CW SVM
Task Perceptron K=1 K=5 K=1 K=5 K=? 1 vs. 1 MC Maxent
20 News 81.07 88.59 88.60 ??92.90 ??92.78 ??92.16 85.18 90.33 88.94
Amazon 7 74.93 76.55 76.72 ??78.70 ??78.04 ??77.98 75.11 76.60 76.40
Amazon 3 92.26 92.47 93.29 ?94.01 ??94.29 93.54 92.83 93.60 93.60
Enron A 74.23 79.27 80.77 ??83.83 ?82.23 ?82.40 80.23 82.60 82.80
Enron B 66.30 69.93 68.90 ??73.57 ??72.27 ??71.80 65.97 71.87 69.47
NYTD 80.67 83.12 81.31 ??84.57 ?83.94 83.43 82.95 82.00 83.54
NYTO 78.47 81.93 81.22 ?82.72 ?82.55 82.02 82.13 81.01 82.53
NYTS 50.80 56.19 55.04 54.67 54.26 52.96 55.81 56.74 53.82
Reuters 92.10 93.12 93.30 93.60 93.67 93.60 92.97 93.32 93.40
Table 3: A comparison of CW learning (k = 1, 5,? with sequential updates) with several baseline
algorithms. CW learning achieves the best performance eight out of nine times. Statistical significance
(McNemar) is measured against all baselines (? indicates 0.05 and ?? 0.001) or against online baselines
(? indicates 0.05 and ?? 0.001).
0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.7528
29
30
31
32
33
entropy
error
MC CWMaxEnt
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
200400
600800
10001200
Bin lower threshold
Number
 of exam
ples per
 bin
MaxEntMC CW
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
2
4
68
1012
Bin lower threshold
Test err
or in bin
MaxEntMC CW
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
0.10.2
0.30.4
0.50.6
0.70.8
Bin lower threshold
Test err
or given
 bin
MaxEntMC CW
Figure 2: First panel: Error versus prediction entropy on Enron B. As CW converges (right to left) error
and entropy are reduced. Second panel: Number of test examples per prediction probability bin. The
red bars correspond to maxent and the blue bars to CW, with increasing numbers of epochs from left
to right. Third panel: The contribution of each bin to the total test error. Fourth panel: Test error
conditioned on prediction probability.
ers have observed that maxent probabilities can
have low entropy and be unreliable for estimating
prediction confidence (Malkin & Bilmes, 2008).
Since CW also produces label probabilities ? and
does so in a conceptually distinct way ? we in-
vestigate in this section some empirical properties
of the label distributions induced by CW?s param-
eter distributions and compare them with those of
maxent.
We trained maxent and CW k = 1 classi-
fiers on the Enron B dataset, optimizing parame-
ters as before (maxent?s Gaussian prior and CW?s
?). We estimated the label distributions from our
CW classifiers after each iteration and on every
test example x by Gibbs sampling weight vec-
tors w ? N (?,?), and for each label y count-
ing the fraction of weight vectors for which y =
arg max
z
w ? f(x, z). Normalizing these counts
yields the label distributions Pr [y|x]. We denote
by y? the predicted label for a given x, and refer to
Pr [y?|x] as the prediction probability.
The leftmost panel of Fig. 2 plots each
method?s prediction error against the nor-
malized entropy of the label distribution
?
(
1
m
?
i
?
z
Pr [z|x
i
] log (Pr [z|x
i
])
)
/ log(K).
Each CW iteration (moving from right to left in
the plot) reduces both error and entropy. From our
maxent results we make the common observation
that maxent distributions have (ironically) low
entropy. In contrast, while CW accuracy exceeds
maxent after its second iteration, normalized
entropy remains high. Higher entropy suggests
a distribution over labels that is less peaked and
potentially more informative than those from
maxent. We found that the average probability
assigned to a correct prediction was 0.75 for
CW versus 0.83 for maxent and for an incorrect
prediction was 0.44 for CW versus 0.56 for
maxent.
Next, we investigate how these probabilities
relate to label accuracy. In the remaining pan-
els, we binned examples according to their pre-
diction probabilities Pr [y?|x] = max
y
Pr [y|x].
The second panel of Fig. 2 shows the numbers
of test examples with Pr [y?|x] ? [?, ? + 0.1) for
? = 0.1, 0.2 . . . 0.9. (Note that since there are 10
502
classes in this problem, we must have Pr [y?|x] ?
0.1.) The red (leftmost) bar corresponds to the
maximum entropy classifier, and the blue bars cor-
respond, from left to right, to CW after each suc-
cessive training epoch.
From the plot we observe that the maxent classi-
fier assigns prediction probability greater than 0.9
to more than 1,200 test examples out of 3,000.
Only 50 examples predicted by maxent fall in the
lowest bin, and the rest of examples are distributed
nearly uniformly across the remaining bins. The
large number of examples with very high predic-
tion probability explains the low entropy observed
for the maximum entropy classifier.
In contrast, the CW classifier shows the oppo-
site behavior after one epoch of training (the left-
most blue bar), assigning low prediction probabil-
ity (less than 0.3) to more than 1,200 examples
and prediction probability of at least 0.9 to only
100 examples. As CW makes additional passes
over the training data, its prediction confidence
increases and shifts toward more peaked distribu-
tions. After seven epochs fewer than 100 examples
have low prediction probability and almost 1,000
have high prediction probability. Nonetheless, we
note that this distribution is still less skewed than
that of the maximum entropy classifier.
Given the frequency of high probability maxent
predictions, it seems likely that many of the high
probability maxent labels will be wrong. This is
demonstrated in the third panel, which shows the
contribution of each bin to the total test error. Each
bar reflects the number of mistakes per bin divided
by the size of the complete test set (3,000). Thus,
the sum of the heights of the corresponding bars
in each bin is proportional to test error. Much of
the error of the maxent classifier comes not only
from the low-probability bins, due to their inac-
curacy, but also from the highest bin, due to its
very high population. In contrast, the CW clas-
sifiers see very little error contribution from the
high-probability bins. As training progresses, we
see again that the CW classifiers move in the direc-
tion of the maxent classifier but remain essentially
unimodal.
Finally, the rightmost panel shows the condi-
tional test error given bin identity, or the fraction
of test examples from each bin where the predic-
tion was incorrect. This is the pointwise ratio be-
tween corresponding values of the previous two
histograms. For both methods, there is a monoton-
ically decreasing trend in error as prediction prob-
ability increases; that is, the higher the value of
the prediction probability, the more likely that the
prediction it provides is correct. As CW is trained,
we see an increase in the conditional test error, yet
the overall error decreases (not shown). This sug-
gests that as CW is trained and its overall accuracy
improves, there are more examples with high pre-
diction probability, and the cost for this is a rela-
tive increase in the conditional test error per bin.
The maxent classifier produces an extremely large
number of test examples with very high prediction
probabilities, which yields relatively high condi-
tional test error. In nearly all cases, the conditional
error values for the CW classifiers are smaller than
the corresponding values for maximum entropy.
These observations suggest that CW assigns prob-
abilities more conservatively than maxent does,
and that the (fewer) high confidence predictions it
makes are of a higher quality. This is a potentially
valuable property, e.g., for system combination.
8 Conclusion
We have proposed a series of approximations for
multi-class confidence weighted learning, where
the simple analytical solutions of binary CW
learning do not apply. Our best CW method out-
performs online and batch baselines on eight of
nine NLP tasks, and is highly scalable due to the
use of a single optimization constraint. Alterna-
tively, our multi-constraint algorithms provide im-
proved performance for systems that can afford
only a single pass through the training data, as in
the true online setting. This result stands in con-
trast to previously observed behaviors in non-CW
settings (McDonald et al, 2004). Additionally, we
found improvements in both label entropy and ac-
curacy as compared to a maximum entropy clas-
sifier. We plan to extend these ideas to structured
problems with exponentially many labels and de-
velop methods that efficiently model label correla-
tions. An implementation of CW multi-class algo-
rithms is available upon request from the authors.
References
Blitzer, J., Dredze, M., & Pereira, F. (2007).
Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment
classification. Association for Computational
Linguistics (ACL).
503
Censor, Y., & Zenios, S. (1997). Parallel opti-
mization: Theory, algorithms, and applications.
Oxford University Press, New York, NY, USA.
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a
library for support vector machines. Software
available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. Empir-
ical Methods in Natural Language Processing
(EMNLP).
Crammer, K., Dredze, M., & Pereira, F. (2008).
Exact confidence-weighted learning. Advances
in Neural Information Processing Systems 22.
Crammer, K., & Singer, Y. (2001). On the al-
gorithmic implementation of multiclass kernel-
based vector machines. Jornal of Machine
Learning Research, 2, 265?292.
Crammer, K., & Singer, Y. (2003). Ultraconserva-
tive online algorithms for multiclass problems.
Jornal of Machine Learning Research (JMLR),
3, 951?991.
Dredze, M., Crammer, K., & Pereira, F. (2008).
Confidence-weighted linear classification. In-
ternational Conference on Machine Learning
(ICML).
Iusem, A., & Pierro, A. D. (1987). A simultaneous
iterative method for computing projections on
polyhedra. SIAM J. Control and Optimization,
25.
Lewis, D. D., Yang, Y., Rose, T. G., & Li, F.
(2004). Rcv1: A new benchmark collection for
text categorization research. Journal of Machine
Learning Research (JMLR), 5, 361?397.
Malkin, J., & Bilmes, J. (2008). Ratio semi-
definite classifiers. IEEE Int. Conf. on Acous-
tics, Speech, and Signal Processing.
McCallum, A. (2002). MALLET: A machine
learning for language toolkit. http://
mallet.cs.umass.edu.
McDonald, R., Crammer, K., & Pereira, F. (2004).
Large margin online learning algorithms for
scalable structured classification. NIPS Work-
shop on Structured Outputs.
Sandhaus, E. (2008). The new york times an-
notated corpus. Linguistic Data Consortium,
Philadelphia.
504
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277?285,
Beijing, August 2010
Entity Disambiguation for Knowledge Base Population
?Mark Dredze and ?Paul McNamee and ?Delip Rao and ?Adam Gerber and ?Tim Finin
?Human Language Technology Center of Excellence, Center for Language and Speech Processing
Johns Hopkins University
?University of Maryland ? Baltimore County
mdredze,mcnamee,delip,adam.gerber@jhu.edu, finin@umbc.edu
Abstract
The integration of facts derived from information extraction
systems into existing knowledge bases requires a system to
disambiguate entity mentions in the text. This is challeng-
ing due to issues such as non-uniform variations in entity
names, mention ambiguity, and entities absent from a knowl-
edge base. We present a state of the art system for entity dis-
ambiguation that not only addresses these challenges but also
scales to knowledge bases with several million entries using
very little resources. Further, our approach achieves perfor-
mance of up to 95% on entities mentioned from newswire
and 80% on a public test set that was designed to include
challenging queries.
1 Introduction
The ability to identify entities like people, orga-
nizations and geographic locations (Tjong Kim
Sang and De Meulder, 2003), extract their at-
tributes (Pasca, 2008), and identify entity rela-
tions (Banko and Etzioni, 2008) is useful for sev-
eral applications in natural language processing
and knowledge acquisition tasks like populating
structured knowledge bases (KB).
However, inserting extracted knowledge into a
KB is fraught with challenges arising from nat-
ural language ambiguity, textual inconsistencies,
and lack of world knowledge. To the discern-
ing human eye, the ?Bush? in ?Mr. Bush left
for the Zurich environment summit in Air Force
One.? is clearly the US president. Further con-
text may reveal it to be the 43rd president, George
W. Bush, and not the 41st president, George H.
W. Bush. The ability to disambiguate a polyse-
mous entity mention or infer that two orthograph-
ically different mentions are the same entity is
crucial in updating an entity?s KB record. This
task has been variously called entity disambigua-
tion, record linkage, or entity linking. When per-
formed without a KB, entity disambiguation is
called coreference resolution: entity mentions ei-
ther within the same document or across multi-
ple documents are clustered together, where each
cluster corresponds to a single real world entity.
The emergence of large scale publicly avail-
able KBs like Wikipedia and DBPedia has spurred
an interest in linking textual entity references to
their entries in these public KBs. Bunescu and
Pasca (2006) and Cucerzan (2007) presented im-
portant pioneering work in this area, but suffer
from several limitations including Wikipedia spe-
cific dependencies, scale, and the assumption of
a KB entry for each entity. In this work we in-
troduce an entity disambiguation system for link-
ing entities to corresponding Wikipedia pages de-
signed for open domains, where a large percent-
age of entities will not be linkable. Further, our
method and some of our features readily general-
ize to other curated KB. We adopt a supervised
approach, where each of the possible entities con-
tained within Wikipedia are scored for a match to
the query entity. We also describe techniques to
deal with large knowledge bases, like Wikipedia,
which contain millions of entries. Furthermore,
our system learns when to withhold a link when
an entity has no matching KB entry, a task that
has largely been neglected in prior research in
cross-document entity coreference. Our system
produces high quality predictions compared with
recent work on this task.
2 Related Work
The information extraction oeuvre has a gamut of
relation extraction methods for entities like per-
sons, organizations, and locations, which can be
classified as open- or closed-domain depending
on the restrictions on extractable relations (Banko
and Etzioni, 2008). Closed domain systems ex-
tract a fixed set of relations while in open-domain
systems, the number and type of relations are un-
bounded. Extracted relations still require process-
ing before they can populate a KB with facts:
namely, entity linking and disambiguation.
277
Motivated by ambiguity in personal name
search, Mann and Yarowsky (2003) disambiguate
person names using biographic facts, like birth
year, occupation and affiliation. When present
in text, biographic facts extracted using regular
expressions help disambiguation. More recently,
the Web People Search Task (Artiles et al, 2008)
clustered web pages for entity disambiguation.
The related task of cross document corefer-
ence resolution has been addressed by several
researchers starting from Bagga and Baldwin
(1998). Poesio et al (2008) built a cross document
coreference system using features from encyclo-
pedic sources like Wikipedia. However, success-
ful coreference resolution is insufficient for cor-
rect entity linking, as the coreference chain must
still be correctly mapped to the proper KB entry.
Previous work by Bunescu and Pasca (2006)
and Cucerzan (2007) aims to link entity men-
tions to their corresponding topic pages in
Wikipedia but the authors differ in their ap-
proaches. Cucerzan uses heuristic rules and
Wikipedia disambiguation markup to derive map-
pings from surface forms of entities to their
Wikipedia entries. For each entity in Wikipedia,
a context vector is derived as a prototype for the
entity and these vectors are compared (via dot-
product) with the context vectors of unknown en-
tity mentions. His work assumes that all entities
have a corresponding Wikipedia entry, but this as-
sumption fails for a significant number of entities
in news articles and even more for other genres,
like blogs. Bunescu and Pasca on the other hand
suggest a simple method to handle entities not in
Wikipedia by learning a threshold to decide if the
entity is not in Wikipedia. Both works mentioned
rely on Wikipedia-specific annotations, such as
category hierarchies and disambiguation links.
We just recently became aware of a system
fielded by Li et al at the TAC-KBP 2009 eval-
uation (2009). Their approach bears a number
of similarities to ours; both systems create candi-
date sets and then rank possibilities using differing
learning methods, but the principal difference is in
our approach to NIL prediction. Where we simply
consider absence (i.e., the NIL candidate) as an-
other entry to rank, and select the top-ranked op-
tion, they use a separate binary classifier to decide
whether their top prediction is correct, or whether
NIL should be output. We believe relying on fea-
tures that are designed to inform whether absence
is correct is the better alternative.
3 Entity Linking
We define entity linking as matching a textual en-
tity mention, possibly identified by a named en-
tity recognizer, to a KB entry, such as a Wikipedia
page that is a canonical entry for that entity. An
entity linking query is a request to link a textual
entity mention in a given document to an entry in
a KB. The system can either return a matching en-
try or NIL to indicate there is no matching entry.
In this work we focus on linking organizations,
geo-political entities and persons to a Wikipedia
derived KB.
3.1 Key Issues
There are 3 challenges to entity linking:
Name Variations. An entity often has multiple
mention forms, including abbreviations (Boston
Symphony Orchestra vs. BSO), shortened forms
(Osama Bin Laden vs. Bin Laden), alternate
spellings (Osama vs. Ussamah vs. Oussama),
and aliases (Osama Bin Laden vs. Sheikh Al-
Mujahid). Entity linking must find an entry de-
spite changes in the mention string.
Entity Ambiguity. A single mention, like
Springfield, can match multiple KB entries, as
many entity names, like people and organizations,
tend to be polysemous.
Absence. Processing large text collections vir-
tually guarantees that many entities will not ap-
pear in the KB (NIL), even for large KBs.
The combination of these challenges makes
entity linking especially challenging. Consider
an example of ?William Clinton.? Most read-
ers will immediately think of the 42nd US pres-
ident. However, the only two William Clintons in
Wikipedia are ?William de Clinton? the 1st Earl
of Huntingdon, and ?William Henry Clinton? the
British general. The page for the 42nd US pres-
ident is actually ?Bill Clinton?. An entity link-
ing system must decide if either of the William
Clintons are correct, even though neither are ex-
act matches. If the system determines neither
278
matches, should it return NIL or the variant ?Bill
Clinton?? If variants are acceptable, then perhaps
?Clinton, Iowa? or ?DeWitt Clinton? should be
acceptable answers?
3.2 Contributions
We address these entity linking challenges.
Robust Candidate Selection. Our system is
flexible enough to find name variants but suffi-
ciently restrictive to produce a manageable can-
didate list despite a large-scale KB.
Features for Entity Disambiguation. We de-
veloped a rich and extensible set of features based
on the entity mention, the source document, and
the KB entry. We use a machine learning ranker
to score each candidate.
Learning NILs. We modify the ranker to learn
NIL predictions, which obviates hand tuning and
importantly, admits use of additional features that
are indicative of NIL.
Our contributions differ from previous efforts
(Bunescu and Pasca, 2006; Cucerzan, 2007) in
several important ways. First, previous efforts de-
pend on Wikipedia markup for significant perfor-
mance gains. We make no such assumptions, al-
though we show that optional Wikipedia features
lead to a slight improvement. Second, Cucerzan
does not handle NILs while Bunescu and Pasca
address them by learning a threshold. Our ap-
proach learns to predict NIL in a more general
and direct way. Third, we develop a rich fea-
ture set for entity linking that can work with any
KB. Finally, we apply a novel finite state machine
method for learning name variations. 1
The remaining sections describe the candidate
selection system, features and ranking, and our
novel approach learning NILs, followed by an
empirical evaluation.
4 Candidate Selection for Name Variants
The first system component addresses the chal-
lenge of name variants. As the KB contains a large
number of entries (818,000 entities, of which 35%
are PER, ORG or GPE), we require an efficient se-
lection of the relevant candidates for a query.
Previous approaches used Wikipedia markup
for filtering ? only using the top-k page categories
1http://www.clsp.jhu.edu/ markus/fstrain
(Bunescu and Pasca, 2006) ? which is limited to
Wikipedia and does not work for general KBs.
We consider a KB independent approach to selec-
tion that also allows for tuning candidate set size.
This involves a linear pass over KB entry names
(Wikipedia page titles): a naive implementation
took two minutes per query. The following sec-
tion reduces this to under two seconds per query.
For a given query, the system selects KB entries
using the following approach:
? Titles that are exact matches for the mention.
? Titles that are wholly contained in or contain
the mention (e.g., Nationwide and Nationwide In-
surance).
? The first letters of the entity mention match the
KB entry title (e.g., OA and Olympic Airlines).
? The title matches a known alias for the entity
(aliases described in Section 5.2).
? The title has a strong string similarity score
with the entity mention. We include several mea-
sures of string similarity, including: character
Dice score > 0.9, skip bigram Dice score > 0.6,
and Hamming distance <= 2.
We did not optimize the thresholds for string
similarity, but these could obviously be tuned to
minimize the candidate sets and maximize recall.
All of the above features are general for any
KB. However, since our evaluation used a KB
derived from Wikipedia, we included a few
Wikipedia specific features. We added an entry if
its Wikipedia page appeared in the top 20 Google
results for a query.
On the training dataset (Section 7) the selection
system attained a recall of 98.8% and produced
candidate lists that were three to four orders of
magnitude smaller than the KB. Some recall er-
rors were due to inexact acronyms: ABC (Arab
Banking; ?Corporation? is missing), ASG (Abu
Sayyaf; ?Group? is missing), and PCF (French
Communist Party; French reverses the order of the
pre-nominal adjectives). We also missed Interna-
tional Police (Interpol) and Becks (David Beck-
ham; Mr. Beckham and his wife are collectively
referred to as ?Posh and Becks?).
279
4.1 Scaling Candidate Selection
Our previously described candidate selection re-
lied on a linear pass over the KB, but we seek
more efficient methods. We observed that the
above non-string similarity filters can be pre-
computed and stored in an index, and that the skip
bigram Dice score can be computed by indexing
the skip bigrams for each KB title. We omitted
the other string similarity scores, and collectively
these changes enable us to avoid a linear pass over
the KB. Finally we obtained speedups by serving
the KB concurrently2. Recall was nearly identical
to the full system described above: only two more
queries failed. Additionally, more than 95% of
the processing time was consumed by Dice score
computation, which was only required to cor-
rectly retrieve less than 4% of the training queries.
Omitting the Dice computation yielded results in
a few milliseconds. A related approach is that of
canopies for scaling clustering for large amounts
of bibliographic citations (McCallum et al, 2000).
In contrast, our setting focuses on alignment vs.
clustering mentions, for which overlapping parti-
tioning approaches like canopies are applicable.
5 Entity Linking as Ranking
We select a single correct candidate for a query
using a supervised machine learning ranker. We
represent each query by a D dimensional vector
x, where x ? RD, and we aim to select a sin-
gle KB entry y, where y ? Y , a set of possible
KB entries for this query produced by the selec-
tion system above, which ensures that Y is small.
The ith query is given by the pair {xi, yi}, where
we assume at most one correct KB entry.
To evaluate each candidate KB entry in Y we
create feature functions of the form f(x, y), de-
pendent on both the example x (document and en-
tity mention) and the KB entry y. The features
address name variants and entity disambiguation.
We take a maximum margin approach to learn-
ing: the correct KB entry y should receive a
higher score than all other possible KB entries
y? ? Y, y? 6= y plus some margin ?. This learning
2Our Python implementation with indexing features and
four threads achieved up to 80? speedup compared to naive
implementation.
constraint is equivalent to the ranking SVM algo-
rithm of Joachims (2002), where we define an or-
dered pair constraint for each of the incorrect KB
entries y? and the correct entry y. Training sets pa-
rameters such that score(y) ? score(y?) + ?. We
used the library SVMrank to solve this optimiza-
tion problem.3 We used a linear kernel, set the
slack parameter C as 0.01 times the number of
training examples, and take the loss function as
the total number of swapped pairs summed over
all training examples. While previous work used
a custom kernel, we found a linear kernel just as
effective with our features. This has the advan-
tage of efficiency in both training and prediction 4
? important considerations in a system meant to
scale to millions of KB entries.
5.1 Features for Entity Disambiguation
200 atomic features represent x based on each
candidate query/KB pair. Since we used a lin-
ear kernel, we explicitly combined certain fea-
tures (e.g., acroynym-match AND known-alias) to
model correlations. This included combining each
feature with the predicted type of the entity, al-
lowing the algorithm to learn prediction functions
specific to each entity type. With feature combina-
tions, the total number of features grew to 26,569.
The next sections provide an overview; for a de-
tailed list see McNamee et al (2009).
5.2 Features for Name Variants
Variation in entity name has long been recog-
nized as a bane for information extraction sys-
tems. Poor handling of entity name variants re-
sults in low recall. We describe several features
ranging from simple string match to finite state
transducer matching.
String Equality. If the query name and KB en-
try name are identical, this is a strong indication of
a match, and in our KB entry names are distinct.
However, similar or identical entry names that
refer to distinct entities are often qualified with
parenthetical expressions or short clauses. As
an example, ?London, Kentucky? is distinguished
3www.cs.cornell.edu/people/tj/svm_light/svm_rank.html
4Bunescu and Pasca (2006) report learning tens of thou-
sands of support vectors with their ?taxonomy? kernel while
a linear kernel represents all support vectors with a single
weight vector, enabling faster training and prediction.
280
from ?London, Ontario?, ?London, Arkansas?,
?London (novel)?, and ?London?. Therefore,
other string equality features were used, such as
whether names are equivalent after some transfor-
mation. For example, ?Baltimore? and ?Baltimore
City? are exact matches after removing a common
GPE word like city; ?University of Vermont? and
?University of VT? match if VT is expanded.
Approximate String Matching. Many entity
mentions will not match full names exactly. We
added features for character Dice, skip bigram
Dice, and left and right Hamming distance scores.
Features were set based on quantized scores.
These were useful for detecting minor spelling
variations or mistakes. Features were also added if
the query was wholly contained in the entry name,
or vice-versa, which was useful for handling ellip-
sis (e.g., ?United States Department of Agricul-
ture? vs. ?Department of Agriculture?). We also
included the ratio of the recursive longest com-
mon subsequence (Christen, 2006) to the shorter
of the mention or entry name, which is effective at
handling some deletions or word reorderings (e.g.,
?Li Gong? and ?Gong Li?). Finally, we checked
whether all of the letters of the query are found in
the same order in the entry name (e.g., ?Univ Wis-
consin? would match ?University of Wisconsin?).
Acronyms. Features for acronyms, using dic-
tionaries and partial character matches, enable
matches between ?MIT? and ?Madras Institute of
Technology? or ?Ministry of Industry and Trade.?
Aliases. Many aliases or nicknames are non-
trivial to guess. For example JAVA is the
stock symbol for Sun Microsystems, and ?Gin-
ger Spice? is a stage name of Geri Halliwell. A
reasonable way to do this is to employ a dictio-
nary and alias lists that are commonly available
for many domains5.
FST Name Matching. Another measure of sur-
face similarity between a query and a candidate
was computed by training finite-state transducers
similar to those described in Dreyer et al (2008).
These transducers assign a score to any string pair
by summing over all alignments and scoring all
5We used multiple lists, including class-specific lists (i.e.,
for PER, ORG, and GPE) lists extracted from Freebase (Bol-
lacker et al, 2008) and Wikipedia redirects. PER, ORG, and
GPE are the commonly used terms for entity types for peo-
ple, organizations and geo-political regions respectively.
contained character n-grams; we used n-grams of
length 3 and less. The scores are combined using a
global log-linear model. Since different spellings
of a name may vary considerably in length (e.g.,
J Miller vs. Jennifer Miller) we eliminated the
limit on consecutive insertions used in previous
applications.6
5.3 Wikipedia Features
Most of our features do not depend on Wikipedia
markup, but it is reasonable to include features
from KB properties. Our feature ablation study
shows that dropping these features causes a small
but statistically significant performance drop.
WikiGraph statistics. We added features de-
rived from the Wikipedia graph structure for an
entry, like indegree of a node, outdegree of a node,
and Wikipedia page length in bytes. These statis-
tics favor common entity mentions over rare ones.
Wikitology. KB entries can be indexed with hu-
man or machine generated metadata consisting of
keywords or categories in a domain-appropriate
taxonomy. Using a system called Wikitology,
Syed et al (2008) investigated use of ontology
terms obtained from the explicit category system
in Wikipedia as well as relationships induced from
the hyperlink graph between related Wikipedia
pages. Following this approach we computed top-
ranked categories for the query documents and
used this information as features. If none of the
candidate KB entries had corresponding highly-
ranked Wikitology pages, we used this as a NIL
feature (Section 6.1).
5.4 Popularity
Although it may be an unsafe bias to give prefer-
ence to common entities, we find it helpful to pro-
vide estimates of entity popularity to our ranker
as others have done (Fader et al, 2009). Apart
from the graph-theoretic features derived from the
Wikipedia graph, we used Google?s PageRank to
by adding features indicating the rank of the KB
entry?s corresponding Wikipedia page in a Google
query for the target entity mention.
6Without such a limit, the objective function may diverge
for certain parameters of the model; we detect such cases and
learn to avoid them during training.
281
5.5 Document Features
The mention document and text associated with a
KB entry contain context for resolving ambiguity.
Entity Mentions. Some features were based on
presence of names in the text: whether the query
appeared in the KB text and the entry name in the
document. Additionally, we used a named-entity
tagger and relation finder, SERIF (Boschee et al,
2005), identified name and nominal mentions that
were deemed co-referent with the entity mention
in the document, and tested whether these nouns
were present in the KB text. Without the NE anal-
ysis, accuracy on non-NIL entities dropped 4.5%.
KB Facts. KB nodes contain infobox attributes
(or facts); we tested whether the fact text was
present in the query document, both locally to a
mention, or anywhere in the text. Although these
facts were derived from Wikipedia infoboxes,
they could be obtained from other sources as well.
Document Similarity We measured similarity
between the query document and the KB text in
two ways: cosine similarity with TF/IDF weight-
ing (Salton and McGill, 1983); and using the Dice
coefficient over bags of words. IDF values were
approximated using counts from the Google 5-
gram dataset as by Klein and Nelson (2008).
Entity Types. Since the KB contained types
for entries, we used these as features as well as
the predicted NE type for the entity mention in
the document text. Additionally, since only a
small number of KB entries had PER, ORG, or
GPE types, we also inferred types from Infobox
class information to attain 87% coverage in the
KB. This was helpful for discouraging selection
of eponymous entries named after famous enti-
ties (e.g., the former U.S. president vs. ?John F.
Kennedy International Airport?).
5.6 Feature Combinations
To take into account feature dependencies we cre-
ated combination features by taking the cross-
product of a small set of diverse features. The
attributes used as combination features included
entity type; a popularity based on Google?s rank-
ings; document comparison using TF/IDF; cov-
erage of co-referential nouns in the KB node
text; and name similarity. The combinations were
cascaded to allow arbitrary feature conjunctions.
Thus it is possible to end up with a feature kbtype-
is-ORG AND high-TFIDF-score AND low-name-
similarity. The combined features increased the
number of features from roughly 200 to 26,000.
6 Predicting NIL Mentions
So far we have assumed that each example has a
correct KB entry; however, when run over a large
corpus, such as news articles, we expect a signifi-
cant number of entities will not appear in the KB.
Hence it will be useful to predict NILs.
We learn when to predict NIL using the SVM
ranker by augmenting Y to include NIL, which
then has a single feature unique to NIL answers.
It can be shown that (modulo slack variables) this
is equivalent to learning a single threshold ? for
NIL predictions as in Bunescu and Pasca (2006).
Incorporating NIL into the ranker has several
advantages. First, the ranker can set the thresh-
old optimally without hand tuning. Second, since
the SVM scores are relative within a single exam-
ple and cannot be compared across examples, set-
ting a single threshold is difficult. Third, a thresh-
old sets a uniform standard across all examples,
whereas in practice we may have reasons to favor
a NIL prediction in a given example. We design
features for NIL prediction that cannot be cap-
tured in a single parameter.
6.1 NIL Features
Integrating NIL prediction into learning means
we can define arbitrary features indicative of NIL
predictions in the feature vector corresponding to
NIL. For example, if many candidates have good
name matches, it is likely that one of them is cor-
rect. Conversely, if no candidate has high entry-
text/article similarity, or overlap between facts
and the article text, it is likely that the entity is
absent from the KB. We included several features,
such as a) the max, mean, and difference between
max and mean for 7 atomic features for all KB
candidates considered, b) whether any of the can-
didate entries have matching names (exact and
fuzzy string matching), c) whether any KB en-
try was a top Wikitology match, and d) if the top
Google match was not a candidate.
282
Micro-Averaged Macro-Averaged
Best Median All Features Best Features Best Median All Features Best Features
All 0.8217 0.7108 0.7984 0.7941 0.7704 0.6861 0.7695 0.7704
non-NIL 0.7725 0.6352 0.7063 0.6639 0.6696 0.5335 0.6097 0.5593
NIL 0.8919 0.7891 0.8677 0.8919 0.8789 0.7446 0.8464 0.8721
Table 1: Micro and macro-averaged accuracy for TAC-KBP data compared to best and median reported performance.
Results are shown for all features as well as removing a small number of features using feature selection on development data.
7 Evaluation
We evaluated our system on two datasets: the
Text Analysis Conference (TAC) track on Knowl-
edge Base Population (TAC-KBP) (McNamee and
Dang, 2009) and the newswire data used by
Cucerzan (2007) (Microsoft News Data).
Since our approach relies on supervised learn-
ing, we begin by constructing our own training
corpus.7 We highlighted 1496 named entity men-
tions in news documents (from the TAC-KBP doc-
ument collection) and linked these to entries in
a KB derived from Wikipedia infoboxes. 8 We
added to this collection 119 sample queries from
the TAC-KBP data. The total of 1615 training ex-
amples included 539 (33.4%) PER, 618 (38.3%)
ORG, and 458 (28.4%) GPE entity mentions. Of
the training examples, 80.5% were found in the
KB, matching 300 unique entities. This set has a
higher number of NIL entities than did Bunescu
and Pasca (2006) (10%) but lower than the TAC-
KBP test set (43%).
All system development was done using a train
(908 examples) and development (707 examples)
split. The TAC-KBP and Microsoft News data
sets were held out for final tests. A model trained
on all 1615 examples was used for experiments.
7.1 TAC-KBP 2009 Experiments
The KB is derived from English Wikipedia pages
that contained an infobox. Entries contain basic
descriptions (article text) and attributes. The TAC-
KBP query set contains 3904 entity mentions for
560 distinct entities; entity type was only provided
for evaluation. The majority of queries were for
organizations (69%). Most queries were missing
from the KB (57%). 77% of the distinct GPEs
in the queries were present in the KB, but for
7Data available from www.dredze.com
8http://en.wikipedia.org/wiki/Help:Infobox
PERs and ORGs these percentages were signifi-
cantly lower, 19% and 30% respectively.
Table 1 shows results on TAC-KBP data us-
ing all of our features as well a subset of features
based on feature selection experiments on devel-
opment data. We include scores for both micro-
averaged accuracy ? averaged over all queries
? and macro-averaged accuracy ? averaged over
each unique entity ? as well as the best and me-
dian reported results for these data (McNamee
and Dang, 2009). We obtained the best reported
results for macro-averaged accuracy, as well as
the best results for NIL detection with micro-
averaged accuracy, which shows the advantage of
our approach to learning NIL. See McNamee et
al. (2009) for additional experiments.
The candidate selection phase obtained a re-
call of 98.6%, similar to that of development data.
Missed candidates included Iron Lady, which
refers metaphorically to Yulia Tymoshenko, PCC,
the Spanish-origin acronym for the Cuban Com-
munist Party, and Queen City, a former nickname
for the city of Seattle, Washington. The system re-
turned a mean of 76 candidates per query, but the
median was 15 and the maximum 2772 (Texas). In
about 10% of cases there were four or fewer can-
didates and in 10% of cases there were more than
100 candidate KB nodes. We observed that ORGs
were more difficult, due to the greater variation
and complexity in their naming, and that they can
be named after persons or locations.
7.2 Feature Effectiveness
We performed two feature analyses on the TAC-
KBP data: an additive study ? starting from a
small baseline feature set used in candidate selec-
tion we add feature groups and measure perfor-
mance changes (omitting feature combinations),
and an ablative study ? starting from all features,
remove a feature group and measure performance.
283
Class All non-NIL NIL
Baseline 0.7264 0.4621 0.9251
Acronyms 0.7316 0.4860 0.9161
NE Analysis 0.7661 0.7181 0.8022
Google 0.7597 0.7421 0.7730
Doc/KB Text Similarity 0.7313 0.6699 0.7775
Wikitology 0.7318 0.4549 0.9399
All 0.7984 0.7063 0.8677
Table 2: Additive analysis: micro-averaged accuracy.
Table 2 shows the most significant features in
the feature addition experiments. The baseline
includes only features based on string similarity
or aliases and is not effective at finding correct
entries and strongly favors NIL predictions. In-
clusion of features based on analysis of named-
entities, popularity measures (e.g., Google rank-
ings), and text comparisons provided the largest
gains. The overall changes are fairly small,
roughly ?1%; however changes in non-NIL pre-
cision are larger.
The ablation study showed considerable redun-
dancy across feature groupings. In several cases,
performance could have been slightly improved
by removing features. Removing all feature com-
binations would have improved overall perfor-
mance to 81.05% by gaining on non-NIL for a
small decline on NIL detection.
7.3 Experiments on Microsoft News Data
We downloaded the evaluation data used in
Cucerzan (2007)9: 20 news stories from MSNBC
with 642 entity mentions manually linked to
Wikipedia and another 113 mentions not having
any corresponding link to Wikipedia.10 A sig-
nificant percentage of queries were not of type
PER, ORG, or GPE (e.g., ?Christmas?). SERIF
assigned entity types and we removed 297 queries
not recognized as entities (counts in Table 3).
We learned a new model on the training data
above using a reduced feature set to increase
speed.11 Using our fast candidate selection sys-
tem, we resolved each query in 1.98 seconds (me-
dian). Query processing time was proportional to
9http://research.microsoft.com/en-us/um/people/silviu/WebAssistant/TestData/
10One of the MSNBC news articles is no longer available
so we used 759 total entities.
11We removed Google, FST and conjunction features
which reduced system accuracy but increased performance.
Num. Queries Accuracy
Total Nil All non-NIL NIL
NIL 452 187 0.4137 0.0 1.0
GPE 132 20 0.9696 1.00 0.8000
ORG 115 45 0.8348 0.7286 1.00
PER 205 122 0.9951 0.9880 1.00
All 452 187 0.9469 0.9245 0.9786
Cucerzan (2007) 0.914 - -
Table 3: Micro-average results for Microsoft data.
the number of candidates considered. We selected
a median of 13 candidates for PER, 12 for ORG
and 102 for GPE. Accuracy results are in Table
3. The high results reported for this dataset over
TAC-KBP is primarily because we perform very
well in predicting popular and rare entries ? both
of which are common in newswire text.
One issue with our KB was that it was derived
from infoboxes in Wikipedia?s Oct 2008 version
which has both new entities, 12 and is missing en-
tities.13 Therefore, we manually confirmed NIL
answers and new answers for queries marked as
NIL in the data. While an exact comparison is not
possible (as described above), our results (94.7%)
appear to be at least on par with Cucerzan?s sys-
tem (91.4% overall accuracy).With the strong re-
sults on TAC-KBP, we believe that this is strong
confirmation of the effectiveness of our approach.
8 Conclusion
We presented a state of the art system to disam-
biguate entity mentions in text and link them to
a knowledge base. Unlike previous approaches,
our approach readily ports to KBs other than
Wikipedia. We described several important chal-
lenges in the entity linking task including han-
dling variations in entity names, ambiguity in en-
tity mentions, and missing entities in the KB, and
we showed how to each of these can be addressed.
We described a comprehensive feature set to ac-
complish this task in a supervised setting. Impor-
tantly, our method discriminately learns when not
to link with high accuracy. To spur further re-
search in these areas we are releasing our entity
linking system.
122008 vs. 2006 version used in Cucerzan (2007) We
could not get the 2006 version from the author or the Internet.
13Since our KB was derived from infoboxes, entities not
having an infobox were left out.
284
References
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search: results of the first evalu-
ation and the plan for the second. In WWW.
Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Conference on Computational
Linguistics (COLING).
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Association for Computational Linguistics.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In SIGMOD Management of Data.
E. Boschee, R. Weischedel, and A. Zamanian. 2005.
Automatic information extraction. In Conference
on Intelligence Analysis.
Razvan C. Bunescu and Marius Pasca. 2006. Using
encyclopedic knowledge for named entity disam-
biguation. In European Chapter of the Assocation
for Computational Linguistics (EACL).
Peter Christen. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Australian National Uni-
versity.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on wikipedia data. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Empirical Methods in
Natural Language Processing (EMNLP).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity dis-
ambiguation to arbitrary web text. In WikiAI09
Workshop at IJCAI 2009.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Knowledge Discovery
and Data Mining (KDD).
Martin Klein and Michael L. Nelson. 2008. A com-
parison of techniques for estimating IDF values to
generate lexical signatures for the web. In Work-
shop on Web Information and Data Management
(WIDM).
Fangtao Li, Zhicheng Zhang, Fan Bu, Yang Tang,
Xiaoyan Zhu, and Minlie Huang. 2009. THU
QUANTA at TAC 2009 KBP and RTE track. In Text
Analysis Conference (TAC).
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Conference
on Natural Language Learning (CONLL).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In
Knowledge Discovery and Data Mining (KDD).
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Text Analysis Conference (TAC).
Paul McNamee, Mark Dredze, Adam Gerber, Nikesh
Garera, Tim Finin, James Mayfield, Christine Pi-
atko, Delip Rao, David Yarowsky, and Markus
Dreyer. 2009. HLTCOE approaches to knowledge
base population at TAC 2009. In Text Analysis Con-
ference (TAC).
Marius Pasca. 2008. Turning web text and search
queries into factual knowledge: hierarchical class
attribute extraction. In National Conference on Ar-
tificial Intelligence (AAAI).
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob
Hall, Janet Hitzeman, Alan Jern, Mijail Kabadjov,
Stanley Yong, Wai Keong, Gideon Mann, Alessan-
dro Moschitti, Simone Ponzetto, Jason Smith, Josef
Steinberger, Michael Strube, Jian Su, Yannick Ver-
sley, Xiaofeng Yang, and Michael Wick. 2008. Ex-
ploiting lexical and encyclopedic resources for en-
tity disambiguation: Final report. Technical report,
JHU CLSP 2007 Summer Workshop.
Gerard Salton and Michael McGill. 1983. Introduc-
tion to Modern Information Retrieval. McGraw-
Hill Book Company.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Confer-
ence on Natural Language Learning (CONLL).
Zareen Syed, Tim Finin, and Anupam Joshi. 2008.
Wikipedia as an ontology for describing documents.
In Proceedings of the Second International Confer-
ence on Weblogs and Social Media. AAAI Press.
285
Coling 2010: Poster Volume, pages 1050?1058,
Beijing, August 2010
Streaming Cross Document Entity Coreference Resolution
Delip Rao and Paul McNamee and Mark Dredze
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
delip,mcnamee,mdredze@jhu.edu
Abstract
Previous research in cross-document en-
tity coreference has generally been re-
stricted to the offline scenario where the
set of documents is provided in advance.
As a consequence, the dominant approach
is based on greedy agglomerative cluster-
ing techniques that utilize pairwise vec-
tor comparisons and thus require O(n2)
space and time. In this paper we ex-
plore identifying coreferent entity men-
tions across documents in high-volume
streaming text, including methods for uti-
lizing orthographic and contextual infor-
mation. We test our methods using several
corpora to quantitatively measure both the
efficacy and scalability of our streaming
approach. We show that our approach
scales to at least an order of magnitude
larger data than previous reported meth-
ods.
1 Introduction
A key capability for successful information ex-
traction, topic detection and tracking, and ques-
tion answering is the ability to identify equiva-
lence classes of entity mentions. An entity is a
real-world person, place, organization, or object,
such as the person who serves as the 44th pres-
ident of the United States. An entity mention is
a string which refers to such an entity, such as
?Barack Hussein Obama?, ?Senator Obama? or
?President Obama?. The goal of coreference res-
olution is to identify and connect all textual entity
mentions that refer to the same entity.
The first step towards this goal is to identify all
references within the same document, or within
document coreference resolution. A document of-
ten has a leading canonical reference to the entity
(?Barack Obama?) followed by additional expres-
sions for the same entity (?President Obama.?)
An intra-document coreference system must first
identify each reference, often relying on named
entity recognition, and then decide if these refer-
ences refer to a single individual or multiple enti-
ties, creating a coreference chain for each unique
entity. Feature representations include surface
form similarity, lexical context of mentions, po-
sition in the document and distance between ref-
erences. A variety of statistical learning meth-
ods have been applied to this problem, including
use of decision trees (Soon et al, 2001; Ng and
Cardie, 2002), graph partitioning (Nicolae and
Nicolae, 2006), maximum-entropy models (Luo
et al, 2004), and conditional random fields (Choi
and Cardie, 2007).
Given pre-processed documents, in which enti-
ties have been identified and entity mentions have
been linked into chains, we seek to identify across
an entire document collection all chains that re-
fer to the same entity. This task is called cross
document coreference resolution (CDCR). Sev-
eral of the challenges associated with CDCR dif-
fer from the within document task. For example,
it is unlikely that the same document will discuss
John Phillips the American football player and
John Phillips the musician, but it is quite proba-
ble that documents discussing each will appear in
the same collection. Therefore, while matching
entities with the same mention string can work
well for within document coreference, more so-
phisticated approaches are necessary for the cross
document scenario where a one-entity-per-name
assumption is unreasonable.
One of the most common approaches to both
within document and cross document corefer-
ence resolution has been based on agglomerative
clustering, where vectors might be bag-of-word
contexts (Bagga and Baldwin, 1998; Mann and
1050
Yarowsky, 2003; Gooi and Allan, 2004; Chen
and Martin, 2007). These algorithms creates a
O(n2) dependence in the number of mentions ?
for within document ? and documents ? for cross
document. This is a reasonable limitation for
within document, since the number of references
will certainly be small; we are unlikely to en-
counter a document with millions of references.
In contrast to the small n encountered within a
document, we fully expect to run a CDCR sys-
tem on hundreds of thousands or millions of doc-
uments. Most previous approaches cannot handle
collections of this size.
In this work, we present a new method for
cross document coreference resolution that scales
to very large corpora. Our algorithm operates in
a streaming setting, in which documents are pro-
cessed one at a time and only a single time. This
creates a linear (O(n)) dependence on the num-
ber of documents in the collection, allowing us
to scale to millions of documents and millions
of unique entities. Our algorithm uses stream-
ing clustering with common coreference similar-
ity computations to achieve large scale. Further-
more, our method is designed to support both
name disambiguation and name variation.
In the next section, we give a survey of related
work. In Section 3 we detail our streaming setup,
giving a description of the streaming algorithm
and presenting efficient techniques for represent-
ing clusters over streams and for computing simi-
larity. Section 4 describes the data sets on which
we evaluate our methods and presents results. We
conclude with a discussion and description of on-
going work.
2 Related Work
Traditional approaches to cross document coref-
erence resolution have first constructed a vector
space representation derived from local (or global)
contexts of entity mentions in documents and then
performed some form of clustering on these vec-
tors. This is a simple extension of Firth?s distribu-
tional hypothesis applied to entities (Firth, 1957).
We describe some of the seminal work in this area.
Some of the earliest work in CDCR was by
Bagga and Baldwin (1998). Key contributions
of their research include: promotion of a set-
theoretic evaluation measure, B-CUBED; intro-
duction of a data set based on 197 New York
Times articles which mention a person named
John Smith; and, use of TF/IDF weighted vec-
tors and cosine similarity in single-link greedy ag-
glomerative clustering.
Mann and Yarowsky (2003) extended Bagga
and Baldwin?s work and contributed several inno-
vations, including: use of biographical attributes
(e.g., year of birth, occupation), and evaluation us-
ing pseudonames. Pseudonames are sets of artifi-
cially conflated names that are used as an efficient
method for producing a set of gold-standard dis-
ambiguations.1 Mann and Yarowsky used 4 pairs
of conflated names in their evaluation. Their sys-
tem did not perform as well on named entities with
little available biographic information.
Gooi and Allan (2004) expanded on the use
of pseudonames by semi-automatically creating
a much larger evaluation set, which they called
the ?Person-X? corpus. They relied on automated
named-entity tagging and domain-focused text re-
trieval. This data consisted of 34,404 documents
where a single person mention in each document
was rewritten as ?Person X?. Besides their novel
construction of a large-scale resource, they in-
vestigated several minor variations in clustering,
namely (a) use of Kullback-Leibler divergence as
a distance measure, (b) use of 55-word snippets
around entity mentions (vs. entire documents or
extracted sentences), and (c) scoring clusters us-
ing average-link instead of single- or complete-
link.
Finally, in more recent work, Chen and Martin
(2007) explore the CDCR task in both English and
Chinese. Their work focuses on use of both lo-
cal, and document-level noun-phrases as features
in their vector-space representation.
There have been a number of open evaluations
of CDCR systems. For example, the Web People
Search (WePS) workshops (Artiles et al, 2008)
have created a task for disambiguating personal
names from HTML pages. A set of ambiguous
names is chosen and each is submitted to a popular
web search engine. The top 100 pages are then
manually clustered.We discuss several other data
1See Sanderson (2000) for use of this technique in word
sense disambiguation.
1051
sets in Section 4.2
All of the papers mentioned above focus on dis-
ambiguating personal names. In contrast, our sys-
tem can also handle organizations and locations.
Also, as was mentioned earlier, we are commit-
ted to a scenario where documents are presented
in sequence and entities must be disambiguated
instantly, without the benefit of observing the en-
tire corpus. We believe that such a system is bet-
ter suited to highly dynamic environments such as
daily news feeds, blogs, and tweets. Additionally,
a streaming system exposes a set of known entity
clusters after each document is processed instead
of waiting until the end of the stream.
3 Approach
Our cross document coreference resolution sys-
tem relies on a streaming clustering algorithm
and efficient calculation of similarity scores. We
assume that we receive a stream of corefer-
ence chains, along with entity types, as they
are extracted from documents. We use SERIF
(Ramshaw and Weischedel, 2005), a state of the
art document analysis system which performs
intra-document coreference resolution. BBN de-
veloped SERIF to address information extraction
tasks in the ACE program and it is further de-
scribed in Pradhan et al (2007).
Each unique entity is represented by an entity
cluster c, comprised of entity chains from many
documents that refer to the same entity. Given
an entity coreference chain e, we identify the best
known entity cluster c. If a suitable entity cluster
is not found, a new entity cluster is formed.
An entity cluster is selected for a given corefer-
ence chain using several similarity scores, includ-
ing document context, predicted entity type, and
orthographic similarity between the entity men-
tion and previously discovered references in the
entity cluster. An efficient implementation of the
similarity score allows the system to identify the
top k most likely mentions without considering all
m entity clusters. The final output of our sys-
tem is a collection of entity clusters, each con-
taining a list of coreference chains and their doc-
uments. Additionally, due to its streaming nature,
2We preferred other data sets to the WePS data in our
evaluation because it is not easily placed in temporal order.
the system can be examined at any time to produce
this information based on only the documents that
have been processed thus far.
In the next sections, we describe both the clus-
tering algorithm and efficient computation of the
entity similarity scores.
3.1 Clustering Algorithm
We use a streaming clustering algorithm to cre-
ate entity clusters as follows. We observe a set
of points from a potentially infinite set X , one at
a time, and would like to maintain a fixed number
of clusters while minimizing the maximum cluster
radius, defined as the radius of the smallest ball
containing all points of the cluster. This setup is
well known in the theory and information retrieval
community and is referred to as the dynamic clus-
tering problem (Can and Ozkarahan, 1987).
Others have attempted to use an incremen-
tal clustering approach, such as Gooi and Al-
lan (2004) (who eventually prefer a hierarchi-
cal clustering approach), and Luo et al (2004),
who use a Bell tree approach for incrementally
clustering within document entity mentions. Our
work closely follows the Doubling Algorithm of
Charikar et al (1997), which has better perfor-
mance guarantees for streaming data. Streaming
clustering means potentially linear performance in
the number of observations since each document
need only be examined a single time, as opposed
to the quadratic cost of agglomerative clustering.3
The Doubling Algorithm consists of two stages:
update and merge. Update adds points to existing
clusters or creates new clusters while merge com-
bines clusters to prevent the clusters from exceed-
ing a fixed limit. New clusters are created accord-
ing to a threshold set using development data. We
selected a threshold of 0.5 since it worked well in
preliminary experiments. Since the number of en-
tities grows with time, we have skipped the merge
step in our initial experiments so as not to limit
cluster growth.
We use a dynamic caching scheme which backs
the actual clusters in a disk based index, but re-
3It is possible to implement hierarchical agglomerative
clustering in O(n logm) time where n is the number of
points and m in the number of clusters. However this is still
superlinear and expensive in situations where m continually
increases like in streaming coreference resolution.
1052
1 2 3 4 5 6log(Rank)0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
log(
Fre
que
ncy
)
PERLOCORG
Figure 1: Frequency vs. rank for 567k people,
136k organizations, and 25k locations in the New
York Times Annotated Corpus (Sandhaus, 2008).
tains basic cluster information in memory (see be-
low). Doing so improves paging performance as
observed in Omiecinski and Scheuermann (1984).
Motivated by the Zipfian distribution of named en-
tities in news sources (Figure 1), we organize our
cluster store using an LRU policy, which facili-
tates easy access to named entities that were ob-
served in the recent past. We obtain additional
performance gains by hashing the clusters based
on the constituent mention string (details below).
This allows us to quickly retrieve a small but re-
lated number of clusters, k. It is always the case
that k << m, the current number of clusters.
3.2 Candidate Cluster Selection
As part of any clustering algorithm, each new item
must be compared against current clusters. As we
see more documents, the number of unique clus-
ters (entities) grows. Therefore, we need efficient
methods to select candidate clusters.
To select the top candidate clusters, we obtain
those that have high orthographic similarity with
the head name mention in the coreference chain e.
We compute this similarity using the dice score on
either word unigrams or character skip bigrams.
For each entity mention string associated with a
cluster c, we generate all possible n-grams using
one of the above two policies. We then index the
cluster by each of its n-grams in a hash maintained
in memory. In addition, we keep the number of n-
grams generated for each cluster.
When given a new head mention e for a coref-
erence chain, we generate all of the n-grams and
look up clusters that contain these n-grams using
the hash. We then compute the dice score:
dice(e, c) = |{ngram(e)} ? {ngram(c)}||{ngram(e)} ? {ngram(c)}| ,
where {ngram(e)} are the set of n-grams in entity
mention e and {ngram(c)} are the set of n-grams
for all entity mentions in cluster c. Note that we
can calculate the numerator (the intersection) by
looking up the n-grams of e in the hash and count-
ing matches with c. The denominator is equivalent
to the number of n-grams unique to e and to c plus
the number that are shared. The number that are
shared is the intersection. The number unique to
e is the total number of n-grams in e minus the in-
tersection. The final term, the number unique to c,
is computed by taking the total number of n-grams
in c (a single integer stored in memory) minus the
intersection.
Through this strategy, we can select only those
clusters that have the highest orthographic simi-
larity to e without requiring the cluster contents,
which may not be stored in memory. In our exper-
iments, we evaluate settings where we select all
candidates with non-zero score and a pruned set of
the top k dice score candidates. We also include
in the n-gram list known aliases to facilitate or-
thographically dissimilar, but reasonable matches
(e.g., IBM or ?Big Blue? for ?International Busi-
ness Machines, Inc.?).4
For further efficiency, we keep separate caches
for each named entity type.5 We then select the
appropriate cache based on the automatically de-
termined type of the named entity provided by the
named entity tagger, which also prevents spurious
matches of non-matching entity types.
3.3 Similarity Metric
After filtering by orthographic information to
quickly obtain a small set of candidate clusters,
a full similarity score is computed for the current
4We generated alias lists for entities from Freebase.
5Persons (PER), organizations (ORG), and locations
(LOC).
1053
entity coreference chain and each retrieved can-
didate cluster. These computations require infor-
mation about each cluster, so the cluster?s suffi-
cient statistics are loaded using the LRU cache de-
scribed above.
We define several similarity metrics between
coreference chains and clusters to deal with both
name variation and disambiguation. For name
variation, we define an orthographic similarity
metric to match similar entity mention strings. As
before, we use word unigrams and character skip
bigrams. For each of these methods, we compute
a similarity score as dice(e, c) and select the high-
est scoring cluster.
To address name disambiguation, we use two
types of context from the document. First, we use
lexical features represented as TF/IDF weighted
vectors. Second, we consider topic features, in
which each word in a document is replaced with
the topic inferred from a topic model. This yields
a distribution over topics for a given document.
We use an LDA (Blei et al, 2003) model trained
on the New York Times Annotated Corpus (Sand-
haus, 2008). We note that LDA can be computed
over streams (Yao et al, 2009).
To compare context vectors we use cosine sim-
ilarity, where the cluster vector is the average of
all document vectors assigned to the cluster. Note
that the filtering step in Section 3.2 returns only
those candidates with some orthographic similar-
ity with the coreference chain, so a similarity met-
ric that uses context only is still restricted to ortho-
graphically similar entities.
Finally, we consider a combination of ortho-
graphic and context similarity as a linear combi-
nation of the two metrics as:
score(e, c) = ? dice(e, c) + (1? ?)cosine(e, c) .
We set ? = 0.8 based on initial experiments.
4 Evaluation
We used several corpora to evaluate our meth-
ods, including two data sets commonly used in the
coreference community. We also created a new
test set using artificially conflated names. And fi-
nally to test scalability, we ran our algorithm over
a large text collection that, while it did not have
Attribute smith nytac ace08 kbp09
Total Documents 197 1.85M 10k 1.2M
Annotated Docs 197 19,360 415 **
Annotated Entities 35 200 3,943 **
Table 1: Data sets used in our experiments. For
the kbp09 data we did not have annotations.
ground truth entity clusters, was useful for com-
puting other performance statistics. Properties for
each data set are given in Table 1.
4.1 John Smith corpus
Bagga and Baldwin (1998) evaluated their disam-
biguation system on a set of 197 articles from the
New York Times that mention a person named
?John Smith?. This data exhibits no name variants
and is strictly a disambiguation task. We include
this data (smith) to allow comparison to previous
work.
4.2 NYTAC Pseudo-name corpus
To study the effects of word sense ambiguity
and disambiguation several researchers have ar-
tificially conflated dissimilar words together and
then attempted to disambiguate them (Sanderson,
2000). The obvious advantage is cheaply obtained
ground truth for disambiguation.
The same trick has also been employed in per-
son name disambiguation (Mann and Yarowsky,
2003; Gooi and Allan, 2004). We adopt the same
method on a somewhat larger scale using annota-
tions from the New York Times Annotated Corpus
(NYTAC) (Sandhaus, 2008), which annotates doc-
uments based on whether or not they mention an
entity. The NYTAC data contains documents from
20 years of the New York Times and contains rich
metadata and document-level annotations that in-
dicate when an entity is mentioned in the docu-
ment using a standard lexicon of entities. (Note
that mention strings are not tagged.) Using these
annotations we created a set of 100 pairs of con-
flated person names.
The names were selected to be medium fre-
quency (i.e., occurring in between 50 and 200 ar-
ticles) and each pair matches in gender. The first
50 pairs are for names that are topically similar,
for example, Tim Robbins and Tom Hanks (both
actors); Barbara Boxer and Olympia Snowe (both
1054
smith nytac ace08
Approach P R F P R F P R F
Baseline 1.000 0.178 0.302 1.000 0.010 0.020 1.000 0.569 0.725
ExactMatch 0.233 1.000 0.377 0.563 0.897 0.692 0.977 0.697 0.814
Ortho 0.603 0.629 0.616 0.611 0.784 0.687 0.975 0.694 0.811
BoW 0.956 0.367 0.530 0.930 0.249 0.349 0.989 0.589 0.738
Topic 0.847 0.592 0.697 0.815 0.244 0.363 0.983 0.605 0.750
Ortho+BoW 0.603 0.634 0.618 0.801 0.601 0.686 0.976 0.691 0.809
Ortho+Topic 0.603 0.634 0.618 0.800 0.591 0.680 0.975 0.704 0.819
Table 2: Best B3 performance on the smith, nytac, and ace08 test sets.
US politicians). We imagined that this would be
a more challenging subset because of presumed
lexical overlap. The second set of 50 name pairs
were arbitrarily conflated. We sub-selected the
data to ensure that no two entities in our collec-
tion co-occur in the same document and this left
us with 19,360 documents for which ground-truth
was known. In each document we rewrote the
conflated name mentions using a single gender-
neutral name; any middle initials or names were
discarded.
4.3 ACE 2008 corpus
The NIST ACE 2008 (ace08) evaluation studied
several related technologies for information ex-
traction, including named-entity recognition, re-
lation extraction, and cross-document coreference
for person names in both English and Arabic. Ap-
proximately 10,000 documents from several gen-
res (predominantly newswire) were given to par-
ticipants, who were expected to cluster person and
organization entities across the entire collection.
However, only a selected set of about 400 docu-
ments were annotated and used to evaluate sys-
tem performance. Baron and Freedman (2008)
describe their work in this evaluation, which in-
cluded a separate task for within-document coref-
erence.
4.4 TAC-KBP 2009 corpus
The NIST TAC 2009 Knowledge Base Popula-
tion track (kbp09) (McNamee and Dang, 2009)
conducted an evaluation of a system?s ability to
link entity mentions to corresponding Wikipedia-
derived knowledge base nodes. The TAC-KBP
task focused on ambiguous person, organization,
and geo-political entities mentioned in newswire,
and required systems to cope with name variation
(e.g., ?Osama Bin Laden? / ?Usama Bin Laden?
or ?Mark Twain? / ?Samuel Clemens?) as well as
name disambiguation. Furthermore, the task re-
quired detection of when no appropriate KB entry
exists, which is a departure from the conventional
disambiguation problem. The collection contains
over 1.2 million documents, primarily newswire.
Wikipedia was used as a surrogate knowledge
base, and it has been used in several previous stud-
ies (e.g., Cucerzan (2007)). This task is closely re-
lated to CDCR, as mentions that are aligned to the
same knowledge base entry create a coreference
cluster. However, there are no actual CDCR anno-
tations for this corpus, though we used it nonethe-
les as a benchmark corpus to evaluate speed and
to demonstrate scalability.
5 Discussion
5.1 Accuracy
In Table 2 we report cross document coreference
resolution performance for a variety of experi-
mental conditions using the B3 method, which
includes precision, recall, and calculated F?=1
values. For each of the three evaluation corpora
(smith, nytac, and ace08) we report values for two
baseline methods and for similarity metrics us-
ing different types of features. The first baseline,
called Baseline, places each coreference chain in
its own cluster while the second baseline, called
ExactMatch, merges all mentions that match ex-
actly orthographically into the same cluster.
Use of name similarity scores as features (in ad-
dition to their use for candidate cluster selection)
is indicated by rows labeled Ortho. Use of lexi-
cal features is indicated by BoW and use of topic
model features by Topic.
Using topic models as features was more help-
ful than lexical contexts on the smith corpus.
1055
#coref chains Baseline ExactMatch Ortho BOW Topics Ortho+BOW Ortho+Topics
1K
10K
20K
30K
40K
50K
60K
70K
80K
90K
100K
110K
120K
130K
140K
1000 702 699 925 857 699 697
10000 4563 4530 7964 7956 4514 4518
20000 8234 8202 15691 15073 8159 8163
30000 11745 11682 23138 21878 11608 11611
40000 15041 14964 30900 28500 14869 14863
50000 18110 18016 38248 34758 17910 17903
60000 20450 20377 44735 40081 20241 20228
70000 22845 22780 51190 45722 22615 22603
80000 25062 25026 57440 51104 24832 24818
90000 27389 27358 64140 56581 27145 27126
100000 29797 29782 71034 62228 29546 29511
110000 32147 32139 77705 67853 31882 31840
120000 34567 34589 84309 73397 34284 34235
130000 36817 36874 90465 78676 36543 36486
140000 38826 38901 96225 83525 38539 38482
0
37500
75000
112500
150000
1K 20K 40K 60K 80K 100K 120K 140K
#
 
o
f
 
c
l
u
s
t
e
r
s
 
p
r
o
d
u
c
e
d
# of entity chains seen
Baseline ExactMatch
Ortho BOW
Topics Ortho+BOW
Ortho+Topics
Figure 2: Number of clusters produced vs. num-
ber of entity chains observed in the stream. Num-
ber of entity chains is proportional to the number
of documents.
When used alone topic beats BoW, but in com-
bination with the ortho features performance is
equivalent. For both nytac and ace08 heavy re-
liance on orthographic similarity proved hard to
beat. On the ace08 corpus Baron and Freedman
(2008) report B3 F-scores of 83.2 for persons and
67.8 for organizations, and our streaming results
appear to be comparable to their offline method.
The cluster growth induced by the various mea-
sures can be seen in Figure 2. The two base-
line methods, Baseline and ExactMatch, provide
bounds on the cluster growth with all other meth-
ods falling in between.
5.2 Hashing Strategies for Candidate
Selection
Table 3 containsB3 F-scores when different hash-
ing strategies are employed for candidate selec-
tion. The trend appears to be that stricter match-
ing outperforms fuzzier matching; full mentions
tended to beat words, which beat use of the char-
acter bigrams. This agrees with the results de-
scribed in the previous section, which show heavy
reliance on orthographic similarity.
5.3 Timing Results
Figure 3 shows how processing time increases
with the number of entities observed in the ace08
#chains 1 5 10 20
1000 2 0 0 1
2000 2 0 0 1
3000 2 0 0 1
4000 2 0 0 1
5000 2 2 0 4
6000 2 2 0 4
7000 2 2 0 4
8000 2 2 0 4
9000 2 2 0 4
10000 2 2 0 4
11000 2 2 0 4
12000 2 2 0 5
13000 2 2 0 6
14000 2 2 0 6
15000 2 2 0 6
16000 2 2 0 6
17000 2 2 0 6
18000 2 2 0 6
19000 2 2 1 7
20000 2 2 1 7
21000 2 2 2 7
22000 2 2 2 7
23000 2 2 3 7
24000 2 2 3 7
25000 2 2 3 7
26000 2 2 3 7
27000 2 2 4 8
28000 2 2 4 9
29000 2 2 5 10
30000 2 2 8 11
31000 2 2 8 12
32000 2 2 8 13
33000 2 2 8 14
34000 2 2 8 15
35000 2 2 8 16
36000 2 2 8 17
37000 2 2 8 18
38000 2 2 9 19
39000 2 2 9 20
40000 2 2 9 21
41000 2 2 10 22
42000 2 2 10 23
43000 2 2 11 24
44000 2 2 12 25
45000 2 2 12 26
46000 2 2 13 27
47000 2 3 14 28
48000 2 3 15 29
49000 2 4 16 30
50000 2 5 17 32
51000 2 6 18 34
52000 2 7 19 36
53000 2 7 20 38
54000 2 7 21 40
55000 2 8 22 41
56000 2 8 23 43
57000 2 9 24 45
58000 2 10 25 47
59000 2 10 26 48
60000 2 11 27 50
61000 2 12 28 52
62000 2 13 29 54
63000 2 13 30 56
64000 2 14 31 58
65000 2 15 32 60
66000 3 16 33 62
67000 3 17 34 63
68000 3 18 35 65
69000 3 19 36 67
70000 3 19 37 68
71000 4 20 38 70
72000 4 21 39 72
73000 4 22 40 73
74000 5 23 41 75
75000 6 24 42 77
76000 6 25 43 79
77000 6 26 44 81
78000 6 27 45 83
79000 7 28 46 85
80000 7 29 47 87
81000 8 30 48 89
82000 8 31 49 91
83000 9 32 50 93
84000 9 33 51 95
85000 9 34 52 97
86000 9 35 53 99
87000 10 36 54 101
88000 10 37 55 102
89000 10 38 56 104
90000 11 39 57 106
91000 13 40 58 108
92000 14 41 59 111
93000 15 42 60 113
94000 15 43 61 115
95000 16 44 62 117
96000 17 45 63 119
97000 17 46 64 121
98000 18 47 65 123
99000 19 48 66 125
100000 20 49 67 127
101000 21 50 68 129
102000 21 51 69 131
103000 22 52 70 133
104000 23 53 71 136
105000 24 54 72 138
106000 25 55 73 140
107000 25 56 74 142
108000 26 57 75 144
109000 27 58 76 146
110000 28 59 77 148
111000 28 60 78 150
112000 29 61 79 153
113000 30 62 80 156
114000 31 63 81 158
115000 32 64 83 161
116000 33 66 84 163
117000 34 67 85 165
118000 35 68 86 167
119000 36 69 87 169
120000 37 70 88 171
121000 38 71 90 174
122000 39 72 92 177
123000 40 73 93 179
124000 41 74 94 181
125000 42 75 95 183
126000 43 76 96 186
127000 44 77 99 188
128000 45 78 100 191
129000 46 79 101 196
130000 47 80 102 198
131000 48 81 103 201
132000 49 82 104 204
133000 50 83 105 207
134000 51 84 106 210
135000 52 85 107 214
136000 53 86 109 217
137000 54 87 110 219
138000 55 89 112 222
139000 56 90 113 225
140000 57 91 115 228
141000 58 92 117 231
142000 59 93 118 234
143000 62 94 120 237
143442 62 94 121 238
1
10
100
1000
1000 25000 49000 73000 97000 121000
T
i
m
e
 
(
s
e
c
s
)
# of chains processed
1 5 10 20
Figure 3: Elapsed processing time as a function of
bounding the number of candidate clusters consid-
ered for an entity. When fewer candidates are con-
sidered, clustering decisions can be made much
faster.
document stream. We experimented with using an
upper bound on the number of candidate clusters
to consider for an entity.
Figure 4 compares the efficiency of using three
different methods for candidate cluster identifica-
tion. The most restrictive hashing strategy, using
exact mention strings, is the most efficient, fol-
lowed by the use of words, then the use of charac-
ter skip bigrams. This makes intuitive sense ? the
strictest matching reduces the number of candi-
date clusters that have to be considered when pro-
cessing an entity.6
The ace08 corpus contained over 10,000 doc-
uments and is one of the largest CDCR test sets.
In Figure 5 we show how processing time grows
when processing the kbp09 corpus. Doubling the
number of entities processed increases the runtime
by about a factor of 5. The curve is not linear
due to the increasing number of entity cluster?s
that must be considered. Future work will exam-
ine how to keep the number of clusters considered
constant over time, such as ignoring older entities.
6 Conclusion
We have presented a new streaming cross doc-
ument coreference resolution system. Our ap-
proach is substantially faster than previous sys-
6In the limit, if names were unique, hashing on strings
would completely solve the CDCR problem and processing
an entity would be O(1)
1056
smith nytac ace08
Approach bigrams words mention bigrams words mention bigrams words mention
Ortho 0.382 0.553 0.616 0.120 0.695 0.687 0.540 0.797 0.811
BoW 0.480 0.530 0.467 0.344 0.339 0.349 0.551 0.700 0.738
Topic 0.697 0.661 0.579 0.071 0.620 0.363 0.544 0.685 0.750
Ortho+BoW 0.389 0.554 0.618 0.340 0.691 0.686 0.519 0.783 0.809
Ortho+Topic 0.398 0.555 0.618 0.120 0.477 0.680 0.520 0.776 0.819
Table 3: B3 F-scores using different hashing strategies for candidate selection. Name/cluster similarity
could be based on character skip bigrams, words appear in names, or exact matching of mention.
#chains mention string word bigram
1000 2 1 1
2000 2 2 5
3000 2 2 6
4000 2 4 7
5000 3 5 9
6000 4 6 11
7000 5 6 13
8000 5 7 15
9000 6 7 17
10000 6 7 19
11000 7 9 21
12000 7 10 23
13000 7 11 25
14000 9 13 27
15000 9 14 29
16000 10 15 31
17000 10 16 33
18000 11 17 36
19000 12 18 39
20000 12 19 42
21000 13 21 44
22000 13 23 47
23000 13 24 50
24000 14 26 53
25000 15 28 56
26000 16 30 60
27000 16 32 64
28000 17 34 68
29000 17 36 71
30000 18 39 75
31000 19 40 79
32000 21 43 83
33000 22 45 88
34000 23 47 92
35000 24 49 96
36000 26 51 100
37000 26 53 104
38000 27 55 108
39000 27 57 112
40000 28 59 117
41000 29 63 121
42000 30 66 125
43000 31 70 129
44000 33 73 133
45000 34 77 137
46000 35 80 142
47000 36 84 146
48000 36 87 150
49000 38 90 154
50000 39 94 158
51000 40 98 162
52000 41 101 167
53000 42 104 171
54000 44 107 175
55000 45 111 179
56000 46 115 183
57000 48 119 187
58000 49 122 192
59000 49 126 196
60000 50 130 200
61000 51 134 204
62000 52 139 208
63000 53 143 212
64000 56 146 217
65000 57 150 222
66000 59 155 227
67000 60 158 232
68000 62 163 237
69000 62 167 242
70000 63 170 247
71000 65 173 252
72000 66 178 257
73000 67 183 262
74000 67 186 267
75000 68 191 273
76000 69 194 277
77000 70 198 282
78000 70 202 287
79000 71 207 292
80000 72 211 297
81000 73 215 302
82000 73 219 307
83000 74 224 312
84000 74 229 317
85000 75 233 322
86000 76 238 328
87000 77 243 333
88000 77 246 338
89000 78 250 343
90000 78 254 348
91000 79 258 353
92000 80 262 358
93000 80 268 363
94000 81 273 368
95000 82 277 373
96000 82 281 378
97000 83 284 384
98000 84 288 389
99000 84 293 394
100000 85 297 400
101000 85 300 406
102000 85 303 413
103000 85 307 419
104000 85 310 425
105000 86 314 431
106000 87 319 438
107000 89 325 444
108000 90 330 450
109000 91 334 456
110000 91 339 463
111000 92 343 469
112000 92 348 476
113000 93 352 482
114000 94 357 489
115000 95 362 495
116000 95 366 501
117000 96 369 507
118000 96 373 513
119000 96 377 520
120000 97 382 526
121000 97 387 532
122000 100 392 539
123000 100 395 546
124000 101 399 552
125000 101 403 558
126000 102 408 564
127000 102 412 571
128000 103 417 577
129000 104 421 583
130000 104 425 589
131000 105 430 596
132000 106 435 602
133000 108 441 609
134000 109 446 615
135000 111 452 622
136000 112 457 628
137000 113 461 634
138000 113 467 640
139000 114 472 648
140000 115 479 655
141000 116 484 662
142000 117 489 669
143000 118 494 676
143442 118 497 679
1
10
100
1000
1000 25000 49000 73000 97000 121000
T
i
m
e
 
(
s
e
c
s
)
# of coref chains processed
mention string word bigram
Figure 4: Comparison of three hashing strategies
for identifying candidate clusters for a given en-
tity. The more restrictive strategies lead to faster
processing as fewer candidates are considered.
tems, and our experiments have demonstrated
scalability to an order of magnitude larger data
than previously published evaluations. Despite its
speed and simplicity, we still obtain competitive
results on a variety of data sets as compared with
batch systems. In future work, we plan to investi-
gate additional similarity metrics that can be com-
puted efficiently, as well as experiments on web
scale corpora.
References
Artiles, Javier, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search: results of the first evalua-
tion and the plan for the second. In World Wide Web
(WWW).
Bagga, Amit and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Conference on Computational
Linguistics (COLING).
Baron, Alex and Marjorie Freedman. 2008. Who
#coref chains processed Time (secs)
1K 1.5
100K 10
200K 40
400K 120
600K 700
900K 920
1.1M 1200
1
10
100
1000
10000
1K 100K 200K 400K 600K 900K 1.1M
T
i
m
e
 
(
s
e
c
s
)
# of coref chains processed
Figure 5: The number of coreference chains pro-
cessed over time in the kbp09 corpus. The pro-
cessing of over 1 million coreference chains is at
least an order of magnitude larger than previous
systems reported.
is Who and What is What: Experiments in cross-
document co-reference. In Empirical Methods in
Natural Language Processing (EMNLP).
Blei, D.M., A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research (JMLR), 3:993?1022.
Can, F. and E. Ozkarahan. 1987. A dynamic clus-
ter maintenance system for information retrieval. In
Conference on Research and Development in Infor-
mation Retrieval (SIGIR).
Charikar, Moses, Chandra Chekuri, Toma?s Feder, and
Rajeev Motwani. 1997. Incremental clustering and
dynamic information retrieval. In ACM Symposium
on Theory of Computing (STOC).
Chen, Ying and James Martin. 2007. Towards ro-
bust unsupervised personal name disambiguation.
In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Choi, Y. and C. Cardie. 2007. Structured local training
and biased potential functions for conditional ran-
dom fields with application to coreference resolu-
tion. In North American Chapter of the Association
1057
for Computational Linguistics (NAACL), pages 65?
72.
Cucerzan, Silviu. 2007. Large-scale named entity
disambiguation based on wikipedia data. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 708?716.
Firth, J.R. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis, pages 1?32.
Oxford: Philological Society.
Gooi, Chung Heong and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
North American Chapter of the Association for
Computational Linguistics (NAACL).
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Association for Computational Linguistics (ACL).
Mann, Gideon S. and David Yarowsky. 2003. Unsu-
pervised personal name disambiguation. In Confer-
ence on Natural Language Learning (CONLL).
McNamee, Paul and Hoa Dang. 2009. Overview of
the TAC 2009 knowledge base population track. In
Text Analysis Conference (TAC).
Ng, V. and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Asso-
ciation for Computational Linguistics (ACL), pages
104?111.
Nicolae, C. and G. Nicolae. 2006. Bestcut: A
graph algorithm for coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 275?283. Association for Compu-
tational Linguistics.
Omiecinski, Edward and Peter Scheuermann. 1984. A
global approach to record clustering and file reorga-
nization. In Conference on Research and Develop-
ment in Information Retrieval (SIGIR).
Pradhan, S.S., L. Ramshaw, R. Weischedel,
J. MacBride, and L. Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events
in ontonotes. In International Conference on
Semantic Computing (ICSC).
Ramshaw, L. and R. Weischedel. 2005. Information
extraction. In IEEE ICASSP.
Sanderson, Mark. 2000. Retrieving with good sense.
Information Retrieval, 2(1):45?65.
Sandhaus, Evan. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics.
Yao, L., D. Mimno, and A. McCallum. 2009. Effi-
cient methods for topic model inference on stream-
ing document collections. In Knowledge discovery
and data mining (KDD).
1058
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 460?470,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
NLP on Spoken Documents without ASR
Mark Dredze, Aren Jansen, Glen Coppersmith, Ken Church
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
mdredze,aren,coppersmith,Kenneth.Church@jhu.edu
Abstract
There is considerable interest in interdis-
ciplinary combinations of automatic speech
recognition (ASR), machine learning, natu-
ral language processing, text classification and
information retrieval. Many of these boxes,
especially ASR, are often based on consid-
erable linguistic resources. We would like
to be able to process spoken documents with
few (if any) resources. Moreover, connect-
ing black boxes in series tends to multiply er-
rors, especially when the key terms are out-of-
vocabulary (OOV). The proposed alternative
applies text processing directly to the speech
without a dependency on ASR. The method
finds long (? 1 sec) repetitions in speech,
and clusters them into pseudo-terms (roughly
phrases). Document clustering and classi-
fication work surprisingly well on pseudo-
terms; performance on a Switchboard task ap-
proaches a baseline using gold standard man-
ual transcriptions.
1 Introduction
Can we do IR-like tasks without ASR? Information
retrieval (IR) typically makes use of simple features
that count terms within/across documents such as
term frequency (tf) and inverse document frequency
(IDF). Crucially, to compute these features, it is suf-
ficient to count repetitions of a term. In particular,
for many IR-like tasks, there is no need for an au-
tomatic speech recognition (ASR) system to label
terms with phonemes and/or words.
This paper builds on Jansen et al (2010), a
method for discovering terms with zero resources.
This approach identifies long, faithfully repeated
patterns in the acoustic signal. These acoustic repe-
titions often correspond to terms useful for informa-
tion retrieval tasks. Critically, this method does not
require a phonetically interpretable acoustic model
or knowledge of the target language.
By analyzing a large untranscribed corpus of
speech, this discovery procedure identifies a vast
number of repeated regions that are subsequently
grouped using a simple graph-based clustering
method. We call the resulting groups pseudo-terms
since they typically represent a single word or phrase
spoken at multiple points throughout the corpus.
Each pseudo-term takes the place of a word or
phrase in bag of terms vector space model of a text
document, allowing us to apply standard NLP algo-
rithms. We show that despite the fully automated
and noisy method by which the pseudo-terms are
created, we can still successfully apply NLP algo-
rithms with performance approaching that achieved
with the gold standard manual transcription.
Natural language processing tools can play a key
role in understanding text document collections.
Given a large collection of text, NLP tools can clas-
sify documents by category (classification) and or-
ganize documents into similar groups for a high
level view of the collection (clustering). For exam-
ple, given a collection of news articles, these tools
can be applied so that the user can quickly see the
topics covered in the news articles, and organize the
collection to find all articles on a given topic. These
tools require little or no human input (annotation)
and work across languages.
Given a large collection of speech, we would like
460
tools that perform many of the same tasks, allow-
ing the user to understand the contents of the col-
lection while listening to only small portions of the
audio. Previous work has applied these NLP tools
to speech corpora with similar results (see Hazen
et al (2007) and the references therein.) However,
unlike text, which requires little or no preprocess-
ing, audio files are typically first transcribed into
text before applying standard NLP tools. Automatic
speech recognition (ASR) solutions, such as large
vocabulary continuous speech recognition (LVCSR)
systems, can produce an automatic transcript from
speech, but they require significant development ef-
forts and training resources, typically hundreds of
hours of manually transcribed speech. Moreover,
the terms that may be most distinctive in particular
spoken documents often lie outside the predefined
vocabulary of an off-the-shelf LVCSR system. This
means that unlike with text, where many tools can be
applied to new languages and domains with minimal
effort, the equivalent tools for speech corpora often
require a significant investment. This greatly raises
the entry threshold for constructing even a minimal
tool set for speech corpora analysis.
The paper proceeds as follows. After a review
of related work, we describe Jansen et al (2010),
a method for finding repetitions in speech. We
then explain how these repetitions are grouped into
pseudo-terms. Document clustering and classifica-
tion work surprisingly well on pseudo-terms; perfor-
mance on a Switchboard task approaches a baseline
based on gold standard manual transcriptions.
2 Related Work
In the low resource speech recognition regime,
most approaches have focused on coupling small
amounts of orthographically transcribed speech (10s
of hours) with much larger collections of untran-
scribed speech (100s or 1000s of hours) to train ac-
curate acoustic models with semi-supervised meth-
ods (Novotney and Schwartz, 2009). In these ef-
forts, the goal is to reduce the annotation require-
ments for the construction of competent LVCSR sys-
tems. This semi-supervised paradigm was relaxed
even further with the pursuit of self organizing units
(SOUs), phone-like units for which acoustic mod-
els are trained with completely unsupervised meth-
ods (Garcia and Gish, 2006). Even though the move
away from phonetic acoustic models improves the
universality of the architecture, small amounts of or-
thographic transcription are still required to connect
the SOUs with the lexicon.
The segmental dynamic time warping (S-DTW)
algorithm (Park and Glass, 2008) was the first truly
zero resource effort, designed to discover portions of
the lexicon directly by searching for repeated acous-
tic patterns in the speech signal. This work im-
plicitly defined a new direction for speech process-
ing research: unsupervised spoken term discovery,
the entry point of our speech corpora analysis sys-
tem. Subsequent extensions of S-DTW (Jansen et
al., 2010) permit applications to much larger speech
collections, a flexibility that is vital to our efforts.
As mentioned above, the application of NLP
methods to speech corpora have traditionally relied
on high resource ASR systems to provide automatic
word or phonetic transcripts. Spoken document
topic classification has been an application of partic-
ular interest (Hazen et al, 2007), for which the rec-
ognized words or phone n-grams are used to charac-
terize the documents. These efforts have produced
admirable results, with ASR transcript-based per-
formance approached that obtained using the gold
standard manual transcripts. Early efforts to per-
form automatic topic segmentation of speech input
without the aid of ASR systems have been promis-
ing (Malioutov et al, 2007), but have yet to exploit
the full the range of NLP tools.
3 Identifying Matched Regions
Our goal is to identify pairs of intervals within and
across utterances of several speakers that contain
the same linguistic content, preferably meaningful
words or terms.
The spoken term discovery algorithm of Jansen et
al. (2010) efficiently searches the space of
(n
2
)
in-
tervals, where n is the number of speech frames.1
Jansen et al (2010) is based on dotplots (Church and
Helfman, 1993), a method borrowed from bioinfor-
matics for finding repetitions in DNA sequences.
1Typically, each frame represents a 25 or 30 ms window of
speech sampled every 10 ms
461
  
t e x t  p r o c e s s i n g  v s .  s p e e c h  p r o c e s s i n gt
ex
t 
pr
oc
es
si
ng
 v
s.
 s
pe
ec
h 
pr
oc
es
si
ng
Figure 1: An example of a dotplot for the string ?text
processing vs. speech processing? plotted against itself.
The box calls out the repeated substring: ?processing.?
3.1 Acoustic Dotplots
When applied to text, the dotplot construct is re-
markably simple: given character strings s1 and s2,
the dotplot is a Boolean similarity matrix K(s1, s2)
defined as
Kij(s1, s2) = ?(s1[i], s2[j]).
Substrings common to s1 and s2 manifest them-
selves as diagonal line segments in the visualization
of K. Figure 1 shows an example text dotplot where
both s1 and s2 are taken to be the string ?text pro-
cessing vs. speech processing.? The boxed diago-
nal line segment arises from the repeat of the word
?processing,? while the main diagonal line trivially
arises from self-similarity. Thus, the search for line
segments inK off the main diagonal provides a sim-
ple algorithmic means to identify repeated terms of
possible interest, albeit sometimes partial, in a col-
lection of text documents. The challenge is to gen-
eralize these dotplot techniques for application to
speech, an inherently noisy, real-valued data stream.
The strategy is to replace character strings with
frame-based speech representations of the form
x = x1, x2, . . . xN , where each xi ? Rd is a d-
dimensional vector space representation of the ith
overlapping window of the signal. Given vector time
series x = x1, x2, . . . xN and y = y1, y2, . . . yM for
two spoken documents, the acoustic dotplot is the
real-valuedN?M cosine similarity matrixK(x,y)
defined as
Time (s)
Tim
e (s
)
K(qi,qj)
 
 
1 2 3 4 5 6 7 8
1
2
3
4
5
6
7
8
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Figure 2: An example of an acoustic dotplot for 8 seconds
of speech (posteriorgrams) plotted against itself. The box
calls out a repetition of interest.
Kij(x,y) =
1
2
[
1 +
?xi, yj?
?xi??yj?
]
. (1)
Even though the application to speech is a distinctly
noisier endeavor, sequences of frames repeated be-
tween the two audio clips will still produce approx-
imate diagonal lines in the visualization of the ma-
trix. The search for matched regions thus reduces
to the robust search for diagonal line segments in
K, which can be efficiently performed with standard
image processing techniques.
Included in this procedure is the application of a
diagonal median filter of duration ? seconds. The
choice of ? determines an approximate threshold
on the duration of the matched regions discovered.
Large ? values (? 1 sec) will produce a relatively
sparse list of matches corresponding to long words
or short phrases; smaller ? values (< 0.5 sec)
will admit shorter words and syllables that may be
less informative from a document analysis perspec-
tive. Given the approximate nature of the procedure,
shorter ? values also admit less reliable matches.
3.2 Posteriorgram Representation
The acoustic dotplot technique can operate on any
vector time series representation of the speech sig-
nal, including a standard spectrogram. However, at
the individual frame level, the cosine similarities be-
tween frequency spectra of distinct speakers produc-
ing the same phoneme are not guaranteed to be high.
462
Time (s)
Pho
ne
 
 
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
silaa
aeah
aoaw
axaxr
ayb
chd
dheh
elem
ener
eyf
ghh
ihiy
jhk
lm
nng
owoy
pr
ssh
tth
uhuw
vw
yz
zh
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 3: An example of a posteriorgram.
Thus, to perform term discovery across a multi-
speaker corpus, we require a speaker-independent
representation. Phonetic posteriorgrams are a suit-
able choice, as each frame is represented as the pos-
terior probability distribution over a set of speech
sounds given the speech observed at the particular
point in time, which is largely speaker-independent
by construction. Figure 3 shows an example poste-
riorgram for the utterance ?I had to do that,? com-
puted with a multi-layer perceptron (MLP)-based
English phonetic acoustic model (see Section 5 for
details). Each row of the figure represents the pos-
terior probability of the given phone as a function of
time through the utterance and each column repre-
sents the posterior distribution over the phone set at
that particular point in time.
The construction of speaker independent acous-
tic models typically requires a significant amount of
transcribed speech. Our proposed strategy is to em-
ploy a speaker independent acoustic model trained
in a high resource language or domain to interpret
multi-speaker data in the zero resource target set-
ting.2 Indeed, we do not need to know a language
to detect when a word of sufficient length has been
repeated in it.3 By computing cosine similarities
2A similarly-minded approach was taken in Hazen et al
(2007) and extended in Hazen and Margolis (2008), where the
authors use Hungarian phonetic trigrams features to character-
ize English spoken documents for a topic classification task.
3While in this paper our acoustic model is based on our eval-
uation corpus, this is not a requirement of our approach. Future
work will investigate performance of other acoustic models.
of phonetic posterior distribution vectors (as op-
posed to reducing the speech to a one-best phonetic
token sequence), the phone set used need not be
matched to the target language. With this approach,
a speaker-independent model trained on the phone
set of a reference language may be used to perform
speaker independent term discovery in any other.
In addition to speaker independence, the use of
phonetic posteriorgrams introduces representational
sparsity that permits efficient dotplot computation
and storage. Notice that the posteriorgram dis-
played in Figure 3 consists of mostly near-zero val-
ues. Since cosine similarity (Equation 1) between
two frames can only be high if they have significant
mass on the same phone, most comparisons need not
be made. Instead, we can apply a threshold and store
each posteriorgram as an inverted file, performing
inner product multiplies and adds only when they
contribute. Using a grid of approximately 100 cores,
we were able to perform theO(n2) dotplot computa-
tion and line segment search for 60+ hours of speech
(corresponding to a 500 terapixel dotplot) in approx-
imately 5 hours.
Figure 2 displays the posteriorgram dotplot for
8 seconds of speech against itself (i.e., x = y).
The prominent main diagonal line results from self-
similarity, and thus is ignored in the search. The
boxed diagonal line segment results from two dis-
tinct occurrences of the term one million dollars.
The large black boxes in the image result from
long stretches of silence of filled pauses; fortunately,
these are easily filtered with speech activity detec-
tion or simple measures of posteriorgram stability.
4 Creating Pseudo-Terms
Spoken documents will be represented as bags of
pseudo-terms, where pseudo-terms are computed
from acoustic repetitions described in the previous
section. Let M be a set of matched regions (m),
each consisting of a pair of speech intervals con-
tained in the corpus (m = [t(i)1 , t
(i)
2 ], [t
(j)
1 , t
(j)
2 ] indi-
cates the speech from t(i)1 to t
(i)
2 is an acoustic match
to the speech from t(j)1 to t
(j)
2 ). If a particular term
occurs k times, the setM can include as many as
(k
2
)
distinct elements corresponding to that term, so we
require a procedure to group them into clusters. We
call the resulting clusters pseudo-terms since each
463
cluster is a placeholder for a term (word or phrase)
spoken in the collection. Given the match list M
and the pseudo-term clusters, it is relatively straight-
forward to represent spoken documents as bags of
pseudo-terms.
To perform this pseudo-term clustering we repre-
sented matched regions as vertices in a graph with
edges representing similarities between these re-
gions. We employ a graph-clustering algorithm that
extracts connected components. Let G = (V,E) be
an unweighted, undirected graph with vertex set V
and edge set E. Each vi ? V corresponds to a sin-
gle speech interval ([t(i)1 , t
(i)
2 ]) present in M (each
m ?M has a pair of such intervals, so |V | = 2|M|)
and each eij ? E is an edge between vertex vi and
vj .
The set E consists of two types of edges. The
first represents repeated speech at distinct points
in the corpus as determined by the match list M.
The second represents near-identical intervals in the
same utterance (i.e. the same speech) since a sin-
gle interval can show up in several matches in M
and the algorithm in Section 3 explicitly ignores
self-similarity. Given the intervals [t(i)1 , t
(i)
2 ] and
[t(j)1 , t
(j)
2 ] contained in the same utterance and with
corresponding vertices vi, vj ? V , we introduce
an edge eij if fractional overlap fij exceeds some
threshold ? , where fij = max(0, rij) and
rij =
(t(i)2 ? t
(i)
1 ) + (t
(j)
2 ? t
(j)
1 )
max(t(i)2 , t
(j)
2 )?min(t
(i)
1 , t
(j)
1 )
? 1. (2)
From the graph G, we produce one pseudo-term
for each connected component. More sophisticated
edge weighting schemes would likely provide ben-
efit. In particular, we expect improved clustering
by introducing weights that reflect acoustic sim-
ilarity between match intervals, rather than rely-
ing solely upon the term discovery algorithm to
make a hard decision. Such confidence weights
would allow even shorter pseudo-terms to be con-
sidered (by reducing ?) without greatly increasing
false alarms. With such a shift, more sophisticated
graph-clustering mechanisms would be warranted
(e.g. Clauset et al (2004)). We plan to pursue this
in future work.
Counts Terms
5 keep track of
5 once a month
2 life insurance
2 capital punishment
9 paper; newspaper
3 talking to you
Table 1: Pseudo-terms resulting from a graph clustering
of matched regions (? = 0.75, ? = 0.95). Counts indi-
cate the number of times the times the pseudo-terms ap-
pear across 360 conversation sides in development data.
Table 1 contains several examples of pseudo-
terms and the matched regions included in each
group. The orthographic forms are taken from the
transcripts in the data (see Section 5). Note that for
some pseudo-terms, the words match exactly, while
for others, the phrases are distinct but phonetically
similar. However, even in this case, there is often
substantial overlap in the spoken terms.
5 Data
For our experiments we used the Switchboard
Telephone Speech Corpus (Godfrey et al, 1992).
Switchboard is a collection of roughly 2,400 two-
sided telephone conversations with a single partici-
pant per side. Over 500 participants were randomly
paired and prompted with a topic for discussion.
Each conversation belongs to one of 70 pre-selected
topics with the two sides restricted to separate chan-
nels of the audio.
To develop and evaluate our methods, we cre-
ated three data sets from the Switchboard corpus: a
development data set, a held out tuning data
set and an evaluation data set. The develop-
ment data set was created by selecting the six most
commonly prompted topics (recycling, capital pun-
ishment, drug testing, family finance, job benefits,
car buying) and randomly selecting 60 sides of con-
versations evenly across the topics (total 360 con-
versation sides.) This corresponds to 35.7 hours of
audio. Note that each participant contributed at most
one conversation side per topic, so these 360 conver-
sation sides represent 360 distinct speakers. All al-
gorithm development and experimentation was con-
ducted exclusively on the development data.
For the tuning data set, we selected an additional
60 sides of conversations evenly across the same six
topics used for development, for a total of 360 con-
464
versations and 37.5 hours of audio. This data was
used to validate our experiments on the develop-
ment data by confirming the heuristic used to select
algorithmic parameters, as described below. This
data was not used for algorithm development. The
evaluation data set was created once parameters had
been selected for a final evaluation of our methods.
We selected this data by sampling 100 conversation
sides from the next six most popular conversation
topics (family life, news media, public education,
exercise/fitness, pets, taxes), yielding 600 conversa-
tion sides containing 61.6 hours of audio.
In our experiments below, we varied the match
duration ? between 0.6 s and 1.0 s and the overlap
threshold ? between 0.75 and 1.0. We measured the
resulting effects on the number of unique pseudo-
terms generated by the process. In general, de-
creasing ? results in more matched regions increas-
ing the number of pseudo-terms. Similarly, increas-
ing ? forces fewer regions to be merged, increasing
the total number of pseudo-terms. Table 2 shows
how these parameters change the number of pseudo-
terms (features) per document and the average num-
ber of occurrences of each pseudo-term. The user
could tune these parameters to select pseudo-terms
that were long and occurred in many documents. In
the next sections, we consider how these parameters
effect performance of various learning settings.
To provide the requisite speaker independent
acoustic model, we compute English phone pos-
teriorgrams using the multi-stream multi-layer
perceptron-based architecture of Thomas et al
(2009), trained on 300 hours of conversational tele-
phone speech. While this is admittedly a large
amount of supervision, it is important to emphasize
our zero resource term discovery algorithm does not
rely on the phonetic interpretability of this refer-
ence acoustic model. The only requirement is that
the same target language phoneme spoken by dis-
tinct speakers map to similar posterior distributions
over the reference language phoneme set. Thus,
even though we evaluate the system on matched-
language Switchboard data, it can be just as easily
applied to any target language with no language-
specific knowledge or training resources required.4
4The generalization of the speaker independence of acous-
tic models across languages is not well understood. Indeed, the
performance of our proposed system would depend to some ex-
? ? Features Feat. Frequency Feat./Doc.
0.6 0.75 5,809 2.15 34.7
0.6 0.85 23,267 2.22 143.4
0.6 0.95 117,788 2.38 779.8
0.6 1.0 333,816 2.32 2153.4
0.75 0.75 8,236 2.31 52.8
0.75 0.85 18,593 2.36 121.7
0.75 0.95 48,547 2.36 318.2
0.75 1.0 90,224 2.18 546.9
0.85 0.75 5,645 2.52 39.5
0.85 0.85 8,832 2.44 59.8
0.85 0.95 15,805 2.24 98.3
0.85 1.0 24,480 2.10 142.4
1.0 0.75 1,844 2.39 12.3
1.0 0.85 2,303 2.24 14.4
1.0 0.95 3,239 2.06 18.6
1.0 1.0 4,205 1.93 22.7
Table 2: Statistics on the number of features (pseudo-
terms) generated for different settings of the match dura-
tion ? and the overlap threshold ? .
6 Document Clustering
We begin by considering document clustering, a
popular approach to discovering latent structure in
document collections. Unsupervised clustering al-
gorithms sort examples into groups, where each
group contains documents that are similar. A user
exploring a corpus can look at a few documents in
each cluster to gain an overview of the content dis-
cussed in the corpus. For example, clustering meth-
ods can be used on search results to provide quick
insight into the coverage of the returned documents
(Zeng et al, 2004).
Typically, documents are clustered based on a bag
of words representation. In the case of clustering
conversations in our collection, we would normally
obtain a transcript of the conversation and then ex-
tract a bag of words representation for clustering.
The resulting clusters may represent topics, such as
the six topics used in our switchboard data. Such
groupings, available with no topic labeled training
data, can be a valuable tool for understanding the
contents of a speech data collection. We would like
to know if similar clustering results can be obtained
without the use of a manual or automatic transcript.
In our case, we substitute the pseudo-terms discov-
ered in a conversation for the transcript, representing
tent on the phonetic similarity of the target and reference lan-
guage. Unsupervised learning of speaker independent acoustic
models remains an important area of future research.
465
the document as a bag of pseudo-terms instead of ac-
tual words. Can a clustering algorithm achieve sim-
ilar results along topical groups with our transcript-
free representation as it can with a full transcript?
In our experiments, we use the six topic labels
provided by Switchboard as the clustering labels.
The goal is to cluster the data into six balanced
groups according to these topics. While Switch-
board topics are relatively straightforward to iden-
tify since the conversations were prompted with spe-
cific topics, we believe this task can still demon-
strate the effectiveness of our representation relative
to the baseline methods. After all, topic classifica-
tion without ASR is still a difficult task.
6.1 Evaluation Metrics
There are numerous approaches to evaluating clus-
tering algorithms. We consider several methods: Pu-
rity, Entropy and B-Cubed. For a full treatment of
these metrics, see Amigo? et al (2009).
Purity measures the precision of each cluster, i.e.,
how many examples in each cluster belong to the
same true topic. Purity ranges between zero and one,
with one being optimal. While optimal purity can be
obtained by putting each document in its own clus-
ter, we fix the number of clusters in all experiments
so purity numbers are comparable. The purity of a
cluster is defined as the largest percentage of exam-
ples in a cluster that have the same topic label. Purity
of the entire clustering is the average purity of each
cluster:
purity(C,L) =
1
N
?
ci?C
max
lj?L
|ci ? lj | (3)
where C is the clustering, L is the reference label-
ing, and N are the number of examples. Following
this notation, ci is a specific cluster and lj is a spe-
cific true label.
Entropy measures how the members of a cluster
are distributed amongst the true labels. The global
metric is computed by taking the weighted aver-
age of the entropy of the members of each cluster.
Specifically, entropy(C,L) is given by:
?
?
ci?C
Ni
N
?
lj?L
P (ci, lj) log2 P (ci, lj) (4)
where Ni is the number of instances in cluster i,
P (ci, lj) is the probability of seeing label lj in clus-
ter ci and the other variables are defined as above.
B-Cubed measures clustering effectiveness from
the perspective of a user?s inspecting the clustering
results (Bagga and Baldwin, 1998). B-Cubed preci-
sion can be defined as an algorithm as follows: sup-
pose a user randomly selects a single example. She
then proceeds to inspect every other example that
occurs in the same cluster. How many of these items
will have the same true label as the selected exam-
ple (precision)? B-Cubed recall operates in a sim-
ilar fashion, but it measures what percentage of all
examples that share the same label as the selected
example will appear in the selected cluster. Since B-
Cubed averages its evaluation over each document
and not each cluster, it is less sensitive to small er-
rors in large clusters as opposed to many small errors
in small clusters. We include results for B-Cubed
F1, the harmonic mean of precision and recall.
6.2 Clustering Algorithms
We considered several clustering algorithms: re-
peated bisection, globally optimal repeated bisec-
tion, and agglomerative clustering (see Karypis
(2003) for implementation details). Each bisection
algorithm is run 10 times and the optimal clustering
is selected according to a provided criteria function
(no true labels needed). For each clustering method,
we evaluated several criteria functions. Addition-
ally, we considered different scalings of the feature
values (the number of times the pseudo-terms ap-
pear in each document). We found that scaling each
feature by the inverse document frequency, effec-
tively TFIDF, produced the best results, so we use
that scaling in all of our experiments. We also ex-
plored various similarity metrics and found cosine
similarity to be the most effective.
We used the Cluto clustering library for all clus-
tering experiments (Karypis, 2003). In the following
section, we report results for the optimal clustering
configuration based on experiments on the develop-
ment data.
6.3 Baselines
We compared our pseudo-term feature set perfor-
mance to two baselines: (1) Phone Trigrams and
466
(2) Word Transcripts. The Phone Trigram base-
line is derived automatically using an approach sim-
ilar to Hazen et al (2007). This baseline is based
on a vanilla phone recognizer on top of the same
MLP-based acoustic model (see Section 5 and the
references therein for details) used to discover the
pseudo-terms. In particular, the phone posterior-
grams were transformed to frame-level monophone
state likelihoods (through division by the frame-
level priors). These state likelihoods were then used
along with frame-level phone transition probabilities
to Viterbi decode each conversation side. It is impor-
tant to emphasize that the reliability of phone recog-
nizers depends on the phone set matching the appli-
cation language. Using the English acoustic model
in this manner on another language will significantly
degrade the performance numbers reported below.
The Word Transcript baseline starts with Switch-
board transcripts. This baseline serves as an upper
bound of what large vocabulary recognition can pro-
vide for this task. n-gram features are computed
from the transcript. Performance is reported sepa-
rately for unigrams, bigrams and trigrams.
6.4 Results
To optimize parameter settings, match duration (?)
and overlap threshold (? ) were swept over a wide
range (0.6 < ? < 1.0 and 0.75 < ? < 1.0) using a
variety of clustering algorithms and training criteria.
Initial results on development data showed promis-
ing performance for the default I2 criteria in Cluto
(repeated bisection set to maximize the square root
of within cluster similarity). Representative results
on development data with various parameter settings
for this clustering configuration appear in Table 3.
A few observations about results on development
data. First, the three evaluation metrics are strongly
correlated. Second, for each ? the same narrow
range of ? values achieve good results. In general,
settings of ? > 0.9 were all comparable. Essen-
tially, setting a high threshold for merging matched
regions was sufficient without further tuning. Third,
we observed that decreasing ? meant more features,
but that these additional features did not necessarily
lead to more useful features for clustering. For ex-
ample, ? = 0.70 gave a small number of reasonably
good features, while ? = 0.60 can give an order of
magnitude more features without much of a change
Pseudo-term Results
? ? Features Purity Entropy B3 F1
0.60 0.95 117,788 0.9639 0.2348 0.9306
0.60 0.96 143,299 0.9750 0.1664 0.9518
0.60 0.97 178,559 0.9667 0.2116 0.9366
0.60 0.98 223,511 0.9528 0.2717 0.9133
0.60 0.99 333,630 0.9583 0.2641 0.9210
0.60 1.0 333,816 0.9583 0.2641 0.9210
0.70 0.93 58,303 0.9528 0.3114 0.9105
0.70 0.94 66,054 0.9667 0.2255 0.9358
0.70 0.95 74,863 0.9583 0.2669 0.9210
0.70 0.96 86,070 0.9611 0.2529 0.9260
0.70 0.97 100,623 0.9639 0.2326 0.9312
0.70 0.98 117,535 0.9556 0.2821 0.9158
0.70 0.99 161,219 0.9056 0.4628 0.8372
0.70 1.0 161,412 0.9333 0.4011 0.8760
Phone Recognizer Baseline
Type Features Purity Entropy B3 F1
Phone Trigram 28,110 0.6194 1.3657 0.5256
Manual Word Transcript Baselines
Type Features Purity Entropy B3 F1
Word Unigram 7,330 0.9917 0.0559 0.9839
Word Bigram 74,216 0.9833 0.1111 0.9678
Word Trigram 224,934 0.9889 0.0708 0.9787
Table 3: Clustering results on development data using
globally optimal repeated bisection and I2 criteria. The
best results over the manual word transcript baselines
and for each match duration (?) are highlighted in bold.
Pseudo-term results are better than the phonetic baseline
and almost as good as the transcript baseline.
in clustering performance. Finally, while pseudo-
term results are not as good as with the manual
transcripts (unigrams), they achieve similar results.
Compared with the phone trigram features deter-
mined by the phone recognizer output, the pseudo-
terms perform significantly better. Note that these
two automatic approaches were built using the iden-
tical MLP-based phonetic acoustic model.
We sought to select the optimal parameter settings
for running on the evaluation data using the devel-
opment data and the held out tuning data. We de-
fined the following heuristic to select the optimal pa-
rameters. We choose settings for ?, ? and the clus-
tering parameters that independently maximize the
performance averaged over all runs on development
data. We then selected the single run correspond-
ing to these parameter settings and checked the re-
sult on the held out tuning data. This setting was
also the best performer on the held out set, so we
used these parameters for evaluation. The best per-
forming parameters were globally optimal repeated
467
? ? Features Purity Entropy B3 F1
0.70 0.98 123,901 0.9778 0.1574 0.9568
Phone Trigram 28,374 0.6389 1.2345 0.5513
Word Unigram 7,640 0.9972 0.0204 0.9945
Word Bigram 77,201 0.9972 0.0204 0.9945
Word Trigram 233,744 0.9972 0.0204 0.9945
Table 4: Results on held out tuning data. The parameters
(globally optimal repeated bisection clustering with I2
criteria, ? = 0.70 seconds and ? = 0.98) were selected
using the development data and validated on tuning data.
Note that the clusters produced by each manual transcript
test were identical in this case.
? ? Features Purity Entropy B3 F1
0.70 0.98 279,239 0.9517 0.3366 0.9073
Phone Trigram 31,502 0.7000 1.0496 0.6355
Word Unigram 9939 0.9883 0.0831 0.9772
Word Bigram 110,859 0.9883 0.0910 0.9771
Word Trigram 357,440 0.9900 0.0775 0.9803
Table 5: Results on evaluation data. The parameters
(globally optimal repeated bisection clustering with I2
criteria, ? = 0.7 seconds and ? = 0.98) were selected
using the development data and validated on tuning data.
bisection clustering with I2 criteria, ? = 0.7 s and
? = 0.98. Note that examining Table 3 alone may
suggest other parameters, but we found our selection
method to yield optimal results on the tuning data.
Results on held out tuning and evaluation data for
this setting compared to the manual word transcripts
and phone recognizer output are shown in Tables
4 and 5. On both the tuning data and evaluation
data, we obtain similar results as on the development
data. While the manual transcript baseline is bet-
ter than our pseudo-term representations, the results
are quite competitive. This demonstrates that use-
ful clustering results can be obtained without a full-
blown word recognizer. Notice also that the pseudo-
term performance remains significantly higher than
the phone recognizer baseline on both sets.
7 Supervised Document Classification
Unsupervised clustering methods are attractive since
they require no human annotations. However, ob-
taining a few labeled examples for a simple label-
ing task can be done quickly, especially with crowd
sourcing systems such as CrowdFlower and Ama-
zon?s Mechanical Turk (Snow et al, 2008; Callison-
Burch and Dredze, 2010). In this setting, a user
may listen to a few conversations and label them by
topic. A supervised classification algorithm can then
be trained on these labeled examples and used to au-
tomatically categorize the rest of the data. In this
section, we evaluate if supervised algorithms can be
trained using the pseudo-term representation of the
speech.
We set up a multi-class supervised classification
task, where each document is labeled using one of
the six Switchboard topics. A supervised learning
algorithm is trained on a sample of labeled docu-
ments and is then asked to label some test data. Re-
sults are measured in terms of accuracy. Since the
documents are a balanced sample of the six topics,
random guessing would yield an accuracy of 0.1667.
We proceed as with the clustering experiments.
We evaluate different representations for various set-
tings of ? and ? and different classifier parameters
on the development data. We then select the opti-
mal parameter settings and validate this selection on
the held out tuning data, before generating the final
representations for the evaluation once the optimal
parameters have been selected.
For learning we require a multi-class classifier
training algorithm. We evaluated four popular
learning algorithms: a) MIRA?a large margin on-
line learning algorithm (Crammer et al, 2006); b)
Confidence Weighted (CW) learning?a probabilis-
tic large margin online learning algorithm (Dredze
et al, 2008; Crammer et al, 2009); c) Maxi-
mum Entropy?a log-linear discriminative classi-
fier (Berger et al, 1996); and d) Support Vec-
tor Machines (SVM)?a large margin discriminator
(Joachims, 1998).5 For each experiment, we used
default settings of the parameters (tuning did not sig-
nificantly change the results) and 10 online iterations
for the online methods (MIRA, CW). Each reported
result is based on 10-fold cross validation.
Table 6 shows results for various parameter set-
tings and the four learning algorithms on develop-
ment data. As before, we observe that values for
? > 0.9 tend to do well. The CW learning algo-
rithm performs the best on this data, followed by
Maximum Entropy, MIRA and SVM. The optimal
? for classification is 0.75, close to the 0.7 value
selected in clustering. As before, pseudo-terms do
5We used the ?variance? formulation with k = 1 for CW
learning, Gaussian regularization for the Maximum Entropy
classifier, and a linear kernel for the SVM.
468
? ? MaxEnt SVM CW MIRA
0.60 0.99 0.8972 0.6944 0.8667 0.8972
0.60 1.0 0.8972 0.6944 0.8639 0.8944
0.70 0.97 0.9000 0.7722 0.8500 0.8056
0.70 0.98 0.8806 0.7417 0.8917 0.8167
0.70 0.99 0.9000 0.6556 0.9194 0.9056
0.70 1.0 0.8917 0.6556 0.9194 0.9083
0.75 0.94 0.8778 0.7806 0.8639 0.8056
0.75 0.95 0.8778 0.7694 0.8889 0.8111
0.75 0.96 0.9028 0.7778 0.9000 0.8778
0.75 0.97 0.9111 0.7722 0.9250 0.9278
0.75 0.98 0.9056 0.7417 0.9194 0.9167
0.85 0.85 0.8639 0.7833 0.8500 0.8167
0.85 0.90 0.8611 0.7528 0.8611 0.8583
0.85 0.91 0.8389 0.7500 0.8722 0.8556
0.85 0.92 0.8528 0.7222 0.8944 0.8556
Phone Trigram 0.6111 0.7139 0.9138 0.5000
Word Unigram 0.9472 0.8861 0.9861 0.9306
Word Bigram 0.9250 0.8833 0.9917 0.9278
Word Trigram 0.9278 0.8611 0.9889 0.9222
Table 6: The top 15 results (measured as average accu-
racy across the 4 algorithms) for pseudo-terms on de-
velopment data. The best pseudo-term and manual tran-
script results for each algorithm are bolded. All results
are based on 10-fold cross validation. Pseudo-term re-
sults are better than the phonetic baseline and almost as
good as the transcript baseline.
well, though not as well as the upper bound based
on manual transcripts. The performance for pseudo-
terms and phone trigrams are roughly comparable,
though we expect pseudo-terms to be more robust
across languages.
Using the same selection heuristic as in cluster-
ing, we select the optimal parameter settings, vali-
date them on the held out tuning data, and compute
results on evaluation data. The best performing con-
figuration was for ? = 0.75 seconds and ? = 0.97.
Notice these parameters are very similar to the best
parameters selected for clustering. Results on held
out tuning and evaluation data for this setting com-
pared to the manual transcripts are shown in Tables
7 and 8. As with clustering, we see good overall
performance as compared with manual transcripts.
While the performance drops, results suggest that
useful output can be obtained without a transcript.
8 Conclusions
We have presented a new strategy for applying stan-
dard NLP tools to speech corpora without the aid
of a large vocabulary word recognizer. Built in-
stead on top of the unsupervised discovery of term-
? ? MaxEnt SVM CW MIRA
0.75 0.97 0.8722 0.7389 0.8972 0.8750
Phone Trigram 0.7167 0.6972 0.9056 0.5083
Word Unigram 0.9500 0.9056 0.9806 0.9250
Word Bigram 0.9444 0.9111 0.9833 0.9250
Word Trigram 0.9417 0.8972 0.9778 0.9250
Table 7: Results on held out tuning data. The parameters
(? = 0.75 seconds and ? = 0.97) were selected using the
development data and validated on tuning data. All re-
sults are based on 10-fold cross validation. Pseudo-term
results are very close to the transcript baseline and often
better than the phonetic baseline.
? ? MaxEnt SVM CW MIRA
0.75 0.97 0.8683 0.7167 0.7850 0.7150
Phone Trigram 0.8600 0.7750 0.9183 0.6233
Word Unigram 0.9533 0.9317 0.9850 0.9267
Word Bigram 0.9467 0.9200 0.9900 0.9367
Word Trigram 0.9383 0.9233 0.9817 0.9367
Table 8: Results on evaluation data. The parameters
(? = 0.75 seconds and ? = 0.97) were selected using the
development data and validated on tuning data. All re-
sults are based on 10-fold cross validation. Pseudo-term
results are very close to the transcript baseline and often
better than the phonetic baseline.
like units in the speech, we perform unsupervised
topic clustering as well as supervised classification
of spoken documents with performance approaching
that achieved with the manual word transcripts, and
generally matching or exceeding that achieved with
a phonetic recognizer. Our study identified several
opportunities and challenges in the development of
NLP tools for spoken documents that rely on little
or no linguistic resources such as dictionaries and
training corpora.
References
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4).
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of the 17th international conference on
Computational linguistics-Volume 1, pages 79?85. As-
sociation for Computational Linguistics.
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
469
Turk. In Workshop on Creating Speech and Language
Data With Mechanical Turk at NAACL-HLT.
K. W. Church and J. I. Helfman. 1993. Dotplot: A
program for exploring self-similarity in millions of
lines of text and code. Journal of Computational and
Graphical Statistics.
Aaron Clauset, Mark E J Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Physical Review E, 70.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
Koby Crammer, Mark Dredze, and Alex Kulesza. 2009.
Multi-class confidence weighted algorithms. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
ICASSP.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In ICASSP.
Timothy J. Hazen and Anna Margolis. 2008. Discrimi-
native feature weighting using MCE training for topic
identification of spoken audio recordings. In ICASSP.
Timothy J. Hazen, Fred Richardson, and Anna Margo-
lis. 2007. Topic identification from audio recordings
using word and phone recognition lattices. In IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding.
Aren Jansen, Kenneth Church, and Hynek Hermansky.
2010. Towards spoken term discovery at scale with
zero resources. In Interspeech.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learning
(ECML).
George Karypis. 2003. CLUTO: A software package for
clustering high-dimensional data sets. Technical Re-
port 02-017, University of Minnesota, Dept. of Com-
puter Science.
Igor Malioutov, Alex Park, Regina Barzilay, and James
Glass. 2007. Making Sense of Sound: Unsupervised
Topic Segmentation Over Acoustic Input. In ACL.
Scott Novotney and Richard Schwartz. 2009. Analysis
of low-resource acoustic model self-training. In Inter-
speech.
Alex Park and James R. Glass. 2008. Unsupervised pat-
tern discovery in speech. IEEE Transactions of Audio,
Speech, and Language Processing.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: Evaluating non-
expert annotations for natural language tasks. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 254?263. Asso-
ciation for Computational Linguistics.
S. Thomas, S. Ganapathy, and H. Hermansky. 2009.
Phoneme recognition using spectral envelope and
modulation frequency features. In Proc. of ICASSP.
H.J. Zeng, Q.C. He, Z. Chen, W.Y. Ma, and J. Ma. 2004.
Learning to cluster web search results. In Conference
on Research and development in information retrieval
(SIGIR).
470
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 585?595,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
We?re Not in Kansas Anymore: Detecting Domain Changes in Streams
??Mark Dredze and ??Tim Oates and ??Christine Piatko
?Human Language Technology Center of Excellence,
?Center for Language and Speech Processing,
?Applied Physics Lab
Johns Hopkins University
?University of Maryland, Baltimore County
mdredze@cs.jhu.edu, oates@umbc.edu, christine.piatko@jhuapl.edu
Abstract
Domain adaptation, the problem of adapting
a natural language processing system trained
in one domain to perform well in a differ-
ent domain, has received significant attention.
This paper addresses an important problem for
deployed systems that has received little at-
tention ? detecting when such adaptation is
needed by a system operating in the wild,
i.e., performing classification over a stream
of unlabeled examples. Our method uses A-
distance, a metric for detecting shifts in data
streams, combined with classification margins
to detect domain shifts. We empirically show
effective domain shift detection on a variety of
data sets and shift conditions.
1 Introduction
Consider a named entity recognition system trained
on newswire stories. Given annotated documents
containing sentences like ?Tony Hayward has faced
fresh criticism for taking time off to go sailing . . .?
we would like to learn a model that will allow us to
recognize that ?Obama? and ?BP? are named enti-
ties in a sentence like ?Obama summoned BP ex-
ecutives . . .?. When all of the documents come
from one data distribution, like newswire articles,
this tends to work well. However, the sentence
?OBAMA SUMMONED BP EXECUTIVES . . .?
from transcribed broadcast news, and others like it,
will probably lead to poor results because the fea-
tures it relies on have changed. For example, capi-
talization patterns are no longer a good indicator of
the presence of a named entity and appositives are
not indicated by punctuation. This problem of do-
main shift is a pervasive problem in NLP in which
any kind of model ? a parser, a POS tagger, a senti-
ment classifier ? is tested on data that do not match
the training data.
Given a model and a stream of unlabeled in-
stances, we are interested in automatically detecting
changes in the feature distribution that negatively
impact classification accuracy. For example, a senti-
ment classification model trained on book reviews
may heavily weight n-grams features like ?uplift-
ing? and ?page turner?. Those features may never
occur in reviews of kitchen appliances that get mixed
in at test time, and useful features in this new do-
main like ?efficient? and ?noisy compressor? will
have never been seen during training and therefore
not be in the model. Furthermore, we do not assume
labeled instances are available to help detect these
harmful changes. Other tasks related to changes
in data distributions, like detecting concept drift in
which the labeling function changes, may require la-
beled instances, but that is not the focus of this paper.
There is significant work on the related problem
of adapting a classifier for a known domain shift.
Versions of this problem include adapting using only
unlabeled target domain data (Blitzer et al, 2006;
Blitzer et al, 2007; Jiang and Zhai, 2007), adapt-
ing using a limited amount of target domain labeled
data (Daume?, 2007; Finkel and Manning, 2009), and
learning across multiple domains simultaneously in
an online setting (Dredze and Crammer, 2008b).
However, in practical settings, we do not know if
the data distribution will change, and certainly not
when. Additionally, we will not know to what do-
585
main the shift will happen. A discussion forum de-
voted to science fiction books may change over time
to focus more on fantasy and then narrow to discus-
sions of vampire fiction. Maybe this shift is harm-
less and it is possible to identify the sentiment of the
discussants with the original model with no loss in
accuracy. If not, we seek methods that detect this
shift and trigger the use of an adaptation method.
Our domain shift detection problem can be de-
composed into two subproblems: detecting distribu-
tional changes in streams of real numbers, and rep-
resenting a stream of examples as a stream of real
numbers informative for distribution change detec-
tion. We select the A-distance metric (Kifer et al,
2004) to solve the first subproblem since it has been
previously used in other domain adaptation work
(Blitzer et al, 2006; Blitzer et al, 2007). Our main
contribution is towards the second problem, repre-
senting examples as real numbers for this task. We
demonstrate that classification margins, which in-
corporate information about features that most im-
pact system accuracy, can effectively solve the sec-
ond subproblem. Furthermore, we show that the pre-
viously proposed Confidence Weighted learning al-
gorithm (Dredze et al, 2008) can provide a more
informative measure than a simple margin for this
task. Our experiments include evaluations on com-
monly used domain adaptation data and false change
scenarios, as well as comparisons to supervised de-
tection methods that observe label values, or have
knowledge of the target domain.
We begin with a description of our task and pre-
vious applications to language data. After describ-
ing the data used in this paper, we discuss the A-
distance metric and how it has previously been used
for adaptation. We then show that margin based
methods effectively capture information to detect
domain shifts, and propose an alternate way of gen-
erating informative margin values. Finally, we com-
pare our results to settings with supervised knowl-
edge, and close with a survey of related work.
2 Domain Shifts in Language Data
The study of domain shifts in language data has been
the purview of domain adaptation and transfer learn-
ing, which seek to adapt or transfer a model learned
on one source domain with labeled data to another
target domain with few or no labeled examples. For-
mally, errors from such transfers have two sources:
differences in feature distributions and changes to la-
beling functions (annotation standards) (Ben-David
et al, 2006; Ben-David et al, 2009). Empirical work
on NLP domain shifts has focused on the former.
For example, Blitzer et al (2007) learned correspon-
dences between features across domains and Jiang
and Zhai (2007) weighted source domain examples
by their similarity to the target distribution.
We continue in this tradition by making two as-
sumptions about our setting. First, a change in do-
main will be signaled by a change in the feature
distributions. That is, new words, phrases, syntac-
tic structures, etc. signal that the system has shifted
to a new domain. Second, while there may be a
change in the labeling function, i.e., features have a
different meaning in each domain, this will be a sec-
ondary concern. For example, both Daume? (2007)
and Dredze and Crammer (2008b) assume that do-
mains are more similar than different.
A similar problem to the one we consider is that
of concept drift, where a stream of examples are
labeled with a shifting labeling function (concept)
(Nishida and Yamauchi, 2007; Widmer and Kubat,
1996). While concept drift is similar there are two
important differences. First, concept drift can be
measured using a stream of labeled examples, so
system accuracy is directly measured. For exam-
ple, Klinkenberg and Joachims (2000) detect con-
cept drift with support vector machines, using es-
timates of leave-one-out performance to adaptively
adjust and maintain a training window that mini-
mizes estimated generalization error. This is pos-
sible only because class labels arrive with the exam-
ples in the stream. Another concept drift detection
algorithm, STEPD, uses a statistical test to continu-
ally monitor the possibly changing stream, measur-
ing system accuracy directly, again using the labels
it receives for each example (Nishida, 2008). Ob-
viously, no such labels are available in our unsuper-
vised setting. Second, concept drift assumes only
changes in the labeling function, whereas domain
adaptation relies on feature distribution changes.
Several properties of detecting domain shifts in
natural language streams distinguish it from tradi-
tional domain adaptation, concept drift, and other
related tasks:
586
? No Target Distribution Examples Blitzer et
al. (2007) estimate the loss in accuracy from
domain shift by discriminating between two
data distributions. In our setting, we have no
knowledge of the target distribution.
? No Labeled Target Data Some approaches to
domain adaptation assume a limited number of
labeled examples (Daume?, 2007; Dredze and
Crammer, 2008b; Finkel and Manning, 2009).
We assume no labels in our setting.
? Online Setting Domain adaptation typically
assumes a batch transfer between two domains.
We consider a purely stream (online) setting.
? Computationally Constrained Our approach
must be fast, as we expect to run our domain
shift detector alongside a deployed NLP sys-
tem. This limits both computation and storage.
? Unknown Adaptation A critical assumption
of previous work is that a domain change has
occurred. We must ascertain this ourselves.
Despite these challenges, we show unsupervised
stream-based methods that effectively identify shifts
in domain in language data. Furthermore, our meth-
ods are tied directly to the learning task so are sen-
sitive to changes in actual task accuracy. Our meth-
ods have low false positive rates of change detection,
which is important since examples within a single
domain display a large amount of variance, which
could be mistaken for a domain change.
Once a change is detected, any number of actions
may be appropriate. The maintainer of the system
may be notified that performance is suffering, la-
bels can be obtained for a sample of instances from
the stream for retraining, or large volumes of unla-
beled instances can be used for instance reweighting
(Jiang and Zhai, 2007).
3 Datasets
We begin the presentation of our methods by de-
scribing the data used in our experiments. We se-
lected three data sets commonly used in domain
adaptation: spam (Jiang and Zhai, 2007), ACE 2005
named entity recognition (Jiang and Zhai, 2007),
and sentiment (Blitzer et al, 2007). Sentiment and
spam are binary and ACE is multi-class. Note that
in all experiments, a shift in the domain yields a de-
crease in system accuracy.
The goal of the spam data is to classify an email
(bag-of-words) as either spam or ham (not-spam).
Each email user may have different preferences and
features. We used unigram and bigram features, fol-
lowing Dredze and Crammer (2008b) for feature ex-
traction, and used the three task A users as three
domains. The ACE 2005 named entity recognition
dataset includes 7 named entity class labels (person,
organization, location, geopolitical entity, facility,
vehicle, weapon) for 5 text genres (newswire, broad-
cast news, broadcast conversations, conversational
telephone speech, weblogs). We use 4000 examples
from each genre and used Jiang and Zhai?s feature-
extracted data.1 The sentiment data contains reviews
from Amazon for four product types: books, dvds,
electronics, and kitchen. We include an additional
two types (music and video from Dredze and Cram-
mer) in our false shift experiments and use unigram
and bigram features, following Blitzer et al
4 The A-Distance
Our approach to detecting domain shifts in data
streams that negatively impact system accuracy is
based on the ability to (1) detect distributional
changes in streams of real numbers and (2) con-
vert document streams to streams of informative real
numbers. This section describes how we achieve the
former, and the next section describes the latter.
Theoretical work on domain adaptation showed
that the A-distance (Kifer et al, 2004), a stream
based measure of difference between two arbitrary
probability distributions P and P ?, can be used to
evaluate the difference between two domain distri-
butions (Ben-David et al, 2006). In a batch set-
ting this corresponds to learning a linear classi-
fier to discriminate the domains, and Blitzer et al
(2007) showed correlations with the error from do-
main adaptation. Given our interest in streaming
data we return to the original stream formulation of
A-distance.
The A-distance detects differences between two
arbitrary probability distributions by dividing the
range of a random variable into a set of (possibly
1We thank Jing Jiang for the feature-extracted ACE data.
587
Figure 1: The A-distance is computed between two win-
dows (P and P ? in a stream of real-valued data. The sam-
ples in each window are divided into intervals, and the
A-distance measures the change in the distributions over
these intervals between the two windows.
overlapping) intervals, and then measures changes
in the probability that a value drawn for that variable
falls into any one of the intervals. If such a change is
large, a change in the underlying distribution is de-
clared. LetA be a set of real intervals and letA ? A
be one such interval. For that interval, P (A) is the
probability that a value drawn from some unknown
distribution falls in A. The A-distance between P
and P ?, i.e. the difference between two distributions
over the intervals, is defined as follows:
dA(P, P
?) = 2 sup A?A|P (A)? P
?(A)|.
Two distributions are said to be different when, for
a user-specified threshold , dA(P, P ?) > . The
A-distance is distribution independent. That is, it
makes no assumptions about the form of the under-
lying distribution nor about the form of the change
that might occur, either algorithmically or in the un-
derlying theory. Unlike the L1 norm, theA-distance
can be shown to require finitely many samples to de-
tect distribution differences, a property that is crucial
for streaming, sample-based approaches.
Since the A-distance processes a stream of real
numbers, we need to represent an example using a
real number, such as the classification margin for
that example. The first n of these numbers in the
stream are a sample from P , and the most recent
n are a sample from P ?. We signal a domain shift
when the A-distance between P and P ? is large
(greater than ). Larger values of n result in more
accurate estimates of P (A) and slower detection of
changes.
The two windows of samples of size n are shown
graphically in Fig. 1. Each increment on the hori-
zontal axis represents the arrival of a new document.
The vertical axis is some value computed from each
document, such as its classification margin. To com-
pute P and P ?, one needs to specifyA and n, which
are shown as two stacks of boxes that are identical
except for their position. The width of each box is
n, the number of examples used to estimate P (A)
and P (A?) for A ? A, where the real interval A cor-
responds to the vertical span of the box. The value
P (A) is simply the number of documents whose real
value falls inside that interval A divided by n. Note
that the first n documents in the stream are used to
compute P , and as each new document arrives the
location of the stack of boxes used to compute P ? is
shifted to the right by one.
In Fig. 1, the number of examples whose real
value falls in the top two intervals for P is approxi-
mately the same, with no example?s value falling in
the lower two intervals. For P ?, almost every one
of the n document values falls in the second interval
from the top, virtually assuring that dA(P, P ?) will
be large. Though the intervals in the figure do not
overlap, they typically do.
Given n and intervals A, the value of  is chosen
by randomization testing. Because theA-distance is
distribution independent, a sample of size m n is
drawn from any distribution that spansA. This sam-
ple is treated as a stream as described above, and
the largest value of dA(P, P ?) is stored. The sam-
ple is permuted and this process is repeated l times.
Note that any change detection would be a false pos-
itive because all values were sampled from the same
distribution. The values dA(P, P ?) are sorted from
largest to smallest, and  is chosen to be the b?lcth
such value where parameter ? is a user specified
false positive probability.
Both the time and space complexity of our ap-
proach based on the A-distance are small. Given
n and A, n instances must be stored in the sliding
window and 2|A| counters are required to represent
P and P ?. Note that both values are constants based
on user specified parameters, not on the size of the
stream. Processing a new instance involves comput-
ing its margin and updating P and P ?, all of which
can occur in constant time.
588
Figure 2: Each column of plots is a representative result using an SVM on a single run over a sentiment data shift:
dvds? electronics, electronics? books, and kitchen? books, from left to right. The horizontal axis is the number
of instances from the stream processed by the classifier. The top plot is the accuracy of the classifier on the last 100
instances. The bottom plot is the absolute value of the SVM classification margin. The vertical line at 500 instances
marks the point of domain shift. Horizontal dotted lines indicate the mean of the accuracy/margin before and after the
domain shift. Note that in all cases, the mean accuracy drops, as do the mean margin values, demonstrating that both
can indicate domain shifts.
5 A-Distance Over Margins
Since shifts in domains correlate with changes in
distributions, it is natural to begin by considering the
observed features in each example. When we shift
from a source domain (e.g., book reviews) to a target
domain (e.g., dvd reviews) we expect a change in the
distribution for common source words (?author? and
?plot? become less common). Since the A-distance
assumes a stream of single values, we can apply an
A-distance detector to each feature (e.g., unigram
and bigram count) individually. However, our exten-
sive experiments with this approach (omitted here)
show that it suffers from a number of flaws, such as
a high false positive rate if all features are tracked,
the difficult problem of identifying an informative
subset of features for tracking, and deciding how
many such features need to change before a shift has
occurred, which turns out to be highly variable be-
tween shifts.
Therefore, our goal is to use a single A-distance
tracker by collapsing each example to a single value.
One way of doing this is to consider the classifica-
tion margin produced by the classifier. The mar-
gin weighs features by their importance in classi-
fication. When more important features disappear,
we expect the magnitude of the margin to decrease.
Additionally, features that change but do not in-
fluence system performance are effectively ignored
since they do not influence the margin. This ap-
proach has the advantage of task sensitivity, only
tracking changes that impact task accuracy. Initial
experiments showed effectiveness with the unsigned
(absolute value of the) margin, which we use in all
experiments.
We begin by examining visually the information
content of the margin with regards to predicting a
domain shift. The caption of Fig. 2 describes the
setup, and the first row of the figure illustrates the
effects of the shift on the source domain classifier?s
empirical accuracy, measured on a window of the
previous 100 examples. The horizontal dashed lines
indicate the average accuracy before and after the
shift. Note that in each case, average classification
accuracy drops after the shift. However, at any one
point the accuracy displays considerable variance.
Thus, while classification accuracy clearly suffers, it
is difficult to measure this even in a supervised set-
ting with labeled examples when considering a small
portion of the stream.
The second row of Fig. 2 shows the average un-
signed margin value of an SVM classifier computed
over the previous 100 examples in the stream. The
two dashed horizontal lines indicate the average
margin value over source and target examples. There
is a clear drop in the average margin value after the
shift. This difference suggests that the margin can
be examined directly to detect a domain shift. How-
ever, these values vary considerably so extracting
useful information is not trivial.
We evaluated the ability of A-distance trackers to
detect such changes in margin values by simulat-
ing domain shifts using each domain pair in a task
(books to dvds, weblogs to newswire, etc.). For
each domain shift setting, we first trained a classi-
fier on 1000 source domain instances. In our ex-
periments, we used three different classification al-
gorithms: Support Vector Machines (SVM) (Chang
589
0 200 400 600 800 1000 1200 1400CWPM0
200
400
600
800
1000
1200
1400
SVM
spamace2005sentiment 0 200 400 600 800 1000 1200 1400CWPM0
200
400
600
800
1000
1200
1400
MIRA
0 200 400 600 800 1000 1200CWPM0
200
400
600
800
1000
1200
CW
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
SVM
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
MIRA
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
CW
Figure 3: The mean number of instances after a domain change at which theA-distance tracker detects a change. Each
point represents the mean number of instances for CWPM (x-axis) and the SVM, MIRA and CW methods (y-axis).
Datasets are indicated by different markers. The second row zooms each plot to the bottom left corner of the first row.
Points above the diagonal indicate SVM, MIRA or CW took longer to detect a change than CWPM.
and Lin, 2001), MIRA (Crammer et al, 2006) and
Confidence Weighted (CW) learning (Dredze et al,
2008). We evaluated each trained classifier on 500
test examples to measure accuracy on the source do-
main, and then used it to label examples in a stream.
The first 500 examples in the stream were used for
calibrating our change detection methods. The next
500 examples were from the source domain, fol-
lowed by 1500 examples from the target domain.
Over these 2000 examples we ran each of our de-
tection methods. Experiments were repeated over
10 fixed random data permutations.
We automatically select A-distance intervals as
follows. First, we computed the mean and variance
of the 500 calibration margins and then added inter-
vals for .5 standard deviations away from the mean
in each direction, .5 to 1 standard deviation in each
direction, and intervals for 1 standard deviation to
??. We also added three evenly spaced overlap-
ping intervals. To calibrate a FP rate of 0.05 we
sampled from a Gaussian with the above mean and
variance and used n = 200, m = 10000 and l = 50.
The results for each experiment (38 shifts re-
peated averaged over 10 runs each) are shown in
Fig. 3.2 Each plot represents one of the three classi-
fiers (SVM, MIRA, CW) plotted on the vertical axis,
where each point?s y-value indicates the number of
examples observed after a shift occurred before the
A-distance detector registered a change. Smaller
values (lower points) are preferred. The second row
of plots highlights the 0 to 300 region of the first
row. (The x-axis will be discussed in the next sec-
tion.) Notice that in many cases, a change was reg-
istered within 300 examples, showing that domain
shifts can be reasonably detected using the margin
values alone.
Equally important to detecting changes is robust-
ness to false changes. We evaluated the margin de-
tector for false positives in two ways. First, we
logged any incorrectly detected changes before the
shift. For all three algorithms, there were very few
false positives (Table 1). The highest false positive
rate was about 1% (CW), while for the SVM experi-
ments, not a single detector fired prematurely in any
experiment.
2The method plotted on the x-axis will be introduced in
the next section. To evaluate the three methods in this section
(SVM, MIRA, CW) compare the y-values.
590
Second, we sought to test the robustness of the
method over a long stream of examples where no
change occurred. In this experiment, we selected 11
domains that had a sufficient number of examples
to consider a long stream of source domain exam-
ples.3 Rather than use 500 source domain examples
followed by 1500 target domain examples, all 2000
examples were from the source domain. All other
settings were the same. For the SVM detector, out
of 110 runs we detected 6 false positives, 3 of which
were for the same data set (kitchen) (see Table 1.)
6 Confidence Weighted Margins
In the previous section, we showed that margin val-
ues could be used to detect domain shifts. We now
explore ways to reduce the number of target domain
examples needed to detect domain shift by improv-
ing the margin values.
Margin values are often taken as a measure of
prediction confidence. From this perspective, the
A-distance margin tracker identifies when predic-
tion confidence drops. Another task that relies on
margins as measures of confidence is active learn-
ing, where uncertainty sampling for margin based
systems is determined based on the magnitude of
the predicted margin. Dredze and Crammer (2008a)
showed how Confidence Weighted (CW) learning
could be used to generate a more informative mea-
sure of confidence for active learning.
CW is an online algorithm inspired by the MIRA
update (Crammer et al, 2006), which ensures a pos-
itive margin while minimizing parameter change.
CW replaces the Euclidean distance used in the
MIRA update with the KL divergence over Gaussian
distributions. CW learning maintains a Gaussian
distribution over linear weight vectors with mean
? ? RN and diagonal covariance ? ? RN?N .
Maintaining a distribution over prediction func-
tions is appropriate for our task where we con-
sider margin values as confidence. We re-
place the margin |w ? x|, where w is a stan-
dard linear classifier, with a probabilistic margin
|
(
Prw?N (?i,?i) [sign(w ? z) = 1]
)
?12 | .Dredze and
Crammer showed that this probabilistic margin can
be translated into a corrected geometric margin,
3ACE: bc, bn, cts, nw, wl; Sentiment: books, dvd, electron-
ics, kitchen, music, video
which is computed as the normalized margin as M? =
M/
?
V , whereM is the meanM = ? ?x and V the
variance V = x>?x of a univariate Gaussian dis-
tribution over the unsigned-margin M = w ? x. We
call this method CWPM, for Confidence Weighted
Probabilistic Margin.
We compared using CWPM to the standard mar-
gins produced by an SVM, MIRA and CW classifier
in the last section. Fig. 3 shows the results of these
comparisons. In each plot, CWPM (normalized mar-
gin) is plotted on the x-axis, indicating how many
examples from the target domain were observed be-
fore the detector identified a change. The y-axis in
each plot is the number of instances observed for
the SVM, MIRA and CW methods. As before, each
point is the average of the 10 randomized runs used
above (assuming that detectors that did not fire do so
at the end of the stream.) Points above the diagonal
indicate that CWPM detected a change sooner than
the comparative method. Of the 38 shifts, CWPM
detected domain shifts faster than an SVM 34 times,
MIRA 26 times and CW 27 times.
We repeated the experiments to detect false posi-
tives for each margin based method. Table 1 shows
the false positives for the 38 domain shifts consid-
ered as well as the 11 false shift domain shifts. The
false positive rates are among the lowest for CWPM.
This shows that CWPM is a more useful indicator
for detecting domain changes.
7 Gradual Shifts
We have shown detection of sudden shifts between
the source and target domains. However, some shifts
may happen gradually over time. We evaluate this
by modifying the stream as follows: the first 500
instances come from the source domain, and the re-
maining 1500 are sampled randomly from the source
and target domains. The probability of an instance
being drawn from the target domain at time i is
pi(x = target) = i1500 , where i counts from the start
of the shift at index 500. The probability of sam-
pling target domain data increases uniformly over
the stream. At index 750 after the start of the shift
each domain is equally likely. The ACE and Sen-
timent datasets had sufficient data to be evaluated
in this setting. Fig. 4 shows CWPM still performs
best, but results are close (SVM: 22 of 32, MIRA &
591
0 200 400 600 800 1000 1200 1400 1600CWPM0
200
400
600
800
1000
1200
1400
1600
SVM
ace2005sentiment 0 200 400 600 800 1000 1200 1400 1600CWPM0
200
400
600
800
1000
1200
1400
1600
MIRA
0 200 400 600 800 1000 1200 1400 1600CWPM0
200
400
600
800
1000
1200
1400
1600
CW
Figure 4: Gradual shift detection with SVM, MIRA or CW vs. CWPM. There were no false positives.
Domain Shift FPs
Algorithm True Shift False Shift
Sec. 5: SVM 0 6
Sec. 6: MIRA 2 13
Sec. 6: CW 5 10
Sec. 6: CWPM 1 6
Total tests 380 110
Table 1: False positives (FPs) observed in true domain
shift and false domain shift experiments for methods in
corresponding sections. Each setting was run 10 times,
resulting in 380 true domain shifts and 110 false shifts.
CW: 17 of 32). As expected, detections happen later
in the stream. The closer results are likely due to
the increased difficulty of the task. With less clear
information, there it is more difficult for all the al-
gorithms to recognize a change, and performance
across the methods begins to equalize. Even in this
more difficult setting, CWPM is the best performer.
8 Comparison to Supervised Information
So far we have considered applying A-distance
tracking to information freely available in a real
world system: the classification margins. As a use-
ful baseline for comparison, we can measure using
supervised sources of information, where additional
information is provided that is not normally avail-
able. In particular, we investigate two types of su-
pervised knowledge: the labels of examples in the
stream and knowledge of the target domain. In each
case, we compare using the A-distance and CWPM
versus applying the A-distance to supervised infor-
mation.
8.1 Classifier Accuracy
In Sec. 4 we showed that both the margin and recent
classifier accuracy indicate when shifts in domains
occur (Fig. 2). We developed techniques based on
the margin, which is available at test time. We now
consider knowledge of the true labels for these test
examples, which allows for tracking classifier accu-
racy. We can use the A-distance to detect when un-
expected changes in accuracy occur.
For each test example classified by the system,
we evaluated whether the system was correct in its
prediction by examining the label. If the classifier
was correct, we output a 1; otherwise, we output a
0. Over this 1/0 stream produced by checking clas-
sifier accuracy we ran an A-distance detector, with
intervals set for 1s and 0s (10,000 uniform samples
to calibrate the threshold for a false positive rate of
0.05.) If an unusual number of 0s or 1s occur ?
more or less mistakes than on the source domain ? a
change is detected.4 Results on this accuracy stream
are compared to CWPM (Fig. 5.) Despite this su-
pervised information, CWPM still detects domain
changes faster than with labeled examples. Consider
again Fig. 2, which shows both accuracy and margin
values over time. While the average accuracy drops,
the instantaneous value is very noisy, suggesting that
even this additional information may not yield bet-
ter domain shift detection. This will be interesting
to explore in future work.
4An alternate approach would be to measure accuracy di-
rectly as a real valued number. However, our experiments
showed the discrete approach to be more effective.
592
0 200 400 600 800 1000 1200 1400 1600CWPM0
200
400
600
800
1000
1200
1400
1600
Accu
racy 
Dete
ctor
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
Accu
racy 
Dete
ctor
Figure 5: An A-distance accuracy detector, run over a stream of 1s and 0s indicating correct and incorrect predictions
of the classifier on examples in a stream. The bulk of points above the line indicate that CWPM is more effective at
detecting domain change. CWPM had a single false positive and the accuracy detector had no false positives.
8.2 Domain Classification
Next, we consider another source of supervision:
a selection of examples known to be from the tar-
get domain. In this setting, we know that a shift
will occur and we know to which domain it will oc-
cur. This requires a sample of (unlabeled) target do-
main examples when the target domain is not known
ahead of time. Using a common approach to detect-
ing domain differences when data is available from
both domains (Ben-David et al, 2009; Blitzer et al,
2007; Rai et al, 2010), we train a binary classifier
to differentiate between the source and target do-
main. We learn a CW classifier on 1000 examples
(500 from each domain) that do not appear in the
test stream. We then label each example as either
?source? or ?target? and output a 1 or 0 accordingly.
Over this 0/1 stream, we run an A-distance detector
with two intervals, one for 1s and one for 0s. The
remaining setup is identical to theA-distance exper-
iments above.
Fig. 6 shows the detection rate of CWPM versus
A-distance over the domain classifier stream. As ex-
pected, the detection rate for the domain classifier
is very fast, in almost every case (save 1) less than
400 examples after the shift happens. When CWPM
is slow to detect a change (over 400 examples), the
domain classifier is the clear winner. However, in
the majority of experiments, especially for ACE and
spam data, both detectors register a change quickly.
These results suggest that while a sample of target
domain examples is very helpful, our CWPM ap-
proach can also be effective when such samples are
not available.
9 Related Work
Early NLP work in the unsupervised setting moni-
tored classification confidence values, setting a con-
fidence threshold based on a break-even heuristic,
monitoring the rate of (presumed) irrelevant exam-
ples based on this threshold, and signaling a change
when this rate increased (Lanquillon, 1999).
Confidence estimation has been used for specific
NLP components such as information extraction.
The correctness of fields extracted via a conditional
random field extractor has been shown to corre-
late well to an estimate obtained by a constrained
forward-backward technique (Culotta and McCal-
lum, 2004). EM-based confidence estimation has
been used to estimate the confidence of patterns
derived from partially supervised relation extrac-
tion (Agichtein, 2006). Confidence estimation has
also been used to improve the overall effectiveness
of NLP systems. Confidence estimates obtained via
neural networks have shown gains for speech recog-
nition, spoken language understanding, and machine
translation (Gandrabur et al, 2006). Pipeline models
using confidence estimates at one stage as weights
for further downstream stages improve over base-
line dependency parsing and named entity recogni-
tion pipeline models (Bunescu, 2008).
An alternative formulation of domain adaptation
trains on different corpora from many different do-
mains, then uses linear combinations of models
trained on the different corpora(McClosky et al,
2010).
Work in novelty detection is relevant to the task
of detecting domain shifts (Scholkopf et al, 2000),
593
0 200 400 600 800 1000 1200CWPM0
200
400
600
800
1000
1200
Dom
ain C
lassif
ier
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
Dom
ain C
lassif
ier
Figure 6: A-distance over a stream of 1s and 0s produced by a supervised classifier trained to differentiate between
the source and target domain. Samples from the unseen target domain is very effective. However, for many shifts, the
margin based A-distance detector is still competitive. CWPM had a single false positive while the domain classifier
stream had 2 false positives in these experiments.
though the rate of occurrence of novel instances is
more informative in our setting than the mere fact
that novel instances are observed.
We are also motivated by the problem of detect-
ing genre shift in addition to domain shift, as in the
ACE 2005 data set shifts from newswire to tran-
scripts and blogs. Different text genres occur in tra-
ditional settings, such as broadcast news transcripts
and newswire, and have begun to proliferate with
the variety of social media technologies now avail-
able including weblogs. Static genre classification
has been explored using a variety of techniques, in-
cluding exploiting punctuation (Kessler et al, 1997;
Dewdney et al, 2001), TF-IDF statistics (Lee and
Myaeng, 2002), and part-of-speech statistics and
histograms (Finn and Kushmerick, 2006; Feldman
et al, 2009).
Finally, statistical estimation in a streaming con-
text has been considered in data mining applica-
tions (Muthukrishnan, 2005). Change detection
via sequential hypothesis testing has been effective
for streaming applications such as network intrusion
detection (Muthukrishnan et al, 2007). Detecting
new events in a stream of Twitter posts can be done
using constant time and space similarity measures
based on a modification of locality sensitive hash-
ing (Petrovic? et al, 2010).
10 Conclusion
While there are a number of methods for domain
adaptation, a system first needs to determine that a
domain shift has occurred. We have presented meth-
ods for automatically detecting such domain shifts
from a stream of (unlabeled) examples that require
limited computation and memory by virtue of op-
erating on fixed-size windows of data. Our meth-
ods were evaluated empirically on a variety of do-
main shifts using NLP data sets and are shown to
be sensitive to shifts while maintaining a low rate of
false positives. Additionally, we showed improved
detection results using a probabilistic margin based
on Confidence Weighted learning. Comparisons to
detection with supervised information show that our
results are effective even in unlabeled settings. Our
methods are promising as tools to accompany the de-
ployment of domain adaptation algorithms, so that a
complete system can first identify when a domain
shift has occurred before automatically adapting to
the new domain.
Acknowledgments
Thanks to the HLTCOE text processing group for
many helpful discussions.
References
Eugene Agichtein. 2006. Confidence estimation meth-
ods for partially supervised information extraction. In
SDM.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Vaughan.
2009. A theory of learning from different domains.
Machine Learning.
594
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In EMNLP.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
Aron Culotta and Andrew McCallum. 2004. Confidence
estimation for information extraction. In North Amer-
ican Chapter of the Association for Computational
Linguistics - Human Language Technologies (NAACL-
HLT).
Hal Daume?. 2007. Frustratingly easy domain adaptation.
In Association for Computational Linguistics (ACL).
Nigel Dewdney, Carol VanEss-Dykema, and Richard
MacMillan. 2001. The form is the substance: clas-
sification of genres in text. In Workshop on Human
Language Technology and Knowledge Management.
Mark Dredze and Koby Crammer. 2008a. Active learn-
ing with confidence. In Association for Computational
Linguistics (ACL).
Mark Dredze and Koby Crammer. 2008b. Online meth-
ods for multi-domain learning and adaptation. In
EMNLP.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.
S. Feldman, M. A. Marin, M. Ostendorf, and M. R.
Gupta. 2009. Part-of-speech histograms for genre
classification of text. In International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian domain adaptation. In NAACL-
HLT.
Aidan Finn and Nicholas Kushmerick. 2006. Learning to
classify documents according to genre: Special topic
section on computational analysis of style. J. Am. Soc.
Inf. Sci. Technol., 57(11):1506?1518.
Simona Gandrabur, George Foster, and Guy Lapalme.
2006. Confidence estimation for NLP applications.
ACM Trans. Speech Lang. Process., 3(3):1?29.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Association for
Computational Linguistics (ACL).
Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Associa-
tion for Computational Linguistics (ACL).
Daniel Kifer, Shai Ben-David, and Johannes Gehrke.
2004. Detecting change in data streams. In Very Large
Data Bases (VLDB).
Ralf Klinkenberg and Thorsten Joachims. 2000. Detect-
ing concept drift with support vector machines. In In-
ternational Conference on Machine Learning (ICML).
C. Lanquillon. 1999. Information filtering in changing
domains. In IJCAI.
Yong-Bae Lee and Sung Hyon Myaeng. 2002. Text
genre classification with genre-revealing and subject-
revealing features. In SIGIR.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
NAACL-HLT, pages 28?36, Los Angeles, California,
June. Association for Computational Linguistics.
S. Muthukrishnan, Eric van den Berg, and Yihua Wu.
2007. Sequential change detection on data streams. In
IEEE International Conference on Data Mining Work-
shops (ICDMW).
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Foundations and Trends in Theoretical
Computer Science, 1(2).
Kyosuke Nishida and Koichiro Yamauchi. 2007. Detect-
ing concept drift using statistical testing. In Discovery
Science.
Kyosuke Nishida. 2008. Learning and Detecting Con-
cept Drift. Ph.D. thesis, Hokkaido University, Japan.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In NAACL-HLT, pages 181?189, June.
P. Rai, A. Saha, H. Daume? III, and S. Venkatasubrama-
nian. 2010. Domain Adaptation meets Active Learn-
ing. In Workshop on Active Learning for Natural Lan-
guage Processing (ALNLP), page 27.
Bernhard Scholkopf, Robert Williamson, Alex Smola,
John Shawe-Taylor, and John Platt. 2000. Support
vector method for novelty detection. In NIPS.
Gerhard Widmer and Miroslav Kubat. 1996. Learning
in the presence of concept drift and hidden contexts.
Machine Learning, 23:69?101.
595
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 344?355, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Name Phylogeny: A Generative Model of String Variation
Nicholas Andrews and Jason Eisner and Mark Dredze
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA
{noa,eisner,mdredze}@jhu.edu
Abstract
Many linguistic and textual processes involve transduc-
tion of strings. We show how to learn a stochastic trans-
ducer from an unorganized collection of strings (rather
than string pairs). The role of the transducer is to orga-
nize the collection. Our generative model explains simi-
larities among the strings by supposing that some strings
in the collection were not generated ab initio, but were in-
stead derived by transduction from other, ?similar? strings
in the collection. Our variational EM learning algorithm
alternately reestimates this phylogeny and the transducer
parameters. The final learned transducer can quickly link
any test name into the final phylogeny, thereby locating
variants of the test name. We find that our method can
effectively find name variants in a corpus of web strings
used to refer to persons inWikipedia, improving over stan-
dard untrained distances such as Jaro-Winkler and Leven-
shtein distance.
1 Introduction
Systematic relationships between pairs of strings
are at the core of problems such as transliteration
(Knight and Graehl, 1998), morphology (Dreyer and
Eisner, 2011), cross-document coreference resolu-
tion (Bagga and Baldwin, 1998), canonicalization
(Culotta et al2007), and paraphrasing (Barzilay and
Lee, 2003). Stochastic transducers such as proba-
bilistic finite-state transducers are often used to cap-
ture such relationships. They model a conditional
distribution p(y | x), and are ordinarily trained on
input-output pairs of strings (Dreyer et al2008).
In this paper, we are interested in learning from
an unorganized collection of strings, some of which
might have been derived from others by transforma-
tive linguistic processes such as abbreviation, mor-
phological derivation, historical sound or spelling
change, loanword formation, translation, transliter-
ation, editing, or transcription error. We assume that
each string was derived from at most one parent, but
may give rise to any number of children.
The difficulty is that most or all of these parent-
child relationships are unobserved. We must recon-
struct this evolutionary phylogeny. At the same time,
we must fit the parameters of a model of the relevant
linguistic process p(y | x), which says what sort of
children y might plausibly be derived from parent x.
Learning this model of p(y | x) helps us organize the
training collection by reconstructing its phylogeny,
and also permits us to generalize to new forms.
We will focus on the problem of name varia-
tion. We observe a collection of person names?full
names, nicknames, abbreviated or misspelled names,
etc. Some of these names can refer to the same per-
son; we hope to detect this. It would be an unlikely
coincidence if two mentions of John Jacob Jingle-
heimer Schmidt referred to different people, since
this is a long and unusual name. Similarly, John Ja-
cob Jingelhimer Smith andDr. J. J. Jingleheimermay
also be related names for this person. That is, these
names may be derived from one another, via unseen
relationships, although we cannot be sure.
Readers may be reminded of unsupervised clus-
tering, in which ?suspiciously similar? points can be
explained as having been generated by the same clus-
ter. Since each name is linked to at most one parent,
our setting resembles single-link clustering?with a
learned, asymmetric distance measure p(y | x).
We will propose a generative process that makes
explicit assumptions about how strings are copied
with mutation. It is assumed to have generated all the
names in the collection, in an unknown order. Given
learned parameters, we can ask the model whether a
name Dr. J. J. Jingelheimer in the collection is more
likely to have been generated from scratch, or derived
from some previous name.
1.1 Related Work
Several previous papers have also considered learn-
ing transducers or other models of word pairs when
344
the pairing between inputs and outputs is not given.
Most commonly, one observes parallel or compa-
rable corpora in two languages, and must recon-
struct a matching from one language?s words to the
other?s before training on the resulting pairs (Schafer,
2006b; Klementiev and Roth, 2006; Haghighi et al
2008; Snyder et al2010; Sajjad et al2011).
Hall and Klein (2010) extend this setting to more
than two languages, where the phylogenetic tree is
known. A given lexeme (abstract word) can be re-
alized in each language by at most one word (string
type), derived from the parent language?s realization
of the same lexeme. The system must match words
that share an underlying lexeme (i.e., cognates), cre-
ating a matching of each language?s vocabulary to its
parent language?s vocabulary. A further challenge is
that the parent words are unobserved ancestral forms.
Similarly, Dreyer and Eisner (2011) organize
words into morphological paradigms of a given
structure. Again words with the same underlying lex-
eme (i.e., morphemes) must be identified. A lexeme
can be realized in each grammatical inflection (such
as ?first person plural present?) by exactly one word
type, related to other inflected forms of the same lex-
eme, which as above may be unobserved. Their in-
ference setting is closer to ours because the input is
an unorganized collection of words?input words are
not tagged with their grammatical inflections. This
contrasts with the usual multilingual setting where
each word is tagged with its true language.
In one way, our problem differs significantly from
the above problems. We are interested in random
variation that may occur within a language as well
as across languages. A person name may have un-
boundedly many different variants. This is unlike
the above problems, in which a lexeme has at most
K realizations, where K is the (small) number of
languages or inflections.1 We cannot assign the ob-
served strings to positions in an existing structure
that is shared across all lexemes, such as a given phy-
logenetic tree whose K nodes represent languages,
or a given inflectional grid whose K cells represent
grammatical inflections. Rather, we must organize
1In the above problems, one learns a set ofO(K) orO(K2)
specialized transducers that relate Latin to Italian, singular to
plural, etc. We instead use one global mutation model that ap-
plies to all names?but see footnote 14 on incorporating special-
ized transductions (Latin to Italian) within our mutation model.
them into a idiosyncratic phylogenetic tree whose
nodes are the string types or tokens themselves.
Names and words are not the only non-biological
objects that are copied with mutation. Documents,
database records, bibliographic entries, code, and
images can evolve in the same way. Reconstructing
these relationships has been considered by a number
of papers on authorship attribution, near-duplicate
detection, deduplication, record linkage, and plagia-
rism detection. A few such papers reconstruct a phy-
logeny, as in the case of chain letters (Bennett et
al., 2003), malware (Karim et al2005), or images
(Dias et al2012). In fact, the last of these uses the
same minimum spanning tree method that we apply
in ?5.3. However, these papers do not train a similar-
ity measure as we do. To our knowledge, these two
techniques have not been combined outside biology.
In molecular evolutionary analysis, phylogenetic
techniques have often been combined with estima-
tion of some parametric model of mutation (Tamura
et al2011). However, names mutate differently
from biological sequences, and our mutation model
for names (?4, ?8) reflects that. We also posit a spe-
cific process (?3) that generates the name phylogeny.
2 An Example
A fragment of a phylogeny for person names is
shown in Figure 1. Our procedure learned this auto-
matically from a collection of name tokens, without
observing any input/output pairs. The nodes of the
phylogeny are the observed name types,2 each one
associated with a count of observed tokens.
Each arrow corresponds to a hypothesized mu-
tation. These mutations reflect linguistic processes
such as misspelling, initialism, nicknaming, translit-
eration, etc. As an exception, however, each ar-
row from the distinguished root node ? generates
an initial name for a new entity. The descendants of
this initial name are other names that subsequently
evolved for that entity. Thus, the child subtrees of ?
give a partition of the name types into entities.
Thanks to the phylogeny, the seemingly disparate
names Ghareeb Nawaz and Muinuddin Chishti are
seen to refer to the same entity. They may be traced
back to their common ancestor Khawaja Gharib-
2We cannot currently hypothesize unobserved intermediate
forms, e.g., common ancestors of similar strings. See ?6.2.
345
Khawaja Gharibnawaz Muinuddin Hasan Chisty
Khwaja Gharib NawazKhwaja Muin al-Din ChishtiGhareeb Nawaz Khwaja Moinuddin ChishtiKhwaja gharibnawaz Muinuddin Chishti
Thomas Ruggles Pynchon, Jr.Thomas Ruggles Pynchon Jr.Thomas R. Pynchon, Jr.Thomas R. Pynchon Jr.Thomas R. Pynchon Thomas Pynchon, Jr.Thomas Pynchon Jr.
Figure 1: A portion of a spanning tree found by our model.
nawaz Muinuddin Hasan Chisty, from which both
were derived via successive mutations.
Not shown in Figure 1 is our learned family p of
conditional probability distributions, which models
the likely mutations in this corpus. Our EM learn-
ing procedure found p jointly with the phylogeny.
Specifically, it alternated between improving p and
improving the distribution over phylogenies. At the
end, we extracted the single best phylogeny.
Together, the learned p and the phylogeny in Fig-
ure 1 form an explanation of the observed collection
of names. What makes it more probable than other
explanations? Informally, two properties:
? Each node in the tree is plausibly derived from
its parent. More precisely, the product of
the edge probabilities under p is comparatively
high. A different p would have reduced the
probability of the events in this phylogeny. A
different phylogeny would have involved a more
improbable collection of events, such as replac-
ing Chishti with Pynchon, or generating many
unrelated copies of Pynchon directly from ?.
? In the phylogeny, the parent names tend to be
used often enough that it is plausible for variants
of these names to have emerged. Our model
says that new tokens are derived from previ-
ously generated tokens. Thus?other things
equal?Barack Obama is more plausibly a vari-
ant of Barack Obama, Jr. than of Barack
Obama, Sr. (which has fewer tokens).
3 A Generative Model of Tokens
Our model should reflect the reasons that name vari-
ation exists. A named entity has the form y = (e, w)
where w is a string being used to refer to entity e. A
single entity e may be referred to on different occa-
sions by different name strings w. We suppose that
this is the result of copying the entity with occasional
mutation of its name (as in asexual reproduction).
Thus, we assume the following simple generative
process that produces an ordered sequence of tokens
y1, y2, . . ., where yi = (ei, wi).
? After the first k tokens y1, . . . yk have been gen-
erated, the author responsible for generating yk+1
must choose whom to talk about next. She is likely
to think of someone she has heard about often in the
past. So to make this choice, she selects one of the
previous tokens yi uniformly at random, each having
probability 1/(k + ?); or else she selects ?, with
probability ?/(k + ?).
? If the author selected a previous token yi, then
with probability 1 ? ? she copies it faithfully, so
yk+1 = yi. But with probability ?, she instead draws
a mutated token yk+1 = (ek+1, wk+1) from the mu-
tation model p(? | yi). This preserves the entity
(ek+1 = ei with probability 1), but the new name
wk+1 is a stochastic transduction of wi drawn from
p(? | wi).3 For example, in referring to ei, the author
may shorten and respellwi = Khwaja Gharib Nawaz
into wk+1 = Ghareeb Nawaz (Figure 1).
? If the author selected?, she must choose a fresh
entity yk+1 = (ek+1, wk+1) to talk about. So she
sets ek+1 to a newly created entity, sampling its name
wk+1 from the distribution p(? | ?). For example,
wk+1 = Thomas Ruggles Pynchon, Jr. (Figure 1).
Nothing prevents wk+1 from being a name that is al-
ready in use for another entity (i.e., wk+1 may equal
wj for some j ? k).
3Straightforward extensions are to allow a variable mutation
rate ?(yi) that depends on properties of yi, and to allow wk+1
to depend on known properties of ei. See footnote 14 for further
discussion of enriched tokens.
346
3.1 Relationship to other models
If we ignore the name strings, we can see that the
sequence of entities e1, e2, . . . eN is being generated
from a Chinese restaurant process (CRP) with con-
centration parameter ?. To the extent that ? is low
(so that  is rarely used), a few randomly chosen en-
tities will dominate the corpus.
The CRP is equivalent to sampling e1, e2, . . . IID
from an unknown distribution that was itself drawn
from a Dirichlet process with concentration ?. This
is indeed a standard model of a distribution over en-
tities. For example, Hall et al2008) use it to model
venues in bibliographic entries.
From this characterization of the CRP, one can see
that any permutation of this entity sequence would
have the same probability. That is, our distribution
over sequences of entities e is exchangeable.
However, our distribution over sequences of
named entities y = (e, w) is non-exchangeable.
It assigns different probabilities to different order-
ings of the same tokens. This is because our model
posits that later authors are influenced by earlier au-
thors, copying entity names from them with muta-
tion. So ordering is important. The mutation process
is not symmetric?for example, Figure 1 reflects a
tendency to shorten rather than lengthen names.
Non-exchangeability is one way that our present
model differs from (parametric) transformationmod-
els (Eisner, 2002) and (non-parametric) transforma-
tion processes (Andrews and Eisner, 2011). These
too are defined using mutation of strings or other
types. From a transformation process, one can draw
a distribution over types, from which the tokens are
then sampled IID. This results in an exchangeable
sequence of tokens, just as in the Dirichlet process.
We avoid transformation models here for three
reasons. (1) Inference is more expensive. (2) A
transformation process seems less realistic as a
model of authorship. It constructs a distribution over
derivational paths, similar to the paths in Figure 1.
It effectively says that each token is generated by re-
capitulating some previously used path from ?, but
with some chance of deviating at each step. For an
author to generate a name token this way, she would
have to know the whole derivational history of the
previous name she was adapting. Our present model
instead allows an author simply to select a name she
previously saw and copy or mutate its surface form.
(3) One should presumably prefer to explain a novel
name y as a mutation of a frequent name x, other
things equal (?2). But surprisingly, inference under
the transformation process does not prefer this.4
Another view of our present model comes from
the literature on random graphs (e.g., for modeling
social networks or the link structure of the web). In
a preferential attachment model, a graph?s vertices
are added one by one, and each vertex selects some
previous vertices as its neighbors. Our phylogeny
is a preferential attachment tree, a random directed
graph in which each vertex selects a single previous
vertex as its parent. Specifically, it is a random recur-
sive tree (Smythe and Mahmoud, 1995) whose ver-
tices are the tokens.5 To this simple random topol-
ogy we have added a random labeling process with
mutation. The first ? vertices are labeled with ?.
4 A Mutation Model for Strings
Our model in ?3 samples the next token y, when it is
not simply a faithful copy, from p(y | x) or p(y | ?).
The key step there is to sample the name string wy
from p(wy | wx) or p(wy | ?).
Our model of these distributions could easily in-
corporate detailed linguistic knowledge of the muta-
tion process (see ?8). Here we describe the specific
model that we use in our experiments. Like many
such models, it can be regarded as a stochastic finite-
state string-to-string transducer parameterized by ?.
There is much prior work on stochastic models of
edit distance (Ristad andYianilos, 1998; Bilenko and
Mooney, 2003; Oncina and Sebban, 2006; Schafer,
2006a; Bouchard-C?t? et al2008; Dreyer et al
2008, among others). For the present experiments,
we designed a moderately simple one that employs
(1) conditioning on one character of right context,
(2) latent ?edit? and ?no-edit? regions to capture the
fact that groups of edits are often made in close prox-
imity, and (3) some simple special handling for the
distribution conditioned on the root p(wy | ?).
We assume a stochastic mutation process which,
when given an input string wx, edits it from left to
4The very fact that x has been frequently observed demon-
strates that it has often chosen to stop mutating. This implies
that it is likely to choose stop again rather than mutate into y.
5This is not the tree shown in Figure 1, whose vertices are
types rather than tokens.
347
right into an output string wy. Then p(wy | wx) is
the total probability of all operation sequences onwx
that would produce wy. This total can be computed
in time O(|wx| ? |wy|) by dynamic programming.
Our process has four character-level edit opera-
tions: copy, substitute, insert, delete. It also has a
distinguished no-edit operation that behaves exactly
like copy. At each step, the process first randomly
chooses whether to edit or no-edit, conditioned only
on whether the previous operation was an edit. If it
chooses to edit, it chooses a random edit type with
some probability conditioned on the next input char-
acter. In the case of insert or substitute, it then ran-
domly chooses an output character, conditioned on
the type of edit and the next input character.
It is common to mutate a name by editing con-
tiguous substrings (e.g., words). Contiguous regions
of copying versus editing can be modeled by a low
probability of transitioning between no-edit and edit
regions.6 Note that an edit region may include some
copy edits (or substitute edits that replace a charac-
ter with itself) without leaving the edit region. This
is why we distinguish copy from no-edit.
Input and output strings are augmented with a
trailing eos (?end-of-string?) symbol that is seen by
the single-character lookahead. If the next character
is eos, the only available edit is insert. Alternatively,
if the process selects no-edit, then eos is copied to
the output string and the process terminates.
In the case of p(wy | ?), the input string is empty,
and both input and output are augmented with a trail-
ing eos? character that behaves like eos. Then wy
is generated by a sequence of insertions followed by
a copy. These are conditioned as usual on the next
character, here eos?, so the model can learn to insert
more or different characters when the input is ?.
The parameters ? determining the conditional
probabilities of the different operations and charac-
ters are estimated with backoff smoothing.
5 Inference
The input to inference is a collection of named entity
tokens y. Most are untagged tokens of the form y =
(?, w). In a semi-supervised setting, however, some
6This somewhat resembles the traditional affine gap penalty
in computational biology (Gusfield, 1997), which makes dele-
tions or insertions cheaper if they are consecutive. We instead
make consecutive edits cheaper regardless of the edit type.
of the tokens may be tagged tokens of the form y =
(e, w), whose true entity is known. The entity tags
place a constraint on the phylogeny, since each child
subtree of ? must correspond to exactly one entity.
5.1 An unrealistically supervised setting
Suppose we were lucky enough to fully observe the
sequence of named entity tokens yi = (ei, wi) pro-
duced by our generative model. That is, suppose all
tokens were tagged and we knew their ordering.
Yet there would still be something to infer: which
tokens were derived from which previous tokens.
This phylogeny is described by a spanning tree over
the tokens. Let us see how to infer it.
For each potential edge x ? y between named
entity tokens, define ?(y | x) to be the probability of
choosing x and copying it (possibly with mutation)
to obtain y. So
?(yj | ?) = ? p(yj | ?) (1)
?(yj | yi) = ? p(yj | yi) + (1? ?)1(yj = yi) (2)
except that if i ? j or if ei 6= ej , then ?(yj | yi) = 0
(since yj can only be derived from an earlier token
yi with the same entity).
Now the prior probability of generating y1, . . . yN
with a given phylogenetic tree is easily seen to be a
product over all tree edges,
?
j ?(yj | pa(yj)) where
pa(yj) is the parent of yj . As a result, it is known
that the following are efficient to compute from the
(N + 1)? (N + 1) matrix of ? values (see ?5.3):
(a) the max-probability spanning tree
(b) the total probability of all spanning trees
(c) the marginal probability of each edge, under the
posterior distribution on spanning trees
(a) is our single best guess of the phylogeny. We use
this during evaluation. (b) gives the model likeli-
hood, i.e., the total probability of the observed data
y1, . . . yN . To locally maximize the model likeli-
hood, (c) can serve as the E step of our EM algorithm
(?6) for tuning our mutation model. The M step then
retrains the mutation model?s parameters ? on input-
output pairs wi ? wj , weighting each pair by its
edge?s posterior marginal probability (c), since that
is the expected count of a wi ? wj mutation. This
computation is iterated.
348
5.2 The unsupervised setting
Now we turn to a real setting?fully unsupervised
data. Two issues will force us to use an approximate
inference algorithm. First, we have an untagged cor-
pus: a token?s entity tag e is never observed. Second,
the order of the tokens is not observed, so we do not
know which other tokens are candidate parents.
Our first approximation is to consider only phylo-
genies over types rather than tokens.7 The type phy-
logeny in Figure 1 represents a set of possible token
phylogenies. Each node of Figure 1 represents an
untagged name type y = (?, w). By grouping all ny
tokens of this type into a single node, we mean that
the first token of y was derived by mutation from the
parent node, while each later token of y was derived
by copying an (unspecified) earlier token of y.
A token phylogeny cannot be represented in this
way if two or more tokens of y were created by mu-
tations. In that case, their name strings are equal only
by coincidence. They may have different parents
(perhaps of different entities), whereas the y node in
a type phylogeny can have only one parent.
We argue, however, that these unrepresentable to-
ken phylogenies are comparatively unlikely a poste-
riori and can be reasonably ignored during inference.
The first token of y is necessarily amutation, but later
tokens are much more likely to be copies. The prob-
ability of generating a later token y by copying some
previous token is at least
(1? ?)/(N + ?),
while the probability of generating it in some other
way is at most
max(? p(y | ?), ? max
x?Y
p(y | x))
where Y is the set of observed types. The second
probability is typically much smaller: an author is
unlikely to invent exactly the observed string y, cer-
tainly from ? but even by mutating a similar string
x (especially when the mutation rate ? is small).
How do we evaluate a type phylogeny? Con-
sider the probability of generating untagged tokens
7Working over types improves the quality of our second ap-
proximation, and also speeds up the spanning tree algorithms.
?6 explains how to regard this approximation as variational EM.
y1, . . . yN in that order and respecting the phylogeny:
(
N?
k=1
1
k + ?
)
?
y?Y
g(y | pa(y))
?
?
ny?1?
i=1
i (1? ?)
?
?
(3)
where g(y | pa(y)) is a factor for generating the first
token of y from its parent pa(y), defined by
g(y | ?) = ? ? p(y | ?) (4)
g(y | x) = ? ? (# tokens of x preceding
first token of y) ? p(y | x) (5)
But we do not actually know the token order: by
assumption, our input corpus is only an unordered
bag of tokens. So we must treat the hidden order-
ing like any other hidden variable and maximize the
marginal likelihood, which sums (3) over all possi-
ble orderings (permutations). This sum can be re-
garded as the number of permutations N ! (which is
fixed given the corpus) times the expectation of (3)
for a permutation chosen uniformly at random.
This leads to our second approximation. We ap-
proximate this expectation of the product (3) with a
product of expectations of its individual factors.8 To
find the expectation of (5), observe that the expected
number of tokens of x that precede the first token of
y is nx/(ny+1), since each of the nx tokens of x has
a 1/(ny + 1) chance of falling before all ny tokens
of y. It follows that the approximated probability of
generating all tokens in some order, with our given
type parentage, is proportional to
?
y?Y
?(y | pa(y)) (6)
where
?(y | ?) = ? ? p(y | ?) (7)
?(y | x) = ? ? p(y | x) ? nx/(ny + 1) (8)
and the constant of proportionality depends on the
corpus.
The above equations are analogous to those in
?5.1. Again, the approximate posterior probability
of a given type parentage tree is edge-factored?it is
the product of individual edge weights defined by ?.
Thus, we are again eligible to use the spanning tree
algorithms in ?5.3 below.
8In general this is an overestimate for each phylogeny.
349
Notice that the ratio ?/? controls the preference
for an entity to descend from ? versus an existing
entity. Thus, by tuning this ratio, we can control
the number of entities inferred by our method, where
each entity corresponds to one of the child subtrees
of ?.
Also note that nx in the numerator of (8) means
that y?s parent is more likely to be frequent. Also,
ny +1 in the denominator means that a frequent y is
not as likely to have any parent x 6= ?, because its
first token probably falls early in the sequence where
there are fewer available parents x 6= ?.
5.3 Spanning tree algorithms
Define a complete directed graphG over the vertices
Y ? {?}. The weight of an edge x ? y is defined
by ?(y | x). The (approximate) posterior probability
of a given phylogeny given our evidence, is propor-
tional to the product of the ? values of its edges.
Formally, let T?(G) denote the set of spanning
trees of G rooted at ?, and define the weight of a
particular spanning tree T ? T?(G) to be the prod-
uct of the weights of its edges:
w(T ) =
?
(x?y)?T
?(y | x) (9)
Then the posterior probability of spanning tree T is
p?(T ) =
w(T )
Z(G)
(10)
where Z(G) =
?
T?T?(G)
w(T ) is the partition
function, i.e. the total probability of generating the
dataG via any spanning tree of the formwe consider.
This distribution is determined by the parameters ?
of the transducer p?, along with the ratio ?/?.
There exist several algorithms to find the sin-
glemaximum-probability spanning tree, notably Tar-
jan?s implementation of the Chu-Liu-Edmonds algo-
rithm, which runs in O(m log n) for a sparse graph
or O(n2) for a dense graph (Tarjan, 1977). Figure 1
shows a spanning tree found by our model using Tar-
jan?s algorithm. Here n is the number of vertices
(in our case, types and ), whilem is the number of
edges (which we can keep small by pruning, ?6.1).
6 Training the Transducer with EM
Our inference algorithm assumes that we know the
transducer parameters ?. We now explain how to op-
timize ? to maximize the marginal likelihood of the
training data. This marginal likelihood sums over all
the other latent variables in the model?the spanning
tree, the alignments between strings, and the hidden
token ordering.
The EMprocedure repeats the following until con-
vergence:
E-step: Given ?, compute the posterior marginal
probabilities cxy of all possible phylogeny
edges.
M-step Given all cxy, retrain ? to assign a high
conditional probability to the mutations on the
probable edges.
We actually use a variational EM algorithm: our
E step approximates the true distribution q over all
phylogenies with the closest distribution p that as-
signs positive probability only to type-based phylo-
genies. This distribution is given by (10) and min-
imizes KL(p || q). We argued in section ?5.2 that
it should be a good approximation. The posterior
marginal probability of a directed edge from vertex
x to vertex y, according to (10), is
cxy =
?
T?T?(G):(x?y)?T
p?(T ) (11)
The probability cxy is a ?pseudocount? for the ex-
pected number of mutations from x to y. This is at
most 1 under our assumptions.
Calculating cxy requires summing over all span-
ning trees of G, of which there are nn?2 for a fully
connected graph with n vertices. Fortunately, Tutte
(1984) shows how to compute this sum by the fol-
lowing method, which extends Kirchhoff?s classi-
cal matrix-tree theorem to weighted directed graphs.
This result has previously been employed in non-
projective dependency parsing (Koo et al2007;
Smith and Smith, 2007).
Let L ? Rn?n denote the Laplacian ofG, namely
L =
{ ?
x? ?(y | x
?) if x = y
??(y | x) if x 6= y
(12)
Tutte?s theorem relates the determinant of the Lapla-
cian to the spanning trees in graph G. In particular,
the cofactor L0,0 is equal to the sum of the weights
350
of all directed spanning trees rooted at 0, which (sup-
posing? is indexed at 0) yields the partition function
Z(G).
The edge marginals of interest are related to the
log partition function by
cxy =
?Z(G)
??(y | x)
(13)
which has the closed-form solution
cxy =
{
?(y | ?)L?1yy if x = y
?(y | x)(L?1xx ? L
?1
xy ) if x 6= y
(14)
Thus, the problem of computing edge marginals re-
duces to that of computing a matrix inverse, which
may be done in O(n3) time.
At the M step, we retrain the mutation model pa-
rameters ? to maximize
?
xy cxy log p(wy | wx).
This is tantamount to maximum conditional likeli-
hood training on a supervised collection of (wx, wy)
pairs that are respectively weighted by cxy.
The M step is nontrivial because the term p(wy |
wx) sums over a hidden alignment between two
strings. It may be performed by an inner loop of EM,
where the E step uses dynamic programming to ef-
ficiently consider all possible alignments, as in (Ris-
tad and Yianilos, 1996). In practice, we have found it
effective to take only a single step of this inner loop.
Such a Generalized EM procedure enjoys the same
convergence properties as EM, but may reach a local
optimum faster (Dempster et al1977).
6.1 Pruning the graph
For large graphs, it is essential to prune the number
of edges to avoid considering all n(n ? 1) input-
output pairs. To prune the graph, we eliminate all
edges between strings that do not share any common
trigrams (case- and diacritic-insensitive), by setting
their matrix entries to 0. As a result, the graph Lapla-
cian is a sparse matrix, which often allows faster
matrix inversion using preconditioned iterative algo-
rithms. Furthermore, pruned edges do not appear in
any spanning tree, so the E step will find that their
posterior marginal probabilities are 0. This means
that the input-output pairs corresponding to these
edges can be ignored when re-estimating the trans-
ducer parameters in the M step. We found that prun-
ing significantly improves training time with no ap-
preciable loss in performance.9
6.2 Training with unobserved tokens?
A deficiency of our method is that it assumes that
authors of our corpus have only been exposed to pre-
vious tokens in our corpus. In principle, one could
also train with U additional tokens (e, w) where we
observe neither e nor w, for very large U . This is the
?universe of discourse? in which our authors oper-
ate.10 In this case, we would need (expensive) new
algorithms to reconstruct the strings w. However,
this model could infer a more realistic phylogeny by
positing unobserved ancestral or intermediate forms
that relate the observed tokens, as in transformation
models (Eisner, 2002; Andrews and Eisner, 2011).
7 Experimental Evaluation
7.1 Data preparation
Scraping Wikipedia. Wikipedia documents many
variant names for entities. As a result, it has fre-
quently been used as a source for mining name vari-
ations, both within and across languages (Parton et
al., 2008; Cucerzan, 2007). We used Wikipedia to
create a list of name aliases for different entities.
Specifically, we mined English Wikipedia11 for all
redirects: page names that lead directly to another
page. Redirects are created by Wikipedia users for
resolving common name variants to the correct page.
For example, the pages titled Barack Obama Ju-
nior and Barack Hussein Obama automatically redi-
rect to the page titled Barack Obama. This redirec-
tion implies that the first two are name variants of
the third. Collecting all such links within English
Wikipedia yields a large number of aliases for each
page. However, many redirects are for topics other
than individual people, and these would be poor ex-
amples of name variation. In addition, some phrases
9For instance, on a dataset of approximately 6000 distinct
names, pruning reduced the number of outgoing edges at each
vertex to fewer than 100 per vertex.
10Notice that theN observed tokens would be approximately
exchangeable in this setting: they are unlikely to depend on one
another when N  U , and hence their order no longer matters
much. In effect, generating theU hidden tokens constructs a rich
distribution (analogous to a sample from the Dirichlet process)
from which the N observed tokens are then sampled IID.
11Using a Wikipedia dump from February 2, 2011.
351
Ho Chi Minh, Ho chi mihn, Ho-Chi Minh, Ho Chih-minh
Guy Fawkes, Guy fawkes, Guy faux, Guy Falks, Guy Faukes, Guy Fawks, Guy foxe, Guy Falkes
Nicholas II of Russia, Nikolai Aleksandrovich Romanov, Nicholas Alexandrovich of Russia, Nicolas II
Bill Gates, Lord Billy, Bill Gates, BillGates, Billy Gates, William Gates III, William H. Gates
William Shakespeare, William shekspere, William shakspeare, Bill Shakespear
Bill Clinton, Billll Clinton, William Jefferson Blythe IV, Bill J. Clinton, William J Clinton
Figure 2: Sample alias lists scraped from Wikipedia. Note that only partial alias lists are shown for space reasons.
that redirect to an entity are descriptions rather than
names. For example, 44th President of the United
States also links to Barack Obama, but it is not a
name variant.
Freebase filtering. To improve data quality we used
Freebase, a structured knowledge base that incorpo-
rates information from Wikipedia. Among its struc-
tured information are entity types, including the type
?person.? We filtered the Wikipedia redirect col-
lection to remove pairs where the target page was
not listed as a person in Freebase. Additionally, to
remove redirects that were not proper names (44th
President of the United States), we applied a series
of rule based filters to remove bad aliases: removing
numerical names, parentheticals after names, quota-
tion marks, and names longer than 5 tokens, since
we found that these long names were rarely person
names (e.g. United States Ambassador to the Eu-
ropean Union, Success Through a Positive Mental
Attitude which links to the author Napoleon Hill.)
While not perfect, these modifications dramatically
improved quality. The result was a list of 78,079 dif-
ferent person entities, each with one or more known
names or aliases. Some typical names are shown in
Figure 2.
Estimating empirical type counts. Our method is
really intended to be run on a corpus of string to-
kens. However, for experimental purposes, we in-
stead use the above dataset of string types because
this allows us to use the ?ground truth? given by
the Wikipedia redirects. To synthesize token counts,
empirical token frequencies for each type were esti-
mated from the LDC Gigaword corpus,12 which is
a corpus of newswire text spanning several years.
Wikipedia name types that did not appear in Giga-
word were assigned a ?backoff count? of one. Note
that by virtue of the domain, many misspellings will
12LDC Catalog No. LDC2003T05.
not appear; however, edges ?popular? names (which
may be canonical names) will be assigned higher
weight.
7.2 Experiments
We begin by evaluating the generalization ability of a
transducer trained using a transformation model. To
do so, we measure log-likelihood on held-out entity
title and alias pairs. We then verify that the general-
ization ability according to log-likelihood translates
into gains for a name matching task. For the experi-
ments in this section, we use ? = 0.9 and ? = 0.1.13
Held-out log-likelihood. We construct pairs of en-
tity title (input) and alias (output) names from the
Wikipedia data. For different amounts of supervised
data, we trained the transformation model on the
training set, and plotted the log-likelihood of held-
out test data for the transducer parameters at each it-
eration of EM. The held-out test set is constructed
from a disjoint set of Wikipedia entities, the same
number of entities as in the training set. We used
different corpora of 1000 and 1500 entities for train
and test.
Name matching. For each alias a in a test set (not
seen at training time), we produce a ranking of test
entity titles t according to transducer probabilities
p?(a | t). A good transducer should assign high
probability to transformations from the correct ti-
tle for the alias. Mean reciprocal rank (MRR) is a
commonly used metric to estimate the quality of a
ranking, which we report in Figure 4. The reported
mean is over all aliases in the test data. In addition to
evaluating the ranking for different initializations of
our transducer, we compare to two baselines: Lev-
enshtein distance and Jaro-Winkler similarity. Jaro-
Winkler is a measure on strings that was specifically
designed for record linkage (Winkler, 1999). The
13We did not find these parameters to be sensitive.
352
0 1 2 3 4 5 6 7 8 9EM iteration150000
140000
130000
120000
110000
100000
90000
Held o
ut log
-likelih
ood
sup=0sup=5sup=25sup=100sup=250
(a) 1000 entities.
0 1 2 3 4 5 6 7 8 9EM iteration240000
230000
220000
210000
200000
190000
180000
170000
160000
150000
Held o
ut log
-likelih
ood
sup=0sup=5sup=25sup=100sup=250
(b) 1500 entities.
Figure 3: Learning curves for different initializations of the transducer parameters. Above, ?sup=100? (for instance)
means that 100 entities were used as training data to initialize the transducer parameters (constructing pairs between
all title-alias pairs for those Wikpedia entities).
15000.60
0.65
0.70
0.75
0.80
0.85
MRR
jwinklevsup10semi10unsupsup
Figure 4: Mean reciprocal rank (MRR) results for differ-
ent training conditions: ?sup10? means that 10 entities
(roughly 40 name pairs) were used as training data for
the transducer; ?semi10? means that the ?sup10? model
was used as initialization before re-estimating the param-
eters using our model; ?unsup? is the transducer trained
using our model without any initial supervision; ?sup? is
trained on all 1500 entities in the training set (an upper
bound on performance); ?jwink? and ?lev? correspond to
Jaro-Winkler and Levenshtein distance baselines.
matching experiments were performed on a corpus
of 1500 entities (with separate corpora of the same
size for training and test).
8 Conclusions and Future Work
We have presented a new unsupervised method for
learning string-to-string transducers. It learns from
a collection of related strings whose relationships are
unknown. The key idea is that some strings are mu-
tations of common strings that occurred earlier. We
compute a distribution over the unknown phyloge-
netic tree that relates these strings, and use it to rees-
timate the transducer parameters via EM.
One direction for future work would be more so-
phisticated transduction models than the one we de-
veloped in ?4. For names, this could include learn-
ing common nicknames (nonparametrically); explic-
itly modeling abbreviation processes such as initials;
conditioning on name components such as title and
middle name; and transliterating across languages.14
In other domains, one could model bibliographic en-
try propagation, derivational morphology, or histor-
ical sound change (again using language tags).
Another future direction would be to incorporate
the context of tokens in order to help reconstruct
which tokens are coreferent. For example, we might
extend the generative story to generate a context for
token (e, w) conditioned on e. Combining contex-
tual similarity with string similarity has previously
proved very useful for identifying cognates (Schafer
and Yarowsky, 2002; Schafer, 2006b; Bergsma and
Van Durme, 2011). In our setting it would help to
distinguish people with identical names, as well as
determining whether two people with similar names
are really the same.
14These last two points suggest that the mutation model
should operate not on simple (entity, string) pairs, but on richer
representations in which the name has been parsed into its com-
ponents (Eisenstein et al2011), labeled with a language ID,
and perhaps labeled with a phonological pronunciation. These
additional properties of a named entity may be either observed
or latent in training data. For example, if wy and `y denote the
string and language of name y, then define p(y | x) = p(`y |
`x) ? p(wy | `y, `x, wx). The second factor captures translitera-
tion from language `x to language `y , e.g., by using ?4?s model
with an (`x, `y)-specific parameter setting.
353
References
Nicholas Andrews and Jason Eisner. 2011. Transformation pro-
cess priors. In NIPS 2011 Workshop on Bayesian Nonpara-
metrics: Hope or Hype?, Sierra Nevada, Spain, December.
Extended abstract (3 pages).
A. Bagga and B. Baldwin. 1998. Algorithms for scoring coref-
erence chains. In LREC.
Regina Barzilay and Lillian Lee. 2003. Learning to para-
phrase: an unsupervised approach using multiple-sequence
alignment. In Proc. of NAACL-HLT, pages 16?23, Strouds-
burg, PA, USA.
C. H. Bennett, M. Li, , and B. Ma. 2003. Chain letters
and evolutionary histories. Scientific American, 288(3):76?
81, June. More mathematical version available at http:
//www.cs.uwaterloo.ca/~mli/chain.html.
Shane Bergsma and Benjamin Van Durme. 2011. Learning
bilingual lexicons using the visual similarity of labeled web
images. In Proc. of IJCAI, pages 1764?1769, Barcelona,
Spain.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive
duplicate detection using learnable string similarity mea-
sures. In Proc. of ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ?03, pages
39?48, New York, NY, USA. ACM.
Alexandre Bouchard-C?t?, Percy Liang, Thomas Griffiths, and
Dan Klein. 2008. A probabilistic approach to language
change. In Proc. of NIPS, pages 169?176.
S. Cucerzan. 2007. Large-scale named entity disambiguation
based on Wikipedia data. In Proc. of EMNLP.
Aron Culotta, Michael Wick, Robert Hall, Matthew Marzilli,
and Andrew McCallum. 2007. Canonicalization of database
records using adaptive similarity measures. In Proc. of ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ?07, pages 201?209.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society. Series B (Method-
ological), 39(1):1?38.
Z. Dias, A. Rocha, and S. Goldenstein. 2012. Image phy-
logeny by minimal spanning trees. IEEE Trans. on Informa-
tion Forensics and Security, 7(2):774?788, April.
Markus Dreyer and Jason Eisner. 2011. Discovering morpho-
logical paradigms from plain text using a Dirichlet process
mixture model. In Proc. of EMNLP, pages 616?627. Sup-
plementary material (9 pages) also available.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-
variable modeling of string transductions with finite-state
methods. In Proc. of EMNLP, pages 1080?1089, Honolulu,
Hawaii, October. Association for Computational Linguistics.
Jacob Eisenstein, Tae Yano, William Cohen, Noah Smith, and
Eric Xing. 2011. Structured databases of named entities
fromBayesian nonparametrics. InProc. of the First workshop
on Unsupervised Learning in NLP, pages 2?12, Edinburgh,
Scotland, July. Association for Computational Linguistics.
Jason Eisner. 2002. Transformational priors over grammars.
In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Philadelphia, July.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences?Computer Science and Computational Biology.
Cambridge University Press.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan
Klein. 2008. Learning bilingual lexicons from monolingual
corpora. In Proc. of ACL-08: HLT, pages 771?779.
David Hall and Dan Klein. 2010. Finding cognates using phylo-
genies. In Association for Computational Linguistics (ACL).
Rob Hall, Charles Sutton, and Andrew McCallum. 2008. Un-
supervised deduplication using cross-field dependencies. In
Proc. of the ACM SIGKDD International Conference On
Knowledge Discovery and Data Mining, KDD ?08, pages
310?317.
Md. Enamul. Karim, Andrew Walenstein, Arun Lakhotia, and
Laxmi Parida. 2005. Malware phylogeny generation using
permutations of code. Journal in Computer Virology, 1(1?
2):13?23.
Alexandre Klementiev and Dan Roth. 2006. Weakly supervised
named entity transliteration and discovery from multilingual
comparable corpora. In Proc. of COLING-ACL, pages 817?
824.
K. Knight and J. Graehl. 1998. Machine transliteration. Com-
putational Linguistics, 24:599?612.
Terry Koo, Amir Globerson, Xavier Carreras, and Michael
Collins. 2007. Structured prediction models via the matrix-
tree theorem. In Proc. of EMNLP-CoNLL, pages 141?150.
Jose Oncina and Marc Sebban. 2006. Using learned conditional
distributions as edit distance. In Proc. of the 2006 Joint IAPR
international Conference on Structural, Syntactic, and Statis-
tical Pattern Recognition, SSPR?06/SPR?06, pages 403?411.
Kristen Parton, Kathleen R. McKeown, James Allan, and En-
rique Henestroza. 2008. Simultaneous multilingual search
for translingual information retrieval. In Proceeding of the
ACM conference on Information and Knowledge Manage-
ment, CIKM ?08, pages 719?728.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learning string
edit distance. Technical Report CS-TR-532-96, Princeton
University, Department of Computer Science.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recognition and
Machine Intelligence, 20(5):522?532, May.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2011.
An algorithm for unsupervised transliteration mining with an
application to word alignment. In Proc. of ACL, pages 430?
439.
Charles Schafer and David Yarowsky. 2002. Inducing transla-
tion lexicons via diverse similarity measures and bridge lan-
guages. In Proc. of CONLL, pages 146?152.
Charles Schafer. 2006a. Novel probabilistic finite-state transduc-
ers for cognate and transliteration modeling. In 7th Biennial
Conference of the Association for Machine Translation in the
Americas (AMTA).
Charles Schafer. 2006b. Translation Discovery Using Diverse
Smilarity Measures. Ph.D. thesis, Johns Hopkins University.
David A. Smith and Noah A. Smith. 2007. Probabilistic mod-
els of nonprojective dependency trees. In Proc. of EMNLP-
CoNLL, pages 132?140.
354
R. T. Smythe and H. M. Mahmoud. 1995. A survey of recur-
sive trees. Theory of Probability andMathematical Statistics,
51(1?27).
Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A
statistical model for lost language decipherment. In Proc. of
ACL, pages 1048?1057.
Koichiro Tamura, Daniel Peterson, Nicholas Peterson, Glen
Stecher, Masatoshi Nei, and Sudhir Kumar. 2011. Mega5:
Molecular evolutionary genetics analysis using maximum
likelihood, evolutionary distance, and maximum parsimony
methods. Molecular Biology and Evolution, 28(10):2731?
2739.
R E Tarjan. 1977. Finding optimum branchings. Networks,
7(1):25?35.
W. Tutte. 1984. Graph Theory. Addison-Wesley.
William E. Winkler. 1999. The state of record linkage and cur-
rent research problems. Technical report, Statistical Research
Division, U.S. Census Bureau.
355
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1302?1312, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Multi-Domain Learning: When Do Domains Matter?
Mahesh Joshi
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
maheshj@cs.cmu.edu
Mark Dredze
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, Maryland 21211
mdredze@cs.jhu.edu
William W. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
wcohen@cs.cmu.edu
Carolyn P. Rose?
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cprose@cs.cmu.edu
Abstract
We present a systematic analysis of exist-
ing multi-domain learning approaches with re-
spect to two questions. First, many multi-
domain learning algorithms resemble ensem-
ble learning algorithms. (1) Are multi-domain
learning improvements the result of ensemble
learning effects? Second, these algorithms are
traditionally evaluated in a balanced class la-
bel setting, although in practice many multi-
domain settings have domain-specific class
label biases. When multi-domain learning
is applied to these settings, (2) are multi-
domain methods improving because they cap-
ture domain-specific class biases? An under-
standing of these two issues presents a clearer
idea about where the field has had success in
multi-domain learning, and it suggests some
important open questions for improving be-
yond the current state of the art.
1 Introduction
Research efforts in recent years have demonstrated
the importance of domains in statistical natural lan-
guage processing. A mismatch between training and
test domains can negatively impact system accuracy
as it violates a core assumption in many machine
learning algorithms: that data points are indepen-
dent and identically distributed (i.i.d.). As a result,
numerous domain adaptation methods (Chelba and
Acero, 2004; Daume? III and Marcu, 2006; Blitzer et
al., 2007) target settings with a training set from one
domain and a test set from another.
Often times the training set itself violates the i.i.d.
assumption and contains multiple domains. In this
case, training a single model obscures domain dis-
tinctions, and separating the dataset by domains re-
duces training data. Instead, multi-domain learn-
ing (MDL) can take advantage of these domain la-
bels to improve learning (Daume? III, 2007; Dredze
and Crammer, 2008; Arnold et al 2008; Finkel and
Manning, 2009; Zhang and Yeung, 2010; Saha et al
2011). One such example is sentiment classification
of product reviews. Training data is available from
many product categories and while all data should
be used to learn a model, there are important differ-
ences between the categories (Blitzer et al 2007)1.
While much prior research has shown improve-
ments using MDL, this paper explores what prop-
erties of an MDL setting matter. Are previous im-
provements from MDL algorithms discovering im-
portant distinctions between features in different do-
mains, as we would hope, or are other factors con-
tributing to learning success? The key question of
this paper is: when do domains matter?
Towards this goal we explore two issues. First,
we explore the question of whether domain distinc-
tions are used by existing MDL algorithms in mean-
ingful ways. While differences in feature behaviors
between domains will hurt performance (Blitzer et
al., 2008; Ben-David et al 2009), it is not clear
if the improvements in MDL algorithms can be at-
tributed to correcting these errors, or whether they
are benefiting from something else. In particular,
there are many similarities between MDL and en-
semble methods, with connections to instance bag-
1Blitzer et al(2007) do not consider the MDL setup, they
consider a single source domain, and a single target domain,
with little or no labeled data available for the target domain.
1302
ging, feature bagging and classifier combination. It
may be that gains in MDL are the usual ensemble
learning improvements.
Second, one simple way in which domains can
change is the distribution of the prior over the la-
bels. For example, reviews of some products may be
more positive on average than reviews of other prod-
uct types. Simply capturing this bias may account
for significant gains in accuracy, even though noth-
ing is learned about the behavior of domain-specific
features. Most prior work considers datasets with
balanced labels. However, in real world applica-
tions, where labels may be biased toward some val-
ues, gains from MDL could be attributed to simply
modeling domain-specific bias. A practical advan-
tage of such a result is ease of implementation and
the ability to scale to many domains.
Overall, irrespective of the answers to these ques-
tions, a better understanding of the performance of
existing MDL algorithms in different settings will
provide intuitions for improving the state of the art.
2 Multi-Domain Learning
In the multi-domain learning (MDL) setting, exam-
ples are accompanied by both a class label and a do-
main indicator. Examples are of the form (xi, y,di),
where xi ? RN , di is a domain indicator, xi is
drawn according to a fixed domain-specific distri-
bution Ddi , and yi is the label (e.g. yi ? {?1,+1}
for binary labels). Standard learning ignores di, but
MDL uses these to improve learning accuracy.
Why should we care about the domain label? Do-
main differences can introduce errors in a number
of ways (Ben-David et al 2007; Ben-David et al
2009). First, the domain-specific distributions Ddi
can differ such that they favor different features, i.e.
p(x) changes between domains. As a result, some
features may only appear in one domain. This aspect
of domain difference is typically the focus of un-
supervised domain adaptation (Blitzer et al 2006;
Blitzer et al 2007). Second, the features may be-
have differently with respect to the label in each do-
main, i.e. p(y|x) changes between domains. As a
result, a learning algorithm cannot generalize the be-
havior of features from one domain to another. The
key idea behind many MDL algorithms is to target
one or both of these properties of domain difference
to improve performance.
Prior approaches to MDL can be broadly catego-
rized into two classes. The first set of approaches
(Daume? III, 2007; Dredze et al 2008) introduce pa-
rameters to capture domain-specific behaviors while
preserving features that learn domain-general be-
haviors. A key of these methods is that they do not
explicitly model any relationship between the do-
mains. Daume? III (2007) proposes a very simple
?easy adapt? approach, which was originally pro-
posed in the context of adapting to a specific target
domain, but easily generalizes to MDL. Dredze et al
(2008) consider the problem of learning how to com-
bine different domain-specific classifiers such that
behaviors common to several domains can be cap-
tured by a shared classifier, while domain-specific
behavior is still captured by the individual classi-
fiers. We describe both of these approaches in ? 3.2.
The second set of approaches to MDL introduce
an explicit notion of relationship between domains.
For example, Cavallanti et al(2008) assume a fixed
task relationship matrix in the context of online
multi-task learning. The key assumption is that in-
stances from two different domains are half as much
related to each other as two instances from the same
domain. Saha et al(2011) improve upon the idea
of simply using a fixed task relationship matrix by
instead learning it adaptively. They derive an online
algorithm for updating the task interaction matrix.
Zhang and Yeung (2010) derive a convex formu-
lation for adaptively learning domain relationships.
We describe their approach in ? 3.2. Finally, Daume?
III (2009) proposes a joint task clustering and multi-
task/multi-domain learning setup, where instead of
just learning pairwise domain relationships, a hier-
archical structure among them is inferred. Hierar-
chical clustering of tasks is performed in a Bayesian
framework, by imposing a hierarchical prior on the
structure of the task relationships.
In all of these settings, the key idea is to learn
both domain-specific behaviors and behaviors that
generalize between (possibly related) domains.
3 Data
To support our analysis we develop several empir-
ical experiments. We first summarize the datasets
and methods that we use in our experiments, then
1303
proceed to our exploration of MDL.
3.1 Datasets
A variety of multi-domain datasets have been used
for demonstrating MDL improvements. In this pa-
per, we focus on two datasets representative of many
of the properties of MDL.
Amazon (AMAZON) Our first dataset is the Multi-
Domain Amazon data (version 2.0), first introduced
by Blitzer et al(2007). The task is binary sentiment
classification, in which Amazon product reviews are
labeled as positive or negative. Domains are defined
by product categories. We select the four domains
used in most studies: books, dvd, electronics
and kitchen appliances.
The original dataset contained 2,000 reviews for
each of the four domains, with 1,000 positive and
1,000 negative reviews per domain. Feature extrac-
tion follows Blitzer et al(2007): we use case insen-
sitive unigrams and bigrams, although we remove
rare features (those that appear less than five times
in the training set). The reduced feature set was se-
lected given the sensitivity to feature size of some of
the MDL methods.
ConVote (CONVOTE) Our second dataset is taken
from segments of speech from United States
Congress floor debates, first introduced by Thomas
et al(2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discus-
sion in the floor debate. We select this dataset be-
cause, unlike the AMAZON data, CONVOTE can be
divided into domains in several ways based on dif-
ferent metadata attributes available with the dataset.
We consider two types of domain divisions: the bill
identifier and the political party of the speaker. Di-
vision based on the bill creates domain differences
in that each bill has its own topic. Division based on
political party implies preference for different issues
and concerns, which manifest as different language.
We refer to these datasets as BILL and PARTY.
We use Version 1.1 of the CONVOTE dataset,
available at http://www.cs.cornell.edu/
home/llee/data/convote.html. More
specifically, we combine the training, development
and test folds from the data stage three/ ver-
sion, and sub-sample to generate different versions
of the dataset required for our experiments. For
BILL we randomly sample speech segments from
three different bills. The three bills and the number
of instances for each were chosen such that we have
sufficient data in each fold for every experiment.
For PARTY we randomly sample speech segments
from the two major political parties (Democrats and
Republicans). Feature processing was identical to
AMAZON, except that the threshold for feature re-
moval was two.
3.2 Learning Methods and Features
We consider three MDL algorithms, two are repre-
sentative of the first approach and one of the second
approach (learning domain similarities) (?2). We fa-
vored algorithms with available code or that were
straightforward to implement, so as to ensure repro-
ducibility of our results.
FEDA Frustratingly easy domain adaptation
(FEDA) (Daume? III, 2007; Daume? III et al 2010b;
Daume? III et al 2010a) is an example of a classifier
combination approach to MDL. The feature space
is a cross-product of the domain and input features,
augmented with the original input features (shared
features). Prediction is effectively a linear combina-
tion of a set of domain-specific weights and shared
weights. We combine FEDA with both the SVM
and logistic regression algorithms described below
to obtain FEDA-SVM and FEDA-LR.
MDR Multi-domain regularization (MDR) (Dredze
and Crammer, 2008; Dredze et al 2009) extends the
idea behind classifier combination by explicitly for-
mulating a classifier combination scheme based on
Confidence-Weighted learning (Dredze et al 2008).
Additionally, classifier updates (which happen in
an online framework) contain an explicit constraint
that the combined classifier should perform well on
the example. Dredze et al(2009) consider several
variants of MDR. We select the two best perform-
ing methods: MDR-L2, which uses the underlying
algorithm of Crammer et al(2008), and MDR-KL,
which uses the underlying algorithm of Dredze et al
(2008). We follow their approach to classifier train-
ing and parameter optimization.
MTRL The multi-task relationship learning
(MTRL) approach proposed by Zhang and Yeung
1304
(2010) achieves states of the art performance on
many MDL tasks. This method is representative
of methods that learn similarities between domains
and in turn regularize domain-specific parameters
accordingly. The key idea in their work is the use
of a matrix-normal distribution p(X|M ,?,?) as
a prior on the matrix W created by column-wise
stacking of the domain-specific classifier weight
vectors. ? represents the covariance matrix for the
variables along the columns of X . When used as
a prior over W it models the covariance between
the domain-specific classifiers (and therefore the
tasks). ? is learned jointly with the domain-specific
classifiers. This method has similar benefits to
FEDA in terms of classifier combination, but also
attempts to model domain relationships. We use
the implementation of MTRL made available by the
authors2. For parameter tuning, we perform a grid
search over the parameters ?1 and ?2, using the fol-
lowing values for each (a total of 36 combinations):
{0.00001, 0.0001, 0.001, 0.01, 0.1, 1}.
In addition to these multi-task learning methods,
we consider a common baseline: ignoring the do-
main distinctions and learning a single classifier
over all the data. This reflects single-domain learn-
ing, in which no domain knowledge is used and will
indicate baseline performance for all experiments.
While some earlier research has included a sepa-
rate one classifier per domain baseline, it almost al-
ways performs worse, since splitting the domains
provides much less data to each classifier (Dredze
et al 2009). So we omit this baseline for simplicity.
To obtain a single classifier we use two classifica-
tion algorithms: SVMs and logistic regression.
Support Vector Machines A single SVM run
over all the training data, ignoring domain labels.
We use the SVM implementation available in the LI-
BLINEAR package (Fan et al 2008). In particular,
we use the L2-regularized L2-loss SVM (option -s
1 in version 1.8 of LIBLINEAR, and also option -B
1 for including a standard bias feature). We tune
the SVM using five-fold stratified cross-validation
on the training set, using the following values for
the trade-off parameterC: {0.0001, 0.001, 0.01, 0.1,
0.2, 0.3, 0.5, 1}.
2http://www.cse.ust.hk/?zhangyu/codes/
MTRL.zip
Logistic Regression (LR) A single logistic re-
gression model run over all the training data, ignor-
ing domain labels. Again, we use the L2-regularized
LR implementation available in the LIBLINEAR
package (option -s 0, and also option -B 1). We
tune the LR model using the same strategy as the
one used for SVM above, including the values of the
trade-off parameter C.
For all experiments, we measure average accu-
racy overK-fold cross-validation, using 10 folds for
AMAZON, and 5 folds for both BILL and PARTY.
4 When Do Domains Matter?
We now empirically explore two questions regarding
the behavior of MDL.
4.1 Ensemble Learning
Question: Are MDL improvements the result of
ensemble learning effects?
Many of the MDL approaches bear a striking
resemblance to ensemble learning. Traditionally,
ensemble learning combines the output from sev-
eral different classifiers to obtain a single improved
model (Maclin and Opitz, 1999). It is well estab-
lished that ensemble learning, applied on top of a
diverse array of quality classifiers, can improve re-
sults for a variety of tasks. The key idea behind
ensemble learning, that of combining a diverse ar-
ray of models, has been applied to settings in which
data preprocessing is used to create many different
classifiers. Examples include instance bagging and
feature bagging (Dietterich, 2000).
The core idea of using diverse inputs in making
classification decisions is common in the MDL liter-
ature. In fact, the top performing and only success-
ful entry to the 2007 CoNLL shared task on domain
adaptation for dependency parsing was a straightfor-
ward implementation of ensemble learning by cre-
ating variants of parsers (Sagae and Tsujii, 2007).
Many MDL algorithms, among them Dredze and
Crammer (2008), Daume? III (2009), Zhang and Ye-
ung (2010) and Saha et al(2011), all include some
notion of learning domain-specific classifiers on the
training data, and combining them in the best way
possible. To be clear, we do not claim that these
approaches can be reduced to an existing ensem-
ble learning algorithm. There are crucial elements
1305
in each of these algorithms that separate them from
existing ensemble learning algorithms. One exam-
ple of such a distinction is the learning of domain
relationships by both Zhang and Yeung (2010) and
Saha et al(2011). However, we argue that their
core approach, that of combining parameters that are
trained on variants of the data (all data or individual
domains), is an ensemble learning idea.
Consider instance bagging, in which multiple
classifiers are each trained on random subsets of the
data. The resulting classifiers are then combined
to form a final model. In MDL, we can consider
each domain a subset of the data, albeit non-random
and non-overlapping. The final model combines the
domain-specific parameters and parameters trained
on other instances, which in the case of FEDA are the
shared parameters. In this light, these methods are a
complex form of instance bagging, and their devel-
opment could be justified from this perspective.
However, given this justification, are improve-
ments from MDL simply the result of standard en-
semble learning effects, or are these methods re-
ally learning something about domain behavior? If
knowledge of domain was withheld from the algo-
rithm, could we expect similar improvements? As
we will do in each empirical experiment, we propose
a contrarian hypothesis:
Hypothesis: Knowledge of domains is irrelevant
for MDL.
Empirical Evaluation We evaluate this hypothe-
sis as follows. We begin by constructing a true MDL
setting, in which we attempt to improve accuracy
through knowledge of the domains. We will apply
three MDL algorithms (FEDA, MDR, and MTRL) to
our three multi-domain datasets (AMAZON, BILL,
and PARTY) and compare them against a single clas-
sifier baseline. We will then withhold knowledge
of the true domains from these algorithms and in-
stead provide them with random ?pseudo-domains,?
and then evaluate the change in their behavior. The
question is whether we can obtain similar benefits
by ignoring domain labels and relying strictly on an
ensemble learning motivation (instance bagging).
For the ?True Domain? setting, we apply the
MDL algorithms as normal. For the ?Random Do-
main? setting, we randomly shuffle the domain la-
bels within a given class label within each fold, thus
maintaining the same number of examples for each
domain label, and also retaining the same class dis-
tribution within each randomized domain. The re-
sulting ?pseudo-domains? are then similar to ran-
dom subsets of the data used in ensemble learning.
Following the standard practice in previous work,
for this experiment we use a balanced number of
examples from each domain and a balanced num-
ber of positive and negative labels (no class bias).
For AMAZON (4 domains), we have 10 folds of 400
examples per fold, for BILL (3 domains) 5 folds of
60 examples per fold, and for PARTY (2 domains) 5
folds of 80 examples per fold. In the ?Random Do-
main? setting, since we are randomizing the domain
labels, we increase the number of trials. We repeat
each cross-validation experiment 5 times with differ-
ent randomization of the domain labels each time.
Results Results are shown in Table 1. The first
row shows absolute (average) accuracy for a single
classifier trained on all data, ignoring domain dis-
tinctions. The remaining cells indicate absolute im-
provements against the baseline.
First, we note for the well-studied AMAZON
dataset that our results with true domains are con-
sistent with the previous literature. FEDA is known
to not improve upon a single classifier baseline for
that dataset (Dredze et al 2009). Both MDR-L2 and
MDR-KL improve upon the single classifier baseline,
again as per Dredze et al(2009). And finally, MTRL
also improves upon the single classifier baseline. Al-
though the MTRL improvement is not as dramatic as
in the original paper3, the average accuracy that we
achieve for MTRL (84.2%) is better than the best av-
erage accuracy in the original paper (83.65%).
The main comparison to make in Table 1 is be-
tween having knowledge of true domains or not.
?Random Domain? in the table is the case where do-
main identifiers are randomly shuffled within a given
fold. Ignoring the significance test results for now,
overall the results indicate that knowing the true do-
mains is useful for MDL algorithms. Randomiz-
ing the domains does not work better than knowing
true domains in any case. However, in all except
one case, the improvements of MDL algorithms are
3This might be due to a different version of the dataset being
used in a cross-validation setup, rather than their train/test setup,
and also because of differences in baseline approaches.
1306
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
83.93% 83.78% 66.67% 68.00% 62.75% 64.00%
FEDA
True Domain -0.35 -0.10 +2.33 + 1.00 +4.25 N +1.25
Random Domain -1.30 H -1.02 H -1.20 -2.07 -2.05 -2.10
MDR-L2
True Domain +1.87 N +2.02 N +0.00 -1.33 +2.25 +1.00
Random Domain +0.91 N +1.07 N -2.67 -4.00 -2.80 -4.05
MDR-KL
True Domain +1.85 N +2.00 N +1.00 -0.33 +3.00 +1.75
Random Domain +1.36 N +1.51 N +0.60 -0.73 -1.30 -2.55 H
MTRL
True Domain +0.27 +0.42 +0.67 -0.67 +1.50 +0.25
Random Domain -0.37 -0.21 -1.47 -2.80 -3.55 -4.80
Table 1: A comparison between MDL methods with access to the ?True Domain? labels and methods that
use ?Random Domain? information, essentially ensemble learning. The first row has raw accuracy numbers,
whereas the remaining entries are absolute improvements over the baseline. N: Significantly better than the
corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. H: Significantly worse than
corresponding baseline, with p < 0.05, using a paired t-test.
significantly better only for the AMAZON dataset4.
And interestingly, exactly in the same case, ran-
domly shuffling the domains also gives significant
improvements compared to the baseline, showing
that there is an ensemble learning effect in operation
for MDR-L2 and MDR-KL on the AMAZON dataset.
For FEDA, randomizing the domains significantly
hurts its performance on the AMAZON data, as is
the case for MDR-KL on the PARTY data. Therefore,
while our contrarian hypothesis about irrelevance of
domains is not completely true, it is indeed the case
that some MDL methods benefit from the ensemble
learning effect.
A second observation to be made from these re-
sults is that, while all of empirical research on MDL
assumes the definition of domains as a given, the
question of how to split a dataset into domains given
various metadata attributes is still open. For exam-
ple, in our experiments, in general, using the po-
litical party as a domain distinction gives us more
improvements over the corresponding baseline ap-
proach5.
We provide a detailed comparison of using true
4Some numbers in Table 1 might appear to be significant,
but are not. That is because of high variance in the performance
of the methods across the different folds.
5The BILL and the PARTY datasets are not directly compa-
rable to each other, although the prediction task is the same.
vs. randomized domains in Table 6, after presenting
the second set of experimental results.
4.2 Domain-specific Class Bias
Question: Are MDL methods improving because
they capture domain-specific class biases?
In previous work, and the above section, experi-
ments have assumed a balanced dataset in terms of
class labels. It has been in these settings that MDL
methods improve. However, this is an unrealistic as-
sumption. Even in our datasets, the original versions
demonstrated class bias: Amazon product reviews
are generally positive, votes on bills are rarely tied,
and political parties vote in blocs. While it is com-
mon to evaluate learning methods on balanced data,
and then adjust for imbalanced real world datasets, it
is unclear what effect domain-specific class bias will
have on MDL methods. Domains can differ in their
proportion of examples of different classes. For ex-
ample, it is quite likely that less controversial bills in
the United States Congress will have more yes votes
than controversial bills. Similarly, if instead of the
category of a product, its brand is considered as a do-
main, it is likely that some brands receive a higher
proportion of positive reviews than others.
Improvements from MDL in such settings may
simply be capturing domain-specific class biases.
1307
domain class cb1 cb2 cb3 cb4
AMAZON
b
- 20 80 60 40
+ 80 20 40 60
d
- 40 20 80 60
+ 60 80 20 40
e
- 60 40 20 80
+ 40 60 80 20
k
- 80 60 40 20
+ 20 40 60 80
BILL
031
N 16 4 8 12
Y 4 16 12 8
088
N 12 16 4 8
Y 8 4 16 12
132
N 8 12 16 4
Y 12 8 4 16
PARTY
D
N 10 30 15 25
Y 30 10 25 15
R
N 30 10 25 15
Y 10 30 15 25
Table 2: The table shows the distribution of in-
stances across domains and class labels within one
fold of each of the datasets, for four different class
bias trials. These datasets with varying class bias
across domains were used for the experiments de-
scribed in ?4.2
Consider two domains, where each domain is biased
towards the opposite label. In this case, domain-
specific parameters may simply be capturing the bias
towards the class label, increasing the weight uni-
formly of features predictive of the dominant class.
Similarly, methods that learn domain similarity may
be learning class bias similarity.
Why does the effectiveness of these domain-
specific bias parameters matter? First, if capturing
domain-specific class bias is the source of improve-
ment, there are much simpler methods for learning
that can be just as effective. This would be espe-
cially important in settings where we have many do-
mains, and learning domain-specific parameters for
each feature becomes infeasible. Second, if class
bias accounted for most of the improvement in learn-
ing, it suggests that such settings could be amenable
to unsupervised adaptation of the bias parameters.
Hypothesis: MDL largely capitalizes on
domain-specific class bias.
Empirical Evaluation To evaluate our hypothe-
sis, for each of our three datasets we create 4 random
versions, each with some domain-specific class-bias.
A summary of the dataset partitions is shown in
Table 2. For example, for the AMAZON dataset,
we create 4 versions (cb1 . . . cb4), where each do-
main has 100 examples per fold and each domain
has a different balance between positive and nega-
tive classes. For each of these settings, we conduct
a 10-fold cross validation experiment, then average
the CV results for each of the 4 settings. The re-
sulting accuracy numbers therefore reflect an aver-
age across many types of bias, each evaluated many
times. We do a similar experiment for the BILL and
PARTY datasets, except we use 5-fold CV.
In addition to the multi-domain and baseline
methods, we add a new baseline: DOM-ID. In this
setting, we augment the baseline classifier (which
ignores domain labels) with a new feature that in-
dicates the domain label. While we already include
a general bias feature, as is common in classifica-
tion tasks, these new features will capture domain-
specific bias. This is the only change to the base-
line classifier, so improvements over the baseline are
indicative of the change in domain-bias that can be
captured using these simple features.
Results Results are shown in Table 3. The table
follows the same structure as Table 1, with the ad-
dition of the results for the DOM-ID approach. We
first examine the efficacy of MDL in this setting. An
observation that is hard to miss is that MDL results
in these experiments show significant improvements
in almost all cases, as compared to only a few cases
in Table 1, despite the fact that even the baseline ap-
proaches have a higher accuracy. This shows that
MDL results can be highly influenced by systematic
differences in class bias across domains. Note that
there is also a significant negative influence of class
bias on MTRL for the AMAZON data.
A comparison of the MDL results on true domains
to the DOM-ID baseline gives us an idea of how
much MDL benefits purely from class bias differ-
ences across domains. We see that in most cases,
about half of the improvement seen in MDL is ac-
counted for by a simple baseline of using the do-
main identifier as a feature, and all but one of the
improvements from DOM-ID are significant. This
1308
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
85.52% 85.46% 70.50% 70.67% 65.44% 65.81%
FEDA
True Domain +0.11 +0.31 +4.25 N +4.00 N +4.81 N +4.69 N
Random Domain +0.94 N +1.03 N +3.68 N +4.03 N +4.24 +3.73
MDR-L2
True Domain +0.92 N +0.98 N +4.42 N +4.25 N +1.31 +0.94
Random Domain +1.86 N +1.92 N +3.93 N +3.77 N +0.65 +0.28
MDR-KL
True Domain +1.54 N +1.59 N +5.17 N +5.00 N +4.25 N +3.88 N
Random Domain +2.84 N +2.90 N +4.13 N +3.97 N +3.81 N +3.44
MTRL
True Domain -1.22 H -1.17 H +4.50 N +4.33 N +6.44 N +6.06 N
Random Domain -0.69 H -0.63 H +3.53 N +3.37 N +4.87 N +4.50 N
DOM-ID
True Domain +0.36 +0.38 N +2.83 N +2.75 N +3.75 N +4.00 N
Random Domain +1.73 N +1.76 N +4.50 N +4.98 N +5.24 N +5.31 N
Table 3: A comparison between MDL methods with class biased data. Similar to the setup where we
evaluate the ensemble learning effect, we have a setting of using randomized domains. N: Significantly
better than the corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. H: Significantly
worse than corresponding baseline, with p < 0.05, using a paired t-test.
suggests that in a real-world scenario where differ-
ence in class bias across domains is quite likely, it is
useful to consider DOM-ID as a simple baseline that
gives good empirical performance. To our knowl-
edge, using this approach as a baseline is not stan-
dard practice in MDL literature.
Finally, we also include the ?Random Domain?
evaluation in the our class biased version of exper-
iments. Each ?Random Domain? result in Table 3
is an average over 20 cross-validation runs (5 ran-
domized trials for each of the four class biased tri-
als cb1 . . . cb4). This setup combines the effects
of ensemble learning and bias difference across do-
mains. As seen in the table, for MDL algorithms the
results are consistently better as compared to know-
ing the true domains for the AMAZON dataset. For
the other datasets, the performance after randomiz-
ing the domains is still significantly better than the
baseline. This evaluation on randomized domains
further strengthens the conclusion that differences in
bias across domains play an important role, even in
the case of noisy domains. Looking at the perfor-
mance of DOM-ID with randomized domains, we
see that in all cases the DOM-ID baseline performs
better with randomized domains. While the dif-
ference is significant mostly only on the AMAZON
domain class cb5 cb6 cb7 cb8
AMAZON
b
- 20 40 60 80
+ 80 60 40 20
d
- 20 40 60 80
+ 80 60 40 20
e
- 20 40 60 80
+ 80 60 40 20
k
- 20 40 60 80
+ 20 40 60 80
Table 4: The table shows the distribution of in-
stances across domains and class labels within one
fold of the AMAZON dataset, for four different class
bias trials. For the BILL and PARTY datasets, similar
folds with consistent bias were created (number of
examples used was different). These datasets with
consistent class bias across domains were used for
the experiments described in ?4.2.1
dataset (details in Table 6, columns under ?Varying
Class Bias,?) this trend is still counter-intuitive. We
suspect this might be because randomization creates
a noisy version of the domain labels, which helps
learners to avoid over-fitting that single feature.
1309
4.2.1 Consistent Class Bias
We also performed a set of experiments that ap-
ply MDL algorithms to a setting where the datasets
have different class biases (unlike the experiments
reported in Table 1, where the classes are balanced),
but, unlike the experiments reported in Table 3, the
class bias is the same within each of the domains.
We refer to this as the case of consistent class bias
across domains. The distribution of classes within
each domain within each fold is shown in Table 4.
The results for this set of experiments are reported
in Table 5. The structure of Table 5 is identical to
that of Table 1. Comparing these results to those
in Table 1, we can see that in most cases the im-
provements seen using MDL algorithms are lower
than those seen in Table 1. This is likely due to
the higher baseline performance in the consistent
class bias case. A notable difference is in the per-
formance of MTRL ? it is significantly worse for
the AMAZON dataset, and significantly better for the
PARTY dataset. For the AMAZON dataset, we be-
lieve that the domain distinctions are less meaning-
ful, and hence forcing MTRL to learn the relation-
ships results in lower performance. For the PARTY
dataset, in the case of a class-biased setup, know-
ing the party is highly predictive of the vote (in the
original CONVOTE dataset, Democrats mostly vote
?no? and Republicans mostly vote ?yes?), and this
is rightly exploited by MTRL.
4.2.2 True vs. Randomized Domains
In Table 6 we analyze the difference in perfor-
mance of MDL methods when using true vs. ran-
domized domain information. For the three sets of
results reported earlier, we evaluated whether using
true domains as compared to randomized domains
gives significantly better, significantly worse or
equal performance. Significance testing was done
using a paired t-test with ? = 0.05 as before. As the
table shows, for the first set of results where the class
labels were balanced (overall, as well as within each
domain), using true domains was significantly better
mostly only for the AMAZON dataset. FEDA-SVM
was the only approach that was consistently better
with true domains across all datasets. Note, how-
ever, that it was significantly better than the baseline
approach only for PARTY.
For the second set of results (Table 3) where the
class bias varied across the different domains, us-
ing true domains was either no different from using
randomized domains, or it was significantly worse.
In particular, it was consistently significantly worse
to use true domains on the AMAZON dataset. This
questions the utility of domains on the AMAZON
dataset in the context of MDL in a domain-specific
class bias scenario. Since randomizing the domains
works better for all of the MDL methods on AMA-
ZON, it suggests that an ensemble learning effect
is primarily responsible for the significant improve-
ments seen on the AMAZON data, when evaluated in
a domain-specific class bias setting.
Finally, for the case of consistent class bias across
domains, the trend is similar to the case of no class
bias ? using true domains is useful. This table
further supports the conclusion that domain-specific
class bias highly influences multi-domain learning.
5 Discussion and Open Questions
Our analysis of MDL algorithms revealed new
trends that suggest further avenues of exploration.
We suggest three open questions in response.
Question: When are MDL methods most effective?
Our empirical results suggest that MDL can be more
effective in settings with domain-specific class bi-
ases. However, we also saw differences in im-
provements for each method, and for different do-
mains. Differences emerge between the AMAZON
and CONVOTE datasets in terms of the ensemble
learning hypothesis. While there has been some the-
oretical analyses on the topic of MDL (Ben-David
et al 2007; Ben-David et al 2009; Mansour et
al., 2009; Daume? III et al 2010a), our results sug-
gest performing new analyses that relate ensemble
learning results with the MDL setting. These anal-
yses could provide insights into new algorithms that
can take advantage of the specific properties of each
multi-domain setting.
Question: What makes a good domain for MDL?
To the best of our knowledge, previous work has
assumed that domain identities are provided to the
learning algorithm. However, in reality, there may
be many ways to split a dataset into domains. For
example, consider the CONVOTE dataset, which we
split both by BILL and PARTY. The choice of splits
1310
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
86.06% 86.22% 76.42% 75.58% 69.31% 68.38%
FEDA
True Domain -0.25 -0.33 -0.83 +0.25 +0.88 +1.25
Random Domain -1.17 H -1.26 H -1.33 -0.82 -0.55 -0.04
MDR-L2
True Domain +0.39 N +0.23 -0.42 +0.42 -2.12 -1.19
Random Domain -0.38 -0.53 H -3.57 -2.73 -4.30 H -3.36 H
MDR-KL
True Domain +0.81 N +0.65 N -0.83 +0.00 +1.31 +2.25 N
Random Domain +0.22 +0.06 -1.90 -1.07 -0.60 +0.34
MTRL
True Domain -1.52 H -1.68 H -1.92 -1.08 +3.12 N +4.06 N
Random Domain -2.12 H -2.28 H -0.95 -0.12 +0.19 +1.12 N
Table 5: A comparison between MDL methods with data that have a consistent class bias across domains.
Similar to the setup where we evaluate the ensemble learning effect, we have a setting of using randomized
domains. N: Significantly better than the corresponding SVM or LR baseline, with p < 0.05, using a paired
t-test. H: Significantly worse than corresponding baseline, with p < 0.05, using a paired t-test.
MDL Method No Class Bias (Tab. 1) Varying Class Bias (Tab. 3) Consistent Class Bias (Tab. 5)
better worse equal better worse equal better worse equal
FEDA-SVM AM, BI, PA AM BI, PA AM, PA BI
FEDA-LR AM BI, PA AM BI, PA AM, BI PA
MDR-L2 AM BI, PA AM BI, PA AM, BI PA
MDR-KL PA AM, BI AM BI, PA AM, PA BI
MTRL AM BI, PA AM BI, PA AM, PA BI
DOM-ID-SVM ? ? ? AM BI, PA ? ? ?
DOM-ID-LR ? ? ? AM, BI PA ? ? ?
Table 6: The table shows the datasets (AM:AMAZON, BI:BILL, PA:PARTY) for which a given MDL method
using true domain information was significantly better, significantly worse, or not significantly different
(equal) as compared to using randomized domain information with the same MDL method.
impacted MDL. This poses new questions: what
makes a good domain? How should we choose to di-
vide data along possible metadata properties? If we
can gain improvements simply by randomly creat-
ing new domains (?Random Domain? setting in our
experiments) then there may be better ways to take
advantage of the provided metadata for MDL.
Question: Can we learn class-bias for
unsupervised domain adaptation?
Experiments with domain-specific class biases re-
vealed that a significant part of the improvements
could be achieved by adding domain-specific bias
features. Limiting the multi-domain improvements
to a small set of parameters raises an interesting
question: can these parameters be adapted to a new
domain without labeled data? Traditionally, domain
adaptation without target domain labeled data has
focused on learning the behavior of new features;
beliefs about existing feature behaviors could not be
corrected without new training data. However, by
collapsing the adaptation into a single bias parame-
ter, we may be able to learn how to adjust this pa-
rameter in a fully unsupervised way. This would
open the door to improvements in this challenging
setting for real world problems where class bias was
a significant factor.
Acknowledgments
Research presented here is supported by the Office
of Naval Research grant number N000141110221.
1311
References
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2008. Exploiting Feature Hierarchy for Transfer
Learning in Named Entity Recognition. In Proceed-
ings of ACL-08: HLT, pages 245?253.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations for
domain adaptation. In Proceedings of NIPS 2006.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 440?447.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman. 2008. Learning
Bounds for Domain Adaptation. In Advances in Neu-
ral Information Processing Systems (NIPS 2007).
Giovanni Cavallanti, Nicolo` Cesa-Bianchi, and Claudio
Gentile. 2008. Linear Algorithms for Online Multi-
task Classification. In Proceedings of COLT.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
Maximum Entropy Capitalizer: Little Data Can Help
a Lot. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 285?292.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2008. Exact convex confidence-weighted learning. In
Advances in Neural Information Processing Systems
(NIPS).
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010a. A Co-regularization Based Semi-supervised
Domain Adaptation. In Neural Information Process-
ing Systems.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010b. Frustratingly Easy Semi-Supervised Domain
Adaptation. In Proceedings of the ACL 2010 Work-
shop on Domain Adaptation for Natural Language
Processing, pages 53?59.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263.
Hal Daume? III. 2009. Bayesian multitask learning with
latent hierarchies. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence.
Thomas G. Dietterich. 2000. An experimental compar-
ison of three methods for constructing ensembles of
decision trees: Bagging, boosting, and randomization.
Machine Learning, 40:139?157.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ?08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ?08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1-2).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR : A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Jenny R. Finkel and Christopher D. Manning. 2009. Hi-
erarchical Bayesian Domain Adaptation. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
602?610.
Richard Maclin and David Opitz. 1999. Popular Ensem-
ble Methods: An Empirical Study. Journal of Artifi-
cial Intelligence Research, 11:169?198.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. Domain Adaptation with Multiple
Sources. In Proceedings of NIPS 2008, pages 1041?
1048.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Conference on Natural Language
Learning (Shared Task).
Avishek Saha, Piyush Rai, Hal Daume? III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
1312
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 216?224,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Contextual Information Improves OOV Detection in Speech
Carolina Parada, Mark Dredze
HLTCOE
Johns Hopkins University
3400 North Charles Street,
Baltimore MD 21210, USA
carolinap@jhu.edu
mdredze@cs.jhu.edu
Denis Filimonov
HLTCOE
University of Maryland,
College Park, MD 20742 USA
den@cs.umd.edu
Frederick Jelinek
HLTCOE
Johns Hopkins University
3400 North Charles Street,
Baltimore MD 21210, USA
jelinek@jhu.edu
Abstract
Out-of-vocabulary (OOV) words represent an
important source of error in large vocabulary
continuous speech recognition (LVCSR) sys-
tems. These words cause recognition failures,
which propagate through pipeline systems im-
pacting the performance of downstream ap-
plications. The detection of OOV regions in
the output of a LVCSR system is typically ad-
dressed as a binary classification task, where
each region is independently classified using
local information. In this paper, we show that
jointly predicting OOV regions, and includ-
ing contextual information from each region,
leads to substantial improvement in OOV de-
tection. Compared to the state-of-the-art, we
reduce the missed OOV rate from 42.6% to
28.4% at 10% false alarm rate.
1 Introduction
Even with a vocabulary of one hundred thou-
sand words, a large vocabulary continuous speech
recognition (LVCSR) system encounters out-of-
vocabulary (OOV) words, especially in new do-
mains or genres. New words often include named
entities, foreign words, rare and invented words.
Since these words were not seen during training, the
LVCSR system has no way to recognize them.
OOV words are an important source of error in
LVCSR systems for three reasons. First, OOVs can
never be recognized by the LVCSR system, even if
repeated. Second, OOV words contribute to recog-
nition errors in surrounding words, which propagate
into to later processing stages (translation, under-
standing, document retrieval, etc.). Third, OOVs
are often information-rich nouns ? mis-recognized
OOVs can have a greater impact on the understand-
ing of the transcript than other words.
One solution is to simply increase the LVCSR
system?s vocabulary, but there are always new
words. Additionally, increasing the vocabulary size
without limit can sometimes produce higher word
error rates (WER), leading to a tradeoff between
recognition accuracy of frequent and rare words.
A more effective solution is to detect the presence
of OOVs directly. Once identified, OOVs can be
flagged for annotation and addition to the system?s
vocabulary, or OOV segments can be transcribed
with a phone recognizer, creating an open vocabu-
lary LVCSR system. Identified OOVs prevent error
propagation in the application pipeline.
In the literature, there are two basic approaches
to OOV detection: 1) filler models, which explicitly
represent OOVs using a filler, sub-word, or generic
word model (Bazzi, 2002; Schaaf, 2001; Bisani and
Ney, 2005; Klakow et al, 1999; Wang, 2009); and
2) confidence estimation models, which use differ-
ent confidence scores to find unreliable regions and
label them as OOV (Lin et al, 2007; Burget et al,
2008; Sun et al, 2001; Wessel et al, 2001).
Recently, Rastrow et al (2009a) presented an ap-
proach that combined confidence estimation models
and filler models to improve state-of-the-art results
for OOV detection. This approach and other confi-
dence based systems (Hazen and Bazzi, 2001; Lin
et al, 2007), treat OOV detection as a binary clas-
sification task; each region is independently classi-
fied using local information as IV or OOV. This
work moves beyond this independence assumption
216
that considers regions independently for OOV de-
tection. We treat OOV detection as a sequence la-
beling problem and add features based on the local
lexical context of each region as well as global fea-
tures from a language model using the entire utter-
ance. Our results show that such information im-
proves OOV detection and we obtain large reduc-
tions in error compared to the best previously re-
ported results. Furthermore, our approach can be
combined with any confidence based system.
We begin by reviewing the current state-of-the-art
results for OOV detection. After describing our ex-
perimental setup, we generalize the framework to a
sequence labeling problem, which includes features
from the local context, lexical context, and entire ut-
terance. Each stage yields additional improvements
over the baseline system. We conclude with a review
of related work.
2 Maximum Entropy OOV Detection
Our baseline system is the Maximum Entropy model
with features from filler and confidence estimation
models proposed by Rastrow et al (2009a). Based
on filler models, this approach models OOVs by
constructing a hybrid system which combines words
and sub-word units. Sub-word units, or fragments,
are variable length phone sequences selected using
statistical methods (Siohan and Bacchiani, 2005).
The vocabulary contains a word and a fragment lex-
icon; fragments are used to represent OOVs in the
language model text. Language model training text
is obtained by replacing low frequency words (as-
sumed OOVs) by their fragment representation. Pro-
nunciations for OOVs are obtained using grapheme
to phoneme models (Chen, 2003).
This approach also includes properties from con-
fidence estimation systems. Using a hybrid LVCSR
system, they obtain confusion networks (Mangu et
al., 1999), compact representations of the recog-
nizer?s most likely hypotheses. For an utterance,
the confusion network is composed of a sequence
of confused regions, indicating the set of most likely
word/sub-word hypotheses uttered and their poste-
rior probabilities1 in a specific time interval.
1P (wi|A): posterior probability of word i given the acous-
tics, which includes the language model and acoustic model
scores, as described in (Mangu et al, 1999).
Figure 1 depicts a confusion network decoded by
the hybrid system for a section of an utterance in our
test-set. Below the network we present the reference
transcription. In this example, two OOVs were ut-
tered: ?slobodan? and ?milosevic? and decoded as
four and three in-vocabulary words, respectively. A
confused region (also called ?bin?) corresponds to
a set of competing hypothesis between two nodes.
The goal is to correctly label each of the ?bins? as
OOV or IV. Note the presence of both fragments
(e.g. s l ow, l aa s) and words in some of the
hypothesis bins.
For any bin of the confusion network, Rastrow et
al. combine features from that region using a binary
Maximum Entropy classifier (White et al, 2007).
Their most effective features were:
Fragment-Posterior =
?
f?tj
p(f |tj)
Word-Entropy = ?
?
w?tj
p(w|tj) log p(w|tj)
tj is the current bin in the confusion network and f
is a fragment in the hybrid dictionary.
We obtained confusion networks for a standard
word based system and the hybrid system described
above. We re-implemented the above features, ob-
taining nearly identical results to Rastrow et al us-
ing Mallet?s MaxEnt classifier (McCallum, 2002). 2
All real-valued features were normalized and quan-
tized using the uniform-occupancy partitioning de-
scribed in White et al (2007).3 The MaxEnt model
is regularized using a Gaussian prior (?2 = 100),
but we found results generally insensitive to ?.
3 Experimental Setup
Before we introduce and evaluate our context ap-
proach, we establish an experimental setup. We used
the dataset constructed by Can et al (2009) to eval-
uate Spoken Term Detection (STD) of OOVs; we
refer to this corpus as OOVCORP. The corpus con-
tains 100 hours of transcribed Broadcast News En-
glish speech emphasizing OOVs. There are 1290
unique OOVs in the corpus, which were selected
with a minimum of 5 acoustic instances per word.
2Small differences are due to a change in MaxEnt library.
3All experiments use 50 partitions with a minimum of 100
training values per partition.
217
Figure 1: Example confusion network from the hybrid system with OOV regions and BIO encoding. Hypothesis are
ordered by decreasing value of posterior probability. Best hypothesis is the concatenation of the top word/fragments
in each bin. We omit posterior probabilities due to spacing.
Common English words were filtered out to ob-
tain meaningful OOVs: e.g. NATALIE, PUTIN,
QAEDA, HOLLOWAY. Since the corpus was de-
signed for STD, short OOVs (less than 4 phones)
were explicitly excluded. This resulted in roughly
24K (2%) OOV tokens.
For a LVCSR system we used the IBM Speech
Recognition Toolkit (Soltau et al, 2005)4 with
acoustic models trained on 300 hours of HUB4 data
(Fiscus et al, 1998) and excluded utterances con-
taining OOV words as marked in OOVCORP. The lan-
guage model was trained on 400M words from var-
ious text sources with a 83K word vocabulary. The
LVCSR system?s WER on the standard RT04 BN
test set was 19.4%. Excluded utterances were di-
vided into 5 hours of training and 95 hours of test
data for the OOV detector. Both train and test sets
have a 2% OOV rate. We used this split for all exper-
iments. Note that the OOV training set is different
from the LVCSR training set.
In addition to a word-based LVCSR system, we
use a hybrid LVCSR system, combining word and
sub-word (fragments) units. Combined word/sub-
word systems have improved OOV Spoken Term
Detection performance (Mamou et al, 2007; Parada
et al, 2009), better phone error rates, especially in
OOV regions (Rastrow et al, 2009b), and state-of-
the-art performance for OOV detection. Our hybrid
system?s lexicon has 83K words and 20K fragments
derived using Rastrow et al (2009a). The 1290 ex-
cluded words are OOVs to both the word and hybrid
4We use the IBM system with speaker adaptive training
based on maximum likelihood with no discriminative training.
systems.
Note that our experiments use a different dataset
than Rastrow et. al., but we have a larger vocabu-
lary (83K vs 20K), which is closer to most modern
LVCSR system vocabularies; the resulting OOVs
are more challenging but more realistic.
3.1 Evaluation
Confusion networks are obtained from both the
word and hybrid LVCSR systems. In order to eval-
uate the performance of the OOV detector, we align
the reference transcript to the audio. The LVCSR
transcript is compared to the reference transcript at
the confused region level, so each confused region
is tagged as either OOV or IV. The OOV detector
assigns a score/probability for IV/OOV to each of
these regions.
Previous research reported OOV detection accu-
racy on all test data. However, once an OOV word
has been observed in the training data for the OOV
detector, even if it never appeared in the LVCSR
training data, it is no longer truly OOV. The fea-
tures used in previous approaches did not necessar-
ily provide an advantage on observed versus unob-
served OOVs, but our features do yield an advan-
tage. Therefore, in the sections that follow we re-
port unobserved OOV accuracy: OOV words that
do not appear in either the OOV detector?s or the
LVCSR?s training data. While this penalizes our re-
sults, it is a more informative metric of true system
performance.
We present results using standard detection error
tradeoff (DET) curves (Martin et al, 1997). DET
218
curves measure tradeoffs between misses and false
alarms and can be used to determine the optimal op-
erating point of a system. The x-axis varies the false
alarm rate (false positive) and the y-axis varies the
miss (false negative) rate; lower curves are better.
4 From MaxEnt to CRFs
As a classification algorithm, Maximum Entropy as-
signs a label to each region independently. However,
OOV words tend to be recognized as two or more IV
words, hence OOV regions tend to co-occur. In the
example of Figure 1, the OOV word ?slobodan? was
recognized as four IV words: ?slow vote i mean?.
This suggests that sequence models, which jointly
assign all labels in a sequence, may be more appro-
priate. Therefore, we begin incorporating context by
moving from classification to sequence models.
MaxEnt classification models the target label as
p(yi|xi), where yi is a discrete variable representing
the ith label (?IV? or ?OOV?) and xi is a feature
vector representing information for position i. The
conditional distribution for yi takes the form
p(yi|xi) =
1
Z(xi)
exp(
K?
k=1
?kfk(yi,xi)) ,
Z(xi) is a normalization term and f(yi,xi) is a vec-
tor ofK features, such as those defined in Section 2.
The model is trained discriminatively: parameters ?
are chosen to maximize conditional data likelihood.
Conditional Random Fields (CRF) (Lafferty et
al., 2001) generalize MaxEnt models to sequence
tasks. While having the same model structure as
Hidden Markov Models (HMMs), CRFs are trained
discriminatively and can use large numbers of corre-
lated features. Their primary advantage over Max-
Ent models is their ability to find an optimal labeling
for the entire sequence rather than greedy local deci-
sions. CRFs have been used successfully used in nu-
merous text processing tasks and while less popular
in speech, still applied successfully, such as sentence
boundary detection (Liu et al, 2005).
A CRF models the entire label sequence y as:
p(y|x) =
1
Z(x)
exp(?F (y,x)) ,
where F (y,x) is a global feature vector for input
sequence x and label sequence y and Z(x) is a nor-
malization term.5
5 Context for OOV Detection
We begin by including a minimal amount of local
context in making OOV decisions: the predicted la-
bels for adjacent confused regions (bins). This infor-
mation helps when OOV bins occur in close proxim-
ity, such as successive OOV bins. This is indeed the
case: in the OOV detector training data only 48% of
OOV sequences contained a single bin; sequences
were of length 2 (40%), 3 (9%) and 4 (2%). We
found similar results in the test data. Therefore, we
expect that even a minimal amount of context based
on the labels of adjacent bins will help.
A natural way of incorporating contextual infor-
mation is through a CRF, which introduces depen-
dencies between each label and its neighbors. If a
neighboring bin is likely an OOV, it increases the
chance that the current bin is OOV.
In sequence models, another technique for cap-
turing contextual dependence is the label encoding
scheme. In information extraction, where sequences
of adjacent tokens are likely to receive the same
tag, the beginning of each sequence receives a dif-
ferent tag from words that continue the sequence.
For example, the first token in a person name is
labeled B-PER and all subsequent tokens are la-
beled I-PER. This is commonly referred to as BIO
encoding (beginning, inside, outside). We applied
this encoding technique to our task, labeling bins
as either IV (in vocabulary), B-OOV (begin OOV)
and I-OOV (inside OOV), as illustrated in Figure 1.
This encoding allows the algorithm to identify fea-
tures which might be more indicative of the begin-
ning of an OOV sequence. We found that this en-
coding achieved a superior performance to a simple
IV/OOV encoding. We therefore utilize the BIO en-
coding in all CRF experiments.
Another means of introducing context is through
the order of the CRF model. A first order model
(n = 1) adds dependencies only between neighbor-
ing labels, whereas an n order model creates depen-
dencies between labels up to a distance of n posi-
tions. Higher order models capture length of label
5CRF experiments used the CRF++ package
http://crfpp.sourceforge.net/
219
regions (up to length n). We experiment with both
a first order and a second order CRF. Higher order
models did not provide any improvements.
In order to establish a comparative baseline, we
first present results using the same features from
the system described in Section 2 (Word-Entropy
and Fragment-Posterior). All real-valued features
were normalized and quantized using the uniform-
occupancy partitioning described in White et al
(2007).6 Quantization of real valued features is stan-
dard for log-linear models as it allows the model to
take advantage of non-linear characteristics of fea-
ture values and is better handled by the regulariza-
tion term. As in White et. al. we found it improved
performance.
Figure 2 depicts DET curves for OOV detection
for the MaxEnt baseline and first and second order
CRFs with BIO encoding on unobserved OOVs in
the test data. We generated predictions at different
false alarm rates by varying a probability threshold.
For MaxEnt we used the predicted label probability
and for CRFs the marginal probability of each bin?s
label. While the first order CRF achieves nearly
identical performance to the MaxEnt baseline, the
second order CRF shows a clear improvement. The
second order model has a 5% absolute improvement
at 10% false alarm rate, despite using the identi-
cal features as the MaxEnt baseline. Even a small
amount of context as expressed through local label-
ing decisions improves OOV detection.
The quantization of the features yields quan-
tized prediction scores, resulting in the non-smooth
curves for the MaxEnt and 1st order CRF results.
However, when using a second order CRF the OOV
score varies more smoothly since more features
(context labels) are considered in the prediction of
the current label.
6 Local Lexical Context
A popular approach in sequence tagging, such as in-
formation extraction or part of speech tagging, is to
include features based on local lexical content and
context. In detecting a name, both the lexical form
?John? and the preceding lexical context ?Mr.? pro-
vide clues that ?John? is a name. While we do not
6All experiments use 50 partitions with a minimum of 100
training values per partition.
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
MaxEnt (Baseline)CRF (First Order)CRF (Second Order)
Figure 2: DET curves for OOV detection using a Max-
imum Entropy (MaxEnt) classifier and contextual infor-
mation using a 1st order and 2nd order CRF. All models
use the same baseline features (Section 2).
know the actual lexical items in the speech sequence,
the speech recognizer output can be used as a best
guess. In the example of Figure 1, the words ?for-
mer president? are good indicators that the following
word is either the word ?of? or a name, and hence a
potential OOV. Combining this lexical context with
hypothesized words can help label the subsequent
regions as OOVs (note that none of the hypothesized
words in the third bin are ?of?, names, or nouns).
Words from the LVCSR decoding of the sentence
are used in the CRF OOV detector. For each bin in
the confusion network, we select the word with the
highest probability (best hypothesis). We then add
the best hypothesis word as a feature of the form:
current word=X. These features capture how the
LVCSR system incorrectly recognizes OOV words.
However, since detection is measured on unobserved
OOVs, these features alone may not help.
Instead, we turn to lexical context, which includes
correctly recognized IV words. We evaluate the fol-
lowing sets of features derived from lexical context:
? Current bin?s best hypothesis. (Current-Word)
? Unigrams and bigrams from the best hypoth-
esis in a window of 5 words around current
bin. This feature ignores the best hypothesis in
the current bin, i.e., word[-2],word[-1]
is included, but word[-1],word[0] is not.
(Context-Bigrams)
220
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
CRF (Second Order)+Current-Word+Context-Bigrams+Current-Trigrams+All-Words+All-Words-Stemmed
Figure 3: A second order CRF (Section 5) and additional
features including including word identities from current
and neighboring bins (Section 6).
? Unigrams, bigrams, and trigrams in a window
of 5 words around and including current bin.
(Current-Trigrams)
? All of the above features. (All-Words)
? All above features and their stems.7 (All-
Words-Stemmed)
We added these features to the second order CRF
with BIO encoding and baseline features (Figure 3).
As expected, the current words did not improve per-
formance on unobserved OOVs. When the current
words are combined with the lexical context and
their lemmas, they give a significant boost in perfor-
mance: a 4.2% absolute improvement at 10% false
alarm rate over the previous CRF system, and 9.3%
over the MaxEnt baseline. Interestingly, only com-
bining context and current word gives a substantial
gain. This indicates that OOVs tend to occur with
certain distributional characteristics that are inde-
pendent of the OOV word uttered (since we consider
only unobserved OOVs), perhaps because OOVs
tend to be named entities, foreign words, or rare
nouns. The importance of distributional features is
well known for named entity recognition and part
of speech tagging (Pereira et al, 1993). Other fea-
tures such as sub-strings or baseline features (Word-
7To obtain stemmed words, we use the CPAN package:
http://search.cpan.org/~snowhare/Lingua-Stem-0.83.
Entropy, Fragment-Posterior) from neighboring bins
did not provide further improvement.
7 Global Utterance Context
We now include features that incorporate informa-
tion from the entire utterance. The probability of an
utterance as computed by a language model is of-
ten used as a measure of fluency of the utterance.
We also observe that OOV words tend to take very
specific syntactic roles (more than half of them are
proper nouns), which means the surrounding context
will have predictive lexical and syntactic properties.
Therefore, we use a syntactic language model.
7.1 Language Models
We evaluated both a standard trigram language
model and a syntactic language model (Filimonov
and Harper, 2009a). The syntactic model estimates
the joint probability of the word and its syntactic tag
based on the preceding words and tags. The proba-
bility of an utterance wn1 of length n is computed by
summing over all latent syntactic tag assignments:
p(utt) = p(wn1 ) =
?
t1...tn
n?
i?1
p(wi, ti|w
i?1
1 , t
i?1
1 )
(1)
where wi and ti are the word and tag at posi-
tion i, and wi?11 and t
i?1
1 are sequences of words
and tags of length i ? 1 starting a position 1.
The model is restricted to a trigram context, i.e.,
p(wi, ti|w
i?1
i?2, t
i?1
i?2); experiments that increased the
order yielded no improvement.
We trained the language model on 130 million
words from Hub4 CSR 1996 (Garofolo et al, 1996).
The corpus was parsed using a modified Berkeley
parser (Huang and Harper, 2009) and tags extracted
from parse trees incorporated the word?s POS, the
label of its immediate parent, and the relative posi-
tion of the word among its siblings. 8 The parser
required separated contractions and possessives, but
we recombined those words after parsing to match
the LVCSR tokenization, merging their tags. Since
we are considering OOV detection, the language
model was restricted to LVCSR system?s vocabu-
lary.
8The parent tagset of Filimonov and Harper (2009a).
221
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
All-Words-Lemmas+3gram-LM+Syntactic-LM+Syntactic-LM+Tags
Figure 4: Features from a language model added to the
best CRF from Section 6 (All-Words-Stemmed).
We also used the standard trigram LM for refer-
ence. It was trained on the same data and with the
same vocabulary using the SRILM toolkit. We used
interpolated modified KN discounting.
7.2 Language Model Features
We designed features based on the entire utterance
using the language model to measure how the utter-
ance is effected by the current token: whether the
utterance is more likely given the recognized word
or some OOV word.
Likelihood-ratio = log
p(utt)
p(utt|wi = unknown)
Norm-LM-score =
log p(utt)
length(utt)
where p(utt) represents the probability of the ut-
terance using the best path hypothesis word of the
LVCSR system, and p(utt|wi = unknown) is the
probability of the entire utterance with the current
word in the LVCSR output replaced by the token
<unk>, used to represent OOVs. Intuitively, when
an OOV word is recognized as an IV word, the flu-
ency of the utterance is disrupted, especially if the
IV is a function word. The Likelihood-ratio is de-
signed to show whether the utterance is more fluent
(more likely) if the current word is a misrecognized
OOV. 9 The second feature (Norm-LM-score) is the
9Note that in the standard n-gram LM the feature reduces to
log
Qi+n?1
k=i p(wk|w
k?1
k?n+1)
Qi+n?1
k=i p(wk|w
k?1
k?n+1,wi=unknown)
, i.e., only n n-grams actu-
0 5 10 15 20 25 30 35 40P(FA)0
10
20
30
40
50
60
70
80
P(M
iss)
MaxEnt (Baseline)CRF All FeaturesCRF All Features (Unobserved)CRF All Features (Observed)
Figure 5: A CRF with all context features compared to
the state-of-the-art MaxEnt baseline. Results for the CRF
are shown for unobserved, observed and both OOVs.
normalized likelihood of the utterance. An unlikely
utterance biases the system to predicting OOVs.
We evaluated a CRF with these features and
all lexical context features (Section 6) using both
the trigram model and the joint syntactic language
model (Figure 4). Each model improved perfor-
mance, but the syntactic model provided the largest
improvement. At 10% false alarm rate it yields a
4% absolute improvement with respect to the pre-
vious best result (All-Words-Stemmed) and 13.3%
over the MaxEnt baseline. Higher order language
models did not improve.
7.3 Additional Syntactic Features
We explored other syntactic features; the most ef-
fective was the 5-tag window of POS tags of the
best hypothesis.10 The additive improvement of this
feature is depicted in Figure 4 labeled ?+Syntactic-
LM+Tags.? With this feature, we achieve a small ad-
ditional gain. We tried other syntactic features with-
out added benefit, such as the most likely POS tag
for <unk>in the utterance.
ally contribute. However, in the syntactic LM, the entire utter-
ance is affected by the change of one word through the latent
states (tags) (Eq. 1), thus making it a truly global feature.
10The POS tags were generated by the same syntactic LM
(see Section 7.1) as described in (Filimonov and Harper,
2009b). In this case, POS tags include merged tags, i.e., the vo-
cabulary word fred?s may be tagged as NNP-POS or NNP-VBZ.
222
8 Final System
Figure 5 summarizes all of the context features in a
single second order BIO encoded CRF. Results are
shown for state-of-the-art MaxEnt (Rastrow et al,
2009a) as well as for the CRF on unobserved, ob-
served and combined OOVs. For unobserved OOVs
our final system achieves a 14.2% absolute improve-
ment at 10% FA rate. The absolute improvement
on all OOVs was 23.7%. This result includes ob-
served OOVs: words that are OOV for the LVCSR
but are encountered in the OOV detector?s training
data. MaxEnt achieved similar performance for ob-
served and unobserved OOVs so we only include a
single combined result.
Note that the MaxEnt curve flattens at 26% false
alarms, while the CRF continues to decrease. The
elbow in the MaxEnt curve corresponds to the prob-
ability threshold at which no other labeled OOV re-
gion has a non-zero OOV score (regions with zero
entropy and no fragments). In this case, the CRF
model can still rely on the context to predict a non-
zero OOV score. This helps applications where
misses are more heavily penalized than false alarms.
9 Related Work
Most approaches to OOV detection in speech can
be categorized as filler models or confidence esti-
mation models. Filler models vary in three dimen-
sions: 1) The type of filler units used: variable-
length phoneme units (as the baseline system) vs
joint letter sound sub-words; 2) Method used to de-
rive units: data-driven (Bazzi and Glass, 2001) or
linguistically motivated (Choueiter, 2009); 3) The
method for incorporating the LVCSR system: hi-
erarchical (Bazzi, 2002) or flat models (Bisani and
Ney, 2005). Our approach can be integrated with
any of these systems.
We have shown that combining the presence of
sub-word units with other measures of confidence
can provided significant improvements, and other
proposed local confidence measures could be in-
cluded in our system as well. Lin et al (2007)
uses joint word/phone lattice alignments and clas-
sifies high local miss-alignment regions as OOVs.
Hazen and Bazzi (2001) combines filler models with
word confidence scores, such as the minimum nor-
malized log-likelihood acoustic model score for a
word and, the fraction of the N-best utterance hy-
potheses in which a hypothesized word appears.
Limited contextual information has been pre-
viously exploited (although maintaining indepen-
dence assumptions on the labels). Burget et al
(2008) used a neural-network (NN) phone-posterior
estimator as a feature for OOV detection. The
network is fed with posterior probabilities from
weakly-constrained (phonetic-based) and strongly-
constrained (word-based) recognizers. Their sys-
tem estimates frame-based scores, and interestingly,
they report large improvements when using tempo-
ral context in the NN input. This context is quite lim-
ited; it refers to posterior scores from one frame on
each side. Other features are considered and com-
bined using a MaxEnt model. They attribute this
gain to sampling from neighboring phonemes. Sun
et al (2001) combines a filler-based model with a
confidence approach by using several acoustic fea-
tures along with context based features, such as
whether the next word is a filler, acoustic confidence
features for next word, number of fillers, etc.
None of these approaches consider OOV detec-
tion as a sequence labeling problem. The work of
Liu et al (2005) is most similar to the approach pre-
sented here, but applies a CRF to sentence boundary
detection.
10 Conclusion and Future Work
We have presented a novel and effective approach to
improve OOV detection in the output confusion net-
works of a LVCSR system. Local and global con-
textual information is integrated with sub-word pos-
terior probabilities obtained from a hybrid LVCSR
system in a CRF to detect OOV regions effectively.
At a 10% FA rate, we reduce the missed OOV rate
from 42.6% to 28.4%, a 33.3% relative error reduc-
tion. Our future work will focus on additional fea-
tures from the recognizer aside from the single best-
hypothesis, as well as other applications of contex-
tual sequence prediction to speech tasks.
Acknowledgments
The authors thank Ariya Rastrow for providing the
baseline system code, Abhinav Sethy and Bhuvana
Ramabhadran for providing the data used in the ex-
periments and for many insightful discussions.
223
References
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Eurospeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
M. Bisani and H. Ney. 2005. Open vocabulary speech
recognition with flag hybrid models. In INTER-
SPEECH.
L. Burget, P. Schwarz, P. Matejka, M. Hannemann,
A. Rastrow, C. White, S. Khudanpur, H. Hermansky,
and J. Cernocky. 2008. Combination of strongly and
weakly constrained recognizers for reliable detection
of OOVS. In ICASSP.
Dogan Can, Erica Cooper, Abhinav Sethy, Chris White,
Bhuvana Ramabhadran, and Murat Saraclar. 2009.
Effect of pronounciations on OOV queries in spoken
term detection. ICASSP.
Stanley F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
G. Choueiter. 2009. Linguistically-motivated sub-
word modeling with applications to speech recogni-
tion. Ph.D. thesis, Massachusetts Institute of Technol-
ogy.
Denis Filimonov and Mary Harper. 2009a. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Denis Filimonov and Mary Harper. 2009b. Measuring
tagging performance of a joint language model. In
Proceedings of the Interspeech 2009.
Jonathan Fiscus, John Garofolo, Mark Przybocki,
William Fisher, and David Pallett, 1998. 1997 En-
glish Broadcast News Speech (HUB4). Linguistic
Data Consortium, Philadelphia.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Timothy J. Hazen and Issam Bazzi. 2001. A comparison
and combination of methods for OOV word detection
and word confidence scoring. In Proceedings of the
International Conference on Acoustics.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In EMNLP.
Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999.
OOV-detection in large vocabulary system using au-
tomatically defined word-fragments as fillers. In Eu-
rospeech.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Interna-
tional Conference on Machine Learning (ICML).
Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007.
OOV detection by joint word/phone lattice alignment.
In ASRU, pages 478?483, Dec.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Jonathan Mamou, Bhuvana Ramabhadran, and Olivier
Siohan. 2007. Vocabulary independent spoken term
detection. In SIGIR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words. In Eurospeech.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocky. 1997. The DET curve in assessment of
detection task performance. In Eurospeech.
Andrew McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.
umass.edu.
Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009. Query-by-example spoken term detec-
tion for OOV terms. In ASRU.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In ACL.
Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramabhad-
ran. 2009a. A new method for OOV detection using
hybrid word/fragment system. ICASSP.
Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,
and Fred Jelinek. 2009b. Towards using hybrid,
word, and fragment units for vocabulary independent
LVCSR systems. INTERSPEECH.
T. Schaaf. 2001. Detection of OOV words using gen-
eralized word models and a semantic class language
model. In Eurospeech.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary-
independent audio search using path-based graph in-
dexing. In INTERSPEECH.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The IBM 2004 conversational
telephony system for rich transcription. In ICASSP.
H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using
word confidence measure for OOV words detection in
a spontaneous spoken dialog system. In Eurospeech.
Stanley Wang. 2009. Using graphone models in au-
tomatic speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001.
Confidence measures for large vocabulary continuous
speech recognition. IEEE Transactions on Speech and
Audio Processing, 9(3).
Christopher White, Jasha Droppo, Alex Acero, and Ju-
lian Odell. 2007. Maximum entropy confidence esti-
mation for speech recognition. In ICASSP.
224
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 60?69,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Entity Clustering Across Languages
Spence Green*, Nicholas Andrews?, Matthew R. Gormley?,
Mark Dredze?, and Christopher D. Manning*
*Computer Science Department, Stanford University
{spenceg,manning}@stanford.edu
?Human Language Technology Center of Excellence, Johns Hopkins University
{noa,mrg,mdredze}@cs.jhu.edu
Abstract
Standard entity clustering systems commonly
rely on mention (string) matching, syntactic
features, and linguistic resources like English
WordNet. When co-referent text mentions ap-
pear in different languages, these techniques
cannot be easily applied. Consequently, we
develop new methods for clustering text men-
tions across documents and languages simulta-
neously, producing cross-lingual entity clusters.
Our approach extends standard clustering algo-
rithms with cross-lingual mention and context
similarity measures. Crucially, we do not as-
sume a pre-existing entity list (knowledge base),
so entity characteristics are unknown. On an
Arabic-English corpus that contains seven dif-
ferent text genres, our best model yields a 24.3%
F1 gain over the baseline.
1 Introduction
This paper introduces techniques for clustering co-
referent text mentions across documents and lan-
guages. On the web today, a breaking news item
may instantly result in mentions to a real-world entity
in multiple text formats: news articles, blog posts,
tweets, etc. Much NLP work has focused on model
adaptation to these diverse text genres. However, the
diversity of languages in which the mentions appear
is a more significant challenge. This was particularly
evident during the 2011 popular uprisings in the Arab
world, in which electronic media played a prominent
role. A key issue for the outside world was the aggre-
gation of information that appeared simultaneously
in English, French, and various Arabic dialects.
To our knowledge, we are the first to consider clus-
tering entity mentions across languages without a pri-
ori knowledge of the quantity or types of real-world
entities (a knowledge base). The cross-lingual set-
ting introduces several challenges. First, we cannot
assume a prototypical name format. For example,
the Anglo-centric first/middle/last prototype used in
previous name modeling work (cf. (Charniak, 2001))
does not apply to Arabic names like Abdullah ibn
Abd Al-Aziz Al-Saud or Chinese names like Hu Jin-
tao (referred to as Mr. Hu, not Mr. Jintao). Sec-
ond, organization names often require both translit-
eration and translation. For example, the Arabic
	PP?

K?? ?Q
	
g.

??Q?? ?General Motors Corp? contains
transliterations of 	PP?K?? ?Q
	
g. ?General Motors?,
but a translation of

??Q?? ?Corporation?.
Our models are organized as a pipeline. First, for
each document, we perform standard mention detec-
tion and coreference resolution. Then, we use pair-
wise cross-lingual similarity models to measure both
mention and context similarity. Finally, we cluster
the mentions based on similarity.
Our work makes the following contributions: (1)
introduction of the task, (2) novel models for cross-
lingual entity clustering of person and organization en-
tities, (3) cross-lingual annotation of the NIST Auto-
matic Content Extraction (ACE) 2008 Arabic-English
evaluation set, and (4) experimental results using both
gold and automatic within-document processing. We
will release our software and annotations to support
future research.
1.1 Task Description via a Simple Example
Consider the toy corpus in Fig. 1. The English docu-
ments contain mentions of two people: Steven Paul
Jobs and Mark Elliot Zuckerberg. Of course, the sur-
face realization of Mr. Jobs? last name in English is
also an ordinary nominal, hence the ambiguous men-
tion string (absent context) in the second document.
The Arabic document introduces an organization en-
tity (Apple Inc.) along with proper and pronominal
references to Mr. Jobs. Finally, the French document
refers to Mr. Jobs by the honorific ?Monsieur,? and to
60
Jobs program details delayed
Steve Jobs admired Mark Zuckerberg
M. Jobs, le fondateur d'Apple, est mort
	
		
?
=
? E1
E2
E3
=
=
doc1:
doc2:
doc3:
doc4:
Figure 1: Clustering entity mentions across languages and documents. The toy corpus contains English (doc1 and
doc2), Arabic (doc3), and French (doc4). Together, the documents make reference to three real-world entities, the
identification of which is the primary objective of this work. We use a separately-trained system for within-document
mention detection and coreference (indicated by the text boxes and intra-document links, respectively). Our experimental
results are for Arabic-English only.
Apple without its corporate designation.
Our goal is to automatically produce the cross-
lingual entity clusters E1 (Mark Elliot Zuckerberg),
E2 (Apple Inc.), and E3 (Steven Paul Jobs). Both the
true number and characteristics of these entities are
unobserved. Our models require two pre-processing
steps: mention detection and within-document coref-
erence/anaphora resolution, shown in Fig. 1 by the
text boxes and intra-document links, respectively. For
example, in doc3, a within-document coreference sys-
tem would pre-link 	QK. ?k. joobz ?Jobs? with the mascu-
line pronoun ? h ?his?. In addition, the mention detec-
tor determines that the surface form ?Jobs? in doc2
is not an entity reference. For this within-document
pre-processing we use Serif (Ramshaw et al, 2011).1
Our models measure cross-lingual similarity of the
coreference chains to make clustering decisions (?
in Fig. 1). The similarity models (indicated by the
= and 6= operators in Fig. 1) consider both mention
string and context similarity (?2). We use the men-
tion similarities as hard constraints, and the context
similarities as soft constraints. In this work, we inves-
tigate two standard constrained clustering algorithms
(?3). Our methods can be used to extend existing sys-
tems for mono-lingual entity clustering (also known
as ?cross-document coreference resolution?) to the
cross-lingual setting.
1Serif is a commercial system that assumes each document
contains only one language. Currently, there are no publicly avail-
able within-document coreference systems for Arabic and many
other languages. To remedy this problem, the CoNNL-2012
shared task aims to develop multilingual coreference systems.
2 Mention and Context Similarity
Our goal is to create cross-lingual sets of co-referent
mentions to real-world entities (people, places, orga-
nizations, etc.). In this paper, we adopt the following
notation. LetM be a set of distinct text mentions in a
collection of documents;C is a partitioning ofM into
document-level sets of co-referent mentions (called
coreference chains); E is a partitioning of C into sets
of co-referent chains (called entities). Let i, j be non-
negative integers less than or equal to |M | and a, b be
non-negative integers less than or equal to |C|. Our
experiments use a separate within-document corefer-
ence system to createC, which is fixed. We will learn
E, which has size no greater than |C| since the set of
mono-lingual chains is the largest valid partitioning.
We define accessor functions to access properties
of mentions and chains. For any mentionmi, define
the following functions: lang(mi) is the language;
doc(mi) is the document containingmi; type(mi) is
the semantic type, which is assigned by the within-
document coreference system. We also extract a set
of mention contexts S, which are the sentences con-
taining each mention (i.e., |S| = |M |).
We learn the partition E by considering mention
and context similarity, which are measured with sep-
arate component models.
2.1 Mention Similarity
We use separate methods for within- and cross-
language mention similarity. The pairwise similarity
61
Arabic Rules
H. ? b
H? t H? th h. ? j
h? h p? kh X? d
	
X? th
P? r 	P? z ?? s ?? sh
?? s 	?? d ?? t 	?? th
?? a
	
?? g
	
?? f

?? q
?? k ?? l ?? m 	?? n
?? h @? a ?? w ?? a

?? ah ?


? ? Z? ?
English Rules
k? c p? b x? ks e,i,o,u? ?
Table 1: English-Arabic mapping rules to a common or-
thographic representation. ??? indicates a null mapping.
For English, we also lowercase and remove determiners
and punctuation. For Arabic, we remove the determiner
?@ Al ?the? and the elongation character tatwil ??.
of any two mentionsmi andmj is:
sim(mi,mj) =
{
jaro-winkler(mi,mj) if lang(mi) = lang(mj)
maxent(mi,mj) otherwise
Jaro-Winkler Distance (within-language) If
lang(mi) = lang(mj), we use the Jaro-Winkler edit
distance (Porter and Winkler, 1997). Jaro-Winkler
rewards matching prefixes, the empirical justification
being that less variation typically occurs at the
beginning of names.2 The metric produces a score in
the range [0,1], where 0 indicates equality.
Maxent model (cross-language) When lang(mi)
6= lang(mj), then the two mentions might be in dif-
ferent writing systems. Edit distance calculations
no longer apply directly. One solution would be
full-blown transliteration (Knight and Graehl, 1998),
followed by application of Jaro-Winkler. However,
transliteration systems are complex and require sig-
nificant training resources. We find that a simpler,
low-resource approach works well in practice.
First, we deterministically map both languages to a
common phonetic representation (Tbl. 1).3 Next, we
align the mention pairs with the Hungarian algorithm,
2For multi-token names, we sort the tokens prior to computing
the score, as suggested by Christen (2006).
3This idea is reminiscent of Soundex, which Freeman et al
(2006) used for cross-lingual name matching.
Overlap Active for each bigram in
cbigrams(mi,u)
?
cbigrams(mj,v)
Bigram-Diff-mi Active for each bigram in
cbigrams(mi)? cbigrams(mj)
Bigram-Diff-mj Active for each bigram in
cbigrams(mj)? cbigrams(mi)
Bigram-Len-Diff Value of abs(size(cbigrams(mi)?
cbigrams(mj)))
Big-Edit-Dist Count of token pairs with
Lev(mi,u,mj,v) > 3.0
Total-Edit-Dist Sum of aligned token edit distances
Length Active for one of:
len(mi) > len(mj) or
len(mi) < len(mj) or
len(mi) = len(mj)
Length-Diff abs(len(mi)? len(mj))
Singleton Active if len(mi) = 1
Singleton-Pair Active if len(mi) = len(mj) = 1
Table 2: Cross-language Maxent feature templates for a
whitespace-tokenized mention pair ?mi,mj? with align-
ment Ami,mj . Let (u, v) ? Ami,mj indicate aligned to-
ken indices. Define the following functions for strings:
cbigrams(?) returns the set of character bigrams; len(?) is
the token length; Lev(?, ?) is the Levenshtein edit distance
between two strings. Prior to feature extraction, we add
unique start and end symbols to the mention strings.
which produces a word-to-word alignment Ami,mj .
4
Finally, we build a simple binary Maxent classifier
p(y|mi,mj ;?) that extracts features from the aligned
mentions (Tbl. 2). We learn the parameters ? using a
quasi-Newton procedure with L1 (lasso) regulariza-
tion (Andrew and Gao, 2007).
2.2 Context Mapping and Similarity
Mention strings alone are not always sufficient for
disambiguation. Consider again the simple exam-
ple in Fig. 1. Both doc3 and doc4 reference ?Steve
Jobs? and ?Apple? in the same contexts. Context co-
occurence and/or similarity can thus disambiguate
these two entities from other entities with similar ref-
erences (e.g., ?Steve Jones? or ?Apple Corps?). As
with the mention strings, the contexts may originate
in different writing systems. We consider both high-
and low-resource approaches for mapping contexts to
a common representation.
4The Hungarian algorithm finds an optimal minimum-cost
alignment. For pairwise costs between tokens, we used the Lev-
enshtein edit distance
62
Machine Translation (MT) For the high-resource
setting, if lang(mi) 6=English, then we translate both
mi and its context si to English with an MT system.
We use Phrasal (Cer et al, 2010), a phrase-based
system which, like most public MT systems, lacks a
transliteration module. We believe that this approach
yields the most accurate context mapping for high-
resource language pairs (like English-Arabic).
Polylingual Topic Model (PLTM) The polylin-
gual topic model (PLTM) (Mimno et al, 2009) is
a generative process in which document tuples?
groups of topically-similar documents?share a topic
distribution. The tuples need not be sentence-aligned,
so training data is easier to obtain. For example, one
document tuple might be the set of Wikipedia articles
(in all languages) for Steve Jobs.
Let D be a set of document tuples, where
there is one document in each tuple for each
of L languages. Each language has vocabu-
lary Vl and each document dlt has N
l
t tokens.
We specify a fixed-size set of topics K. The
PLTM generates the document tuples as follows:
Polylingual Topic Model
?t ? Dir(?K) [cross-lingual tuple-topic prior]
?lk ? Dir(?
Vl) [word-topic prior]
for each token wlt,n with n = {1, . . . , N
l
t}:
zt,n ? Mult(?t)
wlt,n ? Mult(?
l
zt,n)
For cross-lingual context mapping, we infer the 1-
best topic assignments for each token in all S mention
contexts. This technique reduces Vl = k for all l.
Moreover, all languages have a common vocabulary:
the set of K topic indices. Since the PLTM is not
a contribution of this paper, we refer the interested
reader to (Mimno et al, 2009) for more details.
After mapping each mention context to a common
representation, we measure context similarity based
on the choice of clustering algorithm.
3 Clustering Algorithms
We incorporate the mention and context similarity
measures into a clustering framework. We consider
two algorithms. The first is hierarchical agglomera-
tive clustering (HAC), with which we assume basic
familiarity (Manning et al, 2008). A shortcoming of
HAC is that a stop threshold must be tuned. To avoid
this requirement, we also consider non-parametric
probabilistic clustering in the form of a Dirichlet pro-
cess mixture model (DPMM) (Antoniak, 1974) .
Both clustering algorithms can be modified to ac-
commodate pairwise constraints. We have observed
better results by encoding mention similarity as a
hard constraint. Context similarity is thus the cluster
distance measure.5
To turn the Jaro-Winkler distance into a hard
boolean constraint, we tuned a threshold ? on held-out
data, i.e., jaro-winkler(mi,mj) ? ? ? mi = mj .
Likewise, the Maxent model is a binary classifier, so
p(y = 1|mi,mj ;?) > 0.5? mi = mj .
In both clustering algorithms, any two chains Ca
and Cb cannot share the same cluster assignment if:
1. Document origin: doc(Ca) = doc(Cb)
2. Semantic type: type(Ca) 6= type(Cb)
3. Mention Match: sim(mi,mj) = false,
wheremi = repr(Ca) andmj = repr(Cb).
The deterministic accessor function repr(Ca) returns
the representative mention of a chain. The heuristic
we used was ?first mention?: the function returns the
earliest mention that appears in the associated docu-
ment. In many languages, the first mention is typi-
cally more complete than later mentions. This heuris-
tic also makes our system less sensitive to within-
document coreference errors.6 The representative
mention only has special status for mention similar-
ity: context similarity considers all mention contexts.
3.1 Constrained Hierarchical Clustering
HAC iteratively merges the ?nearest? clusters accord-
ing to context similarity. In our system, each cluster
context is a bag of wordsW formed from the contexts
of all coreference chains in that cluster. For each word
inW we estimate a unigram Entity Language Model
(ELM) (Raghavan et al, 2004):
P (w) =
countW (w) + ?PV (w)
?
w? countW (w
?) + ?
PV (w) is the unigram probability in all contexts in
the corpus7 and ? is a smoothing parameter. For any
5Specification of a combined similarity measure is an inter-
esting direction for future work.
6These constraints are similar to the pair-filters of Mayfield
et al (2009).
7Recall that after context mapping, all languages have a com-
mon vocabulary V .
63
two entity clusters Ea and Eb, the distance between
PEa and PEb is given by a metric based on the Jensen-
Shannon Divergence (JSD) (Endres and Schindelin,
2003):
dist(PEa , PEb) =
?
2 ? JSD(PEa ||PEb)
=
?
KL(PEa ||M) +KL(M ||PEb)
where KL(PEa ||M) is the Kullback-Leibler diver-
gence andM = 12(PEa + PEb).
We initialize HAC to E = C, i.e., the initial clus-
tering solution is just the set of all coreference chains.
Thenwe remove all links in the HAC proximitymatrix
that violate pairwise cannot-link constraints. During
clustering, we do not merge Ea and Eb if any pair of
chains violates a cannot-link constraint. This proce-
dure propagates the cannot-link constraints (Klein et
al., 2002). To output E, we stop clustering when the
minimum JSD exceeds a stop threshold ?, which is
tuned on a development set.
3.2 Constrained Dirichlet Process Mixture
Model (DPMM)
Instead of tuning a parameter like ?, it would be prefer-
able to let the data dictate the number of entity clus-
ters. We thus consider a non-parametric Bayesian
mixture model where the mixtures are multinomial
distributions over the entity contexts S. Specifically,
we consider a DPMM, which automatically infers
the number of mixtures. Each Ca has an associated
mixture ?a:
Ca|?a ? Mult(?a)
?a|G ? G
G|?,G0 ? DP(?,G0)
? ? Gamma(1, 1)
where ? is the concentration parameter of the DP
prior and G0 is the base distribution with support V .
For our experiments, we set G0 = Dir(pi1, . . . , piV ),
where pii = PV (wi).
For inference, we use the Gibbs sampler of Vla-
chos et al (2009), which can incorporate pairwise
constraints. The sampler is identical to a standard col-
lapsed, token-based sampler, except the conditional
probability p(Ea = E|E?a, Ca) = 0 if Ca cannot
be merged with the chains in clusterE. This property
makes the model non-exchangeable, but in practice
non-exchangeable models are sometimes useful (Blei
and Frazier, 2010). During sampling, we also learn ?
using the auxiliary variable procedure of West (1995),
so the only fixed parameters are those of the vague
Gamma prior. However, we found that these hyper-
parameters were not sensitive.
4 Training Data and Procedures
We trained our system for Arabic-English cross-
lingual entity clustering.8
Maxent Mention Similarity The Maxent mention
similarity model requires a parallel name list for train-
ing. Name pair lists can be obtained from the LDC
(e.g., LDC2005T34 contains nearly 450,000 parallel
Chinese-English names) or Wikipedia (Irvine et al,
2010). We extracted 12,860 name pairs from the par-
allel Arabic-English translation treebanks,9 although
our experiments show that the model achieves high
accuracy with significantly fewer training examples.
We generated a uniform distribution of training ex-
amples by running a Bernoulli trial for each aligned
name pair in the corpus. If the coin was heads, we
replaced the English name with another English name
chosen randomly from the corpus.
MT Context Mapping For the MT context map-
ping method, we trained Phrasal with all data permit-
ted under the NIST OpenMT Ar-En 2009 constrained
track evaluation. We built a 5-gram language model
from the Xinhua and AFP sections of the Gigaword
corpus (LDC2007T07), in addition to all of the target
side training data. In addition to the baseline Phrasal
feature set, we used the lexicalized re-ordering model
of Galley and Manning (2008).
PLTM Context Mapping For PLTM training, we
formed a corpus of 19,139 English-Arabic topically-
aligned Wikipedia articles. Cross-lingual links in
Wikipedia are abundant: as of February 2010, there
were 77.07M cross-lingual links among Wikipedia?s
272 language editions (de Melo and Weikum, 2010).
To increase vocabulary coverage for our ACE2008
evaluation corpus, we added 20,000 document sin-
gletons from the ACE2008 training corpus. The
8We tokenized all English documents with packages from
the Stanford parser (Klein and Manning, 2003). For Arabic
documents, we used Mada (Habash and Rambow, 2005) for
orthographic normalization and clitic segmentation.
9LDC Catalog numbers LDC2009E82 and LDC2009E88.
64
topically-aligned tuples served as ?glue? to share top-
ics between languages, while the ACE documents
distribute those topics over in-domain vocabulary.10
We used the PLTM implementation in Mallet (Mc-
Callum, 2002). We ran the sampler for 10,000 itera-
tions and set the number of topicsK = 512.
5 Task Evaluation Framework
Our experimental design is a cross-lingual extension
of the standard cross-document coreference resolu-
tion task, which appeared in ACE2008 (Strassel et
al., 2008; NIST, 2008). We evaluate name (NAM)
mentions for cross-lingual person (PER) and organi-
zation (ORG) entities. Neither the number nor the
attributes of the entities are known (i.e., the task does
not include a knowledge base). We report results for
both gold and automatic within-document mention
detection and coreference resolution.
Evaluation Metrics We use entity-level evaluation
metrics, i.e., we evaluate the E entity clusters rather
than the mentions. For the gold setting, we report:
? B3 (Bagga and Baldwin, 1998a): Precision and
recall are computed from the intersection of the
hypothesis and reference clusters.
? CEAF (Luo, 2005): Precision and recall are
computed from a maximum bipartite matching
between hypothesis and reference clusters.
? NVI (Reichart and Rappoport, 2009):
Information-theoretic measure that uti-
lizes the entropy of the clusters and their mutual
information. Unlike the commonly-used Varia-
tion of Information (VI) metric, normalized VI
(NVI) is not sensitive to the size of the data set.
For the automatic setting, we must apply a different
metric since the number of system chains may differ
from the reference. We use B3sys (Cai and Strube,
2010), a variant of B3 that was shown to penalize
both twinless reference chains and spurious system
chains more fairly.
Evaluation Corpus The automatic evaluation of
cross-lingual coreference systems requires annotated
10Mimno et al (2009) showed that so long as the proportion
of topically-aligned to non-aligned documents exceeded 0.25,
the topic distributions (as measured by mean Jensen-Shannon
Divergence between distributions) did not degrade significantly.
Docs Tokens Entities Chains Mentions
Arabic 412 178,269 2,594 4,216 9,222
English 414 246,309 2,278 3,950 9,140
Table 3: ACE2008 evaluation corpus PER and ORG entity
statistics. Singleton chains account for 51.4% of the Arabic
data and 46.2% of the English data. Just 216 entities appear
in both languages.
multilingual corpora. Cross-document annotation
is expensive (Strassel et al, 2008), so we chose the
ACE2008 Arabic-English evaluation corpus as a start-
ing point for cross-lingual annotation. The corpus
consists of seven genres sampled from independent
sources over the course of a decade (Tbl. 3). The
corpus provides gold mono-lingual cross-document
coreference annotations for both PER and ORG enti-
ties. Using these annotations as a starting point, we
found and annotated 216 cross-lingual entities.11
Because a similar corpus did not exist for develop-
ment, we split the evaluation corpus into development
and test sections. However, the usual method of split-
ting by document would not confine all mentions of
each entity to one side of the split. We thus split the
corpus by global entity id. We assigned one-third of
the entities to development, and the remaining two-
thirds to test.
6 Comparison to Related Tasks and Work
Our modeling techniques and task formulation can be
viewed as cross-lingual extensions to cross-document
coreference resolution. The classic work on this task
was by Bagga and Baldwin (1998b), who adapted
the Vector Space Model (VSM) (Salton et al, 1975).
Gooi and Allan (2004) found effective algorithmic
extensions like agglomerative clustering. Successful
feature extensions to the VSM for cross-document
coreference have included biographical information
(Mann and Yarowsky, 2003) and syntactic context
(Chen and Martin, 2007). However, neither of these
feature sets generalize easily to the cross-lingual set-
ting with multiple entity types. Fleischman and Hovy
(2004) added a discriminative pairwise mention clas-
sifier to a VSM-like model, much as we do. More
11The annotators were the first author and another fluent
speaker of Arabic. The annotations, corrections, and corpus
split are available at http://www.spencegreen.com/research/.
65
recent work has considered new models for web-scale
corpora (Rao et al, 2010; Singh et al, 2011).
Cross-document work on languages other than En-
glish is scarce. Wang (2005) used a combination of
the VSM and heuristic feature selection strategies to
cluster transliterated Chinese personal names. For
Arabic, Magdy et al (2007) started with the output of
the mention detection and within-document corefer-
ence system of Florian et al (2004). They clustered
the entities incrementally using a binary classifier.
Baron and Freedman (2008) used complete-link ag-
glomerative clustering, wheremerging decisions were
based on a variety of features such as document topic
and name uniqueness. Finally, Sayeed et al (2009)
translated Arabic name mentions to English and then
formed clusters greedily using pairwise matching.
To our knowledge, the cross-lingual entity cluster-
ing task is novel. However, there is significant prior
work on similar tasks:
? Multilingual coreference resolution: Adapt
English within-document coreference models to
other languages (Harabagiu andMaiorano, 2000;
Florian et al, 2004; Luo and Zitouni, 2005).
? Named entity translation: For a non-English
document, produce an inventory of entities in
English. An ACE2007 pilot task (Song and
Strassel, 2008).
? Named entity clustering: Assign semantic
types to text mentions (Collins and Singer, 1999;
Elsner et al, 2009).
? Cross-language name search / entity linking:
Match a single query name against a list of
known multilingual names (knowledge base). A
track in the 2011NIST Text Analysis Conference
(TAC-KBP) evaluation (Aktolga et al, 2008;
McCarley, 2009; Udupa and Khapra, 2010; Mc-
Namee et al, 2011).
Our work incorporates elements of the first three tasks.
Most importantly, we avoid the key element of entity
linking: a knowledge base.
7 Experiments
We performed intrinsic evaluations for both mention
and context similarity. For context similarity, we
analyzed mono-lingual entity clustering, which also
facilitated comparison to prior work on the ACE2008
Genre #Train #Test Accuracy(%)
wb 125 16 87.5
bn 2,720 340 95.6
nw 7,443 930 96.6
all 10,288 1,286 97.1 (+7.55)
Table 4: Cross-lingual mention matching accuracy [%].
The training data contains names from three genres: broad-
cast news (bn), newswire (nw), and weblog (wb). We used
the full training corpus (all) for the cross-lingual clustering
experiments, but the model achieved high accuracy with
significantly fewer training examples (e.g., bn).
CEAF? NVI? B3 ?
#hyp P R F1
Mono-lingual Arabic (#gold=1,721)
HAC 87.2 0.052 1,669 89.8 89.8 89.8
Mono-lingual English (#gold=1,529)
HAC 88.5 0.042 1,536 93.7 89.0 91.4
Table 5: Mono-lingual entity clustering evaluation (test
set, gold within-document processing). Higher scores (?)
are better for CEAF and B3, whereas lower (?) is better
for NVI. #gold indicates the number of reference entities,
whereas #hyp is the size of E.
evaluation set. Our main results are for the new task:
cross-lingual entity clustering.
7.1 Intrinsic Evaluations
Cross-lingual Mention Matching We created a
random 80/10/10 (train, development, test) split of
the Maxent training corpus and evaluated binary clas-
sification accuracy (Tbl. 4). Of the mis-classified
examples, we observed three major error types. First,
the model learns that high edit distance is predictive
of a mismatch. However, singleton strings that do not
match often have a lower edit distance than longer
strings that do match. As a result, singletons often
cause false positives. Second, names that originate in
a third language tend to violate the phonemic corre-
spondences. For example, the model gives a false neg-
ative for a German football team: 	?QK???P 	Q
? ?

??
	
?@
(phonetic mapping: af s kazrslawtrn) versus ?FC
Kaiserslautern.? Finally, names that require trans-
lation are problematic. For example, the classifier
produces a false negative for ?God, gd?
?
= ? ?

<?

@, allh?.
66
#gold = 3,057 CEAF? NVI? B3 ? B3target ? (#gold = 146)
#hyp P R F1 #hyp P R F1
Singleton 64.9 0.165 5,453 100.0 56.1 71.8 1,587 100.0 9.20 16.9
No-context 57.4 0.136 2,216 65.6 75.2 70.1 517 78.3 41.8 54.5
HAC+MT 79.8 0.070 2,783 84.4 86.4 85.4 310 91.7 69.1 78.8
DPMM+MT 74.3 0.122 3,649 89.3 64.1 74.6 634 93.3 24.3 38.6
HAC+PLTM 72.1 0.110 2,746 76.9 77.6 77.3 506 84.4 44.6 58.4
DPMM+PLTM 57.2 0.180 2,609 64.0 62.8 63.4 715 73.9 22.2 34.1
Table 6: Cross-lingual entity clustering (test set, gold within-document processing). B3target is the standard B
3 metric
applied to the subset of target cross-lingual entities in the test set. For CEAF and B3, Singleton is the stronger baseline
due to the high proportion of singleton entities in the corpus. Of course, cross-lingual entities have at least two chains,
so No-context is a better baseline for cross-lingual clustering.
Mono-lingual Entity Clustering For comparison,
we also evaluated our system on a standard mono-
lingual cross-document coreference task (Arabic and
English) (Tbl. 5). We configured the system with
HAC clustering and Jaro-Winkler (within-language)
mention similarity. We built mono-lingual ELMs for
context similarity.
We used two baselines:
? Singleton: E = C, i.e., the cross-lingual clus-
tering solution is just the set of mono-lingual
coreference chains. This is a common baseline
for mono-lingual entity clustering (Baron and
Freedman, 2008).
? No-context: We run HAC with ? =?. There-
fore, E is the set of fully-connected components
in C subject to the pairwise constraints.
For HAC, we manually tuned the stop threshold ?,
the Jaro-Winkler threshold ?, and the ELM smoothing
parameter ? on the development set. For the DPMM,
no development tuning was necessary, and we evalu-
ated a single sample of E taken after 3,000 iterations.
To our knowledge, Baron and Freedman (2008)
reported the only previous results on the ACE2008
data set. However, they only gave gold results for
English, and clustered the entire evaluation corpus
(test+development). To control for the effect of
within-document errors, we considered their gold in-
put (mention detection and within-document coref-
erence resolution) results. They reported B3 for the
two entity types separately: ORG (91.5% F1) and
PER (94.3% F1). The different experimental designs
preclude a precise comparison, but the accuracy of
#gold = 3,057 B3sys ?
#hyp P R F1
Singleton 7,655 100.0 57.1 72.7
No-context 2,918 63.3 71.1 67.0
HAC+MT 3,804 75.6 77.8 76.7
DPMM+MT 4,491 77.1 62.5 69.0
HAC+PLTM 6,353 94.1 62.8 75.3
DPMM+PLTM 3,522 64.6 62.0 63.3
Table 7: Cross-lingual entity clustering (test set, automatic
(Serif) within-document processing). For HAC, we used
the same parameters as the gold setting.
the two systems are at least in the same range.
7.2 Cross-lingual Entity Clustering
We evaluated four system configurations on the new
task: HAC+MT, HAC+PLTM, DPMM+MT, and
DPMM+PLTM. First, we established an upper bound
by assuming gold within-document mention detection
and coreference resolution (Tbl. 6). This setting iso-
lated the new cross-lingual clustering methods from
within-document processing errors. Then we evalu-
ated with Serif (automatic) within-document process-
ing (Tbl. 7). This second experiment replicated an
application setting. We used the same baselines and
tuning procedures as in the mono-lingual clustering
experiment.
Results In the gold setting, HAC+MTproduces the
best results, as expected. The dimensionality reduc-
tion of the vocabulary imposed by PLTM significantly
reduces accuracy, but HAC+PLTM still exceeds the
67
baseline. We tried increasing the number of PLTM
topics k, but did not observe an improvement in task
accuracy. For both context-mapping methods, the
DPMM suffers from low-recall. Upon inspection, the
clustering solution of DPMM+MT contains a high
proportion of singleton hypotheses, suggesting that
the model finds lower similarity in the presence of a
larger vocabulary. When the context vocabulary con-
sists of PLTM topics, larger clusters are discovered
(DPMM+PLTM).
The effect of dimensionality reduction is also appar-
ent in the clustering solutions of the PLTM models.
For example, for the Serif output, DPMM+PLTM
produces a cluster consisting of ?White House?, ?Sen-
ate?, ?House of Representatives?, and ?Parliament?.
Arabic mentions of the latter three entities pass the
pairwise mention similarity constraints due to the
word ??m.? ?council?, which appears in text mentions
for all three legislative bodies. A cross-language
matching error resulted in the linking of ?White
House?, and the reduced granularity of the contexts
precluded further disambiguation. Of course, these
entities probably appear in similar contexts.
The caveat with the Serif results in Tbl. 7 is that
3,251 of the 7,655 automatic coreference chains are
not in the reference. Consequently, the evaluation is
dominated by the penalty for spurious system coref-
erence chains. Nonetheless, all models except for
DPMM+PLTM exceed the baselines, and the rela-
tionships between models depicted in the gold exper-
iments hold for the this setting.
8 Conclusion
Cross-lingual entity clustering is a natural step to-
ward more robust natural language understanding.
We proposed pipeline models that make clustering
decisions based on cross-lingual similarity. We inves-
tigated two methods for mapping documents in differ-
ent languages to a common representation: MT and
the PLTM. Although MT may achieve more accurate
results for some language pairs, the PLTM training
resources (e.g., Wikipedia) are readily available for
many languages. As for the clustering algorithms,
HAC appears to perform better than the DPMM on
our dataset, but this may be due to the small corpus
size. The instance-level constraints represent tenden-
cies that could be learned from larger amounts of data.
With more data, we might be able to relax the con-
straints and use an exchangeable DPMM,whichmight
be more effective. Finally, we have shown that sig-
nificant quantities of within-document errors cascade
into the cross-lingual clustering phase. As a result,
we plan a model that clusters the mentions directly,
thus removing the dependence on within-document
coreference resolution.
In this paper, we have set baselines and proposed
models that significantly exceeded those baselines.
The best model improved upon the cross-lingual en-
tity baseline by 24.3% F1. This result was achieved
without a knowledge base, which is required by previ-
ous approaches to cross-lingual entity linking. More
importantly, our techniques can be used to extend
existing cross-document entity clustering systems for
the increasingly multilingual web.
AcknowledgmentsWe thank Jason Eisner, David Mimno,
Scott Miller, Jim Mayfield, and Paul McNamee for helpful
discussions. This work was started during the SCALE
2010 summer workshop at Johns Hopkins. The first author
is supported by a National Science Foundation Graduate
Fellowship.
References
E. Aktolga, M. Cartright, and J. Allan. 2008. Cross-document
cross-lingual coreference retrieval. In CIKM.
G. Andrew and J. Gao. 2007. Scalable training of L1-regularized
log-linear models. In ICML.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152?1174.
A. Bagga and B. Baldwin. 1998a. Algorithms for scoring coref-
erence chains. In LREC.
A. Bagga and B. Baldwin. 1998b. Entity-based cross-document
coreferencing using the vector space model. In COLING-ACL.
A. Baron and M. Freedman. 2008. Who is Who and What
is What: Experiments in cross-document co-reference. In
EMNLP.
D. Blei and P. Frazier. 2010. Distance dependent Chinese restau-
rant processes. In ICML.
J. Cai and M. Strube. 2010. Evaluation metrics for end-to-
end coreference resolution systems. In Proceedings of the
SIGDIAL 2010 Conference.
D. Cer, M. Galley, D. Jurafsky, and C. D.Manning. 2010. Phrasal:
A statistical machine translation toolkit for exploring new
model features. In HLT-NAACL, Demonstration Session.
E. Charniak. 2001. Unsupervised learning of name structure
from coreference data. In NAACL.
Y. Chen and J. Martin. 2007. Towards robust unsupervised
personal name disambiguation. In EMNLP-CoNLL.
68
P. Christen. 2006. A comparison of personal name matching:
Techniques and practical issues. Technical Report TR-CS-06-
02, Australian National University.
M. Collins and Y. Singer. 1999. Unsupervised models for named
entity classification. In EMNLP.
G. de Melo and G. Weikum. 2010. Untangling the cross-lingual
link structure of Wikipedia. In ACL.
M. Elsner, E. Charniak, and M. Johnson. 2009. Structured
generative models for unsupervised named-entity clustering.
In HLT-NAACL.
D. M. Endres and J. E. Schindelin. 2003. A new metric for
probability distributions. IEEE Transactions on Information
Theory, 49(7):1858 ? 1860.
M. Fleischman and E. Hovy. 2004. Multi-document person name
resolution. In ACL Workshop on Reference Resolution and its
Applications.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, et al
2004. A statistical model for multilingual entity detection and
tracking. In HLT-NAACL.
A. T. Freeman, S. L. Condon, and C. M. Ackerman. 2006. Cross
linguistic name matching in English and Arabic: a one to
many mapping extension of the Levenshtein edit distance
algorithm. In HLT-NAACL.
M. Galley and C. D. Manning. 2008. A simple and effective
hierarchical phrase reordering model. In EMNLP.
C. H. Gooi and J. Allan. 2004. Cross-document coreference on
a large scale corpus. In HLT-NAACL.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-of-
speech tagging and morphological disambiguation in one fell
swoop. In ACL.
S. M. Harabagiu and S. J. Maiorano. 2000. Multilingual corefer-
ence resolution. In ANLP.
A. Irvine, C. Callison-Burch, and A. Klementiev. 2010. Translit-
erating from all languages. In AMTA.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. Klein, S. D. Kamvar, and C. D.Manning. 2002. From instance-
level constraints to space-level constraints: Making the most
of prior knowledge in data clustering. In ICML.
K. Knight and J. Graehl. 1998. Machine transliteration. Compu-
tational Linguistics, 24:599?612.
X. Luo and I. Zitouni. 2005. Multi-lingual coreference resolution
with syntactic features. In HLT-EMNLP.
X. Luo. 2005. On coreference resolution performance metrics.
In HLT-EMNLP.
W. Magdy, K. Darwish, O. Emam, and H. Hassan. 2007. Arabic
cross-document person name normalization. In Workshop on
Computational Approaches to Semitic Languages.
G. S. Mann and D. Yarowsky. 2003. Unsupervised personal
name disambiguation. In NAACL.
C. D. Manning, P. Raghavan, and H. Sch?tze. 2008. Introduction
to Information Retrieval. Cambridge University Press.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed, et al
2009. Cross-document coreference resolution: A key technol-
ogy for learning by reading. In AAAI Spring Symposium on
Learning by Reading and Learning to Read.
A. K. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
J. S. McCarley. 2009. Cross language name matching. In SIGIR.
P. McNamee, J. Mayfield, D. Lawrie, D.W. Oard, and D. Doer-
mann. 2011. Cross-language entity linking. In IJCNLP.
D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and
A. McCallum. 2009. Polylingual topic models. In EMNLP.
NIST. 2008. Automatic Content Extraction 2008 evaluation
plan (ACE2008): Assessment of detection and recognition
of entities and relations within and across documents. Tech-
nical Report rev. 1.2d, National Institute of Standards and
Technology (NIST), 8 August.
E. H. Porter and W. E. Winkler, 1997. Approximate String Com-
parison and its Effect on an Advanced Record Linkage System,
chapter 6, pages 190?199. U.S. Bureau of the Census.
H. Raghavan, J. Allan, and A. McCallum. 2004. An explo-
ration of entity models, collective classification and relation
description. In KDD Workshop on Link Analysis and Group
Detection.
L. Ramshaw, E. Boschee, M. Freedman, J. MacBride,
R. Weischedel, and A. Zamanian. 2011. SERIF language
processing?effective trainable language understanding. In
J. Olive et al, editors,Handbook of Natural Language Process-
ing and Machine Translation: DARPA Global Autonomous
Language Exploitation, pages 636?644. Springer.
D. Rao, P. McNamee, and M. Dredze. 2010. Streaming cross
document entity coreference resolution. In COLING.
R. Reichart and A. Rappoport. 2009. The NVI clustering evalu-
ation measure. In CoNLL.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model
for automatic indexing. CACM, 18:613?620, November.
A. Sayeed, T. Elsayed, N. Garera, D. Alexander, T. Xu, et al
2009. Arabic cross-document coreference detection. In ACL-
IJCNLP, Short Papers.
S. Singh, A. Subramanya, F. Pereira, and A. McCallum. 2011.
Large-scale cross-document coreference using distributed in-
ference and hierarchical models. In ACL.
Z. Song and S. Strassel. 2008. Entity translation and alignment
in the ACE-07 ET task. In LREC.
S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda.
2008. Linguistic resources and evaluation techniques for
evaluation of cross-document automatic content extraction.
In LREC.
R. Udupa and M. M. Khapra. 2010. Improving the multilin-
gual user experience of Wikipedia using cross-language name
search. In HLT-NAACL.
A. Vlachos, A. Korhonen, and Z. Ghahramani. 2009. Unsuper-
vised and constrained Dirichlet process mixture models for
verb clustering. In Proc. of the Workshop on Geometrical
Models of Natural Language Semantics.
H. Wang. 2005. Cross-document transliterated personal name
coreference resolution. In L. Wang and Y. Jin, editors, Fuzzy
Systems and Knowledge Discovery, volume 3614 of Lecture
Notes in Computer Science, pages 11?20. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
69
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 783?792,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Shared Components Topic Models
Matthew R. Gormley Mark Dredze Benjamin Van Durme Jason Eisner
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,mdredze,vandurme,jason}@cs.jhu.edu
Abstract
With a few exceptions, extensions to latent
Dirichlet alocation (LDA) have focused on
the distribution over topics for each document.
Much less attention has been given to the un-
derlying structure of the topics themselves. As
a result, most topic models generate topics in-
dependently from a single underlying distri-
bution and require millions of parameters, in
the form of multinomial distributions over the
vocabulary. In this paper, we introduce the
Shared Components Topic Model (SCTM), in
which each topic is a normalized product of a
smaller number of underlying component dis-
tributions. Our model learns these component
distributions and the structure of how to com-
bine subsets of them into topics. The SCTM
can represent topics in a much more compact
representation than LDA and achieves better
perplexity with fewer parameters.
1 Introduction
Topic models are probabilistic graphical models
meant to capture the semantic associations underly-
ing corpora. Since the introduction of latent Dirich-
let alocation (LDA) (Blei et al, 2003), these mod-
els have been extended to account for more complex
distributions over topics, such as adding supervision
(Blei and McAuliffe, 2007), non-parametric priors
(Blei et al, 2004; Teh et al, 2006), topic correla-
tions (Li and McCallum, 2006; Mimno et al, 2007;
Blei and Lafferty, 2006) and sparsity (Williamson et
al., 2010; Eisenstein et al, 2011).
While much research has focused on modeling
distributions over topics, less focus has been given to
the makeup of the topics themselves. This emphasis
leads us to find two problems with LDA and its vari-
ants mentioned above: (1) independently generated
topics and (2) overparameterized models.
Independent Topics In the models above, the top-
ics are modeled as independent draws from a single
underlying distribution, typically a Dirichlet. This
violates the topic modeling community?s intuition
that these distributions over words are often related.
As an example, consider a corpus that supports two
related topics, baseball and hockey. These topics
likely overlap in their allocation of mass to high
probability words (e.g. team, season, game, play-
ers), even though the two topics are unlikely to ap-
pear in the same documents. When topics are gen-
erated independently, the model does not provide a
way to capture this sharing between related topics.
Many extensions to LDA have addressed a related
issue, LDA?s inability to model topic correlation,1
by changing the distributions over topics (Blei and
Lafferty, 2006; Li and McCallum, 2006; Mimno et
al., 2007; Paisley et al, 2011). Yet, none of these
change the underlying structure of the topic?s distri-
butions over words.
Overparameterization Topics are most often
parameterized as multinomial distributions over
words: increasing the topics means learning new
multinomials over large vocabularies, resulting in
models consisting of millions of parameters. This
issue was partially addressed in SAGE (Eisenstein
et al, 2011) by encouraging sparsity in the topics
which are parameterized by their difference in log-
frequencies from a fixed background distribution.
Yet the problem of overparameterization is also tied
1Two correlated topics, e.g. nutrition and exercise, are likely
to co-occur, but their word distributions might not overlap.
783
to the number of topics, and though SAGE reduces
the number of non-zero parameters, it still requires
a vocabulary-sized parameter vector for each topic.
We present the Shared Components Topic Model
(SCTM), which addresses both of these issues by
generating each topic as a normalized product of a
smaller number of underlying components. Rather
than learning each new topic from scratch, we model
a set of underlying component distributions that
constrain topic formation. Each topic can then be
viewed as a combination of these underlying com-
ponents, where in a model such as LDA, we would
say that components and topics stand in a one to one
relationship. The key advantages of the SCTM are
that it can learn and share structure between overlap-
ping topics (e.g. baseball and hockey) and that it can
represent the same number of topics in a much more
compact representation, with far fewer parameters.
Because the topics are products of components,
we present a new training algorithm for the sig-
nificantly more complex product case which re-
lies on a Contrastive Divergence (CD) objective.
Since SCTM topics, which are products of distri-
butions, could be represented directly by distribu-
tions as in LDA, our goal is not necessarily to learn
better topics, but to learn models that are substan-
tially smaller in size and generalize better to unseen
data. Experiments on two corpora show that our
model uses fewer underlying multinomials and still
achieves lower perplexity than LDA, which suggests
that these constraints could lead to better topics.
2 Shared Components Topic Models
The Shared Components Topic Model (SCTM) fol-
lows previous topic models in inducing admixture
distributions of topics that are used to generate each
document. However, here each topic multinomial
distribution over words itself results from a normal-
ized product of shared components, each a multino-
mial over words. Each topic selects a subset of com-
ponents. We begin with a review and then introduce
the SCTM.
Latent Dirichlet alocation (LDA) (Blei et al,
2003) is a probabilistic topic model which defines
a generative process whereby sets of observations
are generated from latent topic distributions. In the
SCTM, we use the same generative process of topic
assignments as LDA, but replace the K indepen-
dently generated topics (multinomials over words)
with products of C components.
Latent Dirichlet alocation generative process
For each topic k ? {1, . . . ,K}:
?k ? Dir(?) [draw distribution over words]
For each document m ? {1, . . . ,M}:
?m ? Dir(?) [draw distribution over topics]
For each word n ? {1, . . . , Nm}:
zmn ? Mult(1,?m) [draw topic]
xmn ? ?zmi [draw word]
LDA draws each topic ?k independently from a
Dirichlet. The model generates each document m
of length M , by first sampling a distribution over
topics ?m. Then, for each word n, a topic zmn is
chosen and a word type xmn is generated from that
topic?s distribution over words ?zmi .
A Product of Experts (PoE) model (Hinton,
1999) is the normalized product of the expert dis-
tributions. In the SCTM, each component (an ex-
pert) models an underlying multinomial word dis-
tribution. We let ?c be the parameters of the cth
component, where ?cv is the probability of the cth
component generating word v. If the structure of a
PoE included only components c ? C in the prod-
uct, it would have the form: p(x|?1, . . . ,?C) =Q
c?C ?cx
PV
v=1
Q
c?C ?cv
, where there are C components, and
the summation in the denominator is over the vocab-
ulary. In a PoE, each component can overrule the
others by giving low probability to some word. A
PoE can be viewed as a soft intersection of its com-
ponents, whereas a mixture is a soft union.
The Beta-Bernoulli model (Griffiths and
Ghahramani, 2006) is a distribution over binary
matrices with a fixed number of rows and columns.
It is the finite counterpart to the Indian Buffet
Process. In this work, we use the Beta-Bernoulli as
our prior for an unobserved binary matrix B with C
columns and K rows. In the SCTM, each row bk of
the matrix, a binary feature vector, defines a topic
distribution. The binary vector acts as a selector
for the structure of the PoE for that topic. The row
determines which components to include in the
product by which entries bkc are ?on? (equal to 1)
in that row. Under Beta-Bernoulli prior, for each
column, a coin with weight pic is chosen. For each
entry in the column, the coin is flipped to determine
if the entry is ?on? or ?off?. This corresponds to
784
the notion that some components are a priori more
likely to be included in topics.
The Beta-Bernoulli model generative process
For each component c ? {1, . . . , C}: [columns]
pic ? Beta(
?
C , 1) [draw probability of component c]
For each topic k ? {1, . . . ,K}: [rows]
bkc ? Bernoulli(pic) [draw whether topic includes cth
component in its PoE]
2.1 Shared Components Topic Models
The Shared Components Topic Model generates
each document just like LDA, the only difference
is the topics are not drawn independently from a
Dirichlet prior. Instead, topics are soft intersections
of underlying components, each of which is a multi-
nomial distribution over words. These components
are combined via a PoE model, and each topic is
constructed according to a length C binary vector
bk; where bkc = 1 includes and bkc = 0 excludes
component c. Stacking theK vectors forms aK?C
matrix; rows correspond to topics and columns to
components. Overlapping topics share components
in common.
Generative process SCTM?s generative process
generates topics and words, but must also generate
the binary matrix. For each of the C shared com-
ponents, we generate a distribution ?c over the V
words from a Dirichlet parametrized by ?. Next,
we generate a K ? C binary matrix using the Beta-
Bernoulli prior. These components and the binary
matrix implicitly define the complete set of K topic
distributions, each of which is a PoE.
p(x|bk,?) =
?C
c=1 ?
bkc
cx
?V
v=1
?C
c=1 ?
bkc
cv
(1)
The distribution p(?|bk,?) defines the kth topic.
Conditioned on these K topics, the remainder of the
generative process, which generates the documents,
is just like LDA.
The Shared Components Topic Model generative process
For each component c ? {1, . . . , C}:
?c ? Dir(?) [draw distribution over words]
pic ? Beta(
?
C , 1) [draw probability of component c]
For each topic k ? {1, . . . ,K}:
bkc ? Bernoulli(pic) [draw whether topic includes cth
component in its PoE]
For each document m ? {1, . . . ,M}
?m ? Dir(?) [draw distribution over topics]
For each word n ? {1, . . . , Nm}
zmn ? Mult(1,?m) [draw topic]
xmn ? p(? |bzmn ,?) given by Eq. (1) [draw word]
See Figure 1 for the graphical model.
Discussion An advantage of this formulation is the
ability to model many topics using few components.
While LDA must maintain V ?K parameters for the
topic distributions, the SCTM maintains just V ?C
parameters, plus an additional K?C binary matrix.
Since C < K  V this results in many fewer pa-
rameters for the SCTM.2 Extending the number of
topics (rows) requires storing additional binary vec-
tors, a lightweight requirement. In theory, we could
enable all 2C possible component combinations, al-
though we expect to use far less. On the other hand,
constraining the SCTM?s topics by the components
gives less flexible topics as compared to LDA. How-
ever, we find empirically that a large number of top-
ics can be effectively modeled with a smaller num-
ber of components.
Observe that we can reparameterize the SCTM as
LDA by assuming an identity square matrix; each
component corresponds to a topic in LDA, making
LDA a special case of the SCTM with an identity
matrix IC . Intuitively, SCTM learning could pro-
duce an LDA model where appropriate. Finally, we
can also think of the SCTM as learning the struc-
ture of many PoE models. In applications where ex-
perts abstain, the SCTM could learn in which setting
(row) each expert casts a vote.
3 Parameter Estimation
Parameter estimation infers values for model pa-
rameters ?, pi, and ? from data using an unsuper-
vised training procedure. Because exact inference
is intractable in the SCTM, we turn to approximate
methods. As is common in these models, we will
integrate out pi and ?, sample latent variables Z and
B, and optimize the components ?. Our algorithm
follows the outline of the Monte Carlo EM (MCEM)
algorithm (Wei and Tanner, 1990). In the Monte
Carlo E-step, we will re-sample the latent variables
Z and B based on current model parameters ? and
observed data X . In the M-step, we will find new
model parameters ?. Since these parameters corre-
spond to experts in the PoE, we rely on a contrastive
divergence (CD) objective (Hinton, 2002), popular
for PoE training, rather than maximizing the data
2The vocabulary size V could be much larger if n-grams or
relational triples are used, as opposed to unigrams.
785
log-likelihood. Normally, CD only estimates the pa-
rameters of the expert distributions. However, in our
model, the structure of the PoEs themselves change
based on the E-step. Since we generate multiple
samples in the E-step, we modify the CD objective
to compute the gradient for each E-step sample and
take the average to approximate the expectation un-
der B and Z.3
3.1 E-Step
The E-step approximates an expectation under
p(B,Z|X,?,?, ?) for latent topic assignments Z
and matrix B using Gibbs sampling. The Gibbs
sampler uses the full conditionals for both zi (7) and
bkc (12), which we derive in Appendix A. Using this
sampler, we obtain J samples of Z and B by iterat-
ing through each value of zi and bkc J times (in our
experiments, we use J=1, which appears to work as
well on this task as multiple samples). These J sam-
ples are then used in the M-step as an approximation
of the expectation of the latent variables.
3.2 M-Step
Given many samples of B and Z, the M-step opti-
mizes the component parameters ? which cannot be
collapsed out. We utilize the standard PoE training
procedure for experts: contrastive divergence (CD).
We approximate the CD gradient as the difference of
the data distribution and the one-step reconstruction
of the data according to the current parameters. As
in Generalized EM (Dempster et al, 1977), a single
gradient step in the direction of the contrastive di-
vergence objective is sufficient for each M-step. A
key difference in our model is that we must incor-
porate the expectation of the PoE model structure,
which in our case is a random variable instead of a
fixed observed structure. We achieve this by simply
3CD training within MCEM is not the only possible ap-
proach. One alternative would be to compute the CD gradient
summing over all values of B and Z, effectively training the
entire model using CD. This approach prevents the normal CD
objective derivation from being simplified into a more tractable
form. Another approach would be a pure MCMC algorithm,
which sampled ? directly. While using the natural parameters
allows the sampler to mix, it is too computationally intensive to
be practical. Finally, we could train with Generalized MCEM,
where the exact gradient of the log-likelihood (or log-posterior)
is used, but this easily gets stuck in local minima. After exper-
imenting with these and other options, we present our current
most effective estimation method.
computing the CD gradient for each PoE given each
of the J samples {Z,B}(j) from the E-Step, then
average the result.
Another difficulty arises from computing the gra-
dient directly for the multinomial?c due to the V ?1
degrees of freedom imposed by sum-to-one con-
straints. Therefore, we switch to the natural pa-
rameters, which obviates the need for considering
the sum-to-one constraint in the optimization, by
defining ?c in terms of V real valued parameters
{?c1, . . . , ?cV }:
?cv =
exp(?cv)
?V
t=1 exp(?cv)
(2)
The V parameters ?cv are then used to compute ?cv
for use in the E-step.
As explained above, the M-step does not maxi-
mize the data log-likelihood, but instead minimizes
contrastive divergence. Hinton (2002) explains that
maximizing data log-likelihood is equivalent to min-
imizing Q0||Q?? , the KL divergence between the
observed data distribution, Q0, and the model?s
equilibrium distribution,Q?? .
4 MinimizingQ0||Q??
would require the computation of an intractable ex-
pectation under the equilibrium distribution. We
avoid this by instead minimizing the contrastive di-
vergence objective,
CD(?|{Z,B}(j)) = Q0||Q?? ?Q
1
? ||Q
?
? , (3)
where Q1? is the distribution over one-step recon-
structions of the data, X given Z,B, ?, that are gen-
erated by a single step of Gibbs sampling.
Unlike standard applications of CD training, the
hidden variables (Z,B) are not contained within the
experts. Instead they define the structure of the PoE
model, where B indicates which experts to use in
each product (topic) andZ indicates which PoE gen-
erates each word. Unfortunately, CD training cannot
infer this structure since the CD derivation makes
use of a fixed structure in the one-step reconstruc-
tion. Therefore, we have taken a MCEM approach,
first sampling the PoE structure in the E-step, then
4Hinton (2002) used this notation because the data distribu-
tion,Q0, can be described as the state of a Markov chain at time
0 that was started at the data distribution. Similarly, the equilib-
rium distribution, Q?? could be obtained by running the same
Markov chain to time?.
786
M
Nm
C
K
xmn
zmn
?m
?
?c
bkc
pic
?
?
Figure 1: The graphical model for the SCTM.
fixing these samples for Z and B when computing
the one-step reconstruction of the data, X .
Contrastive Divergence Gradient We provide
the approximate derivative of the contrastive di-
vergence objective, where Z and B are treated as
fixed.5
dCD(?|{Z,B}(j))
d?
? ?
?
d log f(x|bz, ?)
d?
?
Q0
+
?
d log f(x|bz, ?)
d?
?
Q1?
where f(x|bz, ?) =
?C
c=1 ?
bzc
cx is the numerator of
p(x|bz, ?) and the derivative of its log is efficient to
compute:
d log f(x|bz, ?)
d?cv
=
{
bzc(1? ?cv) for x = v
?bzc?cv for x 6= v
To approximate the expectation under Q1? , we hold
Z,B, ? fixed and resample the data, X , using one
step of Gibbs sampling.
3.3 Summary
Our learning algorithm can be viewed
in terms of a Q function: Q(?|?(t)) ?
1
J
?J
j=1 CD(?|{Z,B}
(j))where we average over
J samples. The E-step computes Q(?|?(t)). The
M-step minimizes Q with respect to ? to obtain the
updated ?(t+1) by performing gradient descent on
the Q function as ?(t+1)cv = ?
(t)
cv ? ? ?
dQ(?|?(t))
d?cv
for
all values of c, v.
5The derivative is approximate because we drop the term:
?
dQ1?
d? ?
dQ1?||Q
?
?
dQ1?
, which is ?problematic to compute? (Hinton,
2002). This is the standard use of CD.
Algorithm 1 SCTM Training
Initialize parameters: ?c, bkc, zi.
while not converged do
{E-step:}
for j = 1 to J do
{Draw jth sample {Z,B}(j)}
for i = 1 to N do
Sample zi using Eq. (7)
for k = 1 to K do
for c = 1 to C do
Sample bkc using ratio in Eq. (12)
{M-step:}
for c = 1 to C do
for v = 1 to V do
Single gradient step over ?
?(t+1)cv = ?
(t)
cv ? ? ?
dQ(?|?(t))
d?cv
4 Related Models
The SCTM is closely related to the the Infinite
Overlapping Mixture Model (IOMM) (Heller and
Ghahramani, 2007), yet our model differs from and,
in some ways, extends theirs. The IOMM mod-
els the geometric overlap of Gaussian clusters us-
ing PoEs, and models the structure of the PoEs with
the rows of a binary matrix. The SCTM models a
finite number of columns, where the IOMM mod-
els an infinite number. The IOMM generates a row
for each data point, whereas the SCTM generates a
row for each topic. Thus, the SCTM goes beyond
the IOMM by allowing the rows to be shared among
documents and models document-specific mixtures
over the rows of the matrix.6
SAGE for topic modeling (Eisenstein et al, 2011)
can be viewed as a restricted form of the SCTM.
Consider an SCTM in which the binary matrix is re-
stricted such that the first column, b?,1, consists of
all ones and the remainder forms a diagonal matrix.
If we then set the first component, ?1, to the cor-
pus background distribution, and add a Laplace prior
on the natural parameters, ?cv, we have the SAGE
model. Note that by removing the restriction that
the matrix contain a diagonal, we could allow mul-
tiple components to combine in the SCTM fashion,
while incorporating SAGE?s sparsity benefits.
6The IOMM uses Metropolis-Hastings (MH) to sample the
parameters of the experts. This approach is computationally
feasible because their experts are Gaussian, unlike the SCTM
in which the experts are multinomials and the MH step too ex-
pensive.
787
The relation of TagLDA (Zhu et al, 2006) to
the SCTM is similar to that of SAGE and SCTM.
TagLDA has a PoE of exactly two experts: one ex-
pert for the topic, and one for the supervised word-
level tag. Examples of tags are abstract or body,
indicating which part of a research paper the word
appears in.
Unlike the SCTM and SAGE, most prior exten-
sions to LDA have enhanced the distribution over
topics for each document. One of the closest is hier-
archical LDA (hLDA) (Blei et al, 2004) and its ap-
plication to PAM (Mimno et al, 2007). Though top-
ics are still generated independently from a Dirich-
let prior, hLDA learns a tree structure underlying
the topics. Each document samples a single path
through the tree and samples words from topics
along that path. The SCTM models an orthogonal
issue to topic hierarchy: how the topics themselves
are represented as the intersection of components.
Finally, while prior work has primarily used mix-
tures for the sake of conjugacy, we take a fundamen-
tally different approach to modeling the structure by
using normalized product distributions.
5 Evaluation
We compare the SCTM with LDA in terms of over-
all model performance (held-out perplexity) as well
as parameter usage (varying numbers of components
and topics). We select LDA as our baseline since our
model differs only in how it forms topics, which fo-
cuses evaluation on the benefit of this model change.
We consider two popular data sets for compar-
ison: NIPS: A collection of 1,617 NIPS abstracts
from 1987 to 19997, with 77,952 tokens and 1,632
types. 20NEWS: 1,000 randomly selected articles
from the 20 Newsgroups dataset,8 with 70,011 to-
kens and 1,722 types. Both data sets excluded stop
words and words occurring in fewer than 10 docu-
ments. For 20NEWS, we used the standard by-date
train/test split. For NIPS, we randomly partitioned
the data by document into 75% train and 25% test.
We compare the SCTM to LDA by evaluating
the average perplexity-per-word of the held-out test
7We follow prior work (Blei et al, 2004; Li and Mc-
Callum, 2006; Li et al, 2007) in using only the abstracts:
http://www.cs.nyu.edu/?roweis/data.html
8Williamson et al (2010) created a similar subset:
http://people.csail.mit.edu/jrennie/20Newsgroups/
data, perplexity = 2? log2(data|model)/N . Exact com-
putation is intractable, so we use the left-to-right al-
gorithm (Wallach et al, 2009) as an accurate alter-
native. With the topics fixed, the SCTM is equiva-
lent to LDA and requires no adaptation of the left-
to-right algorithm.
We used a collapsed Gibbs sampler for training
LDA and the algorithm described above for training
the SCTM. Both were trained for 4000 iterations,
sampling topics every 10 iterations after a burn-in of
3000. The hyperparameter ? was optimized as an
asymmetric Dirichlet, ? as a symmetric Dirichlet,
and ? = 3.0 was fixed.9 Following the observation of
Hinton (2002) that CD training benefits from initial-
izing the experts to nearly uniform distributions, we
initialize the component distributions from a sym-
metric Dirichlet with parameter ?? = 1?106. We use
J = 1 samples per iteration and a decaying learning
rate centered at ? = 100.10 We ranged LDA from 10
to 200 topics, and the SCTM from 10 to 100 com-
ponents (C). We then selected the number of SCTM
topics (K) as K ? {C, 2C, 3C, 4C, 5C}. For each
model, we used five random restarts, selecting the
model with the highest training data likelihood.
5.1 Results
Our goal is to demonstrate that (1) modeling topics
as products of components is an expressive alterna-
tive to generating topics independently and (2) the
SCTM can both achieve lower perplexity than LDA
and use fewer model parameters in doing so.
Topics as Products of Components Figures 3b
and 3c show the perplexity for the held-out portions
of 20NEWS and NIPS for different numbers of com-
ponents C. The shaded region shows the full SCTM
perplexity range we observed for different K and
at each value of C, we label the number of topics
K (rows in the binary matrix). For each number of
components, LDA falls within the upper portion of
the shaded region. While for some (small) values of
K for the SCTM, LDA does better, the SCTM can
easily include more K (requiring few new param-
eters) to achieve better results. This supports our
hypothesis that topics can be comprised of the over-
lap between shared underlying components. More-
9On development data the model was rather insensitive to ?.
10We experimented with larger J but it had no effect.
788
Figure 2: SCTM binary matrix and topics from 3599 training documents of 20NEWS for C = 10, K = 20. Blue
squares are ?on? (equal to 1).
x
y
5
10
15
20
2 4 6 8 10
k ?k Top words for topic Top words for topic after ablating component c=1
? 1 0.306 subject organization israel return define law org organization subject israel law peace define israeli
? 2 0.031 encryption chip clipper keys des escrow security law administration president year market money senior
? 3 0.025 turkish armenian armenians war turkey turks armenia years food center year air russian war army
? 4 0.102 drive card disk scsi hard controller mac drives opinions drive hard power support cost research price
? 5 0.071 image jpeg window display code gif color mit pitt file program year center programs image division
? 6 0.018 jews israeli jewish arab peace land war arabs
? 7 0.074 org money back question years thing things point
? 8 0.106 christian bible church question christ christians life
? 9 0.011 administration president year market money senior
? 10 0.055 health medical center research information april
? 11 0.063 gun law state guns control bill rights states
? 12 0.160 world organization system israel state usa cwru reply
? 13 0.042 space nasa gov launch power wire ground air
? 14 0.038 space nasa gov launch power wire ground air
? 15 0.079 team game year play games season players hockey
? 16 0.158 car lines dod bike good uiuc sun cars
? 17 0.136 windows file government key jesus system program
? 18 0.122 article writes center page harvard virginia research
? 19 0.017 max output access digex int entry col line
? 20 0.380 lines people don university posting host nntp time # of Model Parameters (thousands)
Perp
lexi
ty
800
1000
1200
1400
l
l
l
l
l
l
l
l
l
10
100
11
120 140
201
40
60 80
10,2010,30
10,4010,50
100,200
100,300
100,400100,500
20,100
20,40
20,60
20,80
40,120
40,16040,200
40,80
60,120
60,180
60,24060,300
80,160
80,240
80,32080,400
0 100 200 300 400 500 600
l LDASCTM
(a)
# of Components
Perp
lexi
ty
800
1000
1200
1400
1600
1800
l
l
l
ll
l
l
l
10
2030
4050
100
200
300400500
100
20
40
6080
120
160200
40
80
120
180240300
60
160
240320400
80
0 20 40 60 80 100
l LDASCTM
(b)
# of Components
Perp
lexi
ty
300
400
500
600
700
l
l
l
ll
l
l
l
10
20
304050
100
2003400500
100
20
40
60
80
120160200
40
80
120
180240300
60
160240320400
80
0 20 40 60 80 100
l LDASCTM
(c)
# of Model Parameters (thousands)
Perp
lexit
y
300
350
400
450
500
550
600
l
l
l
l l l l
l
l
l
l
l
l
10
100
11
120 140 160 180
20
200
21
40
60
80
10,20
10,3010,4010,50
100,200100,300100,400100,500
20,100
20,40
20,60
20,80
40,120
40,16040,200
40,80
60,120
60,18060,24060,3 0
80,16080,240
80,32080,400
0 100 200 300 400
l LDASCTM
(d)
Figure 3: Perplexity results on held-out data for 20NEWS (b) and NIPS (c) showing the results of LDA and the SCTM
for the same number of components and varying K (SCTM). For the same number of components (multinomials), the
SCTM achieves lower perplexity by combining them into more topics. Results for 20NEWS (a) and NIPS (d) showing
non-square SCTM achieves lower perplexity than LDA with a more compact model.
over, this suggests that our products (PoEs) provide
additional and complementary expressivity over just
mixtures of topics.
Model Compactness Including an additional
topic in the SCTM only adds C binary parameters,
for an extra row in the matrix. Whereas in LDA, an
additional topic requires V (the size of the vocab-
ulary) additional parameters to represent the multi-
nomial. In both cases, the number of document-
specific parameters must increase as well. Figures
3a and 3d present held-out perplexity vs. number
of model parameters on 20NEWS and NIPS, exclud-
ing the case of square (C = K) binary matrices for
the SCTM. The regions show a confidence inter-
val (p = 0.05) around the smoothed fit to the data,
LDA labels show C, and SCTM labels show C,K.
The SCTM achieves lower perplexity with fewer
model parameters, even when the increase in non-
component parameters is taken into account. We ex-
pect that because of its smaller size the SCTM ex-
hibits lower sample complexity, allowing for better
generalization to unseen data.
5.2 Analysis
Figure 2 gives the binary matrix and topics learned
on a larger section of 20NEWS training documents.
These topics evidence that the SCTM is able to
achieve a diversity of topics by combining various
subsets of components, and we expect that the low
perplexity achieved by the SCTM can be attributed
789
k=12 ?k=0.13
problem statecontrolreinforcementproblems modelstime baseddecision markovsystems function
k=11 ?k=0.08
learningnetworks systemrecognition timenetworkdescribes handcontext viewsclassification
k=14 ?k=0.07
models imagesimage problemstructureanalysis mixtureclusteringapproach showcomputational
k=13 ?k=0.05
networksnetwork learningdistributedsystem weightvectors propertybinary pointoptimal real
k=16 ?k=0.11
training unitspaper hiddennumber outputproblem rule setorder unit showpresent methodweights task
k=15 ?k=0.12
cells neuronsvisual cortexmotion responseprocessingspatial cellpropertiespatterns spike
k=18 ?k=0.07
informationanalysiscomponent rulessignalindependentrepresentationsnoise basis
k=17 ?k=0.10
numberfunctionsweights functionlayergeneralizationerror resultsloss linear size
k=20 ?k=0.02
time networkweightsactivation delaycurrent chaoticconnecteddiscreteconnections
k=19 ?k=0.03
system networksset neuronsvisual phasefeatureprocessingfeatures outputassociative
c=1
modelinformationparameterskalman robustmatriceslikelihoodexperimentally
c=2
networknetworks datalearning optimallinear vectorindependentbinary naturalalgorithms pca
c=4
paper unitsoutput layernetworkspatterns unitpattern set rulenetwork rulesweights training
c=9
visual imageimages cellscortex scenesupport spatialfeature visioncues stimulusstatistics
k=10 ?k=0.09
neural neuronsanalog synapticneuron networksmemory timecapacity modelassociativenoise dynamics
k=9 ?k=0.02
vector featureclassificationsupport vectorskernelregressionweight inputsdimensionality
k=2 ?k=0.13
network inputinformation timerecurrent backpropagationunitsarchitectureforward layer
k=1 ?k=0.11
model learningsysteminformationparametersnetworks robustkalman rulesestimation
k=4 ?k=0.12
bayesianresults showestimationmethod basedparameterslikelihoodmethods models
k=3 ?k=0.06
objectrecognitionsystem objectsinformationvisual matchingproblem basedclassification
k=6 ?k=0.23
neural networkpaperrecognitionspeech systemsbased resultsperformanceartificial
k=5 ?k=0.04
objectrecognitionsystem objectsinformationvisual matchingproblem basedclassification
k=8 ?k=0.23
algorithmtraining errorfunction methodperformanceinputclassificationclassifier
k=7 ?k=0.08
data papernetworks networkoutput featurefeaturespatterns settrain introducedunit functions
Figure 4: Hasse diagram on NIPS for C = 10, K = 20 showing the top words for topics and unrepresented com-
ponents (in shaded box). Notice that some topics only consist of a single component. The shaded box contains the
components that didn?t appear as a topic. For the sake of clarity, we only show arrows for the subsumption rela-
tionships between the topics, and we omit the implicit arrows between the components in the shaded box and the
topics.
to the high-level of component re-use across topics.
Topics are typically interpreted by looking at the
top-N words, whereas the top-N words of a compo-
nent often do not even appear in the topics to which
it contributes. Instead, we find that the components
contribution to a topic is typically through vetoing
words. For example, the top words of component
c=1, corresponding to the first column of the binary
matrix in figure 2, are [subject organization posting apple mit
screen write window video port], yet only a few of these ap-
pear in topics k=1,2,3,4,5, which use it.
On the right of figure 2, we show what the top-
ics become when we ablate component c=1 from
the matrix by setting the column to all zeros. Topic
k=2 changes from being about information security
to general politics and is identical to k=9. Topic k=3
changes from the Turkish-Armenian War to a more
general war topic. Topic k=4 changes to a less fo-
cused version of itself. In this way, we can gain fur-
ther insight into the contribution of this component,
and the way in which components tend to increase
the specificity of a topic to which they are added.
The SCTM learns each topic as a soft intersec-
tion of its components, as represented by the binary
matrix. We can describe the overlap between topics
based on the components that they have in common.
One topic subsumes another topic when the parent
consists of a subset of the child?s components. In
this way, the binary matrix defines a Hasse diagram,
a directed acyclic graph describing all the subsump-
tion relationships between topics. Figure 4 shows
such a Hasse diagram on the NIPS data. Several top-
ics consist of only a single component, such as k=12
on reinforcement learning and k=8 on optimization.
These two topics combine with the component c=1
so that their overlap forms the topic k=4 on Bayesian
methods. These subsumption relationships are dif-
ferent from and complementary to hLDA (see ?4),
which models topic co-occurrence, not component
intersection. For example, topic k=10 on connec-
tionism and k=2 on neural networks intersect to
form k=20 which contains words that would only
appear in both of its subsuming topics, thereby ex-
plicitly modeling topic overlap.
790
The SCTM sometimes learns identical topics (two
rows with the same binary entries ?on?) such as
k=13 and k=14 in figure 2 and k=3 and k=5 in fig-
ure 4, which is likely due to the Gibbs sampler for
the binary matrix getting stuck in a local optimum.
6 Discussion
We have presented the Shared Components Topic
Model (SCTM), in which topics are products of
underlying component distributions. This model
change learns shared topic structures?as expressed
through components?as opposed to generating
each topic independently. Reducing the number of
components yields more compact models with lower
perplexity than LDA. The two main limitations of
the current SCTM are, when restricted to a square
binary matrix (C = K), the inference procedure is
unable to recover a model with perplexity as low as
a collapsed Gibbs sampler for LDA, and the compo-
nents are not consistently interpretable.
The use of components opens up interesting di-
rections of research. For example, task specific side
information can be expressed as priors or constraints
over the components, or by adding conditioning
variables tied to the components. Additionally, tasks
beyond document modeling may benefit from repre-
senting topics as products of distributions. For ex-
ample, in vision, where topics are classes of objects,
the components could be features of those objects.
For selectional preference, components could cor-
respond to semantic features that intersect to define
semantic classes (Gormley et al, 2011). We hope
new opportunities will arise as this work explores a
new research area for topic models.
Appendix A: Derivation of Full Conditionals
The model?s complete data likelihood over all
variables?observed words X , latent topic assign-
ments Z, matrix B, and component/expert distribu-
tions ?:
p(X,Z,B,?|?,?, ?) =
p(X|Z,B,?)p(Z|?)p(B|?)p(?|?) (4)
This follows from the conditional independence as-
sumptions. It is tractable to integrate out all parame-
ters except Z,B,? and hyperparameters ?,?, ?. 11
11For simplicity, we switch from indexing examples as xmn
to xi. In this presentation, xi is the ith example in the corpus,
Full conditional of zi Recall that p(Z|?) is
the Dirichlet-Multinomial distribution over topic
assignments, where ? has been integrated out.
The form of this distribution is identical to the
corresponding distribution over topics in LDA.
The derivation of the full conditional of zi ?
{1, . . . ,K}, follows from the factorization in Eq. 4:
p(zi|X,Z
?(i),B,?,?,?, ?) (5)
? p(X|Z,B,?)p(Z|?) (6)
? p(xi|bzi ,?)(n?
?(i)
mzi + ?zi) (7)
Z?(i) is the set of all topic assignments except zi.
We use the independence of each document, recall-
ing that example i belongs to document m. In prac-
tice, we cache p(x|bz,?) for all x, z (V ?K values)
and these are shared by all zi in a sampling iteration.
Above, just as in LDA, p(Z|?) is simplified by
proportionality to (n??(i)mzi + ?zi), where n?
?(i)
mk is the
count of examples for document m that are assigned
topic k excluding zi?s contribution (Heinrich, 2008).
Full conditional of bkc Recall that p(B|?) is the
prior for a Beta-Bernoulli matrix. The full condi-
tional distribution of a position in the binary vector
is (Griffiths and Ghahramani, 2006):
p(bkc = 1|B
?(kc), ?) =
n??(k)c +
?
C
K + ?C
(8)
where n??(k)c is the count of topics with component
c excluding topic k, and B?(kc) is the entire matrix
except for the entry bkc.
To find the full conditional for bkc ? {0, 1}, we
again start with the factorization from Eq. 4.
p(bkc|X,Z,B
?(kc),?,?,?, ?) (9)
? p(X|Z,B,?)p(B|?) (10)
?
[
?
i:zi=k
p(xi|bzi ,?)
]
p(bkc|B
?(kc), ?) (11)
where p(bkc|B?(kc), ?) is given by Eq. 8,
=
?
?
?
(?V
v=1 ?
n?kv
cv
)bkc
(?V
v=1
?C
j=1 ?
bkj
jv
)?||n?k||1
?
?
? p(bkc|B
?(kc), ?)
(12)
and where n?kv is the count of words assigned topic
k that are type v, and ||n?k||1 (the L1-norm of count
vector n?k) is the count of all words with topic k.
which corresponds to some m,n pair.
791
References
David Blei and John Lafferty. 2006. Correlated topic
models. In Advances in Neural Information Process-
ing Systems (NIPS), volume 18.
David Blei and Jon McAuliffe. 2007. Supervised topic
models. In Advances in Neural Information Process-
ing Systems (NIPS).
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet alocation. Journal of Machine Learning
Research, 3.
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2004. Hierarchical topic models
and the nested chinese restaurant process. In Advances
in Neural Information Processing Systems (NIPS), vol-
ume 16.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Interna-
tional Conference on Machine Learning (ICML).
Matthew R. Gormley, Mark Dredze, Benjamin Van
Durme, and Jason Eisner. 2011. Shared components
topic models with application to selectional prefer-
ence. In Learning Semantics Workshop at NIPS 2011,
December.
Thomas Griffiths and Zoubin Ghahramani. 2006. Infinite
latent feature models and the indian buffet process. In
Advances in Neural Information Processing Systems
(NIPS), volume 18.
Gregor Heinrich. 2008. Parameter estimation for text
analysis. Technical report, Fraunhofer IGD.
Katherine A. Heller and Zoubin Ghahramani. 2007. A
nonparametric bayesian approach to modeling over-
lapping clusters. In Artificial Intelligence and Statis-
tics (AISTATS), pages 187?194.
Geoffrey Hinton. 1999. Products of experts. In In-
ternational Conference on Artificial Neural Networks
(ICANN).
Geoffrey Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural Computa-
tion, 14(8):1771?1800.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In International Conference on Machine Learn-
ing (ICML), pages 577?584.
Wei Li, David Blei, and Andrew McCallum. 2007. Non-
parametric bayes pachinko allocation. In Uncertainty
in Artificial Intelligence (UAI).
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with pachinko alloca-
tion. In International Conference on Machine Learn-
ing (ICML), pages 633?640.
John Paisley, Chong Wang, and David Blei. 2011. The
discrete infinite logistic normal distribution for Mixed-
Membership modeling. In International Conference
on Artificial Intelligence and Statistics (AISTATS).
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Hanna Wallach, Ian Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In International Conference on Machine
Learning (ICML), pages 1105?1112.
Greg Wei and Martin Tanner. 1990. A monte carlo im-
plementation of the EM algorithm and the poor man?s
data augmentation algorithms. Journal of the Ameri-
can Statistical Association, 85(411):699?704.
Sinead Williamson, Chong Wang, Katherine Heller, and
David Blei. 2010. The IBP compound dirichlet
process and its application to focused topic model-
ing. In International Conference on Machine Learn-
ing (ICML).
Xiaojin Zhu, David Blei, and John Lafferty. 2006.
TagLDA: bringing document structure knowledge into
topic models. Technical Report TR-1553, University
of Wisconsin.
792
Proceedings of NAACL-HLT 2013, pages 168?178,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Drug Extraction from the Web:
Summarizing Drug Experiences with Multi-Dimensional Topic Models
Michael J. Paul and Mark Dredze
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{mpaul,mdredze}@cs.jhu.edu
Abstract
Multi-dimensional latent text models, such as
factorial LDA (f-LDA), capture multiple fac-
tors of corpora, creating structured output for
researchers to better understand the contents
of a corpus. We consider such models for
clinical research of new recreational drugs and
trends, an important application for mining
current information for healthcare workers.
We use a ?three-dimensional? f-LDA variant
to jointly model combinations of drug (mari-
juana, salvia, etc.), aspect (effects, chemistry,
etc.) and route of administration (smoking,
oral, etc.) Since a purely unsupervised topic
model is unlikely to discover these specific
factors of interest, we develop a novel method
of incorporating prior knowledge by leverag-
ing user generated tags as priors in our model.
We demonstrate that this model can be used
as an exploratory tool for learning about these
drugs from the Web by applying it to the task
of extractive summarization. In addition to
providing useful output for this important pub-
lic health task, our prior-enriched model pro-
vides a framework for the application of f-
LDA to other tasks.
1 Introduction
Topic models aid exploration of the main thematic
elements of large text corpora by revealing latent
structure and producing a high level semantic view
(Blei et al, 2003). Topic models have been used for
understanding the contents of a corpus and identify-
ing interesting aspects of a collection for more in-
depth analysis (Talley et al, 2011; Mimno, 2011).
While standard topic models assume a flat seman-
tic structure, there are potentially many dimen-
sions of a corpus that contribute to word choice,
such as sentiment, perspective and ideology (Mei et
al., 2007; Paul and Girju, 2010; Eisenstein et al,
2011). Rather than studying these factors in isola-
tion, multi-dimensional topic models can consider
multiple factors jointly.
Paul and Dredze (2012b) introduced factorial
LDA (f-LDA), a general framework for multi-
dimensional text models that capture an arbitrary
number of factors (explained in ?3). While a stan-
dard topic model learns distributions over ?topics?
in documents, f-LDA learns distributions over com-
binations of multiple factors (e.g. topic, perspec-
tive) called tuples (e.g. (HEALTHCARE,LIBERAL)).
While f-LDA can model factors without supervision,
it has not been used in situations where the user has
prior information about the factors.
In this paper we consider a setting where the user
has prior knowledge about the end application: min-
ing recreational drug trends from user forums, an
important clinical research problem (?2). We show
how to incorporate available information from these
forums into f-LDA as a novel hierarchical prior over
the model parameters, guiding the model toward the
desired output (?3.1).
We then demonstrate the model?s utility in ex-
ploring a corpus in a targeted manner by using it
to automatically extract interesting sentences from
the text, a simple form of extractive multi-document
summarization (Goldstein et al, 2000). In the
same way that topic models can be used for aspect-
specific summarization (Titov and McDonald, 2008;
Haghighi and Vanderwende, 2009), we use f-LDA
to extract snippets corresponding to fine-grained in-
formation patterns. Our results demonstrate that our
multi-dimensional modeling approach targets more
informative text than a simpler model (?4).
168
2 Analyzing Drug Trends on the Web
Recreational drug use imposes a significant burden
on the health infrastructure of the United States and
other countries. Accurate information on drugs, us-
age profiles and side effects are necessary for sup-
porting a range of healthcare activities, such as ad-
diction treatment programs, toxin diagnosis, preven-
tion and awareness campaigns, and public policy.
These activities rely on up-to-date information on
drug trends, but it is increasingly difficult to keep
up with current drug information, as distribution and
information-sharing of novel drugs is easier than
ever via the web (Wax, 2002). For the third con-
secutive year, a record number of new drugs (49)
were detected in Europe in 2011 (EMCDDA, 2012).
About two-thirds of these new drugs were synthetic
cannabinoids (used as legal marijuana substitutes),
which led to 11,000 hospitalizations in the U.S. in
2010 (SAMHSA, 2012). Treatment is complicated
by the fact that novel substances like these may have
unknown side effects and other properties.
Accurate information on drug trends can be ob-
tained by speaking directly with users, e.g. focus
groups and interviews (Reyes et al, 2012; Hout
and Bingham, 2012), but such studies are slow and
costly, and can fail to identify the emergence of
new drug classes, such as mephedrone (Dunn et
al., 2011). More recently, researchers have begun
to recognize clinical value in information obtained
from the web (Corazza et al, 2011). By (manu-
ally) analyzing YouTube videos, Drugs-Forum (dis-
cussed below), and other social media websites and
online communities, researchers have uncovered de-
tails about the use, effects, and popularity of a va-
riety of new and emerging drugs (Morgan et al,
2010; Corazza et al, 2012; Gallagher et al, 2012),
and comprehensive drug reviews now include non-
standard sources such as web forums in addition to
standard sources (Hill and Thomas, 2011).
Organizing and understanding forums requires
significant effort. We propose automated tools to aid
in the exploration and analysis of these data. While
topic models are a natural fit for corpus exploration
(Eisenstein et al, 2012; Chaney and Blei, 2012), and
have been used for similar public health applications
(Paul and Dredze, 2011), online forums can be orga-
nized in many ways beyond topic. Guided by do-
Factor Components
Drug ALCOHOL AMPHETAMINES BETA-KETONES
CANNABINOIDS CANNABIS COCAINE DMT DOWN-
ERS DXM ECSTASY GHB HERBAL ECSTASY KE-
TAMINE KRATOM LSA LSD NOOTROPICS OPIATES
PEYOTE PHENETHYLAMINES SALVIA TOBACCO
Route INJECTION ORAL SMOKING SNORTING
Aspect CHEMISTRY (Pharmacology, TEK)
CULTURE (Culture, Setting, Social, Spiritual)
EFFECTS (Effects)
HEALTH (Health, Overdose, Side effects)
USAGE (Dose, Storing, Weight)
Table 1: The three factors of our model (details in ?3.1).
The forum tags shown in parentheses are grouped to-
gether to form aspects.
main experts, we seek to model forums as a combi-
nation of drug type, route of intake (oral, injection,
etc.) and aspect (cultural settings, drug chemistry,
etc.) A multi-dimensional topic model can jointly
capture these factors, providing a more informative
understanding of the data, and can be used to pro-
duce fine-grained information such as the effects of
taking a particular drug orally. Our hope is that mod-
els such as f-LDA can lead to exploratory tools that
aide researchers in learning about new drugs.
2.1 Corpus: Drugs-Forum
Our data set is taken from drugs-forum.com, a
site active for more than 10 years with over 100,000
members and more than 1 million monthly readers.
The site is an information hub where people can
freely discuss recreational drugs with psychoactive
effects, ranging from coffee to heroin, hosting in-
formation and discussions on specific drugs, as well
as drug-related politics, law, news, recovery and ad-
diction. With current information on a variety of
drugs and an extensive archive, Drugs-Forum pro-
vides an ideal information source for public health
researchers (Corazza et al, 2012).
Discussion threads are organized into numerous
forums, including drugs, the law, addiction, etc.
Since we are modeling drug use, we focus on the
drug forums. Each thread is assigned to a specific
forum or subforum (drug) and each thread has a user
specified tag, which can indicate categories like ?Ef-
fects? as well as routes of administration like ?Oral.?
We organized the tags and subforum categorizations
into factors and components, as shown in Table 1.
We make use of these tags in ?3.1.
169
3 Multi-Dimensional Text Models
Clinical researchers are interested in specific infor-
mation about drug usage, including drug type, route
of administration, and other aspects of drug use
(e.g. dosage, side effects). Rather than considering
these factors independently, we would like to model
these in a way that can capture interesting interac-
tions between all three factors, because the effects
and other aspects of drugs can vary by route of ad-
ministration. Oral consumption of drugs often pro-
duces longer lasting but milder effects than injec-
tion or smoking, for example. Many mephedrone
users report nose bleeds and nasal pain as a health
effect of snorting the drug: this could be modeled
as the triple (MEPHEDRONE,SNORTING,HEALTH), a
particular combination of all three factors.
To this end, we utilize the multi-dimensional text
model factorial LDA (f-LDA) (Paul and Dredze,
2012b), which jointly models multiple semantic fac-
tors or dimensions. In this section we summarize f-
LDA, then we describe an extension which incorpo-
rates user-generated metadata into the model (?3.1).
In a standard topic model such as LDA (Blei et
al., 2003), each word token is associated with a la-
tent ?topic? variable. f-LDA is conceptually similar
to LDA except that rather than a single topic vari-
able, each token is associated with a K-dimensional
vector of latent variables. In a three-dimensional f-
LDA model, each token has three latent variables?
drug, route, and aspect in this case.
In f-LDA, each document has a distribution
over all possible K-tuples (rather than topics),
and each K-tuple is associated with its own word
distribution. Under this model, words are gen-
erated by first sampling a tuple from the docu-
ment?s tuple distribution, then sampling a word
from that tuple?s word distribution. In our three-
dimensional model, we will consider triples such as
(CANNABIS,SMOKING,EFFECTS).
Formally, each document has a distribution ?(d)
over triples, and each token is associated with a la-
tent vector ~z of sizeK=3. (We?ll describe the model
in terms of the three factors we are modeling in this
paper, but f-LDA generalizes toK dimensions.) The
Cartesian product of the three factors forms a set
of triples and the vector ~z references three discrete
components to form a triple ~t = (t1, t2, t3). The car-
?
?d
? ?
? ?
b?
z w
DN
KK
K
?
k Zk
?
k Zk
?
k Zk
Figure 1: The graphical model for f-LDA augmented
with priors ? learned from labeled data (?3.1). In this
work, K = 3.
dinality of each dimension (denoted Zk) is the num-
ber of drugs, routes, and aspects, as shown in Table
1. Each triple has a corresponding word distribution
?~t. The graphical model is shown in Figure 1.
One would expect that triples that have com-
ponents in common should have similar word
distributions: (CANNABIS,SMOKING,EFFECTS)
is expected to have some commonalities with
(CANNABIS,ORAL,EFFECTS). f-LDA models this
intuition by sharing parameters across priors for
triples which share components: all triples with
CANNABIS as the drug include cannabis-specific
parameters in the prior, and all triples with SMOK-
ING as the route have smoking-specific parameters.
Formally, ?~t (the word distribution for tuple ~t) has a
Dirichlet(??(~t)) prior, where for each word w in the
vector, ??(
~t)
w is a log-linear function:
??(~t )w , exp
(
?(B)+?(0)w +?
(drug)
t1w +?
(route)
t2w +?
(aspect)
t3w
)
(1)
where ?(B) is a corpus-wide precision scalar (the
bias), ?(0)w is a corpus-specific bias for word w, and
?(k)tkw is a bias parameter for word w for component
tk of the kth factor. That is, each drug, route, and
aspect has a weight vector over the vocabulary, and
the prior for a particular triple is influenced by the
weight vectors of each of the three factors. The
? parameters are all independent and normally dis-
tributed around 0 (effectively L2 regularization).
The prior over each document?s distribution over
triples has a similar log-linear prior, where weights
for each factor are combined to influence the dis-
tribution. Under our model, ?(d) is drawn from
Dirichlet(B ? ??(d)), where ? denotes an element-wise
product between B (described below) and ??(d), with
170
??(d)~t for each triple
~t defined as:
??(d)~t , exp
(
?(B) +?(D,drug)t1 +?
(d,drug)
t1
+?(D,route)t2 +?
(d,route)
t2
+?(D,aspect)t3 +?
(d,aspect)
t3
)
(2)
Similar to the ? formulation, ?(B) is a global
bias parameter, while the ?D vectors are corpus-
wide weight vectors and ?d are document-specific
weight vectors over the components of each fac-
tor. Structuring the prior in this way models the
intuition that if a triple with a particular compo-
nent has high probability, other triples containing
that component are likely to also have high proba-
bility. For example, if a message discusses triples
of the form (CANNABIS,*,EFFECTS), it is more
likely to discuss (CANNABIS,*,HEALTH) than (CO-
CAINE,*,HEALTH), because the message is about
cannabis.
Finally, B is a 3-dimensional array that encodes
a sparsity pattern over the space of possible triples.
This is used to accommodate triples that can be gen-
erated by the model but are not supported by the
data. For example, not all routes of administration
may be applicable to certain drugs, or certain aspects
of a drug may happen to not be discussed in the fo-
rum. Each element b~t of the array is a real-valued
scalar in (0, 1) which is multiplied with ??(d)~t to ad-
just the prior for that triple. If the b value is near
0 for a particular triple, then it will have very low
prior probability. The b values have Beta(?0,?1) pri-
ors (? < 1) which encourage them to be near 0 or 1,
so that they function as binary variables.
Posterior inference and parameter estimation con-
sist of a Monte Carlo EM algorithm that alternates
between an iteration of collapsed Gibbs sampler on
the ~z variables (E-step), and an iteration of gradi-
ent ascent on the ? and ? hyperparameters (M-step).
See Paul and Dredze (2012b) for more details.
3.1 Tags and Word Priors
In an unsupervised setting, there is no reason f-LDA
would actually infer parameters corresponding to
the three factors we have been describing. However,
the forums include metadata that can help guide the
model: the messages are organized into forums cor-
responding to drug type (factor 1), and some threads
COCAINE SNORTING HEALTH
? (Prior over ?)
coke snort kidney
cocaine snorting hcv
crack snorted pains
cola nose symptoms COCAINE
blow nasal guidelines SNORTING
lines drip diet HEALTH
? (Prior over ?) ? (Posterior)
coke snort symptoms nose
cocaine snorting long-term cocaine
crack snorted depression coke
cola passages disorder blood
rocks nostril schizophrenia water
coca insufflating severe pain
Figure 2: Example of parameters learned by f-LDA. The
highest weight words in the ? and ? vectors for three
components are shown on the left. These are combined
to form the prior for the word distribution ?. The tripling
of (COCAINE,SNORTING,HEALTH) results in high proba-
bility words about nose bleeds and nasal damage.
are tagged with labels corresponding to routes of ad-
ministration and other aspects (factors 2 and 3). Tags
for aspects are manually grouped into components:
e.g. USAGE (tags: Dose, Storing, Weight). Table 1
shows the factors and components in our model.
One could simply use these tags as labels in a sim-
ple supervised model?this will be our experimental
baseline (?4.1). However, this approach has limita-
tions in that most documents are missing labels (less
than a third of our corpus contains one of the labels
in Table 1) and many messages discuss several com-
ponents, not just the one implied by the tag. For
example, a message tagged ?Side effects? may talk
about both side effects and dosage. While a super-
vised classifier may attribute all words to a single
tag, f-LDA learns per-token assignments.
We will instead use the tags to inform the priors
over our f-LDA word distribution parameters. We
do this with a two-stage approach. First, we use the
tags to train parameters of a related but simplified
model. We then use the learned parameters as priors
over the corresponding f-LDA parameters.
In particular, we will place priors on the ? vectors,
the Dirichlet hyperparameters which influence the
word distributions. Suppose that we are given a vec-
tor ?(0) which is believed to contain desirable values
for ?(0), the weight vector over words in the corpus,
and similarly we are given vectors ?(f)i over the vo-
cabulary for the ith component of factor f , which
are believed to be good values for ?(f)i . One option
171
is to fix ? as ?, forcing the component weights to
match the provided weights. However, in our case ?
will only be an approximation of the optimal com-
ponent parameters since it is estimated from incom-
plete data (only some messages have tags) and the ?
vectors are learned using an approximate model (see
below). Instead, these weight vectors will merely
guide learning as prior knowledge over model pa-
rameters ?. While f-LDA assumes each ? is drawn
from a 0-mean Gaussian, we alter the means of the
appropriate ? parameters to use ?.
?(0)w ? N (?
(0)
w , ?
2);?(k)iw ? N (?
(k)
iw , ?
2) (3)
Recall that ?(0)w are corpus-wide bias parameters for
each word and ?(k)iw are component-specific param-
eters for each word. This yields a hierarchical prior
in which ? parameterizes the prior over ?, while ?
parameterizes the prior over ? (the word distribu-
tions). The resulting ? parameters can vary from the
provided priors to adapt to the data. An example of
learned parameters is shown in Figure 2, illustrating
the hierarchical process behind this model.
Learning the Priors In various applications, pri-
ors can come from many different sources, such as
labeled data (Jagarlamudi et al, 2012). We learn
the prior means ? from tagged messages. However,
these parameters imply a latent division of responsi-
bility for observed words: some are present because
of the tag while others are general words in the cor-
pus. As a result, they must be estimated.
We learn these parameters from the tagged mes-
sages using SAGE, which model words in a docu-
ment as combinations of background and topic word
distributions. Eisenstein et al (2011) present SAGE
models for Naive Bayes (one class per document),
admixture models (one class per token), and admix-
ture models where tokens come from multiple fac-
tors. We combine the first and third models, such
that a document has multiple factors which are given
as labels across the entire document?the drug type
and the tag, which could correspond to a component
of either the route or aspect factors. We posit the
following model of text generation per document:
P (word w|drug = i, factorf = j) (4)
=
exp(?(0)w + ?
(drug)
iw + ?
(f)
jw )
?
w? exp(?
(0)
w? + ?
(drug)
iw? + ?
(f)
jw?)
This log-linear model has a similar form as Eq.
1, but with two factors instead of three, and it is
a distribution rather than a Dirichlet vector. As in
SAGE, we fix ?(0) to be the observed vector of cor-
pus log-frequencies over the vocabulary, which acts
as an ?overall? weight vector, while parameter esti-
mation yields ?(f)i , the logit parameters for the ith
component of factor f .1 These parameters are then
used as the mean of the Gaussian priors over ?.
Standard optimization methods can be used to es-
timate these parameters. The partial derivative of the
likelihood with respect to the parameter ?(drug)iw is:
?
??(drug)iw
=
?
f
?
j?f
c(i, j, w)? pi(i, j, w)c(i, j, ?)
(5)
where c(i, j, w) is the number of times word w ap-
pears in documents labeled with i (drug) and j (tag),
and pi(i, j, w) denotes the probability given by (4).
The partial derivative of each ?(f)j is similar.
4 Experiments with Topic Modeling for
Extractive Summarization
Our corpus consists of messages from
drugs-forum.com (?2.1). The site catego-
rizes threads into many forums and subforums,
including some on specific drugs, which are cat-
egorized hierarchically. We treated higher-level
categories with pharmacologically similar drugs as
a single drug type (e.g. OPIOIDS, AMPHETAMINES);
for others we took the finest-granularity subforum
as the drug type. We selected 22 popular drugs and
from these forums we crawled 410K messages. We
selected a subset of tags to form components for
the route and aspect factors. (Some tags were too
general or infrequent to be useful.) A list of the
tags and drugs used appears in Table 1. We also
included a GENERAL component in the latter two
factors to model word usage which does not pertain
to a particular route or aspect; the prior parameters
? for these components were simply set to 0.
We wish to demonstrate that our modified f-LDA
model can be used to discover useful information in
the text. One way to demonstrate this is by using the
model to extract relevant snippets of text from the
1SAGE models sparsity on the weights via a Laplacian prior.
Such sparsity is not modeled in f-LDA, so we ignore this here.
172
forums, which will form the basis of our evaluation
experiments. Our goal is not to build a complete
summarization system, but rather to use the model
to direct researchers to interesting messages.
While we model all 22 drugs, our summa-
rization experiments will focus on five drugs
which have been studied only relatively recently:
mephedrone and MDPV (?-ketones), Bromo-
Dragonfly (synthetic phenethylamines), Spice/K2
(synthetic cannabinoids), and salvia divinorum. We
will consider these drugs in particular because these
are the five drugs for which technical reports were
created by the EU Psychonaut Project (Schifano et
al., 2006), an online database of novel and emerg-
ing drugs, whose information is collected by reading
drug websites, including Drugs-Forum. Extensive
technical reports were written about these five pop-
ular drugs, and we can use these reports to produce
reference summaries for our experiments (?4.2).
Of these five drugs, only salvia has its own sub-
forum; the others belong to subforums representing
the broader categories shown in parentheses. We
simply model the drug type as a proxy for the spe-
cific drug, as most of the drugs in each category have
similar effects and properties. The first two drugs are
both in the same subforum, so for the purpose of our
model we treat mephedrone and MDPV as the single
drug type, ?-ketones. These two drugs are grouped
together during summarization (?4.2), but the corre-
sponding reference summaries incorporate excepts
from the technical reports on both drugs.
4.1 Model Setup
Of the four drug types being considered for summa-
rization, our data set contains 12K messages with
one of the tags in Table 1 and 30K without. Of
those without tags, we set aside 5K as development
data. There are also over 300K messages (140K
tagged) from the remaining 18 drug types: some
of these messages are utilized when training f-LDA.
Even though we only consider four drug types in our
experiments, our intuition is that it can be benefi-
cial to model other drugs as well, because this will
help to learn parameters for the various aspects and
routes of administration. Our model of the effects of
mephedrone can be informed by also modeling the
effects of other stimulants such as cocaine.
Each message was treated as a document, and we
only used documents with at least five word tokens
after stop words, low-frequency words, and punc-
tuation were removed. The preprocessed data sets
contained an average of 45 tokens per document.
Below, we describe two f-LDA variants as well as
the baseline used in our experiments.
Baseline Our baseline model is a unigram lan-
guage model trained on the subset of messages
which are tagged. We treat the drug subforum as
a label for the drug factor, and each message?s tag
is used as a label for either the route or aspect fac-
tor. For example, the word distribution for the pair
(SALVIA,EFFECTS) is estimated as the empirical dis-
tribution from messages posted in the salvia forum
and tagged with ?Effects.? We use add-? smooth-
ing where ? is chosen to optimize likelihood on the
held-out development set.
This is a two-dimensional model, since we explic-
itly model pairs such as (MEPHEDRONE,SNORTING)
or (SALVIA,EFFECTS). However, we also cre-
ated word distributions for triples such as
(SALVIA,ORAL,EFFECTS) by taking a mixture
of the corresponding pairs: in this example, we
estimate the unigram distribution from salvia
documents tagged with either ?Oral? or ?Effects.?
Factorial LDA Because f-LDA does not rely on
tagged data (the tags are only used to create priors),
we can run inference on larger sets of data. The
drawback is that despite these priors, it is still mostly
unsupervised and we want to be careful to ensure
the model will learn the patterns we care about. We
thus add some reasonable constraints to the parame-
ter space to guide the model further.
First, we treat the drug type as an observed vari-
able based on the subforum the message comes
from, just as with the baseline. For example, only
tuples of the form (SALVIA,?,?) can be assigned to
tokens in the salvia forum. Second, we restrict the
set of possible routes of administration that can be
assigned to tokens in particular drug forums, since
most drugs can be taken through only a subset of
routes. For example, marijuana is typically smoked
or eaten orally, but rarely injected. We therefore
restrict each drug?s allowable set of administration
routes to those which are tagged (e.g. with ?Oral? or
?Snorting?) in at least 1% of that drug?s data. Sim-
ilar ideas are used in Labeled LDA (Ramage et al,
173
Reference Text System Snippet
Mephedrone (?-ketones/Bath salts)
It is recommended by users that Mephedrone
be taken on an empty stomach. Doses usually
vary between 100mg?1g.
? If it is SWIYs first time using Mephedrone SWIM recommends
a 100mg oral dose on an empty stomach.
Reported negative side effects include:
? Loss of appetite.
? Dehydration and dry mouth
? Tense jaw, mild muscle clenching, stiff neck,
and bruxia (teeth grinding)
? Anxiety and paranoia
? Increase in mean body temperature (sweat-
ing/Mephedrone sweat and hot flushes)
? Elevated heart rate (tachycardia) and blood
pressure, and chest pains
? Dermatitis like symptoms (Itch and rash)
? Neutral side effects: Lack of appetite, occasional loss of visual
focus, [...] weight loss, possible diuretic. Negative side effects:
Grinding teeth, ?Cotton mouth?, unable to acheive orgasm
? Aside from his last session he has never experienced any neg-
ative symptoms at all, no raised heart beat, vasoconstriction ,
sweating, headaches, paranoia e.t.c nothing at all except some-
times cold hands the next day.
? lot of people report that anxiety and paranoia are some of the
side effects of taking mephedrone [...] is it also possible that
alot of the chest pains people are experiencing is due to anxiety?
? moisturize the affected areas of skin twice daily with E45 or a
similar unperfumed dermatalogical lotion.
Salvia divinorum
Sublingual ingestion of the leaf (quid): reduces
intensity of effects and can taste disgusting.
When Salvia is consumed as a smokeable for-
mulation the duration of the trip lasts 30 min-
utes or less, whereas if Salvia is consumed sub-
lingually the effects lasts for 1 hour or more.
? The taste of sublingual salvia is foul and it is easy to have a dud
trip unless large amounts of it are used.
? SWIM has heard from many other users that chewing the fresh
leaves of the Salvia plant allow for a much longer and mellower
trip. [...] SWIM has read that a trip this way can last anywhere
from a half on hour or longer.
Dried leaves and/or salvia extract are smoked
(using a butane lighter) either by pipe (consid-
ered to be the most effective but is considered
to be quite painful) or water bong.
? 2. Use a water pipe. Its harsh and needs to be smoked hot so
this should be self explanatory. 3. Use a torch style lighter
[...] Salvinorin A has a VERY high boiling point (around 700
degrees F I believe) so a regular bic just wont do it
Salvia is appealing to recreational users be-
cause of intense, unique, hallucinatory effects.
Brief hallucinations occur rapidly after admin-
istration and are typically very vivid. Users re-
port weird thoughts, feelings of unreality, feel-
ings of immersion in bizarre non-Euclidian di-
mensions/geometries, feelings of floating.
? He noticed very clear [closed eye visuals], which looked similar
to patterns on a persian rug, or ethnic oriental design. SWIM
felt as if he was moving around, that he had got up and run and
fallen, and that falling had shattered the space around his body
as if I?d fallen through many glass framed pictures [...]
? I was aware of my body and my friends and my life below, but
I was [...] standing outside of time and outside of space.
Figure 3: Example snippets generated by f-LDA along with the corresponding reference text. For space, the references
and snippets shown have been shortened in some cases. ?SWIM? and ?SWIY? stand for ?someone who isn?t me/you?
and are used to avoid self-incrimination on the web forum.
2009), in which tags are used to restrict the space of
allowed topics in a document.
We use f-LDA as a three-dimensional model
which explicitly models triples, but we also obtain
distributions for pairs such as (SALVIA,EFFECTS) by
marginalizing across all distributions of the form
(SALVIA,?,EFFECTS). We trained f-LDA on two dif-
ferent data sets, yielding the following models:
? f-LDA-1: We use the 12K messages with tags
and fill the set out with 13K messages with tags
uniformly sampled from the 18 other drugs, for
a total of 25K messages.
? f-LDA-2: We use all 37K messages (many
without tags) and fill the set out with 63K mes-
sages with tags uniformly sampled from the 18
other drugs, for a total of 100K messages.
All f-LDA instances are run with 5000 iterations
alternating between a sweep of Gibbs sampling fol-
lowed by a step of gradient ascent on the hyperpa-
rameters. While we do not use the tags as strict la-
bels during sampling, we initialize the Gibbs sam-
pler so that each token in a document is assigned
to its label given by the tag, when available. In the
absence of tags (in f-LDA-2), we initialize tokens
174
to the GENERAL components. We initialized ? to
its prior mean (Eq. 3), while the variance ?2 and
the initialization of bias ?(B) are chosen to optimize
likelihood on the held-out development set.
We optimized the hyperparameters and sparsity
array using gradient descent after each Gibbs sweep.
We use a decreasing step size of a/(t+1000), where
t is the current iteration and a=10 for ? and 1 for
? and the sparsity values. To learn priors ?, we
ran our version of SAGE for 100 iterations of gra-
dient ascent (fixed step size of 0.1). See Paul and
Dredze (2012a) for examples of parameters (the top
words associated with various triples) learned by
this model on this corpus.
4.2 Summary Generation
We created twelve reference summaries by edit-
ing together excerpts from the five Psychonaut
Project reports ((Psychonaut), 2009). Each refer-
ence is matched to drug-specific pairs and triples.
For example, a paragraph describing the dif-
ferences in effects of salvia between smoking
and oral routes was matched to distributions for
(SALVIA,EFFECTS), (SALVIA,SMOKING,EFFECTS),
(SALVIA,ORAL,EFFECTS). Descriptions of creat-
ing tinctures and blotters for oral consumption were
matched to (SALVIA,ORAL,CHEMISTRY). We con-
sider pairs in addition to triples because not all sum-
maries correspond to particular routes or aspects.
For each tuple-specific word distribution (a pair or
a triple), we create a ?summary? by extracting a set
of five text snippets which minimize KL-divergence
to the target word distribution. We consider all over-
lapping text windows of widths {10,15,20} in the
corpus as candidate snippets. Following Haghighi
and Vanderwende (2009), we greedily add snippets
one by one with the lowest KL-divergence at each
step until we have added five.
We only considered candidate snippets within the
subforum for the particular drug, and snippets are
based on the preprocessed topic model input with no
stop words. Before presenting snippets to users, we
then map the snippets back to the raw text by taking
all sentences which are at least partly spanned by the
window of tokens. Because each reference may be
matched to more than one tuple, there may be more
than five snippets which correspond to a reference.
1 2 3 4 50
50
100
150
200
250
Cou
nt
Histogram of Annotator Scores
RandomBaselinef-LDA-1f-LDA-2
Figure 4: The distribution of annotator scores (?4.3.1).
The ?Random? counts have been scaled to fit the same
range as the other systems, since fewer random snippets
were shown to annotators.
4.3 Experimental Results
Recall that the reports used as reference summaries
were themselves created by reading web forums.
Our hypothesis is that f-LDA could be used as an
exploratory tool to expedite the creation of these re-
ports. Thus in our evaluation we want to measure
how useful the extracted snippets would be in in-
forming the writing of such reports. We performed
both human and automatic evaluation on the sum-
maries generated by f-LDA (variants 1 and 2) as well
as our baseline. We also included randomly selected
snippets as a control (five per reference).
Example output is shown in Figure 3.
4.3.1 Human Judgments of Quality
Three annotators were presented snippets pooled
from all four systems we are evaluating alongside
the corresponding reference text. Within each set
corresponding to a reference summary, the snippets
were shown in a random order. Annotators were
asked to judge each snippet independently on a 5-
point Likert scale as to how useful each snippet
would be in writing the reference text.
The distribution of scores is shown in Figure 4 and
summarized in Table 2. Annotators generally agreed
on the relative quality of snippets: the average cor-
relation of scores between each pair of annotators
was 0.49. Snippets produced by f-LDA were given
more high scores and fewer low scores than the base-
line, while the two f-LDA variants were rated com-
parably. The breakdown is more interesting when
we compare scores for snippets that were matched
175
Rand. Base. f-LDA-1 f-LDA-2
Annotator Scores
Mean 1.67 2.55 2.79 2.81
Pairs only n/a 2.58 2.79 2.72
Triples only n/a 2.50 2.80 2.95
ROUGE
1-gram .112 .326 .355 .327
2-gram .023 .072 .085 .084
Table 2: Summary quality evaluation across four systems.
to word distributions for pairs versus word distri-
butions for triples. The gap in scores between f-
LDA and the baseline increases when we look at the
scores for only triples: f-LDA beats the baseline by
a margin of 0.45 for snippets matched to triples and
0.21 for pairs. This suggests that we produce better
triples by modeling them jointly. For triples, f-LDA-
2 (which uses more data) beats f-LDA-1 (which uses
only tagged data), while the reverse is true for pairs.
While some of the randomly selected control
snippets happened to be useful, the scores for these
snippets were much lower than those extracted
through model-based systems. This suggests that
exploring the forums in a targeted way (e.g. through
our topic model approach) would be more efficient
than exploring the data in a non-targeted way (akin
to the random approach).
Finally, we asked two expert annotators (faculty
members in psychiatry and behavioral pharmacol-
ogy, who have used drug forums in the past to study
emerging drugs) to rate the snippets corresponding
to mephedrone/MDPV. The best f-LDA system had
an average score of 2.57 compared to a baseline
score of 2.45 and random score of 1.63.
4.3.2 Automatic Evaluation of Recall
The human judgments effectively measured a
form of precision, as the quality of snippets were
judged by their correspondence to the reference text,
without regard to how much of the reference text
was covered by all snippets. We also used the au-
tomatic evaluation metric ROUGE (Lin, 2004) as a
rough estimate of summary recall: this metric com-
putes the percentage of n-grams in the reference text
that appeared in the generated summaries.
We computed ROUGE for both 1-grams and 2-
grams. When computing n-gram counts, we applied
Porter?s stemmer to all tokens. We excluded stop
words from 1-gram counts but included them in 2-
gram counts where we care about longer phrases.2
Results are shown in Table 2. We find that f-LDA-
1 has the highest score for both 1- and 2-grams, sug-
gesting that it is extracting a more diverse set of
relevant snippets. When performing a paired t-test
across the 12 reference summaries, we find that f-
LDA is better than the baseline with p-values 0.14
and 0.10 for 1-gram and 2-gram recall, respectively.
f-LDA?s recall advantage may come from the fact
that it learns from a larger amount of data and it
may learn more diverse word distributions by di-
rectly modeling triples. f-LDA-1 had slightly better
recall (under ROUGE), while f-LDA-2 was slightly
better according to the human annotators.
5 Conclusion
We have proposed exploratory tools for the analy-
sis of online drug communities. Such communi-
ties are an emerging source of drug research, but
manually browsing through large corpora is imprac-
tical and important information could be missed.
We have demonstrated that topic models are capa-
ble of modeling informative portions of text, and in
particular multi-dimensional topic models can tar-
get desired structures such as the combination of as-
pect and route of administration for each drug. We
have presented an extension to factorial LDA tai-
lored to a particular application and data set which
was demonstrated to induce desired properties. As
a technical contribution, this study lays out practical
guidelines for customizing and incorporating prior
knowledge into multi-dimensional text models.
Acknowledgments
We are grateful to Dr. Margaret S. Chisolm and
Dr. Ryan Vandrey from the Johns Hopkins School
of Medicine for providing the mephedrone/MDPV
annotations, and Alex Lamb and Hieu Tran for as-
sisting with the full annotations. We also thank Dr.
Matthew W. Johnson for additional advice, and the
anonymous reviewers for helpful feedback and sug-
gestions. This research was partly supported by an
NSF Graduate Research Fellowship.
2In both cases, ROUGE scores were higher when stop words
were included. f-LDA beats the baseline by similar margins
regardless of whether we include stop words.
176
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. JMLR.
A. Chaney and D. Blei. 2012. Visualizing topic models.
In ICWSM.
O. Corazza, F. Schifano, M. Farre, P. Deluca, Z. Davey,
C. Drummond, M. Torrens, Z. Demetrovics, L. Di Fu-
ria, L. Flesland, et al 2011. Designer drugs on the
Internet: a phenomenon out-of-control? The emer-
gence of hallucinogenic drug Bromo-Dragonfly. Cur-
rent Clinical Pharmacology, 6(2):125?129.
Ornella Corazza, Fabrizio Schifano, Pierluigi Simonato,
Suzanne Fergus, Sulaf Assi, Jacqueline Stair, John
Corkery, Giuseppina Trincas, Paolo Deluca, Zoe
Davey, Ursula Blaszko, Zsolt Demetrovics, Jacek
Moskalewicz, Aurora Enea, Giuditta di Melchiorre,
Barbara Mervo, Lucia di Furia, Magi Farre, Liv Fles-
land, Manuela Pasinetti, Cinzia Pezzolesi, Agnieszka
Pisarska, Harry Shapiro, Holger Siemann, Arvid
Skutle, Aurora Enea, Giuditta di Melchiorre, Elias
Sferrazza, Marta Torrens, Peer van der Kreeft, Daniela
Zummo, and Norbert Scherbaum. 2012. Phenomenon
of new drugs on the Internet: the case of ketamine
derivative methoxetamine. Human Psychopharmacol-
ogy: Clinical and Experimental, 27(2):145?149.
Matthew Dunn, Raimondo Bruno, Lucinda Burns, and
Amanda Roxburgh. 2011. Effectiveness of and chal-
lenges faced by surveillance systems. Drug Testing
and Analysis, 3(9):635?641.
J. Eisenstein, A. Ahmed, and E. P. Xing. 2011. Sparse
additive generative models of text. In ICML.
Jacob Eisenstein, Duen Horng ?Polo? Chau, Aniket Kit-
tur, and Eric P. Xing. 2012. Topicviz: Semantic
navigation of document collections. In CHI Work-in-
Progress Paper.
EMCDDA. 2012. 2012 annual report on the state of
the drugs problem in Europe. European Monitoring
Centre for Drugs and Drug Addiction, Lisbon.
Cathal T. Gallagher, Sulaf Assi, Jacqueline L. Stair,
Suzanne Fergus, Ornella Corazza, John M. Corkery,
and Fabrizio Schifano. 2012. 5,6-methylenedioxy-2-
aminoindane: from laboratory curiosity to ?legal high?.
Human Psychopharmacology: Clinical and Experi-
mental, 27(2):106?112.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization
by sentence extraction. In Proceedings of the 2000
NAACL-ANLP Workshop on Automatic summariza-
tion, pages 40?48.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
NAACL ?09: Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362?370.
Simon L. Hill and Simon H. L. Thomas. 2011. Clin-
ical toxicology of newer recreational drugs. Clinical
Toxicology, 49(8):705?719.
Marie Claire Van Hout and Tim Bingham. 2012. Costly
turn on: Patterns of use and perceived consequences of
mephedrone based head shop products amongst Irish
injectors. International Journal of Drug Policy.
Jagadeesh Jagarlamudi, Hal Daume? III, and Raghavendra
Udupa. 2012. Incorporating lexical priors into topic
models. In EACL.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July.
Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In WWW.
David Mimno. 2011. Reconstructing Pompeian house-
holds. In UAI.
Elizabeth M. Morgan, Chareen Snelson, and Patt Elison-
Bowers. 2010. Image and video disclosure of sub-
stance use on social media websites. Computers in
Human Behavior, 26(6):1405?1411. Online Interac-
tivity: Role of Technology in Behavior Change.
Michael J. Paul and Mark Dredze. 2011. You are what
you Tweet: Analyzing Twitter for public health. In 5th
International AAAI Conference on Weblogs and Social
Media (ICWSM).
Michael J. Paul and Mark Dredze. 2012a. Experiment-
ing with drugs (and topic models): Multi-dimensional
exploration of recreational drug discussions. In AAAI
2012 Fall Symposium on Information Retrieval and
Knowledge Discovery in Biomedical Text.
Michael J. Paul and Mark Dredze. 2012b. Factorial
LDA: Sparse multi-dimensional text models. In Neu-
ral Information Processing Systems (NIPS).
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics. In
AAAI.
Psychonaut WebMapping Research Group (Psycho-
naut). 2009. Bromo-Dragonfly, MDPV, Spice,
Mephodrone, and Salvia Divinorum reports.
http://www.psychonautproject.eu/technical.php.
Institute of Psychiatry, King?s College London.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In EMNLP, pages 248?256.
J. Reyes, J. Negro?n, H. Colo?n, A. Padilla, M. Milla?n,
T. Matos, and R. Robles. 2012. The emerging of
177
xylazine as a new drug of abuse and its health con-
sequences among drug users in Puerto Rico. Journal
of Urban Health, pages 1?8.
SAMHSA. 2012. The DAWN report.
http://www.samhsa.gov/data/2k12/DAWN105/SR105-
synthetic-marijuana.pdf, December 4.
Fabrizio Schifano, Paolo Deluca, Alex Baldacchino,
Teuvo Peltoniemi, Norbert Scherbaum, Marta Tor-
rens, Magi Farro?, Irene Flores, Mariangela Rossi,
Dorte Eastwood, Claude Guionnet, Salman Rawaf,
Lisa Agosti, Lucia Di Furia, Raffaella Brigada, Aino
Majava, Holger Siemann, Mauro Leoni, Antonella
Tomasin, Francesco Rovetto, and A. Hamid Ghodse.
2006. Drugs on the web: the Psychonaut 2002 EU
project. Progress in Neuro-Psychopharmacology and
Biological Psychiatry, 30(4):640 ? 646.
Edmund Talley, David Newman, Bruce Herr II, Hanna
Wallach, Gully Burns, Miriam Leenders, and Andrew
McCallum. 2011. A database of National Institutes
of Health (NIH) research using machine learned cate-
gories and graphically clustered grant awards. Nature
Methods.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Interna-
tional World Wide Web Conference (WWW), Beijing.
P.M. Wax. 2002. Just a click away: Recreational drug
web sites on the Internet. Pediatrics, 109(6).
178
Proceedings of NAACL-HLT 2013, pages 685?690,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
What?s in a Domain? Multi-Domain Learning for Multi-Attribute Data
Mahesh Joshi? Mark Dredze? William W. Cohen? Carolyn P. Rose??
? School of Computer Science, Carnegie Mellon University
Pittsburgh, PA, 15213, USA
? Human Language Technology Center of Excellence, Johns Hopkins University
Baltimore, MD, 21211, USA
maheshj@cs.cmu.edu,mdredze@cs.jhu.edu
wcohen@cs.cmu.edu,cprose@cs.cmu.edu
Abstract
Multi-Domain learning assumes that a sin-
gle metadata attribute is used in order to di-
vide the data into so-called domains. How-
ever, real-world datasets often have multi-
ple metadata attributes that can divide the
data into domains. It is not always apparent
which single attribute will lead to the best do-
mains, and more than one attribute might im-
pact classification. We propose extensions to
two multi-domain learning techniques for our
multi-attribute setting, enabling them to si-
multaneously learn from several metadata at-
tributes. Experimentally, they outperform the
multi-domain learning baseline, even when it
selects the single ?best? attribute.
1 Introduction
Multi-Domain Learning (Evgeniou and Pontil,
2004; Daume? III, 2007; Dredze and Crammer, 2008;
Finkel and Manning, 2009; Zhang and Yeung, 2010;
Saha et al, 2011) algorithms learn when training in-
stances are spread across many domains, which im-
pact model parameters. These algorithms use exam-
ples from each domain to learn a general model that
is also sensitive to individual domain differences.
However, many data sets include a host of meta-
data attributes, many of which can potentially define
the domains to use. Consider the case of restaurant
reviews, which can be categorized into domains cor-
responding to the cuisine, location, price range, or
several other factors. For multi-domain learning, we
should use the metadata attribute most likely to char-
acterize a domain: a change in vocabulary (i.e. fea-
tures) that most impacts the classification decision
(Ben-David et al, 2009). This choice is not easy.
First, we may not know which metadata attribute is
most likely to fit this role. Perhaps the location most
impacts the review language, but it could easily be
the price of the meal. Second, multiple metadata
attributes could impact the classification decision,
and picking a single one might reduce classification
accuracy. Therefore, we seek multi-domain learn-
ing algorithms which can simultaneously learn from
many types of domains (metadata attributes).
We introduce the multi-attribute multi-domain
(MAMD) learning problem, in which each learning
instance is associated with multiple metadata at-
tributes, each of which may impact feature behavior.
We present extensions to two popular multi-domain
learning algorithms, FEDA (Daume? III, 2007) and
MDR (Dredze et al, 2009). Rather than selecting
a single domain division, our algorithms consider
all attributes as possible distinctions and discover
changes in features across attributes. We evaluate
our algorithms using two different data sets ? a data
set of restaurant reviews (Chahuneau et al, 2012),
and a dataset of transcribed speech segments from
floor debates in the United States Congress (Thomas
et al, 2006). We demonstrate that multi-attribute al-
gorithms improve over their multi-domain counter-
parts, which can learn distinctions from only a single
attribute.
2 MAMD Learning
In multi-domain learning, each instance x is drawn
from a domain d with distribution x ? Dd over a
vectors space RD and labeled with a domain spe-
cific function fd with label y ? {?1,+1} (for bi-
nary classification). In multi-attribute multi-domain
685
(MAMD) learning, we have M metadata attributes in
a data set, where the mth metadata attribute has Km
possible unique values which represent the domains
induced by that metadata attribute. Each instance xi
is drawn from a distribution xi ? Da specific to a
set of attribute values Ai associated with each in-
stance. Additionally, each unique set of attributes
indexes a function fA.1 Ai could contain a value for
each attribute, or no values for any attribute (which
would index a domain-agnostic ?background? distri-
bution and labeling function). Just as a domain can
change a feature?s probability and behavior, so can
each metadata attribute.
Examples of data for MAMD learning abound. The
commonly used Amazon product reviews data set
(Blitzer et al, 2007) only includes product types, but
the original reviews can be attributed with author,
product price, brand, and so on. Additional exam-
ples include congressional floor debate records (e.g.
political party, speaker, bill) (Joshi et al, 2012). In
this paper, we use restaurant reviews (Chahuneau et
al., 2012), which have upto 20 metadata attributes
that define domains, and congressional floor de-
bates, with two attributes that define domains.
It is difficult to apply multi-domain learning algo-
rithms when it is unclear which metadata attribute
to choose for defining the ?domains?. It is possible
that there is a single ?best? attribute to use for defin-
ing domains, one that when used in multi-domain
learning will yield the best classifier. To find this
attribute, one must rely on one?s intuition about the
problem,2 or perform an exhaustive empirical search
over all attributes using some validation set. Both
these strategies can be brittle, because as the nature
of data changes over time so may the ?best? do-
main distinction. Additionally, multi-domain learn-
ing was not designed to benefit from multiple helpful
attributes.
We note here that Eisenstein et al (2011), as well
as Wang et al (2012), worked with a ?multifaceted
topic model? using the framework of sparse addi-
tive generative models (SAGE). Both those models
capture interactions between topics and multiple as-
1Distributions and functions that share attributes could share
parameters.
2Intuition is often critical for learning and in some cases can
help, such as in the Amazon product reviews data set, where
product type clearly corresponds to domain. However, for other
data sets the choice may be less clear.
pects, and can be adapted to the case of MAMD. While
our problem formulation has significant conceptual
overlap with the SAGE?like multifaceted topic mod-
els framework, our proposed methods are motivated
from a fast online learning perspective.
A naive approach for MAMD would be to treat ev-
ery unique set of attributes as a domain, including
unique proper subsets of different attributes to ac-
count for the case of missing attributes in some in-
stances.3 However, introducing an exponential num-
ber of domains requires a similar increase in train-
ing data, clearly an infeasible requirement. Instead,
we develop multi-attribute extensions for two multi-
domain learning algorithms, such that the increase
in parameters is linear in the number of metadata at-
tributes, and no special handling is required for the
case where some metadata attributes might be miss-
ing from an instance.
Multi-Attribute FEDA The key idea behind
FEDA (Daume? III, 2007) is to encode each domain
using its own parameters, one per feature. FEDA
maps a feature vector x in RD to RD(K+1). This
provides a separate parameter sub-space for every
domain k ? 1 . . .K, and also maintains a domain-
agnostic shared sub-space. Essentially, each feature
is duplicated for every instance in the appropriate
sub-space of RD(K+1) that corresponds to the in-
stance?s domain. We extend this idea to the MAMD
setting by using one parameter per attribute value.
The original instance x ? RD is now mapped into
RD(1+
?
mKm); a separate parameter for each at-
tribute value and a shared set of parameters. In ef-
fect, for every metadata attribute a ? Ai, the original
features are copied into the appropriate sub-space.
This grows linearly with the number of metadata at-
tribute values, as opposed to exponentially in our
naive solution. While this is still substantial growth,
each instance retains the same feature sparsity as in
the original input space. In this new setup, FEDA al-
lows an instance to contribute towards learning the
shared parameters, and the attribute-specific param-
eters for all the attributes present on an instance. Just
like multi-domain FEDA, any supervised learning al-
gorithm can be applied to the transformed represen-
tation.
3While we used a similar setup for formulating our problem,
we did not rule out the potential for factoring the distributions.
686
Multi-Attribute MDR We make a similar change
to MDR (Dredze et al, 2009) to extend it for
the MAMD setting. In the original formulation,
Dredze et al used confidence-weighted (CW)
learning (Dredze et al, 2008) for learning shared
and domain-specific classifiers, which are combined
based on the confidence scores associated with the
feature weights. For training the MDR approaches in
a multi-domain learning setup, they found that com-
puting updates for the combined classifier and then
equally distributing them to the shared and domain-
specific classifiers was the best strategy, although it
approximated the true objective that they aimed to
optimize. In our multi-attribute setup confidence-
weighted (CW) classifiers are learned for each of the
?
mKm attribute values in addition to a shared CW
classifier. At classification time, a combined clas-
sifier is computed for every instance. However, in-
stead of combining the shared classifier and a single
domain-specific classifier, we combine the shared
CW classifier and |Ai| different attribute value-
specific CW classifiers associated with xi. The
combined classifier is found by minimizing the KL-
divergence of the combined classifier with respect to
each of the underlying classifiers.4
When learning the shared and domain-specific
classifiers, we follow the best result in Dredze et
al. and use the ?averaged update? strategy (?7.3 in
Dredze et al), where updates are computed for the
combined classifier, and are then distributed to the
shared and domain-specific classifiers. MDR-U will
indicate that the updates to the combined classifiers
are uniformly distributed to the underlying shared
and domain-specific classifiers.
Dredze et al also used another scheme called
?variance? to distribute the combined update to the
underlying classifiers (?4, last paragraph in Dredze
et al) Their idea was to give a lower portion
of the update to the underlying classifier that has
higher variance (or in their terminology, ?less con-
fidence?) since it contributed less to the combined
classifier. We refer to this as MDR-V. However, this
conflicts with the original CW intuition that features
with higher variance (lower confidence) should re-
ceive higher updates; since they are more in need
of change. Therefore, we implemented a modi-
fied ?variance? scheme, where the updates are dis-
4We also tried the l2 distance method of Dredze et al (2009)
but it gave consistently worse results.
tributed to the underlying classifiers such that higher
variance features receive the larger updates. We re-
fer to this as MDR-NV. We observed significant im-
provements with this modified scheme.
3 Experiments
To evaluate our multi-attribute algorithms we con-
sider two datasets. First, we use two subsets of the
restaurant reviews dataset (1,180,308 reviews) intro-
duced by Chahuneau et al (2012) with the goal of
labeling reviews as positive or negative. The first
subset (50K-RND) randomly selects 50,000 reviews
while the second (50K-BAL) is a class-balanced
sample. Following the approach of Blitzer et al
(2007), scores above and below 3-stars indicated
positive and negative reviews, while 3-star reviews
were discarded. Second, we use the transcribed seg-
ments of speech from the United States Congress
floor debates (Convote), introduced by Thomas
et al (2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discussion
in the floor debate.
In the WordSalad datasets, each restaurant re-
view can have many metadata attributes, including a
unique identifier, name (which may not be unique),
address (we extract the zipcode), and type (Italian,
Chinese, etc.). We select the 20 most common meta-
data attributes (excluding latitude, longitude, and the
average rating). 5 In the Convote dataset, each
speech segment is associated with the political party
affiliation of the speaker (democrat, independent, or
republican) and the speaker identifier (we use bill
identifiers for creating folds in our 10-fold cross-
validation setup).
In addition to our new algorithms, we evalu-
ate several baselines. All methods use confidence-
weighted (CW) learning (Crammer et al, 2012).
BASE A single classifier trained on all the data,
and which ignores metadata attributes and uses uni-
gram features. For CW, we use the best-performing
setting from Dredze et al (2008) ? the ?variance?
algorithm, which computes approximate but closed?
form updates, which also lead to faster learning. Pa-
rameters are tuned over a validation set within each
training fold.
5Our method requires categorical metadata attributes, al-
though real-valued attributes can be discretized.
687
metadata 1-META FEDA MDR-U MDR-V MDR-NV
5
0
K
-
R
N
D NONE (BASE) 92.29 (?0.14)
ALL (META) ? 92.69 (?0.10)
CATEGORY ? 92.48 (?0.11) 92.47 (?0.10) ?? 92.99 (?0.12) 91.16 (?0.16) ?? 93.24 (?0.13)
ZIPCODE 92.40 (?0.09) ? 92.73 (?0.09) ?? 92.99 (?0.12) 91.19 (?0.20) ?? 93.22 (?0.11)
NEIGHBORHOOD 92.42 (?0.11) ? 92.65 (?0.13) ?? 93.02 (?0.13) 91.17 (?0.21) ?? 93.21 (?0.12)
5
0
K
-
B
A
L NONE (BASE) 89.95 (?0.10)
ALL (META) ? 90.39 (?0.09)
CATEGORY 90.09 (?0.11) ? 90.50 (?0.11) ? 90.60 (?0.11) 87.89 (?0.13) ?? 91.33 (?0.08)
ZIPCODE 89.97 (?0.12) ? 90.42 (?0.13) ? 90.56 (?0.09) 87.78 (?0.16) ?? 91.30 (?0.10)
ID ? 90.42 (?0.11) ?? 90.64 (?0.11) ? 90.50 (?0.11) 87.78 (?0.25) ?? 91.27 (?0.09)
Table 1: Average accuracy (? standard error) for the best three metadata attributes, when using a single attribute at
a time. Results that are numerically the best within a row are in bold. Results significantly better than BASE are
marked with ?, and better than META are marked with ?. Significance is measured using a two-tailed paired t-test with
? = 0.05.
#attributes FEDA MDR-U MDR-V MDR-NV
5
0
K
-
R
N
D MAMD ?? 93.07 (?0.19) ?? 93.12 (?0.11) 87.08 (?1.72) ?? 93.19 (?0.12)
1-ORCL ?? 93.06 (?0.11) ?? 93.17 (?0.11) 92.37 (?0.11) ?? 93.39 (?0.12)
1-TUNE ? 92.64 (?0.12) ? 92.81 (?0.16) 92.15 (?0.17) ?? 93.07 (?0.14)
1-MEAN ? 92.61 (?0.09) ? 92.59 (?0.10) 91.41 (?0.12) ? 92.58 (?0.10)
5
0
K
-
B
A
L MAMD ?? 91.42 (?0.09) ?? 91.06 (?0.04) 81.43 (?2.79) ?? 91.40 (?0.08)
1-ORCL ?? 90.89 (?0.10) ?? 90.87 (?0.11) 89.33 (?0.13) ?? 91.45 (?0.07)
1-TUNE ? 90.33 (?0.10) ?? 90.70 (?0.14) 89.13 (?0.16) ?? 91.26 (?0.08)
1-MEAN ? 90.30 (?0.06) 89.92 (?0.07) 88.25 (?0.07) 90.06 (?0.08)
Table 2: Average accuracy (? standard error) using 10-fold cross-validation for methods that use all attributes, either
directly (our proposed methods) or for selecting the ?best? single attribute using one of the strategies described earlier.
Formatting and significance symbols are the same as in Table 1.
META Identical to BASE with a unique bias feature
added for each attribute value (Joshi et al, 2012).
1-META A special case of META where a unique
bias feature is added only for a single attribute.
To use multi-domain learning directly, we could
select a single attribute as the domain. We consider
several strategies for picking this attribute and eval-
uate both FEDA and MDR in this setting.
1-MEAN Choose an attribute randomly, equivalent
to the expected (mean) error over all attributes.
1-TUNE Select the best performing attribute on a
validation set.
1-ORCL Select the best performing attribute on
the test set. Though impossible in practice, this gives
the oracle upper bound on multi-domain learning.
All experiments use ten-fold cross-validation. We
report the mean accuracy, along with standard error.
4 Results
Table 1 shows the results of single-attribute multi-
domain learning methods for the WordSalad
datasets. The table shows the three best-performing
metadata attributes (as decided by the highest accu-
racy among all the methods across all 20 metadata
attributes). Clearly, several of the attributes can pro-
vide meaningful domains, which demonstrates that
methods that can select multiple attributes at once
are desirable. We also see that our modification to
MDR (MDR-NV) works the best.
Table 3 shows the results of single-attribute multi-
domain learning methods for the Convote dataset.
The first observation to be made on this dataset is
that neither the PARTY, nor the SPEAKER attribute
individually achieve significant improvement over
the META baseline, which uses both these attributes
as features. This is in contrast with the results on
the WordSalad dataset, where some attributes by
themselves showed an improvement over the META
baseline. Thus, this dataset represents a more chal-
lenging setup for our multi?attribute multi?domain
learning methods ? they need to exploit the two
weak attributes simultaneously.
We next demonstrate multi-attribute improve-
ments over the multi-domain baselines (Tables 2
and 4). For WordSalad datasets, our exten-
sions that can use all metadata attributes simul-
taneously are consistently better than both the
1-MEAN and the 1-TUNE strategies (except for
the case of the old variance scheme used by
(Dredze et al, 2009)). For the skewed subset
688
metadata 1-META FEDA MDR-U MDR-V MDR-NV
NONE (BASE) 67.08 (?1.74)
ALL (META) ? 82.60 (?1.95)
PARTY ? 78.81 (?1.47) ? 84.19 (?2.44) ? 83.23 (?2.48) ? 81.38 (?2.22) ? 83.92 (?2.31)
SPEAKER ? 77.49 (?1.75) ? 82.88 (?2.43) ? 78.32 (?1.91) 62.43 (?2.20) ? 72.26 (?1.37)
Table 3: Convote: Average accuracy (? standard error) when using a single attribute at a time. Results that are
numerically the best within a row are in bold. Results significantly better than BASE are marked with ?, and better
than META are marked with ?. Significance is measured using a two-tailed paired t-test with ? = 0.05.
#attributes FEDA MDR-U MDR-V MDR-NV
MAMD ?? 85.71 (?2.74) ? 84.12 (?2.56) 50.44 (?1.78) ?? 86.19 (?2.49)
1-ORCL ? 84.77 (?2.47) ? 83.88 (?2.27) ? 81.38 (?2.22) ? 83.92 (?2.31)
1-TUNE ? 84.19 (?2.44) ? 83.23 (?2.48) ? 81.38 (?2.22) ? 83.92 (?2.31)
1-MEAN ? 83.53 (?2.40) ? 80.77 (?1.92) ? 71.91 (?1.82) ? 78.09 (?1.69)
Table 4: Convote: Average accuracy (? standard error) using 10-fold cross-validation for methods that use all
attributes, either directly (our proposed methods) or for selecting the ?best? single attribute using one of the strategies
described earlier. Formatting and significance symbols are the same as in Table 3.
50K-RND, MAMD+FEDA is significantly better than
1-TUNE+FEDA; MAMD+MDR-U is significantly bet-
ter than 1-TUNE+MDR-U; MAMD+MDR-NV is not
significantly different from 1-TUNE+MDR-U. For
the balanced subset 50K-BAL, a similar pattern
holds, except that MAMD+MDR-NV is significantly
better than 1-TUNE+MDR-NV. Clearly, our multi-
attribute algorithms provide a benefit over existing
approaches. Even with oracle knowledge of the test
performance using multi-domain learning, we can
still obtain improvements (FEDA and MDR-U in the
50K-BAL set, and all the Convote results, except
MDR-V).
Although MAMD+MDR-NV is not significantly bet-
ter than 1-TUNE+MDR-NV on the 50K-RND set,
we found that in every single fold in our ten-
fold cross-validation experiments, the ?best? single
metadata attribute decided using a validation set did
not match the best-performing single metadata at-
tribute on the corresponding test set. This shows
the potential instability of choosing a single best at-
tribute. Also, note that MDR-NV is a variant that we
have proposed in the current work, and in fact for
the earlier variant of MDR (MDR-U), as well as for
FEDA, we do see significant improvements when us-
ing all metadata attributes. Furthermore, the compu-
tational cost of evaluating every metadata attribute
independently to tune the single best metadata at-
tribute can be high and often impractical. Our ap-
proach requires no such tuning. Finally, observe
that for FEDA, the 1-TUNE strategy is not signifi-
cantly different from 1-MEAN, which just randomly
picks a single best metadata attribute. For MDR-U,
1-TUNE is significantly better than 1-MEAN on the
balanced subset 50K-BAL, but not on the skewed
subset 50K-RND.
As mentioned earlier, the Convote dataset is a
challenging setting for our methods due to the fact
that no single attribute is strong enough to yield im-
provements over the META baseline. In this setting,
both MAMD+FEDA and MAMD+MDR-NV achieve a
significant improvement over the META baseline,
with MDR-NV being the best (though not signif-
icantly better than FEDA). Additionally, both of
them are significantly better than their correspond-
ing 1-TUNE strategies. This result further supports
our claim that using multiple attributes in combi-
nation for defining domains (even when any single
one of them is not particularly beneficial for multi?
domain learning) is important.
5 Conclusions
We propose multi-attribute multi-domain learning
methods that can utilize multiple metadata attributes
simultaneously for defining domains. Using these
methods, the definition of ?domains? does not have
to be restricted to a single metadata attribute. Our
methods achieve a better performance on two multi-
attribute datasets as compared to traditional multi-
domain learning methods that are tuned to use a sin-
gle ?best? attribute.
Acknowledgments
This research is supported by the Office of Naval
Research grant number N000141110221.
689
References
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440?447.
Association for Computational Linguistics.
Victor Chahuneau, Kevin Gimpel, Bryan R. Routledge,
Lily Scherlis, and Noah A. Smith. 2012. Word
Salad: Relating Food Prices and Descriptions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning (EMNLP 2012).
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification for
text categorization. Journal of Machine Learning Re-
search (JMLR).
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263. Association for Computational Linguistics.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ?08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ?08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1?
2):123?149.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse Additive Generative Models of Text. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning (ICML).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi?task learning. In Proceedings of
the 2004 ACM SIGKDD international conference on
Knowledge discovery and data mining - KDD ?04.
Jenny R Finkel and Christopher D Manning. 2009. Hier-
archical Bayesian Domain Adaptation. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 602?
610. Association for Computational Linguistics.
Mahesh Joshi, Mark Dredze, William W. Cohen, and Car-
olyn P. Rose?. 2012. Multi-domain learning: When do
domains matter? In Proceedings of EMNLP-CoNLL
2012, pages 1302?1312.
Avishek Saha, Piyush Rai, Hal Daume? III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
William Yang Wang, Elijah Mayfield, Suresh Naidu, and
Jeremiah Dittmar. 2012. Historical Analysis of Legal
Opinions with a Sparse Mixed-Effects Latent Variable
Model. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (ACL
2012).
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
690
Proceedings of NAACL-HLT 2013, pages 789?795,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Separating Fact from Fear: Tracking Flu Infections on Twitter
Alex Lamb, Michael J. Paul, Mark Dredze
Human Language Technology Center of Excellence
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
{alamb3,mpaul19,mdredze}@jhu.edu
Abstract
Twitter has been shown to be a fast and reli-
able method for disease surveillance of com-
mon illnesses like influenza. However, previ-
ous work has relied on simple content anal-
ysis, which conflates flu tweets that report
infection with those that express concerned
awareness of the flu. By discriminating these
categories, as well as tweets about the authors
versus about others, we demonstrate signifi-
cant improvements on influenza surveillance
using Twitter.
1 Introduction
Twitter is a fantastic data resource for many tasks:
measuring political (O?Connor et al, 2010; Tumas-
jan et al, 2010), and general sentiment (Bollen et
al., 2011), studying linguistic variation (Eisenstein
et al, 2010) and detecting earthquakes (Sakaki et
al., 2010). Similarly, Twitter has proven useful for
public health applications (Dredze, 2012), primar-
ily disease surveillance (Collier, 2012; Signorini et
al., 2011), whereby public health officials track in-
fection rates of common diseases. Standard govern-
ment data sources take weeks while Twitter provides
an immediate population measure.
Strategies for Twitter influenza surveillance in-
clude supervised classification (Culotta, 2010b; Cu-
lotta, 2010a; Eiji Aramaki and Morita, 2011), un-
supervised models for disease discovery (Paul and
Dredze, 2011), keyword counting1, tracking geo-
graphic illness propagation (Sadilek et al, 2012b),
and combining tweet contents with the social net-
work (Sadilek et al, 2012a) and location informa-
1The DHHS competition relied solely on keyword counting.
http://www.nowtrendingchallenge.com/
tion (Asta and Shalizi, 2012). All of these methods
rely on a relatively simple NLP approach to analyz-
ing the tweet content, i.e. n-gram models for classi-
fying related or not related to the flu. Yet examining
flu tweets yields a more complex picture:
? going over to a friends house to check on her son.
he has the flu and i am worried about him
? Starting to get worried about swine flu...
Both are related to the flu and express worry, but
tell a different story. The first reports an infec-
tion of another person, while the second expresses
the author?s concerned awareness. While infection
tweets indicate a rise in infection rate, awareness
tweets may not. Automatically making these dis-
tinctions may improve influenza surveillance, yet re-
quires more than keywords.
We present an approach for differentiating be-
tween flu infection and concerned awareness tweets,
as well as self vs other, by relying on a deeper analy-
sis of the tweet. We present our features and demon-
strate improvements in influenza surveillance.
1.1 Related Work
Much of the early work on web-based influenza
surveillance relied on query logs and click-through
data from search engines (Eysenbach, 2006), most
famously Google?s Flu Trends service (Ginsberg et
al., 2008; Cook et al, 2011). Other sources of in-
formation include articles from the news media and
online mailing lists (Brownstein et al, 2010).
2 Capturing Nuanced Trends
Previous work has classified messages as being re-
lated or not related to influenza, with promising
surveillance results, but has ignored nuanced differ-
ences between flu tweets. Tweets that are related to
789
flu but do not report an infection can corrupt infec-
tion tracking.
Concerned Awareness vs. Infection (A/I) Many
flu tweets express a concerned awareness as opposed
to infection, including fear of getting the flu, an
awareness of increased infections, beliefs related to
flu infection, and preventative flu measures (e.g. flu
shots.) Critically, these people do not seem to have
the flu, whereas infection tweets report having the
flu. This distinction is similar to modality (Prab-
hakaran et al, 2012a). Conflating these tweets can
hurt surveillance, as around half of our annotated
flu messages were awareness. Identifying awareness
tweets may be of use in-and-of itself, such as for
characterizing fear of illness (Epstein et al, 2008;
Epstein, 2009), public perception, and discerning
sentiment (e.g. flu is negative, flu shots may be pos-
itive.) We focus on surveillance improvements.2
Self vs. Other (S/O) Tweets for both awareness
and infection can describe the author (self) or oth-
ers. It may be that self infection reporting is more
informative. We test this hypothesis by classifying
tweets as self vs. other.
Finding Flu Related Tweets (R/U) We must first
identify messages that are flu related. We construct
a classifier for flu related vs. unrelated.
3 Features
Token sequences (n-grams) are an insufficient fea-
ture set, since our classes share common vocabular-
ies. Consider,
? A little worried about the swine flu epidemic!
? Robbie might have swine flu. I?m worried.
Both tweets mention flu and worried, which distin-
guish them as flu related but not specifically aware-
ness or infection, nor self or other. Motivated by
Bergsma et al (2012), we complement 3-grams with
additional features that capture longer spans of text
and generalize using part of speech tags. We begin
by processing each tweet using the ARK POS tag-
ger (Gimpel et al, 2011) and find phrase segmen-
tations using punctuation tags.3 Most phrases were
two (31.2%) or three (26.6%) tokens long.
2While tweets can both show awareness and report an in-
fection, we formulate a binary task for simplicity since only a
small percentage of tweets were so labeled.
3We used whitespace for tokenization, which did about the
same as Jerboa (Van Durme, 2012).
Class Name Words in Class
Infection getting, got, recovered, have, hav-
ing, had, has, catching, catch, cured,
infected
Possession bird, the flu, flu, sick, epidemic
Concern afraid, worried, scared, fear, worry,
nervous, dread, dreaded, terrified
Vaccination vaccine, vaccines, shot, shots, mist,
tamiflu, jab, nasal spray
Past Tense was, did, had, got, were, or verb with
the suffix ?ed?
Present Tense is, am, are, have, has, or verb with
the suffix ?ing?
Self I, I?ve, I?d, I?m, im, my
Others your, everyone, you, it, its, u, her,
he, she, he?s, she?s, she, they, you?re,
she?ll, he?ll, husband, wife, brother,
sister, your, people, kid, kids, chil-
dren, son, daughter
Table 1: Our manually created set of word class features.
Word Classes For our task, many word types can
behave similarly with regard to the label. We create
word lists for possessive words, flu related words,
fear related words, ?self? words, ?other? words, and
fear words (Table 1). A word?s presence triggers a
count-based feature corresponding to each list.
Stylometry We include Twitter-specific style fea-
tures. A feature is included for retweet, hashtags,
and mentions of other users. We include a feature
for emoticons (based on the emoticon part-of-speech
tag). We include a more specific feature for positive
emoticons (:) :D :)). We also include a feature
for negative emoticons (:( :/). Additionally, we
include a feature for links to URLs.
Part of Speech Templates We include features
based on a number of templates matching specific
sequences of words, word classes, and part of speech
tags. Where any word included in the template
matches a word in one of the word classes, an ad-
ditional feature is included indicating that the word
class was included in that template.
? Tuples of (subject,verb,object) and pairs of (sub-
ject, verb), (subject, object), and (verb, object). We
use a simple rule to construct these tuples: the first
noun or pronoun is taken as the subject, and the first
verb appearing after the subject is taken as the verb.
The object is taken as any noun or pronoun that ap-
pears before a verb or at the end of a phrase.
790
? A pairing of the first pronoun with last noun.
These are useful for S/O, e.g. I am worried that my
son has the flu to recognize the difference between
the author (I) and someone else.
? Phrases that begin with a verb (pro-drop). This is
helpful for S/O, e.g. getting the flu! which can indi-
cate self even without a self-related pronoun. An ad-
ditional feature is included if this verb is past-tense.
? Numeric references. These often indicate aware-
ness (number of people with the flu) and are gen-
erally not detected by an n-gram model. We add a
separate feature if the word following has the root
?died?, e.g. So many people dying from the flu, I?m
scared!
? Pair of first pronoun/noun with last verb in a
phrase. Many phrases have multiple verbs, but the
last verb is critical, e.g. I had feared the flu. Ad-
ditional features are added if the noun/pronoun is in
the ?self? or ?other? word class, and if the verb is in
the ?possessive? word class.
? Flu appears as a noun before first verb in a phrase.
This indicates when flu is a subject, which is more
likely to be about awareness.
? Pair of verb and following noun. This indicates the
verbs object, which can change the focus of A/I,
e.g., I am getting a serious case of the flu vs. I am
getting a flu shot. Additional features are added if
the verb is past tense (based on word list and suffix
?-ed?.)
? Whether a flu related word appears as a noun or
an adjective. When flu is used as an adjective, it
may indicate a more general discussion of the flu,
as opposed to an actual infection I hate this flu vs. I
hate this flu hype.
? If a proper noun is followed by a possessive verb.
This may indicate others for the S/O task Looks like
Denmark has the flu. An additional feature fires for
any verb that follows a proper noun and any past
tense verb that follows a proper noun.
? Pair each noun with ???. While infection tweets
are often statements and awareness questions, the
subject matters, e.g. Do you think that swine flu
is coming to America? as awareness. An equivalent
feature is included for phrases ending with ?!?.
While many of our features can be extracted using
a syntactic parser (Foster et al, 2011), tweets are
very short, so our simple rules and over-generating
features captures the desired effects without parsing.
Self Other Total
Awareness 23.15% 24.07% 47.22%
Infection 37.21% 15.57% 52.78%
Total 60.36% 39.64%
Table 2: The distribution over labels of the data set. In-
fection tweets are more likely to be about the author (self)
than those expressing awareness.
3.1 Learning
We used a log-linear model from Mallet (McCal-
lum, 2002) with L2 regularization. For each task, we
first labeled tweets as related/not-related and then
classified the related tweets as awareness/infection
and self/others. We found this two phase approach
worked better than multi-class.
4 Data Collection
We used two Twitter data sets: a collection of 2
billion tweets from May 2009 and October 2010
(O?Connor et al, 2010)4 and 1.8 billion tweets col-
lected from August 2011 to November 2012. To
obtain labeled data, we first filtered the data sets
for messages containing words related to concern
and influenza,5 and used Amazon Mechanical Turk
(Callison-Burch and Dredze, 2010) to label tweets
as concerned awareness, infection, media and un-
related. We allowed multiple categories per tweet.
Annotators also labeled awareness/infection tweets
as self, other or both. We included tweets we anno-
tated to measure Turker quality and obtained three
annotations per tweet. More details can be found in
Lamb et al (2012).
To construct a labeled data set we removed low
quality annotators (below 80% accuracy on gold
tweets.) This seemed like a difficult task for anno-
tators as a fifth of the data had no annotations after
this step. We used the majority label as truth and ties
were broken using the remaining low quality anno-
tators. We then hand-corrected all tweets, changing
13.5% of the labels. The resulting data set contained
11,990 tweets (Table 2), 5,990 from 2011-2012 for
training and the remaining from 2009-2010 as test.6
4This coincided with the second and larger H1N1 (swine
flu) outbreak of 2009; swine flu is mentioned in 39.6% of the
annotated awareness or infection tweets.
5e.g. ?flu?, ?worried?, ?worry?, ?scared?, ?scare?, etc.
6All development was done using cross-validation on train-
ing data, reserving test data for the final experiments.
791
Feature Removed A/I S/O
n-grams 0.6701 0.8440
Word Classes 0.7735 0.8549
Stylometry 0.8011 0.8522
Pronoun/Last Noun 0.7976 0.8534
Pro-Drop 0.7989 0.8523
Numeric Reference 0.7988 0.8530
Pronoun/Verb 0.7987 0.8530
Flu Noun Before Verb 0.7987 0.8526
Noun in Question 0.8004 0.8534
Subject,Object,Verb 0.8005 0.8541
Table 3: F1 scores after feature ablation.
5 Experiments
We begin by evaluating the accuracy on the bi-
nary classification tasks and then measure the re-
sults from the classifiers for influenza surveillance.
We created precision recall curves on the test data
(Figure 1), and measured the highest F1, for the
three binary classifiers. For A/I and S/O, our addi-
tional features improved over the n-gram baselines.
We performed feature ablation experiments (Table
3) and found that for A/I, the word class features
helped the most by a large margin, while for S/O
the stylometry and pro-drop features were the most
important after n-grams. Interestingly, S/O does
equally well removing just n-gram features, sug-
gesting that the S/O task depends on a few words
captured by our features.
Since live data will have classifiers run in stages
? to filter out not-related tweets ? we evaluated
the performance of two-staged classification. F1
dropped to 0.7250 for A/I and S/O dropped to
0.8028.
5.1 Influenza surveillance using Twitter
We demonstrate how our classifiers can improve in-
fluenza surveillance using Twitter. Our hypothesis
is that by isolating infection tweets we can improve
correlations against government influenza data. We
include several baseline methods:
Google Flu Trends: Trends from search queries.7
Keywords: Tweets that contained keywords from
the DHHS Twitter surveillance competition.
ATAM: We obtained 1.6 million tweets that were
automatically labeled as influenza/other by ATAM
7http://www.google.org/flutrends/
Data System 2009 2011
Google Flu Trends 0.9929 0.8829
Twitter
ATAM 0.9698 0.5131
Keywords 0.9771 0.6597
All Flu 0.9833 0.7247
Infection 0.9897 0.7987
Infection+Self 0.9752 0.6662
Table 4: Correlations against CDC ILI data: Aug 2009-
Aug 2010, Dec 2011 to Aug 2012.
(Paul and Dredze, 2011). We trained a binary classi-
fier with n-grams and marked tweets as flu infection.
We evaluated three trends using our three binary
classifiers trained with a reduced feature set close to
the n-gram features:8
All Flu: Tweets marked as flu by Keywords or
ATAM were then classified as related/unrelated.9
This trend used all flu-related tweets.
Infection: Related tweets were classified as either
awareness or infection. This used infection tweets.
Infection+Self: Infection were then labeled as self
or other. This trend used self tweets.
All five of these trends were correlated against
data from the Centers for Disease Control and Pre-
vention (CDC) weekly estimates of influenza-like
illness (ILI) in the U.S., with Pearson correlations
computed separately for 2009 and 2011 (Table 4).10
Previous work has shown high correlations for 2009
data, but since swine flu had so dominated social me-
dia, we expect weaker correlations for 2011.
Results are show in Table 4 and Figure 2 shows
two classifiers against the CDC ILI data. We see
that in 2009 the Infection curve fits the CDC curve
very closely, while the All Flu curve appears to
substantially overestimate the flu rate at the peak.
While 2009 is clearly easier, and all trends have
similar correlations, our Infection classifier beats the
other Twitter methods. All trends do much worse in
8Classifiers trained on 2011 data and thresholds selected to
maximize F1 on held out 2009 data.
9Since our data set to train related or unrelated focused on
tweets that appeared to mention the flu, we first filtered out ob-
vious non-flu tweets by running ATAM and Keywords.
10While the 2009 data is a 10% sample of Twitter, we used a
different approach for 2011. To increase the amount of data, we
collected Tweets mentioning health keywords and then normal-
ized by the public stream counts. For our analysis, we excluded
days that were missing data. Additionally, we used a geolocator
based on user provided locations to exclude non-US messages.
See (Dredze et al, 2013) for details and code for the geolocator.
792
55 60 65 70 75 80 85Precision0
2040
6080
100
Recall
F1 = 0.7665F1 = 0.7562
N-GramsAll Features 40 50 60 70 80 90 100Precision020
4060
80100
Recall
F1 = 0.7891 F1 = 0.7985
N-GramsAll Features 70 75 80 85 90 95Precision0
2040
6080
100
Recall
F1 = 0.8499 F1 = 0.8550
N-GramsAll Features
Figure 1: Left to right: Precision-recall curves for related vs. not related, awareness vs. infection and self vs. others.
08/30/09 11/08/09 01/17/10 03/28/10 06/06/10 08/15/10Date
Flu R
ate
2009-2010
CDCTwitter (All Flu)Twitter (Infection Only)
11/27/11 01/15/12 03/04/12 04/22/12 06/10/12 07/29/12Date
Flu R
ate
2011-2012
Figure 2: The Twitter flu rate for two years alongside the ILI rates provided by the CDC. The y-axes are not comparable
between the two years due to differences in data collection, but we note that the 2011-12 season was much milder.
the 2011 season, which was much milder and thus
harder to detect. Of the Twitter methods, those us-
ing our system were dramatically higher, with the
Infection curve doing the best by a significant mar-
gin. Separating out infection from awareness (A/I)
led to significant improvements, while the S/O clas-
sifier did not, for unknown reasons.
The best result using Twitter reported to date has
been by Doan et al (2012), whose best system had
a correlation of 0.9846 during the weeks beginning
8/30/09?05/02/10. Our Infection system had a cor-
relation of 0.9887 during the same period. While
Google does better than any of the Twitter systems,
we note that Google has access to much more (pro-
prietary) data, and their system is trained to predict
CDC trends, whereas our Twitter system is intrinsi-
cally trained only on the tweets themselves.
Finally, we are also interested in daily trends in
addition to weekly, but there is no available evalu-
ation data on this scale. Instead, we computed the
stability of each curve, by measuring the day-to-day
changes. In the 2009 season, the relative increase
or decrease from the previous day had a variance of
3.0% under the Infection curve, compared to 4.1%
under ATAM and 6.7% under Keywords.
6 Discussion
Previous papers have implicitly assumed that flu-
related tweets mimick the infection rate. While this
was plausible on 2009 data that focused on the swine
flu epidemic, it is clearly false for more typical flu
seasons. Our results show that by differentiating be-
tween types of flu tweets to isolate reports of infec-
tion, we can recover reasonable surveillance. This
result delivers a promising message for the NLP
community: deeper content analysis of tweets mat-
ters. We believe this conclusion is applicable to nu-
merous Twitter trend tasks, and we encourage others
to investigate richer content analyses for these tasks.
In particular, the community interested in modeling
author beliefs and influence (Diab et al, 2009; Prab-
hakaran et al, 2012b; Biran and Rambow, 2011)
may find our task and data of interest. Finally, be-
yond surveillance, our methods can be used to study
disease awareness and sentiment, which has impli-
cations for how public health officials respond to
outbreaks. We conclude with an example of this dis-
tinction. On June 11th, 2009, the World Health Or-
ganization declared that the swine flu had become a
global flu pandemic. On that day, flu awareness in-
creased 282%, while infections increased only 53%.
793
References
Dena Asta and Cosma Shalizi. 2012. Identifying in-
fluenza trends via Twitter. In NIPS Workshop on So-
cial Network and Social Media Analysis: Methods,
Models and Applications.
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Proc.
NAACL-HLT, pages 327?337.
O. Biran and O. Rambow. 2011. Identifying justifi-
cations in written dialogs. In Semantic Computing
(ICSC), 2011 Fifth IEEE International Conference on,
pages 162?168. IEEE.
J. Bollen, A. Pepe, and H. Mao. 2011. Modeling pub-
lic mood and emotion: Twitter sentiment and socio-
economic phenomena. In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media, pages 450?453.
John S. Brownstein, Clark C. Freifeld, Emily H. Chan,
Mikaela Keller, Amy L. Sonricker, Sumiko R. Mekaru,
and David L. Buckeridge. 2010. Information tech-
nology and global surveillance of cases of 2009
h1n1 influenza. New England Journal of Medicine,
362(18):1731?1735.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Mechanical Turk.
N. Collier. 2012. Uncovering text mining: A survey
of current work on web-based epidemic intelligence.
Global Public Health, 7(7):731?749.
Samantha Cook, Corrie Conrad, Ashley L. Fowlkes, and
Matthew H. Mohebbi. 2011. Assessing google flu
trends performance in the united states during the
2009 influenza virus a (h1n1) pandemic. PLOS ONE,
6(8):e23610.
A. Culotta. 2010a. Towards detecting influenza epi-
demics by analyzing Twitter messages. In ACM Work-
shop on Soc.Med. Analytics.
Aron Culotta. 2010b. Detecting influenza epidemics
by analyzing Twitter messages. arXiv:1007.4748v1
[cs.IR], July.
Mona T. Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009. Committed belief annotation and tagging. In
ACL Third Linguistic Annotation Workshop.
S. Doan, L. Ohno-Machado, and N. Collier. 2012. En-
hancing Twitter data analysis with simple semantic fil-
tering: Example in tracking influenza-like illnesses.
arXiv preprint arXiv:1210.0848.
Mark Dredze, Michael J. Paul, Shane Bergsma, and Hieu
Tran. 2013. A Twitter geolocation system with appli-
cations to public health. Working paper.
Mark Dredze. 2012. How social media will change pub-
lic health. IEEE Intelligent Systems, 27(4):81?84.
Sachiko Maskawa Eiji Aramaki and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using Twitter. In Empirical Natural Language
Processing Conference (EMNLP).
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Empirical Natural Lan-
guage Processing Conference (EMNLP).
Joshua Epstein, Jon Parker, Derek Cummings, and Ross
Hammond. 2008. Coupled contagion dynamics of
fear and disease: Mathematical and computational ex-
plorations. PLoS ONE, 3(12).
J.M. Epstein. 2009. Modelling to contain pandemics.
Nature, 460(7256):687?687.
G. Eysenbach. 2006. Infodemiology: tracking flu-
related searches on the web for syndromic surveil-
lance. In AMIA Annual Symposium, pages 244?248.
AMIA.
J. Foster, O?. C?etinoglu, J. Wagner, J. Le Roux, S. Hogan,
J. Nivre, D. Hogan, J. Van Genabith, et al 2011. #
hardtoparse: Pos tagging and parsing the Twitterverse.
In proceedings of the Workshop On Analyzing Micro-
text (AAAI 2011), pages 20?25.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In As-
sociation for Computational Linguistics (ACL).
J. Ginsberg, M.H. Mohebbi, R.S. Patel, L. Brammer,
M.S. Smolinski, and L. Brilliant. 2008. Detecting
influenza epidemics using search engine query data.
Nature, 457(7232):1012?1014.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2012.
Investigating Twitter as a source for studying behav-
ioral responses to epidemics. In AAAI Fall Symposium
on Information Retrieval and Knowledge Discovery in
Biomedical Text.
A.K. McCallum. 2002. MALLET: A machine learning
for language toolkit.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
Tweets to polls: Linking text sentiment to public
opinion time series. In ICWSM.
Michael J. Paul and Mark Dredze. 2011. You are what
you Tweet: Analyzing Twitter for public health. In
ICWSM.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow, and Benjamin Van Durme. 2012a.
794
Statistical modality tagging from rule-based annota-
tions and crowdsourcing. In Extra-Propositional As-
pects of Meaning in Computational Linguistics (Ex-
ProM 2012).
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012b. Predicting overt display of power in
written dialogs. In North American Chapter of the As-
sociation for Computational Linguistics (NAACL).
Adam Sadilek, Henry Kautz, and Vincent Silenzio.
2012a. Modeling spread of disease from social inter-
actions. In Sixth AAAI International Conference on
Weblogs and Social Media (ICWSM).
Adam Sadilek, Henry Kautz, and Vincent Silenzio.
2012b. Predicting disease transmission from geo-
tagged micro-blog data. In Twenty-Sixth AAAI Con-
ference on Artificial Intelligence.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In WWW, New York,
NY, USA.
A. Signorini, A.M. Segre, and P.M. Polgreen. 2011. The
use of Twitter to track levels of disease activity and
public concern in the US during the influenza a H1N1
pandemic. PLoS One, 6(5):e19467.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with twitter: What
140 characters reveal about political sentiment. In
Proceedings of the fourth international aaai confer-
ence on weblogs and social media, pages 178?185.
B. Van Durme. 2012. Jerboa: A toolkit for randomized
and streaming algorithms. Technical report, Techni-
cal Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
795
Proceedings of NAACL-HLT 2013, pages 1010?1019,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Broadly Improving User Classification via
Communication-Based Name and Location Clustering on Twitter
Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, David Yarowsky
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
shane.a.bergsma@gmail.com, mdredze@cs.jhu.edu, vandurme@cs.jhu.edu, taw@jhu.edu, yarowsky@cs.jhu.edu
Abstract
Hidden properties of social media users, such
as their ethnicity, gender, and location, are of-
ten reflected in their observed attributes, such
as their first and last names. Furthermore,
users who communicate with each other of-
ten have similar hidden properties. We pro-
pose an algorithm that exploits these insights
to cluster the observed attributes of hundreds
of millions of Twitter users. Attributes such
as user names are grouped together if users
with those names communicate with other
similar users. We separately cluster millions
of unique first names, last names, and user-
provided locations. The efficacy of these clus-
ters is then evaluated on a diverse set of clas-
sification tasks that predict hidden users prop-
erties such as ethnicity, geographic location,
gender, language, and race, using only pro-
file names and locations when appropriate.
Our readily-replicable approach and publicly-
released clusters are shown to be remarkably
effective and versatile, substantially outper-
forming state-of-the-art approaches and hu-
man accuracy on each of the tasks studied.
1 Introduction
There is growing interest in automatically classify-
ing users in social media by various hidden prop-
erties, such as their gender, location, and language
(e.g. Rao et al (2010), Cheng et al (2010), Bergsma
et al (2012)). Predicting these and other proper-
ties for users can enable better advertising and per-
sonalization, as well as a finer-grained analysis of
user opinions (O?Connor et al, 2010), health (Paul
and Dredze, 2011), and sociolinguistic phenomena
(Eisenstein et al, 2011). Classifiers for user prop-
erties often rely on information from a user?s social
network (Jernigan and Mistree, 2009; Sadilek et al,
2012) or the textual content they generate (Pennac-
chiotti and Popescu, 2011; Burger et al, 2011).
Here, we propose and evaluate classifiers that bet-
ter exploit the attributes that users explicitly provide
in their user profiles, such as names (e.g., first names
like Mary, last names like Smith) and locations (e.g.,
Brasil). Such attributes have previously been used as
?profile features? in supervised user classifiers (Pen-
nacchiotti and Popescu, 2011; Burger et al, 2011;
Bergsma et al, 2012). There are several motivations
for exploiting these data. Often the only informa-
tion available for a user is a name or location (e.g.
for a new user account). Profiles also provide an
orthogonal or complementary source of information
to a user?s social network and textual content; gains
based on profiles alone should therefore add to gains
based on other data. The decisions of profile-based
classifiers could also be used to bootstrap training
data for other classifiers that use complementary fea-
tures.
Prior work has encoded profile attributes via lex-
ical or character-based features (e.g. Pennacchiotti
and Popescu (2011), Burger et al (2011), Bergsma
et al (2012)). Unfortunately, due to the long-tailed
distribution of user attributes, a profile-based classi-
fier will encounter many examples at test time that
were not observed during training. For example,
suppose a user wassim hassan gives their location as
tanger. If the attribute tokens wassim, hassan, and
tanger do not occur in training (nor indicative sub-
1010
strings), then a classifier can only guess at the user?s
ethnicity and location. In social media, the preva-
lence of fake names and large variations in spelling,
slang, and language make matters worse.
Our innovation is to enhance attribute-based clas-
sifiers with new data, derived from the communica-
tions of Twitter users with those attributes. Users
with the name tokens wassim and hassan often talk
to users with Arab names like abdul and hussein.
Users listing their location as tanger often talk to
users from morocco. Since users who communicate
often share properties such as ethnicity and location
(?8), the user wassim hassan might be an Arab who
uses the French spelling of the city Tangier.
Our challenge is to encode these data in a form
readily usable by a classifier. Our approach is to
represent each unique profile attribute (e.g. tanger
or hassan) as a vector that encodes the communi-
cation pattern of users with that attribute (e.g. how
often they talk to users from morocco, etc.); we then
cluster the vectors to discover latent groupings of
similar attributes. Based on transitive (third party)
connections, tanger and tangier can appear in the
same cluster, even if no two users from these loca-
tions talk directly. To use the clusters in an attribute-
based classifier, we add new features that indicate
the cluster memberships of the attributes. Clustering
thus lets us convert a high-dimensional space of all
attribute pairs to a low-dimensional space of cluster
memberships. This makes it easier to share our data,
yields fewer parameters for learning, and creates at-
tribute groups that are interpretable to humans.
We cluster names and locations in a very large
corpus of 168 million Twitter users (?2) and use a
distributed clustering algorithm to separately clus-
ter millions of first names, last names, and user-
provided locations (?3). We evaluate the use of our
cluster data as a novel feature in supervised classi-
fiers, and compare our result to standard classifiers
using character and token-level features (?4). The
cluster data enables significantly improved perfor-
mance in predicting the gender, location, and lan-
guage of social media users, exceeding both ex-
isting state-of-the-art machine and human perfor-
mance (?6). Our cluster data can likewise im-
prove performance in other domains, on both es-
tablished and new NLP tasks as further evaluated
in this paper (?6). We also propose a way to
First names: maria, david, ana, daniel, michael, john,
alex, jessica, carlos, jose, chris, sarah, laura, juan
Last names: silva, santos, smith, garcia, oliveira, ro-
driguez, jones, williams, johnson, brown, gonzalez
Locations: brasil, indonesia, philippines, london,
jakarta, s?o paulo, rio de janeiro, venezuela, brazil
Table 1: Most frequent profile attributes for our collection
of 168 million Twitter users, in descending order
enhance a geolocation system by using commu-
nication patterns, and show strong improvements
over a hand-engineered baseline (?7). We share
our clusters with the community to use with other
tasks. The clusters, and other experimental data, are
available for download from www.clsp.jhu.edu/
~sbergsma/TwitterClusters/.
2 Attribute Associations on Twitter
Data and Processing Our raw Twitter data com-
prises the union of 2.2 billion tweets from 05/2009
to 10/2010 (O?Connor et al, 2010), 1.8 billion
tweets collected from 07/2011 to 08/2012, and 80
million tweets collected from followers of 10 thou-
sand location and language-specific Twitter feeds.
We implemented each stage of processing using
MapReduce (Dean and Ghemawat, 2008). The total
computation (from extracting profiles to clustering
attributes) was 1300 days of wall-clock CPU time.
Attribute Extraction Tweets provide the name
and self-reported location of the tweeter. We find
126M unique users with these attributes in our data.
When tweets mention other users via an @user con-
struction, Twitter also includes the profile name of
the mentioned user; we obtain a further 42M users
from these cases. We then normalize the extracted
attributes by converting to lower-case, deleting sym-
bols, numbers, and punctuation, and removing com-
mon honorifics and suffixes like mr/mrs and jr/sr.
Common prefixes like van and de la are joined to
the last-name token.1 This processing yields 8.3M
1www.clsp.jhu.edu/~sbergsma/TwitterClusters/
also provides our scripts for normalizing attributes. The scripts
can be used to ensure consistency/compatibility between
arbitrary datasets and our shared cluster data. Note we use no
special processing for the companies, organizations, and spam-
mers among our users, nor for names arising from different
conventions (e.g. 1-word names, reversed first/last names).
1011
henrik: fredrik 5.87, henrik 5.82, anders 5.73, johan
5.69, andreas 5.59, martin 5.54, magnus 5.41
courtney: taylor 8.03, ashley 7.92, courtney 7.92,
emily 7.91, lauren 7.82, katie 7.72, brittany 7.69
ilya: sergey 5.85, alexey 5.62, alexander 5.59, dmitry
5.51, ????????? 5.46, anton 5.44, andrey 5.40
Table 2: Top associates and PMIs for three first names.
unique locations, 7.4M unique last names, and 5.5M
unique first names. These three sets provide the tar-
get attributes that we cluster in ?3. Table 1 shows
the most frequent names in each of these three sets.
User-User Links We extract each user mention as
an undirected communication link between the user
tweeting and the mentioned user (including self-
mentions but not retweets). We consider each user-
user link as a single event; we count it once no mat-
ter how often two specific users interact. We extract
436M user-user links in total.
Attribute-Attribute Pairs We use our profile data
to map each user-user link to an attribute-attribute
pair; we separately count each pair of first names,
last names, and locations. For example, the first-
name pair (henrik, fredrik) occurs 181 times. Rather
than using the raw count, we calculate the associa-
tion between attributes a1 and a2 via their pointwise
mutual information (PMI), following prior work in
distributional clustering (Lin and Wu, 2009):
PMI(a1, a2) = log
P(a1, a2)
P(a1)P(a2)
PMI essentially normalizes the co-occurrence by
what we would expect if the attributes were indepen-
dently distributed. We smooth the PMI by adding a
count of 0.5 to all co-occurrence events.
The most highly-associated name attributes re-
flect similarities in ethnicity and gender (Table 2).
The most highly-ranked associates for locations are
often nicknames and alternate/misspellings of those
locations. For example, the locations charm city,
bmore, balto, westbaltimore, b a l t i m o r e, bal-
timoreee, and balitmore each have the U.S. city of
baltimore as their highest-PMI associate. We show
how this can be used to help geolocate users (?7).
3 Attribute Clustering
Representation We first represent each target at-
tribute as a feature vector, where each feature corre-
sponds to another attribute of the same type as the
target and each value gives the PMI between this at-
tribute and the target (as in Table 2).2 To help cluster
the long-tail of infrequent attributes, we also include
orthographic features. For first and last names, we
have binary features for the last 2 characters in the
string. For locations, we have binary features for
(a) any ideographic characters in the string and (b)
each token (with diacritics removed) in the string.
We normalize the feature vectors to unit length.
Distributed K-Means Clustering Our approach
to clustering follows Lin and Wu (2009) who used k-
means to cluster tens of millions of phrases. We also
use cosine similarity to compute the closest centroid
(i.e., we use the spherical k-means clustering algo-
rithm (Dhillon and Modha, 2001)). We keep track
of the average cosine similarity between each vector
and its nearest centroid; this average is guaranteed
to increase at each iteration.
Like Lin and Wu (2009), we parallelize the al-
gorithm using MapReduce. Each mapper finds the
nearest centroids for a portion of the vectors, while
also computing the partial sums of the vectors as-
signed to each centroid. The mappers emit the cen-
troid IDs as keys and the partial sums as values.
The Reducer aggregates the partial sums from each
partition and re-normalizes each sum vector to unit
length to obtain the new centroids. We also use an
inverted index at each iteration that, for each input
feature, lists which centroids each feature belongs
to. Using this index greatly speeds up the centroid
similarity computations.
Clustering Details We cluster with nine separate
configurations: over first names, last names, and lo-
cations, and each with 50, 200, and 1000 cluster
centroids (denoted C50, C200, and C1000). Since k-
2We decided to restrict the features for a target to be at-
tributes of the same type (e.g., we did not use last name as-
sociations for a first name target) because each attribute type
conveys distinct information. For example, first names convey
gender and age more than last names. By separately cluster-
ing representations using first names, last names, and locations,
each clustering can capture its own distinct latent-class associa-
tions.
1012
Cluster 463 (Serbian): pavlovic?, jovanovic, jo-
vanovic?, stankovic?, srbija, markovic?, petrovic?,
radovic, nenad, milenkovic, nikolic, sekulic, todor-
ovic, stojanovic, petrovic, aleksic, ilic, markovic
Cluster 544 (Black South African): ngcobo, nkosi,
dlamini, ndlovu, mkhize, mtshali, sithole, mathebula,
mthembu, khumalo, ngwenya, shabangu, nxumalo,
buthelezi, radebe, mabena, zwane, mbatha, sibiya
Cluster 449 (Turkish): s?ahin, ?elik, ?zt?rk, ko?, ?ak?r,
karatas?, aktas?, g?ng?r, ?zkan, balc?, g?m?s?, akkaya,
gen?, sar?, y?ksel, g?nes?, yig?it, yal??n, orhan, sag?lam,
g?ler, demirci, k???k, yavuz, bayrak, ?zcan, altun
Cluster 656 (Indonesian): utari, oktaviana, apriani,
mustika, septiana, febrianti, kurniawati, indriani, nur-
janah, septian, cahya, anggara, yuliani, purnamasari,
sukma, wijayanti, pramesti, ningrum, yanti, wulansari
Table 3: Example C1000 last-name clusters
Cluster 56 [sim=0.497]: gregg, bryn, bret, stewart,
lyndsay, howie, elyse, jacqui, becki, rhett, meaghan,
kirstie, russ, jaclyn, zak, katey, seamus, brennan,
fraser, kristie, stu, jaimie, kerri, heath, carley, griffin
Cluster 104 [sim=0.442]: stephon, devonte, deion,
demarcus, janae, tyree, jarvis, donte, dewayne, javon,
destinee, tray, janay, tyrell, jamar, iesha, chyna,
jaylen, darion, lamont, marquise, domonique, alexus
Cluster 132 [sim=0.292]: moustafa, omnya, menna-
tallah, ?C?@, shorouk, ragab, ?


??, radwa, moemen,
mohab, hazem, yehia, ? K
Q k, Z @Q?? @, mennah, ?
 Q?? ?,
abdelrahman, ?


	
????, H. 	Qk, Q?A

K, nermeen, hebatallah
...
Table 4: C200 soft clustering for first name yasmeen
means is not guaranteed to reach a global optimum,
we use ten different random initializations for each
configuration, and select the one with the highest av-
erage similarity after 20 iterations. We run this one
for an additional 30 iterations and take the output as
our final set of centroids for that configuration.
The resulting clusters provide data that could help
classify hidden properties of social media users. For
example, Table 3 shows that last names often clus-
ter by ethnicity, even at the sub-national level (e.g.
Zulu tribe surnames nkosi, dlamini, mathebula, etc.).
Note the Serbian names include two entries that are
not last names: srbija, the Serbian word for Serbia,
and nenad, a common Serbian first name.
Soft Clustering Rather than assigning each at-
tribute to its single highest-similarity cluster, we can
assign each vector to its N most similar clusters.
These soft-cluster assignments often reflect different
social groups where a name or location is used. For
example, the name yasmeen is similar to both com-
mon American names (Cluster 56), African Ameri-
can names (Cluster 104), and Arabic names (Clus-
ter 132) (Table 4). As another example, the C1000
assignments for the location trujillo comprise sep-
arate clusters containing towns and cities in Peru,
Venezuela, Colombia, etc., reflecting the various
places in the Latin world with this name. In general,
the soft cluster assignment is a low-dimensional rep-
resentation of each of our attributes. Although it can
be interpretable to humans, it need not be in order to
be useful to a classifier.
4 Classification with Cluster Features
Our motivating problem is to classify users for hid-
den properties such as their gender, location, race,
ethnicity, and language. We adopt a discriminative
solution. We encode the relevant data for each in-
stance in a feature vector and train a (linear) support
vector machine classifier (Cortes and Vapnik, 1995).
SVMs represent the state-of-the-art on many NLP
classification tasks, but other classifiers could also
be used. For multi-class classification, we use a one-
versus-all strategy, a competitive approach on most
multi-class problems (Rifkin and Klautau, 2004).
The input to our system is one or more observed
user attributes (e.g. name and location fields from
a user profile). We now describe how features are
created from these attributes in both state-of-the-art
systems and via our new cluster data.
Token Features (Tok) are binary features that in-
dicate the presence of a specific attribute (e.g., first-
name=bob). Burger et al (2011) and Bergsma et al
(2012) used Tok features to encode user profile fea-
tures. For multi-token fields (e.g. location), our Tok
features also indicate the specific position of each
token (e.g., loc1=s?o, loc2=paulo, locN=brasil).
Character N-gram Features (Ngm) give the
count of all character n-grams of length 1-to-4 in the
input. Ngm features have been used in user classifi-
cation (Burger et al, 2011) and represent the state-
1013
of-the-art in detecting name ethnicity (Bhargava and
Kondrak, 2010). We add special begin/end charac-
ters to the attributes to mark the prefix and suffix po-
sitions. We also use a smoothed log-count; we found
this to be most effective in preliminary work.
Cluster Features (Clus) indicate the soft-cluster
memberships of the attributes. We have features for
the top-2, 5, and 20 most similar clusters in the C50,
C200, and C1000 clusterings, respectively. Like Lin
and Wu (2009), we ?side-step the matter of choos-
ing the optimal value k in k-means? by using fea-
tures from clusterings at different granularities. Our
feature dimensions correspond to cluster IDs; fea-
ture values give the similarity to the cluster centroid.
Other strategies (e.g. hard clustering, binary fea-
tures) were less effective in preliminary work.
5 Classification Experiments
5.1 Methodology
Our main objective is to assess the value of us-
ing cluster features (Clus). We add these features
to classifiers using Tok+Ngm features, which repre-
sents the current state-of-the-art. We compare these
feature settings on both Twitter tasks (?5.2) and
tasks not related to social-media (?5.3). For each
task, we randomly divide the gold standard data into
50% train, 25% development and 25% test, unless
otherwise noted. As noted above, the gold-standard
datasets for all of our experiments are available for
download. We train our SVM classifiers using the
LIBLINEAR package (Fan et al, 2008). We optimize
the classifier?s regularization parameter on develop-
ment data, and report our final results on the held-
out test examples. We report accuracy: the propor-
tion of test examples classified correctly. For com-
parison, we report the accuracy of a majority-class
baseline on each task (Base).
Classifying hidden properties of social media
users is challenging (Table 5). Pennacchiotti and
Popescu (2011) even conclude that ?profile fields do
not contain enough good-quality information to be
directly used for user classification.? To provide in-
sight into the difficulty of the tasks, we had two hu-
mans annotate 120 examples from each of the test
sets, and we average their results to give a ?Human?
performance number. The two humans are experts in
Country: 53 possible countries
United States courtland dante cali baby
United States tinas twin on the court
Brazil thamires gomez macap? ap
Denmark marte clason NONE
Lang. ID: 9 confusable languages
Bulgarian valentina getova NONE
Russian borisenko yana edinburgh
Bulgarian NONE blagoevgrad
Ukrainian andriy kupyna ternopil
Farsi kambiz barahouei NONE
Urdu musadiq sanwal jammu
Ethnicity: 13 European ethnicities
German dennis hustadt
Dutch bernhard hofstede
French david coste
Swedish mattias bjarsmyr
Portuguese helder costa
Race: black or white
black kerry swain
black darrell foskey
white ty j larocca
black james n jones
white sean p farrell
Table 5: Examples of class (left) and input (names, loca-
tions) for some of our evaluation tasks.
this domain and have very wide knowledge of global
names and locations.
5.2 Twitter Applications
Country A number of recent papers have consid-
ered the task of predicting the geolocation of users,
using both user content (Cheng et al, 2010; Eisen-
stein et al, 2010; Hecht et al, 2011; Wing and
Baldridge, 2011; Roller et al, 2012) and social net-
work (Backstrom et al, 2010; Sadilek et al, 2012).
Here, we first predict user location at the level of
the user?s location country. To our knowledge, we
are the first to exploit user locations and names for
this prediction. For this task, we obtain gold data
from the portion of Twitter users who have GPS en-
abled (geocoded tweets). We were able to obtain a
very large number of gold instances for this task, so
selected only 10K for testing, 10K for development,
and retained the remaining 782K for training.
Language ID Identifying the language of users
is an important prerequisite for building language-
specific social media resources (Tromp and Pech-
1014
enizkiy, 2011; Carter et al, 2013). Bergsma et al
(2012) recently released a corpus of tweets marked
for one of nine languages grouped into three confus-
able character sets: Arabic, Farsi, and Urdu tweets
written in Arabic characters; Hindi, Nepali, and
Marathi written in Devanagari, and Russian, Bulgar-
ian, and Ukrainian written in Cyrillic. The tweets
were marked for language by native speakers via
Amazon Mechanical Turk. We again discard the
tweet content and extract each user?s first name, last
name, and user location as our input data, while tak-
ing the annotated language as the class label.
Gender We predict whether a Twitter user is male
or female using data from Burger et al (2011). This
data was created by linking Twitter users to struc-
tured profile pages on other websites where users
must select their gender. Unlike prior systems using
this data (Burger et al, 2011; Van Durme, 2012), we
make the predictions using only user names.
5.3 Other Applications
Origin Knowing the origin of a name can improve
its automatic pronunciation (Llitjos and Black,
2001) and transliteration (Bhargava and Kondrak,
2010). We evaluate our cluster data on name-origin
prediction using a corpus of names marked as ei-
ther Indian or non-Indian by Bhargava and Kondrak
(2010). Since names in this corpus are not marked
for entity type, we include separate cluster features
from both our first and last name clusters.
Ethnicity We also evaluate on name-origin data
from Konstantopoulos (2007). This data derives
from lists of football players on European national
teams; it marks each name (with diacritics removed)
as arising from one of 13 European languages. Fol-
lowing prior work, we test in two settings: (1) using
last names only, and (2) using first and last names.
Race We also evaluate our ability to identify eth-
nic groups at a sub-national level. To obtain data
for this task, we mined the publicly-available arrest
records on mugshots.com for the U.S. state of New
Jersey (a small but diverse and densely-populated
area). Over 99% of users were listed as either black
or white, and we structure the task as a binary clas-
sification problem between these two classes. We
predict the race of each person based purely on their
name; this contrasts with prior work in social media
which looked at identifying African Americans on
the basis of their Twitter content (Eisenstein et al,
2011; Pennacchiotti and Popescu, 2011).
6 Classification Results
Table 6 gives the results on each task. The system in-
corporating our novel Clus features consistently im-
proves over the Ngm+Tok system; all differences be-
tween All and Ngm+Tok are significant (McNemar?s,
p<0.01). The relative reduction in error from adding
Clus features ranges between 7% and 51%. The All
system including Clus features also exceeds human
performance on all studied tasks.
On Country, the U.S. is the majority class, oc-
curring in 42.5% of cases.3 It is impressive that
All so significantly exceeds Tok+Ngm (86.7% vs.
84.8%); with 782K training examples, we did not
expect such room for improvement. Both names and
locations play an important role: All achieves 66%
using names alone and 70% with only location. On
the subset of data where all three attributes are non-
empty, the full system achieves 93% accuracy.
Both feature classes are likewise important for
Lang. ID; All achieves 67% with only first+last
names, 72% with just locations, but 83% with both.
Our smallest improvement is on Gender. This
task is easier (with higher human/system accuracy)
and has plenty of training data (more data per class
than any other task); there is thus less room to im-
prove. Looking at the feature weights, the strongest-
weighted female cluster apparently captures a sub-
community of Justin Bieber fans (showing loyalty
with ?first names? jbieber, belieb, biebz, beliebing,
jbiebs, etc.). Just because a first name like madison
has a high similarity to this cluster does not imply
girls named Madison are Justin Bieber fans; it sim-
ply means that Madisons have similar names to the
friends of Justin Bieber fans (who tend to be girls).
Also, note that while the majority of the 34K users in
our training data are assigned this cluster somewhere
in their soft clustering, only 6 would be assigned this
3We tried other baselines: e.g., we predict countries if they
are substrings of the location (otherwise predicting U.S.); and
we predict countries if they often occur as a string following
the given location in our profile data (e.g., we predict Spain for
Madrid since Madrid, Spain is common). Variations on these
approaches consistently performed between 48% and 56%.
1015
Task Input
Num. Num.
Base Human Tok Ngm Clus
Tok+
All ?
Train Class Ngm
Country first+last+loc 781920 53 42.5 71.7 83.0 84.5 80.2 84.8 86.7 12.5
Lang. ID first+last+loc 2492 9 27.0 74.2 74.6 80.6 71.1 80.4 82.7 11.7
Gender first+last 33805 2 52.4 88.3 85.3 88.6 79.5 89.5 90.2 6.7
Origin entity name 500 2 52.4 80.4 - 75.6 81.2 75.6 88.0 50.8
Ethnicity last 6026 13 20.8 47.9 - 54.6 48.5 54.6 62.4 17.2
Ethnicity first+last 7457 13 21.2 53.3 67.6 77.5 73.6 78.4 81.3 13.4
Race first+last 7977 2 54.7 71.4 80.4 81.6 84.6 82.4 84.6 12.5
Table 6: Task details and accuracy (%) for attribute-based classification tasks. ? = relative error reduction (%) of All
(Tok+Ngm+Clus) over Ngm+Tok. All always exceeds both Tok+Ngm and the human performance.
cluster in a hard clustering. This clearly illustrates
the value of the soft clustering representation.
Note the All system performed between 83% and
90% on each Twitter task. This level of performance
strongly refutes the prevailing notion that Twitter
profile information is useless in general (Pennac-
chiotti and Popescu, 2011) and especially for geolo-
cation (Cheng et al, 2010; Hecht et al, 2011).
We now move to applications beyond social me-
dia. Bhargava and Kondrak (2010) have the current
state-of-the-art on Origin and Ethnicity based on an
SVM using character-n-gram features; we reimple-
mented this as Ngm. We obtain a huge improvement
over their work using Clus, especially on Origin
where we reduce error by >50%.4 This improve-
ment can partly be attributed to the small amount of
training data; with fewer parameters to learn, Clus
learns more from limited data than Ngm. We like-
wise see large improvements over the state-of-the-
art on Ethnicity, on both last name and full name
settings.
Finally, Clus features also significantly improve
accuracy on the new Race task. Our cluster data can
therefore help to classify names into sub-national
groups, and could potentially be used to infer other
interesting communities such as castes in India and
religious divisions in many countries.
In general, the relative value of our cluster models
varies with the amount of training data; we see huge
gains on the smaller Origin data but smaller gains
on the large Gender set. Figure 1 shows how per-
formance of Clus and Ngm varies with training data
on Race. Again, Clus is especially helpful with less
4Note Tok is not used here because the input is a single token
and training and test splits have distinct instances.
 60
 65
 70
 75
 80
 85
 10  100  1000  10000
A
cc
ur
ac
y
Number of training examples
Clus
Ngm
Figure 1: Learning curve on Race: Clus perform as well
with 30 training examples as Ngm features do with 1000.
data; thousands of training examples are needed for
Ngm to rival the performance of Clus using only a
handful. Since labeled data is generally expensive
to obtain or in short supply, our method for exploit-
ing unlabeled Twitter data can both save money and
improve top-end performance.
7 Geolocation by Association
There is a tradition in computational linguistics of
grouping words both by the similarity of their con-
text vectors (Hindle, 1990; Pereira et al, 1993; Lin,
1998) and directly by their statistical association in
text (Church and Hanks, 1990; Brown et al, 1992).
While the previous sections explored clusters built
by vector similarity, we now explore a direct appli-
cation of our attribute association data (?2).
We wish to use this data to improve an existing
Twitter geolocation system based on user profile lo-
cations. The system operates as follows: 1) normal-
1016
ize user-provided locations using a set of regular ex-
pressions (e.g. remove extra spacing, punctuation);
2) look up the normalized location in an alias list;
3) if found, map the alias to a unique string (target
location), corresponding to a structured location ob-
ject that includes geo-coordinates.
The alias list we are currently using is based on
extensive work in hand-writing aliases for the most
popular Twitter locations. For example, the current
aliases for Nashville, Tennessee include nashville,
nashville tn, music city, etc. Our objective is to im-
prove on this human-designed list by automatically
generating aliases using our association data.
Aliases by Association For each target, we pro-
pose new aliases from the target?s top-PMI asso-
ciates (?2). To become an alias, the PMI between
the alias and target must be above a threshold,
the alias must occur more than a fixed number of
times in our profile data, the alias must be within
the top-N1 associates of the target, and the target
must be within the top-N2 associates of the alias.
We merge our automatic aliases with the manually-
written aliases. The new aliases for Nashville, Ten-
nessee include east nashville, nashville tenn, music
city usa, nashvegas, cashville tn, etc.
Experiments To evaluate the geolocation system,
we use tweets from users with GPS enabled (?5.2).
For each tweet, we resolve the location using the
system and compare to the gold coordinates. The
system can skip a location if it does not match the
alias list; more than half of the locations are skipped,
which is consistent with prior work (Hecht et al,
2011). We evaluate the alias lists using two mea-
sures: (1) its coverage: the percentage of locations it
resolves, and (2) its precision: of the ones resolved,
the percentage that are correct. We define a correct
resolution to be one where the resolved coordinates
are within 50 miles of the gold coordinates.
We use 56K gold tweets to tune the parameters of
our automatic alias-generator, trading off coverage
and precision. We tune such that the system using
these aliases obtains the highest possible coverage,
while being at least as precise as the baseline system.
We then evaluate both the baseline set of aliases and
our new set on 56K held-out examples.
Results On held-out test data, the geolocation sys-
tem using baseline aliases has a coverage of 38.7%
and a precision of 59.5%. Meanwhile, the system
using the new aliases has a coverage of 44.6% and
a precision of 59.4%. With virtually the same pre-
cision, the new aliases are thus able to resolve 15%
more users. This provides an immediate benefit to
our existing Twitter research efforts.
Note that our alias lists can be viewed as clus-
ters of locations. In ongoing work, we are exploring
techniques based on discriminative learning to infer
alias lists using not only Clus information but also
Ngm and Tok features as in the previous sections.
8 Related Work
In both real-world and online social networks, ?peo-
ple socialize with people who are like them in terms
of gender, sexual orientation, age, race, education,
and religion? (Jernigan and Mistree, 2009). So-
cial media research has exploited this for two main
purposes: (1) to predict friendships based on user
properties, and (2) to predict user properties based
on friendships. Friendship prediction systems (e.g.
Facebook?s friend suggestion tool) use features such
as whether both people are computer science ma-
jors (Taskar et al, 2003) or whether both are at the
same location (Crandall et al, 2010; Sadilek et al,
2012). The inverse problem has been explored in the
prediction of a user?s location given the location of
their peers (Backstrom et al, 2010; Cho et al, 2011;
Sadilek et al, 2012). Jernigan and Mistree (2009)
predict a user?s sexuality based on the sexuality of
their Facebook friends, while Garera and Yarowsky
(2009) predict a user?s gender partly based on the
gender of their conversational partner. Jha and El-
hadad (2010) predict the cancer stage of users of
an online cancer discussion board; they derive com-
plementary information for prediction from both the
text a user generates and the cancer stage of the peo-
ple that a user interacts with.
The idea of clustering data in order to provide fea-
tures for supervised systems has been successfully
explored in a range of NLP tasks, including named-
entity-recognition (Miller et al, 2004; Lin and Wu,
2009; Ratinov and Roth, 2009), syntactic chunking
(Turian et al, 2010), and dependency parsing (Koo
et al, 2008; T?ckstr?m et al, 2012). In each case,
1017
the clusters are derived from the distribution of the
words or phrases in text, not from their communica-
tion pattern. It would be interesting to see whether
prior distributional clusters can be combined with
our communication-based clusters to achieve even
better performance. Indeed, there is evidence that
features derived from text can improve the predic-
tion of name ethnicity (Pervouchine et al, 2010).
There has been an explosion of work in recent
years in predicting user properties in social net-
works. Aside from the work mentioned above that
analyzes a user?s social network, a large amount
of work has focused on inferring user properties
based on the content they generate (e.g. Burger
and Henderson (2006), Schler et al (2006), Rao
et al (2010), Mukherjee and Liu (2010), Pennac-
chiotti and Popescu (2011), Burger et al (2011), Van
Durme (2012)).
9 Conclusion and Future Work
We presented a highly effective and readily repli-
cable algorithm for generating language resources
from Twitter communication patterns. We clustered
user attributes based on both the communication of
users with those attributes as well as substring sim-
ilarity. Systems using our clusters significantly out-
perform state-of-the-art algorithms on each of the
tasks investigated, and exceed human performance
on each task as well. The power and versatility of
our clusters is exemplified by the fact we reduce er-
ror by a larger margin on each of the non-Twitter
tasks than on any Twitter task itself.
Twitter provides a remarkably large sample and
effectively a partial census of much of the world?s
population, with associated metadata, descriptive
content and sentiment information. Our ability to
accurately assign numerous often unspecified prop-
erties such as race, gender, language and ethnicity to
such a large user sample substantially increases the
sociological insights and correlations one can derive
from such data.
References
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical predic-
tion with social and spatial proximity. In Proc. WWW,
pages 61?70.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
HLT-NAACL, pages 693?696.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
John D. Burger and John C. Henderson. 2006. An ex-
ploration of observable features related to blogger age.
In Proc. AAAI Spring Symposium: Computational Ap-
proaches to Analyzing Weblogs, pages 15?20.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on Twitter. In
Proc. EMNLP, pages 1301?1309.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog Language Identification: Overcom-
ing the Limitations of Short, Unedited and Idiomatic
Text. Language Resources and Evaluation Journal.
(forthcoming).
Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010.
You are where you tweet: a content-based approach
to geo-locating Twitter users. In Proc. CIKM, pages
759?768.
Eunjoon Cho, Seth A. Myers, and Jure Leskovec. 2011.
Friendship and mobility: user movement in location-
based social networks. In Proc. KDD, pages 1082?
1090.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273?297.
David J. Crandall, Lars Backstrom, Dan Cosley, Sid-
dharth Suri, Daniel Huttenlocher, and Jon Kleinberg.
2010. Inferring social ties from geographic coinci-
dences. Proceedings of the National Academy of Sci-
ences, 107(52):22436?22441.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Mach. Learn., 42(1-2):143?175.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. EMNLP, pages
1277?1287.
1018
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011.
Discovering sociolinguistic associations with struc-
tured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proc. ACL-IJCNLP, pages 710?718.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi.
2011. Tweets from Justin Bieber?s heart: the dynamics
of the location field in user profiles. In Proc. CHI,
pages 237?246.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proc. ACL, pages
268?275.
Carter Jernigan and Behram F. T. Mistree. 2009. Gaydar:
Facebook friendships expose sexual orientation. First
Monday, 14(10). [Online].
Mukund Jha and Noemie Elhadad. 2010. Cancer stage
prediction based on patient online discourse. In Proc.
2010 Workshop on Biomedical Natural Language Pro-
cessing, pages 64?71.
Stasinos Konstantopoulos. 2007. What?s in a name? In
Proc. Computational Phonology Workshop, RANLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL-08: HLT, pages 595?603.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proc. ACL-IJCNLP,
pages 1030??1038.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. Coling-ACL, pages 768?774.
Ariadna Font Llitjos and Alan W. Black. 2001. Knowl-
edge of language origin improves pronunciation accu-
racy of proper names. In Proceedings of EuroSpeech-
01, pages 1919?1922.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proc. HLT-NAACL, pages 337?342.
Arjun Mukherjee and Bing Liu. 2010. Improving gender
classification of blog authors. In Proc. EMNLP, pages
207?217.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. ICWSM, pages 122?129.
Michael Paul and Mark Dredze. 2011. You are what you
tweet: Analyzing Twitter for public health. In Proc.
ICWSM, pages 265?272.
Marco Pennacchiotti and Ana-Maria Popescu. 2011. A
machine learning approach to Twitter user classifica-
tion. In Proc. ICWSM, pages 281?288.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Proc.
ACL, pages 183?190.
Vladimir Pervouchine, Min Zhang, Ming Liu, and
Haizhou Li. 2010. Improving name origin recogni-
tion with context features and unlabelled data. In Col-
ing 2010: Posters, pages 972?978.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. International Workshop on
Search and Mining User-Generated Contents, pages
37?44.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
CoNLL, pages 147?155.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101?
141.
Stephen Roller, Michael Speriosu, Sarat Rallapalli, Ben-
jamin Wing, and Jason Baldridge. 2012. Supervised
text-based geolocation using language models on an
adaptive grid. In Proc. EMNLP-CoNLL, pages 1500?
1510.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proc. WSDM, pages 723?732.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proc. AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 199?205.
Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer
of linguistic structure. In Proc. NAACL-HLT, pages
477?487.
Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne
Koller. 2003. Link prediction in relational data. In
Proc. NIPS, volume 15.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identication on short texts. In
Proc. 20th Machine Learning conference of Belgium
and The Netherlands, pages 27?34.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. ACL, pages
384?394.
Benjamin Van Durme. 2012. Streaming analysis of dis-
course participants. In Proc. EMNLP-CoNLL, pages
48?58.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proc. ACL, pages 955?964.
1019
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 5?9,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
Topic Models and Metadata for Visualizing Text Corpora
Justin Snyder, Rebecca Knowles, Mark Dredze, Matthew R. Gormley, Travis Wolfe
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
{jsnyde32,mdredze,mgormley,twolfe3}@jhu.edu, rknowles@haverford.edu
Abstract
Effectively exploring and analyzing large text
corpora requires visualizations that provide a
high level summary. Past work has relied on
faceted browsing of document metadata or on
natural language processing of document text.
In this paper, we present a new web-based tool
that integrates topics learned from an unsuper-
vised topic model in a faceted browsing expe-
rience. The user can manage topics, filter doc-
uments by topic and summarize views with
metadata and topic graphs. We report a user
study of the usefulness of topics in our tool.
1 Introduction
When analyzing text corpora, such as newspaper ar-
ticles, research papers, or historical archives, users
need an intuitive way to understand and summa-
rize numerous documents. Exploratory search (Mar-
chionini, 2006) is critical for large corpora that can
easily overwhelm users. Corpus visualization tools
can provide a high-level view of the data and help di-
rect subsequent exploration. Broadly speaking, such
systems can be divided into two groups: those that
rely on structured metadata, and those that use infor-
mation derived from document content.
Metadata Approaches based on metadata include
visualizing document metadata alongside a domain
ontology (Seeling and Becks, 2003), providing tools
to select passages based on annotated words (Cor-
rell et al, 2011), and using images and metadata for
visualizing related documents (Cataldi et al, 2011).
A natural solution for exploring via metadata is
faceted browsing (English et al, 2002; Hearst, 2006;
Smith et al, 2006; Yee et al, 2003), a paradigm
for filtering commonly used in e-commerce stores.
This consists of filtering based on metadata like
?brand? or ?size?, which helps summarize the con-
tent of the current document set (Ka?ki, 2005). Stud-
ies have shown improved user experiences by facil-
itating user interactions through facets (Oren et al,
2006) and faceted browsing has been used for aid-
ing search (Fujimura et al, 2006) and exploration
(Collins et al, 2009) of text corpora.
However, facets require existing structured meta-
data fields, which may be limited or unavailable. An
alternative is to use NLP to show document content.
Content Topic modeling (Blei et al, 2003), has
become very popular for corpus and document un-
derstanding. Recent research has focused on aspects
highlighted by the topic model, such as topic distri-
butions across the corpus, topic distributions across
documents, related topics and words that make up
each topic (Chaney and Blei, 2012; Eisenstein et al,
2012), or document relations through topic compo-
sitions (Chuang et al, 2012; Gardner et al, 2010).
Newer work has begun to visualize documents in
the context of their topics and their metadata, such as
topics incorporated with keywords and events (Cui
et al, 2011). Other examples include displaying
topic prevalence over time (Liu et al, 2009) or help-
ing users understand how real events shape textual
trends (Dou et al, 2011). While interfaces may be
customized for specific metadata types, e.g. the top-
ical map of National Institutes of Health funding
agencies (Talley et al, 2011), these interfaces do not
incorporate arbitrary metadata.
5
2 Combining Metadata and Topics
We present MetaToMATo (Metadata and Topic
Model Analysis Toolkit), a visualization tool that
combines both metadata and topic models in a single
faceted browsing paradigm for exploration and anal-
ysis of document collections. While previous work
has shown the value of metadata facets, we show that
topic model output complements metadata. Provid-
ing both in a single interface yields a flexible tool.
We illustrate MetaToMATo with an example
adapted from our user study. Consider Sarah, a
hypothetical intern in the New York Times archive
room who is presented with the following task.
Your boss explains that although the New
York Times metadata fields are fairly compre-
hensive, sometimes human error leads to over-
sights or missing entries. Today you?ve been
asked to keep an eye out for documents that
mention the New York Marathon but do not
include descriptors linking them to that event.
This is corpus exploration: a user is asked to dis-
cover relevant information by exploring the corpus.
We illustrate the tool with a walk-through.
Corpus Selection The corpus selection page (tool
home page) provides information about all available
corpora, and allows for corpora upload and deletion.
Sarah selects the New York Times corpus.
Corpus Overview After selecting a corpus, the
user sees the corpus overview and configuration
page. Across four tabs, the user is presented with
more detailed corpus statistics and can customize
her visualization experience. The first tab shows
general corpus information. The second allows for
editing the inferred type (date, quantity, or string)
for each metadata attribute to change filtering be-
havior, hide unhelpful attributes, and choose which
attributes to ?quick display? in the document col-
lapsed view. On the remaining two tabs, the user can
customize date display formats and manage tags.
She selects attributes ?Date? and ?Byline? for
quick display, hides ?Series Name?, and formats
?Date? to show only the date (no times).
Topics View Each topic is displayed in a box con-
taining its name (initially set to its top 3 words) and a
list of the top 10 words. Top words within a topic are
words with the highest probability of appearing in
the corpus. Each topic word is highlighted to show a
Figure 1: Topics Page A view of the first row of top-
ics, and the sorting selector at the top of the page. The
left topic is being renamed. The second topic has been
marked as junk.
normalized probability of that word within the topic.
(Figure 1) Clicking a topic box provides more infor-
mation. Users can rename topics, label unhelpful or
low-quality topics as JUNK, or sort them in terms of
frequency in the corpus,1 predicted quality,2 or junk.
Sarah renames several topics, including the topic
?{running, athletes, race}? as SPORTS and marks
the ?{share, listed, bath}? topic as JUNK.
Documents View The document view provides a
faceted browsing interface of the corpus. (Figure 2)
The pane on the right side displays the set of docu-
ments returned by the current filters (search). Each
document is summarized by the first 100 words and
any quick view metadata. Users can expand doc-
uments to see all document metadata, a graph of
the distribution of the topics in this document, and
a graph of topics distinctive to this document com-
pared to corpus-wide averages.3
Sarah begins by looking at the types of documents
in the corpus, opening and closing a few documents
as she scrolls down the page.
The facets pane on the left side of the page dis-
plays the available facets given the current filters.
Topics in a drop-down menu can be used to filter
given a threshold.
Sarah selects the value ?New York City? for the
Location attribute and a threshold of 5% for the
SPORTS topic, filtering on both facets.
Values next to each metadata facet show the num-
ber of documents in the current view with those at-
tribute values, which helps tell the user what to ex-
1Frequency is computed using topic assignments from a
Gibbs sampler (Griffiths and Steyvers, 2004).
2Topic quality is given by the entropy of its word distribu-
tion. Other options include Mimno and Blei (2011).
3The difference of the probability of a topic in the current
document and the topic overall, divided by value overall.
6
Figure 2: Left: Documents Page. The left pane shows the available facets (topics and metadata) and the right pane
shows the matching documents (collapsed view.) Right: Expanded Document. An expanded collapsed document is
replaced with this more detailed view, showing the entire document as well as metadata and topic graphs.
pect if she refines her query.
Sarah notices that the News Desk value of
?Sports? matches a large number of documents in
the current view. She adds this filter to the current
facet query, updating the document view.
At the top of the document pane are the cur-
rent view?s ?Aggregate Statistics?, which shows how
many documents match the current query. An ex-
pandable box shows graphs for the current docu-
ments topic distribution and distinctive topics.4
Looking at the topic graph for the current query,
Sarah sees that another topic with sports related
words appears with high probability. She adds it to
the search and updates the document view.
Any document can be tagged with user-created
tags. Tags and their associated documents are dis-
played in the corpus overview on the configuration
page. If a user finds a search query of interest, she
can save and name the search to return to it later.
Sarah sees many documents relevant to the New
York City Marathon. She tags documents of interest
and saves the query for later reference.
2.1 Implementation Details
Our web based tool makes it easy for users to share
results, maintain the system, and make the tool
widely available. The application is built with a
JSP front-end, a Java back-end, and a MongoDB
database for storing the corpus and associated data.
To ensure a fast UI, filters use an in-memory meta-
data and topic index. Searches are cached so incre-
mental search queries are very fast. The UI uses
4Computed as above but with more topics displayed.
Ajax and JQuery UI for dynamic loading and inter-
active elements. We easily hosted more than a dozen
corpora on a single installation.
3 Evaluation
Our primary goal was to investigate whether incor-
porating topic model output along with document
metadata into a faceted browser provided an effec-
tive mechanism for filtering documents. Participants
were presented with four tasks consisting of a ques-
tion to answer using the tool and a paragraph provid-
ing context. The first three tasks tested exploration
(find documents) while the last tested analysis (learn
about article authors). At the end of each task, the
users were directed to a survey on the tool?s useful-
ness. We also logged user actions to further evaluate
how they used the tool.
3.1 Participants and Experimental Setup
Twelve participants (3 female, 9 male) volunteered
after receiving an email from a local mailing list.
They received no compensation for their participa-
tion and they were able to complete the experiment
in their preferred environment at a convenient time
by accessing the tool online. They were provided
with a tool guide and were encouraged to familiarize
themselves with the tool before beginning the tasks;
logs suggest 8 of 12 did exploration before starting.
The study required participants to find informa-
tion from a selection of 10,000 documents from
the New York Times Annotated Corpus (Sandhaus,
2008), which contains a range of metadata.5 All
5The full list of metadata fields that we allowed users to ac-
7
documents in the corpus were published in January
of 1995 and we made no effort at deduplication.
Topics were generated using the Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) implementation
in MALLET (McCallum, 2002). We used 100 top-
ics trained with 1500 Gibbs iterations and hyper-
parameter optimization.
3.2 Quantitative Results
The length of time required to complete individual
tasks ranged from 1 minute and 3 seconds to 24 min-
utes and 54 seconds (average 9 minutes.) 6
Within the scope of each task, each user initi-
ated on average 5.75 searches. The time between
searches was on average 1 minute and 53 seconds.
Of all the searches, 21.4% were new searches and
78.6% built on previous searches when users chose
to expand or narrow the scope of the search. When
users initiated new search queries, they began with
queries on topics 59.3% of the time, with queries on
metadata 37.3% of the time, and queries that used
both topics and metadata 3.4% of the time. This
lends credence to the claim that the ability to access
both metadata and topics is crucial.
We asked users to rate features in terms of their
usefulness on a Likert scale from 1 (not helpful at
all) to 5 (extremely helpful). The most preferred fea-
tures were filtering on topics (mean 4.217, median 5)
and compacted documents (mean 3.848, median 5)
The least preferred were document graphs of topic
usage (mean 1.848, median 1) and aggregate statis-
tics (mean 1.891, median 1).7 The fact that filtering
on topics was the most preferred feature validates
our approach of including topics as a facet. Addi-
tionally, topic names were critical to this success.
3.3 Surveys
Users provided qualitative feedback8 by describing
their approaches to the task, and offering sugges-
cess in the study was: online section, organization, news desk,
date, locations, series name, byline (author), people, title, fea-
ture page, and descriptors.
6These times do not include the 3 instances in which a user
felt unable to complete a task. Also omitted are 11 tasks (from
4 users) for which log files could not provide accurate times.
7Ratings are likely influenced by the specific nature of the
sample user tasks. In tasks that required seeking out metadata,
expanded document views rated higher than their average.
8The survey results presented here consist of one survey per
participant per task, with two exceptions where two participants
tions, the most common of which was an increase
in allowed query complexity, a feature we intend to
enhance. In the current version, all search terms are
combined using AND; 7 of the 12 participants made
requests for a NOT option.
Some users (6 of 12) admitted to using their
browser?s search feature to help complete the tasks.
We chose to forgo a keyword search capability in the
study-ready version of the tool because we wanted
to test the ability of topic information to provide a
way to navigate the content. Given the heavy us-
age of topic searches and the ability of users to com-
plete tasks with or without browser search, we have
demonstrated the usefulness of the topics as a win-
dow into the content. In future versions, we envision
incorporating keyword search capabilities, including
suggested topic filters for searched queries.
As users completed the tasks, their comfort with
the tool increased. One user wrote, ?After the last
task I knew exactly what to do to get my results. I
knew what information would help me find docu-
ments.? Users also began to suggest new ways that
they would like to see topics and metadata com-
bined. Task 4 led one user to say ?It would be in-
teresting to see a page on each author and what top-
ics they mostly covered.? We could provide this in a
general way by showing a page for each metadata at-
tribute that contains relevant topics and other meta-
data. We intend to implement such features.
4 Conclusion
A user evaluation of MetaToMATo, our toolkit for
visualizing text corpora that incorporates both topic
models and metadata, confirms the validity of our
approach to use topic models and metadata in a sin-
gle faceted browser. Users searched with topics a
majority of the time, but also made use of metadata.
This clearly demonstrates a reliance on both, sug-
gesting that users went back and forth as needed.
Additionally, while metadata is traditionally used for
facets, users ranked filtering by topic more highly
than metadata. This suggests a new direction in
which advances in topic models can be used to aid
corpus exploration.
each failed to record one of their four surveys.
8
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
M. Cataldi, L. Di Caro, and C. Schifanella. 2011. Im-
mex: Immersive text documents exploration system.
In Content-Based Multimedia Indexing (CBMI), 2011
9th International Workshop on, pages 1?6. IEEE.
A.J.B. Chaney and D.M. Blei. 2012. Visualizing topic
models. In AAAI.
J. Chuang, C.D. Manning, and J. Heer. 2012. Ter-
mite: visualization techniques for assessing textual
topic models. In Proceedings of the International
Working Conference on Advanced Visual Interfaces,
pages 74?77. ACM.
Christopher Collins, Fernanda B. Vie?gas, and Martin
Wattenberg. 2009. Parallel tag clouds to explore and
analyze faceted text corpora. In Proc. of the IEEE
Symp. on Visual Analytics Science and Technology
(VAST).
M. Correll, M. Witmore, and M. Gleicher. 2011. Explor-
ing collections of tagged text for literary scholarship.
Computer Graphics Forum, 30(3):731?740.
W. Cui, S. Liu, L. Tan, C. Shi, Y. Song, Z. Gao, H. Qu,
and X. Tong. 2011. Textflow: Towards better un-
derstanding of evolving topics in text. Visualiza-
tion and Computer Graphics, IEEE Transactions on,
17(12):2412?2421.
W. Dou, X. Wang, R. Chang, and W. Ribarsky. 2011.
Paralleltopics: A probabilistic approach to exploring
document collections. In Visual Analytics Science and
Technology (VAST), 2011 IEEE Conference on, pages
231?240. IEEE.
Jacob Eisenstein, Duen Horng ?Polo? Chau, Aniket Kit-
tur, and Eric P. Xing. 2012. Topicviz: Interactive topic
exploration in document collections. In CHI.
Jennifer English, Marti Hearst, Rashmi Sinha, Kirsten
Swearingen, and Ka-Ping Yee. 2002. Flexible search
and navigation using faceted metadata. In ACM SIGIR
Conference on Information Retrieval (SIGIR).
Ko Fujimura, Hiroyuki Toda, Takafumi Inoue, Nobuaki
Hiroshima, Ryoji Kataoka, and Masayuki Sugizaki.
2006. Blogranger - a multi-faceted blog search engine.
In World Wide Web (WWW).
Matthew J. Gardner, Joshua Lutes, Jeff Lund, Josh
Hansen, Dan Walker, Eric Ringger, and Kevin Seppi.
2010. The topic browser: An interactive tool for
browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.
T.L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences of the United States of America, 101(Suppl
1):5228?5235.
Marti Hearst. 2006. Clustering versus faceted categories
for information exploration. Communications of the
ACM, 49(4).
Mika Ka?ki. 2005. Findex: search result categories help
users when document ranking fails. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, CHI ?05, pages 131?140, New York,
NY, USA. ACM.
S. Liu, M.X. Zhou, S. Pan, W. Qian, W. Cai, and X. Lian.
2009. Interactive, topic-based visual text summariza-
tion and analysis. In Proceedings of the 18th ACM
conference on Information and knowledge manage-
ment, pages 543?552. ACM.
G. Marchionini. 2006. Exploratory search: from find-
ing to understanding. Communications of the ACM,
49(4):41?46.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
D. Mimno and D. Blei. 2011. Bayesian checking for
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 227?237. Association for Computational Lin-
guistics.
Eyal Oren, Renaud Delbru, and Stefan Decker. 2006.
Extending faceted navigation for rdf data. In Interna-
tional Semantic Web Conference (ISWC).
Evan Sandhaus. 2008. The new york times annotated
corpus.
Christian Seeling and Andreas Becks. 2003. Exploit-
ing metadata for ontology-based visual exploration of
weakly structured text documents. In Proceedings of
the 7th International Conference on Information Visu-
alisation (IV03, pages 0?7695. IEEE Press, ISBN.
Greg Smith, Mary Czerwinski, Brian Meyers, Daniel
Robbins, George Robertson, and Desney S. Tan. 2006.
FacetMap: A Scalable Search and Browse Visualiza-
tion. IEEE Transactions on Visualization and Com-
puter Graphics, 12(5):797?804.
E.M. Talley, D. Newman, D. Mimno, B.W. Herr II,
H.M. Wallach, G.A.P.C. Burns, A.G.M. Leenders, and
A. McCallum. 2011. Database of nih grants using
machine-learned categories and graphical clustering.
Nature Methods, 8(6):443?444.
Ping Yee, Kirsten Swearingen, Kevin Li, and Marti
Hearst. 2003. Faceted metadata for image search and
browsing. In Computer-Human Interaction (CHI).
9
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 712?721,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Sub-Word Units for Open Vocabulary Speech Recognition
Carolina Parada1, Mark Dredze1, Abhinav Sethy2, and Ariya Rastrow1
1Human Language Technology Center of Excellence, Johns Hopkins University
3400 N Charles Street, Baltimore, MD, USA
carolinap@jhu.edu, mdredze@cs.jhu.edu, ariya@jhu.edu
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
asethy@us.ibm.com
Abstract
Large vocabulary speech recognition systems
fail to recognize words beyond their vocab-
ulary, many of which are information rich
terms, like named entities or foreign words.
Hybrid word/sub-word systems solve this
problem by adding sub-word units to large vo-
cabulary word based systems; new words can
then be represented by combinations of sub-
word units. Previous work heuristically cre-
ated the sub-word lexicon from phonetic rep-
resentations of text using simple statistics to
select common phone sequences. We pro-
pose a probabilistic model to learn the sub-
word lexicon optimized for a given task. We
consider the task of out of vocabulary (OOV)
word detection, which relies on output from
a hybrid model. A hybrid model with our
learned sub-word lexicon reduces error by
6.3% and 7.6% (absolute) at a 5% false alarm
rate on an English Broadcast News and MIT
Lectures task respectively.
1 Introduction
Most automatic speech recognition systems operate
with a large but limited vocabulary, finding the most
likely words in the vocabulary for the given acoustic
signal. While large vocabulary continuous speech
recognition (LVCSR) systems produce high quality
transcripts, they fail to recognize out of vocabulary
(OOV) words. Unfortunately, OOVs are often infor-
mation rich nouns, such as named entities and for-
eign words, and mis-recognizing them can have a
disproportionate impact on transcript coherence.
Hybrid word/sub-word recognizers can produce a
sequence of sub-word units in place of OOV words.
Ideally, the recognizer outputs a complete word for
in-vocabulary (IV) utterances, and sub-word units
for OOVs. Consider the word ?Slobodan?, the given
name of the former president of Serbia. As an un-
common English word, it is unlikely to be in the vo-
cabulary of an English recognizer. While a LVCSR
system would output the closest known words (e.x.
?slow it dawn?), a hybrid system could output a
sequence of multi-phoneme units: s l ow, b ax,
d ae n. The latter is more useful for automatically
recovering the word?s orthographic form, identify-
ing that an OOV was spoken, or improving perfor-
mance of a spoken term detection system with OOV
queries. In fact, hybrid systems have improved OOV
spoken term detection (Mamou et al, 2007; Parada
et al, 2009), achieved better phone error rates, espe-
cially in OOV regions (Rastrow et al, 2009b), and
obtained state-of-the-art performance for OOV de-
tection (Parada et al, 2010).
Hybrid recognizers vary in a number of ways:
sub-word unit type: variable-length phoneme
units (Rastrow et al, 2009a; Bazzi and Glass, 2001)
or joint letter sound sub-words (Bisani and Ney,
2005); unit creation: data-driven or linguistically
motivated (Choueiter, 2009); and how they are in-
corporated in LVCSR systems: hierarchical (Bazzi,
2002) or flat models (Bisani and Ney, 2005).
In this work, we consider how to optimally cre-
ate sub-word units for a hybrid system. These units
are variable-length phoneme sequences, although in
principle our work can be use for other unit types.
Previous methods for creating the sub-word lexi-
712
con have relied on simple statistics computed from
the phonetic representation of text (Rastrow et al,
2009a). These units typically represent the most fre-
quent phoneme sequences in English words. How-
ever, it isn?t clear why these units would produce the
best hybrid output. Instead, we introduce a prob-
abilistic model for learning the optimal units for a
given task. Our model learns a segmentation of a
text corpus given some side information: a mapping
between the vocabulary and a label set; learned units
are predictive of class labels.
In this paper, we learn sub-word units optimized
for OOV detection. OOV detection aims to identify
regions in the LVCSR output where OOVs were ut-
tered. Towards this goal, we are interested in select-
ing units such that the recognizer outputs them only
for OOV regions while prefering to output a com-
plete word for in-vocabulary regions. Our approach
yields improvements over state-of-the-art results.
We begin by presenting our log-linear model for
learning sub-word units with a simple but effective
inference procedure. After reviewing existing OOV
detection approaches, we detail how the learned
units are integrated into a hybrid speech recognition
system. We show improvements in OOV detection,
and evaluate impact on phone error rates.
2 Learning Sub-Word Units
Given raw text, our objective is to produce a lexicon
of sub-word units that can be used by a hybrid sys-
tem for open vocabulary speech recognition. Rather
than relying on the text alone, we also utilize side
information: a mapping of words to classes so we
can optimize learning for a specific task.
The provided mapping assigns labels Y to the cor-
pus. We maximize the probability of the observed
labeling sequence Y given the text W : P (Y |W ).
We assume there is a latent segmentation S of this
corpus which impacts Y . The complete data likeli-
hood becomes: P (Y |W ) =
?
S P (Y, S|W ) during
training. Since we are maximizing the observed Y ,
segmentation S must discriminate between different
possible labels.
We learn variable-length multi-phone units by
segmenting the phonetic representation of each word
in the corpus. Resulting segments form the sub-
word lexicon.1 Learning input includes a list of
words to segment taken from raw text, a mapping
between words and classes (side information indi-
cating whether token is IV or OOV), a pronuncia-
tion dictionaryD, and a letter to sound model (L2S),
such as the one described in Chen (2003). The cor-
pus W is the list of types (unique words) in the raw
text input. This forces each word to have a unique
segmentation, shared by all common tokens. Words
are converted into phonetic representations accord-
ing to their most likely dictionary pronunciation;
non-dictionary words use the L2S model.2
2.1 Model
Inspired by the morphological segmentation model
of Poon et al (2009), we assume P (Y, S|W ) is a
log-linear model parameterized by ?:
P?(Y, S|W ) =
1
Z(W )
u?(Y, S,W ) (1)
where u?(Y, S,W ) defines the score of the pro-
posed segmentation S for words W and labels Y
according to model parameters ?. Sub-word units
? compose S, where each ? is a phone sequence, in-
cluding the full pronunciation for vocabulary words;
the collection of ?s form the lexicon. Each unit
? is present in a segmentation with some context
c = (?l, ?r) of the form ?l??r. Features based on
the context and the unit itself parameterize u?.
In addition to scoring a segmentation based on
features, we include two priors inspired by the Min-
imum Description Length (MDL) principle sug-
gested by Poon et al (2009). The lexicon prior
favors smaller lexicons by placing an exponential
prior with negative weight on the length of the lex-
icon
?
? |?|, where |?| is the length of the unit ?
in number of phones. Minimizing the lexicon prior
favors a trivial lexicon of only the phones. The
corpus prior counters this effect, an exponential
prior with negative weight on the number of units
in each word?s segmentation, where |si| is the seg-
mentation length and |wi| is the length of the word
in phones. Learning strikes a balance between the
two priors. Using these definitions, the segmenta-
tion score u?(Y, S,W ) is given as:
1Since sub-word units can expand full-words, we refer to
both words and sub-words simply as units.
2The model can also take multiple pronunciations (?3.1).
713
s l ow b ax d ae n
s l ow
(#,#, , b, ax)
b ax
(l,ow, , d, ae)
d ae n
(b,ax, , #, #)
Figure 1: Units and bigram phone context (in parenthesis)
for an example segmentation of the word ?slobodan?.
u?(Y, S,W ) = exp
(
?
?,y
??,yf?,y(S, Y )
+
?
c,y
?c,yfc,y(S, Y )
+ ? ?
?
??S
|?|
+ ? ?
?
i?W
|si|/|wi|
)
(2)
f?,y(S, Y ) are the co-occurrence counts of the pair
(?, y) where ? is a unit under segmentation S and y
is the label. fc,y(S, Y ) are the co-occurrence counts
for the context c and label y under S. The model
parameters are ? = {??,y, ?c,y : ??, c, y}. The neg-
ative weights for the lexicon (?) and corpus priors
(?) are tuned on development data. The normalizer
Z sums over all possible segmentations and labels:
Z(W ) =
?
S?
?
Y ?
u?(Y
?, S?,W ) (3)
Consider the example segmentation for the word
?slobodan? with pronunciation s,l,ow,b,ax,d,ae,n
(Figure 1). The bigram phone context as a four-tuple
appears below each unit; the first two entries corre-
spond to the left context, and last two the right con-
text. The example corpus (Figure 2) demonstrates
how unit features f?,y and context features fc,y are
computed.
3 Model Training
Learning maximizes the log likelihood of the ob-
served labels Y ? given the words W :
`(Y ?|W ) = log
?
S
1
Z(W )
u?(Y
?, S,W ) (4)
We use the Expectation-Maximization algorithm,
where the expectation step predicts segmentations S
Labeled corpus: president/y = 0 milosevic/y = 1
Segmented corpus: p r eh z ih d ih n t/0 m ih/1 l aa/1
s ax/1 v ih ch/1
Unit-feature:Value p r eh z ih d ih n t/0:1 m ih/1:1
l aa/1:1 s ax/1:1 v ih ch/1:1
Context-feature:Value
(#/0,#/0, ,l/1,aa/1):1,
(m/1,ih/1, ,s/1,ax/1):1,
(l/1,aa/1, ,v/1,ih/1):1,
(s/1,ax/1, ,#/0,#/0):1,
(#/0,#/0, ,#/0,#/0):1
Figure 2: A small example corpus with segmentations
and corresponding features. The notation m ih/1:1
represents unit/label:feature-value. Overlapping context
features capture rich segmentation regularities associated
with each class.
given the model?s current parameters ? (?3.1), and
the maximization step updates these parameters us-
ing gradient ascent. The partial derivatives of the
objective (4) with respect to each parameter ?i are:
?`(Y ?|W )
??i
= ES|Y ?,W [fi]? ES,Y |W [fi] (5)
The gradient takes the usual form, where we en-
courage the expected segmentation from the current
model given the correct labels to equal the expected
segmentation and expected labels. The next section
discusses computing these expectations.
3.1 Inference
Inference is challenging since the lexicon prior ren-
ders all word segmentations interdependent. Con-
sider a simple two word corpus: cesar (s,iy,z,er),
and cesium (s,iy,z,iy,ax,m). Numerous segmen-
tations are possible; each word has 2N?1 possible
segmentations, where N is the number of phones in
its pronunciation (i.e., 23 ? 25 = 256). However,
if we decide to segment the first word as: {s iy,
z er}, then the segmentation for ?cesium?:{s iy,
z iy ax m} will incur a lexicon prior penalty for
including the new segment z iy ax m. If instead
we segment ?cesar? as {s iy z, er}, the segmen-
tation {s iy, z iy ax m} incurs double penalty
for the lexicon prior (since we are including two new
units in the lexicon: s iy and z iy ax m). This
dependency requires joint segmentation of the entire
corpus, which is intractable. Hence, we resort to ap-
proximations of the expectations in Eq. (5).
One approach is to use Gibbs Sampling: it-
erating through each word, sampling a new seg-
714
mentation conditioned on the segmentation of all
other words. The sampling distribution requires
enumerating all possible segmentations for each
word (2N?1) and computing the conditional prob-
abilities for each segmentation: P (S|Y ?,W ) =
P (Y ?, S|W )/P (Y ?|W ) (the features are extracted
from the remaining words in the corpus). Using M
sampled segmentations S1, S2, . . . Sm we compute
ES|Y ?,W [fi] as follows:
ES|Y ?,W [fi] ?
1
M
?
j
fi[Sj ]
Similarly, to compute ES,Y |W we sample a seg-
mentation and a label for each word. We com-
pute the joint probability of P (Y, S|W ) for each
segmentation-label pair using Eq. (1). A sampled
segmentation can introduce new units, which may
have higher probability than existing ones.
Using these approximations in Eq. (5), we update
the parameters using gradient ascent:
??new = ??old + ??`??(Y
?|W )
where ? > 0 is the learning rate.
To obtain the best segmentation, we use determin-
istic annealing. Sampling operates as usual, except
that the parameters are divided by a value, which
starts large and gradually drops to zero. To make
burn in faster for sampling, the sampler is initialized
with the most likely segmentation from the previous
iteration. To initialize the sampler the first time, we
set al the parameters to zero (only the priors have
non-zero values) and run deterministic annealing to
obtain the first segmentation of the corpus.
3.2 Efficient Sampling
Sampling a segmentation for the corpus requires
computing the normalization constant (3), which
contains a summation over all possible corpus seg-
mentations. Instead, we approximate this constant
by sampling words independently, keeping fixed all
other segmentations. Still, even sampling a single
word?s segmentation requires enumerating probabil-
ities for all possible segmentations.
We sample a segmentation efficiently using dy-
namic programming. We can represent all possible
segmentations for a word as a finite state machine
(FSM) (Figure 3), where arcs weights arise from
scoring the segmentation?s features. This weight is
the negative log probability of the resulting model
after adding the corresponding features and priors.
However, the lexicon prior poses a problem for
this construction since the penalty incurred by a new
unit in the segmentation depends on whether that
unit is present elsewhere in that segmentation. For
example, consider the segmentation for the word
ANJANI: AA N, JH, AA N, IY. If none of these units
are in the lexicon, this segmentation yields the low-
est prior penalty since it repeats the unit AA N. 3 This
global dependency means paths must encode the full
unit history, making computing forward-backward
probabilities inefficient.
Our solution is to use the Metropolis-Hastings al-
gorithm, which samples from the true distribution
P (Y, S|W ) by first sampling a new label and seg-
mentation (y?, s?) from a simpler proposal distribu-
tion Q(Y, S|W ). The new assignment (y?, s?) is ac-
cepted with probability:
?(Y ?, S?|Y, S,W )=min
?
1,
P (Y ?, S?|W )Q(Y, S|Y ?, S?,W )
P (Y, S|W )Q(Y ?, S?|Y, S,W )
?
We choose the proposal distribution Q(Y, S|W )
as Eq. (1) omitting the lexicon prior, removing the
challenge for efficient computation. The probability
of accepting a sample becomes:
?(Y ?, S?|Y, S,W )=min
?
1,
P
??S? |?|P
??S |?|
?
(6)
We sample a path from the FSM by running the
forward-backward algorithm, where the backward
computations are carried out explicitly, and the for-
ward pass is done through sampling, i.e. we traverse
the machine only computing forward probabilities
for arcs leaving the sampled state.4 Once we sample
a segmentation (and label) we accept it according to
Eq. (6) or keep the previous segmentation if rejected.
Alg. 1 shows our full sub-word learning proce-
dure, where sampleSL (Alg. 2) samples a segmen-
tation and label sequence for the entire corpus from
P (Y, S|W ), and sampleS samples a segmentation
from P (S|Y ?,W ).
3Splitting at phone boundaries yields the same lexicon prior
but a higher corpus prior.
4We use OpenFst?s RandGen operation with a costumed arc-
selector (http://www.openfst.org/).
715
0 1AA
5
AA_N_JH_AA_N
4
AA_N_JH_AA
3
AA_N_JH 2
AA_N
N_JH_AA_N
N_JH_AA
N_JH
N
6
N_JH_AA_N_IY
IY
N
AA_NAA
AA_N_IY
JH_AA_N
JH_AA
JH
JH_AA_N_IY
Figure 3: FSM representing all segmentations for the word ANJANI with pronunciation: AA,N,JH,AA,N,IY
Algorithm 1 Training
Input: Lexicon L from training text W , Dictionary D,
Mapping M , L2S pronunciations, Annealing temp T .
Initialization:
Assign label y?m = M [wm]. ??0 = 0?
S0 = random segmentation for each word in L.
for i = 1 to K do
/* E-Step */
Si = bestSegmentation(T, ?i?1, Si?1).
for k = 1 to NumSamples do
(S?k, Y
?
k) = sampleSL(P (Y, Si|W ),Q(Y, Si|W ))
S?k = sampleS(P (Si|Y ?,W ),Q(Si|Y ?,W ))
end for
/* M-Step */
ES,Y |W [fi] =
1
NumSamples
?
k f?,l[S
?
k, Y
?
k]
ES|Y ?,W [f?,l] =
1
NumSamples
?
k f?,l[S?k, Y
?]
??i = ??i?1 + ??L??(Y
?|W )
end for
S = bestSegmentation(T, ?K , S0)
Output: Lexicon Lo from S
4 OOV Detection Using Hybrid Models
To evaluate our model for learning sub-word units,
we consider the task of out-of-vocabulary (OOV)
word detection. OOV detection for ASR output can
be categorized into two broad groups: 1) hybrid
(filler) models: which explicitly model OOVs us-
ing either filler, sub-words, or generic word mod-
els (Bazzi, 2002; Schaaf, 2001; Bisani and Ney,
2005; Klakow et al, 1999; Wang, 2009); and
2) confidence-based approaches: which label un-
reliable regions as OOVs based on different con-
fidence scores, such as acoustic scores, language
models, and lattice scores (Lin et al, 2007; Burget
et al, 2008; Sun et al, 2001; Wessel et al, 2001).
In the next section we detail the OOV detection
approach we employ, which combines hybrid and
Algorithm 2 sampleSL(P (S, Y |W ), Q(S, Y |W ))
for m = 1 to M (NumWords) do
(s?m, y
?
m) = Sample segmentation/label pair for
word wm according to Q(S, Y |W )
Y ? = {y1 . . . ym?1y?mym+1 . . . yM}
S? = {s1 . . . sm?1s?msm+1 . . . sM}
?=min
(
1,
P
??S? |?|P
??S |?|
)
with prob ? : ym,k = y?m, sm,k = s
?
m
with prob (1? ?) : ym,k = ym, sm,k = sm
end for
return (S?k, Y
?
k) = [(s1,k, y1,k) . . . (sM,k, yM,k)]
confidence-based models, achieving state-of-the art
performance for this task.
4.1 OOV Detection Approach
We use the state-of-the-art OOV detection model of
Parada et al (2010), a second order CRF with fea-
tures based on the output of a hybrid recognizer.
This detector processes hybrid recognizer output, so
we can evaluate different sub-word unit lexicons for
the hybrid recognizer and measure the change in
OOV detection accuracy.
Our model (?2.1) can be applied to this task by
using a dictionary D to label words as IV (yi = 0 if
wi ? D) and OOV (yi = 1 if wi /? D). This results
in a labeled corpus, where the labeling sequence Y
indicates the presence of out-of-vocabulary words
(OOVs). For comparison we evaluate a baseline
method (Rastrow et al, 2009b) for selecting units.
Given a sub-word lexicon, the word and sub-
words are combined to form a hybrid language
model (LM) to be used by the LVCSR system. This
hybrid LM captures dependencies between word and
sub-words. In the LM training data, all OOVs are
represented by the smallest number of sub-words
which corresponds to their pronunciation. Pronun-
ciations for all OOVs are obtained using grapheme
716
to phone models (Chen, 2003).
Since sub-words represent OOVs while building
the hybrid LM, the existence of sub-words in ASR
output indicate an OOV region. A simple solution to
the OOV detection problem would then be reduced
to a search for the sub-words in the output of the
ASR system. The search can be on the one-best
transcripts, lattices or confusion networks. While
lattices contain more information, they are harder
to process; confusion networks offer a trade-off be-
tween richness (posterior probabilities are already
computed) and compactness (Mangu et al, 1999).
Two effective indications of OOVs are the exis-
tence of sub-words (Eq. 7) and high entropy in a
network region (Eq. 8), both of which are used as
features in the model of Parada et al (2010).
Sub-word Posterior =
?
??tj
p(?|tj) (7)
Word-Entropy =?
?
w?tj
p(w|tj) log p(w|tj) (8)
tj is the current bin in the confusion network and
? is a sub-word in the hybrid dictionary. Improving
the sub-word unit lexicon, improves the quality of
the confusion networks for OOV detection.
5 Experimental Setup
We used the data set constructed by Can et al
(2009) (OOVCORP) for the evaluation of Spoken
Term Detection of OOVs since it focuses on the
OOV problem. The corpus contains 100 hours of
transcribed Broadcast News English speech. There
are 1290 unique OOVs in the corpus, which were
selected with a minimum of 5 acoustic instances per
word and short OOVs inappropriate for STD (less
than 4 phones) were explicitly excluded. Example
OOVs include: NATALIE, PUTIN, QAEDA,
HOLLOWAY, COROLLARIES, HYPERLINKED,
etc. This resulted in roughly 24K (2%) OOV tokens.
For LVCSR, we used the IBM Speech Recogni-
tion Toolkit (Soltau et al, 2005)5 to obtain a tran-
script of the audio. Acoustic models were trained
on 300 hours of HUB4 data (Fiscus et al, 1998)
and utterances containing OOV words as marked in
OOVCORP were excluded. The language model was
trained on 400M words from various text sources
5The IBM system used speaker adaptive training based on
maximum likelihood with no discriminative training.
with a 83K word vocabulary. The LVCSR system?s
WER on the standard RT04 BN test set was 19.4%.
Excluded utterances amount to 100hrs. These were
divided into 5 hours of training for the OOV detec-
tor and 95 hours of test. Note that the OOV detector
training set is different from the LVCSR training set.
We also use a hybrid LVCSR system, combin-
ing word and sub-word units obtained from ei-
ther our approach or a state-of-the-art baseline ap-
proach (Rastrow et al, 2009a) (?5.2). Our hybrid
system?s lexicon has 83K words and 5K or 10K
sub-words. Note that the word vocabulary is com-
mon to both systems and only the sub-words are se-
lected using either approach. The word vocabulary
used is close to most modern LVCSR system vo-
cabularies for English Broadcast News; the result-
ing OOVs are more challenging but more realistic
(i.e. mostly named entities and technical terms). The
1290 words are OOVs to both the word and hybrid
systems.
In addition we report OOV detection results on a
MIT lectures data set (Glass et al, 2010) consisting
of 3 Hrs from two speakers with a 1.5% OOV rate.
These were divided into 1 Hr for training the OOV
detector and 2 Hrs for testing. Note that the LVCSR
system is trained on Broadcast News data. This out-
of-domain test-set help us evaluate the cross-domain
performance of the proposed and baseline hybrid
systems. OOVs in this data set correspond mainly to
technical terms in computer science and math. e.g.
ALGORITHM, DEBUG, COMPILER, LISP.
5.1 Learning parameters
For learning the sub-words we randomly selected
from training 5,000 words which belong to the 83K
vocabulary and 5,000 OOVs6. For development we
selected an additional 1,000 IV and 1,000 OOVs.
This was used to tune our model hyper parameters
(set to ? = ?1, ? = ?20). There is no overlap
of OOVs in training, development and test sets. All
feature weights were initialized to zero and had a
Gaussian prior with variance ? = 100. Each of the
words in training and development was converted to
their most-likely pronunciation using the dictionary
6This was used to obtain the 5K hybrid system. To learn sub-
words for the 10K hybrid system we used 10K in-vocabulary
words and 10K OOVs. All words were randomly selected from
the LM training text.
717
for IV words or the L2S model for OOVs.7
The learning rate was ?k =
?
(k+1+A)? , where k is
the iteration,A is the stability constant (set to 0.1K),
? = 0.4, and ? = 0.6. We used K = 40 itera-
tions for learning and 200 samples to compute the
expectations in Eq. 5. The sampler was initialized
by sampling for 500 iterations with deterministic an-
nealing for a temperature varying from 10 to 0 at 0.1
intervals. Final segmentations were obtained using
10, 000 samples and the same temperature schedule.
We limit segmentations to those including units of at
most 5 phones to speed sampling with no significant
degradation in performance. We observed improved
performance by dis-allowing whole word units.
5.2 Baseline Unit Selection
We used Rastrow et al (2009a) as our baseline
unit selection method, a data driven approach where
the language model training text is converted into
phones using the dictionary (or a letter-to-sound
model for OOVs), and a N-gram phone LM is es-
timated on this data and pruned using a relative en-
tropy based method. The hybrid lexicon includes
resulting sub-words ? ranging from unigrams to 5-
gram phones, and the 83K word lexicon.
5.3 Evaluation
We obtain confusion networks from both the word
and hybrid LVCSR systems. We align the LVCSR
transcripts with the reference transcripts and tag
each confusion region as either IV or OOV. The
OOV detector classifies each region in the confusion
network as IV/OOV. We report OOV detection accu-
racy using standard detection error tradeoff (DET)
curves (Martin et al, 1997). DET curves measure
tradeoffs between false alarms (x-axis) and misses
(y-axis), and are useful for determining the optimal
operating point for an application; lower curves are
better. Following Parada et al (2010) we separately
evaluate unobserved OOVs.8
7In this work we ignore pronunciation variability and sim-
ply consider the most likely pronunciation for each word. It
is straightforward to extend to multiple pronunciations by first
sampling a pronunciation for each word and then sampling a
segmentation for that pronunciation.
8Once an OOV word has been observed in the OOV detector
training data, even if it was not in the LVCSR training data, it is
no longer truly OOV.
6 Results
We compare the performance of a hybrid sys-
tem with baseline units9 (?5.2) and one with units
learned by our model on OOV detection and phone
error rate. We present results using a hybrid system
with 5k and 10k sub-words.
We evaluate the CRF OOV detector with two dif-
ferent feature sets. The first uses only Word En-
tropy and Sub-word Posterior (Eqs. 7 and 8) (Fig-
ure 4)10. The second (context) uses the extended
context features of Parada et al (2010) (Figure 5).
Specifically, we include all trigrams obtained from
the best hypothesis of the recognizer (a window of 5
words around current confusion bin). Predictions at
different FA rates are obtained by varying a proba-
bility threshold.
At a 5% FA rate, our system (This Paper 5k) re-
duces the miss OOV rate by 6.3% absolute over the
baseline (Baseline 5k) when evaluating all OOVs.
For unobserved OOVs, it achieves 3.6% absolute
improvement. A larger lexicon (Baseline 10k and
This Paper 10k ) shows similar relative improve-
ments. Note that the features used so far do not nec-
essarily provide an advantage for unobserved ver-
sus observed OOVs, since they ignore the decoded
word/sub-word sequence. In fact, the performance
on un-observed OOVs is better.
OOV detection improvements can be attributed to
increased coverage of OOV regions by the learned
sub-words compared to the baseline. Table 1 shows
the percent of Hits: sub-word units predicted in
OOV regions, and False Alarms: sub-word units
predicted for in-vocabulary words. We can see
that the proposed system increases the Hits by over
8% absolute, while increasing the False Alarms by
0.3%. Interestingly, the average sub-word length
for the proposed units exceeded that of the baseline
units by 0.3 phones (Baseline 5K average length
was 2.92, while that of This Paper 5K was 3.2).
9Our baseline results differ from Parada et al (2010). When
implementing the lexicon baseline, we discovered that their hy-
brid units were mistakenly derived from text containing test
OOVs. Once excluded, the relative improvements of previous
work remain, but the absolute error rates are higher.
10All real-valued features were normalized and quantized us-
ing the uniform-occupancy partitioning described in White et
al. (2007). We used 50 partitions with a minimum of 100 train-
ing values per partition.
718
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(a)
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(b)
Figure 4: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-
criminative hybrid system on OOVCORP data set. Evaluation on un-observed OOVs (a) and all OOVs (b).
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(a)
0 5 10 15 20%FA10
20
30
40
50
60
70
80
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(b)
Figure 5: Effect of adding context features to baseline and discriminative hybrid systems on OOVCORP data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).
Consistent with previously published results, in-
cluding context achieves large improvement in per-
formance. The proposed hybrid system (This Pa-
per 10k + context-features) still improves over the
baseline (Baseline 10k + context-features), however
the relative gain is reduced. In this case, we ob-
tain larger gains for un-observed OOVs which ben-
efit less from the context clues learned in training.
Lastly, we report OOV detection performance on
MIT Lectures. Both the sub-word lexicon and the
LVCSR models were trained on Broadcast News
data, helping us evaluate the robustness of learned
sub-words across domains. Note that the OOVs
in these domains are quite different: MIT Lec-
tures? OOVs correspond to technical computer sci-
Hybrid System Hits FAs
Baseline (5k) 18.25 1.49
This Paper (5k) 26.78 1.78
Baseline (10k) 24.26 1.82
This Paper (10k) 28.96 1.92
Table 1: Coverage of OOV regions by baseline and pro-
posed sub-words in OOVCORP.
ence and math terms, while in Broadcast News they
are mainly named-entities.
Figure 6 and 7 show the OOV detection results in
the MIT Lectures data set. For un-observed OOVs,
the proposed system (This Paper 10k) reduces the
miss OOV rate by 7.6% with respect to the base-
line (Baseline 10k) at a 5% FA rate. Similar to
Broadcast News results, we found that the learned
sub-words provide larger coverage of OOV regions
in MIT Lectures domain. These results suggest that
the proposed sub-words are not simply modeling the
training OOVs (named-entities) better than the base-
line sub-words, but also describe better novel unex-
pected words. Furthermore, including context fea-
tures does not seem as helpful. We conjecture that
this is due to the higher WER11 and the less struc-
tured nature of the domain: i.e. ungrammatical sen-
tences, disfluencies, incomplete sentences, making
it more difficult to predict OOVs based on context.
11WER = 32.7% since the LVCSR system was trained on
Broadcast News data as described in Section 5.
719
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(a)
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(b)
Figure 6: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-
criminative hybrid system on MIT Lectures data set. Evaluation on un-observed OOVs (a) and all OOVs (b).
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(a)
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(b)
Figure 7: Effect of adding context features to baseline and discriminative hybrid systems on MIT Lectures data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).
6.1 Improved Phonetic Transcription
We consider the hybrid lexicon?s impact on Phone
Error Rate (PER) with respect to the reference tran-
scription. The reference phone sequence is obtained
by doing forced alignment of the audio stream to the
reference transcripts using acoustic models. This
provides an alignment of the pronunciation variant
of each word in the reference and the recognizer?s
one-best output. The aligned words are converted to
the phonetic representation using the dictionary.
Table 2 presents PERs for the word and differ-
ent hybrid systems. As previously reported (Ras-
trow et al, 2009b), the hybrid systems achieve bet-
ter PER, specially in OOV regions since they pre-
dict sub-word units for OOVs. Our method achieves
modest improvements in PER compared to the hy-
brid baseline. No statistically significant improve-
ments in PER were observed on MIT Lectures.
7 Conclusions
Our probabilistic model learns sub-word units for
hybrid speech recognizers by segmenting a text cor-
pus while exploiting side information. Applying our
System OOV IV All
Word 1.62 6.42 8.04
Hybrid: Baseline (5k) 1.56 6.44 8.01
Hybrid: Baseline (10k) 1.51 6.41 7.92
Hybrid: This Paper (5k) 1.52 6.42 7.94
Hybrid: This Paper (10k) 1.45 6.39 7.85
Table 2: Phone Error Rate for OOVCORP.
method to the task of OOV detection, we obtain an
absolute error reduction of 6.3% and 7.6% at a 5%
false alarm rate on an English Broadcast News and
MIT Lectures task respectively, when compared to a
baseline system. Furthermore, we have confirmed
previous work that hybrid systems achieve better
phone accuracy, and our model makes modest im-
provements over a baseline with a similarly sized
sub-word lexicon. We plan to further explore our
new lexicon?s performance for other languages and
tasks, such as OOV spoken term detection.
Acknowledgments
We gratefully acknowledge Bhuvaha Ramabhadran
for many insightful discussions and the anonymous
reviewers for their helpful comments. This work
was funded by a Google PhD Fellowship.
720
References
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
eling. In EuroSpeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
M. Bisani and H. Ney. 2005. Open vocabulary speech
recognition with flat hybrid models. In INTER-
SPEECH.
L. Burget, P. Schwarz, P. Matejka, M. Hannemann,
A. Rastrow, C. White, S. Khudanpur, H. Hermansky,
and J. Cernocky. 2008. Combination of strongly and
weakly constrained recognizers for reliable detection
of OOVS. In ICASSP.
D. Can, E. Cooper, A. Sethy, M. Saraclar, and C. White.
2009. Effect of pronounciations on OOV queries in
spoken term detection. Proceedings of ICASSP.
Stanley F. Chen. 2003. Conditional and joint models
for grapheme-to-phoneme conversion. In Eurospeech,
pages 2033?2036.
G. Choueiter. 2009. Linguistically-motivated sub-
word modeling with applications to speech recogni-
tion. Ph.D. thesis, Massachusetts Institute of Technol-
ogy.
Jonathan Fiscus, John Garofolo, Mark Przybocki,
William Fisher, and David Pallett, 1998. 1997 En-
glish Broadcast News Speech (HUB4). Linguistic
Data Consortium, Philadelphia.
James Glass, Timothy Hazen, Lee Hetherington, and
Chao Wang. 2010. Analysis and processing of lec-
ture audio data: Preliminary investigations. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999.
OOV-detection in large vocabulary system using au-
tomatically defined word-fragments as fillers. In Eu-
rospeech.
Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007.
OOV detection by joint word/phone lattice alignment.
In ASRU, pages 478?483, Dec.
Jonathan Mamou, Bhuvana Ramabhadran, and Olivier
Siohan. 2007. Vocabulary independent spoken term
detection. In Proceedings of SIGIR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words. In Eurospeech.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocky. 1997. The det curve in assessment of
detection task performance. In Eurospeech.
Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009. Query-by-example spoken term detec-
tion for oov terms. In ASRU.
Carolina Parada, Mark Dredze, Denis Filimonov, and
Fred Jelinek. 2010. Contextual information improves
oov detection in speech. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsu-
pervised morphological segmentation with log-linear
models. In ACL.
Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009a. A new method for OOV detection
using hybrid word/fragment system. Proceedings of
ICASSP.
Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,
and Fred Jelinek. 2009b. Towards using hybrid,
word, and fragment units for vocabulary independent
LVCSR systems. INTERSPEECH.
T. Schaaf. 2001. Detection of OOV words using gen-
eralized word models and a semantic class language
model. In Eurospeech.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The ibm 2004 conversational
telephony system for rich transcription. In ICASSP.
H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using
word confidence measure for OOV words detection in
a spontaneous spoken dialog system. In Eurospeech.
Stanley Wang. 2009. Using graphone models in au-
tomatic speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001.
Confidence measures for large vocabulary continuous
speech recognition. IEEE Transactions on Speech and
Audio Processing, 9(3).
Christopher White, Jasha Droppo, Alex Acero, and Ju-
lian Odell. 2007. Maximum entropy confidence esti-
mation for speech recognition. In ICASSP.
721
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175?183,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast Syntactic Analysis for Statistical Language Modeling
via Substructure Sharing and Uptraining
Ariya Rastrow, Mark Dredze, Sanjeev Khudanpur
Human Language Technology Center of Excellence
Center for Language and Speech Processing, Johns Hopkins University
Baltimore, MD USA
{ariya,mdredze,khudanpur}@jhu.edu
Abstract
Long-span features, such as syntax, can im-
prove language models for tasks such as
speech recognition and machine translation.
However, these language models can be dif-
ficult to use in practice because of the time
required to generate features for rescoring a
large hypothesis set. In this work, we pro-
pose substructure sharing, which saves dupli-
cate work in processing hypothesis sets with
redundant hypothesis structures. We apply
substructure sharing to a dependency parser
and part of speech tagger to obtain significant
speedups, and further improve the accuracy
of these tools through up-training. When us-
ing these improved tools in a language model
for speech recognition, we obtain significant
speed improvements with bothN -best and hill
climbing rescoring, and show that up-training
leads to WER reduction.
1 Introduction
Language models (LM) are crucial components in
tasks that require the generation of coherent natu-
ral language text, such as automatic speech recog-
nition (ASR) and machine translation (MT). While
traditional LMs use word n-grams, where the n ? 1
previous words predict the next word, newer mod-
els integrate long-span information in making deci-
sions. For example, incorporating long-distance de-
pendencies and syntactic structure can help the LM
better predict words by complementing the predic-
tive power of n-grams (Chelba and Jelinek, 2000;
Collins et al, 2005; Filimonov and Harper, 2009;
Kuo et al, 2009).
The long-distance dependencies can be modeled
in either a generative or a discriminative framework.
Discriminative models, which directly distinguish
correct from incorrect hypothesis, are particularly
attractive because they allow the inclusion of arbi-
trary features (Kuo et al, 2002; Roark et al, 2007;
Collins et al, 2005); these models with syntactic in-
formation have obtained state of the art results.
However, both generative and discriminative LMs
with long-span dependencies can be slow, for they
often cannot work directly with lattices and require
rescoring large N -best lists (Khudanpur and Wu,
2000; Collins et al, 2005; Kuo et al, 2009). For dis-
criminative models, this limitation applies to train-
ing as well. Moreover, the non-local features used in
rescoring are usually extracted via auxiliary tools ?
which in the case of syntactic features include part of
speech taggers and parsers ? from a set of ASR sys-
tem hypotheses. Separately applying auxiliary tools
to each N -best list hypothesis leads to major ineffi-
ciencies as many hypotheses differ only slightly.
Recent work on hill climbing algorithms for ASR
lattice rescoring iteratively searches for a higher-
scoring hypothesis in a local neighborhood of the
current-best hypothesis, leading to a much more ef-
ficient algorithm in terms of the number, N , of hy-
potheses evaluated (Rastrow et al, 2011b); the idea
also leads to a discriminative hill climbing train-
ing algorithm (Rastrow et al, 2011a). Even so, the
reliance on auxiliary tools slow LM application to
the point of being impractical for real time systems.
While faster auxiliary tools are an option, they are
usually less accurate.
In this paper, we propose a general modifica-
175
tion to the decoders used in auxiliary tools to uti-
lize the commonalities among the set of generated
hypotheses. The key idea is to share substructure
states in transition based structured prediction al-
gorithms, i.e. algorithms where final structures are
composed of a sequence of multiple individual deci-
sions. We demonstrate our approach on a local Per-
ceptron based part of speech tagger (Tsuruoka et al,
2011) and a shift reduce dependency parser (Sagae
and Tsujii, 2007), yielding significantly faster tag-
ging and parsing of ASR hypotheses. While these
simpler structured prediction models are faster, we
compensate for the model?s simplicity through up-
training (Petrov et al, 2010), yielding auxiliary tools
that are both fast and accurate. The result is signif-
icant speed improvements and a reduction in word
error rate (WER) for both N -best list and the al-
ready fast hill climbing rescoring. The net result
is arguably the first syntactic LM fast enough to be
used in a real time ASR system.
2 Syntactic Language Models
There have been several approaches to include syn-
tactic information in both generative and discrimi-
native language models.
For generative LMs, the syntactic information
must be part of the generative process. Structured
language modeling incorporates syntactic parse
trees to identify the head words in a hypothesis for
modeling dependencies beyond n-grams. Chelba
and Jelinek (2000) extract the two previous exposed
head words at each position in a hypothesis, along
with their non-terminal tags, and use them as con-
text for computing the probability of the current po-
sition. Khudanpur and Wu (2000) exploit such syn-
tactic head word dependencies as features in a maxi-
mum entropy framework. Kuo et al (2009) integrate
syntactic features into a neural network LM for Ara-
bic speech recognition.
Discriminative models are more flexible since
they can include arbitrary features, allowing for
a wider range of long-span syntactic dependen-
cies. Additionally, discriminative models are di-
rectly trained to resolve the acoustic confusion in the
decoded hypotheses of an ASR system. This flexi-
bility and training regime translate into better perfor-
mance. Collins et al (2005) uses the Perceptron al-
gorithm to train a global linear discriminative model
which incorporates long-span features, such as head-
to-head dependencies and part of speech tags.
Our Language Model. We work with a discrimi-
native LM with long-span dependencies. We use a
global linear model with Perceptron training. We
rescore the hypotheses (lattices) generated by the
ASR decoder?in a framework most similar to that
of Rastrow et al (2011a).
The LM score S(w,a) for each hypothesis w of
a speech utterance with acoustic sequence a is based
on the baseline ASR system score b(w,a) (initial n-
gram LM score and the acoustic score) and ?0, the
weight assigned to the baseline score.1 The score is
defined as:
S(w,a) = ?0 ? b(w,a) + F (w, s1, . . . , sm)
= ?0 ? b(w,a) +
d?
i=1
?i ? ?i(w, s1, . . . , sm)
where F is the discriminative LM?s score for the
hypothesis w, and s1, . . . , sm are candidate syntac-
tic structures associated with w, as discussed be-
low. Since we use a linear model, the score is a
weighted linear combination of the count of acti-
vated features of the word sequence w and its as-
sociated structures: ?i(w, s1, . . . , sm). Perceptron
training learns the parameters ?. The baseline score
b(w,a) can be a feature, yielding the dot product
notation: S(w,a) = ??,?(a,w, s1, . . . , sm)? Our
LM uses features from the dependency tree and part
of speech (POS) tag sequence. We use the method
described in Kuo et al (2009) to identify the two
previous exposed head words, h?2, h?1, at each po-
sition i in the input hypothesis and include the fol-
lowing syntactic based features into our LM:
1. (h?2.w ? h?1.w ? wi) , (h?1.w ? wi) , (wi)
2. (h?2.t ? h?1.t ? ti) , (h?1.t ? ti) , (ti) , (tiwi)
where h.w and h.t denote the word identity and the
POS tag of the corresponding exposed head word.
2.1 Hill Climbing Rescoring
We adopt the so called hill climbing framework of
Rastrow et al (2011b) to improve both training and
rescoring time as much as possible by reducing the
1We tune ?0 on development data (Collins et al, 2005).
176
number N of explored hypotheses. We summarize
it below for completeness.
Given a speech utterance?s lattice L from a first
pass ASR decoder, the neighborhood N (w, i) of a
hypothesis w = w1w2 . . . wn at position i is de-
fined as the set of all paths in the lattice that may
be obtained by editing wi: deleting it, substituting
it, or inserting a word to its left. In other words,
it is the ?distance-1-at-position i? neighborhood of
w. Given a position i in a word sequence w, all
hypotheses in N (w, i) are rescored using the long-
span model and the hypothesis w??(i) with the high-
est score becomes the new w. The process is re-
peated with a new position ? scanned left to right
? until w = w??(1) = . . . = w??(n), i.e. when w
itself is the highest scoring hypothesis in all its 1-
neighborhoods, and can not be furthered improved
using the model. Incorporating this into training
yields a discriminative hill climbing algorithm (Ras-
trow et al, 2011a).
3 Incorporating Syntactic Structures
Long-span models ? generative or discriminative,
N -best or hill climbing ? rely on auxiliary tools,
such as a POS tagger or a parser, for extracting
features for each hypothesis during rescoring, and
during training for discriminative models. The top-
m candidate structures associated with the ith hy-
pothesis, which we denote as s1i , . . . , smi , are gener-
ated by these tools and used to score the hypothesis:
F (wi, s1i , . . . , smi ). For example, s
j
i can be a part of
speech tag or a syntactic dependency. We formally
define this sequential processing as:
w1
tool(s)????? s11, . . . , sm1
LM??? F (w1, s11, . . . , sm1 )
w2
tool(s)????? s12, . . . , sm2
LM??? F (w2, s12, . . . , sm2 )
...
wk
tool(s)????? s1k, . . . , smk
LM??? F (wk, s1k, . . . , smk )
Here, {w1, . . . ,wk} represents a set of ASR output
hypotheses that need to be rescored. For each hy-
pothesis, we apply an external tool (e.g. parser) to
generate associated structures s1i , . . . , smi (e.g. de-
pendencies.) These are then passed to the language
model along with the word sequence for scoring.
3.1 Substructure Sharing
While long-span LMs have been empirically shown
to improve WER over n-gram LMs, the computa-
tional burden prohibits long-span LMs in practice,
particularly in real-time systems. A major complex-
ity factor is due to processing 100s or 1000s of hy-
potheses for each speech utterance, even during hill
climbing, each of which must be POS tagged and
parsed. However, the candidate hypotheses of an
utterance share equivalent substructures, especially
in hill climbing methods due to the locality present
in the neighborhood generation. Figure 1 demon-
strates such repetition in an N -best list (N=10) and
a hill climbing neighborhood hypothesis set for a
speech utterance from broadcast news. For exam-
ple, the word ?ENDORSE? occurs within the same
local context in all hypotheses and should receive
the same part of speech tag in each case. Processing
each hypothesis separately wastes time.
We propose a general algorithmic approach to re-
duce the complexity of processing a hypothesis set
by sharing common substructures among the hy-
potheses. Critically, unlike many lattice parsing al-
gorithms, our approach is general and produces ex-
act output. We first present our approach and then
demonstrate its generality by applying it to a depen-
dency parser and part of speech tagger.
We work with structured prediction models that
produce output from a series of local decisions: a
transition model. We begin in initial state pi0 and
terminate in a possible final state pif . All states
along the way are chosen from the possible states
?. A transition (or action) ? ? ? advances the
decoder from state to state, where the transition ?i
changes the state from pii to pii+1. The sequence
of states {pi0 . . . pii, pii+1 . . . pif} can be mapped to
an output (the model?s prediction.) The choice of
action ? is given by a learning algorithm, such as
a maximum-entropy classifier, support vector ma-
chine or Perceptron, trained on labeled data. Given
the previous k actions up to pii, the classifier g :
? ? ?k ? R|?| assigns a score to each possi-
ble action, which we can interpret as a probability:
pg(?i|pii, ?i?1?i?2 . . . ?i?k). These actions are ap-
plied to transition to new states pii+1. We note that
state definitions can encode the k previous actions,
which simplifies the probability to pg(?i|pii). The
177
N -best list Hill climbing neighborhood
(1) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(2) TO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(3) AL GORE HAS PROMISE THAT HE WOULD ENDORSE A CANDIDATE
(4) SO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE (1) YEAH FIFTY CENT GALLON NOMINATION WHICH WAS GREAT
(5) IT?S AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE (2) YEAH FIFTY CENT A GALLON NOMINATION WHICH WAS GREAT
(6) AL GORE HAS PROMISED HE WOULD ENDORSE A CANDIDATE (3) YEAH FIFTY CENT GOT A NOMINATION WHICH WAS GREAT
(7) AL GORE HAS PROMISED THAT HE WOULD ENDORSE THE CANDIDATE
(8) SAID AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(9) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE FOR
(10) AL GORE HIS PROMISE THAT HE WOULD ENDORSE A CANDIDATE
Figure 1: Example of repeated substructures in candidate hypotheses.
score of the new state is then
p(pii+1) = pg(?i|pii) ? p(pii) (1)
Classification decisions require a feature represen-
tation of pii, which is provided by feature functions
f : ?? Y , that map states to features. Features are
conjoined with actions for multi-class classification,
so pg(?i|pii) = pg(f(pi) ? ?i), where ? is a conjunc-
tion operation. In this way, states can be summarized
by features.
Equivalent states are defined as two states pi and
pi? with an identical feature representation:
pi ? pi? iff f(pi) = f(pi?)
If two states are equivalent, then g imposes the same
distribution over actions. We can benefit from this
substructure redundancy, both within and between
hypotheses, by saving these distributions in mem-
ory, sharing a distribution computed just once across
equivalent states. A similar idea of equivalent states
is used by Huang and Sagae (2010), except they use
equivalence to facilitate dynamic programming for
shift-reduce parsing, whereas we generalize it for
improving the processing time of similar hypotheses
in general models. Following Huang and Sagae, we
define kernel features as the smallest set of atomic
features f?(pi) such that,
f?(pi) = f?(pi?) ? pi ? pi?. (2)
Equivalent distributions are stored in a hash table
H : ?? ??R; the hash keys are the states and the
values are distributions2 over actions: {?, pg(?|pi)}.
2For pure greedy search (deterministic search) we need only
retain the best action, since the distribution is only used in prob-
abilistic search, such as beam search or best-first algorithms.
H caches equivalent states in a hypothesis set and re-
sets for each new utterance. For each state, we first
check H for equivalent states before computing the
action distribution; each cache hit reduces decod-
ing time. Distributing hypotheses wi across differ-
ent CPU threads is another way to obtain speedups,
and we can still benefit from substructure sharing by
storing H in shared memory.
We use h(pi) = ?|f?(pi)|i=1 int(f?i(pi)) as the hash
function, where int(f?i(pi)) is an integer mapping of
the ith kernel feature. For integer typed features
the mapping is trivial, for string typed features (e.g.
a POS tag identity) we use a mapping of the cor-
responding vocabulary to integers. We empirically
found that this hash function is very effective and
yielded very few collisions.
To apply substructure sharing to a transition based
model, we need only define the set of states ? (in-
cluding pi0 and pif ), actions ? and kernel feature
functions f? . The resulting speedup depends on the
amount of substructure duplication among the hy-
potheses, which we will show is significant for ASR
lattice rescoring. Note that our algorithm is not an
approximation; we obtain the same output {sji} as
we would without any sharing. We now apply this
algorithm to dependency parsing and POS tagging.
3.2 Dependency Parsing
We use the best-first probabilistic shift-reduce de-
pendency parser of Sagae and Tsujii (2007), a
transition-based parser (Ku?bler et al, 2009) with a
MaxEnt classifier. Dependency trees are built by
processing the words left-to-right and the classifier
assigns a distribution over the actions at each step.
States are defined as pi = {S,Q}: S is a stack of
178
Kernel features f?(pi) for state pi = {S,Q}
S = s0, s1, . . . & Q = q0, q1, . . .
(1) s0.w s0.t s0.r (5) ts0?1
s0.lch.t s0.lch.r ts1+1
s0.rch.t s0.rch.r
(2) s1.w s1.t s1.r (6) dist(s0, s1)
s1.lch.t s1.lch.r dist(q0, s0)
s1.rch.t s1.rch.r
(3) s2.w s2.t s2.r
(4) q0.w q0.t (7) s0.nch
q1.w q1.t s1.nch
q2.w
Table 1: Kernel features for defining parser states. si.w
denotes the head-word in a subtree and t its POS tag.
si.lch and si.rch are the leftmost and rightmost children
of a subtree. si.r is the dependency label that relates a
subtree head-word to its dependent. si.nch is the number
of children of a subtree. qi.w and qi.t are the word and
its POS tag in the queue. dist(s0,s1) is the linear distance
between the head-words of s0 and s1.
subtrees s0, s1, . . . (s0 is the top tree) and Q are
words in the input word sequence. The initial state is
pi0 = {?, {w0, w1, . . .}}, and final states occur when
Q is empty and S contains a single tree (the output).
? is determined by the set of dependency labels
r ? R and one of three transition types:
? Shift: remove the head of Q (wj) and place it on
the top of S as a singleton tree (only wj .)
? Reduce-Leftr: replace the top two trees in S (s0
and s1) with a tree formed by making the root of
s1 a dependent of the root of s0 with label r.
? Reduce-Rightr: same as Reduce-Leftr except re-
verses s0 and s1.
Table 1 shows the kernel features used in our de-
pendency parser. See Sagae and Tsujii (2007) for a
complete list of features.
Goldberg and Elhadad (2010) observed that pars-
ing time is dominated by feature extraction and
score calculation. Substructure sharing reduces
these steps for equivalent states, which are persis-
tent throughout a candidate set. Note that there are
far fewer kernel features than total features, hence
the hash function calculation is very fast.
We summarize substructure sharing for depen-
dency parsing in Algorithm 1. We extend the def-
inition of states to be {S,Q, p} where p denotes the
score of the state: the probability of the action se-
quence that resulted in the current state. Also, fol-
Algorithm 1 Best-first shift-reduce dependency parsing
w ? input hypothesis
S0 = ?, Q0 = w, p0 = 1
pi0 ? {S0, Q0, p0} [initial state]
H ?Hash table (?? ?? R)
Heap? Heap for prioritizing states and performing best-first search
Heap.push(pi0) [initialize the heap]
while Heap 6= ? do
picurrent ?Heap.pop() [the best state so far]
if picurrent = pif [if final state]
return picurrent [terminate if final state]
else ifH.find(picurrent)
ActList? H[picurrent] [retrieve action list from the hash table]
else [need to construct action list]
for all ? ? ? [for all actions]
p? ? pg(?|picurrent) [action score]
ActList.insert({?, p?})
H.insert(picurrent,ActList) [Store the action list into hash table]
end if
for all {?, p?} ? ActList [compute new states]
pinew ? picurrent ? ?
Heap.push(pinew) [push to the heap]
end while
lowing Sagae and Tsujii (2007) a heap is used to
maintain states prioritized by their scores, for apply-
ing the best-first strategy. For each step, a state from
the top of the heap is considered and all actions (and
scores) are either retrieved from H or computed us-
ing g.3 We use pinew ? picurrent ? ? to denote the
operation of extending a state by an action ? ? ?4.
3.3 Part of Speech Tagging
We use the part of speech (POS) tagger of Tsuruoka
et al (2011), a transition based model with a Per-
ceptron and a lookahead heuristic process. The tag-
ger processes w left to right. States are defined as
pii = {ci,w}: a sequence of assigned tags up to wi
(ci = t1t2 . . . ti?1) and the word sequence w. ? is
defined simply as the set of possible POS tags (T )
that can be applied. The final state is reached once
all the positions are tagged. For f we use the features
of Tsuruoka et al (2011). The kernel features are
f?(pii) = {ti?2, ti?1, wi?2, wi?1, wi, wi+1, wi+2}.
While the tagger extracts prefix and suffix features,
it suffices to look at wi for determining state equiv-
alence. The tagger is deterministic (greedy) in that
it only considers the best tag at each step, so we do
not store scores. However, this tagger uses a depth-
3 Sagae and Tsujii (2007) use a beam strategy to increase
speed. Search space pruning is achieved by filtering heap states
for probability greater than 1b the probability of the most likely
state in the heap with the same number of actions. We use b =
100 for our experiments.
4We note that while we have demonstrated substructure
sharing for dependency parsing, the same improvements can
be made to a shift-reduce constituent parser (Sagae and Lavie,
2006).
179
t2t1 ti 2 ti 1
t1i
t2i
t|T |i t|T |i+1
t1i+1
t2i+1
w1 w2 wi 1wi 2 wi wi+1 wi+2 wi+3? ? ?
? ? ?
lookahead search
Figure 2: POS tagger with lookahead search of d=1. At
wi the search considers the current state and next state.
first search lookahead procedure to select the best
action at each step, which considers future decisions
up to depth d5. An example for d = 1 is shown
in Figure 2. Using d = 1 for the lookahead search
strategy, we modify the kernel features since the de-
cision forwi is affected by the state pii+1. The kernel
features in position i should be f?(pii) ? f?(pii+1):
f?(pii) =
{ti?2, ti?1, wi?2, wi?1, wi, wi+1, wi+2, wi+3}
4 Up-Training
While we have fast decoding algorithms for the pars-
ing and tagging, the simpler underlying models can
lead to worse performance. Using more complex
models with higher accuracy is impractical because
they are slow. Instead, we seek to improve the accu-
racy of our fast tools.
To achieve this goal we use up-training, in which
a more complex model is used to improve the accu-
racy of a simpler model. We are given two mod-
els, M1 and M2, as well as a large collection of
unlabeled text. Model M1 is slow but very accu-
rate while M2 is fast but obtains lower accuracy.
Up-training applies M1 to tag the unlabeled data,
which is then used as training data for M2. Like
self-training, a model is retrained on automatic out-
put, but here the output comes form a more accurate
model. Petrov et al (2010) used up-training as a
domain adaptation technique: a constituent parser ?
which is more robust to domain changes ? was used
to label a new domain, and a fast dependency parser
5 Tsuruoka et al (2011) shows that the lookahead search
improves the performance of the local ?history-based? models
for different NLP tasks
was trained on the automatically labeled data. We
use a similar idea where our goal is to recover the
accuracy lost from using simpler models. Note that
while up-training uses two models, it differs from
co-training since we care about improving only one
model (M2). Additionally, the models can vary in
different ways. For example, they could be the same
algorithm with different pruning methods, which
can lead to faster but less accurate models.
We apply up-training to improve the accuracy of
both our fast POS tagger and dependency parser. We
parse a large corpus of text with a very accurate but
very slow constituent parser and use the resulting
data to up-train our tools. We will demonstrate em-
pirically that up-training improves these fast models
to yield better WER results.
5 Related Work
The idea of efficiently processing a hypothesis set is
similar to ?lattice-parsing?, in which a parser con-
sider an entire lattice at once (Hall, 2005; Chep-
palier et al, 1999). These methods typically con-
strain the parsing space using heuristics, which are
often model specific. In other words, they search in
the joint space of word sequences present in the lat-
tice and their syntactic analyses; they are not guaran-
teed to produce a syntactic analysis for all hypothe-
ses. In contrast, substructure sharing is a general
purpose method that we have applied to two differ-
ent algorithms. The output is identical to processing
each hypothesis separately and output is generated
for each hypothesis. Hall (Hall, 2005) uses a lattice
parsing strategy which aims to compute the marginal
probabilities of all word sequences in the lattice by
summing over syntactic analyses of each word se-
quence. The parser sums over multiple parses of a
word sequence implicitly. The lattice parser there-
fore, is itself a language model. In contrast, our
tools are completely separated from the ASR sys-
tem, which allows the system to create whatever fea-
tures are needed. This independence means our tools
are useful for other tasks, such as machine transla-
tion. These differences make substructure sharing a
more attractive option for efficient algorithms.
While Huang and Sagae (2010) use the notion of
?equivalent states?, they do so for dynamic program-
ming in a shift-reduce parser to broaden the search
space. In contrast, we use the idea to identify sub-
180
structures across inputs, where our goal is efficient
parsing in general. Additionally, we extend the defi-
nition of equivalent states to general transition based
structured prediction models, and demonstrate ap-
plications beyond parsing as well as the novel setting
of hypothesis set parsing.
6 Experiments
Our ASR system is based on the 2007 IBM
Speech transcription system for the GALE Distilla-
tion Go/No-go Evaluation (Chen et al, 2006) with
state of the art discriminative acoustic models. See
Table 2 for a data summary. We use a modi-
fied Kneser-Ney (KN) backoff 4-gram baseline LM.
Word-lattices for discriminative training and rescor-
ing come from this baseline ASR system.6 The long-
span discriminative LM?s baseline feature weight
(?0) is tuned on dev data and hill climbing (Rastrow
et al, 2011a) is used for training and rescoring. The
dependency parser and POS tagger are trained on su-
pervised data and up-trained on data labeled by the
CKY-style bottom-up constituent parser of Huang et
al. (2010), a state of the art broadcast news (BN)
parser, with phrase structures converted to labeled
dependencies by the Stanford converter.
While accurate, the parser has a huge grammar
(32GB) from using products of latent variable gram-
mars and requires O(l3) time to parse a sentence of
length l. Therefore, we could not use the constituent
parser for ASR rescoring since utterances can be
very long, although the shorter up-training text data
was not a problem.7 We evaluate both unlabeled
(UAS) and labeled dependency accuracy (LAS).
6.1 Results
Before we demonstrate the speed of our models, we
show that up-training can produce accurate and fast
models. Figure 3 shows improvements to parser ac-
curacy through up-training for different amount of
(randomly selected) data, where the last column in-
dicates constituent parser score (91.4% UAS). We
use the POS tagger to generate tags for depen-
dency training to match the test setting. While
there is a large difference between the constituent
and dependency parser without up-training (91.4%
6For training a 3-gram LM is used to increase confusions.
7Speech utterances are longer as they are not as effectively
sentence segmented as text.
84.0	 ?
85.0	 ?
86.0	 ?
87.0	 ?
88.0	 ?
89.0	 ?
90.0	 ?
91.0	 ?
92.0	 ?
0M	 ? 2.5M	 ? 5M	 ? 10M	 ? 20M	 ? 40M	 ? Cons?tuent	 ?Parser	 ?
Accu
racy
	 ?(%)
	 ?
Amount	 ?of	 ?Added	 ?Uptraining	 ?Data	 ?
Unlabeled	 ?A?achment	 ?Score	 ?
Labeled	 ?A?achment	 ?Score	 ?
Figure 3: Up-training results for dependency parsing for
varying amounts of data (number of words.) The first
column is the dependency parser with supervised training
only and the last column is the constituent parser (after
converting to dependency trees.)
vs. 86.2% UAS), up-training can cut the differ-
ence by 44% to 88.5%, and improvements saturate
around 40m words (about 2m sentences.)8 The de-
pendency parser remains much smaller and faster;
the up-trained dependency model is 700MB with
6m features compared with 32GB for constituency
model. Up-training improves the POS tagger?s accu-
racy from 95.9% to 97%, when trained on the POS
tags produced by the constituent parser, which has a
tagging accuracy of 97.2% on BN.
We train the syntactic discriminative LM, with
head-word and POS tag features, using the faster
parser and tagger and then rescore the ASR hypothe-
ses. Table 3 shows the decoding speedups as well as
the WER reductions compared to the baseline LM.
Note that up-training improvements lead to WER re-
ductions. Detailed speedups on substructure sharing
are shown in Table 4; the POS tagger achieves a 5.3
times speedup, and the parser a 5.7 speedup with-
out changing the output. We also observed speedups
during training (not shown due to space.)
The above results are for the already fast hill
climbing decoding, but substructure sharing can also
be used for N -best list rescoring. Figure 4 (logarith-
mic scale) illustrates the time for the parser and tag-
ger to processN -best lists of varying size, with more
substantial speedups for larger lists. For example,
for N=100 (a typical setting) the parsing time re-
8Better performance is due to the exact CKY-style ? com-
pared with best-first and beam? search and that the constituent
parser uses the product of huge self-trained grammars.
181
Usage Data Size
Acoustic model training Hub4 acoustic train 153k uttr, 400 hrs
Baseline LM training: modified KN 4-gram TDT4 closed captions+EARS BN03 closed caption 193m words
Disc. LM training: long-span w/hill climbing Hub4 (length <50) 115k uttr, 2.6m words
Baseline feature (?0) tuning dev04f BN data 2.5 hrs
Supervised training: dep. parser, POS tagger Ontonotes BN treebank+ WSJ Penn treebank 1.3m words, 59k sent.
Supervised training: constituent parser Ontonotes BN treebank + WSJ Penn treebank 1.3m words, 59k sent.
Up-training: dependency parser, POS tagger TDT4 closed captions+EARS BN03 closed caption 193m words available
Evaluation: up-training BN treebank test (following Huang et al (2010)) 20k words, 1.1k sent.
Evaluation: ASR transcription rt04 BN evaluation 4 hrs, 45k words
Table 2: A summary of the data for training and evaluation. The Ontonotes corpus is from Weischedel et al (2008).
10	 ?
100	 ?
1000	 ?
10000	 ?
100000	 ?
1000000	 ?
1	 ? 10	 ? 100	 ? 1000	 ?
Elap
sed
	 ?Tim
e	 ?(s
ec)	 ?
N-??best	 ?Size	 ?(N)	 ?
No	 ?Sharing	 ?
Substructure	 ?Sharing	 ?
(a)
1	 ?
10	 ?
100	 ?
1000	 ?
10000	 ?
1	 ? 10	 ? 100	 ? 1000	 ?
Elap
sed
	 ?Tim
e	 ?(s
ec)	 ?
N-??best	 ?Size	 ?(N)	 ?
No	 ?Sharing	 ?
Substructure	 ?Sharing	 ?
(b)
Figure 4: Elapsed time for (a) parsing and (b) POS tagging the N -best lists with and without substructure sharing.
Substr. Share (sec)
LM WER No Yes
Baseline 4-gram 15.1 - -
Syntactic LM 14.8
8,658 1,648
+ up-train 14.6
Table 3: Speedups and WER for hill climbing rescor-
ing. Substructure sharing yields a 5.3 times speedup. The
times for with and without up-training are nearly identi-
cal, so we include only one set for clarity. Time spent
is dominated by the parser, so the faster parser accounts
for much of the overall speedup. Timing information in-
cludes neighborhood generation and LM rescoring, so it
is more than the sum of the times in Table 4.
duces from about 20,000 seconds to 2,700 seconds,
about 7.4 times as fast.
7 Conclusion
The computational complexity of accurate syntac-
tic processing can make structured language models
impractical for applications such as ASR that require
scoring hundreds of hypotheses per input. We have
Substr. Share Speedup
No Yes
Parser 8,237.2 1,439.5 5.7
POS tagger 213.3 40.1 5.3
Table 4: Time in seconds for the parser and POS tagger
to process hypotheses during hill climbing rescoring.
presented substructure sharing, a general framework
that greatly improves the speed of syntactic tools
that process candidate hypotheses. Furthermore, we
achieve improved performance through up-training.
The result is a large speedup in rescoring time, even
on top of the already fast hill climbing framework,
and reductions in WER from up-training. Our re-
sults make long-span syntactic LMs practical for
real-time ASR, and can potentially impact machine
translation decoding as well.
Acknowledgments
Thanks to Kenji Sagae for sharing his shift-reduce
dependency parser and the anonymous reviewers for
helpful comments.
182
References
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283?332.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
J. Cheppalier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Sixth Conference sur le Traitement Automatique du
Langage Naturel (TANL?99).
M Collins, B Roark, and M Saraclar. 2005. Discrimina-
tive syntactic language modeling for speech recogni-
tion. In ACL.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Proc. HLT-NAACL, number
June, pages 742?750.
Keith B Hall. 2005. Best-first word-lattice parsing:
techniques for integrated syntactic language modeling.
Ph.D. thesis, Brown University.
L. Huang and K. Sagae. 2010. Dynamic Programming
for Linear-Time Incremental Parsing. In Proceedings
of ACL.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with Products of Latent Variable Gram-
mars. In Proc. EMNLP, number October, pages 12?
22.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, pages 355?372.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 2(1):1?127.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,
and Chin-Hui Lee. 2002. Discriminative training of
language models for speech recognition. In ICASSP.
H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and
L. Young-Suk. 2009. Syntactic features for Arabic
speech recognition. In Proc. ASRU.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October. Association for Computational Linguistics.
Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.
2011a. Efficient discrimnative training of long-span
language models. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU).
Ariya Rastrow, Markus Dreyer, Abhinav Sethy, San-
jeev Khudanpur, Bhuvana Ramabhadran, and Mark
Dredze. 2011b. Hill climbing on speech lattices : A
new rescoring framework. In ICASSP.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech & Language, 21(2).
K. Sagae and A. Lavie. 2006. A best-first probabilis-
tic shift-reduce parser. In Proc. ACL, pages 691?698.
Association for Computational Linguistics.
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. EMNLP-CoNLL, volume 7, pages
1044?1050.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with Lookahead :
Can History-Based Models Rival Globally Optimized
Models ? In Proc. CoNLL, number June, pages 238?
246.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
183
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 63?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PARMA: A Predicate Argument Aligner
Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,
Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,
Jonathan Weese, Tan Xu?, and Xuchen Yao
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland USA
?University of Maryland, College Park, Maryland USA
Abstract
We introduce PARMA, a system for cross-
document, semantic predicate and argu-
ment alignment. Our system combines a
number of linguistic resources familiar to
researchers in areas such as recognizing
textual entailment and question answering,
integrating them into a simple discrimina-
tive model. PARMA achieves state of the
art results on an existing and a new dataset.
We suggest that previous efforts have fo-
cussed on data that is biased and too easy,
and we provide a more difficult dataset
based on translation data with a low base-
line which we beat by 17% F1.
1 Introduction
A key step of the information extraction pipeline
is entity disambiguation, in which discovered en-
tities across many sentences and documents must
be organized to represent real world entities. The
NLP community has a long history of entity dis-
ambiguation both within and across documents.
While most information extraction work focuses
on entities and noun phrases, there have been a
few attempts at predicate, or event, disambigua-
tion. Commonly a situational predicate is taken to
correspond to either an event or a state, lexically
realized in verbs such as ?elect? or nominaliza-
tions such as ?election?. Similar to entity coref-
erence resolution, almost all of this work assumes
unanchored mentions: predicate argument tuples
are grouped together based on coreferent events.
The first work on event coreference dates back to
Bagga and Baldwin (1999). More recently, this
task has been considered by Bejan and Harabagiu
(2010) and Lee et al (2012). As with unanchored
entity disambiguation, these methods rely on clus-
tering methods and evaluation metrics.
Another view of predicate disambiguation seeks
to link or align predicate argument tuples to an ex-
isting anchored resource containing references to
events or actions, similar to anchored entity dis-
ambiguation (entity linking) (Dredze et al, 2010;
Han and Sun, 2011). The most relevant, and per-
haps only, work in this area is that of Roth and
Frank (2012) who linked predicates across docu-
ment pairs, measuring the F1 of aligned pairs.
Here we present PARMA, a new system for pred-
icate argument alignment. As opposed to Roth and
Frank, PARMA is designed as a a trainable plat-
form for the incorporation of the sort of lexical se-
mantic resources used in the related areas of Rec-
ognizing Textual Entailment (RTE) and Question
Answering (QA). We demonstrate the effective-
ness of this approach by achieving state of the art
performance on the data of Roth and Frank despite
having little relevant training data. We then show
that while the ?lemma match? heuristic provides a
strong baseline on this data, this appears to be an
artifact of their data creation process (which was
heavily reliant on word overlap). In response, we
evaluate on a new and more challenging dataset for
predicate argument alignment derived from multi-
ple translation data. We release PARMA as a new
framework for the incorporation and evaluation of
new resources for predicate argument alignment.1
2 PARMA
PARMA (Predicate ARguMent Aligner) is a
pipelined system with a wide variety of features
used to align predicates and arguments in two doc-
uments. Predicates are represented as mention
spans and arguments are represented as corefer-
ence chains (sets of mention spans) provided by
in-document coreference resolution systems such
as included in the Stanford NLP toolkit. Results
indicated that the chains are of sufficient quality
so as not to limit performance, though future work
1https://github.com/hltcoe/parma
63
RF
? Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic
[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7
? Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , the
suspect s [attorney]6 [said]7 Tuesday
LDC MTC
? As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5
in which the poetic lines in praise of Prophet Mohammad are recited -RRB-
? I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4
[meeting]5 .
Figure 1: Example of gold-standard alignment pairs from Roth and Frank?s data set and our data set
created from the LDC?s Multiple Translation Corpora. The RF data set exhibits high lexical overlap,
where most of the alignments are between identical words like police-police and said-said. The LDC
MTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-
balcony and tent-camp
may relax this assumption.
We refer to a predicate or an argument as an
?item? with type predicate or argument. An align-
ment between two documents is a subset of all
pairs of items in either documents with the same
type.2 We call the two documents being aligned
the source document S and the target document
T . Items are referred to by their index, and ai,j is a
binary variable representing an alignment between
item i in S and item j in T . A full alignment is an
assignment ~a = {aij : i ? NS , j ? NT }, where
NS and NT are the set of item indices for S and T
respectively.
We train a logistic regression model on exam-
ple alignmentsand maximize the likelihood of a
document alignment under the assumption that the
item alignments are independent. Our objective
is to maximize the log-likelihood of all p(S, T )
with an L1 regularizer (with parameter ?). After
learning model parameters w by regularized max-
imum likelihood on training data, we introducing
a threshold ? on alignment probabilities to get a
classifier. We perform line search on ? and choose
the value that maximizes F1 on dev data. Train-
ing was done using the Mallet toolkit (McCallum,
2002).
2.1 Features
The focus of PARMA is the integration of a diverse
range of features based on existing lexical seman-
tic resources. We built PARMA on a supervised
framework to take advantage of this wide variety
of features since they can describe many different
correlated aspects of generation. The following
features cover the spectrum from high-precision
2Note that type is not the same thing as part of speech: we
allow nominal predicates like ?death?.
to high-recall. Each feature has access to the pro-
posed argument or predicate spans to be linked and
the containing sentences as context. While we use
supervised learning, some of the existing datasets
for this task are very small. For extra training data,
we pool material from different datasets and use
the multi-domain split feature space approach to
learn dataset specific behaviors (Daume?, 2007).
Features in general are defined over mention
spans or head tokens, but we split these features
to create separate feature-spaces for predicates and
arguments.3
For argument coref chains we heuristically
choose a canonical mention to represent each
chain, and some features only look at this canon-
ical mention. The canonical mention is cho-
sen based on length,4 information about the head
word,5 and position in the document.6 In most
cases, coref chains that are longer than one are
proper nouns and the canonical mention is the first
and longest mention (outranking pronominal ref-
erences and other name shortenings).
PPDB We use lexical features from the Para-
phrase Database (PPDB) (Ganitkevitch et al,
2013). PPDB is a large set of paraphrases ex-
tracted from bilingual corpora using pivoting tech-
niques. We make use of the English lexical portion
which contains over 7 million rules for rewriting
terms like ?planet? and ?earth?. PPDB offers a
variety of conditional probabilities for each (syn-
chronous context free grammar) rule, which we
3While conceptually cleaner, In practice we found this
splitting to have no impact on performance.
4in tokens, not counting some words like determiners and
auxiliary verbs
5like its part of speech tag and whether the it was tagged
as a named entity
6mentions that appear earlier in the document and earlier
in a given sentence are given preference
64
treat as independent experts. For each of these rule
probabilities (experts), we find all rules that match
the head tokens of a given alignment and have a
feature for the max and harmonic mean of the log
probabilities of the resulting rule set.
FrameNet FrameNet is a lexical database based
on Charles Fillmore?s Frame Semantics (Fill-
more, 1976; Baker et al, 1998). The database
(and the theory) is organized around seman-
tic frames that can be thought of as descrip-
tions of events. Frames crucially include spec-
ification of the participants, or Frame Elements,
in the event. The Destroying frame, for in-
stance, includes frame elements Destroyer or
Cause Undergoer. Frames are related to other
frames through inheritance and perspectivization.
For instance the frames Commerce buy and
Commerce sell (with respective lexical real-
izations ?buy? and ?sell?) are both perspectives of
Commerce goods-transfer (no lexical re-
alizations) which inherits from Transfer (with
lexical realization ?transfer?).
We compute a shortest path between headwords
given edges (hypernym, hyponym, perspectivized
parent and child) in FrameNet and bucket by dis-
tance to get features. We also have a binary feature
for whether two tokens evoke the same frame.
TED Alignments Given two predicates or argu-
ments in two sentences, we attempt to align the
two sentences they appear in using a Tree Edit
Distance (TED) model that aligns two dependency
trees, based on the work described by (Yao et al,
2013). We represent a node in a dependency tree
with three fields: lemma, POS tag and the type
of dependency relation to the node?s parent. The
TED model aligns one tree with the other using
the dynamic programming algorithm of Zhang and
Shasha (1989) with three predefined edits: dele-
tion, insertion and substitution, seeking a solution
yielding the minimum edit cost. Once we have
built a tree alignment, we extract features for 1)
whether the heads of the two phrases are aligned
and 2) the count of how many tokens are aligned
in both trees.
WordNet WordNet (Miller, 1995) is a database
of information (synonyms, hypernyms, etc.) per-
taining to words and short phrases. For each entry,
WordNet provides a set of synonyms, hypernyms,
etc. Given two spans, we use WordNet to deter-
mine semantic similarity by measuring how many
synonym (or other) edges are needed to link two
terms. Similar words will have a short distance.
For features, we find the shortest path linking the
head words of two mentions using synonym, hy-
pernym, hyponym, meronym, and holonym edges
and bucket the length.
String Transducer To represent similarity be-
tween arguments that are names, we use a stochas-
tic edit distance model. This stochastic string-to-
string transducer has latent ?edit? and ?no edit?
regions where the latent regions allow the model
to assign high probability to contiguous regions of
edits (or no edits), which are typical between vari-
ations of person names. In an edit region, param-
eters govern the relative probability of insertion,
deletion, substitution, and copy operations. We
use the transducer model of Andrews et al (2012).
Since in-domain name pairs were not available, we
picked 10,000 entities at random from Wikipedia
to estimate the transducer parameters. The entity
labels were used as weak supervision during EM,
as in Andrews et al (2012).
For a pair of mention spans, we compute the
conditional log-likelihood of the two mentions go-
ing both ways, take the max, and then bucket to get
binary features. We duplicate these features with
copies that only fire if both mentions are tagged as
PER, ORG or LOC.
3 Evaluation
We consider three datasets for evaluating PARMA.
For richer annotations that include lemmatiza-
tions, part of speech, NER, and in-doc corefer-
ence, we pre-processed each of the datasets using
tools7 similar to those used to create the Annotated
Gigaword corpus (Napoles et al, 2012).
Extended Event Coreference Bank Based on
the dataset of Bejan and Harabagiu (2010), Lee et
al. (2012) introduced the Extended Event Coref-
erence Bank (EECB) to evaluate cross-document
event coreference. EECB provides document clus-
ters, within which entities and events may corefer.
Our task is different from Lee et al but we can
modify the corpus setup to support our task. To
produce source and target document pairs, we se-
lect the first document within every cluster as the
source and each of the remaining documents as
target documents (i.e. N ? 1 pairs for a cluster
of size N ). This yielded 437 document pairs.
Roth and Frank The only existing dataset for
our task is from Roth and Frank (2012) (RF), who
7https://github.com/cnap/anno-pipeline
65
annotated documents from the English Gigaword
Fifth Edition corpus (Parker et al, 2011). The data
was generated by clustering similar news stories
from Gigaword using TF-IDF cosine similarity of
their headlines. This corpus is small, containing
only 10 document pairs in the development set and
60 in the test set. To increase the training size,
we train PARMA with 150 randomly selected doc-
ument pairs from both EECB and MTC, and the
entire dev set from Roth and Frank using multi-
domain feature splitting. We tuned the threshold
? on the Roth and Frank dev set, but choose the
regularizer ? based on a grid search on a 5-fold
version of the EECB dataset.
Multiple Translation Corpora We constructed
a new predicate argument alignment dataset
based on the LDC Multiple Translation Corpora
(MTC),8 which consist of multiple English trans-
lations for foreign news articles. Since these mul-
tiple translations are semantically equivalent, they
provide a good resource for aligned predicate ar-
gument pairs. However, finding good pairs is a
challenge: we want pairs with significant overlap
so that they have predicates and arguments that
align, but not documents that are trivial rewrites
of each other. Roth and Frank selected document
pairs based on clustering, meaning that the pairs
had high lexical overlap, often resulting in mini-
mal rewrites of each other. As a result, despite ig-
noring all context, their baseline method (lemma-
alignment) worked quite well.
To create a more challenging dataset, we se-
lected document pairs from the multiple transla-
tions that minimize the lexical overlap (in En-
glish). Because these are translations, we know
that there are equivalent predicates and arguments
in each pair, and that any lexical variation pre-
serves meaning. Therefore, we can select pairs
with minimal lexical overlap in order to create
a system that truly stresses lexically-based align-
ment systems.
Each document pair has a correspondence be-
tween sentences, and we run GIZA++ on these
sentences to produce token-level alignments. We
take all aligned nouns as arguments and all aligned
verbs (excluding be-verbs, light verbs, and report-
ing verbs) as predicates. We then add negative ex-
amples by randomly substituting half of the sen-
tences in one document with sentences from an-
8LDC2010T10, LDC2010T11, LDC2010T12,
LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,
LDC2003T18, and LDC2005T05
0.3 0.4 0.5 0.6 0.7 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Performance vs Lexical Overlap
Doc-pair Cosine Similarity
F1
Figure 2: We plotted the PARMA?s performance on
each of the document pairs. Red squares show the
F1 for individual document pairs drawn from Roth
and Frank?s data set, and black circles show F1 for
our Multiple Translation Corpora test set. The x-
axis represents the cosine similarity between the
document pairs. On the RF data set, performance
is correlated with lexical similarity. On our more
lexically diverse set, this is not the case. This
could be due to the fact that some of the docu-
ments in the RF sets are minor re-writes of the
same newswire story, making them easy to align.
other corpus, guaranteed to be unrelated. The
amount of substitutions we perform can vary the
?relatedness? of the two documents in terms of
the predicates and arguments that they talk about.
This reflects our expectation of real world data,
where we do not expect perfect overlap in predi-
cates and arguments between a source and target
document, as you would in translation data.
Lastly, we prune any document pairs that have
more than 80 predicates or arguments or have a
Jaccard index on bags of lemmas greater than 0.5,
to give us a dataset of 328 document pairs.
Metric We use precision, recall, and F1. For the
RF dataset, we follow Roth and Frank (2012) and
Cohn et al (2008) and evaluate on a version of F1
that considers SURE and POSSIBLE links, which
are available in the RF data. Given an alignment
to be scored A and a reference alignment B which
contains SURE and POSSIBLE links, Bs andBp re-
spectively, precision and recall are:
P = |A ?Bp||A| R =
|A ?Bs|
|Bs|
(1)
66
F1 P R
EECB lemma 63.5 84.8 50.8
PARMA 74.3 80.5 69.0
RF lemma 48.3 40.3 60.3
Roth and Frank 54.8 59.7 50.7
PARMA 57.6 52.4 64.0
MTC lemma 42.1 51.3 35.7
PARMA 59.2 73.4 49.6
Table 1: PARMA outperforms the baseline lemma
matching system on the three test sets, drawn from
the Extended Event Coreference Bank, Roth and
Frank?s data, and our set created from the Multiple
Translation Corpora. PARMA achieves a higher F1
and recall score than Roth and Frank?s reported
result.
and F1 as the harmonic mean of the two. Results
for EECB and MTC reflect 5-fold cross validation,
and RF uses the given dev/test split.
Lemma baseline Following Roth and Frank we
include a lemma baseline, in which two predicates
or arguments align if they have the same lemma.9
4 Results
On every dataset PARMA significantly improves
over the lemma baselines (Table 1). On RF,
compared to Roth and Frank, the best published
method for this task, we also improve, making
PARMA the state of the art system for this task.
Furthermore, we expect that the smallest improve-
ments over Roth and Frank would be on RF, since
there is little training data. We also note that com-
pared to Roth and Frank we obtain much higher
recall but lower precision.
We also observe that MTC was more challeng-
ing than the other datasets, with a lower lemma
baseline. Figure 2 shows the correlation between
document similarity and document F1 score for
RF and MTC. While for RF these two measures
are correlated, they are uncorrelated for MTC. Ad-
ditionally, there is more data in the MTC dataset
which has low cosine similarity than in RF.
5 Conclusion
PARMA achieves state of the art performance on
three datasets for predicate argument alignment.
It builds on the development of lexical semantic
resources and provides a platform for learning to
utilize these resources. Additionally, we show that
9We could not reproduce lemma from Roth and Frank
(shown in Table 1) due to a difference in lemmatizers. We ob-
tained 55.4; better than their system but worse than PARMA.
task difficulty can be strongly tied to lexical simi-
larity if the evaluation dataset is not chosen care-
fully, and this provides an artificially high baseline
in previous work. PARMA is robust to drops in lex-
ical similarity and shows large improvements in
those cases. PARMA will serve as a useful bench-
mark in determining the value of more sophis-
ticated models of predicate-argument alignment,
which we aim to address in future work.
While our system is fully supervised, and thus
dependent on manually annotated examples, we
observed here that this requirement may be rela-
tively modest, especially for in-domain data.
Acknowledgements
We thank JHU HLTCOE for hosting the winter
MiniSCALE workshop that led to this collabora-
tive work. This material is based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes. The views and con-
clusions contained in this publication are those of
the authors and should not be interpreted as repre-
senting official policies or endorsements of NSF,
DARPA, or the U.S. Government.
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Empirical Methods in Natural
Language Processing (EMNLP).
Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 1?8. Association for Computational Linguis-
tics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
67
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Annual meeting-association for computa-
tional linguistics, volume 45, page 256.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20?
32.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945?
954. Association for Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Robert Parker, David Graff, Jumbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword fifth
edition.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218?
227, Montre?al, Canada, 7-8 June. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as
sequence tagging with tree edit distance. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775?785,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Robust Entity Clustering via Phylogenetic Inference
Nicholas Andrews and Jason Eisner and Mark Dredze
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA
{noa,eisner,mdredze}@jhu.edu
Abstract
Entity clustering must determine when two
named-entity mentions refer to the same
entity. Typical approaches use a pipeline ar-
chitecture that clusters the mentions using
fixed or learned measures of name and con-
text similarity. In this paper, we propose a
model for cross-document coreference res-
olution that achieves robustness by learn-
ing similarity from unlabeled data. The
generative process assumes that each entity
mention arises from copying and option-
ally mutating an earlier name from a sim-
ilar context. Clustering the mentions into
entities depends on recovering this copying
tree jointly with estimating models of the
mutation process and parent selection pro-
cess. We present a block Gibbs sampler for
posterior inference and an empirical evalu-
ation on several datasets.
1 Introduction
Variation poses a serious challenge for determin-
ing who or what a name refers to. For instance,
Wikipedia contains more than 100 variations of the
name Barack Obama as redirects to the U.S. Presi-
dent article, including:
President Obama Barack H. Obama, Jr.
Barak Obamba Barry Soetoro
To relate different names, one solution is to use
specifically tailored measures of name similarity
such as Jaro-Winkler similarity (Winkler, 1999; Co-
hen et al, 2003). This approach is brittle, however,
and fails to adapt to the test data. Another option is
to train a model like stochastic edit distance from
known pairs of similar names (Ristad and Yian-
ilos, 1998; Green et al, 2012), but this requires
supervised data in the test domain.
Even the best model of name similarity is not
enough by itself, since two names that are similar?
even identical?do not necessarily corefer. Docu-
ment context is needed to determine whether they
may be talking about two different people.
In this paper, we propose a method for jointly
(1) learning similarity between names and (2) clus-
tering name mentions into entities, the two major
components of cross-document coreference reso-
lution systems (Baron and Freedman, 2008; Finin
et al, 2009; Rao et al, 2010; Singh et al, 2011;
Lee et al, 2012; Green et al, 2012). Our model
is an evolutionary generative process based on the
name variation model of Andrews et al (2012),
which stipulates that names are often copied from
previously generated names, perhaps with mutation
(spelling edits). This can deduce that rather than
being names for different entities, Barak Obamba
and Barock obama more likely arose from the fre-
quent name Barack Obama as a common ancestor,
which accounts for most of their letters. This can
also relate seemingly dissimilar names via multiple
steps in the generative process:
Taylor Swift? T-Swift? T-Swizzle
Our model learns without supervision that these all
refer to the the same entity. Such creative spellings
are especially common on Twitter and other so-
cial media; we give more examples of coreferents
learned by our model in Section 8.4.
Our primary contributions are improvements on
Andrews et al (2012) for the entity clustering task.
Their inference procedure only clustered types (dis-
tinct names) rather than tokens (mentions in con-
text), and relied on expensive matrix inversions for
learning. Our novel approach features:
?4.1 A topical model of which entities from previ-
ously written text an author tends to mention
from previously written text.
?4.2 A name mutation model that is sensitive to
features of the input and output characters and
takes a reader?s comprehension into account.
?5 A scalable Markov chain Monte Carlo sam-
pler used in training and inference.
775
?7 A minimum Bayes risk decoding procedure
to pick an output clustering. The procedure is
applicable to any model capable of producing
a posterior over coreference decisions.
We evaluate our approach by comparing to sev-
eral baselines on datasets from three different gen-
res: Twitter, newswire, and blogs.
2 Overview and Related Work
Cross-document coreference resolution (CDCR)
was first introduced by Bagga and Baldwin (1998b).
Most approaches since then are based on the intu-
itions that coreferent names tend to have ?similar?
spellings and tend to appear in ?similar? contexts.
The distinguishing feature of our system is that both
notions of similarity are learned together without
supervision.
We adopt a ?phylogenetic? generative model of
coreference. The basic insight is that coreference is
created when an author thinks of an entity that was
mentioned earlier in a similar context, and men-
tions it again in a similar way. The author may
alter the name mention string when copying it, but
both names refer to the same entity. Either name
may later be copied further, leading to an evolution-
ary tree of mentions?a phylogeny. Phylogenetic
models are new to information extraction. In com-
putational historical linguistics, Bouchard-C?ot?e et
al. (2013) have also modeled the mutation of strings
along the edges of a phylogeny; but for them the
phylogeny is observed and most mentions are not,
while we observe the mentions only.
To apply our model to the CDCR task, we ob-
serve that the probability that two name mentions
are coreferent is the probability that they arose from
a common ancestor in the phylogeny. So we design
a Monte Carlo sampler to reconstruct likely phylo-
genies. A phylogeny must explain every observed
name. While our model is capable of generating
each name independently, a phylogeny will gener-
ally achieve higher probability if it explains similar
names as being similar by mutation (rather than
by coincidence). Thus, our sampled phylogenies
tend to make similar names coreferent?especially
long or unusual names that would be expensive to
generate repeatedly, and especially in contexts that
are topically similar and therefore have a higher
prior probability of coreference.
For learning, we iteratively adjust our model?s
parameters to better explain our samples. That is,
we do unsupervised training via Monte Carlo EM.
What is learned? An important component of
a CDCR system is its model of name similarity
(Winkler, 1999; Porter and Winkler, 1997), which
is often fixed up front. This role is played in our sys-
tem by the name mutation model, which we take to
be a variant of stochastic edit distance (Ristad and
Yianilos, 1996). Rather than fixing its parameters
before we begin CDCR, we learn them (without
supervision) as part of CDCR, by training from
samples of reconstructed phylogenies.
Name similarity is also an important component
of within-document coreference resolution, and ef-
forts in that area bear resemblance to our approach.
Haghighi and Klein (2010) describe an ?entity-
centered? model where a distance-dependent Chi-
nese restaurant process is used to pick previous
coreferent mentions within a document. Similarly,
Durrett and Klein (2013) learn a mention similarity
model based on labeled data. Our cross-document
setting has no observed mention ordering and no
observed entities: we must sum over all possibili-
ties, a challenging inference problem.
The second major component of CDCR is
context-based disambiguation of similar or iden-
tical names that refer to the same entity. Like
Kozareva and Ravi (2011) and Green et al (2012)
we use topics as the contexts, but learn mention
topics jointly with other model parameters.
3 Generative Model of Coreference
Let x = (x
1
, . . . , x
N
) denote an ordered sequence
of distinct named-entity mentions in documents
d = (d
1
, . . . , d
D
). We assume that each doc-
ument has a (single) known language, and that
its mentions and their types have been identified
by a named-entity recognizer. We use the object-
oriented notation x.v for attribute v of mention x.
Our model generates an ordered sequence x al-
though we do not observe its order. Thus each men-
tion x has latent position x.i (e.g., x
729
.i = 729).
The entire corpus, including these entities, is gen-
erated according to standard topic model assump-
tions; we first generate a topic distribution for a
document, then sample topics and words for the
document (Blei et al, 2003). However, any topic
may generate an entity type, e.g. PERSON, which is
then replaced by a specific name: when PERSON is
generated, the model chooses a previous mention
of any person and copies it, perhaps mutating its
name.
1
Alternatively, the model may manufacture
1
We make the closed-world assumption that the author is
776
a name for a new person, though the name itself
may not be new.
If all previous mentions were equally likely, this
would be a Chinese Restaurant Process (CRP) in
which frequently mentioned entities are more likely
to be mentioned again (?the rich get richer?). We
refine that idea by saying that the current topic, lan-
guage, and document influence the choice of which
previous mention to copy, similar to the distance-
dependent CRP (Blei and Frazier, 2011).
2
This will
help distinguish multiple John Smith entities if they
tend to appear in different contexts.
Formally, each mention x is derived from a par-
ent mention x.p where x.p.i < x.i (the parent
came first), x.e = x.p.e (same entity) and x.n is
a copy or mutation of x.p.n. In the special case
where x is a first mention of x.e, x.p is the special
symbol ?, x.e is a newly allocated entity of some
appropriate type, and the name x.n is generated
from scratch.
Our goal is to reconstruct mappings p, i, z that
specify the latent properties of the mentions x. The
mapping p : x 7? x.p forms a phylogenetic tree on
the mentions, with root?. Each entity corresponds
to a subtree that is rooted at some child of ?. The
mapping i : x 7? x.i gives an ordering consistent
with that tree in the sense that (?x)x.p.i < x.i.
Finally, the mapping z : x 7? x.z specifies, for
each mention, the topic that generated it. While i
and z are not necessary for creating coref clusters,
they are needed to produce p.
4 Detailed generative story
Given a few constants that are referenced in the
main text, we assume that the corpus d was gener-
ated as follows.
First, for each topic z = 1, . . .K and each lan-
guage `, choose a multinomial ?
z`
over the word
vocabulary, from a symmetric Dirichlet with con-
centration parameter ?. Then set m = 0 (entity
only aware of previous mentions from our corpus. This means
that two mentions cannot be derived from a common ancestor
outside our corpus. To mitigate this unrealistic assumption, we
allow any ordering x of the observed mentions, not respecting
document timestamps or forcing the mentions from a given
document to be generated as a contiguous subsequence of x.
2
Unlike the ddCRP, our generative story is careful to pro-
hibit derivational cycles: each mention is copied from a previ-
ous mention in the latent ordering. This is why our phylogeny
is a tree, and why our sampler is more complex. Also unlike
the ddCRP, we permit asymmetric ?distances?: if a certain
topic or language likes to copy mentions from another, the
compliment is not necessarily returned.
count), i = 0 (mention count), and for each docu-
ment index d = 1, . . . , D:
1. Choose the document?s length L and language
`. (The distributions used to choose these
are unimportant because these variables are
always observed.)
2. Choose its topic distribution ?
d
from an
asymmetric Dirichlet prior with parameters
m (Wallach et al, 2009).
3
3. For each token position k = 1, . . . , L:
(a) Choose a topic z
dk
? ?
d
.
(b) Choose a word conditioned on the topic
and language, w
dk
? ?
z
dk
`
.
(c) If w
dk
is a named entity type (PERSON,
PLACE, ORG, . . . ) rather than an ordinary
word, then increment i and:
i. create a new mention x with
x.e.t = w
dk
x.d = d x.` = `
x.i = i x.z = z
dk
x.k = k
ii. Choose the parent x.p from a distri-
bution conditioned on the attributes
just set (see ?4.1).
iii. If x.p = ?, increment m and set
x.e = a new entity e
m
. Else set
x.e = x.p.e.
iv. Choose x.n from a distribution con-
ditioned on x.p.n and x.` (see ?4.2).
Notice that the tokens w
dk
in document d are
exchangeable: by collapsing out ?
d
, we can re-
gard them as having been generated from a CRP.
Thus, for fixed values of the non-mention tokens
and their topics, the probability of generating the
mention sequence x is proportional to the prod-
uct of the probabilities of the choices in step 3 at
the positions dk where mentions were generated.
These choices generate a topic x.z (from the CRP
for document d), a type x.e.t (from ?
x.z
), a par-
ent mention (from the distribution over previous
mentions), and a name string (conditioned on the
parent?s name if any). ?5 uses this fact to construct
an MCMC sampler for the latent parts of x.
4.1 Sub-model for parent selection
To select a parent for a mention x of type t = x.e.t,
a simple model (as mentioned above) would be a
CRP: each previous mention of the same type is
selected with probability proportional to 1, and? is
3
Extension: This choice could depend on the language d.`.
777
selected with probability proportional to ?
t
> 0. A
larger choice of ?
t
results in smaller entity clusters,
because it prefers to create new entities of type t
rather than copying old ones.
We modify this story by re-weighting ? and
previous mentions according to their relative suit-
ability as the parent of x:
Pr?(x.p | x) =
exp (? ? f(x.p, x))
Z(x)
(1)
where x.p ranges over ? and all previous mentions
of the same type as x, that is, mentions p such that
p.i < x.i and p.e.t = x.e.t. The normalizing con-
stant Z(x)
def
=
?
p
exp (? ? f(x.p, x)) is chosen
so that the probabilities sum to 1.
This is a conditional log-linear model parameter-
ized by ?, where ?
k
? N (0, ?
2
k
). The features f
are extracted from the attributes of x and x.p. Our
most important feature tests whether x.p.z = x.z.
This binary feature has a high weight if authors
mainly choose mentions from the same topic. To
model which (other) topics tend to be selected, we
also have a binary feature for each parent topic
x.p.z and each topic pair (x.p.z, x.z).
4
4.2 Sub-model for name mutation
Let x denote a mention with parent p = x.p. As in
Andrews et al (2012), its name x.n is a stochastic
transduction of its parent?s name p.n. That is,
Pr?(x.n | p.n) (2)
is given by the probability that applying a random
sequence of edits to the characters of p.n would
yield x.n. The contextual probabilities of different
edits depend on learned parameters ?.
(2) is the total probability of all edit sequences
that derive x.n from p.n. It can be computed in
time O(|x.n| ? |p.n|) by dynamic programming.
The probability of a single edit sequence, which
corresponds to a monotonic alignment of x.n to
p.n, is a product of individual edit probabilities of
the form Pr?((
a
b
) | a?), which is conditioned on the
next input character a?. The edit (
a
b
) replaces input
a ? {, a?} with output b ? {} ? ? (where  is
4
Many other features could be added. In a multilingual
setting, one would similarly want to model whether English
authors select Arabic mentions. One could also imagine fea-
tures that reward proximity in the generative order (x.p.i ?
x.i), local linguistic relationships (when x.p.d = x.d and
x.p.k ? x.k), or social information flow (e.g., from main-
stream media to Twitter). One could also make more specific
versions of any feature by conjoining it with the entity type t.
the empty string and ? is the alphabet of language
x.`). Insertions and deletions are the cases where
respectively a =  or b = ?we do not allow both
at once. All other edits are substitutions. When
a? is the special end-of-string symbol #, the only
allowed edits are the insertion (

b
) and the substi-
tution (
#
#
). We define the edit probability using a
locally normalized log-linear model:
Pr?((
a
b
) | a?) =
exp(? ? f(a?, a, b))
?
a
?
,b
?
exp(? ? f(a?, a
?
, b
?
))
(3)
We use a small set of simple feature functions f ,
which consider conjunctions of the attributes of the
characters a? and b: character, character class (letter,
digit, etc.), and case (upper vs. lower).
More generally, the probability (2) may also be
conditioned on other variables such as on the lan-
guages p.` and x.`?this leaves room for a translit-
eration model when x.` 6= p.`?and on the entity
type x.t. The features in (3) may then depend on
these variables as well.
Notice that we use a locally normalized proba-
bility for each edit. This enables faster and sim-
pler training than the similar model of Dreyer et al
(2008), which uses a globally normalized probabil-
ity for the whole edit sequence.
When p = ?, we are generating a new name x.n.
We use the same model, taking?.n to be the empty
string (but with #
?
rather than # as the end-of-
string symbol). This yields a feature-based unigram
language model (whose character probabilities may
differ from usual insertion probabilities because
they see #
?
as the lookahead character).
Pragmatics. We can optionally make the model
more sophisticated. Authors tend to avoid names
x.n that readers would misinterpret (given the pre-
viously generated names). The edit model thinks
that Pr?(CIA | ?) is relatively high (because CIA is
a short string) and so is Pr?(CIA | Chuck?s Ice Art).
But in fact, if CIA has already been frequently used
to refer to the Central Intelligence Agency, then an
author is unlikely to use it for a different entity.
To model this pragmatic effect, we multiply
our definition of Pr?(x.n | p.n) by an extra fac-
tor Pr(x.e | x)
?
, where ? ? 0 is the effect
strength.
5
Here Pr(x.e | x) is the probability that
a reader correctly identifies the entity x.e. We
take this to be the probability that a reader who
knows our sub-models would guess some parent
5
Currently we omit the step of renormalizing this deficient
model. Our training procedure also ignores the extra factor.
778
having the correct entity (or ? if x is a first men-
tion):
?
p
?
:p
?
.e=x.e
w(p
?
, x)/
?
p
?
w(p
?
, x). Here p
?
ranges over mentions (including ?) that precede
x in the ordering i, and w(p
?
, x)?defined later in
sec. 5.3?is proportional to the posterior probabil-
ity that x.p = p
?
, given name x.n and topic x.z.
6
5 Inference by Block Gibbs Sampling
We use a block Gibbs sampler, which from an ini-
tial state (p
0
, i
0
, z
0
) repeats these steps:
1. Sample the ordering i from its conditional
distribution given all other variables.
2. Sample the topic vector z likewise.
3. Sample the phylogeny p likewise.
4. Output the current sample s
t
= (p, i, z).
It is difficult to draw exact samples at steps 1
and 2. Thus, we sample i or z from a simpler
proposal distribution, but correct the discrepancy
using the Independent Metropolis-Hastings (IMH)
strategy: with an appropriate probability, reject the
proposed new value and instead use another copy
of the current value (Tierney, 1994).
5.1 Resampling the ordering i
We resample the ordering i of the mentions x,
conditioned on the other variables. The current
phylogeny p already defines a partial order on x,
since each parent must precede its children. For
instance, phylogeny (a) below requires ? ? x and
? ? y. This partial order is compatible with 2
total orderings, ? ? x ? y and ? ? y ? x. By
contrast, phylogeny (b) requires the total ordering
? ? x ? y.
?
yx
(a)
?
x
y
(b)
We first sample an ordering i
?
(the ordering
of mentions with parent ?, i.e. all mentions) uni-
formly at random from the set of orderings compat-
ible with the current p. (We provide details about
this procedure in Appendix A.)
7
However, such or-
derings are not in fact equiprobable given the other
variables?some orderings better explain why that
phylogeny was chosen in the first place, according
6
Better, one could integrate over the reader?s guess of x.z.
7
The full version of this paper is available at
http://cs.jhu.edu/
?
noa/publications/
phylo-acl-14.pdf
to our competitive parent selection model (?4.1).
To correct for this bias using IMH, we accept the
proposed ordering i
?
with probability
a = min
(
1,
Pr(p, i?, z,x | ?,?)
Pr(p, i, z,x | ?,?)
)
(4)
where i is the current ordering. Otherwise we reject
i
?
and reuse i for the new sample.
5.2 Resampling the topics z
Each context word and each named entity is asso-
ciated with a latent topic. The topics of context
words are assumed exchangeable, and so we re-
sample them using Gibbs sampling (Griffiths and
Steyvers, 2004).
Unfortunately, this is prohibitively expensive for
the (non-exchangeable) topics of the named men-
tions x. A Gibbs sampler would have to choose
a new value for x.z with probability proportional
to the resulting joint probability of the full sample.
This probability is expensive to evaluate because
changing x.z will change the probability of many
edges in the current phylogeny p. (Equation (1)
puts x is in competition with other parents, so ev-
ery mention y that follows x must recompute how
happy it is with its current parent y.p.)
Rather than resampling one topic at a time, we re-
sample z as a block. We use a proposal distribution
for which block sampling is efficient, and use IMH
to correct the error in this proposal distribution.
Our proposal distribution is an undirected graph-
ical model whose random variables are the topics
z and whose graph structure is given by the current
phylogeny p:
Q(z) ?
?
x 6=?
?
x
(x.z)?
x.p,x
(x.p.z, x.z) (5)
Q(z) is an approximation to the posterior distri-
bution over z. As detailed below, a proposal can
be sampled from Q(z) in time O(|z|K
2
) where K
is the number of topics, because the only interac-
tions among topics are along the edges of the tree
p. The unary factor ?
x
gives a weight for each
possible value of x.z, and the binary factor ?
x.p,x
gives a weight for each possible value of the pair
(x.p.z, x.z).
The ?
x
(x.z) factors in (5) approximate the topic
model?s prior distribution over z. ?
x
(x.z) is pro-
portional to the probability that a Gibbs sampling
step for an ordinary topic model would choose this
value of x.z. This depends on whether?in the
779
current sample?x.z is currently common in x?s
document and x.t is commonly generated by x.z.
It ignores the fact that we will also be resampling
the topics of the other mentions.
The ?
x.p,x
factors in (5) approximate Pr(p |
z, i) (up to a constant factor), where p is the current
phylogeny. Specifically, ?
x.p,x
approximates the
probability of a single edge. It ought to be given
by (1), but we use only the numerator of (1), which
avoids modeling the competition among parents.
We sample from Q using standard methods, sim-
ilar to sampling from a linear-chain CRF by run-
ning the backward algorithm followed by forward
sampling. Specifically, we run the sum-product
algorithm from the leaves up to the root ?, at each
node x computing the following for each topic z:
?
x
(z)
def
= ?
x
(z) ?
?
y?children(x)
?
z
?
?
x,y
(z, z
?
) ? ?
y
(z
?
)
Then we sample from the root down to the leaves,
first sampling ?.z from ?
?
, then at each x 6= ?
sampling the topic x.z to be z with probability
proportional to ?
x.p,x
(x.p.z, z) ? ?
x
(z).
Again we use IMH to correct for the bias in Q:
we accept the resulting proposal
?
z with probability
min
(
1,
Pr(p, i,
?
z,x | ?,?)
Pr(p, i, z,x | ?,?)
?
Q(z)
Q(
?
z)
)
(6)
While Pr(p, i,
?
z,x | ?,?) might seem slow to
compute because it contains many factors (1) with
different denominators Z(x), one can share work
by visiting the mentions x in their order i. Most
summands in Z(x) were already included in Z(x
?
),
where x
?
is the latest previous mention having the
same attributes as x (e.g., same topic).
5.3 Resampling the phylogeny p
It is easy to resample the phylogeny. For each x, we
must choose a parent x.p from among the possible
parents p (having p.i < x.i and p.e.t = x.e.t).
Since the ordering i prevents cycles, the resulting
phylogeny p is indeed a tree.
Given the topics z, the ordering i, and the ob-
served names, we choose an x.p value according
to its posterior probability. This is proportional to
w(x.p, x)
def
= Pr?(x.p | x) ? Pr?(x.n | x.p.n),
independent of any other mention?s choice of par-
ent. The two factors here are given by (1) and (2)
respectively. As in the previous section, the de-
nominators Z(x) in the Pr(x.p | x) factors can be
computed efficiently with shared work.
With the pragmatic model (section 4.2), the par-
ent choices are no longer independent; then the
samples of p should be corrected by IMH as usual.
5.4 Initializing the sampler
The initial sampler state (z
0
,p
0
, i
0
) is obtained as
follows. (1) We fix topics z
0
via collapsed Gibbs
sampling (Griffiths and Steyvers, 2004). The sam-
pler is run for 1000 iterations, and the final sam-
pler state is taken to be z
0
. This process treats all
topics as exchangeable, including those associated
with named entities.(2) Given the topic assignment
z
0
, initialize p
0
to the phylogeny rooted at ? that
maximizes
?
x
logw(x.p, x). This is a maximum
rooted directed spanning tree problem that can be
solved in time O(n
2
) (Tarjan, 1977). The weight
w(x.p, x) is defined as in section 5.3?except that
since we do not yet have an ordering i, we do not
restrict the possible values of x.p to mentions p
with p.i < x.p.i. (3) Given p
0
, sample an ordering
i
0
using the procedure described in ?5.1.
6 Parameter Estimation
Evaluating the likelihood and its partial derivatives
with respect to the parameters of the model requires
marginalizing over our latent variables. As this
marginalization is intractable, we resort to Monte
Carlo EM procedure (Levine and Casella, 2001)
which iterates the following two steps:
E-step: Collect samples by MCMC simulation as
in ?5, given current model parameters ? and ?.
M-step: Improve ? and ? to increase
8
L
def
=
1
S
S
?
s=1
log Pr?,?(x,ps, is, zs) (7)
It is not necessary to locally maximize L at each
M-step, merely to improve it if it is not already
at a local maximum (Dempster et al, 1977). We
improve it by a single update: at the tth M-step, we
update our parameters to ?
t
= (?
t
,?
t
)
?
t
= ?
t?1
+ ??
t
?
?
L(x,?
t?1
) (8)
where ? is a fixed scaling term and ?
t
is an adap-
tive learning rate given by AdaGrad (Duchi et al,
2011).
We now describe how to compute the gradient
?
?
L. The gradient with respect to the parent se-
8
We actually do MAP-EM, which augments (7) by adding
the log-likelihoods of ? and ? under a Gaussian prior.
780
lection parameters ? is
?
1
S
?
?
f(p, x)?
?
p
?
Pr?(p
?
| x)f(p
?
, x)
?
?
(9)
The outer summation ranges over all edges in the
S samples. The other variables in (9) are associ-
ated with the edge being summed over. That edge
explains a mention x as a mutation of some parent
p in the context of a particular sample (p
s
, i
s
, z
s
).
The possible parents p
?
range over ? and the men-
tions that precede x according to the ordering i
s
,
while the features f and distribution Pr? depend
on the topics z
s
.
As for the mutation parameters, let c
p,x
be the
fraction of samples in which p is the parent of x.
This is the expected number of times that the string
p.n mutated into x.n. Given this weighted set of
string pairs, let c
a?,a,b
be the expected number of
times that edit (
a
b
) was chosen in context a?: this
can be computed using dynamic programming to
marginalize over the latent edit sequence that maps
p.n to x.n, for each (p, x). The gradient of L with
respect to ? is
?
a?,a,b
c
a?,a,b
(f(a?, a, b)?
?
a
?
,b
?
Pr?(a
?
, b
?
| a?)f(a?, a
?
, b
?
))
(10)
7 Consensus Clustering
From a single phylogeny p, we deterministically
obtain a clustering e by removing the root ?. Each
of the resulting connected components corresponds
to a cluster of mentions. Our model gives a distribu-
tion over phylogenies p (given observations x and
learned parameters ?)?and thus gives a posterior
distribution over clusterings e, which can be used
to answer various queries.
A traditional query is to request a single cluster-
ing e. We prefer the clustering e
?
that minimizes
Bayes risk (MBR) (Bickel and Doksum, 1977):
e
?
= argmin
e?
?
e
L(e
?
, e) Pr(e | x,?,?) (11)
This minimizes our expected loss, where L(e
?
, e)
denotes the loss associated with picking e
?
when
the true clustering is e. In practice, we again esti-
mate the expectation by sampling e values.
The Rand index (Rand, 1971)?unlike our actual
evaluation measure?is an efficient choice of loss
function L for use with (11):
R(e
?
, e)
def
=
TP + TN
TP + FP + TN + FN
=
TP + TN
(
N
2
)
where the true positives (TP), true negatives (TN),
false positives (FP), and false negatives (FN) use
the clustering e to evaluate how well e
?
classi-
fies the
(
N
2
)
mention pairs as coreferent or not.
More similar clusterings achieve larger R, with
R(e
?
, e) = 1 iff e
?
= e. In all cases, 0 ?
R(e
?
, e) = R(e, e
?
) ? 1.
The MBR decision rule for the (negated) Rand
index is easily seen to be equivalent to
e
?
= argmax
e?
E[TP] + E[TN] (12)
= argmax
e?
?
i,j: x
i
?x
j
s
ij
+
?
i,j: x
i
6?x
j
(1? s
ij
)
where ? denotes coreference according to e
?
. As
explained above, the s
ij
are coreference probabil-
ities s
ij
that can be estimated from a sample of
clusterings e.
This objective corresponds to min-max graph
cut (Ding et al, 2001), an NP-hard problem with
an approximate solution (Nie et al, 2010).
9
8 Experiments
In this section, we describe experiments on three
different datasets. Our main results are described
first: Twitter features many instances of name vari-
ation that we would like our model to be able to
learn. We also report the performance of different
ablations of our full approach, in order to see which
consistently helped across the different splits. We
report additional experiments on the ACE 2008 cor-
pus, and on a political blog corpus, to demonstrate
that our approach is applicable in different settings.
For Twitter and ACE 2008, we report the stan-
dard B
3
metric (Bagga and Baldwin, 1998a). For
the political blog dataset, the reference does not
consist of entity annotations, and so we follow the
evaluation procedure of Yogatama et al (2012).
8.1 Twitter
Data. We use a novel corpus of Twitter posts dis-
cussing the 2013 Grammy Award ceremony. This
is a challenging corpus, featuring many instances
9
In our experiments, we run the clustering algorithm five
times, initialized from samples chosen at random from the last
10% of the sampler run, and keep the clustering that achieved
highest expected Rand score.
781
of name variation. The dataset consists of five splits
(by entity), the smallest of which is 604 mentions
and the largest is 1374. We reserve the largest split
for development purposes, and report our results
on the remaining four. Appendix B provides more
detail about the dataset.
Baselines. We use the discriminative entity cluster-
ing algorithm of Green et al (2012) as our baseline;
their approach was found to outperform another
generative model which produced a flat cluster-
ing of mentions via a Dirichlet process mixture
model. Their method uses Jaro-Winkler string sim-
ilarity to match names, then clusters mentions with
matching names (for disambiguation) by compar-
ing their unigram context distributions using the
Jenson-Shannon metric. We also compare to the
EXACT-MATCH baseline, which assigns all strings
with the same name to the same entity.
Procedure. We run four test experiments in which
one split is used to pick model hyperparameters
and the remaining three are used for test. For the
discriminative baseline, we tune the string match
threshold, context threshold, and the weight of the
context model prior (all via grid search). For our
model, we tune only the fixed weight of the root
feature, which determines the precision/recall trade-
off (larger values of this feature result in more
attachments to ? and hence more entities). We
leave other hyperparameters fixed: 16 latent top-
ics, and Gaussian priors N (0, 1) on all log-linear
parameters. For PHYLO, the entity clustering is
the result of (1) training the model using EM, (2)
sampling from the posterior to obtain a distribu-
tion over clusterings, and (3) finding a consensus
clustering. We use 20 iterations of EM with 100
samples per E-step for training, and use 1000 sam-
ples after training to estimate the posterior. We
report results using three variations of our model:
PHYLO does not consider mention context (all men-
tions effectively have the same topic) and deter-
mines mention entities from a single sample of
p (the last); PHYLO+TOPIC adds context (?5.2);
PHYLO+TOPIC+MBR uses the full posterior and
consensus clustering to pick the output clustering
(?7). Our results are shown in Table 1.
10
10
Our single-threaded implementation took around 15 min-
utes per fold of the Twitter corpus on a personal laptop with
a 2.3 Ghz Intel Core i7 processor (including time required to
parse the data files). Typical acceptance rates for ordering and
topic proposals ranged from 0.03 to 0.08.
Mean Test B
3
P R F1
EXACT-MATCH 99.6 53.7 69.8
Green et al (2012) 92.1 69.8 79.3
PHYLO 85.3 91.4 88.7
PHYLO+TOPIC 92.8 90.8 91.8
PHYLO+TOPIC+MBR 92.9 90.9 91.9
Table 1: Results for the Twitter dataset. Higher B
3
scores
are better. Note that each number is averaged over four
different test splits. In three out of four experiments,
PHYLO+TOPIC+MBR achieved the highest F1 score; in one
case PHYLO+TOPIC won by a small margin.
Test B
3
P R F1
PER
EXACT-MATCH 98.0 81.2 88.8
Green et al (2012) 95.0 88.9 91.9
PHYLO+TOPIC+MBR 97.2 88.6 92.7
ORG
EXACT-MATCH 98.2 78.3 87.1
Green et al (2012) 92.1 88.5 90.3
PHYLO+TOPIC+MBR 95.5 80.9 87.6
Table 2: Results for the ACE 2008 newswire dataset.
8.2 Newswire
Data. We use the ACE 2008 dataset, which is
described in detail in Green et al (2012). It is
split into a development portion and a test portion.
The baseline system took the first mention from
each (gold) within-document coreference chain as
the canonical mention, ignoring other mentions in
the chain; we follow the same procedure in our
experiments.
11
Baselines & Procedure. We use the same base-
lines as in ?8.1. On development data, modeling
pragmatics as in ?4.2 gave large improvements for
organizations (8 points in F-measure), correcting
the tendency to assume that short names like CIA
were coincidental homonyms. Hence we allowed
? > 0 and tuned it on development data.
12
Results
are in Table 2.
8.3 Blogs
Data. The CMU political blogs dataset consists of
3000 documents about U.S. politics (Yano et al,
2009). Preprocessed as described in Yogatama et al
(2012), the data consists of 10647 entity mentions.
11
That is, each within-document coreference chain is
mapped to a single mention as a preprocessing step.
12
We used only a simplified version of the pragmatic model,
approximating w(p
?
, x) as 1 or 0 according to whether p
?
.n =
x.n. We also omitted the IMH step from section 5.3. The
other results we report do not use pragmatics at all, since we
found that it gave only a slight improvement on Twitter.
782
Unlike our other datasets, mentions are not anno-
tated with entities: the reference consists of a table
of 126 entities, where each row is the canonical
name of one entity.
Baselines. We compare to the system results
reported in Figure 2 of Yogatama et al (2012).
This includes a baseline hierarchical clustering ap-
proach, the ?EEA? name canonicalization system
of Eisenstein et al (2011), as well the model pro-
posed by Yogatama et al (2012). Like the output
of our model, the output of their hierarchical clus-
tering baseline is a mention clustering, and there-
fore must be mapped to a table of canonical entity
names to compare to the reference table.
Procedure & Results We tune our method as in
previous experiments, on the initialization data
used by Yogatama et al (2012) which consists of
a subset of 700 documents of the full dataset. The
tuned model then produced a mention clustering
on the full political blog corpus. As the mapping
from clusters to a table is not fully detailed in Yo-
gatama et al (2012), we used a simple heuristic:
the most frequent name in each cluster is taken as
the canonical name, augmented by any titles from
a predefined list appearing in any other name in
the cluster. The resulting table is then evaluated
against the reference, as described in Yogatama et
al. (2012). We achieved a response score of 0.17
and a reference score of 0.61. Though not state-of-
the-art, this result is close to the score of the ?EEA?
system of Eisenstein et al (2011), as reported in
Figure 2 of Yogatama et al (2012), which is specif-
ically designed for the task of canonicalization.
8.4 Discussion
On the Twitter dataset, we obtained a 12.6-point F1
improvement over the baseline. To understand our
model?s behavior, we looked at the sampled phy-
logenetic trees on development data. One reason
our model does well in this noisy domain is that
it is able to relate seemingly dissimilar names via
successive steps. For instance, our model learned
to relate many variations of LL Cool J:
Cool James LLCoJ El-El Cool John
LL LL COOL JAMES LLCOOLJ
In the sample we inspected, these mentions were
also assigned the same topic, further boosting the
probability of the configuration.
The ACE dataset, consisting of editorialized
newswire, naturally contains less name variation
than Twitter data. Nonetheless, we find that the
variation that does appear is often properly handled
by our model. For instance, we see several in-
stances of variation due to transliteration that were
all correctly grouped together, such as Megawati
Soekarnoputri and Megawati Sukarnoputri. The prag-
matic model was also effective in grouping com-
mon acronyms into the same entity.
We found that multiple samples tend to give dif-
ferent phylogenies (so the sampler is mobile), but
essentially the same clustering into entities (which
is why consensus clustering did not improve much
over simply using the last sample). Random restarts
of EM might create more variety by choosing dif-
ferent locally optimal parameter settings. It may
also be beneficial to explore other sampling tech-
niques (Bouchard-C?ot?e, 2014).
Our method assembles observed names into an
evolutionary tree. However, the true tree must in-
clude many names that fall outside our small ob-
served corpora, so our model would be a more
appropriate fit for a far larger corpus. Larger cor-
pora also offer stronger signals that might enable
our Monte Carlo methods to mix faster and detect
regularities more accurately.
A common error of our system is to connect
mentions that share long substrings, such as dif-
ferent PERSONs who share a last name, or differ-
ent ORGANIZATIONs that contain University of. A
more powerful name mutation than the one we use
here would recognize entire words, for example
inserting a common title or replacing a first name
with its common nickname. Modeling the internal
structure of names (Johnson, 2010; Eisenstein et
al., 2011; Yogatama et al, 2012) in the mutation
model is a promising future direction.
9 Conclusions
Our primary contribution consists of new model-
ing ideas, and associated inference techniques, for
the problem of cross-document coreference resolu-
tion. We have described how writers systematically
plunder (?) and then systematically modify (?) the
work of past writers. Inference under such models
could also play a role in tracking evolving memes
and social influence, not merely in establishing
strict coreference. Our model also provides an al-
ternative to the distance-dependent CRP.
2
Our implementation is available for re-
search use at: https://bitbucket.org/
noandrews/phyloinf.
783
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 344?355, Jeju, Korea, July.
Amit Bagga and Breck Baldwin. 1998a. Algorithms
for scoring coreference chains. In In The First In-
ternational Conference on Language Resources and
Evaluation Workshop on Linguistics Coreference,
pages 563?566.
Amit Bagga and Breck Baldwin. 1998b. Entity-
based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 1, ACL ?98, pages
79?85, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Alex Baron and Marjorie Freedman. 2008. Who
is who and what is what: Experiments in cross-
document co-reference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 274?283, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe-
matical Statistics : Basic Ideas and Selected Topics.
Holden-Day, Inc.
David M. Blei and Peter I. Frazier. 2011. Distance
dependent chinese restaurant processes. J. Mach.
Learn. Res., 12:2461?2488, November.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Alexandre Bouchard-C?ot?e, David Hall, Thomas L.
Griffiths, and Dan Klein. 2013. Automated re-
construction of ancient languages using probabilis-
tic models of sound change. Proceedings of the Na-
tional Academy of Sciences.
Alexandre Bouchard-C?ot?e. 2014. Sequential Monte
Carlo (SMC) for Bayesian phylogenetics. Bayesian
phylogenetics: methods, algorithms, and applica-
tions.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A comparison of string metrics for
matching names and records. In KDD Workshop on
data cleaning and object consolidation.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38.
C.H.Q. Ding, Xiaofeng He, Hongyuan Zha, Ming Gu,
and H.D. Simon. 2001. A min-max cut algorithm
for graph partitioning and data clustering. In Data
Mining, 2001. ICDM 2001, Proceedings IEEE Inter-
national Conference on, pages 107 ?114.
Mark Dredze, Michael J Paul, Shane Bergsma, and
Hieu Tran. 2013. Carmen: A twitter geolocation
system with applications to public health. In AAAI
Workshop on Expanding the Boundaries of Health
Informatics Using AI (HIAI).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1080?1089, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1971?1982.
Association for Computational Linguistics.
Jacob Eisenstein, Tae Yano, William W. Cohen,
Noah A. Smith, and Eric P. Xing. 2011. Structured
databases of named entities from bayesian nonpara-
metrics. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ?11, pages
2?12, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
T. Finin, Z. Syed, J. Mayfield, P. McNamee, and C. Pi-
atko. 2009. Using Wikitology for cross-document
entity coreference resolution. In AAAI Spring Sym-
posium on Learning by Reading and Learning to
Read.
Spence Green, Nicholas Andrews, Matthew R. Gorm-
ley, Mark Dredze, and Christopher D. Manning.
2012. Entity clustering across languages. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 60?69, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Stephen Guo, Ming-Wei Chang, and Emre K?c?man.
2013. To link or not to link? a study on end-to-end
tweet entity linking. In Proceedings of NAACL-HLT,
pages 1020?1030.
784
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 385?393,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Mark Johnson. 2010. Pcfgs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 1148?1157,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Zornitsa Kozareva and Sujith Ravi. 2011. Unsuper-
vised name ambiguity resolution using a generative
model. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ?11, pages
105?112, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Richard A. Levine and George Casella. 2001. Im-
plementations of the Monte Carlo EM Algorithm.
Journal of Computational and Graphical Statistics,
10(3):422?439.
Feiping Nie, Chris H. Q. Ding, Dijun Luo, and Heng
Huang. 2010. Improved minmax cut graph cluster-
ing with nonnegative relaxation. In Jos?e L. Balc?azar,
Francesco Bonchi, Aristides Gionis, and Mich`ele
Sebag, editors, ECML/PKDD (2), volume 6322 of
Lecture Notes in Computer Science, pages 451?466.
Springer.
E. H. Porter and W. E. Winkler, 1997. Approximate
String Comparison and its Effect on an Advanced
Record Linkage System, chapter 6, pages 190?199.
U.S. Bureau of the Census.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66(336):846?850.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 1050?1058, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learn-
ing string edit distance. Technical Report CS-TR-
532-96, Princeton University, Department of Com-
puter Science.
Eric Sven Ristad and Peter N. Yianilos. 1998.
Learning string edit distance. IEEE Transactions
on Pattern Recognition and Machine Intelligence,
20(5):522?532, May.
Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-scale
cross-document coreference using distributed infer-
ence and hierarchical models. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 793?803, Portland, Oregon, USA, June.
Association for Computational Linguistics.
R E Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?35.
Luke Tierney. 1994. Markov Chains for Exploring
Posterior Distributions. The Annals of Statistics,
22(4):1701?1728.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking lda: Why priors matter. In
Advances in Neural Information Processing Systems,
pages 1973?1981.
Michael Wick, Sameer Singh, and Andrew McCallum.
2012. A discriminative hierarchical model for fast
coreference at large scale. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 379?388, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
William E. Winkler. 1999. The state of record link-
age and current research problems. Technical report,
Statistical Research Division, U.S. Census Bureau.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
477?485, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dani Yogatama, Yanchuan Sim, and Noah A. Smith.
2012. A probabilistic model for canonicalizing
named entity mentions. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers - Volume 1, ACL
?12, pages 685?693, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
785
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1177?1187,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Low-Resource Semantic Role Labeling
Matthew R. Gormley
1
Margaret Mitchell
2
Benjamin Van Durme
1
Mark Dredze
1
1
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD 21211
2
Microsoft Research
Redmond, WA 98052
mrg@cs.jhu.edu | memitc@microsoft.com | vandurme@cs.jhu.edu | mdredze@cs.jhu.edu
Abstract
We explore the extent to which high-
resource manual annotations such as tree-
banks are necessary for the task of se-
mantic role labeling (SRL). We examine
how performance changes without syntac-
tic supervision, comparing both joint and
pipelined methods to induce latent syn-
tax. This work highlights a new applica-
tion of unsupervised grammar induction
and demonstrates several approaches to
SRL in the absence of supervised syntax.
Our best models obtain competitive results
in the high-resource setting and state-of-
the-art results in the low resource setting,
reaching 72.48% F1 averaged across lan-
guages. We release our code for this work
along with a larger toolkit for specifying
arbitrary graphical structure.
1
1 Introduction
The goal of semantic role labeling (SRL) is to
identify predicates and arguments and label their
semantic contribution in a sentence. Such labeling
defines who did what to whom, when, where and
how. For example, in the sentence ?The kids ran
the marathon?, ran assigns a role to kids to denote
that they are the runners; and a role to marathon to
denote that it is the race course.
Models for SRL have increasingly come to rely
on an array of NLP tools (e.g., parsers, lem-
matizers) in order to obtain state-of-the-art re-
sults (Bj?orkelund et al, 2009; Zhao et al, 2009).
Each tool is typically trained on hand-annotated
data, thus placing SRL at the end of a very high-
resource NLP pipeline. However, richly annotated
data such as that provided in parsing treebanks is
expensive to produce, and may be tied to specific
domains (e.g., newswire). Many languages do
1
http://www.cs.jhu.edu/
?
mrg/software/
not have such supervised resources (low-resource
languages), which makes exploring SRL cross-
linguistically difficult.
The problem of SRL for low-resource lan-
guages is an important one to solve, as solutions
pave the way for a wide range of applications: Ac-
curate identification of the semantic roles of enti-
ties is a critical step for any application sensitive to
semantics, from information retrieval to machine
translation to question answering.
In this work, we explore models that minimize
the need for high-resource supervision. We ex-
amine approaches in a joint setting where we
marginalize over latent syntax to find the optimal
semantic role assignment; and a pipeline setting
where we first induce an unsupervised grammar.
We find that the joint approach is a viable alterna-
tive for making reasonable semantic role predic-
tions, outperforming the pipeline models. These
models can be effectively trained with access to
only SRL annotations, and mark a state-of-the-art
contribution for low-resource SRL.
To better understand the effect of the low-
resource grammars and features used in these
models, we further include comparisons with (1)
models that use higher-resource versions of the
same features; (2) state-of-the-art high resource
models; and (3) previous work on low-resource
grammar induction. In sum, this paper makes
several experimental and modeling contributions,
summarized below.
Experimental contributions:
? Comparison of pipeline and joint models for
SRL.
? Subtractive experiments that consider the re-
moval of supervised data.
? Analysis of the induced grammars in un-
supervised, distantly-supervised, and joint
training settings.
1177
Modeling contributions:
? Simpler joint CRF for syntactic and semantic
dependency parsing than previously reported.
? New application of unsupervised grammar
induction: low-resource SRL.
? Constrained grammar induction using SRL
for distant-supervision.
? Use of Brown clusters in place of POS tags
for low-resource SRL.
The pipeline models are introduced in ? 3.1 and
jointly-trained models for syntactic and semantic
dependencies (similar in form to Naradowsky et
al. (2012)) are introduced in ? 3.2. In the pipeline
models, we develop a novel approach to unsu-
pervised grammar induction and explore perfor-
mance using SRL as distant supervision. The joint
models use a non-loopy conditional random field
(CRF) with a global factor constraining latent syn-
tactic edge variables to form a tree. Efficient exact
marginal inference is possible by embedding a dy-
namic programming algorithm within belief prop-
agation as in Smith and Eisner (2008).
Even at the expense of no dependency path fea-
tures, the joint models best pipeline-trained mod-
els for state-of-the-art performance in the low-
resource setting (? 4.4). When the models have ac-
cess to observed syntactic trees, they achieve near
state-of-the-art accuracy in the high-resource set-
ting on some languages (? 4.3).
Examining the learning curve of the joint and
pipeline models in two languages demonstrates
that a small number of labeled SRL examples may
be essential for good end-task performance, but
that the choice of a good model for grammar in-
duction has an even greater impact.
2 Related Work
Our work builds upon research in both seman-
tic role labeling and unsupervised grammar in-
duction (Klein and Manning, 2004; Spitkovsky
et al, 2010a). Previous related approaches to se-
mantic role labeling include joint classification of
semantic arguments (Toutanova et al, 2005; Jo-
hansson and Nugues, 2008), latent syntax induc-
tion (Boxwell et al, 2011; Naradowsky et al,
2012), and feature engineering for SRL (Zhao et
al., 2009; Bj?orkelund et al, 2009).
Toutanova et al (2005) introduced one of
the first joint approaches for SRL and demon-
strated that a model that scores the full predicate-
argument structure of a parse tree could lead to
significant error reduction over independent clas-
sifiers for each predicate-argument relation.
Johansson and Nugues (2008) and Llu??s et al
(2013) extend this idea by coupling predictions of
a dependency parser with predictions from a se-
mantic role labeler. In the model from Johans-
son and Nugues (2008), the outputs from an SRL
pipeline are reranked based on the full predicate-
argument structure that they form. The candidate
set of syntactic-semantic structures is reranked us-
ing the probability of the syntactic tree and seman-
tic structure. Llu??s et al (2013) use a joint arc-
factored model that predicts full syntactic paths
along with predicate-argument structures via dual
decomposition.
Boxwell et al (2011) and Naradowsky et al
(2012) observe that syntax may be treated as la-
tent when a treebank is not available. Boxwell
et al (2011) describe a method for training a se-
mantic role labeler by extracting features from a
packed CCG parse chart, where the parse weights
are given by a simple ruleset. Naradowsky et
al. (2012) marginalize over latent syntactic depen-
dency parses.
Both Boxwell et al (2011) and Naradowsky
et al (2012) suggest methods for SRL without
supervised syntax, however, their features come
largely from supervised resources. Even in their
lowest resource setting, Boxwell et al (2011) re-
quire an oracle CCG tag dictionary extracted from
a treebank. Naradowsky et al (2012) limit their
exploration to a small set of basic features, and
included high-resource supervision in the form
of lemmas, POS tags, and morphology available
from the CoNLL 2009 data.
There has not yet been a comparison of tech-
niques for SRL that do not rely on a syntactic
treebank, and no exploration of probabilistic mod-
els for unsupervised grammar induction within an
SRL pipeline that we have been able to find.
Related work for the unsupervised learning of
dependency structures separately from semantic
roles primarily comes from Klein and Manning
(2004), who introduced the Dependency Model
with Valence (DMV). This is a robust generative
model that uses a head-outward process over word
classes, where heads generate arguments.
Spitkovsky et al (2010a) show that Viterbi
(hard) EM training of the DMV with simple uni-
form initialization of the model parameters yields
higher accuracy models than standard soft-EM
1178
  
ParsingModel SemanticDependencyModelCorpusText
Text LabeledWith SemanticRoles
Train Time, Constrained Grammar Induction:Observed Constraints
Figure 1: Pipeline approach to SRL. In this sim-
ple pipeline, the first stage syntactically parses the
corpus, and the second stage predicts semantic
predicate-argument structure for each sentence us-
ing the labels of the first stage as features. In our
low-resource pipelines, we assume that the syntac-
tic parser is given no labeled parses?however, it
may optionally utilize the semantic parses as dis-
tant supervision. Our experiments also consider
?longer? pipelines that include earlier stages: a
morphological analyzer, POS tagger, lemmatizer.
training. In Viterbi EM, the E-step finds the max-
imum likelihood corpus parse given the current
model parameters. The M-step then finds the
maximum likelihood parameters given the corpus
parse. We utilize this approach to produce unsu-
pervised syntactic features for the SRL task.
Grammar induction work has further demon-
strated that distant supervision in the form of
ACE-style relations (Naseem and Barzilay, 2011)
or HTML markup (Spitkovsky et al, 2010b)
can lead to considerable gains. Recent work in
fully unsupervised dependency parsing has sup-
planted these methods with even higher accuracies
(Spitkovsky et al, 2013) by arranging optimiz-
ers into networks that suggest informed restarts
based on previously identified local optima. We do
not reimplement these approaches within the SRL
pipeline here, but provide comparison of these
methods against our grammar induction approach
in isolation in ? 4.5.
In both pipeline and joint models, we use fea-
tures adapted from state-of-the-art approaches to
SRL. This includes Zhao et al (2009) features,
who use feature templates from combinations
of word properties, syntactic positions including
head and children, and semantic properties; and
features from Bj?orkelund et al (2009), who utilize
features on syntactic siblings and the dependency
path concatenated with the direction of each edge.
Features are described further in ? 3.3.
3 Approaches
We consider an array of models, varying:
1. Pipeline vs. joint training (Figures 1 and 2)
2. Types of supervision
3. The objective function at the level of syntax
3.1 Unsupervised Syntax in the Pipeline
Typical SRL systems are trained following a
pipeline where the first component is trained on
supervised data, and each subsequent component
is trained using the 1-best output of the previous
components. A typical pipeline consists of a POS
tagger, dependency parser, and semantic role la-
beler. In this section, we introduce pipelines that
remove the need for a supervised tagger and parser
by training in an unsupervised and distantly super-
vised fashion.
Brown Clusters We use fully unsupervised
Brown clusters (Brown et al, 1992) in place of
POS tags. Brown clusters have been used to good
effect for various NLP tasks such as named entity
recognition (Miller et al, 2004) and dependency
parsing (Koo et al, 2008; Spitkovsky et al, 2011).
The clusters are formed by a greedy hierachi-
cal clustering algorithm that finds an assignment
of words to classes by maximizing the likelihood
of the training data under a latent-class bigram
model. Each word type is assigned to a fine-
grained cluster at a leaf of the hierarchy of clusters.
Each cluster can be uniquely identified by the path
from the root cluster to that leaf. Representing this
path as a bit-string (with 1 indicating a left and 0
indicating a right child) allows a simple coarsen-
ing of the clusters by truncating the bit-strings. We
train 1000 Brown clusters for each of the CoNLL-
2009 languages on Wikipedia text.
2
Unsupervised Grammar Induction Our first
method for grammar induction is fully unsuper-
vised Viterbi EM training of the Dependency
Model with Valence (DMV) (Klein and Manning,
2004), with uniform initialization of the model pa-
rameters. We define the DMV such that it gener-
ates sequences of word classes: either POS tags
or Brown clusters as in Spitkovsky et al (2011).
The DMV is a simple generative model for pro-
jective dependency trees. Children are generated
recursively for each node. Conditioned on the par-
ent class, the direction (right or left), and the cur-
rent valence (first child or not), a coin is flipped to
decide whether to generate another child; the dis-
tribution over child classes is conditioned on only
the parent class and direction.
2
The Wikipedia text was tokenized for Polyglot (Al-Rfou?
et al, 2013): http://bit.ly/embeddings
1179
Constrained Grammar Induction Our second
method, which we will refer to as DMV+C, in-
duces grammar in a distantly supervised fashion
by using a constrained parser in the E-step of
Viterbi EM. Since the parser is part of a pipeline,
we constrain it to respect the downstream SRL an-
notations during training. At test time, the parser
is unconstrained.
Dependency-based semantic role labeling can
be described as a simple structured prediction
problem: the predicted structure is a labeled di-
rected graph, where nodes correspond to words
in the sentence. Each directed edge indicates that
there is a predicate-argument relationship between
the two words; the parent is the predicate and the
child the argument. The label on the edge indi-
cates the type of semantic relationship. Unlike
syntactic dependency parsing, the graph is not re-
quired to be a tree, nor even a connected graph.
Self-loops and crossing arcs are permitted.
The constrained syntactic DMV parser treats
the semantic graph as observed, and constrains the
syntactic parent to be chosen from one of the se-
mantic parents, if there are any. In some cases,
imposing this constraint would not permit any pro-
jective dependency parses?in this case, we ignore
the semantic constraint for that sentence. We parse
with the CKY algorithm (Younger, 1967; Aho and
Ullman, 1972) by utilizing a PCFG corresponding
to the DMV (Cohn et al, 2010). Each chart cell al-
lows only non-terminals compatible with the con-
strained sets. This can be viewed as a variation of
Pereira and Schabes (1992).
Semantic Dependency Model As described
above, semantic role labeling can be cast as a
structured prediction problem where the structure
is a labeled semantic dependency graph. We de-
fine a conditional random field (CRF) (Lafferty et
al., 2001) for this task. Because each word in a
sentence may be in a semantic relationship with
any other word (including itself), a sentence of
length n has n
2
possible edges. We define a single
L+1-ary variable for each edge, whose value can
be any of L semantic labels or a special label indi-
cating there is no predicate-argument relationship
between the two words. In this way, we jointly
perform identification (determining whether a se-
mantic relationship exists) and classification (de-
termining the semantic label). This use of an L+1-
ary variable is in contrast to the model of Narad-
owsky et al (2012), which used a more complex
  
DEPTREE
Dep
1,1
Role
1,1
Role
1,2
Role
1,3
Role
n,n
Dep
1,2
Dep
1,3
Dep
n,n ...
 ...
Figure 2: Factor graph for the joint syntac-
tic/semantic dependency parsing model.
set of binary variables and required a constraint
factor permitting AT-MOST-ONE. We include one
unary factor for each variable.
We optionally include additional variables that
perform word sense disambiguation for each pred-
icate. Each has a unary factor and is completely
disconnected from the semantic edge (similar to
Naradowsky et al (2012)). These variables range
over all the predicate senses observed in the train-
ing data for the lemma of that predicate.
3.2 Joint Syntactic and Semantic Parsing
Model
In Section 3.1, we introduced pipeline-trained
models for SRL, which used grammar induction
to predict unlabeled syntactic parses. In this sec-
tion, we define a simple model for joint syntactic
and semantic dependency parsing.
This model extends the CRF model in Section
3.1 to include the projective syntactic dependency
parse for a sentence. This is done by includ-
ing an additional n
2
binary variables that indicate
whether or not a directed syntactic dependency
edge exists between a pair of words in the sen-
tence. Unlike the semantic dependencies, these
syntactic variables must be coupled so that they
produce a projective dependency parse; this re-
quires an additional global constraint factor to en-
sure that this is the case (Smith and Eisner, 2008).
The constraint factor touches all n
2
syntactic-edge
variables, and multiplies in 1.0 if they form a pro-
jective dependency parse, and 0.0 otherwise. We
couple each syntactic edge variable to its semantic
edge variable with a binary factor. Figure 2 shows
the factor graph for this joint model.
Note that our factor graph does not contain any
loops, thereby permitting efficient exact marginal
inference just as in Naradowsky et al (2012). We
1180
Property Possible values
1 word form all word forms
2 lower case word form all lower-case forms
3 5-char word form prefixes all 5-char form prefixes
4 capitalization True, False
5 top-800 word form top-800 word forms
6 brown cluster 000, 1100, 010110001, ...
7 brown cluster, length 5 length 5 prefixes of brown clusters
8 lemma all word lemmas
9 POS tag NNP, CD, JJ, DT, ...
10 morphological features Gender, Case, Number, ...
(different across languages)
11 dependency label SBJ, NMOD, LOC, ...
12 edge direction Up, Down
Table 1: Word and edge properties in templates.
i, i-1, i+1 noFarChildren(w
i
) linePath(w
p
, w
c
)
parent(w
i
) rightNearSib(w
i
) depPath(w
p
, w
c
)
allChildren(w
i
) leftNearSib(w
i
) depPath(w
p
, w
lca
)
rightNearChild(w
i
) firstVSupp(w
i
) depPath(w
c
, w
lca
)
rightFarChild(w
i
) lastVSupp(w
i
) depPath(w
lca
, w
root
)
leftNearChild(w
i
) firstNSupp(w
i
)
leftFarChild(w
i
) lastNSupp(w
i
)
Table 2: Word positions used in templates. Based
on current word position (i), positions related to
current word w
i
, possible parent, child (w
p
, w
c
),
lowest common ancestor between parent/child
(w
lca
), and syntactic root (w
root
).
train our CRF models by maximizing conditional
log-likelihood using stochastic gradient descent
with an adaptive learning rate (AdaGrad) (Duchi
et al, 2011) over mini-batches.
The unary and binary factors are defined with
exponential family potentials. In the next section,
we consider binary features of the observations
(the sentence and labels from previous pipeline
stages) which are conjoined with the state of the
variables in the factor.
3.3 Features for CRF Models
Our feature design stems from two key ideas.
First, for SRL, it has been observed that fea-
ture bigrams (the concatenation of simple fea-
tures such as a predicate?s POS tag and an ar-
gument?s word) are important for state-of-the-art
(Zhao et al, 2009; Bj?orkelund et al, 2009). Sec-
ond, for syntactic dependency parsing, combining
Brown cluster features with word forms or POS
tags yields high accuracy even with little training
data (Koo et al, 2008).
We create binary indicator features for each
model using feature templates. Our feature tem-
plate definitions build from those used by the top
performing systems in the CoNLL-2009 Shared
Task, Zhao et al (2009) and Bj?orkelund et al
(2009) and from features in syntactic dependency
parsing (McDonald et al, 2005; Koo et al, 2008).
Template Possible values
relative position before, after, on
distance, continuity Z
+
binned distance > 2, 5, 10, 20, 30, or 40
geneological relationship parent, child, ancestor, descendant
path-grams the NN went
Table 3: Additional standalone templates.
Template Creation Feature templates are de-
fined over triples of ?property, positions, order?.
Properties, listed in Table 1, are extracted from
word positions within the sentence, shown in Ta-
ble 2. Single positions for a word w
i
include
its syntactic parent, its leftmost farthest child
(leftFarChild), its rightmost nearest sibling (rightNearSib),
etc. Following Zhao et al (2009), we include the
notion of verb and noun supports and sections of
the dependency path. Also following Zhao et al
(2009), properties from a set of positions can be
put together in three possible orders: as the given
sequence, as a sorted list of unique strings, and re-
moving all duplicated neighbored strings. We con-
sider both template unigrams and bigrams, com-
bining two templates in sequence.
Additional templates we include are the relative
position (Bj?orkelund et al, 2009), geneological re-
lationship, distance (Zhao et al, 2009), and binned
distance (Koo et al, 2008) between two words in
the path. From Llu??s et al (2013), we use 1, 2, 3-
gram path features of words/POS tags (path-grams),
and the number of non-consecutive token pairs in
a predicate-argument path (continuity).
3.4 Feature Selection
Constructing all feature template unigrams and bi-
grams would yield an unwieldy number of fea-
tures. We therefore determine the top N template
bigrams for a dataset and factor a according to an
information gain measure (Martins et al, 2011):
IG
a,m
=
?
f?T
m
?
x
a
p(f, x
a
) log
2
p(f, x
a
)
p(f)p(x
a
)
where T
m
is the mth feature template, f is a par-
ticular instantiation of that template, and x
a
is an
assignment to the variables in factor a. The proba-
bilities are empirical estimates computed from the
training data. This is simply the mutual informa-
tion of the feature template instantiation with the
variable assignment.
This filtering approach was treated as a sim-
ple baseline in Martins et al (2011) to contrast
with increasingly popular gradient based regular-
ization approaches. Unlike the gradient based ap-
1181
proaches, this filtering approach easily scales to
many features since we can decompose the mem-
ory usage over feature templates.
As an additional speedup, we reduce the dimen-
sionality of our feature space to 1 million for each
clique using a common trick referred to as fea-
ture hashing (Weinberger et al, 2009): we map
each feature instantiation to an integer using a hash
function
3
modulo the desired dimentionality.
4 Experiments
We are interested in the effects of varied super-
vision using pipeline and joint training for SRL.
To compare to prior work (i.e., submissions to the
CoNLL-2009 Shared Task), we also consider the
joint task of semantic role labeling and predicate
sense disambiguation. Our experiments are sub-
tractive, beginning with all supervision available
and then successively removing (a) dependency
syntax, (b) morphological features, (c) POS tags,
and (d) lemmas. Dependency syntax is the most
expensive and difficult to obtain of these various
forms of supervision. We explore the importance
of both the labels and structure, and what quantity
of supervision is useful.
4.1 Data
The CoNLL-2009 Shared Task (Haji?c et al, 2009)
dataset contains POS tags, lemmas, morpholog-
ical features, syntactic dependencies, predicate
senses, and semantic roles annotations for 7 lan-
guages: Catalan, Chinese, Czech, English, Ger-
man, Japanese,
4
Spanish. The CoNLL-2005 and
-2008 Shared Task datasets provide English SRL
annotation, and for cross dataset comparability we
consider only verbal predicates (more details in
? 4.4). To compare with prior approaches that use
semantic supervision for grammar induction, we
utilize Section 23 of the WSJ portion of the Penn
Treebank (Marcus et al, 1993).
4.2 Feature Template Sets
Our primary feature set IG
C
consists of 127 tem-
plate unigrams that emphasize coarse properties
(i.e., properties 7, 9, and 11 in Table 1). We also
explore the 31 template unigrams
5
IG
B
described
3
To reduce hash collisions, We use MurmurHash v3
https://code.google.com/p/smhasher.
4
We do not report results on Japanese as that data was
only made freely available to researchers that competed in
CoNLL 2009.
5
Because we do not include a binary factor between pred-
icate sense and semantic role, we do not include sense as a
by Bj?orkelund et al (2009). Each of IG
C
and IG
B
also include 32 template bigrams selected by in-
formation gain on 1000 sentences?we select a
different set of template bigrams for each dataset.
We compare against the language-specific fea-
ture sets detailed in the literature on high-resource
top-performing SRL systems: From Bj?orkelund et
al. (2009), these are feature sets for German, En-
glish, Spanish and Chinese, obtained by weeks of
forward selection (B
de,en,es,zh
); and from Zhao et
al. (2009), these are features for Catalan Z
ca
.
6
4.3 High-resource SRL
We first compare our models trained as a pipeline,
using all available supervision (syntax, morphol-
ogy, POS tags, lemmas) from the CoNLL-2009
data. Table 4(a) shows the results of our model
with gold syntax and a richer feature set than
that of Naradowsky et al (2012), which only
looked at whether a syntactic dependency edge
was present. This highlights an important advan-
tage of the pipeline trained model: the features can
consider any part of the syntax (e.g., arbitrary sub-
trees), whereas the joint model is limited to those
features over which it can efficiently marginalize
(e.g., short dependency paths). This holds true
even in the pipeline setting where no syntactic su-
pervision is available.
Table 4(b) contrasts our high-resource results
for the task of SRL and sense disambiguation
with the top systems in the CoNLL-2009 Shared
Task, giving further insight into the performance
of the simple information gain feature selection
technique. With supervised syntax, our sim-
ple information gain feature selection technique
(? 3.4) performs admirably. However, the orig-
inal unigram Bj?orkelund features (B
de,en,es,zh
),
which were tuned for a high-resource model, ob-
tain higher F1 than our information gain set us-
ing the same features in unigram and bigram tem-
plates (IG
B
). This suggests that further work on
feature selection may improve the results. We
find that IG
B
obtain higher F1 than the original
Bj?orkelund feature sets (B
de,en,es,zh
) in the low-
resource pipeline setting with constrained gram-
mar induction (DMV+C).
feature for argument prediction.
6
This covers all CoNLL languages but Czech, where fea-
ture sets were not made publicly available in either work. In
Czech, we disallowed template bigrams involving path-grams.
1182
(a)
(b)
(c)
SRL Approach Feature Set Dep. Parser Avg. ca cs de en es zh
Pipeline IG
C
Gold 84.98 84.97 87.65 79.14 86.54 84.22 87.35
Pipeline IG
B
Gold 84.74 85.15 86.64 79.50 85.77 84.40 86.95
Naradowsky et al (2012) Gold 72.73 69.59 74.84 66.49 78.55 68.93 77.97
Bj?orkelund et al (2009) Supervised 81.55 80.01 85.41 79.71 85.63 79.91 78.60
Zhao et al (2009) Supervised 80.85 80.32 85.19 75.99 85.44 80.46 77.72
Pipeline IG
C
Supervised 78.03 76.24 83.34 74.19 81.96 76.12 76.35
Pipeline Z
ca
Supervised *77.62 77.62 ? ? ? ? ?
Pipeline B
de,en,es,zh
Supervised *76.49 ? ? 72.17 81.15 76.65 75.99
Pipeline IG
B
Supervised 75.68 74.59 81.61 69.08 78.86 74.51 75.44
Joint IG
C
Marginalized 72.48 71.35 81.03 65.15 76.16 71.03 70.14
Joint IG
B
Marginalized 72.40 71.55 80.04 64.80 75.57 71.21 71.21
Naradowsky et al (2012) Marginalized 71.27 67.99 73.16 67.26 76.12 66.74 76.32
Pipeline IG
C
DMV+C (bc) 70.08 68.21 79.63 62.25 73.81 68.73 67.86
Pipeline Z
ca
DMV+C (bc) *69.67 69.67 ? ? ? ? ?
Pipeline IG
C
DMV (bc) 69.26 68.04 79.58 58.47 74.78 68.36 66.35
Pipeline IG
B
DMV (bc) 66.81 63.31 77.38 59.91 72.02 65.96 62.28
Pipeline IG
B
DMV+C (bc) 65.61 61.89 77.48 58.97 69.11 63.31 62.92
Pipeline B
de,en,es,zh
DMV+C (bc) *63.06 ? ? 57.75 68.32 63.70 62.45
Table 4: Test F1 for SRL and sense disambiguation on CoNLL?09 in high-resource and low-resource
settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are
ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
*Indicates partial averages for the language-specific feature sets (Z
ca
and B
de,en,es,zh
), for which we show results only on the
languages for which the sets were publicly available.
train
test
2008
heads
2005
spans
2005
spans
(oracle
tree)
X PRY?08
2
0
0
5
s
p
a
n
s
84.32 79.44
 B?11 (tdc) ? 71.5
 B?11 (td) ? 65.0
X JN?08
2
0
0
8
h
e
a
d
s
85.93 79.90
 Joint, IG
C
72.9 35.0 72.0
 Joint, IG
B
67.3 37.8 67.1
Table 5: F1 for SRL approaches (without sense
disambiguation) in matched and mismatched
train/test settings for CoNLL 2005 span and 2008
head supervision. We contrast low-resource ()
and high-resource settings (X), where latter uses a
treebank. See ? 4.4 for caveats to this comparison.
4.4 Low-Resource SRL
CoNLL-2009 Table 4(c) includes results for our
low-resource approaches and Naradowsky et al
(2012) on predicting semantic roles as well as
sense. In the low-resource setting of the CoNLL-
2009 Shared task without syntactic supervision,
our joint model (Joint) with marginalized syntax
obtains state-of-the-art results with features IG
C
described in ? 4.2. This model outperforms prior
work (Naradowsky et al, 2012) and our pipeline
model (Pipeline) with contrained (DMV+C) and
unconstrained grammar induction (DMV) trained
on brown clusters (bc).
In the low-resource setting, training and decod-
ing times for the pipeline and joint methods are
similar as computation time tends to be dominated
by feature extraction.
These results begin to answer a key research
question in this work: The joint models outper-
form the pipeline models in the low-resource set-
ting. This holds even when using the same feature
selection process. Further, the best-performing
low-resource features found in this work are those
based on coarse feature templates and selected
by information gain. Templates for these fea-
tures generalize well to the high-resource setting.
However, analysis of the induced grammars in
the pipeline setting suggests that the book is not
closed on the issue. We return to this in ? 4.5.
CoNLL-2008, -2005 To finish out comparisons
with state-of-the-art SRL, we contrast our ap-
proach with that of Boxwell et al (2011), who
evaluate on SRL in isolation (without sense disam-
biguation, as in CoNLL-2009). They report results
on Prop-CCGbank (Boxwell and White, 2008),
which uses the same training/testing splits as the
CoNLL-2005 Shared Task. Their results are there-
fore loosely
7
comparable to results on the CoNLL-
2005 dataset, which we can compare here.
There is an additional complication in com-
paring SRL approaches directly: The CoNLL-
2005 dataset defines arguments as spans instead of
7
The comparison is imperfect for two reasons: first, the
CCGBank contains only 99.44% of the original PTB sen-
tences (Hockenmaier and Steedman, 2007); second, because
PropBank was annotated over CFGs, after converting to CCG
only 99.977% of the argument spans were exact matches
(Boxwell and White, 2008). However, this comparison was
adopted by Boxwell et al (2011), so we use it here.
1183
heads, which runs counter to our head-based syn-
tactic representation. This creates a mismatched
train/test scenario: we must train our model to pre-
dict argument heads, but then test on our models
ability to predict argument spans.
8
We therefore
train our models on the CoNLL-2008 argument
heads,
9
and post-process and convert from heads
to spans using the conversion algorithm available
from Johansson and Nugues (2008).
10
The heads
are either from an MBR tree or an oracle tree. This
gives Boxwell et al (2011) the advantage, since
our syntactic dependency parses are optimized to
pick out semantic argument heads, not spans.
Table 5 presents our results. Boxwell et al
(2011) (B?11) uses additional supervision in the
form of a CCG tag dictionary derived from su-
pervised data with (tdc) and without (tc) a cut-
off. Our model does very poorly on the ?05 span-
based evaluation because the constituent bracket-
ing of the marginalized trees are inaccurate. This
is elucidated by instead evaluating on the ora-
cle spans, where our F1 scores are higher than
Boxwell et al (2011). We also contrast with rela-
vant high-resource methods with span/head con-
versions from Johansson and Nugues (2008): Pun-
yakanok et al (2008) (PRY?08) and Johansson and
Nugues (2008) (JN?08).
Subtractive Study In our subsequent experi-
ments, we study the effectiveness of our models
as the available supervision is decreased. We in-
crementally remove dependency syntax, morpho-
logical features, POS tags, then lemmas. For these
experiments, we utilize the coarse-grained feature
set (IG
C
), which includes Brown clusters.
Across languages, we find the largest drop in
F1 when we remove POS tags; and we find a
gain in F1 when we remove lemmas. This indi-
cates that lemmas, which are a high-resource an-
notation, may not provide a significant benefit for
this task. The effect of removing morphological
features is different across languages, with little
change in performance for Catalan and Spanish,
8
We were unable to obtain the system output of Boxwell
et al (2011) in order to convert their spans to dependencies
and evaluate the other mismatched train/test setting.
9
CoNLL-2005, -2008, and -2009 were derived from Prop-
Bank and share the same source text; -2008 and -2009 use
argument heads.
10
Specifically, we use their Algorithm 2, which produces
the span dominated by each argument, with special handling
of the case when the argument head dominates that of the
predicate. Also following Johansson and Nugues (2008), we
recover the ?05 sentences missing from the ?08 evaluation set.
Rem #FT ca de es
? 127+32 74.46 72.62 74.23
Dep 40+32 67.43 64.24 67.18
Mor 30+32 67.84 59.78 66.94
POS 23+32 64.40 54.68 62.71
Lem 21+32 64.85 54.89 63.80
Table 6: Subtractive experiments. Each row con-
tains the F1 for SRL only (without sense disam-
biguation) where the supervision type of that row
and all above it have been removed. Removed su-
pervision types (Rem) are: syntactic dependencies
(Dep), morphology (Mor), POS tags (POS), and
lemmas (Lem). #FT indicates the number of fea-
ture templates used (unigrams+bigrams).
20
30
40
50
60
70
0 20000 40000 60000
Number of Training Sentences
Lab
eled
 F1
Language / Dependency Parser
Catalan / Marginalized
Catalan / DMV+C
German / Marginalized
German / DMV+C
Figure 3: Learning curve for semantic dependency
supervision in Catalan and German. F1 of SRL
only (without sense disambiguation) shown as the
number of training sentences is increased.
but a drop in performance for German. This may
reflect a difference between the languages, or may
reflect the difference between the annotation of the
languages: both the Catalan and Spanish data orig-
inated from the Ancora project,
11
while the Ger-
man data came from another source.
Figure 3 contains the learning curve for SRL su-
pervision in our lowest resource setting for two
example languages, Catalan and German. This
shows how F1 of SRL changes as we adjust
the number of training examples. We find that
the joint training approach to grammar induction
yields consistently higher SRL performance than
its distantly supervised counterpart.
4.5 Analysis of Grammar Induction
Table 7 shows grammar induction accuracy in
low-resource settings. We find that the gap be-
tween the supervised parser and the unsupervised
methods is quite large, despite the reasonable ac-
curacy both methods achieve for the SRL end task.
11
http://clic.ub.edu/corpus/ancora
1184
Dependency
Parser
Avg. ca cs de en es zh
Supervised* 87.1 89.4 85.3 89.6 88.4 89.2 80.7
DMV (pos) 30.2 45.3 22.7 20.9 32.9 41.9 17.2
DMV (bc) 22.1 18.8 32.8 19.6 22.4 20.5 18.6
DMV+C (pos) 37.5 50.2 34.9 21.5 36.9 49.8 32.0
DMV+C (bc) 40.2 46.3 37.5 28.7 40.6 50.4 37.5
Marginal, IG
C
43.8 50.3 45.8 27.2 44.2 46.3 48.5
Marginal, IG
B
50.2 52.4 43.4 41.3 52.6 55.2 56.2
Table 7: Unlabeled directed dependency accuracy
on CoNLL?09 test set in low-resource settings.
DMV models are trained on either POS tags (pos)
or Brown clusters (bc). *Indicates the supervised parser
outputs provided by the CoNLL?09 Shared Task.
WSJ
?
Distant
Supervision
SAJM?10 44.8 none
SAJ?13 64.4 none
SJA?10 50.4 HTML
NB?11 59.4 ACE05
DMV (bc) 24.8 none
DMV+C (bc) 44.8 SRL
Marginalized, IG
C
48.8 SRL
Marginalized, IG
B
58.9 SRL
Table 8: Comparison of grammar induction ap-
proaches. We contrast the DMV trained with
Viterbi EM+uniform initialization (DMV), our
constrained DMV (DMV+C), and our model?s
MBR decoding of latent syntax (Marginalized)
with other recent work: Spitkovsky et al (2010a)
(SAJM?10), Spitkovsky et al (2010b) (SJA?10),
Naseem and Barzilay (2011) (NB?11), and the CS
model of Spitkovsky et al (2013) (SAJ?13).
This suggests that refining the low-resource gram-
mar induction methods may lead to gains in SRL.
Interestingly, the marginalized grammars best
the DMV grammar induction method; however,
this difference is less pronounced when the DMV
is constrained using SRL labels as distant super-
vision. This could indicate that a better model for
grammar induction would result in better perfor-
mance for SRL. We therefore turn to an analysis of
other approaches to grammar induction in Table 8,
evaluated on the Penn Treebank. We contrast with
methods using distant supervision (Naseem and
Barzilay, 2011; Spitkovsky et al, 2010b) and fully
unsupervised dependency parsing (Spitkovsky et
al., 2013). Following prior work, we exclude
punctuation from evaluation and convert the con-
stituency trees to dependencies.
12
The approach from Spitkovsky et al (2013)
12
Naseem and Barzilay (2011) and our results use the
Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et
al. (2010b; 2013) use Collins (1999) head percolation rules.
(SAJ?13) outperforms all other approaches, in-
cluding our marginalized settings. We therefore
may be able to achieve further gains in the pipeline
model by considering better models of latent syn-
tax, or better search techniques that break out
of local optima. Similarly, improving the non-
convex optimization of our latent-variable CRF
(Marginalized) may offer further gains.
5 Discussion and Future Work
We have compared various approaches for low-
resource semantic role labeling at the state-of-the-
art level. We find that we can outperform prior
work in the low-resource setting by coupling the
selection of feature templates based on informa-
tion gain with a joint model that marginalizes over
latent syntax.
We utilize unlabeled data in both generative and
discriminative models for dependency syntax and
in generative word clustering. Our discriminative
joint models treat latent syntax as a structured-
feature to be optimized for the end-task of SRL,
while our other grammar induction techniques op-
timize for unlabeled data likelihood?optionally
with distant supervision. We observe that careful
use of these unlabeled data resources can improve
performance on the end task.
Our subtractive experiments suggest that lemma
annotations, a high-resource annotation, may not
provide a large benefit for SRL. Our grammar in-
duction analysis indicates that relatively low accu-
racy can still result in reasonable SRL predictions;
still, the models do not outperform those that use
supervised syntax, and we aim to explore how well
the pipeline models in particular improve when we
apply higher accuracy unsupervised grammar in-
duction techniques.
We have utilized well studied datasets in order
to best understand the quality of our models rela-
tive to prior work. In future work, we hope to ex-
plore the effectiveness of our approaches on truly
low resource settings by using crowdsourcing to
develop semantic role datasets in other languages
and domains.
Acknowledgments We thank Richard Johans-
son, Dennis Mehay, and Stephen Boxwell for help
with data. We also thank Jason Naradowsky, Jason
Eisner, and anonymous reviewers for comments
on the paper.
1185
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling.
Prentice-Hall, Inc.
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of the 17th
Conference on Computational Natural Language
Learning (CoNLL 2013). Association for Computa-
tional Linguistics.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task. Association for Computational Linguistics.
Stephen Boxwell and Michael White. 2008. Project-
ing propbank roles onto the CCGbank. In Proceed-
ings of the International Conference on Language
Resources and Evaluation (LREC 2008). European
Language Resources Association.
Stephen Boxwell, Chris Brew, Jason Baldridge, Dennis
Mehay, and Sujith Ravi. 2011. Semantic role label-
ing without treebanks? In Proceedings of the 5th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP). Asian Federation of Natural
Language Processing.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4).
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. The
Journal of Machine Learning Research, 11.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL 2009): Shared Task. Association for
Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008). Association for Computational Lin-
guistics.
Dan Klein and Christopher Manning. 2004. Corpus-
Based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL 2004). Association for Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT. Association for
Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Ma-
chine Learning (ICML 2001). Morgan Kaufmann.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics (TACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The Penn Treebank. Com-
putational linguistics, 19(2).
Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011. Structured sparsity in struc-
tured prediction. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011). Association for Compu-
tational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005).
Association for Computational Linguistics.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Susan Dumais, Daniel
Marcu, and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings. Association for Compu-
tational Linguistics.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization of
hidden syntactic structure. In Proceedings of the
2012 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2012). Association
for Computational Linguistics.
Tahira Naseem and Regina Barzilay. 2011. Using
semantic cues to learn syntax. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI 2011). AAAI Press.
1186
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting of
the Association for Computational Linguistics (ACL
1992).
Nugues Pierre and Kalep Heiki-Jaan. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. NODALIDA 2007 Proceedings.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Juraf-
sky, and Christopher D Manning. 2010a. Viterbi
training improves unsupervised dependency parsing.
In Proceedings of the 14th Conference on Computa-
tional Natural Language Learning (CoNLL 2010).
Association for Computational Linguistics.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010b. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2010). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011). Association for Computational Lin-
guistics.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2013). Association for
Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics (ACL 2005). Association for Computational Lin-
guistics.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In L?eon
Bottou and Michael Littman, editors, Proceedings
of the 26th Annual International Conference on Ma-
chine Learning (ICML 2009). Omnipress.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n
3
. Information and
Control, 10(2).
Hai Zhao, Wenliang Chen, Chunyu Kity, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task. Association for Com-
putational Linguistics.
1187
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 545?550,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Improving Lexical Embeddings with Semantic Knowledge
Mo Yu
?
Machine Translation Lab
Harbin Institute of Technology
Harbin, China
gflfof@gmail.com
Mark Dredze
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
mdredze@cs.jhu.edu
Abstract
Word embeddings learned on unlabeled
data are a popular tool in semantics, but
may not capture the desired semantics. We
propose a new learning objective that in-
corporates both a neural language model
objective (Mikolov et al, 2013) and prior
knowledge from semantic resources to
learn improved lexical semantic embed-
dings. We demonstrate that our embed-
dings improve over those learned solely on
raw text in three settings: language mod-
eling, measuring semantic similarity, and
predicting human judgements.
1 Introduction
Word embeddings are popular representations for
syntax (Turian et al, 2010; Collobert and We-
ston, 2008; Mnih and Hinton, 2007), semantics
(Huang et al, 2012; Socher et al, 2013), morphol-
ogy (Luong et al, 2013) and other areas. A long
line of embeddings work, such as LSA and ran-
domized embeddings (Ravichandran et al, 2005;
Van Durme and Lall, 2010), has recently turned
to neural language models (Bengio et al, 2006;
Collobert and Weston, 2008; Turian et al, 2010).
Unsupervised learning can take advantage of large
corpora, which can produce impressive results.
However, the main drawback of unsupervised
learning is that the learned embeddings may not
be suited for the task of interest. Consider se-
mantic embeddings, which may capture a notion
of semantics that improves one semantic task but
harms another. Controlling this behavior is chal-
lenging with an unsupervised objective. However,
rich prior knowledge exists for many tasks, and
there are numerous such semantic resources.
We propose a new training objective for learn-
ing word embeddings that incorporates prior
?
This work was done while the author was visiting JHU.
knowledge. Our model builds on word2vec
(Mikolov et al, 2013), a neural network based
language model that learns word embeddings by
maximizing the probability of raw text. We extend
the objective to include prior knowledge about
synonyms from semantic resources; we consider
both the Paraphrase Database (Ganitkevitch et al,
2013) and WordNet (Fellbaum, 1999), which an-
notate semantic relatedness between words. The
latter was also used in (Bordes et al, 2012) for
training a network for predicting synset relation.
The combined objective maximizes both the prob-
ability of the raw corpus and encourages embed-
dings to capture semantic relations from the re-
sources. We demonstrate improvements in our
embeddings on three tasks: language modeling,
measuring word similarity, and predicting human
judgements on word pairs.
2 Learning Embeddings
We present a general model for learning word em-
beddings that incorporates prior knowledge avail-
able for a domain. While in this work we con-
sider semantics, our model could incorporate prior
knowledge from many types of resources. We be-
gin by reviewing the word2vec objective and then
present augmentations of the objective for prior
knowledge, including different training strategies.
2.1 Word2vec
Word2vec (Mikolov et al, 2013) is an algorithm
for learning embeddings using a neural language
model. Embeddings are represented by a set of
latent (hidden) variables, and each word is rep-
resented by a specific instantiation of these vari-
ables. Training learns these representations for
each word w
t
(the tth word in a corpus of size T )
so as to maximize the log likelihood of each token
given its context: words within a window sized c:
max
1
T
T
?
t=1
log p
(
w
t
|w
t+c
t?c
)
, (1)
545
where w
t+c
t?c
is the set of words in the window of
size c centered at w
t
(w
t
excluded).
Word2vec offers two choices for modeling of
Eq. (1): a skip-gram model and a continuous bag-
of-words model (cbow). The latter worked better
in our experiments so we focus on it in our presen-
tation. cbow defines p(w
t
|w
t+c
t?c
) as:
exp
(
e
?
w
t
>
?
?
?c?j?c,j 6=0
e
w
t+j
)
?
w
exp
(
e
?
w
>
?
?
?c?j?c,j 6=0
e
w
t+j
)
, (2)
where e
w
and e
?
w
represent the input and output
embeddings respectively, i.e., the assignments to
the latent variables for word w. While some learn
a single representation for each word (e
?
w
, e
w
),
our results improved when we used a separate em-
bedding for input and output in cbow.
2.2 Relation Constrained Model
Suppose we have a resource that indicates rela-
tions between words. In the case of semantics,
we could have a resource that encodes semantic
similarity between words. Based on this resource,
we learn embeddings that predict one word from
another related word. We defineR as a set of rela-
tions between two words w and w
?
. R can contain
typed relations (e.g., w is related to w
?
through
a specific type of semantic relation), and rela-
tions can have associated scores indicating their
strength. We assume a single relation type of uni-
form strength, though it is straightforward to in-
clude additional characteristics into the objective.
Define R
w
to be the subset of relations in R
which involve word w. Our objective maximizes
the (log) probability of all relations by summing
over all words N in the vocabulary:
1
N
N
?
i=1
?
w?R
w
i
log p (w|w
i
) , (3)
p(w|w
i
) = exp
(
e
?
w
T
e
w
i
)
/
?
w?
exp
(
e
?
w?
T
e
w
i
)
takes a form similar to Eq. (2) but without the
context: e and e
?
are again the input and output
embeddings. For our semantic relations e
?
w
and
e
w
are symmetrical, so we use a single embedding.
Embeddings are learned such that they are predic-
tive of related words in the resource. We call this
the Relation Constrained Model (RCM).
2.3 Joint Model
The cbow and RCM objectives use separate data
for learning. While RCM learns embeddings
suited to specific tasks based on knowledge re-
sources, cbow learns embeddings for words not in-
cluded in the resource but appear in a corpus. We
form a joint model through a linear combination
of the two (weighted by C):
1
T
T
?
t=1
log p
(
w
t
|w
t+c
t?c
)
+
C
N
N
?
i=1
?
w?R
w
i
log p (w|w
i
)
Based on our initial experiments, RCM uses the
output embeddings of cbow.
We learn embeddings using stochastic gradient
ascent. Updates for the first term for e
?
and e are:
e
?
w
? ?
cbow
(
?(f(w))? I
[w=w
t
]
)
?
t+c
?
j=t?c
e
w
j
e
w
j
? ?
cbow
?
w
(
?(f(w))? I
[w=w
t
]
)
? e
?
w
,
where ?(x) = exp{x}/(1 + exp{x}), I
[x]
is 1
when x is true, f(w) = e
?
w
>
?
t+c
j=t?c
e
w
j
. Second
term updates are:
e
?
w
? ?
RCM
(
?(f
?
(w))? I
[w?R
w
i
]
)
? e
?
w
i
e
?
w
i
? ?
RCM
?
w
(
?(f
?
(w))? I
[w?R
w
i
]
)
? e
?
w
,
where f
?
(w) = e
?
w
>
e
?
w
i
. We use two learning
rates: ?
cbow
and ?
RCM
.
2.4 Parameter Estimation
All three models (cbow, RCM and joint) use the
same training scheme based on Mikolov et al
(2013). There are several choices to make in pa-
rameter estimation; we present the best perform-
ing choices used in our results.
We use noise contrastive estimation (NCE)
(Mnih and Teh, 2012), which approximately max-
imizes the log probability of the softmax objec-
tive (Eq. 2). For each objective (cbow or RCM),
we sample 15 words as negative samples for each
training instance according to their frequencies in
raw texts (i.e. training data of cbow). Suppose w
has frequency u(w), then the probability of sam-
pling w is p(w) ? u(w)
3/4
.
We use distributed training, where shared em-
beddings are updated by each thread based on
training data within the thread, i.e., asynchronous
stochastic gradient ascent. For the joint model,
we assign threads to the cbow or RCM objective
with a balance of 12:1(i.e. C is approximately
1
12
).
We allow the cbow threads to control convergence;
training stops when these threads finish process-
ing the data. We found this an effective method
546
for balancing the two objectives. We trained each
cbow objective using a single pass over the data set
(except for those in Section 4.1), which we empir-
ically verified was sufficient to ensure stable per-
formances on semantic tasks.
Model pre-training is critical in deep learning
(Bengio et al, 2007; Erhan et al, 2010). We eval-
uate two strategies: random initialization, and pre-
training the embeddings. For pre-training, we first
learn using cbow with a random initialization. The
resulting trained model is then used to initialize
the RCM model. This enables the RCM model to
benefit from the unlabeled data, but refine the em-
beddings constrained by the given relations.
Finally, we consider a final model for training
embeddings that uses a specific training regime.
While the joint model balances between fitting the
text and learning relations, modeling the text at
the expense of the relations may negatively impact
the final embeddings for tasks that use the embed-
dings outside of the context of word2vec. There-
fore, we use the embeddings from a trained joint
model to pre-train an RCM model. We call this
setting Joint?RCM.
3 Evaluation
For training cbow we use the New York Times
(NYT) 1994-97 subset from Gigaword v5.0
(Parker et al, 2011). We select 1,000 paragraphs
each for dev and test data from the December 2010
portion of the NYT. Sentences are tokenized using
OpenNLP
1
, yielding 518,103,942 tokens for train-
ing, 42,953 tokens for dev and 41,344 for test.
We consider two resources for training the
RCM term: the Paraphrase Database (PPDB)
(Ganitkevitch et al, 2013) and WordNet (Fell-
baum, 1999). For each semantic pair extracted
from these resources, we add a relation to the
RCM objective. Since we use both resources for
evaluation, we divide each into train, dev and test.
PPDB is an automatically extracted dataset con-
taining tens of millions of paraphrase pairs, in-
cluding words and phrases. We used the ?lexi-
cal? version of PPDB (no phrases) and filtered to
include pairs that contained words found in the
200,000 most frequent words in the NYT corpus,
which ensures each word in the relations had sup-
port in the text corpus. Next, we removed dupli-
cate pairs: if <A,B> occurred in PPDB, we re-
moved relations of <B,A>. PPDB is organized
1
https://opennlp.apache.org/
PPDB Relations WordNet Relations
Train XL 115,041 Train 68,372
XXL 587,439 (not used in
XXXL 2,647,105 this work)
Dev 1,582 Dev 1,500
Test 1,583 Test 1,500
Table 1: Sizes of semantic resources datasets.
into 6 parts, ranging from S (small) to XXXL.
Division into these sets is based on an automat-
ically derived accuracy metric. Since S contains
the most accurate paraphrases, we used these for
evaluation. We divided S into a dev set (1582
pairs) and test set (1583 pairs). Training was based
on one of the other sets minus relations from S.
We created similar splits using WordNet, ex-
tracting synonyms using the 100,000 most fre-
quent NYT words. We divide the vocabulary into
three sets: the most frequent 10,000 words, words
with ranks between 10,001-30,000 and 30,001-
100,000. We sample 500 words from each set to
construct a dev and test set. For each word we
sample one synonym to form a pair. The remain-
ing words and their synonyms are used for train-
ing. However we did not use the training data be-
cause it is too small to affect the results. Table 1
summarizes the datasets.
4 Experiments
The goal of our experiments is to demonstrate the
value of learning semantic embeddings with infor-
mation from semantic resources. In each setting,
we will compare the word2vec baseline embed-
ding trained with cbow against RCM alone, the
joint model and Joint?RCM. We consider three
evaluation tasks: language modeling, measuring
semantic similarity, and predicting human judge-
ments on semantic relatedness. In all of our ex-
periments, we conducted model development and
tuned model parameters (C, ?
cbow
, ?
RCM
, PPDB
dataset, etc.) on development data, and evaluate
the best performing model on test data. The mod-
els are notated as follows: word2vec for the base-
line objective (cbow or skip-gram), RCM-r/p and
Joint-r/p for random and pre-trained initializations
of the RCM and Joint objectives, and Joint?RCM
for pre-training RCM with Joint embeddings. Un-
less otherwise notes, we train using PPDB XXL.
We initially created WordNet training data, but
found it too small to affect results. Therefore,
we include only RCM results trained on PPDB,
but show evaluations on both PPDB and WordNet.
547
Model NCE HS
word2vec (cbow) 8.75 6.90
RCM-p 8.55 7.07
Joint-r (?
RCM
= 1? 10
?2
) 8.33 6.87
Joint-r (?
RCM
= 1? 10
?3
) 8.20 6.75
Joint?RCM 8.40 6.92
Table 2: LM evaluation on held out NYT data.
We trained 200-dimensional embeddings and used
output embeddings for measuring similarity. Dur-
ing the training of cbow objectives we remove all
words with frequencies less than 5, which is the
default setting of word2vec.
4.1 Language Modeling
Word2vec is fundamentally a language model,
which allows us to compute standard evaluation
metrics on a held out dataset. After obtaining
trained embeddings from any of our objectives,
we use the embeddings in the word2vec model
to measure perplexity of the test set. Measuring
perplexity means computing the exact probability
of each word, which requires summation over all
words in the vocabulary in the denominator of the
softmax. Therefore, we also trained the language
models with hierarchical classification (Mikolov
et al, 2013) strategy (HS). The averaged perplexi-
ties are reported on the NYT test set.
While word2vec and joint are trained as lan-
guage models, RCM is not. In fact, RCM does not
even observe all the words that appear in the train-
ing set, so it makes little sense to use the RCM em-
beddings directly for language modeling. There-
fore, in order to make fair comparison, for every
set of trained embeddings, we fix them as input
embedding for word2vec, then learn the remain-
ing input embeddings (words not in the relations)
and all the output embeddings using cbow. Since
this involves running cbow on NYT data for 2 it-
erations (one iteration for word2vec-training/pre-
training/joint-modeling and the other for tuning
the language model), we use Joint-r (random ini-
tialization) for a fair comparison.
Table 2 shows the results for language mod-
eling on test data. All of our proposed models
improve over the baseline in terms of perplexity
when NCE is used for training LMs. When HS is
used, the perplexities are greatly improved. How-
ever in this situation only the joint models improve
the results; and Joint?RCM performs similar to
the baseline, although it is not designed for lan-
guage modeling. We include the optimal ?
RCM
in the table; while set ?
cbow
= 0.025 (the default
setting of word2vec). Even when our goal is to
strictly model the raw text corpus, we obtain im-
provements by injecting semantic information into
the objective. RCM can effectively shift learning
to obtain more informative embeddings.
4.2 Measuring Semantic Similarity
Our next task is to find semantically related words
using the embeddings, evaluating on relations
from PPDB and WordNet. For each of the word
pairs in the evaluation set <A,B>, we use the co-
sine distance between the embeddings to score A
with a candidate word B
?
. We use a large sample
of candidate words (10k, 30k or 100k) and rank all
candidate words for pairs where B appears in the
candidates. We then measure the rank of the cor-
rect B to compute mean reciprocal rank (MRR).
Our goal is to use word A to select word B as
the closest matching word from the large set of
candidates. Using this strategy, we evaluate the
embeddings from all of our objectives and mea-
sure which embedding most accurately selected
the true correct word.
Table 3 shows MRR results for both PPDB
and WordNet dev and test datasets for all models.
All of our methods improve over the baselines in
nearly every test set result. In nearly every case,
Joint?RCM obtained the largest improvements.
Clearly, our embeddings are much more effective
at capturing semantic similarity.
4.3 Human Judgements
Our final evaluation is to predict human judge-
ments of semantic relatedness. We have pairs of
words from PPDB scored by annotators on a scale
of 1 to 5 for quality of similarity. Our data are
the judgements used by Ganitkevitch et al (2013),
which we filtered to include only those pairs for
which we learned embeddings, yielding 868 pairs.
We assign a score using the dot product between
the output embeddings of each word in the pair,
then order all 868 pairs according to this score.
Using the human judgements, we compute the
swapped pairs rate: the ratio between the number
of swapped pairs and the number of all pairs. For
pair p scored y
p
by the embeddings and judged y?
p
by an annotator, the swapped pair rate is:
?
p
1
,p
2
?D
I[(y
p
1
? y
p
2
) (y?
p
2
? y?
p
1
) < 0]
?
p
1
,p
2
?D
I[y
p
1
6= y
p
2
]
(4)
where I[x] is 1 when x is true.
548
PPDB WordNet
Model
Dev Test Dev Test
10k 30k 100k 10k 30k 100k 10k 30k 100k 10k 30k 100k
word2vec (cbow) 49.68 39.26 29.15 49.31 42.53 30.28 10.24 8.64 5.14 10.04 7.90 4.97
word2vec (skip-gram) 48.70 37.14 26.20 - - - 8.61 8.10 4.62 - - -
RCM-r 55.03 42.52 26.05 - - - 13.33 9.05 5.29 - - -
RCM-p 61.79 53.83 40.95 65.42 55.82 41.20 15.25 12.13 7.46 14.13 11.23 7.39
Joint-r 59.91 50.87 36.81 - - - 15.73 11.36 7.14 13.97 10.51 7.44
Joint-p 59.75 50.93 37.73 64.30 53.27 38.97 15.61 11.20 6.96 - - -
Joint?RCM 64.22 54.99 41.34 68.20 57.87 42.64 16.81 11.67 7.55 16.16 11.21 7.56
Table 3: MRR for semantic similarity on PPDB and WordNet dev and test data. Higher is better. All
RCM objectives are trained with PPDB XXL. To preserve test data integrity, only the best performing
setting of each model is evaluated on the test data.
Model Swapped Pairs Rate
word2vec (cbow) 17.81
RCM-p 16.66
Joint-r 16.85
Joint-p 16.96
Joint?RCM 16.62
Table 4: Results for ranking the quality of PPDB
pairs as compared to human judgements.
PPDB Dev
Model Relations 10k 30k 100k
RCM-r XL 24.02 15.26 9.55
RCM-p XL 54.97 45.35 32.95
RCM-r XXL 55.03 42.52 26.05
RCM-p XXL 61.79 53.83 40.95
RCM-r XXXL 51.00 44.61 28.42
RCM-p XXXL 53.01 46.35 34.19
Table 5: MRR on PPDB dev data for training on
an increasing number of relations.
Table 4 shows that all of our models obtain
reductions in error as compared to the baseline
(cbow), with Joint?RCM obtaining the largest re-
duction. This suggests that our embeddings are
better suited for semantic tasks, in this case judged
by human annotations.
PPDB Dev
Model ?
RCM
10k 30k 100k
Joint-p 1? 10
?1
47.17 36.74 24.50
5? 10
?2
54.31 44.52 33.07
1? 10
?2
59.75 50.93 37.73
1? 10
?3
57.00 46.84 34.45
Table 6: Effect of learning rate ?
RCM
on MRR for
the RCM objective in Joint models.
4.4 Analysis
We conclude our experiments with an analysis of
modeling choices. First, pre-training RCM models
gives significant improvements in both measuring
semantic similarity and capturing human judge-
ments (compare ?p? vs. ?r? results.) Second, the
number of relations used for RCM training is an
important factor. Table 5 shows the effect on dev
data of using various numbers of relations. While
we see improvements from XL to XXL (5 times as
many relations), we get worse results on XXXL,
likely because this set contains the lowest quality
relations in PPDB. Finally, Table 6 shows different
learning rates ?
RCM
for the RCM objective.
The baseline word2vec and the joint model have
nearly the same averaged running times (2,577s
and 2,644s respectively), since they have same
number of threads for the CBOW objective and the
joint model uses additional threads for the RCM
objective. The RCM models are trained with sin-
gle thread for 100 epochs. When trained on the
PPDB-XXL data, it spends 2,931s on average.
5 Conclusion
We have presented a new learning objective for
neural language models that incorporates prior
knowledge contained in resources to improve
learned word embeddings. We demonstrated that
the Relation Constrained Model can lead to better
semantic embeddings by incorporating resources
like PPDB, leading to better language modeling,
semantic similarity metrics, and predicting hu-
man semantic judgements. Our implementation is
based on the word2vec package and we made it
available for general use
2
.
We believe that our techniques have implica-
tions beyond those considered in this work. We
plan to explore the embeddings suitability for
other semantics tasks, including the use of re-
sources with both typed and scored relations. Ad-
ditionally, we see opportunities for jointly learn-
ing embeddings across many tasks with many re-
sources, and plan to extend our model accordingly.
Acknowledgements Yu is supported by China
Scholarship Council and by NSFC 61173073.
2
https://github.com/Gorov/JointRCM
549
References
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo
Larochelle, et al 2007. Greedy layer-wise training
of deep networks. In Neural Information Processing
Systems (NIPS).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text semantic
parsing. In International Conference on Artificial
Intelligence and Statistics, pages 127?135.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Interna-
tional Conference on Machine Learning (ICML).
Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research (JMLR), 11:625?660.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Association for Computational Lin-
guistics (ACL), pages 873?882.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Conference on Natural Language Learning
(CoNLL).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. arXiv preprint arXiv:1310.4546.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In International Conference on Machine Learning
(ICML).
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. arXiv preprint arXiv:1206.6426.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword fifth edi-
tion. Technical report, Linguistic Data Consortium.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: us-
ing locality sensitive hash function for high speed
noun clustering. In Association for Computational
Linguistics (ACL).
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1631?1642.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Association for
Computational Linguistics (ACL).
Benjamin Van Durme and Ashwin Lall. 2010. On-
line generation of locality sensitive hash signatures.
In Association for Computational Linguistics (ACL),
pages 231?235.
550
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674?679,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Polylingual Topic Models from
Code-Switched Social Media Documents
Nanyun Peng Yiming Wang Mark Dredze
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD USA
{npeng1,freewym,mdredze}@jhu.edu
Abstract
Code-switched documents are common
in social media, providing evidence for
polylingual topic models to infer aligned
topics across languages. We present
Code-Switched LDA (csLDA), which in-
fers language specific topic distributions
based on code-switched documents to fa-
cilitate multi-lingual corpus analysis. We
experiment on two code-switching cor-
pora (English-Spanish Twitter data and
English-Chinese Weibo data) and show
that csLDA improves perplexity over
LDA, and learns semantically coherent
aligned topics as judged by human anno-
tators.
1 Introduction
Topic models (Blei et al, 2003) have become stan-
dard tools for analyzing document collections, and
topic analyses are quite common for social media
(Paul and Dredze, 2011; Zhao et al, 2011; Hong
and Davison, 2010; Ramage et al, 2010; Eisen-
stein et al, 2010). Their popularity owes in part to
their data driven nature, allowing them to adapt to
new corpora and languages. In social media espe-
cially, there is a large diversity in terms of both the
topic and language, necessitating the modeling of
multiple languages simultaneously. A good candi-
date for multi-lingual topic analyses are polylin-
gual topic models (Mimno et al, 2009), which
learn topics for multiple languages, creating tuples
of language specific distributions over monolin-
gual vocabularies for each topic. Polylingual topic
models enable cross language analysis by group-
ing documents by topic regardless of language.
Training of polylingual topic models requires
parallel or comparable corpora: document tuples
from multiple languages that discuss the same
topic. While additional non-aligned documents
User 1: ?Don Samuel es un crack! #VamosM?exico #DaleTri
RT @User4: Arriba! Viva Mexico! Advanced to GOLD.
medal match in ?Football?!
User 2: @user1 rodo que tal el nuevo Mountain ?
User 3: @User1 @User4 wow this is something !! Ja ja ja
Football well said
Figure 1: Three users discuss Mexico?s football
team advancing to the Gold medal game in the
2012 Olympics in code-switched Spanish and En-
glish.
can be folded in during training, the ?glue? doc-
uments are required to aid in the alignment across
languages. However, the ever changing vocabu-
lary and topics of social media (Eisenstein, 2013)
make finding suitable comparable corpora diffi-
cult. Standard techniques ? such as relying on ma-
chine translation parallel corpora or comparable
documents extracted from Wikipedia in different
languages ? fail to capture the specific terminol-
ogy of social media. Alternate methods that rely
on bilingual lexicons (Jagarlamudi and Daum?e,
2010) similarly fail to adapt to shifting vocabular-
ies. The result: an inability to train polylingual
models on social media.
In this paper, we offer a solution: utilize code-
switched social media to discover correlations
across languages. Social media is filled with ex-
amples of code-switching, where users switch be-
tween two or more languages, both in a conversa-
tion and even a single message (Ling et al, 2013).
This mixture of languages in the same context sug-
gests alignments between words across languages
through the common topics discussed in the con-
text.
We learn from code-switched social media by
extending the polylingual topic model framework
to infer the language of each token and then auto-
matically processing the learned topics to identify
aligned topics. Our model improves both in terms
of perplexity and a human evaluation, and we pro-
vide some example analyses of social media that
rely on our learned topics.
674
2 Code-Switching
Code-switched documents has received consider-
able attention in the NLP community. Several
tasks have focused on identification and analysis,
including mining translations in code-switched
documents (Ling et al, 2013), predicting code-
switched points (Solorio and Liu, 2008a), identi-
fying code-switched tokens (Lignos and Marcus,
2013; Yu et al, 2012; Elfardy and Diab, 2012),
adding code-switched support to language mod-
els (Li and Fung, 2012), linguistic processing of
code switched data (Solorio and Liu, 2008b), cor-
pus creation (Li et al, 2012; Diab and Kamboj,
2011), and computational linguistic analyses and
theories of code-switching (Sankofl, 1998; Joshi,
1982).
Code-switching specifically in social media has
also received some recent attention. Lignos and
Marcus (2013) trained a supervised token level
language identification system for Spanish and
English code-switched social media to study code-
switching behaviors. Ling et al (2013) mined
translation spans for Chinese and English in code-
switched documents to improve a translation sys-
tem, relying on an existing translation model to aid
in the identification and extraction task. In contrast
to this work, we take an unsupervised approach,
relying only on readily available document level
language ID systems to utilize code-switched data.
Additionally, our focus is not on individual mes-
sages, rather we aim to train a model that can be
used to analyze entire corpora.
In this work we consider two types of code-
switched documents: single messages and conver-
sations, and two language pairs: Chinese-English
and Spanish-English. Figure 1 shows an exam-
ple of a code-switched Spanish-English conversa-
tion, in which three users discuss Mexico?s foot-
ball team advancing to the Gold medal game in
the 2012 Summer Olympics. In this conversation,
some tweets are code-switched and some are in a
single language. By collecting the entire conver-
sation into a single document we provide the topic
model with additional content. An example of a
Chinese-English code-switched messages is given
by Ling et al (2013):
watup Kenny Mayne!! - Kenny Mayne
??????!!
Here a user switches between languages in a single
message. We empirically evaluate our model on
both conversations and messages. In the model
presentation we will refer to both as ?documents.?
3 csLDA
To train a polylingual topic model on social me-
dia, we make two modifications to the model of
Mimno et al (2009): add a token specific language
variable, and a process for identifying aligned top-
ics.
First, polylingual topic models require paral-
lel or comparable corpora in which each docu-
ment has an assigned language. In the case of
code-switched social media data, we require a per-
token language variable. However, while docu-
ment level language identification (LID) systems
are common place, very few languages have per-
token LID systems (King and Abney, 2013; Lig-
nos and Marcus, 2013).
To address the lack of available LID systems,
we add a per-token latent language variable to the
polylingual topic model. For documents that are
not code-switched, we observe these variables to
be the output of a document level LID system. In
the case of code-switched documents, these vari-
ables are inferred during model inference.
Second, polylingual topic models assume the
aligned topics are from parallel or comparable cor-
pora, which implicitly assumes that a topics pop-
ularity is balanced across languages. Topics that
show up in one language necessarily show up in
another. However, in the case of social media,
we can make no such assumption. The topics
discussed are influenced by users, time, and lo-
cation, all factors intertwined with choice of lan-
guage. For example, English speakers will more
likely discuss Olympic basketball while Spanish
speakers football. There may be little or no docu-
ments on a given topic in one language, while they
are plentiful in another. In this case, a polylin-
gual topic model, which necessarily infers a topic-
specific word distribution for each topic in each
language, would learn two unrelated word dis-
tributions in two languages for a single topic.
Therefore, naively using the produced topics as
?aligned? across languages is ill-advised.
Our solution is to automatically identify aligned
polylingual topics after learning by examining
a topic?s distribution across code-switched docu-
ments. Our metric relies on distributional proper-
ties of an inferred topic across the entire collec-
tion.
675
To summarize, based on the model of Mimno et
al. (2009) we will learn:
? For each topic, a language specific word distri-
bution.
? For each (code-switched) token, a language.
? For each topic, an identification as to whether
the topic captures an alignment across lan-
guages.
The first two goals are achieved by incorporat-
ing new hidden variables in the traditional polylin-
gual topic model. The third goal requires an auto-
mated post-processing step. We call the resulting
model Code-Switched LDA (csLDA). The gener-
ative process is as follows:
? For each topic z ? T
? For each language l ? L
? Draw word distribution
?
l
z
?Dir(?
l
)
? For each document d ? D:
? Draw a topic distribution ?
d
? Dir(?)
? Draw a language distribution
?
d
?Dir(?)
? For each token i ? d:
? Draw a topic z
i
? ?
d
? Draw a language l
i
? ?
d
? Draw a word w
i
? ?
l
z
For monolingual documents, we fix l
i
to the LID
tag for all tokens. Additionally, we use a single
background distribution for each language to cap-
ture stopwords; a control variable pi, which fol-
lows a Dirichlet distribution with prior parameter-
ized by ?, is introduced to decide the choice be-
tween background words and topic words follow-
ing (Chemudugunta et al, 2006)
1
. We use asym-
metric Dirichlet priors (Wallach et al, 2009), and
let the optimization process learn the hyperparam-
eters. The graphical model is shown in Figure 2.
3.1 Inference
Inference for csLDA follows directly from LDA.
A Gibbs sampler learns the word distributions ?
l
z
for each language and topic. We use a block Gibbs
sampler to jointly sample topic and language vari-
ables for each token. As is customary, we collapse
out ?, ? and ?. The sampling posterior is:
P (z
i
, l
i
|w, z
?i
, l
?i
, ?, ?, ?) ?
(n
l,z
w
i
)
?i
+ ?
n
l,z
?i
+W?
?
m
z,d
?i
+ ?
m
d
?i
+ T ?
?
o
l,d
?i
+ ?
o
d
?i
+ L?
(1)
where (n
l,z
w
i
)
?i
is the number of times the type for
word w
i
assigned to topic z and language l (ex-
1
Omitted from the generative process but shown in Fig. 2.
?
?
l
i
?
d
?
d
?
l
z
?
l
b
?
B?
z
i
b
i
w
i
D
N
L
T
Figure 2: The graphical model for csLDA.
cluding current word w
i
), m
z,d
?i
is the number of
tokens assigned to topic z in document d (exclud-
ing current word w
i
), o
l,d
?i
is the number of tokens
assigned to language l in document d (excluding
current word w
i
), and these variables with super-
scripts or subscripts omitted are totals across all
values for the variable. W is the number of words
in the corpus. All counts omit words assigned
to the background. During sampling, words are
first assigned to the background/topic distribution
and then topic and language are sampled for non-
background words.
We optimize the hyperparameters ?, ?, ? and ?
by interleaving sampling iterations with a Newton-
Raphson update to obtain the MLE estimate for
the hyperparameters. Taking ? as an example, one
step of the Newton-Raphson update is:
?
new
= ?
old
?H
?1
?L
??
(2)
where H is the Hessian matrix and
?L
??
is the gra-
dient of the likelihood function with respect to
the optimizing hyperparameter. We interleave 200
sampling iterations with one Newton-Raphson up-
date.
3.2 Selecting Aligned Topics
We next identify learned topics (a set of related
word-distributions) that truly represent an aligned
topic across languages, as opposed to an unrelated
set of distributions for which there is no support-
ing alignment evidence in the corpus. We begin by
measuring how often each topic occurs in code-
switched documents. If a topic never occurs in
a code-switched document, then there can be no
evidence to support alignment across languages.
For the topics that appear at least once in a code-
switched document, we estimate their probability
676
in the code-switched documents by a MAP esti-
mate of ?. Topics appearing in at least one code-
switched document with probability greater than
a threshold p are selected as candidates for true
cross-language topics.
4 Data
We used two datasets: a Sina Weibo Chinese-
English corpus (Ling et al, 2013) and a Spanish-
English Twitter corpus.
Weibo Ling et al (2013) extracted over 1m
Chinese-English parallel segments from Sina
Weibo, which are code-switched messages. We
randomly sampled 29,705 code-switched mes-
sages along with 42,116 Chinese and 42,116 En-
glish messages from the the same time frame. We
used these data for training. We then sampled
an additional 2475 code-switched messages, 4221
English and 4211 Chinese messages as test data.
Olympics We collected tweets from July 27,
2012 to August 12, 2012, and identified 302,775
tweets about the Olympics based on related hash-
tags and keywords (e.g. olympics, #london2012,
etc.) We identified code-switched tweets using
the Chromium Language Detector
2
. This system
provides the top three possible languages for a
given document with confidence scores; we iden-
tify a tweet as code-switched if two predicted lan-
guages each have confidence greater than 33%.
We then used the tagger of Lignos and Marcus
(2013) to obtain token level LID tags, and only
tweets with tokens in both Spanish and English are
used as code-switched tweets. In total we iden-
tified 822 Spanish-English code-switched tweets.
We further expanded the mined tweets to full con-
versations, yielding 1055 Spanish-English code-
switched documents (including both tweets and
conversations), along with 4007 English and 4421
Spanish tweets composes our data set. We reserve
10% of the data for testing.
5 Experiments
We evaluated csLDA on the two datasets and eval-
uated each model using perplexity on held out data
and human judgements. While our goal is to learn
polylingual topics, we cannot compare to previous
polylingual models since they require comparable
data, which we lack. Instead, we constructed a
baseline from LDA run on the entire dataset (no
2
https://code.google.com/p/chromium-compact-language-detector/
language information.) For each model, we mea-
sured the document completion perplexity (Rosen-
Zvi et al, 2004) on the held out data. We ex-
perimented with different numbers of topics (T ).
Since csLDA duplicates topic distributions (T ?L)
we used twice as many topics for LDA.
Figure 3 shows test perplexity for varying T and
perplexity for the best setting of csLDA (T =60)
and LDA (T =120). The table lists both mono-
lingual and code-switched test data; csLDA im-
proves over LDA in almost every case, and across
all values of T . The background distribution (-bg)
has mixed results for LDA, whereas for csLDA
it shows consistent improvement. Table 4 shows
some csLDA topics. While there are some mis-
takes, overall the topics are coherent and aligned.
We use the available per-token LID system
(Lignos and Marcus, 2013) for Spanish/English
to justify csLDA?s ability to infer the hidden lan-
guage variables. We ran csLDA-bg with l
i
set to
the value provided by the LID system for code-
switched documents (csLDA-bg with LID), which
gives csLDA high quality LID labels. While we
see gains for the code-switched data, overall the
results for csLDA-bg and csLDA-bg with LID are
similar, suggesting that the model can operate ef-
fectively even without a supervised per-token LID
system.
5.1 Human Evaluation
We evaluate topic alignment quality through a hu-
man judgements (Chang et al, 2009). For each
aligned topic, we show an annotator the 20 most
frequent words from the foreign language topic
(Chinese or Spanish) with the 20 most frequent
words from the aligned English topic and two ran-
dom English topics. The annotators are asked to
select the most related English topic among the
three; the one with the most votes is considered
the aligned topic. We count how often the model?s
alignments agree.
LDA may learn comparable topics in different
languages but gives no explicit alignments. We
create alignments by classifying each LDA topic
by language using the KL-divergence between the
topic?s words distribution and a word distribution
for the English/foreign language inferred from the
monolingual documents. Language is assigned to
a topic by taking the minimum KL. For Weibo
data, this was not effective since the vocabularies
of each language are highly unbalanced. Instead,
677
20/40 30/60 40/80 50/100 60/120 70/140
# Topics
8000
8500
9000
9500
10000
P
e
r
p
l
e
x
i
t
y
LDA
LDA-bg
csLDA-bg with LID
csLDA-bg
csLDA
20/40 30/60 40/80 50/100 60/120 70/140
# Topics
18000
20000
22000
24000
26000
28000
30000
P
e
r
p
l
e
x
i
t
y
LDA
LDA-bg
csLDA-bg
csLDA
T =60/120 Olympics Weibo
En Es CS En Cn CS
LDA 11.32 9.44 6.97 29.19 23.06 11.69
LDA-bg 11.35 9.51 6.79 40.87 27.56 10.91
csLDA 8.72 7.94 6.17 18.20 17.31 12.72
csLDA-bg 8.72 7.73 6.04 18.25 17.74 12.46
csLDA-bg 8.73 7.93 4.91 - - -
with LID
Figure 3: Plots show perplexity for different T (Olympics left, Weibo right). Perplexity in the table are
in magnitude of 1? 10
3
.
Football Basketball
English Spanish English Spanish
mexico mucho game espa?na
brazil argentina basketball baloncesto
soccer m?exico year basketball
vs brasil finals bronce
womens ganar?a gonna china
football tri nba final
mens yahel castillo obama rusia
final delpo lebron espa?nola
Social Media Transportation
English Chinese English Chinese
twitter ??? car ??
bitly ?? drive ??
facebook ?? road ??
check ?? line ??
use ?? train ???
blog ?? harry ??
free pm ?? ??
post ?? bus ??
Figure 4: Examples of aligned topics from Olympics (left) and Weibo (right).
we manually labeled the topics by language. We
then pair topics across languages using the cosine
similarity of their co-occurrence statistics in code-
switched documents. Topic pairs with similarity
above t are considered aligned topics. We also
used a threshold p (?3.2) to select aligned topics
in csLDA. To ensure a fair comparison, we select
the same number of aligned topics for LDA and
csLDA.
3
. We used the best performing setting:
csLDA T =60, LDA T =120, which produced 12
alignments from Olympics and 28 from Weibo.
Using Mechanical Turk we collected multiple
judgements per alignment. For Spanish, we re-
moved workers who disagreed with the majority
more than 50% of the time (83 deletions), leav-
ing 6.5 annotations for each alignment (85.47%
inter-annotator agreement.) For Chinese, since
quality of general Chinese turkers is low (Pavlick
et al, 2014) we invited specific workers and
obtained 9.3 annotations per alignment (78.72%
inter-annotator agreement.) For Olympics, LDA
alignments matched the judgements 25% of the
time, while csLDA matched 50% of the time.
While csLDA found 12 alignments and LDA 29,
the 12 topics evaluated from both models show
that csLDA?s alignments are higher quality. For
the Weibo data, LDA matched judgements 71.4%,
while csLDA matched 75%. Both obtained high
3
We used thresholds p = 0.2 and t = 0.0001. We limited
the model with more alignments to match the one with less.
quality alignments ? likely due both to the fact
that the code-switched data is curated to find trans-
lations and we hand labeled topic language ? but
csLDA found many more alignments: 60 as com-
pared to 28. These results confirm our automated
results: csLDA finds higher quality topics that
span both languages.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research (JMLR), 3:993?1022.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L
Boyd-graber, and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Advances in neural information processing systems,
pages 288?296.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In NIPS.
Mona Diab and Ankit Kamboj. 2011. Feasibility of
leveraging crowd sourcing for the creation of a large
scale annotated resource for Hindi English code
switched data: A pilot annotation. In Proceedings
of the 9th Workshop on Asian Language Resources,
pages 36?40, Chiang Mai, Thailand, November.
Asian Federation of Natural Language Processing.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A latent variable model
678
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1277?1287. Asso-
ciation for Computational Linguistics.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In NAACL.
Heba Elfardy and Mona Diab. 2012. Token level
identification of linguistic code switching. In Pro-
ceedings of COLING 2012: Posters, pages 287?296,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Liangjie Hong and Brian D Davison. 2010. Empirical
study of topic modeling in twitter. In Proceedings of
the First Workshop on Social Media Analytics, pages
80?88. ACM.
Jagadeesh Jagarlamudi and Hal Daum?e. 2010. Ex-
tracting multilingual topics from unaligned compa-
rable corpora. Advances in Information Retrieval,
pages 444?456.
Aravind K Joshi. 1982. Processing of sentences
with intra-sentential code-switching. In Proceed-
ings of the 9th Conference on Computational lin-
guistics (COLING), pages 145?150.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In NAACL.
Ying Li and Pascale Fung. 2012. Code-switch lan-
guage model with inversion constraints for mixed
language speech recognition. In Proceedings of
COLING 2012, pages 1671?1680, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
mandarin-english code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL
Anthology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. To-
ward web-scale analysis of codeswitching. In An-
nual Meeting of the Linguistic Society of America.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting
on Association for Computational Linguistics, ACL
?13. Association for Computational Linguistics.
David Mimno, Hanna M Wallach, Jason Naradowsky,
David A Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2-Volume 2, pages
880?889. Association for Computational Linguis-
tics.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,
and Chris Callison-Burch. 2014. The language de-
mographics of Amazon Mechanical Turk. Transac-
tions of the Association for Computational Linguis-
tics, 2(Feb):79?92.
Daniel Ramage, Susan T Dumais, and Daniel J
Liebling. 2010. Characterizing microblogs with
topic models. In ICWSM.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of the
20th conference on Uncertainty in artificial intelli-
gence, pages 487?494. AUAI Press.
David Sankofl. 1998. The production of code-mixed
discourse. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics, Volume 1, pages 8?21, Montreal,
Quebec, Canada, August. Association for Computa-
tional Linguistics.
Thamar Solorio and Yang Liu. 2008a. Learning to
predict code-switching points. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 973?981, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-Speech
tagging for English-Spanish code-switched text. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
1051?1060, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Hanna M Wallach, David M Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter.
In NIPS, volume 22, pages 1973?1981.
Liang-Chih Yu, Wei-Cheng He, and Wei-Nan Chien.
2012. A language modeling approach to identify-
ing code-switched sentences and words. In Pro-
ceedings of the Second CIPS-SIGHAN Joint Confer-
ence on Chinese Language Processing, pages 3?8,
Tianjin, China, December. Association for Compu-
tational Linguistics.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing
He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li.
2011. Comparing twitter and traditional media us-
ing topic models. In Advances in Information Re-
trieval, pages 338?349. Springer.
679
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 42?50,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Simple Wikipedia:
A Cogitation in Ascertaining Abecedarian Language
Courtney Napoles and Mark Dredze
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
courtneyn@jhu.edu, mdredze@cs.jhu.edu
Abstract
Text simplification is the process of changing
vocabulary and grammatical structure to cre-
ate a more accessible version of the text while
maintaining the underlying information and
content. Automated tools for text simplifica-
tion are a practical way to make large corpora
of text accessible to a wider audience lacking
high levels of fluency in the corpus language.
In this work, we investigate the potential of
Simple Wikipedia to assist automatic text sim-
plification by building a statistical classifica-
tion system that discriminates simple English
from ordinary English. Most text simplifica-
tion systems are based on hand-written rules
(e.g., PEST (Carroll et al, 1999) and its mod-
ule SYSTAR (Canning et al, 2000)), and
therefore face limitations scaling and trans-
ferring across domains. The potential for us-
ing Simple Wikipedia for text simplification
is significant; it contains nearly 60,000 ar-
ticles with revision histories and aligned ar-
ticles to ordinary English Wikipedia. Us-
ing articles from Simple Wikipedia and ordi-
nary Wikipedia, we evaluated different classi-
fiers and feature sets to identify the most dis-
criminative features of simple English for use
across domains. These findings help further
understanding of what makes text simple and
can be applied as a tool to help writers craft
simple text.
1 Introduction
The availability of large collections of electronic
texts is a boon to information seekers, however, ad-
vanced texts often require fluency in the language.
Text simplification (TS) is an emerging area of text-
to-text generation that focuses on increasing the
readability of a given text. Potential applications
can increase the accessibility of text, which has great
value in education, public health, and safety, and can
aid natural language processing tasks such as ma-
chine translation and text generation.
Corresponding to these applications, TS can be
broken down into two rough categories depending
on the target ?reader.? The first type of TS aims to
increase human readability for people lacking high-
level language skills, either because of age, educa-
tion level, unfamiliarity with the language, or dis-
ability. Historically, generating this text has been
done by hand, which is time consuming and expen-
sive, especially when dealing with material that re-
quires expertise, such as legal documents. Most cur-
rent automatic TS systems rely on handwritten rules,
e.g., PEST (Carroll et al, 1999), its SYSTAR mod-
ule (Canning et al, 2000), and the method described
by Siddharthan (2006). Systems using handwritten
rules can be susceptible to changes in domains and
need to be modified for each new domain or lan-
guage. There has been some research into automat-
ically learning the rules for simplifying text using
aligned corpora (Daelemans et al, 2004; Yatskar et
al., 2010), but these have yet to match the perfor-
mance hand-crafted rule systems. An example of
a manually simplified sentence can be found in ta-
ble 1.
The second type of TS has the goal of increas-
ing the machine readability of text to aid tasks such
as information extraction, machine translation, gen-
erative summarization, and other text generation
42
tasks for selecting and evaluating the best candi-
date output text. In machine translation, the eval-
uation tool most commonly used for evaluating out-
put, the BLEU score (Papineni et al, 2001), rates the
?goodness? of output based on n-gram overlap with
human-generated text. However this metric has been
criticized for not accurately measuring the fluency
of text and there is active research into other met-
rics (Callison-Burch et al, 2006; Ye et al, 2007).
Previous studies suggest that text simplified for ma-
chine and human comprehension are categorically
different (Chae and Nenkova, 2009). Our research
considers text simplified for human readers, but the
findings can be used to identify features that dis-
criminate simple text for both applications.
The process of TS can be divided into three as-
pects: removing extraneous or superfluous text, sub-
stituting more complex lexical and syntactic forms,
and inserting information to offer further clarifica-
tion where needed (Alu??sio et al, 2008). In this re-
gard, TS is related to several different natural lan-
guage processing tasks such as text summarization,
compression, machine translation, and paraphras-
ing.
While none of these tasks alone directly provide
a solution to text simplification, techniques can be
drawn from each. Summarization techniques can
be used to identify the crucial, most informative
parts of a text and compression can be used to re-
move superfluous words and phrases. In fact, in the
Wikipedia documents analyzed for this research, the
average length of a ?simple? document is only 21%
the length of an ?ordinary? English document (al-
though this may be an unintentional byproduct of
how articles were simplified, as discussed in section
6.1).
In this paper we study the properties of language
that differentiate simple from ordinary text for hu-
man readers. Specifically, we use statistical learn-
ing techniques to identify the most discriminative
features of simple English and ?ordinary? English
using articles from Simple Wikipedia and English
Wikipedia. We use cognitively motivated features
as well as statistical measurements of a document?s
lexical, syntactic, and surface features. Our study
demonstrates the validity and potential benefits of
using Simple Wikipedia as a resource for TS re-
search.
Ordinary text
Every person has the right to a name, in which is
included a first name and surname. . . . The alias
chosen for legal activities has the same protection
as given to the name.
Same text in simple language
Every person has the right to have a name, and
the law protects people?s names. Also, the law
protects a person?s alias. . . . The name is made
up of a first name and a surname (name = first
name + surname).
Table 1: A text in ordinary and simple language from
Alu??sio et al (2008).
2 Wikipedia as a Corpus
Wikipedia is a unique resource for natural lan-
guage processing tasks due to its sheer size, acces-
sibility, language diversity, article structure, inter-
document links, and inter-language document align-
ments. Denoyer and Gallinari (2006) introduced
the Wikipedia XML Corpus, with 1.5 million doc-
uments in eight languages from Wikipedia, that
stored the rich structural information of Wikipedia
with XML. This corpus was designed specifically
for XML retrieval but has uses in natural language
processing, categorization, machine translation, en-
tity ranking, etc. YAWN (Schenkel et al, 2007), a
Wikipedia XML corpus with semantic tags, is an-
other example of exploiting Wikipedia?s structural
information. Wikipedia provides XML site dumps
every few weeks in all languages as well as static
HTML dumps.
A diverse array of NLP research in the past
few years has used Wikipedia, such as for word
sense disambiguation (Mihalcea, 2007), classifica-
tion (Gantner and Schmidt-Thieme, 2009), machine
translation (Smith et al, 2010), coreference resolu-
tion (Versley et al, 2008; Yang and Su, 2007), sen-
tence extraction for summarization (Biadsy et al,
2008), information retrieval (Mu?ller and Gurevych,
2008), and semantic role labeling (Ponzetto and
Strube, 2006), to name a few. However, except for
very recent work by Yatskar et al (2010), to our
knowledge there has not been comparable research
in using Wikipedia for text simplification.
43
Ordinary Wikipedia
Hawking was the Lucasian Professor of Mathe-
matics at the University of Cambridge for thirty
years, taking up the post in 1979 and retiring on 1
October 2009.
Simple Wikipedia
Hawking was a professor of mathematics at the
University of Cambridge (a position that Isaac
Newton once had). He retired on October 1st
2009.
Table 2: Comparable sentences from the ordinary
Wikipedia and Simple Wikipedia entry for ?Stephen
Hawking.?
What makes Wikipedia an excellent resource for
text simplification is the new Simple Wikipedia
project1, a collection of 58,000 English Wikipedia
articles that have been rewritten in Simple English,
which uses basic vocabulary and less complex gram-
mar to make the content of Wikipedia accessible to
students, children, adults with learning difficulties,
and non-native English speakers. In addition to be-
ing a large corpus, these articles are linked to their
ordinary Wikipedia counterparts, so for each article
both a simple and an ordinary version are available.
Furthermore, on inspection many articles in Simple
Wikipedia appear to be copied and edited from the
corresponding ordinary Wikipedia article. This in-
formation, together with revision history and flags
signifying unsimplified text, can provide a scale of
information on the text-simplification process previ-
ously unavailable. Example sentences from Simple
Wikipedia and ordinary Wikipedia are shown in ta-
ble 2.
We used articles from Simple Wikipedia and or-
dinary English Wikipedia to create a large cor-
pus of simple and ordinary articles for our exper-
iments. In order to experiment with models that
work across domains, the corpus includes articles
from nine of the primary categories identified in
Simple Wikipedia: Everyday Life, Geography, His-
tory, Knowledge, Literature, Media, People, Reli-
gion, and Science. A total of 55,433 ordinary and
42,973 simple articles were extracted and processed
from English Wikipedia and Simple Wikipedia, re-
1http://simple.wikipedia.org/
Coarse Tag Penn Treebank Tags
DET DT, PDT
ADJ JJ, JJR, JJS
N NN, NNS, NP, NPS, PRP, FW
ADV RB, RBR, RBS
V VB, VBN, VBG, VBP, VBZ, MD
WH WDT, WP, WP$, WRB
Table 3: A mapping of the Penn Treebank tags to a coarse
tagset used to generate features.
spectively. Each document contains at least two sen-
tences. Additionally, the corpus contains only the
main text body of each article and does not con-
sider info boxes, tables, lists, external and cross-
references, and other structural features. The exper-
iments that follow randomly extract documents and
sentences from this collection.
Before extracting features, we ran a series of nat-
ural language processing tools to preprocess the col-
lection. First, all of the XML and ?wiki markup?
was removed. Each document was split into sen-
tences using the Punkt sentence tokenizer (Kiss and
Strunk, 2006) in NLTK (Bird and Loper, 2004). We
then parsed each sentence using the PCFG parser
of Huang and Harper (2009), a modified version
of the Berkeley parser (Petrov et al, 2006; Petrov
and Klein, 2007), for the tree structure and part-of-
speech tags.
3 Task Setup
To evaluate the feasibility of learning simple and or-
dinary texts, we sought to identify text properties
that differentiated between these classes. Using the
two document collections, we constructed a simple
binary classification task: label a piece of text as ei-
ther simple or ordinary. The text was labeled ac-
cording to its source: simple or ordinary Wikipedia.
From each piece of text, we extracted a set of fea-
tures designed to capture differences between the
texts, using cognitively motivated features based on
a document?s lexical, syntactic, and surface features.
We first describe our features and then our experi-
mental setup.
44
4 Features
We began by examining the guidelines for writing
Simple Wikipedia pages.2 These guidelines suggest
that articles use only the 1000 most common and ba-
sic English words and contain simple grammar and
short sentences. Articles should be short but can be
longer if they need to explain vocabulary words nec-
essary to understand the topic. Additionally, words
should appear on lists of basic English words, such
as the Voice of America Special English words list
(Voice Of America, 2009) or the Ogden Basic En-
glish list (Ogden, 1930). Idioms should be avoided
as well as compounds and the passive voice as op-
posed to a single simple verb.
To capture these properties in the text, we created
four classes of features: lexical, part-of-speech, sur-
face, and parse. Several of our features have previ-
ously been used for measuring text fluency (Alu??sio
et al, 2008; Chae and Nenkova, 2009; Feng et al,
2009; Petersen and Ostendorf, 2007).
Lexical. Previous work by Feng et al (2009) sug-
gests that the document vocabulary is a good predic-
tor of document readability. Simple texts are more
likely to use basic words more often as opposed to
more complicated, domain-specific words used in
ordinary texts. To capture these features we used a
unigram bag-of-words representation. We note that
lexical features are unlikely to be useful unless we
have access to a large training corpus that allowed
the estimation of the relative frequency of words
(Chae and Nenkova, 2009). Additionally, we can
expect lexical features to be very fragile for cross-
domain experiments as they are especially suscepti-
ble to changes in domain vocabulary. Nevertheless,
we include these features as a baseline in our exper-
iments.
Parts of speech. A clear focus of the simple text
guidelines is grammar and word type. One way
of representing this information is by measuring
the relative frequency of different types of parts
of speech. We consider simple unigram part-of-
speech tag information. We measured the nor-
malized counts and relative frequency of part-of-
speech tags and counts of bigram part-of-speech tags
2http://simple.wikipedia.org/wiki/Wikipedia:
Simple_English_Wikipedia
Feature Simple Ordinary
Tokens 158 4332
Types 100 1446
Sentences 10 172
Average sentence length 15.80 25.19
Type-token ratio 0.63 0.33
Percent simple words 0.31 0.08
Not BE850 type-token ratio 0.65 0.30
BE850 type-token ratio 0.59 0.67
Table 4: A comparison of the article ?Stephen Hawking?
from Simple and ordinary Wikipedia.
in each piece of text. Since Devlin and Unthank
(2006) has shown that word order (subject verb ob-
ject (SVO), object verb subject (OVS), etc.) is cor-
related with readability, we also included a reduced
tagset to capture grammatical patterns (table 3). We
also included normalized counts of these reduced
tags in the model.
Surface features. While lexical items may be im-
portant, more general properties can be extracted
from the lexical forms. We can also include fea-
tures that correspond to surface information in the
text. These features include document length, sen-
tence length, word length, numbers of lexical types
and tokens, and the ratio of types to tokens. All
words are labeled as basic or not basic according
to Ogden?s Basic English 850 (BE850) list (Ogden,
1930).3 In order to measure the lexical complexity
of a document, we include features for the number
of BE850 words, the ratio of BE850 words to total
words, and the type-token ratio of BE850 and non-
BE850 words. Investigating the frequency and pro-
ductivity of words not in the BE850 list will hope-
fully improve the flexibility of our model to work
across domains and not learn any particular jargon.
We also hope that the relative frequency and pro-
ductivity measures of simple and non-simple words
will codify the lexical choices of a sentence while
avoiding the aforementioned problems with includ-
ing specific lexical items.
3Wikipedia advocates using words that appear on the BE850
list. Ogden also provides extended Basic English vocabulary
lists, totaling 2000 Basic English words, but these words tend
to be more specialized or domain specific. For the purposes of
this study only words in BE850 were used.
45
Table 4 shows the difference in some surface
statistics in an aligned document from Simple and
ordinary Wikipedia. In this example, nearly one-
third of the words in the simple document are from
the BE850 while less than a tenth of the words in the
ordinary document are. Additionally, the productiv-
ity of words, particularly non-BE850 words, is much
higher in the ordinary document. There are also
clear differences in the length of the documents, and
on average documents from ordinary Wikipedia are
more than four times longer than documents from
Simple Wikipedia.
Syntactic parse. As previously mentioned, a
number of Wikipedia?s writing guidelines focus on
general grammatical rules of sentence structure. Ev-
idence of these rules may be captured in the syn-
tactic parse of the sentences in the text. Chae and
Nenkova (2009) studied text fluency in the context
of machine translation and found strong correlations
between parse tree structures and sentence fluency.
In order to represent the structural complexity of
the text, we collected extracted features from the
parse trees. Our features included the frequency and
length of noun phrases, verb phrases, prepositional
phrases, and relative clauses (including embedded
structures). We also considered relative ratios, such
as the ratio of noun to verb phrases, prepositional to
noun phrases, and relative clauses to noun phrases.
We used the length of the longest noun phrase as
a signal of complexity, and we also sought features
that measured how typical the sentences were of En-
glish text. We included some of the features from
the parser reranking work of Charniak and Johnson
(2005): the height of the parse tree and the number
of right branches from the root of the tree to the fur-
thest right leaf that is not punctuation.
5 Experiments
Using the feature sets described above, we evalu-
ated a simple/ordinary text classifier in several set-
tings on each category. First, we considered the task
of document classification, where a classifier deter-
mines whether a full Wikipedia article was from
ordinary English Wikipedia or Simple Wikipedia.
For each category of articles, we measured accu-
racy on this binary classification task using 10-fold
cross-validation. In the second setting, we consid-
Category Documents Sentences
Everyday Life 15,124 7,392
Geography 10,470 5,852
History 5,174 1,644
Literature 992 438
Media 502 429
People 4,326 1,562
Religion 1,863 1,581
Science 25,787 21,054
All 64,238 39,952
Table 5: The number of examples available in each cate-
gory. To compare experiments in each category we used
at most 2000 instances in each experiment.
Feature class Features
Lexical 522,153
Part of speech 2478
tags 45
tag pairs 1972
tags (reduced) 22
tag pairs (reduced) 484
Parse 11
Surface 9
Table 6: The number of features in each feature class.
ered the performance of a sentence-level classifier.
The classifier labeled each sentence as either ordi-
nary or simple and we report results using 10-fold
cross-validation on a random split of the sentences.
For both settings we also evaluated a single classifier
trained on all categories.
We next considered cross-category performance:
how would a classifier trained to detect differences
between simple and ordinary examples from one
category do when tested on another category. In
this experiment, we trained a single classifier on data
from a single category and used the classifier to label
examples from each of the other categories. We re-
port the accuracy on each category in these transfer
experiments.
For learning we require a binary classifier train-
ing algorithm. We evaluated several learning algo-
rithms for classification and report results for each
one: a) MIRA?a large margin online learning al-
gorithm (Crammer et al, 2006). Online learning
algorithms observe examples sequentially and up-
46
date the current hypothesis after each observation; b)
Confidence Weighted (CW) learning?a probabilis-
tic large margin online learning algorithm (Dredze et
al., 2008); c) Maximum Entropy?a log-linear dis-
criminative classifier (Berger et al, 1996); and d)
Support Vector Machines (SVM)?a large margin
discriminator (Joachims, 1998).
For each experiment, we used default settings of
the parameters and 10 online iterations for the online
methods (MIRA, CW). To create a fair comparison
for each category, we limited the number of exam-
ples to a maximum of 2000.
6 Results
For the first task of document classification, we saw
at least 90% mean accuracy with each of the clas-
sifiers. Using all features, SVM and Maximum En-
tropy performed almost perfectly. The online clas-
sifiers, CW and MIRA, displayed similar preference
to the larger feature sets, lexical and part-of-speech
counts. When using just lexical counts, both CW
and MIRA were more accurate than the SVM and
Maximum Entropy (reporting 92.95% and 86.55%
versus 75.00% and 78.75%, respectively). For all
classifiers, the models using the counts of part-of-
speech tags did better than classifiers trained on the
surface features and on the parse features. This is
surprising, since we expected the surface features to
be robust predictors of the document class, mainly
because the average ordinary Wikipedia article in
our corpus is about four times longer than the av-
erage Simple Wikipedia article. We also expected
the syntactic features to be a strong predictor of the
document class since more complicated parse trees
correspond to more complex sentences.
For each classifier, we looked at its performance
without its less predictive feature categories, and
for CW the inclusion of the surface features de-
creased performance noticeably. The best CW
classifiers used either part-of-speech and lexical
features (95.95%) or just part-of-speech features
(95.80%). The parse features, which by themselves
only yielded 64.60% accuracy, when combined with
part-of-speech and lexical features showed high ac-
curacy as well (95.60%). MIRA also showed higher
accuracy when surface features were not included
(from 97.50% mean accuracy with all features to
97.75% with all but surface features).
The best SVM classifier used all four feature
classes, but had nearly as good accuracy with just
part-of-speech counts and surface features (99.85%
mean accuracy) and with surface and parse features
(also 99.85% accuracy). Maximum Entropy, on
the other hand, improved slightly when the lexical
and parse features were not included (from 99.45%
mean accuracy with all feature classes to 99.55%).
We examined the weights learned by the classi-
fiers to determine the features that were effective for
learning. We selected the features with the highest
absolute weight for a MIRA classifier trained on all
categories. The most predictive features for docu-
ment classification were the sentence length (shorter
favors Simple), the length of the longest NP (longer
favors ordinary), the number of sentences (more fa-
vors ordinary), the average number of prepositional
phrases and noun phrases per sentence, the height
of the parse tree, and the number of adjectives. The
most predictive features for sentence classification
were the ratio of different tree non-terminals (VP, S,
NP, S-Bar) to the number of words in the sentence,
the ratio of the total height of the productions in a
tree to the height of the tree, and the extent to which
the tree was right branching. These features are con-
sistent with the rules described above for simple text.
Next we looked at a pairwise comparison of how
the classifiers perform when trained on one category
and tested on another. Surprisingly, the results were
robust across categories, across classifiers. Using
the best feature class as determined in the first task,
the average drop in accuracy when trained on each
domain was very low across all classifiers (the mean
accuracy rate of each cross-category classification
was at least 90%). Table 6 shows the mean change in
accuracy from CW models trained and tested on the
same category to the models trained and tested on
different categories. When trained on the Everyday
Life category, the model actually showed a mean in-
crease in accuracy when predicting other categories.
In the final task, we trained binary classifiers to
identify simple sentences in isolation. The mean
accuracy was lower for this task than for the doc-
ument classification task, and we anticipated indi-
vidual sentences to be more difficult to classify be-
cause each sentence only carries a fraction of the
47
Classifier All features Lexical POS Surface Parse
CW 86.40% 92.95% 95.80% 69.80% 64.60%
MIRA 97.50% 86.55% 94.55% 79.65% 66.90%
MaxEnt 99.45% 78.75% 96.25% 86.90% 80.70%
SVM 99.90% 75.00% 96.60% 89.75% 82.70%
Table 7: Mean accuracy of all classifiers on the document classification task.
Classifier All features POS Surface Parse
CW 73.20% 74.45% 57.40% 62.25%
MIRA 71.15% 72.65% 56.50% 56.45%
MaxEnt 80.80% 77.65% 71.30% 69.00%
SVM 77.00% 76.40% 72.55% 73.00%
Table 8: Mean accuracy of all classifiers on the sentence classification task.
Category Mean accuracy change
Everyday life +1.42%
Geography ?4.29%
History ?1.01%
Literature ?1.84%
Media ?0.56%
People ?0.20%
Religion ?0.56%
Science ?2.50%
Table 9: Mean accuracy drop for a CW model trained on
one category and tested on all other categories. Negative
numbers indicate a decrease in performance.
information held in an entire document. It is com-
mon to have short, simple sentences as part of ordi-
nary English text, although they will not make up the
whole. However results were still promising, with
between 72% and 80% mean accuracy. With CW
and MIRA, the classifiers benefited from training on
all categories, while MaxEnt and SVM in-category
and all-category models achieved similar accuracy
levels, but the results on cross-category tests were
more variable than in the document classification.
There was also no consistency across features and
classifiers with regard to category-to-category clas-
sification. Overall the results of the sentence classi-
fication task are encouraging and show promise for
detecting individual simple sentences taken out of
context.
6.1 Discussion
The classifiers performed robustly for the document-
level classification task, although the corpus itself
may have biased the model due to the longer aver-
age length of ordinary documents, which we tried
to address by filtering out articles with only one
or two sentences. Cursory inspection suggests that
there is overlap between many Simple Wikipedia ar-
ticles and their corresponding ordinary English arti-
cles, since a large number of Simple Wikipedia doc-
uments appear to be generated directly from the En-
glish Wikipedia articles with more complicated sub-
sections of the documents omitted from the Simple
article.
The sentence classification task could be im-
proved by better labeling of sentences. In these ex-
periments, we assumed that every sentence in an or-
dinary document would be ordinary (i.e., not simple)
and vice versa for simple documents. However it is
not the case that ordinary English text contains only
complicated sentences. In future research we can
use human annotated sentences for building the clas-
sifiers. The features we used in this research suggest
that simple text is created from categorical lexical
and syntactic replacement, but more complicated,
technical, or detailed oriented text may require more
rewriting, and would be of more interest in future
research.
7 Conclusion and Future Work
We have demonstrated the ability to automatically
identify texts as either simple or ordinary at both
48
the document and sentence levels using a variety of
features based on the word usage and grammatical
structures in text. Our statistical analysis has identi-
fied relevant features for this task accessible to com-
putational systems. Immediate applications of the
classifiers created in this research for text simplifi-
cation include editing tools that can identify parts of
a text that may be difficult to understand or for word
processors, in order to notify writers of complicated
sentences in real time.
Using this initial exploration of Simple
Wikipedia, we plan to continue working in a
number of directions. First, we will explore ad-
ditional robust indications of text difficulty. For
example, Alu??sio et al (2008) claim that sentences
that are easier to read are also easier to parse, so
the entropy of the parser or confidence in the output
may be indicative of a text?s difficulty. Additionally,
language models trained on large corpora can assign
probability scores to texts, which may indicate
text difficulty. Of particular interest are syntactic
language models that incorporate some of the
syntactic observations in this paper (Filimonov and
Harper, 2009).
Our next goal will be to look at parallel sentences
to learn rules for simplifying text. One of the ad-
vantages of the Wikipedia collection is the parallel
articles in ordinary English Wikipedia and Simple
Wikipedia. While the content of the articles can dif-
fer, these are excellent examples of comparable texts
that can be useful for learning simplification rules.
Such learning can draw from machine translation,
which learns rules that translate between languages.
The related task of paraphrase extraction could also
provide comparable phrases, one of which can be
identified as a simplified version of the other (Ban-
nard and Callison-Burch, 2005). An additional re-
source available in Simple Wikipedia is the flagging
of articles as not simple. By examining the revision
history of articles whose flags have been changed,
we can discover changes that simplified texts. Initial
work on this topic has automatically learned which
edits correspond to text simplifications (Yatskar et
al., 2010).
Text simplification may necessitate the removal of
whole phrases, sentences, or even paragraphs, as, ac-
cording to the writing guidelines for Wikipedia Sim-
ple (Wikipedia, 2009), the articles should not exceed
a specified length, and some concepts may not be
explainable using the lexicon of Basic English. In
some situations, adding new text to explain confus-
ing but crucial points may serve to aid the reader,
and text generation needs to be further investigated
to make text simplification an automatic process.
Acknowledgements
The authors would like to thank Mary Harper for her
help in parsing our corpus.
References
S.M. Alu??sio, L. Specia, T.A.S. Pardo, E.G. Maziero, and
R.P.M. Fortes. 2008. Brazilian portuguese automatic
text simplification systems. In DocEng.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Associa-
tion for Computational Linguistics (ACL).
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational linguistics, 22(1):39?71.
F. Biadsy, J. Hirschberg, E. Filatova, and LLC InforS-
ense. 2008. An unsupervised approach to biography
production using Wikipedia. In Association for Com-
putational Linguistics (ACL).
S. Bird and E. Loper. 2004. NLTK: The natural lan-
guage toolkit. Proceedings of the ACL demonstration
session, pages 214?217.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In European Conference for Computational
Linguistics (EACL), volume 2006, pages 249?256.
Y. Canning, J. Tait, J. Archibald, and R. Crawley. 2000.
Cohesive generation of syntactically simplified news-
paper text. Lecture notes in computer science, pages
145?150.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,
and J. Tait. 1999. Simplifying text for language-
impaired readers. In European Conference for Com-
putational Linguistics (EACL), pages 269?270.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features. In European
Conference for Computational Linguistics (EACL),
pages 139?147.
E. Charniak andM. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In As-
sociation for Computational Linguistics (ACL), page
180. Association for Computational Linguistics.
49
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
W. Daelemans, A. Ho?thker, and E Tjong Kim Sang.
2004. Automatic sentence simplification for subtitling
in Dutch and English. In Conference on Language Re-
sources and Evaluation (LREC), pages 1045?1048.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
S. Devlin and G. Unthank. 2006. Helping aphasic peo-
ple process online information. In SIGACCESS Con-
ference on Computers and Accessibility.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
L. Feng, N. Elhadad, and M. Huenerfauth. 2009. Cog-
nitively motivated features for readability assessment.
In European Conference for Computational Linguis-
tics (EACL).
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
Empirical Methods in Natural Language Processing
(EMNLP).
Z. Gantner and L. Schmidt-Thieme. 2009. Automatic
content-based categorization of Wikipedia articles. In
Association for Computational Linguistics (ACL).
Z. Huang and M. Harper. 2009. Self-training pcfg
grammars with latent annotations across languages. In
Empirical Methods in Natural Language Processing
(EMNLP).
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learning
(ECML).
T. Kiss and J. Strunk. 2006. Unsupervised multilingual
sentence boundary detection. Computational Linguis-
tics, 32(4):485?525.
R. Mihalcea. 2007. Using Wikipedia for automatic
word sense disambiguation. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
C. Mu?ller and I. Gurevych. 2008. Using Wikipedia
and Wiktionary in domain-specific information re-
trieval. In Working Notes of the Annual CLEF Meet-
ing. Springer.
C.K. Ogden. 1930. Basic English: A General Introduc-
tion with Rules and Grammar. Paul Treber & Co., Ltd.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Association for Computational Linguis-
tics (ACL).
S.E. Petersen and M. Ostendorf. 2007. Text simplifi-
cation for language learners: A corpus analysis. In
The Speech and Language Technology for Education
Workshop, pages 69?72.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Association for Computa-
tional Linguistics (ACL).
S.P. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, WordNet and Wikipedia for coreference
resolution. In North American Chapter of the Associ-
ation for Computational Linguistics (NAACL).
R. Schenkel, F. Suchanek, and G. Kasneci. 2007.
YAWN: A semantically annotated Wikipedia XML
corpus. In Proceedings of GI-Fachtagung fu?r
Datenbanksysteme in Business, Technologie und Web
(BTW2007).
A. Siddharthan. 2006. Syntactic simplification and text
cohesion. Research on Language & Computation,
4(1):77?109.
Jason Smith, Chris Quirk, and Kristina Toutanova. 2010.
Extracting parallel sentences from comparable corpora
using document level alignment. In North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman,
A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Association for Computational Linguistics (ACL)
Demo Session.
Voice Of America. 2009. Word book, 2009 edition.
www.voaspecialenglish.com, February.
Wikipedia. 2009. Simple Wikipedia English.
http://en.wikipedia.org/wiki/Citing Wikipedia, Octo-
ber.
X. Yang and J. Su. 2007. Coreference resolution using
semantic relatedness information from automatically
discovered patterns. In Association for Computational
Linguistics (ACL).
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Experiments with unsupervised extraction of lexi-
cal simplifications. In North American Chapter of the
Association for Computational Linguistics (NAACL).
Y. Ye, M. Zhou, and C.Y. Lin. 2007. Sentence level ma-
chine translation evaluation as a ranking problem: one
step aside from BLEU. In ACL Workshop on statisti-
cal machine translation.
50
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Creating Speech and Language Data With Amazon?s Mechanical Turk
Chris Callison-Burch and Mark Dredze
Human Language Technology Center of Excellence
& Center for Language and Speech Processing
Johns Hopkins University
ccb,mdredze@cs.jhu.edu
Abstract
In this paper we give an introduction to us-
ing Amazon?s Mechanical Turk crowdsourc-
ing platform for the purpose of collecting
data for human language technologies. We
survey the papers published in the NAACL-
2010 Workshop. 24 researchers participated
in the workshop?s shared task to create data for
speech and language applications with $100.
1 Introduction
This paper gives an overview of the NAACL-2010
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk. A number of re-
cent papers have evaluated the effectiveness of us-
ing Mechanical Turk to create annotated data for
natural language processing applications. The low
cost, scalable workforce available through Mechan-
ical Turk (MTurk) and other crowdsourcing sites
opens new possibilities for annotating speech and
text, and has the potential to dramatically change
how we create data for human language technolo-
gies. Open questions include: What kind of research
is possible when the cost of creating annotated train-
ing data is dramatically reduced? What new tasks
should we try to solve if we do not limit ourselves to
reusing existing training and test sets? Can complex
annotation be done by untrained annotators? How
can we ensure high quality annotations from crowd-
sourced contributors?
To begin addressing these questions, we orga-
nized an open-ended $100 shared task. Researchers
were given $100 of credit on Amazon Mechanical
Turk to spend on an annotation task of their choos-
ing. They were required to write a short paper de-
scribing their experience, and to distribute the data
that they created. They were encouraged to ad-
dress the following questions: How did you convey
the task in terms that were simple enough for non-
experts to understand? Were non-experts as good as
experts? What did you do to ensure quality? How
quickly did the data get annotated? What is the cost
per label? Researchers submitted a 1 page proposal
to the workshop organizers that described their in-
tended experiments and expected outcomes. The
organizers selected proposals based on merit, and
awarded $100 credits that were generously provided
by Amazon Mechanical Turk. In total, 35 credits
were awarded to researchers.
Shared task participants were given 10 days to run
experiments between the distribution of the credit
and the initial submission deadline. 30 papers were
submitted to the shared task track, of which 24 were
accepted. 14 papers were submitted to the general
track of which 10 were accepted, giving a 77% ac-
ceptance rate and a total of 34 papers. Shared task
participants were required to provide the data col-
lected as part of their experiments. All of the shared
task data is available on the workshop website.
2 Mechanical Turk
Amazon?s Mechanical Turk1 is an online market-
place for work. Amazon?s tag line for Mechani-
cal Turk is artificial artificial intelligence, and the
name refers to a historical hoax from the 18th cen-
1http://www.mturk.com/
1
< 1
1-2
2-4
4-8
8-20
20-40
40+
< 1 HIT
1-5
5-10
10-20
20-50
50-100
100-200
200-500
500-1k
1k-5k
5k+ HITs
< $1
$1-5
$5-10
10-20
20-50
50-100
100-200
$200+
5%
17%
22%
26%
19%
10%
3%
100%
1%
6%
9%
13%
19%
17%
12%
13%
5%
4%
1%
100%
11%
36%
22%
15%
11%
4%
2%
0%
100%
0%
5%
10%
15%
20%
25%
30%
< 1 1-2 2-4 4-8 8-20 40+
Hours spent on Mechanical Turk per week
0%
5%
10%
15%
20%
< 1 HIT 5-10 20-50 100-200 500-1k 5k+ HITs
Number of HITs completed per week
0%
10%
20%
30%
40%
< $1 $5-10 20-50 100-200
Weekly income from Mechanical Turk
Figure 1: Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010).
tury where a chess-playing automaton appeared to
be able to beat human opponents using a mecha-
nism, but was, in fact, controlled by a person hiding
inside the machine. These hint at the the primary fo-
cus of the web service, which is to get people to per-
form tasks that are simple for humans but difficult
for computers. The basic unit of work on MTurk is
even called a Human Intelligence Task (HIT).
Amazon?s web service provides an easy way to
pay people small amounts of money to perform
HITs. Anyone with an Amazon account can either
submit HITs or work on HITs that were submitted
by others. Workers are referred to as ?Turkers? and
people designing the HITs are called ?Requesters.?
Requesters set the amount that they will pay for each
item that is completed. Payments are frequently as
low as $0.01. Turkers are free to select whichever
HITs interest them.], and to disregard HITs that they
find uninteresting or which they deem pay too little.
Because of its focus on tasks requiring human in-
telligence, Mechanical Turk is obviously applicable
to the field of natural language processing. Snow
et al (2008) used Mechanical Turk to inexpensively
collect labels for several NLP tasks including word
sense disambiguation, word similarity, textual en-
tailment, and temporal ordering of events. Snow et
al. had two exciting findings. First, they showed that
a strong correlation between non-expert and expert
annotators can be achieved by combining the judg-
ments of multiple non-experts, for instance by vot-
ing on each label using 10 different Turkers. Cor-
relation and accuracy of labeling could be further
improved by weighting each Turker?s vote by cal-
ibrating them on a small amount of gold standard
data created by expert annotators. Second, they col-
lected a staggering number of labels for a very small
amount of money. They collected 21,000 labels for
just over $25. Turkers put in over 140+ hours worth
Why do you complete tasks in MTurk? US India
To spend free time fruitfully and get
cash (e.g., instead of watching TV)
70% 60%
For ?primary? income purposes (e.g.,
gas, bills, groceries, credit cards)
15% 27%
For ?secondary? income purposes,
pocket change (for hobbies, gadgets)
60% 37%
To kill time 33% 5%
The tasks are fun 40% 20%
Currently unemployed or part time work 30% 27%
Table 1: Motivations for participating on Mechanical
Turk from a survey of 1,000 Turkers by Ipeirotis (2010).
of human effort to generate the labels. The amount
of participation is surprisingly high, given the small
payment.
Turker demographics
Given the amount of work that can get done for so
little, it is natural to ask: who would contribute so
much work for so little pay, and why? The answers
to these questions are often mysterious because
Amazon does not provide any personal informa-
tion about Turkers (each Turker is identifiable only
through a serial number like A23KO2TP7I4KK2).
Ipeirotis (2010) elucidates some of the reasons by
presenting a demographic analysis of Turkers. He
built a profile of 1000 Turkers by posting a survey to
MTurk and paying $0.10 for people to answer ques-
tions about their reasons for participating on Me-
chanical Turk, the amount that they earn each week,
and how much time they spend, as well as demo-
graphic information like country of origin, gender,
age, education level, and household income.
One suspicion that people often have when they
first hear about MTurk is that it is some sort of dig-
ital sweatshop that exploits workers in third world
countries. However, Ipeirotis reports that nearly half
2
(47%) of the Turkers who answered his survey were
from the United States, with the next largest group
(34%) coming from India, and the remaining 19%
spread between 66 other countries.
Table 1 gives the survey results for questions
relating to why people participate on Mechanical
Turk. It shows that most US-based workers use Me-
chanical Turk for secondary income purposes (to
have spending money for hobbies or going out),
but that the overwhelming majority of them use
it to spend their time more fruitfully (i.e., instead
of watching TV). The economic downturn may
have increased participation, with 30% of the US-
based Turkers reporting that they are unemployed
or underemployed. The public radio show Mar-
ketplace recently interviewed unemployed Turkers
(Rose, 2010). It reports that they earn a little in-
come, but that they do not earn enough to make a
living. Figure 1 confirms this, giving a break down
of how much time people spend on Mechanical Turk
each week, how many HITs they complete, and how
much money they earn. Most Turkers spend less
than 8 hours per week on Mechanical Turk, and earn
less than $10 per week through the site.
3 Quality Control
Ipeirotis (2010) reports that just over half of Turkers
have a college education. Despite being reasonably
well educated, it is important to keep in mind that
Turkers do not have training in specialized subjects
like NLP. Because the Turkers are non-experts, and
because the payments are generally so low, quality
control is an important consideration when creating
data with MTurk.
Amazon provides three mechanisms to help en-
sure quality:
? Requesters have the option of rejecting the
work of individual Turkers, in which case they
are not paid.2 Turkers can also be blocked from
doing future work for a requester.
2Since the results are downloadable even if they are rejected,
this could allow unscrupulous Requesters to abuse Turkers by
rejecting all of their work, even if it was done well. Turkers have
message boards at http://www.turkernation.com/,
where they discuss Requesters. They even have a Firefox plu-
gin called Turkopticon that lets them see ratings of how good
the Requesters are in terms of communicating with Turkers, be-
ing generous and fair, and paying promptly.
? Requesters can specify that each HIT should
be redundantly completed by several different
Turkers. This allows higher quality labels to
be selected, for instance, by taking the majority
label.
? Requesters can require that all workers meet
a particular set of qualifications, such as suffi-
cient accuracy on a small test set or a minimum
percentage of previously accepted submissions.
Amazon provides two qualifications that a Re-
quester can use by default. These are past HIT Ap-
proval Rate and Location. The location qualifica-
tion allows the Requester to have HITs done only by
residents of a certain country (or to exclude Turk-
ers from certain regions). Additionally, Requesters
can design custom Qualification Tests that Turkers
must complete before working on a particular HIT.
These can be created through the MTurk API, and
can either be graded manually or automatically. An
important qualification that isn?t among Amazon?s
default qualifications is language skills. One might
design a qualification test to determine a Turker?s
ability to speak Arabic or Farsi before allowing them
to do part of speech tagging in those languages, for
instance.
There are several reasons that poor quality data
might be generated. The task may be too complex or
the instructions might not be clear enough for Turk-
ers to follow. The financial incentives may be too
low for Turkers to act conscientiously, and certain
HIT designs may allow them to simply randomly
click instead of thinking about the task. Mason and
Watts (2009) present a study of financial incentives
on Mechanical Turk and find, counterintuitively, that
increasing the amount of compensation for a partic-
ular task does not tend to improve the quality of the
results. Anecdotally, we have observed that some-
times there is an inverse relationship between the
amount of payment and the quality of work, because
it is more tempting to cheat on high-paying HITs if
you don?t have the skills to complete them. For ex-
ample, a number of Turkers tried to cheat on an Urdu
to English translation HIT by cutting-and-pasting
the Urdu text into an online machine translation sys-
tem (expressly forbidden in the instructions) because
we were paying the comparatively high amount of
$1.
3
3.1 Designing HITs for quality control
We suggest designing your HITs in a way that will
deter cheating or that will make cheating obvious.
HIT design is part of the art of using MTurk. It
can?t be easily quantified, but it has a large impact on
the outcome. For instance, we reduced cheating on
our translation HIT by changing the design so that
we displayed images of the Urdu sentences instead
of text, which made it impossible to copy-and-paste
into an MT system for anyone who could not type in
Arabic script.
Another suggestion is to include information
within the data that you upload to MTurk that will
not be displayed to the Turkers, but will be useful
to you when reviewing the HITs. For example, we
include machine translation output along with the
source sentences. Although this is not displayed to
Turkers, when we review the Turkers? translations
we compare them to the MT output. This allows us
to reject translations that are identical to the MT, or
which are just random sentences that are unrelated to
the original Urdu. We also use a javascript3 to gather
the IP addresses of the Turkers and do geolocation
to look up their location. Turkers in Pakistan require
less careful scrutiny since they are more likely to be
bilingual Urdu speakers than those in Romania, for
instance.
CrowdFlower4 provides an interface for design-
ing HITs that includes a phase for the Requester to
input gold standard data with known labels. Insert-
ing items with known labels alongside items which
need labels allows a Requester to see which Turkers
are correctly replicating the gold standard labels and
which are not. This is an excellent idea. If it is possi-
ble to include positive and negative controls in your
HITs, then do so. Turkers who fail the controls can
be blocked and their labels can be excluded from the
final data set. CrowdFlower-generated HITs even
display a score to the Turkers to give them feedback
on how well they are doing. This provides training
for Turkers, and discourages cheating.
3http://wiki.github.com/callison-burch/
mechanical_turk_workshop/geolocation
4http://crowdflower.com/
3.2 Iterative improvements on MTurk
Another class of quality control on Mechanical Turk
is through iterative HITs that build on the output of
previous HITs. This could be used to have Turkers
judge whether the results from a previous HIT con-
formed to the instructions, and whether it is of high
quality. Alternately, the second set of Turkers could
be used to improve the quality of what the first Turk-
ers created. For instance, in a translation task, a sec-
ond set of US-based Turkers could edit the English
produced by non-native speakers.
CastingWords,5 a transcription company that uses
Turker labor, employs this strategy by having a first-
pass transcription graded and iteratively improved
in subsequent passes. Little et al (2009) even de-
signed an API specifically for running iterative tasks
on MTurk.6
4 Recommended Practices
Although it is hard to define a set of ?best practices?
that applies to all HITs, or even to all NLP HITs, we
recommend the following guidelines to Requesters.
First and foremost, it is critical to convey instruc-
tions appropriately for non-experts. The instructions
should be clear and concise. To calibrate whether
the HIT is doable, you should first try the task your-
self, and then have a friend from outside the field try
it. This will help to ensure that the instructions are
clear, and to calibrate how long each HIT will take
(which ought to allow you to price the HITs fairly).
If possible, you should insert positive and nega-
tive controls so that you can quickly screen out bad
Turkers. This is especially important for HITs that
only require clicking buttons to complete. If pos-
sible, you should include a small amount of gold
standard data in each HIT. This will allow you to
determine which Turkers are good, but will also al-
low you weight the Turkers if you are combining
the judgments of multiple Turkers. If you are hav-
ing Turkers evaluate the output of systems, then ran-
domize the order that the systems are shown in.
When publishing papers that use Mechanical Turk
as a source of training data or to evaluate the output
of an NLP system, report how you ensured the qual-
ity of your data. You can do this by measuring the
5http://castingwords.com/
6http://groups.csail.mit.edu/uid/turkit/
4
inter-annotator agreement of the Turkers against ex-
perts on small amounts of gold standard data, or by
stating what controls you used and what criteria you
used to block bad Turkers. Finally, whenever possi-
ble you should publish the data that you generate on
Mechanical Turk (and your analysis scripts and HIT
templates) alongside your paper so that other people
can verify it.
5 Related work
In the past two years, several papers have published
about applying Mechanical Turk to a diverse set of
natural language processing tasks, including: cre-
ating question-answer sentence pairs (Kaisser and
Lowe, 2008), evaluating machine translation qual-
ity and crowdsouring translations (Callison-Burch,
2009), paraphrasing noun-noun compouds for Se-
mEval (Butnariu et al, 2009), human evaluation of
topic models (Chang et al, 2009), and speech tran-
scription (McGraw et al, 2010; Marge et al, 2010a;
Novotney and Callison-Burch, 2010a). Others have
used MTurk for novel research directions like non-
simulated active learning for NLP tasks such as sen-
timent classification (Hsueh et al, 2009) or doing
quixotic things like doing human-in-the-loop min-
imum error rate training for machine translation
(Zaidan and Callison-Burch, 2009).
Some projects have demonstrated the super-
scalability of crowdsourced efforts. Deng et al
(2009) used MTurk to construct ImageNet, an anno-
tated image database containing 3.2 million that are
hierarchically categorized using the WordNet ontol-
ogy (Fellbaum, 1998). Because Mechanical Turk
allows researchers to experiment with crowdsourc-
ing by providing small incentives to Turkers, other
successful crowdsourcing efforts like Wikipedia or
Games with a Purpose (von Ahn and Dabbish, 2008)
also share something in common with MTurk.
6 Shared Task
The workshop included a shared task in which par-
ticipants were provided with $100 to spend on Me-
chanical Turk experiments. Participants submitted a
1 page proposal in advance describing their intended
use of the funds. Selected proposals were provided
$100 seed money, to which many participants added
their own funds. As part of their participation, each
team submitted a workshop paper describing their
experiments as well as the data collected and de-
scribed in the paper. Data for the shared papers is
available at the workshop website.7
This section describes the variety of data types ex-
plored and collected in the shared task. Of the 24
participating teams, most did not exceed the $100
that they were awarded by a significant amount.
Therefore, the variety and extent of data described in
this section is the result of a minimal $2,400 invest-
ment. This achievement demonstrates the potential
for MTurk?s impact on the creation and curation of
speech and language corpora.
6.1 Traditional NLP Tasks
An established core set of computational linguistic
tasks have received considerable attention in the nat-
ural language processing community. These include
knowledge extraction, textual entailment and word
sense disambiguation. Each of these tasks requires a
large and carefully curated annotated corpus to train
and evaluate statistical models. Many of the shared
task teams attempted to create new corpora for these
tasks at substantially reduced costs using MTurk.
Parent and Eskenazi (2010) produce new corpora
for the task of word sense disambiguation. The
study used MTurk to create unique word definitions
for 50 words, which Turkers then also mapped onto
existing definitions. Sentences containing these 50
words were then assigned to unique definitions ac-
cording to word sense.
Madnani and Boyd-Graber (2010) measured the
concept of transitivity of verbs in the style of Hop-
per and Thompson (1980), a theory that goes beyond
simple grammatical transitivity ? whether verbs take
objects (transitive) or not ? to capture the amount of
action indicated by a sentence. Videos that portrayed
verbs were shown to Turkers who described the ac-
tions shown in the video. Additionally, sentences
containing the verbs were rated for aspect, affirma-
tion, benefit, harm, kinesis, punctuality, and volition.
The authors investigated several approaches for elic-
iting descriptions of transitivity from Turkers.
Two teams explored textual entailment tasks.
Wang and Callison-Burch (2010) created data for
7http://sites.google.com/site/
amtworkshop2010/
5
recognizing textual entailment (RTE). They submit-
ted 600 text segments and asked Turkers to identify
facts and counter-facts (unsupported facts and con-
tradictions) given the provided text. The resulting
collection includes 790 facts and 203 counter-facts.
Negri and Mehdad (2010) created a bi-lingual en-
tailment corpus using English and Spanish entail-
ment pairs, where the hypothesis and text come from
different languages. The authors took a publicly
available English RTE data set (the PASCAL-RTE3
dataset1) and created an English-Spanish equivalent
by having Turkers translating the hypotheses into
Spanish. The authors include a timeline of their
progress, complete with total cost over the 10 days
that they ran the experiments.
In the area of natural language generation, Heil-
man and Smith (2010) explored the potential of
MTurk for ranking of computer generated questions
about provided texts. These questions can be used to
test reading comprehension and understanding. 60
Wikipedia articles were selected, for each of which
20 questions were generated. Turkers provided 5 rat-
ings for each of the 1,200 questions, creating a sig-
nificant corpus of scored questions.
Finally, Gordon et al (2010) relied on MTurk to
evaluate the quality and accuracy of automatically
extracted common sense knowledge (factoids) from
news and Wikipedia articles. Factoids were pro-
vided by the KNEXT knowledge extraction system.
6.2 Speech and Vision
While MTurk naturally lends itself to text tasks,
several teams explored annotation and collection of
speech and image data. We note that one of the pa-
pers in the main track described tools for collecting
such data (Lane et al, 2010).
Two teams used MTurk to collect text annotations
on speech data. Marge et al (2010b) identified easy
and hard sections of meeting speech to transcribe
and focused data collection on difficult segments.
Transcripts were collected on 48 audio clips from
4 different speakers, as well as other types of an-
notations. Kunath and Weinberger (2010) collected
ratings of accented English speech, in which non-
native speakers were rated as either Arabic, Man-
darin or Russian native speakers. The authors ob-
tained multiple annotations for each speech sample,
and tracked the native language of each annotator,
allowing for an analysis of rating accuracy between
native English and non-native English annotators.
Novotney and Callison-Burch (2010b) used
MTurk to elicit new speech samples. As part of an
effort to increase the accessibility of public knowl-
edge, such as Wikipedia, the team prompted Turkers
to narrate Wikipedia articles. This required Turkers
to record audio files and upload them. An additional
HIT was used to evaluate the quality of the narra-
tions.
A particularly creative data collection approach
asked Turkers to create handwriting samples and
then to submit images of their writing (Tong et al,
2010). Turkers were asked to submit handwritten
shopping lists (large vocabulary) or weather descrip-
tions (small vocabulary) in either Arabic or Spanish.
Subsequent Turkers provided a transcription and a
translation. The team collected 18 images per lan-
guage, 2 transcripts per image and 1 translation per
transcript.
6.3 Sentiment, Polarity and Bias
Two papers investigated the topics of sentiment, po-
larity and bias. Mellebeek et al (2010) used several
methods to obtain polarity scores for Spanish sen-
tences expressing opinions about automative topics.
They evaluated three HITs for collecting such data
and compared results for quality and expressiveness.
Yano et al (2010) evaluated the political bias of blog
posts. Annotators labeled 1000 sentences to deter-
mine biased phrases in political blogs from the 2008
election season. Knowledge of the annotators own
biases allowed the authors to study how bias differs
on the different ends of the political spectrum.
6.4 Information Retrieval
Large scale evaluations requiring significant human
labor for evaluation have a long history in the in-
formation retrieval community (TREC). Grady and
Lease (2010) study four factors that influence Turker
performance on a document relevance search task.
The authors present some negative results on how
these factors influence data collection. For further
work on MTurk and information retrieval, readers
are encouraged to see the SIGIR 2010 Workshop on
Crowdsourcing for Search Evaluation.8
8http://www.ischool.utexas.edu/?cse2010/
call.htm
6
6.5 Information Extraction
Information extraction (IE) seeks to identify specific
types of information in natural languages. The IE
papers in the shared tasks focused on new domains
and genres as well as new relation types.
The goal of relation extraction is to identify rela-
tions between entities or terms in a sentence, such as
born in or religion. Gormley et al (2010) automat-
ically generate potential relation pairs in sentences
by finding relation pairs appearing in news articles
as given by a knowledge base. They ask Turkers if
a sentence supports a relation, does not support a re-
lation, or whether the relation makes sense. They
collected close to 2500 annotations for 17 different
person relation types.
The other IE papers explored new genres and do-
mains. Finin et al (2010) obtained named entity an-
notations (person, organization, geopolitical entity)
for several hundred Twitter messages. They con-
ducted experiments using both MTurk and Crowd-
Flower. Yetisgen-Yildiz et al (2010) explored
medical named entity recognition. They selected
100 clinical trial announcements from ClinicalTri-
als.gov. 4 annotators for each of the 100 announce-
ments identified 3 types of medical entities: medical
conditions, medications, and laboratory test.
6.6 Machine Translation
The most popular shared task topic was Machine
Translation (MT). MT is a data hungry task that re-
lies on huge corpora of parallel texts between two
languages. Performance of MT systems depends
on the size of training corpora, so there is a con-
stant search for new and larger data sets. Such data
sets are traditionally expensive to produce, requiring
skilled translators. One of the advantages to MTurk
is the diversity of the Turker population, making it
an especially attractive source of MT data. Shared
task papers in MT explored the full range of MT
tasks, including alignments, parallel corpus creation,
paraphrases and bilingual lexicons.
Gao and Vogel (2010) create alignments in a 300
sentence Chinese-English corpus (Chinese aligned
to English). Both Ambati and Vogel (2010) and
Bloodgood and Callison-Burch (2010) explore the
potential of MTurk in the creation of MT paral-
lel corpora for evaluation and training. Bloodgood
and Callison-Burch replicate the NIST 2009 Urdu-
English test set of 1792 sentences, paying only $0.10
a sentence, a substantially reduced price than the
typical annotator cost. The result is a data set that is
still effective for comparing MT systems in an eval-
uation. Ambati and Vogel create corpora with 100
sentences and 3 translations per sentence for all the
language pairs between English, Spanish, Urdu and
Telugu. This demonstrates the feasibility of creating
cheap corpora for high and low resource languages.
Two papers focused on the creation and evalua-
tion of paraphrases. Denkowski et al (2010) gen-
erated and evaluated 728 paraphrases for Arabic-
English translation. MTurk was used to identify
correct and fix incorrect paraphrases. Over 1200
high quality paraphrases were created. Buzek et
al. (2010) evaluated error driven paraphrases for
MT. In this setting, paraphrases are used to sim-
plify potentially difficult to translate segments of
text. Turkers identified 1780 error regions in 1006
English/Chinese sentences. Turkers provided 4821
paraphrases for these regions.
External resources can be an important part of an
MT system. Irvine and Klementiev (2010) created
lexicons for low resource languages. They evaluated
translation candidates for 100 English words in 32
languages and solicited translations for 10 additional
languages. Higgins et al (2010) expanded name
lists in Arabic by soliciting common Arabic nick-
names. The 332 collected nicknames were primar-
ily provided by Turkers in Arab speaking countries
(35%), India (46%), and the United States (13%).
Finally, Zaidan and Ganitkevitch (2010) explored
how MTurk could be used to directly improve an MT
grammar. Each rule in an Urdu to English transla-
tion system was characterized by 12 features. Turk-
ers were provided examples for which their feed-
back was used to rescore grammar productions di-
rectly. This approach shows the potential of fine
tuning an MT system with targeted feedback from
annotators.
7 Future Directions
Looking ahead, we can?t help but wonder what im-
pact MTurk and crowdsourcing will have on the
speech and language research community. Keep-
ing in mind Niels Bohr?s famous exhortation ?Pre-
7
diction is very difficult, especially if it?s about the
future,? we attempt to draw some conclusions and
predict future directions and impact on the field.
Some have predicted that access to low cost,
highly scalable methods for creating language and
speech annotations means the end of work on un-
supervised learning. Many a researcher has advo-
cated his or her unsupervised learning approach be-
cause of annotation costs. However, if 100 exam-
ples for any task are obtainable for less than $100,
why spend the time and effort developing often infe-
rior unsupervised methods? Such a radical change is
highly debatable, in fact, one of this paper?s authors
is a strong advocate of such a position while the
other disagrees, perhaps because he himself works
on unsupervised methods. Certainly, we can agree
that the potential exists for a change in focus in a
number of ways.
In natural language processing, data drives re-
search. The introduction of new large and widely
accessible data sets creates whole new areas of re-
search. There are many examples of such impact,
the most famous of which is the Penn Treebank
(Marcus. et al, 1994), which has 2910 citations in
Google scholar and is the single most cited paper
on the ACL anthology network (Radev et al, 2009).
Other examples include the CoNLL named entity
corpus (Sang and Meulder (2003) with 348 citations
on Google Scholar), the IMDB movie reviews senti-
ment data (Pang et al (2002) with 894 citations) and
the Amazon sentiment multi-domain data (Blitzer et
al. (2007) with 109 citations) . MTurk means that
creating similar data sets is now much cheaper and
easier than ever before. It is highly likely that new
MTurk produced data sets will achieve prominence
and have significant impact. Additionally, the cre-
ation of shared data means more comparison and
evaluation against previous work. Progress is made
when it can be demonstrated against previous ap-
proaches on the same data. The reduction of data
cost and the rise of independent corpus producers
likely means more accessible data.
More than a new source for cheap data, MTurk is
a source for new types of data. Several of the pa-
pers in this workshop collected information about
the annotators in addition to their annotations. This
creates potential for studying how different user de-
mographics understand language and allow for tar-
geting specific demographics in data creation. Be-
yond efficiencies in cost, MTurk provides access to
a global user population far more diverse than those
provided by more professional annotation settings.
This will have a significant impact on low resource
languages as corpora can be cheaply built for a much
wider array of languages. As one example, Irvine
and Klementiev (2010) collected data for 42 lan-
guages without worrying about how to find speak-
ers of such a wide variety of languages. Addition-
ally, the collection of Arabic nicknames requires a
diverse and numerous Arabic speaking population
(Higgins et al, 2010). In addition to extending into
new languages, MTurk also allows for the creation
of evaluation sets in new genres and domains, which
was the focus of two papers in this workshop (Finin
et al, 2010; Yetisgen-Yildiz et al, 2010). We ex-
pect to see new research emphasis on low resource
languages and new domains and genres.
Another factor is the change of data type and its
impact on machine learning algorithms. With pro-
fessional annotators, great time and care are paid to
annotation guidelines and annotator training. These
are difficult tasks with MTurk, which favors simple
intuitive annotations and little training. Many papers
applied creative methods of using simpler annota-
tion tasks to create more complex data sets. This
process can impact machine learning in a number
of ways. Rather than a single gold standard, anno-
tations are now available for many users. Learn-
ing across multiple annotations may improve sys-
tems (Dredze et al, 2009). Additionally, even with
efforts to clean up MTurk annotations, we can ex-
pect an increase in noisy examples in data. This will
push for new more robust learning algorithms that
are less sensitive to noise. If we increase the size
of the data ten-fold but also increase the noise, can
learning still be successful? Another learning area
of great interest is active learning, which has long
relied on simulated user experiments. New work
evaluated active learning methods with real users us-
ing MTurk (Baker et al, 2009; Ambati et al, 2010;
Hsueh et al, 2009; ?). Finally, the composition of
complex data set annotations from simple user in-
puts can transform the method by which we learn
complex outputs. Current approaches expect exam-
ples of labels that exactly match the expectation of
the system. Can we instead provide lower level sim-
8
pler user annotations and teach systems how to learn
from these to construct complex output? This would
open more complex annotation tasks to MTurk.
A general trend in research is that good ideas
come from unexpected places. Major transforma-
tions in the field have come from creative new ap-
proaches. Consider the Penn Treebank, an ambitious
and difficult project of unknown potential. Such
large changes can be uncommon since they are often
associated with high cost, as was the Penn Treebank.
However, MTurk greatly reduces these costs, en-
couraging researchers to try creative new tasks. For
example, in this workshop Tong et al (2010) col-
lected handwriting samples in multiple languages.
Their creative data collection may or may not have
a significant impact, but it is unlikely that it would
have been tried had the cost been very high.
Finally, while obtaining new data annotations
from MTurk is cheap, it is not trivial. Workshop par-
ticipants struggled with how to attract Turkers, how
to price HITs, HIT design, instructions, cheating de-
tection, etc. No doubt that as work progresses, so
will a communal knowledge and experience of how
to use MTurk. There can be great benefit in new
toolkits for collecting language data using MTurk,
and indeed some of these have already started to
emerge (Lane et al, 2010)9.
Acknowledgements
Thanks to Sharon Chiarella of Amazon?s Mechan-
ical Turk for providing $100 credits for the shared
task, and to CrowdFlower for allowing free use of
their tool to workshop participants.
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are the
authors? alone.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk for
subjectivity word sense disambiguation. In NAACL
9http://wiki.github.com/callison-burch/
mechanical_turk_workshop/
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In NAACL Workshop on Creating Speech and Lan-
guage Data With Amazon?s Mechanical Turk.
Vamshi Ambati, Stephan Vogel, and Jamie Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. Language Resources and Evalua-
tion (LREC).
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically-informed machine translation.
Technical Report 002, Johns Hopkins Human Lan-
guage Technology Center of Excellence, Summer
Camp for Applied Language Exploration, Johns Hop-
kins University, Baltimore, MD.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Michael Bloodgood and Chris Callison-Burch. 2010a.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In 48th
Annual Meeting of the Association for Computational
Linguistics, Uppsala, Sweden.
Michael Bloodgood and Chris Callison-Burch. 2010b.
Using Mechanical Turk to build machine translation
evaluation sets. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechan-
ical Turk.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2009. Semeval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Workshop on Semantic Evaluations.
Olivia Buzek, Philip Resnik, and Ben Bederson. 2010.
Error driven paraphrase annotation using Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazon?s mechan-
ical turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2009), Singapore.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems.
9
Jonathan Chang. 2010. Not-so-Latent Dirichlet Allo-
cation: Collapsed Gibbs sampling using human judg-
ments. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR, Miami Beach, Floriday.
Michael Denkowski and Alon Lavie. 2010. Explor-
ing normalization techniques for human judgments of
machine translation adequacy collected using Amazon
Mechanical Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechani-
cal Turk.
Michael Denkowski, Hassan Al-Haj, and Alon Lavie.
2010. Turker-assisted paraphrasing for English-
Arabic machine translation. In NAACL Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multiple
labels. In ECML/PKDD Workshop on Learning from
Multi-Label Data (MLD).
Keelan Evanini, Derrick Higgins, and Klaus Zechner.
2010. Using Amazon Mechanical Turk for transcrip-
tion of non-native speech. In NAACL Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Touring PER, ORG and LOC on $100 a day. In
NAACL Workshop on Creating Speech and Language
Data With Amazon?s Mechanical Turk.
Qin Gao and Stephan Vogel. 2010. Semi-supervised
word alignment with Mechanical Turk. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Dan Gillick and Yang Liu. 2010. Non-expert evaluation
of summarization systems is risky. In NAACL Work-
shop on Creating Speech and Language Data With
Amazon?s Mechanical Turk.
Jonathan Gordon, Benjamin Van Durme, and Lenhart
Schubert. 2010. Evaluation of commonsense knowl-
edge with Mechanical Turk. In NAACL Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk.
Matthew R. Gormley, Adam Gerber, Mary Harper, and
Mark Dredze. 2010. Non-expert correction of auto-
matically generated relation annotations. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Catherine Grady and Matthew Lease. 2010. Crowd-
sourcing document relevance assessment with Me-
chanical Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechan-
ical Turk.
Michael Heilman and Noah A. Smith. 2010. Rating
computer-generated questions with Mechanical Turk.
In NAACL Workshop on Creating Speech and Lan-
guage Data With Amazon?s Mechanical Turk.
Chiara Higgins, Elizabeth McGrath, and Laila Moretto.
2010. AMT crowdsourcing: A viable method for rapid
discovery of Arabic nicknames? In NAACL Workshop
on Creating Speech and Language Data With Ama-
zon?s Mechanical Turk.
Paul J. Hopper and Sandra A. Thompson. 1980. Transi-
tivity in grammar and discourse. Language, 56:251?
299.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: A study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27?35, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Panos Ipeirotis. 2010. New demographics of Mechanical
Turk. http://behind-the-enemy-lines.
blogspot.com/2010/03/
new-demographics-of-mechanical-turk.
html.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechan-
ical Turk.
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-
thal, and Kathleen McKeown. 2010. Corpus creation
for new genres: A crowdsourced approach to PP at-
tachment. In NAACL Workshop on Creating Speech
and Language Data With Amazon?s Mechanical Turk.
Michael Kaisser and John Lowe. 2008. Creating a re-
search collection of question answer sentence pairs
with Amazons Mechanical Turk. In Proceedings of
the Sixth International Language Resources and Eval-
uation (LREC?08), Marrakech, Morocco.
Stephen Kunath and Steven Weinberger. 2010. The wis-
dom of the crowd?s ear: Speech accent rating and an-
notation with Amazon Mechanical Turk. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Ian Lane, Matthias Eck, Kay Rottmann, and Alex
Waibel. 2010. Tools for collecting speech corpora via
Mechanical-Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechani-
cal Turk.
10
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen Yildiz. 2010. Annotating large email
datasets for named entity recognition with Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-
man. 2009. Turkit: Tools for iterative tasks on me-
chanical turk. In Proceedings of the Workshop on
Human Computation at the International Conference
on Knowledge Discovery and Data Mining (KDD-
HCOMP ?09), Paris.
Nitin Madnani and Jordan Boyd-Graber. 2010. Measur-
ing transitivity using untrained annotators. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Mitch Marcus., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational lin-
guistics, 19(2):313?330.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010a. Using the Amazon Mechanical
Turk for transcription of spoken language. ICASSP,
March.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010b. Using the Amazon Mechanical
Turk to transcribe and annotate meeting speech for ex-
tractive summarization. In NAACL Workshop on Cre-
ating Speech and Language Data With Amazon?s Me-
chanical Turk.
Winter Mason and Duncan J. Watts. 2009. Financial
incentives and the ?performance of crowds?. In Pro-
ceedings of the Workshop on Human Computation at
the International Conference on Knowledge Discovery
and Data Mining (KDD-HCOMP ?09), Paris.
Ian McGraw, Chia ying Lee, Lee Hetherington, and Jim
Glass. 2010. Collecting voices from the crowd.
LREC, May.
Bart Mellebeek, Francesc Benavent, Jens Grivolla, Joan
Codina, Marta R. Costa-Jussa`, and Rafael Banchs.
2010. Opinion mining of spanish customer comments
with non-expert annotations on Mechanical Turk. In
NAACL Workshop on Creating Speech and Language
Data With Amazon?s Mechanical Turk.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,
Tyler Schnoebelen, and Harry Tily. 2010. Crowd-
sourcing and language studies: the new generation
of linguistic data. In NAACL Workshop on Creating
Speech and Language Data With Amazon?s Mechani-
cal Turk.
Matteo Negri and Yashar Mehdad. 2010. Creating a
bi-lingual entailment corpus through translations with
Mechanical Turk: $100 for a 10 days rush. In NAACL
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk.
Scott Novotney and Chris Callison-Burch. 2010a.
Cheap, fast and good enough: Automatic speech
recognition with non-expert transcription. NAACL,
June.
Scott Novotney and Chris Callison-Burch. 2010b.
Crowdsourced accessibility: Elicitation of Wikipedia
articles. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Empirical Methods in
Natural Language Processing (EMNLP).
Gabriel Parent and Maxine Eskenazi. 2010. Cluster-
ing dictionary definitions using Amazon Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network. In Pro-
ceedings of the 2009 Workshop on Text and Citation
Analysis for Scholarly Digital Libraries, pages 54?61,
Suntec City, Singapore, August. Association for Com-
putational Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia
Hockenmaier. 2010. Collecting image annotations us-
ing Amazon?s Mechanical Turk. In NAACL Workshop
on Creating Speech and Language Data With Ama-
zon?s Mechanical Turk.
Joel Rose. 2010. Some turn to ?Mechanical? job search.
http://marketplace.publicradio.org/
display/web/2009/06/30/pm_turking/.
Marketplace public radio. Air date: June 30, 2009.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In CoNLL-
2003.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2008), Honolulu, Hawaii.
Audrey Tong, Jerome Ajot, Mark Przybocki, and
Stephanie Strassel. 2010. Document image collection
using Amazon?s Mechanical Turk. In NAACL Work-
shop on Creating Speech and Language Data With
Amazon?s Mechanical Turk.
Luis von Ahn and Laura Dabbish. 2008. General tech-
niques for designing games with a purpose. Commu-
nications of the ACM.
Rui Wang and Chris Callison-Burch. 2010. Cheap facts
and counter-facts. In NAACL Workshop on Creating
11
Speech and Language Data With Amazon?s Mechani-
cal Turk.
Tae Yano, Philip Resnik, and Noah A Smith. 2010.
Shedding (a thousand points of) light on biased lan-
guage. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Meliha Yetisgen-Yildiz, Imre Solti, Scott Halgrim, and
Fei Xia. 2010. Preliminary experiments with Ama-
zon?s Mechanical Turk for annotating medical named
entities. In NAACL Workshop on Creating Speech and
Language Data With Amazon?s Mechanical Turk.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of EMNLP 2009, pages 52?61,
August.
Omar Zaidan and Juri Ganitkevitch. 2010. An enriched
MT grammar for under $100. In NAACL Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk.
12
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 80?88,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Annotating Named Entities in Twitter Data with Crowdsourcing
Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller and Justin Martineau
Computer Science and Electrical Engineering
University of Maryland, Baltimore County
Baltimore MD 21250
(finin,willm1,anandk1,nick6,jm1)@umbc.edu
Mark Dredze
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore MD 21211
mdredze@cs.jhu.edu
Abstract
We describe our experience using both Ama-
zon Mechanical Turk (MTurk) and Crowd-
Flower to collect simple named entity anno-
tations for Twitter status updates. Unlike most
genres that have traditionally been the focus of
named entity experiments, Twitter is far more
informal and abbreviated. The collected anno-
tations and annotation techniques will provide
a first step towards the full study of named en-
tity recognition in domains like Facebook and
Twitter. We also briefly describe how to use
MTurk to collect judgements on the quality of
?word clouds.?
1 Introduction and Dataset Description
Information extraction researchers commonly work
on popular formal domains, such as news arti-
cles. More diverse studies have included broadcast
news transcripts, blogs and emails (Strassel et al,
2008). However, extremely informal domains, such
as Facebook, Twitter, YouTube or Flickr are start-
ing to receive more attention. Any effort aimed at
studying these informal genres will require at least a
minimal amount of labeled data for evaluation pur-
poses.
This work details how to efficiently annotate large
volumes of data, for information extraction tasks, at
low cost using MTurk (Snow et al, 2008; Callison-
Burch, 2009). This paper describes a case study for
information extraction tasks involving short, infor-
mal messages from Twitter. Twitter is a large multi-
user site for broadcasting short informal messages.
Twitter is an extreme example of an informal genre
(Java et al, 2007) as users frequently abbreviate
their posts to fit within the specified limit. Twitter
is a good choice because it is very popular: Twitter
users generate a tremendous number of status up-
dates (tweets) every day1. This is a good genre to
work on named entity extraction since many tweets
refer to and contain updates about named entities.
Our Twitter data set has over 150 million tweets
from 1.5 million users collected over a period of
three years. Tweets are unlike formal text. They are
limited to a maximum of 140 characters, a limit orig-
inally set to allow them to fit into an SMS message.
Consequently, the use of acronyms and both stan-
dard and non-standard abbreviations (e.g., b4 for be-
fore and ur for your) are very common. Tweets tend
to be telegraphic and often consist of sentence frag-
ments or other ungrammatical sequences. Normal
capitalization rules (e.g., for proper names, book ti-
tles, etc.) are commonly ignored.
Furthermore, users have adopted numerous con-
ventions including hashtags, user mentions, and
retweet markers. A hashtag (e.g., #earthquake) is
a token beginning with a ?#? character that denotes
one of the topic of a status. Hashtags can be used as
pure metadata or serve both as a word and as meta-
data, as the following two examples show.
? EvanEcullen: #chile #earthquake #tsunami They
heard nothing of a tsunami until it slammed into
their house with an unearthly http://tl.gd/d798d
? LarsVonD: Know how to help #Chile after the
#Earthquake
1Pingdom estimated that there were nearly 40 million tweets
a day in January 2010 (pingdom.com, 2010).
80
(1) report from the economist: #chile counts the cost
of a devastating earthquake and makes plans for re-
covery. http://bit.ly/dwoQMD
Note: ?the economist? was not recognized as an
ORG.
(2) how come when george bush wanted to take out
millions for the war congress had no problem...but
whe obama wants money for healthcare the ...
Note: Both ?george bush? and ?obama? were missed
as PERs.
(3) RT @woodmuffin: jay leno interviewing sarah
palin: the seventh seal starts to show a few cracks
Note: RT (code for a re-tweet) was mistaken as a po-
sition and sarah palin missed as a person.
Table 1: Standard named entity systems trained on text from
newswire articles and other well formed documents lose accu-
racy when applied to short status updates.
The Twitter community also has a convention where
user names preceded by an @ character (known as
?mentions?) at the beginning of a status indicate that
it is a message directed at that user. A user mention
in the middle of a message is interpreted as a general
reference to that user. Both uses are shown in this
status:
? paulasword: @obama quit calling @johnboener a
liar, you liar
The token RT is used as a marker that a person is for-
warding a tweet originally sent by another user. Nor-
mally the re-tweet symbol begins the message and
is immediately followed by the user mention of the
original author or sometimes a chain of re-tweeters
ending with the original author, as in
? politicsiswar: RT @KatyinIndy @SamiShamieh:
Ghost towns on rise under Obama
http://j.mp/cwJSUg #tcot #gop (Deindustrial-
ization of U.S.- Generation Zero)
Finally, ?smileys? are common in Twitter statuses to
signal the users? sentiment, as in the following.
? sallytherose: Just wrote a 4-page paper in an hour
and a half. BOiiiiii I?m getting good at this. :) Left-
over Noodles for dinner as a reward. :D
The Twitter search service also uses these to retrieve
tweets matching a query with positive or negative
sentiment.
Typical named entity recognition systems have
been trained on formal documents, such as news
Figure 1: Our Twitter collection is stored in a relational
database and also in the Lucene information retrieval system.
wire articles. Their performance on text from very
different sources, especially informal genres such as
Twitter tweets or Facebook status updates, is poor.
In fact, ?Systems analyzing correctly about 90% of
the sequences from a journalistic corpus can have a
decrease of performance of up to 50% on more in-
formal texts.? (Poibeau and Kosseim, 2001) How-
ever, many large scale information extraction sys-
tems require extracting and integrating useful in-
formation from online social networking sources
that are informal such as Twitter, Facebook, Blogs,
YouTube and Flickr.
To illustrate the problem we applied both the
NLTK (Bird et al, 2009) and the Stanford named
entity recognizers (Finkel et al, 2005) without re-
training to a sample Twitter dataset with mixed re-
sults. We have observed many failures, both false
positives and false negatives. Table 1 shows some
examples of these.
2 Task design
We developed separate tasks on CrowdFlower and
MTurk using a common collection of Twitter sta-
tuses and asked workers to perform the same anno-
tation task in order to fully understand the features
that each provides, and to determine the total amount
of work necessary to produce a result on each ser-
vice. MTurk has the advantage of using standard
HTML and Javascript instead of CrowdFlower?s
CML. However MTurk has inferior data verifica-
tion, in that the service only provides a threshold
on worker agreement as a form of quality control.
81
This is quite poor when tasks are more complicated
than a single boolean judgment, as with the case at
hand. CrowdFlower works across multiple services
and does verification against gold standard data, and
can get more judgements to improve quality in cases
where it?s necessary.
3 Annotation guidelines
The task asked workers to look at Twitter individ-
ual status messages (tweets) and use a toggle but-
ton to tag each word with person (PER), organiza-
tion (ORG), location (LOC), or ?none of the above?
(NONE). Each word also had a check box (labeled
???) to indicate that uncertainty. We provided the
workers with annotation guidelines adapted from the
those developed by the Linguistic Data Consortium
(Linguistic Data Consortium ? LCTL Team, 2006)
which were in turn based on guidelines used for
MUC-7 (Chinchor and Robinson, 1997).
We deliberately kept our annotation goals simple:
We only asked workers to identify three basic types
of named entities.
Our guidelines read:
An entity is a object in the world like a place
or person and a named entity is a phrase that
uniquely refers to an object by its proper name
(Hillary Clinton), acronym (IBM), nickname
(Opra) or abbreviation (Minn.).
Person (PER) entities are limited to humans
(living, deceased, fictional, deities, ...) iden-
tified by name, nickname or alias. Don?t in-
clude titles or roles (Ms., President, coach).
Include suffix that are part of a name (e.g., Jr.,
Sr. or III).
Organization (ORG) entities are limited to
corporations, institutions, government agen-
cies and other groups of people defined by
an established organizational structure. Some
examples are businesses (Bridgestone Sports
Co.), stock ticker symbols (NASDAQ), multi-
national organizations (European Union), po-
litical parties (GOP) non-generic government
entities (the State Department), sports teams
(the Yankees), and military groups (the Tamil
Tigers). Do not tag ?generic? entities like ?the
government? since these are not unique proper
names referring to a specific ORG.
Location (LOC) entities include names of
politically or geographically defined places
(cities, provinces, countries, international re-
gions, bodies of water, mountains, etc.). Lo-
cations also include man-made structures like
airports, highways, streets, factories and mon-
uments.
We instructed annotators to ignore other types of
named entities, e.g., events (World War II), products
(iPhone), animals (Cheetah), inanimate objects and
monetary units (the Euro) and gave them four prin-
ciples to follow when tagging:
? Tag words according to their meaning in the
context of the tweet.
? Only tag names, i.e., words that directly and
uniquely refer to entities.
? Only tag names of the types PER,ORG, and
LOC.
? Use the ??? checkbox to indicate uncertainty
in your tag.
3.1 Data selection
We created a ?gold standard? data set of about 400
tweets to train and screen workers on MTurk, to salt
the MTurk data with worker evaluation data, for use
on CrowdFlower, and to evaluate the performance
of the final NER system after training on the crowd-
sourced annotations. We preselected tweets to an-
notate using the NLTK named entity recognizer to
select statuses that were thought to contain named
entities of the desired types (PER, ORG, LOC).
Initial experiments suggested that a worker can
annotate about 400 tweets an hour. Based on this, we
loaded each MTurk Human Intelligence Tasks (HIT)
with five tweets, and paid workers five cents per HIT.
Thus, if we require that each tweet be annotated by
two workers, we would be able to produce about
4,400 raw annotated tweets with the $100 grant from
Amazon, accounting for their 10% overhead price.
3.2 CrowdFlower
We also experimented with CrowdFlower, a crowd-
sourcing service that uses various worker channels
like MTurk and SamaSource2 and provides an en-
hanced set of management and analytic tools. We
were interested in understanding the advantages and
disadvantages compared to using MTurk directly.
2http://www.samasource.org/
82
Figure 2: CrowdFlower is an enhanced service that feeds into
MTurk and other crowdsourcing systems. It provides conve-
nient management tools that show the performance of workers
for a task.
We prepared a basic front-end for our job using the
CrowdFlower Markup Language (CML) and custom
JavaScript. We used the CrowdFlower interface to
calibrate our job and to decide the pay rate. It con-
siders various parameters like amount of time re-
quired to complete a sample task and the desired ac-
curacy level to come up with a pay rate.
One attractive feature lets one provide a set of
?gold standard? tasks that pair data items with cor-
rect responses. These are automatically mixed into
the stream of regular tasks that workers process. If
a worker makes errors in one of these gold stan-
dard tasks, she gets immediate feedback about her
error and the correct answer is shown. CrowdFlower
claims that error rates are reduced by a factor of
two when gold standards are used(crowdflower.com,
2010). The interface shown in Figure 2 shows the
number of gold tasks the user has seen, and how
many they have gotten correct.
CrowdFlower?s management tools provides a de-
tailed analysis of the workers for a job, including
the trust level, accuracy and past accuracy history
associated with each worker. In addition, the output
records include the geographical region associated
with each worker, information that may be useful
for some tasks.
3.3 MTurk
The current iteration of our MTurk interface is
shown in Figure 3. Each tweet is shown at the top
of the HIT interface so that it can easily be read for
context. Then a table is displayed with each word
of the tweet down the side, and radio buttons to pick
Figure 3: In the MTurk interface a tweet is shown in its entirety
at the top, then a set of radio buttons and a checkbox is shown
for each word of the tweet. These allow the user to pick the
annotation for each word, and indicate uncertainty in labeling.
what kind of entity each word is. Every ten rows,
the header is repeated, to allow the worker to scroll
down the page and still see the column labels. The
interface also provides a checkbox allows the worker
to indicate uncertainty in labeling a word.
We expect that our data will include some tricky
cases where an annotator, even an experienced one,
may be unsure whether a word is part of a named
entity and/or what type it is. For example, is ?Bal-
timore Visionary Art Museum? a LOC followed by
a three word ORG, or a four-word ORG? We con-
sidered and rejected using hierarchical named enti-
ties in order to keep the annotation task simple. An-
other example that might give an annotator pause is
a phrase like ?White House? can be used as a LOC
or ORG, depending on the context.
This measure can act as a measure of a worker?s
quality: if they label many things as ?uncertain?,
we might guess that they are not producing good
results in general. Also, the uncertainty allows for
a finer-grained measure of how closely the results
from two workers for the same tweet match: if the
workers disagree on the tagging of a particular word,
but agree that it is not certain, we could decide that
this word is a bad example and not use it as training
data.
Finally, a help screen is available. When the user
mouses over the word ?Help? in the upper right, the
guidelines discussed in Section 3 are displayed. The
screenshot in Figure 3 shows the help dialog ex-
panded.
The MTurk interface uses hand-written Javascript
to produce the table of words, radio buttons, and
83
Figure 4: Only about one-third of the workers did more than
three HITs and a a few prolific workers accounted for most of
our data.
checkboxes. The form elements have automatically
generated names, which MTurk handles neatly. Ad-
ditional Javascript code collects location informa-
tion from the workers, based on their IP address. A
service provided by Geobytes3 provides the location
data.
4 Results from MTurk
Our dataset was broken into HITs of four previ-
ously unlabeled tweets, and one previously labeled
tweet (analogous to the ?gold? data used by Crowd-
Flower). We submitted 251 HITs, each of which was
to be completed twice, and the job took about 15
hours. Total cost for this job was $27.61, for a total
cost per tweet of about 2.75 cents each (although we
also paid to have the gold tweets annotated again).
42 workers participated, mostly from the US and
India, with Australia in a distant third place. Most
workers did only a single HIT, but most HITs were
done by a single worker. Figure 4 shows more detail.
After collecting results from MTurk, we had to
come up with a strategy for determining which
of the results (if any) were filled randomly. To
do this, we implemented an algorithm much like
Google?s PageRank (Brin and Page, 1998) to judge
the amount of inter-worker agreement. Pseudocode
for our algorithm is presented in Figure 5.
This algorithm doesn?t strictly measure worker
quality, but rather worker agreement, so it?s impor-
3http://www.geobytes.com/
WORKER-AGREE : results ? scores
1 worker ids ? ENUMERATE(KEYS(results))
 Initialize A
2 for worker1 ? worker ids
3 do for worker2 ? worker ids
4 do A[worker1 ,worker2 ]
? SIMILARITY(results[worker1 ],
results[worker2 ])
 Normalize columns of A so that they sum to 1 (elided)
 Initialize x to be normal: each worker
is initially trusted equally.
5 x?
?
1?
n
, . . . , 1?
n
?
 Find the largest eigenvector of A, which
corresponds to the agreement-with-group
value for each worker.
6 i? 0
7 while i < max iter
8 do xnew ? NORMALIZE(A? x)
9 diff ? xnew ? x
10 x = xnew
11 if diff < tolerance
12 then break
13 i? i + 1
14 for workerID ,workerNum ? worker ids
15 do scores[workerID ]? x[workerNum]
16 return scores
Figure 5: Intra-worker agreement algorithm. MTurk results are
stored in an associative array, with worker IDs as keys and lists
of HIT results as values, and worker scores are floating point
values. Worker IDs are mapped to integers to allow standard
matrix notation. The Similarity function in line four just returns
the fraction of HITs done by two workers where their annota-
tions agreed.
tant to ensure that the workers it judges as having
high agreement values are actually making high-
quality judgements. Figure 6 shows the worker
agreement values plotted against the number of re-
sults a particular worker completed. The slope of
this plot (more results returned tends to give higher
scores) is interpreted to be because practice makes
perfect: the more HITs a worker completes, the
more experience they have with the task, and the
more accurate their results will be.
So, with this agreement metric established, we set
out to find out how well it agreed with our expecta-
tion that it would also function as a quality metric.
Consider those workers that completed only a sin-
gle HIT (there are 18 of them): how well did they
do their jobs, and where did they end up ranked as a
result? Since each HIT is composed of five tweets,
84
Figure 6: This log-log plot of worker agreement scores versus
the number of results clearly shows that workers who have done
more HITs have better inter-annotator agreement scores.
even such a small sample can contain a lot of data.
Figure 7 shows a sample annotation for three
tweets, each from a worker who did only one HIT,
and the ranking that the worker received for doing
that annotation. The worst scoring one is apparently
a random fill: there?s no correlation at all between
the answers and the correct ones. The middle tweet
is improved: ?Newbie? isn?t a person in this con-
text, but it?s a mistake a non-native speaker might
make, and everything else is right, and the score is
higher. The last tweet is correctly labeled within our
parameters, and scores the highest. This experiment
shows that our agreement metric functions well as a
correctness metric.
Also of interest is the raw effectiveness of MTurk
workers; did they manage to tag tweets as well as
our experts? After investigating the data, our verdict
is that the answer is not quite?but by carefully com-
bining the tags that two people give the same tweet
it is possible to get good answers nevertheless, at
much lower cost than employing a single expert.
5 Results from CrowdFlower
Our CrowdFlower task involved 30 tweets. Each
tweet was further split into tokens resulting in 506
units as interpreted by CrowdFlower?s system. We
required a total 986 judgments. In addition, we were
Score 0.0243 Score 0.0364 Score 0.0760
Trying org Newbie person Trying none
to org here none out none
decide org nice none TwittEarth org
if org to none - none
it?s org meet none Good none
worth place you none graphics. none
hanging org all none Fun none
around org but none
until org useless. none
the none (URL) none
final org
implosion org
Figure 7: These sample annotations represent the range of
worker quality for three workers who did only one HIT. The
first is an apparently random annotation, the second a plausible
but incorrect one, and the third a correct annotation. Our algo-
rithm assigned these workers scores aligned with their product
quality.
Figure 8: CrowdFlower provides good interfaces to manage
crowdsourcing tasks. This view lets us to monitor the number
of judgements in each category.
required to generate thirteen ?gold? data, which is
the minimum required by the service. Every gold
answer has an optional text with it to inform work-
ers why we believe our answer is the correct one and
theirs is incorrect. This facilitates gradually train-
ing workers up to the point where they can provide
reliably correct results. Figure 8 shows the inter-
face CrowdFlower provides to monitor the number
of judgements in each category.
We used the calibration interface that Crowd-
Flower provides to fix the price for our task (Fig-
ure 9). It considers various parameters like the time
required per unit and desired accuracy level, and also
adds a flat 33%markup on the actual labor costs. We
divided the task into a set of assignments where each
assignment had three tweets and was paid five cents.
We set the time per unit as 30 seconds, so, based on
the desired accuracy level and markup overhead, our
job?s cost was $2.19. This comes to $2 hourly pay
per worker, assuming they take the whole 30 sec-
85
Figure 9: CrowdFlower has an interface that makes it easy to
select an appropriate price for a task.
onds to complete the task.
6 Cloud Comparison
MTurk can also be used to efficiently evaluate re-
sults requiring human judgments. We implemented
an additional HIT to evaluate a new technique we
developed to generate ?word clouds.? In this task
workers choose which of two word clouds generated
from query results by two different algorithms pro-
vides a more useful high level description that can
highlight important features and opinions about the
query topic.
Evaluating how well a set of words describes
and highlights the important features and opinions
pertaining to the subject of the query is subjec-
tive, which necessitates human evaluations. MTurk
workers were given two word clouds, one from our
technique and the other from a baseline relevance
feedback technique (Rocchio (Rocchio, 1971)), for
each query. Queries were shown with a short de-
scriptive blurb to disambiguate it from possible al-
ternatives, reveal the intent of the user who created
the query, and provide a short description of it for
workers who were unfamiliar with the query subject.
Wikipedia links were provided, when applicable, for
anyone needing further information about the query
subject. Workers were asked to use a slider to de-
termine which cloud better represented the key con-
cepts related to the query. The slider would snap
into one of eleven positions, which were labeled
with value judgments they represented. The cen-
ter value indicates that the two clouds were equally
good. Figure 10 shows the final query interface.
Figure 10: MTurk workers were asked which word cloud they
thought best represented returned the results of a query, in this
case ?Buffy the Vampire Slayer?.
6.1 Results
Since MTurk workers are paid per task they com-
plete, there is an incentive to do low quality work
and even to randomly guess to get tasks done as
fast as possible. To ensure a high quality evaluation
we included in every batch of five queries a qual-
ity control question. Quality control questions were
designed to look exactly like the regular cloud com-
parisons, but only one of the two clouds displayed
was actually from the query in the description. The
other word cloud was generated from a different
query with no relation to the real query, and hand
checked to make sure that anyone who was doing a
respectable job would agree that the off-topic word
cloud was a poor result for the query. If a worker?s
response indicated that the off topic cloud was as
good as or better than the real cloud then they failed
that control question, otherwise they passed.
86
We asked that twelve workers label each set of
questions. We only used results from workers that
answered at least seven control questions with an
average accuracy rating of at least 75%. This left
us with a pool of eight reliable workers with an av-
erage accuracy on control questions of about 91%.
Every question was labeled by at least five different
workers with a mode of seven.
Workers were not told which technique produced
which cloud. Techniques were randomly assigned to
either cloud A or B to prevent people from entering
into a ?cloud A is always better? mentality. The po-
sition of the quality control questions were randomly
assigned in each set of five cloud comparisons. The
links to the cloud images were anonymized to ran-
dom numbers followed by the letter A or B for their
position to prevent workers from guessing anything
about either the query or the technique that gener-
ated the cloud.
We applied a filter to remove the query words
from all word clouds. First of all, it would be a
dead giveaway on the control questions. Second,
the query words are already known and thus pro-
vide no extra information about the query to the user
while simultaneously taking up the space that could
be used to represent other more interesting words.
Third, their presence and relative size compared to
the baseline could cause users to ignore other fea-
tures especially when doing a quick scan.
The slider scores were converted into numerical
scores ranging from -5 to +5, with zero represent-
ing that the two clouds were equal. We averaged
the score for each cloud comparison, and determined
that for 44 out of 55 clouds workers found our tech-
nique to be better than the baseline approach.
6.2 Issues
We faced some issues with the CrowdFlower sys-
tem. These included incorrect calibration for jobs,
errors downloading results from completed jobs,
price displayed on MTurk being different that what
was set through CrowdFlower and gold standard
data not getting stored on CrowdFlower system. An-
other problem was with the system?s 10-token limit
on gold standards, which is not yet resolved at the
time of this writing. On the whole, the CrowdFlower
team has been very quick to respond to our problems
and able to correct the problems we encountered.
Figure 11: Statistics for worker #181799. The interface has an
option to ?forgive? the worker for missing gold and an option
to ?flag? the worker so that the answers are excluded while re-
turning the final set of judgments. It also displays workers ID,
past accuracy and source, e.g. MTurk.
6.3 Live Analytics
CrowdFlower?a analytics panel facilitates viewing
the live responses. The trust associated with each
worker can be seen under the workers panel. Work-
ers who do a large amount of work with low trust are
likely scammers or automated bots. Good gold data
ensures that their work is rejected. The system auto-
matically pauses a job when the ratio of untrusted to
trusted judgments exceeds a certain mark. This was
particularly helpful for us to rectify some of our gold
data. Currently, the job is being completed with 61%
accuracy for gold data. This could be due to the cur-
rent issue we are facing as described above. It?s also
possible to view statistics for individual workers, as
shown in Figure 11.
7 Conclusion
Crowdsourcing is an effective way to collect annota-
tions for natural language and information retrieval
research. We found both MTurk and CrowdFlower
to be flexible, relatively easy to use, capable of pro-
ducing usable data, and very cost effective.
Some of the extra features and interface options
that CrowdFlower provided were very useful, but
did their were problems with their ?gold standard?
agreement evaluation tools. Their support staff was
very responsive and helpful, mitigating some of
these problems. We were able to duplicate some of
the ?gold standard? functionality on MTurk directly
by generating our own mix of regular and quality
control queries. We did not attempt to provide im-
87
mediate feedback to workers who enter a wrong an-
swer for the ?gold standard? queries, however.
With these labeled tweets, we plan to train an en-
tity recognizer using the Stanford named entity rec-
ognizer4, and run it on our dataset. After using this
trained entity recognizer to find the entities in our
data, we will compare its accuracy to the existing
recognized entities, which were recognized by an
ER trained on newswire articles. We will also at-
tempt to do named entity linking and entity resolu-
tion on the entire corpus.
We look forward to making use of the data we
collected in our research and expect that we will use
these services in the future when we need human
judgements.
Acknowledgments
This work was done with partial support from the
Office of Naval Research and the Johns Hopkins
University Human Language Technology Center of
Excellence. We thank both Amazon and Dolores
Labs for grants that allowed us to use their systems
for the experiments.
References
S. Bird, E. Klein, and E. Loper. 2009. Natural language
processing with Python. Oreilly & Associates Inc.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. In Seventh Interna-
tional World-Wide Web Conference (WWW 1998).
C. Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazons Me-
chanical Turk. In Proceedings of EMNLP 2009.
N. Chinchor and P. Robinson. 1997. MUC-7 named en-
tity task definition. In Proceedings of the 7th Message
Understanding Conference. NIST.
crowdflower.com. 2010. The error rates with-
out the gold standard is more than twice as
high as when we do use a gold standard.
http://crowdflower.com/general/examples. Accessed
on April 11, 2010.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43nd Annual Meeting of the Association for Com-
putational Linguistics (ACL 2005), volume 100, pages
363?370.
4http://nlp.stanford.edu/software/CRF-NER.shtml
A. Java, X. Song, T. Finin, and B. Tseng. 2007. Why we
twitter: understanding microblogging usage and com-
munities. In Proceedings of the 9th WebKDD and 1st
SNA-KDD 2007 workshop on Web mining and social
network analysis, pages 56?65. ACM.
Linguistic Data Consortium ? LCTL Team. 2006. Sim-
ple named entity guidelines for less commonly taught
languages, March. Version 6.5.
pingdom.com. 2010. Twitter: Now more than 1
billion tweets per month. http://royal.pingdom.com-
/2010/02/10/twitter-now-more-than-1-billion-tweets-
per-month/, February. Accessed on February 15,
2010.
T. Poibeau and L. Kosseim. 2001. Proper Name Extrac-
tion from Non-Journalistic Texts. In Computational
linguistics in the Netherlands 2000: selected papers
from the eleventh CLIN Meeting, page 144. Rodopi.
J. Rocchio. 1971. Relevance feedback in information re-
trieval. In G. Salton, editor, The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing.
Prentice-Hall.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association for
Computational Linguistics.
Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi
Song, and Kazuaki Maeda. 2008. Linguistic re-
sources and evaluation techniques for evaluation of
cross-document automatic content extraction. In Pro-
ceedings of the 6th International Conference on Lan-
guage Resources and Evaluation.
88
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 204?207,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Non-Expert Correction of Automatically Generated Relation Annotations
Matthew R. Gormley?? and Adam Gerber?? and Mary Harper?? and Mark Dredze ??
?Human Language Technology Center of Excellence
?Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21211, USA
?Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742 USA
mrg@cs.jhu.edu,adam.gerber@jhu.edu,mharper@umd.edu,mdredze@cs.jhu.edu
Abstract
We explore a new way to collect human an-
notated relations in text using Amazon Me-
chanical Turk. Given a knowledge base of
relations and a corpus, we identify sentences
which mention both an entity and an attribute
that have some relation in the knowledge base.
Each noisy sentence/relation pair is presented
to multiple turkers, who are asked whether the
sentence expresses the relation. We describe
a design which encourages user efficiency and
aids discovery of cheating. We also present
results on inter-annotator agreement.
1 Introduction
Relation extraction (RE) is the task of determining
the existence and type of relation between two tex-
tual entity mentions. Slot filling, a general form of
relation extraction, includes relations between non-
entities, such as a person and an occupation, age, or
cause of death (McNamee and Dang, 2009).
RE annotated data, such as ACE (2008), is expen-
sive to produce so systems take different approaches
to minimizing data needs. For example, tree kernels
can reduce feature sparsity and generalize across
many examples (GuoDong et al, 2007; Zhou et
al., 2009). Distant supervision automatically gen-
erates noisy training examples from a knowledge
base (KB) without needing annotations (Bunescu
and Mooney, 2007; Mintz et al, 2009). While
this method can quickly generate training data, it
also generates many false examples. We reduce the
noise in such examples by using Amazon Mechani-
cal Turk (MTurk), which has been shown to produce
high quality annotations for a variety of natural lan-
guage processing tasks (Snow et al, 2008).
We use MTurk for annotation of textual relations
to establish an inexpensive and rapid method of cre-
ating data for slot filling. We present a two step an-
notation process: (1) automatic creation of noisy ex-
amples, and (2) human validation of examples.
2 Method
2.1 Automatic generation of noisy examples
To create noisy examples we use a similar approach
to Mintz et al (2009). We extract relations from a
KB in the form of tuples, (e, r, v), where e is an
entity, v is a value, and r is a relation that holds
between them; for example (J.R.R. Tolkien, occu-
pation, author). Our KB is Freebase1, an online
database of structured information, and our corpus
is from the TAC KBP task (McNamee and Dang,
2009)2. For each tuple, we find sentences in a cor-
pus that contain both an exact mention of the entity
e and of the value v. Of course, such sentences may
not attest to the relation r, so the process produces
many incorrect examples.
2.2 Human Intelligence Tasks
A Human Intelligence Task (HIT) is a short paid task
on MTurk. In our HITs, we present the turker with
ten relation examples as sentence/relation pairs. For
each example, the user is asked to select from three
annotation options: the sentence (1) expresses the
relation, (2) does not express the relation, or (3) the
1http://www.freebase.com
2http://projects.ldc.upenn.edu/kbp/
204
1. The sentence expresses the relation.
Sentence: For the past eleven years, James has
lived in Tucson.
Relation: ?Tucson? is the residence of ?James?
2. The sentence does not express the relation.
Sentence: Samuel first met Divya in 1990, while
she was still a student.
Relation: ?Divya? is a spouse of ?Samuel?
3. The relation does not make sense.
Sentence: Soojin was born in January.
Relation: ?January? is the birth place of ?Soojin?
Figure 1: The three annotation options with examples.
relation does not make sense (figure 1.)
Of the ten examples that comprise each HIT,
seven are automatically generated by the method
above. The correct answer is known for the three re-
maining examples; these are included for quality as-
surance (control examples.) The three control exam-
ples are a positive example (expresses the relation,) a
negative example (contradicts the true relation,) and
a nonsense example (relation is nonsensical.)
All control examples derive from a subset of the
automatically generated person examples. Positive
examples were randomly sampled and hand anno-
tated. Negative examples are familial relations in
which we change the relation type so that it would
not be expressed in the sentence. For example,
the relation ?Barack Obama is the parent of Malia
Obama? would be changed to ?Barack Obama is a
sibling of Malia Obama.? To generate nonsense ex-
amples we employ the same method for a different
mapping of relations, which produces relations like
?New Zealand is the gender of John Key.?
2.3 HIT Design
MTurk is a marketplace so users have total freedom
in choosing which HITs to complete. As such, HIT
design should maximize its appeal. We assume that
users find appealing those HITs through which they
may maximize their own monetary gain, while mini-
mizing moment-to-moment frustrations. We empha-
sized clarity and ease of use.
The layout consists of three sections (figure 2).
The leftmost section is a progress list, which shows
the user?s answers and current position; the middle
section contains the current relation example and an-
notation options; the rightmost section (not pictured)
# HITs Cost Time (hours)
Trial 50 $2.75 27
Batch 1 500 $27.50 34
Batch 2 765 $42.08 25
Batch 3 500 $27.50 22
Total 1815 $99.83 108
Table 1: Size, cost and time to complete each HITs batch.
contains instructions. All sections and all UI ele-
ments remain visible and in the same position for the
duration of the HIT, with only the text of the sen-
tence and relation changing according to question
number. Because only a single question is displayed
at a time, we are able to minimize user actions such
as scrolling, clicking small targets, or making large
mouse movements. Additionally, we can monitor
how much time a user spends on each question.
At all times the user is able to consult the instruc-
tions for the task, which include examples of each
annotation option. The user is also reminded of the
technical requirements for the HIT and expectations
for honesty and accuracy. A comment box provides
users with the opportunity to ask questions, make
suggestions, or clarify their responses.
3 Results
We submitted a trial run and three full batches of
HITs. Table 1 summarizes the costs and completion
times for all HITs. The HITs were labeled rapidly
and for a low cost ($0.05 per HIT, i.e., .5? per anno-
tation). Each HIT was assigned to five unique work-
ers. We found that 50% of the 352 different workers
completed 2 or more HITs (figure 3.) Our results
exclude a trial run of 50 hits. Across the 17,650 ex-
amples the mean time spent was 20.77 seconds, with
a standard deviation of 99.96 seconds. The median
time per example was 10.0 seconds.
3.1 Analysis
To evaluate the annotations, two of the authors an-
notated a random sample of 247 (10%) of the 2471
noisy examples. In addition, we analyzed the work-
ers agreement with the control examples.
We used two metrics to assess agreement. The
first metric is pairwise percent agreement (Pair-
wise): the average of the example agreement scores,
where the example agreement score is the percent of
205
Figure 2: An example HIT with instructions excluded.
0 
20 
40 
60 
80 
100 
120 
140 
0 50 100
 
150
 
200
 
250
 
300
 
350
 
# H
ITs 
Workers 
Figure 3: The number of HITs per worker, with columns
sorted left to right.
pairs of annotators that agreed for a particular exam-
ple. The second metric is the exact kappa coefficient
(Exact-?) (Conger, 1980), which takes into account
that agreement can occur by chance. The number of
annotators (R) varies with the test scenario.
Table 2 presents the inter-annotator agreement
scores for various subsets of the examples and com-
binations of annotators. On a sample of examples,
we evaluated agreement between the first and sec-
ond expert annotators (E1/E2) and also the agree-
ment between each expert and the majority vote of
the workers (E1/M and E2/M). The agreement be-
tween the two experts is substantially higher than
their individual agreements with the majority. Yet,
we achieve our goal of reducing noise.
We also analyzed the agreement between the
known control answer and the majority vote of the
workers (C/M). This high level of agreement sup-
ports our belief that the automatically generated neg-
ative and nonsense examples were easier to identify
# Ex. R Exact-? Pairwise
E1/E2 247 2 0.64 0.81
E1/M 247 2 0.29 0.60
E2/M 247 2 0.39 0.70
C/M 1059 2 0.90 0.93
T(sample) 247 5 0.31 0.69
T(control) 1059 5 0.52 0.68
T(all) 3530 5 0.45 0.68
Table 2: Inter-annotator agreement
than noisy negative and nonsense examples. Finally,
we evaluated the agreement between the five work-
ers for different subsets of the data: the sample of
noisy examples (T(sample)), the control examples
only (T(control)), and all examples (T(all)). Table 3
lists the number of examples collected and the agree-
ment scores for all workers for each relation type.
Table 4 shows the divergence of the workers? an-
notations from those of an expert. The high level
of confusability for those examples which the expert
annotated as Not Expressed suggests their inherent
difficulty. The workers labeled more examples as
Expressed than the expert, but both labeled few ex-
amples as Nonsense.
4 Quality Control
We identify spurious responses and unreliable users
in two ways. First, worker responses are compared
to control examples; greater agreement with controls
should indicate greater confidence in the user. We
filtered any worker whose agreement with the con-
trols was less than 0.85 (Control Filtered). The sec-
ond approach uses behavioral data. Because only a
single example is visible at any time, we can mea-
206
Relation # Ex. Exact-? Pairwise
siblings 13 0.67 0.82
children 12 0.57 0.83
gender 80 0.46 0.70
place of death 40 0.43 0.68
parent 12 0.40 0.64
spouse 54 0.37 0.65
title 71 0.30 0.78
residences 228 0.29 0.60
ethnicity 38 0.28 0.54
occupation 551 0.26 0.77
activism 4 0.26 0.55
religion 22 0.23 0.55
place of birth 160 0.20 0.64
nationality 1044 0.19 0.67
schools attended 8 0.16 0.55
employee of 132 0.16 0.70
charges 2 0.14 0.70
Total 2471 0.35 0.69
Table 3: Inter-annotator agreement across relation type.
# Ex. is the number of noisy examples. Exact-? and Pair-
wise agreement are among the five workers.
Worker
E NE Nn Total
E
xp
er
t-
1 E 561 89 20 670
NE 284 248 28 560
Nn 1 1 3 5
Total 846 338 51 1235
Table 4: Confusion matrix of expert-1 and user?s anno-
tations on the sample of noisy examples, for the choices
Expressed (E), Not Expressed (NE), and Nonsense (Nn)
sure how much time a user spends on each exam-
ple. The UI is designed to allow for the extremely
rapid completion of examples and of the HIT in gen-
eral. Thus, a user could complete the HIT in only a
few seconds without even reading any of the exam-
ples. Still other users spend only a moment on all-
but-one question, and then several minutes on the
remaining question. Here, we filter a user answering
three or more questions each in under three seconds
(Time Filtered). We combine these two approaches
(Control and Time), which yields the highest expert-
agreement levels (table 5.)
5 Conclusion
Using non-expert annotators from Amazon Mechan-
ical Turk for the correction of noisy, automatically
E1/M E2/M
Unfiltered 0.28 0.38
Time Filtered 0.32 0.43
Control Filtered 0.34 0.47
Control and Time 0.37 0.48
Table 5: Exact-? scores for three levels of quality control
and a baseline, between each expert and the majority vote
on 231 sampled examples. For a fair comparison, we re-
duced the sample size to include only examples for which
each level of quality control had at least one worker an-
notation remaining.
generated examples is inexpensive and fast. We
achieve good inter-annotator agreement using qual-
ity assurance measures to detect cheating. The result
is thousands of new annotated slot filling example
sentences for 17 person relations.
Acknowledgments
We would like to thank the 352 turkers who made
this work possible.
References
ACE. 2008. Automatic content extraction.
http://projects.ldc.upenn.edu/ace/.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Association for Computational Linguistics (ACL).
A.J. Conger. 1980. Integration and generalization of
kappas for multiple raters. Psychological Bulletin,
88(2):322?328.
Z. GuoDong, M. Zhang, D. H Ji, and Z. H. U. QiaoM-
ing. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In
Empirical Methods in Natural Language Processing
(EMNLP).
Paul McNamee and Hoa Dang. 2009. Overview of the
TAC 2009 knowledge base population track. In Text
Analysis Conference (TAC).
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Association for Computational Linguistics
(ACL).
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Empirical
Methods in Natural Language Processing (EMNLP).
G. Zhou, L. Qian, and J. Fan. 2009. Tree kernel-based
semantic relation extraction with rich syntactic and se-
mantic information. Information Sciences.
207
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 50?58,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Revisiting the Case for Explicit Syntactic Information in Language Models
Ariya Rastrow, Sanjeev Khudanpur, Mark Dredze
Human Language Technology Center of Excellence,
Center for Language and Speech Processing, Johns Hopkins University
Baltimore, MD USA
{ariya,khudanpur,mdredze}@jhu.edu
Abstract
Statistical language models used in deployed
systems for speech recognition, machine
translation and other human language tech-
nologies are almost exclusively n-gram mod-
els. They are regarded as linguistically na??ve,
but estimating them from any amount of text,
large or small, is straightforward. Further-
more, they have doggedly matched or out-
performed numerous competing proposals for
syntactically well-motivated models. This un-
usual resilience of n-grams, as well as their
weaknesses, are examined here. It is demon-
strated that n-grams are good word-predictors,
even linguistically speaking, in a large major-
ity of word-positions, and it is suggested that
to improve over n-grams, one must explore
syntax-aware (or other) language models that
focus on positions where n-grams are weak.
1 Introduction
Language models (LM) are crucial components in
tasks that require the generation of coherent natu-
ral language text, such as automatic speech recog-
nition (ASR) and machine translation (MT). Most
language models rely on simple n-gram statistics
and a wide range of smoothing and backoff tech-
niques (Chen and Goodman, 1998). State-of-the-art
ASR systems use (n ? 1)-gram equivalence classi-
fication for the language model (which result in an
n-gram language model).
While simple and efficient, it is widely believed
that limiting the context to only the (n ? 1) most
recent words ignores the structure of language, and
several statistical frameworks have been proposed
to incorporate the ?syntactic structure of language
back into language modeling.? Yet despite consider-
able effort on including longer-dependency features,
such as syntax (Chelba and Jelinek, 2000; Khudan-
pur and Wu, 2000; Collins et al, 2005; Emami
and Jelinek, 2005; Kuo et al, 2009; Filimonov and
Harper, 2009), n-gram language models remain the
dominant technique in automatic speech recognition
and machine translation (MT) systems.
While intuition suggests syntax is important, the
continued dominance of n-gram models could in-
dicate otherwise. While no one would dispute that
syntax informs word choice, perhaps sufficient in-
formation aggregated across a large corpus is avail-
able in the local context for n-gram models to per-
form well even without syntax. To clearly demon-
strate the utility of syntactic information and the de-
ficiency of n-gram models, we empirically show that
n-gram LMs lose significant predictive power in po-
sitions where the syntactic relation spans beyond the
n-gram context. This clearly shows a performance
gap in n-gram LMs that could be bridged by syntax.
As a candidate syntactic LM we consider the
Structured Language Model (SLM) (Chelba and Je-
linek, 2000), one of the first successful attempts to
build a statistical language model based on syntac-
tic information. The SLM assigns a joint probabil-
ity P (W,T ) to every word sequence W and every
possible binary parse tree T , where T ?s terminals
are words W with part-of-speech (POS) tags, and
its internal nodes comprise non-terminal labels and
lexical ?heads? of phrases. Other approaches in-
clude using the exposed headwords in a maximum-
entropy based LM (Khudanpur and Wu, 2000), us-
50
ing exposed headwords from full-sentence parse tree
in a neural network based LM (Kuo et al, 2009),
and the use of syntactic features in discriminative
training (Rastrow et al, 2011). We show that the
long-dependencies modeled by SLM, significantly
improves the predictive power of the LM, specially
in positions where the syntactic relation is beyond
the reach of regular n-gram models.
2 Weaknesses of n-gram LMs
Consider the following sentence, which demon-
strates why the (n? 1)-gram equivalence classifica-
tion of history in n-gram language models may be
insufficient:
<s> i asked the vice president for
his endorsement </s>
In an n-gram LM, the word for would be modeled
based on a 3-gram or 4-gram history, such as <vice
president> or <the vice president>.
Given the syntactic relation between the preposition
for and the verb asked (which together make a
compound verb), the strongest evidence in the his-
tory (and hence the best classification of the history)
for word for should be <asked president>,
which is beyond the 4-gram LM. Clearly, the
syntactic relation between a word position and the
corresponding words in the history spans beyond
the limited (n ? 1)-gram equivalence classification
of the history.
This is but one of many examples used for moti-
vating syntactic features (Chelba and Jelinek, 2000;
Kuo et al, 2009) in language modeling. How-
ever, it is legitimate to ask if this deficiency could
be overcome through sufficient data, that is, accu-
rate statistics could somehow be gathered for the n-
grams even without including syntactic information.
We empirically show that (n? 1)-gram equivalence
classification of history is not adequate to predict
these cases. Specifically, n-gram LMs lose predic-
tive power in the positions where the headword rela-
tion, exposed by the syntactic structure, goes beyond
(n? 1) previous words (in the history.)
We postulate the following three hypotheses:
Hypothesis 1 There is a substantial difference in
the predictive power of n-gram LMs at positions
within a sentence where syntactic dependencies
reach further back than the n-gram context versus
positions where syntactic dependencies are local.
Hypothesis 2 This difference does not diminish by
increasing training data by an order of magnitude.
Hypothesis 3 LMs that specifically target positions
with syntactically distant dependencies will comple-
ment or improve over n-gram LMs for these posi-
tions.
In the following section (Section 3), we present a set
of experiments to support the hypotheses 1 and 2.
Section 4 introduces a SLM which uses dependency
structures followed by experiments in Section 5.
3 Experimental Evidence
In this section, we explain our experimental evi-
dence for supporting the hypotheses stated above.
First, Section 3.1 presents our experimental design
where we use a statistical constituent parser to iden-
tify two types of word positions in a test data,
namely positions where the headword syntactic re-
lation spans beyond recent words in the history and
positions where the headword syntactic relation is
within the n-gram window. The performance of
an n-gram LM is measured on both types of posi-
tions to show substantial difference in the predictive
power of the LM in those positions. Section 3.3 de-
scribes the results and analysis of our experiments
which supports our hypotheses.
Throughout the rest of the paper, we refer to
a position where the headword syntactic relation
reaches further back than the n-gram context as a
syntactically-distant position and other type of posi-
tions is referred to as a syntactically-local position.
3.1 Design
Our experimental design is based on the idea of
comparing the performance of n-gram LMs for
syntactically-distant vs. syntactically-local . To this
end, we first parse each sentence in the test set us-
ing a constituent parser, as illustrated by the exam-
ple in Figure 1. For each word wi in each sentence,
we then check if the ?syntactic heads? of the preced-
ing constituents in the parse ofw1, w2, ? ? ? , wi?1 are
within an (n? 1) window of wi. In this manner, we
split the test data into two disjoint sets, M and N ,
51
!"!#$%&'!!!!!()&!*"+&!,-&$"'&.(!!!!!!/0-!!!!!!!)"$!&.'0-$&1&.(!
232!
42!
5!
67!
62!
89! 442! 442!
42!
:4! 232;! 44!
442!
22!
#$%&'!
#$%&'!
/0-!
&.'0-$&1&.(!,-&$"'&.(!"!
Figure 1: Example of a syntactically distant position in
a sentence: the exposed headwords preceding for are
h.w?2 =asked and h.w?1 = president, while the
two preceding words are wi?2 = vice and wi?1 =
president.
as follows,
M = {j|positions s.t h.w?1, h.w?2 = wj?1, wj?2}
N = {j|positions s.t h.w?1, h.w?2 6= wj?1, wj?2}
Here, h?1 and h?2 correspond, respectively, to the
two previous exposed headwords at position i, based
on the syntactic structure. Therefore, M corre-
sponds to the positions in the test data for which two
previous exposed heads match exactly the two previ-
ous words. Whereas, N corresponds to the position
where at least on of the exposed heads is further back
in the history than the two previous words, possibly
both.
To extract the exposed headwords at each posi-
tion, we use a constituent parser to obtain the syn-
tactic structure of a sentence followed by headword
percolation procedure to get the headwords of cor-
responding syntactic phrases in the parse tree. The
following method, described in (Kuo et al, 2009),
is then used to extract exposed headwords from the
history of position i from the full-sentence parse
trees:
1. Start at the leaf corresponding to the word posi-
tion (wi) and the leaf corresponding to the pre-
vious context word (wi?1).
2. From each leaf, go up the tree until the two
paths meet at the lowest common ancestor
(LCA).
3. Cut the link between the LCA and the child that
is along the path from the context word wi?1.
The head word of the the LCA child, the one
that is cut, is chosen as previous exposed head-
word h.w?1.
These steps may be illustrated using the parse tree
shown in Figure 1. Let us show the procedure for
our example from Section 2. Figure 1 shows the cor-
responding parse tree of our example. Considering
word position wi=for and wi?1=president and
applying the above procedure, the LCA is the node
VPasked. Now, by cutting the link from VPasked to
NPpresident the word president is obtained as
the first exposed headword (h.w?1).
After the first previous exposed headword has
been extracted, the second exposed headword also
can be obtained using the same procedure, with
the constraint that the node corresponding the sec-
ond headword is different from the first (Kuo et al,
2009). More precisely,
1. set k = 2
2. Apply the above headword extraction method
between wi and wi?k.
3. if the extracted headword has previously been
chosen, set k = k + 1 and go to step (2).
4. Otherwise, return the headword as h.w?2.
Continuing with the example of Figure 1, after
president is chosen as h.w?1, asked is cho-
sen as h.w?2 of position for by applying the pro-
cedure above. Therefore, in this example the po-
sition corresponding to word for belongs to the
set N as the two extracted exposed headwords
(asked,president) are different from the two
previous context words (vice,president).
After identifying sets N andM in our test data,
we measure perplexity of n-gram LMs on N , M
and N ?M separately. That is,
PPLN?M = exp
[
?
?
i?N?M log p(wi|W i?1i?n+1)
|N ?M|
]
PPLN = exp
[
?
?
i?N
log p(wi|W i?1i?n+1)
|N |
]
PPLM = exp
[
?
?
i?M
log p(wi|W i?1i?n+1)
|M|
]
,
52
where p(wi|wi?1wi?2 ? ? ?wi?n+1) is the condi-
tional probability calculated by an n-gram LM at
position i and |.| is the size (in number of words)
of the corresponding portion of the test.
In addition, to show the performance of n-gram
LMs as a function of training data size, we train
different n-gram LMs on 10%,20%,? ? ? ,100% of a
large corpus of text and report the PPL numbers us-
ing each trained LM with different training data size.
For all sizes less than 100%, we select 10 random
subset of the training corpus of the required size, and
report the average perplexity of 10 n-gram models.
This will enable us to observe the improvement of
the n-gram LMs on as we increase the training data
size. The idea is to test the hypothesis that not only
is there significant gap between predictive power of
the n-gram LMs on setsN andM, but also that this
difference does not diminish by adding more train-
ing data. In other words, we want to show that the
problem is not due to lack of robust estimation of
the model parameters but due to the fact that the in-
cluded features in the model (n-grams) are not in-
formative enough for the positions N .
3.2 Setup
The n-gram LMs are built on 400M words from
various Broadcast News (BN) data sources includ-
ing (Chen et al, 2006): 1996 CSR Hub4 Language
Model data, EARS BN03 closed captions, GALE
Phase 2 Distillation GNG Evaluation Supplemen-
tal Multilingual data, Hub4 acoustic model training
scripts (corresponding to the 300 Hrs), TDT4 closed
captions, TDT4 newswire, GALE Broadcast Con-
versations, and GALE Broadcast News. All the LMs
are trained using modified Kneser-Ney smoothing.
To build the LMs, we sample from each source and
build a source specific LM on the sampled data. The
final LMs are then built by interpolating those LMs.
Also, we do not apply any pruning to the trained
LMs, a step that is often necessary for speech recog-
nition but not so for perplexity measurement. The
test set consists of the NIST rt04 evaluation data set,
dev04f evaluation set, and rt03 evaluation set. The
test data includes about 70K words.
We use the parser of (Huang and Harper, 2009),
which achieves state-of-the-art performance on
broadcast news data, to identify the word poisons
that belong to N and M, as was described in Sec-
tion 3.1. The parser is trained on the Broadcast News
treebank from Ontonotes (Weischedel et al, 2008)
and the WSJ Penn Treebank (Marcus et al, 1993)
along with self-training on 1996 Hub4 CSR (Garo-
folo et al, 1996) utterances.
3.3 Analysis
We found that |N ||N?M| ? 0.25 in our test data. In
other words, two previous exposed headwords go
beyond 2-gram history for about 25% of the test
data.
!"#
$%#
$"#
&%#
&"#
'%%#
'%# (%# )%# *%# "%# +%# !%# $%# &%# '%%#
,-./
01-#
223#
456#
78/9:9:;#</=/#>9?-#456#
@AB# @# B#
(a)
!"#
$%#
$"#
&%#
&"#
'%#
'"#
(%%#
(%# )%# *%# +%# "%# !%# $%# &%# '%# (%%#
,-./
01-#
223#4
56#
78/9:9:;#</=/#>9?-#456#
@AB# @# B#
(b)
Figure 2: Reduction in perplexity with increasing training
data size on the entire test setN +M, on its syntactically
local subset M, and the syntactically distant subset N .
The figure shows relative perplexity instead of absolute
perplexity ? 100% being the perplexity for the smallest
training set size ? so that (a) 3-gram and (b) 4-gram LMs
may be directly compared.
We train 3-gram and 4-gram LMs on
10%,20%,? ? ? ,100% of the BN training data,
where each 10% increase corresponds to about
40M words of training text data. Figure 2 shows
reduction in perplexity with increasing training data
size on the entire test setN+M, on its syntactically
local subsetM, and the syntactically distant subset
N . The figure basically shows relative perplexity
instead of absolute perplexity ? 100% being the
53
Position Training Data Size
in 40M words 400M words
Test Set 3-gram 4-gram 3-gram 4-gram
M 166 153 126 107
N 228 217 191 171
N +M 183 170 143 123
PPLN
PPLM
138% 142% 151% 161%
Table 1: Perplexity of 3-gram and 4-gram LMs on syntac-
tically local (M) and syntactically distant (N ) positions
in the test set for different training data sizes, showing the
sustained higher perplexity in distant v/s local positions.
perplexity for the smallest training set size ? so the
rate of improvement for 3-grams and 4-gram LMs
can be compared. As can be seen from Figure 2,
there is a substantial gap between the improvement
rate of perplexity in syntactically distant positions
compared to that in syntactically local positions
(with 400M woods of training data, this gap is about
10% for both 3-gram and 4-gram LMs). In other
words, increasing the training data size has much
more effect on improving the predictive power of
the model for the positions included inM. Also, by
comparing Figure 2(a) to 2(b) one can observe that
the gap is not overcome by increasing the context
length (using 4-gram features).
Also, to better illustrate the performance of the n-
gram LMs for different portions of our test data, we
report the absolute values of PPL results in Table 1.
It can be seen that there exits a significant difference
between perplexity of sets N and M and that the
difference gets larger as we increase the training data
size.
4 Dependency Language Models
To overcome the lack of predictive power of n-gram
LMs in syntactically-distant positions, we use the
SLM framework to build a long-span LM. Our hope
is to show not only that long range syntactic depen-
dencies improve over n-gram features, but also that
the improvement is largely due to better predictive
power in the syntactically distant positions N .
Syntactic information may be encoded in terms
of headwords and headtags of phrases, which may
be extracted from a syntactic analysis of a sen-
tence (Chelba and Jelinek, 2000; Kuo et al, 2009),
such as a dependency structure. A dependency in
a sentence holds between a dependent (or modifier)
word and a head (or governor) word: the dependent
depends on the head. These relations are encoded in
a dependency tree (Figure 3), a directed graph where
each edge (arc) encodes a head-dependent relation.
The specific parser used to obtain the syntactic
structure is not important to our investigation. What
is crucial, however, is that the parser proceeds left-
to-right, and only hypothesized structures based on
w1, . . . , wi?1 are used by the SLM to predict wi.
Similarly, the specific features used by the parser
are also not important: more noteworthy is that the
SLM uses (h.w?3, h.w?2, h.w?1) and their POS
tags to predict wi. The question is whether this
yields lower perplexity than predicting wi from
(wi?3, wi?2, wi?1).
For the sake of completeness, we next describe
the parser and SLM in some detail, but either may
be skipped without loss of continuity.
The Parser: We use the shift-reduce incremen-
tal dependency parser of (Sagae and Tsujii, 2007),
which constructs a tree from a transition sequence
governed by a maximum-entropy classifier. Shift-
reduce parsing places input words into a queue Q
and partially built structures are organized by a stack
S. Shift and reduce actions consume the queue and
build the output parse on the stack. The classi-
fier g assigns probabilities to each action, and the
probability of a state pg(pi) can be computed as the
product of the probabilities of a sequence of ac-
tions that resulted in the state. The parser therefore
provides (multiple) syntactic analyses of the history
w1, . . . , wi?1 at each word position wi.
The Dependency Language Model: Parser states
at position wi, called history-states, are denoted
??i = {pi0?i, pi1?i ? ? ? , piKi?i }, where Ki is the total
number of such states. Given ??i, the probability
assignment for wi is given by
p(wi|W?i) =
|??i|?
j=1
p
(
wi|f(pij?i)
)
pg(pij?i|W?i) (1)
where, W?i is the word history w1, . . . , wi?1 for
wi, pij?i is the jth history-state of position i,
pg(pij?i|W?i) is the probability assigned to pi
j
?i by
54
step
action stack queue
i asked the vice president ...-0
asked the vice president ...shift1 i
the vice president for ...shift2 i asked
the vice president for ...left-reduce3 asked
i
for his endorsement ...shift6 asked the vice president
i
for his endorsement ...left-reduce7 asked the president
i vice
<s>   i   asked   the vice president   for    his  endorsement
Thursday, March 29, 12
for his endorse ent ...left-reduce8 asked president
i
vicethe
for his endorsement ...right-reduce9 asked
i
vicethe
president
Thursday, March 29, 12
step
action stack queue
i asked the vice president ...-0
asked the vice president ...shift1 i
the vice president for ...shift2 i asked
the vice president for ...left-reduce3 asked
i
for his endorsement ...shift6 asked the vice president
i
for his endorsement ...left-reduce7 asked the president
i vice
<s>   i   asked   the vice president   for    his  endorsement
Thursday, March 29, 12
Tuesday, April 3, 12
Figure 3: Actions of a shift-reduce parser to produce
the dependency structure (up to the word president)
shown above.
the parser, and f(pij?i) denotes an equivalence clas-
sification of the parser history-state, capturing fea-
tures from pij?i that are useful for predicting wi.
We restrict f(pi) to be based on only the heads of
the partial trees {s0 s1 ? ? ? } in the stack. For exam-
ple, in Figure 3, one possible parser state for pre-
dicting the word for is the entire stack shown after
step 8, but we restrict f(?) to depend only on the
headwords asked/VB and president/NNP.
Given a choice of f(?), the parameters of the
model p(wi|f(pij?i)) are estimated to maximize the
log-likelihood of the training data T using the
Baum-Welch algorithm (Baum, 1972), and the re-
sulting estimate is denoted pML(wi|f(pij?i)).
The estimate pML(w|f(?)) must be smoothed to
handle unseen events, which we do using the method
of Jelinek and Mercer (1980). We use a fine-to-
coarse hierarchy of features of the history-state as
illustrated in Figure 4. With
fM (pi?i) ? fM?1(pi?i) ? . . . ? f1(pi?i)
denoting the set of M increasingly coarser equiv-
alence classifications of the history-state pi?i,
we linearly interpolate the higher order esti-
mates pML
(
w|fm(pi?i)
)
with lower order estimates
pML
(
w|fm?1(pi?i)
)
as
pJM(wi|fm(pi?i))
= ?fmpML(wi|fm(pi?i))
+(1? ?fm)pJM(wi|fm?1(pi?i)),
for 1 ? m ? M , where the 0-th order model
pJM(wi|f0(pi?i)) is a uniform distribution.
HW+HT :
(h.w0h.t0, h.w 1h.t 1, h.w 2h.t 2)
(h.w0h.t0)
()
(h.w0, h.t0, h.w 1, h.t 1, h.t 2)
(h.w0, h.t0, h.t 1)
(h.t0)
Saturday, April 14, 12
Figure 4: The hierarchal scheme of fine-to-coarse con-
texts used for Jelinek-Mercer smoothing in the SLM.
The coefficients ?fm(pi?i) are estimated on a held-
out set using the bucketing algorithm suggested by
Bahl (1983), which ties ?fm(pi?i)?s based on the
count of fm(pi?i)?s in the training data. We use the
expected count of the features from the last iteration
of EM training, since the pi?i are latent states.
We perform the bucketing algorithm for each level
f1, f2, ? ? ? , fM of equivalence classification sepa-
rately, and estimate the bucketed ?c(fm) using the
Baum-Welch algorithm (Baum, 1972) to maximize
the likelihood of held out data, where the word prob-
ability assignment in Eq. 1 is replaced with:
p(wi|W?i) =
|?i|?
j=1
pJM
(
wi|fM (pij?i)
)
pg(pij?i|W?i).
The hierarchy shown in Figure 4 is used1 for obtain-
ing a smooth estimate pJM(?|?) at each level.
5 SLM Experiments
We train a dependency SLM for two different tasks,
namely Broadcast News (BN) and Wall Street Jour-
nal (WSJ). Unlike Section 3.2, where we swept
through multiple training sets of multiple sizes,
1The original SLM hierarchical interpolation scheme is ag-
gressive in that it drops both the tag and headword from the
history. However, in many cases the headword?s tag alone is
sufficient, suggesting a more gradual interpolation. Keeping the
headtag adds more specific information and at the same time
is less sparse. A similar idea is found, e.g., in the back-off hi-
erarchical class n-gram language model (Zitouni, 2007) where
instead of backing off from the n-gram right to the (n ? 1)-
gram a more gradual backoff ? by considering a hierarchy of
fine-to-coarse classes for the last word in the history? is used.
55
training the SLM is computationally intensive. Yet,
useful insights may be gained from the 40M word
case. So we choose the source of text most suitable
for each task, and proceed as follows.
5.1 Setup
The following summarizes the setup for each
task:
? BN setup : EARS BN03 corpus, which has
about 42M words serves as our training text.
We also use rt04 (45K words) as our evaluation
data. Finally, to interpolate our structured lan-
guage models with the baseline 4-gram model,
we use rt03+dev04f (about 40K words) data sets
to serve as our development set. The vocabulary
we use in BN experiments has about 84K words.
? WSJ setup : The training text consists of about
37M words. We use eval92+eval93 (10K
words) as our evaluation set and dev93 (9K
words) serves as our development set for inter-
polating SLMs with the baseline 4-gram model.
In both cases, we sample about 20K sentences from
the training text (we exclude them from training
data) to serve as our heldout data for applying the
bucketing algorithm and estimating ??s. To apply
the dependency parser, all the data sets are first
converted to Treebank-style tokenization and POS-
tagged using the tagger of (Tsuruoka et al, 2011)2.
Both the POS-tagger and the shift-reduce depen-
dency parser are trained on the Broadcast News tree-
bank from Ontonotes (Weischedel et al, 2008) and
the WSJ Penn Treebank (after converting them to
dependency trees) which consists of about 1.2M to-
kens. Finally, we train a modified kneser-ney 4-gram
LM on the tokenized training text to serve as our
baseline LM, for both experiments.
5.2 Results and Analysis
Table 2 shows the perplexity results for BN and WSJ
experiments, respectively. It is evident that the 4-
gram baseline for BN is stronger than the 40M case
of Table 1. Yet, the interpolated SLM significantly
improves over the 4-gram LM, as it does for WSJ.
2To make sure we have a proper LM, the POS-tagger and
dependency parser only use features from history to tag a word
position and produce the dependency structure. All lookahead
features used in (Tsuruoka et al, 2011) and (Sagae and Tsujii,
Language Model Dev Eval
BN
Kneser-Ney 4-gram 165 158
SLM 168 159
KN+SLM Interpolation 147 142
WSJ
Kneser-Ney 4-gram 144 121
SLM 149 125
KN+SLM Interpolation 132 110
Table 2: Test set perplexities for different LMs on the BN
and WSJ tasks.
Also, to show that, in fact, the syntactic depen-
dencies modeled through the SLM parameterization
is enhancing predictive power of the LM in the prob-
lematic regions, i.e. syntactically-distant positions,
we calculate the following (log) probability ratio for
each position in the test data,
log pKN+SLM(wi|W?i)pKN(wi|W?i)
, (2)
where pKN+SLM is the word probability assign-
ment of the interpolated SLM at each position, and
pKN(wi) is the probability assigned by the baseline
4-gram model. The quantity above measures the im-
provement (or degradation) gained as a result of us-
ing the SLM parameterization3.
Figures 5(a) and 5(b) illustrate the histogram of
the above probability ratio for all the word positions
in evaluation data of BN and WSJ tasks, respectively.
In these figures the histograms for syntactically-
distant and syntactically-local are shown separately
to measure the effect of the SLM for either of the
position types. It can be observed in the figures
that for both tasks the percentage of positions with
log pKN+SLM(wi|W?i)pKN(wi|W?i) around zero is much higher for
syntactically-local (blue bars) than the syntactically-
distant (red bars). To confirm this, we calculate
the average log pKN+SLM(wi|W?i)pKN(wi|W?i) ?this is the aver-
age log-likelihood improvement, which is directly
2007) are excluded.
3If log pKN+SLM(wi|W?i)pKN(wi|W?i) is greater than zero, then the SLM
has a better predictive power for word position wi. This is a
meaningful comparison due to the fact that the probability as-
signment using both SLM and n-gram is a proper probability
(which sums to one over all words at each position).
56
?1 ?0.5 0 0.5 1 1.5 2 2.5 3 3.5 40
2
4
6
8
10
12
14
16
Probability Ratio (Log)
Percent
age Pos
itions (%)
 
 Syntactically?local positions    (mean=0.1372)Syntactically?distant postions  (mean=0.2351)
(a) BN
?1 ?0.5 0 0.5 1 1.5 2 2.5 3 3.5 402
46
810
1214
1618
2022
Probability Ratio (Log)
Percent
age Pos
itions (%)
 
 Syntactically?local positions    (mean=0.0984)Syntactically?distant postions  (mean=0.2124)
(b) WSJ
Figure 5: Probability ratio histogram of SLM to 4-gram
model for (a) BN task (b) WSJ task.
related to perplexity improvement? for each posi-
tion type in the figures.
Table 3, reports the perplexity performance of
each LM (baseline 4-gram, SLM and interpolated
SLM) on different positions of the evaluation data
for BN and WSJ tasks. As it can be observed from
this table, the use of long-span dependencies in the
SLM partially fills the gap between the performance
of the baseline 4-gram LM on syntactically-distant
positionsN versus syntactically-local positionsM.
In addition, it can be seen that the SLM by itself
fills the gap substantially, however, due to its under-
lying parameterization which is based on Jelinek-
Mercer smoothing it has a worse performance on
regular syntactically-local positions (which account
for the majority of the positions) compared to the
Kneser-Ney smoothed LM4. Therefore, to improve
the overall performance, the interpolated SLM takes
advantage of both the better modeling performance
of Kneser-Ney for syntactically-local positions and
4This is merely due to the superior modeling power and
better smoothing of the Kneser-Ney LM (Chen and Goodman,
1998).
Test Set 4-gram SLM 4-gram + SLM
Position BN
M 146 152 132
N 201 182 171
N +M 158 159 142
PPLN
PPLM
138% 120% 129%
WSJ
M 114 120 105
N 152 141 131
N +M 121 125 110
PPLN
PPLM
133% 117% 125%
Table 3: Perplexity on the BN and WSJ evaluation sets for
the 4-gram LM, SLM and their interpolation. The SLM
has lower perplexity than the 4-gram in syntactically dis-
tant positions N , and has a smaller discrepancy PPLNPPLM
between preplexity on the distant and local predictions,
complementing the 4-gram model.
the better features included in the SLM for improv-
ing predictive power on syntactically-distant posi-
tions.
6 Conclusion
The results of Table 1 and Figure 2 suggest that
predicting the next word is about 50% more diffi-
cult when its syntactic dependence on the history
reaches beyond n-gram range. They also suggest
that this difficulty does not diminish with increas-
ing training data size. If anything, the relative diffi-
culty of word positions with nonlocal dependencies
relative to those with local dependencies appears to
increase with increasing training data and n-gram
order. Finally, it appears that language models that
exploit long-distance syntactic dependencies explic-
itly at positions where the n-gram is least effective
are beneficial as complementary models.
Tables 2 and 3 demonstrates that a particular,
recently-proposed SLM with such properties im-
proves a 4-gram LM trained on a large corpus.
Acknowledgments
Thanks to Kenji Sagae for sharing his shift-reduce
dependency parser and the anonymous reviewers for
helpful comments.
57
References
LR Bahl. 1983. A maximum likelihood approach to
continuous speech recognition. IEEE Transactions
on Pattern Analysis and Machine Inteligence (PAMI),
5(2):179?190.
L. E. Baum. 1972. An equality and associated maxi-
mization technique in statistical estimation for proba-
bilistic functions of Markov processes. Inequalities,
3:1?8.
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283?332.
SF Chen and J Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report, Computer Science Group, Harvard Univer-
sity.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
M Collins, B Roark, and M Saraclar. 2005. Discrimina-
tive syntactic language modeling for speech recogni-
tion. In ACL.
Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine learning,
60:195?227.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In EMNLP.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, pages 355?372.
H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and
L. Young-Suk. 2009. Syntactic features for Arabic
speech recognition. In Proc. ASRU.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):330.
Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.
2011. Efficient discrimnative training of long-span
language models. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. EMNLP-CoNLL, volume 7, pages
1044?1050.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with Lookahead :
Can History-Based Models Rival Globally Optimized
Models ? In Proc. CoNLL, number June, pages 238?
246.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model unseen
events in speech recognition. Computer Speech &
Language, 21(1):88?104.
58
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 51?60,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Quantifying Mental Health Signals in Twitter
Glen Coppersmith Mark Dredze Craig Harman
Human Language Technology Center of Excellence
Johns Hopkins University
Balitmore, MD, USA
Abstract
The ubiquity of social media provides a
rich opportunity to enhance the data avail-
able to mental health clinicians and re-
searchers, enabling a better-informed and
better-equipped mental health field. We
present analysis of mental health phe-
nomena in publicly available Twitter data,
demonstrating how rigorous application of
simple natural language processing meth-
ods can yield insight into specific disor-
ders as well as mental health writ large,
along with evidence that as-of-yet undis-
covered linguistic signals relevant to men-
tal health exist in social media. We present
a novel method for gathering data for
a range of mental illnesses quickly and
cheaply, then focus on analysis of four in
particular: post-traumatic stress disorder
(PTSD), depression, bipolar disorder, and
seasonal affective disorder (SAD). We in-
tend for these proof-of-concept results to
inform the necessary ethical discussion re-
garding the balance between the utility of
such data and the privacy of mental health
related information.
1 Introduction
While mental health issues pose a significant
health burden on the general public, mental health
research lacks the quantifiable data available to
many physical health disciplines. This is partly
due to the complexity of the underlying causes
of mental illness and partly due to longstanding
societal stigma making the subject all but taboo.
Lack of data has hampered mental health research
in terms of developing reliable diagnoses and ef-
fective treatment for many disorders. Moreover,
population-level analysis via traditional methods
is time consuming, expensive, and often comes
with a significant delay.
In contrast, social media is plentiful and has
enabled diverse research on a wide range of top-
ics, including political science (Boydstun et al.,
2013), social science (Al Zamal et al., 2012), and
health at an individual and population level (Paul
and Dredze, 2011; Dredze, 2012; Aramaki et al.,
2011; Hawn, 2009). Of the numerous health top-
ics for which social media has been considered,
mental health may actually be the most appropri-
ate. A major component of mental health research
requires the study of behavior, which may be man-
ifest in how an individual acts, how they com-
municate, what activities they engage in and how
they interact with the world around them includ-
ing friends and family. Additionally, capturing
population level behavioral trends from Web data
has previously provided revolutionary capabilities
to health researchers (Ayers et al., 2014). Thus,
social media seems like a perfect fit for study-
ing mental health in both individual and overall
trends in the population. Such topics have already
been the focus of several studies (Coppersmith et
al., 2014; De Choudhury et al., 2014; De Choud-
hury et al., 2013d; De Choudhury et al., 2013b;
De Choudhury et al., 2013c; Ayers et al., 2013).
What can we expect to learn about mental health
by studying social media? How does a service like
Twitter inform our knowledge in this area? Nu-
merous studies indicate that language use, social
expression and interaction are telling indicators of
mental health. The well-known Linguistic Inquiry
Word Count (LIWC), a validated tool for the psy-
chometric analysis of language data (Pennebaker
et al., 2007), has been repeatedly used to study
language associated with all types of disorders
(Resnik et al., 2013; Alvarez-Conrad et al., 2001;
Tausczik and Pennebaker, 2010). Furthermore, so-
cial media is by nature social, which means that
social patterns, a critical part of mental health and
illness, may be readily observable in raw Twitter
data. Thus, Twitter and other social media provide
51
a unique quantifiable perspective on human behav-
ior that may otherwise go unobserved, suggesting
it as a powerful tool for mental health researchers.
The main vehicle for studying mental health in
social media has been the use of surveys, e.g.,
depression battery (De Choudhury, 2013) or per-
sonality test (Schwartz et al., 2013), to deter-
mine characteristics of a user coupled with analyz-
ing their corresponding social media data. Work
in this area has mostly focused on depression
(De Choudhury et al., 2013d; De Choudhury et al.,
2013b; De Choudhury et al., 2013c), and the num-
ber of users is limited by those that can complete
the appropriate survey. For example, De Choud-
hury et al. (2013d) solicited Twitter users to take
the CES-D and to share their public Twitter pro-
file, analyzing linguistic and behavioral patterns.
While this type of study has produced high qual-
ity data, it is limited in size (by survey respon-
dents) and scope (to diagnoses which have a bat-
tery amenable to administration over the internet).
In this paper we examine a range of mental
health disorders using automatically derived sam-
ples from large amounts of Twitter data. Rather
than rely on surveys, we automatically identify
self-expressions of mental illness diagnoses and
leverage these messages to construct a labeled data
set for analysis. Using this dataset, we make the
following contributions:
? We demonstrate the effectiveness of our au-
tomatically derived data by showing that sta-
tistical classifiers can differentiate users with
four different mental health disorders: de-
pression, bipolar, post traumatic stress disor-
der and seasonal affective disorder.
? We conduct a LIWC analysis of each dis-
order to measure deviations in each illness
group from a control group, replicating pre-
vious findings for depression and providing
new findings for bipolar, PTSD and SAD.
? We conduct an open-vocabulary analysis that
captures language use relevant to mental
health beyond what is captured with LIWC.
Our results open the door to a range of large scale
analysis of mental health issues using Twitter.
2 Related Work
For a good retrospective and prospective sum-
mary of the role of social media in mental health
research, we refer the reader to De Choudhury
(2013). De Choudhury identifies ways in which
NLP has and can be used on social media data to
produce what the relevant mental health literature
would predict, both at an individual level and a
population level. She proceeds to identify ways
in which these types of analyses can be used in
the near and far term to influence mental health
research and interventions alike.
Differences in language use have been observed
in the personal writing of students who score
highly on depression scales (Rude et al., 2004),
forum posts for depression (Ramirez-Esparza et
al., 2008), self narratives for PTSD (He et al.,
2012; D?Andrea et al., 2011; Alvarez-Conrad et
al., 2001), and chat rooms for bipolar (Kramer
et al., 2004). Specifically in social media, dif-
ferences have previously been observed between
depressed and control groups (as assessed by
internet-administered batteries) via LIWC: de-
pressed users more frequently use first person pro-
nouns (Chung and Pennebaker, 2007) and more
frequently use negative emotion words and anger
words on Twitter, but show no differences in posi-
tive emotion word usage (Park et al., 2012). Simi-
larly, an increase in negative emotion and first per-
son pronouns, and a decrease in third person pro-
nouns, (via LIWC) is observed, as well as many
manifestations of literature findings in the pattern
of life of depressed users (e.g., social engagement,
demographics) (De Choudhury et al., 2013d). Dif-
ferences in language use in social media via LIWC
have also been observed between PTSD and con-
trol groups (Coppersmith et al., 2014).
For population-level analysis, surveys such as
the Behavioral Risk Factor Surveillance System
(BRFSS) are conducted via telephone (Centers
for Disease Control and Prevention (CDC), 2010).
Some of these surveys cover relatively few par-
ticipants (often in the thousands), have significant
cost, and have long delays between data collec-
tion and dissemination of the findings. However,
De Choudhury et al. (2013c) presents a promising
population-level analysis of depression that high-
lights the role of NLP and social media.
3 Data
All data we obtain is public, posted between
2008 and 2013, and made available from Twitter
via their application programming interface (API).
Specifically, this does not include any data that has
52
Genuine Statements of Diagnosis
In loving memory my mom, she was only 42, I was 17 & taken away from me. I was diagnosed with having P.T.S.D LINK
So today I started therapy, she diagnosed me with anorexia, depression, anxiety disorder, post traumatic stress disorder and
wants me to
@USER The VA diagnosed me with PTSD, so I can?t go in that direction anymore
I wanted to share some things that have been helping me heal lately. I was diagnosed with severe complex PTSD and... LINK
Disingenuous Statements of Diagnosis
?I think I?m I?m diagnosed with SAD. Sexually active disorder? -anonymous
LOL omg my bro the ?psychologist? just diagnosed me with seasonal ADHD AHAHAHAAAAAAAAAAA IM DYING.
The winter blues: Yesterday I was diagnosed with seasonal affective disorder. Now, this sounds a lot more dramat... LINK
Table 1: Examples found via regular expression keyword search for diagnosis tweets.
been marked as ?private? by the author or any di-
rect messages.
Diagnosed Group We seek users who publicly
state that they have been diagnosed with various
mental illnesses. Users may make such a state-
ment to seek support from others in their social
network, to fight the taboo of mental illness, or
perhaps as an explanation of some of their behav-
ior. Tweets were obtained using regular expres-
sions on a large multi-year health related collec-
tion, e.g. ?I was diagnosed with X.? We searched
for four conditions: depression, bipolar disorder,
post traumatic stress disorder (PTSD) and sea-
sonal affective disorder (SAD). The matched diag-
nosis tweets were manually labeled as to whether
the tweet contained a genuine statement of a men-
tal health diagnosis. Table 1 shows examples of
both genuine statements of diagnosis and disin-
genuous statements (often jokes or quotes).
Next, we retrieved the most recent tweets (up
to 3200) for each user with a genuine diagnosis
tweet. We then filtered the users to remove those
with fewer than 25 tweets and those whose tweets
were not at least 75% in English (measured using
the Compact Language Detector
1
). These filter-
ing steps left us with users that were considered
positive examples. Table 2 indicates the number
of users and tweets found for each of the mental
health categories examined. We manually exam-
ined and annotated only half the diagnosis state-
ments for depression ? indicating there are likely
800-900 depression users available via these auto-
matic methods from our collection, compared to
the 117 obtained via the methods of De Choud-
hury et al. (2013d). Additionally, we emphasize
the low cost and effort of our automated effort
as compared to their crowdsourced survey meth-
1
https://code.google.com/p/cld2/
ods. The difference in collection methods also
suggests that the two have a reasonable chance of
being complementary. This is especially signif-
icant when considering disorders with lower in-
cidence rates than depression (arguably the high-
est), where respondents to crowdsourced surveys
or self-stated diagnoses alike are rare.
This method is similar in spirit to that of De
Choudhury et al. (2013c), where they inferred
a tweet-level classifier for depression from user-
level labels (specifically, tweets from the past three
months from users scoring highly on CES-D for
the positive class and conversely for the negative).
Control Group To build models for analysis
and to validate the data, we also need a sample of
the general population to use as an approximation
of community controls. We follow a similar pro-
cess: randomly select 10k usernames from a list
of Twitter users who posted to a separate random
historical collection within a selected two week
window, downloaded the 3200 most recent tweets
from these users, and apply our two filters: at least
25 tweets and 75% English. This yields a control
group of 5728 random users, whose 13.7 million
tweets were used as negative examples.
Caveats Our method for finding users with
mental health diagnoses has significant caveats: 1)
the method may only capture a subpopulation of
each disorder (i.e., those who are speaking pub-
licly about what is usually a very private mat-
ter), which may not truly represent all aspects of
the population as a whole. 2) This method in
no way verifies whether this diagnosis is genuine
(i.e., people are not always truthful in self-reports).
However, given the stigma often associated with
mental illness, it seems unlikely users would tweet
that they are diagnosed with a condition they do
not have. 3) The control group is likely contami-
53
Match Users Tweets
Bipolar 6k 394 992k
Depression 5k 441 1.0m
PTSD 477 244 573k
SAD 389 159 421k
Control 10k 5728 13.7m
Table 2: Number of users matching the diagnosis regular
expression, users labeled with genuine diagnoses and tweets
retrieved from diagnosed users for each mental health condi-
tion.
nated by the presence of users that are diagnosed
with the various conditions investigated. We make
no attempt to remove these users, and if we as-
sume that the prevalence of each disorder in the
general population is similar in our control groups,
we likely have hundreds of such diagnosed users
contaminating our control training data. 4) Twitter
users are not an entirely representative sample of
the population as a whole. Despite these caveats,
we find that this method yielded promising results
as discussed in the next sections.
Comorbidity Since some of these disorders
have high comorbidity, there are some users in
more than one class (e.g., those that state a diagno-
sis for PTSD and depression): Bipolar and depres-
sion have 19 users in common (4.8% of the bipo-
lar users, 4.3% of the depression users), PTSD and
depression share 10 (4.0% of PTSD, 2.2% of de-
pression), and bipolar and PTSD share 9 (2.2% of
bipolar, 3.6% of PTSD). Two users state diagnosis
of bipolar, PTSD and depression (less than 1% of
each set). No users stated diagnoses of both SAD
and any other condition investigated.
4 Methods
We quantify various aspects of each user?s lan-
guage usage and pattern of life via automated
methods, extracting features for subsequent ma-
chine learning. We use these to (1) replicate pre-
vious findings, (2) build classifiers to separate di-
agnosed from control users, and (3) introspect on
those classifiers. Introspection here shows us what
quantified signals in the content the classifiers base
their decision on, and thus we can gain intuition
about what signals are present in the content rele-
vant to mental health.
4.1 Linguistic Inquiry Word Count (LIWC)
LIWC provides clinicians with a tool for gather-
ing quantitative data regarding the state of a pa-
tient from the patient?s writing (Pennebaker et al.,
2007). Previous work has found signal in the ?pos-
itive affect? and ?negative affect? categories of the
LIWC when applied to social media (including
Twitter), so we examine their correlations sepa-
rately, as well as in the context of other LIWC
categories (De Choudhury et al., 2013a). In all,
we examine some of the LIWC categories directly
(Swear, Anger, PosEmo, NegEmo, Anx) and com-
bine pronoun classes by linguistic form: I and We
classes are combined to form Pro1, You becomes
Pro2 and SheHe and They become Pro3. Each of
these classes provides one feature used by subse-
quent machine learning and our other analyses.
4.2 Language Models (LMs)
Language models are commonly used to estimate
how likely a given sequence of words is. Gener-
ally, an n-gram language model refers to a model
that examines strings of up to n words long. This
is less than ideal for applications in social me-
dia: spelling errors, shortenings, space removal,
and other aspects of social media data (especially
Twitter) confounds many traditional word-based
approaches. Thus, we employ two LMs, first a
traditional 1-gram LM (ULM) that examines the
probability of each whole word. Second, a char-
acter 5-gram LM (CLM) to examine sequences of
up to 5 characters.
LMs model the likelihood of sequences from
training data. In our case, we build one of each
model from the positive class (tweets from one
class of diagnosed users ? e.g., PTSD), yield-
ing ULM
+
and CLM
+
. We also build one of
each model from the negative class (control users),
yielding ULM
?
and CLM
?
. We score each tweet
by computing these probabilities and classifying it
according to which model has a higher probability
(e.g., for a given tweet, is ULM
+
> ULM
?
?).
4.3 Pattern of Life Analytics
For brevity, we only briefly discuss the pattern of
life analytics, since they do not depend on sig-
nificant NLP. They examine how correlates found
to be significant in the mental health literature
may manifest and be measured in social media
data. These are all imperfect proxies for the find-
ings from the literature, but our experiments will
demonstrate that they do collectively provide in-
formation relevant to mental health.
For each of the following analytics we extract
one feature to use in subsequent machine learn-
ing. Social engagement has been correlated with
54
??
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?? ??
?
?
?
?
?
?
??
??
???
?
?
?
?
?
??
?
?
?
?
?
?
??
??
?
?
?
??? ??
?
? ?
?
??
?
? ?
???
?
?
????
?
???
?
?
?
???
?
?? ?
?
??
? ?
??
?
?
?
??
??
?
?
?
?
?????
?
?
?
?
?
?
?
?
??
?
?
?
??
?
?
?
?
?
?
?
?
?
???
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
????
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
0.00
0.05
0.10
0.15 Pro1* * Pro2* Pro3* Swear* Anger* PosEmo NegEmo* Anxiety****
0
0.00
5
0.01
0.01
5
Figure 1: Box and whiskers plot of proportion of tweets each user has (y-axis) matching various LIWC categories. Each
bar represents one LIWC category for one condition ? PTSD in purple, depression in blue, SAD in orange, bipolar in red and
control in gray. Anxiety occurs an order of magnitude less often than the others, so its proportion is on the right y-axis (and thus
not comparable to the others). Statistically significant deviations from control users are denoted by asterisks.
positive mental health outcomes (Greetham et al.,
2011; Berkman et al., 2000; Organization, 2001;
De Choudhury et al., 2013d), which is difficult
to measure directly so we examine various ways
in which this may be manifest in a user?s tweet
stream: Tweet rate measures how often a twit-
ter user posts (a measure of overall engagement
with this social media platform) and Proportion
of tweets with @mentions measures how often
a user posts ?in conversation? (for lack of better
terms) with other users. Number of @mentions is
a measure of how often the user in question en-
gages other users, while Number of self @men-
tions is a measure of how often the user responds
to mentions of themselves (since users rarely in-
clude their own username in a tweet). To estimate
the size of a user?s social network, we calculate
Number of unique users @mentioned and Number
of users @mentioned at least 3 times, respectively.
For each of the following analytics, we calcu-
late the proportion of a user?s tweets that the ana-
lytic finds evidence in: Insomnia and sleep distur-
bance is often a symptom of mental health disor-
ders (Weissman et al., 1996; De Choudhury et al.,
2013d), so we calculate the proportion of tweets
that a user makes between midnight and 4am ac-
cording to their local timezone. Exercise has
also been correlated with positive mental health
outcomes (Penedo and Dahn, 2005; Callaghan,
2004), so we examine tweets mentioning one of a
small set of exercise-related terms. We also use an
English sentiment analysis lexicon from Mitchell
et al. (2013) to score individual tweets according
to the presence and valence of sentiment words.
We apply no thresholds, so any tweet with a senti-
ment score above 0 was considered positive, below
0 was considered negative, and those with score 0
were considered to have no sentiment. Thus we
use the proportion of Insomnia, Exercise, Positive
Sentiment and Negative Sentiment tweets as fea-
tures in subsequent machine learning and analysis.
5 Results
We present three types of experiments to evalu-
ate the quality and character of these data, and to
demonstrate some quantifiable mental health sig-
nals in Twitter. First, we validate our method for
obtaining data by replicating previous findings us-
ing LIWC. Next, we build classifiers to distinguish
each group from the control group, demonstrating
that there is useful signal in the language of each
group, and compare these classifiers. Finally, we
analyze the correlations between our analytics and
classifiers to uncover relationships between them
and derive insight into quantifiable and relevant
mental health signals in Twitter.
Validation First, we provide some validation
for our novel method for gathering samples. We
demonstrate that language use, as measured by
LIWC, is statistically significantly different be-
tween control and diagnosed users. Figure 1
shows the proportion of tweets from each user
that scores positively on various LIWC categories
(i.e., have at least one word from that category).
Box-and-whiskers plots (Tukey, 1977)
2
summa-
rize a distribution of observations and ease com-
2
For a modern implementation see Wickham (2009).
55
False Alarm: 0.1 0.2
Bipolar 0.64 0.82
Depression 0.48 0.68
PTSD 0.67 0.81
SAD 0.42 0.65
Figure 2: ROC curves for separating diagnosed from con-
trol users, compared across disorders: bipolar in red, depres-
sion in blue, PTSD in purple, SAD in orange. The preci-
sion (diagnosed, correctly labeled) for each disorder at false
alarm (control, labeled as diagnosed) rates of 10% and 20%
are shown to the right of the ROC curve. Chance performance
is indicated by the dotted black line.
parison between them (here, each observation is
the proportion of a user?s tweets that score posi-
tively on LIWC). The median of the distribution
is the black horizontal line in the middle of the
bar, the bar covers the inter quartile range (where
50% of the observations lie), the whiskers are a
robust estimate of the extent of the data, with out-
liers plotted as circles beyond the whiskers. An
approximation of statistical significance is indi-
cated by the pinched in notches on each bar. If
the notches on the bars do not overlap, the dif-
ferences between those distributions is different
(?<0.05, 95% confidence interval). Each bar is
colored according to diagnosis, and each group
of 5 bars notes the scores for one LIWC cat-
egory. Differences that reach statistical signifi-
cance from the control group are noted with as-
terisks (e.g., Pro1, Swear, Anger, NegEmo and
Anxiety are statistically significantly different for
the depression group). Importantly, this repli-
cates previous findings of significant differences
between depressed users (according to an internet-
administered diagnostic battery): significant in-
creases are expected in NegEmo, Anger, Pro1 and
Pro3 and no change in PosEmo, given all previous
work (Park et al., 2012; Chung and Pennebaker,
2007; De Choudhury et al., 2013d). We repli-
cate all these findings except the increase in Pro3
(which only De Choudhury et al. (2013d) found),
which validates our data collection methods.
Classification We next explore the ability of
the various analytics to separate diagnosed from
control users and assess performance on a leave-
one-out cross-validation task. We train a log lin-
ear classifier on the features described in ?4 using
scikit-learn (Pedregosa et al., 2011).
Bipolar Depression
PTSD SAD
Figure 3: ROC curves of performance of individual analyt-
ics for each disorder: LIWC in blue, pattern of life in yellow,
CLM in red, ULM in green, all in black. Chance performance
is indicated by the dotted black line.
The receiver operating characteristic (ROC)
curves in Figures 2 and 3 demonstrate perfor-
mance of the various classifiers at the task of sepa-
rating diagnosed from control groups. In all cases,
the correct detections (or hits) are on the y-axis
and the false detections (or false alarms) are on
the x-axis. Figure 2 compares performance across
diagnoses, one line per disorder.
Figure 3 shows one plot per mental health con-
dition, with the performance of the various an-
alytics, individually and in concert as individual
ROC curves. A few trends emerge ? 1) All an-
alytics show some ability to separate the classes,
indicating they are finding useful signals. 2) The
LMs provide superior performance to the other an-
alytics, indicating there are more signals present
in the language than are captured by LIWC and
pattern-of-life analytics. For readability we do not
show the performance of all combinations of an-
alytics, but they perform as expected: any set of
them perform equal to or better than their indi-
vidual components. Taken together, this indicates
that there is information relevant to separating di-
agnosed users from controls in all the analytics
discussed here. Furthermore, this highlights that
there remains significant signals to be uncovered
and understood in the language of social media.
These trends also allow us to compare the dis-
orders as manifest in language usage, though this
56
tends to raise more questions than it answers. Gen-
erally, the pattern-of-life analytics and LIWC are
on par, but this is decidedly not true for depres-
sion, where pattern-of-life seems to perform espe-
cially poorly, and for SAD, where pattern-of-life
seems to perform especially well. This indicates
that the depression users have patterns-of-life that
look more similar to the controls than is the case
for the other disorders (perhaps especially surpris-
ing given the inclusion of the sentiment lexicon)
and that there may be significant correlation be-
tween pattern-of-life factors and SAD.
5.1 Analytic Introspection
To examine correlations between the analytics and
the linguistic content they depend on, we scored
a random subset of 1 million tweets from control
users with each of the linguistic analytics, and plot
their Pearson?s correlation coefficients (r) in Fig-
ure 4. A simple overlap of wordlists is not suf-
ficient to assess the true utility of these methods
since it does not take into account the frequency
of occurrence of each word, nor the correlation be-
tween these words in real data (e.g., does a classi-
fier based on the LIWC category Swear provide
redundant information to the sentiment analysis).
Each row and column in Figure 4 represents one of
the 17 analytics, in the same order. Colors denote
Bonferroni-corrected Pearson?s r for statistically
significant correlations between the analytic on the
row and column. Correlations that do not reach
statistical significance are in aquamarine (corre-
sponding to r=0). Excluded for brevity is a sanity
check of a ?
2
test between the analytics to assert
they were scoring significantly differently.
The strong correlations between the various
LIWC analytics, notably Swear, Anger and
NegEmo, likely indicates that the analytics are
triggered by the same word(s) ? in this case pro-
fanity. Similarly for LIWC?s PosEmo and the sen-
timent lexicon ? ?happy? for example. The corre-
lation between CLM for various diagnoses is par-
ticularly intriguingly, as it is in line with known
patterns of comorbidity: major depressive disor-
der, PTSD, and bipolar all have observed comor-
bidity (Brady et al., 2000; Campbell et al., 2007;
McElroy et al., 2001) while SAD is currently con-
sidered a specifier of major depressive disorder or
bipolar disorder (American Psychiatric Associa-
tion, 2013; Lurie et al., 2006), without published
findings indicating comorbidity. Indeed our small
Figure 4: Pearson?s r correlations between various analyt-
ics, color indicates the strength of statistically significant cor-
relations, or 0 (aquamarine) otherwise. Bonferroni corrected,
each comparison is significant only if ?<0.0002). Rows and
columns represent the analytics in the same order, so the di-
agonal is self-correlation.
sample dataset follows the same trends, where
we observed users with multiple diagnoses exist
within depression, PTSD, and bipolar, but none
exist with SAD. The correlation observed is too
large to be solely attributed to those users shared
between the groups, though (correlations at most
r = 0.05 would be attributable to that alone). Fur-
thermore, when taken in combination with the dif-
ferent patterns exhibited by the groups as seen in
Figure 1, this correlation is not solely attributable
to LIWC categories either. At its core, these cor-
relations seem to suggest that similar language
is employed by users diagnosed with these occa-
sionally comorbid disorders, and dissimilar lan-
guage by users with SAD. This should be taken as
merely suggestive of the type of analysis one could
do, though, since the literature does not present a
strong and clear prediction for the comorbidity and
exhibited symptoms (to include language use).
Interestingly, the lack of (or negative) correla-
tion between most of the analytics again highlights
the complexity of the mental illnesses and the di-
vergent signals it presents. Additionally, the lack
of correlation between ULM and the other models
is to be expected, since they are basing their scores
on significantly more words (or different signals as
is the case for CLM). Each one of these analytics is
highly imperfect, and often give contradictory ev-
idence, but when combined, the machine learning
algorithms are able to sort through the conflicting
signals with some success.
57
Analytic Example Tweet Text
Bipolar LM I?m insecure because being around your ex of 4 years little sister, makes me feel a slight bit uncom-
fortable. Ok.
Depression LM Pain has a weird way of working. You?re still the same person from before the pain, but that person is
underneath & doesn?t come out.
PTSD LM Don?t wanna get out my bed but I really need to get up & prepare myself for work
Sentiment(+) NAME is absolutely unbelievable, he just gets better and better every time I see him. The best play in
the world, no doubt about it.
Sentiment(-) I hate losing people in my life. I try so hard to not let it happen
PosEmo Wowee...that was a hectic day... Got more done than expected but so glad to be in bed now. Grateful
for my supportive husband & loving pooch
Functioning if i had a dollar for all the grammatical errors ive ever typed, my college tuition, book cost, and dorm
rent would be paid in full
NegEmo My tooth hurts, my neck hurts, my mouth hurts, my toungue hurts, my head hurts...kill me now.
Anx don?t stress over someone who is going to stress over you..
Anger Ugly n arrogant sums everytin up.shdnt hv ffd her seff
Table 3: Example high scoring tweets from each analytic.
6 Conclusion
We demonstrate quantifiable signals in Twitter
data relevant to bipolar disorder, major depres-
sive disorder, post-traumatic-stress disorder and
seasonal affective disorder. We introduce a novel
method for automatic data collection and validate
its veracity by 1) replicating observations of sig-
nificant differences between depressed and control
user groups and 2) constructing classifiers capa-
ble of separating diagnosed from control users for
each disorder. This data allows us to demonstrate
equivalent differences in language use (according
to LIWC) for bipolar, PTSD, and SAD. Further-
more, we provide evidence that more information
relevant to mental health is encoded in language
use in social media (above and beyond that cap-
tured by methods based on the mental health lit-
erature). By examining correlations between the
various analytics investigated, we provide some
insight into what quantifiable linguistic informa-
tion is captured by our classifiers. We finally
demonstrate the utility of examining multiple dis-
orders simultaneously and other larger analyses,
difficult or impossible with other methods.
Crucially, we expect that these novel data col-
lection methods can provide complementary infor-
mation to existing survey-based methods, rather
than supplant them. For many disorders rarer
than depression (which has comparatively high in-
cidence rates), we suspect that finding any data
will be a challenge, in which case combining
these methods with the existing survey collection
methods may be the best way to obtain sufficient
amounts of data for statistical analyses.
Since the LMs take more information into ac-
count when modeling the language usage of di-
agnosed and control users, it is unsurprising that
they outperform LIWC and pattern-of-life analy-
ses alone, but this is evidence of as-of-yet undis-
covered linguistic differences between diagnosed
and control users for all disorders investigated.
Uncovering and interpreting these signals can be
best accomplished through collaboration between
NLP and mental health researchers.
Naturally, some caveats come with these re-
sults: while identifying genuine self-statements of
diagnosis in Twitter works well for some condi-
tions, others exist for which there were few or
no diagnoses stated. For Alzheimer?s, the demo-
graphic with the majority of diagnoses does not
frequently use Twitter (or likely any social me-
dia). Eating disorders are also elusive via this
method, though related automatic methods (e.g.,
using disorder-related hashtags) may address this.
Finally, those willing to publicly reveal a mental
health diagnosis may not be representative of the
population suffering from that mental illness.
All these experiments, taken together, indicate
that there are a diverse set of quantifiable signals
relevant to mental health observable in Twitter.
They indicate that individual- and population-level
analyses can be made cheaper and more timely
than current methods, yet there remains as-of-yet
untapped information encoded in language use ?
promising a rich collaboration between the fields
of natural language processing and mental health.
Acknowledgments: The authors would like to
thank Kristy Hollingshead for thoughtful com-
ments and contributions throughout this research.
58
References
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Jennifer Alvarez-Conrad, Lori A. Zoellner, and
Edna B. Foa. 2001. Linguistic predictors of trauma
pathology and physical health. Applied Cognitive
Psychology, 15(7):S159?S170.
American Psychiatric Association. 2013. Diagnostic
Statistical Manual 5. American Psychiatric Associ-
ation.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using twitter. In Empirical Natural Lan-
guage Processing Conference (EMNLP).
John W. Ayers, Benjamin M. Althouse, Jon-Patrick
Allem, J. Niels Rosenquist, and Daniel E. Ford.
2013. Seasonality in seeking mental health infor-
mation on google. American journal of preventive
medicine, 44(5):520?525.
John W. Ayers, Benjamin M. Althouse, and Mark
Dredze. 2014. Could behavioral medicine lead the
web data revolution? Journal of the American Med-
ical Association (JAMA), February 27.
Lisa F. Berkman, Thomas Glass, Ian Brissette, and
Teresa E. Seeman. 2000. From social integration
to health: Durkheim in the new millennium? Social
Science & Medicine, 51(6):843?857, September.
Amber Boydstun, Rebecca Glazier, Timothy Jurka, and
Matthew Pietryka. 2013. Examining debate effects
in real time: A report of the 2012 React Labs: Ed-
ucate study. The Political Communication Report,
23(1), February. [Online; accessed 25-February-
2014].
Kathleen T. Brady, Therese K. Killeen, Tim Brewerton,
and Sylvia Lucerini. 2000. Comorbidity of psy-
chiatric disorders and posttraumatic stress disorder.
Journal of Clinical Psychiatry.
Patrick Callaghan. 2004. Exercise: a neglected inter-
vention in mental health care? Journal of Psychi-
atric and Mental Health Nursing, 11:476?483.
Duncan G. Campbell, Bradford L. Felker, Chuan-Fen
Liu, Elizabeth M. Yano, JoAnn E. Kirchner, Domin
Chan, Lisa V. Rubenstein, and Edmund F. Chaney.
2007. Prevalence of depression-PTSD comorbidity:
Implications for clinical practice guidelines and pri-
mary care-based interventions. Journal of General
Internal Medicine, 22(6):711?718.
Centers for Disease Control and Prevention (CDC).
2010. Behavioral risk factor surveillance system
survey data.
Cindy Chung and James Pennebaker. 2007. The psy-
chological functions of function words. Social com-
munication, pages 343?359.
Glen A. Coppersmith, Craig T. Harman, and Mark
Dredze. 2014. Measuring post traumatic stress
disorder in Twitter. In Proceedings of the Interna-
tional AAAI Conference on Weblogs and Social Me-
dia (ICWSM).
Wendy D?Andrea, Pearl H. Chiu, Brooks R. Casas,
and Patricia Deldin. 2011. Linguistic predictors of
post-traumatic stress disorder symptoms following
11 September 2001. Applied Cognitive Psychology,
26(2):316?323, October.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013a. Major life changes and behav-
ioral markers in social media: Case of childbirth. In
Proceedings of the ACM Conference on Computer
Supported Cooperative Work and Social Computing
(CSCW).
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013b. Predicting postpartum changes in
emotion and behavior via social media. In Proceed-
ings of the ACM Annual Conference on Human Fac-
tors in Computing Systems (CHI), pages 3267?3276.
ACM.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013c. Social media as a measurement
tool of depression in populations. In Proceedings of
the Annual ACM Web Science Conference.
Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013d. Predicting de-
pression via social media. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).
Munmun De Choudhury, Andres Monroy-Hernandez,
and Gloria Mark. 2014. ? narco? emotions: Affect
and desensitization in social media during the mexi-
can drug war.
Munmun De Choudhury. 2013. Role of social media
in tackling challenges in mental health. In Proceed-
ings of the 2nd International Workshop on Socially-
Aware Multimedia, pages 49?52.
Mark Dredze. 2012. How social media will change
public health. IEEE Intelligent Systems, 27(4):81?
84.
Danica Vukadinovic Greetham, Robert Hurling,
Gabrielle Osborne, and Alex Linley. 2011. Social
networks and positive and negative affect. Procedia
- Social and Behavioral Sciences, 22:4?13, January.
Carleen Hawn. 2009. Take Two Aspirin And Tweet
Me In The Morning: How Twitter, Facebook, And
Other Social Media Are Reshaping Health Care.
Health Affairs, 28(2):361?368.
59
Qiwei He, Bernard P. Veldkamp, and Theo de Vries.
2012. Screening for posttraumatic stress disorder
using verbal features in self narratives: A text min-
ing approach. Psychiatry Research.
Adam D. I. Kramer, Susan R. Fussell, and Leslie D.
Setlock. 2004. Text analysis as a tool for analyz-
ing conversation in online support groups. In Pro-
ceedings of the ACM Annual Conference on Human
Factors in Computing Systems (CHI).
Stephen J. Lurie, Barbara Gawinski, Deborah Pierce,
and Sally J. Rousseau. 2006. Seasonal affective dis-
order. American family physician, 74(9).
Susan L. McElroy, Lori L. Altshuler, Trisha Suppes,
Paul E. Keck, Mark A. Frye, Kirk D. Denicoff,
Willem A. Nolen, Ralph W. Kupka, Gabriele S. Lev-
erich, Jennifer R. Rochussen, A. John Rush Rush,
and Robert M. Post Post. 2001. Axis I psychi-
atric comorbidity and its relationship to historical ill-
ness variables in 288 patients with bipolar disorder.
American Journal of Psychiatry, 158(3):420?426.
Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-
son, and Benjamin Van Durme. 2013. Open domain
targeted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643?1654.
World Health Organization. 2001. The world health
report 2001 - Mental health: New understanding,
new hope. Technical report, Genf, Schweiz.
Minsu Park, Chiyoung Cha, and Meeyoung Cha. 2012.
Depressive moods of users portrayed in Twitter. In
Proceedings of the ACM SIGKDD Workshop on
Healthcare Informatics (HI-KDD).
Michael J. Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
and Matthieu Perrot
?
Edouard Duchesnay. 2011.
scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825?2830.
Frank J. Penedo and Jason R. Dahn. 2005. Exer-
cise and well-being: a review of mental and phys-
ical health benefits associated with physical activ-
ity. Current Opinion in Psychiatry, 18(2):189?193,
March.
James W. Pennebaker, Cindy K. Chung, Molly Ire-
land, Amy Gonzales, and Roger J. Booth. 2007.
The development and psychometric properties of
LIWC2007.
Nairan Ramirez-Esparza, Cindy K. Chung, Ewa
Kacewicz, and James W. Pennebaker. 2008. The
psychology of word use in depression forums in En-
glish and in Spanish: Testing two text analytic ap-
proaches. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM).
Philip Resnik, Anderson Garron, and Rebecca Resnik.
2013. Using topic modeling to improve prediction
of neuroticism and depression. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural, pages 1348?1353.
Stephanie S. Rude, Eva-Maria Gortner, and James W.
Pennebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133, December.
H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M.
Ramones, Megha Agrawal, Achal Shah, Michal
Kosinski, David Stillwell, Martin E. P. Seligman,
and Lyle H. Ungar. 2013. Personality, gender,
and age in the language of social media: The open-
vocabulary approach. PLOS One, 8(9).
Yla R. Tausczik and James W. Pennebaker. 2010. The
psychological meaning of words: LIWC and com-
puterized text analysis methods. Journal of Lan-
guage and Social Psychology, 29(1):24?54.
John W. Tukey. 1977. Box-and-whisker plots. Ex-
ploratory Data Analysis, pages 39?43.
Myrna M. Weissman, Roger C. Bland, Glorisa J.
Canino, Carlo Faravelli, Steven Greenwald, Hai-
Gwo Hwu, Peter R. Joyce, Eile G. Karam, Chung-
Kyoon Lee, Joseph Lellouch, Jean-Pierre L?epine,
Stephen C. Newman, Maritza Rubio-Stipec, J. Elis-
abeth Wells, Priya J. Wickramaratne, Hans-Ulrich
Wittchen, and Eng-Kung Yeh. 1996. Cross-national
epidemiology of major depression and bipolar dis-
order. Journal of the American Medical Association
(JAMA), 276(4):293?299.
Hadley Wickham. 2009. ggplot2: elegant graphics for
data analysis. Springer.
60
