Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 263?270,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminative Reranking for Semantic Parsing
Ruifang Ge Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712
{grf,mooney}@cs.utexas.edu
Abstract
Semantic parsing is the task of mapping
natural language sentences to complete
formal meaning representations. The per-
formance of semantic parsing can be po-
tentially improved by using discrimina-
tive reranking, which explores arbitrary
global features. In this paper, we investi-
gate discriminative reranking upon a base-
line semantic parser, SCISSOR, where the
composition of meaning representations is
guided by syntax. We examine if features
used for syntactic parsing can be adapted
for semantic parsing by creating similar
semantic features based on the mapping
between syntax and semantics. We re-
port experimental results on two real ap-
plications, an interpreter for coaching in-
structions in robotic soccer and a natural-
language database interface. The results
show that reranking can improve the per-
formance on the coaching interpreter, but
not on the database interface.
1 Introduction
A long-standing challenge within natural language
processing has been to understand the meaning of
natural language sentences. In comparison with
shallow semantic analysis tasks, such as word-
sense disambiguation (Ide and Jeane?ronis, 1998)
and semantic role labeling (Gildea and Jurafsky,
2002; Carreras and Ma`rquez, 2005), which only
partially tackle this problem by identifying the
meanings of target words or finding semantic roles
of predicates, semantic parsing (Kate et al, 2005;
Ge and Mooney, 2005; Zettlemoyer and Collins,
2005) pursues a more ambitious goal ? mapping
natural language sentences to complete formal
meaning representations (MRs), where the mean-
ing of each part of a sentence is analyzed, includ-
ing noun phrases, verb phrases, negation, quanti-
fiers and so on. Semantic parsing enables logic
reasoning and is critical in many practical tasks,
such as speech understanding (Zue and Glass,
2000), question answering (Lev et al, 2004) and
advice taking (Kuhlmann et al, 2004).
Ge and Mooney (2005) introduced an approach,
SCISSOR, where the composition of meaning rep-
resentations is guided by syntax. First, a statis-
tical parser is used to generate a semantically-
augmented parse tree (SAPT), where each internal
node includes both a syntactic and semantic label.
Once a SAPT is generated, an additional meaning-
composition process guided by the tree structure is
used to translate it into a final formal meaning rep-
resentation.
The performance of semantic parsing can be po-
tentially improved by using discriminative rerank-
ing, which explores arbitrary global features.
While reranking has benefited many tagging and
parsing tasks (Collins, 2000; Collins, 2002c;
Charniak and Johnson, 2005) including semantic
role labeling (Toutanova et al, 2005), it has not
yet been applied to semantic parsing. In this paper,
we investigate the effect of discriminative rerank-
ing to semantic parsing.
We examine if the features used in reranking
syntactic parses can be adapted for semantic pars-
ing, more concretely, for reranking the top SAPTs
from the baseline model SCISSOR. The syntac-
tic features introduced by Collins (2000) for syn-
tactic parsing are extended with similar semantic
features, based on the coupling of syntax and se-
mantics. We present experimental results on two
corpora: an interpreter for coaching instructions
263
in robotic soccer (CLANG) and a natural-language
database interface (GeoQuery). The best rerank-
ing model significantly improves F-measure on
CLANG from 82.3% to 85.1% (15.8% relative er-
ror reduction), however, it fails to show improve-
ments on GEOQUERY.
2 Background
2.1 Application Domains
2.1.1 CLANG: the RoboCup Coach Language
RoboCup (www.robocup.org) is an inter-
national AI research initiative using robotic soccer
as its primary domain. In the Coach Competition,
teams of agents compete on a simulated soccer
field and receive advice from a team coach in
a formal language called CLANG. In CLANG,
tactics and behaviors are expressed in terms of
if-then rules. As described in Chen et al (2003),
its grammar consists of 37 non-terminal symbols
and 133 productions. Negation and quantifiers
like all are included in the language. Below is a
sample rule with its English gloss:
((bpos (penalty-area our))
(do (player-except our {4})
(pos (half our))))
?If the ball is in our penalty area, all our players
except player 4 should stay in our half.?
2.1.2 GEOQUERY: a DB Query Language
GEOQUERY is a logical query language for
a small database of U.S. geography containing
about 800 facts. The GEOQUERY language
consists of Prolog queries augmented with several
meta-predicates (Zelle and Mooney, 1996). Nega-
tion and quantifiers like all and each are included
in the language. Below is a sample query with its
English gloss:
answer(A,count(B,(city(B),loc(B,C),
const(C,countryid(usa))),A))
?How many cities are there in the US??
2.2 SCISSOR: the Baseline Model
SCISSOR is based on a fairly standard approach
to compositional semantics (Jurafsky and Martin,
2000). First, a statistical parser is used to con-
struct a semantically-augmented parse tree that
captures the semantic interpretation of individual
NP-PLAYER
PRP$-TEAM
our
NN-PLAYER
player
CD-UNUM
2
Figure 1: A SAPT for describing a simple CLANG
concept PLAYER .
words and the basic predicate-argument structure
of a sentence. Next, a recursive deterministic pro-
cedure is used to compose the MR of a parent
node from the MR of its children following the
tree structure.
Figure 1 shows the SAPT for a simple natural
language phrase describing the concept PLAYER
in CLANG. We can see that each internal node
in the parse tree is annotated with a semantic la-
bel (shown after dashes) representing concepts in
an application domain; when a node is semanti-
cally vacuous in the application domain, it is as-
signed with the semantic label NULL. The seman-
tic labels on words and non-terminal nodes repre-
sent the meanings of these words and constituents
respectively. For example, the word our repre-
sents a TEAM concept in CLANG with the value
our, whereas the constituent OUR PLAYER 2 rep-
resents a PLAYER concept. Some type concepts
do not take arguments, like team and unum (uni-
form number), while some concepts, which we
refer to as predicates, take an ordered list of ar-
guments, like player which requires both a TEAM
and a UNUM as its arguments.
SAPTs are given to a meaning composition
process to compose meaning, guided by both
tree structures and domain predicate-argument re-
quirements. In figure 1, the MR of our and 2
would fill the arguments of PLAYER to generate
the MR of the whole constituent PLAYER(OUR,2)
using this process.
SCISSOR is implemented by augmenting
Collins? (1997) head-driven parsing model II to
incorporate the generation of semantic labels on
internal nodes. In a head-driven parsing model,
a tree can be seen as generated by expanding
non-terminals with grammar rules recursively.
To deal with the sparse data problem, the expan-
sion of a non-terminal (parent) is decomposed
into primitive steps: a child is chosen as the
head and is generated first, and then the other
children (modifiers) are generated independently
264
BACK-OFFLEVEL PL1(Li|...)
1 P,H,w,t,?,LC
2 P,H,t,?,LC
3 P,H,?,LC
4 P,H
5 P
Table 1: Extended back-off levels for the semantic
parameter PL1(Li|...), using the same notation as
in Ge and Mooney (2005). The symbols P , H and
Li are the semantic label of the parent , head, and
the ith left child, w is the head word of the parent,
t is the semantic label of the head word, ? is the
distance between the head and the modifier, and
LC is the left semantic subcat.
constrained by the head. Here, we only describe
changes made to SCISSOR for reranking, for a
full description of SCISSOR see Ge and Mooney
(2005).
In SCISSOR, the generation of semantic labels
on modifiers are constrained by semantic subcat-
egorization frames, for which data can be very
sparse. An example of a semantic subcat in Fig-
ure 1 is that the head PLAYER associated with NN
requires a TEAM as its modifier. Although this
constraint improves SCISSOR?s precision, which
is important for semantic parsing, it also limits
its recall. To generate plenty of candidate SAPTs
for reranking, we extended the back-off levels for
the parameters generating semantic labels of mod-
ifiers. The new set is shown in Table 1 using the
parameters for the generation of the left-side mod-
ifiers as an example. The back-off levels 4 and 5
are newly added by removing the constraints from
the semantic subcat. Although the best SAPTs
found by the model may not be as precise as be-
fore, we expect that reranking can improve the re-
sults and rank correct SAPTs higher.
2.3 The Averaged Perceptron Reranking
Model
Averaged perceptron (Collins, 2002a) has been
successfully applied to several tagging and parsing
reranking tasks (Collins, 2002c; Collins, 2002a),
and in this paper, we employed it in reranking
semantic parses generated by the base semantic
parser SCISSOR. The model is composed of three
parts (Collins, 2002a): a set of candidate SAPTs
GEN , which is the top n SAPTs of a sentence
from SCISSOR; a function ? that maps a sentence
Inputs: A set of training examples (xi, y?i ), i = 1...n, where xi
is a sentence, and y?i is a candidate SAPT that has the highest
similarity score with the gold-standard SAPT
Initialization: Set W? = 0
Algorithm:
For t = 1...T, i = 1...n
Calculate yi = arg maxy?GEN(xi) ?(xi, y) ? W?
If (yi 6= y?i ) then W? = W? + ?(xi, y?i ) ? ?(xi, yi)
Output: The parameter vector W?
Figure 2: The perceptron training algorithm.
x and its SAPT y into a feature vector ?(x, y) ?
Rd; and a weight vector W? associated with the set
of features. Each feature in a feature vector is a
function on a SAPT that maps the SAPT to a real
value. The SAPT with the highest score under a
parameter vector W? is outputted, where the score
is calculated as:
score(x, y) = ?(x, y) ? W? (1)
The perceptron training algorithm for estimat-
ing the parameter vector W? is shown in Fig-
ure 2. For a full description of the algorithm,
see (Collins, 2002a). The averaged perceptron, a
variant of the perceptron algorithm is often used in
testing to decrease generalization errors on unseen
test examples, where the parameter vectors used
in testing is the average of each parameter vector
generated during the training process.
3 Features for Reranking SAPTs
In our setting, reranking models discriminate be-
tween SAPTs that can lead to correct MRs and
those that can not. Intuitively, both syntactic and
semantic features describing the syntactic and se-
mantic substructures of a SAPT would be good in-
dicators of the SAPT?s correctness.
The syntactic features introduced by Collins
(2000) for reranking syntactic parse trees have
been proven successfully in both English and
Spanish (Cowan and Collins, 2005). We exam-
ine if these syntactic features can be adapted for
semantic parsing by creating similar semantic fea-
tures. In the following section, we first briefly de-
scribe the syntactic features introduced by Collins
(2000), and then introduce two adapted semantic
feature sets. A SAPT in CLANG is shown in Fig-
ure 3 for illustrating the features throughout this
section.
265
VP-ACTION.PASS
VB
be
VP-ACTION.PASS
VBN-ACTION.PASS
passed
PP-POINT
TO
to
NP-POINT
PRN-POINT
-LRB?POINT
(
NP-NUM1
CD-NUM
36
COMMA
,
NP-NUM2
CD-NUM
10
-RRB-
)
Figure 3: A SAPT for illustrating the reranking features, where the syntactic label ?,? is replaced by
COMMA for a clearer description of features, and the NULL semantic labels are not shown. The head
of the rule ?PRN-POINT? -LRB?POINT NP-NUM1 COMMA NP-NUM2 -RRB-? is -LRB?POINT. The
semantic labels NUM1 and NUM2 are meta concepts in CLANG specifying the semantic role filled since
NUM can fill multiple semantic roles in the predicate POINT.
3.1 Syntactic Features
All syntactic features introduced by Collins (2000)
are included for reranking SAPTs. While the full
description of all the features is beyond the scope
of this paper, we still introduce several feature
types here for the convenience of introducing se-
mantic features later.
1. Rules. These are the counts of unique syntac-
tic context-free rules in a SAPT. The example
in Figure 3 has the feature f (PRN? -LRB- NP
COMMA NP -RRB-)=1.
2. Bigrams. These are the counts of unique
bigrams of syntactic labels in a constituent.
They are also featured with the syntactic la-
bel of the constituent, and the bigram?s rel-
ative direction (left, right) to the head of the
constituent. The example in Figure 3 has the
feature f (NP COMMA, right, PRN)=1.
3. Grandparent Rules. These are the same as
Rules, but also include the syntactic label
above a rule. The example in Figure 3 has
the feature f ([PRN? -LRB- NP COMMA NP
-RRB-], NP)=1, where NP is the syntactic la-
bel above the rule ?PRN? -LRB- NP COMMA
NP -RRB-?.
4. Grandparent Bigrams. These are the same
as Bigrams, but also include the syntactic
label above the constituent containing a bi-
gram. The example in Figure 3 has the
feature f ([NP COMMA, right, PRN], NP)=1,
where NP is the syntactic label above the con-
stituent PRN.
3.2 Semantic Features
3.2.1 Semantic Feature Set I
A similar semantic feature type is introduced for
each syntactic feature type used by Collins (2000)
by replacing syntactic labels with semantic ones
(with the semantic label NULL not included). The
corresponding semantic feature types for the fea-
tures in Section 3.1 are:
1. Rules. The example in Figure 3 has the fea-
ture f (POINT? POINT NUM1 NUM2)=1.
2. Bigrams. The example in Figure 3 has the
feature f (NUM1 NUM2, right, POINT)=1,
where the bigram ?NUM1 NUM2?appears to
the right of the head POINT.
3. Grandparent Rules. The example in Figure 3
has the feature f ([POINT? POINT NUM1
NUM2], POINT)=1, where the last POINT is
266
ACTION.PASS
ACTION.PASS
passed
POINT
POINT
(
NUM1
NUM
36
NUM2
NUM
10
Figure 4: The tree generated by removing purely-
syntactic nodes from the SAPT in Figure 3 (with
syntactic labels omitted.)
the semantic label above the semantic rule
?POINT? POINT NUM1 NUM2?.
4. Grandparent Bigrams. The example in Fig-
ure 3 has the feature f ([NUM1 NUM2, right,
POINT], POINT)=1, where the last POINT is
the semantic label above the POINT associ-
ated with PRN.
3.2.2 Semantic Feature Set II
Purely-syntactic structures in SAPTs exist with
no meaning composition involved, such as the ex-
pansions from NP to PRN, and from PP to ?TO NP?
in Figure 3. One possible drawback of the seman-
tic features derived directly from SAPTs as in Sec-
tion 3.2.1 is that they could include features with
no meaning composition involved, which are in-
tuitively not very useful. For example, the nodes
with purely-syntactic expansions mentioned above
would trigger a semantic rule feature with mean-
ing unchanged (from POINT to POINT). Another
possible drawback of these features is that the fea-
tures covering broader context could potentially
fail to capture the real high-level meaning compo-
sition information. For example, the Grandparent
Rule example in Section 3.2.1 has POINT as the
semantic grandparent of a POINT composition, but
not the real one ACTION.PASS.
To address these problems, another semantic
feature set is introduced by deriving semantic fea-
tures from trees where purely-syntactic nodes of
SAPTs are removed (the resulting tree for the
SAPT in Figure 3 is shown in Figure 4). In this
tree representation, the example in Figure 4 would
have the Grandparent Rule feature f ([POINT?
POINT NUM1 NUM2], ACTION.PASS)=1, with the
correct semantic grandparent ACTION.PASS in-
cluded.
4 Experimental Evaluation
4.1 Experimental Methodology
Two corpora of natural language sentences paired
with MRs were used in the reranking experiments.
For CLANG, 300 pieces of coaching advice were
randomly selected from the log files of the 2003
RoboCup Coach Competition. Each formal in-
struction was translated into English by one of
four annotators (Kate et al, 2005). The average
length of an natural language sentence in this cor-
pus is 22.52 words. For GEOQUERY, 250 ques-
tions were collected by asking undergraduate stu-
dents to generate English queries for the given
database. Queries were then manually translated
into logical form (Zelle and Mooney, 1996). The
average length of a natural language sentence in
this corpus is 6.87 words.
We adopted standard 10-fold cross validation
for evaluation: 9/10 of the whole dataset was used
for training (training set), and 1/10 for testing (test
set). To train a reranking model on a training set,
a separate ?internal? 10-fold cross validation over
the training set was employed to generate n-best
SAPTs for each training example using a base-
line learner, where each training set was again
separated into 10 folds with 9/10 for training the
baseline learner, and 1/10 for producing the n-
best SAPTs for training the reranker. Reranking
models trained in this way ensure that the n-best
SAPTs for each training example are not gener-
ated by a baseline model that has already seen that
example. To test a reranking model on a test set, a
baseline model trained on a whole training set was
used to generate n-best SAPTs for each test ex-
ample, and then the reranking model trained with
the above method was used to choose a best SAPT
from the candidate SAPTs.
The performance of semantic parsing was mea-
sured in terms of precision (the percentage of com-
pleted MRs that were correct), recall (the percent-
age of all sentences whose MRs were correctly
generated) and F-measure (the harmonic mean of
precision and recall). Since even a single mistake
in an MR could totally change the meaning of an
example (e.g. having OUR in an MR instead of OP-
PONENT in CLANG), no partial credit was given
for examples with partially-correct SAPTs.
Averaged perceptron (Collins, 2002a), which
has been successfully applied to several tag-
ging and parsing reranking tasks (Collins, 2002c;
Collins, 2002a), was employed for training rerank-
267
CLANG GEOQUERY
P R F P R F
SCISSOR 89.5 73.7 80.8 98.5 74.4 84.8
SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4
Table 2: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result in
bold), where P = precision, R = recall, and F = F-measure.
n 1 2 5 10 20 50
CLANG 78.0 81.3 83.0 84.0 85.0 85.3
GEOQUERY 77.2 77.6 80.0 81.2 81.6 81.6
Table 3: Oracle recalls on CLANG and GEOQUERY as a function of number n of n-best SAPTs.
ing models. To choose the correct SAPT of a
training example required for training the aver-
aged perceptron, we selected a SAPT that results
in the correct MR; if multiple such SAPTs exist,
the one with the highest baseline score was cho-
sen. Since no partial credit was awarded in evalua-
tion, a training example was discarded if it had no
correct SAPT. Rerankers were trained on the 50-
best SAPTs provided by SCISSOR, and the num-
ber of perceptron iterations over the training exam-
ples was limited to 10. Typically, in order to avoid
over-fitting, reranking features are filtered by re-
moving those occurring in less than some mini-
mal number of training examples. We only re-
moved features that never occurred in the training
data since experiments with higher cut-offs failed
to show any improvements.
4.2 Results
4.2.1 Baseline Results
Table 2 shows the results comparing the base-
line learner SCISSOR using both the back-off pa-
rameters in Ge and Mooney (2005) (SCISSOR) and
the revised parameters in Section 2.2 (SCISSOR+).
As we expected, SCISSOR+ has better recall and
worse precision than SCISSOR on both corpora
due to the additional levels of back-off. SCISSOR+
is used as the baseline model for all reranking ex-
periments in the next section.
Table 3 gives oracle recalls for CLANG and
GEOQUERY where an oracle picks the correct
parse from the n-best SAPTs if any of them are
correct. Results are shown for increasing values
of n. The trends for CLANG and GEOQUERY are
different: small values of n show significant im-
provements for CLANG, while a larger n is needed
to improve results for GEOQUERY.
4.2.2 Reranking Results
In this section, we describe the experiments
with reranking models utilizing different feature
sets. All models include the score assigned to a
SAPT by the baseline model as a special feature.
Table 4 shows results using different feature sets
derived directly from SAPTs. In general, rerank-
ing improves the performance of semantic parsing
on CLANG, but not on GEOQUERY. This could
be explained by the different oracle recall trends of
CLANG and GEOQUERY. We can see that in Ta-
ble 3, even a small n can increase the oracle score
on CLANG significantly, but not on GEOQUERY.
With the baseline score included as a feature, cor-
rect SAPTs closer to the top are more likely to
be reranked to the top than the ones in the back,
thus CLANG is more likely to have more sentences
reranked correct than GEOQUERY. On CLANG,
using the semantic feature set alne achieves the
best improvements over the baseline with 2.8%
absolute improvement in F-measure (15.8% rel-
ative error reduction), which is significant at the
95% confidence level using a paired Student?s t-
test. Nevertheless, the difference between SEM1
and SYN+SEM1 is very small (only one example).
Using syntactic features alone only slightly im-
proves the results because the syntactic features
do not directly discriminate between correct and
incorrect meaning representations. To put this
in perspective, Charniak and Johnson (2005) re-
ported that reranking improves the F-measure of
syntactic parsing from 89.7% to 91.0% with a 50-
best oracle F-measure score of 96.8%.
Table 5 compares results using semantic fea-
tures directly derived from SAPTs (SEM1), and
from trees with purely-syntactic nodes removed
(SEM2). It compares reranking models using these
268
CLANG GEOQUERY
P R F P R F
SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4
SYN 87.7 78.7 83.0 95.5 77.2 85.4
SEM1 90.0(23.1) 80.7(12.3) 85.1(15.8) 95.5 76.8 85.1
SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9
Table 4: Reranking results on CLANG and GEOQUERY using different feature sets derived directly from
SAPTs (with the best results in bold and relative error reduction in parentheses). The reranking model
SYN uses the syntactic feature set in Section 3.1, SEM1 uses the semantic feature set in Section 3.2.1, and
SYN+SEM1 uses both.
CLANG GEOQUERY
P R F P R F
SEM1 90.0 80.7 85.1 95.5 76.8 85.1
SEM2 88.1 79.0 83.3 96.0 77.2 85.6
SEM1+SEM2 88.5 79.3 83.7 95.5 76.4 84.9
SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9
SYN+SEM2 88.1 79.0 83.3 95.5 76.8 85.1
SYN+SEM1+SEM2 88.9 79.7 84.0 95.5 76.4 84.9
Table 5: Reranking results on CLANG and GEOQUERY comparing semantic features derived directly from
SAPTs, and semantic features from trees with purely-syntactic nodes removed. The symbol SEM1 and SEM2
refer to the semantic feature sets in Section 3.2.1 and 3.2.1 respectively, and SYN refers to the syntactic
feature set in Section 3.1.
feature sets alone and together, and using them
along with the syntactic feature set (SYN) alone
and together. Overall, SEM1 provides better results
than SEM2 on CLANG and slightly worse results
on GEOQUERY (only in one sentence), regard-
less of whether or not syntactic features are in-
cluded. Using both semantic feature sets does not
improve the results over just using SEM1. On one
hand, the better performance of SEM1 on CLANG
contradicts our expectation because of the reasons
discussed in Section 3.2.2; the reason behind this
needs to be investigated. On the other hand, how-
ever, it also suggests that the semantic features de-
rived directly from SAPTs can provide good evi-
dence for semantic correctness, even with redun-
dant purely syntactically motivated features.
We have also informally experimented with
smoothed semantic features utilizing domain on-
tology given by CLANG, which did not show im-
provements over reranking models not using these
features.
5 Conclusion
We have applied discriminative reranking to se-
mantic parsing, where reranking features are de-
veloped from features for reranking syntactic
parses based on the coupling of syntax and se-
mantics. The best reranking model significantly
improves F-measure on a Robocup coaching task
(CLANG) from 82.3% to 85.1%, while it fails to
improve the performance on a geography database
query task (GEOQUERY).
Future work includes further investigation of
the reasons behind the different utility of rerank-
ing for the CLANG and GEOQUERY tasks. We
also plan to explore other types of reranking
features, such as the features used in semantic
role labeling (SRL) (Gildea and Jurafsky, 2002;
Carreras and Ma`rquez, 2005), like the path be-
tween a target predicate and its argument, and
kernel methods (Collins, 2002b). Experimenting
with other effective reranking algorithms, such as
SVMs (Joachims, 2002) and MaxEnt (Charniak
and Johnson, 2005), is also a direction of our fu-
ture research.
6 Acknowledgements
We would like to thank Rohit J. Kate and anony-
mous reviewers for their insightful comments.
This research was supported by Defense Ad-
269
vanced Research Projects Agency under grant
HR0011-04-1-0007.
References
Xavier Carreras and Lu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proc. of 9th Conf. on Computational
Natural Language Learning (CoNLL-2005), pages
152?164, Ann Arbor, MI, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 43nd Annual Meeting
of the Association for Computational Linguistics
(ACL-05), pages 173?180, Ann Arbor, MI, June.
Mao Chen, Ehsan Foroughi, Fredrik Heintz, Spiros
Kapetanakis, Kostas Kostiadis, Johan Kummeneje,
Itsuki Noda, Oliver Obst, Patrick Riley, Timo Stef-
fens, Yi Wang, and Xiang Yin. 2003. Users
manual: RoboCup soccer server manual for soccer
server version 7.07 and later. Available at http://
sourceforge.net/projects/sserver/.
Michael J. Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. of the 35th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-97), pages 16?23.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. of 17th Intl. Conf.
on Machine Learning (ICML-2000), pages 175?182,
Stanford, CA, June.
Michael Collins. 2002a. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proc. of the
2002 Conf. on Empirical Methods in Natural Lan-
guage Processing (EMNLP-02), Philadelphia, PA,
July.
Michael Collins. 2002b. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proc. of the
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 263?270,
Philadelphia, PA, July.
Michael Collins. 2002c. Ranking algorithms for
named-entity extraction: Boosting and the voted
perceptron. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2002), pages 489?496, Philadelphia, PA.
Brooke Cowan and Michael Collins. 2005. Mor-
phology and reranking for the statistical parsing of
Spanish. In Proc. of the Human Language Technol-
ogy Conf. and Conf. on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP-05), Van-
couver, B.C., Canada, October.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proc. of 9th Conf. on Computational
Natural Language Learning (CoNLL-2005), pages
9?16, Ann Arbor, MI, July.
Daniel Gildea and Daniel Jurafsky. 2002. Automated
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Nancy A. Ide and Jeane?ronis. 1998. Introduction to
the special issue on word sense disambiguation: The
state of the art. Computational Linguistics, 24(1):1?
40.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proc. of 8th ACM
SIGKDD Intl. Conf. on Knowledge Discovery and
Data Mining (KDD-2002), Edmonton, Canada.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall, Upper
Saddle River, NJ.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages.
In Proc. of 20th Natl. Conf. on Artificial Intelli-
gence (AAAI-2005), pages 1062?1068, Pittsburgh,
PA, July.
Gregory Kuhlmann, Peter Stone, Raymond J. Mooney,
and Jude W. Shavlik. 2004. Guiding a reinforce-
ment learner with natural language advice: Initial
results in RoboCup soccer. In Proc. of the AAAI-04
Workshop on Supervisory Control of Learning and
Adaptive Systems, San Jose, CA, July.
Iddo Lev, Bill MacCartney, Christopher D. Manning,
and Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In Proc. of
2nd Workshop on Text Meaning and Interpretation,
ACL-04, Barcelona, Spain.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proc. of the 43nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), Ann Arbor, MI, June.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proc. of 13th Natl. Conf. on Artifi-
cial Intelligence (AAAI-96), pages 1050?1055, Port-
land, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of 21th Conf. on Uncertainty in
Artificial Intelligence (UAI-2005), Edinburgh, Scot-
land, July.
Victor W. Zue and James R. Glass. 2000. Conversa-
tional interfaces: Advances and challenges. In Proc.
of the IEEE, volume 88(8), pages 1166?1180.
270
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 611?619,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning a Compositional Semantic Parser
using an Existing Syntactic Parser
Ruifang Ge Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712
{grf,mooney}@cs.utexas.edu
Abstract
We present a new approach to learning a
semantic parser (a system that maps natu-
ral language sentences into logical form).
Unlike previous methods, it exploits an ex-
isting syntactic parser to produce disam-
biguated parse trees that drive the compo-
sitional semantic interpretation. The re-
sulting system produces improved results
on standard corpora on natural language
interfaces for database querying and sim-
ulated robot control.
1 Introduction
Semantic parsing is the task of mapping a natu-
ral language (NL) sentence into a completely for-
mal meaning representation (MR) or logical form.
A meaning representation language (MRL) is a
formal unambiguous language that supports au-
tomated inference, such as first-order predicate
logic. This distinguishes it from related tasks
such as semantic role labeling (SRL) (Carreras
and Marquez, 2004) and other forms of ?shallow?
semantic analysis that do not produce completely
formal representations. A number of systems for
automatically learning semantic parsers have been
proposed (Ge and Mooney, 2005; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2007; Lu et al,
2008). Given a training corpus of NL sentences
annotated with their correct MRs, these systems
induce an interpreter for mapping novel sentences
into the given MRL.
Previous methods for learning semantic parsers
do not utilize an existing syntactic parser that pro-
vides disambiguated parse trees.1 However, ac-
curate syntactic parsers are available for many
1Ge and Mooney (2005) use training examples with
semantically annotated parse trees, and Zettlemoyer and
Collins (2005) learn a probabilistic semantic parsing model
which initially requires a hand-built, ambiguous CCG gram-
mar template.
(a) If our player 2 has the ball,
then position our player 5 in the midfield.
((bowner (player our {2}))
(do (player our {5}) (pos (midfield))))
(b) Which river is the longest?
answer(x1,longest(x1,river(x1)))
Figure 1: Sample NLs and their MRs in the
ROBOCUP and GEOQUERY domains respectively.
languages and could potentially be used to learn
more effective semantic analyzers. This paper
presents an approach to learning semantic parsers
that uses parse trees from an existing syntactic
analyzer to drive the interpretation process. The
learned parser uses standard compositional seman-
tics to construct alternative MRs for a sentence
based on its syntax tree, and then chooses the best
MR based on a trained statistical disambiguation
model. The learning system first employs a word
alignment method from statistical machine trans-
lation (GIZA++ (Och and Ney, 2003)) to acquire
a semantic lexicon that maps words to logical
predicates. Then it induces rules for composing
MRs and estimates the parameters of a maximum-
entropy model for disambiguating semantic inter-
pretations. After describing the details of our ap-
proach, we present experimental results on stan-
dard corpora demonstrating improved results on
learning NL interfaces for database querying and
simulated robot control.
2 Background
In this paper, we consider two domains. The
first is ROBOCUP (www.robocup.org). In the
ROBOCUP Coach Competition, soccer agents
compete on a simulated soccer field and receive
coaching instructions in a formal language called
CLANG (Chen et al, 2003). Figure 1(a) shows a
sample instruction. The second domain is GEO-
QUERY, where a logical query language based on
Prolog is used to query a database on U.S. geog-
raphy (Zelle and Mooney, 1996). The logical lan-
611
CONDITION
(bowner PLAYER )
(player TEAM
our
{UNUM})
2
(a)
P BOWNER
P PLAYER
P OUR P UNUM
(b)
S
NP
PRP$
our
NP
NN
player
CD
2
VP
VB
has
NP
DET
the
NN
ball
(c)
Figure 2: Parses for the condition part of the CLANG in Figure 1(a): (a) The parse of the MR. (b) The
predicate argument structure of (a). (c) The parse of the NL.
PRODUCTION PREDICATE
RULE?(CONDITION DIRECTIVE) P RULE
CONDITION?(bowner PLAYER) P BOWNER
PLAYER?(player TEAM {UNUM}) P PLAYER
TEAM?our P OUR
UNUM?2 P UNUM
DIRECTIVE?(do PLAYER ACTION) P DO
ACTION?(pos REGION) P POS
REGION?(midfield) P MIDFIELD
Table 1: Sample production rules for parsing the
CLANG example in Figure 1(a) and their corre-
sponding predicates.
guage consists of both first-order and higher-order
predicates. Figure 1(b) shows a sample query in
this domain.
We assume that an MRL is defined by an un-
ambiguous context-free grammar (MRLG), so that
MRs can be uniquely parsed, a standard require-
ment for computer languages. In an MRLG, each
production rule introduces a single predicate in the
MRL, where the type of the predicate is given in
the left hand side (LHS), and the number and types
of its arguments are defined by the nonterminals in
the right hand side (RHS). Therefore, the parse of
an MR also gives its predicate-argument structure.
Figure 2(a) shows the parse of the condition
part of the MR in Figure 1(a) using the MRLG
described in (Wong, 2007), and its predicate-
argument structure is in Figure 2(b). Sample
MRLG productions and their predicates for pars-
ing this example are shown in Table 1, where the
predicate P PLAYER takes two arguments (a1 and
a2) of type TEAM and UNUM (uniform number).
3 Semantic Parsing Framework
This section describes our basic framework, which
is based on a fairly standard approach to computa-
tional semantics (Blackburn and Bos, 2005). The
framework is composed of three components: 1)
an existing syntactic parser to produce parse trees
for NL sentences; 2) learned semantic knowledge
(cf. Sec. 5), including a semantic lexicon to assign
possible predicates (meanings) to words, and a set
of semantic composition rules to construct possi-
ble MRs for each internal node in a syntactic parse
given its children?s MRs; and 3) a statistical dis-
ambiguation model (cf. Sec. 6) to choose among
multiple possible semantic constructs as defined
by the semantic knowledge.
The process of generating the semantic parse
for an NL sentence is as follows. First, the syn-
tactic parser produces a parse tree for the NL
sentence. Second, the semantic lexicon assigns
possible predicates to each word in the sentence.
Third, all possible MRs for the sentence are con-
structed compositionally in a recursive, bottom-up
fashion following its syntactic parse using com-
position rules. Lastly, the statistical disambigua-
tion model scores each possible MR and returns
the one with the highest score. Fig. 3(a) shows
one possible semantically-augmented parse tree
(SAPT) (Ge and Mooney, 2005) for the condition
part of the example in Fig. 1(a) given its syntac-
tic parse in Fig. 2(c). A SAPT adds a semantic
label to each non-leaf node in the syntactic parse
tree. The label specifies the MRL predicate for
the node and its remaining (unfilled) arguments.
The compositional process assumes a binary parse
tree suitable for predicate-argument composition;
parses in Penn-treebank style are binarized using
Collins? (1999) method.
Consider the construction of the SAPT in
Fig. 3(a). First, each word is assigned a semantic
label. Most words are assigned an MRL predicate.
For example, the word player is assigned the pred-
icate P PLAYER with its two unbound arguments,
a1 and a2, indicated using ?. Words that do not
introduce a predicate are given the label NULL,
like the and ball.2 Next, a semantic label is as-
2The words the and ball are not truly ?meaningless? since
the predicate P BOWNER (ball owner) is conveyed by the
612
P BOWNER
P PLAYER
P OUR
our
?a1P PLAYER
??a1?a2?P PLAYER
player
P UNUM
2
?a1P BOWNER
?a1P BOWNER
has
NULL
NULL
the
NULL
ball
(a) SAPT
(bowner (player our {2}))
(player our {2})
our
our
?a1 (player a1 {2})
??a1?a2?(player a1 {a2} )
player
2
2
?a1(bowner a1)
?a1(bowner a1)
has
NULL
NULL
the
NULL
ball
(b) Semantic Derivation
Figure 3: Semantic parse for the condition part of the example in Fig. 1(a) using the syntactic parse in
Fig. 2(c): (a) A SAPT with syntactic labels omitted for brevity. (b) The semantic derivation of the MR.
signed to each internal node using learned compo-
sition rules that specify how arguments are filled
when composing two MRs (cf. Sec. 5). The label
?a1P PLAYER indicates that the remaining argu-
ment a2 of the P PLAYER child is filled by the MR
of the other child (labeled P UNUM).
Finally, the SAPT is used to guide the composi-
tion of the sentence?s MR. At each internal node,
an MR for the node is built from the MRs of its
children by filling an argument of a predicate, as
illustrated in the semantic derivation shown in Fig.
3(b). Semantic composition rules (cf. Sec. 5) are
used to specify the argument to be filled. For the
node spanning player 2, the predicate P PLAYER
and its second argument P UNUM are composed to
form the MR: ?a1 (player a1 {2}). Composing
an MR with NULL leaves the MR unchanged. An
MR is said to be complete when it contains no re-
maining ? variables. This process continues up the
phrase has the ball. For simplicity, predicates are intro-
duced by a single word, but statistical disambiguation (cf.
Sec. 6) uses surrounding words to choose a meaning for a
word whose lexicon entry contains multiple possible predi-
cates.
tree until a complete MR for the entire sentence is
constructed at the root.
4 Ensuring Meaning Composition
The basic compositional method in Sec. 3 only
works if the syntactic parse tree strictly follows
the predicate-argument structure of the MR, since
meaning composition at each node is assumed to
combine a predicate with one of its arguments.
However, this assumption is not always satisfied,
for example, in the case of verb gapping and flex-
ible word order. We use constructing the MR for
the directive part of the example in Fig. 1(a) ac-
cording to the syntactic parse in Fig. 4(b) as an
example. Given the appropriate possible predicate
attached to each word in Fig. 5(a), the node span-
ning position our player 5 has children, P POS and
P PLAYER, that are not in a predicate-argument re-
lation in the MR (see Fig. 4(a)).
To ensure meaning composition in this case,
we automatically create macro-predicates that
combine multiple predicates into one, so that
the children?s MRs can be composed as argu-
613
P DO
P PLAYER
P OUR P UNUM
P POS
P MIDFIELD
(a)
VP
ADVP
RB
then
VP
VP
VB
position
NP
our player 5
PP
IN
in
NP
DT
the
NN
midfield
(b)
Figure 4: Parses for the directive part of the CLANG in Fig. 1(a): (a) The predicate-argument structure
of the MR. (b) The parse of the NL (the parse of the phrase our player 5 is omitted for brevity).
ments to a macro-predicate. Fig. 5(b) shows
the macro-predicate P DO POS (DIRECTIVE?(do
PLAYER (pos REGION))) formed by merging the
P DO and P POS in Fig. 4(a). The macro-predicate
has two arguments, one of type PLAYER (a1)
and one of type REGION (a2). Now, P POS and
P PLAYER can be composed as arguments to this
macro-predicate as shown in Fig. 5(c). However,
it requires assuming a P DO predicate that has
not been formally introduced. To indicate this, a
lambda variable, p1, is introduced that ranges over
predicates and is provisionally bound to P DO, as
indicated in Fig. 5(c) using the notation p1:do.
Eventually, this predicate variable must be bound
to a matching predicate introduced from the lexi-
con. In the example, p1:do is eventually bound to
the P DO predicate introduced by the word then to
form a complete MR.
Macro-predicates are introduced as needed dur-
ing training in order to ensure that each MR in
the training set can be composed using the syn-
tactic parse of its corresponding NL given reason-
able assignments of predicates to words. For each
SAPT node that does not combine a predicate with
a legal argument, a macro-predicate is formed by
merging all predicates on the paths from the child
predicates to their lowest common ancestor (LCA)
in the MR parse. Specifically, a child MR be-
comes an argument of the macro-predicate if it
is complete (i.e. contains no ? variables); other-
wise, it also becomes part of the macro-predicate
and its ? variables become additional arguments
of the macro-predicate. For the node spanning po-
sition our player 5 in the example, the LCA of the
children P PLAYER and P POS is their immedi-
ate parent P DO, therefore P DO is included in the
macro-predicate. The complete child P PLAYER
becomes the first argument of the macro-predicate.
The incomplete child P POS is added to the macro-
predicate P DO POS and its ? variable becomes
another argument.
For improved generalization, once a predicate
in a macro-predicate becomes complete, it is re-
moved from the corresponding macro-predicate
label in the SAPT. For the node spanning position
our player 5 in the midfield in Fig. 5(a), P DO POS
becomes P DO once the arguments of pos are
filled.
In the following two sections, we describe the
two subtasks of inducing semantic knowledge and
a disambiguation model for this enhanced compo-
sitional framework. Both subtasks require a train-
ing set of NLs paired with their MRs. Each NL
sentence also requires a syntactic parse generated
using Bikel?s (2004) implementation of Collins
parsing model 2. Note that unlike SCISSOR (Ge
and Mooney, 2005), training our method does not
require gold-standard SAPTs.
5 Learning Semantic Knowledge
Learning semantic knowledge starts from learning
the mapping from words to predicates. We use
an approach based on Wong and Mooney (2006),
which constructs word alignments between NL
sentences and their MRs. Normally, word align-
ment is used in statistical machine translation to
match words in one NL to words in another; here
it is used to align words with predicates based on
a ?parallel corpus? of NL sentences and MRs. We
assume that each word alignment defines a possi-
ble mapping from words to predicates for building
a SAPT and semantic derivation which compose
the correct MR. A semantic lexicon and compo-
sition rules are then extracted directly from the
614
P DO
??a1?a2?P DO
then
?p1P DO POS = ?p1P DO
??p1?a2?P DO POS
?a1P POS
position
P PLAYER
our player 5
P MIDFIELD
NULL
in
P MIDFIELD
NULL
the
P MIDFIELD
midfield
(a) SAPT
P DO
a1:PLAYER P POS
a2:REGION
(b) Macro-Predicate P DO POS
(do (player our {5}) (pos (midfield)))
??a1?a2?(do a1a2)
then
?p1(p1:do (player our {5}) (pos (midfield)))
??p1?a2?(p1:do (player our {5}) (pos a2))
?a1(pos a1)
position
(player our {5})
our player 5
(midfield)
NULL
in
(midfield)
NULL
the
(midfield)
midfield
(c) Semantic Derivation
Figure 5: Semantic parse for the directive part of the example in Fig. 1(a) using the syntactic parse in
Fig. 4(b): (a) A SAPT with syntactic labels omitted for brevity. (b) The predicate-argument structure of
macro-predicate P DO POS (c) The semantic derivation of the MR.
nodes of the resulting semantic derivations.
Generation of word alignments for each train-
ing example proceeds as follows. First, each MR
in the training corpus is parsed using the MRLG.
Next, each resulting parse tree is linearized to pro-
duce a sequence of predicates by using a top-
down, left-to-right traversal of the parse tree. Then
the GIZA++ implementation (Och and Ney, 2003)
of IBM Model 5 is used to generate the five best
word/predicate alignments from the corpus of NL
sentences each paired with the predicate sequence
for its MR.
After predicates are assigned to words using
word alignment, for each alignment of a training
example and its syntactic parse, a SAPT is gener-
ated for composing the correct MR using the pro-
cesses discussed in Sections 3 and 4. Specifically,
a semantic label is assigned to each internal node
of each SAPT, so that the MRs of its children are
composed correctly according to the MR for this
example.
There are two cases that require special han-
dling. First, when a predicate is not aligned to any
word, the predicate must be inferred from context.
For example, in CLANG, our player is frequently
just referred to as player and the our must be in-
ferred. When building a SAPT for such an align-
ment, the assumed predicates and arguments are
simply bound to their values in the MR. Second,
when a predicate is aligned to several words, i.e. it
is represented by a phrase, the alignment is trans-
formed into several alignments where each predi-
cate is aligned to each single word in order to fit
the assumptions of compositional semantics.
Given the SAPTs constructed from the results
of word-alignment, a semantic derivation for each
training sentence is constructed using the methods
described in Sections 3 and 4. Composition rules
615
are then extracted from these derivations.
Formally, composition rules are of the form:
?1.P1 + ?2.P2 ? {?p.Pp, R} (1)
where P1, P2 and Pp are predicates for the left
child, right child, and parent node, respectively.
Each predicate includes a lambda term ? of
the form ??pi1 , . . . , ?pim , ?aj1 , . . . , ?ajn?, an un-
ordered set of all unbound predicate and argument
variables for the predicate. The component R
specifies how some arguments of the parent predi-
cate are filled when composing the MR for the par-
ent node. It is of the form: {ak1=R1, . . . , akl=Rl},
where Ri can be either a child (ci), or a child?s
complete argument (ci, aj) if the child itself is not
complete.
For instance, the rule extracted for the node for
player 2 in Fig. 3(b) is:
??a1?a2?.P PLAYER + P UNUM ? {?a1.P PLAYER, a2=c2},
and for position our player 5 in Fig. 5(c):
?a1.P POS + P PLAYER ? {??p1?a2?.P DO POS, a1=c2},
and for position our player 5 in the midfield:
??p1?a2?.P DO POS + P MIDFIELD
? {?p1.P DO POS, {a1=(c1,a1), a2=c2}}.
The learned semantic knowledge is necessary
for handling ambiguity, such as that involving
word senses and semantic roles. It is also used to
ensure that each MR is a legal string in the MRL.
6 Learning a Disambiguation Model
Usually, multiple possible semantic derivations for
an NL sentence are warranted by the acquired se-
mantic knowledge, thus disambiguation is needed.
To learn a disambiguation model, the learned se-
mantic knowledge (see Section 5) is applied to
each training example to generate all possible se-
mantic derivations for an NL sentence given its
syntactic parse. Here, unique word alignments are
not required, and alternative interpretations com-
pete for the best semantic parse.
We use a maximum-entropy model similar
to that of Zettlemoyer and Collins (2005) and
Wong and Mooney (2006). The model defines a
conditional probability distribution over semantic
derivations (D) given an NL sentence S and its
syntactic parse T :
Pr(D|S, T ; ??) = exp
?
i ?ifi(D)
Z??(S, T )
(2)
where f? (f1, . . . , fn) is a feature vector parame-
terized by ??, and Z??(S, T ) is a normalizing fac-
tor. Three simple types of features are used in
the model. First, are lexical features which count
the number of times a word is assigned a particu-
lar predicate. Second, are bilexical features which
count the number of times a word is assigned a
particular predicate and a particular word precedes
or follows it. Last, are rule features which count
the number of times a particular composition rule
is applied in the derivation.
The training process finds a parameter ??? that
(approximately) maximizes the sum of the condi-
tional log-likelihood of the MRs in the training set.
Since no specific semantic derivation for an MR is
provided in the training data, the conditional log-
likelihood of an MR is calculated as the sum of the
conditional probability of all semantic derivations
that lead to the MR. Formally, given a set of NL-
MR pairs {(S1,M1), (S2,M2), ..., (Sn,Mn)} and
the syntactic parses of the NLs {T1, T2, ..., Tn},
the parameter ??? is calculated as:
??? = argmax
??
n
?
i=1
log Pr(Mi|Si, Ti; ??) (3)
= argmax
??
n
?
i=1
log
?
D?i
Pr(D?i |Si, Ti; ??)
where D?i is a semantic derivation that produces
the correct MR Mi.
L-BFGS (Nocedal, 1980) is used to estimate the
parameters ???. The estimation requires statistics
that depend on all possible semantic derivations
and all correct semantic derivations of an exam-
ple, which are not feasibly enumerated. A vari-
ant of the Inside-Outside algorithm (Miyao and
Tsujii, 2002) is used to efficiently collect the nec-
essary statistics. Following Wong and Mooney
(2006), only candidate predicates and composi-
tion rules that are used in the best semantic deriva-
tions for the training set are retained for testing.
No smoothing is used to regularize the model; We
tried using a Gaussian prior (Chen and Rosenfeld,
1999), but it did not improve the results.
7 Experimental Evaluation
We evaluated our approach on two standard cor-
pora in CLANG and GEOQUERY. For CLANG,
300 instructions were randomly selected from
the log files of the 2003 ROBOCUP Coach
616
Competition and manually translated into En-
glish (Kuhlmann et al, 2004). For GEOQUERY,
880 English questions were gathered from vari-
ous sources and manually translated into Prolog
queries (Tang and Mooney, 2001). The average
sentence lengths for the CLANG and GEOQUERY
corpora are 22.52 and 7.48, respectively.
Our experiments used 10-fold cross validation
and proceeded as follows. First Bikel?s imple-
mentation of Collins parsing model 2 was trained
to generate syntactic parses. Second, a seman-
tic parser was learned from the training set aug-
mented with their syntactic parses. Finally, the
learned semantic parser was used to generate the
MRs for the test sentences using their syntactic
parses. If a test example contains constructs that
did not occur in training, the parser may fail to re-
turn an MR.
Wemeasured the performance of semantic pars-
ing using precision (percentage of returned MRs
that were correct), recall (percentage of test exam-
ples with correct MRs returned), and F-measure
(harmonic mean of precision and recall). For
CLANG, an MR was correct if it exactly matched
the correct MR, up to reordering of arguments
of commutative predicates like and. For GEO-
QUERY, an MR was correct if it retrieved the same
answer as the gold-standard query, thereby reflect-
ing the quality of the final result returned to the
user.
The performance of a syntactic parser trained
only on the Wall Street Journal (WSJ) can de-
grade dramatically in new domains due to cor-
pus variation (Gildea, 2001). Experiments on
CLANG and GEOQUERY showed that the perfor-
mance can be greatly improved by adding a small
number of treebanked examples from the corre-
sponding training set together with the WSJ cor-
pus. Our semantic parser was evaluated using
three kinds of syntactic parses. Listed together
with their PARSEVAL F-measures these are:
gold-standard parses from the treebank (GoldSyn,
100%), a parser trained on WSJ plus a small
number of in-domain training sentences required
to achieve good performance, 20 for CLANG
(Syn20, 88.21%) and 40 for GEOQUERY (Syn40,
91.46%), and a parser trained on no in-domain
data (Syn0, 82.15% for CLANG and 76.44% for
GEOQUERY).
We compared our approach to the following al-
ternatives (where results for the given corpus were
Precision Recall F-measure
GOLDSYN 84.73 74.00 79.00
SYN20 85.37 70.00 76.92
SYN0 87.01 67.00 75.71
WASP 88.85 61.93 72.99
KRISP 85.20 61.85 71.67
SCISSOR 89.50 73.70 80.80
LU 82.50 67.70 74.40
Table 2: Performance on CLANG.
Precision Recall F-measure
GOLDSYN 91.94 88.18 90.02
SYN40 90.21 86.93 88.54
SYN0 81.76 78.98 80.35
WASP 91.95 86.59 89.19
Z&C 91.63 86.07 88.76
SCISSOR 95.50 77.20 85.38
KRISP 93.34 71.70 81.10
LU 89.30 81.50 85.20
Table 3: Performance on GEOQUERY.
available): SCISSOR (Ge and Mooney, 2005), an
integrated syntactic-semantic parser; KRISP (Kate
and Mooney, 2006), an SVM-based parser using
string kernels; WASP (Wong and Mooney, 2006;
Wong and Mooney, 2007), a system based on
synchronous grammars; Z&C (Zettlemoyer and
Collins, 2007)3, a probabilistic parser based on re-
laxed CCG grammars; and LU (Lu et al, 2008),
a generative model with discriminative reranking.
Note that some of these approaches require ad-
ditional human supervision, knowledge, or engi-
neered features that are unavailable to the other
systems; namely, SCISSOR requires gold-standard
SAPTs, Z&C requires hand-built template gram-
mar rules, LU requires a reranking model using
specially designed global features, and our ap-
proach requires an existing syntactic parser. The
F-measures for syntactic parses that generate cor-
rect MRs in CLANG are 85.50% for syn0 and
91.16% for syn20, showing that our method can
produce correct MRs even when given imperfect
syntactic parses. The results of semantic parsers
are shown in Tables 2 and 3.
First, not surprisingly, more accurate syntac-
tic parsers (i.e. ones trained on more in-domain
data) improved our approach. Second, in CLANG,
all of our methods outperform WASP and KRISP,
which also require no additional information dur-
ing training. In GEOQUERY, Syn0 has signifi-
cantly worse results than WASP and our other sys-
tems using better syntactic parses. This is not sur-
prising since Syn0?s F-measure for syntactic pars-
ing is only 76.44% in GEOQUERY due to a lack
3These results used a different experimental setup, train-
ing on 600 examples, and testing on 280 examples.
617
Precision Recall F-measure
GOLDSYN 61.14 35.67 45.05
SYN20 57.76 31.00 40.35
SYN0 53.54 22.67 31.85
WASP 88.00 14.37 24.71
KRISP 68.35 20.00 30.95
SCISSOR 85.00 23.00 36.20
Table 4: Performance on CLANG40.
Precision Recall F-measure
GOLDSYN 95.73 89.60 92.56
SYN20 93.19 87.60 90.31
SYN0 91.81 85.20 88.38
WASP 91.76 75.60 82.90
SCISSOR 98.50 74.40 84.77
KRISP 84.43 71.60 77.49
LU 91.46 72.80 81.07
Table 5: Performance on GEO250 (20 in-domain
sentences are used in SYN20 to train the syntactic
parser).
of interrogative sentences (questions) in the WSJ
corpus. Note the results for SCISSOR, KRISP and
LU on GEOQUERY are based on a different mean-
ing representation language, FUNQL, which has
been shown to produce lower results (Wong and
Mooney, 2007). Third, SCISSOR performs better
than our methods on CLANG, but it requires extra
human supervision that is not available to the other
systems. Lastly, a detailed analysis showed that
our improved performance on CLANG compared
to WASP and KRISP is mainly for long sentences
(> 20 words), while performance on shorter sen-
tences is similar. This is consistent with their
relative performance on GEOQUERY, where sen-
tences are normally short. Longer sentences typ-
ically have more complex syntax, and the tradi-
tional syntactic analysis used by our approach re-
sults in better compositional semantic analysis in
this situation.
We also ran experiments with less training data.
For CLANG, 40 random examples from the train-
ing sets (CLANG40) were used. For GEOQUERY,
an existing 250-example subset (GEO250) (Zelle
and Mooney, 1996) was used. The results are
shown in Tables 4 and 5. Note the performance
of our systems on GEO250 is higher than that
on GEOQUERY since GEOQUERY includes more
complex queries (Tang and Mooney, 2001). First,
all of our systems gave the best F-measures (ex-
cept SYN0 compared to SCISSOR in CLANG40),
and the differences are generally quite substantial.
This shows that our approach significantly im-
proves results when limited training data is avail-
able. Second, in CLANG, reducing the training
data increased the difference between SYN20 and
SYN0. This suggests that the quality of syntactic
parsing becomes more important when less train-
ing data is available. This demonstrates the advan-
tage of utilizing existing syntactic parsers that are
learned from large open domain treebanks instead
of relying just on the training data.
We also evaluated the impact of the word align-
ment component by replacing Giza++ by gold-
standard word alignments manually annotated for
the CLANG corpus. The results consistently
showed that compared to using gold-standard
word alignment, Giza++ produced lower seman-
tic parsing accuracy when given very little training
data, but similar or better results when given suf-
ficient training data (> 160 examples). This sug-
gests that, given sufficient data, Giza++ can pro-
duce effective word alignments, and that imper-
fect word alignments do not seriously impair our
semantic parsers since the disambiguation model
evaluates multiple possible interpretations of am-
biguous words. Using multiple potential align-
ments from Giza++ sometimes performs even bet-
ter than using a single gold-standard word align-
ment because it allows multiple interpretations to
be evaluated by the global disambiguation model.
8 Conclusion and Future work
We have presented a new approach to learning a
semantic parser that utilizes an existing syntactic
parser to drive compositional semantic interpre-
tation. By exploiting an existing syntactic parser
trained on a large treebank, our approach produces
improved results on standard corpora, particularly
when training data is limited or sentences are long.
The approach also exploits methods from statisti-
cal MT (word alignment) and therefore integrates
techniques from statistical syntactic parsing, MT,
and compositional semantics to produce an effec-
tive semantic parser.
Currently, our results comparing performance
on long versus short sentences indicates that our
approach is particularly beneficial for syntactically
complex sentences. Follow up experiments us-
ing a more refined measure of syntactic complex-
ity could help confirm this hypothesis. Reranking
could also potentially improve the results (Ge and
Mooney, 2006; Lu et al, 2008).
Acknowledgments
This research was partially supported by NSF
grant IIS?0712097.
618
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Patrick Blackburn and Johan Bos. 2005. Represen-
tation and Inference for Natural Language: A First
Course in Computational Semantics. CSLI Publica-
tions, Stanford, CA.
Xavier Carreras and Luis Marquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proc. of 8th Conf. on Computational
Natural Language Learning (CoNLL-2004), Boston,
MA.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy model.
Technical Report CMU-CS-99-108, School of Com-
puter Science, Carnegie Mellon University.
Mao Chen, Ehsan Foroughi, Fredrik Heintz, Spiros
Kapetanakis, Kostas Kostiadis, Johan Kummeneje,
Itsuki Noda, Oliver Obst, Patrick Riley, Timo Stef-
fens, Yi Wang, and Xiang Yin. 2003. Users
manual: RoboCup soccer server manual for soccer
server version 7.07 and later. Available at http://
sourceforge.net/projects/sserver/.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Ruifang Ge and Raymond J. Mooney. 2005. A statisti-
cal semantic parser that integrates syntax and seman-
tics. In Proc. of 9th Conf. on Computational Natural
Language Learning (CoNLL-2005), pages 9?16.
Ruifang Ge and Raymond J. Mooney. 2006. Dis-
criminative reranking for semantic parsing. In Proc.
of the 21st Intl. Conf. on Computational Linguis-
tics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL-06),
Sydney, Australia, July.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc. of the 2001 Conf. on Empirical
Methods in Natural Language Processing (EMNLP-
01), Pittsburgh, PA, June.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proc. of the 21st Intl. Conf. on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL-06),
pages 913?920, Sydney, Australia, July.
Greg Kuhlmann, Peter Stone, Raymond J. Mooney, and
Jude W. Shavlik. 2004. Guiding a reinforcement
learner with natural language advice: Initial results
in RoboCup soccer. In Proc. of the AAAI-04 Work-
shop on Supervisory Control of Learning and Adap-
tive Systems, San Jose, CA, July.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proc. of the Conf. on Empirical Methods in Natu-
ral Language Processing (EMNLP-08), Honolulu,
Hawaii, October.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc.
of Human Language Technology Conf.(HLT-2002),
San Diego, CA, March.
Jorge Nocedal. 1980. Updating quasi-Newton matri-
ces with limited storage. Mathematics of Computa-
tion, 35(151):773?782, July.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proc. of the
12th European Conf. on Machine Learning, pages
466?477, Freiburg, Germany.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proc. of Human Language
Technology Conf. / N. American Chapter of the
Association for Computational Linguistics Annual
Meeting (HLT-NAACL-2006), pages 439?446.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proc. of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL-07), pages 960?967.
Yuk Wah Wong. 2007. Learning for Semantic Pars-
ing and Natural Language Generation Using Statis-
tical Machine Translation Techniques. Ph.D. the-
sis, Department of Computer Sciences, University of
Texas, Austin, TX, August. Also appears as Artifi-
cial Intelligence Laboratory Technical Report AI07-
343.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proc. of 13th Natl. Conf. on Artifi-
cial Intelligence (AAAI-96), pages 1050?1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of the 21th Annual Conf. on Un-
certainty in Artificial Intelligence (UAI-05).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proc. of the 2007 Joint Conf. on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing (EMNLP-CoNLL-07), pages 678?687, Prague,
Czech Republic, June.
619
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 9?16, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Statistical Semantic Parser that Integrates Syntax and Semantics
Ruifang Ge Raymond J. Mooney
Department of Computer Sciences
University of Texas, Austin
TX 78712, USA
fgrf,mooneyg@cs.utexas.edu
Abstract
We introduce a learning semantic parser,
SCISSOR, that maps natural-language sen-
tences to a detailed, formal, meaning-
representation language. It first uses
an integrated statistical parser to pro-
duce a semantically augmented parse tree,
in which each non-terminal node has
both a syntactic and a semantic label.
A compositional-semantics procedure is
then used to map the augmented parse
tree into a final meaning representation.
We evaluate the system in two domains,
a natural-language database interface and
an interpreter for coaching instructions in
robotic soccer. We present experimental
results demonstrating that SCISSOR pro-
duces more accurate semantic representa-
tions than several previous approaches.
1 Introduction
Most recent work in learning for semantic parsing
has focused on ?shallow? analysis such as seman-
tic role labeling (Gildea and Jurafsky, 2002). In this
paper, we address the more ambitious task of learn-
ing to map sentences to a complete formal meaning-
representation language (MRL). We consider two
MRL?s that can be directly used to perform useful,
complex tasks. The first is a Prolog-based language
used in a previously-developed corpus of queries to
a database on U.S. geography (Zelle and Mooney,
1996). The second MRL is a coaching language for
robotic soccer developed for the RoboCup Coach
Competition, in which AI researchers compete to
provide effective instructions to a coachable team of
agents in a simulated soccer domain (et al, 2003).
We present an approach based on a statisti-
cal parser that generates a semantically augmented
parse tree (SAPT), in which each internal node in-
cludes both a syntactic and semantic label. We aug-
ment Collins? head-driven model 2 (Collins, 1997)
to incorporate a semantic label on each internal
node. By integrating syntactic and semantic inter-
pretation into a single statistical model and finding
the globally most likely parse, an accurate combined
syntactic/semantic analysis can be obtained. Once a
SAPT is generated, an additional step is required to
translate it into a final formal meaning representa-
tion (MR).
Our approach is implemented in a system called
SCISSOR (Semantic Composition that Integrates
Syntax and Semantics to get Optimal Representa-
tions). Training the system requires sentences an-
notated with both gold-standard SAPT?s and MR?s.
We present experimental results on corpora for both
geography-database querying and Robocup coach-
ing demonstrating that SCISSOR produces more ac-
curate semantic representations than several previ-
ous approaches based on symbolic learning (Tang
and Mooney, 2001; Kate et al, 2005).
2 Target MRL?s
We used two MRLs in our experiments: CLANG and
GEOQUERY. They capture the meaning of linguistic
utterances in their domain in a formal language.
9
2.1 CLANG: the RoboCup Coach Language
RoboCup (www.robocup.org) is an interna-
tional AI research initiative using robotic soccer
as its primary domain. In the Coach Competition,
teams of agents compete on a simulated soccer field
and receive advice from a team coach in a formal
language called CLANG. In CLANG, tactics and
behaviors are expressed in terms of if-then rules.
As described in (et al, 2003), its grammar consists
of 37 non-terminal symbols and 133 productions.
Below is a sample rule with its English gloss:
((bpos (penalty-area our))
(do (player-except our {4})
(pos (half our))))
?If the ball is in our penalty area, all our players
except player 4 should stay in our half.?
2.2 GEOQUERY: a DB Query Language
GEOQUERY is a logical query language for a small
database of U.S. geography containing about 800
facts. This domain was originally chosen to test
corpus-based semantic parsing due to the avail-
ability of a hand-built natural-language interface,
GEOBASE, supplied with Turbo Prolog 2.0 (Borland
International, 1988). The GEOQUERY language
consists of Prolog queries augmented with several
meta-predicates (Zelle and Mooney, 1996). Below
is a sample query with its English gloss:
answer(A,count(B,(city(B),loc(B,C),
const(C,countryid(usa))),A))
?How many cities are there in the US??
3 Semantic Parsing Framework
This section describes our basic framework for se-
mantic parsing, which is based on a fairly stan-
dard approach to compositional semantics (Juraf-
sky and Martin, 2000). First, a statistical parser
is used to construct a SAPT that captures the se-
mantic interpretation of individual words and the
basic predicate-argument structure of the sentence.
Next, a recursive procedure is used to composition-
ally construct an MR for each node in the SAPT
from the semantic label of the node and the MR?s
has2
VP?bowner
player the ball
NN?player CD?unum NP?null
NN?null
VB?bowner
S?bowner
NP?player
DT?null
PRP$?team
our
Figure 1: An SAPT for a simple CLANG sentence.
Function:BUILDMR(N;K)
Input: The root node N of a SAPT;
predicate-argument knowledge, K, for the MRL.
Notation: X
MR
is the MR of node X .
Output: N
MR
C
i
:= the ith child node of N; 1  i  n
C
h
= GETSEMANTICHEAD(N ) // see Section 3
C
h
MR
= BUILDMR(C
h
; K)
for each other child C
i
where i 6= h
C
i
MR
= BUILDMR(C
i
; K)
COMPOSEMR(C
h
MR
, C
i
MR
; K) // see Section 3
N
MR
= C
h
MR
Figure 2: Computing an MR from a SAPT.
of its children. Syntactic structure provides infor-
mation of how the parts should be composed. Am-
biguities arise in both syntactic structure and the se-
mantic interpretation of words and phrases. By in-
tegrating syntax and semantics in a single statistical
parser that produces an SAPT, we can use both se-
mantic information to resolve syntactic ambiguities
and syntactic information to resolve semantic ambi-
guities.
In a SAPT, each internal node in the parse tree
is annotated with a semantic label. Figure 1 shows
the SAPT for a simple sentence in the CLANG do-
main. The semantic labels which are shown after
dashes are concepts in the domain. Some type con-
cepts do not take arguments, like team and unum
(uniform number). Some concepts, which we refer
to as predicates, take an ordered list of arguments,
like player and bowner (ball owner). The predicate-
argument knowledge, K , specifies, for each predi-
cate, the semantic constraints on its arguments. Con-
straints are specified in terms of the concepts that
can fill each argument, such as player(team, unum)
and bowner(player). A special semantic label null
is used for nodes that do not correspond to any con-
cept in the domain.
Figure 2 shows the basic algorithm for build-
ing an MR from an SAPT. Figure 3 illustrates the
10
player the ball
N3?bowner(_)N7?player(our,2)
N2?null
      null      null
N4?player(_,_)    N5?team
our
N6?unum
2
N1?bowner(_)
has
N8?bowner(player(our,2))
Figure 3: MR?s constructed for each SAPT Node.
construction of the MR for the SAPT in Figure 1.
Nodes are numbered in the order in which the con-
struction of their MR?s are completed. The first
step, GETSEMANTICHEAD , determines which of a
node?s children is its semantic head based on hav-
ing a matching semantic label. In the example, node
N3 is determined to be the semantic head of the
sentence, since its semantic label, bowner, matches
N8?s semantic label. Next, the MR of the seman-
tic head is constructed recursively. The semantic
head of N3 is clearly N1. Since N1 is a part-of-
speech (POS) node, its semantic label directly de-
termines its MR, which becomes bowner( ). Once
the MR for the head is constructed, the MR of all
other (non-head) children are computed recursively,
and COMPOSEMR assigns their MR?s to fill the ar-
guments in the head?s MR to construct the com-
plete MR for the node. Argument constraints are
used to determine the appropriate filler for each ar-
gument. Since, N2 has a null label, the MR of N3
also becomes bowner( ). When computing the MR
for N7, N4 is determined to be the head with the
MR: player( , ). COMPOSEMR then assigns N5?s
MR to fill the team argument and N6?s MR to fill
the unum argument to construct N7?s complete MR:
player(our, 2). This MR in turn is composed with
the MR for N3 to yield the final MR for the sen-
tence: bowner(player(our,2)).
For MRL?s, such as CLANG, whose syntax does
not strictly follow a nested set of predicates and ar-
guments, some final minor syntactic adjustment of
the final MR may be needed. In the example, the
final MR is (bowner (player our f2g)). In the fol-
lowing discussion, we ignore the difference between
these two.
There are a few complications left which re-
quire special handling when generating MR?s,
like coordination, anaphora resolution and non-
compositionality exceptions. Due to space limita-
tions, we do not present the straightforward tech-
niques we used to handle them.
4 Corpus Annotation
This section discusses how sentences for training
SCISSOR were manually annotated with SAPT?s.
Sentences were parsed by Collins? head-driven
model 2 (Bikel, 2004) (trained on sections 02-21
of the WSJ Penn Treebank) to generate an initial
syntactic parse tree. The trees were then manually
corrected and each node augmented with a semantic
label.
First, semantic labels for individual words, called
semantic tags, are added to the POS nodes in the
tree. The tag null is used for words that have no cor-
responding concept. Some concepts are conveyed
by phrases, like ?has the ball? for bowner in the pre-
vious example. Only one word is labeled with the
concept; the syntactic head word (Collins, 1997) is
preferred. During parsing, the other words in the
phrase will provide context for determining the se-
mantic label of the head word.
Labels are added to the remaining nodes in a
bottom-up manner. For each node, one of its chil-
dren is chosen as the semantic head, from which it
will inherit its label. The semantic head is chosen
as the child whose semantic label can take the MR?s
of the other children as arguments. This step was
done mostly automatically, but required some man-
ual corrections to account for unusual cases.
In order for COMPOSEMR to be able to construct
the MR for a node, the argument constraints for
its semantic head must identify a unique concept
to fill each argument. However, some predicates
take multiple arguments of the same type, such as
point.num(num,num), which is a kind of point that
represents a field coordinate in CLANG.
In this case, extra nodes are inserted in the tree
with new type concepts that are unique for each ar-
gument. An example is shown in Figure 4 in which
the additional type concepts num1 and num2 are in-
troduced. Again, during parsing, context will be
used to determine the correct type for a given word.
The point label of the root node of Figure 4 is the
concept that includes all kinds of points in CLANG.
Once a predicate has all of its arguments filled, we
11
,0.5 , ?RRB?
?RRB??null  
?LRB? 0.1
CD?num CD?num
?LRB??point.num
PRN?point
CD?num1 CD?num2
Figure 4: Adding new types to disambiguate argu-
ments.
use the most general CLANG label for its concept
(e.g. point instead of point.num). This generality
avoids sparse data problems during training.
5 Integrated Parsing Model
5.1 Collins Head-Driven Model 2
Collins? head-driven model 2 is a generative, lexi-
calized model of statistical parsing. In the following
section, we follow the notation in (Collins, 1997).
Each non-terminal X in the tree is a syntactic label,
which is lexicalized by annotating it with a word,
w, and a POS tag, t
syn
. Thus, we write a non-
terminal as X(x), where X is a syntactic label and
x = hw; t
syn
i. X(x) is then what is generated by
the generative model.
Each production LHS ) RHS in the PCFG is
in the form:
P (h)!L
n
(l
n
):::L
1
(l
1
)H(h)R
1
(r
1
):::R
m
(r
m
)
where H is the head-child of the phrase, which in-
herits the head-word h from its parent P . L
1
:::L
n
and R
1
:::R
m
are left and right modifiers of H .
Sparse data makes the direct estimation of
P(RHSjLHS) infeasible. Therefore, it is decom-
posed into several steps ? first generating the head,
then the right modifiers from the head outward,
then the left modifiers in the same way. Syntactic
subcategorization frames, LC and RC, for the left
and right modifiers respectively, are generated be-
fore the generation of the modifiers. Subcat frames
represent knowledge about subcategorization prefer-
ences. The final probability of a production is com-
posed from the following probabilities:
1. The probability of choosing a head constituent
label H: P
h
(HjP; h).
2. The probabilities of choosing the left and right
subcat frames LC and RC: P
l
(LCjP;H; h)
and P
r
(RCjP;H; h).
has2our player the
PRP$?team NN?player CD?unum
NN?nullDT?null
NP?player(player) VP?bowner(has)
NP?null(ball)
ball
S?bowner(has)
VB?bowner
Figure 5: A lexicalized SAPT.
3. The probabilities of generat-
ing the left and right modifiers:
Q
i=1::m+1
P
r
(R
i
(r
i
)jH;P; h;
i 1
; RC) 
Q
i=1::n+1
P
l
(L
i
(l
i
)jH;P; h;
i 1
; LC).
Where  is the measure of the distance from
the head word to the edge of the constituent,
and L
n+1
(l
n+1
) and R
m+1
(r
m+1
) are STOP .
The model stops generating more modifiers
when STOP is generated.
5.2 Integrating Semantics into the Model
We extend Collins? model to include the genera-
tion of semantic labels in the derivation tree. Un-
less otherwise stated, notation has the same mean-
ing as in Section 5.1. The subscript syn refers to
the syntactic part, and sem refers to the semantic
part. We redefine X and x to include semantics,
each non-terminal X is now a pair of a syntactic la-
bel X
syn
and a semantic label X
sem
. Besides be-
ing annotated with the word, w, and the POS tag,
t
syn
, X is also annotated with the semantic tag,
t
sem
, of the head child. Thus, X(x) now consists of
X = hX
syn
;X
sem
i, and x = hw; t
syn
; t
sem
i. Fig-
ure 5 shows a lexicalized SAPT (but omitting t
syn
and t
sem
).
Similar to the syntactic subcat frames, we also
condition the generation of modifiers on semantic
subcat frames. Semantic subcat frames give se-
mantic subcategorization preferences; for example,
player takes a team and a unum. Thus LC and RC
are now: hLC
syn
; LC
sem
i and hRC
syn
; RC
sem
i.
X(x) is generated as in Section 5.1, but using the
new definitions of X(x), LC and RC . The imple-
mentation of semantic subcat frames is similar to
syntactic subcat frames. They are multisets speci-
fying the semantic labels which the head requires in
its left or right modifiers.
As an example, the probability of generating the
phrase ?our player 2? using NP-[player](player) !
12
PRP$-[team](our) NN-[player](player) CD-[unum](2)
is (omitting only the distance measure):
P
h
(NN-[player]jNP-[player],player)
P
l
(hfg,fteamgijNP-[player],player)
P
r
(hfg,funumgijNP-[player],player)
P
l
(PRP$-[team](our)jNP-[player],player,hfg,fteamgi)
P
r
(CD-[unum](2)jNP-[player],player,hfg,funumgi)
P
l
(STOPjNP-[player],player,hfg,fgi)
P
r
(STOPjNP-[player],player,hfg,fgi)
5.3 Smoothing
Since the left and right modifiers are independently
generated in the same way, we only discuss smooth-
ing for the left side. Each probability estimation in
the above generation steps is called a parameter. To
reduce the risk of sparse data problems, the parame-
ters are decomposed as follows:
P
h
(HjC) = P
h
syn
(H
syn
jC)
P
h
sem
(H
sem
jC;H
syn
)
P
l
(LCjC) = P
l
syn
(LC
syn
jC)
P
l
sem
(LC
sem
jC;LC
syn
)
P
l
(L
i
(l
i
)jC) = P
l
syn
(L
i
syn
(lt
i
syn
; lw
i
)jC)
P
l
sem
(L
i
sem
(lt
i
sem
; lw
i
)jC;L
i
syn
(lt
i
syn
))
For brevity, C is used to represent the context on
which each parameter is conditioned; lw
i
, lt
i
syn
, and
lt
i
sem
are the word, POS tag and semantic tag gener-
ated for the non-terminal L
i
. The word is generated
separately in the syntactic and semantic outputs.
We make the independence assumption that the
syntactic output is only conditioned on syntactic fea-
tures, and semantic output on semantic ones. Note
that the syntactic and semantic parameters are still
integrated in the model to find the globally most
likely parse. The syntactic parameters are the same
as in Section 5.1 and are smoothed as in (Collins,
1997). We?ve also tried different ways of condition-
ing syntactic output on semantic features and vice
versa, but they didn?t help. Our explanation is the
integrated syntactic and semantic parameters have
already captured the benefit of this integrated ap-
proach in our experimental domains.
Since the semantic parameters do not depend on
any syntactic features, we omit the sem subscripts
in the following discussion. As in (Collins, 1997),
the parameter P
l
(L
i
(lt
i
; lw
i
)jP;H;w; t;; LC) is
further smoothed as follows:
P
l1
(L
i
jP;H;w; t;; LC) 
P
l2
(lt
i
jP;H;w; t;; LC;L
i
)
P
l3
(lw
i
jP;H;w; t;; LC;L
i
(lt
i
))
Note this smoothing is different from the syntactic
counterpart. This is due to the difference between
POS tags and semantic tags; namely, semantic tags
are generally more specific.
Table 1 shows the various levels of back-off for
each semantic parameter. The probabilities from
these back-off levels are interpolated using the tech-
niques in (Collins, 1997). All words occurring less
than 3 times in the training data, and words in test
data that were not seen in training, are unknown
words and are replaced with the ?UNKNOWN? to-
ken. Note this threshold is smaller than the one used
in (Collins, 1997) since the corpora used in our ex-
periments are smaller.
5.4 POS Tagging and Semantic Tagging
For unknown words, the POS tags allowed are lim-
ited to those seen with any unknown words during
training. Otherwise they are generated along with
the words using the same approach as in (Collins,
1997). When parsing, semantic tags for each known
word are limited to those seen with that word dur-
ing training data. The semantic tags allowed for an
unknown word are limited to those seen with its as-
sociated POS tags during training.
6 Experimental Evaluation
6.1 Methodology
Two corpora of NL sentences paired with MR?s
were used to evaluate SCISSOR. For CLANG, 300
pieces of coaching advice were randomly selected
from the log files of the 2003 RoboCup Coach Com-
petition. Each formal instruction was translated
into English by one of four annotators (Kate et al,
2005). The average length of an NL sentence in
this corpus is 22.52 words. For GEOQUERY, 250
questions were collected by asking undergraduate
students to generate English queries for the given
database. Queries were then manually translated
13
BACK-OFFLEVEL P
h
(Hj:::) P
LC
(LCj:::) P
L1
(L
i
j:::) P
L2
(lt
i
j:::) P
L3
(lw
i
j:::)
1 P,w,t P,H,w,t P,H,w,t,,LC P,H,w,t,,LC, L
i
P,H,w,t,,LC, L
i
, lt
i
2 P,t P,H,t P,H,t,,LC P,H,t,,LC, L
i
P,H,t,,LC, L
i
, lt
i
3 P P,H P,H,,LC P,H,,LC, L
i
L
i
, lt
i
4 ? ? ? L
i
lt
i
Table 1: Conditioning variables for each back-off level for semantic parameters (sem subscripts omitted).
into logical form (Zelle and Mooney, 1996). The
average length of an NL sentence in this corpus is
6.87 words. The queries in this corpus are more
complex than those in the ATIS database-query cor-
pus used in the speech community (Zue and Glass,
2000) which makes the GEOQUERY problem harder,
as also shown by the results in (Popescu et al, 2004).
The average number of possible semantic tags for
each word which can represent meanings in CLANG
is 1.59 and that in GEOQUERY is 1.46.
SCISSOR was evaluated using standard 10-fold
cross validation. NL test sentences are first parsed
to generate their SAPT?s, then their MR?s were built
from the trees. We measured the number of test sen-
tences that produced complete MR?s, and the num-
ber of these MR?s that were correct. For CLANG,
an MR is correct if it exactly matches the correct
representation, up to reordering of the arguments of
commutative operators like and. For GEOQUERY,
an MR is correct if the resulting query retrieved
the same answer as the correct representation when
submitted to the database. The performance of the
parser was then measured in terms of precision (the
percentage of completed MR?s that were correct)
and recall (the percentage of all sentences whose
MR?s were correctly generated).
We compared SCISSOR?s performance to several
previous systems that learn semantic parsers that can
map sentences into formal MRL?s. CHILL (Zelle and
Mooney, 1996) is a system based on Inductive Logic
Programming (ILP). We compare to the version
of CHILL presented in (Tang and Mooney, 2001),
which uses the improved COCKTAIL ILP system and
produces more accurate parsers than the original ver-
sion presented in (Zelle and Mooney, 1996). SILT is
a system that learns symbolic, pattern-based, trans-
formation rules for mapping NL sentences to formal
languages (Kate et al, 2005). It comes in two ver-
sions, SILT-string, which maps NL strings directly
to an MRL, and SILT-tree, which maps syntactic
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250
Pr
ec
is
io
n 
(%
)
Training sentences
SCISSOR
SILT-string
SILT-tree
CHILL
GEOBASE
Figure 6: Precision learning curves for GEOQUERY.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  50  100  150  200  250
R
ec
al
l (%
)
Training sentences
SCISSOR
SILT-string
SILT-tree
CHILL
GEOBASE
Figure 7: Recall learning curves for GEOQUERY.
parse trees (generated by the Collins parser) to an
MRL. In the GEOQUERY domain, we also compare
to the original hand-built parser GEOBASE.
6.2 Results
Figures 6 and 7 show the precision and recall learn-
ing curves for GEOQUERY, and Figures 8 and 9 for
CLANG. Since CHILL is very memory intensive,
it could not be run with larger training sets of the
CLANG corpus.
Overall, SCISSOR gives the best precision and re-
call results in both domains. The only exception
is with recall for GEOQUERY, for which CHILL is
slightly higher. However, SCISSOR has significantly
higher precision (see discussion in Section 7).
14
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300
Pr
ec
is
io
n 
(%
)
Training sentences
SCISSOR
SILT-string
SILT-tree
CHILL
Figure 8: Precision learning curves for CLANG.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  50  100  150  200  250  300
R
ec
al
l (%
)
Training sentences
SCISSOR
SILT-string
SILT-tree
CHILL
Figure 9: Recall learning curves for CLANG.
Results on a larger GEOQUERY corpus with 880
queries have been reported for PRECISE (Popescu et
al., 2003): 100% precision and 77.5% recall. On
the same corpus, SCISSOR obtains 91.5% precision
and 72.3% recall. However, the figures are not com-
parable. PRECISE can return multiple distinct SQL
queries when it judges a question to be ambigu-
ous and it is considered correct when any of these
SQL queries is correct. Our measure only considers
the top result. Due to space limitations, we do not
present complete learning curves for this corpus.
7 Related Work
We first discuss the systems introduced in Section
6. CHILL uses computationally-complex ILP meth-
ods, which are slow and memory intensive. The
string-based version of SILT uses no syntactic in-
formation while the tree-based version generates a
syntactic parse first and then transforms it into an
MR. In contrast, SCISSOR integrates syntactic and
semantic processing, allowing each to constrain and
inform the other. It uses a successful approach to sta-
tistical parsing that attempts to find the SAPT with
maximum likelihood, which improves robustness
compared to purely rule-based approaches. How-
ever, SCISSOR requires an extra training input, gold-
standard SAPT?s, not required by these other sys-
tems. Further automating the construction of train-
ing SAPT?s from sentences paired with MR?s is a
subject of on-going research.
PRECISE is designed to work only for the spe-
cific task of NL database interfaces. By comparison,
SCISSOR is more general and can work with other
MRL?s as well (e.g. CLANG). Also, PRECISE is not
a learning system and can fail to parse a query it con-
siders ambiguous, even though it may not be consid-
ered ambiguous by a human and could potentially be
resolved by learning regularities in the training data.
In (Lev et al, 2004), a syntax-driven approach
is used to map logic puzzles described in NL to
an MRL. The syntactic structures are paired with
hand-written rules. A statistical parser is used to
generate syntactic parse trees, and then MR?s are
built using compositional semantics. The meaning
of open-category words (with only a few exceptions)
is considered irrelevant to solving the puzzle and
their meanings are not resolved. Further steps would
be needed to generate MR?s in other domains like
CLANG and GEOQUERY. No empirical results are
reported for their approach.
Several machine translation systems also attempt
to generate MR?s for sentences. In (et al, 2002),
an English-Chinese speech translation system for
limited domains is described. They train a statisti-
cal parser on trees with only semantic labels on the
nodes; however, they do not integrate syntactic and
semantic parsing.
History-based models of parsing were first in-
troduced in (Black et al, 1993). Their original
model also included semantic labels on parse-tree
nodes, but they were not used to generate a formal
MR. Also, their parsing model is impoverished com-
pared to the history included in Collins? more recent
model. SCISSOR explores incorporating semantic
labels into Collins? model in order to produce a com-
plete SAPT which is then used to generate a formal
MR.
The systems introduced in (Miller et al, 1996;
Miller et al, 2000) also integrate semantic labels
into parsing; however, their SAPT?s are used to pro-
15
duce a much simpler MR, i.e., a single semantic
frame. A sample frame is AIRTRANSPORTATION
which has three slots ? the arrival time, origin and
destination. Only one frame needs to be extracted
from each sentence, which is an easier task than
our problem in which multiple nested frames (pred-
icates) must be extracted. The syntactic model in
(Miller et al, 2000) is similar to Collins?, but does
not use features like subcat frames and distance mea-
sures. Also, the non-terminal label X is not further
decomposed into separately-generated semantic and
syntactic components. Since it used much more spe-
cific labels (the cross-product of the syntactic and
semantic labels), its parameter estimates are poten-
tially subject to much greater sparse-data problems.
8 Conclusion
SCISSOR learns statistical parsers that integrate syn-
tax and semantics in order to produce a semanti-
cally augmented parse tree that is then used to com-
positionally generate a formal meaning representa-
tion. Experimental results in two domains, a natural-
language database interface and an interpreter for
coaching instructions in robotic soccer, have demon-
strated that SCISSOR generally produces more accu-
rate semantic representations than several previous
approaches. By augmenting a state-of-the-art statis-
tical parsing model to include semantic information,
it is able to integrate syntactic and semantic clues
to produce a robust interpretation that supports the
generation of complete formal meaning representa-
tions.
9 Acknowledgements
We would like to thank Rohit J. Kate , Yuk Wah
Wong and Gregory Kuhlmann for their help in an-
notating the CLANG corpus and providing the eval-
uation tools. This research was supported by De-
fense Advanced Research Projects Agency under
grant HR0011-04-1-0007.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Ezra Black, Frederick Jelineck, John Lafferty, David M. Mager-
man, Robert L. Mercer, and Salim Roukos. 1993. Towards
history-based grammars: Using richer models for probabilis-
tic parsing. In Proc. of ACL-93, pages 31?37, Columbus,
Ohio.
Borland International. 1988. Turbo Prolog 2.0 Reference
Guide. Borland International, Scotts Valley, CA.
Mao Chen et al 2003. Users manual: RoboCup
soccer server manual for soccer server version 7.07
and later. Available at http://sourceforge.net/
projects/sserver/.
Michael J. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of ACL-97, pages 16?23,
Madrid, Spain.
Yuqing Gao et al 2002. Mars: A statistical semantic parsing
and generation-based multilingual automatic translation sys-
tem. Machine Translation, 17:185?212.
Daniel Gildea and Daniel Jurafsky. 2002. Automated labeling
of semantic roles. Computational Linguistics, 28(3):245?
288.
Daniel Jurafsky and James H. Martin. 2000. Speech and Lan-
guage Processing: An Introduction to Natural Language
Processing, Computational Linguistics, and Speech Recog-
nition. Prentice Hall, Upper Saddle River, NJ.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005.
Learning to transform natural to formal languages. To ap-
pear in Proc. of AAAI-05, Pittsburgh, PA.
Iddo Lev, Bill MacCartney, Christopher D. Manning, and Roger
Levy. 2004. Solving logic puzzles: From robust process-
ing to precise semantics. In Proc. of 2nd Workshop on Text
Meaning and Interpretation, ACL-04, Barcelona, Spain.
Scott Miller, David Stallard, Robert Bobrow, and Richard
Schwartz. 1996. A fully statistical approach to natural lan-
guage interfaces. In ACL-96, pages 55?61, Santa Cruz, CA.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M.
Weischedel. 2000. A novel use of statistical parsing to ex-
tract information from text. In Proc. of NAACL-00, pages
226?233, Seattle, Washington.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003. To-
wards a theory of natural language interfaces to databases. In
Proc. of IUI-2003, pages 149?157, Miami, FL. ACM.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko,
and Alexander Yates. 2004. Modern natural language in-
terfaces to databases: Composing statistical parsing with se-
mantic tractability. In COLING-04, Geneva, Switzerland.
Lappoon R. Tang and Raymond J. Mooney. 2001. Using multi-
ple clause constructors in inductive logic programming for
semantic parsing. In Proc. of ECML-01, pages 466?477,
Freiburg, Germany.
John M. Zelle and Raymond J. Mooney. 1996. Learning to
parse database queries using inductive logic programming.
In Proc. of AAAI-96, pages 1050?1055, Portland, OR.
Victor W. Zue and James R. Glass. 2000. Conversational in-
terfaces: Advances and challenges. In Proc. of the IEEE,
volume 88(8), pages 1166?1180.
16
