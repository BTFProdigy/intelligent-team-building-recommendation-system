Semantic Case Role Detection for Information Extraction 
 
Rik DE BUSSER and Roxana ANGHELUTA and Marie-Francine MOENS 
Interdisciplinary Centre for Law and IT 
Katholieke Universiteit Leuven 
Tiensestraat 41 
B-3000 Leuven, Belgium 
rik.debusser, roxana.angheluta, marie-france.moens@law.kuleuven.ac.be 
 
Abstract 
If information extraction wants to make its 
results more accurate, it will have to resort 
increasingly to a coherent implementation of 
natural language semantics. In this paper, we 
will focus on the extraction of semantic case 
roles from texts. After setting the essential 
theoretical framework, we will argue that it 
is possible to detect case roles on the basis 
of morphosyntactic and lexical surface 
phenomena. We will give a concise 
overview of our methodology and of a 
preliminary test that seems to confirm our 
hypotheses. 
Introduction 
Information extraction (IE) from texts currently 
receives a large research interest. Traditionally, 
it has been associated with the ? often verbatim 
? extraction of domain-specific information 
from free text (Riloff & Lorenzen 1999). Input 
documents are scanned for very specific relevant 
information elements on a particular topic, 
which are used to fill out empty slots in a 
predefined frame. Other types of systems try to 
acquire this knowledge automatically by 
detecting reoccurring lexical and syntactic 
information from manually annotated example 
texts (e.g. Soderland 1999). 
Most of these techniques are inherently limited 
because they exclude natural language semantics 
as much as possible. This is understandable for 
reasons of efficiency and genericity but it 
restricts the algorithms' possibilities and it 
disregards the fact that ? at least in free text ? IE 
has much to do with identifying semantic roles. 
In most of these systems, case role detection as a 
goal in itself has been treated in a rather trivial 
way. Our research will try to provide a 
systematic approach to case role detection as an 
independent extraction task. Using notions from 
systemic-functional grammar and presupposing 
a possible mapping between morphosyntactic 
properties and functional role patterns, we will 
develop a general model for case role extraction. 
The idea is to learn domain-independent case 
role patterns from a tagged corpus, which are 
then (automatically) specialized to particular 
domain-dependent case role sets and which can 
be reassigned to previously unseen text. In this 
paper, we will focus on the first part of this task.  
For IE, an accurate and speedy detection of 
functional case roles is of major importance, 
since they describe events (or states) and 
participants to these events and thus allow for 
identifying real-world entities, their properties 
and interactions between them. 
1 Theoretical setting 
One of the earliest and most notable accounts on 
case roles is without any doubt Charles 
Fillmore's groundbreaking article (Fillmore 
1968). His most fundamental argument is that 
the notion of case is not so much connected to 
morphosyntactic surface realisation as to 
syntactico-semantic categories in the deep 
structure of a language. Particular constellations 
of case roles determine distinctive functional 
patterns, a considerable part of which (according 
to Fillmore) is likely to be universally valid. 
This deep-structure case system can be realized 
in the surface structure by means of a set of 
language-dependent transformation rules (see 
Fillmore 1968). As a consequence there has to 
be a regular mapping between the case system 
and its surface realization ? which includes case 
markers, word order, grammatical roles, etc.  For 
our research, we will disregard the 
transformational dimension in Fillmore's theory 
but we will nevertheless assume that there is at 
least some degree of correspondence between 
the case role system underlying a language and 
its (1) morphosyntax, (2) relative word order and 
(3) lexicon. 
In Halliday's systemic-functional grammar 
(Halliday 1994; Halliday & Matthiessen 1999), 
functional patterns that are part of the language's 
deep structure are organized as figures, i.e. 
configurations of case roles which consist of: 
1. A nuclear process, which is typically realized 
by a verb phrase. Processes express an event 
or state as it is distinctly perceived by the 
language user. 
2. A limited number of participants, which are 
inherent to the process and are typically 
realized by noun phrases. They represent 
entities or abstractions that participate in the 
process. 
3. An in theory unlimited number of 
circumstantial elements. Circumstances are 
in most cases optional and are typically 
realized by prepositional or adverbial 
phrases. They allocate the process and its 
participants in a temporal, spatial, causal, ? 
context.  
Processes are classified into types and subtypes, 
each having its particular participant 
combinations. We discern four main process 
types: Material, Mental, Verbal and Relational 
(Halliday 1994). Figure 1 is an example of a 
verbal process, the Sayer being the participant 
'doing' the process and the Receiver the one to 
whom the (implicit) verbal message is directed. 
 
Invesco in merger talks with AIM Management 
Sayer Verbal Process Receiver 
Figure 1 ? Example of a verbal process 
 
Since these main types (and some secondary 
ones) correspond to universal experiential modi, 
it is to be expected that they will have a certain 
universal validity, i.e. that they are in some way 
or another present in all languages of the world. 
For our preliminary experiments, we use a 
reduced version of the case role model proposed 
by Halliday (1994, p. 106-175), as it is a 
consistent, well-developed and relatively simple 
system, which makes it very suitable for testing 
the validity of our assumptions. For actual 
applications, we will replace it by a more 
elaborate variant, most likely Bateman's 
Generalized Upper Model (Bateman 1990; 
Bateman et al in progress). Bateman's model is 
finer-grained than Halliday's; it is to a large 
extent language-independent; and it has been 
specifically developed for implementation into 
NLP systems (see Bateman et al in progress).    
2 Our approach 
Given the framework outlined above, we 
consider case role detection to be a standard 
classification task. In pattern classification one 
attempts to learn certain patterns or rules from 
classified examples and to use them for 
classifying previously unseen instances (Hand 
1997). In our case, a class is a concatenation of 
case roles that constitute one particular process 
(i.e. the deep structure figure) and the pattern 
itself is to be derived from the morphosyntactic 
and lexical properties corresponding to that 
process (its surface realisation). 
Taking that point-of-view, individual 
realisations of figures ? roughly corresponding 
to stripped-down clauses ? are translated into 
fixed-length sets of lexical and morphosyntactic 
features (word order is implicitly encoded) and a 
functional classification is manually assigned to 
them. For each verb the classification algorithm 
then attempts to match all functional patterns to 
one or a few relevant sets of distinctive features. 
The latter are translated into patterns that can be 
used to match an occurrence in a text to a 
particular constellation of case roles. 
The entire learning process consists of five main 
steps: 
1. Preprocessing 
2. Annotation 
3. Feature selection 
4. Training of the classifier 
5. Translation into rules 
In the preprocessing phase, the input text is 
tagged, lemmatised and chunked. The output is 
standardized and passed to the annotation tool, 
in which the user is asked to assign case role 
patterns to individual clauses. For now, we will 
only take into account processes, participants 
and circumstantial elements of Extent and 
Location. 
In a next step, individual training examples ? 
each example corresponding to one figure ? are 
converted to a fixed-length feature vector. For 
each phrase, the lexical and morphosyntactic 
features of the head and of the left and right 
context boundaries (i.e. the first and the last 
token of the strings pre- and postmodifying the 
head) are automatically extracted from the 
tagged text and added to the vector. This enables 
us to align corresponding features quite 
accurately without having to resort to any 
complex form of phrasal analysis. Although this 
reduction of the context of the head word may 
seem to be counter-intuitive from a grammatical 
point-of-view, our initial tests indicate that it 
does capture most constructions that are relevant 
to the extraction task. 
Feature selection is necessary for two main 
reasons. Firstly, it is impossible to take into 
account all lexical and morphosyntactic features, 
since that would boost the time-complexity, 
incorporate many irrelevant features and bring 
down accuracy when a limited set of training 
examples is available. Secondly, natural 
language utterances have the uncanny habit of 
being of variable length. The latter aspect is 
problematic not only because classification 
algorithms usually expect a clearly delineated 
set of features, but also because it is crucial to 
align examples in order to compare 
correspondent features. 
In our test setting, we will constrain the maximal 
number of case roles per figure to four. Since 
each case role is transformed into a set of 10 
features, a figure will be translated into a 40-
dimensional feature vector (see Figure 2). 
As a result, a particular constellation of case 
roles is treated as one pattern in which each role 
and each of its relevant features has a fixed 
position. We expect this vector representation to 
be relevant in most languages apart from free 
word order languages. Currently, our model 
focuses on English. 
In the fourth step, the classifier is trained to 
discriminate features that are distinctive for each 
process type associated with a particular verb. 
These features are again translated into rules that 
can be used for reassigning case roles that have 
been learned to previously unseen text. 
This is necessary because the variable length of 
figures and ? within figures ? of phrases is 
bound to cause difficulties when applying the 
patterns that were learned to new sentences. 
Rules have the advantage over feature vectors in 
that they allow us to use head-centred 
stretching: when figures are assigned to 
previously unseen sentences and no pattern can 
immediately be matched, the nearest equivalent 
according to the head of the figure will be 
assigned; the rest of the pattern will be allocated 
by shifting the left and right context of the head 
towards the left and right sentence boundaries. A 
similar approach will be used for matching 
individual roles to phrases. 
3 An experiment 
Before engaging in the laboursome task of 
building a set of tools and tagging an entire 
corpus, we decided to test the practical validity 
of our ideas on a small scale on the verb be. We 
manually constructed a limited set of training 
examples (76 occurrences) from the new Reuters 
corpus (CD-rom, Reuters Corpus. Volume 1: 
English Language, 1996-08-20 to 1997-08-19) 
and processed it with the C4.5 classification 
algorithm (Quinlan 1993). 
Figure 3 gives an overview of the process. The 
tagged text1 (step 1) is translated into a set of 
                                                      
1 For our first experiment we used TnT 
(http://www.coli.uni-sb.de/~thorsten/tnt/).  In our 
 
Figure 2 ? The feature set 
 
features (step 2). A functional pattern is used as 
the class corresponding to the feature set (the 
last entry in step 2). The classifier extracts one 
or more distinctive features (step 3), which are 
in turn transposed into a rule (step 4) that is used 
in case role assignment. 
 
 
Figure 3 ? Schematic illustration of the experiment 
 
Initial results are encouraging. The evaluation 
component of C4.5 revealed an error rate of 
9.2% when reapplying its rule extractions on the 
training data. Given the limited amount of data, 
these results are reasonable. Manual application 
of the rules (from step 4) to new text confirmed 
their natural look-and-feel. We are currently 
testing the approach with larger amounts of 
training and testing data. Most of the current 
errors are caused by the limited amount of 
training data in our experiment: in a number of 
cases there was only one instance of a particular 
figure. 
4 Discussion and future improvements 
Although most shortcomings that arose in our 
present set-up can be settled relatively easily, a 
number of issues still remains to be resolved. 
From a theoretical angle, the most urgent 
problem is the underspecification of the material 
domain (or the disagreement on exactly how 
material processes ought to be subclassified). 
Unfortunately, most verb meanings are material 
                                                                                
present experiment, it is replaced by LT POS 
(http://www.ltg.ed.ac.uk/~mikheev/software.html). 
We manually lemmatised the tokens, but we are 
currently using a lemmatizer based on WordNet. 
and distinctions in the material domain tend to 
be rather crucial in most IE applications. 
Two major implementational difficulties are 
related to circumstantial elements. Since 
circumstances are normally not inherent to the 
process, they do not tend to have a fixed position 
in a figure. In addition, no formal parameters 
exist to distinguish obligatory circumstances 
from optional ones. Since it would be absurd to 
encode all variation in separate patterns, it is 
tempting just to add empty slots at the most 
predictable positions where circumstances could 
appear, but that would still not tell apart optional 
and obligatory circumstances and it would be a 
rather ad hoc solution. We are currently 
investigating whether both problems might be 
dealt with by encoding the relative position of 
case roles explicitly. 
In our current information society, it will 
become increasingly important to extract 
information on well-specified events or entities 
from documents. Case role detection will 
provide a way to do this by integrating 'real' 
semantics into the systems without 
overburdening the algorithms. For instance, in 
our example analysis (Figure 1) we can 
immediately identify two entities involved in a 
communicative action, one that does the talking 
('Invesco') and one that is being talked to ('AIM 
management'). An immediate application of case 
role detection is straightforward IE, which 
typically attempts to extract specific information 
from a text. However, the algorithm could also 
be used for optimising information retrieval 
applications, in the construction of knowledge 
bases, in questioning-answering systems or in 
case-based reasoning. Actually, for real natural 
language understanding a highly accurate model 
for interpreting case roles in some form will be 
unavoidable. 
A major advantage of our approach is that the 
pattern base resulting from it will contain 
semantic information and yet be fully domain-
independent. In a next stage of our research, we 
will try to specialize the generic case roles 
automatically to domain-dependent ones. At first 
sight, this two-step approach might appear 
cumbersome, but it will enable us to easily 
expand the pattern base while reusing the hard-
won patterns. 
5 Related research 
Historically, case role detection has its roots in 
frame-based approaches to IE (e.g. Schank & 
Abelson 1977). The main problem here is that to 
build case frames one needs prior knowledge on 
which information exactly one wants to extract. 
In recent years, different solutions have been 
offered to automatically generate those frames 
from annotated examples (e.g. Riloff & 
Schmelzenbach 1998, Soderland 1999) or by 
using added knowledge (e.g. Harabagiu & 
Maiorano 2000). Many of those approaches 
were very successful but most of them have a 
tendency to blend syntactic and semantic 
concepts and they still have to be trained on 
individual domains. 
Some very interesting research on case frame 
detection has been done by Gildea (Gildea 2000, 
Gildea 2001). He uses statistical methods to 
learn case frames from parsed examples from 
FrameNet (Johnson et al 2001). 
Conclusion 
There is a definite need for case role analysis in 
IE and in natural language processing in general. 
In this article, we have tried to argue that generic 
case role detection is possible by using shallow 
text analysis methods. We outlined our 
functional framework and presented a model 
that considers case role pattern extraction to be a 
standard classification task. Our main focus for 
the near future will be on automating as many 
aspects of the annotation process as possible and 
on the construction of the case role assignment 
algorithm. In these tasks, the emphasis will be 
on genericity and reusability. 
Acknowledgements 
We would like to thank the Institute for the 
Promotion of Innovation by Science and 
Technology in Flanders (IWT-Flanders) for the 
research funding. 
References  
Bateman J. (1990) Upper Modelling: a general 
organization of knowledge for natural language 
processing. In "Proceedings of the International 
Language Generation Workshop", Pittsburgh, June 
1990. 
Bateman, J. (in progress) The Generalized Upper 
Model 2.0. http://www.darmstadt.gmd.de/publish/ 
komet/gen-um/newUM.html. Checked 15 February 
2002. 
Fillmore Ch. (1968) The case for case. In "Universals 
in Linguistic Theory", E. Bach & R.T. Harms, ed., 
Holt, Rinehart and Winston, New York, pp. 1-88. 
Gildea D. (2000) Automatic labeling of semantic 
roles. Qualifying exam proposal, University of 
California, January 2000, 21 p. 
Gildea D. (2001) Statistical Language Understanding 
Using Frame Semantics. PhD. dissertation, 
University of California at Berkeley, 2001, 109 p. 
Halliday M.A.K. (1994) An introduction to functional 
grammar. Arnold, London, 434 p. 
Halliday M.A.K. and Matthiessen C. (1999) 
Construing Experience Through Meaning. A 
Language-Based Approach to Cognition. Cassell, 
London, 657 p. 
Hand D. (1997) Construction and Assessment of 
Classification Rules. Chichester: John Wiley & 
Sons, Chichester, 214 p. 
Harabagiu S. and Maiorano S. (2000) Acquisition of 
linguistic patterns for knowledge-based 
information extraction. In "Proceedings of LREC-
2000", Athens, June 2000. 
Johnson C. et al (2001). The FrameNet Project: 
Tools for Lexicon Building. http://www.icsi. 
berkeley.edu/~framenet/book.pdf. Checked 15 
February 2002. 
Quinlan J. (1993) C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Mateo, 302 p. 
Riloff E. and Lorenzen J. (1999) Extraction-based 
text categorization: generating domain-specific 
role relationships automatically. In "Natural 
Language Information Retrieval", T. Strzalkowski, 
ed., Kluwer Academic Publishers, Dordrecht, pp. 
167-195. 
Riloff E. and Schelzenbach M. (1998) An empirical 
approach to conceptual case frame acquisition. In 
"Proceedings of the Sixth Workshop on Very large 
Corpora", Montreal, Canada, August 1998. 
Schank R. and Abelson R. (1977) Scripts, Plans, 
Goals and Understanding. An Inquiry into Human 
Knowledge Structures. Erlbaum, Hillsdale, NJ, 
248p. 
Soderland S. (1999) Learning information extraction 
rules for semi-structured and free text. In Machine 
Learning 34, 1/3, pp. 233-272.  
 
Workshop on TextGraphs, at HLT-NAACL 2006, pages 25?28,
New York City, June 2006. c?2006 Association for Computational Linguistics
Measuring Aboutness of an Entity in a Text 
 
Marie-Francine Moens Patrick Jeuniaux 
Legal Informatics and Information Retrieval Department. of Psychology 
Katholieke Universiteit Leuven, Belgium University of Memphis, USA 
marie-france.moens@law.kuleuven.be pjeuniaux@mail.psyc.memphis.edu 
Roxana Angheluta Rudradeb Mitra 
Legal Informatics and Information Retrieval Mission Critical, IT 
Katholieke Universiteit Leuven, Belgium Brussels, Belgium 
anghelutar@yahoo.com rdm@missioncriticalit.com  
  
Abstract 
In many information retrieval and selec-
tion tasks it is valuable to score how much 
a text is about a certain entity and to com-
pute how much the text discusses the en-
tity with respect to a certain viewpoint. In 
this paper we are interested in giving an 
aboutness score to a text, when the input 
query is a person name and we want to 
measure the aboutness with respect to the 
biographical data of that person. We pre-
sent a graph-based algorithm and compare 
its results with other approaches. 
1 Introduction 
In many information processing tasks one is inter-
ested in measuring how much a text or passage is 
about a certain entity. This is called aboutness or 
topical relevance (Beghtol 1986; Soergel 1994). 
Simple word counts of the entity term often give 
only a rough estimation of aboutness. The true fre-
quency of the entity might be hidden by corefer-
ents. Two entities are considered as coreferents 
when they both refer to the same entity in the situa-
tion described in the text (e.g., in the sentences: 
"Dan Quayle met his wife in college. The Indiana 
senator married her shortly after he finished his 
studies": "his", "Indiana senator" and "he" all core-
fer to "Dan Quayle"). If we want to score the 
aboutness of an entity with respect to a certain 
viewpoint, the aboutness is also obfuscated by the 
referents that refer to the chosen viewpoint and in 
which context the entity is mentioned. In the ex-
ample ?Dan Quayle ran for presidency?, ?presi-
dency? can be considered as a referent for ?Dan 
Quayle?. Because, coreferents and referents can be 
depicted in a graphical representation of the dis-
course content, it seems interesting to exploit this 
graph structure in order to compute aboutness. This 
approach is inspired by studies in cognitive science 
on text comprehension (van Dijk and Kintsch, 
1983). When humans read a text, they make many 
inferences about and link information that is found 
in the text, a behavior that influences aboutness 
assessment. Automated aboutness computation has 
many applications such as text indexing, summari-
zation, and text linking.  
We focus on estimating the aboutness score of a 
text given an input query in the form of a person 
proper name. The score should reflect how much 
the text deals with biographical information about 
the person. We present an algorithm based on ei-
genvector analysis of the link matrix of the dis-
course graph built by the noun phrase coreferents 
and referents. We test the approach with a small set 
of documents, which we rank by decreasing about-
ness of the input entity. We compare the results 
with results obtained by traditional approaches 
such as a normalized term frequency (possibly cor-
rected by coreference resolution and augmented 
with other referent information). Although the re-
sults on a small test set do not pretend to give firm 
evidence on the validity of our approach, our con-
tribution lies in the reflection of using graph based 
document representations of discourse content and 
exploiting this structure in content recognition.  
2 Methods  
Our approach involves the detection of entities and 
their noun phrase coreferents, the generation of 
terms that are correlated with biographical infor-
25
mation, the detection of references between enti-
ties, and the computation of the aboutness score. 
As linguistic resources we used the LT-POS tagger 
developed at the University of Edinburgh and the 
Charniak parser developed at Brown University.  
2.1 Noun Phrase Coreference Resolution 
Coreference resolution focuses on detecting ?iden-
tity'' relationships between noun phrases (i.e. not 
on is-a or whole/part links). It is natural to view 
coreferencing as a partitioning or clustering of the 
set of entities. The idea is to group coreferents into 
the same cluster, which is accomplished in two 
steps: 1) detection of the entities and extraction of 
their features set; 2) clustering of the entities. For 
the first subtask we use the same set of features as 
in Cardie and Wagstaff (1999). For the second step 
we used the progressive fuzzy clustering algorithm 
described in Angheluta et al (2004).  
2.2 Learning Biographical Terms 
We learn a term?s biographical value as the corre-
lation of the term with texts of biographical nature. 
There are different ways of learning associations 
present in corpora (e.g., use of the mutual informa-
tion statistic, use of the chi-square statistic). We 
use the likelihood ratio for a binomial distribution 
(Dunning 1993), which tests the hypothesis 
whether the term occurs independently in texts of 
biographical nature given a large corpus of bio-
graphical and non-biographical texts. For consider-
ing a term as biography-related, we set a likelihood 
ratio threshold such that the hypothesis can be re-
jected with a certain significance level.  
2.3 Reference Detection between Entities  
We assume that the syntactic relationships between 
entities (proper or common nouns) in a text give us 
information on their semantic reference status. In 
our simple experiment, we consider reference rela-
tionships found within a single sentence, and more 
specifically we take into account relationships be-
tween two noun phrase entities. The analysis re-
quires that the sentences are syntactically analyzed 
or parsed. The following syntactic relationships are 
detected in the parse tree of each sentence:   
1) Subject-object: An object refers to the subject 
(e.g., in the sentence He eats an apple, an apple 
refers to He). This relationship type also covers 
prepositional phrases that are the argument of a 
verb (e.g., in the sentence He goes to Hollywood, 
Hollywood refers to He). The relationship holds 
between the heads of the respective noun phrases 
in case other nouns modify them.    
2) NP-PP{NP}: A noun phrase is modified by a 
prepositional noun phrase: the head of the preposi-
tional noun phrase refers to the head of the domi-
nant noun phrase (e.g., in the chunk The nominee 
for presidency, presidency refers to The nominee). 
3) NP-NP: A noun phrase modifies another noun 
phrase: the head of the modifying noun phrase re-
fers to the head of the dominant noun phrase (e.g., 
in the chunk Dan Quayle's sister, Dan Quayle re-
fers to sister, in the chunk sugar factory, sugar 
refers to factory). 
 When a sentence is composed of different sub-
clauses and when one of the components of the 
first two relationships has the form of a subclause, 
the first noun phrase of the subclause is consid-
ered. When computing a reference relation with an 
entity term, we only consider biographical terms 
found as described in (2.2).  
2.4 Computing the Aboutness Score  
The aboutness of a document text D for the input 
entity E is computed as follows:  
 
aboutness(D,E) = entity _ score(E)
entity _ score(F)
F?distinctentities of D
?  
 
entity_score is zero when E does not occur in D. 
Otherwise we compute the entity score as follows. 
We represent D as a graph, where nodes represent 
the entities as mentioned in the text and the 
weights of the connections represent the reference 
score (in our experiments set to 1 when the entities 
are coreferents, 0.5 when the entities are other ref-
erents). The values 1 and 0.5 were selected ad hoc. 
Future fine-tuning of the weights of the edges of 
the discourse graph based on discourse features 
could be explored (cf. Giv?n 2001). The edge val-
ues are stored in a link matrix A. The authority of 
an entity is computed by considering the values of 
the principal eigenvector of ATA. (cf. Kleinberg 
1998) (in the results below this approach is re-
ferred to as LM). In this way we compute the au-
thority of each entity in a text.  
26
 We implemented four other entity scores: the 
term frequency (TF), the term frequency aug-
mented with noun phrase coreference information 
(TFCOREF), the term frequency augmented with 
reference information (weighted by 0.5) (TFREF) 
and the term frequency augmented with corefer-
ence and reference information (TFCOREFREF). 
The purpose is not that the 4 scoring functions are 
mutually comparable, but that the ranking of the 
documents that is produced by each of them can be 
compared against an ideal ranking built by hu-
mans.  
3 Experiments and Results 
For learning person related words we used a train-
ing corpus consisting of biographical texts of per-
sons obtained from the Web (from 
http://www.biography.com) and biographical and 
non-biographical texts from DUC-2002 and DUC-
2003. For considering a term as biography-related, 
we set a likelihood ratio threshold such that the 
hypothesis of independence can be rejected with a 
significance level of less than 0.0025, assuring that 
the selected terms are really biography-related.  
 In order to evaluate the aboutness computation, 
we considered five input queries consisting of a 
proper person name phrase ("Dan Quayle" (D), 
"Hillary Clinton" (H), "Napoleon" (N), "Sadam 
Hussein" (S) and "Sharon Stone" (ST)) and 
downloaded for each of the queries 5 texts from 
the Web (each text contains minimally once an 
exact match with the input query). Two persons 
were asked to rank the texts according to rele-
vancy, if they were searching biographical infor-
mation on the input person (100% agreement was 
obtained). Two aspects are important in determin-
ing relevancy: a text should really and almost ex-
clusively contain biographical information of the 
input person in order not to lose time with other 
information. For each query, at least one of the 
texts is a biographical text and one of the texts only 
marginally mentions the person in question. All 
texts except for the biography texts speak about 
other persons, and pronouns are abundantly used. 
The "Hillary Clinton" texts do not contain many 
other persons except for Hillary, in contrast with 
the "Dan Quayle", "Napoleon" and "Sadam Hus-
sein" texts. The "Hillary Clinton" texts are in gen-
eral quite relevant for this first lady. For 
"Napoleon" there is one biographical text on Napo-
leon's surgeon that mentions Napoleon only mar-
ginally. The ?Dan Quayle? texts contain a lot of 
direct speech. For "Sharon Stone" 4 out of the 5 
texts described a movie in which this actress 
played a role, thus being only marginally relevant 
for a demand of biographical data of the actress.  
 Then we ranked the texts based on the TF, 
TFCOREF, TFREF, TFCOREFREF and LM 
scores and computed the congruence of each rank-
ing (Rx) with the manual ranking (Rm). We used the 
following measure of similarity of the rankings:  
 
sim(Rx, Rm) =1?
rx, i? rm, i
i
?
floor
n2
2
*100
 
where n is the number of items in the 2 rankings 
and rx,i and rm,i denote the position of the ith item in 
Rx and Rm. respectively. Table 1 shows the results.  
4 Discussion of the Results and Related 
Research 
From our limited experiments we can draw the 
following findings. It is logical that erroneous 
coreference resolution worsens the results com-
pared to the TF baseline. In one of the "Napoleon? 
texts, one mention of Napoleon and one mention of 
the name of his surgeon entail that a large number 
of pronouns in the text are wrongly resolved. They 
all refer to the surgeon, but the system considers 
them as referring to Napoleon, making that the 
ranking of this text is completely inversed com-
pared to the ideal one. Adding other reference in-
formation gives some mixed results. The ranking 
based on the principal eigenvector computation of 
the link matrix of the text that represents reference 
relationships between entities provides a natural 
way of computing a ranking of the texts with re-
gard to the person entity. This can be explained as 
follows. Decomposition into eigenvectors breaks 
down the original relationships into linear inde-
pendent components. Sorting them according to 
their corresponding eigenvalues sorts the compo-
nents from the most important information to the 
less important one. When keeping the principal 
eigenvector, we keep the most important informa-
tion which best distinguishes it from other infor-
mation while ignoring marginal information. In 
this way we hope to smooth some noise that is 
generated when building the links. On the other 
hand, when relationships that are wrongly detected 
27
are dominant, they will be reinforced (as is the case 
in the ?Napoleon? text). Although an aboutness 
score is normalized by the sum of a text?s entity 
scores, the effect of this normalization and the be-
havior of eigenvectors in case of texts of different 
length should be studied.  
 The work is inspired by link analysis algorithms 
such as HITS, which uses theories of spectral parti-
tioning of a graph for detecting authoritative pages 
in a graph of hyperlinked pages (Kleinberg 1998). 
Analogically, Zha (2002) detects terms and sen-
tences with a high salience in a text and uses these 
for summarization. The graph here is made of 
linked term and sentence nodes. Other work on 
text summarization computes centrality on graphs 
(Erkan and Radev 2004; Mihalcea and Tarau 
2004). We use a linguistic motivation for linking 
terms in texts founded in reference relationships 
such as coreference and reference by biographical 
terms in certain syntactical constructs. Intuitively, 
an important entity is linked to many referents; the 
more important the referents are, the more impor-
tant the entity is. Latent semantic indexing (LSI) is 
also used to detect main topics in a set of docu-
ments/sentences, it will not explicitly model the 
weights of the edges between entities.  
 Our implementation aims at measuring the 
aboutness of an entity from a biographical view-
point. One can easily focus upon other viewpoints 
when determining the terms that enter into a refer-
ence relationship with the input entity (e.g., com-
puting the aboutness of an input animal name with 
regard to its reproductive activities). 
5 Conclusion 
In this paper we considered the problem of ranking 
texts when the input query is in the form of a per-
son proper name and when we are interested in 
biographical information. The ranking based on the 
computation of the principal eigenvector of the 
link matrix that represents coreferent and other 
referent relationships between noun phrase entities 
offers novel directions for future research. 
6 Acknowledgements 
The research was sponsored by the IWT-grant Nr. 
ADV/000135/KUL).  
 
Table 1. Similarity of the system made rankings com-
pared to the ideal ranking for the methods used with 
regard to the input queries.  
 
 TF TFCOREF TFREF TFCOREFREF LM 
D 0.33 0.00 0.33 0.00 0.50 
H 0.33 0.50 0.33 0.33 0.66 
N 0.66 0.33 0.66 0.66 0.33 
S 0.83 0.66 0.66 0.66 1.00 
ST 0.00 0.33 0.16 0.50 0.83 
 
7 References 
Angheluta, R., Jeuniaux, P., Mitra, R. and Moens, M.-F. 
(2004). Clustering algorithms for noun phrase 
coreference resolution. In Proceedings JADT - 2004. 
7?mes Journ?es internationales d'Analyse statistique 
des Donn?es Textuelles.  Louvain-La-Neuve, Bel-
gium. 
Beghtol, C. (1986). Bibliographic classification theory 
and text linguistics: Aboutness analysis, intertextual-
ity and the cognitive act of classifying documents. 
Journal of Documentation, 42(2): 84-113.   
Cardie C. and Wagstaff K. (1999). Noun phrase co-
reference as clustering. In Proceedings of the Joint 
Conference on Empirical Methods in NLP and Very 
Large Corpora.  
Dunning, T. (1993). Accurate methods for the statistics 
of surprise and coincidence. Computational Linguis-
tics, 19: 61-74.  
Erkan, G. and Radev, D.R. (2004). LexRank: Graph-
based lexical centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research, 22: 
457-479.  
Giv?n, T. (2001). Syntax. An Introduction. Amsterdam: 
John Benjamins.  
Kleinberg, J.M. (1998). Authoritative sources in a hy-
perlinked environment. In Proceedings 9th ACM-
SIAM Symposium on Discrete Algorithms (pp. 668-
677).  
Mihalcea, R. and Tarau, P. (2004). TextRank : Bringing 
order into texts. In Proceedings of EMNLP (pp. 404-
411).  
Soergel, D. (1994). Indexing and retrieval performance: 
The logical evidence. Journal of the American Soci-
ety for Information Science, 45 (8): 589-599.    
Van Dijk, T. A. and Kintsch, W. (1983). Strategies of 
Discourse Comprehension. New York: Academic 
Press. 
Zha, H. (2002). Generic summarization and keyphrase 
extraction using mutual reinforcement principle and 
sentence clustering. In Proceedings of the 25th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval 
(pp. 113-120). New York : ACM. 
28
Improving Name Discrimination: A Language Salad Approach
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
Roxana Angheluta
Attentio SA
B-1030 Brussels, Belgium
roxana@attentio.com
Zornitsa Kozareva
Dept. de Lenguajes y Sistemas Informa?ticos
University of Alicante
03080 Alicante, Spain
zkozareva@dlsi.ua.es
Thamar Solorio
Department of Computer Science
University of Texas at El Paso
El Paso, TX 79902 USA
tsolorio@utep.edu
Abstract
This paper describes a method of discrim-
inating ambiguous names that relies upon
features found in corpora of a more abun-
dant language. In particular, we discrim-
inate ambiguous names in Bulgarian, Ro-
manian, and Spanish corpora using infor-
mation derived from much larger quan-
tities of English data. We also mix to-
gether occurrences of the ambiguous name
found in English with the occurrences of
the name in the language in which we are
trying to discriminate. We refer to this as
a language salad, and find that it often re-
sults in even better performance than when
only using English or the language itself
as the source of information for discrimi-
nation.
1 Introduction
Name ambiguity is a problem that is increasing
in complexity and scope as online information
sources grow and expand their coverage. Like
words, names are often ambiguous and can refer
to multiple underlying entities or concepts. Web
searches for names can often return results asso-
ciated with multiple people or organizations in a
disorganized and unclear fashion. For example,
the top 10 results of a Google search for George
Miller includes a mixture of entries for two dif-
ferent entities, one a psychology professor from
Princeton University and the other the director of
the film Mad Max.1
Name discrimination takes some number of
contexts that include an ambiguous name, and di-
vides them into groups or clusters, where the con-
1Search conducted January 4, 2006.
texts in each cluster should ideally refer to the
same underlying entity (and each cluster should
refer to a different entity). Thus, if we are given
10,000 contexts that include the name John Smith,
we would want to divide those contexts into clus-
ters corresponding to each of the different under-
lying entities that share that name.
We have developed an unsupervised method of
name discrimination (Pedersen et al, 2005). We
have shown the method to be language indepen-
dent (Pedersen et al, 2006), which is to say we
can apply it to English contexts as easily as we
can apply it to Romanian or French. However,
we have observed that there are situations where
the number of contexts in which an ambiguous
name occurs is relatively small, perhaps because
the name itself is unusual, or because the quantity
of data available for language is limited in general.
These problems of scarcity can make it difficult to
apply these methods and discriminate ambiguous
names, especially in languages with fewer online
resources.
This paper presents a method of name discrim-
ination is based on using a larger number of con-
texts in English that include an ambiguous name,
and applying information derived from these con-
texts to the discrimination of that name in another
language, where there are many fewer contexts.
We also show that mixing English contexts with
the contexts to be discriminated can result in a
performance improvement over only using the En-
glish or the original contexts alone.
2 Discrimination by Clustering Contexts
Our method of name discrimination is described in
more detail in (Pedersen et al, 2005), but in gen-
eral is based on an unsupervised approach to word
sense discrimination introduced by (Purandare and
25
Pedersen, 2004), which builds upon earlier work
in word sense discrimination, including (Schu?tze,
1998) and (Pedersen and Bruce, 1997).
Our method treats each occurrence of an am-
biguous name as a context that is to be clustered
with other contexts that also include the same
name. In this paper, each context consists of about
50 words, where the ambiguous name is generally
in the middle of the context. The goal is to cluster
similar contexts together, based on the presump-
tion that the occurrences of a name that appear
in similar contexts will refer to the same underly-
ing entity. This approach is motivated by both the
distributional hypothesis (Harris, 1968) and the
strong contextual hypothesis (Miller and Charles,
1991).
2.1 Feature Selection
The contexts to be clustered are represented by
lexical features which may be selected from either
the contexts being clustered, or from a separate
corpus. In this paper we use both approaches. We
cluster the contexts based on features identified in
those very same contexts, and we also cluster the
contexts based on features identified in a separate
set of data (in this case English). We explore the
use of a mixed feature selection strategy where we
identify features both from the data to be clustered
and the separate corpus of English text. Thus, our
feature selection data may come from one of three
sources: the contexts to be clustered (which we
will refer to as the evaluation contexts), English
contexts which include the same name but are not
to be clustered, and the combination of these two
(our so-called Language Salad or Mix).
The lexical features we employ are bigrams,
that is consecutive words that occur together in the
corpora from which we are identifying features. In
this work we identify bigram features using Point-
wise Mutual Information (PMI). This is defined as
the log of the ratio of the observed frequency with
which the two words occur together in the feature
selection data, to the expected number of times
the two words would occur together in a corpus if
they were independent. This expected value is es-
timated simply by taking the product of the num-
ber of times the two words occur individually, and
dividing this by the total number of bigrams in the
feature selection data. Thus, larger values of PMI
indicate that the observed frequency of the bigram
is greater than would be expected if the two words
were independent.
In these experiments we take the top 500 ranked
bigrams that occur five or more times in the feature
selection data. We also exclude any bigram from
consideration that is made up of one or two stop
words, which are high frequency function words
that have been specified in a manually created list.
Note that with smaller numbers of contexts (usu-
ally 200 or fewer), we lower the frequency thresh-
old to two or more.
In general PMI is known to have a bias towards
pairs of words (bigrams) that occur a small num-
ber of times and only with each other. In this work
that is a desirable quality, since that will tend to
identify pairs of words that are very strongly as-
sociated with each other and also provide unique
discriminating information.
2.2 Context Representation
Once the bigram features have been identified,
then the contexts to be clustered are represented
using second order co-occurrences that are de-
rived from those bigrams. In general a second
order co-occurrence is a pair of words that may
not occur with each other, but that both occur fre-
quently with a third word. For example, garden
and fire may not occur together often, but both
commonly occur with hose. Thus, garden hose
and fire hose represent first order co?occurrences,
and garden and fire represent a second order co?
occurrence.
The process of creating the second order repre-
sentation has several steps. First, the bigram fea-
tures identified by PMI (the top ranked 500 bi-
grams that have occurred 5 or more times in the
feature selection data) are used to create a word
by word co?occurrence matrix. The first word in
each bigram represents a row in the matrix, and the
second word in each bigram represents a column.
The cells in the matrix contain the PMI scores.
Note that this matrix is not symmetric, and that
there are many words that only occur in either a
row or a column (and not both) because they tend
to occur as the first or second word in a bigram.
For example, President might tend to be a first
word in a bigram (e.g., President Clinton, Presi-
dent Putin), whereas last names will tend to be the
second word.
Once the co?occurrence matrix is created, then
the contexts to be clustered can be represented.
Each word in the context is checked to see if it
26
has a corresponding row (i.e., vector) in the co?
occurrence matrix. If it does, that word is replaced
in the context by the row from the matrix, so that
the word in the context is now represented by the
vector of words with which it occurred in the fea-
ture selection data. If a word does not have a corre-
sponding entry in the co?occurrence matrix, then
it is simply removed from the context. After all
the words in the context are checked, then all of
the vectors that are selected are averaged together
to create a vector representation of the context.
Then these contexts are clustered into a pre?
specified number of clusters using the k?means
algorithm. Note that we are currently develop-
ing methods to automatically select the number of
clusters in the data (e.g., (Pedersen and Kulkarni,
2006)), although we have not yet applied them to
this particular work.
3 The Language Salad
In this paper, we explore the creation of a second
order representation for a set of evaluation con-
texts using three different sets of feature selection
data. The co?occurrence matrix may be derived
from the evaluation contexts themselves, or from
a separate set of contexts in a different language,
or from the combination of these two (the Salad or
Mix).
For example, suppose we have 100 Romanian
evaluation contexts that include an ambiguous
name, and that same name also occurs 10,000
times in an English language corpus.2 Our goal
is to cluster the 100 Romanian contexts, which
contain all the information that we have about the
name in Romanian. While we could derive a sec-
ond order representation of the contexts, the re-
sulting co?occurrence matrix would likely be very
small and sparse, and insufficient for making good
discrimination decisions. We could instead rely
on first order features, that is look for frequent
words or bigrams that occur in the evaluation con-
texts, and try and find evaluation contexts that
share some of the same words or phrases, and clus-
ter them based on this type of information. How-
ever, again, the small number of contexts available
would likely result in very sparse representations
for the contexts, and unreliable clustering results.
Thus, our method is to derive a co?occurrence
matrix from a language for which we have many
2We assume that the names either have the same spelling
in both languages, or that translations are readily available.
occurrences of the ambiguous name, and then use
that co?occurrence matrix to represent the evalua-
tion contexts. This relies on the fact that the eval-
uation contexts will contain at least a few names
or words that are also used in the larger corpus (in
this case English). In general, we have found that
while this is not always true, it is often the case.
We have also experimented with combining the
English contexts with the evaluation contexts, and
building a co?occurrence matrix based on this
combined or mixed collection of contexts. This
is the language salad that we refer to, a mixture of
contexts in two different languages that are used to
derive a representation of the evaluation contexts.
4 Experimental Data
We use data in four languages in these experi-
ments, Bulgarian, English, Romanian, and Span-
ish.
4.1 Raw Corpora
The Romanian data comes from the 2004 archives
of the newspaper Adevarul (The Truth)3. This is a
daily newspaper that is among the most popular in
Romania. While Romanian normally has diacrit-
ical markings, this particular newspaper does not
include those in their online edition, so the alpha-
bet used was the same as English.
The Bulgarian data is from the Sega 2002 news
corpus, which was originally prepared for the
CLEF competition.4 This is a corpus of news arti-
cles from the Newspaper Sega5, which is based in
Sofia, Bulgaria. The Bulgarian text was translit-
erated (phonetically) from Cyrillic to the Roman
alphabet. Thus, the alphabet used was the same
as English, although the phonetic transliteration
leads to fewer cognates and borrowed English
words that are spelled exactly the same as in En-
glish text.
The Spanish corpora comes from the Spanish
news agency EFE from the year 1994 and 1995.
This collection was used in the Question Answer-
ing Track at CLEF-2003, and also for CLEF-2005.
This text is represented in Latin-1, and includes
the usual accents that appear in Spanish.
The English data comes from the GigaWord
corpus (2nd edition) that is distributed by the Lin-
guistic Data Consortium. This consists of more
3http://www.adevarulonline.ro/arhiva
4http://www.clef-campaign.org
5http://www.segabg.com
27
than 2 billion words of newspaper text that comes
from five different news sources between the years
1994 and 2004. In fact, we subdivide the English
data into three different corpora, where one is from
2004, another from 2002, and the third from 1994-
95, so that for each of the evaluation languages
(Bulgarian, Spanish, and Romanian) we have an
English corpus from the same time period.
4.2 Evaluation Contexts
Our experimental data consists of evaluation con-
texts derived from the Bulgarian, Romanian, and
Spanish corpora mentioned above. We also have
English corpora that includes the same ambiguous
names as found in the evaluation contexts.
In order to quickly generate a large volume of
experimental data, we created evaluation contexts
from the corpora for each of our four languages
by conflating together pairs of well known names
or places, and that are generally not highly am-
biguous (although some might be rather general).
For example, one of the pairs of names we con-
flate is George Bush and Tony Blair. To do that,
every occurrence of both of these names is con-
verted to an ambiguous form (GB TB, for exam-
ple), and the discrimination task is to cluster these
contexts such that their original and correct name
is re?discovered. We retain a record of the orig-
inal name for each occurrence, so as to evaluate
the results of our method. Of course we do not use
this information anywhere in the process outside
of evaluation.
The following pairs of names were conflated in
all four of the languages: George Bush-Tony Blair,
Mexico-India, USA-Paris, Ronaldo-David Beck-
ham (2002 and 2004), Diego Maradona-Roberto
Baggio (1994-95 only), and NATO-USA. Note
that some of these names have different spellings
in some of our languages, so we look for and con-
flate the native spelling of the names in the differ-
ent language corpora. These pairs were selected
because they occur in all four of our languages,
and they represent name distinctions that are com-
monly of interest, that is they represent ambiguity
in names of people and places. With these pairs
we are also following (Nakov and Hearst, 2003)
who suggest that if one is introducing ambiguity
by creating pseudo?words or conflating names,
then these words should be related in some way
(in order to avoid the creation of very sharp or ob-
vious sense distinctions).
4.3 Discussion
For each of the three evaluation languages (Bul-
garian, Romanian, and Spanish) we have contexts
for five different name conflate pairs that we wish
to discriminate. We have corresponding English
contexts for each evaluation language, where the
dates of both are approximately the same. This
temporal consistency between the evaluation lan-
guage and English is important because the con-
texts in which a name is used may change over
time. In 1994, for example, Tony Blair was not
yet Prime Minister of England (he became PM in
1997), and references to George Bush most likely
refer to the US President who served from 1988
until 1992, rather than the current US President
(who began his term in office in 2001). In 1994
the current (as of 2006) US President had just been
elected governor of Texas, and was not yet a na-
tional figure. This points out that George Bush is
an example of an ambiguous name, but our ob-
servation has been that in the 2002 and 2004 data
(Romanian and Bulgarian) nearly all occurrences
are associated with the current president, and that
most of the occurrences in 1994-95 (Spanish) re-
fer to the former US President. This illustrates
an important point: it is necessary to consider the
perspective represented by the different corpora.
There is little reason to expect that news articles
from Spain in 1994 and 1995 would focus much
attention on the newly elected governor of Texas
in the United States.
Tables 1, 2, and 3 show the number of contexts
that have been collected for each name conflate
pair. For example, in Table 1 we see that there are
746 Bulgarian contexts that refer to either Mex-
ico or India, and that of these 51.47% truly re-
fer to Mexico, and 48.53% to India. There are
149,432 English contexts that mention Mexico or
India, and the Mix value shown is simply the sum
of the number of Bulgarian and English contexts.
In general these tables show that the English
contexts are much larger in number, however,
there are a few exceptions with the Spanish data.
This is because the EFE corpus is relatively large
as compared to the Bulgarian and Romanian cor-
pora, and provides frequency counts that are in
some cases comparable to those in the English cor-
pus.
28
5 Experimental Methodology
For each of the three evaluation languages (Bul-
garian, Romanian, Spanish) there are five name
conflate pairs. The same name conflate pairs
are used for all three languages, except for
Diego Maradona-Roberto Baggio which is only
used with Spanish, and Ronaldo-David Beckham,
which is only used with Bulgarian and Romanian.
This is due to the fact that in 1994-95 (the era
of the Spanish data) neither Ronaldo nor David
Beckham were as famous as they later became, so
they were mentioned somewhat less often than in
the 2002 and 2004 corpora. The other four name
conflate pairs are used in all of the languages.
For each name conflate pair we create a second
order representation using three different sources
of features selection data: the evaluation contexts
themselves, the corresponding English contexts,
and then the mix of the evaluation contexts and the
English contexts (the Mix). The objective of these
experiments is to determine which of these sources
of feature selection data results in the highest F-
Measure, which is the harmonic mean of the pre-
cision and recall of an experiment.
The precision of each experiment is the num-
ber of evaluation contexts clustered correctly, di-
vided by the number of contexts that are clustered.
The clustering algorithm may choose not to assign
every context to a cluster, which is why that de-
nominator may not be the same as the number of
evaluation contexts. The recall of each experiment
is the the number of correctly clustered evaluation
contexts divided by the total number of evaluation
contexts. Note that for each of the three variations
for each name conflate pair experiment exactly the
same evaluation language contexts are being dis-
criminated, all that is changing in each experiment
is the source of the feature selection data. Thus the
F-measures for a name conflate pair in a particular
language can be compared directly. Note however
that the F-measures across languages are harder to
compare directly, since different evaluation con-
texts are used, and different English contexts are
used as well.
There is a simple baseline that can be used as a
point of comparison, and that is to place all of the
contexts for each name conflate pair into one clus-
ter, and say that there is no ambiguity. If that is
done, then the resulting F-Measure will be equal
to the majority percentage of the true underlying
entity as shown in Tables 1, 2, and 3. For exam-
ple, for Bulgarian, if the 746 Bulgarian contexts
for Mexico and India are all put into the same clus-
ter, the resulting F-Measure would be 51.47%, be-
cause we would simply assign all the contexts in
the cluster to the more common of the two entities,
which is Mexico in this case.
6 Experimental Results
Tables 1, 2, and 3 show the results for our exper-
iments, language by language. Each table shows
the results for the 15 experiments done for each
language: five name conflate pairs, each with
three different sources of feature selection data.
The row labeled with the name of the evalua-
tion language reports the F-Measure for the eval-
uation contexts (whose number of occurrences is
shown in the far right column) when the fea-
ture selection data is the evaluation contexts them-
selves. The rows labeled English and Mix report
the F-Measures obtained for the evaluation con-
texts when the feature selection data is the English
contexts, or the Mix of the English and evaluation
contexts.
6.1 Bulgarian Results
The Bulgarian results are shown in Table 1. Note
that the number of contexts for English is consid-
erably larger than for Bulgarian for all five name
conflate pairs. The Bulgarian and English data
came from 2002 news reports.
The Mix of feature selection data results in the
best performance for three of the five name con-
flate pairs: George Bush - Tony Blair, Ronaldo -
David Beckham, and NATO - USA. For remain-
ing two name conflate pairs, just using the Bul-
garian evaluation contexts results in the highest F-
Measure (Mexico-India, USA-Paris).
We believe that this may be partially due to the
fact that the two cases where Bulgarian leads to the
best results are for very general or generic underly-
ing entities: Mexico and India, and then the USA
and Paris. In both cases, contexts that mention
these entities could be discussing a wide range of
topics, and the larger volumes of English data may
simply overwhelm the process with a huge num-
ber of second order features. In addition, it may
be that the English and Bulgarian corpora contain
different content that reflects the different interests
of the original readership of this text. For example,
news that is reported about India might be rather
different in the United States (the source of most
29
Table 1: Bulgarian Results (2002): Feature Selec-
tion Data, F-Measure, and Number of Contexts
George Bush (73.43) - Tony Blair (26.57)
Mix 68.37 11,570
Bulgarian 55.78 651
English 36.15 10,919
Mexico (51.47) - India (48.53)
Bulgarian 70.97 746
Mix 55.01 150,178
English 48.15 149,432
USA (79.53) - Paris (20.47)
Bulgarian 58.67 3,283
Mix 51.68 56,044
English 49.66 52,761
Ronaldo (61.25) - David Beckham (38.75)
Mix 64.88 8,649
Bulgarian 52.75 320
English 48.11 8,329
NATO (87.37) - USA (12.63)
Mix 75.44 54,193
Bulgarian 65.92 3,770
English 60.44 50,423
of the English data) than in Bulgaria. Thus, the
use of the English corpora might not have been
as helpful in those cases where the names to be
discriminated are more global figures. For exam-
ple, Tony Blair and George Bush are probably in
the news in the USA and Bulgaria for many of the
same reasons, thus the underlying content is more
comparable than that of the more general entities
(like Mexico and India) that might have much dif-
ferent content associated with them.
We observed that Bulgarian tends to have fewer
cognates or shared names with English than do
Romanian and English. This is due to the fact
that the Bulgarian text is transliterated. This may
account for the fact that the English-only results
for Bulgarian are very poor, and it is only in com-
bination with the Bulgarian contexts that the En-
glish contexts show any positive effect. This sug-
gests that there are only a few words in the Bulgar-
ian contexts that also occur in English, but those
that do have a positive impact on clustering per-
formance.
6.2 Romanian Results
The Romanian results are shown in Table 2. The
Romanian and English contexts come from 2004.
Table 2: Romanian Results (2004): Feature Selec-
tion Data, F-Measure, and Number of Contexts
Tony Blair (72.00) - George Bush (28.00)
English 64.23 11,616
Mix 54.31 11,816
Romanian 50.75 200
India (53.66) - Mexico (46.34)
Romanian 50.93 82
English 47.30 88,247
Mix 42.55 88,329
USA (60.29) - Paris (39.71)
English 59.05 45,346
Romanian 58.76 700
Mix 57.91 46,046
David Beckham (55.56) - Ronaldo (44.44)
Mix 81.00 4,365
English 70.85 4,203
Romanian 52.47 162
NATO (58.05) - USA (41.95)
Mix 60.48 43,508
Romanian 51.20 1,168
English 38.91 42,340
The Mix of Romanian and English contexts for
feature selection results in improvements for two
of the five pairs (David Beckham - Ronaldo, and
NATO - USA). The use of English contexts only
provides the best results for two other pairs (Tony
Blair - George Bush, and USA - Paris, although in
the latter case the difference in the F-Measures that
result from the three sources of data is minimal).
There is one case (Mexico-India) where using the
Romanian contexts as feature selection data re-
sults in a slightly better F-measure than when us-
ing English contexts.
The improvement that the Mix shows for David
Beckham-Ronaldo is significant, and is perhaps
due to fact that in both English and Romanian text,
the content about Beckham and Ronaldo is simi-
lar, making it more likely that the mix of English
and Romanian contexts will be helpful. However,
it is also true that the Mix results in a significant
improvement for NATO-USA, and it seems likely
that the local perspective in Romania and the USA
would be somewhat different on these two entities.
However, NATO-USA has a relatively large num-
ber of contexts in Romanian as well as English, so
perhaps the difference in perspective had less of
an impact in those cases where the number of Ro-
30
Table 3: Spanish Results (1994-95): Feature Se-
lection Data, F-Measure, and Number of Contexts
George Bush (75.58) - Tony Blair (24.42)
Mix 78.59 2,353
Spanish 64.45 1,163
English 54.29 1,190
D. Maradona (51.55) - R. Baggio (48.45)
English 67.65 1,588
Mix 61.35 3,594
Spanish 60.70 2,006
India (92.34) - Mexico (7.66)
English 72.76 19,540
Spanish 66.57 2,377
Mix 61.54 21,917
USA (62.30) - Paris (37.70)
Spanish 69.31 1,000
English 64.30 17,344
Mix 59.40 18,344
NATO (63.86) - USA (36.14)
Spanish 62.04 2,172
Mix 58.47 27,426
English 56.00 25,254
manian contexts is much smaller (as is the case for
Beckham and Ronaldo).
6.3 Spanish Results
The Spanish results are shown in Table 3. The
Spanish and English contexts come from 1994-
1995, which puts them in a slightly different his-
torical era than the Bulgarian and Romanian cor-
pora.
Due to this temporal difference, we used Diego
Maradona and Roberto Baggio as a conflated pair,
rather than David Beckham and Ronaldo, who
were much younger and somewhat less famous at
that time. Also, Ronaldo is a highly ambiguous
name in Spanish, as it is a very common first name.
This is true in English text as well, although casual
inspection of the English text from 2002 and 2004
(where the Ronaldo-Beckham pair was included
experimentally) reveals that Ronaldo the soccer
player tends to occur more so than any other single
entity named Ronaldo, so while there is a bit more
noise for Ronaldo, there is not really a significant
ambiguity.
For the Spanish results we only note one pair
(George Bush - Tony Blair) where the Mix of En-
glish and Spanish results in the best performance.
This again suggests that the perspective of the
Spanish and English corpora were similar with re-
spect to these entities, and their combination was
helpful. In two other cases (Maradona-Baggio,
India-Mexico) English only contexts achieve the
highest F-Measure, and then in the two remaining
cases (USA-Paris, NATO-USA) the Spanish con-
texts are the best source of features.
Note that for Spanish we have reasonably large
numbers of contexts (as compared to Bulgarian
and Romanian). Given that, it is especially inter-
esting that English-only contexts are the most ef-
fective in two of five cases. This suggests that this
approach may have merit even when the evalua-
tion language does not suffer from problems of ex-
treme scarcity. It may simply be that the informa-
tion in the English corpora provides more discrim-
inating information than does the Spanish, and that
it is somewhat different in content than the Span-
ish, otherwise we would expect the Mix of English
and Spanish contexts to do better than being most
accurate for just one of five cases.
7 Discussion
Of the 15 name conflate experiments (five pairs,
three languages), in only five cases did the use of
the evaluation contexts as a source of feature se-
lection data result in better F-Measure scores than
did either using the English contexts alone or as a
Mix with the evaluation language contexts. Thus,
we conclude that there is a clear benefit to using
feature selection data that comes from a different
language than the one for which discrimination is
being performed.
We believe that this is due to the volume of
the English data, as well as to the nature of the
name discrimination task. For example, a per-
son is often best described or identified by observ-
ing the people he or she tends to associate with,
or the places he or she visits, or the companies
with which he or she does business. If we ob-
serve that George Miller and Mel Gibson occur
together, then it seems we can safely infer that
George Miller the movie director is being referred
to, rather than George Miller the psychologist and
father of WordNet.
This argument might suggest that first order
co?occurrences would be sufficient to discrimi-
nate among the names. That is, simply group the
evaluation contexts based on the features that oc-
cur within them, and essentially cluster evaluation
31
contexts based on the number of features they have
in common with other evaluation contexts. In fact,
results on word sense discrimination (Purandare
and Pedersen, 2004) suggest that first order rep-
resentations are more effective with larger number
of context than second order methods. However,
we see examples in these results that suggests this
may not always be the case. In the Bulgarian re-
sults, the largest number of Bulgarian contexts are
for NATO-USA, but the Mix performs quite a bit
better than Bulgarian only. In the case of Roma-
nian, again NATO-USA has the largest number of
contexts, but the Mix still does better than Roma-
nian only. And in Spanish, Mexico-India has the
largest number of contexts and English-only does
better. Thus, even in cases where we have an abun-
dant number of evaluation contexts, the indirect
nature of the second order representation provides
some added benefit.
We believe that the perspective of the news or-
ganizations providing the corpora certainly has an
impact on the results. For example, in Romanian,
the news about David Beckham and Ronaldo is
probably much the same as in the United States.
These are international figures that are both ex-
ternal to countries where the news originates, and
there is no reason to suppose there would be a
unique local perspective represented by any of the
news sources. The only difference among them
might be in the number of contexts available. In
this situation, the addition of the English contexts
may provide enough additional information to im-
prove discrimination performance in another lan-
guage.
For example, in the 162 Romanian contexts
for Ronaldo-Beckham, there is one occurrence of
Posh, which was the stage name of Beckham?s
wife Victoria. This is below our frequency cut-
off threshold for feature selection, so it would be
discarded when using Romanian?only contexts.
However, in the English contexts Posh is men-
tioned 6 times, and is included as a feature. Thus,
the one occurrence of Posh in the Romanian cor-
pus can be well represented by information found
in the English contexts, thus allowing that Roma-
nian context to be correctly discriminated.
8 Conclusions
This paper shows that a method of name discrim-
ination based on second order context representa-
tions can take advantage of English contexts, and
the mix of English and evaluation contexts, in or-
der to perform more accurate name discrimination.
9 Acknowledgments
This research is supported by a National Sci-
ence Foundation Faculty Early CAREER Devel-
opment Award (#0092784). All of the experiments
in this paper were carried out with version 0.71
SenseClusters package, which is freely available
from http://senseclusters.sourceforge.net.
References
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
P. Nakov and M. Hearst. 2003. Category-based pseu-
dowords. In Companion Volume to the Proceedings
of HLT-NAACL 2003 - Short Papers, pages 67?69,
Edmonton, Alberta, Canada, May 27 - June 1.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 197?207, Providence,
RI, August.
T. Pedersen and A. Kulkarni. 2006. Selecting the
r?ightn?umber of senses based on clustering criterion
functions. In Proceedings of the Posters and Demo
Program of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Trento, Italy, April.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005.
Name discrimination by clustering similar contexts.
In Proceedings of the Sixth International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 220?231, Mexico City, February.
T. Pedersen, A. Kulkarni, R. Angheluta, Z. Kozareva,
and T. Solorio. 2006. An unsupervised language in-
dependent method of name discrimination using sec-
ond order co-occurrence features. In Proceedings
of the Seventh International Conference on Intelli-
gent Text Processing and Computational Linguistics,
pages 208?222, Mexico City, February.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41?48, Boston, MA.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
32
