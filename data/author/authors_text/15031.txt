Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 345?350,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JU_CSE_TEMP: A First Step towards Evaluating Events, Time Ex-
pressions and Temporal Relations 
Anup Kumar Kolya1, Asif Ekbal2 and Sivaji Bandyopadhyay3 
 
1,3Department of Computer Science and Engineering, Jadavpur University,  
Kolkata-700032, India 
2Department of Computational Linguistics, Heidelberg University,  
Heidelberg-69120, Germany 
Email: anup.kolya@gmail.com1, asif.ekbal@gmail.com2  
and sivaji_cse_ju@yahoo.com3  
 
Abstract 
Temporal information extraction is a 
popular and interesting research field in 
the area of Natural Language Processing 
(NLP). In this paper, we report our works 
on TempEval-2 shared task. This is our 
first participation and we participated in 
all the tasks, i.e., A, B, C, D, E and F. We 
develop rule-based systems for Tasks A 
and B, whereas the remaining tasks are 
based on a machine learning approach, 
namely Conditional Random Field 
(CRF). All our systems are still in their 
development stages, and we report the 
very initial results. Evaluation results on 
the shared task English datasets yield the 
precision, recall and F-measure values of 
55%, 17% and 26%, respectively for 
Task A and 48%, 56% and 52%, respec-
tively for Task B (event recognition).  
The rest of tasks, namely C, D, E and F 
were evaluated with a relatively simpler 
metric: the number of correct answers di-
vided by the number of answers. Experi-
ments on the English datasets yield the 
accuracies of 63%, 80%, 56% and 56% 
for tasks C, D, E and F, respectively.        
1 Introduction 
Temporal information extraction is, nowadays, a 
popular and interesting research area of Natural 
Language Processing (NLP). Generally, events 
are described in different newspaper texts, sto-
ries and other important documents where 
events happen in time and the temporal location 
and ordering of these events are specified. One 
of the important tasks of text analysis clearly re-
quires identifying events described in a text and 
locating these in time. This is also important in a 
wide range of NLP applications that include 
temporal question answering, machine transla-
tion and document summarization.  
   In the literature, temporal relation identifica-
tion based on machine learning approaches can 
be found in Boguraev et el. (2005), Mani et al 
(2006), Chambers et al (2007) and some of the 
TempEval 2007 participants (Verhagen et al, 
2007). Most of these works tried to improve 
classification accuracies through feature engi-
neering. The performance of any machine learn-
ing based system is often limited by the amount 
of available training data. Mani et al (2006) in-
troduced a temporal reasoning component that 
greatly expands the available training data. The 
training set was increased by a factor of 10 by 
computing the closure of the various temporal 
relations that exist in the training data. They re-
ported significant improvement of the classifica-
tion accuracies on event-event and event-time 
relations. Their experimental result showed the 
accuracies of 62.5%-94.95% and 73.68%-
90.16% for event-event and event-time relations, 
respectively. However, this has two shortcom-
ings, namely feature vector duplication caused 
by the data normalization process and the unreal-
istic evaluation scheme.  The solutions to these 
issues are briefly described in Mani et al (2007).  
In TempEval 2007 task, a common standard da-
taset was introduced that involves three temporal 
relations. The participants reported F-measure 
scores for event-event relations ranging from 
42% to 55% and for event-time relations from 
73% to 80%. Unlike (Mani et al, 2007; 2006), 
event-event temporal relations were not dis-
course-wide (i.e., any pair of events can be tem-
porally linked) in TempEval 2007. Here, the 
event-event relations were restricted to events 
within two consecutive sentences. Thus, these 
two frameworks produced highly dissimilar re-
345
sults for solving the problem of temporal relation 
classification.  
   In order to apply various machine learning al-
gorithms, most of the authors formulated tempo-
ral relation as an event paired with a time or an-
other event and translated these into a set of fea-
ture values. Some of the popularly used machine 
learning techniques were Naive-Bayes, Decision 
Tree (C5.0), Maximum Entropy (ME) and Sup-
port Vector Machine (SVM). Machine learning 
techniques alone cannot always yield good accu-
racies. To achieve reasonable accuracy, some 
researchers (Mao et al, 2006) used hybrid ap-
proach. The basic principle of hybrid approach is 
to combine the rule-based component with ma-
chine learning.  It has been shown in (Mao et al, 
2006) that classifiers make most mistakes near 
the decision plane in feature space. The authors 
carried out a series of experiments for each of the 
three tasks on four models, namely naive-Bayes, 
decision tree (C5.0), maximum entropy and sup-
port vector machine. The system was designed in 
such a way that they can take the advantage of 
rule-based as well as machine learning during 
final decision making. But, they did not explain 
exactly in what situations machine learning or 
rule based system should be used given a particu-
lar instance. They had the option to call either 
component on the fly in different situations so 
that they can take advantage of the two empirical 
approaches in an integrated way. 
The rest of the paper is structured as follows. 
We present very brief descriptions of the differ-
ent tasks in Section 2. Section 3 describes our 
approach in details with rule-based techniques 
for tasks A and B in Subsection 3.1, CRF based 
techniques in Subsection 3.2 for tasks C, D, E 
and F, and features in Subsection 3.3. Detailed 
evaluation results are reported in Section 4. Fi-
nally, Section 5 concludes the paper with a direc-
tion to future works.  
2 Task Description 
The main research in this area involves identifi-
cation of all temporal referring expressions, 
events and temporal relations within a text. The 
main challenges involved in this task were first 
addressed during TempEval-1 in 2007 (Verhagen 
et al, 2007). This was an initial evaluation exer-
cise based on three limited tasks that were con-
sidered realistic both from the perspective of as-
sembling resources for development and testing 
and from the perspective of developing systems 
capable of addressing the tasks. In TempEval 
2007, following types of event-time temporal 
relations were considered: Task A (relation be-
tween the events and times within the same sen-
tence), Task B (relation between events and 
document creation time) and Task C (relation 
between verb events in adjacent sentences). The 
data sets were based on TimeBank, a hand-built 
gold standard of annotated texts using the Ti-
meML markup scheme1. The data sets included 
sentence boundaries, timex3 tags (including the 
special document creation time tag), and event 
tags. For tasks A and B, a restricted set of events 
was used, namely those events that occur more 
than 5 times in TimeBank. For all three tasks, the 
relation labels used were before, after, overlap, 
before-or-overlap, overlap-or-after and vague. 
Six teams participated in the TempEval tasks. 
Three of the teams used statistics exclusively, 
one used a rule-based system and the other two 
employed a hybrid approach. For task A, the 
range of F-measure scores were from 0.34 to 
0.62 for the strict scheme and from 0.41 to 0.63 
for the relaxed scheme. For task B, the scores 
were from 0.66 to 0.80 (strict) and 0.71 to 0.81 
(relaxed). Finally, task C scores range from 0.42 
to 0.55 (strict) and from 0.56 to 0.66 (relaxed). 
   In TempEval-2, the following six tasks were 
proposed:  
 A:  The main task was to determine the extent of 
the time expressions in a text as defined by the 
TimeML timex3 tag. In addition, values of the 
features type and val had to be determined. The 
possible values of type are time, date, duration, 
and set; the value of val is a normalized value as 
defined by the timex2 and timex3 standards. 
B. Task was to determine the extent of the events 
in a text as defined by the TimeML event tag. In 
addition, the values of the features tense, aspect, 
polarity, and modality had to be determined. 
C. Task was to determine the temporal relation 
between an event and a time expression in the 
same sentence. 
D. Temporal relation between an event and the 
document creation time had to be determined. 
E. Temporal relation between two main events in 
consecutive sentences had to be determined.  
F. Temporal relation between two events, where 
one event syntactically dominates the other 
event.  
     In our present work, use handcrafted rules for 
Task A and Task B. All the other tasks, i.e., C, 
D, E and F are developed based on the well 
known statistical algorithm, Conditional Random 
                                                 
1www.timeml.org for details on TimeML  
346
Field (CRF). For CRF, we use only those fea-
tures that are available in the training data. All 
the systems are evaluated on the TempEval-
2 shared task English datasets. Evaluation results 
yield the precision, recall and F-measure values 
of 55%, 17% and 26%, respectively for Task A 
and 48%, 56% and 52%, respectively for Task B. 
Experiments on the other tasks demonstrate the 
accuracies of 63%, 80%, 56% and 56% for C, D, 
E and F, respectively.   
3 Our Approach  
In this section, we present our systematic ap-
proach for evaluating events, time expressions 
and temporal relations as part of our first partici-
pation in the TempEval shared task. We partici-
pated in all the six tasks of TempEval-2. Rule-
based systems are developed using a preliminary 
handcrafted set of rules for tasks A and B. We 
use machine learning approach, namely CRF for 
solving the remaining tasks, i.e., C, D, E and F.  
 
3.1 Rules for Task A and Task B 
We manually identify a set of rules studying the 
various features available in the training data. 
There were some exceptions to these rules. How-
ever, a rule is used if it is found to be correct 
most of the time throughout the training data. It 
is to be noted that these are the very preliminary 
rules, and we are still working on finding out 
more robust rules. Below, we present the rules 
for tasks A and B.  
 
Task A. The time expression is identified by de-
fining appropriate regular expression. The regu-
lar expressions are based on several entities that 
denote month names, year, weekdays and the 
various digit expressions. We also use a list of 
keywords (e.g., day, time, AM, PM etc.) that de-
note the various time expressions. The values of 
various attributes (e.g., type and value) of time 
expressions are computed by some simple tem-
plate matching algorithms.  
 
Task B. In case of Task B, the training data is 
initially passed through the Stanford PoS tagger2. 
We consider the tokens as the events that are 
tagged with POS tags such as VB, VBG, VBN, 
VBP, VBZ and VBD, denoting the various verb 
expressions. Values of different attributes are 
computed as follows.  
                                                 
2 http://nlp.stanford.edu/software/tagger.shtml 
 
a. Tense: A manually augmented suffix list such 
as: "ed","d","t" etc. is used to capture the proper 
tense of any event verb from surface level ortho-
graphic variations. 
b. Aspect: The Tense-Aspect-Modality (TAM) 
for English verbs is generally associated with 
auxiliaries. A list is manually prepared. Any oc-
currence of main verb with continuous aspect 
leads to search for the adjacent previous auxil-
iary and rules are formulated to extract TAM 
relation using the manually generated checklist. 
A separate list of auxiliaries is prepared and suc-
cessfully used for detection of progressive verbs.  
c. Polarity: Verb-wise polarity is assigned by the 
occurrence of previous negation words. If any 
negation word appears before any event verb 
then the resultant polarity is negative; otherwise, 
the verb considered as positive by default. 
d. Modality: We prepare a manual list that con-
tains the words such as: may, could, would etc. 
The presence of these modal auxiliaries gives 
modal tag to the targeted verb in a sentence oth-
erwise it is considered a non-modal. 
e. Class: We select ?occurrence? to be class val-
ue by default.  
 
3.2 Machine Learning Approach for Tasks 
C, D, E and F 
 
For tasks C-F, we use a supervised machine 
learning approach that is based on CRF. We con-
sider the temporal relation identification task as a 
pair-wise classification problem in which the 
target pairs?a TIMEX3 tag and an EVENT?are 
modelled using CRF, which can include arbitrary 
set of features, and still can avoid overfitting in a 
principled manner.  
 
Introduction to CRF.  CRF (Lafferty et al, 
2001), is used to calculate the conditional prob-
ability of values on designated output nodes 
given values on other designated input nodes. 
The conditional probability of a state sequence 
1, 2, ..., TS s s s=<
1 2,O o
>  given an observation se-
quence , ....., )To o=<  is calculated as: 
1 ,
1 1
1
( | ) exp( ( , , ))
T K
k k t t
o t k
P s o f s s o t
Z
??
= =
= ?? ?
)
                                 
where, 1 ,( , ,k t tf s s o t?
k
is a feature function 
whose weight ? is to be learned via training. 
The values of the feature functions may range 
between .....? ? + ? , but typically they are 
347
binary. To make all conditional probabilities sum 
up to 1, we must calculate the normalization 
factor, 
0
1 1
exp( ( , , ))
T K
s k k t
t k
1 ,tZ f s s o t? ?
= =
= ? ? ? ,                                             
which, as in HMMs, can be obtained efficiently 
by dynamic programming. 
   To train a CRF, the objective function to be 
maximized is the penalized log-likelihood of the 
state sequences given the observation sequence: 
2
( ) ( )
2
1
log( ( | ))
2
N
i i k
i
L P s o
1
K
k
?
?? ?==? =??
>
,                                         
where, { } is the labeled training da-
ta. The second sum corresponds to a zero-mean, 
( ) ( ),i io s<
2? -variance Gaussian prior over parameters, 
which facilitates optimization by making the li-
kelihood surface strictly convex.  
  CRFs generally can use real-valued functions 
but it is often required to incorporate the binary 
valued features. A feature function 
1 ,( , ,k t t )f s s o t? has a value of 0 for most cases 
and is only set to  1, when 1,t ts s?  are certain 
states and the observation has certain properties. 
Here, we set parameters ?  to maximize the pe-
nalized log-likelihood using Limited-memory 
BFGS (Sha and Pereira, 2003) a quasi-Newton 
method that is significantly more efficient, and 
which results in only minor changes in accuracy 
due to changes in ? . 
   We use the OpenNLP C++ based CRF++ pack-
age 3 , a simple, customizable, and open source 
implementation of CRF for segmenting /labeling 
sequential data.  
 
3.3 Features of Tasks C, D, E and F 
 
We extract the gold-standard TimeBank features 
for events and times in order to train/test the 
CRF. In the present work, we mainly use the 
various combinations of the following features:  
 
(i). Part of Speech (POS) of event terms: It de-
notes the POS information of the event. The fea-
tures values may be either of ADJECTIVE, 
NOUN, VERB, and PREP. 
 (ii). Event Tense: This feature is useful to cap-
ture the standard distinctions among the gram-
matical categories of verbal phrases. The tense 
attribute can have values, PRESENT, PAST, 
                                                 
3http://crfpp.sourceforge.net  
FUTURE, INFINITIVE, PRESPART, PAST-
PART, or NONE. 
 (iii). Event Aspect: It denotes the aspect of the 
events. The aspect attribute may take values, 
PROGRESSIVE, PERFECTIVE and PERFEC-
TIVE PROGRESSIVE or NONE. 
(iv). Event Polarity: The polarity of an event 
instance is a required attribute represented by the 
boolean attribute, polarity. If it is set to ?NEG?, 
the event instance is negated.  If it is set to ?POS? 
or not present in the annotation, the event in-
stance is not negated. 
(v). Event Modality: The modality attribute is 
only present if there is a modal word that modi-
fies the instance. 
(vi). Event Class: This is denoted by the 
?EVENT? tag and used to annotate those ele-
ments in a text that mark the semantic events 
described by it. Typically, events are verbs but 
can be nominal also. It may belong to one of the 
following classes:  
 REPORTING: Describes the action of a person 
or an organization declaring something, narrating 
an event, informing about an event, etc.  For ex-
ample, say, report, tell, explain, state etc. 
 PERCEPTION: Includes events involving the 
physical perception of another event. Such 
events are typically expressed by verbs like: see, 
watch, glimpse, behold, view, hear, listen, over-
hear etc. 
ASPECTUAL: Focuses on different facets of 
event history. For example, initiation, reinitia-
tion, termination, culmination, continuation etc. 
 I_ACTION: An intentional action. It introduces 
an event argument which must be in the text ex-
plicitly describing an action or situation from 
which we can infer something given its relation 
with the I_ ACTION. 
I_STATE: Similar to the I_ACTION class. This 
class includes states that refer to alternative or 
possible words, which can be introduced by sub-
ordinated clauses, nominalizations, or untensed 
verb phrases (VPs). 
 STATE: Describes circumstances in which 
something obtains or holds true. 
 Occurrence: Includes all of the many other 
kinds of events that describe something that hap-
pens or occurs in the world. 
(vii). Type of temporal expression: It repre-
sents the temporal relationship holding between 
events, times, or between an event and a time of 
the event.  
(viii). Event Stem:  It denotes the stem of the 
head event.  
348
(ix). Document Creation Time: The document 
creation time of the event.  
4 Evaluation Results 
Each of the tasks is evaluated with the Tem-
pEval-2 shared task datasets. 
  
4.1 Evaluation Scheme 
 
For the extents of events and time expressions 
(tasks A and B), precision, recall and the F-
measure are used as evaluation metrics, using the 
following formulas: 
Precision (P) = tp/ (tp + fp) 
Recall (R) = tp/ (tp + fn) 
F-measure = 2 *(P * R)/ (P + R) 
   Where, tp is the number of tokens that are part 
of an extent in both keys and response,  
fp is the number of tokens that are part of an ex-
tent in the response but not in the key, and  
fn is the number of tokens that are part of an ex-
tent in the key but not in the response. 
  An even simpler evaluation metric similar to 
the definition of ?accuracy? is used to evaluate 
the attributes of events and time expressions (the 
second part of tasks, A and B) and for relation 
types (tasks C through F). The metric, henceforth 
referred to as ?accuracy?, is defined as below:  
    Number of correct answers/ Number of an-
swers present in the test data  
 
4.2 Results 
 
For tasks A and B, we identify a set of rules from 
the training set and apply them on the respective 
test sets.  
   The tasks C, D, E and F are based on CRF. We 
develop a number of models based on CRF using 
the different features included into it. A feature 
vector consisting of the subset of the available 
features as described in Section 2.3 is extracted 
for each of <event, timex>, <event, DCT>, 
<event, event> and <event, event> pairs in tasks 
C, D, E and F, respectively. Now, we have a 
training data in the form ( , , where,  is 
the ith pair along with its feature vector and  is 
it?s corresponding TempEval relation class. 
Models are built based on the training data and 
the feature template. The procedure of training is 
summarized below: 
)i iW T iW
iT
1. Define the training corpus, C. 
2. Extract the corresponding relation from 
the training corpus. 
3. Create a file of candidate features, in-
cluding lexical features derived from the 
training corpus. 
4. Define a feature template.  
5. Compute the CRF weights ?k for every fK 
using the CRF toolkit with the training 
file and feature template as input. 
  During evaluation, we consider the following 
feature templates for the respective tasks:  
 
(i) Task C: Feature vector consisting of current 
token, polarity, POS, tense, class and value; 
combination of token and type, combination of 
tense and value of the current token, combination 
of aspect and type of current token, combination 
of aspect, value and type of the current token.      
(ii) Task D: Feature vector consisting of current 
token and POS; combination of POS and tense of 
the current token, combination of polarity and 
POS of the current token, combination of POS 
and aspect of current token, combination of po-
larity and POS of current token, combination of 
POS, tense and aspect of the current token.      
(iii). Task E: Current token, combination of 
event-class and event-id of the current token, 
combination of POS tags of the pair of events, 
combination of (tense, aspect) values of the event 
pairs. 
(iv). Task F: Current token, combination of POS 
tags of the pair of events, combination of tense 
values of the event pairs, combination of the as-
pect values of the event pairs, combination of the 
event classes of the event pairs. 
  Experimental results of tasks A and B are re-
ported in Table 1 for English datasets. The re-
sults for task A, i.e., recognition and normaliza-
tion of time expressions, yield the precision, re-
call and F-measure values of 55%, 17% and 
26%, respectively. For task B, i.e., event recogni-
tion, the system yields precision, recall and F-
measure values of 48%, 56% and 52%, respec-
tively. Event attribute identification shows the 
accuracies of 98%, 98%, 30%, 95% and 53% for 
polarity, mood, modality, tense, aspect and class, 
respectively. These systems are the baseline 
models, and the performance can further be im-
proved with a more carefully handcrafted set of 
robust rules. In further experiments, we would 
also like to apply machine learning methods to 
these problems.  
 
 
 
349
Task  precision 
(in %)  
recall   
(in %) 
F-measure  
(in %) 
A 55% 17% 26% 
B 48% 56% 52% 
 
Table 1. Experimental results on tasks A and B 
 
  Evaluation results on the English datasets for 
tasks C, D, E and F are presented in Table 2. Ex-
periments show the accuracies of 63%, 80%, 
56% and 56% for tasks C, D, E and F, respec-
tively. Results show that our system performs 
best for task D, i.e., relationships between event 
and document creation time. The system 
achieves an accuracy of 63% for task C that finds 
the temporal relation between an event and a time 
expression in the same sentence. The system per-
forms quite similarly for tasks E and F. It is to be 
noted that there is still the room for performance 
improvement. In the present work, we did not 
carry out sufficient experiments to identify the 
most suitable feature templates for each of the 
tasks. In future, we would experiment after se-
lecting a development set for each task; and find 
out appropriate feature template depending upon 
the performance on the development set.  
 
 
Task  Accuracy (in %) 
C 63%  
D 80% 
E 56% 
F 56% 
 
Table 2. Experimental results on tasks C, D, E 
and F 
   
5 Conclusion and Future Works 
In this paper, we report very preliminary results 
of our first participation in the TempEval shared 
task. We participated in all the tasks of Tem-
pEval-2, i.e., A, B, C, D, E and F for English. 
We develop the rule-based systems for tasks A 
and B, whereas the remaining tasks are based on 
a machine learning approach, namely CRF. All 
our systems are still in their development stages. 
Evaluation results on the shared task English 
datasets yield the precision, recall and F-measure 
values of 55%, 17% and 26%, respectively for 
Task A and 48%, 56% and 52%, respectively for 
Task B (event recognition).  Experiments on the 
English datasets yield the accuracies of 63%, 
80%, 56% and 56% for tasks C, D, E and F, re-
spectively. 
  Future works include identification of more 
precise rules for tasks A and B. We would also 
like to experiment with CRF for these two tasks.  
We would experiment with the various feature 
templates for tasks C, D, E and F. Future works 
also include experimentations with other ma-
chine learning techniques like maximum entropy 
and support vector machine.          
References  
Boguraev, B. and R. K. Ando. 2005. TimeML 
Compliant Text Analysis for Temporal Rea-
soning. In Proceedings of Nineteenth Interna-
tional Joint Conference on Artificial Intelli-
gence (IJCAI-05), Edinburgh, Scotland, Au-
gust, pages 997?1003. 
Chambers, N., S., Wang, and D., Jurafsky. , 
2007. Classifying Temporal Relations between 
Events. In Proceedings of the ACL 2007 Demo 
and Poster Sessions, Prague, Czech Republic, 
June, pages 173?176. 
 Lafferty, J., McCallum, A., and Pereira, F. 
Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Se-
quence Data. In Proceedings of 18th Interna-
tional Conference on Machine Learning, 
2001. 
Mani, I., B., Wellner, M., Verhagen, and J. 
Pustejovsky. 2007. Three Approaches to 
Learning TLINKs in TimeML. Technical Re-
port CS-07-268, Computer Science Depart-
ment, Brandeis University, Waltham, USA. 
Mani, I., Wellner, B., Verhagen, M., Lee C.M.,   
Pustejovsky, J. 2006. Machine Learning of 
Temporal Relation. In Proceedings of the 
COLING/ACL, Sydney, Australia, ACL. 
Mao, T., Li., T., Huang, D., Yang, Y. 2006. Hy-
brid Models for Chinese Named Entity Rec-
ognition. In Proceedings of the Fifth SIGHAN 
Workshop on Chinese Language Processing. 
Sha, F., Pereira, F. 2003. Shallow  Parsing  with  
Conditional Random Fields. In Proceedings of  
HLT-NAACL, 2003. 
Verhagen, M., Gaizauskas, R., Schilder, F., Hep-
ple, M., Katz, G., Pustejovsky, and J.: SemE-
val-2007 Task 15: TempEval Temporal Rela-
tion Identification. 2007. In Proceedings of the 
SemEval-2007, Prague, June 2007, pages 75-
80. 
350
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 64?72, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
JU_CSE: A CRF Based Approach to Annotation of Temporal Expres-
sion, Event and Temporal Relations 
 
 
Anup Kumar Kolya1, Amitava Kundu1, 
 Rajdeep Gupta1  
Asif Ekbal2, Sivaji Bandyopadhyay1 
1Dept. of Computer Science & Engineering 2Dept. of Computer Science & Engineering 
Jadavpur Univeristy IIT Patna 
Kolkata-700 032, India Patna-800 013, India 
{anup.kolya,amitava.jucse, 
rajdeepgupta20}@gmail.com 
asif@iitp.ac.in, 
sivaji_ju_cse@yahoo.com 
 
 
 
 
 
Abstract 
In this paper, we present the JUCSE system, 
designed for the TempEval-3 shared task. The 
system extracts events and temporal infor-
mation from natural text in English. We have 
participated in all the tasks of TempEval-3, 
namely Task A, Task B & Task C. We have 
primarily utilized the Conditional Random 
Field (CRF) based machine learning tech-
nique, for all the above tasks. Our system 
seems to perform quite competitively in Task 
A and Task B. In Task C, the system?s per-
formance is comparatively modest at the ini-
tial stages of system development. We have 
incorporated various features based on differ-
ent lexical, syntactic and semantic infor-
mation, using Stanford CoreNLP and Wordnet 
based tools. 
1 Introduction 
Temporal information extraction has been a popu-
lar and interesting research area of Natural Lan-
guage Processing (NLP) for quite some time. 
Generally, a lot of events are described in a variety 
of newspaper texts, stories and other important 
documents where the different events described 
happen at different time instants. The temporal 
location and ordering of these events are either 
specified or implied. Automatic identification of 
time expressions and events and annotation of 
temporal relations constitute an important task in 
text analysis. These are also important in a wide 
range of NLP applications that include temporal 
question answering, machine translation and doc-
ument summarization.  
A lot of research in the area of temporal infor-
mation extraction has been conducted on multiple 
languages, including English and several European 
languages. The TimeML was first developed in 
2002 in an extended workshop called TERQAS 
(Time and Event Recognition for Question An-
swering Systems) and, in 2003, it was further de-
veloped in the context of the TANGO workshop 
(TimeML Annotation Graphical Organizer). Since 
then most of the works in this research arena have 
been conducted in English. The variety of works 
include TimeML (Pustejovsky et al, 2003), the 
development of a temporally annotated corpus 
Time-Bank (Pustejovsky et al, 2003), the temporal 
evaluation challenges TempEval-1 (Verhagen et 
al., 2007), TempEval-2 (Pustejovsky and Verha-
gen, 2010). In the series of Message Understanding 
Conferences (MUCs) that started from 1987 and 
the Sheffield Temporal Annotation scheme 
(STAG) (Setzer &Gaizauskas, 2000) the  aim  was 
to identify events in news text and determine their 
relationship with points on a temporal line. 
In the series of TempEval evaluation exercises, 
TempEval-1 was the first one where the focus was 
on identification of three types of temporal rela-
tion: relation between an event and a time expres-
sion in the same sentence, relation between an 
64
event and the document creation time, and relation 
between two main events in consecutive sentences. 
 TempEval-2 was a follow up to TempEval-1 
and consisted of six subtasks rather than three. It 
added (i) identification of time expressions and 
determination of values of the attributes TYPE and 
VAL (ii) identification of event expressions and 
determination of its attribute values. It included the 
previous three relation tasks from TempEval-1 and 
an additional task of annotating temporal relation 
between a pair of events where one subordinates 
the other.  
We have participated in all three tasks of 
TempEval-3- Task A, Task B and Task C. A com-
bination of CRF based machine learning and rule 
based techniques has been adopted for temporal 
expression extraction and determination of attrib-
ute values of the same   (Task A). We have used a 
CRF based technique for event extraction (Task 
B), with the aid of lexical, semantic and syntactic 
features. For determination of event attribute val-
ues we have used simple rule based techniques. 
Automatic annotation of temporal relation between 
event-time in the same sentence, event-DCT rela-
tions, mainevent-mainevent relations in consecu-
tive sentences and subevent-subevent relations in 
the same sentences has been introduced as a new 
task (Task-C) in the TempEval-3 exercise. We 
have adopted a CRF based technique for the same 
as well. 
2 The JU_CSE System Approach  
The JU_CSE system for the TempEval-3 shared 
task uses mainly a Conditional Random Field 
(CRF) machine learning approach to achieve Task 
A, Task B & Task C. The workflow of our system 
is illustrated in Figure 1. 
2.1 Task A: Temporal Expression Identifica-
tion and Normalization 
Temporal Expression Identification: 
 We have used CRF++ 0.571, an open source im-
plementation of the Conditional Random Field 
(CRF) machine learning classifier for our experi-
ments. CRF++ templates have been used to capture 
the relation between the different features in a se-
quence to identify temporal expressions. Temporal 
                                                        
1 http://crfpp.googlecode.com/svn/trunk/doc/index.html 
expressions mostly appear as multi-word entities 
such as ?the next three days?. Therefore the use of 
CRF classifier that uses context information of a 
token seemed most appropriate.  
 Initially, all the sentences have been changed to 
a vertical token-by-token level sequential structure 
for temporal expressions representation by a B-I-O 
encoding, using a set of mostly lexical features. In 
this encoding of temporal expression, ?B? indi-
cates the ?beginning of sequence?, ?I? indicates a 
token inside a sequence and ?O? indicates an out-
side word. We have carefully chosen the features 
list based on the several entities that denote month 
names, year, weekdays, various digit expressions 
(day, time, AM, PM etc.) In certain temporal ex-
pression patterns (several months, last evening) 
some words (several, last) act as modifiers to the 
following words that represent the time expression. 
Temporal expressions include time expression 
modifiers, relative days, periodic temporal set, 
year-eve day, month name with their short pattern 
forms, season of year, time of day, decade list and 
so on. We have used the POS information of each 
token as a feature. We have carefully accounted for 
a simple intuition revelation that most temporal 
expressions contain some tokens conveying the 
?time? information while others possibly convey-
ing the ?quantity? of time. For example, in the ex-
pression ?next three days?, ?three? quantifies 
?days?. Following are the different temporal ex-
pressions lists that have been utilized: 
 
? A list of time expression modifiers: this, 
mid, recent, earlier, beginning, late etc. 
? A list of relative days: yesterday, tomor-
row etc. 
? A list of periodic temporal set: hourly, 
nightly etc. 
? A list of year eve day: Christmas Day, 
Valentine Day etc. 
? A list of month names with their short pat-
tern forms: April, Apr. etc. 
? A list of season of year: spring, winter etc. 
? A list of time of day: morning, afternoon, 
evening etc. 
? A list of decades list: twenties, thirties etc. 
 
 
65
  
  
Raw Text: 
For his part, Fidel Castro is the ultimate political 
survivor. People have predicted his demise so 
many times, and the US has tried to hasten it on 
several occasions. Time and again, he endures.  
? Tokenize with Stanford CoreNLP 
? Obtain POS tags of tokens 
? Extract features from tokens 
? Identify the features for event annotation and 
temporal annotation separately 
 
CRF  
 
Event & 
Time 
 Features 
T
ag E
V
E
N
T
 
tokens 
Tag 
TIMEX3 
tokens 
. 
       For???  OTHERS 
  nearly ???.. TIMEX3 
       forty?. ?  TIMEX3 
years??.. TIMEX3 
. 
. 
 
. 
People???  OTHERS 
have ???..   OTHERS 
      predicted ?. ?  EVENT 
his ????.. OTHERS 
. 
. 
Annotated Text 
 
For his part, Fidel Castro is the ultimate political survivor. 
People have <EVENT class="I_ACTION" 
eid="e1">predicted</EVENT> his <EVENT 
class="OCCURRENCE" eid="e2">demise</EVENT> so 
many times, and the US has <EVENT class="I_ACTION" 
eid="e3">tried</EVENT> to <EVENT 
class="OCCURRENCE" eid="e4">hasten</EVENT> it on 
several occasions. 
D
eterm
ine 
E
vent 
C
lass 
CoreNLP 
for ?type? 
& ?velue? 
<MAKEINSTANCE eiid="ei1? eventID="e1" pos="VERB" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
 
<MAKEINSTANCE eiid="ei2? eventID="e2" pos="NOUN" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
 
<MAKEINSTANCE eiid="ei3? eventID="e3" pos="VERB" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
R
ule based approach to obtain tense, as-
pect, polarity, m
odality etc. for events 
 
Enlist entity pairs with features 
<mainevent-mainevent> 
<event-event> 
<event-dct>  
<event-time> 
 
 
CRF  
 
Temporal Relations: 
 
<TLINK lid="l1" relType="BEFORE" 
eventInstanceID="ei1" relatedTo-
Time="t0" /> 
 
<TLINK lid="l2" relType="BEFORE" 
eventInstanceID="ei2" relatedToEven-
tInstance="ei1" /> 
Figure 1.The JU_CSE System Architecture 
66
Determination of Normalized value and type 
of Temporal Expressions: 
 Temporal expressions in documents are generally 
defined with the type and value attributes. All the 
temporal expressions can be differentiated into 
three types (i) explicit (ii) relative and (iii) implicit 
temporal expressions. For example, the expression 
?October 1998? refers to a specific month of the 
year which can be normalized without any addi-
tional information. On the other hand, the relative 
expression ?yesterday? can?t be normalized with-
out the knowledge of a corresponding reference 
time. The reference time can either be a temporal 
expression or the Document Creation Time marked 
in the document. Consider the following piece of 
text: ?Yesterday was the 50th independence of In-
dia?. The First Independence Day of India is 15th 
august 1947.? Here ?Yesterday? can be normal-
ized as ?15-08-1997?. It may be noted that infor-
mation such as ?First Independence Day of India? 
can be directly accessed from the timestamp calen-
dar, through the metadata of a document. The third 
type of temporal expressions includes implicit ex-
pressions such as names of festival days, birthdays 
and holidays or events. These expressions are 
mapped to available calendar timeline to find out 
their normalized values. 
 
Temporal 
Expression 
Type Value 
A couple of 
years 
 
DURATION P2Y 
October DATE ?1997-10? 
Every day SET P1D 
2 P.M. TIME 2013-02-01T14:00 
Now DATE PRESENT_REF" 
Table 1: TimeML normalized type and value attributes 
for temporal expressions 
 
We have implemented a combined technique us-
ing our handcrafted rules and annotations given by 
the Stanford CoreNLP tool to determine the ?type?-
s and ?value?-s. Four types TIME, DATE, 
DURATION and SET of temporal expressions are 
defined in the TimeML framework. Next, we have 
evaluated the normalized value of temporal expres-
sions using Document Creation Time (DCT) from 
the documents.  In this way, values of different 
dates have been inferred e.g. last year, Monday, 
and today. 
2.2 Task B: Extraction of Event Words and 
Determination of Event Attribute Values  
Event Extraction 
In our evaluation framework, we have used the 
Stanford CoreNLP tool extensively to tokenize, 
lemmatize, named-entity annotate and part-of-
speech tag the text portions of the input files. For 
event extraction, the features have been considered 
at word level, where each word has its own set of 
features. The general features used to train our 
CRF model are: 
Morphological Features: Event words are rep-
resented mostly as verbs and nouns. The major 
problem is detecting the events having non-verbal 
PoS labels. Linguistically, non-verbal wordforms 
are derived from verbal wordforms. Various inflec-
tional and derivational morphological rules are 
involved in the process of evolving from verbal to 
non-verbal wordforms. We have used a set of 
handcrafted rules to identify the suffixes such as (?-
ci?n?, ?-tion? or ?-ion?), i.e., the morphological 
markers of word token, where Person, Location 
and Organization words are not considered. The 
POS and lemma, in a 5-window (-2, +2), has been 
used for event extraction. 
Syntactic Feature: Different event words no-
tions are contained in the sentences such as: verb-
noun combinations structure, the complements of 
aspectual prepositional phrases (PPs) headed by 
prepositions and a particular type of complex 
prepositions. These notions are captured to be used 
as syntactic features for event extraction. 
WordNet Feature: The RiTa Wordnet2 package 
has been effectively used to extract different prop-
erties of words, such as Synonyms, Antonyms, 
Hypernyms, & Hyponyms, Holonyms, Meronyms, 
Coordinates, & Similars, Nominalizations, Verb-
Groups, & Derived-terms. We have used these 
Wordnet properties in the training file for the CRF 
in the form of binary features for verbs and nouns 
indicating if  the words like ?act?, ?activity?, ?phe-
nomenon? etc. occur  in different relations of the 
Wordnet ontology. 
                                                        
2 http://www.rednoise.org/rita/wordnet/documentation/ 
67
Features using Semantic Roles: We use Se-
mantic Role Label (SRL) (Gildea et el, 2002; Pra-
dhan et al 2004; Gurevich et al 2006) to identify 
different useful features for event extraction. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents; determine 
their arguments (agent, patient, etc.) and adjuncts 
(locative, temporal, etc.). Some of the other fea-
tures like predicate, voice and verb sub-
categorization are shared by all the nodes in the 
tree. In the present work, we use predicate as an 
event.  Semantic roles can be used to detect the 
events that are nominalizations of verbs such as 
agreement for agree or construction for construct.  
Event nominalizations often share the same seman-
tic roles as verbs, and often replace them in written 
language. Noun words, morphologically derived 
from verbs, are commonly defined as deverbal 
nouns. Event and result nominalizations constitute 
the bulk of deverbal nouns. The first class refers to 
an event/activity/process, with the nominal ex-
pressing this action (e.g., killing, destruction etc.). 
Nouns in the second class describe the result or 
goal of an action (e.g., agreement, consensus etc.). 
Many nominals denote both the event and result 
(e.g., selection). A smaller class is agent/patient 
nominalizations that are usually identified by suf-
fixes such as -er, -or etc., while patient nominaliza-
tions end with -ee, -ed (e.g. employee).   
Object information of Dependency Relations 
(DR): We have developed handcrafted rules to 
identify features for CRF training, based on the 
object information present in the dependency rela-
tions of parsed sentences. Stanford Parser (de 
Marneffe et al, 2006), a probabilistic lexicalized 
parser containing 45 different Part-of-Speech 
(PoS) tags of Penn Treebank is used to get the 
parsed sentences with dependency relations. The 
dependency relations are found out for the predi-
cates ?dobj? so that the direct object related com-
ponents in the ?dobj? predicate is considered as the 
feature for the event expression. Initially the input 
sentences are passed to the dependency parser3.  
From the parsed output verb noun combination 
direct object (dobj) dependency relations are ex-
tracted. These dobj relations basically inform us 
that direct object of a VP is the noun phrase which 
is the (accusative) object of the verb; the direct 
object of a clause is the direct object of the VP 
                                                        
3 http://nlp.stanford.edu:8080/parser/ 
which is the predicate of that clause. Within the 
dobj relation governing verb word and dependent 
noun words are acting as important features for 
event identification when dependent word is not 
playing any role in other dependency relation 
(nsubj, prep_of, nn ,etc.) of the sentence. 
 
In this way, we have set list of word tokens and 
its features to train the recognition model. Then the 
model will give to each word one of the valid la-
bels.  
Determination of various Event Attribute 
Values: 
Values of different event attributes have been 
computed as follows: 
Class: Identification of the class of an event has 
been done using a simple, intuitive, rule based ap-
proach. Here too, the hypernym list of an event 
token from RitaWordnet has been deployed to de-
termine the class of the respective event. In this 
case, OCCURRENCE has been considered the de-
fault class. 
Tense, Aspect, POS: These three attributes are 
the obligatory attributes of MAKEINSTANCE 
tags. To determine the tense, aspect and polarity of 
an event, we have used the ?parse? annotator in 
CoreNLP. We annotated each sentence with the 
Stanford dependency relations using the above an-
notator. Thereafter various specific relations were 
used to determine the tense, aspect and POS of an 
event token, with another rule based approach. For 
example, in the phrase ?has been abducted?, the 
token ?been? appears as the dependent in an ?aux? 
relation with the event token ?abducted?; and 
hence the aspect ?PERFECTIVE? is inferred. The 
value ?NONE? has been used as the default value 
for both tense and aspect. 
Polarity and Modality: Polarity of event tokens 
are determined using Stanford dependency rela-
tions too; here the ?neg? relation. To determine the 
modality we search for modal words in ?aux? rela-
tions with the event token. 
2.3 Task C: Temporal Relation Annotation 
We have used the gold-standard TimeBank fea-
tures for events and times for training the CRF. In 
the present work, we mainly use the various com-
binations of the following features:  
68
 
(i)  Part of Speech (POS) 
(ii)  Event Tense 
(iii)  Event Aspect 
(iv)  Event Polarity 
(v)  Event Modality 
(vi)  Event Class 
(vii)       Type of temporal expression 
(vii)  Event Stem 
(viii)  Document Creation Time (DCT). 
 
The following subsections describe how various 
temporal relations are computed. 
Event-DCT 
We take the combined features of every event pre-
sent in the text and the DCT for this purpose. 
 
Derived Features: We have identified different 
types of context based syntactic features which are 
derived from text to distinguish the different types 
of temporal relations. In this task, following fea-
tures help us to identify the event-DCT relations, 
specially ?AFTER? temporal relations: 
(i)Modal Context: Whether or not the event word 
has one of the modal context words like- will, 
shall, can, may, or any of their variants (might, 
could, would, etc.).In the sentence: ?The entire 
world will [EVENT see] images of the Pope in Cu-
ba?. Here ?will? context word helps us to deter-
mine event-DCT relation ?AFTER?. 
(ii)Preposition Context: Any prepositions preced-
ing an event or time expression. We consider an 
example:?Children and invalids would be permit-
ted to [EVENT leave] Iraq?. Here the preposition 
to helps us to determine event-DCT relation 
?AFTER?. The same principle goes for time too: in 
the expressions on Friday and for nearly forty 
years, the prepositions on and for governs the time.  
(iii)Context word before or after temporal expres-
sion: context words like before, after, less than, 
greater than etc. help us to determine event-time 
temporal relation identification. Consider an ex-
ample: ?After ten years of [EVENT boom] ?.? 
Event-Time 
Derived Features: We extract all events from eve-
ry sentence. For every temporal expression in a 
sentence, we pair an event in the sentence with the 
former so that the temporal relation can be deter-
mined. 
Similar to annotation of event-DCT relations, 
here too, we have identified different types of con-
text based temporal expression features which are 
derived from text to distinguish the different types 
of temporal relations. In this task, the following 
features help us to distinguish between event and 
time relations, specially ?AFTER? and ?BEFORE? 
temporal relations. The following features are de-
rived from text. 
(i)Type of temporal expression: Represents the 
temporal relationship holding between events, 
times, or between an event and a time of the event.   
(ii)Temporal signal: Represents temporal preposi-
tions ?on? (on this coming Sunday) and slightly 
contribute to the overall score of classifiers 
(iii)Temporal Expression in the target sentence: 
Takes the values greater than, less than, equal or 
none. These values contribute to the overall score 
of classifiers. 
Mainevent-Mainevent and Subevent-
Subevent 
The task demands that the main event of every sen-
tence be determined. As a heuristic decision, we 
have assumed that the first event that appears in a 
sentence is its main event. We pair up main events 
(if present) from consecutive sentences and use 
their combined features to determine their temporal 
relation. For the events belonging to a single sen-
tence, we take into account the combined features 
of all possible pairs of sentential events. 
   
Derived Features: We have identified different 
types of context based syntactic features which are 
derived from text to distinguish the different types 
of temporal relations. 
(i)Relational context: If a relation holding be-
tween the previous event and the current event is 
?AFTER?, the current one is in the past. This in-
formation helps us to identify the temporal relation 
between the current event and successive event. 
(ii)Modal Context: Whether or not the event word 
has one of the context words like, will, shall, can, 
may, or any of their variants (might, could, would, 
etc.).  The verb and auxiliaries governing the next 
event play as an important feature in event-event 
temporal relation identification.   
69
(iii)Ordered based context: In event-event rela-
tion identification, when EVENT-1, EVENT-2, 
and EVENT-3 are linearly ordered, then we have 
assigned true/false as feature value from tense and 
aspect shifts in this ordered pair.  
(iv) Co-reference  based feature: We have used 
co-referential features as derived feature using our 
in-house system based on Standford CoreNLP tool, 
where two event words within or outside one sen-
tence are referring to the same event, i.e. two event 
words co-refer in a discourse.  
(v)Event-DCT relation based feature: We have 
included event-document creation times (DCT) 
temporal relation types as feature of event-event 
relation identification. 
(ii) Preposition Context: Any prepositions before 
the event or time, we consider an exam-
ple:?Children and invalids would be permitted to 
[EVENT leave] Iraq?. Here the preposition to 
helps us determine the event-DCT relation 
?AFTER?.  
(vi) Context word before or after temporal ex-
pression: Context words like before, after, less 
than, greater than help us determine event- event 
temporal relations .We consider an example:?After 
ten years of [EVENT boom] ?.? 
(vii)Stanford parser based clause boundaries 
features: The two consecutive sentences are first 
parsed using Stanford dependency parser and then 
clause boundaries are identified. Then, considering 
the prepositional context and tense verb of the 
clause, temporal relations are identified where all 
temporal expressions are situated in the same 
clause.  
 
 
3 Results and Evaluation 
For the extraction of time expressions and events 
(tasks A and B), precision, recall and F1-score 
have been used as evaluation metrics, using the 
following formulae: 
 
precision (P) = tp/(tp + fp) 
recall (R) = tp/(tp + fn) 
F-measure = 2 *(P * R) / (P + R). 
 
Where, tp is the number of tokens that are part of 
an extent in keys and response, fp is the number of 
tokens that are part of an extent in the response but 
not in the key, and fn is the number of tokens that 
are part of an extent in the key but not in the re-
sponse. Additionally attribute accuracies computed 
according to the following formulae have also been 
reported. 
 
Attr. Accuracy = Attr. F1 / Entity Extraction F1  
Attr. R = Attr. Accuracy * Entity R 
Attr. P = Attr. Accuracy * Entity P 
 
Performance in task C is judged with the aid of the 
Temporal Awareness score proposed by UzZaman 
and Allen (2011) 
The JU_CSE system was evaluated on the TE-3 
platinum data. Table 2 reports JU_CSE?s perfor-
mance in timex extraction Task A. Under the re-
laxed match scheme, the F1-score stands at 
86.38% while the strict match scheme yields a F1-
score of 75.41%. As far as TIMEX attributes are 
concerned, the F1-scores are 63.81% and 73.15% 
for value and type respectively.  
 
Timex Extraction Timex Attribute 
F1 P R Strict F1 Strict P Strict R 
Value 
F1 
Type 
F1 
Value 
Accuracy 
Type 
Accuracy 
86.38 93.28 80.43 75.49 81.51 70.29 63.81 73.15 73.87 84.68 
Table 2:JU_CSE system?s TE-3 Results on Timex Task A 
 
 
 
 
Event Extraction Event Attribute 
F1 P R 
Class 
F1 
Tense 
F1 
Aspect 
F1 
Class 
Accuracy 
Tense 
Accuracy 
Aspect 
Accuracy 
78.57 80.85 76.41 52.65 58.58 72.09 67.01 74.56 91.75 
Table 3:JU_CSE system?s TE-3 Results on Event Task B 
  
70
  
Table 3 reports the system?s performance in 
event extraction (Task B) on TE-3 platinum da-
ta. F1-score for event extraction is 78.57%. At-
tribute F1-scores are 52.65%, 58.58% and 
72.09% for class, tense and aspect respectively.  
In both entities extraction tasks recall is nota-
bly lower than precision. The F1-scores for 
event attributes are modest given that the attrib-
utes were computed using handcrafted rules. 
However, the handcrafted approach can be treat-
ed as a good baseline to start with. Normaliza-
tion is proved to be a challenging task. 
 
Task F1 P R 
Task-ABC 24.61 19.17 34.36 
Task-C 26.41 21.04 35.47 
Task-C-relation-only 34.77 35.07 34.48 
 
Table 4: JU_CSE system?s TE-3 Temporal Aware-
ness results on Task ABC, TaskC-only & TaskC-
relation-only 
 
 
Table 4 presents the Temporal Awareness F1-
score for TaskABC, TaskC and TaskC-relation-
only. For TaskC-only evaluation, the event and 
timex annotated data was provided and one had 
to identify the TLINKs and classify the temporal 
relations. In the TaskC-relation-only version the 
timex and event annotations including their at-
tributes as well as TLINKs were provided save 
the relation classes. Only the relation classes had 
to be determined. The system yielded a temporal 
awareness F1-score of 24.6% for TaskABC, 
26.41% for TaskC-only and 34.77% for TaskC-
relation-only version. 
 
4 Conclusions and Future Directions 
  
In this paper, we have presented the JU_CSE 
system for the TempEval-3 shared task. Our sys-
tem in TempEval-3 may be seen upon as an im-
provement over our earlier endeavor in 
TempEval-2. We have participated in all tasks of 
the TempEval-3 exercise. We have incorporated 
a CRF based approach in our system for all 
tasks. The JU_CSE system for temporal infor-
mation extraction is currently undergoing a lot 
of extensive experimentation. The one reported 
in this article seemingly has a significant scope 
of improvement. Preliminarily, the results yield-
ed are quite competitive and encouraging. Event 
extraction and Timex extraction F1-scores at 
78.58% and 86.38% encourage us to further de-
velop our CRF based scheme. We expect better 
results with additional features and like to con-
tinue our experimentations with other semantic 
features for the CRF classifier. Our rule-based 
approach for event attribute determination how-
ever yields modest F1-scores- 52.65% & 
58.58% for class and tense. We intend to explore 
other machine learning techniques for event at-
tribute classification. We also intend to use parse 
tree based approaches for temporal relation an-
notation. 
Acknowledgments 
This work has been partially supported by a 
grant from the English to Indian language Ma-
chine Translation (EILMT) project funded by 
the Department of Information and Technology 
(DIT), Government of India. We would also like 
to thank to Mr. Jiabul Sk. for his technical con-
tribution.  
 
References  
A. Setzer, and R. Gaizauskas. 2000. Annotating 
Events and Temporal Information in Newswire 
Texts. In LREC 2000, pages 1287?1294, Athens. 
D. Gildea, and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics, 
28(3):245?288. 
James Pustejovsky, Jos? Castano, Robert Ingria, 
Roser Sauri, Robert Gaizauskas, Andrea Setzer, 
Graham Katz, and Dragomir Radev. 2003. 
TimeML: Robust specification of event and tem-
poral expressions in text. New directions in ques-
tion answering, 3: 28-34. 
Marc Verhagen, Robert Gaizauskas, Frank Schilder, 
Mark Hepple, Graham Katz, and James 
Pustejovsky. 2007. Semeval-2007 task 15: 
Tempeval temporal relation identification. In Pro-
ceedings of the 4th International Workshop on 
Semantic Evaluations, pages 75-80, ACL. 
71
Marc Verhagen, Roser Sauri, Tommaso Caselli, and 
James Pustejovsky. 2010. Semeval-2010 task 13: 
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 
57- 62. ACL. 
Olga Gurevich, Richard Crouch, Tracy H. King, and 
V. de Paiva. 2006. Deverbal Nouns in Knowledge 
Representation. Proceedings of FLAIRS, pages 
670?675, Melbourne Beach, FL. 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low Semantic Parsing using Support Vector Ma-
chine. Proceedings of HLT/NAACL-2004, 
Boston, MA. 
UzZaman, N. and J.F. Allen (2011), ?Temporal 
Evaluation.? In Proceedings of The 49th Annual 
Meeting of the Association for Computational 
Linguistics: Human Language Technologies 
(Short Paper), Portland, Oregon, USA.
   
 
72
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 19?27,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Identifying Event ? Sentiment Association using Lexical Equivalence and 
Co-reference Approaches 
 
 
Anup Kumar Kolya1       Dipankar Das1      Asif Ekbal2      Sivaji Bandyopadhyay1 
1 Computer Science and Engineering Department, Jadavpur University, India  
2 Indian Institute of Technology, Patna (IITP), India 
anup.kolya@gmail.com, dipankar.dipnil2005@gmail.com 
asif.ekbal@gmail.com, sivaji_cse_ju@yahoo.com 
 
 
 
Abstract 
In this paper, we have identified event and sen-
timent expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. A hybrid approach that con-
sists of Conditional Random Field (CRF) based 
machine learning framework in conjunction 
with several rule based strategies has been 
adopted for event identification within the 
TimeML framework. The strategies are based 
on semantic role labeling, WordNet relations 
and some handcrafted rules. The sentiment ex-
pressions are identified simply based on the 
cues that are available in the sentiment lexicons 
such as Subjectivity Wordlist, SentiWordNet 
and WordNet Affect. The identification of lexi-
cal equivalence between event and sentiment 
expressions based on the part-of-speech (POS) 
categories is straightforward. The emotional 
verbs from VerbNet have also been employed 
to improve the coverage of lexical equivalence. 
On the other hand, the association of sentiment 
and event has been analyzed using the notion of 
co-reference. The parsed dependency relations 
along with basic rhetoric knowledge help to 
identify the co-reference between event and 
sentiment expressions. Manual evaluation on 
the 171 sentences of TempEval-2010 dataset 
yields the precision, recall and F-Score values 
of 61.25%, 70.29% and 65.23% respectively.  
1 Introduction 
Event and Sentiment are two abstract entities 
closely coupled with each other from social, psy-
chological and commercial perspectives. Some 
kind of action that is going on or something that is 
being happened are addressed as events in general 
by the Natural Language (NL) researchers. The 
events are described in texts where the time, tem-
poral location and ordering of the events are speci-
fied. Event entities are represented by finite 
clauses, nonfinite clauses, nominalizations, event-
referring nouns, adjectives and even some kinds of 
adverbial clauses.  
On the other hand, text not only contains the in-
formative contents, but also some attitudinal pri-
vate information that includes sentiments. 
Nowadays, in the NLP communities, research ac-
tivities on sentiment analysis are in full swing. But, 
the identification of sentiment from texts is not an 
easy task as it is not open to any objective observa-
tion or verification (Quirk et al, 1985).  
Sometimes, similar or different types of senti-
ments are expressed on a single or multiple events. 
Sentiment of people over different events is impor-
tant as it has great influence on our society. Track-
ing users? sentiments about products or events or 
about political candidates as expressed in online 
forums, customer relationship management, stock 
market prediction, social networking etc., temporal 
question answering, document summarization, in-
formation retrieval systems are some of the impor-
tant applications of sentiment analysis.  
The identification of the association between 
event and sentiment is becoming more popular and 
interesting research challenge in the area of Natu-
ral Language Processing (NLP). Our present task is 
to identify the event and sentiment expressions 
from the text, analyze their associative relationship 
19
and investigate the insides of event-sentiment rela-
tions.  
For example, in the following sentence, the an-
notated events are, talked, sent and hijacked .But, 
it also shows the presence of underlying sentiments 
(as shown in underlined script) inscribed in the 
sentence. Here, sentiment helps to evoke the event 
property at lexical entity level (e.g. negative (-ve) 
sentiment for only the event word hijacked) as well 
as at context level (e.g. positive (+ve) sentiment 
associated with the event hijacked as the event 
word appears with the evaluative expression, re-
cover that gives the +ve polarity).  
 
?The prime minister of India told Friday that he 
has talked with top commander of Indian military 
force and sent a team to recover the host of Taj 
Hotel hijacked.?  
 
 Hence, we have organized the entire task into 
three different steps i) event identification, ii) sen-
timent expression identification and iii) identifica-
tion of event sentiment relationships at context 
level using lexical equivalence and co-reference 
approaches.  
In the first step, we propose a hybrid approach 
for event extraction from the text under the Tem-
pEval-2010 framework. Initially, we have used a 
Conditional Random Field (CRF) (Lafferty et al, 
2001) machine learning framework but we observe 
that it often makes the errors in extracting the 
events denoted by deverbial entities. This observa-
tion prompts us to employ several strategies in 
conjunction with machine learning. These strate-
gies are implemented based on semantic role labe-
ling, WordNet (Miller, 1990) and some 
handcrafted rules. We have experimented with the 
TempEval-2010 evaluation challenge setup (Kolya 
et al, 2010).  Evaluation results yield the preci-
sion, recall and F-measure values of approximate-
ly 93.00%, 96.00% and 94.47% respectively. This 
is approximately 12% higher F-measure in com-
parison to the best system (Llorens et al, 2010) of 
TempEval-2010. 
    On the other hand, the identification of the sen-
timent expressions is carried out based on the sen-
timent word. The words are searched in three 
different sentiment lexicons, the Subjectivity Word 
lists (Banea et al, 2008), SentiWordNet (Baccia-
nella et al, 2010) and WordNet Affect (Strapparava 
and Valitutti, 2004). The coarse-grained (positive 
and negative) as well as Ekman?s (1993) six fine- 
grained sentiment or emotion expressions (happy, 
sadness, anger, disgust, fear and surprise) are 
tagged in the corpus. As there is no annotation in 
the TemEval-2010 corpus for sentiment expres-
sions, the evaluation has been carried out by the 
authors and it achieves the precision, recall and F-
measure values of approximately 73.54%, 86.04% 
and 79.30% respectively 
Determining the lexical equivalence of event 
and sentiment expressions based on the POS prop-
erty at the lexical entity level is straightforward. If 
an event word also expresses the sentiment word, 
we have associated the corresponding sentiment 
type with the event word directly. In addition to the 
sentiment lexicons, the emotional verbs extracted 
from the VerbNet (Kipper-Schuler, 2005) are used 
in this phase. It improves the coverage of lexical 
equivalence by 12.76%. 
But, if the event and sentiment expressions oc-
cupy separate text spans in a sentence, we have 
adopted a co-reference approach for identifying 
their association. The parsed dependency relations 
along with some basic rhetoric components, such 
as nucleus, satellite and locus help in identifying 
the co-reference between the event and sentiment 
expressions. The text span containing sentiment 
word is hypothesized as the locus, the main effec-
tive part of the nucleus or satellite. The text span 
that reflects the primary goal of the writer is 
termed as nucleus (marked as ?{ }?) whereas the 
span that provides supplementary material is 
termed as satellite (marked as ?[ ]?). The distin-
guished identification of nucleus and satellite as 
well as their separation from each other is carried 
out based on the direct and transitive dependency 
relations, causal verbs, relaters or discourse mark-
ers. If both the locus and event are identified to-
gether in either nucleus or satellite, we term their 
association as co-referenced. If they occur sepa-
rately in nucleus and satellite and share at least one 
direct dependency relation, we consider their asso-
ciation as co-referenced.  
The evaluation of the lexical equivalence as 
well as co-reference systems has been performed 
by the authors. Primarily, the evaluation of both 
systems has been conducted on the random sam-
ples of 200 sentences of the TempEval-2010 train-
ing dataset.  Finally, the co-reference system 
achieves the precision, recall and F-Scores of 
20
61.25%, 70.29% and 65.23% respectively on 171 
sentences of the TempEval-2010 test corpus.  
The rest of the paper is organized as follows. 
Section 2 describes the related work. The event 
identification is discussed in Section 3. The identi-
fication of sentiment expressions is described in 
Section 4. Determination of lexical equivalence 
between event and sentiment expressions is speci-
fied in Section 5. The co-reference approach for 
identifying the association between event and sen-
timent is described in Section 6. Finally Section 7 
concludes the paper. 
2 Related Work 
The existing works on event extraction are based 
either on pattern-matching rules (Mani and Wilson 
2000), or on the machine learning approach (Bo-
guraev and Ando, 2005). But, still the problems 
persist with the high complexities involved in the 
proper extractions of events. The events expres-
sions were annotated in the TempEval 2007 
source in accordance with the TimeML standard 
(Pustejovsky et al, 2003). On the other hand, the 
Task B of TempEval-2010 evaluation challenge 
setup (Verhagen et al, 2010) was aimed at identi-
fying events from text. The best achieved result 
was obtained by (Llorens et al, 2010). 
The majority of subjective analysis methods 
that are related to emotion is based on textual key-
words spotting that use specific lexical resources. 
A lexicon that provides appraisal attributes for 
terms was constructed and the features were used 
for emotion classification (Whitelaw et al, 2005). 
The features along with the bag-of-words model 
give 90.2% accuracy. UPAR7 (Chaumartin, 2007), 
a rule-based system uses a combination of Word-
Net Affect and SentiWordNet. The system was 
semi-automatically enriched with the original trial 
data provided during the SemEval task (Strappara-
va and Mihalcea, 2007). SWAT (Katz et al, 2007) 
is another supervised system that uses a unigram 
model trained to annotate emotional content. 
Our motivation is that though events and senti-
ments are closely coupled with each other from 
social, psychological and commercial perspectives, 
very little attention has been given about their de-
tection and analysis. To the best of our knowledge, 
only a few tasks have been attempted (Fukuhara et 
al., 2007) (Das et al, 2010).  
Sometimes, the opinion topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics, as 
well as opinions that are on the same topic sepa-
rated by opinions that do not share that topic 
(Stoyanov and Cardie 2008). The authors have es-
tablished their hypothesis by applying the co-
reference technique. Similarly, we have adopted 
the co-reference technique based on basic rhetoric 
components for identifying the association be-
tween event and sentiment expressions.  In addi-
tion to that, we have also employed the lexical 
equivalence approach for identifying their associa-
tion.  
3 Event Identification 
In this work, we propose a hybrid approach for 
event identification from the text under the Tem-
pEval-2010 framework. We use Conditional Ran-
dom Field (CRF) as the underlying machine 
learning algorithm. We observe that this machine 
learning based system often makes the errors in 
identifying the events denoted by deverbial enti-
ties. This observation prompts us to employ several 
strategies in conjunction with machine learning 
techniques. These strategies have been imple-
mented based on semantic role labeling, WordNet 
senses and some handcrafted rules.  
We have experiment with the TempEval-2010 
evaluation challenge setup (Kolya et al, 2010).  
Evaluation results yield the precision, recall and F-
measure values of approximately 93.00%, 96.00% 
and 94.47% respectively. This is approximately 
12% higher F-measure in comparison to the best 
system (Llorens et al, 2010) of TempEval-2010. 
3.1 CRF based Approach for Event Identifi-
cation 
We extract the gold-standard TimeBank features 
for events in order to train/test the CRF model. In 
the present work, we mainly use the various com-
binations of the following features:  
Part of Speech (POS) of event terms (e.g. Ad-
jective, Noun and Verb), Tense (Present, Past, Fu-
ture, Infinitive, Present part, Past part, or NONE), 
Aspect (Progressive, Perfective and Perfective 
Progressive or NONE), Class (Reporting, Percep-
tion, Aspectual, I_action, I_state, State, Occur-
rence), Stem (e.g., discount /s/).  
21
3.2 Use of Semantic Roles for Event Identifi-
cation 
We use an open source Semantic Role Labeler 
1(SRL) (Gildea et al, 2002) (Pradhan et al, 2004) 
to identify different features of the sentences. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents, determining 
their arguments (agent, patient etc.) and adjuncts 
(locative, temporal etc.). Semantic roles can be 
used to detect the events that are the nominaliza-
tions of verbs such as agreement for agree or con-
struction for construct. Nominalizations (or, 
deverbal nouns) are commonly defined as nouns 
that are morphologically derived from verbs, 
usually by suffixation (Quirk et al, 1985). Event 
nominalizations often afford the same semantic 
roles as verbs and often replace them in written 
language (Gurevich et al, 2006).  Event nominali-
zations constitute the bulk of deverbal nouns.  The 
following example sentence shows how semantic 
roles can be used for event identification.  
 
[ARG1 All sites] were [TARGET inspected] to the satis-
faction of the inspection team and with full coope-
ration of Iraqi authorities, [ARG0 Dacey] [TARGET 
said]. 
 
   The extracted target words are treated as the 
event words. It has been observed that many of 
these target words are identified as the event ex-
pressions by the CRF model. But, there exists ma-
ny nominalised event expressions (i.e., deverbal 
nouns) that are not identified as events by the su-
pervised CRF. These nominalised expressions are 
correctly identified as events by SRL.  
3.3 Use of WordNet for Event Identification 
WordNet is mainly used to identify non-deverbal 
event nouns. We observed that the event entities 
like ?war?, ?attempt?, ?tour? are not properly identi-
fied. These words have noun (NN) POS informa-
tion as the previous approaches, i.e., CRF and SRL 
can only identify those event words that have verb 
(VB) POS information. We know from the lexical 
information of WordNet that the words like ?war? 
and ?tour? are generally used as both noun and 
verb forms in the sentence. Therefore, we have 
                                                        
1 http://cemantix.org/assert.html 
designed the following two rules based on the 
WordNet: 
 
Rule 1: The word tokens having Noun (NN) POS 
categories are looked into the WordNet. If it ap-
pears in the WordNet with noun and verb senses, 
then that word token is considered as an event.  For 
example, war has both noun and verb senses in the 
WordNet, and hence war is considered as an event.  
 
Rule 2: The stems of the noun word tokens are 
looked into the WordNet. If one of the WordNet 
senses is verb then the token is considered as verb. 
For example, the stem of proposal, i.e., propose 
has two different senses, noun and verb in the 
WordNet, and thus it is considered as an event.  
3.4    Use of Rules for Event Identification 
Here, we mainly concentrate on the identification 
of specific lexical classes like ?inspection? and 
?resignation?. These can be identified by the suf-
fixes such as (?-ci?n?), (?-tion?) or (?-ion?), i.e., the 
morphological markers of deverbal derivations. 
  Initially, we have employed the CRF based Stan-
ford Named Entity (NE) tagger2 on the TempEval-
2 test dataset. The output of the system is tagged 
with Person, Location, Organization and Other 
classes. The words starting with the capital letters 
are also considered as NEs. Thereafter, we came 
up with the following rules for event identification: 
  
Cue-1: The deverbal nouns are usually identified 
by the suffixes like ?-tion?, ?-ion?, ?-ing? and ?-ed? 
etc. The nouns that are not NEs, but end with these 
suffixes are considered as the event words. 
  
Cue 2: The verb-noun combinations are searched 
in the sentences of the test set. The non-NE noun 
word tokens are considered as the events.  
 
Cue 3: Nominals and non-deverbal event nouns 
can be identified by the complements of aspectual 
PPs headed by prepositions like during, after and 
before, and complex prepositions such as at the 
end of and at the beginning of etc.  The next word 
token(s) appearing after these clue word(s) or 
phrase(s) are considered as events.  
                                                        
2 http://nlp.stanford.edu/software/CRF-NER.shtml 
22
Cue 4: The non-NE nouns occurring after the ex-
pressions such as frequency of, occurrence of and 
period of are most probably the event nouns. 
 
Cue 5: Event nouns can also appear as objects of 
aspectual and time-related verbs, such as have be-
gun a campaign or have carried out a campaign 
etc. The non-NEs that appear after the expressions 
like ?have begun a?, ?have carried out a? etc.  are 
also denoted as the events.   
4 Sentiment Expression Identification 
Sentiment is an important cue that effectively de-
scribes the events associated with it. The binary 
classification of the sentiments (positive and nega-
tive) as well as the fine-grained categorization into 
Ekman?s (1993) six emotions is therefore em-
ployed for identifying the sentiment expressions. 
200 sentences are randomly selected from the 
training dataset of the TempEval-2010 corpus. 
These sentences have been considered as our de-
velopment set. On the other hand, 171 sentences 
were already provided as the test sentences in the 
TempEval-2010 evaluation challenge.   
The events are already annotated in the Tem-
pEval-2010 corpus. But, no sentiment or emotion 
related annotation is available in the corpus. 
Hence, we have annotated the sentiment expres-
sions at word level in a semi-supervised way. The 
word level entities are tagged by their coarse and 
fine grained sentiment tags using the available sen-
timent related lexical resources. Then the automat-
ic annotation has been evaluated manually by the 
authors. The semi-supervised sentiment annotation 
agreements were 90.23% for the development set 
and 92.45% for the test sets respectively.  
4.1 Lexicon based Approach 
The tagging of the evaluative expressions or more 
specifically the sentiment expressions on the Tem-
pEval-2010 corpus has been carried out using the 
available sentiment lexicons. We passed the sen-
tences through three sentiment lexicons, Subjectivi-
ty Wordlists (Banea et al, 2008), SentiWordNet 
(Baccianella et al, 2010) and WordNet Affect 
(Strapparava and Valitutti, 2004). Subjectivity 
Wordlist assigns words with the strong or weak 
subjectivity and prior polarities of types positive, 
negative and neutral. SentiWordNet, used in opi-
nion mining and sentiment analysis, assigns three 
sentiment scores such as positive, negative and 
objective to each synset of WordNet. WordNet Af-
fect, a small well-used lexical resource but valua-
ble for its affective annotation contains the words 
that convey emotion.  
The algorithm is that, if a word in a sentence is 
present in any of these resources; the word is 
tagged as the sentiment expression. But, if any 
word is not found in any of them, each word of the 
sentence is passed through the WordNet Morpho-
logical analyzer (Miller, 1990) to identify its root 
form and the root form is searched through the re-
sources again. If the root form is found, the corres-
ponding word is tagged as sentiment expression 
accordingly.  
The identified sentiment expressions have been 
evaluated by the authors and it achieves the preci-
sion, recall and F-Score of 73.54%, 86.04% and 
79.30%, respectively on a total of 171 test sen-
tences of the TempEval-2010 corpus.   
The identification of event words that also ex-
press sentiment is straightforward. But, the prob-
lem arises when the event and sentiment 
expressions are present separately in a sentence 
and the sentiment is either closely associated with 
the event or affects it. In case of the former, we 
have adopted the approach of lexical equivalence 
between the event and sentiment entities whereas 
the co-reference technique has been introduced for 
resolving the latter case.  
5 Lexical Equivalence between Event and 
Sentiment Expressions  
It is observed that in general the verbs, nouns and 
adjectives represent events. The sentences are 
passed through an open source Stanford Maximum 
Entropy based POS tagger (Manning and Toutano-
va, 2000). The best reported accuracy for the POS 
tagger on the Penn Treebank is 96.86% overall and 
86.91% on previously unseen words. Our objective 
was to identify the event words that also express 
sentiments. Hence, we have identified the event 
words that have also been tagged as the sentiment 
expressions. The coverage of these lexical re-
sources in identifying the event sentiment associa-
tion is shown in Table 1. 
On the other hand, not only the adjectives or 
nouns, the sentiment or emotional verbs play an 
important role in identifying the sentiment expres-
23
sions. Hence, in addition to the above mentioned 
sentiment resources, we have also incorporated 
English VerbNet (Kipper-Schuler, 2005) for the 
automatic annotation process. VerbNet associates 
the semantics of a verb with its syntactic frames 
and combines traditional lexical semantic informa-
tion such as thematic roles and semantic predi-
cates, with syntactic frames and selectional 
restrictions. Verb entries in the same VerbNet class 
share common syntactic frames and thus they are 
believed to have the same syntactic behavior. For 
example, the emotional verbs ?love? and ?enjoy? 
are members of the admire-31.2-1 class and ?en-
joy? also belongs to the class want-32.1-1.  
The XML files of VerbNet are preprocessed to 
build up a general list that contains all member 
verbs and their available syntax information re-
trieved from VerbNet. The main criterion for se-
lecting the member verbs as sentiment expressions 
is the presence of ?emotional_state? type predicate 
in their frame semantics. The frequencies of the 
event words matched against the above said four 
resources are shown in Table 1.  It has been ob-
served that the adjective events are not identified 
by the lexical resources as their frequency in the 
test corpus was very low. But, the lexical coverage 
has been improved by 12.76% by incorporating 
VerbNet. 
 
Resources Noun   Adjective  Verb 
#114    #4              #380 
Subjectivity Wordlists 
SentiWordNet 
WordNet Affect List 
VerbNet (emotional 
verbs) 
24            --             35 
32            --             59  
12            --             25 
 --            --             79 
Accuracy (in %) 59.64                    52.57 
 
Table 1: Results of Lexical Equivalence between 
Event and Sentiment based on different resources  
6 Co-reference between Event and Senti-
ment Expressions  
The opinion and/or sentiment topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics. 
Sometimes, the opinions that are on the same topic 
are separated by opinions that do not share that 
topic (Stoyanov and Cardie, 2008). We observe the 
similar situation in case of associating sentiments 
with events. Hence, the hypothesis for opinion top-
ic is established for sentiment events by applying 
the co-reference technique along with the rhetori-
cal structure. We have proposed two different sys-
tems for identifying the association of sentiments 
with the events at context level. 
6.1 Baseline Co-reference System 
The baseline system has been developed based on 
the object information present in the dependency 
relations of the parsed sentences. Stanford Parser 
(Marneffe et al, 2006), a probabilistic lexicalized 
parser containing 45 different part of speech (POS) 
tags of Pen Treebank tagset  has been used to get 
the parsed sentences and dependency relations. 
The dependency relations are checked for the pre-
dicates ?dobj? so that the related components 
present in the predicate are considered as the prob-
able candidates for the events.  
If a dependency relation contains both the event 
and sentiment words, we have considered the pres-
ence of co-reference between them. But, it has 
been observed that the event and sentiment expres-
sions are also present in two different relations that 
share a common word element. Hence, if the event 
and sentiment words appear in two different rela-
tions but both of the relations contain at least one 
common element, the event and sentiment words 
are termed as co-referenced.    
Overall, the baseline co-reference system 
achieves the precision, recall and F-Scores of 
40.03%, 46.10% and 42.33% for event-sentiment 
co-reference identification. For example in the fol-
lowing sentence, the writer?s direct as well as indi-
rect emotional intentions are reflected by 
mentioning one or more topics or events (spent, 
thought) and their associated sentiments (great).  
 
?When Wong Kwan spent seventy million dol-
lars for this house, he thought it was a great deal.? 
 
The baseline co-reference system fails to asso-
ciate the sentiment expressions with their corres-
ponding event expressions. Hence, we aimed for 
the rhetoric structure based co-reference system to 
identify their association. 
6.2  Rhetoric Co-reference System 
The distribution of events and sentiment expres-
sions in different text spans of a sentence needs the 
24
analysis of sentential structure. We have incorpo-
rated the knowledge of Rhetorical Structure 
Theory (RST) (Mann and Thompson 1987) for 
identifying the events that are co-referred by their 
corresponding sentiment expressions.  
The theory maintains that consecutive discourse 
elements, termed text spans, are related by a rela-
tively small set (20?25) of rhetorical relations. 
But, instead of identifying the rhetorical relations, 
the present task acquires the basic and coarse rhe-
torical components such as locus, nucleus and sa-
tellite from a sentence.  These rhetoric clues help 
in identifying the individual event span associated 
with the span denoting the corresponding senti-
ment expression in a sentence. The text span that 
reflects the primary goal of the writer is termed as 
nucleus (marked as ?{ }?) whereas the span that 
provides supplementary material is termed as satel-
lite (marked as ?[ ]?). For example, the nucleus and 
satellite textual spans are shown in the following 
sentence as, 
 
{Traders said the market remains extremely 
nervous} because [the wild swings seen on the 
New York Stock Exchange last week]. 
 
The event or topic of an opinion or sentiment 
depends on the context in which the associated 
opinion or sentiment expression occurs (Stoyanov 
and Cardie 2008). Considering the similar hypo-
thesis in case of events instead of topics, the co-
reference between an event and a sentiment ex-
pression is identified from the nucleus and/or satel-
lite by positioning the sentiment expression as 
locus. We have also incorporated the WordNet?s 
(Miller 1990) morphological analyzer to identify 
the stemmed forms of the sentiment words.  
The preliminary separation of nucleus from sa-
tellite was carried out based on the list of frequent-
ly used causal keywords (e.g., as, because, that, 
while, whether etc) and punctuation markers (,) (!) 
(?).The discourse markers and causal verbs are 
also the useful clues if they are explicitly specified 
in the text. The identification of discourse markers 
from written text itself is a research area (Azar 
1999). Hence, our task was restricted to identify 
only the explicit discourse markers that are tagged 
by conjunctive_() or mark_() type dependency re-
lations of the parsed constituents. The dependency 
relations containing conjunctive markers (e.g., 
conj_and(), conj_or(), conj_but()) were considered 
for separating nucleus from satellite if the markers 
are present in between two successive clauses. 
Otherwise, the word token contained in the 
mark_() type dependency relation was considered 
as a discourse marker. 
The list of causal verbs is prepared by 
processing the XML files of VerbNet. If any Verb-
Net class file contains any frame with semantic 
type as Cause, we collect the member verbs of that 
XML class file and term the member verbs as 
causal verbs. We used a list that contains a total 
number of 253 causal verbs.  
If any clause tagged as S or SBAR in the parse 
tree contains any causal verb, that clause is consi-
dered as the nucleus and the rest of the clauses de-
note the satellites. Considering the basic theory of 
rhetorical structure (Mann and Thompson 1987), 
the clauses were separated into nucleus and satel-
lite to identify the event and sentiment expressions. 
The direct dependency is identified based on the 
simultaneous presence of locus and the event word 
in the same dependency relation whereas the tran-
sitive dependency is verified if the word is con-
nected to locus and event via one or more 
intermediate dependency relations.  
If the event and sentiment words are together 
present in either nucleus or satellite, the associa-
tion between the two expressions is considered as 
co-referenced. If they occur in nucleus and satellite 
separately, but the event and sentiment words are 
present in at least one direct dependency relation, 
the expressions are termed as co-referenced.  
In the previous example, the event expressions, 
?said? and ?remains? are associated with the sen-
timent expression ?nervous? as both the event ex-
pressions share the direct dependency relations 
?cop(nervous-7, remains-5)? and ?ccomp(said-2, 
nervous-7)? in the nucleus segment. Similarly, the 
event word, ?seen? and sentiment word ?wild? are 
present in the satellite part and they share a direct 
dependency relation ?partmod(swings-12, seen-
13)?. But, no direct dependency relation is present 
between the ?nervous? and ?seen? or ?said? and 
?wild? or ?remains? and ?wild?.  
6.3 Results 
Though the event annotation is specified in the 
TempEval-2010 corpus, the association between 
the event and sentiment expressions was not speci-
fied in the corpus. Hence, we have carried out the 
25
evaluation manually. The 200 random samples of 
the training set that were used in sentiment expres-
sion identification task have been considered as 
our development set. The Evaluation Vectors 
(EvalV) are prepared manually from each sentence 
of the development and test sets. The vectors 
<EvExp, SentiExp> are filled with the annotated 
events and sentiment expressions by considering 
their association. The annotation of sentiment ex-
pressions using the semi-supervised process has 
been described in Section 4. 
    The rule based baseline and rhetoric based co-
reference systems identify the event and sentiment 
expressions from each sentence and stores them in 
a Co-reference Vector (CorefV). The evaluation is 
carried out by comparing the system generated Co-
reference Vectors (CorefV) with their correspond-
ing Evaluation Vectors (EvalV). The evaluation 
results on 171 test sentences are shown in Table 2. 
 
Co-reference  
Approaches 
Prec.     Rec.    F-Score 
(in %) 
Baseline System 40.03    46.10       42.33 
Rhetoric System 61.25    70.29       65.23 
 
Table 2: Precision (Prec.), Recall (Rec.) and F-
Scores (in %) of the event-sentiment co-reference 
systems  
 
Overall, the precision, recall and F-Scores are 
61.25%, 70.29% and 65.23% for event-sentiment 
co-reference identification using rhetoric clues. 
Though the co-reference technique performs satis-
factorily for identifying the event-sentiment co-
reference, the problem arises in distinguishing the 
corresponding spans of events from an overlapped 
text span of multi-word tokens.  
7 Conclusion  
In this present work, we have identified event and 
sentiment expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. It has been observed that the lex-
ical equivalence based on lexicons performs satis-
factorily but overall, the co-reference entails that 
the presence of indirect affective clues can also be 
traced with the help of rhetoric knowledge and de-
pendency relations. The association of the senti-
ments with their corresponding events can be used 
in future concerning the time based sentiment 
change over events.  
Acknowledgments 
The work is supported by a grant from the India-
Japan Cooperative Programme (DST-JST) 2009 
Research project entitled ?Sentiment Analysis 
where AI meets Psychology? funded by Depart-
ment of Science and Technology (DST), Govern-
ment of India. 
References  
Baccianella Stefano, Esuli Andrea and Sebas-tiani Fa-
brizio. 2010. SentiWordNet 3.0: An Enhanced Lexi-
cal Re-source for Sentiment Analysis and Opinion 
Mining. In Proceedings of the 7th Conference on 
Language Resources and Evaluation, pp. 2200-2204. 
Banea, Carmen, Mihalcea Rada, Wiebe Janyce. 2008.  
A Bootstrapping Method for Building Subjectivity 
Lexicons for Languages with Scarce Resources. The 
Sixth International Conference on Language Re-
sources and Evaluation. 
Boguraev, B., Ando, R. K. 2005. TimeBank-
DrivenTimeML Analysis. Annotating, Extracting and 
Reasoning about Time and Events 2005. 
Chaumartin, F. 2007. Upar7: A knowledge-based sys-
tem for headline sentiment tagging. SemEval-200,  
Czech Republic. 
Ekman Paul. 1993. An argument for basic emotions, 
Cognition and Emotion, 6(3-4):169-200. 
Fukuhara T., Nakagawa, H. and Nishida, T. 2007. Un-
derstanding Sentiment of People from News Articles: 
Temporal Sentiment Analysis of Social Events. 
ICWSM?2007, Boulder, Colorado. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics, 
28(3):245?288. 
Gurevich, O., R. Crouch, T. King, and V. de Paiva. 
2006. Deverbal Nouns in Knowledge Representation. 
Proceedings of FLAIRS, pages 670?675, Melbourne 
Beach, FL. 
Katz, P., Singleton, M. and Wicentowski, R. 2007. 
Swat-mp: the semeval-2007 systems for task 5 and 
task SemEval-2007.  
Kipper-Schuler, K. 2005.  VerbNet: A broad-coverage, 
comprehensive verb lexicon. Ph.D. thesis, Computer 
and Information Science Dept., University of Penn-
sylvania, Philadelphia, PA. 
26
Kolya, A., Ekbal, A. and Bandyopadhyay, S. 2010. 
JU_CSE_TEMP: A First Step towards Evaluating 
Events, Time Expressions and Temporal Relations. 
In Proceedings of the 5th International Workshop on 
Semantic Evaluation, ACL 2010, July 15-16, Swe-
den, pp. 345?350. 
Lafferty, J., McCallum, A.K., Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. International 
Conference on Machine Learning. 
Llorens Hector, Estela Saquete, Borja Navarro. 2010. 
TIPSem (English and Spanish): Evaluating CRFs and 
Semantic Roles. Proceedings of the 5th International 
Workshop on Semantic Evaluation, ACL 2010, pages 
284?291, Uppsala, Sweden, 15-16 July 2010. 
Mani, I., and Wilson G. 2000. Processing of News. In 
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 69-76. 
Mann, W. and S. Thompson. 1987. Rhetorical Structure 
Theory: Description and Construction of Text Struc-
ture. In G. Kempen (ed.), Natural Language Genera-
tion, Martinus Nijhoff, The Hague, pp. 85?96. 
Manning Christopher and Toutanova, Kristina. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. Proceedings of 
the Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora (EMNLP/VLC)  
Marneffe, Marie-Catherine de, Bill MacCartney, and 
Christopher D.Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 5th 
International Conference on Language Resources 
and Evaluation.  
Miller George A. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235?312 
Pradhan S., Wayne W., Hacioglu, K., Martin, J.H. and 
Jurafsky, D. 2004. Shallow Semantic Parsing using 
Support Vector Machines. Proceedings of the Human 
Language Technology Conference/North American 
chapter of the Association for Computational Lin-
guistics annual meeting Boston, MA, May 2-7. 
Pustejovsky, J., Castano, J., Ingria, R., Sauri, R., Gai-
zauskas, R., Setzer, A., Katz, G. and Radev, D. 
TimeML: Robust specification of event and temporal 
expressions in text. In AAAI Spring Symposium on 
New Directions in Question-Answering, pp. 28-34, 
CA, 2003. 
Quirk, R., Greenbaum, S. Leech, G. and Svartvik, J. 
1985. A Comprehensive Grammar of the English 
Language. Longman.  
Strapparava C. and Valitutti, A. 2004. Wordnet-affect: 
an affective extension of wordnet. In 4th Internation-
al Conference on Language Resources and Evalua-
tion, pp. 1083-1086. 
Strapparava Carlo and Mihalcea Rada. 2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meeting 
of Association for Computational linguistics. 
Stoyanov, V., and Cardie, C. 2008. Annotating topics of 
opinions. In Proceedings of LREC.  
 
27
