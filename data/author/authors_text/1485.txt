Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 73?76,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Outilex, a Linguistic Platform for Text Processing
Olivier Blanc
IGM, University of Marne-la-Valle?e
5, bd Descartes - Champs/Marne
77454 Marne-la-Valle?e, France
oblanc@univ-mlv.fr
Matthieu Constant
IGM, University of Marne-la-Valle?e
5, bd Descartes - Champs/Marne
77 454 Marne-la-Valle?e, france
mconstan@univ-mlv.fr
Abstract
We present Outilex, a generalist linguis-
tic platform for text processing. The plat-
form includes several modules implement-
ing the main operations for text processing
and is designed to use large-coverage Lan-
guage Resources. These resources (dictio-
naries, grammars, annotated texts) are for-
matted into XML, in accordance with cur-
rent standards. Evaluations on efficiency
are given.
1 Credits
This project has been supported by the French
Ministry of Industry and the CNRS. Thanks to Sky
and Francesca Sigal for their linguistic expertise.
2 Introduction
The Outilex Project (Blanc et al, 2006) aims to de-
velop an open-linguistic platform, including tools,
electronic dictionaries and grammars, dedicated to
text processing. It is the result of the collaboration
of ten French partners, composed of 4 universities
and 6 industrial organizations. The project started
in 2002 and will end in 2006. The platform which
will be made freely available to research, develop-
ment and industry in April 2007, comprises soft-
ware components implementing all the fundamen-
tal operations of written text processing: text seg-
mentation, morphosyntactic tagging, parsing with
grammars and language resource management.
All Language Resources are structured in XML
formats, as well as binary formats more adequate
to efficient processing; the required format con-
verters are included in the platform. The grammar
formalism allows for the combination of statis-
tical approaches with resource-based approaches.
Manually constructed lexicons of substantial cov-
erage for French and English, originating from the
former LADL1, will be distributed with the plat-
form under LGPL-LR2 license.
The platform aims to be a generalist base for di-
verse processings on text corpora. Furthermore, it
uses portable formats and format converters that
would allow for combining several software com-
ponents. There exist a lot of platforms dedicated
to NLP, but none are fully satisfactory for various
reasons. Intex (Silberztein, 1993), FSM (Mohri et
al., 1998) and Xelda3 are closed source. Unitex
(Paumier, 2003), inspired by Intex has its source
code under LGPL license4 but it does not support
standard formats for Language Resources (LR).
Systems like NLTK (Loper and Bird, 2002) and
Gate (Cunningham, 2002) do not offer functional-
ity for Lexical Resource Management.
All the operations described below are imple-
mented in C++ independent modules which in-
teract with each others through XML streams.
Each functionality is accessible by programmers
through a specified API and by end users through
binary programs. Programs can be invoked by
a Graphical User Interface implemented in Java.
This interface allows the user to define his own
processing flow as well as to work on several
projects with specific texts, dictionaries and gram-
mars.
1French Laboratory for Linguistics and Information Re-
trieval
2Lesser General Public License for Language Resources,
http://infolingu.univ-mlv.fr/lgpllr.html.
3http://www.dcs.shef.ac.uk/ hamish/dalr/baslow/xelda.pdf.
4Lesser General Public License,
http://www.gnu.org/copyleft/lesser.html.
73
3 Text segmentation
The segmentation module takes raw texts or
HTML documents as input. It outputs a text
segmented into paragraphs, sentences and tokens
in an XML format. The HTML tags are kept
enclosed in XML elements, which distinguishes
them from actual textual data. It is therefore pos-
sible to rebuild at any point the original docu-
ment or a modified version with its original layout.
Rules of segmentation in tokens and sentences are
based on the categorization of characters defined
by the Unicode norm. Each token is associated
with information such as its type (word, number,
punctuation, ...), its alphabet (Latin, Greek), its
case (lowercase word, capitalized word, ...), and
other information for the other symbols (opening
or closing punctuation symbol, ...). When applied
to a corpus of journalistic telegrams of 352,464
tokens, our tokenizer processes 22,185 words per
second5.
4 Morphosyntactic tagging
By using lexicons and grammars, our platform in-
cludes the notion of multiword units, and allows
for the handling of several types of morphosyntac-
tic ambiguities. Usually, stochastic morphosyn-
tactic taggers (Schmid, 1994; Brill, 1995) do not
handle well such notions. However, the use of lex-
icons by companies working in the domain has
much developed over the past few years. That
is why Outilex provides a complete set of soft-
ware components handling operations on lexicons.
IGM also contributed to this project by freely dis-
tributing a large amount of the LADL lexicons6
with fine-grained tagsets7: for French, 109,912
simple lemmas and 86,337 compound lemmas; for
English, 166,150 simple lemmas and 13,361 com-
pound lemmas. These resources are available un-
der LGPL-LR license. Outilex programs are com-
patible with all European languages using inflec-
tion by suffix. Extensions will be necessary for
the other types of languages.
Our morphosyntactic tagger takes a segmented
text as an input ; each form (simple or compound)
is assigned a set of possible tags, extracted from
5This test and further tests have been carried out on a PC
with a 2.8 GHz Intel Pentium Processor and a 512 Mb RAM.
6http://infolingu.univ-mlv.fr/english/, follow links Lin-
guistic data then Dictionnaries.
7For instance, for French, the tagset combines 13 part-of-
speech tags, 18 morphological features and several syntactic
and semantic features.
indexed lexicons (cf. section 6). Several lexicons
can be applied at the same time. A system of pri-
ority allows for the blocking of analyses extracted
from lexicons with low priority if the considered
form is also present in a lexicon with a higher pri-
ority. Therefore, we provide by default a general
lexicon proposing a large set of analyses for stan-
dard language. The user can, for a specific appli-
cation, enrich it by means of complementary lexi-
cons and/or filter it with a specialized lexicon for
his/her domain. The dictionary look-up can be pa-
rameterized to ignore case and diacritics, which
can assist the tagger to adapt to the type of pro-
cessed text (academic papers, web pages, emails,
...). Applied to a corpus of AFP journalistic tele-
grams with the above mentioned dictionaries, Out-
ilex tags about 6,650 words per second8.
The result of this operation is an acyclic au-
tomaton (sometimes, called word lattice in this
context), that represents segmentation and tag-
ging ambiguities. This tagged text can be serial-
ized in an XML format, compatible with the draft
model MAF (Morphosyntactic Annotation Frame-
work)(Cle?ment and de la Clergerie, 2005).
All further processing described in the next sec-
tion will be run on this automaton, possibly modi-
fying it.
5 Text Parsing
Grammatical formalisms are very numerous in
NLP. Outilex uses a minimal formalism: Recur-
sive Transition Network (RTN)(Woods, 1970) that
are represented in the form of recursive automata
(automata that call other automata). The termi-
nal symbols are lexical masks (Blanc and Dister,
2004), which are underspecified word tags i.e. that
represent a set of tagged words matching with the
specified features (e.g. noun in the plural). Trans-
ductions can be put in our RTNs. This can be used,
for instance, to insert tags in texts and therefore
formalize relations between identified segments.
This formalism allows for the construction of
local grammars in the sense of (Gross, 1993).
It has been successfully used in different types
of applications: information extraction (Poibeau,
84.7 % of the token occurrences were not found in the dic-
tionary; This value falls to 0.4 % if we remove the capitalized
occurrences.
The processing time could appear rather slow; but, this task
involves not so trivial computations such as conversion be-
tween different charsets or approximated look-up using Uni-
code character properties.
74
2001; Nakamura, 2005), named entity localization
(Krstev et al, 2005), grammatical structure iden-
tification (Mason, 2004; Danlos, 2005)). All of
these experiments resulted in recall and precision
rates equaling the state-of-the-art.
This formalism has been enhanced with weights
that are assigned to the automata transitions. Thus,
grammars can be integrated into hybrid systems
using both statistical methods and methods based
on linguistic resources. We call the obtained for-
malism Weighted Recursive Transition Network
(WRTN). These grammars are constructed in the
form of graphs with an editor and are saved in an
XML format (Sastre, 2005).
Each graph (or automaton) is optimized with
epsilon transition removal, determinization and
minimization operations. It is also possible to
transform a grammar in an equivalent or approx-
imate finite state transducer, by copying the sub-
graphs into the main automaton. The result gen-
erally requires more memory space but can highly
accelerate processing.
Our parser is based on Earley algorithm (Earley,
1970) that has been adapted to deal with WRTN
(instead of context-free grammar) and a text in the
form of an acyclic finite state automaton (instead
of a word sequence). The result of the parsing
consists of a shared forest of weighted syntactic
trees for each sentence. The nodes of the trees
are decorated by the possible outputs of the gram-
mar. This shared forest can be processed to get
different types of results, such as a list of con-
cordances, an annotated text or a modified text
automaton. By applying a noun phrase grammar
(Paumier, 2003) on a corpus of AFP journalistic
telegrams, our parser processed 12,466 words per
second and found 39,468 occurrences.
The platform includes a concordancer that al-
lows for listing in their occurring context differ-
ent occurrences of the patterns described in the
grammar. Concordances can be sorted according
to the text order or lexicographic order. The con-
cordancer is a valuable tool for linguists who are
interested in finding the different uses of linguis-
tic forms in corpora. It is also of great interest to
improve grammars during their construction.
Also included is a module to apply a transducer
on a text. It produces a text with the outputs of the
grammar inserted in the text or with recognized
segments replaced by the outputs. In the case of
a weighted grammar, weights are criteria to select
between several concurrent analyses. A criterion
on the length of the recognized sequences can also
be used.
For more complex processes, a variant of this
functionality produces an automaton correspond-
ing to the original text automaton with new transi-
tions tagged with the grammar outputs. This pro-
cess is easily iterable and can then be used for
incremental recognition and annotation of longer
and longer segments. It can also complete the mor-
phosyntactic tagging for the recognition of semi-
frozen lexical units, whose variations are too com-
plex to be enumerated in dictionaries, but can be
easily described in local grammars.
Also included is a deep syntactic parser based
on unification grammars in the decorated WRTN
formalism (Blanc and Constant, 2005). This for-
malism combines WRTN formalism with func-
tional equations on feature structures. Therefore,
complex syntactic phenomena, such as the extrac-
tion of a grammatical element or the resolution of
some co-references, can be formalized. In addi-
tion, the result of the parsing is also a shared for-
est of syntactic trees. Each tree is associated with a
feature structure where are represented grammati-
cal relations between syntactical constituents that
have been identified during parsing.
6 Linguistic Resource Management
The reuse of LRs requires flexibility: a lexicon or a
grammar is not a static resource. The management
of lexicons and grammars implies manual con-
struction and maintenance of resources in a read-
able format, and compilation of these resources in
an operational format. These techniques require
strong collaborations between computer scientists
and linguists; few systems provide such function-
ality (Xelda, Intex, Unitex). The Outilex platform
provides a complete set of management tools for
LRs. For instance, the platform offers an inflection
module. This module takes a lexicon of lemmas
with syntactic tags as input associated with inflec-
tion rules. It produces a lexicon of inflected words
associated with morphosyntactic features. In order
to accelerate word tagging, these lexicons are then
indexed on their inflected forms by using a mini-
mal finite state automaton representation (Revuz,
1991) that allows for both fast look-up procedure
and dictionary compression.
75
7 Conclusion
The Outilex platform in its current version pro-
vides all fundamental operations for text pro-
cessing: processing without lexicon, lexicon and
grammar exploitation and LR management. Data
are structured both in standard XML formats and
in more compact ones. Format converters are in-
cluded in the platform. The WRTN formalism al-
lows for combining statistical methods with meth-
ods based on LRs. The development of the plat-
form required expertise both in computer science
and in linguistics. It took into account both needs
in fundamental research and applications. In the
future, we hope the platform will be extended to
other languages and will be enriched with new
functionality.
References
Olivier Blanc and Matthieu Constant. 2005. Lexi-
calization of grammars with parameterized graphs.
In Proc. of RANLP 2005, pages 117?121, Borovets,
Bulgarie, September. INCOMA Ltd.
Olivier Blanc and Anne Dister. 2004. Automates lexi-
caux avec structure de traits. In Actes de RECITAL,
pages 23?32.
Olivier Blanc, Matthieu Constant, and ?Eric Laporte.
2006. Outilex, plate-forme logicielle de traitements
de textes e?crits. In Ce?drick Fairon and Piet Mertens,
editors, Actes de TALN 2006 (Traitement automa-
tique des langues naturelles), page to appear, Leu-
ven. ATALA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Lionel Cle?ment and ?Eric de la Clergerie. 2005. MAF:
a morphosyntactic annotation framework. In Proc.
of the Language and Technology Conference, Poz-
nan, Poland, pages 90?94.
Hamish Cunningham. 2002. GATE, a general archi-
tecture for text engineering. Computers and the Hu-
manities, 36:223?254.
Laurence Danlos. 2005. Automatic recognition of
French expletive pronoun occurrences. In Compan-
ion Volume of the International Joint Conference
on Natural Language Processing, Jeju, Korea, page
2013.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Comm. ACM, 13(2):94?102.
Maurice Gross. 1993. Local grammars and their rep-
resentation by finite automata. In M. Hoey, editor,
Data, Description, Discourse, Papers on the English
Language in honour of John McH Sinclair, pages
26?38. Harper-Collins, London.
Cvetana Krstev, Dus?ko Vitas, Denis Maurel, and
Mickae?l Tran. 2005. Multilingual ontology of
proper names. In Proc. of the Language and Tech-
nology Conference, Poznan, Poland, pages 116?119.
Edward Loper and Steven Bird. 2002. NLTK: the nat-
ural language toolkit. In Proc. of the ACL Workshop
on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational
Linguistics, Philadelphia.
Oliver Mason. 2004. Automatic processing of lo-
cal grammar patterns. In Proc. of the 7th Annual
CLUK (the UK special-interest group for computa-
tional linguistics) Research Colloquium.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1998. A rational design for a weighted finite-state
transducer library. Lecture Notes in Computer Sci-
ence, 1436.
Takuya Nakamura. 2005. Analysing texts in a specific
domain with local grammars: The case of stock ex-
change market reports. In Linguistic Informatics -
State of the Art and the Future, pages 76?98. Ben-
jamins, Amsterdam/Philadelphia.
Se?bastien Paumier. 2003. De la reconnaissance de
formes linguistiques a` l?analyse syntaxique. Volume
2, Manuel d?Unitex. Ph.D. thesis, IGM, Universite?
de Marne-la-Valle?e.
Thierry Poibeau. 2001. Extraction d?information dans
les bases de donne?es textuelles en ge?nomique au
moyen de transducteurs a` e?tats finis. In Denis Mau-
rel, editor, Actes de TALN 2001 (Traitement automa-
tique des langues naturelles), pages 295?304, Tours,
July. ATALA, Universite? de Tours.
Dominique Revuz. 1991. Dictionnaires et lexiques:
me?thodes et alorithmes. Ph.D. thesis, Universite?
Paris 7.
Javier M. Sastre. 2005. XML-based representation
formats of local grammars for NLP. In Proc. of
the Language and Technology Conference, Poznan,
Poland, pages 314?317.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Max Silberztein. 1993. Dictionnaires e?lectroniques et
analyse automatique de textes. Le syste`me INTEX.
Masson, Paris. 234 p.
William A. Woods. 1970. Transition network gram-
mars for natural language analysis. Communica-
tions of the ACM, 13(10):591?606.
76
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1875?1885, Dublin, Ireland, August 23-29 2014.
Syntactic Parsing and Compound Recognition via Dual Decomposition:
Application to French
Joseph Le Roux
1
and Matthieu Constant
2
and Antoine Rozenknop
1
(1) LIPN, Universit? Paris 13 ? Sorbonne Paris Cit?, CNRS
(2) LIGM, Universit? Paris Est, CNRS
leroux@univ-paris13.fr, mconstan@univ-mlv.fr, antoine.rozenknop@lipn.univ-paris13.fr
Abstract
In this paper we show how the task of syntactic parsing of non-segmented texts, including com-
pound recognition, can be represented as constraints between phrase-structure parsers and CRF
sequence labellers. In order to build a joint system we use dual decomposition, a way to com-
bine several elementary systems which has proven successful in various NLP tasks. We evaluate
this proposition on the French SPMRL corpus. This method compares favorably with pipeline
architectures and improves state-of-the-art results.
1 Introduction
Dual decomposition (DD), which can be used as a method to combine several elementary systems, has
already been successfully applied to many NLP tasks, in particular syntactic parsing, see (Rush et al.,
2010; Koo et al., 2010) inter alia. Intuitively, the principle can be described quite simply: at decoding
time, the combined systems seek for a consensus on common subtasks, in general the prediction of some
parts of the overall structure, via an iterative process imposing penalties where the systems disagree. If
the systems converge to a solution, it is formally guaranteed to be optimal. Besides, this approach is
quite flexible and easy to implement. One can add or remove elementary systems without rebuilding
the architecture from the ground up. Moreover, the statistical models for the subsystems can often be
estimated independently at training time.
In this paper we show how syntactic parsing of unsegmented texts, integrating compound recognition,
can be represented by constraints between phrase-structure parsers and sequence labellers, either for
compound recognition or part-of-speech (POS) tagging, and solved using DD. We compare this approach
experimentally with pipeline architectures: our system demonstrates state-of-the-art performance. While
this paper focuses on French, the approach is generic and can be applied to any treebank with compound
information, and more generally to tasks combining segmentation and parsing.
This paper is structured as follows. First, we describe the data we use to build our elementary systems.
Second, we present related work in compound recognition, in particular for French, and the type of
information one is able to incorporate in tag sets. Third, we show how CRF-based sequence labellers with
these different tag sets can be combined using DD to obtain an efficient decoding algorithm. Fourth, we
extend our method to add phrase-structure parsers in the combination. Finally, we empirically evaluate
these systems and compare them with pipeline architectures.
2 Data
We use the phrase-structure treebank released for the SPMRL 2013 shared task (Seddah et al., 2013).
This corresponds to a new version of the French Treebank (Abeill? et al., 2003). One of the key dif-
ferences between French data and other treebanks of the shared task is the annotation of compounds.
Compounds are sequences of words with a certain degree of semantic non-compositionality. They form
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1875
a single lexical unit to which one can assign a single POS. In the SPRML corpus 15% of the tokens be-
long to a compound, or 12.7% if we omit numerals: the training, development and test sets respectively
comprise 23658, 2120 and 4049 compounds.
In the treebank, compounds are annotated as subtrees whose roots are labelled with the POS of the
compounds with a + suffix. Each leaf under a compound is the daughter of its own POS tag, which is in
turn the daughter of the root of the compound. For example, the tree in Figure 1 contains a subtree with
the compound adverb pour l?instant (so far) whose category ADV+ dominates the preposition pour, the
determiner l?, and the noun instant.
SENT
PONCT
.
VN
VP
bloqu?e
ADV
compl?tement
V
est
NP-SUJ
NC
situation
DET
la
PONCT
,
ADV+
NC
instant
DET
l?
P
Pour
Figure 1: Syntatic annotation in the SPRML FTB: So far, the situation has been completely blocked.
The sequence labellers used in the experiments are able to exploit external lexical resources that will
help coping with data missing from the training corpus. These resources are dictionaries, consisting of
triplets (flexed form, lemma, POS tag), where form and lemmamay be compound or simple.
Several such dictionaries exist for French. We use:
? DELA (Courtois et al., 1997) contains a million entries, among which 110,000 are compounds;
? Lefff (Sagot, 2010) contains 500,000 entries, among which 25,000 are compounds;
? Prolex (Piton et al., 1999) is a toponym dictionary with approximately 100,000 entries.
The described resources are additional to the SPMRL shared task data (Seddah et al., 2013), but were
also used in (Constant et al., 2013a) for the shared task.
3 Compound Recognition
3.1 Related Work
The compound recognition traditionally relies on 2 types of information: lexical and syntactic clues. A
strong lexical association between the tokens of a compound can be detected using a compound dictio-
nary or by measuring a degree of relatedness, which can be learnt on a corpus. Some recent approaches
use sequence labellers. The linear chain CRF model (Lafferty et al., 2001) is widely used, see for ex-
ample (Vincze et al., 2011; Constant and Tellier, 2012). It has proven to be a very adequate model: it
is flexible enough to incorporate information from labelled data and external resources (POS taggers,
compound lexicons or named entity recognisers).
The compound recognition may also be directly performed by syntactic parsers learnt from corpora
where compounds are marked, such as the one we use in this paper
1
(Arun and Keller, 2005; Green et al.,
2011; Green et al., 2013; Constant et al., 2013b), but these results are contradictory. Green et al. (2011)
experimentally show that a lexicalised model is better than an unlexicalised one. On the other hand,
Constant et al. (2013b) show that, using products of PCFG-LA (Petrov, 2010), unlexicalised models can
be more accurate. They obtain performance on a par with a linear chain CRF system without external
information. But such information is difficult to incorporate directly in a PCFG-LA model. Constant et
al. (2012) resort to a reranker to add arbitrary features in the parse selection process, but their system
showed inferior performance compared with a CRF model with access to the same external information.
1
Such an approach has been used already for joint named entity recognition and parsing based on CRF (Finkel and Manning,
2009).
1876
3.2 Annotation schemes
Compound recognition can be seen as a segmentation task which consists in assigning to each token a
label with segmentation information. We use label B if the token is the beginning of a word (single or
compound), and label I if the token is inside a compound, but not in initial position. This lexical seg-
mentation can be enriched with additional information, for example POS tags of compounds or tokens in
compounds, and gives a variety of tag sets. This leads us to define 5 simple tag sets for our problem, each
with very simple information, that will be combined in the next section. These tag sets are exemplified
on a sentence with the compound vin rouge (red wine).
1. (basic) recognition with two labels (B and I)
Luc/B aime/B le/B vin/B rouge/I (Luc likes red wine)
2. (partial) recognition with compound POS tags: [BI]-POS for tokens in compounds; B for others
Luc/B aime/B le/B vin/B-NC+ rouge/I-NC+
3. (partial-internal) recognition with token POS tags in compounds
Luc/B aime/B le/B vin/B-NC rouge/I-ADJ
4. (complete) recognition with POS tags for all tokens; in compounds use compound POS tags
Luc/B-NPP aime/B-V le/B-DET vin/B-NC+ rouge/I-NC+
5. (complete-internal) recognition with POS tags for all tokens; in compounds use token POS tags
Luc/B-NPP aime/B-V le/B-DET vin/B-NC rouge/I-ADJ
4 Dual decomposition for compound recognition using CRFs
4.1 A maximisation problem
4.1.1 CRF
A conditional random field (Lafferty et al., 2001), or CRF, is a tuple c = (?,L
c
, w
c
, {f
c
p
}
p
) which
defines the conditional probability of a sequence of labels y ? L
?
c
given a sequence of words of the same
length x ? ?
?
as a logistic regression of the form:
P
c
(y|x) =
exp
(
?
p?P(x)
w
c
? f
c
p
(x, y
p
)
)
Z(w
c
, x)
, where (1)
? w
c
? R
d
is a d-dimensional weight vector, where d is the number of features of the system,
? Z is the partition function
? P(x) is a set of places, in our case the set of unigram and bigram decompositions of sequences
of words. A place p is of the form [i]
x
for unigrams and [i, i + 1]
x
for bigrams. We omit x when
context is unambiguous.
? y
p
is the restriction of y to the place p, and we will write y
i
for y
[i]
and y
i
y
i+1
for y
[i,i+1]
? f
c
p
is the feature function for the place p that projects (x, y
p
) on R
d
.
Our goal is to find the best labelling, i.e. the one that maximises the conditional probability given a
sequence of tokens. One can observe that this labelling also maximises the numerator of Equation 1, as
Z(w
c
, x) does not depend on y. We therefore write:
y?
c
= argmax
y
?
c
(x, y) = argmax
y
?
p?P(x)
w
c
? f
c
p
(x, y
p
) (2)
1877
4.1.2 Viterbi Algorithm for CRFs
Since our combination of CRF systems relies on the Viterbi algorithm, we review it briefly. For a given
input sentence x = x
1
. . . x
n
, we represent the problem of finding the best labelling with a CRF c as a
best path algorithm in a directed acyclic graph G
c
= (V, E) built from a set of nodes V and a set of edges
E . Nodes are pairs (x
i
, l) where x
i
is an input token and l is an admissible label for x
i
.
2
Edges connect
nodes of the form (x
i
, l) to nodes of the form (x
i+1
, l
?
) and the weights of these arcs are given by c. In
order to find the weight of the best path in this graph, that corresponds to the score of the best labelling,
we use Algorithm 1.
3
One can remark that the score of a node decomposes as a score s
1
, computed from
a vector of unigram features, written f
c
[i]
(x, l), and a score s
2
computed from a vector of bigram features,
written f
c
[i?1,i]
(x, l
?
, l).
4
The Viterbi algorithm has a time complexity linear in the length of the sentence
and quadratic in the number of labels of the CRF.
Algorithm 1 Viterbi Algorithm for CRFs with unigram and bigram features
1: Viterbi(G
c
, w
c
, {f
c
p
}
p
,?
BI
,?
IB
):
2: for all node v do
3: ?[v] = ??
4: end for
5: ?[ (<s>,START)] = w ? f
p
0
(x, START)
6: for all non initial node v = (x
i
, l) in topological order do
7: s
1
? w
c
? f
c
[i]
(x, l)
8: s
2
? ??
9: for all incoming edge v
?
= (x
i?1
, l
?
)? v do
10: t? ?[v
?
] + w
c
? f
c
[i?1,i]
(x, l
?
, l)
11: t? t? b(l
?
)i(l) ? ?
BI
[i]? i(l
?
)b(l) ? ?
IB
[i] ? only for DD: we ignore this line otherwise
12: if t > s
2
then
13: s
2
= t
14: end if
15: end for
16: ?[v]? s
1
+ s
2
17: end for
18: return the best scoring path ?[ (</s>,STOP) ]
4.2 Dual decomposition for CRF combinations
In Section 3.2 we described several annotation schemes that lead to different CRF models. These
schemes give the same lexical segmentation information but they use more or less rich part-of-speech
tag sets. It is not clear a priori if the richness of the tag set has a beneficial effect over segmentation
prediction: there is a compromise between linguistic informativeness and data sparseness. Instead of
trying to find the best annotation scheme, we propose a consensus-based joint system between several
CRF-based sequence labellers for the task of text segmentation relying on dual decomposition (Rush
et al., 2010). This system maximises the sum of the scores of combined CRFs, while enforcing global
consistency between systems in terms of constraints over the admissible solutions. These constraints are
specifically realised as reparametrisations of the elementary CRFs until a consensus is reached. Since
we deal with several annotation schemes, we will use predicates to abstract from them:
? b(l) is true if l starts with B;
? i(l) is true if l starts with I;
? bi(i, y) is true if b(y
i?1
) and i(y
i
) are true;
? ib(i, y) is true if i(y
i?1
) and b(y
i
) are true.
For a labelling y, we define 2 boolean vectors that indicate where the compounds begin and end:
2
We also include two additional nodes: an initial state (<s>,START) and a final state (</s>, STOP).
3
Algorithm 1 calculates the score and backpointers must be added to retrieve the corresponding path.
4
This algorithm takes as input 2 vectors that will be used for DD and will be explained in ? 4.2. They can be ignored now.
1878
? D(y), such that D(y)[i] = 1 if bi(i, y), and 0 otherwise;
? F (y), such that F (y)[i] = 1 if ib(i, y), and 0 otherwise.
As we want to combine CRFs, the solution of our system will be a tuple of label sequences with the
same compound segmentation. For an input sequence x, this new maximisation problem is:
(P ) : find max
(y
1
,...,y
n
)
n
?
c=1
?
c
(y
c
) =
n
?
c=1
?
p?P(x)
w
c
? f
c
p
(x, y
c
p
) (3)
s.t. ?u
1
, u
2
?c ? J1, nK, D(y
c
) = u
1
, F (y
c
) = u
2
(4)
Objective (3) indicates that we seek for a tuple for which the sum of the scores of its elements is
maximal. Constraints (4) imply that the compound frontiers ? transitions B to I and I to B ? must be
the same for each element of the tuple. There are several ways to tackle this problem. The first one
is by swapping the sum signs in (3) and noticing that the problem could then be represented by a joint
system relying on dynamic programming ? a CRF for which labels would be elements of the product
L = L
1
? ? ? ? ? L
n
? and for which it is straightforward to define a weight vector and feature functions.
We can therefore reuse the Viterbi algorithm but the complexity is quadratic in the size of L, which is
impractical
5
.
In any case, this approach would be inadequate for inclusion of parsers, and we therefore rely on
lagrangian relaxation. We modify the objective by introducing Lagrange multipliers, two real vectors
?
BI
c
and ?
IB
c
indexed by bigram places
6
for each CRF c of the combination. We obtain a new problem
with the same solution as the previous one, since constraints (4) are garanteed to be satisfied at optimality:
max
(y
1
,...,y
n
,u
1
,u
2
)
min
(?
BI
,?
IB
)
n
?
c=1
?
c
(y
c
) ?
n
?
c=1
[
(D(y
c
) ? u
1
) ? ?
BI
c
+ (F (y
c
) ? u
2
) ? ?
IB
c
]
(5)
The next step is dualisation, which gives an upper bound of our problem:
min
(?
BI
,?
IB
)
max
(y
1
,...,y
n
,u
1
,u
2
)
n
?
c=1
?
c
(y
c
)?
n
?
c=1
D(y
c
) ??
BI
c
+u
1
n
?
c=1
?
BI
c
?
n
?
c=1
F (y
c
) ??
IB
c
+u
2
n
?
c=1
?
IB
c
(6)
We then remark that
?
n
c=1
?
BI
c
and
?
n
c=1
?
IB
c
must be zeros at optimum (if the problem is feasible).
7
It is convenient to convert this remark to hard constraints in order to remove any reference to vectors u
i
? and therefore to the coupling contraints ? from the objective. We obtain the constrained problem with
the same optimal solution :
(Du) : find min
(?
BI
,?
IB
)
n
?
c=1
max
y
c
[
?
c
(y
c
) ? D(y
c
) ? ?
BI
c
? F (y
c
) ? ?
IB
c
]
(7)
s.t.
n
?
c=1
?
BI
c
= 0 and
n
?
c=1
?
IB
c
= 0 (8)
In order to solve (Du) we use the projected subgradient descent method that has already been used
in many problems, for example MRF decoding (Komodakis et al., 2007). For the problem at hand, this
method gives Algorithm 2. This iterative algorithm consists in reparametrising the elementary CRFs of
the system, by modifying the weights associated with the bigram features in places that correspond to
compound frontiers, penalising them on CRFs that diverge from the average solution. This is performed
5
One could object that some combinations are forbidden. It remains that the number of labels still grows exponentially.
6
Bigram places are identified by their second position.
7
Otherwise the sum expressions would be unbounded and their maximum is +? for an appropriate value of u
i
.
1879
by amending the vectors ?
BI
c
and ?
IB
c
that are updated at each iteration proportionally to the difference
between the feature vectors for c and the average values of these vectors. Hence the farther a solution
is from the consensus, the more penalised it gets at the next iteration. This algorithm stops when the
updates are null for all CRFs: in this case the consensus is reached.
Algorithm 2 Best segmentation with combined CRF system via subgradient descent
Require: n CRF c = (?,L
c
, w
c
, {f
c
p
}
p
), an input sentence x, a maximum number of iterations ? , stepsizes {?
t
}
0?t??
1: for all CRF c, bigram end position i, bigram label pair (l,m) do
2: ?
BI
c
[i, l,m]
(0)
= 0; ?
IB
c
[i, l,m]
(0)
= 0
3: end for
4: for t = 0? ? do
5: for all CRF c do
6: y
c
(t)
= V iterbi(G
c
, w
c
, f
c
,?
BI
(t)
c
,?
IB
(t)
c
)
7: end for
8: for all CRF c do
9: ?
BI
(t)
c
? ?
t
(
D
(
y
c
(t)
)
?
?
1?d?n
D
(
y
d
(t)
)
n
)
; ?
IB
(t)
c
? ?
t
(
F
(
y
c
(t)
)
?
?
1?d?n
F
(
y
d
(t)
)
n
)
10: ?
BI
(t+1)
c
? ?
BI
(t)
c
+ ?
BI
(t)
c
; ?
IB
(t+1)
c
? ?
IB
(t)
c
+ ?
IB
(t)
c
11: end for
12: if ?
BI
(t)
c
= 0 and ?
IB
(t)
c
= 0 for all c then
13: Exit loop
14: end if
15: end for
16: return (y
1
(t)
, ? ? ? , y
n
(t)
)
We set the maximum number of iteration ? to 1000. For the step size, we use a common heuristic:
?
t
=
1
1+k
where k is the number of times that (Du) has increased between two successive iterations.
4.3 Experimental results for CRF combinations
We modified the wapiti software (Lavergne et al., 2010) with Algorithm 2. Table 1 reports segmen-
tation results on the development set with the different tag sets, the best DD combination, and the best
voting system.
8
Tag Set CRF / combination Recall Precision F-score
partial-internal 79.59 85.49 82.44
partial 78.98 85.57 82.14
basic 79.74 84.65 82.12
complete 79.69 83.10 81.36
complete-internal 79.03 82.66 80.80
MWE basic complete partial-internal 80.82 86.07 83.36
vote (basic complete partial-internal) 80.49 85.46 82.90
Table 1: Segmentation scores of CRF systems (dev)
System F-score (all) F-score (compounds)
complete 94.29 78.32
MWE 94.59 80.00
Table 2: Segmentation + POS tagging (dev)
We see that the best system is a combination of 3 CRFs (tag sets basic, complete and partial-internal)
with DD, that we call MWE in the remaining of the paper. The subgradient descent converges on all
instances in 2.14 iterations on average. The DD combination is better than the voting system.
We can also evaluate the POS tagging accuracy of the system for systems including the complete
tag set. We compare the results of the complete CRF with the MWE combination on Table 2. The second
column gives the F-score of the complete task, segmentation and POS tagging. The third column restricts
the evaluation to compounds. Again, the MWE combination outperforms the single system.
8
Each system has one vote and in case of a draw, we pick the best system?s decision.
1880
In some preliminary experiments, the weights of the CRF systems were based on unigram features
mainly ? i.e. those described in (Constant et al., 2012). As our CRFs are constrained on transitions from
B to I and I to B, penalising systems resulted in modifying (low) bigram weights and had only a minor
effect on the predictions and consequently the projected gradient algorithm was slow to converge. We
therefore added bigram templates for some selected unigram templates, so that our system can converge
in a reasonable time. Adding these bigram features resulted in slower elementary CRFs. On average the
enriched CRFs were 1.8 times slower that their preliminary counterparts.
5 Dual Decomposition to combine parsers and sequence labellers
We now present an extension of the previous method to incorporate phrase-structure parsers in the com-
bination. Our approach relies on the following requirement for the systems to agree: if the parser predicts
a compound between positions i and j, then the CRFs must predict compound frontiers at the same po-
sitions. In this definition, like in previous CRF combinations, only the positions are taken into account,
not the grammatical categories. From a parse tree a, we define two feature vectors:
? D(a), such that D(a)[i] = 1 if a contains a subtree for a compound starting at position i ? 1
? F (a), such that F (a)[i] = 1 if a contains a subtree for a compound ending at position i ? 1
In other words, D(a)[i] indicates whether the CRFs should label position i ? 1 with B and position i
with I, while F (a)[i] indicates whether the CRFs should label position i ? 1 with I and position i with
B. See Figure 2 for an example.
NP
ADJ
vol?e
B
NC+
ADJ
bleue
I
NC
carte
B
DET
une
B
Figure 2: Parser and CRF alignments (A stolen credit card)
5.1 Parsing with probabilistic context-free grammars
We follow the type of reasoning we used in ? 4.2. With a PCFG g, we can define the score of a parse for
an input sentence x as the logarithm of the probability assigned to this parse by g. Finding the best parse
takes a form analogous to the one in Equation 2, and we can write the CKY algorithm as a best path
algorithm with penalties on nodes, as we did for the Viterbi algorithm previously. This is closely related
to the PCFG combinations of (Le Roux et al., 2013). We introduce penalties through two real vectors
?
BI
and ?
IB
indexed by compound positions. The modified CKY is presented in Algorithm 3
9
where
the parse forest F is assumed to be already available and we note w the vector of rule log-probabilities.
5.2 System combination
As in ? 4.2, our problem amounts to finding a tuple that now consists of a parse tree and several labellings.
All systems must agree on compound frontiers. Our objective is:
(P
?
) : find max
(a,y
1
,...,y
n
)
?
p
(a) + ?
n
?
c=1
?
c
(y
c
) (9)
s.t. ?u
1
, u
2
?c ? J1, nK, D(y
c
) = u
1
, F (y
c
) = u
2
, D(a) = u
1
, F (a) = u
2
(10)
9
Without loss of generality, only binary rules are taken into account.
1881
Algorithm 3 CKY with node penalties for compound start/end positions
1: CKY(F , w,?
BI
,?
IB
):
2: for all node v in the forest F do
3: ?[v] = ??
4: end for
5: for all leaf node x do
6: ?[x] = 0
7: end for
8: for all non-terminal node (A, i, j) in topological order do
9: for all incoming hyperedge u = (B, i, k)(C, k + 1, j)? (A, i, j) do
10: s? ?[(B, i, k)] + ?[(C, k + 1, j)] + w
A?BC
? w
A?BC
is the score for rule A? BC
11: if A is a compound label then
12: s? s? ?
BI
[i]? ?
IB
[j + 1]
13: end if
14: if s > ?[(A, i, j)] then
15: ?[(A, i, j)]? s
16: end if
17: end for
18: end for
19: return hyperpath with score ?[(ROOT, 1, n)]
We use ? to set the relative weights of the CRFs and the PCFG. It will be tuned on the develop-
ment set. We then reuse the same procedure as before: lagrangian relaxation, dualisation, and projected
subgradient descent. Algorithm 4 presents the function we derive from these operations.
Algorithm 4 Find the best segmentation with a PCFG and CRFs
Require: a PCFG parser p, n CRFs, an input sentence x, a bound ?
1: set Lagrange multipliers (penalty vectors) to zero
2: for t = 0? ? do
3: for all CRF c do
4: y
c
(t)
? V iterbi(G
c
, w
c
, f
c
,?
BI
(t)
c
,?
IB
(t)
c
)
5: end for
6: a
(t)
? CKY (F , w,?
BI
(t)
p
,?
IB
(t)
p
)
7: for all CRF c and parser p do
8: Update penalty vectors proportionally to the difference between corresponding solution and average solution
9: end for
10: if update is null for all c and p then
11: Exit loop
12: end if
13: end for
14: return (a
(t)
, y
1
(t)
, ? ? ? , y
n
(t)
)
Algorithm 4 follows the method used in ? 4 and simply adds the PCFG parser as another subsystem.
This method can then be extended further: for instance, we can add a POS tagger (Rush et al., 2010) or
multiple PCFG parsers (Le Roux et al., 2013). Due to lack of space, we omit the presentation of these
systems, but we experiment with them in the following section.
6 Experiments
For this series of experiments, we used wapiti as in ? 4.3 and the LORG PCFG-LA parser in the
configuration presented in (Le Roux et al., 2013) that we modified by implementing Algorithm 4. This
parser already implements a combination of parsers based on DD, a very competitive baseline.
For parse evaluation, we used the EVALB tool, modified by the SPMRL 2013 organisers, in order
to compare our results with the shared task participants. We evaluated several configurations: (i) the
LORG parser alone, a combination of 4 PCFG-LA parsers as in (Le Roux et al., 2013), (ii) a pipeline
of POS, a CRF-based POS tagger, and LORG, (iii) joint LORG and POS, using DD as in (Rush et al.,
2010), (iv) joint LORG and MWE (our best CRF combination for compound segmentation) using DD, and
(v) joint LORG, POS et MWE using DD. We also compare these architectures with 2 additional pipelines,
in which we first run MWE and then merge compounds as single tokens. The converted sentences are
then sent to a version of LORG learnt on this type of corpus. After parsing, compounds are unmerged,
1882
replaced with the corresponding subtree. In one of these two architectures, we add a POS tagger.
The evaluations for the parsing task of all these configurations are summarised in Table 3. The best
system is the DD joint system combining the POS tagger, the parser and the compound recognisers.
System Recall Precision Fscore EX Tag
LORG 82.01 82.37 82.19 18.06 97.35
pipeline POS? LORG 82.36 82.59 82.47 19.22 97.73
DD POS + LORG 82.48 82.73 82.61 19.19 97.84
DD MWE + LORG 82.91 83.07 82.99 19.19 97.41
DD POS + MWE + LORG 83.38 83.42 83.40 20.73 97.85
pipeline MWE MERGE? LORG? UNMERGE 82.56 82.63 82.59 18.79 97.39
pipeline MWE MERGE/POS? LORG? UNMERGE 82.73 82.64 82.69 20.02 97.57
Table 3: Parse evaluation on dev set (recall, precision and F-score, exactness and POS tagging).
Table 4 shows evaluation results of our best system and comparisons with baseline or alternative
configurations on the SPMRL 2013 test set.
Parsing The DD method performs better than our baseline, and better than the best system in the
SPMRL 2013 shared task (Bj?rkelund et al., 2013). This system is a pipeline consisting of a
morpho-syntactic tagger with a very rich and informative tag set, a product of PCFG-LAs, and
a parse reranker. Although this approach is quite different from ours, we believe our system is more
accurate overall because our method is more resilient to an error from one of its components.
Compound recognition and labelling For the task of recognition alone, where only the frontiers are
evaluated, the DD combinations of CRFs performs better than the best single CRF which itself
performs better than the parser alone, but the complete architecture is again the best system. If we
also evaluate compound POS tags, we get similar results. The DD combination is always beneficial.
System Recall Precision Fscore EX Tag
LORG 82.79 83.06 82.92 22.00 97.39
(Bj?rkelund et al., 2013) ? ? 82.86 ? ?
DD POS + MWE + LORG 83.74 83.85 83.80 23.81 97.87
compound recognition LORG 78.03 78.63 78.49 ? ?
compound recognition best single CRF (partial-internal) 78.27 82.84 80.49 ? ?
compound recognition MWE 79.68 83.50 81.54 ? ?
compound recognition DD POS + MWE + LORG 80.76 84.19 82.44 ? ?
compound recognition + POS tagging LORG 75.43 75.71 75.57 ? ?
compound recognition + POS tagging MWE 76.49 80.10 78.28 ? ?
compound recognition + POS tagging DD POS + MWE + LORG 77.92 81.23 79.54 ? ?
Table 4: Evaluation on SPMRL 2013 test set: parsing (first 3 lines), and compound recognition.
7 Conclusion
We have presented an original architecture for the joint task of syntactic parsing and compound recogni-
tion. We first introduced a combination of recognisers based on linear chain CRFs, and a second system
that adds in a phrase-structure parser. Our experimental prototype improves state-of-the-art on the French
SPMRL corpus.
In order to derive decoding algorithms for these joint systems, we used dual decomposition. This
approach, leading to simple and efficient algorithms, can be extended further to incorporate additional
components. As opposed to pipeline approaches, a component prediction can be corrected if its solution
is too far from the general consensus. As opposed to joint systems relying on pure dynamic programming
to build a complex single system, the search space does not grow exponentially, so we can avoid using
pruning heuristics such as beam search. The price to pay is an iterative algorithm.
Finally, this work paves the way towards component-based NLP software systems that perform com-
plex processing based on consensus between components, as opposed to previous pipelined approaches.
1883
Acknowledgements
We would like to thank Nadi Tomeh and Davide Buscaldi for their feedback on an early draft of this
paper, as well as the three anonymous reviewers for their helpful comments. This work is supported by
a public grant overseen by the French National Research Agency (ANR) as part of the Investissements
d?Avenir program (ANR-10-LABX-0083).
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel. 2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization in crosslinguistic probabilistic parsing: The case of French.
In Proceedings of the Annual Meeting of the Association For Computational Linguistics (ACL?05), pages 306?
313.
Anders Bj?rkelund, Rich?rd Farkas, Thomas M?ller, and Wolfgang Seeker. 2013. (re) ranking meets morphosyn-
tax: State-of-the-art results from the spmrl 2013 shared task. In Proceedings of the 4th Workshop on Statistical
Parsing of Morphologically Rich Languages: Shared Task.
Matthieu Constant and Isabelle Tellier. 2012. Evaluating the impact of external lexical resources into a CRF-based
multiword segmenter and part-of-speech tagger. In Proceedings of the 8th conference on Language Resources
and Evaluation.
Matthieu Constant, Anthony Sigogne, and Patrick Watrin. 2012. Discriminative strategies to integrate multi-
word expression recognition and parsing. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL?12), pages 204?212.
Matthieu Constant, Marie Candito, and Djam? Seddah. 2013a. The LIGM-Alpage Architecture for the SPMRL
2013 Shared Task: Multiword Expression Analysis and Dependency Parsing. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich Languages: Shared Task, Seattle, WA.
Matthieu Constant, Joseph Le Roux, and Anthony Sigogne. 2013b. Combining compound recognition and PCFG-
LA parsing with word lattices and conditional random fields. ACM Transaction in Speech and Language Pro-
cessing, 10(3).
Blandine Courtois, Myl?ne Garrigues, Gaston Gross, Maurice Gross, Ren? Jung, Michel Mathieu-Colas, Anne
Monceaux, Anne Poncet-Montange, Max Silberztein, and Robert Viv?s. 1997. Dictionnaire ?lectronique
DELAC : les mots compos?s binaires. Technical Report 56, University Paris 7, LADL.
Jenny Rose Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proceed-
ings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, NAACL ?09, pages 326?334, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Spence Green, Marie-Catherine de Marneffe, John Bauer, and Christopher D. Manning. 2011. Multiword expres-
sion identification with tree substitution grammars: A parsing tour de force with french. In Proceedings of the
conference on Empirical Method for Natural Language Processing (EMNLP?11), pages 725?735.
Spence Green, Marie-Catherine de Marneffe, and Christopher D Manning. 2013. Parsing models for identifying
multiword expressions. Computational Linguistics, 39(1):195?227.
Nikos Komodakis, Nikos Paragios, and Georgios Tziritas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1?8. IEEE.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition
for parsing with non-projective head automata. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on
Machine Learning.
1884
Thomas Lavergne, Olivier Capp?, and Fran?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL?10), pages 504?513.
Joseph Le Roux, Antoine Rozenknop, and Jennifer Foster. 2013. Combining PCFG-LA models with dual de-
composition: A case study with function labels and binarization. In Association for Computational Linguistics,
editor, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, October.
Slav Petrov. 2010. Products of random latent variable grammars. In Proceedings of the Annual Conference of the
North American Chapter of the Association for Computational Linguistics - Human Language Technologies,
pages 19?27.
Odile Piton, Denis Maurel, and Claude Belleil. 1999. The Prolex Data Base : Toponyms and gentiles for
NLP. In Proceedings of the Third International Workshop on Applications of Natural Language to Data Bases
(NLDB?99), pages 233?237.
Alexander Rush, David Sontag, Michael Collins, and Tommi Jaakola. 2010. On dual decomposition and linear
programming relaxations for natural language processing. In ACL, editor, Proceedings of the Conference on
Empirical Methods in Natural Language Processing. Association for Computational Linguistics.
Beno?t Sagot. 2010. The lefff, a freely available, accurate and large-coverage lexicon for french. In Proceedings
of the 7th International Conference on Language Resources and Evaluation.
Djam? Seddah, Reut Tsarfaty, Sandra K
?
?ubler, Marie Candito, Jinho Choi, Rich?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier,
Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin
Woli?nski, Alina Wr?blewska, and Eric Villemonte de la Cl?rgerie. 2013. Overview of the spmrl 2013 shared
task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the 4th
Workshop on Statistical Parsing of Morphologically Rich Languages, Seattle, WA.
Veronica Vincze, Istv?n Nagy, and G?bor Berend. 2011. Multiword expressions and named entities in the wiki50
corpus. In Proceedings of the conference on Recent Advances in Natural Language Processing (RANLP?11),
pages 289?295.
1885
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 204?212,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Discriminative Strategies to Integrate Multiword Expression Recognition
and Parsing
Matthieu Constant
Universite? Paris-Est
LIGM, CNRS
France
mconstan@univ-mlv.fr
Anthony Sigogne
Universite? Paris-Est
LIGM, CNRS
France
sigogne@univ-mlv.fr
Patrick Watrin
Universite? de Louvain
CENTAL
Belgium
patrick.watrin
@uclouvain.be
Abstract
The integration of multiword expressions in a
parsing procedure has been shown to improve
accuracy in an artificial context where such
expressions have been perfectly pre-identified.
This paper evaluates two empirical strategies
to integrate multiword units in a real con-
stituency parsing context and shows that the
results are not as promising as has sometimes
been suggested. Firstly, we show that pre-
grouping multiword expressions before pars-
ing with a state-of-the-art recognizer improves
multiword recognition accuracy and unlabeled
attachment score. However, it has no statis-
tically significant impact in terms of F-score
as incorrect multiword expression recognition
has important side effects on parsing. Sec-
ondly, integrating multiword expressions in
the parser grammar followed by a reranker
specific to such expressions slightly improves
all evaluation metrics.
1 Introduction
The integration of Multiword Expressions (MWE)
in real-life applications is crucial because such ex-
pressions have the particularity of having a certain
level of idiomaticity. They form complex lexical
units which, if they are considered, should signifi-
cantly help parsing.
From a theoretical point of view, the integra-
tion of multiword expressions in the parsing pro-
cedure has been studied for different formalisms:
Head-Driven Phrase Structure Grammar (Copestake
et al, 2002), Tree Adjoining Grammars (Schuler
and Joshi, 2011), etc. From an empirical point of
view, their incorporation has also been considered
such as in (Nivre and Nilsson, 2004) for depen-
dency parsing and in (Arun and Keller, 2005) in con-
stituency parsing. Although experiments always re-
lied on a corpus where the MWEs were perfectly
pre-identified, they showed that pre-grouping such
expressions could significantly improve parsing ac-
curacy. Recently, Green et al (2011) proposed in-
tegrating the multiword expressions directly in the
grammar without pre-recognizing them. The gram-
mar was trained with a reference treebank where
MWEs were annotated with a specific non-terminal
node.
Our proposal is to evaluate two discriminative
strategies in a real constituency parsing context:
(a) pre-grouping MWE before parsing; this would
be done with a state-of-the-art recognizer based
on Conditional Random Fields; (b) parsing with
a grammar including MWE identification and then
reranking the output parses thanks to a Maxi-
mum Entropy model integrating MWE-dedicated
features. (a) is the direct realistic implementation of
the standard approach that was shown to reach the
best results (Arun and Keller, 2005). We will evalu-
ate if real MWE recognition (MWER) still positively
impacts parsing, i.e., whether incorrect MWER does
not negatively impact the overall parsing system.
(b) is a more innovative approach to MWER (de-
spite not being new in parsing): we select the final
MWE segmentation after parsing in order to explore
as many parses as possible (as opposed to method
(a)). The experiments were carried out on the French
Treebank (Abeille? et al, 2003) where MWEs are an-
notated.
204
The paper is organized as follows: section 2 is
an overview of the multiword expressions and their
identification in texts; section 3 presents the two dif-
ferent strategies and their associated models; sec-
tion 4 describes the resources used for our exper-
iments (the corpus and the lexical resources); sec-
tion 5 details the features that are incorporated in the
models; section 6 reports on the results obtained.
2 Multiword expressions
2.1 Overview
Multiword expressions are lexical items made up
of multiple lexemes that undergo idiosyncratic con-
straints and therefore offer a certain degree of id-
iomaticity. They cover a wide range of linguistic
phenomena: fixed and semi-fixed expressions, light
verb constructions, phrasal verbs, named entities,
etc. They may be contiguous (e.g. traffic light) or
discontinuous (e.g. John took your argument into
account). They are often divided into two main
classes: multiword expressions defined through lin-
guistic idiomaticity criteria (lexicalized phrases in
the terminology of Sag et al (2002)) and those de-
fined by statistical ones (i.e. simple collocations).
Most linguistic criteria used to determine whether a
combination of words is a MWE are based on syn-
tactic and semantic tests such as the ones described
in (Gross, 1986). For instance, the utterance at night
is a MWE because it does display a strict lexical
restriction (*at day, *at afternoon) and it does not
accept any inserting material (*at cold night, *at
present night). Such linguistically defined expres-
sions may overlap with collocations which are the
combinations of two or more words that cooccur
more often than by chance. Collocations are usu-
ally identified through statistical association mea-
sures. A detailed description of MWEs can be found
in (Baldwin and Nam, 2010).
In this paper, we focus on contiguous MWEs that
form a lexical unit which can be marked by a part-of-
speech tag (e.g. at night is an adverb, because of is a
preposition). They can undergo limited morphologi-
cal and lexical variations ? e.g. traffic (light+lights),
(apple+orange+...) juice ? and usually do not al-
low syntactic variations1 such as inserts (e.g. *at
1Such MWEs may very rarely accept inserts, often limited
to single word modifiers: e.g. in the short term, in the very short
cold night). Such expressions can be analyzed at the
lexical level. In what follows, we use the term com-
pounds to denote such expressions.
2.2 Identification
The idiomaticity property of MWEs makes them
both crucial for Natural Language Processing appli-
cations and difficult to predict. Their actual iden-
tification in texts is therefore fundamental. There
are different ways for achieving this objective. The
simpler approach is lexicon-driven and consists in
looking the MWEs up in an existing lexicon, such
as in (Silberztein, 2000). The main drawback is
that this procedure entirely relies on a lexicon and
is unable to discover unknown MWEs. The use
of collocation statistics is therefore useful. For in-
stance, for each candidate in the text, Watrin and
Franc?ois (2011) compute on the fly its association
score from an external ngram base learnt from a
large raw corpus, and tag it as MWE if the associa-
tion score is greater than a threshold. They reach ex-
cellent scores in the framework of a keyword extrac-
tion task. Within a validation framework (i.e. with
the use of a reference corpus annotated in MWEs),
Ramisch et al (2010) developped a Support Vector
Machine classifier integrating features correspond-
ing to different collocation association measures.
The results were rather low on the Genia corpus
and Green et al (2011) confirmed these bad results
on the French Treebank. This can be explained by
the fact that such a method does not make any dis-
tinctions between the different types of MWEs and
the reference corpora are usually limited to certain
types of MWEs. Furthermore, the lexicon-driven
and collocation-driven approaches do not take the
context into account, and therefore cannot discard
some of the incorrect candidates. A recent trend is
to couple MWE recognition with a linguistic ana-
lyzer: a POS tagger (Constant and Sigogne, 2011)
or a parser (Green et al, 2011). Constant and Si-
gogne (2011) trained a unified Conditional Random
Fields model integrating different standard tagging
features and features based on external lexical re-
sources. They show a general tagging accuracy of
94% on the French Treebank. In terms of Multi-
word expression recognition, the accuracy was not
term.
205
clearly evaluated, but seemed to reach around 70-
80% F-score. Green et al (2011) proposed to in-
clude the MWER in the grammar of the parser. To
do so, the MWEs in the training treebank were anno-
tated with specific non-terminal nodes. They used a
Tree Substitution Grammar instead of a Probabilis-
tic Context-free Grammar (PCFG) with latent anno-
tations in order to capture lexicalized rules as well
as general rules. They showed that this formalism
was more relevant to MWER than PCFG (71% F-
score vs. 69.5%). Both methods have the advantage
of being able to discover new MWEs on the basis
of lexical and syntactic contexts. In this paper, we
will take advantage of the methods described in this
section by integrating them as features of a MWER
model.
3 Two strategies, two discriminative
models
3.1 Pre-grouping Multiword Expressions
MWER can be seen as a sequence labelling task
(like chunking) by using an IOB-like annotation
scheme (Ramshaw and Marcus, 1995). This implies
a theoretical limitation: recognized MWEs must be
contiguous. The proposed annotation scheme is
therefore theoretically weaker than the one proposed
by Green et al (2011) that integrates the MWER in
the grammar and allows for discontinuous MWEs.
Nevertheless, in practice, the compounds we are
dealing with are very rarely discontinuous and if so,
they solely contain a single word insert that can be
easily integrated in the MWE sequence. Constant
and Sigogne (2011) proposed to combine MWE seg-
mentation and part-of-speech tagging into a single
sequence labelling task by assigning to each token a
tag of the form TAG+X where TAG is the part-of-
speech (POS) of the lexical unit the token belongs to
and X is either B (i.e. the token is at the beginning
of the lexical unit) or I (i.e. for the remaining posi-
tions): John/N+B hates/V+B traffic/N+B jams/N+I.
In this paper, as our task consists in jointly locating
and tagging MWEs, we limited the POS tagging to
MWEs only (TAG+B/TAG+I), simple words being
tagged by O (outside): John/O hates/O traffic/N+B
jams/N+I.
For such a task, we used Linear chain Conditional
Ramdom Fields (CRF) that are discriminative prob-
abilistic models introduced by Lafferty et al (2001)
for sequential labelling. Given an input sequence of
tokens x = (x1, x2, ..., xN ) and an output sequence
of labels y = (y1, y2, ..., yN ), the model is defined
as follows:
P?(y|x) =
1
Z(x)
.
N?
t
K?
k
log?k.fk(t, yt, yt?1, x)
where Z(x) is a normalization factor depending
on x. It is based on K features each of them be-
ing defined by a binary function fk depending on
the current position t in x, the current label yt, the
preceding one yt?1 and the whole input sequence
x. The tokens xi of x integrate the lexical value
of this token but can also integrate basic properties
which are computable from this value (for example:
whether it begins with an upper case, it contains a
number, its tags in an external lexicon, etc.). The
feature is activated if a given configuration between
t, yt, yt?1 and x is satisfied (i.e. fk(t, yt, yt?1, x) =
1). Each feature fk is associated with a weight ?k.
The weights are the parameters of the model, to be
estimated. The features used for MWER will be de-
scribed in section 5.
3.2 Reranking
Discriminative reranking consists in reranking the n-
best parses of a baseline parser with a discriminative
model, hence integrating features associated with
each node of the candidate parses. Charniak and
Johnson (2005) introduced different features that
showed significant improvement in general parsing
accuracy (e.g. around +1 point in English). For-
mally, given a sentence s, the reranker selects the
best candidate parse p among a set of candidates
P (s) with respect to a scoring function V?:
p? = argmaxp?P (s)V?(p)
The set of candidates P (s) corresponds to the n-best
parses generated by the baseline parser. The scor-
ing function V? is the scalar product of a parameter
vector ? and a feature vector f :
V?(p) = ?.f(p) =
m?
j=1
?j .fj(p)
where fj(p) corresponds to the number of occur-
rences of the feature fj in the parse p. According to
206
Charniak and Johnson (2005), the first feature f1 is
the probability of p provided by the baseline parser.
The vector ? is estimated during the training stage
from a reference treebank and the baseline parser
ouputs.
In this paper, we slightly deviate from the original
reranker usage, by focusing on improving MWER
in the context of parsing. Given the n-best parses,
we want to select the one with the best MWE seg-
mentation by keeping the overall parsing accuracy as
high as possible. We therefore used MWE-dedicated
features that we describe in section 5. The training
stage was performed by using a Maximum entropy
algorithm as in (Charniak and Johnson, 2005).
4 Resources
4.1 Corpus
The French Treebank2 [FTB] (Abeille? et al, 2003)
is a syntactically annotated corpus made up of jour-
nalistic articles from Le Monde newspaper. We
used the latest edition of the corpus (June 2010)
that we preprocessed with the Stanford Parser pre-
processing tools (Green et al, 2011). It contains
473,904 tokens and 15,917 sentences. One benefit of
this corpus is that its compounds are marked. Their
annotation was driven by linguistic criteria such as
the ones in (Gross, 1986). Compounds are identified
with a specific non-terminal symbol ?MWX? where
X is the part-of-speech of the expression. They have
a flat structure made of the part-of-speech of their
components as shown in figure 1.
MWN


HH
H
N
part
P
de
N
marche?
Figure 1: Subtree of MWE part de marche? (market
share): The MWN node indicates that it is a multiword
noun; it has a flat internal structure N P N (noun ? pre-
prosition ? noun)
The French Treebank is composed of 435,860 lex-
ical units (34,178 types). Among them, 5.3% are
compounds (20.8% for types). In addition, 12.9%
2http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-
fr.php
of the tokens belong to a MWE, which, on average,
has 2.7 tokens. The non-terminal tagset is composed
of 14 part-of-speech labels and 24 phrasal ones (in-
cluding 11 MWE labels). The train/dev/test split is
the same as in (Green et al, 2011): 1,235 sentences
for test, 1,235 for development and 13,347 for train-
ing. The development and test sections are the same
as those generally used for experiments in French,
e.g. (Candito and Crabbe?, 2009).
4.2 Lexical resources
French is a resource-rich language as attested by
the existing morphological dictionaries which in-
clude compounds. In this paper, we use two large-
coverage general-purpose dictionaries: Dela (Cour-
tois, 1990; Courtois et al, 1997) and Lefff (Sagot,
2010). The Dela was manually developed in the
90?s by a team of linguists. We used the distribution
freely available in the platform Unitex3 (Paumier,
2011). It is composed of 840,813 lexical entries in-
cluding 104,350 multiword ones (91,030 multiword
nouns). The compounds present in the resources re-
spect the linguistic criteria defined in (Gross, 1986).
The lefff is a freely available dictionary4 that has
been automatically compiled by drawing from dif-
ferent sources and that has been manually validated.
We used a version with 553,138 lexical entries in-
cluding 26,311 multiword ones (22,673 multiword
nouns). Their different modes of acquisition makes
those two resources complementary. In both, lexical
entries are composed of a inflected form, a lemma,
a part-of-speech and morphological features. The
Dela has an additional feature for most of the mul-
tiword entries: their syntactic surface form. For in-
stance, eau de vie (brandy) has the feature NDN be-
cause it has the internal flat structure noun ? prepo-
sition de ? noun.
In order to compare compounds in these lexical
resources with the ones in the French Treebank, we
applied on the development corpus the dictionar-
ies and the lexicon extracted from the training cor-
pus. By a simple look-up, we obtained a prelimi-
nary lexicon-based MWE segmentation. The results
are provided in table 1. They show that the use of
external resources may improve recall, but they lead
3http://igm.univ-mlv.fr/?unitex
4http://atoll.inria.fr/?sagot/lefff.html
207
to a decrease in precision as numerous MWEs in the
dictionaries are not encoded as such in the reference
corpus; in addition, the FTB suffers from some in-
consistency in the MWE annotations.
T L D T+L T+D T+L+D
recall 75.9 31.7 59.0 77.3 83.4 84.0
precision 61.2 52.0 55.6 58.7 51.2 49.9
f-score 67.8 39.4 57.2 66.8 63.4 62.6
Table 1: Simple context-free application of the lexical
resources on the development corpus: T is the MWE lex-
icon of the training corpus, L is the lefff, D is the Dela.
The given scores solely evaluate MWE segmentation and
not tagging.
In terms of statistical collocations, Watrin and
Franc?ois (2011) described a system that lists all the
potential nominal collocations of a given sentence
along with their association measure. The authors
provided us with a list of 17,315 candidate nominal
collocations occurring in the French treebank with
their log-likelihood and their internal flat structure.
5 MWE-dedicated Features
The two discriminative models described in sec-
tion 3 require MWE-dedicated features. In order to
make these models comparable, we use two compa-
rable sets of feature templates: one adapted to se-
quence labelling (CRF-based MWER) and the other
one adapted to reranking (MaxEnt-based reranker).
The MWER templates are instantiated at each posi-
tion of the input sequence. The reranker templates
are instantiated only for the nodes of the candidate
parse tree, which are leaves dominated by a MWE
node (i.e. the node has a MWE ancestor). We define
a template T as follows:
? MWER: for each position n in the input se-
quence x,
T = f(x, n)/yn
? RERANKER: for each leaf (in position n)
dominated by a MWE node m in the current
parse tree p,
T = f(p, n)/label(m)/pos(p, n)
where f is a function to be defined; yn is the out-
put label at position n; label(m) is the label of node
m and pos(p, n) indicates the position of the word
corresponding to n in the MWE sequence: B (start-
ing position), I (remaining positions).
5.1 Endogenous Features
Endogenous features are features directly extracted
from properties of the words themselves or from a
tool learnt from the training corpus (e.g. a tagger).
Word n-grams. We use word unigrams and bigrams
in order to capture multiwords present in the training
section and to extract lexical cues to discover new
MWEs. For instance, the bigram coup de is often
the prefix of compounds such as coup de pied (kick),
coup de foudre (love at first sight), coup de main
(help).
POS n-grams. We use part-of-speech unigrams
and bigrams in order to capture MWEs with irreg-
ular syntactic structures that might indicate the id-
iomacity of a word sequence. For instance, the POS
sequence preposition ? adverb associated with the
compound depuis peu (recently) is very unusual in
French. We also integrated mixed bigrams made up
of a word and a part-of-speech.
Specific features. Due to their different use, each
model integrates some specific features. In order to
deal with unknown words and special tokens, we in-
corporate standard tagging features in the CRF: low-
ercase forms of the words, word prefixes of length 1
to 4, word suffice of length 1 to 4, whether the word
is capitalized, whether the token has a digit, whether
it is an hyphen. We also add label bigrams. The
reranker models integrate features associated with
each MWE node, the value of which is the com-
pound itself.
5.2 Exogenous Features
Exogenous features are features that are not entirely
derived from the (reference) corpus itself. They are
computed from external data (in our case, our lexical
resources). The lexical resources might be useful to
discover new expressions: usually, expressions that
have standard syntax like nominal compounds and
are difficult to predict from the endogenous features.
The resources are applied to the corpus through a
lexical analysis that generates, for each sentence, a
finite-state automaton TFSA which represents all the
possible analyses. The features are computed from
the automaton TFSA.
Lexicon-based features. We associate each word
with its part-of-speech tags found in our external
morphological lexicon. All tags of a word constitute
208
an ambiguity class ac. If the word belongs to a com-
pound, the compound tag is also incorporated in the
ambiguity class. For instance, the word night (either
a simple noun or a simple adjective) in the context at
night, is associated with the class adj noun adv+I as
it is located inside a compound adverb. This feature
is directly computed from TFSA. The lexical anal-
ysis can lead to a preliminary MWE segmentation
by using a shortest path algorithm that gives priority
to compound analyses. This segmentation is also a
source of features: a word belonging to a compound
segment is assigned different properties such as the
segment part-of-speech mwt and its syntactic struc-
turemws encoded in the lexical resource, its relative
position mwpos in the segment (?B? or ?I?).
Collocation-based features. In our collocation re-
source, each candidate collocation of the French
treebank is associated with its internal syntactic
structure and its association score (log-likelihood).
We divided these candidates into two classes: those
whose score is greater than a threshold and the other
ones. Therefore, a given word in the corpus can be
associated with different properties whether it be-
longs to a potential collocation: the class c and the
internal structure cs of the collocation it belongs to,
its position cpos in the collocation (B: beginning; I:
remaining positions; O: outside). We manually set
the threshold to 150 after some tuning on the devel-
opment corpus.
All feature templates are given in table 2.
Endogenous Features
w(n+ i), i ? {?2,?1, 0, 1, 2}
w(n+ i)/w(n+ i+ 1), i ? {?2,?1, 0, 1}
t(n+ i), i ? {?2,?1, 0, 1, 2}
t(n+ i)/t(n+ i+ 1), i ? {?2,?1, 0, 1}
w(n+ i)/t(n+ j), (i, j) ? {(1, 0), (0, 1), (?1, 0), (0,?1)}
Exogenous Features
ac(n)
mwt(n)/mwpos(n)
mws(n)/mwpos(n)
c(n)/cs(n)/cpos(n)
Table 2: Feature templates (f ) used both in the MWER
and the reranker models: n is the current position in the
sentence, w(i) is the word at position i; t(i) is the part-
of-speech tag of w(i); if the word at absolute position i
is part of a compound in the Shortest Path Segmentation,
mwt(i) and mws(i) are respectively the part-of-speech
tag and the internal structure of the compound,mwpos(i)
indicates its relative position in the compound (B or I).
6 Evaluation
6.1 Experiment Setup
We carried out 3 different experiments. We first
tested a standalone MWE recognizer based on CRF.
We then combined MWE pregrouping based on
this recognizer and the Berkeley parser5 (Petrov
et al, 2006) trained on the FTB where the com-
pounds were concatenated (BKYc). Finally, we
combined the Berkeley parser trained on the FTB
where the compounds are annotated with specific
non-terminals (BKY), and the reranker. In all exper-
iments, we varied the set of features: endo are all en-
dogenous features; coll and lex include all endoge-
nous features plus collocation-based features and
lexicon-based ones, respectively; all is composed of
both endogenous and exogenous features. The CRF
recognizer relies on the software Wapiti6 (Lavergne
et al, 2010) to train and apply the model, and on
the software Unitex (Paumier, 2011) to apply lexical
resources. The part-of-speech tagger used to extract
POS features was lgtagger7 (Constant and Sigogne,
2011). To train the reranker, we used a MaxEnt al-
gorithm8 as in (Charniak and Johnson, 2005).
Results are reported using several standard mea-
sures, the F1score, unlabeled attachment and Leaf
Ancestor scores. The labeled F1score [F1]9, de-
fined by the standard protocol called PARSEVAL
(Black et al, 1991), takes into account the brack-
eting and labeling of nodes. The unlabeled attache-
ment score [UAS] evaluates the quality of unlabeled
5We used the version adapted to French in
the software Bonsai (Candito and Crabbe?, 2009):
http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html.
The original version is available at:
http://code.google.com/p/berkeleyparser/. We trained the
parser as follows: right binarization, no parent annotation, six
split-merge cycles and default random seed initialisation (8).
6Wapiti can be found at http://wapiti.limsi.fr/. It was con-
figured as follows: rprop algorithm, default L1-penalty value
(0.5), default L2-penalty value (0.00001), default stopping cri-
terion value (0.02%).
7Available at http://igm.univ-
mlv.fr/?mconstan/research/software/.
8We used the following mathematical libraries PETSc et
TAO, freely available at http://www.mcs.anl.gov/petsc/ and
http://www.mcs.anl.gov/research/projects/tao/
9Evalb tool available at http://nlp.cs.nyu.edu/evalb/. We
also used the evaluation by category implemented in the class
EvalbByCat in the Stanford Parser.
209
dependencies between words of the sentence10. And
finally, the Leaf-Ancestor score [LA]11 (Sampson,
2003) computes the similarity between all paths (se-
quence of nodes) from each terminal node to the root
node of the tree. The global score of a generated
parse is equal to the average score of all terminal
nodes. Punctuation tokens are ignored in all met-
rics. The quality of MWE identification was evalu-
ated by computing the F1 score on MWE nodes. We
also evaluated the MWE segmentation by using the
unlabeled F1 score (U). In order to compare both ap-
proaches, parse trees generated by BKYc were auto-
matically transformed in trees with the same MWE
annotation scheme as the trees generated by BKY.
In order to establish the statistical significance of
results between two parsing experiments in terms of
F1 and UAS, we used a unidirectional t-test for two
independent samples12. The statistical significance
between two MWE identification experiments was
established by using the McNemar-s test (Gillick
and Cox, 1989). The results of the two experiments
are considered statistically significant with the com-
puted value p < 0.01.
6.2 Standalone Multiword recognition
The results of the standalone MWE recognizer are
given in table 3. They show that the lexicon-based
system (lex) reaches the best score. Accuracy is im-
proved by an absolute gain of +6.7 points as com-
pared with BKY parser. The strictly endogenous
system has a +4.9 point absolute gain, +5.4 points
when collocations are added. That shows that most
of the work is done by fully automatically acquired
features (as opposed to features coming from a man-
ually constructed lexicon). As expected, lexicon-
based features lead to a 5.3 point recall improve-
ment (with respect to non-lexicon based features)
whereas precision is stable. The more precise sys-
tem is the base one because it almost solely detects
compounds present in the training corpus; neverthe-
less, it is unable to capture new MWEs (it has the
10This score is computed by using the tool available at
http://ilk.uvt.nl/conll/software.html. The constituent trees are
automatically converted into dependency trees with the tool
Bonsai.
11Leaf-ancestor assessment tool available at
http://www.grsampson.net/Resources.html
12Dan Bikel?s tool available at
http://www.cis.upenn.edu/?dbikel/software.html.
lowest recall). BKY parser has the best recall among
the non lexicon-based systems, i.e. it is the best one
to discover new compounds as it is able to precisely
detect irregular syntactic structures that are likely to
be MWEs. Nevertheless, as it does not have a lex-
icalized strategy, it is not able to filter out incorrect
candidates; the precision is therefore very low (the
worst).
P R F1 F1 ? 40 U
base 78.0 68.3 72.8 71.2 74.3
endo 75.5 74.5 75.0 74.0 76.3
coll 76.6 74.4 75.5 74.9 77.0
lex 76.0 79.8 77.8 77.8 79.0
all 76.2 79.2 77.7 77.3 78.8
BKY 67.6 75.1 71.1 70.7 72.5
Stanford* - - - 70.1 -
DP-TSG* - - - 71.1 -
Table 3: MWE identification with CRF: base are the
features corresponding to token properties and word n-
grams. The differences between all systems are statisti-
cally significant with respect to McNemar?s test (Gillick
and Cox, 1989), except lex/all and all/coll;
lex/coll is ?border-line?. The results of the systems
based on the Stanford Parser and the Tree Substitution
Parser (DP-TSG) are reported from (Green et al, 2011).
6.3 Combination of Multiword Expression
Recognition and Parsing
We tested and compared the two proposed dis-
criminative strategies by varying the sets of MWE-
dedicated features. The results are reported in ta-
ble 4. Table 5 compares the parsing systems, by
showing the score differences between each of the
tested system and the BKY parser.
Strat. Feat. Parser F1 LA UAS F1(MWE)
- - BKY 80.61 92.91 82.99 71.1
pre - BKYc 75.47 91.10 76.74 0.0
pre endo BKYc 80.23 92.69 83.62 74.9
pre coll BKYc 80.32 92.73 83.77 75.5
pre lex BKYc 80.66 92.81 84.16 77.4
pre all BKYc 80.51 92.77 84.05 77.2
post endo BKY 80.87 92.94 83.49 72.9
post coll BKY 80.71 92.85 83.16 71.2
post lex BKY 81.08 92.98 83.98 74.5
post all BKY 81.03 92.96 83.97 74.3
pre gold BKYc 83.73 93.77 90.08 95.8
Table 4: Parsing evaluation: pre indicates a MWE pre-
grouping strategy, whereas post is a reranking strategy
with n = 50. The feature gold means that we have ap-
plied the parser on a gold MWE segmentation.
210
?F1 ?UAS ?F1(MWE)
pre post pre post pre post
endo -0.38 +0.26 +0.63 +0.50 +3.8 +1.8
coll -0.29 +0.10 +0.78 +0.17 +4.4 +0.1
lex +0.05 +0.47 +1.17 +0.99 +6.3 +3.4
Table 5: Comparison of the strategies with respect to
BKY parser.
Firstly, we note that the accuracy of the best re-
alistic parsers is much lower than that of a parser
with a golden MWE segmentation13 (-2.65 and -5.92
respectively in terms of F-score and UAS), which
shows the importance of not neglecting MWE recog-
nition in the framework of parsing. Furthermore,
pre-grouping has no statistically significant impact
on the F-score14, whereas reranking leads to a sta-
tistically significant improvement (except for col-
locations). Both strategies also lead to a statisti-
cally significant UAS increase. Whereas both strate-
gies improve the MWE recognition, pre-grouping
is much more accurate (+2-4%); this might be due
to the fact that an unlexicalized parser is limited in
terms of compound identification, even within n-
best analyses (cf. Oracle in table 6). The benefits of
lexicon-based features are confirmed in this experi-
ment, whereas the use of collocations in the rerank-
ing strategy seems to be rejected.
endo coll lex all oracle
n=1 80.61
(71.1)
n=5 80.74 80.88 81.03 81.05 83.17
(71.5) (71.7) (73.4) (73.3) (74.6)
n=20 80.98 80.72 81.09 81.01 84.76
(72.9) (70.6) (73.6) (73.0) (75.5)
n=50 80.87 80.71 81.08 81.03 85.21
(72.9) (71.2) (74.5) (74.3) (76.4)
n=100 80.69 80.53 81.12 80.93 85.54
(72.0) (70.0) (74.4) (73.7) (76.4)
Table 6: Reranker F1 evaluation with respect to n and the
types of features. The F1(MWE) is given in parenthesis.
Table 7 shows the results by category. It indi-
cates that both discriminative strategies are of in-
terest in locating multiword adjectives, determiners
and prepositions; the pre-grouping method appears
to be particularly relevant for multiword nouns and
13The F1(MWE) is not 100% with a golden segmentation be-
cause of tagging errors by the parser.
14Note that we observe an increase of +0.5 in F1 on the de-
velopment corpus with lexicon-based features.
adverbs. However, it performs very poorly in multi-
word verb recognition. In terms of standard parsing
accuracy, the pre-grouping approach has a very het-
erogeneous impact: Adverbial and Adjective Modi-
fier phrases tend to be more accurate; verbal kernels
and higher level constituents such as relative and
subordinate clauses see their accuracy level drop,
which shows that pre-recognition of MWE can have
a negative impact on general parsing accuracy as
MWE errors propagate to higher level constituents.
cat #gold BKY endo lex endo lex
(pre) (pre) (post) (post)
MWET 4 0.0 N/A N/A N/A N/A
MWA 22 37.2 +15.2 +21.3 +0.9 +4.7
MWV 47 62.1 -9.7 -13.2 +1.7 +2.5
MWD 24 62.1 +7.3 +10.2 0.0 +1.2
MWN 860 68.2 +4.0 +7.0 +1.7 +4.2
MWADV 357 72.1 +3.8 +6.4 +3.4 +4.1
MWPRO 31 84.2 -3.5 -0.9 0.0 0.0
MWP 294 79.1 +4.3 +5.8 +0.4 +1.1
MWC 86 85.7 +0.9 +3.7 +0.2 +1.0
Sint 209 47.2 -7.7 -8.7 +0.1 -0.2
AdP 86 48.8 +1.2 +3.0 +3.4 +5.1
Ssub 406 60.8 -1.1 -1.1 -0.3 -0.5
VPpart 541 63.2 -2.8 -2.1 -0.5 -1.6
Srel 408 74.8 -3.4 -3.5 -0.3 -0.6
VPinf 781 75.2 0.0 -0.1 -0.3 -0.3
COORD 904 75.2 +0.2 +0.4 -0.3 -0.4
PP 4906 76.7 -0.8 -0.3 +0.5 +0.7
AP 1482 74.5 +3.2 +3.9 +0.7 +1.6
NP 9023 79.8 -1.1 -0.8 +0.1 +0.2
VN 3089 94.0 -2.0 -1.0 0.0 0.0
Table 7: Evaluation by category with respect to BKY
parser. The BKY column indicates the F1 of BKY parser.
7 Conclusions and Future Work
In this paper, we evaluated two discriminative strate-
gies to integrate Multiword Expression Recognition
in probabilistic parsing: (a) pre-grouping MWEs
with a state-of-the-art recognizer and (b) MWE
identification with a reranker after parsing. We
showed that MWE pre-grouping significantly im-
proves compound recognition and unlabeled depen-
dency annotation, which implies that this strategy
could be useful for dependency parsing. The rerank-
ing procedure evenly improves all evaluation scores.
Future work could consist in combining both strate-
gies: pre-grouping could suggest a set of potential
MWE segmentations in order to make it more flexi-
ble for a parser; final decisions would then be made
by the reranker.
211
Acknowlegments
The authors are very grateful to Spence Green for his
useful help on the treebank, and to Jennifer Thewis-
sen for her careful proof-reading.
References
A. Abeille? and L. Cle?ment and F. Toussenel. 2003.
Building a treebank for French. Treebanks. In A.
Abeille? (Ed.). Kluwer. Dordrecht.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of French. In
ACL.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos, B.
Santorini and T. Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic coverage of En-
glish grammars. In Proceedings of the DARPA Speech
and Natural Language Workshop.
T. Baldwin and K.S. Nam. 2010. Multiword Ex-
pressions. Handbook of Natural Language Process-
ing, Second Edition. CRC Press, Taylor and Francis
Group.
M. -H. Candito and B. Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. Proceedings of IWPT 2009.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05).
M. Constant and A. Sigogne. 2011. MWU-aware Part-
of-Speech Tagging with a CRF model and lexical re-
sources. In Proceedings of the Workshop on Multi-
word Expressions: from Parsing and Generation to the
Real World (MWE?11).
A. Copestake, F. Lambeau, A. Villavicencio, F. Bond,
T. Baldwin, I. Sag, D. Flickinger. 2002. Multi-
word Expressions: Linguistic Precision and Reusabil-
ity. Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC 2002).
B. Courtois. 1990. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise. Vol. 87.
B. Courtois, M. Garrigues, G. Gross, M. Gross, R.
Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-
Montange, M. Silberztein and R. Vive?s. 1997. Dic-
tionnaire e?lectronique DELAC : les mots compose?s bi-
naires. Technical Report. n. 56. LADL, University
Paris 7.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP?89.
S. Green, M.-C. de Marneffe, J. Bauer and C. D. Man-
ning. 2011. Multiword Expression Identification with
Tree Substitution Grammars: A Parsing tour de force
with French. In Empirical Method for Natural Lan-
guage Processing (EMNLP?11).
M. Gross. 1986. Lexicon Grammar. The Representa-
tion of Compound Words. In Proceedings of Compu-
tational Linguistics (COLING?86).
J. Lafferty and A. McCallum and F. Pereira. 2001. Con-
ditional random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML 2001).
T. Lavergne, O. Cappe? and F. Yvon. 2010. Practical Very
Large Scale CRFs. In ACL.
J. Nivre and J. Nilsson. 2004. Multiword units in syntac-
tic parsing. In Methodologies and Evaluation of Mul-
tiword Units in Real-World Applications (MEMURA).
S. Paumier. 2011. Unitex 3.9 documentation.
http://igm.univ-mlv.fr/?unitex.
S. Petrov, L. Barrett, R. Thibaux and D. Klein. 2006.
Learning accurate, compact and interpretable tree an-
notation. In ACL.
C. Ramisch, A. Villavicencio and C. Boitet. 2010. mwe-
toolkit: a framework for multiword expression identi-
fication. In LREC.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the 3rd Workshop on Very Large Corpora.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake and D.
Flickinger. 2002. Multiword Expressions: A Pain in
the Neck for NLP. In CICLING 2002. Springer.
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC?10).
G. Sampson and A. Babarczy. 2003. A test of the leaf-
ancestor metric for parsing accuracy. Natural Lan-
guage Engineering. Vol. 9 (4).
Seddah D., Candito M.-H. and Crabb B. 2009. Cross-
parser evaluation and tagset variation: a French tree-
bank study. Proceedings of International Workshop
on Parsing Technologies (IWPT?09).
W. Schuler, A. Joshi. 2011. Tree-rewriting models of
multi-word expressions. Proceedings of the Workshop
on Multiword Expressions: from Parsing and Genera-
tion to the Real World (MWE?11).
M. Silberztein. 2000. INTEX: an FST toolbox. Theoret-
ical Computer Science, vol. 231(1).
P. Watrin and T. Franc?ois. 2011. N-gram frequency
database reference to handle MWE extraction in NLP
applications. In Proceedings of the 2011 Workshop on
MultiWord Expressions.
212
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 743?753,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Strategies for Contiguous Multiword Expression Analysis and
Dependency Parsing
Marie Candito
Alpage
Paris Diderot Univ
INRIA
marie.candito@
linguist.univ-paris-diderot.fr
Matthieu Constant
Universite? Paris-Est
LIGM
CNRS
Matthieu.Constant@
u-pem.fr
Abstract
In this paper, we investigate various strate-
gies to predict both syntactic dependency
parsing and contiguous multiword expres-
sion (MWE) recognition, testing them on
the dependency version of French Tree-
bank (Abeille? and Barrier, 2004), as in-
stantiated in the SPMRL Shared Task
(Seddah et al, 2013). Our work focuses
on using an alternative representation of
syntactically regular MWEs, which cap-
tures their syntactic internal structure. We
obtain a system with comparable perfor-
mance to that of previous works on this
dataset, but which predicts both syntactic
dependencies and the internal structure of
MWEs. This can be useful for capturing
the various degrees of semantic composi-
tionality of MWEs.
1 Introduction
A real-life parsing system should comprise the
recognition of multi-word expressions (MWEs1),
first because downstream semantic-oriented ap-
plications need some marking in order to dis-
tinguish between regular semantic composition
and the typical semantic non-compositionality of
MWEs. Second, MWE information, is intuitively
supposed to help parsing.
That intuition is confirmed in a classical but
non-realistic setting in which gold MWEs are pre-
grouped (Arun and Keller, 2005; Nivre and Nils-
son, 2004; Eryig?it et al, 2011). But the situation
is much less clear when switching to automatic
MWE prediction. While Cafferkey et al (2007)
report a small improvement on the pure parsing
1Multiword expressions can be roughly defined as con-
tinuous or discontinuous sets of tokens, which either do not
exhibit full freedom in lexical selection or whose meaning is
not fully compositional. We focus in this paper on contiguous
multiword expressions, also known as ?words with spaces?.
task when using external MWE lexicons to help
English parsing, Constant et al (2012) report re-
sults on the joint MWE recognition and parsing
task, in which errors in MWE recognition allevi-
ate their positive effect on parsing performance.
While the realistic scenario of syntactic pars-
ing with automatic MWE recognition (either done
jointly or in a pipeline) has already been investi-
gated in constituency parsing (Green et al, 2011;
Constant et al, 2012; Green et al, 2013), the
French dataset of the SPMRL 2013 Shared Task
(Seddah et al, 2013) only recently provided the
opportunity to evaluate this scenario within the
framework of dependency syntax.2 In such a sce-
nario, a system predicts dependency trees with
marked groupings of tokens into MWEs. The
trees show syntactic dependencies between se-
mantically sound units (made of one or several
tokens), and are thus particularly appealing for
downstream semantic-oriented applications, as de-
pendency trees are considered to be closer to
predicate-argument structures.
In this paper, we investigate various strate-
gies for predicting from a tokenized sentence
both MWEs and syntactic dependencies, using the
French dataset of the SPMRL 13 Shared Task. We
focus on the use of an alternative representation
for those MWEs that exhibit regular internal syn-
tax. The idea is to represent these using regular
syntactic internal structure, while keeping the se-
mantic information that they are MWEs.
We devote section 2 to related work. In sec-
tion 3, we describe the French dataset, how MWEs
are originally represented in it, and we present
and motivate an alternative representation. Sec-
tion 4 describes the different architectures we test
2The main focus of the Shared Task was on pre-
dicting both morphological and syntactic analysis for
morphologically-rich languages. The French dataset is the
only one containing MWEs: the French treebank has the
particularity to contain a high ratio of tokens belonging to
a MWE (12.7% of non numerical tokens).
743
for predicting both syntax and MWEs. Section 5
presents the external resources targeted to improve
MWE recognition. We describe experiments and
discuss their results in section 6 and conclude in
section 7.
2 Related work
We gave in introduction references to previous
work on predicting MWEs and constituency pars-
ing. To our knowledge, the first works3 on predict-
ing both MWEs and dependency trees are those
presented to the SPMRL 2013 Shared Task that
provided scores for French (which is the only
dataset containing MWEs). Constant et al (2013)
proposed to combine pipeline and joint systems in
a reparser (Sagae and Lavie, 2006), and ranked
first at the Shared Task. Our contribution with
respect to that work is the representation of the
internal syntactic structure of MWEs, and use of
MWE-specific features for the joint system. The
system of Bjo?rkelund et al (2013) ranked second
on French, though with close UAS/LAS scores. It
is a less language-specific system that reranks n-
best dependency parses from 3 parsers, informed
with features from predicted constituency trees. It
uses no feature nor treatment specific to MWEs as
it focuses on the general aim of the Shared Task,
namely coping with prediction of morphological
and syntactic analysis.
Concerning related work on the representa-
tion of MWE internal structure, we can cite the
Prague Dependency Bank, which captures both
regular syntax of non-compositional MWEs and
their MWE status, in two distinct annotation lay-
ers (Bejc?ek and Stranak, 2010). Our represen-
tation also resembles that of light-verb construc-
tions (LVC) in the hungarian dependency treebank
(Vincze et al, 2010): the construction has regular
syntax, and a suffix is used on labels to express it
is a LVC (Vincze et al, 2013).
3 Data: MWEs in Dependency Trees
The data we use is the SPMRL 13 dataset for
French, in dependency format. It contains pro-
jective dependency trees that were automatically
derived from the latest status of the French Tree-
bank (Abeille? and Barrier, 2004), which con-
sists of constituency trees for sentences from the
3Concerning non contiguous MWEs, we can cite the work
of Vincze et al (2013), who experimented joint dependency
parsing and light verb construction identification.
newspaper Le Monde, manually annotated with
phrase structures, morphological information, and
grammatical functional tags for dependents of
verbs. The Shared Task used an enhanced version
of the constituency-to-dependency conversion of
Candito et al (2010), with different handling of
MWEs. The dataset consists of 18535 sentences,
split into 14759, 1235 and 2541 sentences for
training, development, and final evaluation respec-
tively.
We describe below the flat representation of
MWEs in this dataset, and the modified represen-
tation for regular MWEs that we propose.
a. Flat representation:
L? abus de biens sociaux fut de?nonce? en vain
suj
de
t
dep
cpd
dep cpd
dep cpd
au
x
tps
mod
dep
cpd
b. Structured representation:
L? abus de biens sociaux fut de?nonce? en vain
suj
det
dep
obj
.p
mod
au
x
tps
mod
dep
cpd
Figure 1: French dependency tree for L?abus de
biens sociaux fut de?nonce? en vain (literally the
misuse of assets social was denounced in vain,
meaning The misuse of corporate assets was de-
nounced in vain), containing two MWEs (in red).
Top: original flat representation. Bottom: Tree af-
ter regular MWEs structuring.
3.1 MWEs in Gold Data: Flat representation
In gold data, the MWEs appear in an expanded
flat format: each MWE bears a part-of-speech
and consists of a sequence of tokens (hereafter
the ?components? of the MWE), each having their
proper POS, lemma and morphological features.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node per MWE com-
ponent (more generally one node per token). The
first component of a MWE is taken as the head
of the MWE. All subsequent components of the
MWE depend on the first one, with the special
label dep_cpd (hence the name flat represen-
744
tation). Furthermore, the first MWE component
bears a feature mwehead equal to the POS of the
MWE. An example is shown in Figure 1. The
MWE en vain (pointlessly) is an adverb, contain-
ing a preposition and an adjective. The latter de-
pends on former, which bears mwehead=ADV+.
The algorithm to recover MWEs is: any node
having dependents with the dep_cpd label forms
a MWE with such dependents.
3.2 Alternative representation for regular
MWEs
In the alternative representation we propose, ir-
regular MWEs are unchanged and appear as flat
MWEs (e.g. en vain in Figure 1 has pattern prepo-
sition+adjective, which is not considered regular
for an adverb, and is thus unchanged). Regular
MWEs appear with ?structured? syntax: we mod-
ify the tree structure to recover the regular syn-
tactic dependencies. For instance, in the bottom
tree of the figure, biens is attached to the prepo-
sition, and the adjective sociaux is attached to bi-
ens, with regular labels. Structured MWEs can-
not be spotted using the tree topology and la-
bels only. Features are added for that purpose:
the syntactic head of the structured MWE bears
a regmwehead for the POS of the MWE (abus
in Figure 1), and the other components of the
MWE bear a regcomponent feature (the orange
tokens in Figure 1).4 With this representation,
the algorithm to recover regular MWEs is: any
node bearing regmwehead forms a MWE with
the set of direct or indirect dependents bearing a
regcomponent feature.
3.2.1 Motivations
Our first motivation is to increase the quantity of
information conveyed by the dependency trees,
by distinguishing syntactic regularity and seman-
tic regularity. Syntactically regular MWEs (here-
after regular MWEs) show various degrees of se-
mantic non-compositionality. For instance, in the
French Treebank, population active (lit. active
population, meaning ?working population?) is a
partially compositional MWE. Furthermore, some
sequences are both syntactically and semantically
regular, but encoded as MWE due to frozen lexi-
cal selection. This is the case for de?ficit budge?taire
(lit. budgetary deficit, meaning ?budget deficit?),
4The syntactic head of a structured MWE may not be the
first token, whereas the head token of a flat MWE is always
the first one.
because it is not possible to use de?ficit du bud-
get (budget deficit). Our alternative representa-
tion distinguishes between syntactic internal reg-
ularity and semantic regularity. This renders the
syntactic description more uniform and it provides
an internal structure for regular MWEs, which is
meaningful if the MWE is fully or partially com-
positional. For instance, it is meaningful to have
the adjective sociaux attach to biens instead of on
the first component abus. Moreover, such a dis-
tinction opens the way to a non-binary classifica-
tion of MWE status: the various criteria leading to
classify a sequence as MWE could be annotated
separately and using nominal or scaled categories
for each criteria. For instance, de?ficit budge?taire
could be marked as fully compositional, but with
frozen lexical selection. Further, annotation is of-
ten incoherent for the MWEs with both regular
syntax and a certain amount of semantic compo-
sitionality, the same token sequence (with same
meaning) being sometimes annotated as MWE
and sometimes not.
More generally, keeping a regular representa-
tion would allow to better deal with the interac-
tion between idiomatic status and regular syntax,
such as the insertion of modifiers on MWE sub-
parts (e.g. make a quick decision).
Finally, using regular syntax for MWEs pro-
vides a more uniform training set. For instance for
a sequence N1 preposition N2, though some exter-
nal attachments might vary depending on whether
the sequence forms a MWE or not, some may
not, and the internal dependency structure (N1 ?
(preposition ? N2)) is quite regular. One objec-
tive of the current work is to investigate whether
this increased uniformity eases parsing or whether
it is mitigated by the additional difficulty of find-
ing the internal structure of a MWE.
Total Nb of regular MWEs
nb of (% of nouns, adverbs,
MWEs prepositions, verbs)
train 23658 12569 (64.7, 19.2, 14.6, 1.5)
dev 2120 1194 (66.7, 17.7, 14.7, 0.8)
test 4049 2051 (64.5, 19.9, 13.6, 2.0)
Table 1: Total number of MWEs and number of
regular MWEs in training, development and test
set (and broken down by POS of MWE).
745
3.2.2 Implementation
We developed an ad hoc program for structur-
ing the regular MWEs in gold data. MWEs are
first classified as regular or irregular, using reg-
ular expressions over the sequence of parts-of-
speech within the MWE. To define the regular
expressions, we grouped gold MWEs according
to the pair [global POS of the MWE + sequence
of POS of the MWE components], and designed
regular expressions to match the most frequent
patterns that looked regular according to our lin-
guistic knowledge. The internal structure for the
matching MWEs was built deterministically, us-
ing heuristics favoring local attachments.5 Table 1
shows the proportions of MWEs classified as regu-
lar, and thus further structured. About half MWEs
are structured, and about two thirds of structured
MWEs are nouns.
For predicted parses with structured MWEs, we
use an inverse transformation of structured MWEs
into flat MWEs, for evaluation against the gold
data. When a predicted structured MWE is flat-
tened, all the dependents of any token of the MWE
that are not themselves belonging to the MWE are
attached to the head component of the MWE.
3.3 Integration of MWE features into labels
In some experiments, we make use of alterna-
tive representations, which we refer later as ?la-
beled representation?, in which the MWE features
are incorporated in the dependency labels, so that
MWE composition and/or the POS of the MWE be
totally contained in the tree topology and labels,
and thus predictable via dependency parsing. Fig-
ure shows the labeled representation for the sen-
tence of Figure 1.
For flat MWEs, the only missing information is
the MWE part-of-speech: we concatenate it to the
dep_cpd labels. For instance, the arc from en
to vain is relabeled dep_cpd_ADV. For struc-
tured MWEs, in order to get full MWE account
within the tree structure and labels, we need to in-
corporate both the MWE POS, and to mark it as
5The six regular expressions that we obtained cover nomi-
nal, prepositional, adverbial and verbal compounds. We man-
ually evaluated both the regular versus irregular classification
and the structuring of regular MWEs on the first 200 MWEs
of the development set. 113 of these were classified as regu-
lar, and we judged that all of them were actually regular, and
were correctly structured. Among the 87 classified as irregu-
lar, 7 should have been tagged as regular and structured. For
4 of them, the classification error is due to errors on the (gold)
POS of the MWE components.
c. Labeled representation:
L? abus de biens sociaux fut de?nonce? en vain
suj
det
dep
r N
obj
.p
r N
m
od
r N
au
x
tps
mod
dep
cpd
ADV
Figure 2: Integration of all MWE information into
labels for the example of Figure 1.
belonging to a MWE. The suffixed label has the
form FCT_r_POS. For instance, in bottom tree
of Figure 1, arcs pointing to the non-head compo-
nents (de, biens, sociaux) are suffixed with _r to
mark them as belonging to a structured MWE, and
with _N since the MWE is a noun.
In both cases, this label suffixing is translated
back into features for evaluation against gold data.
4 Architectures for MWE Analysis and
Parsing
The architectures we investigated vary depending
on whether the MWE status of sequences of to-
kens is predicted via dependency parsing or via an
external tool (described in section 5), and this di-
chotomy applies both to structured MWEs and flat
MWEs. More precisely, we consider the following
alternative for irregular MWEs:
? IRREG-MERGED: gold irregular MWEs are
merged for training; for parsing, irregular
MWEs are predicted externally, merged into
one token at parsing time, and re-expanded
into several tokens for evaluation;
? IRREG-BY-PARSER: the MWE status, flat
topology and POS are all predicted via de-
pendency parsing, using representations for
training and parsing, with all information for
irregular MWEs encoded in topology and la-
bels (as for in vain in Figure 2).
For regular MWEs, their internal structure is al-
ways predicted by the parser. For instance the un-
labeled dependencies for abus de biens sociaux are
the same, independently of predicting whether it
746
forms a MWE or not. But we use two kinds of
predictions for their MWE status and POS:
? REG-POST-ANNOTATION: the regular
MWEs are encoded/predicted as shown for
abus de biens sociaux in bottom tree of
Figure 1, and their MWE status and POS is
predicted after parsing, by an external tool.
? REG-BY-PARSER: all regular MWE infor-
mation (topology, status, POS) is predicted
via dependency parsing, using representa-
tions with all information for regular MWEs
encoded in topology and labels (Figure 2).
Name prediction of prediction of
reg MWEs irreg MWEs
JOINT irreg-by-parser reg-by-parser
JOINT-REG irreg-merged reg-by-parser
JOINT-IRREG irreg-by-parser reg-post-annot
PIPELINE irreg-merged reg-post-annot
Table 2: The four architectures, depending on how
regular and irregular MWEs are predicted.
We obtain four architectures, schematized in ta-
ble 2. We describe more precisely two of them,
the other two being easily inferable:
JOINT-REG architecture:
? training set: irregular MWEs merged into
one token, regular MWEs are structured, and
integration of regular MWE information into
the labels (FCT_r_POS).
? parsing: (i) MWE analysis with classifica-
tion of MWEs into regular or irregular, (ii)
merge of predicted irregular MWEs, (iii) tag-
ging and morphological prediction, (iv) pars-
ing
JOINT-IRREG architecture:
? training set: flat representation of irregu-
lar MWEs, with label suffixing (dep_cpd_
POS), structured representation of regular
MWEs without label suffixing.
? parsing: (i) MWE analysis and classifica-
tion into regular or irregular, used for MWE-
specific features, (ii) tagging and morpholog-
ical prediction, (iii) parsing,
We compare these four architectures between
them and also with two simpler architectures used
by (Constant et al, 2013) within the SPMRL 13
Shared Task, in which regular and irregular MWEs
are not distinguished:
Uniform joint architecture: The joint systems
perform syntactic parsing and MWE analysis via
a single dependency parser, using representations
as in 3.3.
Uniform pipeline architecture:
? training set: MWEs merged into one token
? parsing: (i) MWE analysis, (ii) merge of pre-
dicted MWEs, (iii) tagging and morphologi-
cal prediction, (iv) parsing
For each architecture, we apply the appropriate
normalization procedures on the predicted parses,
in order to evaluate against (i) the pseudo-gold
data in structured representation, and (ii) the gold
data in flat representation.
5 Use of external MWE resources
In order to better deal with MWE prediction, we
use external MWE resources, namely MWE lexi-
cons and an MWE analyzer. Both resources help
to predict MWE-specific features (section 5.3) to
guide the MWE-aware dependency parser. More-
over, in some of the architectures, the external
MWE analyzer is used either to pre-group irreg-
ular MWEs (for the architectures using IRREG-
MERGED), or to post-annotate regular MWEs.
5.1 MWE lexicons
MWE lexicons are exploited as sources of fea-
tures for both the dependency parser and the ex-
ternal MWE analyzer. In particular, two large-
coverage general-language lexicons are used: the
Lefff6 lexicon (Sagot, 2010), which contains ap-
proximately half a million inflected word forms,
among which approx. 25, 000 are MWEs; and
the DELA7 (Courtois, 2009; Courtois et al, 1997)
lexicon, which contains approx. one million in-
flected forms, among which about 110, 000 are
MWEs. These resources are completed with spe-
cific lexicons freely available in the platform Uni-
tex8: the toponym dictionary Prolex (Piton et al,
1999) and a dictionary of first names. Note that the
lexicons do not include any information on the ir-
regular or the regular status of the MWEs. In order
to compare the MWEs present in the lexicons and
those encoded in the French treebank, we applied
the following procedure (hereafter called lexicon
6We use the version available in the POS tagger MElt (De-
nis and Sagot, 2009).
7We use the version in the platform Unitex
(http://igm.univ-mlv.fr/?unitex). We had to convert the
DELA POS tagset to that of the French Treebank.
8http://igm.univ-mlv.fr/?unitex
747
lookup): in a given sentence, the maximum num-
ber of non overlapping MWEs according to the
lexicons are systematically marked as such. We
obtain about 70% recall and 50% precision with
respect to MWE spanning.
5.2 MWE Analyzer
The MWE analyzer is a CRF-based sequential la-
beler, which, given a tokenized text, jointly per-
forms MWE segmentation and POS tagging (of
simple tokens and of MWEs), both tasks mutu-
ally helping each other9. The MWE analyzer inte-
grates, among others, features computed from the
external lexicons described in section 5.1, which
greatly improve POS tagging (Denis and Sagot,
2009) and MWE segmentation (Constant and Tel-
lier, 2012). The MWE analyzer also jointly classi-
fies its predicted MWEs as regular or irregular (the
distinction being learnt on gold training set, with
structured MWEs cf. section 3.2).
5.3 MWE-specific features
We introduce information from the external MWE
resources in different ways:
Flat MWE features: MWE information can
be integrated as features to be used by the de-
pendency parser. We tested to incorporate the
MWE-specific features as defined in the gold flat
representation (section 3.1): the mwehead=POS
feature for the MWE head token, POS being the
part-of-speech of the MWE; the component=y
feature for the non-first MWE component.
Switch: instead or on top of using the mwehead
feature, we use the POS of the MWE instead of the
POS of the first component of a flat MWE. For in-
stance in Figure 1, the token en gets pos=ADV in-
stead of pos=P. The intuition behind this feature
is that for an irregular MWE, the POS of the lin-
early first component, which serves as head, is not
always representative of the external distribution
of the MWE. For regular MWEs, the usefulness of
such a trick is less obvious. The first component
of a regular MWE is not necessarily its head (for
instance for a nominal MWE with internal pattern
adjective+noun), so the switch trick could be detri-
mental in such cases.10
9Note that in our experiments, we use this analyzer for
MWE analysis only, and discard the POS tagging predic-
tion. Tagging is performed along with lemmatization with
the Morfette tool (section 6.1).
10We also experimented to use POS of MWE plus suffixes
to force disjoint tagsets for single words, irregular MWEs and
6 Experiments
6.1 Settings and evaluation metrics
MWE Analysis and Tagging: For the MWE
analyzer, we used the tool lgtagger11 (version
1.1) with its default set of feature templates, and a
10-fold jackknifing on the training corpus.
Parser: We used the second-order graph-based
parser available in Mate-tools12 (Bohnet, 2010).
We used the Anna3.3 version, in projective
mode, with default feature sets and parameters
proposed in the documentation, augmented or not
with MWE-specific features, depending on the
experiments.
Morphological prediction: Predicted lemmas,
POS and morphology features are computed
with Morfette version 0.3.5 (Chrupa?a et al,
2008; Seddah et al, 2010)13, using 10 iterations
for the tagging perceptron, 3 iterations for the
lemmatization perceptron, default beam size for
the decoding of the joint prediction, and the
Lefff (Sagot, 2010) as external lexicon used for
out-of-vocabulary words. We performed a 10-fold
jackknifing on the training corpus.
Evaluation metrics: we evaluate our parsing sys-
tems by using the standard metrics for depen-
dency parsing: Labeled Attachment Score (LAS)
and Unlabeled Attachment Score (UAS), com-
puted using all tokens including punctuation. To
evaluate statistical significance of parsing perfor-
mance differences, we use eval07.pl14 with -b op-
tion, and then Dan Bikel?s comparator.15 For
MWEs, we use the Fmeasure for recognition of
untagged MWEs (hereafter FUM) and for recog-
nition of tagged MWEs (hereafter FTM).
6.2 MWE-specific feature prediction
In all our experiments, for the switch trick (section
5.3), the POS of MWE is always predicted using
the MWE analyzer. For the flat MWE features, we
experimented both with features predicted by the
MWE analyzer, and with features predicted using
the external lexicons mentioned in section 5.1 (us-
ing the lexicon lookup procedure). Both kinds of
regular MWEs, but this showed comparable results.
11http://igm.univ-mlv.fr/?mconstan
12http://code.google.com/p/mate-tools/
13https://sites.google.com/site/morfetteweb/
14http://nextens.uvt.nl/depparse-wiki/SoftwarePage
15The compare.pl script, formerly available at
www.cis.upenn.edu/ dbikel/
748
LABELED STRUCTURED FLAT
REPRES. REPRESENTATION REPRESENTATION
MWE swi. swi. LAS UAS LAS FUM FTM LAS UAS FUM FTM
ARCHI feats irreg reg irreg irreg
bsline - - - 84.5 89.3 87.0 83.6 80.6 84.2 88.1 73.5 70.7
JOINT best + + + 85.3 89.7 87.5 85.4 82.6 85.2 88.8 77.6 74.5
JOINT- bsline - - - 84.7 89.4 87.0 83.5 80.3 84.5 88.0 78.3 75.9
IRREG best + + + 85.1 89.8 87.4 85.0 81.6 84.9 88.3 79.0 76.5
JOINT- bsline - NA - 84.2 89.1 86.7 84.2 80.8 84.0 88.0 73.3 70.3
REG best + NA + 84.7 89.3 86.9 84.1 80.7 84.6 88.3 76.3 73.2
PIPE bsline - NA - 84.6 89.2 86.9 84.1 80.7 84.5 87.9 78.8 76.3
LINE best - NA + 84.7 89.4 87.0 84.2 80.8 84.6 88.1 78.8 76.3
Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for
statistical significance evaluation). The UAS for the structured representation is the same as the one for
the labeled representation, and is not repeated.
prediction lead to fairly comparable results, so in
all the following, the MWE features, when used,
are predicted using the external lexicons.
6.3 Tuning features for each architecture
We ran experiments for all value combinations
of the following parameters: (i) the architecture,
(ii) whether MWE features are used, whether the
switch trick is applied or not (iii) for irregular
MWEs and (iv) for regular MWEs.
We performed evaluation of the predicted parses
using the three representations described in sec-
tion 3, namely flat, structured and labeled repre-
sentations. In the last two cases, the evaluation
is performed against an instance of the gold data
automatically transformed to match the represen-
tation type. Moreover, for the ?labeled representa-
tion? evaluation, though the MWE information in
the predicted parses is obtained in various ways,
depending on the architecture, we always map all
this information in the dependency labels, to ob-
tain predicted parses matching the ?labeled repre-
sentation?. While the evaluation in flat represen-
tation is the only one comparable to other works
on this dataset, the other two evaluations provide
useful information. In the ?labeled representation?
evaluation, the UAS provides a measure of syn-
tactic attachments for sequences of words, inde-
pendently of the (regular) MWE status of subse-
quences. For the sequence abus de biens sociaux,
suppose that the correct internal structure is pre-
dicted, but not the MWE status. The UAS for
labeled representation will be maximal, whereas
for the flat representation, the last two tokens will
count as incorrect for UAS. For LAS, in both cases
the three last tokens will count as incorrect if the
wrong MWE status is predicted. So to sum up on
the ?labeled evaluation?, we obtain a LAS eval-
uation for the whole task of parsing plus MWE
recognition, but an UAS evaluation that penalizes
less errors on MWE status, while keeping a rep-
resentation that is richer: predicted parses contain
not only the syntactic dependencies and MWE in-
formation, but also a classification of MWEs into
regular and irregular, and the internal syntactic
structure of regular MWEs.
The evaluation on ?structured representation?
can be interpreted as an evaluation of the parsing
task plus the recognition of irregular MWEs only:
both LAS and UAS are measured independently
of errors on regular MWE status (note the UAS is
exactly the same than in the ?labeled? case).
For each architecture, Table 3 shows the results
for two systems: first the baseline system without
any MWE features nor switches and immediately
below the best settings for the architecture. The
JOINT baseline corresponds to a ?pure? joint sys-
tem without external MWE resources (hence the
minus sign for the first three columns). For each
architecture except the PIPELINE one, differences
between the baseline and the best setting are sta-
tistically significant (p < 0.01). Differences be-
tween best PIPELINE and best JOINT-REG are
not. Best JOINT has statistically significant dif-
ference (p < 0.01) over both best JOINT-REG
and best PIPELINE. The situation for best JOINT-
IRREG with respect to the other three is borderline
(with various p-values depending on the metrics).
Concerning the tuning of parameters, it appears
that the best setting is to use MWE-features, and
switch for both regular and irregular MWEs, ex-
cept for the pipeline architecture for which results
without MWE features are slightly better. So over-
all, informing the parser with independently pre-
749
LABELED STRUCTURED FLAT
REPRESENTATION REPRESENTATION REPRESENTATION
LAS UAS LAS UAS FUM FTM LAS UAS FUM FTM
SYSTEM irreg irreg
baseline JOINT 84.13 88.93 86.62 88.93 83.6 79.2 83.97 87.80 73.9 70.5
best JOINT 84.59 89.21 86.92 89.21 85.7 81.4 84.48 88.13 77.0 73.5
best JOINT-IRREG 84.50 89.21 86.97 89.24 86.3 82.1 84.36 87.75 78.6 75.4
best JOINT-REG 84.31 89.0 86.63 89.00 84.5 80.4 84.18 87.95 76.4 73.3
best PIPELINE 84.02 88.83 86.49 88.83 84.4 80.4 83.88 87.33 77.6 74.4
Table 4: Final results on test set for baseline and the best system for each architecture.
dicted POS of MWE has positive impact. The
best architectures are JOINT and JOINT-IRREG,
with the former slightly better than the latter for
parsing metrics, though only some of the differ-
ences are significant between the two. It can be
noted though, that JOINT-IRREG performs over-
all better on MWEs (last two columns of table
3), whereas JOINT performs better on irregular
MWEs: the latter seems to be beneficial for pars-
ing, but is less efficient to correctly spot the regular
MWEs.
Concerning the three distinct representations,
evaluating on structured representation (hence
without looking at regular MWE status) leads to
a rough 2 point performance increase for the LAS
and a one point increase for the UAS, with respect
to the evaluation against flat representation. This
quantifies the additional difficulty of deciding for
a regular sequence of tokens whether it forms a
MWE or not. The evaluation on the labeled rep-
resentation provides an evaluation of the full task
(parsing, regular/irregular MWE recognition and
regular MWEs structuring), with a UAS that is less
impacted by errors on regular MWE status, while
LAS reflects the full difficulty of the task.16
6.4 Results on test set and comparison
We provide the final results on the test set in
table 4. We compare the baseline JOINT sys-
tem with the best system for all four reg/irreg
architectures (cf. section 6.3). We observe the
same general trend as in the development corpus,
but with tinier differences. JOINT and JOINT-
IRREG significantly outperform the baseline and
the PIPELINE, on labeled representation and flat
representation. We can see that there is no sig-
nificant difference between JOINT and JOINT-
16The slight differences in LAS between the labeled and
the flat representations are due to side effects of errors on
MWE status: some wrong reattachments performed to obtain
flat representation decrease the UAS, but also in some cases
the LAS.
DEV TEST
System UAS LAS UAS LAS
reg/irreg joint 88.79 85.15 88.13 84.48
Bjork13 88.30 84.84 87.87 84.37
Const13 pipeline 88.73 85.28 88.35 84.91
Const13 joint 88.21 84.60 87.76 84.14
uniform joint 88.81 85.42 87.96 84.59
Table 5: Comparison on dev set of our best archi-
tecture with reg/irregular MWE distinction (first
row), with the single-parser architectures of (Con-
stant et al, 2013) (Const13) and (Bjo?rkelund et
al., 2013) (Bjork13). Uniform joint is our reimple-
mentation of Const13 joint, enhanced with mwe-
features and switch.
IRREG and between JOINT-REG and JOINT-
IRREG. JOINT slightly outperforms JOINT-REG
(p < 0.05). On the structured representation, the
two best systems (JOINT and JOINT-IRREG) sig-
nificantly outperform the other systems (p < 0.01
for all; p < 0.05 for JOINT-REG).
Moreover, we provide in table 5 a comparison
of our best architecture with reg/irregular MWE
distinction with other architectures that do not
make this distinction, namely the two best com-
parable systems designed for the SPMRL Shared
Task (Seddah et al, 2013): the pipeline sim-
ple parser based on Mate-tools of Constant et
al. (2013) (Const13) and the Mate-tools system
(without reranker) of Bjo?rkelund et al (2013)
(Bjork13). We also reimplemented and improved
the uniform joint architecture of Constant et al
(2013), by adding MWE features and switch. Re-
sults can only be compared on the flat representa-
tion, because the other systems output poorer lin-
guistic information. We computed statistical sig-
nificance of differences between our systems and
Const13. On dev, the best system is the enhanced
uniform joint, but differences are not significant
between that and the best reg/irreg joint (1st row)
and the Const13 pipeline. But on the test corpus
(which is twice bigger), the best system is Const13
750
Tasks LAS UAS ALL MWE REG MWE IRREG MWE
System Parsing MWE FUM FTM FUM FTM FUM FTM
Our best system (best JOINT) + all 85.15 88.78 77.6 74.5 70.8 67.8 85.4 82.6
Uniform pipeline/gold MWEs + - 88.73 90.60 - - - - - -
CRF-based MWE analyzer - all - - 78.8 76.3 73.5 71.9 84.2 80.8
JOINT-REG + all 84.58 88.34 76.3 73.2 69.3 66.5 84.1 80.7
JOINT-REG/gold irreg. MWE + reg. 85.86 89.19 82.9 78.8 70.0 67.2 - -
Table 6: Comparison with simpler tasks on the flat representation of the development set.
pipeline, with statistically significant differences
over our joint systems. So the first observation
is that our architectures that distinguish between
reg/irreg MWEs do not outperform uniform ar-
chitectures. But we note that the differences are
slight, and the output we obtain is enhanced with
regular MWE internal structure. It can thus be
noted that the increased syntactic uniformity ob-
tained by our MWE representation is mitigated so
far by the additional complexity of the task. The
second observation is that currently the best sys-
tem on this dataset is a pipeline system, as results
on test set show (and somehow contrary to results
on dev set). The joint systems that integrate MWE
information in the labels seem to suffer from in-
creased data sparseness.
6.5 Evaluating the double task with respect
to simpler tasks
In this section, we propose to better evaluate the
difficulty of combining the tasks of MWE analy-
sis and dependency parsing by comparing our sys-
tems with systems performing simpler tasks: i.e.
MWE recognition without parsing, and parsing
with no or limited MWE recognition, simulated by
using gold MWEs. We also provide a finer eval-
uation of the MWE recognition task, in particular
with respect to their regular/irregular status.
We first compare our best system with a parser
where all MWEs have been perfectly pre-grouped,
in order to quantify the difficulty that MWEs add
to the parsing task. We also compare the per-
formance on MWEs of our best system with that
achieved by the CRF-based analyzer described in
section 5.2. Next, we compare the best JOINT-
REG system with the one based on the same ar-
chitecture but where the irregular MWEs are per-
fectly pre-identified, in order to quantify the dif-
ficulty added by the irregular MWEs. Results are
given in table 6. Without any surprise, the task
is much easier without considering MWE recog-
nition. We can see that without considering MWE
analysis the parsing accuracy is about 2.5 points
better in terms of LAS. In the JOINT-REG ar-
chitecture, assuming gold irregular MWE identi-
fication, increases LAS by 1.3 point. In terms
of MWE recognition, as compared with the CRF-
based analyzer, our best system is around 2 points
below. But the situation is quite different when
breaking the evaluation by MWE type. Our sys-
tem is 1 point better than the CRF-based analyzer
for irregular MWEs. This shows that considering
a larger syntactic context helps recognition of ir-
regular MWEs. The ?weak point? of our system is
therefore the identification of regular MWEs.
7 Conclusion
We experimented strategies to predict both MWE
analysis and dependency structure, and tested
them on the dependency version of French Tree-
bank (Abeille? and Barrier, 2004), as instantiated
in the SPMRL Shared Task (Seddah et al, 2013).
Our work focused on using an alternative repre-
sentation of syntactically regular MWEs, which
captures their syntactic internal structure. We ob-
tain a system with comparable performance to that
of previous works on this dataset, but which pre-
dicts both syntactic dependencies and the internal
structure of MWEs. This can be useful for captur-
ing the various degrees of semantic composition-
ality of MWEs. The main weakness of our system
comes from the identification of regular MWEs, a
property which is highly lexical. Our current use
of external lexicons does not seem to suffice, and
the use of data-driven external information to bet-
ter cope with this identification can be envisaged.
References
Anne Abeille? and Nicolas Barrier. 2004. Enriching
a french treebank. In Proceedings of LREC 2004,
Lisbon, Portugal.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
751
french. In Proceedings of ACL 2005, Ann Arbor,
USA.
Eduard Bejc?ek and Pavel Stranak. 2010. Annota-
tion of multiword expressions in the prague depen-
dency treebank. Language Resources and Evalua-
tion, 44:7?21.
Anders Bjo?rkelund, ?Ozlem C?etinog?lu, Thomas Farkas,
Richa?rdand Mu?ller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the spmrl 2013 shared task. In Pro-
ceedings of the 4th Workshop on Statistical Parsing
of Morphologically Rich Languages: Shared Task,
Seattle, WA.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of COLING 2010, Beijing, China.
Conor Cafferkey, Deirdre Hogan, and Josef van Gen-
abith. 2007. Multi-word units in treebank-based
probabilistic parsing and generation. In Proceed-
ings of the 10th International Conference on Re-
cent Advances in Natural Language Processing
(RANLP?07), Borovets, Bulgaria.
Marie Candito, Benoit Crabbe?, and Pascal Denis.
2010. Statistical french dependency parsing : Tree-
bank conversion and first results. In Proceedings of
LREC 2010, Valletta, Malta.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van
Genabith. 2008. Learning morphology with mor-
fette. In Proceedings of LREC 2008, Marrakech,
Morocco. ELDA/ELRA.
Matthieu Constant and Isabelle Tellier. 2012. Eval-
uating the impact of external lexical resources into
a crf-based multiword segmenter and part-of-speech
tagger. In Proceedings of LREC 2012, Istanbul,
Turkey.
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate
multiword expression recognition and parsing. In
Proceedings of ACL 2012, Stroudsburg, PA, USA.
Matthieu Constant, Marie Candito, and Djame? Sed-
dah. 2013. The ligm-alpage architecture for the
spmrl 2013 shared task: Multiword expression anal-
ysis and dependency parsing. In Proceedings of the
4th Workshop on Statistical Parsing of Morphologi-
cally Rich Languages: Shared Task, Seattle, WA.
Blandine Courtois, Myle`ne Garrigues, Gaston Gross,
Maurice Gross, Rene? Jung, Mathieu-Colas Michel,
Anne Monceaux, Anne Poncet-Montange, Max Sil-
berztein, and Robert Vive?s. 1997. Dictionnaire
e?lectronique DELAC : les mots compose?s binaires.
Technical Report 56, University Paris 7, LADL.
Blandine Courtois. 2009. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise, 87:11?22.
Pascal Denis and Beno??t Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art POS tagging with less human ef-
fort. In Proceedings of the 23rd Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC?09), Hong Kong.
Gu?ls?en Eryig?it, Tugay Ilbay, and Ozan Arkan Can.
2011. Multiword expressions in statistical depen-
dency parsing. In Proceedings of the IWPT Work-
shop on Statistical Parsing of Morphologically-Rich
Languages (SPMRL?11), Dublin, Ireland.
Spence Green, Marie-Catherine de Marneffe, John
Bauer, and Christofer D. Manning. 2011. Multi-
word expression identification with tree substitution
grammars: A parsing tour de force with french. In
Proceedings of EMNLP 2011, Edinburgh, Scotland.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Proceedings of the LREC
Workshop : Methodologies and Evaluation of Multi-
word Units in Real-World Applications (MEMURA),
Lisbon, Portugal.
Odile Piton, Denis Maurel, and Claude Belleil. 1999.
The prolex data base : Toponyms and gentiles for
nlp. In Proceedings of the Third International Work-
shop on Applications of Natural Language to Data
Bases (NLDB?99), Klagenfurt, Austria.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of NAACL/HLT
2006, Companion Volume: Short Papers, Strouds-
burg, PA, USA.
Beno??t Sagot. 2010. The lefff, a freely available, accu-
rate and large-coverage lexicon for french. In Pro-
ceedings of LREC 2010, Valletta, Malta.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Djame? Seddah, Reut Tsarfaty, Sandra K??ubler, Marie
Candito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,
Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Joakim Nivre, Adam Przepi-
orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Wolin?ski, Alina
Wro?blewska, and Eric Villemonte de la Cle?rgerie.
2013. Overview of the spmrl 2013 shared task: A
cross-framework evaluation of parsing morpholog-
ically rich languages. In Proceedings of the 4th
Workshop on Statistical Parsing of Morphologically
Rich Languages: Shared Task, Seattle, WA.
752
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In Proceedings of
LREC 2010, Valletta, Malta.
Veronika Vincze, Ja?nos Zsibrita, and Istva`n Nagy T.
2013. Dependency parsing for identifying hungar-
ian light verb constructions. In Proceedings of In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2013), Nagoya, Japan.
753
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 49?56,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
MWU-aware Part-of-Speech Tagging with a CRF model and lexical
resources
Matthieu Constant
Universite? Paris-Est, LIGM
5, bd Descartes - Champs/Marne
77454 Marne-la-Valle?e cedex 2, France
mconstan@univ-mlv.fr
Anthony Sigogne
Universite? Paris-Est, LIGM
5, bd Descartes - Champs/Marne
77454 Marne-la-Valle?e cedex 2, France
sigogne@univ-mlv.fr
Abstract
This paper describes a new part-of-speech tag-
ger including multiword unit (MWU) identifi-
cation. It is based on a Conditional Random
Field model integrating language-independent
features, as well as features computed from
external lexical resources. It was imple-
mented in a finite-state framework composed
of a preliminary finite-state lexical analysis
and a CRF decoding using weighted finite-
state transducer composition. We showed that
our tagger reaches state-of-the-art results for
French in the standard evaluation conditions
(i.e. each multiword unit is already merged in
a single token). The evaluation of the tagger
integrating MWU recognition clearly shows
the interest of incorporating features based on
MWU resources.
1 Introduction
Part-of-speech (POS) tagging reaches excellent
results thanks to powerful discriminative multi-
feature models such as Conditional Random Fields
(Lafferty et al, 2001), Support Vector Machine
(Gime?nez and Ma?rquez, 2004), Maximum Entropy
(Ratnaparkhi, 1996). Some studies like (Denis and
Sagot, 2009) have shown that featuring these models
by means of external morphosyntactic resources still
improves accuracy. Nevertheless, current taggers
rarely take multiword units such as compound words
into account, whereas they form very frequent lexi-
cal units with strong syntactic and semantic particu-
larities (Sag et al, 2001; Copestake et al, 2002) and
their identification is crucial for applications requir-
ing semantic processing. Indeed, taggers are gen-
erally evaluated on perfectly tokenized texts where
multiword units (MWU) have already been identi-
fied.
Our paper presents a MWU-aware POS tagger
(i.e. a POS tagger including MWU recognition1).
It is based on a Conditional Random Field (CRF)
model that integrates features computed from large-
coverage morphosyntactic lexicons and fine-grained
MWU resources. We implemented it in a finite-state
framework composed of a finite-state lexical ana-
lyzer and a CRF-decoder using weighted transducer
composition.
In section 2, we will first describe statistical tag-
ging based on CRF. Then, in section 3, we will
show how to adapt the tagging models in order to
also identify multiword unit. Next, section 4 will
present the finite-state framework used to implement
the tagger. Section 5 will focus on the description of
our working corpus and the set of lexical resources
used. In section 6, we then evaluate our tagger on
French.
2 Statistical POS tagging with Linear
Chain Conditional Random Fields
Linear chain Conditional Ramdom Fields (CRF) are
discriminative probabilistic models introduced by
(Lafferty et al, 2001) for sequential labelling. Given
an input sequence x = (x1, x2, ..., xN ) and an out-
1This strategy somewhat resembles the popular approach of
joint word segmentation and part-of-speech tagging for Chi-
nese, e.g. (Zhang and Clark, 2008). Moreover, other similar
experiments on the same task for French are reported in (Con-
stant et al, 2011).
49
put sequence of labels y = (y1, y2, ..., yN ), the
model is defined as follows:
P?(y|x) =
1
Z(x) .
N
?
t
K
?
k
?k.fk(t, yt, yt?1, x)
where Z(x) is a normalization factor depending
on x. It is based on K features each of them be-
ing defined by a binary function fk depending on
the current position t in x, the current label yt,
the preceding one yt?1 and the whole input se-
quence x. The feature is activated if a given con-
figuration between t, yt, yt?1 and x is satisfied (i.e.
fk(t, yt, yt?1, x) = 1). Each feature fk is associated
with a weight ?k. The weights are the parameters
of the model. They are estimated during the train-
ing process by maximizing the conditional loglikeli-
hood on a set of examples already labeled (training
data). The decoding procedure consists in labelling
a new input sequence with respect to the model, by
maximizing P (y|x) (or minimizing ?logP (y|x)).
There exist dynamic programming procedures such
as Viterbi algorithm in order to efficiently explore all
labelling possibilities.
Features are defined by combining different prop-
erties of the tokens in the input sequence and the la-
bels at the current position and the preceding one.
Properties of tokens can be either binary or tex-
tual: e.g. token contains a digit, token is capital-
ized (binary property), form of the token, suffix of
size 2 of the token (textual property). Most tag-
gers exclusively use language-independent proper-
ties ? e.g. (Ratnaparkhi, 1996; Toutanova et al,
2003; Gime?nez and Ma?rquez, 2004; Tsuruoka et
al., 2009). It is also possible to integrate language-
dependant properties computed from an external
broad-coverage morphosyntactic lexicon, that are
POS tags found in the lexicon for the given token
(e.g. (Denis and Sagot, 2009)). It is of great interest
to deal with unknown words2 as most of them are
covered by the lexicon, and to somewhat filter the
list of candidate tags for each token. We therefore
added to our system a language-dependent property:
a token is associated with the concatenation of its
possible tags in an external lexicon, i.e. the am-
bibuity class of the token (AC).
2Unknown words are words that did not occur in the training
data.
In practice, we can divide features fk in two
families: while unigram features (uk) do not de-
pend on the preceding tag, i.e. fk(t, yt, yt?1, x) =
uk(t, yt, x), bigram features (bk) depend on both
current and preceding tags, i.e. fk(t, yt, yt?1, x) =
bk(t, yt, yt?1, x). In our practical case, bigrams
exlusively depends on the two tags, i.e. they are in-
dependent from the input sequence and the current
position like in the Hidden Markov Model (HMM)3.
Unigram features can be sub-divided into internal
and contextual ones. Internal features provide solely
characteristics of the current token w0: lexical form
(i.e. its character sequence), lowercase form, suf-
fice, prefix, ambiguity classes in the external lexi-
cons, whether it contains a hyphen, a digit, whether
it is capitalized, all capitalized, multiword. Contex-
tual features indicate characteristics of the surround-
ings of the current token: token unigrams at relative
positions -2,-1,+1 and +2 (w?2, w?1, w+1,w+2); to-
ken bigrams w?1w0, w0w+1 and w?1w+1; ambi-
guity classes at relative positions -2,-1,+1 and +2
(AC?2, AC?1, AC+1,AC+2). The different feature
templates used in our tagger are given in table 2.
Internal unigram features
w0 = X &t0 = T
Lowercase form of w0 = L &t0 = T
Prefix of w0 = P with |P | < 5 &t0 = T
Suffix of w0 = S with |S| < 5 &t0 = T
w0 contains a hyphen &t0 = T
w0 contains a digit &t0 = T
w0 is capitalized &t0 = T
w0 is all capital &t0 = T
w0 is capitalized and BOS4 &t0 = T
w0 is multiword &t0 = T
Lexicon tags AC0 of w0 = A & w0 is multiword &t0 = T
Contextual unigram features
wi = X , i ? {?2,?1, 1, 2} &t0 = T
wiwj = XY , (j, k) ? {(?1, 0), (0, 1), (?1, 1)} &t0 = T
ACi = A & wi is multiword, i ? {?2,?1, 1, 2} &t0 = T
Bigram features
t?1 = T ? &t0 = T
Table 1: Feature templates
3 MWU-aware POS tagging
MWU-aware POS tagging consists in identifying
and labelling lexical units including multiword ones.
3Hidden Markov Models of order n use strong indepen-
dance assumptions: a word only depends on its corresponding
tag, and a tag only depends on its n previous tags. In our case,
n=1.
50
It is somewhat similar to segmentation tasks like
chunking or Named Entity Recognition, that iden-
tify the limits of chunk or Named Entity segments
and classify these segments. By using an IOB5
scheme (Ramshaw and Marcus, 1995), this task is
then equivalent to labelling simple tokens. Each to-
ken is labeled by a tag in the form X+B or X+I,
where X is the POS labelling the lexical unit the to-
ken belongs to. Suffix B indicates that the token is at
the beginning of the lexical unit. Suffix I indicates
an internal position. Suffix O is useless as the end
of a lexical unit corresponds to the beginning of an-
other one (suffix B) or the end of a sentence. Such
procedure therefore determines lexical unit limits, as
well as their POS.
A simple approach is to relabel the training data
in the IOB scheme and to train a new model with the
same feature templates. With such method, most of
multiword units present in the training corpus will
be recognized as such in a new text. The main issue
resides in the identification of unknown multiword
units. It is well known that statistically inferring new
multiword units from a rather small training corpus
is very hard. Most studies in the field prefer finding
methods to automatically extract, from very large
corpus, multiword lexicons, e.g. (Dias, 2003; Caseli
et al, 2010), to be integrated in Natural Language
Processing tools.
In order to improve the number of new multiword
units detected, it is necessary to plug the tagger to
multiword resources (either manually built or auto-
matically extracted). We incorporate new features
computed from such resources. The resources that
we use (cf. section 5) include three exploitable fea-
tures. Each MWU encoded is obligatory assigned
a part-of-speech, and optionally an internal sur-
face structure and a semantic feature. For instance,
the organization name Banque de Chine (Bank of
China) is a proper noun (NPP) with the semantic
feature ORG; the compound noun pouvoir d?achat
(purchasing power) has a syntactic form NPN be-
cause it is composed of a noun (N), a preposition (P)
and a noun (N). By applying these resources to texts,
it is therefore possible to add four new properties
for each token that belongs to a lexical multiword
5I: Inside (segment); O: Outside (segment); B: Beginning
(of segment)
unit: the part-of-speech of the lexical multiword unit
(POS), its internal structure (STRUCT), its semantic
feature (SEM) and its relative position in the IOB
scheme (POSITION). Table 2 shows the encoding
of these properties in an example. The property ex-
traction is performed by a longest-match context-
free lookup in the resources. From these properties,
we use 3 new unigram feature templates shown in
table 3: (1) one combining the MWU part-of-speech
with the relative position; (2) another one depending
on the internal structure and the relative position and
(3) a last one composed of the semantic feature.
FORM POS STRUCT POSITION SEM Translation
un - - O - a
gain - - O - gain
de - - O - of
pouvoir NC NPN B - purchasing
d? NC NPN I -
achat NC NPN I - power
de - - O - of
celles - - O - the ones
de - - O - of
la - - O - the
Banque NPP - B ORG Bank
de NPP - I ORG of
Chine NPP - I ORG China
Table 2: New token properties depending on Multiword
resources
New internal unigram features
POS0/POSITION0 &t0 = T
STRUCT0/POSITION0 &t0 = T
SEM0 &t0 = T
Table 3: New features based on the MW resources
4 A Finite-state Framework
In this section, we describe how we implemented a
unified Finite-State Framework for our MWU-aware
POS tagger. It is organized in two separate clas-
sical stages: a preliminary resource-based lexical
analyzer followed by a CRF-based decoder. The
lexical analyzer outputs an acyclic finite-state trans-
ducer (noted TFST) representing candidate tagging
sequences for a given input. The decoder is in charge
of selecting the most probable one (i.e. the path in
the TFST which has the best probability).
51
4.1 Weighted finite-state transducers
Finite-state technology is a very powerful machin-
ery for Natural Language Processing (Mohri, 1997;
Kornai, 1999; Karttunen, 2001), and in particu-
lar for POS tagging, e.g. (Roche and Schabes,
1995). It is indeed very convenient because it
has simple factorized representations and interest-
ing well-defined mathematical operations. For in-
stance, weighted finite-state transducers (WFST) are
often used to represent probabilistic models such as
Hidden Markov Models. In that case, they map in-
put sequences into output sequences associated with
weights following a probability semiring (R+,+,?,
0, 1) or a log semiring (R ? {??,+?},?log,+,
+?, 0) for numerical stability6. A WFST is a finite-
state automaton which each transition is composed
of an input symbol, an output symbol and a weight.
A path in a WFST is therefore a sequence of consec-
utive transitions of the WFST going from an initial
state to a final state, i.e. it puts a binary relation
between an input sequence and an output sequence
with a weight that is the product of the weights of the
path transitions in a probability semiring (the sum
in the log semiring). Note that a finite-state trans-
ducer is a WFST with no weights. A very nice oper-
ation on WFSTs is composition (Salomaa and Soit-
tola, 1978). Let T1 be a WFST mapping an input
sequence x into an output sequence y with a weight
w1(x, y), and T2 be another WFST mapping a se-
quence y into a sequence z with a weight w2(y, z).
The composition of T1 with T2 results in a WFST T
mapping x into z with a weight w1(x, y).w2(y, z) in
the probability semiring (w1(x, y) + w2(y, z) in the
log semiring).
4.2 Lexical analysis and decoding
The lexical analyzer is driven by lexical resources
represented by finite-state transducers like in (Sil-
berztein, 2000) (cf. section 5) and generates a TFST
containing candidate analyses. Transitions of the
TFST are labeled by a simple token (as input) and
a POS tag (as output). This stage allows for re-
ducing the global ambiguity of the input sentence in
two different ways: (1) tag filtering, i.e. each token
6A semiring K is a 5-tuple (K,?,?, 0?, 1?) where the set K
is equipped with two operations ? and ?; 0? and 1? are their
respective neutral elements. The log semiring is an image of
the probability semiring via the ?log function.
is only assigned its possible tags in the lexical re-
sources; (2) segment filtering, i.e. we only keep lex-
ical multiword units present in the resources. This
implies the use of large-coverage and fine-grained
lexical resources.
The decoding stage selects the most probable path
in the TFST. This involves that the TFST should
be weighted by CRF-based probabilities in order
to apply a shortest path algorithm. Our weighing
procedure consists in composing a WFST encoding
the sentence unigram probabilities (unigram WFST)
and a WFST encoding the bigram probabilities (bi-
gram WFST). The two WFSTs are defined over the
log semiring. The unigram WFST is computed from
the TFST. Each transition corresponds to a (xt,yt)
pair at a given position t in the sentence x. So each
transition is weighted by summing the weights of
the unigram features activated at this position. In our
practical case, bigram features are independent from
the sentence x. The bigram WFST can therefore be
constructed once and for all for the whole tagging
process, in the same way as for order-1 HMM tran-
sition diagrams (Nasr and Volanschi, 2005).
5 Linguistic resources
5.1 French TreeBank
The French Treebank (FTB) is a syntactically an-
notated corpus7 of 569,039 tokens (Abeille? et al,
2003). Each token can be either a punctuation
marker, a number, a simple word or a multiword
unit. At the POS level, it uses a tagset of 14 cate-
gories and 34 sub-categories. This tagset has been
optimized to 29 tags for syntactic parsing (Crabbe?
and Candito, 2008) and reused as a standard in a
POS tagging task (Denis and Sagot, 2009). Below
is a sample of the FTB version annotated in POS.
, PONCT ,
soit CC i.e.
une DET a
augmentation NC raise
de P of
1 , 2 DET 1 , 2
% NC %
par rapport au P+D compared with the
mois NC preceding
pre?ce?dent ADJ month
7It is made of journalistic texts from Le Monde newspaper.
52
Multiword tokens encode multiword units of dif-
ferent types: compound words and named enti-
ties. Compound words mainly include nominals
such as acquis sociaux (social benefits), verbs such
as faire face a` (to face) adverbials like dans l?
imme?diat (right now), prepositions such as en de-
hors de (beside). Some Named Entities are also en-
coded: organization names like Socie?te? suisse de mi-
croe?lectronique et d? horlogerie, family names like
Strauss-Kahn, location names like Afrique du Sud
(South Africa) or New York. For the purpose of our
study, this corpus was divided in three parts: 80%
for training (TRAIN), 10% for development (DEV)
and 10% for testing (TEST).
5.2 Lexical resources
The lexical resources are composed of both mor-
phosyntactic dictionaries and strongly lexicalized
local grammars. Firstly, there are two general-
language dictionaries of simple and multiword
forms: DELA (Courtois, 1990; Courtois et al, 1997)
and Lefff (Sagot, 2010). DELA has been devel-
opped by a team of linguists. Lefff has been au-
tomatically acquired and then manually validated.
It also resulted from the merge of different lexical
sources. In addition, we applied specific manually
built lexicons: Prolex (Piton at al., 1999) contain-
ing toponyms ; others including organization names
and first names (Martineau et al, 2009). Figures on
these dictionaries are detailed in table 4.
Name # simple forms #MW forms
DELA 690,619 272,226
Lefff 553,140 26,311
Prolex 25,190 97,925
Organizations 772 587
First names 22,074 2,220
Table 4: Morphosynctatic dictionaries
This set of dictionaries is completed by a library
of strongly lexicalized local grammars (Gross, 1997;
Silberztein, 2000) that recognize different types of
multiword units such as Named Entities (organiza-
tion names, person names, location names, dates),
locative prepositions, numerical determiners. A lo-
cal grammar is a graph representing a recursive
finite-state transducer, which recognizes sequences
belonging to an algebraic language. Practically, they
describe regular grammars and, as a consequence,
can be compiled into equivalent finite-state trans-
ducers. We used a library of 211 graphs. We man-
ually constructed from those available in the online
library GraalWeb (Constant and Watrin, 2007).
5.3 Lexical resources vs. French Treebank
In this section, we compare the content of the re-
sources described above with the encodings in the
FTB-DEV corpus. We observed that around 97,4%
of lexical units encoded in the corpus (excluding
numbers and punctuation markers) are present in our
lexical resources (in particular, 97% are in the dic-
tionaries). While 5% of the tokens are unknown (i.e.
not present in the training corpus), 1.5% of tokens
are unknown and not present in the lexical resources,
which shows that 70% of unknown words are cov-
ered by our lexical resources.
The segmentation task is mainly driven by the
multiword resources. Therefore, they should match
as much as possible with the multiword units en-
coded in the FTB. Nevertheless, this is practically
very hard to achieve because the definition of MWU
can never be the same between different people as
there exist a continuum between compositional and
non-compositional sequences. In our case, we ob-
served that 75.5% of the multiword units in the FTB-
DEV corpus are in the lexical resources (87.5% in-
cluding training lexicon). This means that 12.5%
of the multiword tokens are totally unknown and,
as a consequence, will be hardly recognized. An-
other significant issue is that many multiword units
present in our resources are not encoded in the FTB.
For instance, many Named Entities like dates, per-
son names, mail addresses, complex numbers are ab-
sent. By applying our lexical resources8 in a longest-
match context-free manner with the platform Unitex
(Paumier, 2011), we manually observed that 30% of
the multiword units found were not considered as
such in the FTB-DEV corpus.
6 Experiments and Evaluation
We firstly evaluated our system for standard tag-
ging without MWU segmentation and compare it
with other available statistical taggers that we all
trained on the FTB-TRAIN corpus. We tested the
8We excluded local grammars recognizing dates, person
names and complex numbers.
53
well-known TreeTagger (Schmid, 1994) based on
probabilistic decision trees, as well as TnT (Brants,
2000) implementing second-order Hidden Markov.
We also compared our system with two existing
discriminative taggers: SVMTool (Gime?nez and
Ma?rquez, 2004) based on Support Vector Models
with language-independent features; MElt (Denis
and Sagot, 2009) based on a Maximum Entropy
model also incorporating language-dependent fea-
ture computed from an external lexicon. The lexicon
used to train and test MElt included all lexical re-
sources9 described in section 5. For our CRF-based
system, we trained two models with CRF++10: (a)
STD using language-independent template features
(i.e. excluding AC-based features); (b) LEX using
all feature templates described in table 2. We note
CRF-STD and CRF-LEX the two related taggers
when no preliminary lexical analysis is performed;
CRF-STD+ and CRF-LEX+ when a lexical analy-
sis is performed. The lexical analysis in our exper-
iment consists in assigning for each token its possi-
ble tags found in the lexical resources11 . Tokens not
found in the resources are assigned all possible tags
in the tagset in order to ensure the system robust-
ness. If no lexical analysis is applied, our system
constructs a TFST representing all possible analyzes
over the tagset. The results obtained on the TEST
corpus are summed up in table 5. Column ACC in-
dicates the tagger accuracy in percentage. We can
observe that our system (CRF-LEX+) outperforms
the other existing taggers, especially MElt whose
authors claimed state-of-the-art results for French.
We can notice the great interest of a lexical analysis
as CRF-STD+ reaches similar results as a MaxEnt
model based on features from an external lexicon.
We then evaluated our MWU-aware tagger
trained on the TRAIN corpus whose complex tokens
have been decomposed in a sequence of simple to-
kens and relabeled in the IOB representation. We
used three different sets of feature templates lead-
9Dictionaries were all put together, as well as with the result
of the application of the local grammars on the corpus.
10CRF++ is an open-source toolkit to train and test CRF mod-
els (http://crfpp.sourceforge.net/). For training, we set the cut-
off threshold for features to 2 and the C value to 1. We also used
the L2 regularization algorithm.
11Practically, as the tagsets of the lexical resources and the
FTB were different, we had to first map tags used in the dictio-
naries into tags belonging to the FTB tagset.
Tagger Model ACC
TnT HMM 96.3
TreeTagger Decision trees 96.4
SVMTool SVM 97.2
CRF-STD CRF 97.4
MElt MaxEnt 97.6
CRF-STD+ CRF 97.6
CRF-LEX CRF 97.7
CRF-LEX+ CRF 97.7
Table 5: Comparison of different taggers for French
ing to three CRF models: CRF-STD,CRF-LEX and
CRF-MWE. The two first ones (STD and LEX) use
the same feature templates as in the previous ex-
periment. MWE includes all feature templates de-
cribed in sections 2 and 3. CRF-MWE+ indicates
that a preliminary lexical analysis is performed be-
fore applying CRF-MWE decoding. The lexical anal-
ysis is achieved by assigning all possible tags of sim-
ple tokens found in our lexical resources, as well as
adding, in the TFST, new transitions corresponding
to MWU segments found in the lexical resources.
We compared the three models with a baseline and
SVMTool that have been learnt on the same training
corpus. The baseline is a simple context-free lookup
in the training MW lexicon, after a standard CRF-
based tagging with no MW segmentation. We eval-
uated each MWU-aware tagger on the decomposed
TEST corpus and computed the f-score, combining
precision and recall12. The results are synthesized
in table 6. The SEG column shows the segmentation
f -score solely taking into account the segment limits
of the identified lexical unit. The TAG column also
accounts for the label assigned. The first observation
is that there is a general drop in the performances for
all taggers, which is not a surprise as regards with
the complexity of MWU recognition (97.7% for the
best standard tagger vs. 94.4% for the best MWU-
aware tagger). Clearly, MWU-aware taggers which
models incorporate features based on external MWU
resources outperform the others. Nevertheless, the
scores for the identification and the tagging of the
MWUs are still rather low: 91%-precision and 71%
recall. We can also see that a preliminary lexical
analysis slightly lower the scores, which is due to
12f-score f = 2prp+r where p is precision and r is recall.
54
missing MWUs in the resources and is a side effect
of missing encodings in the corpus.
Tagger Model TAG SEG
Baseline CRF 91.2 93.6
SVMTool SVM 92.1 94.7
CRF-STD CRF 93.7 95.8
CRF-LEX CRF 93.9 95.9
CRF-MWE CRF 94.4 96.4
CRF-MWE+ CRF 94.3 96.3
Table 6: Evaluation of MWU-aware tagging
With respect to the statistics given in section 5.3,
it appears clearly that the evaluation of MWU-aware
taggers is somewhat biased by the fact that the def-
inition of the multiword units encoded in the FTB
and the ones listed in our lexical resources are not
exactly the same. Nevertheless, this evaluation that
is the first in this context, brings new evidences
on the importance of multiword unit resources for
MWU-aware tagging.
7 Conclusions and Future Work
This paper presented a new part-of-speech tagger in-
cluding multiword unit identification. It is based on
a CRF model integrating language-independent fea-
tures, as well as features computed from external
lexical resources. It was implemented in a finite-
state framework composed of a preliminary finite-
state lexical analysis and a CRF decoding using
weighted finite-state transducer composition. The
tagger is freely available under the LGPL license13.
It allows users to incorporate their own lexicons in
order to easily integrate it in their own applications.
We showed that the tagger reaches state-of-the-art
results for French in the standard evaluation environ-
ment (i.e. each multiword unit is already merged in
a single token). The evaluation of the tagger inte-
grating MWU recognition clearly shows the interest
of incorporating features based on MWU resources.
Nevertheless, as there exist some differences in the
MWU definitions between the lexical resources and
the working corpus, this first experiment requires
further investigations. First of all, we could test our
tagger by incorporating lexicons of MWU automat-
ically extracted from large raw corpora in order to
13http://igm.univ-mlv.fr/?mconstan/research/software
deal with low recall. We could as well combine the
lexical analyzer with a Named Entity Recognizer.
Another step would be to modify the annotations of
the working corpus in order to cover all MWU types
and to make it more homogeneous with our defini-
tion of MWU. Another future work would be to test
semi-CRF models that are well-suited for segmenta-
tion tasks.
References
A. Abeille?, L. Cle?ment and F. Toussenel. 2003. Building
a treebank for French. in A. Abeille? (ed), Treebanks,
Kluwer, Dordrecht.
T. Brants. 2000. TnT - A Statistical Part-of-Speech Tag-
ger. In Proceedings of the Sixth Applied Natural Lan-
guage Processing Conference (ANLP 2000), 224?231.
H. Caseli, C. Ramisch, M. das Graas Volpe Nunes, A.
Villavicencio. 2010. Alignment-based extraction
of multiword expressions. Language Resources and
Evaluation, Springer, vol. 44(1), 59?77.
M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Si-
gogne, S. Billot. 2011. Inte?grer des connaissances lin-
guistiques dans un CRF : application a` l?apprentissage
d?un segmenteur-e?tiqueteur du franc?ais. In Actes de la
Confe?rence sur le traitement automatique des langues
naturelles (TALN?11).
M. Constant and P. Watrin. 2007. Networking Mul-
tiword Units. In Proceedings of the 6th Interna-
tional Conference on Natural Language Processing
(GoTAL?08), Lecture Notes in Artificial Intelligence,
Springer-Verlag, vol. 5221: 120 ? 125.
A. Copestake, F. Lambeau, A. Villavicencio, F. Bond, T.
Baldwin, I. A. Sag and D. Flickinger. 2002. Multi-
word expressions: linguistic precision and reusability.
In Proceedings of the Third conference on Language
Resources and Evaluation (LREC? 02), 1941 ? 1947.
B. Courtois. 1990. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise, vol. 87: 1941 ? 1947.
B. Courtois, M. Garrigues, G. Gross, M. Gross, R.
Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-
Montange, M. Silberztein, R. Vive?s. 1990. Dictio-
nnaire e?lectronique DELAC : les mots compose?s bi-
naires. Technical report, LADL, University Paris 7,
vol. 56.
B. Crabbe? and M. -H. Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franais. In Pro-
ceedings of Traitement des Langues Naturelles (TALN
2008).
P. Denis et B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
55
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation (PACLIC 2009).
G. Dias. 2003. Multiword Unit Hybrid Extraction. In
proceedings of the Workshop on Multiword Expres-
sions of the 41st Annual Meeting of the Association
of Computational Linguistics (ACL 2003), 41?49.
J. Gime?nez and L. Ma?rquez. 2004. SVMTool: A gen-
eral POS tagger generator based on Support Vector
Machines. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04).
M. Gross. 2007. The construction of local grammars. In
E. Roche and Y. Schabes (eds.). Finite-State Language
Processing. The MIT Press, Cambridge, Mass. 329?
352
L. Karttunen. 2001. Applications of Finite-State Trans-
ducers in Natural Language Processing. In proceed-
ings of the 5th International Conference on Implemen-
tation and Application of Automata (CIAA 2000). Lec-
ture Notes in Computer Science. vol. 2088, Springer,
34?46
A. Kornai (Ed.). 1999. Extended Finite State Models of
Language. Cambridge University Press
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML 2001), 282?289.
C. Martineau, T. Nakamura, L. Varga and Stavroula Voy-
atzi. 2009. Annotation et normalisation des entite?s
nomme?es. Arena Romanistica. vol. 4:234?243.
M. Mohri 1997. Finite-state transducers in language
and speech processing. Computational Linguistics 23
(2):269?311.
A. Nasr, A. Volanschi. 2005. Integrating a POS Tagger
and a Chunker Implemented as Weighted Finite State
Machines. Finite-State Methods and Natural Lan-
guage Processing, Lecture Notes in Computer Sci-
ence, vol. 4002, Springer 167?178.
S. Paumier. 2011. Unitex 2.1 user manual.
http://igm.univ-mlv.fr/?unitex.
O. Piton, D. Maurel, C. Belleil. 1999. The Prolex Data
Base : Toponyms and gentiles for NLP. In proceedings
of the Third International Workshop on Applications
of Natural Language to Data Bases (NLDB?99), 233?
237.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the 3rd Workshop on Very Large Corpora, 88 ? 94.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 1996), 133 ? 142.
E. Roche, Y. Schabes. 1995. Deterministic part-of-
speech tagging with finite-state transducers. Compu-
tational Linguistics, MIT Press, vol. 21(2), 227?253
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, D.
Flickinger. 2001. Multiword Expressions: A Pain in
the Neck for NLP. In Proceedings of the 3rd Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2002), 1?15
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC?10).
A. Salomaa, M. Soittola. 1978. Automata-Theoretic As-
pects of Formal Power Series. Springer-Verlag.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. Proceedings of International
Conference on New Methods in Language Processing.
M. Silberztein. 2000. INTEX: an FST toolbox. Theoret-
ical Computer Science, vol. 231 (1): 33?46.
K. Toutanova, D. Klein, C. D. Manning, Y. Yoram
Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. Proceedings of
HLT-NAACL 2003, 252 ? 259.
Y. Tsuruoka, J. Tsujii, S. Ananiadou. 2009. Fast Full
Parsing by Linear-Chain Conditional Random Fields.
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2009), 790?798.
Y. Zhang, S. Clark. 2008. Joint Word Segmentation and
POS Tagging Using a Single Perceptron. Proceedings
of ACL 2008, 888 ? 896.
56
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 46?52,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
The LIGM-Alpage Architecture for the SPMRL 2013 Shared Task:
Multiword Expression Analysis and Dependency Parsing
Matthieu Constant
Universite? Paris-Est
LIGM
CNRS
Marie Candito
Alpage
Paris Diderot Univ
INRIA
Djame? Seddah
Alpage
Paris Sorbonne Univ
INRIA
Abstract
This paper describes the LIGM-Alpage sys-
tem for the SPMRL 2013 Shared Task. We
only participated to the French part of the de-
pendency parsing track, focusing on the real-
istic setting where the system is informed nei-
ther with gold tagging and morphology nor
(more importantly) with gold grouping of to-
kens into multi-word expressions (MWEs).
While the realistic scenario of predicting both
MWEs and syntax has already been investi-
gated for constituency parsing, the SPMRL
2013 shared task datasets offer the possibil-
ity to investigate it in the dependency frame-
work. We obtain the best results for French,
both for overall parsing and for MWE recog-
nition, using a reparsing architecture that com-
bines several parsers, with both pipeline archi-
tecture (MWE recognition followed by pars-
ing), and joint architecture (MWE recognition
performed by the parser).
1 Introduction
As shown by the remarkable permanence over the
years of specialized workshops, multiword expres-
sions (MWEs) identification is still receiving consid-
erable attention. For some languages, such as Ara-
bic, French, English, or German, a large quantity of
MWE resources have been generated (Baldwin and
Nam, 2010). Yet, while special treatment of com-
plex lexical units, such as MWEs, has been shown to
boost performance in tasks such as machine transla-
tion (Pal et al, 2011), there has been relatively little
work exploiting MWE recognition to improve pars-
ing performance.
Indeed, a classical parsing scenario is to pre-
group MWEs using gold MWE annotation (Arun
and Keller, 2005). This non-realistic scenario has
been shown to help parsing (Nivre and Nilsson,
2004; Eryigit et al, 2011), but the situation is quite
different when switching to automatic MWE predic-
tion. In that case, errors in MWE recognition al-
leviate their positive effect on parsing performance
(Constant et al, 2012). While the realistic scenario
of syntactic parsing with automatic MWE recogni-
tion (either done jointly or in a pipeline) has already
been investigated in constituency parsing (Caffer-
key et al, 2007; Green et al, 2011; Constant et al,
2012; Green et al, 2013), the French dataset of the
SPMRL 2013 Shared Task (Seddah et al, 2013) of-
fers one of the first opportunities to evaluate this sce-
nario within the framework of dependency syntax.
In this paper, we discuss the systems we submit-
ted to the SPMRL 2013 shared task. We focused
our participation on the French dependency parsing
track using the predicted morphology scenario, be-
cause it is the only data set that massively contains
MWEs. Our best system ranked first on that track
(for all training set sizes). It is a reparsing system
that makes use of predicted parses obtained both
with pipeline and joint architectures. We applied it
to the French data set only, as we focused on MWE
analysis for dependency parsing. Section 2 gives its
general description, section 3 describes the handling
of MWEs. We detail the underlying parsers in sec-
tion 4 and their combination in section 5. Experi-
ments are described and discussed in sections 6 and
7.
2 System Overview
Our whole system is made of several single statisti-
cal dependency parsing systems whose outputs are
combined into a reparser. We use two types of sin-
46
gle parsing architecture: (a) pipeline systems; (b)
?joint? systems.
The pipeline systems first perform MWE analy-
sis before parsing. The MWE analyzer (section 3)
merges recognized MWEs into single tokens and
the parser is then applied on the sentences with this
new tokenization. The parsing model is learned on
a gold training set where all marked MWEs have
been merged into single tokens. For evaluation, the
merged MWEs appearing in the resulting parses are
expanded, so that the tokens are exactly the same in
gold and predicted parses.
The ?joint? systems directly output dependency
trees whose structure comply with the French
dataset annotation scheme. As shown in Figure 1,
such trees contain not only syntactic dependencies,
but also the grouping of tokens into MWEs, since the
first component of an MWE bears dependencies to
the subsequent components of the MWE with a spe-
cific label dep_cpd. At that stage, the only missing
information is the POS of the MWEs, which we pre-
dict by applying a MWE tagger in a post-processing
step.
la caisse d? e?pargne avait ferme? la veille
suj
de
t
dep
cpd
dep cpd
au
x
tps
mod
dep
cpd
Figure 1: French dependency tree for La caisse
d?e?pargne avait ferme? la veille (The savings bank had
closed the day before), containing two MWEs (in red).
3 MWE Analyzer and MWE Tagger
The MWE analyzer we used in the pipeline sys-
tems is based on Conditional Random Fields (CRF)
(Lafferty et al, 2001) and on external lexicons fol-
lowing (Constant and Tellier, 2012). Given a tok-
enized text, it jointly performs MWE segmentation
and POS tagging (of simple tokens and of MWEs),
both tasks mutually helping each other1. CRF is a
prominent statistical model for sequence segmenta-
1Note though that we keep only the MWE segmentation, and
use rather the Morfette tagger-lemmatizer, cf. section 4.
tion and labelling. External lexicons used as sources
of features greatly improve POS tagging (Denis
and Sagot, 2009) and MWE segmentation (Constant
and Tellier, 2012). Our lexical resources are com-
posed of two large-coverage general-language lexi-
cons: the Lefff2 lexicon (Sagot, 2010), which con-
tains approx. half a million inflected word forms,
among which approx. 25, 000 are MWEs; and the
DELA3 (Courtois, 2009; Courtois et al, 1997) lex-
icon, which contains approx. one million inflected
forms, among which about 110, 000 are MWEs.
These resources are completed with specific lexi-
cons freely available in the platform Unitex4: the
toponym dictionary Prolex (Piton et al, 1999) and a
dictionary of first names.
The MWE tagger we used in the joint systems
takes as input a MWE within a dependency tree, and
outputs its POS. It is a pointwise classifier, based
on a MaxEnt model that integrates different features
capturing the MWE local syntactic context, and in
particular the POS at the token level (and not at
the MWE level). The features comprise: the MWE
form, its lemma, the sequence of POS of its compo-
nents, the POS of its first component, its governor?s
POS in the syntactic parse, the POS following the
MWE, the POS preceding the MWE, the bigram of
the POS following and preceding the MWE.
4 Dependency Parsers
For our development, we trained 3 types of parsers,
both for the pipeline and the joint architecture:
? MALT, a pure linear-complexity transition-
based parser (Nivre et al, 2006)
? Mate-tools 1, the graph-based parser available
in Mate-tools5 (Bohnet, 2010)
? Mate-tools 2, the joint POS tagger and
transition-based parser with graph-based com-
pletion available in Mate-tools (Bohnet and
Nivre, 2012).
2We use the version available in the POS tagger MElt (Denis
and Sagot, 2009).
3We use the version in the platform Unitex (http://igm.univ-
mlv.fr/?unitex). We had to convert the DELA POS tagset to the
FTB one.
4http://igm.univ-mlv.fr/?unitex
5Available at http://code.google.com/p/mate-tools/. We
used the Anna3.3 version.
47
Such parsers require some preprocessing of the
input text: lemmatization, POS tagging, morphol-
ogy analyzer (except the joint POS tagger and
transition-based parser that does not require prepro-
cessed POS tagging). We competed for the scenario
in which this information is not gold but predicted.
Instead of using the predicted POS, lemma and mor-
phological features provided by the shared task orga-
nizers, we decided to retrain the tagger-lemmatizer
Morfette (Chrupa?a et al, 2008; Seddah et al, 2010),
in order to apply a jackknifing on the training set, so
that parsers are made less sensitive to tagging errors.
Note that no feature pertaining to MWEs are used at
this stage.
5 Reparser
The reparser is an adaptation to labeled dependency
parsing of the simplest6 system proposed in (Sagae
and Lavie, 2006). The principle is to build an arc-
factored merge of the parses produced by n input
parsers, and then to find the maximum spanning
tree among the resulting merged graph7. We im-
plemented the maximum spanning tree algorithm8
of (Eisner, 1996) devoted to projective dependency
parsing. During the parse merging, each arc is unla-
beled, and is given a weight, which is the frequency
it appears in the n input parses. Once the maxi-
mum spanning tree is found, each arc is labeled by
its most voted label among the m input parses con-
taining such an arc (with arbitrary choice in case of
ties).
6 Experiments
6.1 Settings
MWE Analysis and Tagging
For the MWE analyzer, we used the tool lgtag-
ger9 (version 1.1) with its default set of feature tem-
6The other more complex systems were producing equiva-
lent scores.
7In order to account for labeled MWE recognition, we in-
tegrated in the ?dep cpd? arcs the POS of the corresponding
MWE. For instance, if the label ?dep cpd? corresponds to an arc
in a multiword preposition (P), the arc is relabeled ?dep cpd P?.
At evaluation time, the output parse labels are remapped to the
official annotation scheme.
8More precisely, we based our implementation on the
pseudo-code given in (McDonald, 2006).
9http://igm.univ-mlv.fr/?mconstan
plates. The MWE tagger model was trained using
the Wapiti software(Lavergne et al, 2010). We used
the default parameters and we forced the MaxEnt
mode.
Parsers
For MALT (version 1.7.2), we used the arceager
algorithm, and the liblinear library for training. As
far as the features are concerned, we started with
the feature templates given in Bonsai10 (Candito et
al., 2010), and we added some templates (essentially
lemma bigrams) during the development tests, that
slightly improved performance. For the two Mate-
tools parsers, we used the default feature sets and
parameters proposed in the documentation.
Morphological prediction
Predicted lemmas, POS and morphology features
are computed with Morfette version 0.3.5 (Chrupa?a
et al, 2008; Seddah et al, 2010)11, using 10 iter-
ations for the tagging perceptron, 3 iterations for
the lemmatization perceptron, default beam size for
the decoding of the joint prediction, and the Lefff
(Sagot, 2010) as external lexicon used for out-of-
vocabulary words. We performed a jackknifing on
the training corpus, with 10 folds for the full corpus,
and 20 folds for the 5k track12.
6.2 Results
We first provide the results on the development cor-
pus. Table 1 shows the general parsing accuracy of
our different systems. Results are displayed in three
different groups corresponding to each kind of sys-
tems: the two single parser architectures ones (joint
and pipeline) and the reparsing one. Each system
was tested both when learned on the full training
data set and on the 5k one. The joint and pipeline
systems were evaluated with the three parsers de-
scribed in section 4. For the reparser, we tested dif-
ferent combinations of parsers in the full training
data set mode. We found that the best combination
includes all parsers but MALT in joint mode. We did
not tune our reparsing system in the 5k training data
set mode. We assumed that the best combination in
this mode was the same as with full training.
10http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html
11Available at https://sites.google.com/site/morfetteweb/
12Note that for the 5k track, we retrained Morfette using the
5k training corpus only, whereas the official 5k training set con-
tains predicted morphology trained on the full training set.
48
full 5k
type parser LAS UAS LaS LAS UAS LaS
Joint
MALT 80.91 84.74 89.18 78.61 83.16 87.51
Mate-tools 1 84.60 88.21 91.43 82.02 86.23 90.02
Mate-tools 2 84.40 88.08 91.02 81.66 85.97 89.38
Pipeline
MALT 82.56 86.22 90.22 80.79 84.71 89.19
Mate-tools 1 85.28 88.73 91.85 83.23 86.97 90.67
Mate-tools 2 84.82 88.31 91.45 82.79 86.56 90.26
Reparser
joint only 85.28 88.77 91.70 - - -
pipeline only 85.79 89.17 91.94 - - -
all 86.12 89.36 92.22 - - -
best ensemble 86.23 89.55 92.21 84.25 87.88 91.17
Table 1: Parsing results on development corpus (38820 tokens)
COMP MWE MWE+POS
R P F R P F R P F
joint Mate-tools 1 76.3 82.4 79.2 74.3 80.6 77.3 70.7 76.7 73.6
pipeline Mate-tools 1 80.8 82.7 81.7 79.0 83.6 81.2 75.6 80.1 77.8
best reparser 81.1 82.5 81.8 79.2 83.0 81.0 76.1 79.8 77.9
Table 2: MWE Results on the development corpus (2119 MWEs) with full training.
Table 2 contains the MWE results on the devel-
opment data set with full training, for three systems:
the best single-parser joint and pipeline systems (i.e.
with Mate-tools 1) and the best reparser. We do not
provide results for the 5k training because they show
similar trends. We provide the 9 MWE-related mea-
sures defined in the shared task. The symbols R, P
and F respectively correspond to recall, precision
and F-measure. COMP corresponds to evaluation
of the non-head MWE components (i.e. the non-first
MWE components, cf. Figure 1). MWE corresponds
to the recognition of a complete MWE. MWE+POS
stands for the recognition of a complete MWE asso-
ciated with its correct POS.
We submitted to the shared task our best
(reparser) system according to the tuning described
above. We also sent the two best pipeline systems
(Mate-tools 1 and Mate-tools 2) and the best joint
system (Mate-tools 1), in order to compare our sin-
gle systems to the other competitors. The official re-
sults of our systems are provided in table 3 for gen-
eral parsing and in table 4 for MWE recognition. We
also show the ranking of each of these systems in the
competition.
7 Discussion
In table 3, we can note that for the 5k training set
scenario, there is a general drop of parsing perfor-
mance (approximately 2 points), but the trends are
exactly the same as for the full training set sce-
nario. Concerning the performance on MWE analy-
sis (table 4), the pipeline Mate-tools-1 system very
slightly outperforms the best reparser system in the
5k scenario, contrary to the full training set scenario,
but the difference is not significant. In the following,
we focus on the full training set scenario.
Let us first discuss the overall parsing perfor-
mance, by looking at the results on the develop-
ment corpus (table 1). As far as the single-parser
systems are concerned, we can note that for both
the joint and pipeline systems, MALT achieves
lower performance than the graph-based (Mate-
tools-1) and the joint tagger-parser (Mate-tools-2),
which have comparable performance. Moreover,
the pipeline systems achieve overall better than their
joint counterpart, though the increase between joint
and pipeline architecture is much bigger for MALT
than for the Mate parsers (for MALT, compare
49
training type parser LAS UAS LaS Rank
Full
Reparser best 85.86 89.19 92.20 1
Pipeline Mate-tools 1 84.91 88.35 91.73 3
Pipeline Mate-tools 2 84.87 88.40 91.51 4
Joint Mate-tools 1 84.14 87.67 91.24 7
5k
Reparser best 83.60 87.40 90.76 1
Pipeline Mate-tools 1 82.53 86.51 90.14 4
Pipeline Mate-tools 2 82.15 86.18 89.79 6
Joint Mate-tools 1 81.63 85.76 89.56 7
Table 3: Official parsing results on the evaluation corpus (75216 tokens)
training type parser COMP MWE MWE+POS Rank
Full
Reparser best ensemble 81.3 80.7 77.5 1
Pipeline Mate-tools 1 81.2 80.8 77.4 2
Pipeline Mate-tools 2 81.2 80.8 76.6 3
Joint Mate-tools 1 79.6 77.4 74.1 6
5k
Pipeline Mate-tools 1 78.7 77.7 74.0 1
Reparser best ensemble 78.9 77.2 73.8 2
Pipeline Mate-tools 2 78.7 77.7 73.3 5
Joint Mate-tools 1 75.9 72.2 75.9 10
Table 4: Official MWE results on the evaluation corpus (4043 MWEs). The scores correspond to the F-measure.
LAS=80.91 for the joint system, and LAS=82.56
for the pipeline architecture, while for Mate-tools-
1, compare LAS=84.60 with LAS=85.28). The best
reparser system provides a performance increase of
approximately one point over the best single-parser
system (Mate-tools-1), both for LAS and UAS,
which suggests that the parsers have complementary
strengths.
When looking at performance on MWE recog-
nition and tagging (2), we can note greater varia-
tion between the F-measures obtained by the single-
parser systems, but this is due to the much lower
number of MWEs with respect to the number of
tokens (there are 38820 tokens and 2119 MWEs
in the dev set). The MWE analyzer used in the
pipeline systems leads to better MWE recognition
(F ? measure = 81.2 on dev set) than when the
analysis is left to the bare ?joint? parsers (joint Mate-
tools 1 achieves F-measure= 77.3).
Contrary to the situation for overall parsing per-
formance, the reparser system does not lead to better
MWE recognition with respect to the MWE analyzer
of the pipeline systems. Indeed the performance on
MWEs are quite similar between the reparser sys-
tem and the MWE analyzer (for the MWE metric,
on the dev set we get F=81.0 versus 81.2 for best
reparser and pipeline systems respectively, whereas
we get 80.7 and 80.8 on the test set. These differ-
ences are not significant). This is because the MWEs
predicted by the MWE analyzer are present in three
of the single-parser systems taken into account in the
reparsing process, and are thus much favored in the
voting.
In order to understand better our parsing systems?
performance on MWE recognition, we provide in ta-
ble 5 the MWE+POS results broken down by MWE
part-of-speech, for the dev set. Not surprisingly,
we can note that performance varies greatly de-
pending on the POS, with better performance on
closed classes (conjunctions, determiners, preposi-
tions, pronouns) than on open classes. The lowest
performance is on adjectives and verbs, but given the
raw numbers of gold MWEs, the major impact on
overall performance is given by the results on nom-
inal MWEs (either common or proper nouns). A lit-
tle less than one third of the nominal gold MWEs
50
R P F Nb gold Nb predicted Nb correct
adjectives 46.9 75.0 57.7 32 20 15
adverbs 74.7 83.0 78.7 360 324 269
conjunctions 90.1 83.7 86.8 91 98 82
clitics - 0.00 - 0 1 0
determiners 96.0 96.8 96.4 252 250 242
nouns 72.7 76.2 74.4 973 928 707
prepositions 84.6 84.9 84.8 345 344 292
pronouns 75.0 87.5 80.8 28 24 21
verbs 66.7 66.7 66.67 33 33 22
unknown 0 0 0 5 0 0
ALL 77.9 81.6 79.7 2119 2022 1650
Table 5: MWE+POS results on the development corpus, broken down by POS (recall, precision, F-measure, number
of gold MWEs, predicted MWEs, correct MWEs with such POS.
is not recognized (R = 72.7), and about one quar-
ter of the predicted nominal MWEs are wrong (P =
76.2). Though these results can be partly explained
by some inconsistencies in MWE annotation in the
French Treebank (Constant et al, 2012), there re-
mains room for improvement for open class MWE
recognition.
8 Conclusion
We have described the LIGM-Alpage system for the
SPMRL 2013 shared task, restricted to the French
track. We provide the best results for the realistic
scenario of predicting both MWEs and dependency
syntax, using a reparsing architecture that combines
several parsers, both pipeline (MWE recognition
followed by parsing) and joint (MWE recognition
performed by the parser). In the future, we plan to
integrate features specific to MWEs into the joint
system, so that the reparser outperforms both the
joint and pipeline systems, not only on parsing (as
it is currently the case) but also on MWE recogni-
tion.
References
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of french. In
Proceedings of the Annual Meeting of the Association
For Computational Linguistics (ACL?05), pages 306?
313.
T. Baldwin and K.S. Nam. 2010. Multiword expressions.
In Handbook of Natural Language Processing, Second
Edition. CRC Press, Taylor and Francis Group.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1455?1465,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING?10), Beijing, China.
C. Cafferkey, D. Hogan, and J. van Genabith. 2007.
Multi-word units in treebank-based probabilistic pars-
ing and generation. In Proceedings of the 10th Inter-
national Conference on Recent Advances in Natural
Language Processing (RANLP?07).
M.-H. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010. Benchmarking of statistical depen-
dency parsers for french. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING?10), Beijing, China.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette.
In In Proc. of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Matthieu Constant and Isabelle Tellier. 2012. Evaluat-
ing the impact of external lexical resources into a crf-
based multiword segmenter and part-of-speech tagger.
In Proceedings of the 8th conference on Language Re-
sources and Evaluation (LREC?12).
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate mul-
51
tiword expression recognition and parsing. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Volume
1, ACL ?12, pages 204?212, Stroudsburg, PA, USA.
Association for Computational Linguistics.
B. Courtois, M. Garrigues, G. Gross, M. Gross,
R. Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-
Montange, M. Silberztein, and R. Vive?s. 1997. Dic-
tionnaire e?lectronique DELAC : les mots compose?s
binaires. Technical Report 56, University Paris 7,
LADL.
B. Courtois. 2009. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise, 87:11?22.
P. Denis and B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation (PACLIC?09), pages 110?
119.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of the 16th conference on Computational lin-
guistics - Volume 1, COLING ?96, pages 340?345,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
G. Eryigit, T. Ilbay, and O. Arkan Can. 2011. Multiword
expressions in statistical dependency parsing. In Pro-
ceedings of the IWPT Workshop on Statistical Pars-
ing of Morphologically-Rich Languages (SPRML?11),
pages 45?55.
S. Green, M.-C. de Marneffe, J. Bauer, and C. D. Man-
ning. 2011. Multiword expression identification with
tree substitution grammars: A parsing tour de force
with french. In Proceedings of the conference on
Empirical Method for Natural Language Processing
(EMNLP?11), pages 725?735.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML?01), pages 282?289.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL?10), pages 504?513.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and J. Nilsson. 2004. Multiword units in syn-
tactic parsing. In Proceedings of Methodologies and
Evaluation of Multiword Units in Real-World Applica-
tions (MEMURA).
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proceedings of the fifth international conference
on Language Resources and Evaluation (LREC?06),
pages 2216?2219, Genoa, Italy.
Santanu Pal, Tanmoy Chkraborty, and Sivaji Bandy-
opadhyay. 2011. Handling multiword expressions
in phrase-based statistical machine translation. In
Proceedings of the Machine Translation Summit XIII,
pages 215?224.
O. Piton, D. Maurel, and C. Belleil. 1999. The prolex
data base : Toponyms and gentiles for nlp. In Proceed-
ings of the Third International Workshop on Applica-
tions of Natural Language to Data Bases (NLDB?99),
pages 233?237.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of the Human Language
Technology Conference of the NAACL, Companion
Volume: Short Papers, NAACL-Short ?06, pages 129?
132, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
B. Sagot. 2010. The lefff, a freely available, accurate
and large-coverage lexicon for french. In Proceedings
of the 7th International Conference on Language Re-
sources and Evaluation (LREC?10).
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proc. of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Djame? Seddah, Reut Tsarfaty, Sandra K??ubler, Marie
Candito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Gold-
berg, Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Joakim Nivre, Adam Przepi-
orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Wolin?ski, Alina
Wro?blewska, and Eric Villemonte de la Cle?rgerie.
2013. Overview of the spmrl 2013 shared task: A
cross-framework evaluation of parsing morphologi-
cally rich languages. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich
Languages: Shared Task, Seattle, WA.
52
