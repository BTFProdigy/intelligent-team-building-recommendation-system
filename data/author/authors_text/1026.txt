Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773?782,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Summarizing Spoken and Written Conversations
Gabriel Murray and Giuseppe Carenini
Department of Computer Science
University of British Columbia
Vancouver, BC V6T 1Z4 Canada
Abstract
In this paper we describe research on sum-
marizing conversations in the meetings and
emails domains. We introduce a conver-
sation summarization system that works in
multiple domains utilizing general conversa-
tional features, and compare our results with
domain-dependent systems for meeting and
email data. We find that by treating meet-
ings and emails as conversations with general
conversational features in common, we can
achieve competitive results with state-of-the-
art systems that rely on more domain-specific
features.
1 Introduction
Our lives are increasingly comprised of multimodal
conversations with others. We email for business
and personal purposes, attend meetings in person
and remotely, chat online, and participate in blog or
forum discussions. It is clear that automatic summa-
rization can be of benefit in dealing with this over-
whelming amount of interactional information. Au-
tomatic meeting abstracts would allow us to prepare
for an upcoming meeting or review the decisions of a
previous group. Email summaries would aid corpo-
rate memory and provide efficient indices into large
mail folders.
When summarizing in each of these domains,
there will be potentially useful domain-specific fea-
tures ? e.g. prosodic features for meeting speech,
subject headers for emails ? but there are also un-
derlying similarites between these domains. They
are all multiparty conversations, and we hypothe-
size that effective summarization techniques can be
designed that would lead to robust summarization
performance on a wide array of such conversation
types. Such a general conversation summarization
system would make it possible to summarize a wide
variety of conversational data without needing to
develop unique summarizers in each domain and
across modalities. While progress has been made in
summarizing conversations in individual domains,
as described below, little or no work has been done
on summarizing unrestricted, multimodal conversa-
tions.
In this research we take an extractive approach
to summarization, presenting a novel set of conver-
sational features for locating the most salient sen-
tences in meeting speech and emails. We demon-
strate that using these conversational features in a
machine-learning sentence classification framework
yields performance that is competitive or superior
to more restricted domain-specific systems, while
having the advantage of being portable across con-
versational modalities. The robust performance of
the conversation-based system is attested via several
summarization evaluation techniques, and we give
an in-depth analysis of the effectiveness of the indi-
vidual features and feature subclasses used.
2 Related Work on Meetings and Emails
In this section we give a brief overview of previous
research on meeting summarization and email sum-
marization, respectively.
773
2.1 Meeting Summarization
Among early work on meeting summarization,
Waibel et al (1998) implemented a modified version
of the Maximal Marginal Relevance algorithm (Car-
bonell and Goldstein, 1998) applied to speech tran-
scripts, presenting the user with the n best sentences
in a meeting browser interface. Zechner (2002) in-
vestigated summarizing several genres of speech, in-
cluding spontaneous meeting speech. Though rele-
vance detection in his work relied largely on tf.idf
scores, Zechner also explored cross-speaker infor-
mation linking and question/answer detection.
More recently, researchers have investigated
the utility of employing speech-specific features
for summarization, including prosodic information.
Murray et al (2005a; 2005b) compared purely
textual summarization approaches with feature-
based approaches incorporating prosodic features,
with human judges favoring the feature-based ap-
proaches. In subsequent work (2006; 2007), they
began to look at additional speech-specific char-
acteristics such as speaker status, discourse mark-
ers and high-level meta comments in meetings, i.e.
comments that refer to the meeting itself. Galley
(2006) used skip-chain Conditional Random Fields
to model pragmatic dependencies between paired
meeting utterances (e.g. QUESTION-ANSWER re-
lations), and used a combination of lexical, prosodic,
structural and discourse features to rank utterances
by importance. Galley found that while the most
useful single feature class was lexical features, a
combination of acoustic, durational and structural
features exhibited comparable performance accord-
ing to Pyramid evaluation.
2.2 Email Summarization
Work on email summarization can be divided into
summarization of individual email messages and
summarization of entire email threads. Muresan et
al. (2001) took the approach of summarizing indi-
vidual email messages, first using linguistic tech-
niques to extract noun phrases and then employ-
ing machine learning methods to label the extracted
noun phrases as salient or not. Corston-Oliver et al
(2004) focused on identifying speech acts within a
given email, with a particular interest in task-related
sentences.
Rambow et al (2004) addressed the challenge of
summarizing entire threads by treating it as a binary
sentence classification task. They considered three
types of features: basic features that simply treat the
email as text (e.g. tf.idf, which scores words highly if
they are frequent in the document but rare across all
documents), features that consider the thread to be a
sequence of turns (e.g. the position of the turn in the
thread), and email-specific features such as number
of recipients and subject line similarity.
Carenini et al (2007) took an approach to thread
summarization using the Enron corpus (described
below) wherein the thread is represented as a
fragment quotation graph. A single node in the
graph represents an email fragment, a portion of
the email that behaves as a unit in a fine-grain
representation of the conversation structure. A
fragment sometimes consists of an entire email and
sometimes a portion of an email. For example, if a
given email has the structure
A
> B
C
where B is a quoted section in the middle of
the email, then there are three email fragments in
total: two new fragments A and C separated by
one quoted fragment B. Sentences in a fragment
are weighted according to the Clue Word Score
(CWS) measure, a lexical cohesion metric based
on the recurrence of words in parent and child
nodes. In subsequent work, Carenini et al (2008)
determined that subjectivity detection (i.e., whether
the sentence contains sentiments or opinions from
the author) gave additional improvement for email
thread summaries.
Also on the Enron corpus, Zajic et al (2008) com-
pared Collective Message Summarization (CMS)
to Individual Message Summarization (IMS) and
found the former to be a more effective technique
for summarizing email data. CMS essentially treats
thread summarization as a multi-document summa-
rization problem, while IMS summarizes individual
emails in the thread and then concatenates them to
form a thread summary.
In our work described below we also address the
task of thread summarization as opposed to sum-
774
marization of individual email messages, following
Carenini et al and the CMS approach of Zajic et al
3 Experimental Setup
In this section we describe the classifier employed
for our machine learning experiments, the corpora
used, the relevant summarization annotations for
each corpus, and the evaluation methods employed.
3.1 Statistical Classifier
Our approach to extractive summarization views
sentence extraction as a classification problem. For
all machine learning experiments, we utilize logistic
regression classifiers. This choice was partly moti-
vated by our earlier summarization research, where
logistic regression classifiers were compared along-
side support vector machines (SVMs) (Cortes and
Vapnik, 1995). The two classifier types yielded very
similar results, with logistic regression classifiers
being much faster to train and thus expediting fur-
ther development.
The liblinear toolkit 1 implements simple feature
subset selection based on the F statistic (Chen and
Lin, 2006) .
3.2 Corpora Description
For these experiments we utilize two corpora, the
Enron corpus for email summarization and the AMI
corpus for meeting summarization.
3.2.1 The Enron Email Corpus
The Enron email corpus2 is a collection of emails
released as part of the investigation into the Enron
corporation (Klimt and Yang, 2004). It has become
a popular corpus for NLP research (e.g. (Bekkerman
et al, 2004; Yeh and Harnly, 2006; Chapanond et al,
2005; Diesner et al, 2005)) due to being realistic,
naturally-occurring data from a corporate environ-
ment, and moreover because privacy concerns mean
that there is very low availability for other publicly
available email data.
39 threads have been annotated for extractive
summarization, with five annotators assigned to
each thread. The annotators were asked to select
30% of the sentences in a thread, subsequently la-
beling each selected sentence as either ?essential? or
1http://www.csie.ntu.edu.tw/?cjlin/liblinear/
2http://www.cs.cmu.edu/?enron/
?optional.? Essential sentences are weighted three
times as highly as optional sentences. A sentence
score, or GSValue, can therefore range between 0
and 15, with the maximum GSValue achieved when
all five annotators consider the sentence essential,
and a score of 0 achieved when no annotator selects
the given sentence. For the purpose of training a bi-
nary classifier, we rank the sentences in each email
thread according to their GSValues, then extract sen-
tences until our summary reaches 30% of the to-
tal thread word count. We label these sentences as
positive instances and the remainder as the negative
class. Approximately 19% of sentences are labeled
as positive, extractive examples.
Because the amount of labeled data available for
the Enron email corpus is fairly small, for our classi-
fication experiments we employ a leave-one-out pro-
ceedure for the 39 email threads. The labeled data as
a whole total just under 1400 sentences.
3.2.2 The AMI Meetings Corpus
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Carletta
et al, 2005). The corpus consists of about 100 hours
of recorded and annotated meetings. In the scenario
meetings, groups of four participants take part in a
series of four meetings and play roles within a ficti-
tious company. While the scenario given to them is
artificial, the speech and the actions are completely
spontaneous and natural. There are 96 meetings in
the training set, 24 in the development set, and 20
meetings for the test set.
For this corpus, annotators wrote abstract sum-
maries of each meeting and extracted transcript dia-
logue act segments (DAs) that best conveyed or sup-
ported the information in the abstracts. A many-
to-many mapping between transcript DAs and sen-
tences from the human abstract was obtained for
each annotator, with three annotators assigned to
each meeting. It is possible for a DA to be extracted
by an annotator but not linked to the abstract, but for
training our binary classifiers, we simply consider a
dialogue act to be a positive example if it is linked
to a given human summary, and a negative example
otherwise. This is done to maximize the likelihood
that a data point labeled as ?extractive? is truly an
informative example for training purposes. Approx-
imately 13% of the total DAs are ultimately labeled
775
as positive, extractive examples.
The AMI corpus contains automatic speech
recognition (ASR) output in addition to manual
meeting transcripts, and we report results on both
transcript types. The ASR output was provided by
the AMI-ASR team (Hain et al, 2007), and the word
error rate for the AMI corpus is 38.9%.
3.3 Summarization Evaluation
For evaluating our extractive summaries, we imple-
ment existing evaluation schemes from previous re-
search, with somewhat similar methods for meet-
ings versus emails. These are described and com-
pared below. We also evaluate our extractive classi-
fiers more generally by plotting the receiver operator
characteristic (ROC) curve and calculating the area
under the curve (AUROC). This allows us to gauge
the true-positive/false-positive ratio as the posterior
threshold is varied.
We use the differing evaluation metrics for emails
versus meetings for two primary reasons. First,
the differing summarization annotations in the AMI
and Enron corpora naturally lend themselves to
slightly divergent metrics, one based on extract-
abstract links and the other based on the essen-
tial/option/uninformative distinction. Second, and
more importantly, using these two metrics allow us
to compare our results with state-of-the-art results
in the two fields of speech summarization and email
summarization. In future work we plan to use a sin-
gle evaluation metric.
3.3.1 Evaluating Meeting Summaries
To evaluate meeting summaries we use the
weighted f-measure metric (Murray et al, 2006).
This evaluation scheme relies on the multiple human
annotated summary links described in Section 3.2.2.
Both weighted precision and recall share the same
numerator
num =
M
?
i=1
N
?
j=1
L(si, aj) (1)
where L(si, aj) is the number of links for a DA
si in the machine extractive summary according to
annotator ai, M is the number of DAs in the ma-
chine summary, and N is the number of annotators.
Weighted precision is defined as:
precision = numN ? M (2)
and weighted recall is given by
recall = num
?O
i=1
?N
j=1 L(si, aj)
(3)
where O is the total number of DAs in the meeting,
N is the number of annotators, and the denominator
represents the total number of links made between
DAs and abstract sentences by all annotators. The
weighted f-measure is calculated as the harmonic
mean of weighted precision and recall. The intuition
behind weighted f-score is that DAs that are linked
multiple times by multiple annotators are the most
informative.
3.3.2 Evaluating Email Summaries
For evaluating email thread summaries, we follow
Carenini et al (2008) by implementing their pyra-
mid precision scheme, inspired by Nenkova?s pyra-
mid scheme (2004). In Section 3.2.1 we introduced
the idea of a GSValue for each sentence in an email
thread, based on multiple human annotations. We
can evaluate a summary of a given length by com-
paring its total GSValues to the maximum possible
total for that summary length. For instance, if in a
thread the three top scoring sentences had GSValues
of 15, 12 and 12, and the sentences selected by a
given automatic summarization method had GSVal-
ues of 15, 10 and 8, the pyramid precision would be
0.85.
Pyramid precision and weighted f-score are simi-
lar evaluation schemes in that they are both sentence
based (as opposed to, for example, n-gram based)
and that they score sentences based on multiple hu-
man annotations. Pyramid precision is very simi-
lar to equation 3 normalized by the maximum score
for the summary length. For now we use these two
slightly different schemes in order to maintain con-
sistency with prior art in each domain.
4 A Conversation Summarization System
In our conversation summarization approach, we
treat emails and meetings as conversations com-
prised of turns between multiple participants. We
follow Carenini et al (2007) in working at the finer
776
granularity of email fragments, so that for an email
thread, a turn consists of a single email fragment in
the exchange. For meetings, a turn is a sequence of
dialogue acts by one speaker, with the turn bound-
aries delimited by dialogue acts from other meet-
ing participants. The features we derive for summa-
rization are based on this view of the conversational
structure.
We calculate two length features. For each sen-
tence, we derive a word-count feature normalized
by the longest sentence in the conversation (SLEN)
and a word-count feature normalized by the longest
sentence in the turn (SLEN2). Sentence length has
previously been found to be an effective feature in
speech and text summarization (e.g. (Maskey and
Hirschberg, 2005; Murray et al, 2005a; Galley,
2006)).
There are several structural features used, in-
cluding position of the sentence in the turn (TLOC)
and position of the sentence in the conversation
(CLOC). We also include the time from the begin-
ning of the conversation to the current turn (TPOS1)
and from the current turn to the end of the conversa-
tion (TPOS2). Conversations in both modalities can
be well-structured, with introductory turns, general
discussion, and ultimate resolution or closure, and
sentence informativeness might significantly corre-
late with this structure. We calculate two pause-style
features: the time between the following turn and the
current turn (SPAU), and the time between the cur-
rent turn and previous turn (PPAU), both normalized
by the overall length of the conversation. These fea-
tures are based on the email and meeting transcript
timestamps. We hypothesize that pause features may
be useful if informative turns tend to elicit a large
number of responses in a short period of time, or if
they tend to quickly follow a preceding turn, to give
two examples.
There are two features related to the conversation
participants directly. One measures how dominant
the current participant is in terms of words in the
conversation (DOM), and the other is a binary fea-
ture indicating whether the current participant ini-
tiated the conversation (BEGAUTH), based simply
on whether they were the first contributor. It is hy-
pothesized that informative sentences may more of-
ten belong to participants who lead the conversation
or have a good deal of dominance in the discussion.
There are several lexical features used in these
experiments. For each unique word, we calculate
two conditional probabilities. For each conversation
participant, we calculate the probability of the par-
ticipant given the word, estimating the probability
from the actual term counts, and take the maximum
of these conditional probabilities as our first term
score, which we will call Sprob.
Sprob(t) = max
S
p(S|t)
where t is the word and S is a participant. For ex-
ample, if the word budget is used ten times in total,
with seven uses by participant A, three uses by par-
ticipant B and no uses by the other participants, then
the Sprob score for this term is 0.70. The intuition
is that certain words will tend to be associated with
one conversation participant more than the others,
owing to varying interests and expertise between the
people involved.
Using the same procedure, we calculate a score
called Tprob based on the probability of each turn
given the word.
Tprob(t) = max
T
p(T |t)
The motivating factor for this metric is that certain
words will tend to cluster into a small number of
turns, owing to shifting topics within a conversation.
Having derived Sprob and Tprob, we then calcu-
late several sentence-level features based on these
term scores. Each sentence has features related to
max, mean and sum of the term scores for the
words in that sentence (MXS, MNS and SMS for
Sprob, and MXT, MNT and SMT for Tprob). Us-
ing a vector representation, we calculate the cosine
between the conversation preceding the given sen-
tence and the conversation subsequent to the sen-
tence, first using Sprob as the vector weights (COS1)
and then using Tprob as the vector weights (COS2).
This is motivated by the hypothesis that informative
sentences might change the conversation in some
fashion, leading to a low cosine between the preced-
ing and subsequent portions. We similarly calculate
two scores measuring the cosine between the cur-
rent sentence and the rest of the converation, using
each term-weight metric as vector weights (CENT1
for Sprob and CENT2 for Tprob). This measures
777
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cos. of conv. splits, w/ Sprob
COS2 cos. of conv. splits, w/ Tprob
PENT entro. of conv. up to sentence
SENT entro. of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob
Table 1: Features Key
whether the candidate sentence is generally similar
to the conversation overall.
There are three word entropy features, calculated
using the formula
went(s) =
?N
i=1 p(xi) ? ? log(p(xi))
( 1N ? ? log( 1N )) ? M
where s is a string of words, xi is a word type
in that string, p(xi) is the probability of the word
based on its normalized frequency in the string, N
is the number of word types in the string, and M is
the number of word tokens in the string.
Note that word entropy essentially captures infor-
mation about type-token ratios. For example, if each
word token in the string was a unique type then the
word entropy score would be 1. We calculate the
word entropy of the current sentence (THISENT),
as well as the word entropy for the conversation up
until the current sentence (PENT) and the word en-
tropy for the conversation subsequent to the current
sentence (SENT). We hypothesize that informative
sentences themselves may have a diversity of word
types, and that if they represent turning points in the
conversation they may affect the entropy of the sub-
sequent conversation.
Finally, we include a feature that is a rough ap-
proximation of the ClueWordScore (CWS) used by
Carenini et al (2007). For each sentence we remove
stopwords and count the number of words that occur
in other turns besides the current turn. The CWS is
therefore a measure of conversation cohesion.
For ease of reference, we hereafter refer to this
conversation features system as ConverSumm.
5 Comparison Summarization Systems
In order to compare the ConverSumm system with
state-of-the-art systems for meeting and email sum-
marization, respectively, we also present results us-
ing the features described by Murray and Renals
(2008) for meetings and the features described by
Rambow (2004) for email. Because the work by
Murray and Renals used the same dataset, we can
compare our scores directly. However, Rambow car-
ried out summarization work on a different, unavail-
able email corpus, and so we re-implemented their
summarization system for our current email data.
In their work on meeting summarization, Murray
and Renals creating 700-word summaries of each
meeting using several classes of features: prosodic,
lexical, structural and speaker-related. While there
are two features overlapping between our systems
(word-count and speaker/participant dominance),
their system is primarily domain-dependent in its
use of prosodic features while our features represent
a more general conversational view.
Rambow presented 14 features for the summa-
rization task, including email-specific information
such as the number of recipients, number of re-
sponses, and subject line overlap. There is again a
slight overlap in features between our two systems,
as we both include length and position of the sen-
tence in the thread/conversation.
6 Results
Here we present, in turn, the summarization results
for meeting and email data.
6.1 Meeting Summarization Results
Figure 1 shows the F statistics for each Conver-
summ feature in the meeting data, providing a mea-
sure of the usefulness of each feature in discriminat-
ing between the positive and negative classes. Some
778
 0
 0.05
 0.1
 0.15
 0.2
 0.25
CENT2
CENT1
CWS
BEGAUTH
SPAU
PPAU
THISENT
SENT
PENT
COS2
COS1
DOM
TPOS2
TPOS1
SLEN2
SLEN
CLOC
TLOC
SMT
MNT
MXT
SMS
MNS
MXS
f s
ta
tis
tic
feature ID (see key)
manual
ASR
Figure 1: Feature F statistics for AMI meeting corpus
System Weighted F-Score AUROC
Speech - Man 0.23 0.855
Speech - ASR 0.24 0.850
Conv. - Man 0.23 0.852
Conv. - ASR 0.22 0.853
Table 2: Weighted F-Scores and AUROCs for Meeting
Summaries
features such as participant dominance have very
low F statistics because each sentence by a given
participant will receive the same score; so while the
feature itself may have a low score because it does
not discriminate informative versus non-informative
sentences on its own, it may well be useful in con-
junction with the other features. The best individual
ConverSumm features for meeting summarization
are sentence length (SLEN), sum of Sprob scores,
sum of Tprob scores, the simplified CWS score
(CWS), and the two centroid measures (CENT1 and
CENT2). The word entropy of the candidate sen-
tence is very effective for manual transcripts but
much less effective on ASR output. This is due to
the fact that ASR errors can incorrectly lead to high
entropy scores.
Table 2 provides the weighted f-scores for all
summaries of the meeting data, as well as AUROC
scores for the classifiers themselves. For our 700-
word summaries, the Conversumm approach scores
comparably to the speech-specific approach on both
manual and ASR transcripts according to weighted
f-score. There are no significant differences accord-
ing to paired t-test. For the AUROC measures, there
are again no significant differences between the con-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
lexical features
structural features
participant features
length features
Fea. Subset AUROC
Structural 0.652
Participant 0.535
Length 0.837
Lexical 0.852
Figure 2: AUROC Values for Feature Subclasses, AMI
Corpus
versation summarizers and speech-specific summa-
rizers. The AUROC for the conversation system
is slightly lower on manual transcripts and slightly
higher when applied to ASR output.
For all systems the weighted f-scores are some-
what low. This is partly owing to the fact that out-
put summaries are very short, leading to high pre-
cision and low recall. The low f-scores are also in-
dicative of the difficulty of the task. Human perfor-
mance, gauged by comparing each annotator?s sum-
maries to the remaining annotators? summaries, ex-
hibits an average weighted f-score of 0.47 on the
same test set. The average kappa value on the test set
is 0.48, showing the relatively low inter-annotator
agreement that is typical of summarization annota-
tion. There is no additional benefit to combining the
conversational and speech-specific features. In that
case, the weighted f-scores are 0.23 for both manual
and ASR transcripts. The overall AUROC is 0.85
for manual transcripts and 0.86 for ASR.
We can expand the features analysis by consid-
ering the effectiveness of certain subclasses of fea-
tures. Specifically, we group the summarization fea-
tures into lexical, structural, participant and length
features. Figure 2 shows the AUROCs for the fea-
ture subset classifiers, illustrating that the lexical
subclass is very effective while the length features
also constitute a challenging baseline. A weakness
779
System Pyramid Precision AUROC
Rambow 0.50 0.64
Conv. 0.46 0.75
Table 3: Pyramid Precision and AUROCs for Email Sum-
maries
of systems that depend heavily on length features,
however, is that recall scores tend to decrease be-
cause the extracted units are much longer - weighted
recall scores for the 700 word summaries are sig-
nificantly worse according to paired t-test (p<0.05)
when using just length features compared to the full
feature set.
6.2 Email Summarization Results
Figure 3 shows the F statistic for each ConverSumm
feature in the email data.The two most useful fea-
tures are sentence length and CWS. The Sprob and
Tprob features rate very well according to the F
statistic. The two centroid features incorporating
Sprob and Tprob are comparable to one another and
are very effective features as well.
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0.09
CENT2
CENT1
CWS
BEGAUTH
SPAU
PPAU
THISENT
SENT
PENT
COS2
COS1
DOM
TPOS2
TPOS1
SLEN2
SLEN
CLOC
TLOC
SMT
MNT
MXT
SMS
MNS
MXS
f s
ta
tis
tic
feature ID (see key)
Figure 3: Feature F statistics for Enron email corpus
After creating 30% word compression summaries
using both the ConverSumm and Rambow ap-
proaches, we score the 39 thread summaries using
Pyramid Precision. The results are given in Table 3.
On average, the Rambow system is slightly higher
with a score of 0.50 compared with 0.46 for the con-
versational system, but there is no statistical differ-
ence according to paired t-test.
The average AUROC for the Rambow system is
0.64 compared with 0.75 for the ConverSumm sys-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
lexical features
structural features
participant features
length features
Fea. Subset AUROC
Structural 0.63
Participant 0.51
Length 0.71
Lexical 0.71
Figure 4: AUROC Values for Feature Subclasses, Enron
Corpus
tem, with ConverSumm system significantly better
according to paired t-test (p<0.05). Random classi-
fication performance would yield an AUROC of 0.5.
Combining the Rambow and ConverSumm fea-
tures does not yield any overall improvement. The
Pyramid Precision score in that case is 0.47 while
the AUROC is 0.74.
Figure 4 illustrates that the lexical and length
features are the most effective feature subclasses,
though the best results overall are derived from a
combination of all feature classes.
7 Discussion
According to multiple evaluations, the ConverSumm
features yield competitive summarization perfor-
mance with the comparison systems. There is a clear
set of features that are similarly effective in both do-
mains, especially CWS, the centroid features, the
Sprob features, the Tprob features, and sentence
length. There are other features that are more ef-
fective in one domain than the other. For exam-
ple, the BEGAUTH feature, indicating whether the
current participant began the conversation, is more
useful for emails. It seems that being the first per-
son to speak in a meeting is not as significant as
being the first person to email in a given thread.
SLEN2, which normalizes sentence length by the
longest sentence in the turn, also is much more ef-
780
fective for emails. The reason is that many meet-
ing turns consist of a single, brief utterance such as
?Okay, yeah.?
The finding that the summary evaluations are
not significantly worse on noisy ASR compared
with manual transcripts has been previously attested
(Valenza et al, 1999; Murray et al, 2005a), and it is
encouraging that our ConverSumm features are sim-
ilarly robust to this noisy data.
8 Conclusion
We have shown that a general conversation summa-
rization approach can achieve results on par with
state-of-the-art systems that rely on features specific
to more focused domains. We have introduced a
conversation feature set that is similarly effective in
both the meetings and emails domains. The use of
multiple summarization evaluation techniques con-
firms that the system is robust, even when applied
to the noisy ASR output in the meetings domain.
Such a general conversation summarization system
is valuable in that it may save time and effort re-
quired to implement unique systems in a variety of
conversational domains.
We are currently working on extending our sys-
tem to other conversation domains such as chats,
blogs and telephone speech. We are also investigat-
ing domain adaptation techniques; for example, we
hypothesize that the relatively well-resourced do-
main of meetings can be leveraged to improve email
results, and preliminary findings are encouraging.
References
R. Bekkerman, A. McCallum, and G. Huang. 2004. Au-
tomatic categorization of email into folders: Bench-
mark experiments on Enron and SRI corpora. Tech-
nical Report IR-418, Center of Intelligent Information
Retrieval, UMass Amherst.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval 1998, Melbourne, Australia, pages 335?
336.
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proc. of ACM
WWW 07, Banff, Canada.
G. Carenini, X. Zhou, and R. Ng. 2008. Summarizing
emails with conversational cohesion and subjectivity.
In Proc. of ACL 2008, Columbus, Ohio, USA.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Well-
ner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
A. Chapanond, M. Krishnamoorthy, and B. Yener. 2005.
Graph theoretic and spectral analysis of enron email
data. Comput. Math. Organ. Theory, 11(3):265?281.
Y-W. Chen and C-J. Lin. 2006. Combining SVMs
with various feature selection strategies. In I. Guyon,
S. Gunn, M. Nikravesh, and L. Zadeh, editors, Feature
extraction, foundations and applications. Springer.
S. Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-
bell. 2004. Integration of email and task lists. In Proc.
of CEAS 2004, Mountain View, CA, USA.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning, 20(3):273?297.
J. Diesner, T. Frantz, and K. Carley. 2005. Communi-
cation networks from the enron email corpus ?it?s al-
ways about the people. enron is no different?. Comput.
Math. Organ. Theory, 11(3):201?228.
M. Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP 2006, Sydney, Australia, pages 364?
372.
T. Hain, L. Burget, J. Dines, G. Garau, V. Wan,
M. Karafiat, J. Vepa, and M. Lincoln. 2007. The
AMI system for transcription of speech in meetings.
In Proc. of ICASSP 2007,, pages 357?360.
B. Klimt and Y. Yang. 2004. Introducing the enron cor-
pus. In Proc. of CEAS 2004, Mountain View, CA, USA.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features for
speech summarization. In Proc. of Interspeech 2005,
Lisbon, Portugal, pages 621?624.
S. Muresan, E. Tzoukermann, and J. Klavans. 2001.
Combining linguistic and machine learning techniques
for email summarization. In Proc. of ConLL 2001,
Toulouse, France.
G. Murray and S. Renals. 2008. Meta comments for
summarizing meeting speech. In Proc. of MLMI 2008,
Utrecht, Netherlands.
G. Murray, S. Renals, and J. Carletta. 2005a. Extrac-
tive summarization of meeting recordings. In Proc. of
Interspeech 2005, Lisbon, Portugal, pages 593?596.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005b.
Evaluating automatic summaries of meeting record-
ings. In Proc. of the ACL 2005 MTSE Workshop, Ann
Arbor, MI, USA, pages 33?40.
781
G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. In-
corporating speaker and discourse features into speech
summarization. In Proc. of the HLT-NAACL 2006,
New York City, USA, pages 367?374.
G. Murray. 2007. Using Speech-Specific Features for
Automatic Speech Summarization. Ph.D. thesis, Uni-
versity of Edinburgh.
A. Nenkova and B. Passonneau. 2004. Evaluating con-
tent selection in summarization: The Pyramid method.
In Proc. of HLT-NAACL 2004, Boston, MA, USA,
pages 145?152.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proc. of HLT-
NAACL 2004, Boston, USA.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proc. of the ESCA Workshop on
Accessing Information in Spoken Audio, Cambridge
UK, pages 111?116.
A. Waibel, M. Bett, M. Finke, and R. Stiefelhagen. 1998.
Meeting browser: Tracking and summarizing meet-
ings. In D. E. M. Penrose, editor, Proc. of the Broad-
cast News Transcription and Understanding Work-
shop, Lansdowne, VA, USA, pages 281?286.
J. Yeh and A. Harnly. 2006. Email thread reassembly
using similarity matching. In Proc of CEAS 2006.
D. Zajic, B. Dorr, and J. Lin. 2008. Single-document and
multi-document summarization techniques for email
threads using sentence compression. Information Pro-
cessing and Management, to appear.
K. Zechner. 2002. Automatic summarization of open-
domain multiparty dialogues in diverse genres. Com-
putational Linguistics, 28(4):447?485.
782
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1348?1357,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Predicting Subjectivity in Multimodal Conversations
Gabriel Murray and Giuseppe Carenini
University of British Columbia
Vancouver, Canada
(gabrielm, carenini)@cs.ubc.ca
Abstract
In this research we aim to detect sub-
jective sentences in multimodal conversa-
tions. We introduce a novel technique
wherein subjective patterns are learned
from both labeled and unlabeled data, us-
ing n-gram word sequences with vary-
ing levels of lexical instantiation. Ap-
plying this technique to meeting speech
and email conversations, we gain signifi-
cant improvement over state-of-the-art ap-
proaches. Furthermore, we show that cou-
pling the pattern-based approach with fea-
tures that capture characteristics of gen-
eral conversation structure yields addi-
tional improvement.
1 Introduction
Conversations are rich in subjectivity. Conversa-
tion participants agree and disagree with one other,
argue for and against various proposals, and gen-
erally take turns expressing their private states.
Being able to separate these subjective utterances
from more objective utterances would greatly fa-
cilitate the analysis, mining and summarization of
a large number of conversations.
Two of the most prevalent conversational me-
dia are meetings and emails. Face-to-face meet-
ings enable numerous people to exchange a large
amount of information and opinions in a short pe-
riod of time, while emails allow for concise ex-
changes between potentially far-flung participants.
Meetings and emails can also feed into one an-
other, with face-to-face meetings occurring at reg-
ular intervals and emails continuing the conver-
sations in the interim. This poses several inter-
esting questions, such as whether subjective utter-
ances are more or less likely to be found in email
exchanges compared with meetings, and whether
the ratios of positive and negative subjective utter-
ances differ between the two modalities.
In this paper we describe a novel approach for
predicting subjectivity, and test it in two sets of
experiments on meetings and emails. Our ap-
proach combines a new general purpose method
for learning subjective patterns, with features that
capture basic characteristics of conversation struc-
ture across modalities. The subjective patterns are
essentially n-gram sequences with varying levels
of lexical instantiation, and we demonstrate how
they can be learned from both labeled and un-
labeled data. The conversation features capture
structural characteristics of multimodal conversa-
tions as well as participant information.
We test our approach in two sets of experi-
ments. The goal of the first set of experiments is to
discriminate subjective from non-subjective utter-
ances, comparing the novel approach to existing
state-of-the-art techniques. In the second set of
experiments, the goal is to discriminate positive-
subjective and negative-subjective utterances, es-
tablishing their polarity. In both sets of experi-
ments, we assess the impact of features relating
to conversation structure.
2 Related Research
Raaijmakers et al (2008) have approached
the problem of detecting subjectivity in meeting
speech by using a variety of multimodal features
such as prosodic features, word n-grams, charac-
ter n-grams and phoneme n-grams. For subjec-
tivity detection, they found that a combination of
all features was best, while prosodic features were
less useful for discriminating between positive and
negative utterances. They found character n-grams
to be particularly useful.
Riloff and Wiebe (2004) presented a method for
learning subjective extraction patterns from a large
amount of data, which takes subjective and non-
subjective text as input, and outputs significant
lexico-syntactic patterns. These patterns are based
on syntactic structure output by the Sundance shal-
1348
low dependency parser (Riloff and Phillips, 2004).
They are extracted by exhaustively applying syn-
tactic templates such as < subj > passive-verb
and active-verb < dobj > to a training cor-
pus, with an extracted pattern for every instan-
tiation of the syntactic template. These patterns
are scored according to probability of relevance
given the pattern and frequency of the pattern. Be-
cause these patterns are based on syntactic struc-
ture, they can represent subjective expressions that
are not fixed word sequences and would therefore
be missed by a simple n-gram approach.
Riloff et al (2006) explore feature subsumption
for opinion detection, where a given feature may
subsume another feature representationally if the
strings matched by the first feature include all of
the strings matched by the second feature. To give
their own example, the unigram happy subsumes
the bigram very happy. The first feature will be-
haviorally subsume the second if it representa-
tionally subsumes the second and has roughly the
same information gain, within an acceptable mar-
gin. They show that they can improve opinion
analysis results by modeling these relations and
reducing the feature set.
Our approach for learning subjective patterns
like Raaijmakers et al relies on n-grams, but like
Riloff et al moves beyond fixed sequences of
words by varying levels of lexical instantiation.
Yu and Hatzivassiloglou (2003) addressed three
challenges in the news article domain: discrimi-
nating between objective documents and subjec-
tive documents such as editorials, detecting sub-
jectivity at the sentence level, and determining po-
larity at the sentence level. They found that the
latter two tasks were substantially more difficult
than classification at the document level. Of par-
ticular relevance here is that they found that part-
of-speech (POS) features were especially useful
for assigning polarity scores, with adjectives, ad-
verbs and verbs comprising the best set of POS
tags. This work inspired us to look at generaliza-
tion of n-grams based on POS.
On the slightly different task of classifying the
intensity of opinions, Wilson et al (2006) em-
ployed several types of features including depen-
dency structures in which words can be backed off
to POS tags. They found that this feature class im-
proved the overall accuracy of their system.
Somasundaran et al (2007) investigated subjec-
tivity classification in meetings. Their findings in-
dicate that both lexical features (list of words and
expressions) and discourse features (dialogue acts
and adjacency pairs) can be beneficial. In the same
spirit, we effectively combine lexical patterns and
conversational features.
The approach to predicting subjectivity we
present in this paper is a novel contribution to the
field of opinion and sentiment analysis. Pang and
Lee (2008) give an overview of the state of the art,
discussing motivation, features, approaches and
available resources.
3 Subjectivity Detection
In this section we describe our approach to sub-
jectivity detection. We begin by describing how
to learn subjective n-gram patterns with varying
levels of lexical instantiation. We then describe a
set of features characterizing multimodal conver-
sation structure which can be used to supplement
the n-gram approach. Finally, we describe the
baseline subjectivity detection approaches used
for comparison.
3.1 Partially Instantiated N-Grams
Our approach to subjectivity detection and polar-
ity detection is to learn significant patterns that
correlate with the subjective and polar utterances.
These patterns are word trigrams, but with varying
levels of lexical instantiation, so that each unit of
the n-gram can be either a word or the word?s part-
of-speech (POS) tag. This contrasts, then, with
work such as that of Raaijmakers et al (2008)
who include trigram features in their experiments,
but where their learned trigrams are fully instanti-
ated. As an example, while they may learn that a
trigram really great idea is positive, we may addi-
tionally find that really great NN and RB great NN
are informative patterns, and these patterns may
sometimes be better cues than the fully instanti-
ated trigrams. To differentiate this approach from
the typical use of trigrams, we will refer to it as the
VIN (varying instantiation n-grams) method.
In some respects, our approach to subjectiv-
ity detection is similar to Riloff and Wiebe?s
work cited above, in the sense that their extrac-
tion patterns are partly instantiated. However,
the AutoSlog-TS approach relies on deriving syn-
tactic structure with the Sundance shallow parser
(Riloff and Phillips, 2004). We hypothesize that
our trigram approach may be more robust to dis-
fluent and fragmented meeting speech and emails
1349
1 2 3
really great idea
really great NN
really JJ idea
RB great idea
really JJ NN
RB great NN
RB JJ idea
RB JJ NN
Table 1: Sample Instantiation Set
on which syntactic parsers may perform poorly.
Also, our learned trigram patterns range from fully
instantiated to completely uninstantiated. For ex-
ample, we might find that the pattern RB JJ NN
is a very good indicator of subjective utterances
because it matches a variety of scenarios where
people are ascribing qualities to things, e.g. re-
ally bad movie, horribly overcooked steak. Notice
that we do not see our approach and AutoSlog-TS
as mutually exclusive, and indeed we demonstrate
through these experiments that they can be effec-
tively combined.
Our approach begins by running the Brill POS
tagger (Brill, 1992) over all sentences in a doc-
ument. We then extract all of the word trigrams
from the document, and represent each trigram us-
ing every possible instantiation. Because we are
working at the trigram level, and each unit of the
trigram can be a word or its POS tag there are
2
3
= 8 representations in each trigram?s instantia-
tion set. To continue the example from above, the
instantiation set for the trigram really great idea is
given in Table 1. As we scan down the instanti-
ation set, we can see that the level of abstraction
increases until it is completely uninstantiated. It is
this multilevel abstraction that we are hypothesiz-
ing will be useful for learning new subjective and
polar cues.
All trigrams are then scored according to their
prevalence in relevant versus irrelevant documents
(e.g. subjective vs. non-subjective sentences),
following the scoring methodology of Riloff and
Wiebe (2003). We calculate the conditional prob-
ability p(relevance|trigram) using the actual tri-
gram counts in relevant and irrelevant text. For
learning negative-subjective patterns, we treat all
negative sentences as the relevant text and the re-
mainder of the sentences as irrelevant text, and
conduct the same process for learning positive-
subjective patterns. We consider significant pat-
terns to be those where the conditional proba-
bility is greater than 0.65 and the pattern occurs
more than five times in the entire document set
(slightly higher than probability >= 0.60 and
frequency >= 2 used by Riloff and Wiebe
(2003)).
We possess a fairly small amount of conversa-
tional data annotated for subjectivity and polarity.
The AMI meeting corpus and BC3 email corpus
are described in more detail in Section 4.1. To ad-
dress this shortfall in annotated data, we take two
approaches to learning patterns. In the first, we
learn a set of patterns from the annotated conversa-
tion data. In the second approach, we complement
those patterns by learning additional patterns from
unannotated data that are typically overwhelm-
ingly subjective or objective in nature. We de-
scribe these two approaches here in turn.
3.1.1 Supervised Learning of Patterns from
Conversation Data
The first learning strategy is to apply the above-
described methods to the annotated conversation
data, learning the positive patterns by compar-
ing positive-subjective utterances to all other ut-
terances, and learning the negative patterns by
comparing the negative-subjective utterances to
all other utterances, using the described methods.
This results in 759 significant positive patterns and
67 significant negative patterns. This difference in
pattern numbers can be explained by negative ut-
terances being less common in the AMI meetings,
as noted by Wilson (2008). It may be that people
are less comfortable in expressing negative sen-
timents in face-to-face conversations, particularly
when the meeting participants do not know each
other well (in the AMI scenario meetings, many
participants were meeting each other for the first
time). But there may be a further explanation for
why we learn many more positive than negative
patterns. When conversation participants do ex-
press negative sentiments, they may couch those
sentiments in more euphemistic or guarded terms
compared with positive sentiments. Table 2 gives
examples of significant positive and negative pat-
terns learned from the labeled meeting data. The
last two rows in Table 2 show how two patterns
in the same instantiation set can have substantially
different probabilities.
1350
POS p(r|t) NEG p(r|t)
you MD change 1.0 VBD not RB 1.0
should VBP DT 1.0 doesn?t RB VB 0.875
very easy to 0.88 a bit JJ 0.66
we could VBP 0.78 think PRP might 0.66
NNS should VBP 0.71 be DT problem 0.71
PRP could do 0.66 doesn?t really VB 0.833
it could VBP 83 doesn?t RB VB 0.875
Table 2: Example Pos. and Neg. Patterns (AMI)
3.1.2 Unsupervised Learning of Patterns
from Blog Data
The second pattern learning strategy we take to
learning subjective patterns is to use a relevant,
but unannotated corpus. We focus on weblog
(blog) data for several reasons. First, blog posts
share many characteristics with both meetings and
emails: they are conversational, informal and the
language can be very ungrammatical. Second,
blog posts are known for being subjective; blog-
gers post on issues that are passionate to them, of-
fering arguments, opinions and invective. Third,
there is a huge amount of available blog data. But
because we do not possess blog data annotated
for subjectivity, we take the following approach
to learning subjective patterns from this data. We
work on the assumption that a great many blog
posts are inherently subjective, and that compar-
ing this data to inherently objective text such as
newswire articles, treating the latter as our irrele-
vant text, should lead to the detection of many new
subjective patterns and greatly increase our cover-
age. While the patterns learned will be noisy, we
hypothesize that the increased coverage will im-
prove our subjectivity detection overall.
For our blog data, we use the BLOG06 Corpus1
that was featured as training and testing data for
the Text Analysis Conference (TAC) 2008 track
on summarizing blog opinions. The portion used
totals approximately 4,000 documents on all man-
ner of topics. Treating that dataset as our rele-
vant, subjective data, we then learn the subjec-
tive trigrams by comparing with the irrelevant
TAC/DUC newswire data from the 2007 and 2008
update summarization tasks. To try to reduce the
amount of noise in our learned patterns, we set the
conditional probability threshold at 0.75 (vs. 0.65
for annotated data), and stipulate that all signif-
icant patterns must occur at least once in the ir-
relevant text. This last rule is meant to prevent
1http://ir.dcs.gla.ac.uk/test collections/blog06info.html
Pattern p(r|t)
can not VB 0.99
i can RB 0.99
i have not 0.98
do RB think 0.97
RB think that 0.95
RB agree with 0.95
IN PRP opinion 0.95
Table 3: Example Subjective Patterns (BLOG06)
us from learning completely blog-specific patterns
such as posted by NN or linked to DT. In the end,
more than 20,000 patterns were learned from the
blog data. While manual inspection does show
that many undesirable patterns were extracted,
among the highest-scoring patterns are many sen-
sible subjective trigrams such as those indicated in
Table 3.
This approach is similar in spirit to the work of
Biadsy et al (2008) on unsupervised biography
production. Without access to labeled biographi-
cal data, the authors chose to use sentences from
Wikipedia biographies as their positive set and
sentences from newswire articles as their negative
set, on the assumption that most of the Wikipedia
sentences would be relevant to biographies and
most of the newswire sentences would not.
3.2 Deriving VIN Features
For our machine learning experiments, we derive,
for each sentence, features indicating the presence
of the significant VIN patterns. Patterns are binned
according to their conditional probability range
(i.e., 0.65 <= p < 0.75, 0.75 <= p < 0.85,
0.85 <= p < 0.95, and 0.95 <= p). There are
three bins for the blog patterns, since the proba-
bility cutoff is 0.75 For each bin, there is a feature
indicating the count of its patterns in the given sen-
tence. When attempting to match these trigram
patterns to sentences, we allow up to two wild-
card lexical items between the trigram units. In
this way a sentence can match a learned pattern
even if the units of the n-gram are not contiguous
(Raaijmakers et al (2008) similarly include an n-
gram feature allowing such intervening material).
A key reason for counting the number of
matched patterns for each probability range as just
described, rather than including a feature for each
individual pattern, is to maintain the same level
of dimensionality in our machine learning exper-
iments when comparing the VIN approach to the
baseline approaches described in Section 3.4.
1351
3.3 Conversational Features
While we hypothesize that the general pur-
pose pattern-based approach described above will
greatly aid subjectivity and polarity detection, we
also recognize that there are many additional fea-
tures specific for characterizing multimodal con-
versations that may correlate well with subjectiv-
ity and polarity. Such features include structural
characteristics like the position of a sentence in a
turn and the position of a turn in the conversation,
and participant features relating to dominance or
leadership. For example, it may be that subjective
sentences are more likely to come at the end of a
conversation, or that a person who dominates the
conversation may utter more negative sentences.
We use the feature set provided by Murray and
Carenini (2008), which they used for automatic
summarization of conversations and which are
shown in Table 4. Many of the features are based
on so-called Sprob and Tprob term-weights, the
former of which weights words based on their dis-
tributions across conversation participants and the
latter of which similarly weights words based on
their distributions across conversation turns. Other
features include word entropy of the candidate
sentence, lexical cohesion of the sentence with the
greater conversation, and structural features indi-
cating position of the candidate sentence in the
turn and in the conversation, such as the elapsed
time since the beginning of the conversation.
3.4 Baseline Approaches
There are two baselines in particular to which
we are interested in comparing the VIN ap-
proach. As stated earlier, we are hypothesiz-
ing that the increasing levels of abstraction found
with partially instantiated trigrams will lead to im-
proved classification compared with using only
fully instantiated trigrams. To test this, we
also run the subjective/non-subjective and posi-
tive/negative experiments using only fully instan-
tiated trigrams. There are 71 such positive tri-
grams and 5 such negative trigrams learned from
the AMI data. There are just over 1200 fully in-
stantiated trigrams learned from the unannotated
BLOG06 data.
Believing that the current approach may offer
benefits over state-of-the-art pattern-based subjec-
tivity detection, we also implement the AutoSlog-
TS method of Riloff and Wiebe (2003) for extract-
ing subjective extraction patterns. In AutoSlog-
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cosine of conv. splits, w/ Sprob
COS2 cosine of conv. splits, w/ Tprob
PENT entropy of conv. up to sentence
SENT entropy of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore (cohesion)
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob
Table 4: Features Key
TS, once all of the patterns are extracted using
the Sundance parser, the scoring methodology is
much the same as desribed in Section 3.1. Con-
ditional probabilities are calculated by comparing
pattern occurrences in the relevant text with oc-
currences in all text, and we again use a thresh-
old of p >= 0.65 and frequency >= 5 for sig-
nificant patterns. For the BLOG06 data, we use
a probability cutoff of 0.75 as before. For deriv-
ing the features used in our machine learning ex-
periments, the patterns are similarly grouped ac-
cording to conditional probability. From the anno-
tated data, 48 patterns are learned in total, 46 pos-
itive and only 2 negative. From the BLOG06 data,
more than 3000 significant patterns are learned.
Among significant patterns learned from the AMI
corpus are < subj > BE good, change < dobj >,
< subj > agree and problem with < NP >.
To gauge the effectiveness of the various feature
types, for both sets of experiments we build multi-
ple models on a variety of feature combinations:
fully instantiated trigrams (TRIG), varying in-
stantiation n-grams (VIN), AutoSlog-TS (SLOG),
conversational structure features (CONV), and the
set of all features.
4 Experimental Setup
In this section we describe the corpora used, the
relevant subjectivity annotation, and the statistical
1352
classifiers employed.
4.1 Corpora
We use two annotated corpora for these experi-
ments. The AMI corpus (Carletta et al, 2005) con-
sists of meetings in which participants take part in
role-playing exercises concerning the design and
development of a remote control. Participants are
grouped in fours, and each group takes part in a
sequence of four meetings, bringing the remote
control from design to market. The four members
of the group are assigned roles of project man-
ager, industrial designer, user interface designer,
and marketing expert. In total there are 140 such
scenario meetings, with individual meetings rang-
ing from approximately 15 to 45 minutes.
The BC3 corpus (Ulrich et al, 2008) contains
email threads from the World Wide Web Consor-
tium (W3C) mailing list. The threads feature a va-
riety of topics such as web accessibility and plan-
ning face-to-face meetings. The annotated portion
of the mailing list consists of 40 threads.
4.2 Subjectivity Annotation
Wilson (2008) has annotated 20 AMI meetings for
a variety of subjective phenomena which fall into
the broad classes of subjective utterances, objec-
tive polar utterances and subjective questions. It
is this first class in which we are primarily in-
terested here. Two subclasses of subjective utter-
ances are positive subjective and negative subjec-
tive utterances. Such subjective utterances involve
the expression of a private state, such as a posi-
tive/negative opinion, positive/negative argument,
and agreement/disagreement. The 20 meetings
were labeled by a single annotator, though Wilson
(2008) did conduct a study of annotator agreement
on two meetings, reporting a ? of 0.56 for detect-
ing subjective utterances. Of the roughly 20,000
dialogue acts total in the 20 AMI meetings, nearly
4000 are labeled as positive-subjective and nearly
1300 as negative-subjective. For the first exper-
imental task, we consider the subjective class to
be the union of positive-subjective and negative-
subjective dialogue acts. For the second experi-
mental task, the goal is to discriminate positive-
subjective from negative-subjective.
For the BC3 emails, annotators were initially
asked to create extractive and abstractive sum-
maries of each thread, in addition to labeling a
variety of sentence-level phenomena, including
whether each sentence was subjective. In a second
round of annotations, three different annotators
were asked to go through all of the sentences pre-
viously labeled as subjective and indicate whether
each sentence was positive, negative, positive-
negative, or other. The definitions for positive and
negative subjectivity mirrored those given by Wil-
son (2008). For the purpose of these experiments,
we consider a sentence to be subjective if at least
two of the annotators labeled it as subjective, and
similarly consider a subjective sentence to be pos-
itive or negative if at least two annotators label it
as such. Using this majority vote labeling, 172
of 1800 sentences are considered subjective, with
44% of those labeled as positive-subjective and
37% as negative-subjective, showing that there is
much more of a balance between positive and neg-
ative sentiment in these email threads compared
with meeting speech (note that some subjective
sentences are not positive or negative). The ? for
labeling subjective sentences in the email corpus
is 0.32. The lower annotator agreement on emails
compared with meetings suggests that subjectiv-
ity in email text may be manifested more subtly or
conveyed somewhat amibiguously.
4.3 Classifier and Experimental Setup
For these experiments we use a maximum entropy
classifier using the liblinear toolkit2 (Fan et al,
2008). Feature subset selection is carried out by
calculating the F-statistic for each feature, ranking
the features according to the statistic, and train-
ing on increasingly smaller subsets of feature in
a cross-validation procedure, ultimately choosing
the feature set with the highest balanced accuracy
during cross-validation.
Because the annotated portions of our corpora
are fairly small (20 meetings, 40 email threads),
we employ a leave-one-out method for training
and testing rather than using dedicated training
and test sets. For the polarity labeling task ap-
plied to the BC3 corpus, we pool all of the sen-
tences and perform 10-fold cross-validation at the
sentence level.
4.4 Evaluation Metrics
We employ two sets of metrics for evaluating all
classifiers: precision/recall/f-measure and the re-
ceiver operator characteristic (ROC) curve. The
ROC curve plots the true-positive/false-positive
ratio while the posterior threshold is varied, and
2http://www.csie.ntu.edu.tw/ cjlin/liblinear/
1353
we report the area under the curve (AUROC) as the
measure of interest. Random performance would
feature an AUROC of approximately 0.5, while
perfect classification would yield an AUROC of
1. The advantage of the AUROC score compared
with precision/recall/f-measure is that it evaluates
a given classifier across all thresholds, indicating
the classifier?s overall discriminating power. This
metric is also known to be appropriate when class
distributions are skewed (Fawcett, 2003), as is our
case. For completeness we report both AUROC
and p/r/f, but our discussions focus primarily on
the AUROC comparisons.
5 Results
In this section we describe the experimental re-
sults, first for the subjective/non-subjective clas-
sification task, and subsequently for the positive-
negative classification task.
5.1 Subjective / Non-Subjective Classification
For the subjectivity detection task, the results on
the AMI and BC3 data closely mirrored each
other, with the VIN approach constituting a very
effective feature set, outperforming both baselines.
We report the results on meeting and emails in
turn.
5.1.1 AMI corpus
For the subjectivity task with the AMI corpus, we
first report the precision, recall and f-measure re-
sults in Table 5 where the various classifiers are
compared with a lower bound (LB) in which the
positive class is always predicted, leading to per-
fect recall. It can be seen that the novel systems
exhibit substantial improvement in precision and
f-measure over this lower-bound. While the VIN
approach yields the best precision scores, the full
feature set achieves the highest f-measure.
As shown in Figure 1, the average AUROC with
the VIN approach is 0.69, compared with 0.61 for
AutoSlog-TS, a significant difference according to
paired t-test (p<0.01). The VIN approach is also
significantly better than the standard fully instan-
tiated trigram pattern approach (p<0.01). This
latter result suggests that the increased level of
abstraction found in the varying instantiation n-
grams does improve performance.
The conversational features alone give compa-
rable performance to the VIN method (no signifi-
cant difference), and the best results are found us-
ing the full feature set, which gives an average AU-
Sys Precision Recall F-Measure
AMI Corpus
LB 26 100 41
Trig 25 63 36
Slog 39 48 43
VIN 41 58 48
Conv 36 73 49
All Feas 38 70 49
BC3 Corpus
LB 10 100 17
Trig 27 10 14
Slog 24 13 17
VIN 27 22 24
Conv 25 29 27
All Feas 33 34 33
Table 5: P/R/F Results, Subjectivity Task
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Trig - AMI
Trig - BC3
Slog - AMI
Slog - BC3
VIN - AMI
VIN - BC3
Conv - AMI
Conv - BC3
All Feas - AMI
All Feas - BC3
AU
RO
C
Figure 1: AUROCs on Subjectivity Task for AMI
and BC3 corpora
ROC of 0.71, a significant improvement over VIN
only (p<0.05).
5.1.2 BC3 corpus
For the subjectivity task with the BC3 corpus, the
best precision and f-measure scores are found by
combining all features, as displayed in Table 5.
The f-measure for the VIN approach is ten points
higher than for the standard trigram approach.
The average AUROC with the VIN approach is
0.77, compared with 0.70 for AutoSlog-TS (sig-
nificant at p<0.05). The varying instantiation ap-
proach is significantly better than the standard tri-
gram pattern approach (p<0.01), where the aver-
age AUROC is 0.66. We again find that conver-
sational features are very useful for this task, and
that the best overall results utilize the entire fea-
ture set. These results are displayed in Figure 1.
5.1.3 Impact of Blog Data
An interesting question is whether our use of the
BLOG06 data was worthwhile. We can measure
this by comparing the VIN AUROC results re-
1354
Sys Precision Recall F-Measure
AMI Corpus
LB 76 100 86
Trig 87 8 14
Slog 75 46 57
VIN 83 60 70
Conv 82 47 60
All Feas 83 56 67
BC3 Corpus
LB 54 100 70
Trig 50 84 63
Slog 58 56 57
VIN 53 84 65
Conv 63 80 71
All Feas 60 76 67
Table 6: P/R/F Results, Polarity Task
ported above with the VIN AUROC scores using
only the annotated data for learning the significant
patterns. The finding is that the blog data was
very helpful, as the VIN approach averages only
0.66 on the BC3 data and 0.63 on the AMI data
when the blog patterns are not used, both signif-
icantly lower (p<0.01). Figure 2 shows the ROC
curves for the VIN approach with and without blog
patterns applied to the AMI subjectivity detection
task, illustrating the impact of the unsupervised
pattern-learning strategy.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
VIN with Blog Patterns
VIN without Blog Patterns
chance level
Figure 2: Effect of Blog Patterns on AMI Subjec-
tivity Task
5.2 Positive / Negative Classification
For the polarity classification task, the results dif-
fer between the two corpora. We describe the re-
sults on meetings and emails in turn.
5.2.1 AMI corpus
The p/r/f results for the AMI polarity task are pre-
sented in Table 6, with the scores pertaining to
the positive-subjective class. The VIN classifier
and full features classifier achieve the highest pre-
cision, but the f-measures are below the lower-
bound.
Comparing AUROC results, the VIN approach
is again significantly better than AutoSlog-TS,
averaging 0.65 compared with 0.56, and signifi-
cantly better than the standard trigram approach
(p<0.01 in both cases). The results are dis-
played in Figure 3. The conversational features are
significantly less effective than the VIN features
(p<0.05), and the best overall results are found by
utilizing all features, with significant improvement
over VIN only at p<0.05 and significant improve-
ment over AutoSlog-TS only at p<0.01.
5.2.2 BC3 corpus
The results of the polarity task on the BC3 cor-
pus are markedly different from the other exper-
imental results. In this case, neither VIN nor
AutoSlog-TS are particularly good for discrimi-
nating between positive and negative sentences,
and the best strategy is to use features relating to
conversational structure. According to p/r/f (Ta-
ble 6), the only method outperforming the lower-
bound in terms of f-measure is the conversational
features classifier. According to AUROC scores
shown in Figure 3, the conversational features by
themselves are significantly better than the VIN
approach (p<0.01 for non-paired t-test). So for
emails, we are more likely to correctly classify
positive and negative sentence by looking at fea-
tures such as position in the turn and participant
dominance than by matching our learned patterns.
While we showed previously that pattern-based
approaches perform well for the subjectivity task
on this dataset, there was less success in using the
patterns to discern the polarity of email sentences.
We are again interested in whether the use of the
BLOG06 data was beneficial. For the BC3 data,
there is very little difference between the VIN ap-
proach with and without the blog patterns, as they
both perform poorly, but with the AMI corpus, the
blog patterns yield significant improvement in po-
larity classification, increasing from an average of
0.56 without the blog patterns to 0.65 with them
(p<0.01).
6 Discussion and Future Work
A key difference between the AMI and BC3 data
with regards to subjectivity is that negative ut-
terances are much more common in the BC3
email threads. Additionally, the pattern-based ap-
proaches fared worst in discriminating between
1355
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Trig - AMI
Trig - BC3
Slog - AMI
Slog - BC3
VIN - AMI
VIN - BC3
Conv - AMI
Conv - BC3
All Feas - AMI
All Feas - BC3
AU
RO
C
Figure 3: AUROCs on Polarity Task for AMI and
BC3 corpora
negative and positive utterances in that corpus.
Positive and negative email sentences are more
easily recognized via features relating to conver-
sation structure and participant status than through
the learned lexical patterns.
The use of patterns learned from unlabeled blog
data significantly improved performance. We are
currently developing further techniques for learn-
ing subjective and polar patterns from such raw,
natural text.
A potential area of improvement is to reduce the
feature set by eliminating some of the subjective
patterns. In Section 2, we briefly described the
work of Riloff et al (2006) on feature subsump-
tion relationships. In our case, in a VIN instantia-
tion set a given trigram instantiation subsumes the
less abstract instantiations in the set, so the most
abstract instantiation (i.e. completely uninstanti-
ated trigram) representationally subsumes the rest.
Eliminating some of the representationally sub-
sumed instantiations when they are also behav-
iorally subsumed may improve our results.
It is difficult to compare our results directly with
those of Raaijmakers et al (2008) as they used a
smaller set of AMI meetings for their experiments,
and because for the first experiment we consider
the subjective class to be the union of positive-
subjective and negative-subjective dialogue acts
whereas they additionally include subjective ques-
tions and dialogue acts expressing uncertainty.
These differences are reflected by the substantially
differing scores reported for majority-vote base-
lines on each task. However, their success with
character n-gram features suggests that we could
improve our system by incorporating a variety of
character features. Character n-grams were the
best single feature class in their experiments.
The VIN representation is a general one and
may hold promise for learning patterns relevant to
other interesting conversation phenomena such as
decision-making and action items. We plan to ap-
ply the methods described here to these other ap-
plications in the near future.
7 Conclusion
In this work we have shown that learning subjec-
tive trigrams with varying instantiation levels from
both annotated and raw data can improve subjec-
tivity detection and polarity labeling for meeting
speech and email threads. The novel pattern-based
approach was significantly better than standard tri-
grams for three of the four tasks, and was signif-
icantly better than a state-of-the-art syntactic ap-
proach for those same tasks. We also found that
features relating to conversational structure were
beneficial for all tasks, and particularly for polar-
ity labeling in email data. Interestingly, in three
out of four cases combining all the features pro-
duced the best performance.
References
F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An un-
supervised approach to biography production using
wikipedia. In Proc. of ACL-HLT 2008, Columbus,
OH, USA.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. of DARPA Speech and Natural Lan-
guage Workshop, San Mateo, CA, USA, pages 112?
116.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large lin-
ear classification. Journal of Machine Learning Re-
search, 9:1871?1874.
T. Fawcett. 2003. Roc graphs: Notes and practical
considerations for researchers.
G. Murray and G. Carenini. 2008. Summarizing spo-
ken and written conversations. In Proc. of EMNLP
2008, Honolulu, HI, USA.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 1-2(2):1?135.
1356
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In Proc. of EMNLP 2008, Honolulu, HI, USA.
E. Riloff and W. Phillips. 2004. An introduction to the
sundance and autoslog systems.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of EMNLP
2003, Sapporo, Japan.
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Fea-
ture subsumption for opinion analysis. In Proc. of
EMNLP 2006, Sydney, Australia.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
Proc. of SIGDIAL 2007, Antwerp, Belgium.
J. Ulrich, G. Murray, and G. Carenini. 2008. A
publicly available annotated corpus for supervised
email summarization. In Proc. of AAAI EMAIL-
2008 Workshop, Chicago, USA.
T. Wilson, J. Wiebe, and R. Hwa. 2006. Recognizing
strong and weak opinion clauses. Computational In-
telligence, 22(2):73?99.
T. Wilson. 2008. Annotating subjective content in
meetings. In Proc. of LREC 2008, Marrakech, Mo-
rocco.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. of EMNLP 2003, Sapporo, Japan.
1357
Multi-Document Summarization of Evaluative Text
Giuseppe Carenini, Raymond Ng, and Adam Pauls
Deptartment of Computer Science
University of British Columbia Vancouver, Canada
 
carenini,rng,adpauls  @cs.ubc.ca
Abstract
We present and compare two approaches
to the task of summarizing evaluative ar-
guments. The first is a sentence extraction-
based approach while the second is a lan-
guage generation-based approach. We
evaluate these approaches in a user study
and find that they quantitatively perform
equally well. Qualitatively, however, we
find that they perform well for different but
complementary reasons. We conclude that
an effective method for summarizing eval-
uative arguments must effectively synthe-
size the two approaches.
1 Introduction
Many organizations are faced with the challenge
of summarizing large corpora of text data. One im-
portant application is evaluative text, i.e. any doc-
ument expressing an evaluation of an entity as ei-
ther positive or negative. For example, many web-
sites collect large quantities of online customer re-
views of consumer electronics. Summaries of this
literature could be of great strategic value to prod-
uct designers, planners and manufacturers. There
are other equally important commercial applica-
tions, such as the summarization of travel logs, and
non-commercial applications, such as the summa-
rization of candidate reviews.
The general problem we consider in this paper
is how to effectively summarize a large corpora of
evaluative text about a single entity (e.g., a prod-
uct). In contrast, most previous work on multi-
document summarization has focused on factual
text (e.g., news (McKeown et al, 2002), biogra-
phies (Zhou et al, 2004)). For factual documents,
the goal of a summarizer is to select the most im-
portant facts and present them in a sensible or-
dering while avoiding repetition. Previous work
has shown that this can be effectively achieved by
carefully extracting and ordering the most infor-
mative sentences from the original documents in
a domain-independent way. Notice however that
when the source documents are assumed to con-
tain inconsistent information (e.g., conflicting re-
ports of a natural disaster (White et al, 2002)),
a different approach is needed. The summarizer
needs first to extract the information from the doc-
uments, then process such information to identify
overlaps and inconsistencies between the different
sources and finally produce a summary that points
out and explain those inconsistencies.
A corpus of evaluative text typically contains a
large number of possibly inconsistent ?facts? (i.e.
opinions), as opinions on the same entity feature
may be uniform or varied. Thus, summarizing a
corpus of evaluative text is much more similar to
summarizing conflicting reports than a consistent
set of factual documents. When there are diverse
opinions on the same issue, the different perspec-
tives need to be included in the summary.
Based on this observation, we argue that any
strategy to effectively summarize evaluative text
about a single entity should rely on a preliminary
phase of information extraction from the target
corpus. In particular, the summarizer should at
least know for each document: what features of
the entity were evaluated, the polarity of the eval-
uations and their strengths.
In this paper, we explore this hypothesis by con-
sidering two alternative approaches. First, we de-
veloped a sentence-extraction based summarizer
that uses the information extracted from the cor-
pus to select and rank sentences from the corpus.
We implemented this system, called MEAD*, by
305
adapting MEAD (Radev et al, 2003), an open-
source framework for multi-document summariza-
tion. Second, we developed a summarizer that
produces summaries primarily by generating lan-
guage from the information extracted from the
corpus. We implemented this system, called the
Summarizer of Evaluative Arguments (SEA), by
adapting the Generator of Evaluative Arguments
(GEA) (Carenini and Moore, expected 2006) a
framework for generating user tailored evaluative
arguments.
We have performed an empirical formative eval-
uation of MEAD* and SEA in a user study. In
this evaluation, we also tested the effectiveness of
human generated summaries (HGS) as a topline
and of summaries generated by MEAD without
access to the extracted information as a baseline.
The results indicate that SEA and MEAD* quan-
titatively perform equally well above MEAD and
below HGS. Qualitatively, we find that they per-
form well for different but complementary rea-
sons. While SEA appears to provide a more gen-
eral overview of the source text, MEAD* seems to
provide a more varied language and detail about
customer opinions.
2 Information Extraction from
Evaluative Text
2.1 Feature Extraction
Knowledge extraction from evaluative text about
a single entity is typically decomposed into three
distinct phases: the determination of features of
the entity evaluated in the text, the strength of
each evaluation, and the polarity of each evalua-
tion. For instance, the information extracted from
the sentence ?The menus are very easy to navi-
gate but the user preference dialog is somewhat
difficult to locate.? should be that the ?menus?
and the ?user preference dialog? features are eval-
uated, and that the ?menus? receive a very posi-
tive evaluation while the ?user preference dialog?
is evaluated rather negatively.
For these tasks, we adopt the approach de-
scribed in detail in (Carenini et al, 2005). This ap-
proach relies on the work of (Hu and Liu, 2004a)
for the tasks of strength and polarity determina-
tion. For the task of feature extraction, it en-
hances earlier work (Hu and Liu, 2004c) by map-
ping the extracted features into a hierarchy of fea-
tures which describes the entity of interest. The re-
sulting mapping reduces redundancy and provides
conceptual organization of the extracted features.
Camera
Lens
Digital Zoom
Optical Zoom
. . .
Editing/Viewing
Viewfi nder
. . .
Flash
. . .
Image
Image Type
TIFF
JPEG
. . .
Resolution
Effective Pixels
Aspect Ratio
. . .
Figure 1: Partial view of UDF taxonomies for a
digital camera.
Before continuing, we shall describe the ter-
minology we use when discussing the extracted
knowledge. The features evaluated in a corpus of
reviews and extracted by following Hu and Liu?s
approach are called Crude Features.
CF  

c f j  j   1  n
For example, crude features for a digital cam-
era might include ?picture quality?, ?viewfinder?,
and ?lens?. Each sentence sk in the corpus con-
tains a set of evaluations (of crude features) called
eval  sk  . Each evaluation contains both a polar-
ity and a strength represented as an integer in the
range 	 3 
 2 
 1 
 1 
 2 
 3  where  3 is the
most positive possible evaluation and  3 is the
most negative possible evaluation.
There is also a hierarchical set of possibly more
abstract user-defined features 1
UDF  

ud fi  i   1  m
See Figure 1 for a sampleUDF. The process of hi-
erarchically organizing the extracted features pro-
duces a mapping from CF to UDF features (see
(Carenini et al, 2005) for details). We call the set
of crude features mapped to the user-defined fea-
ture ud fi map  ud fi  . For example, the crude fea-
tures ?unresponsiveness?, ?delay?, and ?lag time?
would all be mapped to the ud f ?delay between
shots?.
For each c f j, there is a set of polarity and
strength evaluations ps  c f j  corresponding to
each evaluation of c f j in the corpus. We call the
set of polarity/strength evaluations directly associ-
ated with ud fi
PSi   
c f j?map  ud fi 
ps  c f j 
The total set of polarity/strength evaluations as-
sociated with ud fi, including its descendants, is
1We call them here user-defi ned features for consistency
with (Carenini et al, 2005). In this paper, they are not as-
sumed to be and are not in practice defi ned by the user.
306
TPSi   PSi 
 


ud fk?desc  ud fi 
PSk 

where desc  ud fi  refers to all descendants of ud fi.
3 MEAD*: Sentence Extraction
Most modern summarization systems use sen-
tences extracted from the source text as the ba-
sis for summarization (see (Nat, 2005b) for a rep-
resentative sample). Extraction-based approaches
have the advantage of avoiding the difficult task
of natural language generation, thus maintaining
domain-independence because the system need
not be aware of specialized vocabulary for its tar-
get domain. The main disadvantage of extraction-
based approaches is the poor linguistic coherence
of the extracted summaries.
Because of the widespread and well-developed
use of sentence extractors in summarization, we
chose to develop our own sentence extractor as
a first attempt at summarizing evaluative argu-
ments. To do this, we adapted MEAD (Radev et
al., 2003), an open-source framework for multi-
document summarization, to suit our purposes.
We refer to our adapted version of MEAD as
MEAD*. The MEAD framework decomposes
sentence extraction into three steps: (i) Feature
Calculation: Some numerical feature(s) are cal-
culated for each sentence, for example, a score
based on document position and a score based on
the TF*IDF of a sentence. (ii) Classification: The
features calculated during step (i) are combined
into a single numerical score for each sentence.
(iii) Reranking: The numerical score for each sen-
tence is adjusted relative to other sentences. This
allows the system to avoid redundancy in the final
set of sentences by lowering the score of sentences
which are similar to already selected sentences.
We found from early experimentation that
the most informative sentences could be accu-
rately determined by examining the extractedCFs.
Thus, we created our own sentence-level feature
based on the number, strength, and polarity ofCFs
extracted for each sentence.
CF sum  sk    ?
psi? eval  sk 
psi

During system development, we found this
measure to be effective because it was sensitive
to the number of CFs mentioned in a given sen-
tence as well as to the strength of the evaluation for
each CF . However, many sentences may have the
same CF sum score (especially sentences which
contain an evaluation for only one CF). In such
cases, we used the MEAD 3.072 centroid feature
as a ?tie-breaker?. The centroid is a common fea-
ture in multidocument summarization (cf. (Radev
et al, 2003), (Saggion and Gaizauskas, 2004)).
At the reranking stage, we adopted a different
algorithm than the default in MEAD. We placed
each sentence which contained an evaluation of a
given CF into a ?bucket? for that CF . Because a
sentence could contain more than one CF , a sen-
tence could be placed in multiple buckets. We
then selected the top-ranked sentence from each
bucket, starting with the bucket containing the
most sentences (largest

ps  c f j 

), never selecting
the same sentence twice. Once one sentence had
been selected from each bucket, the process was
repeated3. This selection algorithm accomplishes
two important tasks: firstly, it avoids redundancy
by only selecting one sentence to represent each
CF (unless all other CFs have already been rep-
resented), and secondly, it gives priority to CFs
which are mentioned more frequently in the text.
The sentence selection algorithm permits us to
select an arbitrary number of sentences to fit a de-
sired word length. We then ordered the sentences
according to a primitive discourse planning strat-
egy in which the most general CF (i.e. the CF
mapped to the topmost node in the UDF) is dis-
cussed first. The remaining sentences were then
ordered according to a depth-first traversal of the
UDF hierarchy. In this way, general features are
followed immediately by their more specific chil-
dren in the hierarchy.
4 SEA: Natural Language Generation
The extraction-based approach described in the
previous section has several disadvantages. We al-
ready discussed problems with the linguistic co-
herence of the summary, but more specific prob-
lems arise in our particular task of summarizing
a corpus of evaluative text. Firstly, sentence ex-
traction does not give the reader any explicit infor-
mation about of the distribution of evaluations, for
example, how many users mentioned a given fea-
2The centroid calculation requires an IDF database. We
constructed an IDF database from several corpora of reviews
and a set of stop words.
3In practice the process would only be repeated in sum-
maries long enough to contain sentences for each CF, which
is very rare.
307
ture and whether user opinions were uniform or
varied. It also does not give an aggregate view of
user evaluations because typically it only presents
one evaluation for each CF . It may be that a very
positive evaluation for oneCF was selected for ex-
traction, even though most evaluations were only
somewhat positive and some were even negative.
We thus also developed a system, SEA, that
presents such information in generated natural lan-
guage. This system calculates several important
characteristics of the source corpus by aggregat-
ing the extracted information including the CF to
UDF mapping. We first describe these character-
istics and then discuss their presentation in natural
language.
4.1 Aggregation of Extracted Information
In order to provide an aggregate view of the eval-
uation expressed in a corpus of evaluative text a
summarizer should at least determine: (i) which
features of the evaluated entity were most ?impor-
tant? to the users (ii) some aggregate of the user
opinions for the important features (iii) the distri-
bution of those opinions and (iv) the reasons be-
hind each user opinion. We now discuss each of
these aspects in detail.
4.1.1 Feature Selection
We approach the task of selecting the most ?im-
portant? features by defining a ?measure of impor-
tance? for each feature of the evaluated entity. We
define the ?direct importance? of a feature in the
UDF as
dir moi  ud fi    ?
psk?PSi

psk

2
where by ?direct? we mean the importance de-
rived only from that feature and not from its chil-
dren. This metric produces high scores for fea-
tures which either occur frequently in the corpus
or have strong evaluations (or both). This ?direct?
measure of importance, however, is incomplete, as
each non-leaf node in the UDF effectively serves
a dual purpose. It is both a feature upon which
a user might comment and a category for group-
ing its sub-features. Thus, a non-leaf node should
be important if either its children are important or
the node itself is important (or both). To this end,
we have defined the total measure of importance
moi  ud fi  as
  

 

dir moi  ud fi  ch  ud fi    /0
 ? dir moi  ud fi  
 1  ?

?ud fk?ch  ud fi  moi  ud fk   otherwise
where ch  ud fi  refers to the children of ud fi in
the hierarchy and ? is some real parameter in the
range  0  5 
 1  . In this measure, the importance of a
node is a combination of its direct importance and
of the importance of its children. The parameter
? may be adjusted to vary the relative weight of
the parent and children. We used ?   0  9 for our
experiments. This setting resulted in more infor-
mative summaries during system development.
In order to perform feature selection using this
metric, we must also define a selection procedure.
The most obvious is a simple greedy selection ?
sort the nodes in the UDF by the measure of im-
portance and select the most important node until
a desired number of features is included. How-
ever, because a node derives part of its ?impor-
tance? from its children, it is possible for a node?s
importance to be dominated by one or more of its
children. Including both the child and parent node
would be redundant because most of the informa-
tion is contained in the child. We thus choose a
dynamic greedy selection algorithm in which we
recalculate the importance of each node after each
round of selection, with all previously selected
nodes removed from the tree. In this way, if a
node that dominates its parent?s importance is se-
lected, its parent?s importance will be reduced dur-
ing later rounds of selection. This approach mim-
ics the behaviour of several sentence extraction-
based summarizers (e.g. (Schiffman et al, 2002;
Saggion and Gaizauskas, 2004)) which define a
metric for sentence importance and then greed-
ily select the sentence which minimizes similarity
with already selected sentences and maximizes in-
formativeness.
4.1.2 Opinion Aggregation
We approach the task of aggregating opinions
from the source text in a similar fashion to de-
termining the measure of importance. We cal-
culate an ?orientation? for each UDF by aggre-
gating the polarity/strength evaluations of all re-
lated CFs into a single value. We define the ?di-
rect orientation? of a UDF as the average of the
strength/polarity evaluations of all related CFs
dir ori  ud fi    avg
psk?PSi
psk
308
As with our measure of importance, we must
also include the orientation of a feature?s children
in its orientation. Because a feature in the UDF
conceptually groups its children, the orientation of
a feature should include some information about
the orientation of its children. We thus define the
total orientation ori  ud fi  as
  

 

dir ori  ud fi  ch  ud fi    /0
 ? dir ori  ud fi  
 1  ?

avgud fk?ch  ud fi  ori  ud fk   otherwise
This metric produces a real number between  3
and  3 which serves as an aggregate of user opin-
ions for a feature. We use the same value of ? as
in moi  ud fi  .
4.1.3 Distribution of Opinions
Communicating user opinions to the reader is
not simply a matter of classifying each feature
as being evaluated negatively or positively ? the
reader may also want to know if all users evalu-
ated a feature in a similar way or if evaluations
were varied. We thus also need a method of de-
termining the modality of the distribution of user
opinions. We calculate the sum of positive polar-
ity/strength evaluations (or negative if ori  ud fi  is
negative) for a node and its children as a fraction
of all polarity/strength evaluations
?vi?   psk?TPSi  signum  psk  signum  ori  ud fi  

vi
?vi?TPSi

vi

If this fraction is very close to 0.5, this indicates
an almost perfect split of user opinions on that
features. So we classify the feature as ?bimodal?
and we report this fact to the user. Otherwise, the
feature is classified as ?unimodal?, i.e. we need
only to communicate one aggregate opinion to the
reader.
4.2 Generating Language: Adapting the
Generator of Evaluative Arguments
(GEA)
The first task in generating a natural language
summary from the information extracted from the
corpus is content selection. This task is accom-
plished in SEA by the feature selection strategy
described in Section 4.1.1. After content selection,
the automatic generation of a natural language
summary involves the following additional tasks
(Reiter and Dale, 2000): (i) structuring the content
by ordering and grouping the selected content ele-
ments as well as by specifying discourse relations
(e.g., supporting vs. opposing evidence) between
the resulting groups; (ii) microplanning, which
involves lexical selection and sentence planning;
and (iii) sentence realization, which produces En-
glish text from the output of the microplanner. For
most of these tasks, we have adapted the Genera-
tor of Evaluative Arguments (GEA) (Carenini and
Moore, expected 2006), a framework for generat-
ing user tailored evaluative arguments. For lack of
space we cannot discuss the details here. These
are provided on the online version of this paper,
which is available at the first author?s Web page.
That version also includes a detailed discussion of
related and future work.
5 Evaluation
We evaluated our two summarizers by performing
a user study in which four treatments were consid-
ered: SEA, MEAD*, human-written summaries
as a topline and summaries generated by MEAD
(with all options set to default) as a baseline.
5.1 The Experiment
Twenty-eight undergraduate students participated
in our experiment, seven for each treatment. Each
participant was given a set of 20 customer reviews
randomly selected from a corpus of reviews. In
each treatment three participants received reviews
from a corpus of 46 reviews of the Canon G3 dig-
ital camera and four received them from a cor-
pus of 101 reviews of the Apex 2600 Progressive
Scan DVD player, both obtained from Hu and Liu
(2004b). The reviews from these corpora which
serve as input to our systems have been manually
annotated with crude features, strength, and polar-
ity. We used this ?gold standard? for crude fea-
ture, strength, and polarity extraction because we
wanted our experiments to focus on our summary
and not be confounded by errors in the knowledge
extraction phase.
The participant was told to pretend that they
work for the manufacturer of the product (either
Canon or Apex). They were told that they would
have to provide a 100 word summary of the re-
views to the quality assurance department. The
purpose of these instructions was to prime the user
to the task of looking for information worthy of
summarization. They were then given 20 minutes
to explore the set of reviews.
After 20 minutes, the participant was asked to
stop. The participant was then given a set of in-
309
structions which explained that the company was
testing a computer-based system for automatically
generating a summary of the reviews s/he has
been reading. S/he was then shown a 100 word
summary of the 20 reviews generated either by
MEAD, MEAD*, SEA, or written by a human 4.
Figure 2 shows four summaries of the same 20 re-
views, one of each type.
In order to facilitate their analysis, summaries
were displayed in a web browser. The upper por-
tion of the browser contained the text of the sum-
mary with ?footnotes? linking to reviews on which
the summary was based. For MEAD and MEAD*,
for each sentence the footnote pointed to the re-
view from which the sentence had been extracted.
For SEA and human-generated summaries, for
each aggregate evaluation the footnote pointed to
the review containing a sample sentence on which
that evaluation was based. In all summaries, click-
ing on one of the footnotes caused the correspond-
ing review to be displayed in which the appropri-
ate sentence was highlighted.
Once finished, the participant was asked to fill
out a questionnaire assessing the summary along
several dimensions related to its effectiveness. The
participant could still access the summary while
s/he worked on the questionnaire.
Our questionnaire consisted of nine questions.
The first five questions were the SEE linguistic
well-formedness questions used at the 2005 Doc-
ument Understanding Conference (DUC) (Nat,
2005a). The next three questions were designed to
assess the content of the summary. We based our
questions on the Responsive evaluation at DUC
2005; however, we were interested in a more spe-
cific evaluation of the content that one overall
rank. As such, we split the content into the fol-
lowing three separate questions:
  (Recall) The summary contains all of the information
you would have included from the source text.
  (Precision) The summary contains no information you
would NOT have included from the source text.
  (Accuracy) All information expressed in the summary
accurately reflects the information contained in the
source text.
The final question in the questionnaire asked the
participant to rank the overall quality of the sum-
mary holistically.
4For automatically generated summaries, we generated
the longest possible summary with less than 100 words.
5.2 Quantitative Results
Table 1 consists of two parts. The first top half fo-
cuses on linguistic questions while the second bot-
tom half focuses on content issues. We performed
a two-way ANOVA test with summary type as
rows and the question sets as columns. Overall,
it is easy to conclude that MEAD* and SEA per-
formed at a roughly equal level, while the baseline
MEAD performed significantly lower and the Hu-
man summarizer significantly higher (p   001).
When individual questions/categories are consid-
ered, there are few questions that differentiate be-
tween MEAD* and SEA with a p-value below
0.05. The primary reason is our small sample size.
Nonetheless, if we relax the p-value threshold, we
can make the following observations/hypotheses.
To validate some of these hypotheses, we would
conduct a larger user study in future work.
On the linguistic side, the average
score suggests the ordering of: Human 

MEAD  
 SEA

 MEAD. Both MEAD* and
SEA are also on par with the median DUC score
(Nat, 2005b). On the focus question, in fact,
SEA?s score is tied with the Human?s score, which
may be a beneficial effect of the UDF guiding
content structuring in a top-down fashion. It
is also interesting to see that SEA outperforms
MEAD* on grammaticality, showing that the
generative text approach may be more effective
than simply extracting sentences on this aspect of
grammaticality. On the other hand, MEAD* out-
performs SEA on non-redundancy, and structure
and coherence. SEA?s disappointing performance
on structure and coherence was among the most
surprising finding. One possibility is that our
adaptation of GEA content structuring strategy
was suboptimal or even inappropriate. We plan to
investigate possible causes in the future.
On the content side, the average score sug-
gests the ordering of: Human  SEA  MEAD 
MEAD. As for the three individual content ques-
tions, on the recall one, both SEA and MEAD*
were dominated by the Human summarizer. This
indicates that both SEA and MEAD* omit some
features considered important. We feel that if a
longer summary was allowed, the gap between the
two and the Human summarizer would be nar-
rower. The precision question is somewhat sur-
prising in that SEA actually performs better than
the Human summarizer. In general this indicates
that the feature selection strategy was quite suc-
310
MEAD*: Bottom line , well made camera , easy to use , very flexible and powerful features to include the ability to use external flash and lense / fi lters
choices . 1It has a beautiful design , lots of features , very easy to use , very confi gurable and customizable , and the battery duration is amazing ! Great
colors , pictures and white balance. The camera is a dream to operate in automode , but also gives tremendous flexibility in aperture priority , shutter priority
, and manual modes . I ?d highly recommend this camera for anyone who is looking for excellent quality pictures and a combination of ease of use and the
flexibility to get advanced with many options to adjust if you like.
SEA: Almost all users loved the Canon G3 possibly because some users thought the physical appearance was very good. Furthermore, several users found
the manual features and the special features to be very good. Also, some users liked the convenience because some users thought the battery was excellent.
Finally, some users found the editing/viewing interface to be good despite the fact that several customers really disliked the viewfi nder . However, there
were some negative evaluations. Some customers thought the lens was poor even though some customers found the optical zoom capability to be excellent.
Most customers thought the quality of the images was very good.
MEAD: I am a software engineer and am very keen into technical details of everything i buy , i spend around 3 months before buying the digital camera ;
and i must say , g3 worth every single cent i spent on it . I do n?t write many reviews but i ?m compelled to do so with this camera . I spent a lot of time
comparing different cameras , and i realized that there is not such thing as the best digital camera . I bought my canon g3 about a month ago and i have to
say i am very satisfi ed .
Human: The Canon G3 was received exceedingly well. Consumer reviews from novice photographers to semi-professional all listed an impressive number
of attributes, they claim makes this camera superior in the market. Customers are pleased with the many features the camera offers, and state that the camera
is easy to use and universally accessible. Picture quality, long lasting battery life, size and style were all highlighted in glowing reviews. One flaw in the
camera frequently mentioned was the lens which partially obsructs the view through the view fi nder, however most claimed it was only a minor annoyance
since they used the LCD sceen.
Figure 2: Sample automatically generated summaries.
SEA MEAD* MEAD Human DUC
Question Avg. Dev. Avg. Dev. Avg. Dev. Avg. Dev. Med. Min. Max.
Grammaticality 3.43 1.13 2.71 0.76 3.14 0.90 4.29 0.76 3.86 2.60 4.34
Non-redundancy 3.14 1.57 3.86 0.90 3.57 0.98 4.43 1.13 4.44 3.96 4.74
Referential clarity 3.86 0.69 4.00 1.15 3.00 1.15 4.71 0.49 2.98 2.16 4.14
Focus 4.14 0.69 3.71 1.60 2.29 1.60 4.14 0.69 3.16 2.38 3.94
Structure and Coherence 2.29 0.95 3.00 1.41 1.86 0.90 4.43 0.53 2.10 1.60 3.24
Linguistic Average 3.37 1.19 3.46 1.24 2.77 1.24 4.4 0.74 3.31 2.54 4.08
Recall 2.33 1.03 2.57 0.98 1.57 0.53 3.57 1.27 ? ? ?
Precision 4.17 1.17 3.50 1.38 2.17 1.17 3.86 1.07 ? ? ?
Accuracy 4.00 0.82 3.57 1.13 2.57 1.4 4.29 0.76 ? ? ?
Content Average 3.5 1.26 3.21 1.2 2.1 1.12 3.9 1.04 ? ? ?
Overall 3.14 0.69 3.14 1.21 2.14 1.21 4.43 0.79 ? ? ?
Macro Average 3.39 0.73 3.34 0.51 2.48 0.65 4.24 0.34 ? ? ?
Table 1: Quantative results of user responses to our questionnaire on a scale from 1 (Strongly Disagree)
to 5 (Strongly Agree).
cessful. Finally, for the accuracy question, SEA is
closer to the Human summarizer than MEAD*. In
sum, recall that for evaluative text, it is very pos-
sible that different reviews express different opin-
ions on the same question. Thus, for the summa-
rization of evaluative text, when there is a differ-
ence in opinions, it is desirable that the summary
accurately covers both angles or conveys the dis-
agreement. On this count, according to the scores
on the precision and accuracy questions, SEA ap-
pears to outperform MEAD*.
5.3 Qualitative Results
MEAD*: The most interesting aspect of the
comments made by participants who evaluated
MEAD*-based summaries was that they rarely
criticized the summary for being nothing more
than a set of extracted sentences. For example,
one user claimed that the summary had a ?simple
sentence first, then ideas are fleshed out, and ends
with a fun impact statement?. Other users, while
noticing that the summary was solely quotation,
still felt the summary was adequate (?Shouldn?t
just copy consumers . . . However, it summarized
various aspects of the consumer?s opinions . . . ?).
With regard to content, two main complaints by
participants were: (i) the summary did not reflect
overall opinions (e.g., included positive evalua-
tions of the DVD player even though most eval-
uations were negative), and (ii) the evaluations
of some features were repeated. The first com-
plaint is consistent with the relatively low score of
MEAD* on the accuracy question.
We could address this complaint by only includ-
ing sentences whose CF evaluations have polari-
ties matching the majority polarity for each CF .
The second complaint could be avoided by not
selecting sentences which contain evaluations of
CFs already in the summary.
SEA: Comments about the structure of the sum-
maries generated by SEAmentioned the ?coherent
but robotic? feel of the summaries, the repetition
of ?users/customers? and lack of pronoun use, the
lack of flow between sentences, and the repeated
use of generic terms such as ?good?. These prob-
lems are largely a result of simplistic microplan-
ning and seems to contradict SEA?s disappointing
performance on the structure and coherence ques-
311
tion.
In terms of content, there were two main sets of
complaints. Firstly, participants wanted more ?de-
tails? in the summary, for instance, they wanted
examples of the ?manual features? mentioned by
SEA. Note that this is one complaint absent from
the MEAD* summaries. That is, where the
MEAD* summaries lack structure but contain de-
tail, SEA summaries provide a general, structured
overview while lacking in specifics.
The other set of complaints related to the prob-
lem that participants disagreed with the choice of
features in the summary. We note that this is actu-
ally a problem common to MEAD* and even the
Human summarizer. The best example to illus-
trate this point is on the ?physical appearance? of
the digital camera. One reason participants may
have disagreed with the summarizer?s decision to
include the physical appearance in the summary
is that some evaluations of the physical appear-
ance were quite subtle. For example, the sentence
?This camera has a design flaw? was annotated in
our corpus as evaluating the physical appearance,
although not all readers would agree with that an-
notation.
6 Conclusions
We have presented and compared a sentence
extraction- and language generation based ap-
proach to summarizing evaluative text. A forma-
tive user study of our MEAD* and SEA summa-
rizers found that, quantitatively, they performed
equally well relative to each other, while signifi-
cantly outperforming a baseline standard approach
to multidocument summarization. Trends that we
identified in the results as well as qualitative com-
ments from participants in the user study indicate
that the summarizers have different strengths and
weaknesses. On the one hand, though providing
varied language and detail about customer opin-
ions, MEAD* summaries lack in accuracy and
precision, failing to give and overview of the opin-
ions expressed in the evaluative text. On the other,
SEA summaries provide a general overview of the
source text, while sounding ?robotic?, repetitive,
and surprisingly rather incoherent.
Some of these differences are, fortunately, quite
complimentary. We plan in the future to investi-
gate how SEA and MEAD* can be integrated and
improved.
References
G. Carenini and J. D. Moore. expected 2006. Generat-
ing and evaluating evaluative arguments. AI Journal
(accepted for publication, contact fi rst author for a
draft).
G. Carenini, R.T Ng, and E. Zwart. 2005. Extracting
knowlede from evaluative text. In Proc. Third Inter-
national Conference on Knowledge Capture.
M. Hu and B. Liu. 2004a. Mining and summariz-
ing customer reviews. In Proc. of the 10th ACM
SIGKDD Conf., pages 168?177, New York, NY,
USA. ACM Press.
Minqing Hu and Bing Liu. 2004b. Fea-
ture based summary of customer reviews dataset.
http://www.cs.uic.edu/ liub/FBS/FBS.html.
Minqing Hu and Bing Liu. 2004c. Mining opinion
features in customer reviews. In Proc. AAAI.
K. R. McKeown, R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Tracking and
summarizing news on a daily basis with Columbia?s
Newsblaster. In Proceedings of the HLT Conf.
2005a. Linguistic quality questions from the
2005 DUC. http://duc.nist.gov/duc2005/quality-
questions.txt.
2005b. Proc. of DUC 2005.
D. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer,
H. Qi, A. elebi, D. Liu, and E. Drabek. 2003. Eval-
uation challenges in large-scale document summa-
rization. In Proc. of the 41st ACL, pages 375?382.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Studies in Natural
Language Processing. Cambridge University Press.
H. Saggion and R. Gaizauskas. 2004. Multi-document
summarization by cluster/profi le relevance and re-
dundancy removal. In Proc. of DUC04.
B. Schiffman, A. Nenkova, and K. McKeown. 2002.
Experiments in multidocument summarization. In
Proc. of HLT02, San Diego, Ca.
M. White, C. Cardie, and V. Ng. 2002. Detecting
discrepancies in numeric estimates using multidoc-
ument hypertext summaries. In Proc of HLT02.
L. Zhou, M. Ticrea, and E. Hovy. 2004. Multi-
document biography summarization. In Proceed-
ings of EMNLP.
312
Proceedings of ACL-08: HLT, pages 353?361,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Summarizing Emails with Conversational Cohesion and Subjectivity
Giuseppe Carenini, Raymond T. Ng and Xiaodong Zhou
Department of Computer Science
University of British Columbia
Vancouver, BC, Canada
{carenini, rng, xdzhou}@cs.ubc.ca
Abstract
In this paper, we study the problem of sum-
marizing email conversations. We first build
a sentence quotation graph that captures the
conversation structure among emails. We
adopt three cohesion measures: clue words,
semantic similarity and cosine similarity as
the weight of the edges. Second, we use
two graph-based summarization approaches,
Generalized ClueWordSummarizer and Page-
Rank, to extract sentences as summaries.
Third, we propose a summarization approach
based on subjective opinions and integrate it
with the graph-based ones. The empirical
evaluation shows that the basic clue words
have the highest accuracy among the three co-
hesion measures. Moreover, subjective words
can significantly improve accuracy.
1 Introduction
With the ever increasing popularity of emails, it is
very common nowadays that people discuss spe-
cific issues, events or tasks among a group of peo-
ple by emails(Fisher and Moody, 2002). Those dis-
cussions can be viewed as conversations via emails
and are valuable for the user as a personal infor-
mation repository(Ducheneaut and Bellotti, 2001).
In this paper, we study the problem of summariz-
ing email conversations. Solutions to this problem
can help users access the information embedded in
emails more effectively. For instance, 10 minutes
before a meeting, a user may want to quickly go
through a previous discussion via emails that is go-
ing to be discussed soon. In that case, rather than
reading each individual email one by one, it would
be preferable to read a concise summary of the pre-
vious discussion with the major information summa-
rized. Email summarization is also helpful for mo-
bile email users on a small screen.
Summarizing email conversations is challenging
due to the characteristics of emails, especially the
conversational nature. Most of the existing meth-
ods dealing with email conversations use the email
thread to represent the email conversation struc-
ture, which is not accurate in many cases (Yeh and
Harnly, 2006). Meanwhile, most existing email
summarization approaches use quantitative features
to describe the conversation structure, e.g., number
of recipients and responses, and apply some general
multi-document summarization methods to extract
some sentences as the summary (Rambow et al,
2004) (Wan and McKeown, 2004). Although such
methods consider the conversation structure some-
how, they simplify the conversation structure into
several features and do not fully utilize it into the
summarization process.
In contrast, in this paper, we propose new summa-
rization approaches by sentence extraction, which
rely on a fine-grain representation of the conversa-
tion structure. We first build a sentence quotation
graph by content analysis. This graph not only cap-
tures the conversation structure more accurately, es-
pecially for selective quotations, but it also repre-
sents the conversation structure at the finer granular-
ity of sentences. As a second contribution of this pa-
per, we study several ways to measure the cohesion
between parent and child sentences in the quotation
graph: clue words (re-occurring words in the reply)
353
(Carenini et al, 2007), semantic similarity and co-
sine similarity. Hence, we can directly evaluate the
importance of each sentence in terms of its cohesion
with related ones in the graph. The extractive sum-
marization problem can be viewed as a node ranking
problem. We apply two summarization algorithms,
Generalized ClueWordSummarizer and Page-Rank
to rank nodes in the sentence quotation graph and
to select the corresponding most highly ranked sen-
tences as the summary.
Subjective opinions are often critical in many con-
versations. As a third contribution of this paper, we
study how to make use of the subjective opinions
expressed in emails to support the summarization
task. We integrate our best cohesion measure to-
gether with the subjective opinions. Our empirical
evaluations show that subjective words and phrases
can significantly improve email summarization.
To summarize, this paper is organized as follows.
In Section 2, we discuss related work. After building
a sentence quotation graph to represent the conver-
sation structure in Section 3, we apply two summa-
rization methods in Section 4. In Section 5, we study
summarization approaches with subjective opinions.
Section 6 presents the empirical evaluation of our
methods. We conclude this paper and propose fu-
ture work in Section 7.
2 Related Work
Rambow et al proposed a sentence extraction sum-
marization approach for email threads (Rambow et
al., 2004). They described each sentence in an email
conversations by a set of features and used machine
learning to classify whether or not a sentence should
be included into the summary. Their experiments
showed that features about emails and the email
thread could significantly improve the accuracy of
summarization.
Wan et al proposed a summarization approach
for decision-making email discussions (Wan and
McKeown, 2004). They extracted the issue and re-
sponse sentences from an email thread as a sum-
mary. Similar to the issue-response relationship,
Shrestha et al(Shrestha and McKeown, 2004) pro-
posed methods to identify the question-answer pairs
from an email thread. Once again, their results
showed that including features about the email
thread could greatly improve the accuracy. Simi-
lar results were obtained by Corston-Oliver et al
They studied how to identify ?action? sentences
in email messages and use those sentences as a
summary(Corston-Oliver et al, 2004). All these ap-
proaches used the email thread as a coarse represen-
tation of the underlying conversation structure.
In our recent study (Carenini et al, 2007), we
built a fragment quotation graph to represent an
email conversation and developed a ClueWordSum-
marizer (CWS) based on the concept of clue words.
Our experiments showed that CWS had a higher
accuracy than the email summarization approach
in (Rambow et al, 2004) and the generic multi-
document summarization approach MEAD (Radev
et al, 2004). Though effective, the CWS method
still suffers from the following four substantial limi-
tations. First, we used a fragment quotation graph to
represent the conversation, which has a coarser gran-
ularity than the sentence level. For email summa-
rization by sentence extraction, the fragment granu-
larity may be inadequate. Second, we only adopted
one cohesion measure (clue words that are based on
stemming), and did not consider more sophisticated
ones such as semantically similar words. Third, we
did not consider subjective opinions. Finally, we did
not compared CWS to other possible graph-based
approaches as we propose in this paper.
Other than for email summarization, other docu-
ment summarization methods have adopted graph-
ranking algorithms for summarization, e.g., (Wan et
al., 2007), (Mihalcea and Tarau, 2004) and (Erkan
and Radev, 2004). Those methods built a complete
graph for all sentences in one or multiple documents
and measure the similarity between every pair of
sentences. Graph-ranking algorithms, e.g., Page-
Rank (Brin and Page, 1998), are then applied to rank
those sentences. Our method is different from them.
First, instead of using the complete graph, we build
the graph based on the conversation structure. Sec-
ond, we try various ways to compute the similarity
among sentences and the ranking of the sentences.
Several studies in the NLP literature have ex-
plored the reoccurrence of similar words within one
document due to text cohesion. The idea has been
formalized in the construct of lexical chains (Barzi-
lay and Elhadad, 1997). While our approach and
lexical chains both rely on lexical cohesion, they are
354
quite different with respect to the kind of linkages
considered. Lexical chain is only based on similar-
ities between lexical items in contiguous sentences.
In contrast, in our approach, the linkage is based on
the existing conversation structure. In our approach,
the ?chain? is not only ?lexical? but also ?conversa-
tional?, and typically spans over several emails.
3 Extracting Conversations from Multiple
Emails
In this section, we first review how to build a frag-
ment quotation graph through an example. Then we
extend this structure into a sentence quotation graph,
which can allow us to capture the conversational re-
lationship at the level of sentences.
3.1 Building the Fragment Quotation Graph
b
> a
E2
c
> b
> > a
E3 E4
d
e
> c
> > b
> > > a
E5
g
h
> > d
> f
> > e
E6
> g
i
> h
j
a
E1
(a) Conversation involving 6 Emails
ba c
e
d
f
h
g i
j
(b) Fragment Quotation Graph
Figure 1: A Real Example
Figure 1(a) shows a real example of a conversa-
tion from a benchmark data set involving 6 emails.
For the ease of representation, we do not show the
original content but abbreviate them as a sequence
of fragments. In the first step, all new and quoted
fragments are identified. For instance, email E3 is
decomposed into 3 fragments: new fragment c and
quoted fragments b, which in turn quoted a. E4
is decomposed into de, c, b and a. Then, in the
second step, to identify distinct fragments (nodes),
fragments are compared with each other and over-
laps are identified. Fragments are split if necessary
(e.g., fragment gh in E5 is split into g and h when
matched with E6), and duplicates are removed. At
the end, 10 distinct fragments a, . . . , j give rise to
10 nodes in the graph shown in Figure 1(b).
As the third step, we create edges, which repre-
sent the replying relationship among fragments. In
general, it is difficult to determine whether one frag-
ment is actually replying to another fragment. We
assume that any new fragment is a potential reply to
neighboring quotations ? quoted fragments immedi-
ately preceding or following it. Let us consider E6
in Figure 1(a). there are two edges from node i to g
and h, while there is only a single edge from j to h.
For E3, there are the edges (c, b) and (c, a). Because
of the edge (b, a), the edge (c, a) is not included in
Figure 1(b). Figure 1(b) shows the fragment quota-
tion graph of the conversation shown in Figure 1(a)
with all the redundant edges removed. In contrast,
if threading is done at the coarse granularity of en-
tire emails, as adopted in many studies, the thread-
ing would be a simple chain from E6 to E5, E5 to
E4 and so on. Fragment f reflects a special and im-
portant phenomenon, where the original email of a
quotation does not exist in the user?s folder. We call
this as the hidden email problem. This problem and
its influence on email summarization were studied
in (Carenini et al, 2005) and (Carenini et al, 2007).
3.2 Building the Sentence Quotation Graph
A fragment quotation graph can only represent the
conversation in the fragment granularity. We no-
tice that some sentences in a fragment are more rel-
evant to the conversation than the remaining ones.
The fragment quotation graph is not capable of rep-
resenting this difference. Hence, in the following,
we describe how to build a sentence quotation graph
from the fragment quotation graph and introduce
several ways to give weight to the edges.
In a sentence quotation graph GS, each node rep-
resents a distinct sentence in the email conversation,
and each edge (u, v) represents the replying rela-
tionship between node u and v. The algorithm to
create the sentence quotation graph contains the fol-
lowing 3 steps: create nodes, create edges and assign
weight to edges. In the following, we first illustrate
how to create nodes and edges. In Section 3.3, we
discuss different ways to assign weight to edges.
Given a fragment quotation graph GF , we first
split each fragment into a set of sentences. For each
sentence, we create a node in the sentence quotation
graph GS. In this way, each sentence in the email
conversation is represented by a distinct node in GS.
As the second step, we create the edges in GS.
The edges in GS are based on the edges in GF
355
Pk
s1 s2 sn
P1
C1 Ck
(a) Fragment Quotation Graph
(b) Sentence Quotation Graph
F: 
Ct
s1, s2,...,sn
... ...
P1
C1
Pk... ...
Figure 2: Create the Sentence Quotation Graph from the
Fragment Quotation Graph
because the edges in GF already reflect the reply-
ing relationship among fragments. For each edge
(u, v) ? GF , we create edges from each sentence
of u to each sentence of v in the sentence quotation
graph GS. This is illustrated in Figure 2.
Note that when each distinct sentence in an email
conversation is represented as one node in the sen-
tence quotation graph, the extractive email sum-
marization problem is transformed into a standard
node ranking problem within the sentence quotation
graph. Hence, general node ranking algorithms, e.g.,
Page-Rank, can be used for email summarization as
well.
3.3 Measuring the Cohesion Between
Sentences
After creating the nodes and edges in the sentence
quotation graph, a key technical question is how to
measure the degree that two sentences are related to
each other, e.g., a sentence su is replying to or be-
ing replied by sv. In this paper, we use text cohe-
sion between two sentences su and sv to make this
assessment and assign this as the weight of the cor-
responding edge (su, sv). We explore three types
of cohesion measures: (1) clue words that are based
on stems, (2) semantic distance based on WordNet
and (3) cosine similarity that is based on the word
TFIDF vector. In the following, we discuss these
three methods separately in detail.
3.3.1 Clue Words
Clue words were originally defined as re-
occurring words with the same stem between two
adjacent fragments in the fragment quotation graph.
In this section, we re-define clue words based on the
sentence quotation graph as follows. A clue word in
a sentence S is a non-stop word that also appears
(modulo stemming) in a parent or a child node (sen-
tence) of S in the sentence quotation graph.
The frequency of clue words in the two sentences
measures their cohesion as described in Equation 1.
weight(su, sv) =
?
wi?su
freq(wi, sv) (1)
3.3.2 Semantic Similarity Based on WordNet
Other than stems, when people reply to previous
messages they may also choose some semantically
related words, such as synonyms and antonyms, e.g.,
?talk? vs. ?discuss?. Based on this observation, we
propose to use semantic similarity to measure the
cohesion between two sentences. We use the well-
known lexical database WordNet to get the seman-
tic similarity of two words. Specifically, we use the
package by (Pedersen et al, 2004), which includes
several methods to compute the semantic similarity.
Among those methods, we choose ?lesk? and ?jcn?,
which are considered two of the best methods in (Ju-
rafsky and Martin, 2008). Similar to the clue words,
we measure the semantic similarity of two sentences
by the total semantic similarity of the words in both
sentences. This is described in the following equa-
tion.
weight(su, sv) =
?
wi?su
?
wj?sv
?(wi, wj), (2)
3.3.3 Cosine Similarity
Cosine similarity is a popular metric to compute
the similarity of two text units. To do so, each sen-
tence is represented as a word vector of TFIDF val-
ues. Hence, the cosine similarity of two sentences
su and sv is then computed as
??su ???sv
||??su ||?||??sv || .
356
4 Summarization Based on the Sentence
Quotation Graph
Having built the sentence quotation graph with dif-
ferent measures of cohesion, in this section, we de-
velop two summarization approaches. One is the
generalization of the CWS algorithm in (Carenini
et al, 2007) and one is the well-known Page-
Rank algorithm. Both algorithms compute a score,
SentScore(s), for each sentence (node) s, which is
used to select the top-k% sentences as the summary.
4.1 Generalized ClueWordSummarizer
Given the sentence quotation graph, since the weight
of an edge (s, t) represents the extent that s is related
to t, a natural assumption is that the more relevant a
sentence (node) s is to its parents and children, the
more important s is. Based on this assumption, we
compute the weight of a node s by summing up the
weight of all the outgoing and incoming edges of s.
This is described in the following equation.
SentScore(s) =
?
(s,t)?GS
weight(s, t) +
?
(p,s)?GS
weight(p, s)
(3)
The weight of an edge (s, t) can be any of the
three metrics described in the previous section. Par-
ticularly, when the weight of the edge is based on
clue words as in Equation 1, this method is equiva-
lent to Algorithm CWS in (Carenini et al, 2007). In
the rest of this paper, let CWS denote the General-
ized ClueWordSummarizer when the edge weight is
based on clue words, and let CWS-Cosine and CWS-
Semantic denote the summarizer when the edge
weight is cosine similarity and semantic similarity
respectively. Semantic can be either ?lesk? or ?jcn?.
4.2 Page-Rank-based Summarization
The Generalized ClueWordSummarizer only con-
siders the weight of the edges without considering
the importance (weight) of the nodes. This might
be incorrect in some cases. For example, a sentence
replied by an important sentence should get some of
its importance. This intuition is similar to the one
inspiring the well-known Page-Rank algorithm. The
traditional Page-Rank algorithm only considers the
outgoing edges. In email conversations, what we
want to measure is the cohesion between sentences
no matter which one is being replied to. Hence, we
need to consider both incoming and outgoing edges
and the corresponding sentences.
Given the sentence quotation graph Gs, the Page-
Rank-based algorithm is described in Equation 4.
PR(s) is the Page-Rank score of a node (sentence)
s. d is the dumping factor, which is initialized to
0.85 as suggested in the Page-Rank algorithm. In
this way, the rank of a sentence is evaluated globally
based on the graph.
5 Summarization with Subjective
Opinions
Other than the conversation structure, the measures
of cohesion and the graph-based summarization
methods we have proposed, the importance of a sen-
tence in emails can be captured from other aspects.
In many applications, it has been shown that sen-
tences with subjective meanings are paid more at-
tention than factual ones(Pang and Lee, 2004)(Esuli
and Sebastiani, 2006). We evaluate whether this is
also the case in emails, especially when the conver-
sation is about decision making, giving advice, pro-
viding feedbacks, etc.
A large amount of work has been done on deter-
mining the level of subjectivity of text (Shanahan
et al, 2005). In this paper we follow a very sim-
ple approach that, if successful, could be extended
in future work. More specifically, in order to as-
sess the degree of subjectivity of a sentence s, we
count the frequency of words and phrases in s that
are likely to bear subjective opinions. The assump-
tion is that the more subjective words s contains, the
more likely that s is an important sentence for the
purpose of email summarization. Let SubjScore(s)
denote the number of words with a subjective mean-
ing. Equation 5 illustrates how SubjScore(s) is com-
puted. SubjList is a list of words and phrases that
indicate subjective opinions.
SubjScore(s) =
?
wi?SubjList,wi?s
freq(wi) (5)
The SubjScore(s) alone can be used to evaluate
the importance of a sentence. In addition, we can
combine SubjScore with any of the sentence scores
based on the sentence quotation graph. In this paper,
we use a simple approach by adding them up as the
final sentence score.
357
PR(s) = (1 ? d) + d ?
?
si?child(s)
weight(s, si) ? PR(si) +
?
sj?parent(s)
weight(sj , s) ? PR(sj)
?
si?child(s)
weight(s, si) +
?
sj?parent(s)
weight(sj , s)
(4)
As to the subjective words and phrases, we
consider the following two lists generated by re-
searchers in this area.
? OpFind: The list of subjective words in (Wil-
son et al, 2005). The major source of this list is
from (Riloff and Wiebe, 2003) with additional
words from other sources. This list contains
8,220 words or phrases in total.
? OpBear: The list of opinion bearing words
in (Kim and Hovy, 2005). This list contains
27,193 words or phrases in total.
6 Empirical Evaluation
6.1 Dataset Setup
There are no publicly available annotated corpora to
test email summarization techniques. So, the first
step in our evaluation was to develop our own cor-
pus. We use the Enron email dataset, which is the
largest public email dataset. In the 10 largest in-
box folders in the Enron dataset, there are 296 email
conversations. Since we are studying summarizing
email conversations, we required that each selected
conversation contained at least 4 emails. In total, 39
conversations satisfied this requirement. We use the
MEAD package to segment the text into 1,394 sen-
tences (Radev et al, 2004).
We recruited 50 human summarizers to review
those 39 selected email conversations. Each email
conversation was reviewed by 5 different human
summarizers. For each given email conversation,
human summarizers were asked to generate a sum-
mary by directly selecting important sentences from
the original emails in that conversation. We asked
the human summarizers to select 30% of the total
sentences in their summaries.
Moreover, human summarizers were asked to
classify each selected sentence as either essential
or optional. The essential sentences are crucial to
the email conversation and have to be extracted in
any case. The optional sentences are not critical but
are useful to help readers understand the email con-
versation if the given summary length permits. By
classifying essential and optional sentences, we can
distinguish the core information from the support-
ing ones and find the most convincing sentences that
most human summarizers agree on.
As essential sentences are more important than
the optional ones, we give more weight to the es-
sential selections. We compute a GSV alue for each
sentence to evaluate its importance according to the
human summarizers? selections. The score is de-
signed as follows: for each sentence s, one essen-
tial selection has a score of 3, one optional selec-
tion has a score of 1. Thus, the GSValue of a sen-
tence ranges from 0 to 15 (5 human summarizers x
3). The GSValue of 8 corresponds to 2 essential and
2 optional selections. If a sentence has a GSValue
no less than 8, we take it as an overall essential sen-
tence. In the 39 conversations, we have about 12%
overall essential sentences.
6.2 Evaluation Metrics
Evaluation of summarization is believed to be a dif-
ficult problem in general. In this paper, we use two
metrics to measure the accuracy of a system gener-
ated summary. One is sentence pyramid precision,
and the other is ROUGE recall. As to the statistical
significance, we use the 2-tail pairwise student t-test
in all the experiments to compare two specific meth-
ods. We also use ANOVA to compare three or more
approaches together.
The sentence pyramid precision is a relative pre-
cision based on the GSValue. Since this idea is
borrowed from the pyramid metric by Nenkova et
al.(Nenkova et al, 2007), we call it the sentence
pyramid precision. In this paper, we simplify it as
the pyramid precision. As we have discussed above,
with the reviewers? selections, we get a GSValue for
each sentence, which ranges from 0 to 15. With
this GSValue, we rank all sentences in a descendant
order. We also group all sentences with the same
GSValue together as one tier Ti, where i is the corre-
358
sponding GSValue; i is called the level of the tier Ti.
In this way, we organize all sentences into a pyra-
mid: a sequence of tiers with a descendant order of
levels. With the pyramid of sentences, the accuracy
of a summary is evaluated over the best summary we
can achieve under the same summary length. The
best summary of k sentences are the top k sentences
in terms of GSValue.
Other than the sentence pyramid precision, we
also adopt the ROUGE recall to evaluate the gen-
erated summary with a finer granularity than sen-
tences, e.g., n-gram and longest common subse-
quence. Unlike the pyramid method which gives
more weight to sentences with a higher GSValue,
ROUGE is not sensitive to the difference between
essential and optional selections (it considers all sen-
tences in one summary equally). Directly applying
ROUGE may not be accurate in our experiments.
Hence, we use the overall essential sentences as the
gold standard summary for each conversation, i.e.,
sentences in tiers no lower than T8. In this way,
the ROUGE metric measures the similarity of a sys-
tem generated summary to a gold standard summary
that is considered important by most human sum-
marizers. Specifically, we choose ROUGE-2 and
ROUGE-L as the evaluation metric.
6.3 Evaluating the Weight of Edges
In Section 3.3, we developed three ways to com-
pute the weight of an edge in the sentence quotation
graph, i.e., clue words, semantic similarity based on
WordNet and cosine similarity. In this section, we
compare them together to see which one is the best.
It is well-known that the accuracy of the summariza-
tion method is affected by the length of the sum-
mary. In the following experiments, we choose the
summary length as 10%, 12%, 15%, 20% and 30%
of the total sentences and use the aggregated average
accuracy to evaluate different algorithms.
Table 1 shows the aggregated pyramid preci-
sion over all five summary lengths of CWS, CWS-
Cosine, two semantic similarities, i.e., CWS-lesk
and CWS-jcn. We first use ANOVA to compare the
four methods. For the pyramid precision, the F ratio
is 50, and the p-value is 2.1E-29. This shows that the
four methods are significantly different in the aver-
age accuracy. In Table 1, by comparing CWS with
the other methods, we can see that CWS obtains the
CWS CWS-Cosine CWS-lesk CWS-jcn
Pyramid 0.60 0.39 0.57 0.57
p-value <0.0001 0.02 0.005
ROUGE-2 0.46 0.31 0.39 0.35
p-value <0.0001 <0.001 <0.001
ROUGE-L 0.54 0.43 0.49 0.45
p-value <0.0001 <0.001 <0.001
Table 1: Generalized CWS with Different Edge Weights
highest precision (0.60). The widely used cosine
similarity does not perform well. Its precision (0.39)
is about half of the precision of CWS with a p-value
less than 0.0001. This clearly shows that CWS is
significantly better than CWS-Cosine. Meanwhile,
both semantic similarities have lower accuracy than
CWS, and the differences are also statistically sig-
nificant even with the conservative Bonferroni ad-
justment (i.e., the p-values in Table 1 are multiplied
by three).
The above experiments show that the widely used
cosine similarity and the more sophisticated seman-
tic similarity in WordNet are less accurate than the
basic CWS in the summarization framework. This is
an interesting result and can be viewed at least from
the following two aspects. First, clue words, though
straight forward, are good at capturing the impor-
tant sentences within an email conversation. The
higher accuracy of CWS may suggest that people
tend to use the same words to communicate in email
conversations. Some related words in the previous
emails are adopted exactly or in another similar for-
mat (modulo stemming). This is different from other
documents such as newspaper articles and formal re-
ports. In those cases, the authors are usually profes-
sional in writing and choose their words carefully,
even intentionally avoid repeating the same words
to gain some diversity. However, for email conver-
sation summarization, this does not appear to be the
case.
Moreover, in the previous discussion we only con-
sidered the accuracy in precision without consider-
ing the runtime issue. In order to have an idea of
the runtime of the two methods, we did the follow-
ing comparison. We randomly picked 1000 pairs of
words from the 20 conversations and compute their
semantic distance in ?jcn?. It takes about 0.056 sec-
onds to get the semantic similarity for one pair on the
359
average. In contrast, when the weight of edges are
computed based on clue words, the average runtime
to compute the SentScore for all sentences in a con-
versation is only 0.05 seconds, which is even a little
less than the time to compute the semantic similar-
ity of one pair of words. In other words, when CWS
has generated the summary of one conversation, we
can only get the semantic distance between one pair
of words. Note that for each edge in the sentence
quotation graph, we need to compute the distance
for every pair of words in each sentence. Hence, the
empirical results do not support the use of semantic
similarity. In addition, we do not discuss the runtime
performance of CWS-cosine here because of its ex-
tremely low accuracy.
6.4 Comparing Page-Rank and CWS
Table 2 compares Page-Rank and CWS under differ-
ent edge weights. We compare Page-Rank only with
CWS because CWS is better than the other Gener-
alized CWS methods as shown in the previous sec-
tion. This table shows that Page-Rank has a lower
accuracy than that of CWS and the difference is sig-
nificant in all four cases. Moreover, when we com-
pare Table 1 and 2 together, we can find that, for
each kind of edge weight, Page-Rank has a lower
accuracy than the corresponding Generalized CWS.
Note that Page-Rank computes a node?s rank based
on all the nodes and edges in the graph. In contrast,
CWS only considers the similarity between neigh-
boring nodes. The experimental result indicates that
for email conversation, the local similarity based on
clue words is more consistent with the human sum-
marizers? selections.
6.5 Evaluating Subjective Opinions
Table 3 shows the result of using subjective opinions
described in Section 5. The first 3 columns in this ta-
ble are pyramid precision of CWS and using 2 lists
of subjective words and phrases alone. We can see
that by using subjective words alone, the precision of
each subjective list is lower than that of CWS. How-
ever, when we integrate CWS and subjective words
together, as shown in the remaining 2 columns, the
precisions get improved consistently for both lists.
The increase in precision is at least 0.04 with statisti-
cal significance. A natural question to ask is whether
clue words and subjective words overlap much. Our
CWS PR-Clue PR-Cosine PR-lesk PR-jcn
Pyramid 0.60 0.51 0.37 0.54 0.50
p-value < 0.0001 < 0.0001 < 0.0001 < 0.0001
ROUGE-2 0.46 0.4 0.26 0.36 0.39
p-value 0.05 < 0.0001 0.001 0.02
ROUGE-L 0.54 0.49 0.36 0.44 0.48
p-value 0.06 < 0.0001 0.0005 0.02
Table 2: Compare Page-Rank with CWS
CWS OpFind OpBear CWS+OpFind CWS+OpBear
Pyramid 0.60 0.52 0.59 0.65 0.64
p-value 0.0003 0.8 <0.0001 0.0007
ROUGE-2 0.46 0.37 0.44 0.50 0.49
p-value 0.0004 0.5 0.004 0.06
ROUGE-L 0.54 0.48 0.56 0.60 0.59
p-value 0.01 0.6 0.0002 0.002
Table 3: Accuracy of Using Subjective Opinions
analysis shows that the overlap is minimal. For the
list of OpFind, the overlapped words are about 8%
of clue words and 4% of OpFind that appear in the
conversations. This result clearly shows that clue
words and subjective words capture the importance
of sentences from different angles and can be used
together to gain a better accuracy.
7 Conclusions
We study how to summarize email conversations
based on the conversational cohesion and the sub-
jective opinions. We first create a sentence quota-
tion graph to represent the conversation structure on
the sentence level. We adopt three cohesion metrics,
clue words, semantic similarity and cosine similar-
ity, to measure the weight of the edges. The Gener-
alized ClueWordSummarizer and Page-Rank are ap-
plied to this graph to produce summaries. Moreover,
we study how to include subjective opinions to help
identify important sentences for summarization.
The empirical evaluation shows the following two
discoveries: (1) The basic CWS (based on clue
words) obtains a higher accuracy and a better run-
time performance than the other cohesion measures.
It also has a significant higher accuracy than the
Page-Rank algorithm. (2) By integrating clue words
and subjective words (phrases), the accuracy of
CWS is improved significantly. This reveals an in-
teresting phenomenon and will be further studied.
References
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
360
the Intelligent Scalable Text Summarization Workshop
(ISTS?97), ACL, Madrid, Spain.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
Proceedings of the seventh international conference on
World Wide Web, pages 107?117.
Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou.
2005. Scalable discovery of hidden emails from large
folders. In ACM SIGKDD?05, pages 544?549.
Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou.
2007. Summarizing email conversations with clue
words. In WWW ?07: Proceedings of the 16th interna-
tional conference on World Wide Web, pages 91?100.
Simon Corston-Oliver, Eric K. Ringger, Michael Gamon,
and Richard Campbell. 2004. Integration of email
and task lists. In First conference on email and anti-
Spam(CEAS), Mountain View, California, USA, July
30-31.
Nicolas Ducheneaut and Victoria Bellotti. 2001. E-mail
as habitat: an exploration of embedded personal infor-
mation management. Interactions, 8(5):30?38.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search(JAIR), 22:457?479.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In Proceedings of the International Confer-
ence on Language Resources and Evaluation, May 24-
26.
Danyel Fisher and Paul Moody. 2002. Studies of au-
tomated collection of email records. In University of
Irvine ISR Technical Report UCI-ISR-02-4.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition (Second Edition). Prentice-Hall.
Soo-Min Kim and Eduard Hovy. 2005. Automatic de-
tection of opinion bearing words and sentences. In
Proceedings of the Second International Joint Con-
ference on Natural Language Processing: Companion
Volume, Jeju Island, Republic of Korea, October 11-
13.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2004), July.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transaction on Speech and Language
Processing, 4(2):4.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL ?04: Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 271?278.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of Fifth Annual
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-04),
pages 38?41, May 3-5.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys?, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40(6):919?938, November.
Owen Rambow, Lokesh Shrestha, John Chen, and
Chirsty Lauridsen. 2004. Summarizing email threads.
In HLT/NAACL, May 2?7.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2003), pages 105?
112.
James G. Shanahan, Yan Qu, and Janyce Wiebe. 2005.
Computing Attitude and Affect in Text: Theory
and Applications (The Information Retrieval Series).
Springer-Verlag New York, Inc.
Lokesh Shrestha and Kathleen McKeown. 2004. Detec-
tion of question-answer pairs in email conversations.
In Proceedings of COLING?04, pages 889?895, Au-
gust 23?27.
Stephen Wan and Kathleen McKeown. 2004. Generat-
ing overview summaries of ongoing email thread dis-
cussions. In Proceedings of COLING?04, the 20th In-
ternational Conference on Computational Linguistics,
August 23?27.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 552?
559, Prague, Czech Republic, June.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, pages 34?35.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread
reassembly using similarity matching. In Third Con-
ference on Email and Anti-Spam (CEAS), July 27 - 28.
361
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 7?14,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Optimization-based Content Selection for Opinion Summarization
Jackie Chi Kit Cheung
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
jcheung@cs.toronto.edu
Giuseppe Carenini and Raymond T. Ng
Department of Computer Science
University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
{carenini,rng}@cs.ubc.ca
Abstract
We introduce a content selection method
for opinion summarization based on a
well-studied, formal mathematical model,
the p-median clustering problem from fa-
cility location theory. Our method re-
places a series of local, myopic steps to
content selection with a global solution,
and is designed to allow content and re-
alization decisions to be naturally inte-
grated. We evaluate and compare our
method against an existing heuristic-based
method on content selection, using human
selections as a gold standard. We find that
the algorithms perform similarly, suggest-
ing that our content selection method is
robust enough to support integration with
other aspects of summarization.
1 Introduction
It is now possible to find a large amount of in-
formation on people?s opinions on almost every
subject online. The ability to analyze such infor-
mation is critical in complex, high-stakes decision
making processes. At the individual level, some-
one wishing to buy a laptop may read customer
reviews from others who have purchased and used
the product. At the corporate level, customer feed-
back on a newly launched product may help to
identify weaknesses and features that are in need
of improvement (Dellarocas et al, 2004).
Effective summarization systems are thus
needed to convey people?s opinions to users. A
challenging problem in implementing this ap-
proach in a particular domain is to devise a con-
tent selection strategy that identifies what key in-
formation should be presented. In general, content
selection is a critical task at the core of both sum-
marization and NLG and it represents a promising
area for cross-fertilization.
Existing NLG systems tend to approach con-
tent selection by defining a heuristic based on sev-
eral relevant factors, and maximizing this heuristic
function. ILEX (Intelligent Labelling Explorer) is
a system for generating labels for sets of objects
defined in a database, such as for museum arti-
facts (O?Donnell et al, 2001). Its content selection
strategy involves computing a heuristic relevance
score for knowledge elements, and returning the
items with the highest scores.
In GEA (Generator of Evaluative Arguments),
evaluative arguments are generated to describe an
entity as positive or negative (Carenini and Moore,
2006). An entity is decomposed into a hierarchy
of features, and a relevance score is independently
calculated for each feature, based on the prefer-
ences of the user and the value of that feature for
the product. Content selection involves selecting
the most relevant features for the current user.
There is also work in sentiment analysis relying
on optimization or clustering-based approaches.
Pang and Lee (2004) frame the problem of detect-
ing subjective sentences as finding the minimum
cut in a graph representation of the sentences.
They produce compressed versions of movie re-
views using just the subjective sentences, which
retain the polarity information of the review. Ga-
mon et al (2005) use a heuristic approach to
cluster sentences drawn from car reviews, group-
ing sentences that share common terms, especially
those salient in the domain such as ?drive? or ?han-
dling?. The resulting clusters are displayed by a
Treemap visualization.
Our work is most similar to the content se-
lection method of the multimedia conversation
system RIA (Responsive Information Architect)
(Zhou and Aggarwal, 2004). In RIA, content
selection involves selecting dimensions (such as
price in the real estate domain) in response to a
query such that the desirability of the dimensions
selected for the query is maximized while respect-
7
ing time and space constraints. The maximization
of desirability is implemented as an optimization
problem similar to a knapsack problem. RIA?s
content selection method performs similarly to ex-
pert human designers, but the evaluation is limited
in scale (two designers, each annotating two se-
ries of queries to the system), and no heuristic al-
ternative is compared against it. Our work also
frames content selection as a formal optimization
problem, but we apply this model to the domain of
opinion summarization.
A key advantage of formulating a content selec-
tion strategy as a p-median optimization problem
is that the resulting framework can be extended to
select other characteristics of the summary at the
same time as the information content, such as the
realization strategy with which the content is ex-
pressed. The p-median clustering works as a mod-
ule separate from its interpretation as the solution
to a content selection problem, so we can freely
modify the conversion process from the selection
problem to the clustering problem. Work in NLG
and summarization has shown that content and
realization decisions (including media allocation)
are often dependent on each other, which should
be reflected in the summarization process. For
example, in multi-modal summarization, complex
information can be more effectively conveyed by
combining graphics and text (Tufte et al, 1998).
While graphics can present large amounts of data
compactly and support the discovery of trends and
relationships, text is much more effective at ex-
plaining key points about the data. In another case
specific to opinion summarization, the controver-
siality of the opinions in a corpus was found to cor-
relate with the type of text summary, with abstrac-
tive summarization being preferred when the con-
troversiality is high (Carenini and Cheung, 2008).
We first test whether our optimization-based
approach can achieve reasonable performance on
content selection alone. As a contribution of this
paper, we compare our optimization-based ap-
proach to a previously proposed heuristic method.
Because our approach replaces a set of myopic de-
cisions with an extensively studied procedure (the
p-median problem) that is able to find a global so-
lution, we hypothesized our approach would pro-
duce better selections. The results of our study
indicate that our optimization-based content selec-
tion strategy performs about as well as the heuris-
tic method. These results suggest that our frame-
work is robust enough for integrating other aspects
of summarization with content selection.
2 Previous Heuristic Approach
2.1 Assumed Input Information
We now define the expected input into the summa-
rization process, then describe a previous greedy
heuristic method. The first phase of the summa-
rization process is to extract opinions about an en-
tity from free text or some other source, such as
surveys. and express the extracted information in a
structured format for further processing. We adopt
the approach to opinion extraction described by
Carenini et al (2006), which we summarize here.
Given a corpus of documents expressing opin-
ions about an entity, the system extracts a set of
evaluations on aspects or features of the product.
An evaluation consists of a polarity, a score for
the strength of the opinion, and the feature be-
ing evaluated. The polarity expresses whether the
opinion is positive or negative, and the strength
expresses the degree of the sentiment, which is
represented as an integer from 1 to 3. Possi-
ble polarity/strength (P/S) scores are thus [-3,-
2,-1,+1,+2,+3], with +3 being the most positive
evaluation, and -3 the most negative. For exam-
ple, using a DVD player as the entity, the com-
ment ?Excellent picture quality?on par with my
Pioneer, Panasonic, and JVC players.? contains an
opinion on the picture quality, and is a very posi-
tive evaluation (+3).
The features and their associated opinions are
organized into a hierarchy of user-defined features
(UDFs), so named because they can be defined by
a user according to the user?s needs or interests.
1
The outcome of the process of opinion extraction
and structuring is a UDF hierarchy in which each
node is annotated with all the evaluations it re-
ceived in the corpus (See Figure 1 for an example).
2.2 Heuristic Content Selection Strategy
Using the input information described above, con-
tent selection is framed as the process of selecting
a subset of those features that are deemed more
1
Actually, the system first extracts a set of surface-level
crude features (CFs) on which opinions were expressed, us-
ing methods described by Hu and Liu (2004). Next, the CFs
are mapped onto the UDFs using term similarity scores. The
process of mapping CFs to UDFs groups together semanti-
cally similar CFs and reduces redundancy. Our study ab-
stracts away from this mapping process, as well as the pro-
cess of creating the UDF structure. We leave the explanation
of the details to the original papers.
8
Camera
Lens [+1,+1,+3,-
2,+2]
Digital Zoom
Optical Zoom
. . .
Editing/Viewing
[+1,+1]
Viewfinder [-2,-
2,-1]
. . .
Flash
[+1,+1,+3,+2,+2]
. . .
Image
Image Type
TIFF
JPEG
. . .
Resolution
Effective Pixels
Aspect Ratio
. . .
Figure 1: Partial view of assumed input informa-
tion (UDF hierarchy annotated with user evalua-
tions) for a digital camera.
important and relevant to the user. This is done
using an importance measure defined on the avail-
able features (UDFs). This measure is calculated
from the P/S scores of the evaluations associated
to each UDF. Let PS(u) be the set of P/S scores
that UDF u receives. Then, a measure of im-
portance is defined as some function of the P/S
scores. Previous work considered only summing
the squares of the scores. In this work, we also
consider summing the absolute value of the scores.
So, the importance measure is defined as
dir moi(u) =
?
psPS(u)
ps
2
or
?
psPS(u)
|ps|
where the term ?direct? means the importance is
derived only from that feature and not from its
descendant features. The basic premises of these
metrics are that a feature?s importance should be
proportional to the number of evaluations of that
feature in the corpus, and that stronger evaluations
should be given more weight. The two versions
implement the latter differently, using the sum of
squares or the absolute values respectively. No-
tice that each non-leaf node in the feature hierar-
chy effectively serves a dual purpose. It is both a
feature upon which a user might comment, as well
as a category for grouping its sub-features. Thus,
a non-leaf node should be important if either its
descendants are important or the node itself is im-
portant. To this end, a total measure of importance
moi(u) is defined as
moi(u) =
?
?
?
?
?
?
?
dir moi(u) if CH(u) = ?
[? dir moi(u) +
(1? ?)?
?
v?CH(u)
moi(v)] otherwise
where CH(u) refers to the children of u in
the hierarchy and ? is some real parameter in the
range [0.5, 1] that adjusts the relative weights of
the parent and children. We found in our experi-
mentation that the parameter setting does not sub-
stantially change the performance of the system,
so we select the value 0.9 for ?, following previ-
ous work. As a result, the total importance of a
node is a combination of its direct importance and
of the importance of its children.
The selection procedure proceeds as follows.
First, the most obvious simple greedy selection
strategy was considered?sort the nodes in the UDF
by the measure of importance and select the most
important node until a desired number of features
is included. However, since a node derives part
of its ?importance? from its children, it is possible
for a node?s importance to be dominated by one or
more of its children. Including both the child and
parent node would be redundant because most of
the information is contained in the child. Thus, a
dynamic greedy selection algorithm was devised
in which the importance of each node was recal-
culated after each round of selection, with all pre-
viously selected nodes removed from the tree. In
this way, if a node that dominates its parent?s im-
portance is selected, its parent?s importance will
be reduced during later rounds of selection. No-
tice, however, that this greedy selection consists of
a series of myopic steps to decide which features
to include in the summary next, based on what has
been selected already and what remains to be se-
lected at this step. Although this series of local
decisions may be locally optimal, it may result in
a suboptimal choice of contents overall.
3 Clustering-Based Optimization
Strategy
To address the limitation of local optimality of
this initial strategy, we explore if the content se-
lection problem for opinion summarization can
be naturally and effectively solved by a global
optimization-based approach. Our approach as-
sumes the same input information as the previ-
ous approach, and we also use the direct measure
9
of importance defined above. Our framework is
UDF-based in the following senses. First, a UDF
is the basic unit of content that is selected for in-
clusion in the summary. Also, the information
content that needs to be ?covered? by the summary
is the sum of the information content in all of the
UDFs in the UDF hierarchy.
To reduce content selection to a clustering prob-
lem, we need the following components. First, we
need a cost function to quantify how well a UDF
(if selected) can express the information content
in another UDF. We call this measure the infor-
mation coverage cost. To define this cost func-
tion, we need to define the semantic relatedness
between the selected content and the covered con-
tent, which is domain-dependent. For example, we
can rely on similarity metrics such as ones based
on WordNet similarity scores (Fellbaum and oth-
ers, 1998). In the consumer product domain in
which we test our method, we use the UDF hi-
erarchy of the entity being summarized.
Second, we need a clustering paradigm that de-
fines the quality of a proposed clustering; that is,
a way to globally quantify how well all the infor-
mation content is represented by the set of UDFs
that we select. The clustering paradigm that we
found to most naturally fit our task is the p-median
problem (also known as the k-median problem),
from facility location theory. In its original in-
terpretation, p-median is used to find optimal lo-
cations for opening facilities which provide ser-
vices to customers, such that the cost of serving
all of the customers with these facilities is mini-
mized. This matches our intuition that the quality
of a summary of opinions depends on how well it
represents all of the opinions to be summarized.
Formally, given a set F of m potential locations
for facilities, a set U of n customers, a cost func-
tion d : F ? U ?? < representing the cost of
serving a customer u ? U with a facility f ? F ,
and a constant p ? m, an optimal solution to the
p-median problem is a subset S of F , such that the
expression
?
u?U
min
f?S
d(f, u)
is minimized, and |S| = p. The subset S is exactly
the set of UDFs that we would include in the sum-
mary, and the parameter p can be set to determine
the summary length.
Although solving the p-median problem is NP-
hard in general (Kariv and Hakimi, 1979), viable
approximation methods do exist. We use POP-
STAR, an implementation of an approximate so-
lution (Resende and Werneck, 2004) which has
an average error rate of less than 0.4% on all the
problem classes it was tested on in terms of the p-
median problem value. As an independent test of
the program?s efficacy, we compare the program?s
output to solutions which we obtained by brute-
force search on 12 of the 36 datasets we worked
with which are small enough such that an exact so-
lution can be feasibly found. POPSTAR returned
the exact solution in all 12 instances.
We now reinterpret the p-median problem for
summarization content selection by specifying the
sets U , F , and the information coverage cost d in
terms of properties of the summarization process.
We define the basic unit of the summarization pro-
cess to be UDFs, so the sets U and F correspond
to the set of UDFs describing the product. The
constant p is a parameter to the p-median prob-
lem, determining the summary size in terms of the
number of features.
The cost function is d(u, v), where u is a UDF
that is being considered for inclusion in the sum-
mary, and v is the UDF to be ?covered? by u. To
specify this cost, we need to consider both the to-
tal amount of information in v as well as the se-
mantic relationship between the two features. We
use the importance measure defined earlier, based
on the number and strength of evaluations of the
covered feature to quantify the former. The raw
importance score is modified by multipliers which
depend on the relationship between u and v. One
is the semantic relatedness between the two fea-
tures, which is modelled by the UDF tree hierar-
chy. We hypothesize that it is easier for a more
general feature to cover information about a more
specific feature than the reverse, and that features
that are not in a ancestor-descendant relationship
cannot cover information about each other because
of the tenuous semantic connection between them.
For example, knowing that a camera is well-liked
in general provides stronger evidence that its dura-
bility is also well-liked than the reverse. Based on
these assumptions, we define a multiplier for the
above measure of importance based on the UDF
tree structure, T (u, v), as follows.
T (u, v) =
?
?
?
T
up
? k, if u is a descendant of v
k, if u is an ancestor of v
?, otherwise
k is the length of the path from u to v in the UDF
10
hierarchy. T
up
is a parameter specifying the rela-
tive difficulty of covering information in a feature
that is an ancestor in the UDF hierarchy. Mirror-
ing our experience with the heuristic method, the
value of the parameter does not affect performance
very much. In our experiments and the example to
follow, we pick the values T
up
= 3, meaning that
covering information in an ancestor node is three
times more difficult than covering information in
a descendant node.
Another multiplier to the opinion domain is the
distribution of evaluations of the features. Cover-
age is expected to be less if the features are evalu-
ated differently; for example, if users rated a cam-
era well overall but the feature zoom poorly, a sen-
tence about how well the camera is rated in gen-
eral does not provide much evidence that the zoom
is not well liked, and vice versa. Since evalua-
tions are labelled with P/S ratings in our data, it is
natural to define this multiplier based on the dis-
tributions of ratings for the features. Given these
P/S ratings between -3 and +3, we first aggregate
the positive and negative evaluations. As before,
we test both summing absolute values and squared
values. Define:
imp pos(u) =
?
ps?PS(u)?ps>0
ps
2
or |ps|
imp neg(u) =
?
ps?PS(u)?ps<0
ps
2
or |ps|
Then, we calculate the parameter to the Bernoulli
distribution corresponding to the ratio of the im-
portance of the two polarities. That is, Bernoulli
with parameter
?(u) = imp pos(u)/(imp pos(u)+imp neg(u))
The distribution-based multiplier E(u, v) is the
Jensen-Shannon divergence from Ber(?(u)) to
Ber(?(v)), plus one for multiplicative identity
when the divergence is zero.
E(u, v) = JS(?(u), ?(v)) + 1
The final formula for the information coverage
cost is thus
d(u, v) = dir moi(v)? T (u, v)? E(u, v)
Consider the following example consisting of
four-node UDF tree and importance scores.
i. Covered ii. Solutions
A B C D p Selected Val.
C
o
v
e
r
i
n
g
A 0 50 30 240 1 A 320
B 165 0 ? 120 2 A,D 80
C 165 ? 0 ? 3 A,B,D 30
D 330 150 ? 0 4 A,B,C,D 0
Table 1: i. Information coverage cost scores for
the worked example. Rows represent the covering
feature, while columns represent the covered fea-
ture. ii. Optimal solution to p-median problem in
the worked example at different numbers of fea-
tures selected.
A dir moi(A) = 55
??
B C dir moi(B) = 50, dir moi(C) = 30
?
D dir moi(D) = 120
With parameter T
up
= 3 and setting the
distribution-based multiplier E to 1 to simplify
calculations (or for example, if the features re-
ceived the same distributions of evaluations), this
tree yields the information coverage cost scores
found in Table 1i. Running p-median on these val-
ues produces the optimal results found in Table 1ii.
This method trades off selecting centrally located
nodes near the root of the UDF tree and the im-
portance of the individual nodes. In this example,
D is selected after the root node A even though D
has a greater importance value.
4 Comparative Evaluation
4.1 Stochastic Data Generation
In our experiments we wanted to compare the two
content selection strategies (heuristic vs. p-median
optimization) on datasets that were both realistic
and diverse. Despite the widespread adoption of
user reviews in online websites, there is to our
knowledge no publicly available corpus of cus-
tomer reviews of sufficient size which is annotated
with features arranged in a hierarchy. While small-
scale corpora do exist for a small number of prod-
ucts, the size of the corpora is too small to be rep-
resentative of all possible distributions of evalu-
ations and feature hierarchies of products, which
limits our ability to draw any meaningful conclu-
sion from the dataset.
2
Thus, we stochastically
2
Using a constructed dataset based on real data where no
resources or agreed-upon evaluation methodology yet exists
has been done in other NLP tasks such as topic boundary de-
tection (Reynar, 1994) and local coherence modelling (Barzi-
lay and Lapata, 2005). We are encouraged, however, that sub-
sequent to our experiment, more resources for opinion anal-
11
mean std.
# Features 55.3889 8.5547
# Evaluated Features 21.6667 5.9722
# Children (depth 0) 11.3056 0.7753
# Children (depth 1 fertile) 5.5495 1.7724
Table 2: Statistics on the 36 generated data sets.
At depth 1, 134 of the 407 features in total across
the trees were barren. The generated tree hierar-
chies were quite flat, with a maximum depth of 2.
generated the data for the products to mimic real
product feature hierarchies and evaluations. We
did this by gathering statistics from existing cor-
pora of customer reviews about electronics prod-
ucts (Hu and Liu, 2004), which contain UDF hier-
archies and evaluations that have been defined and
annotated. Using these statistics, we created dis-
tributions over the characteristics of the data, such
as the number of nodes in a UDF hierarchy, and
sampled from these distributions to generate new
UDF hierarchies and evaluations. In total, we gen-
erated 36 sets of data, which covered a realistic set
of possible scenarios in term of feature hierarchy
structures as well as in term of distribution of eval-
uations for each feature. Table 2 presents some
statistics on the generated data sets.
4.2 Building a Human Performance Model
We adopt the evaluation approach that a good con-
tent selection strategy should perform similarly to
humans, which is the view taken by existing sum-
marization evaluation schemes such as ROUGE
(Lin, 2004) and the Pyramid method (Nenkova et
al., 2007). For evaluating our content selection
strategy, we conducted a user study asking human
participants to perform a selection task to create
?gold standard? selections. Participants viewed
and selected UDF features using a Treemap infor-
mation visualization. See Figure 2 for an example.
We recruited 25 university students or gradu-
ates, who were each presented with 19 to 20 of
the cases we generated as described above. Each
case represented a different hypothetical product,
which was represented by a UDF hierarchy, as
well as P/S evaluations from -3 to +3. These were
displayed to the participants by a Treemap visual-
ization (Shneiderman, 1992), which is able to give
an overview of the feature hierarchy and the eval-
uations that each feature received. Treemaps have
been shown to be a generally successful tool for
ysis such as a user review corpus by Constant et al (2008)
have been released, as an anonymous reviewer pointed out.
visualizing data in the customer review domain,
even for novice users (Carenini et al, 2006). In
a Treemap, the feature hierarchy is represented by
nested rectangles, with parent features being larger
rectangles, and children features being smaller
rectangles contained within its parent rectangle.
The size of the rectangles depends on the number
of evaluations that this feature received directly,
as well as indirectly through its children features.
Each evaluation is also shown as a small rectangle,
coloured according to its P/S rating, with -3 being
bright red, and +3 being bright green.
Participants received 30 minutes of interactive
training in using Treemaps, and were presented
with a scenario in which they were told to take the
role of a friend giving advice on the purchase of
an electronics product based on existing customer
reviews. They were then shown 22 to 23 scenar-
ios corresponding to different products and eval-
uations, and asked to select features which they
think would be important to include in a summary
to send to a friend. We discarded the first three
selections that participants made to allow them to
become further accustomed to the visualization.
The number of features that participants were
asked to select from each tree was 18% of the
number of selectable features. A feature is con-
sidered selectable if it appears in the Treemap vi-
sualization; that is, the feature receives at least
one evaluation, or one of its descendant features
does. This proportion was the average propor-
tion at which the selections made by the heuristic
greedy strategy and p-median diverged the most
when we were initially testing the algorithms. Be-
cause each tree contained a different number of
features, the actual number of features selected
ranged from two to seven. Features were given
generic labels like Feature 34, so that participants
cannot rely on preexisting knowledge about that
Figure 2: A sample Treemap visualization of the
customer review data sets shown to participants.
12
Selection method Cohen?s Kappa
heuristic, squared moi 0.4839
heuristic, abs moi 0.4841
p-median, squared moi 0.4679
p-median, abs moi 0.4821
Table 3: Cohen?s kappa for heuristic greedy and
p-median methods against human selections. Two
versions of the measure of importance were tested,
one using squared P/S scores, the other using ab-
solute values.
kind of product in their selections.
4.3 Evaluation Metrics
Using this human gold standard, we can now com-
pare the greedy heuristic and the p-median strate-
gies. We report the agreement between the hu-
man and machine selections in terms of kappa
and a version of the Pyramid method. The Pyra-
mid method is a summarization evaluation scheme
built upon the observation that human summaries
can be equally informative despite being divergent
in content (Nenkova et al, 2007). In the Pyramid
method, Summary Content Units (SCUs) in a set
of human-written model summaries are manually
identified and annotated. These SCUs are placed
into a pyramid with different tiers, corresponding
to the number of model (i.e. human) summaries
in which each SCU appears. A summary to be
evaluated is similarly annotated by SCUs and is
scored by the scores of its SCUs, which are the
tier of the pyramid in which the SCU appears. The
Pyramid score is defined as the sum of the weights
of the SCUs in the evaluated summary divided by
the maximum score achievable with this number
of SCUs, if we were to take SCUs starting from
the highest tier of the pyramid. Thus, a summary
scores highly if its SCUs are found in many of
the model summaries. We use UDFs rather than
text passages as SCUs, since UDFs are the ba-
sic units of content in our selections. Moderate
inter-annotator agreement between human feature
selections shows that our data fits the assumption
of the Pyramid method (i.e. diversity of human an-
notations); the Fleiss? kappa (1971) scores for the
human selections ranged from 0.2984 to 0.6151,
with a mean of 0.4456 among all 33 sets which
were evaluated. A kappa value above 0.6 is gener-
ally taken to indicate substantial agreement (Lan-
dis and Koch, 1977).
Figure 3: Pyramid scores for the two selection ap-
proaches at different numbers of features i. using
the squared importance measure, ii. using the ab-
solute value importance measure.
4.4 Results
The greedy heuristic method and p-median per-
form similarly at the number of features that the
human participants were asked to select. The dif-
ference is not statistically significant by a two-
tailed t-test. Table 3 shows that using absolute
values of P/S scores in the importance measure
is better than using squares. Squaring seems to
give too much weight to extreme evaluations over
more neutral evaluations. P-median is particu-
larly affected, which is not surprising as it uses the
measure of importance both in the raw importance
score and in the distribution-based multiplier.
The Pyramid method allows us to compare the
algorithms at different numbers of features. Fig-
ure 3 shows the average pyramid score for the
two methods over the proportion of features that
are selected. Overall, both algorithms perform
well, and reach a score of about 0.9 at 10% of
features selected. The heuristic method performs
slightly better when the proportion is below 25%,
but slightly worse above that proportion.
We consider several possible explanations for
the surprising result that the heuristic greedy
method and p-median methods perform similarly.
One possibility is that the approximate p-median
solution we adopted (POPSTAR) is error-prone on
this task, but this is unlikely as the approximate
method has been rigorously tested both externally
on much larger problems and internally on a sub-
set of our data. Another possibility is that the au-
tomatic methods have reached a ceiling in perfor-
mance by these evaluation metrics.
Nevertheless, these results are encouraging in
showing that our optimization-based method is a
viable alternative to a heuristic strategy for con-
tent selection, and validate that incorporating other
13
summarization decisions into content selection is
an option worth exploring.
5 Conclusions and Future Work
We have proposed a formal optimization-based
method for summarization content selection based
on the p-median clustering paradigm, in which
content selection is viewed as selecting clusters
of related information. We applied the frame-
work to opinion summarization of customer re-
views. An experiment evaluating our p-median
algorithm found that it performed about as well
as a comparable existing heuristic approach de-
signed for the opinion domain in terms of similar-
ity to human selections. These results suggest that
the optimization-based approach is a good starting
point for integration with other parts of the sum-
marization/NLG process, which is a promising av-
enue of research.
6 Acknowledgements
We would like to thank Lucas Rizoli, Gabriel Mur-
ray and the anonymous reviewers for their com-
ments and suggestions.
References
R. Barzilay and M. Lapata. 2005. Modeling Local Co-
herence: An Entity-based Approach. In Proc. 43rd
ACL, pages 141?148.
G. Carenini and J.C.K. Cheung. 2008. Extractive vs.
NLG-based abstractive summarization of evaluative
text: The effect of corpus controversiality. In Proc.
5th INLG.
G. Carenini and J.D. Moore. 2006. Generating and
evaluating evaluative arguments. Artificial Intelli-
gence, 170(11):925?952.
G. Carenini, R.T. Ng, and A. Pauls. 2006. Interac-
tive multimedia summaries of evaluative text. In
Proc. 11th Conference on Intelligent User Inter-
faces, pages 124?131.
N. Constant, C. Davis, C. Potts, and F. Schwarz.
2008. The pragmatics of expressive content: Evi-
dence from large corpora. Sprache und Datenverar-
beitung.
C. Dellarocas, N. Awad, and X. Zhang. 2004. Explor-
ing the Value of Online Reviews to Organizations:
Implications for Revenue Forecasting and Planning.
In Proc. 24th International Conference on Informa-
tion Systems.
C. Fellbaum et al 1998. WordNet: an electronic lexi-
cal database. Cambridge, Mass: MIT Press.
J.L. Fleiss et al 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free
text. Lecture Notes in Computer Science, 3646:121?
132.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proc. 2004 ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 168?177. ACM Press New
York, NY, USA.
O. Kariv and S.L. Hakimi. 1979. An algorithmic
approach to network location problems. II: the p-
medians. SIAM Journal on Applied Mathematics,
37(3):539?560.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
C.Y. Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. Workshop on
Text Summarization Branches Out, pages 74?81.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The Pyramid Method: Incorporating human con-
tent selection variation in summarization evaluation.
ACM Transactions on Speech and Language Pro-
cessing (TSLP), 4(2).
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: an architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7(03):225?250.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. 42nd ACL, pages
271?278.
M.G.C. Resende and R.F. Werneck. 2004. A Hy-
brid Heuristic for the p-Median Problem. Journal
of Heuristics, 10(1):59?88.
J.C. Reynar. 1994. An automatic method of finding
topic boundaries. In Proc. 32nd ACL, pages 331?
333.
B. Shneiderman. 1992. Tree visualization with tree-
maps: 2-d space-filling approach. ACM Transac-
tions on Graphics (TOG), 11(1):92?99.
E.R. Tufte, S.R. McKay, W. Christian, and J.R Matey.
1998. Visual Explanations: Images and Quanti-
ties, Evidence and Narrative. Computers in Physics,
12(2):146?148.
M.X. Zhou and V. Aggarwal. 2004. An optimization-
based approach to dynamic data content selection
in intelligent multimedia interfaces. In Proc. 17th
annual ACM symposium on User interface software
and technology, pages 227?236. ACM Press New
York, NY, USA.
14
An Empirical Study of the Influence of Argument Conciseness on 
Argument Effectiveness 
Giuseppe Carenini  
Intelligent Systems Program 
University of Pittsburgh,  
Pittsburgh, PA 15260, USA 
carenini@cs.pitt.edu 
Johanna D. Moore 
The Human Communication Research Centre, 
University of Edinburgh, 
2 Buccleuch Place, Edinburgh EH8 9LW, UK.  
jmoore@cogsci.ed.ac.uk 
  
Abstract  
We have developed a system that generates 
evaluative arguments that are tailored to the 
user, properly arranged and concise. We have 
also developed an evaluation framework in 
which the effectiveness of evaluative arguments 
can be measured with real users. This paper 
presents the results of a formal experiment we 
have performed in our framework to verify the 
influence of argument conciseness on argument 
effectiveness 
1 Introduction 
Empirical methods are critical to gauge the 
scalability and robustness of proposed 
approaches, to assess progress and to stimulate 
new research questions. In the field of natural 
language generation, empirical evaluation has 
only recently become a top research priority 
(Dale, Eugenio et al 1998). Some empirical 
work has been done to evaluate models for 
generating descriptions of objects and processes 
from a knowledge base (Lester and Porter March 
1997), text summaries of quantitative data 
(Robin and McKeown 1996), descriptions of 
plans (Young to appear) and concise causal 
arguments (McConachy, Korb et al 1998). 
However, little attention has been paid to the 
evaluation of systems generating evaluative 
arguments, communicative acts that attempt to 
affect the addressee?s attitudes (i.e. evaluative 
tendencies typically phrased in terms of like and 
dislike or favor and disfavor). 
The ability to generate evaluative arguments is 
critical in an increasing number of online 
systems that serve as personal assistants, 
advisors, or shopping assistants1. For instance, a 
shopping assistant may need to compare two 
similar products and argue why its current user 
should like one more than the other. 
                                                   
1
 See for instance www.activebuyersguide.com 
In the remainder of the paper, we first describe a 
computational framework for generating 
evaluative arguments at different levels of 
conciseness. Then, we present an evaluation 
framework in which the effectiveness of 
evaluative arguments can be measured with real 
users. Next, we describe the design of an 
experiment we ran within the framework to 
verify the influence of argument conciseness on 
argument effectiveness. We conclude with a 
discussion of the experiment?s results. 
2 Generating concise evaluative 
arguments 
Often an argument cannot mention all the 
available evidence, usually for the sake of 
brevity. According to argumentation theory, the 
selection of what evidence to mention in an 
argument should be based on a measure of the 
evidence strength of support (or opposition) to 
the main claim of the argument (Mayberry and 
Golden 1996). Furthermore, argumentation 
theory suggests that for evaluative arguments the 
measure of evidence strength should be based on 
a model of the intended reader?s values and 
preferences. 
Following argumentation theory, we have 
designed an argumentative strategy for 
generating evaluative arguments that are 
properly arranged and concise (Carenini and 
Moore 2000).  In our strategy, we assume that 
the reader?s values and preferences are 
represented as an additive multiattribute value 
function (AMVF), a conceptualization based on 
multiattribute utility theory (MAUT)(Clemen 
1996). This allows us to adopt and extend a 
measure of evidence strength proposed in 
previous work on explaining decision theoretic 
advice based on an AMVF (Klein1994).
Figure 1 Sample additive multiattribute value function (AMVF) 
The argumentation strategy has been 
implemented as part of a complete argument 
generator. Other modules of the generator 
include a microplanner, which performs 
aggregation, pronominalization and makes 
decisions about cue phrases and scalar 
adjectives, along with a sentence realizer, which 
extends previous work on realizing evaluative 
statements (Elhadad 1995). 
2.1 Background on AMVF 
An AMVF is a model of a person?s values and 
preferences with respect to entities in a certain 
class. It comprises a value tree and a set of 
component value functions, one for each 
primitive attribute of the entity. A value tree is a 
decomposition of the value of an entity into a 
hierarchy of aspects of the entity2, in which the 
leaves correspond to the entity primitive 
attributes (see Figure 1 for a simple value tree in 
the real estate domain). The arcs of the tree are 
weighted to represent the importance of the 
value of an objective in contributing to the value 
of its parent in the tree (e.g., in Figure 1 location 
is more than twice as important as size in 
determining the value of a house). Note that the 
sum of the weights at each level is equal to 1. A 
component value function for an attribute 
expresses the preferability of each attribute 
value as a number in the [0,1] interval. For 
instance, in Figure 1 neighborhood n2 has 
preferability 0.3, and a distance-from-park of 1 
mile has preferability (1 - (1/5 * 1))=0.8). 
                                                   
2
 In decision theory these aspects are called 
objectives. For consistency with previous work, we 
will follow this terminology in the remainder of the 
paper. 
Formally, an AMVF predicts the value )(ev of 
an entity e as follows: 
v(e) = v(x1,?,xn) = ?wi vi(xi), where 
- (x1,?,xn) is the vector of attribute values for an 
entity e 
- ?attribute i, vi is the component value function, 
which maps the least preferable xi to 0, the most 
preferable to 1, and the other xi to values in [0,1] 
- wi is the weight for attribute i, with 0? wi ?1 
and ?wi =1 
- wi is equal to the product of all the weights 
from the root of the value tree to the attribute i 
 
A function vo(e) can also be defined for each 
objective. When applied to an entity, this 
function returns the value of the entity with 
respect to that objective. For instance, assuming 
the value tree shown in Figure 1, we have: 
  
))(6.0())(4.0(
)(
evev
ev
parkfromDistodNeighborho
Location
??
?+?=
=
 
Thus, given someone?s AMVF, it is possible to 
compute how valuable an entity is to that 
individual. Furthermore, it is possible to 
compute how valuable any objective (i.e., any 
aspect of that entity) is for that person. All of 
these values are expressed as a number in the 
interval [0,1]. 
2.2 A measure of evidence strength 
Given an AMVF for a user applied to an entity 
(e.g., a house), it is possible to define a precise 
measure of an objective strength in determining 
the evaluation of its parent objective for that 
entity. This measure is proportional to two 
factors: (A) the weight of the objective 
? +???
k =1
k = -1
k = 0
compellingness
Figure 2 Sample population of objectives 
represented by dots and ordered by their 
compellingness 
 
(which is by itself a measure of importance), (B) 
a factor that increases equally for high and low 
values of the objective, because an objective can 
be important either because it is liked a lot or 
because it is disliked a lot. We call this measure 
s-compellingness and provide the following 
definition: 
s-compellingness(o, e, refo) = (A)? (B) = 
      = w(o,refo)? max[[vo(e)]; [1 ? vo(e)]],  where 
? o is an objective, e is an entity, refo is an 
ancestor of o in the value tree 
? w(o,refo) is the product of the weights of all 
the links from o to refo 
? vo is the component value function for leaf 
objectives (i.e., attributes), and it is the 
recursive evaluation over children(o) for 
nonleaf objectives 
Given a measure of an objective's strength, a 
predicate indicating whether an objective should 
be included in an argument (i.e., worth 
mentioning) can be defined as follows:  
s-notably-compelling?(o,opop,e, refo) ? 
?s-compellingness(o, e, refo)?>?x+k?x , where 
? o, e, and refo are defined as in the previous 
Def; opop is an objective population (e.g., 
siblings(o)), and ?opop?>2 
? p? opop; x?X = ?s-compellingness(p, e, 
refo)? 
? ?x is the mean of X, ?x  is the standard 
deviation and k is a user-defined constant 
Similar measures for the comparison of two 
entities are defined and extensively discussed in 
(Klein 1994). 
2.3 The constant k 
In the definition of s-notably-compelling?, the 
constant k determines the lower bound of s-
compellingness for an objective to be included 
in an argument. As shown in Figure 2, for k=0 
only objectives with s-compellingness greater  
Figure 3 Arguments about the same house, 
tailored to the same subject but with k ranging 
from 1 to ?1 
 
than the average s-compellingness in a 
population are included in the argument (4 in the 
sample population). For higher positive values 
of k less objectives are included (only 2, when 
k=1), and the opposite happens for negative 
values (8 objectives are included, when k=-1). 
Therefore, by setting the constant k to different 
values, it is possible to control in a principled 
way how many objectives (i.e., pieces of 
evidence) are included in an argument, thus 
controlling the degree of conciseness of the 
generated arguments. 
Figure 3 clearly illustrates this point by showing 
seven arguments generated by our argument 
generator in the real-estate domain. These 
arguments are about the same house, tailored to 
the same subject, for k ranging from 1 to ?1. 
3 The evaluation framework 
In order to evaluate different aspects of the 
argument generator, we have developed an 
evaluation framework based on the task efficacy 
evaluation method. This method allows 
Figure 4 The evaluation framework architecture
the experimenter to evaluate a generation model 
by measuring the effects of its output on user?s 
behaviors, beliefs and attitudes in the context of 
a task. 
Aiming at general results, we chose a rather 
basic and frequent task that has been extensively 
studied in decision analysis: the selection of a 
subset of preferred objects (e.g., houses) out of a 
set of possible alternatives. In the evaluation 
framework that we have developed, the user 
performs this task by using a computer 
environment (shown in Figure 5) that supports 
interactive data exploration and analysis (IDEA) 
(Roth, Chuah et al 1997). The IDEA 
environment provides the user with a set of 
powerful visualization and direct manipulation 
techniques that facilitate the user?s autonomous 
exploration of the set of alternatives and the 
selection of the preferred alternatives. 
Let?s examine now how an argument generator 
can be evaluated in the context of the selection 
task, by going through the architecture of the 
evaluation framework. 
3.1 The evaluation framework architecture 
Figure 4 shows the architecture of the evaluation 
framework. The framework consists of three 
main sub-systems: the IDEA system, a User 
Model Refiner and the Argument Generator. The 
framework assumes that a model of the user?s 
preferences (an AMVF) has been previously 
acquired from the user, to assure a reliable initial 
model. 
At the onset, the user is assigned the task to 
select from the dataset the four most preferred 
alternatives and to place them in a Hot List (see 
Figure 5, upper right corner) ordered by 
preference. The IDEA system supports the user 
in this task (Figure 4 (1)). As the interaction 
unfolds, all user actions are monitored and 
collected in the User?s Action History (Figure 4 
(2a)). Whenever the user feels that the task is 
accomplished, the ordered list of preferred 
alternatives is saved as her Preliminary Decision 
(Figure 4 (2b)). After that, this list, the User?s 
Action History and the initial Model of User?s 
Preferences are analysed by the User Model 
Refiner (Figure 4 (3)) to produce a Refined 
Model of the User?s Preferences (Figure 4 (4)).  
At this point, the stage is set for argument 
generation. Given the Refined Model of the 
User?s Preferences, the Argument Generator 
produces an evaluative argument tailored to the 
model (Figure 4 (5-6)), which is presented to the 
user by the IDEA system (Figure 4 (7)).The 
argument goal is to introduce a new alternative 
(not included in the dataset initially presented to 
the user) and to persuade the user that the 
alternative is worth being considered. The new 
alternative is designed on the fly to be preferable 
for the user given her preference model.  
 3-26
HotList
NewHouse 3-26
Figure 5 The IDEA environment display at the end of the interaction
All the information about the new alternative is 
also presented graphically. Once the argument is 
presented, the user may (a) decide immediately 
to introduce the new alternative in her Hot List, 
or (b) decide to further explore the dataset, 
possibly making changes to the Hot List adding 
the new instance to the Hot List, or (c) do 
nothing. Figure 5 shows the display at the end of 
the interaction, when the user, after reading the 
argument, has decided to introduce the new 
alternative in the Hot List first position (Figure 
5, top right). 
Whenever the user decides to stop exploring and 
is satisfied with her final selections, measures 
related to argument?s effectiveness can be 
assessed (Figure 4  (8)). These measures are 
obtained either from the record of the user 
interaction with the system or from user self-
reports in a final questionnaire (see Figure 6 for 
an example of self-report) and include: 
- Measures of behavioral intentions and attitude 
change: (a) whether or not the user adopts the 
new proposed alternative, (b) in which position 
in the Hot List she places it and (c) how much 
she likes the new alternative and the other 
objects in the Hot List.  
- A measure of the user?s confidence that she has 
selected the best for her in the set of alternatives.  
- A measure of argument effectiveness derived 
by explicitly questioning the user at the end of 
the interaction about the rationale for her 
decision (Olso and Zanna 1991). This can 
provide valuable information on what aspects of 
the argument were more influential (i.e., better 
understood and accepted by the user). 
- An additional measure of argument 
effectiveness is to explicitly ask the user at the 
end of the interaction to judge the argument with 
respect to several dimensions of quality, such as 
content, organization, writing style and 
convincigness. However, evaluations based on 
Figure 6 Self -report on user?s satisfaction with 
houses in the HotList 
Figure 7 Hypotheses on experiment outcomes 
judgements along these dimensions are clearly 
weaker than evaluations measuring actual 
behavioural and attitudinal changes (Olso and 
Zanna 1991).  
To summarize, the evaluation framework just 
described supports users in performing a 
realistic task at their own pace by interacting 
with an IDEA system. In the context of this task, 
an evaluative argument is generated and 
measurements related to its effectiveness can be 
performed. 
We now discuss an experiment that we have 
performed within the evaluation framework 
4 The Experiment 
The argument generator has been designed to 
facilitate testing the effectiveness of different 
aspects of the generation process.  The 
experimenter can easily control whether the 
generator tailors the argument to the current 
user, the degree of conciseness of the argument 
(by varying k as explained in Section 2.3), and 
what microplanning tasks the generator 
performs. In the experiment described here, we 
focused on studying the influence of argument 
conciseness on argument effectiveness. A 
parallel experiment about the influence of 
tailoring is described elsewhere. 
We followed a between-subjects design with 
three experimental conditions: 
No-Argument - subjects are simply informed that 
a new house came on the market.  
Tailored-Concise - subjects are presented with 
an evaluation of the new house tailored to their 
preferences and at a level of conciseness that we 
hypothesize to be optimal. To start our 
investigation, we assume that an effective 
argument (in our domain) should contain 
slightly more than half of the available evidence. 
By running the generator with different values 
for k on the user models of the pilot subjects, we 
found that this corresponds to k=-0.3. In fact, 
with k=-0.3 the arguments contained on average 
10 pieces of evidence out of the 19 available. 
Tailored-Verbose - subjects are presented with 
an evaluation of the new house tailored to their 
preferences, but at a level of conciseness that we 
hypothesize to be too low (k=-1, which 
corresponds on average, in our analysis of the 
pilot subjects, to 16 pieces of evidence out of the 
possible 19).  
In the three conditions, all the information about 
the new house is also presented graphically, so 
that no information is hidden from the subject. 
Our hypotheses on the outcomes of the 
experiment are summarized in Figure 7. We 
expect arguments generated for the Tailored-
Concise condition to be more effective than 
arguments generated for the Tailored-Verbose 
condition. We also expect the Tailored-Concise 
condition to be somewhat better than the No-
Argument condition, but to a lesser extent, 
because subjects, in the absence of any 
argument, may spend more time further 
exploring the dataset, thus reaching a more 
informed and balanced decision. Finally, we do 
not have strong hypotheses on comparisons of 
argument effectiveness between the No-
Argument and Tailored-Verbose conditions. 
The experiment is organized in two phases. In 
the first phase, the subject fills out a 
questionnaire on the Web. The questionnaire 
implements a method form decision theory to 
acquire an AMVF model of the subject?s 
preferences (Edwards and Barron 1994). In the 
second phase of the experiment, to control for 
possible confounding variables (including 
subject?s argumentativeness (Infante and Rancer 
1982), need for cognition (Cacioppo, Petty et al 
1983), intelligence and self-esteem), the subject 
Tailored
Concise
Tailored
Verbose
No-Argument
>
>> ?
 
a) How would you judge the houses in your Hot List? 
The more you like the house the closer you should  
        put a cross to ?good choice? 
 1st house
 
bad choice  : __:__:__:__ :__:__:__:__:__: good choice 
2nd house
 
bad choice  : __:__:__:__ :__:__:__:__:__: good choice 
3rd house 
 
bad choice  : __:__:__:__ :__:__:__:__:__: good choice 
4th house
 
bad choice  : __:__:__:__ :__:__:__:__:__: good choice 
 
 
Figure 8 Sample filled-out self-report on user?s 
satisfaction with houses in the Hot List3 
is randomly assigned to one of the three 
conditions. 
Then, the subject interacts with the evaluation 
framework and at the end of the interaction 
measures of the argument effectiveness are 
collected, as described in Section 3.1.  
After running the experiment with 8 pilot 
subjects to refine and improve the experimental 
procedure, we ran a formal experiment involving 
30 subjects, 10 in each experimental condition. 
5 Experiment Results 
5.1 A precise measure of satisfaction 
According to literature on persuasion, the most 
important measures of arguments effectiveness 
are the ones of behavioral intentions and attitude 
change. As explained in Section 3.1, in our 
framework such measures include (a) whether or 
not the user adopts the new proposed alternative, 
(b) in which position in the Hot List she places 
it, (c) how much she likes the proposed new 
alternative and  the other objects in the Hot List. 
Measures (a) and (b) are obtained from the 
record of the user interaction with the system, 
whereas measures in (c) are obtained from user 
self-reports. 
A closer analysis of the above measures 
indicates that the measures in (c) are simply a 
more precise version of measures (a) and (b). In 
fact, not only they assess the same information 
as measures (a) and (b), namely a preference 
ranking among the new alternative and the 
objects in the Hot List, but they also offer two 
additional critical advantages: 
                                                   
3
 If the subject does not adopt the new house, she is 
asked to express her satisfaction with the new house 
in an additional self-report.  
(i) Self-reports allow a subject to express 
differences in satisfaction more precisely than 
by ranking. For instance, in the self-report 
shown in Figure 8, the subject was able to 
specify that the first house in the Hot List was 
only one space (unit of satisfaction) better then 
the house preceding it in the ranking, while the 
third house was two spaces better than the house 
preceding it.  
(ii) Self-reports do not force subjects to express 
a total order between the houses. For instance, in 
Figure 8 the subject was allowed to express that 
the second and the third house in the Hot List 
were equally good for her. 
Furthermore, measures of satisfaction obtained 
through self-reports can be combined in a single, 
statistically sound measure that concisely 
express how much the subject liked the new 
house with respect to the other houses in the Hot 
List. This measure is the z-score of the subject?s 
self-reported satisfaction with the new house, 
with respect to the self-reported satisfaction with 
the houses in the Hot List. A z-score is a 
normalized distance in standard deviation units 
of a measure xi from the mean of a population X. 
Formally:  
xi? X; z-score( xi ,X) = [xi - ? (X)] / ?(X) 
For instance, the satisfaction z-score for the new 
instance, given the sample self-reports shown in 
Figure 8, would be:  
[7 - ? ({8,7,7,5})] /  ?({8,7,7,5}) = 0.2 
The satisfaction z-score precisely and concisely 
integrates all the measures of behavioral 
intentions and attitude change. We have used 
satisfaction z-scores as our primary measure of 
argument effectiveness. 
5.2 Results 
As shown in Figure 9, the satisfaction z-scores 
obtained in the experiment confirmed our 
hypotheses. Arguments generated for the 
Tailored-Concise condition were significantly 
more effective than arguments generated for 
Tailored-Verbose condition. The Tailored-
Concise condition was also significantly better 
than the No-Argument condition, but to a lesser 
extent. Logs of the interactions suggest that this 
happened because subjects in the No-Argument 
condition spent significantly more time further 
exploring the dataset. Finally, there was no 
significant difference in argument effectiveness  
a)
    How would you judge the houses in your Hot List? 
The more you like the house the closer you should  
        put a cross to ?good choice? 
 1st house
 
bad choice  : __:__:__:__ :__:__:__:X :__: good choice 
2nd house(New house)
 
bad choice  : __:__:__:__ :__:__:X :__:__: good choice 
3rd house 
 
bad choice  : __:__:__:__ :__:__:X :__:__: good choice 
4th house
 
bad choice  : __:__:__:__ :X :__:__:__:__: good choice 
 
 
 Figure 9
 
Results for satisfaction z-scores. The 
average z-scores for the three conditions are 
shown in the grey boxes and the p-values are 
reported beside the links 
 
between the No-Argument and Tailored-
Verbose conditions. 
With respect to the other measures of argument 
effectiveness mentioned in Section 3.1, we have 
not found any significant differences among the 
experimental conditions. 
6 Conclusions and Future Work 
Argumentation theory indicates that effective 
arguments should be concise, presenting only 
pertinent and cogent information. However, 
argumentation theory does not tell us what is the 
most effective degree of conciseness. As a 
preliminary attempt to answer this question for 
evaluative arguments, we have compared in a 
formal experiment the effectiveness of 
arguments generated by our argument generator 
at two different levels of conciseness. The 
experiment results show that arguments 
generated at the more concise level are 
significantly better than arguments generated at 
the more verbose level. However, further 
experiments are needed to determine what is the 
optimal level of conciseness.  
Acknowledgements 
Our thanks go to the members of the Autobrief 
project: S. Roth, N. Green, S. Kerpedjiev and J. 
Mattis. We also thank C. Conati for comments 
on drafts of this paper. This work was supported 
by grant number DAA-1593K0005 from the 
Advanced Research Projects Agency (ARPA).  
 
References 
Cacioppo, J. T., R. E. Petty, et al (1983). ?Effects of 
Need for Cognition on Message Evaluation, Recall, 
and Persuasion.? Journal of Personality and Social 
Psychology 45(4): 805-818. 
Carenini, G. and J. Moore (2000). A Strategy for 
Generating Evaluative Arguments. International 
Conference on Natural Language Generation, 
Mitzpe Ramon, Israel. 
Clemen, R. T. (1996). Making Hard Decisions: an 
introduction to decision analysis. Belmont, 
California, Duxbury Press. 
Dale, R., B. d. Eugenio, et al (1998). ?Introduction to 
the Special Issue on Natural Language 
Generation.? Computational Linguistics 24(3): 
345-353. 
Edwards, W. and F. H. Barron (1994). ?SMARTS 
and SMARTER: Improved Simple Methods for 
Multi-attribute Utility Measurements.? 
Organizational Behavior and Human Decision 
Processes 60: 306-325. 
Elhadad, M. (1995). ?Using argumentation in text 
generation.? Journal of Pragmatics 24: 189-220. 
Infante, D. A. and A. S. Rancer (1982). ?A 
Conceptualization and Measure of 
Argumentativeness.? Journal of Personality 
Assessment 46: 72-80. 
Klein, D. (1994). Decision Analytic Intelligent 
Systems: Automated Explanation and Knowledge   
Acquisition, Lawrence Erlbaum Associates. 
Lester, J. C. and B. W. Porter (March 1997). 
?Developing and Empirically Evaluating Robust 
Explanation Generators: The KNIGHT 
Experiments.? Computational Linguistics 23(1): 
65-101. 
Mayberry, K. J. and R. E. Golden (1996). For 
Argument's Sake: A Guide to Writing Effective 
Arguments, Harper Collins, College Publisher. 
McConachy, R., K. B. Korb, et al (1998). Deciding 
What Not to Say: An Attentional-Probabilistic 
Approach to Argument Presentation. Cognitive 
Science Conference. 
Olso, J. M. and M. P. Zanna (1991). Attitudes and 
beliefs ; Attitude change and attitude-behavior 
consistency. Social Psychology. R. M. Baron and 
W. G. Graziano. 
Robin, J. and K. McKeown (1996). ?Empirically 
Designing and Evaluating a New Revision-Based 
Model for Summary Generation.? Artificial 
Intelligence journal 85: 135-179. 
Roth, S. F., M. C. Chuah, et al (1997). Towards an 
Information Visualization Workspace: Combining 
Multiple Means of   Expression. Human-Computer 
Interaction Journal. 
Young, M. R. ?Using Grice's Maxim of Quantity to 
Select the Content of Plan Descriptions.? Artificial 
Intelligence Journal, to appear. 
Tailored
Concise
Tailored
Verbose
No-Argument
>
?
 0.88
 0.05
 0.25
 0.02
>>
 0.03
 0.31
 
A Task-based Framework to Evaluate Evaluative Arguments 
Giuseppe Carenini 
Intelligent Systems Program 
University of Pittsburgh, Pittsburgh, PA 15260, USA 
carenini@cs.pitt.edu 
Abstract 
We present an evaluation framework in 
which the effectiveness of evaluative 
arguments can be measured with real users. 
The framework is based on the task-efficacy 
evaluation method. An evaluative argument 
is presented in the context of a decision task 
and measures related to its effectiveness are 
assessed. Within this framework, we are 
currently running a formal experiment o 
verify whether argument effectiveness can 
be increased by tailoring the argument to the 
user and by varying the degree of argument 
conciseness. 
Introduction 
Empirical methods are fundamental in any 
scientific endeavour to assess progress and to 
stimulate new research questions. As the field of 
NLG matures, we are witnessing a growing 
interest in studying empirical methods to 
evaluate computational models of discourse 
generation (Dale, Eugenio et al 1998). 
However, with the exception of (Chu-Carroll 
and Carberry 1998), little attention as been paid 
to the evaluation of systems generating 
evaluative arguments, communicative acts that 
attempt o affect the addressee's attitudes (i.e. 
evaluative tendencies typically phrased in terms 
of like and dislike or favor and disfavor). 
The ability to generate valuative arguments i
critical in an increasing number of online 
systems that serve as .personal :assistants, 
advisors, or sales asslstants ~.For instance, a 
travel assistant may need to compare two 
vacation packages and argue that its current user 
should like one more than the other. 
i See for instance www.activebuyersguide.com 
In this paper, we present an evaluation 
framework in which the effectiveness of 
evaluative arguments can be measured with real 
users. The measures of argument effectiveness 
used in our framework are based on principles 
developed in social psychology to study 
persuasion (Miller and Levine 1996). We are 
currently applying the framework to evaluate 
arguments generated by an argument generator 
we have developed (Carenini 2000). To facilitate 
the evaluation of specific aspects of the 
generation process, the argument generator has 
been designed so that its functional components 
can be easily turned-offor changed. 
In the remainder of the paper, we first describe 
our argument generator. Then, we summarize 
literature on persuasion from social psychology. 
Next, we discuss previous work on evaluating 
NLG models. Finally, we describe our 
evaluation framework and the design of an 
experiment we are currently running. 
1 The Argument Generator 
The architecture of the argument generator is a 
typical pipelined architecture comprising a 
discourse planner, a microplanner and a sentence 
real izer. 
The input to the planning process is an abstract 
evaluative communicative action expressing: 
- The subject of the evaluation, which can be 
an entity or a comparison between two 
entities in the~.domain of interest (e.g., a 
house or a comparison between two houses 
in the real-estate domain). 
An evaluation, which is a number in the 
interval \[0,1\] where, depending on the 
subject, 0 means "terrible" or "much worse" 
and 1 means "excellent" or "much better"). 
Given an abstract communicative action, the 
discourse planner (Young and Moore 1994) 
selects and arranges the content of the argument 
3.17 Is an Interesting house. In fact. Ihe quality of house 3-17 Is 
,.louse 3-17 offers a beaull/tJI view. And also it looks wonderful. 
rrnore, house 3-17 has a convenient location in the Weslend 
orhood. Even Ihough It Is somewhat ar from lhe pan (1.8 miles). 
3-17 Is close Io work (1.7 miles). And also the Iraffic Is moderate on 
3.-17 iS on interesllng house. In fact. n has a reasonable location in ~i~.L 
~lle.~estend .nelglilbomoodz.~wenllaor, lghl~ouse :.3ot~'- ts',aomewhM,tarltom -~I: 
:~he park (1.8 miles) and far fi-om shops (4 mlles), tl Is close to work (1.7 miles) (~\[,- 
iland a rapid Iranspodatlon slop ( 1 miles). And also the traffic is moderate on 
', 3rd street. Fudhermore. amenities are attractive, nhas a very spacious 
i garden (2000 sqll.), a spac ous porch (250 sqll.) and a large deck (220 sqh.). 
IFIna,y. Ihe qualm of house 3-17 Is good. Allhough house 3-17 is In the ~ ;-i~ 
~?torian style. N offers a beaut~l v|ew. And also it loom wonderful ~I~i 
House 3- t 7 is an interesting house. House 3-17 has e reasonable location. 
House 3-17 Issomewhat far hom the park i l .8 mllesl. House 1-17 Isfar i~i~! 
~om shops (4. mUesl. Howae 3--17 Is close Io a rapid transportation stop (1 : 
mtles).House 3-17 is?loseloworktl.7ralles).Thelratllclsmoderateon 
3rd street. House 3- t 7 has a location In the Weslend neighborhood. House 
3`17 hes excellent amenities. House 3-17 has a very spaclous garden 
(2000 sqfl.). House 3-17 has a spacious porch (250 sqft.), House 3,-17 has 
a large deck (220 sqlt.), The quality of house 3-17 is good. House 3-.17 is in 
Ihe viclorian slyte. House 3- t 7 offers a beautiful view. House 3-17 looks ~i 
wondelflll. 
Figure 1 Sample 
by decomposing abstract communicative actions 
into primitive ones and by imposing appropriate 
ordering constraints among communicative 
actions. Two knowledge sources are involved in 
this process: 
- A complex model of the user's preferences 
based on multiattribute utilility theory 
(MAUT)(Clemen 1996). 
- A set of plan operators, implementing 
guidelines for content selection and 
organ(sat(on from argumentation theory 
(Carenini and Moore 2000). 
By using these two knowledge sources, the 
discourse planner produces a text plan for an 
argument whose content and organization are 
tailored to the user according to argumentation 
theory. 
Next, the text plan is passed tO the"microptanner 
which performs aggregation, pronominalization 
and makes decisions about cue phrases. 
Aggregation is performed according to heuristics 
similar to the ones proposed in (Shaw 1998). For 
pronominalization, simple rules based on 
centering are applied (Grosz. Josh( et al 1995). 
arguments in order of decreasing expected effectiveness for the target user SUB J-26 
Finally, decisions about cue phrases are made 
according to a decision tree based on 
suggestions from (Knott 1996; di Eugenio, 
Moore et el. 1997) . The sentence realizer 
extends previous work on realizing evaluative 
statements (Elhadad 1995). 
The argument generator has been designed to 
facilitate the testing of the effectiveness of 
different aspects of the generation process. The 
experimenter can easily vary the expected 
effectiveness of the generated arguments by 
controlling whether the generator tailors the 
argument o the current user, the degree of 
conciseness of the generated arguments and 
what microplanning tasks are performed. 
Figure 1 shows three arguments generated by the 
argument generator that clearly illustrate this 
feature. We-expect the first-argument to be very 
effective for the target user. Its content and 
organization has been tailored to her 
preferences. Also, the argument is reasonably 
fluent because of aggregation, pronominalization 
and cue phrases. In contrast, we expect the 
second argument to be less effective with our 
10 
target user, because it is not tailored to her 
preferences 2, and it appears to be somewhat too 
verbose 3.Finally, we expect he third arguments 
not to be effective at all. It suffers from all the 
shortcomings of the second argument, with the 
additional weakness of not being fluent (no 
microplannig tasks were performed). 
2 
A final note-on the evaluation of arguments. An ........ 
argument can also be evaluated by the argument 
addressee with respect o several dimensions of 
quality, such as coherence, content, 
organization, writing style and convincingness. 
However, evaluations based on judgements 
along these dimensions are clearly weaker than 
evaluations measuring actual attitudinal and 
Arguing an evaluation involves an intentional 
communicative act that attempts to affect the 
current or future behavior of the addressees by 
creating, changing or reinforcing the addressees' 
attitudes. It follows that the effectiveness of an 
evaluative argument can be tested by comparing 
measurements of subjects' attitudes or behavior 
before and after their exposure to the argument. 
In many experimental situations, however, 
measuring effects on overt behavior can be 
problematic (Miller and Levine 1996), therefore 
most research on persuasion has been based 
either on measurements of attitudes or on 
declaration of behavioral intentions. The most 
common technique to measure attitudes is 
subject self-report (Miller and Levine 1996). 
Typically, self-report measurements involve the 
use of a scale that consists of two "'bipolar" 
terms (e.g., good-choice vs. bad-choice), usually 
separated by seven or nine equal spaces that 
participants use to evaluate an attitude or belief 
statement (see Figure 4 for examples). 
Research in persuasion suggests that some 
individuals may be naturally more resistant o 
persuasion than others (Miller and Levine 1996). 
Individual features that seem to matter are: 
argumentativeness (tendency to argue)(lnfante 
and Rancer 1982), intelligence, self-esteem and 
need for cognition (tendency to engage in and to 
enjoy effortful cognitive endeavours)(Cacioppo, 
Petty et al 1983). Any experiment in persuasion 
should control for these variables. 
. Research  ....... An=,~:x~s~P~sychotogy., :~. :~-.on. ~, :.=~boh~viomLchanges:(Qlso;amt~anna 199 !-). ? 
Persuas ion  
3 Eva luat ion  o f  NLG Mode ls  
. . . . .  _2 This argument .was tailored"to" a .default average 
user, for whom all aspects of a house are equally 
important. With respect o the first argument, notice 
the different evaluation for the location and the 
different order between the two text segments about 
location and quality. 
3 A threshold controlling verbosity was set to its 
maximum value. 
Several empirical methods have been proposed 
and applied in the literature for evaluating NLG 
models. We discuss now why, among the three 
main evaluation methods (i.e., human judges, 
corpus-based and task efficacy), task efficacy 
appears to be the most appropriate for testing the 
effectiveness of evaluative arguments that are 
tailored to a complex model of the user's 
preferences. 
The human judges evaluation method requires a 
panel of judges to score outputs of generation 
models (Chu-Carroll and Carberry 1998; Lester 
and Porter March 1997). The main limitation of  
this approach is that the input of the generation 
process needs to be simple enough to be easily 
understood by judges 4. Unfortunately, this is not 
the case for our argument generator, where the 
input consists of a possibly complex and novel 
argument subject (e.g., a new house with a large 
number of features), and a complex model of the 
user's preferences. 
The corpus-based valuation method (Robin and 
McKeown 1996) can be applied only when a 
corpus of input/output pairs is available. A 
portion of the corpus (the training set) is used to 
develop a computational model of how the 
output can be generated from the input. The rest 
of the corpus (the testing set) is used to evaluate 
the model. Unfortunately, a corpus for our 
generator does. not exist. Furthermore, it would 
be difficult and extremely time-consuming to 
obtain and analyze such a corpus given the 
complexity of our. generator..input/output pairs. 
4 See (Chu-Carroll and Carberry 1998) tbr an 
illustration of how the specification of the context 
can become xtremely complex when human judges 
are used to evaluate content selection strategies for a 
dialog system. 
11 
? When a generator-is designed to generate output 
for users engaged in certain tasks, a natural way 
to evaluate its effectiveness is by experimenting 
with users performing those tasks. For instance, 
in (Young, to appear) different models for 
generating natural language descriptions of plans 
are evaluated by measuring how effectively 
users execute those plans given the descriptions. 
main sub-systems: the'IDEA system,- a Usei- ...... 
Model Refiner and the Argument Generator. The 
framework assumes that a model of the user's 
preferences based on MAUT has been 
previously acquired using traditional methods 
from decision theory (Edwards and Barron 
1994), to assure a reliable initial model. 
At the onset, the user is assigned the task to 
.This evaluatiort~ ,method,.,,~cal!ed~.:task.~.~effiea~y,,~ ~. , z,select~.?rara~,~\]~,:dataset..:.the:zfaur.most p~e?eazed ..... 
allows one to evaluate a generation model 
without explicitly evaluating its output but by 
measuring the output's effects on user's 
behaviors, beliefs and attitudes in the context of 
the task. The only requirement for this method is 
the specification of a sensible task. 
Task efficacy is the method we have adopted in 
our evaluation framework. 
4 The Evaluation Framework 
4.1 The task 
Aiming at general results, we chose a rather 
basic and frequent task that has been extensively 
studied in decision analysis: the selection of a 
subset of preferred objects (e.g., houses) out of a 
set of possible alternatives by considering trade- 
offs among multiple objectives (e.g., house 
location, house quality). The selection is 
performed by evaluating objects with respect to 
their values for a set of primitive attributes (e.g., 
house distance form the park, size of the 
garden). In the evaluation framework we have 
developed, the user performs this task by using a 
computer environment (shown in Figure 3) that 
supports interactive data exploration and 
analysis (IDEA) (Roth, Chuah et al 1997). The 
IDEA environment provides the user with a set 
of powerful visualization and direct 
manipulation techniques that facilitate user's 
autonomous exploration of the set of alternatives 
and the selection of the preferred alternatives. 
Let's examine now how the argument generator, 
that we described in Section 1, can be evaluated 
in the ,context of the ~selection =ta.sk, by gging. 
through the architecture of the evaluation 
framework. 
4.2 The f ramework  architecture 
alternatives and to place them in the Hot List 
(see Figure 3 upper right comer) ordered by 
preference. The IDEA system supports the user 
in this task (Figure 2 (I)). As the interaction 
unfolds, all user actions are monitored and 
collected in the User's Action History (Figure 2 
(2a)). Whenever the user feels that she has 
accomplished the task, the ordered list of 
preferred alternatives is saved as her Preliminary 
Decision (Figure 2 (2b)). After that, this list, the 
User's Action History and the initial Model of 
User's Preferences are analysed by the User 
Model Refiner (Figure 2 (3)) to produce a 
Refined Model of the User's Preferences (Figure 
2 (4)). 
At this point, the stage is set for argument 
generation. Given the Refined Model of the 
User's Preferences for the target selection task, 
the Argument Generator produces an evaluative 
argument tailored to the user's model (Figure 2 
(5-6)). Finally, the argument is presented to the 
user by the IDEA system (Figure 2 (7)). 
The argument goal is to introduce a new 
alternative (not included in the dataset initially 
presented to the user) and to persuade the user 
that the alternative is worth being considered. 
The new alternative is designed on the fly to be 
preferable for the user given her preference 
model. Once the argument is presented, the user 
may (a) decide to introduce the new alternative 
in her Hot List, or (b) decide to further explore 
the dataset, possibly making changes to the Hot 
List and introducing the new instance in the Hot 
List, or (c) do nothing. Figure 3 shows the 
display at the end of the interaction, when the 
user; after reading the argument, has decided to 
introduce the new alternative in the first 
position. 
Figure 2 shows the architecture of the evaluation 
framework. The framework consists of three 
12 
. . . . . . . . . . . . . . . . .  L .   ? . . . . . . . . . .  , . 
MeasmeJ of sxgmmmqs _ . efe?live~m arewxxxSnaS~ned / Da~e~ ~ 5  
\]. 3 4 ~  User, Pre~ere~ 
User's Actions ~ 
~ ~  Pre~ren~a 
- ~  J Cna,ed o~ MAW 
Sub-sy~em.~ Inpu~sK3utputs 
Figure 2 The evaluation f ramework  architecture 
Whenever the user decides to stop exploring and 
is satisfied and confident with her final 
selections, measures related to argument's 
effectiveness can be assessed (Figure 2 (8)). 
These measures are obtained either from the 
record of the user interaction with the system or 
from user self-reports (see Section 2). 
First, and most important, are measures of 
behavioral intentions and attitude change: (a) 
whether or not the user adopts the new proposed 
alternative, (b) in which position in the Hot List 
she places it, (c) how much she likes it, (d) 
whether or not the user revises the Hot List and 
(e) how much the user likes the objects in the 
Hot List. Second, a measure can be obtained of 
the user's confidence that she has selected the 
best for her in the set of alternatives. Third, a 
measure of argument effectiveness can also be 
derived by explicitly questioning the user at the 
end of the interaction, about the rationale for her 
decision. This can be done either by asking the 
user to justify her decision in a written 
.paragraph, or by asking,the user to :self-report 
for each attribute of the new house how 
important the attribute was in her decision (Olso 
and Zanna 1991). Both methods can provide 
valuable information on what aspects of the 
argument were more influential (i.e., better 
understood and accepted by the user). 
A fourth measure of argument effectiveness is to 
explicitly ask the user at the end of the 
interaction to judge the argument with respect to 
several dimensions of quality, such as content, 
organization, writing style and convincigness. 
Evaluations based on judgments along these 
dimensions are clearly weaker than evaluations 
measuring actual behavioural and attitudinal 
changes (Olso and Zanna 1991). However, these 
judgments may provide more information than 
judgments from independent judges (as in the 
"human judges" method iscussed in Section 3), 
because they are performed by the addressee of 
the argument, when the experience of the task is 
still vivid in her memory. 
To summarize, the evaluation framework just 
described supports users in performing a 
realistic task at their own pace by-interacting 
with an IDEA system. In the context of this task, 
an evaluative argument is generated and 
tneasurements ,related to its effectiveness can be.'  
performed. 
In the next section, we discuss an experiment 
that we are currently running by using the 
evaluation framework. 
13 
? ' " , ' -  '. : / : - . ' " " ; ,  ' : ~ i l i ~  
z ~ L . _ - - . - - - - - . - -3  I.,.,-7...2,.....,.1 
141 
~nl  I I 
Glwden_glze P ~?=h_Sl:r~ Deck 
? m ~.  ? 
I m lie 
, /  - . . " " ' ,  Norghs lde  
-4 ~f .%'~"  " \ ~o . . . . . .  
2~d , .  - . >% 2rld 
............... ~ ~.. T~ . ~ 
A University \[8 "\:~ ~ 
I " i 8h,~pplng \] 
3rd  
Wes~end i: :~ ~Eas  tend  
,?PIO ?.rcf~ r,~. : ;,, C ~'Io ~nrnR rtt~, 
z ,  
i he safe Eastend neigflbOdl0od. Even |hough house 3-,26 is somewhat far 
am a rapid i ranspod~idn  stop ! ! .6 miles). It is ?idso towork  {1.8 miles ). And 
Ihe traffic Is modera le  on ~d streel. Furlherrnore. the quality of house 
is good. Houso ~ offecz ? be~lh~ ~ o( Iho  rlrver Anti aItso 
ooks beautiful, 
Figure 3 The IDEA environment display at the end of the interaction 
5 The Exper iment  
As explained in Section 1, the argument 
generator has been designed to facilitate testing 
of the effectiveness of different aspects of the 
generation process. The experimenter can easily 
control whether the generator tailors the 
argument to the current user, the degree of  
conciseness of the argument, and what 
microplanning tasks are performed. In our initial 
experiment, because of  limited financial and 
human resources, we focus on the first two 
aspects for arguments about a single entity. Not 
because we =are not int~ested in  effecti.veness of 
performing microplanning tasks, but because we 
consider effectiveness of  tailoring and 
conciseness somewhat more difficult, and 
therefore more interesting to prove. 
Thus, we designed a between-subjects 
experiment with four experimental conditions: 
No-Argument  - subjects are simply informed that 
a new house came on the market. 
Ta i lo red-Conc ise  - subjects are presented with 
an evaluation of  the new house tailored to their 
preferences and at a level of conciseness that we 
hypothesize to be optimal. 
Non-Ta i lo red-Conc ise  - subjects are presented 
with an evaluation of the new house which is not 
tailored to their preferences 5, but is at a level of 
conciseness that we hypothesize to be optimal. 
Ta i lo red-Verbose  - subjects are presented with 
an evaluation,of the .new- house..tailored to their 
preferences, but at a level of conciseness that we 
hypothesize to be too low. 
s The evaluative argument is tailored to a default 
average user, for whom all aspects of a house are 
equally important. 
14 
a) How would you judge the houses in your Hot List? 
The more you like the house the closer you should put a cross to "good cho ice"  
I st house  
bad cho ice  : " " : : _ _  : _ _  : _ _  : _ _  : : good  cho ice  
2 nd house  
bad cho ice  : 
3 rd house 
bad cho ice  : 
4 th house  
bad cho ice  : 
_ _  : " " : : _ _  " : _ _  : : good  cho ice  
: : : : : : _ _  : : : good  cho ice  
: : " " : _ _  : _ _  "' " : :: _ _  i good  cho ice  " 
b) How sure are you that you have selected the four best houses among the ones available? 
Unsure  : . . . .  : : : : : : Sure  
. . . .  " />  Z .  - '~ . ~ i ? ' .~~ 
Figure 4 Excerpt from questionnaire that subjects fill out at the end of-the interaction 
In the four conditions, all the information about 
the new house is also presented graphically. Our 
hypotheses on the outcomes of the experiment 
can be summarized as follows. We expect 
arguments generated for the Tailored-Concise 
condition to be more effective than arguments 
generated for both the Non-Tailored-Concise 
and Tailored-Verbose conditions. We also 
expect the Tailored-Concise condition to be 
somewhat better than the No-Argument 
condititon, but to a lesser extent, because 
subjects, in the absence of any argument, may 
spend more time further exploring the dataset, 
therefore reaching a more informed and 
balanced decision. Finally, we do not have 
strong hypotheses on comparisons of argument 
effectiveness among the No-Argument, Non- 
Tailored-Concise and Tailored-Verbose 
conditions. 
The design of our evaluation framework and 
consequently the design of this experiment take 
into account hat the effectiveness of arguments 
is determined not only by the argument itselK 
but also by user's traits such as 
argumentativeness, need for cognition, self- 
esteem and intelligence (as described in Section 
2). Furthermore, we assume that argument 
effectiveness can be measured by means of the 
behavioral intentions and self-reports described 
in Section 4.2. 
The experiment is organized in two phases.-In 
the first phase, the subject fills out three 
questionnaires on the Web. One questionnaire 
implements a method from decision theory to 
acquire a model of the subject's preferences 
(Edwards and Barton 1994). The second 
questionnaire assesses the subject's 
argumentativeness (lnfante and Rancer 1982). 
The last one assesses the subject's need for 
cognition (Cacioppo, PeW et al 1984). In the 
second phase of the experiment, o control for 
other possible confounding variables (including 
intelligence and self-esteem), the subject is 
randomly assigned to one of the four conditions. 
Then, the subject interacts with the evaluation 
framework and at the end of the interaction 
measures of the argument effectiveness are 
collected. Some details on measures based on 
subjects' self-reports can be examined in Figure 
4, which shows an excerpt from the final 
questionnaire that subjects are asked to fill out at 
the end of the interaction. 
After running the experiment with 8 pilot 
subjects to refine and improve the experimental 
procedure, we are currently running a formal 
experiment involving 40 subjects, I0 in each 
experimental conditions. 
Future  Work  
In this paper, we propose a task-based 
framework to evaluate evaluative arguments. 
We are currently using this framework to run a 
formal experiment to evaluate arguments about a 
single entity. However, this is only a first step. 
The power of the framework is that it enables 
the design and execution of many different 
:experiments . about- evaluative .arguments. The 
goal of our current experiment is to verify 
whether tailoring an evaluative argument to the 
user and varying the degree of argument 
conciseness influence argument effectiveness. 
We envision further experiments along the 
following lines. 
15 
In the short "term, we plan to stcidy more 
complex arguments, including comparisons 
between two entities, as well as comparisons 
between mixtures of  entities and set of  entities. 
One experiment could assess the influence of 
tailoring and conciseness on the effectiveness of
these more complex arguments. Another 
possible experiment could compare different 
. argumentative, strategies for:~ selecting ~and 
organizing the content of these arguments. In the 
long term, we intend to evaluate techniques to 
generate evaluative arguments that combine 
natural anguage and information graphics (e.g., 
maps, tables, charts). 
Acknowledgements 
My thanks go to the members of the Autobrief 
project: J. Moore, S. Roth, N. Green, S. 
Kerpedjiev and J. Mattis. I also thank C. Conati 
for comments on drafts of  this paper. This work 
was supported by grant number DAA- 
1593K0005 from the Advanced Research 
Projects Agency (ARPA). Its contents are solely 
responsibility of the author. 
References 
Cacioppo, J. T., R. E. Petty, et al (1984). The 
efficient Assessment ofneed for Cognition. Journal 
of Personality Assessment 48(3): 306-307. 
Cacioppo, J. T., R. E. Petty, et al (1983). Effects of 
Need for Cognition on Message Evaluation, Recall, 
and Persuasion. Journal of Personality and Social 
Psychology 45(4): 805-8 l8. 
Carenini, G. (2000). Evaluating Multimedia 
Interactive Arguments in the Context of Data 
Exploration Tasks. PhD Thesis, Intelligent System 
Program, University of Pittsburgh. 
Carenini, G. and J. Moore (2000). A Strategy for 
Evaluating Evaluative arguments Int. Conference 
on NLG, Mitzpe Ramon, Israel. 
C, hu-Carroll, J. and S. Carberry (1998). Collaborative 
Response Generation in Planning Dialogues. 
Computational Linguistics 24(2): 355-400. 
Clemen, R. T. (1996).=Making l-lard Decisions: an 
introduction to decision analysis. Belmont, 
California, Duxbury Press. 
Dale, R., B. di Eugenio, et al (1998). hm'oduction to 
the Special Issue on NLG. Computational 
Linguistics 24(3): 345-353. 
Edwards, W. ~afid F: H. Barron '( 1994 ).:~MJJ?'?s~n~a :-::~':~2~-:- 
SMARTER: Improved Simple Methods for 
Multiattribute Utility Measurements. 
Organizational Behavior and Human Decision 
Processes 60: 306-325. 
Elhadad, M. (1995). Using argumentation in text 
generation. Journal of Pragmatics 24:189-220. 
Eugenio, B. D., J. Moore, et al (1997). Learning 
_ ..PTeatures;~:~hat. :Br.edicts:. ~Cue.: Usage. ACL97, 
Madrid, Spain. 
Grosz, B. J., A. K. Joshi, et al (1995). Centering: A 
Framework for Modelling the Local Coherence of 
Discourse. Computational Linguistics 21(2):203- 
226. 
Infante, D. A. and A. S. Rancer (1982). A 
Conceptualization and Measure of 
Argumentativeness. Journal of Personal ity 
Assessment 46: 72-80. 
Knott, A. (1996). A Data-Driven Methodology for 
Motivating a Set of Coherence Relations, 
University of Edinburgh. 
Lester, J. C. and B. W. Porter (I 997). Developing and 
Empirically Evaluating Robust Explanation 
Generators: The KNIGHT Experiments. 
Computational Linguistics 23(1): 65-101. 
Miller, M. D. and T. R. Levine (1996). Persuasion. 
An Integrated Approach to Communication Theory 
and Research. M. B. Salwen and D. W. Stack. 
Mahwah, New Jersey: 261-276. 
Olso, J. M. and M. P. Zanna (1991). Attitudes and 
beliefs; Attitude change and attitude-behavior 
consistency. Social Psychology. R. M. Baron and 
W. G. Graziano. 
Robin, J. and K. McKeown (1996). Empirically 
Designing and Evaluating a New Revision-Based 
Model for Summary Generation. Artificial 
Intelligence Journal, 85, 135-I 79. 
Roth, S. F., M. C. Chuah, et al (1997). Towards an 
Information Visualization Workspace: Combining 
Multiple Means of ?vpression. Human-Computer 
Interaction Journal.Vol. 12, No. 1 & 2, pp. 131-185 
Shaw, J. (I 998). Clause Aggregation Using 
Linguistic Knowledge. 9th Int. Workshop on NLG, 
Niagara-on-the-Lake, Canada. 
Young, M. R. Using Grice's McLrim of Quantity to 
Selecr the Corrtent o f  Plan Descriptions: Artificial 
Intelligence Journal, to appear. 
Young, M. R. and J. D. Moore (1994). Does 
Discourse Planning Require a Special-Purpose 
Planner? Proceedings of the AAAI-94 Workshop 
on planning for Interagent Communication. Seattle, 
WA. 
16 
A Strategy for Generating Evaluative Arguments .. 
Giuseppe Carenini 
Intelligent Systems Program 
University of Pittsburgh, 
Pittsburgh, PA 15260, USA 
carenini@cs.pitt.edu 
Abstract 
We propose an argumentation strategy for 
generating evaluative arguments that can be 
applied in systems erving as personal assistants 
or advisors. By following guidelines from 
argumentation theory and by employing a 
quantitative model of the user's preferences, the 
strategy generates arguments hat are tailored to 
the user, properly arranged and concise. Our 
proposal extends the scope of previous 
approaches both in terms of types of arguments 
generated, and in terms of compliance with 
principles from argumentation theory. 
Introduction 
Arguing involves an intentional communicative 
act that attempts to create, change or reinforce 
the beliefs and attitudes of another person. 
Factual and causal arguments attempt to affect 
beliefs (i.e. assessments that something is or is 
not the case), whereas evaluative arguments 
attempt to affect attitudes (i.e., evaluative 
tendencies typically phrased in terms of like and 
dislike or favor and disfavor). 
With the ever growing use of the Web, an 
increasing number of systems that serve as 
personal assistants, advisors, or sales assistants 
are becoming available online ~. These systems 
frequently need to generate evaluative 
arguments for domain entities. For instance, a 
real-estate assistant may need to compare two 
houses, arguing that one would be a better 
choice than the other for its user. 
Argumentation theory (Mayberry and Golden 
1996; Miller and Levine 1996; Corbett and 
Connors 1999) indicates that effective 
arguments should be constructed tbllowing three 
Johanna D. Moore 
The Human Communication Research Centre, 
University of Edinburgh, 
2 Buccleuch Place, Edinburgh EH8 9LW, UK. 
jmoore@cogsci.ed.ac.uk 
general principles. First, arguments hould be 
constructed considering the dispositions of the 
audience towards the information presented. 
Second, sub-arguments supporting or opposing 
the main argument claim should be carefully 
arranged by considering their strength of support 
or opposition. Third, effective arguments should 
be concise, presenting only pertinent and cogent 
information. 
In this paper, we propose an argumentation 
strategy for generating evaluative arguments hat 
can be applied in systems erving as personal 
assistants or advisors. By following principles 
and guidelines from argumentation theory and 
by employing a quantitative model of the user's 
preference, our strategy generates evaluative 
arguments hat are tailored to the user, properly 
arranged and concise. 
Although a preliminary version of our 
argumentative strategy was cursorily described 
in a previous short paper (Carenini and Moore 
1999), this paper includes several additional 
contributions. First, we discuss how the strategy 
is grounded in the argumentation literature. 
Then, we provide details on the measures of 
argument strength and importance used in 
selecting and ordering argument support. Next, 
we generalize the argumentative strategy and 
correct some errors in its preliminary version. 
Finally, we discuss how our strategy extends the 
scope of previous approaches to generating 
evaluative arguments in terms of coverage (i.e., 
types of arguments), and in terms of compliance 
with principles from argumentation theory. 
Because of  space limitations, we only discuss' 
previous work on generating evaluative 
arguments, rather than previous work on 
generating arguments in general. 
See llbr instance www.activebuyersguide.com 
47 
1 Guidelines from Argumentation Theory 
An argumentation strategy specifies what 
content should be included in the argument and 
how it should be arranged. This comprises 
several decisions: what represents supporting (or 
opposing) evidence for the main claim, where to 
position the main claim of the argument; what 
supporting (or opposing) evidence to include 
andhow to order it, and.how to order supp6rfing 
and opposing evidence with respect to each 
other. 
Argumentation theory has developed guidelines 
specifying how these decisions can be 
effectively made (see (Mayberry and Golden 
1996; Miller and Levine 1996; Corbett and 
Connors 1999; McGuire 1968) for details; see 
also (Marcu 1996) for an alternative discussion 
of some of the same guidelines). 
(a) What represents supporting (or opposing) 
evidence for  a claim - Guidelines for this 
decision vary depending on the argument type. 
Limiting our analysis to evaluative arguments, 
argumentation theory indicates that supporting 
and opposing evidence should be identified 
according to a model of the reader's values and 
preferences. For instance, the risk involved in a 
game can be used as evidence for why your 
reader should like the game, only if the reader 
likes risky situations. 
(b) Posit ioning the main claim - Claims are 
often presented up front, usually for the sake of 
clarity. Placing the claim early helps readers 
follow the line of reasoning. However, delaying 
the claim until the end of the argument can be 
effective, particularly when readers are likely to 
find the claim objectionable or emotionally 
shattering. 
(c) Selecting supporting (and opposing) 
evidence - Often an argument cannot mention all 
the available evidence, usually for the sake of 
brevity. Only strong evidence should, be 
presented in detail, whereas weak evidence 
should be either briefly mentioned or omitted 
entirely. 
(d) Arranging/Ordering~supporiing evicleiTce - 
Typically the strongest support should be 
presented first, in order to get at least provisional 
agreement from the reader early on. If at all 
possible, at least one very effective piece of 
supporting evidence should be saved for the end 
of the argument, inorder to leave the reader with 
a final impression of the argument's strength. 
This guideline proposed in (Mayberry and 
Golden 1996) is a compromise between the 
climax and the anti-climax approaches discussed 
in (McGuire 1968). 
(e) Addressing and ordering the 
counterarguments (opposing evidence) - There 
........ ? ~ar~ . ~three,.~options. ~ .for, :Ihis~ :.ateeision: not ~to . . . .  
mention any counterarguments, to acknowledge 
them without directly refuting them, to 
acknowledge them and directly refuting them. 
Weak counterarguments may be omitted. 
Stronger counterarguments should be briefly 
acknowledged, because that shows the reader 
that you are aware of the issue's complexity; and 
it also contributes to the impression that you are 
reasonable and broad-minded. You may need to 
refute a counterargument once you have 
acknowledged it, if the reader agrees with a 
position substantially different from yours. 
Counterarguments should be ordered to 
minimize their effectiveness: strong ones should 
be placed in the middle, weak ones upfront and 
at the end. 
(09 Ordering supporting and opposing evidence 
- A preferred ordering between supporting and 
opposing evidence appears to depend on 
whether the reader is aware of the opposing 
evidence. If so, the preferred ordering is 
opposing before supporting, and the reverse 
otherwise. 
Although these guidelines provide useful 
information on the types of content to include in 
an evaluative argument and how to arrange it, 
the design of a computational rgumentative 
strategy based on these guidelines requires that 
the concepts mentioned in the guidelines be 
formalized in a coherent computational 
framework. This includes: explicitly 
representing the reader's values and preferences 
(used in guideline a); operationally defining the 
term "objectionable claim v (used in guideline b) 
through a measure of the discrepancy between 
the readerrs-initial positionand-the argument's 
main claim2; providing a measure of evidence 
strength (needed in guidelines c, d, and e); and 
3 An operational definition for "emotionally 
shattering" isoutside the scope of this paper. 
48 
House 
Value 
OBJECTIVES ~OMPONENT VALUE FUNCTIONS 
ATTRIBUTES 
Location ?.y 
.Size 0.8 
0.2 
~.  bTeighborhoo d 
Distance-from- park 
"---- t-of-room 
Storage-space 
xl=nl  0 
xl=n2 0.3 
xl=n3 1 
0=<:x2<:5 1-(1/5" X2) 
X~5 0 
Figure 1 Sample additive multiattribute value function (AMVF) 
representing whether the reader is or is not 
aware of  certain facts (needed in guideline tO. 
2 From Guidelines to the Argumentation 
Strategy 
We assume that the reader's values and 
preferences are represented as an additive 
multiattribute value function (AMVF), a 
conceptualization based on multiattribute utility 
theory (MAUT)(Clemen 1996). Besides being 
widely used in decision theory (where they were 
originally developed), conceptualizations based 
on MAUT have recently become a common 
choice in the field of  user modelling (Jameson, 
Schafer et al 1995). Similar models are also 
used in Psychology, in the study of consumer 
behaviour (Solomon 1998). 
2.1 Background on AMVF 
An AMVF is a model of a person's values and 
preferences with respect o entities in a certain 
class. It comprises a value tree and a set of 
component  value funct ions,  one for each 
attribute of the entity. A value tree is a 
decomposition of the value of an entity into a 
hierarchy of aspects of the entity 3, in which the 
leaves correspond to the entity primitive 
a~ributes (see Figure 1 for a simple value tree in 
the real estate domain). The arcs of the tree are 
weighted to represent he importance of the 
value of  an objective in contributing to the value 
3 In decision theory these aspects are called 
objectives. For consistency with previous work, we 
will follow this terminology in the remainder of the 
paper. 
of its parent in the tree (e.g., in Figure 1 location 
is more than twice as important as size in 
determining the value of a house). Note that the 
sum of the weights at each level is equal to 1. A 
component value function for an attribute 
expresses the preferability of each attribute 
value as a number in the \[0,1\] interval. For 
instance, in Figure 1, neighborhood n2 has 
preferability 0.3, and a distance-from-park of 1 
mile has preferability (1 - (1/5" 1))=0.8. 
Formally, an AMVF predicts the value v(e) of an 
entity e as follows: 
v(e) = v(xl ..... x,) = Y~w, v /x9,  where 
- (x/ ..... x,,) is the vector of attribute values for 
an entity e 
- Vattribute i, v, is the component value 
function, which maps the least preferable x, 
to 0, the most preferable to I, and the other 
x, to values in \[0,1\] 
- w, is the weight for attribute i, with 0_< w, _<1 
and Zw, =1 
- w, is equal to the product of all the weights 
from the root of the value tree to the 
attribute i 
A function vo(e) can also be defined for each 
objective. When applied to an entity, this 
? - function "returns ~the value o f  the entity with 
respect o that objective. For instance, assuming 
the value tree shown in Figure 1, we have: 
v,. . . . . . . . .  (e )  = 
= (0.4 * V~,,h~orhooa (e)) + (0.6 * vl~,~,_/,~,,,_r~rk (e)) 
Thus, given someone's AMVF, it is possible to 
compute how valuable an entity is to that 
49 
individual. Furthermore, it is possible to 
compute how valuable any objective (i.e., any 
aspect of that entity) is for that person. All of  
these values are expressed as a number in the 
interval \[0, i \]. 
2.2 Computational Definition of Concepts 
Mentioned in Guidelines 
Presenting an evaluative argument is an attempt 
to persuade the reader that a value judgment 
applies to a subject. The value judgement, also 
called the argumentative intent, can either be 
positive (in favour of  the subject), or negative 
(against the subject) 4. The subject can be a 
single entity (e.g., "This book is very good"), the 
difference between two entities (e.g., "City-a is 
somewhat better than city-b'), or any other form 
of comparison among entities in a set (e.g., 
"This city is the best in North America"). 
Guideline (a) - Given the reader's AMVF, it is 
straightforward to establish what represent 
supporting or opposing evidence for an 
argument with a given argumentative intent and 
a given subject. In fact, if the argumentative 
intent is positive, objectives for which the 
subject has positive value can be used as 
supporting evidence, whereas objectives for 
which the subject has a negative value can be 
used as opposing evidence (the opposite holds 
when the argumentative intent is negative). The 
value of different subjects is measured as 
follows. If the subject is a single entity e, the 
value of the subject for an objective o is vo(e), 
and it is positive when it is greater than 0.5, the 
midpoint of \[0,1\] (negative otherwise). In 
contrast, if the subject is a comparison between 
two entities (e.g., v(ed > v(e_,)), the value of the 
subject for an objective o is \[vo(e9 - Vo(e,)\], and 
it is positive when it is greater than 0 (negative 
otherwise). 
Guidelines (b) - Since argumentative intent is a 
value judgment, we canreasonab\[y assume that 
instead of  being simply positive or negative, it 
may be specified more precisely as a number in 
the interval \[0,1\] (or as a specification that can 
be normalized in this interval), Then, the term 
4 Arguments can also be neutral. However, in this 
paper we do not discuss arguments with a neutral 
argumentative intent. 
"objectionable claim" can be operationally 
defined. If we introduce a measure-of- 
discrepancy(MD) as the absolute value of the 
difference between the argumentative intent and 
the reader's expected value of the subject before 
the argument is presented (based on her AMVF), 
a claim becomes more and more "objectionable!' 
for a reader as MD moves from 0 to 1. 
,~,. ,:,_.~.uidelin~;,(c) ~(d), (e). ~,:~The,,~strength o? the .... 
evidence in support of (or opposition to) the 
main argument claim is critical in selecting and 
organizing the argument content. To define a 
measure of the strength of support (or 
opposition), we adopt and extend previous work 
on explaining decision theoretic advice based on 
an AMVF. (Klein 1994) presents explanation 
strategies (not based on argumentation theory) to 
justify the preference of one alternative from a 
pair. In these strategies, the compellingness of an 
objective measures the objective's strength in 
determining the overall value difference between 
the two alternatives, other things being equal. 
And an objective is notably-compell ing? (i.e., 
worth mentioning) if it is an outlier in a 
population of objectives with respect to 
compeilingness. The formal definitions are: 
compellingness(o, al a2, refo) = 
= w(o, refo)\[vo(at) - Vo(a2)\], where 
- o is an objective, a /and a2 are alternatives, 
refo is an ancestor of o in the value tree 
- w(o, refo) is the product of the weights of all 
the links from o to refo 
- vo is the component value function for leaf 
objectives (i.e., attributes), and it is the 
recursive evaluation over children(o) for 
nonleaf objectives 
notably-compelling?(o, opop. al, a2, refo) - 
\[ compellingness(o, al a2, refo) \[ >px+ko'x, where 
- o, al, a2 and refo are defined as in the 
previous Def; opop is an objective 
population (e.g., siblings(o)), and I opopl >2 
- pe  opop; xeX = \[compellingness(p, al, a_~, 
refo) l 
- gx is the mean of X, ~x is the standard 
deviation and k is a user-defined constant 
We have defined similar measures for arguing 
the value of a single entity and we named them 
s-compellingness and s-notably-compell ing?. 
50 
An objective can be s-compelling either because 
of its strength or because of its weakness in 
contributing to the value of an alternative. So, if 
m~ measures how much the value of an objective 
contributes to the overall value difference of an 
alternative from the worst possible case 5and m2 
measures how much the value of an objective 
contributes to the overall value difference of the 
is either a single entity or a pair of entities in the 
domain of interest. Root can be any objective in 
the value tree for the evaluation (e.g., the overall 
value of a house, its location, its amenities). 
ArgInt is the argumentative intent of the 
argument, a number in \[0,1 \]. The constant k, part 
of the definitions of notably-compelling? and s- 
notably-compelling?, determines the degree of 
:, .,,alternative ,from., th~_b~st:,possible:~ease,:.~e-: :,~ eoneisenessofithe;argument,,, The~Express-Value 
define s-compellingness a  the greatest of the 
two quantities m~ and m2. Following the 
terminology introduced in the two previous 
Equations we have: 
s-compellingness(o, a, refo) = 
= w(o, refo)\[max\[vo(a) - 0\],'\[1 - vo(a)\]\] 
We give to s-notably-compelling? a definition 
analogous to the one for notably-compelling? 
s-notably-compelling? (o,opop, a, refo) - 
\] s-compellingness(o,a, refo) \[ >~+k~x, 
Guideline 09 - An AMVF does not represent 
whether the reader is or is not aware of certain 
facts. We assume this information is represented 
separately. 
2.3 The Argumentation Strategy 
We have applied the formal definitions 
described in the previous ection to develop the 
argumentative strategy shown in Figure 2. The 
strategy is designed for generating honest and 
balanced arguments, which present an 
evaluation of the subject equivalent to the one 
you would expect he reader to hold according to 
her model of preferences (i.e., the argumentative 
intent is equal to the expected value, so MD=0) 6. 
We now examine the strategy in detail, after 
introducing necessary, terminology. The subject 
5 a,.or~, is an alternative such that Vo v~,(a,,,,r~,)=O, 
whereas abL., is an alternative suchthat Vo vo(abe.?~)=l 
6 An alternative strategy, for generating arguments 
whose argumentative intent was-greater (or lower) 
than the expected value, could also be defined in our 
framework. However, this strategy should boost the 
evaluation of supporting evidence and include only 
weak counterarguments, or hide them overall (the 
opposite if the target value was lower than the 
expected value) 
function, used at the end of the strategy, 
indicates that the objective applied to the subject 
must be realized in natural language with a 
certain argumentative intent. 
In the first part of the strategy, depending on the 
nature of the subject, an appropriate measure of 
evidence strength is assigned, along with the 
appropriate predicate that determines whether a 
piece of evidence is worth mentioning. After 
that, only evidence that is worth mentioning is 
assigned as supporting or opposing evidence by 
comparing its value to the argument intent. In 
the second part, ordering constraints from 
argumentation theory are applied 7. Notice that 
we assume a predicate Aware that is true when 
the user is aware of a certain fact, false 
otherwise. Finally, in the third part of the 
strategy, the argument claim is expressed in 
natural language. The opposing evidence (i.e., 
ContrastingSubObjectives), that must be 
considered, but not in detail, is also expressed in
natural language. In contrast, supporting 
evidence is presented in detail, by recursively 
calling the strategy on each supporting piece of 
evidence. 
2.4 Implementation and Application 
The argumentation strategy has been 
implemented as a set of plan operators. Using 
these operators the Longbow discourse planner 
(Young and Moore 1994) selects and arranges 
the content of the argument. We have applied 
our strategy in a system that serves as a real- 
estate personal assistant (Carenini 2000a). The 
system presents information about houses 
available on the market in graphical format. The 
user explores this information by means of 
interactive techniques, and can request a natural 
7 The steps in the strategy are marked with the 
guideline they are based on. 
51 
Argue(subject, Root, Argint, k ) 
;; ass ignments  and content  select ion 
I f  subject = single-entity = e then SVo, = Vol (e) 
Measure-of-strength = s-compel!ingness 
" Worth-mention? = s-notably-compelling? 
Else I f  subject = e~,e 2 then SVo, = \[%, (e,) - vo, (e2)\] 
Measure-of-strength = compellingness 
Worth-mention? = notably-compelling? 
Eliminate all objectives oil ~ Worth-mention? (o,, siblings(o,), subject, Root) ;guideline(c) 
AllEvidence ~- ehildren(RooO 
AlllnFavor~-- all o \] o e AllEvidence/x (SVo ..~ArglnO ;guideline(a) 
SecondBestObjlnFavor~-second most compelling objective o lo E AlllnFavor 
RemainingObjectiveslnFavor ~- AlllnFavor - SecondBestObjlnFavor 
ContrastingObjectives ~- AllEvidence - AlllnFavor ;guideline(a) 
;; ordering the selected content 
AddOrdering(Root -~AllEvidence) ;; we assume MD=0, so claim is not objectionable ;guideline(b) 
I f  Aware(User, ContrastingObjectives) then ;guideline(f) 
AddOrdering( ContrastingObjectives -~ AlllnFavor) 
Else AddOrdering(ContrastingObjectives ~- A lllnFavor ); 
A ddOrdering( RemainingObjectiveslnFavor -~ SecondBestObjlnFavor ) ;guideline(d) 
Sort(RemainingObjectiveslnFavor," decreasing order according to Measure-of-strength) ;guideline(d) 
Sort(ContrastingObjectives," strong ones in the middle, weak ones upfront and at the end) ;guideline(e) 
;; steps for expressing or further argue the content  
Express-Value(subject, Root, Arglnt) 
For all o ~ AlllnFavor, I f  ~leaffo) then Argue(subject, o SVo, k) 
Else Express-Value(subject, o, SVo) 
For all o E ContrastingObjectives, Express-Value(subject, o, SVo) ;guideline(e) 
Legend: (a -~ b) ~ a preceeds b 
(v~ ~- v 2) ~ vl and v 2 are both positive or negative values 
(see Section O for what this means for d~erent subjects) 
-, . -= Figure 2 The,Argumentation strategy 
52 
language evaluation of any house just by 
dragging the graphical representation of the 
house to a query button. The evaluative 
arguments generated by the system are concise, 
properly arranged and tailored to the user's 
preferences s. For sample arguments generated 
by our strategy see (Carenini 2000b) in this 
proceedings. 
(Elzer, Chu-Carroli et al 1994; Chu-Carroll and 
Carberry 1998) studied the generation of 
evaluative arguments in the context of 
collaborative planning dialogues. Although they 
also adopt a qualitative measure of evidence 
strength, when an evaluation is needed this 
measure is mapped into numerical values so that 
preferences can be compared and combined 
. . . . .  ...= :- . . . . . . .  :,, ~- .-: :-;.~ ~,xnore:.:e:ffeeti~ely:,Rl~ve.~t?,~ittr,,respeet =-~tO:our 
3 Previous Work 
Although considerable research has been 
devoted to study the generation of evaluative 
arguments, all approaches proposed so far are 
limited in the type of evaluative arguments 
generated, and in the extent to which they 
comply with guidelines from argumentation 
literature. 
(Elhadad 1992) investigated a general 
computational framework that covers all aspects 
of generating evaluative arguments of single 
entities, from content selection and structuring to 
fine-grained realization decisions. However, his 
work concentrates on the linguistic aspects. His 
approach to content selection and structuring 
does not provide a measure of evidence strength, 
which is necessary to implement several of the 
guidelines from argumentation literature we 
have examined. 
Other studies have focused more on the process 
of content selection and structuring. However, 
with respect o our proposal, they still suffer 
from some limitations. (Morik 1989) describes a 
system that uses a measure of evidence strength 
to tailor evaluations of hotel rooms to its users. 
However, her system adopts a qualitative 
measure of evidence strength (an ordinal scale 
that appears to range from very-important tonot- 
important). This limits the ability of the system 
to select and arrange argument evidence, 
because qualitative measures only support 
approximate comparisons and are ~ notoriously 
difficult to combine (e.g., how many 
"somewhat-important" pieces of evidence are 
equivalent to. :an #important" .:.piece of.. 
evidence?). 
s The generation of fluent English also required the 
development of microplanning and realization 
components. For lack of space, we do not discuss 
them in this paper. 
approach, this work makes two strong 
simplifying assumptions. It only considers the 
decomposition of the preference for an entity 
into preferences for its primitive attributes (not 
considering that complex preferences frequently 
have a hierarchical structure). Additionally, it 
assumes that the same dialogue turn cannot 
provide both supporting and opposing evidence. 
(Kolln 1995) proposes a framework for 
generating evaluative arguments which is based 
on a quantitative measure of evidence strength. 
Evidence strength is computed on a ~zzy 
hierarchical representation of user preferences. 
Although this fuzzy representation may 
represent a viable alternative to the AMVF we 
have discussed in this paper, Kolln's proposal is 
rather sketchy in describing how his measure of 
strength can be used to select and arrange the 
argument content. 
Finally, (Klein 1994) is the previous work most 
relevant to our proposal. Klein developed a 
framework for generating explanations to justify 
the preference of an entity out of a pair. These 
strategies were not based on argumentation 
theory. As described in Section 2.2, from this 
work, we have adapted a measure of evidence 
strength (i.e., compellingness), and a measure 
that defines when a piece of evidence is worth 
mentioning (i.e., notably-compelling?). 
Conclusions and Future Work  
In this paper, we propose.an argumentation 
strategy that extends? previous research on 
generating evaluative arguments in two ways. 
Our .  strategy -covers ~ the: <generation. : :of 
evaluations of a single entity, as well as 
comparisons between two entities. Furthermore, 
our strategy generates arguments, which are 
concise, properly arranged and tailored to a 
hierarchical model of user's preferences, by 
53 
following a comprehensive set of guidelines 
from argumentation theory. 
Several issues require further investigation. 
First, we plan to generalize our approach to 
more complex models of user preferences. 
Second, although our strategy is based on 
insights from argumentation theory, the ultimate 
arbiter for effectiveness is empirical evaluation. 
Clemen, R. T. (1'996). Making Hard Decisions: an 
introduction to decision analysis. Duxbury Press 
Corbett, E. P. J. and R. J. Connors (1999). Classical 
Rhetoric for the Modern Student, Oxford 
University Press. 
Elhadad, M. (1992). Using Argumentation toControl 
Lexical Choice: A Functional Unification 
Implementation. PhD Thesis, CS. Columbia. NY. 
Therefore, we have~..developed~an+~v.atuation ......... Elzer,.S.,..I_Giatt.-.Carrolk..et.al.(.1994).Recogn&ing 
environment o verify whether arguments 
generated by our strategy actually affect user 
attitudes in the intended irection (Carenini 
2000b). A third area for future work is the 
exploration of techniques to improve the 
coherence of arguments generated by our 
strategy. In the short term, we intend to integrate 
the ordering heuristics uggested in (Reed and 
Long 1997). In the long term, by modelling user 
attention and retention, we intend to enable our 
strategy to assess in a principled way when 
repeating the same information can strengthen 
argument force. Finally, we plan to extend our 
strategy to evaluative arguments for 
comparisons between mixtures of entities and 
set of entities. 
Acknowledgements 
Our thanks go to the members of the Autobrief 
project: S. Roth, N. Green, S. Kerpedjiev and J. 
Mattis. We also thank C. Conati for comments 
on drafts of this paper. This work was supported 
by grant number DAA-1593K0005 from the 
Advanced Research Projects Agency (ARPA). 
Its contents are solely responsibility of the 
authors. 
References 
Carenini, G. (2000a). Evaluating Multimedia 
Interactive Arguments in the Context of Data 
Exploration Tasks. PhD Thesis, Intelligent System 
Program, University of Pittsburgh. 
Carenini, G. (2000b). A Framework to Evaluate 
Evaluative Arguments. Int. Conference on Natural 
Language-Generations. Mitzpe~,Ramon, Israel. 
Carenini, G. and J. Moore (1999). Tailoring 
Evaluative Arguments to User's Preferences. User 
Modelling, Banff; Canada : 299-301. 
Chu-Carroll, J. and S, Carberry (1998). Collaborative 
Response Generation in Planning Dialogues. 
Computational Linguistics 24(2): 355-400. 
and Utilizing User Preferences in Collaborative 
Consultation Dialogues. Proceedings of Fourth 
Int. Conf. of User Modeling. Hyannis, MA: 19-24. 
Jameson, A., R. Schafer, et al (1995). Adaptive 
provision of Evaluation-Oriented Information: 
Tasks and techniques. Proc. of 14th IJCAI. 
Montreal, Canada. 
Klein, D. (1994). Decision Analytic Intelligent 
Systems: Automated Explanation and Knowledge 
Acquisition, Lawrence Erlbaum Associates. 
Kolln, M. E. (1995). Employing User Attitudes in 
Text Planning. 5th European Workshop on Natural 
Language Generation, Leiden, The Netherlands. 
Marcu, D. (1996). The Conceptual and Linguistic 
Facets of Persuasive Arguments. ECAI workshop - 
Gaps and Bridges: New Directions in Planning and 
Natural Language Generation. 
Mayberry, K. J. and R. E. Golden (1996). For 
Argument's Sake: A Guide to Writing Effective 
Arguments, Harper Collins, College Publisher. 
McGuire, W. J. (1968). The Nature of Attitudes and 
Attitudes Change. The Handbook of Social 
Psychology. G. Lindzey and E. Aronson, Addison- 
Wesley. 3: 136-314. 
Miller, M. D. and T. R. Levine (1996). Persuasion. 
An Integrated Approach to Communication Theot T
and Research. M. B. Salwen and D. W. Stack. 
Mahwah, New Jersey: 261-276. 
Morik, K. (1989). User Models and Conversational 
Settings: Modeling the User's Wants. User Models 
in Dialog Systems. A. Kobsa and W. Wahlster, 
Springer-Verlag: 364-385. 
Reed, C. and D. Long (1997). Content Ordering in 
the Generation of Persuasive Discourse. Proc, of 
the 15th IJCAI, Nagoya; Japan. 
Solomon, M. R. (1998). Consumer Behavior: Bzo,ing, 
Having. and Being. ~ Prentice Hall. 
Young, M. R. and J. D. Moore (1994). Does 
Discourse Planning Require a Special-Purpose 
Planner? Proc. of the AAAI-94 Workshop on 
planning for lnteragent Communication. Seattle, 
WA. 
54 
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 388?398,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Exploiting Conversation Structure in Unsupervised Topic Segmentation for
Emails
Shafiq Joty and Giuseppe Carenini and Gabriel Murray and Raymond T. Ng
{rjoty, carenini, gabrielm, rng}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
Abstract
This work concerns automatic topic segmen-
tation of email conversations. We present a
corpus of email threads manually annotated
with topics, and evaluate annotator reliabil-
ity. To our knowledge, this is the first such
email corpus. We show how the existing topic
segmentation models (i.e., Lexical Chain Seg-
menter (LCSeg) and Latent Dirichlet Alloca-
tion (LDA)) which are solely based on lex-
ical information, can be applied to emails.
By pointing out where these methods fail and
what any desired model should consider, we
propose two novel extensions of the models
that not only use lexical information but also
exploit finer level conversation structure in a
principled way. Empirical evaluation shows
that LCSeg is a better model than LDA for
segmenting an email thread into topical clus-
ters and incorporating conversation structure
into these models improves the performance
significantly.
1 Introduction
With the ever increasing popularity of emails and
web technologies, it is very common for people to
discuss issues, events, agendas or tasks by email.
Effective processing of the email contents can be
of great strategic value. In this paper, we study
the problem of topic segmentation for emails, i.e.,
grouping the sentences of an email thread into a
set of coherent topical clusters. Adapting the stan-
dard definition of topic (Galley et al, 2003) to con-
versations/emails, we consider a topic is something
about which the participant(s) discuss or argue or
express their opinions. For example, in the email
thread shown in Figure 1, according to the major-
ity of our annotators, participants discuss three top-
ics (e.g., ?telecon cancellation?, ?TAG document?,
and ?responding to I18N?). Multiple topics seem to
occur naturally in social interactions, whether syn-
chronous (e.g., chats, meetings) or asynchronous
(e.g., emails, blogs) conversations. In multi-party
chat (Elsner and Charniak, 2008) report an average
of 2.75 discussions active at a time. In our email cor-
pus, we found an average of 2.5 topics per thread.
Topic segmentation is often considered a pre-
requisite for other higher-level conversation analy-
sis and applications of the extracted structure are
broad, encompassing: summarization (Harabagiu
and Lacatusu, 2005), information extraction and or-
dering (Allan, 2002), information retrieval (Dias et
al., 2007), and intelligent user interfaces (Dredze et
al., 2008). While extensive research has been con-
ducted in topic segmentation for monologues (e.g.,
(Malioutov and Barzilay, 2006), (Choi et al, 2001))
and synchronous dialogs (e.g., (Galley et al, 2003),
(Hsueh et al, 2006)), none has studied the problem
of segmenting asynchronous multi-party conversa-
tions (e.g., email). Therefore, there is no reliable an-
notation scheme, no standard corpus, and no agreed-
upon metrics available. Also, it is our key hypothe-
sis that, because of its asynchronous nature, and the
use of quotation (Crystal, 2001), topics in an email
thread often do not change in a sequential way. As a
result, we do not expect models which have proved
successful in monologue or dialog to be as effective
when they are applied to email conversations.
Our contributions in this paper aim to remedy
388
these problems. First, we present an email corpus
annotated with topics and evaluate annotator agree-
ment. Second, we adopt a set of metrics to mea-
sure the local and global structural similarity be-
tween two annotations from the work on multi-party
chat disentanglement (Elsner and Charniak, 2008).
Third, we show how the two state-of-the-art topic
segmentation methods (i.e., LCSeg and LDA) which
are solely based on lexical information and make
strong assumptions on the resulting topic models,
can be effectively applied to emails, by having them
to consider, in a principled way, a finer level struc-
ture of the underlying conversations. Experimen-
tal results show that both LCSeg and LDA benefit
when they are extended to consider the conversa-
tional structure. When comparing the two methods,
we found that LCSeg is better than LDA and this
advantage is preserved when they are extended to
incorporate conversational structure.
2 Related Work
Three research areas are directly related to our study:
a) text segmentation models, b) probabilistic topic
models, and c) extracting and representing the con-
versation structure of emails.
Topic segmentation has been extensively studied
both for monologues and dialogs. (Malioutov and
Barzilay, 2006) uses the minimum cut model to seg-
ment spoken lectures (i.e., monologue). They form a
weighted undirected graph where the vertices repre-
sent sentences and the weighted links represent the
similarity between sentences. Then the segmenta-
tion problem can be solved as a graph partitioning
problem, where the assumption is that the sentences
in a segment should be similar, while sentences in
different segments should be dissimilar. They op-
timize the ?normalized cut? criterion to extract the
segments. In general, the minimization of the nor-
malized cut criterion is NP-complete. However, the
linearity constraint on text segmentation for mono-
logue allows them to find an exact solution in poly-
nomial time. In our extension of LCSeg, we use
a similar method to consolidate different segments;
however, in our case the linearity constraint is ab-
sent. Therefore, we approximate the optimal solu-
tion by spectral clustering (Shi and Malik, 2000).
Moving to the task of segmenting dialogs, (Galley
et al, 2003) first proposed the lexical chain based
unsupervised segmenter (LCSeg) and a supervised
segmenter for segmenting meeting transcripts. Their
supervised approach uses C4.5 and C4.5 rules binary
classifiers with lexical and conversational features
(e.g., cue phrase, overlap, speaker, silence, and lex-
ical cohesion function). Their supervised approach
performs significantly better than LCSeg. (Hsueh
et al, 2006) follow the same approaches as (Galley
et al, 2003) on both manual transcripts and ASR
output of meetings. They perform segmentation at
both coarse (topic) and fine (subtopic) levels. For
the topic level, they achieve similar results as (Gal-
ley et al, 2003), with the supervised approach out-
performing LCSeg. However, for the subtopic level,
LCSeg performs significantly better than the super-
vised one. In our work, we show how LCSeg per-
forms when applied to the temporal ordering of the
emails in a thread. We also propose its extension to
leverage the finer conversation structure of emails.
The probabilistic generative topic models, such
as LDA and its variants (e.g., (Blei et al, 2003),
(Steyvers and Griffiths, 2007)), have proven to be
successful for topic segmentation in both mono-
logue (e.g., (Chen et al, 2009)) and dialog (e.g.,
(Georgescul et al, 2008)). (Purver et al, 2006) uses
a variant of LDA for the tasks of segmenting meet-
ing transcripts and extracting the associated topic
labels. However, their approach for segmentation
does not perform better than LCSeg. In our work,
we show how the general LDA performs when ap-
plied to email conversations and describe how it can
be extended to exploit the conversation structure of
emails.
Several approaches have been proposed to cap-
ture an email conversation . Email programs (e.g.,
Gmail, Yahoomail) group emails into threads using
headers. However, our annotations show that top-
ics change at a finer level of granularity than emails.
(Carenini et al, 2007) present a method to capture an
email conversation at the finer level by analyzing the
embedded quotations in emails. A fragment quota-
tion graph (FQG) is generated, which is shown to be
beneficial for email summarization. In this paper, we
show that topic segmentation models can also bene-
fit significantly from this fine conversation structure
of email threads.
389
3 Corpus and Evaluation Metrics
There are no publicly available email corpora anno-
tated with topics. Therefore, the first step was to
develop our own corpus. We have annotated the
BC3 email corpus (Ulrich et al, 2008) with top-
ics1. The BC3 corpus, previously annotated with
sentence level speech acts, meta sentence, subjectiv-
ity, extractive and abstractive summaries, is one of a
growing number of corpora being used for email re-
search. The corpus contains 40 email threads from
the W3C corpus2. It has 3222 sentences and an av-
erage of 5 emails per thread.
3.1 Topic Annotation
Topic segmentation in general is a nontrivial and
subjective task (Hsueh et al, 2006). The conver-
sation phenomenon called ?Schism? makes it even
more challenging for conversations. In schism a
new conversation takes birth from an existing one,
not necessarily because of a topic shift but because
some participants refocus their attention onto each
other, and away from whoever held the floor in the
parent conversation and the annotators can disagree
on the birth of a new topic (Aoki et al, 2006). In the
example email thread shown in Figure 1, a schism
takes place when people discuss about ?responding
to I18N?. All the annotators do not agree on the fact
that the topic about ?responding to I18N? swerves
from the one about ?TAG document?. The annota-
tors can disagree on the number of topics (i.e., some
are specific and some are general), and on the topic
assignment of the sentences3. To properly design an
effective annotation manual and procedure we per-
formed a two-phase pilot study before carrying out
the actual annotation. For the pilot study we picked
five email threads randomly from the corpus. In the
first phase of the pilot study we selected five uni-
versity graduate students to do the annotation. We
then revised our instruction manual based on their
feedback and the source of disagreement found. In
1The BC3 corpus had already been annotated for email sum-
marization, speech act recognition and subjectivity detection.
This new annotation with topics will be also made publicly
available at http://www.cs.ubc.ca/labs/lci/bc3.html
2http://research.microsoft.com/en-
us/um/people/nickcr/w3c-summary.html
3The annotators also disagree on the topic labels, however
in this work we are not interested in finding the topic labels.
the second phase we tested with a university postdoc
doing the annotation.
For the actual annotation we selected three com-
puter science graduates who are also native speakers
of English. They annotated 39 threads of the BC3
corpus4. On an average they took seven hours to an-
notate the whole dataset.
BC3 contains three human written abstract sum-
maries for each email thread. With each email thread
the annotators were also given an associated human
written summary to give a brief overview of the cor-
responding conversation. The task of finding topics
was carried out in two phases. In the first phase, the
annotators read the conversation and the associated
summary and list the topics discussed. They spec-
ify the topics by a short description (e.g., ?meeting
agenda?, ?location and schedule?) which provides a
high-level overview of the topic. The target number
of topics and the topic labels were not given in ad-
vance and they were instructed to find as many top-
ics as needed to convey the overall content structure
of the conversation.
In the second phase the annotators identify the
most appropriate topic for each sentence. However,
if a sentence covers more than one topic, they were
asked to label it with all the relevant topics according
to their order of relevance. If they find any sentence
that does not fit into any topic, they are told to label
those as the predefined topic ?OFF-TOPIC?. Wher-
ever appropriate they were also asked to make use of
two other predefined topics: ?INTRO? and ?END?.
INTRO (e.g., ?hi?, ?hello?) signifies the section (usu-
ally at the beginning) of an email that people use to
begin their email. Likewise, END (e.g., ?Cheers?,
?Best?) signifies the section (usually at the end) that
people use to end their email. The annotators car-
ried out the task on paper. We created the hierar-
chical thread view (?reply to? relation) using ?TAB?s
(indentation) and each participant?s name is printed
in a different color as in Gmail.
Table 1 shows some basic statistics computed on
the three annotations of the 39 email threads5. On
4The annotators in the pilot and in the actual study were dif-
ferent so we could reuse the threads used in pilot study. How-
ever, one thread on which the pilot annotators agree fully, was
used as an example in the instruction manual. This gives 39
threads left for the actual study.
5We got 100% agreement on the two predefined topics ?IN-
390
average we have 26.3 sentences and 2.5 topics per
thread. A topic contains an average of 12.6 sen-
tences. The average number of topics active at a
time is 1.4. The average entropy is 0.94 and cor-
responds (as described in detail in the next section)
to the granularity of the annotation. These statistics
(number of topics and topic density) indicate that the
dataset is suitable for topic segmentation.
Mean Max Min
Number of sentences 26.3 55 13
Number of topics 2.5 7 1
Avg. topic length 12.6 35 3
Avg. topic density 1.4 3.1 1
Entropy 0.94 2.7 0
Table 1: Corpus statistics of human annotations
Metrics Mean Max Min
1-to-1 0.804 1 0.31
lock 0.831 1 0.43
m-to-1 0.949 1 0.61
Table 2: Annotator agreement in the scale of 0 to 1
3.2 Evaluation Metrics
In this section we describe the metrics used to com-
pare different human annotations and system?s out-
put. As different annotations (or system?s output)
can group sentences in different number of clusters,
metrics widely used in classification, such as the ?
statistic, are not applicable. Again, our problem of
topic segmentation for emails is not sequential in na-
ture. Therefore, the standard metrics widely used in
sequential topic segmentation for monologues and
dialogs, such as Pk and WindowDiff(WD), are
also not applicable. We adopt the more appropri-
ate metrics 1-to-1, lock and m-to-1, introduced re-
cently by (Elsner and Charniak, 2008). The 1-to-1
metric measures the global similarity between two
annotations. It pairs up the clusters from the two
annotations in a way that maximizes (globally) the
total overlap and then reports the percentage of over-
lap. lock measures the local agreement within a con-
TRO? and ?END?. In all our computation (i.e., statistics, agree-
ment, system?s input) we excluded the sentences marked as ei-
ther ?INTRO? or ?END?
text of k sentences. To compute the loc3 metric for
the m-th sentence in the two annotations, we con-
sider the previous 3 sentences: m-1, m-2 and m-3,
and mark them as either ?same? or ?different? de-
pending on their topic assignment. The loc3 score
between two annotations is the mean agreement on
these ?same? or ?different? judgments, averaged over
all sentences. We report the agreement found in 1-
to-1 and lock in Table 2. In both of the metrics we
get high agreement, though the local agreement (av-
erage of 83%) is little higher than the global agree-
ment (average of 80%).
If we consider the topic of a randomly picked sen-
tence as a random variable then its entropy measures
the level of detail in an annotation. If the topics are
evenly distributed then the uncertainty (i.e., entropy)
is higher. It also increases with the increase of the
number of topics. Therefore, it is a measure of how
specific an annotator is and in our dataset it varies
from 0 6 to 2.7. To measure how much the annota-
tors agree on the general structure we use the m-to-1
metric. It maps each of the source clusters to the
single target cluster with which it gets the highest
overlap, then computes the total percentage of over-
lap. This metric is asymmetrical and not a measure
to be optimized7, but it gives us some intuition about
specificity (Elsner and Charniak, 2008). If one an-
notator divides a cluster into two clusters then, the
m-to-1 metric from fine to coarse is 1. In our corpus
by mapping from fine to coarse we get an m-to-1
average of 0.949.
4 Topic Segmentation Models
Developing automatic tools for segmenting an email
thread is challenging. The example email thread in
Figure 1 demonstrates why. We use different col-
ors and fonts to represent sentences of different top-
ics8. One can notice that email conversations are
different from written monologues (e.g., newspaper)
and dialogs (e.g., meeting, chat) in various ways.
As a communication media Email is distributed (un-
like face to face meeting) and asynchronous (unlike
60 uncertainty happens when there is only one topic found
7hence we do not use it to compare our models.
82 of the 3 annotators agree on this segmentation. Green rep-
resents topic 1 (?telecon cancellation?), orange indicates topic 2
(?TAG document?) and magenta represents topic 3 (?responding
to I18N?)
391
chat), meaning that different people from different
locations can collaborate at different times. There-
fore, topics in an email thread may not change in
sequential way. In the example, we see that topic 1
(i.e., ?telecon cancellation?) is revisited after some
gaps.
The headers (i.e., subjects) do not convey much
information and are often misleading. In the exam-
ple thread, participants use the same subject (i.e.,
20030220 telecon) but they talk about ?responding
to I18N? and ?TAG document? instead of ?telecon
cancellation?. Writing style varies among partici-
pants, and many people tend to use informal, short
and ungrammatical sentences. These properties of
email limit the application of techniques that have
been successful in monologues and dialogues.
LDA and LCSeg are the two state-of-the-art mod-
els for topic segmentation of multi-party conversa-
tion (e.g., (Galley et al, 2003), (Hsueh et al, 2006),
(Georgescul et al, 2008)). In this section, at first we
describe how the existing models of topic segmen-
tation can be applied to emails. We then point out
where these methods fail and propose extensions of
these basic models for email conversations.
4.1 Latent Dirichlet Allocation (LDA)
Our first model is the probabilistic LDA model
(Steyvers and Griffiths, 2007). This model relies on
the fundamental idea that documents are mixtures of
topics, and a topic is a multinomial distribution over
words. The generative topic model specifies the fol-
lowing distribution over words within a document:
P (wi) =
T?
j=1
P (wi|zi = j)P (zi = j)
Where T is the number of topics. P (wi|zi = j) is
the probability of word wi under topic j and P (zi =
j) is the probability that jth topic was sampled for
the ith word token. We refer the multinomial dis-
tributions ?(j) = P (w|zi = j) and ?(d) = P (z)
as topic-word distribution and document-topic dis-
tribution respectively. (Blei et al, 2003) refined this
basic model by placing a Dirichlet (?) prior on ?.
(Griffiths and Steyvers, 2003) further refined it by
placing a Dirichlet (?) prior on ?. The inference
problem is to find ? and ? given a document set.
Variational EM has been applied to estimate these
two parameters directly. Instead of estimating ? and
?, one can also directly estimate the posterior distri-
bution over z = P (zi = j|wi) (topic assignments
for words). One efficient estimation technique uses
Gibbs sampling to estimate this distribution.
This framework can be directly applied to an
email thread by considering each email as a doc-
ument. Using LDA we get z = P (zi = j|wi)
(i.e., topic assignments for words). By assuming the
words in a sentence occur independently we can esti-
mate the topic assignments for sentences as follows:
P (zi = j|sk) =
?
wi?sk
P (zi = j|wi)
where, sk is the kth sentence for which we can
assign the topic by: j? = argmaxjP (zi = j|sk).
4.2 Lexical Chain Segmenter (LCSeg)
Our second model is the lexical chain based seg-
menter LCSeg, (Galley et al, 2003). LCSeg as-
sumes that topic shifts are likely to occur where
strong term repetitions start and end9. LCSeg at first
computes ?lexical chains? for each non-stop word
based on word repetitions. It then ranks the chains
according to two measures: ?number of words in the
chain? and ?compactness of the chain?. The more
compact (in terms of number of sentences) and the
more populated chains get higher scores.
The algorithm then works with two adjacent anal-
ysis windows, each of a fixed size k which is em-
pirically determined. For each sentence boundary,
LCSeg computes the cosine similarity (or lexical co-
hesion function) at the transition between the two
windows. Low similarity indicates low lexical cohe-
sion, and a sharp change signals a high probability
of an actual topic boundary. This method is similar
to TextTiling (Hearst, 1997) except that the similar-
ity is computed based on the scores of the ?lexical
chains? instead of ?term counts?. In order to apply
LCSeg on email threads we arrange the emails based
on their temporal relation (i.e., arrival time) and ap-
ply the LCSeg algorithm to get the topic boundaries.
9One can also consider other lexical semantic relations (e.g.,
synonym, hypernym, hyponym) in lexical chaining. However,
Galley et al, (Galley et al, 2003) uses only repetition relation
as previous research results (e.g., (Choi, 2000)) account only
for repetition.
392
From: Brian To: rdf core Subject: 20030220 telecon Date: Tue Feb 17 13:52:15 
 	
		
	

Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 904?915, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Novel Discriminative Framework for Sentence-Level Discourse Analysis
Shafiq Joty and Giuseppe Carenini and Raymond T. Ng
{rjoty, carenini, rng}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
Abstract
We propose a complete probabilistic discrim-
inative framework for performing sentence-
level discourse analysis. Our framework com-
prises a discourse segmenter, based on a bi-
nary classifier, and a discourse parser, which
applies an optimal CKY-like parsing algo-
rithm to probabilities inferred from a Dynamic
Conditional Random Field. We show on two
corpora that our approach outperforms the
state-of-the-art, often by a wide margin.
1 Introduction
Automatic discourse analysis has been shown to
be critical in several fundamental Natural Lan-
guage Processing (NLP) tasks including text gener-
ation (Prasad et al2005), summarization (Marcu,
2000b), sentence compression (Sporleder and Lap-
ata, 2005) and question answering (Verberne et al
2007). Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988), one of the most influential
theories of discourse, posits a tree representation of
a discourse, known as a Discourse Tree (DT), as
exemplified by the sample DT shown in Figure 1.
The leaves of a DT correspond to contiguous atomic
text spans, also called Elementary Discourse Units
(EDUs) (three in the example). The adjacent EDUs
are connected by a rhetorical relation (e.g., ELAB-
ORATION), and the resulting larger text spans are
recursively also subject to this relation linking. A
span linked by a rhetorical relation can be either
a NUCLEUS or a SATELLITE depending on how
central the message is to the author. Discourse anal-
ysis in RST involves two subtasks: (i) breaking the
text into EDUs (known as discourse segmentation)
and (ii) linking the EDUs into a labeled hierarchical
tree structure (known as discourse parsing).
Figure 1: Discourse structure of a sentence in RST-DT.
Previous studies on discourse analysis have been
quite successful in identifying what machine learn-
ing approaches and what features are more useful for
automatic discourse segmentation and parsing (Sori-
cut and Marcu, 2003; Subba and Eugenio, 2009; du-
Verle and Prendinger, 2009). However, all the pro-
posed solutions suffer from at least one of the fol-
lowing two key limitations: first, they make strong
independence assumptions on the structure and the
labels of the resulting DT, and typically model the
construction of the DT and the labeling of the rela-
tions separately; second, they apply a greedy, sub-
optimal algorithm to build the structure of the DT.
In this paper, we propose a new sentence-level
discourse parser that addresses both limitations. The
crucial component is a probabilistic discriminative
parsing model, expressed as a Dynamic Conditional
Random Field (DCRF) (Sutton et al2007). By
representing the structure and the relation of each
discourse tree constituent jointly and by explicitly
capturing the sequential and hierarchical dependen-
cies between constituents of a discourse tree, our
DCRF model does not make any independence as-
sumption among these properties. Furthermore, our
904
parsing model supports a bottom-up parsing algo-
rithm which is non-greedy and provably optimal.
The discourse parser assumes that the input text
has been already segmented into EDUs. As an addi-
tional contribution of this paper, we propose a novel
discriminative approach to discourse segmentation
that not only achieves state-of-the-art performance,
but also reduces the time and space complexities by
using fewer features. Notice that the combination
of our segmenter with our parser forms a complete
probabilistic discriminative framework for perform-
ing sentence-level discourse analysis.
Our framework was tested in a series of experi-
ments. The empirical evaluation indicates that our
approach to discourse parsing outperforms the state-
of-the-art by a wide margin. Moreover, we show this
to be the case on two very different genres: news ar-
ticles and instructional how-to-do manuals.
In the rest of the paper, after discussing related
work, we present our discourse parser. Then, we
describe our segmenter. The experiments and the
corpora we used are described next, followed by a
discussion of the key results and some error analysis.
2 Related work
Automatic discourse analysis has a long history;
see (Stede, 2011) for a detailed overview. Sori-
cut and Marcu (2003) present the publicly available
SPADE1 system that comes with probabilistic mod-
els for sentence-level discourse segmentation and
parsing based on lexical and syntactic features de-
rived from the lexicalized syntactic tree of a sen-
tence. Their parsing algorithm finds the most proba-
ble DT for a sentence, where the probabilities of the
constituents are estimated by their parsing model.
A constituent (e.g., ATTRIBUTION-NS[(1,2),3] in
Figure 1) in a DT has two components, first, the la-
bel denoting the relation and second, the structure
indicating which spans are being linked by the rela-
tion. The nuclearity statuses of the spans are built
into the relation labels (e.g., NS[(1,2),3] means that
span (1,2) is the NUCLEUS and it comes before
span 3 which is the SATELLITE). SPADE is limited
in several ways. It makes an independence assump-
tion between the label and the structure while mod-
eling a constituent, and it ignores the sequential and
1http://www.isi.edu/licensed-sw/spade/
hierarchical dependencies between the constituents
in the parsing model. Furthermore, SPADE relies
only on lexico-syntactic features, and it follows a
generative approach to estimate the model param-
eters for the segmentation and the parsing models.
SPADE was trained and tested on the RST-DT cor-
pus (Carlson et al2002), which contains human-
annotated discourse trees for news articles.
Subsequent research addresses the question of
how much syntax one really needs in discourse
analysis. Sporleder and Lapata (2005) focus on
discourse chunking, comprising the two subtasks
of segmentation and non-hierarchical nuclearity as-
signment. More specifically, they examine whether
features derived via part of speech (POS) and chunk
taggers would be sufficient for these purposes. Their
results on RST-DT turn out to be comparable to
SPADE without using any features from the syntac-
tic tree. Later, Fisher and Roark (2007) demonstrate
over 4% absolute ?performance gain? in segmenta-
tion, by combining the features extracted from the
syntactic tree with the ones derived via taggers. Us-
ing quite a large number of features in a binary log-
linear model they achieve the state-of-the-art seg-
mentation performance on the RST-DT test set.
On the different genre of instructional manuals,
Subba and Eugenio (2009) propose a shift-reduce
parser that relies on a classifier to find the appro-
priate relation between two text segments. Their
classifier is based on Inductive Logic Programming
(ILP), which learns first-order logic rules from a
large set of features including the linguistically rich
compositional semantics coming from a semantic
parser. They show that the compositional seman-
tics improves the classification performance. How-
ever, their discourse parser implements a greedy ap-
proach (hence not optimal) and their classifier disre-
gards the sequence and hierarchical dependencies.
Using RST-DT, Hernault et al2010) present
the HILDA system that comes with a segmenter
and a parser based on Support Vector Machines
(SVMs). The segmenter is a binary SVM classi-
fier which relies on the same lexico-syntactic fea-
tures used in SPADE, but with more context. The
discourse parser builds a DT iteratively utilizing two
SVM classifiers in each iteration: (i) a binary classi-
fier decides which of the two adjacent spans to link,
and (ii) a multi-class classifier then connects the se-
905
lected spans with the appropriate relation. They use
a very large set of features in their parser. How-
ever, taking a radically-greedy approach, they model
structure and relations separately, and ignore the se-
quence dependencies in their models.
Recently, there has been an explosion of interest
in Conditional Random Fields (CRFs) (Lafferty et
al., 2001) for solving structured output classification
problems, with many successful applications in NLP
including syntactic parsing (Finkel et al2008), syn-
tactic chunking (Sha and Pereira, 2003) and dis-
course chunking (Ghosh et al2011) in Penn Dis-
course Treebank (Prasad et al2008). CRFs being a
discriminative approach to sequence modeling (i.e.,
directly models the conditional p(y|x,?)), have sev-
eral advantages over its generative counterparts such
as Hidden Markov Models (HMMs) and Markov
Random Fields (MRFs), which first model the joint
p(y, x|?), then infer the conditional p(y|x,?)). Key
advantages include the ability to incorporate arbi-
trary overlapping local and global features, and the
ability to relax strong independence assumptions. It
has been advocated that CRFs are generally more
accurate since they do not ?waste effort? modeling
complex distributions (i.e., p(x)) that are not rele-
vant for the target task (Murphy, 2012).
3 The Discourse Parser
Assuming that a sentence is already segmented into
a sequence of EDUs e1, e2, . . . en manually or by an
automatic segmenter (see Section 4), the discourse
parsing problem is to decide which spans to con-
nect (i.e., structure of the DT) and which relations
(i.e., labels of the internal nodes) to use in the pro-
cess of building the hierarchical DT. To build the
DTs effectively, a common assumption is that they
are binary trees (Soricut and Marcu, 2003; duVerle
and Prendinger, 2009). That is, multi-nuclear re-
lations (e.g., LIST, JOINT, SEQUENCE) involving
more than two EDUs are mapped to a hierarchi-
cal right-branching binary tree. For example, a flat
LIST (e1, e2, e3, e4) is mapped to a right-branching
binary tree LIST (e1, LIST (e2, LIST (e3, e4))).
Our discourse parser has two components. The
first component, the parsing model, assigns a proba-
bility to every possible DT. The second component,
the parsing algorithm, finds the most probable DT
among the candidate discourse trees.
3.1 Parsing Model
A DT can be represented as a set of constituents
of the form R[i,m, j], which denotes a rhetorical
relation R that holds between the span containing
EDUs i through m, and the span containing EDUs
m+1 through j. For example, the DT in Figure 1
can be written as {ELABORATION-NS[1,1,2],
ATTRIBUTION-NS[1,2,3]}. Notice that a rela-
tion R also indicates the nuclearity assignments
of the spans being connected, which can be one
of NUCLEUS-SATELLITE (NS), SATELLITE-
NUCLEUS (SN) and NUCLEUS-NUCLEUS (NN).
Given the model parameters ? and a candi-
date DT T , for all the constituents c in T , our
parsing model estimates the conditional probabil-
ity P (c|C,?), which specifies the joint probabil-
ity of the relation R and the structure [i,m, j]
associated with the constituent c, given that c
has a set of sub-constituents C. For instance,
for the DT shown in Figure 1, our model
would estimate P (R?[1, 1, 2]|?), P (R?[2, 2, 3]|?),
P (R?[1, 2, 3]|R??[1, 1, 2],?) etc. for all R? and R??
ranging on the set of relations. In what follows we
describe our probabilistic parsing model to compute
all these conditional probabilities P (c|C,?). We
will demonstrate how our approach not only models
the structure and the relation jointly, but it also cap-
tures linear sequence dependencies and hierarchical
dependencies between constituents of a DT.
Our novel parsing model is the Dynamic Condi-
tional Random Field (DCRF) (Sutton et al2007)
shown in Figure 2. A DCRF is a generalization
of linear-chain CRFs to represent complex interac-
tion between labels, such as when performing mul-
tiple labeling tasks on the same sequence. The ob-
served nodes Wj in the figure are the text spans.
A text span can be either an EDU or a concatena-
tion of a sequence of EDUs. The structure nodes
Sj?{0, 1} in the figure represent whether text spans
Wj?1 and Wj should be connected or not. The re-
lation nodes Rj?{1 . . .M} denote the discourse re-
lation between spans Wj?1 and Wj , given that M is
the total number of relations in our relation set. No-
tice that we now model the structure and the relation
jointly and also take the sequential dependencies be-
tween adjacent constituents into consideration.
906
Figure 2: A Dynamic CRF as a discourse parsing model.
We can obtain the conditional probabilities of
the constituents (i.e., P (c|C,?)) of all candidate
DTs for a sentence by applying the DCRF pars-
ing model recursively at different levels, and by
computing the posterior marginals of the relation-
structure pairs. To illustrate, consider the example
sentence in Figure 1 where we have three EDUs
e1, e2 and e3. The DCRF model for the first
level is shown in Figure 3(a), where the (observed)
EDUs are the spans in the span sequence. Given
this model, we obtain the probabilities of the con-
stituents R[1, 1, 2] and R[2, 2, 3] by computing the
posterior marginals P (R2, S2=1|e1, e2, e3,?) and
P (R3, S3=1|e1, e2, e3,?), respectively. At the sec-
ond level (see Figure 3(b)), there are two possi-
ble span sequences (e1:2, e3) and (e1, e2:3). In the
first sequence, EDUs e1 and e2 are linked into
a larger span, and in the second one, EDUs e2
and e3 are connected into a larger span. We ap-
ply our DCRF model to the two possible span se-
quences and obtain the probabilities of the con-
stituents R[1, 2, 3] and R[1, 1, 3] by computing
the posterior marginals P (R3, S3=1|e1:2, e3,?) and
P (R2:3, S2:3=1|e1, e2:3,?), respectively.
Figure 3: DCRF model applied to the sequences at differ-
ent levels in the example in Fig. 1. (a) A sequence at the
first level (b) Two possible sequences at the second level.
To further clarify the process, let us as-
sume that the sentence contains four EDUs
e1, e2, e3 and e4. At the first level (Fig-
ure 4(a)), there is only one possible span se-
quence to which we apply our DCRF model.
We obtain the probabilities of the constituents
R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4] by computing the
posterior marginals P (R2, S2=1|e1, e2, e3, e4,?),
P (R3, S3=1|e1, e2, e3, e4,?) and P (R4, S4=1|e1,
e2, e3, e4,?), respectively. At the second level
(Figure 4(b)), there are three possible sequences
(e1:2, e3, e4), (e1, e2:3, e4) and (e1, e2, e3:4). When
the DCRF model is applied to the sequence
(e1:2, e3, e4), we obtain the probabilities of the
constituent R[1, 2, 3] by computing the posterior
marginal P (R3, S3=1|e1:2, e3, e4,?). Likewise, the
posterior marginals P (R2:3, S2:3=1|e1, e2:3, e4,?)
and P (R4, S4=1|e1, e2:3, e4,?) in the DCRF model
applied to the sequence (e1, e2:3, e4) represents
the probabilities of the constituents R[1, 1, 3]
and R[2, 3, 4], respectively. Similarly, we at-
tain the probabilities of the constituent R[2, 2, 4]
from the DCRF model applied to the sequence
(e1, e2, e3:4) by computing the posterior marginal
P (R3:4, S3:4=1|e1, e2, e3:4,?). At the third level
(Figure 4(c)), there are three possible sequences
(e1:3, e4), (e1, e2:4) and (e1:2, e3:4), to which we ap-
ply our model and acquire the probabilities of the
constituents R[1, 3, 4], R[1, 1, 4] and R[1, 2, 4] by
computing their respective posterior marginals.
Figure 4: DCRF model applied to the sequences at differ-
ent levels of a discourse tree. (a) A sequence at the first
level, (b) Three possible sequences at the second level,
(c) Two possible sequences at the third level.
Our DCRF model is designed using MALLET
(McCallum, 2002). In order to avoid overfitting we
regularize the DCRF model with l2 regularization
and learn the model parameters using the limited-
memory BFGS (L-BFGS) fitting algorithm. Since
exact inference can be intractable in DCRF models,
907
we perform approximate inference (to compute the
posterior marginals) using tree-based reparameteri-
zation (Wainwright et al2002).
3.1.1 Features Used in the Parsing Model
Crucial to parsing performance is the set of fea-
tures used, as summarized in Table 1. Note that
these features are defined on two consecutive spans
Wj?1 and Wj of a span sequence. Most of the fea-
tures have been explored in previous studies. How-
ever, we improve some of these as explained below.
Organizational features encode useful informa-
tion about the surface structure of a sentence as
shown by (duVerle and Prendinger, 2009). We mea-
sure the length of the spans in terms of the number of
EDUs and tokens in it. However, in order to better
adjust to the length variations, rather than comput-
ing their absolute numbers in a span, we choose to
measure their relative numbers with respect to their
total numbers in the sentence. For example, in a sen-
tence containing three EDUs, a span containing two
of these EDUs will have a relative EDU number of
0.67. We also measure the distances of the spans
from the beginning and to the end of the sentence in
terms of the number of EDUs.
8 organizational features
Relative number of EDUs in span 1 and span 2.
Relative number of tokens in span 1 and span 2.
Distances of span 1 in EDUs to the beginning and to the end.
Distances of span 2 in EDUs to the beginning and to the end.
8 N-gram features
Beginning and end lexical N-grams in span 1.
Beginning and end lexical N-grams in span 2.
Beginning and end POS N-grams in span 1.
Beginning and end POS N-grams in span 2.
5 dominance set features
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two text spans.
2 contextual features
Previous and next feature vectors.
2 substructure features
Root nodes of the left and right rhetorical subtrees.
Table 1: Features used in the DCRF parsing model.
Discourse connectives (e.g., because, but), when
present, signal rhetorical relations between two text
segments (Knott and Dale, 1994; Marcu, 2000a).
However, previous studies (e.g., Hernault et al
(2010), Biran and Rambow (2011)) suggest that an
empirically acquired lexical N-gram dictionary is
more effective than a fixed list of connectives, since
this approach is domain independent and capable
of capturing non-lexical cues such as punctuations.
To build the lexical N-gram dictionary empirically
from the training corpus we consider the first and
last N tokens (N?{1, 2}) of each span and rank
them according to their mutual information2 with
the two labels, Structure and Relation. Intuitively,
the most informative cues are not only the most fre-
quent, but also the ones that are indicative of the la-
bels in the training data (Blitzer, 2008). In addition
to the lexical N-grams we also encode POS tags of
the first and last N tokens (N?{1, 2}) as features.
Figure 5: A discourse segmented lexicalized syntactic
tree. Boxed nodes form the dominance set D.
Dominance set extracted from the Discourse Seg-
mented Lexicalized Syntactic Tree (DS-LST) (Sori-
cut and Marcu, 2003) has been shown to be a very
effective feature in SPADE. Figure 5 shows the DS-
LST for our running example (see Figure 1 and 3).
In a DS-LST, each EDU except the one with the root
node must have a head node NH that is attached to
an attachment node NA residing in a separate EDU.
A dominance set D (shown at the bottom of Figure 5
for our example) contains these attachment points of
the EDUs in a DS-LST. In addition to the syntactic
and lexical information of the head and attachment
nodes, each element in D also represents a domi-
nance relationship between the EDUs involved. The
EDU with NA dominates the EDU with NH . In or-
2In contrast, HILDA ranks the N-grams by frequencies.
908
der to extract dominance set features for two consec-
utive spans ei:j and ej+1:k, we first compute D from
the DS-LST of the sentence. We then extract the
element from D that holds across the EDUs j and
j + 1. In our running example, for the spans e1 and
e2 (Figure 3(a)), the relevant dominance set element
is (1, efforts/NP)>(2, to/S). We encode the syntac-
tic labels and lexical heads of NH and NA and the
dominance relationship (i.e., which of the two spans
is dominating) as features in our model.
We also incorporate more contextual information
by including the above features computed for the
neighboring span pairs in the current feature vector.
We incorporate hierarchical dependencies be-
tween constituents in a DT by means of the sub-
structure features. For the two adjacent spans ei:j
and ej+1:k, we extract the roots of the rhetorical
subtrees spanning over ei:j (left) and ej+1:k (right).
In our example (see Figure 1 and Figure 3 (b)),
the root of the rhetorical subtree spanning over e1:2
is ELABORATION-NS. However, this assumes the
presence of a labeled DT which is not the case when
we apply the parser to a new sentence. This problem
can be easily solved by looping twice through build-
ing the model and the parsing algorithm (described
below). We first build the model without considering
the substructure features. Then we find the optimal
DT employing our parsing algorithm. This interme-
diate DT will now provide labels for the substruc-
tures. Next we can build a new, more accurate model
by including the substructure features, and run again
the parsing algorithm to find the final optimal DT.
3.2 Parsing Algorithm
Our parsing model above assigns a conditional prob-
ability to every possible DT constituent for a sen-
tence, the job of the parsing algorithm is to find the
most probable DT. Formally, this can be written as,
DT ? = argmax DTP (DT |?)
Our discourse parser implements a probabilistic
CKY-like bottom-up algorithm for computing the
most likely parse of a sentence using dynamic pro-
gramming; see (Jurafsky and Martin, 2008) for a
description. Specifically, with n number of EDUs
in a sentence, we use the upper-triangular por-
tion of the n ? n Dynamic Programming Table
(DPT). The cell [i, j] in the DPT represents the
span containing EDUs i through j and stores the
probability of a constituent R[i,m, j], where m =
argmax i?k?jP (R[i, k, j]).
In contrast to HILDA which implements a greedy
algorithm, our approach finds a DT that is glob-
ally optimal. Our approach is also different from
SPADE?s implementation. SPADE first finds the
tree structure that is globally optimal, then it assigns
the most probable relations to the internal nodes.
More specifically, the cell [i, j] in SPADE?s DPT
stores the probability of a constituent R[i,m, j],
where m = argmax i?k?jP ([i, k, j]). Disregard-
ing the relation label R while building the DPT, this
approach may find a tree that is not globally optimal.
4 The Discourse Segmenter
Our discourse parser above assumes that the input
sentences have been already segmented into EDUs.
Since it has been shown that discourse segmentation
is a primary source of inaccuracy for discourse pars-
ing (Soricut and Marcu, 2003), we have developed
our own segmenter, that not only achieves state-of-
the-art performance as shown later, but also reduces
the time complexity by using fewer features.
Our segmenter implements a binary classifier to
decide for each word (except the last word) in a sen-
tence, whether to put an EDU boundary after that
word. We use a Logistic Regression (LR) (i.e., dis-
criminative) model with l2 regularization and learn
the model parameters using the L-BFGS algorithm,
which gives quadratic convergence rate. To avoid
overfitting, we use 5-fold cross validation to learn
the regularization strength parameter from the train-
ing data. We also use a simple bagging technique
(Breiman, 1996) to deal with the sparsity of bound-
ary tags. Note that, our first attempt at this task im-
plemented a linear-chain CRF model to capture the
sequence dependencies between the tags in a dis-
criminative way. However, the binary LR classifier,
using the same features, not only outperforms the
CRF model, but also reduces the space complexity.
4.1 Features Used in the Segmentation Model
Our set of features for discourse segmentation are
mostly inspired from previous studies but used in a
novel way as we describe below.
Our first subset of features which we call SPADE
features, includes the lexico-syntactic patterns ex-
909
tracted from the lexicalized syntactic tree for the
given sentence. These features replicates the fea-
tures used in SPADE, but used in a discriminative
way. To decide on an EDU boundary after a token
wk, we find the lowest constituent in the lexicalized
syntactic tree that spans over tokens wi . . . wj such
that i?k<j. The production that expands this con-
stituent in the tree and its different variations, form
the feature set. For example in Figure 5, the produc-
tion NP(efforts)?PRP$(its)NNS(efforts)?S(to) and
its different variations depending on whether they
include the lexical heads and how many non-
terminals (up to two) to consider before and after
the potential EDU boundary (?), are used to de-
termine the existence of a boundary after the word
efforts (see (Fisher and Roark, 2007) for details).
SPADE uses these features in a generative way,
meaning that, it inserts an EDU boundary if the rela-
tive frequency (i.e., Maximum Likelihood Estimate
(MLE)) of a potential boundary given the production
in the training corpus is greater than 0.5. If the pro-
duction has not been observed frequently enough, it
uses its other variations to perform further smooth-
ing. In contrast, we compute the MLE estimates for
a production and its other variations, and use those
as features with/without binarizing the values.
Shallow syntactic parse (or Chunk) and POS tags
have been shown to possess valuable cues for dis-
course segmentation (Fisher and Roark, 2007). For
example, it is less likely that an EDU boundary oc-
curs within a chunk. We, therefore, annotate the to-
kens of a sentence with chunk and POS tags by a
state-of-the-art tagger3 and encode these as features.
EDUs are normally multi-word strings. Thus, a
token near the beginning or end of a sentence is un-
likely to be the end of a segment. Therefore, for each
token we include its relative position in the sentence
and distances to the beginning and end as features.
It is unlikely that two consecutive tokens are
tagged with EDU boundaries. We incorporate con-
textual information for a token by including the
above features computed for its neighboring tokens.
We also experimented with different N-gram
(N?{1, 2, 3}) features extracted from the token se-
quence, POS sequence and chunk sequence. How-
ever, since such features did not improve the seg-
3http://cogcomp.cs.illinois.edu/page/software
mentation accuracy on the development set, they
were excluded from our final set of features.
5 Experiments
5.1 Corpora
To demonstrate the generality of our model, we ex-
periment with two different genres. First, we use the
standard RST-DT corpus (Carlson et al2002) that
contains discourse annotations for 385 Wall Street
Journal news articles from the Penn Treebank (Mar-
cus et al1994). Second, we use the Instructional
corpus developed by Subba and Eugenio (2009) that
contains discourse annotations for 176 instructional
how-to-do manuals on home-repair.
The RST-DT corpus is partitioned into a training
set of 347 documents (7673 sentences) and a test set
of 38 documents (991 sentences), and 53 documents
(1208 sentences) have been (doubly) annotated by
two human annotators, based on which we compute
the human agreement. We use the human-annotated
syntactic trees from Penn Treebank to train SPADE
in our experiments using RST-DT as done in (Sori-
cut and Marcu, 2003). We extracted a sentence-level
DT from a document-level DT by finding the subtree
that exactly spans over the sentence. By our count,
7321 sentences in the training set, 951 sentences
in the test set and 1114 sentences in the doubly-
annotated set have a well-formed DT in RST-DT.
The Instructional corpus contains 3430 sentences in
total, out of which 3032 have a well-formed DT.
This forms our sentence-level corpora for discourse
parsing. However, the existence of a well-formed
DT in not a necessity for discourse segmentation,
therefore, we do not exclude any sentence in our dis-
course segmentation experiments.
5.2 Experimental Setup
We perform our experiments on discourse pars-
ing in RST-DT with the 18 coarser relations (see
Figure 6) defined in (Carlson and Marcu, 2001)
and also used in SPADE and HILDA. By attach-
ing the nuclearity statuses (i.e., NS, SN, NN) to
these relations we get 39 distinct relations4. Our
experiments on the Instructional corpus consider
the same 26 primary relations (e.g., GOAL:ACT,
CAUSE:EFFECT, GENERAL-SPECIFIC) used in
4Not all relations take all the possible nuclearity statuses.
910
(Subba and Eugenio, 2009) and also treat the re-
versals of non-commutative relations as separate re-
lations. That is, PREPARATION-ACT and ACT-
PREPARATION are two different relations. Attach-
ing the nuclearity statuses to these relations gives 70
distinct relations in the Instructional corpus.
We use SPADE as our baseline model and apply
the same modifications to its default setting as de-
scribed in (Fisher and Roark, 2007), which delivers
improved performance. Specifically, in testing, we
replace the Charniak parser (Charniak, 2000) with a
more accurate reranking parser (Charniak and John-
son, 2005). We use the reranking parser in all our
models to generate the syntactic trees. This parser
was trained on the sections of the Penn Treebank not
included in the test set. For a fair comparison, we ap-
ply the same canonical lexical head projection rules
(Magerman, 1995; Collins, 2003) to lexicalize the
syntactic trees as done in SPADE and HILDA. Note
that, all the previous works described in Section 2,
report their models? performance on a particular test
set of a specific corpus. To compare our results with
the previous studies, we test our models on those
specific test sets. In addition, we show more general
performance based on 10-fold cross validation.
5.3 Parsing based on Manual Segmentation
First, we present the results of our discourse parser
based on manual segmentation. The parsing perfor-
mance is assessed using the unlabeled (i.e., span)
and labeled (i.e., nuclearity, relation) precision, re-
call and F-score as described in (Marcu, 2000b, page
143). For brevity, we report only the F-scores in Ta-
ble 2. Notice that, our parser (DCRF) consistently
outperforms SPADE (SP) on the RST-DT test set5.
Especially, on relation labeling, which is the hardest
among the three tasks, we get an absolute F-score
improvement of 9.5%, which represents a relative
error rate reduction of 29.3%. Our F-score of 77.1
in relation labeling is also close to the human agree-
ment (i.e., F-score of 83.0) on the doubly-annotated
data. Our results on the RST-DT test set are con-
sistent with the mean scores over 10-folds, when we
perform 10-fold cross validation on RST-DT.
The improvement is even larger on the Instruc-
tional corpus, where we compare our mean results
5The improvements are statistically significant (p < 0.01).
over 10-folds with the results reported in Subba and
Eugenio (S&E) (2009) on a test set6, giving ab-
solute F-score improvements of 4.8%, 15.5% and
10.6% in span, nuclearity and relations, respectively.
Our parser reduces the errors by 67.6%, 54.6% and
28.6% in span, nuclearity and relations, respectively.
RST-DT Instructional
Test set 10-fold Doubly S&E 10-fold
Scores SP DCRF DCRF Human ILP DCRF
Span 93.5 94.6 93.7 95.7 92.9 97.7
Nuc. 85.8 86.9 85.2 90.4 71.8 87.2
Rel. 67.6 77.1 75.4 83.0 63.0 73.6
Table 2: Parsing results using manual segmentation.
If we compare the performance of our model on
the two corpora, we see that our model is more accu-
rate in finding the right tree structure (see Span) on
the Instructional corpus. This may be due to the fact
that sentences in the Instructional domain are rela-
tively short and contain fewer EDUs than sentences
in the News domain, thus making it easier to find
the right tree structure. However, when we compare
the performance on the relation labeling task, we ob-
serve a decrease on the Instructional corpus. This
may be due to the small amount of data available for
training and the imbalanced distribution of a large
number of discourse relations in this corpus.
To analyze the features, Table 3 presents the pars-
ing results on the RST-DT test set using different
subsets of features. Every new subset of features
appears to improve the accuracy. More specifically,
when we add the organizational features with the
dominance set features (see S2), we get about 2%
absolute improvement in nuclearity and relations.
With N-gram features (S3), the gain is even higher;
6% in relations and 3.5% in nuclearity, demonstrat-
ing the utility of the N-gram features. This is con-
sistent with the findings of (duVerle and Prendinger,
2009; Schilder, 2002). Including the Contextual fea-
tures (S4), we get further 3% and 2.2% improve-
ments in nuclearity and relations, respectively. No-
tice that, adding the substructure features (S5) does
not help much in sentence-level parsing, giving only
6Subba and Eugenio (2009) report their results based on an
arbitrary split between a training set and a test set. We asked the
authors for their particular split. However, since we could not
obtain that information, we compare our model?s performance
based on 10-fold cross validation with their reported results.
911
an improvement of 0.8% in relations. Therefore, one
may choose to avoid using this computationally ex-
pensive feature in time-constrained scenarios. How-
ever, in the future, it will be interesting to see its im-
portance in document-level parsing with large trees.
Scores S1 S2 S3 S4 S5
Span 91.3 92.1 93.3 94.6 94.6
Nuclearity 78.2 80.3 83.8 86.8 86.9
Relation 66.2 68.1 74.1 76.3 77.1
Table 3: Parsing results based on manual segmentation
using different subsets of features on RST-DT test set.
Feature subsets S1 = {Dominance set}, S2 = {Dominance
set, Organizational}, S3 = {Dominance set, Organiza-
tional, N-gram}, S4 = {Dominance set, Organizational,
N-gram, Contextual}, S5 (all) = {Dominance set, Orga-
nizational, N-gram, Contextual, Substructure}.
5.4 Evaluation of the Discourse Segmenter
We evaluate the segmentation accuracy with respect
to the intra-sentential segment boundaries following
(Fisher and Roark, 2007). Specifically, if a sen-
tence contains n EDUs, which corresponds to n? 1
intra-sentence segment boundaries, we measure the
model?s ability to correctly identify these n ? 1
boundaries. Human agreement for this task is quite
high (F-score of 98.3) on RST-DT.
Table 4 shows the results of different models in
(P)recision, (R)ecall, and (F)-score on the two cor-
pora. We compare our model?s (LR) results with
HILDA (HIL), SPADE (SP) and the results reported
in Fisher and Roark (F&R) (2007) on the RST-DT
test set. HILDA gives the weakest performance7.
Our results are also much better than SPADE8, with
an absolute F-score improvement of 4.9%, and com-
parable to the results of F&R, even though we use
fewer features. Furthermore, we perform 10-fold
cross validation on both corpora and compare with
SPADE. However, SPADE does not come with a
training module for its segmenter. We reimple-
mented this module and verified it on the RST-DT
test set. Due to the lack of human-annotated syntac-
tic trees in the Instructional corpus, we train SPADE
in this corpus using the syntactic trees produced
7Note that, the high segmentation accuracy reported in (Her-
nault et al2010) is due to a less stringent evaluation metric.
8The improvements are statistically significant (p<2.4e-06)
by the reranking parser. Our model delivers abso-
lute F-score improvements of 3.8% and 8.1% on the
RST-DT and the Instructional corpora, respectively,
which is statistically significant in both cases (p <
3.0e-06). However, when we compare our results on
the two corpora, we observe a substantial decrease in
performance on the Instructional corpus. This could
be due to a smaller amount of data in this corpus and
the inaccuracies in the syntactic parser and taggers,
which are trained on news articles.
RST-DT Instructional
Test Set 10-fold 10-fold 10-fold
HIL SP F&R LR SP LR SP LR
P 77.9 83.8 91.3 88.0 83.7 87.5 65.1 73.9
R 70.6 86.8 89.7 92.3 86.2 89.9 82.8 89.7
F 74.1 85.2 90.5 90.1 84.9 88.7 72.8 80.9
Table 4: Segmentation results of different models.
5.5 Parsing based on Automatic Segmentation
In order to evaluate our full system, we feed our
discourse parser the output of our discourse seg-
menter. Table 5 shows the F-score results. We com-
pare our results with SPADE on the RST-DT test set.
We achieve absolute F-score improvements of 3.6%,
3.4% and 7.4% in span, nuclearity and relation, re-
spectively. These improvements are statistically sig-
nificant (p<0.001). Our system, therefore, reduces
the errors by 15.5%, 11.4%, and 17.6% in span, nu-
clearity and relations, respectively. These results are
also consistent with the mean results over 10-folds.
RST-DT Instructional
Test set 10-fold 10-fold
Scores SPADE DCRF DCRF DCRF
Span 76.7 80.3 78.7 71.9
Nuclearity 70.2 73.6 72.2 64.3
Relation 58.0 65.4 64.2 54.8
Table 5: Parsing results using automatic segmentation.
For the Instructional corpus, the last column of
Table 5 shows the mean 10-fold cross validation re-
sults. We cannot compare with S&E because no re-
sults were reported using an automatic segmenter.
However, it is interesting to observe how much our
full system is affected by an automatic segmenter
on both RST-DT and the Instructional corpus (see
Table 2 and Table 5). Nevertheless, taking into ac-
count the segmentation results in Table 4, this is
912
not surprising because previous studies (Soricut and
Marcu, 2003) have already shown that automatic
segmentation is the primary impediment to high ac-
curacy discourse parsing. This demonstrates the
need for a more accurate segmentation model in the
Instructional genre. A promising future direction
would be to apply effective domain adaptation meth-
ods (e.g., easyadapt (Daume, 2007)) to improve
the segmentation performance in the Instructional
domain by leveraging the rich data in RST-DT.
5.6 Error Analysis and Discussion
The results in Table 2 suggest that given a manually
segmented discourse, our sentence-level discourse
parser finds the unlabeled (i.e., span) discourse tree
and assigns the nuclearity statuses to the spans at a
performance level close to human annotators. We,
therefore, look more closely into the performance of
our parser on the hardest task of relation labeling.
Figure 6 shows the confusion matrix for the rela-
tion labeling task using manual segmentation on the
RST-DT test set. The relation labels are ordered ac-
cording to their frequency in the RST-DT training
set and represented by their initial letters. For exam-
ple, EL represents ELABORATION and CA repre-
sents CAUSE. In general, errors can be explained by
two different phenomena acting together: (i) the fre-
quency of the relations in the training data, and (ii)
the semantic (or pragmatic) similarity between the
relations. The most frequent relations (e.g., ELAB-
ORATION) tend to confuse the less frequent ones
(e.g., SUMMARY), and the relations which are se-
mantically similar (e.g., CAUSE, EXPLANATION)
confuse each other, making it hard to distinguish for
the computational models. Notice that, the confu-
sions caused by JOINT appears to be high consid-
ering its frequency. The confusion between JOINT
and TEMPORAL may be due to the fact that both of
these coarser relations9 contain finer relations (i.e.,
list in JOINT and sequence in TEMPORAL), which
are semantically similar, as pointed out by Carlson
and Marcu (2001). The confusion between JOINT
and BACKGROUND may be explained by their dif-
ferent (semantic vs. pragmatic) interpretation in the
RST theory (Stede, 2011, page 85).
9JOINT is actually not a relation, but is characterized by
juxtaposition of two EDUs without a relation.
Figure 6: Confusion matrix for the relation labels on
the RST-DT test set. Y-axis represents true and X-axis
represents predicted labels. The relation labels are TOPIC-
COMMENT, EVALUATION, SUMMARY, MANNER-MEANS,
COMPARISON, EXPLANATION, CONDITION, TEMPORAL,
CAUSE, ENABLEMENT, BACKGROUND, CONTRAST, JOINT,
SAME-UNIT, ATTRIBUTION, ELABORATION.
Based on these observations we will pursue two
ways to improve our discourse parser. We need a
more robust (e.g., bagging) method to deal with the
imbalanced distribution of relations, along with a
better representation of semantic knowledge. For
example, compositional semantics (Subba and Eu-
genio, 2009) and subjectivity (Somasundaran, 2010)
can be quite relevant for identifying relations.
6 Conclusion
In this paper, we have described a complete prob-
abilistic discriminative framework for performing
sentence-level discourse analysis. Experiments indi-
cate that our approach outperforms the state-of-the-
art on two corpora, often by a wide margin.
In ongoing work, we plan to generalize our
DCRF-based parser to multi-sentential text and also
verify to what extent parsing and segmentation can
be jointly performed. A longer term goal is to extend
our framework to also work with graph structures
of discourse, as recommended by several recent dis-
course theories (Wolf and Gibson, 2005). Once we
achieve similar performance on graph structures, we
will perform extrinsic evaluation to determine their
relative utility for various NLP tasks.
Acknowledgments
We are grateful to G. Murray, J. CK Cheung, the 3
reviewers and the NSERC CGS-D award.
913
References
Or Biran and Owen Rambow. 2011. Identifying Justifi-
cations in Written Dialogs by Classifying Text as Ar-
gumentative. Int. J. Semantic Computing, 5(4):363?
381.
J. Blitzer, 2008. Domain Adaptation of Natural Lan-
guage Processing Systems. PhD thesis, University of
Pennsylvania.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140, August.
L. Carlson and D. Marcu. 2001. Discourse Tagging Ref-
erence Manual. Technical Report ISI-TR-545, Univer-
sity of Southern California Information Sciences Insti-
tute.
L. Carlson, D. Marcu, and M. Okurowski. 2002. RST
Discourse Treebank (RST-DT) LDC2002T07. Lin-
guistic Data Consortium, Philadelphia.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 173?
180, NJ, USA. ACL.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the 1st North American
Chapter of the Association for Computational Linguis-
tics Conference, pages 132?139, Seattle, Washington.
ACL.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics, 29(4):589?637, December.
H. Daume. 2007. Frustratingly Easy Domain Adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
256?263, Prague, Czech Republic. ACL.
D. duVerle and H. Prendinger. 2009. A Novel Discourse
Parser based on Support Vector Machine Classifica-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 665?673, Suntec, Singapore.
ACL.
J. Finkel, A. Kleeman, and C. Manning. 2008. Efficient,
Feature-based, Conditional Random Field Parsing. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 959?967,
Columbus, Ohio, USA. ACL.
S. Fisher and B. Roark. 2007. The Utility of Parse-
derived Features for Automatic Discourse Segmenta-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
488?495, Prague, Czech Republic. ACL.
S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli.
2011. Shallow Discourse Parsing with Conditional
Random Fields. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1071?1079, Chiang Mai, Thailand. AFNLP.
H. Hernault, H. Prendinger, D. duVerle, and M. Ishizuka.
2010. HILDA: A Discourse Parser Using Support
Vector Machine Classification. Dialogue and Dis-
course, 1(3):1?33.
D. Jurafsky and J. Martin, 2008. Speech and Language
Processing, chapter 14. Prentice Hall.
A. Knott and R. Dale. 1994. Using Linguistic Phenom-
ena to Motivate a Set of Coherence Relations. Dis-
course Processes, 18(1):35?62.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceedings
of the Eighteenth International Conference on Ma-
chine Learning, pages 282?289, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
D. Magerman. 1995. Statistical Decision-tree Mod-
els for Parsing. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 276?283, Cambridge, Massachusetts. ACL.
W. Mann and S. Thompson. 1988. Rhetorical Structure
Theory: Toward a Functional Theory of Text Organi-
zation. Text, 8(3):243?281.
D. Marcu. 2000a. The Rhetorical Parsing of Unrestricted
Texts: A Surface-based Approach. Computational
Linguistics, 26:395?448.
D. Marcu. 2000b. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press, Cambridge,
MA, USA.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313?330.
A. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
K. Murphy. 2012. Machine Learning A Probabilistic
Perspective (Forthcoming, August 2012). MIT Press,
Cambridge, MA, USA.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and
B. Webber. 2005. The Penn Discourse TreeBank as a
Resource for Natural Language Generation. In Pro-
ceedings of the Corpus Linguistics Workshop on Us-
ing Corpora for Natural Language Generation, pages
25?32, Birmingham, U.K.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The Penn Discourse
TreeBank 2.0. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 2961?2968, Marrakech, Mo-
rocco. ELRA.
914
F. Schilder. 2002. Robust Discourse Parsing via Dis-
course Markers, Topicality and Position. Natural Lan-
guage Engineering, 8(3):235?255, June.
F. Sha and F. Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, pages 134?141, Ed-
monton, Canada. ACL.
S. Somasundaran, 2010. Discourse-Level Relations for
Opinion Analysis. PhD thesis, University of Pitts-
burgh.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing Using Syntactic and Lexical Informa-
tion. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 149?156, Edmonton, Canada.
ACL.
C. Sporleder and M. Lapata. 2005. Discourse Chunk-
ing and its Application to Sentence Compression. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 257?264, Vancouver, British
Columbia, Canada. ACL.
M. Stede. 2011. Discourse Processing. Synthesis Lec-
tures on Human Language Technologies. Morgan And
Claypool Publishers, November.
R. Subba and B. Di Eugenio. 2009. An Effective
Discourse Parser that Uses Rich Linguistic Informa-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 566?574, Boulder, Colorado. ACL.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting Se-
quence Data. Journal of Machine Learning Research
(JMLR), 8:693?723.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating Discourse-based Answer Extrac-
tion for Why-question Answering. In Proceedings of
the 30th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 735?736, Amsterdam, The Netherlands. ACM.
M. Wainwright, T. Jaakkola, and A. Willsky. 2002. Tree-
based Reparameterization for Approximate Inference
on Loopy Graphs. In Advances in Neural Information
Processing Systems 14, pages 1001?1008. MIT Press.
F. Wolf and E. Gibson. 2005. Representing Discourse
Coherence: A Corpus-Based Study. Computational
Linguistics, 31:249?288, June.
915
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1169?1180,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Detecting Disagreement in Conversations using Pseudo-Monologic
Rhetorical Structure
Kelsey Allen Giuseppe Carenini Raymond T. Ng
Department of Computer Science, University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
{kelseyra, carenini, rng}@cs.ubc.ca
Abstract
Casual online forums such as Reddit,
Slashdot and Digg, are continuing to in-
crease in popularity as a means of com-
munication. Detecting disagreement in
this domain is a considerable challenge.
Many topics are unique to the conversa-
tion on the forum, and the appearance
of disagreement may be much more sub-
tle than on political blogs or social me-
dia sites such as twitter. In this analy-
sis we present a crowd-sourced annotated
corpus for topic level disagreement detec-
tion in Slashdot, showing that disagree-
ment detection in this domain is difficult
even for humans. We then proceed to
show that a new set of features determined
from the rhetorical structure of the con-
versation significantly improves the per-
formance on disagreement detection over
a baseline consisting of unigram/bigram
features, discourse markers, structural fea-
tures and meta-post features.
1 Introduction
How does disagreement arise in conversation? Be-
ing able to detect agreement and disagreement has
a range of applications. For an online educator,
dissent over a newly introduced topic may alert
the teacher to fundamental misconceptions about
the material. For a business, understanding dis-
putes over features of a product may be helpful in
future design iterations. By better understanding
how debate arises and propagates in a conversa-
tion, we may also gain insight into how authors?
opinions on a topic can be influenced over time.
The long term goal of our research is to lay
the foundations for understanding argumentative
structure in conversations, which could be applied
to NLP tasks such as summarization, information
retrieval, and text visualization. Argumentative
structure theory has been thoroughly studied in
both psychology and rhetoric, with negation and
discourse markers, as well as hedging and dis-
preferred responses, being known to be indicative
of argument (Horn, 1989; Brown and Levinson,
1987). As a starting point, in this paper we focus
on the detection of disagreement in casual con-
versations between users. This requires a gener-
alized approach that can accurately identify dis-
agreement in topics ranging from something as
mundane as whether GPS stands for galactic po-
sitioning system or global positioning system, to
more ideological debates about distrust in science.
Motivated by the widespread consensus in both
computational and theoretical linguistics on the
utility of discourse markers for signalling prag-
matic functions such as disagreement and personal
opinions (Webber and Prasad, 2008; Abbott et
al., 2011; J. E. Fox-Tree, 2010), we introduce a
new set of features based on the Discourse Tree
(DT) of a conversational text. Discourse Trees
were formalized by Mann and Thompson (1988)
as part of their Rhetorical Structure Theory (RST)
to represent the structure of discourse. Although
this theory is for monologic discourse, we propose
to treat conversational dialogue as a collection of
linked monologues, and subsequently build a rela-
tion graph describing both rhetorical connections
within user posts, as well as between different
users. Features obtained from this graph offer sig-
nificant improvements on disagreement detection
over a baseline consisting of meta-post features,
lexical features, discourse markers and conversa-
tional features. Not only do these features improve
disagreement detection, but the discovered impor-
tance of relations known to be theoretically rele-
vant to disagreement detection, such as COMPAR-
ISON (Horn, 1989), suggest that this approach may
be capturing the essential aspects of the conversa-
tional argumentative structure.
1169
As a second contribution of this work, we pro-
vide a new dataset consisting of 95 topics anno-
tated for disagreement. Unlike the publicly avail-
able ARGUE corpus based on the online debate
forum 4forums.com (Abbott et al., 2011), our cor-
pus is based on Slashdot, which is a general pur-
pose forum not targeted to debates. Therefore, we
expect that detecting disagreement may be a more
difficult task in our new corpus, as certain topics
(like discussing GPS systems) may be targeted to-
wards objective information sharing without any
participants expressing opinions or stances. Be-
cause of this, our corpus represents an excellent
testbed to examine methods for more subtle dis-
agreement detection, as well as the major differ-
ences between news-style and argument-style dia-
logue.
2 Related Work
In the past decade, there have been a number of
computational approaches developed for the task
of disagreement and controversy detection, partic-
ularly in synchronous conversations such as meet-
ings (Somasundaran et al., 2007; Raaijmakers et
al., 2008) and in monologic corpora such as news
collections (Awadallah et al., 2012) and reviews
(Popescu et al., 2005; Mukherjee and Liu, 2012).
In the domain of synchronous conversations,
prosodic features such as duration, speech rate and
pause have been used for spoken dialogue (Wang
et al., 2011; Galley et al., 2004). Galley et al.
(2004) found that local features, such as lexical
and structural features, as well as global contex-
tual features, were particularly useful for identify-
ing agreement/disagreement in the ICSI meeting
corpus. Germesin and Wilson (2009) also showed
accuracies of 98% in detecting agreement in the
AMI corpus using lexical, subjectivity and dia-
logue act features. However, they note that their
system could not classify disagreement accurately
due to the small number of training examples in
this category. Somasundaran et al. additionally
show that dialogue act features complement lexi-
cal features in the AMI meeting corpus (Somasun-
daran et al., 2007). These observations are taken
into account with our feature choices, and we use
contextual, discourse and lexical features in our
analysis.
In the monologic domain, Conrad et al. (2012)
recently found rhetorical relations to be useful for
argument labelling and detection in articles on the
topic of healthcare. Additionally, discourse mark-
ers and sentiment features have been found to as-
sist with disagreement detection in collections of
news documents on a particular topic, as well as
reviews (Choi et al., 2010; Awadallah et al., 2012;
Popescu et al., 2005).
In the asynchronous domain, there has been re-
cent work in disagreement detection, especially as
it pertains to stance identification. Content based
features, including sentiment, duration, and dis-
course markers have been used for this task (Yin
et al., 2012; Somasundaran and Wiebe, 2009;
Somasundaran and Wiebe, 2010). The structure
of a conversation has also been used, although
these approaches have focused on simple rules for
disagreement identification (Murakami and Ray-
mond, 2010), or have assumed that adjacent posts
always disagree (Agrawal et al., 2003). More re-
cent work has focused on identifying users? atti-
tudes towards each other (Hassan et al., 2010), in-
fluential users and posts (Nguyen et al., 2014), as
well as identifying subgroups of users who share
viewpoints (Abu-Jbara et al., 2010). In Slashdot,
the h-index of a discussion has been used to rank
articles according to controversiality, although no
quantitative evaluation of this approach has been
given, and, unlike in our analysis, they did not
consider any other features (Gomez et al., 2008).
Content based features such as polarity and co-
sine similarity have also been used to study influ-
ence, controversy and opinion changes on micro-
blogging sites such as Twitter (Lin et al., 2013;
Popescu and Pennacchiotti, 2010).
The simplified task of detecting disagreement
between just two users (either a question/response
pair (Abbott et al., 2011) or two adjacent para-
graphs (Misra and Walker, 2013)) has also been re-
cently approached on the ARGUE corpus. Abbott
et al. (2011) use discourse markers, generalized
dependency features, punctuation and structural
features, while Misra and Walker (2013) focus on
n-grams indicative of denial, hedging and agree-
ment, as well as cue words and punctuation. Most
similar to our work is that by Mishne and Glance
(2006). They performed a general analysis of we-
blog comments, using punctuation, quotes, lexi-
con counts, subjectivity, polarity and referrals to
detect disputative and non-disputative comments.
Referrals and questions, as well as polarity mea-
sures in the first section of the post, were found to
be most useful. However, their analysis did not
1170
Type Num P/A S/P W/P Num Authors W/S TBP TT TP Length
Disagreement - C 19.00 1.02 3.21 65.86 15.84 20.47 4.60 50.90 16.21 49.11
Disagreement - NC 27.00 1.00 3.08 59.80 14.07 19.33 3.89 42.29 14.11 42.26
No disagreement - NC 28.00 1.03 2.85 57.25 10.29 19.94 6.83 50.12 10.50 28.00
No disagreement - C 21.00 1.00 3.69 69.66 6.29 20.22 6.14 18.22 6.29 19.81
Table 1: Characteristics of the four categories determined from the crowd-sourced annotation. All values except for the number
of topics in the category are given as the average score per topic across all topics in that category. Key: C and NC: Confident
(score ? 0.75) and Not confident (score < 0.75), Num: Number of topics in category, P/A: Posts per author, S/P: Sentences
per post, W/P: Words per post, Num Authors: Number of authors, W/S: Words per sentence, TBP: Time between posts
(minutes), TT: Total time in minutes, TP: Total posts, and Length: Length of topic in sentences
take into account many features that have been
subsequently shown to be relevant, such as dis-
course markers and conversational structure, and
was hampered by a severe imbalance in the test
set (with very few disputative comments).
Our method takes advantage of insights from
many of these previous studies, focusing on dis-
cussion thread structure as well as text based fea-
tures to form our basic feature set. It is unlike
Mishne and Glance?s work in that we incorporate
several new features, have a balanced testing and
training set, and only use comments from one type
of online blog. Furthermore, it is a very different
task from those so far performed on the ARGUE
corpus, as we consider topics discussed by more
than two users. We aim to compare our features to
those found to be previously useful in these related
tasks, and expect similar feature sets to be useful
for the task of disagreement detection in this new
corpus.
3 Corpus
The corpus stems from the online forum Slash-
dot.
1
Slashdot is a casual internet forum, includ-
ing sections for users to ask questions, post arti-
cles, and review books and games. For the task of
disagreement detection, we focus our analysis on
the section of the site where users can post arti-
cles, and then comment either on the article or re-
spond to other users? posts. This results in a tree-
like dialogue structure for which the posted arti-
cle is the root, and branches correspond to threads
of comments. Each comment has a timestamp at
the minute resolution as well as author information
(although it is possible to post on the forum anony-
mously). Additionally, other users can give differ-
ent posts scores (in the range -1 to 5) as well as cat-
egorizing posts under ?funny?, ?interesting?, ?in-
formative?, ?insightful?, ?flamebait?, ?off topic?,
or ?troll?. This user moderation, as well as the
1
www.slashdot.org
formalized reply-to structure between comments,
makes Slashdot attractive over other internet fo-
rums as it allows for high-quality and structured
conversations.
In a previous study, Joty et al. (2013) selected
20 articles and their associated comments to be an-
notated for topic segmentation boundaries and la-
bels by an expert Slashdot contributor. They de-
fine a topic as a subset of the utterances in a con-
versation, while a topic label describes what the
given topic is about (e.g., Physics in videogames).
Of the 98 annotated topics from their dataset, we
filtered out those with only one contributing user,
for a total of 95 topics. Next, we developed a
Human Intelligence Task (HIT) using the crowd-
sourcing platform Crowdflower.
2
The objective of
this task was to both develop a corpus for testing
our disagreement detection system, as well as to
investigate how easily human annotators can de-
tect disagreement in casual online forums. For
training, users were shown 3 sample topics, la-
belling them as containing disagreement or not. In
each round, annotators were shown 5 topics, with
a set of radio buttons for participants to choose
?Yes?, ?No?, or ?Not sure? in response to asking
whether or not the users in the conversation dis-
agree on the topic. In order to limit the number of
spam responses, users were shown test questions,
which consisted of topics where there was obvi-
ous disagreement, as well as topics where there
was obviously no disagreement (either agreement,
or more news-style information sharing). We re-
quired that users correctly identify 4 of these test
topics before they were allowed to continue with
the task. Users were also shown test questions
throughout the task, which, if answered incor-
rectly, would reduce the amount of money they re-
ceived for the task, and ultimately disqualify them.
For each topic, five different judgements were
obtained. We consider the trust of each partici-
2
www.Crowdflower.com
1171
(a) Discourse tree (b) Relation graph
Figure 1: Discourse tree (left) with extracted relation graph (right) for a sample conversation involving three users with three
different posts P1, P2 and P3. N1, N2 and N3 are the corresponding nodes in the relation graph.
pant as the fraction of test questions which they
answered correctly. Then, each topic is assigned
a score according to a weighted average of the re-
sponses, with the weight being the trust of each
participant:
score = A
?
users
(
test
correct
test
total
)
user
i
? (0, 0.5, 1)
(1)
where 0, 0.5 and 1 represent the answers ?No?,
?Not sure? and ?Yes? to the question of disagree-
ment existence, and A is a normalization factor.
If the score is less than 0.5, its confidence would
be 1?score towards ?No disagreement?, whereas
greater than 0.5 would be a confidence of score
towards ?Disagreement?. The average confidence
score across all topics was 0.73. Our corpus con-
sists of 49 topics without disagreement and 46 top-
ics with disagreement. Interestingly, 22 topics had
confidence scores below 55%, which suggests that
subtle disagreement detection is a subjective and
difficult task. Further statistics for the developed
corpus are given in Table 1.
4 Features for Disagreement Detection
The features we use in our experiments combine
information from conversational structure, rhetor-
ical relations, sentiment features, n-gram models,
Slashdot meta-features, structural features, and
lexicon features.
4.1 Rhetorical Relation Graphs
Discourse markers have been found to aid in
argument and disagreement detection, and for
tasks such as stance identification (Abbott et al.,
2011; Misra and Walker, 2013; Somasundaran
and Wiebe, 2009). We aim to improve over dis-
course markers by capturing the underlying dis-
course structure of the conversation in terms of
discourse relations.
In Rhetorical Structure Theory, Discourse trees
are a hierarchical representation of document
structure for monologues (Mann and Thompson,
1988). At the lowest level, Elementary Discourse
Units (EDUs) are connected by discourse rela-
tions (such as ELABORATION and COMPARISON),
which in turn form larger discourse units that are
also linked by these relations. Computational sys-
tems (discourse parsers) have been recently devel-
oped to automatically generate a discourse tree for
a given monologue (Joty et al., 2013). Although
theoretically the rhetorical relations expected in
dialogues are different from those in monologues
(Stent and Allen, 2000), no sophisticated compu-
tational tools exist yet for detecting these relations
reliably. The core idea of this work is that some
useful (although noisy) information about the dis-
course structure of a conversation can be obtained
by applying state-of-the-art document level dis-
course parsing to parts of the conversation.
More specifically, posts on a particular topic
are concatenated according to their temporal order.
This pseudo-monologic document is then fed to a
publicly available document level discourse parser
(Joty et al., 2013). A discourse tree such as that
seen in Figure 1a is output by the parser. Then,
we extract the novel relation graph (Figure 1b)
from the discourse tree. In this graph, each node
(N1, N2, N3) corresponds to a post (P1, P2, P3)
and links aim to capture the argumentative struc-
ture. There are three cases when a link is added
between two nodes in the relation graph. Firstly,
links existing between two posts directly, such as
the COMPARISON relation between P2 and P3, are
added between the corresponding nodes in the re-
1172
lation graph (N2 and N3). Secondly, links existing
between fractions of posts in the discourse tree are
added to the relation graph (e.g. if (S2,P2) was
connected to (S1,P3) directly, N2 and N3 would
have an additional link with that label). Finally,
when posts are connected through internal nodes
(such as P1 and I1 in Figure 1a), a labelled link is
added for each post in the internal node to the re-
lation graph (N1->N2 and N1->N3 in Figure 1b).
This relation graph allows for the extraction
of many features that may reflect argumenta-
tive structure, such as the number of connec-
tions, the frequency of each rhetorical relation
in the graph per post (diff per post), and the
frequency as a percentage of all rhetorical rela-
tions (diff percentage). For example, COMPAR-
ISON relations are known to indicate disagree-
ment (Horn, 1989), so we expect higher fre-
quencies of this relation if the conversation con-
tains argument. Features from the discourse tree
such as the average depth of each rhetorical re-
lation are also added to reflect the cohesiveness
of conversation. Moreover, features combining
the graph and tree representations, such as the ra-
tio of the frequency of a rhetorical relation occur-
ring between different posts to the average depth
(
CONTRAST between different posts
Average depth of CONTRAST
), called avg ratio
are implemented. These reflect the hypothesis
that relations connecting larger chunks of text (or
whole posts) may be more important than those
connecting sentences or only partial posts.
Finally, the sub-trees corresponding to individ-
ual posts are used to extract the average frequency
of rhetorical relations within a post (same per
post) and the average frequency of a rhetorical re-
lation with respect to other rhetorical relations in
the post (same percentage). A measure of how
often a rhetorical relation connects different users
compared to how often it connects discourse units
in the same post (same to diff ), is also added.
These capture the intuition that certain rhetorical
relations such as CAUSE, EVIDENCE and EXPLA-
NATION are expected to appear more within a post
if users are trying to support their perspective in
an argument. In total, there are 18 (rhetorical
relations)?7 (avg ratio, avg depth, same per post,
same percentage, diff percentage, diff per post,
same to diff ) + 1 (number of connections) = 127
features.
4.2 Discourse Markers
Motivated by previous work, we include a fre-
quency count of 17 discourse markers which were
found to be the most common across the ARGUE
corpus (Abbott et al., 2011). Furthermore, we hy-
pothesize that individual discourse markers might
have low frequency counts in the text. Therefore,
we also include an aggregated count of all 17 dis-
course markers in each fifth of the posts in a topic
(e.g. the count of all 17 discourse markers in the
first fifth of every post in the topic). Altogether,
there are 5 aggregated discourse marker features
in addition to the 17 frequency count features.
4.3 Sentiment Features
Sentiment polarity features have been shown to be
useful in argument detection (Mishne and Glance,
2006). For this work, we use four sentiment
scoring categories: the variance, average score,
number of negative sentences, and controversiality
score (Carenini and Cheung, 2008) of sentences
in a post. These are determined using SoCAL
(Taboada et al., 2011), which gives each sentence
a polarity score and has been shown to work well
on user-generated content.
Overall, we have two main classes of sentiment
features. The first type splits all the posts in a topic
into 4 sections corresponding to the sentences in
each quarter of the post. The sentiment scores de-
scribed above are then applied to each section of
the posts (e.g. one feature is the number of neg-
ative sentences in the first quarter of each post).
As a separate feature, we also include the scores
on just the first sentence, as Mishne and Glance
(2006) previously found this to be beneficial. This
gives a total of 4?5 = 20 features. We refer to this
set as ?sentiment?.
Motivated by the rhetorical features, our sec-
ond main class of sentiment features aims to iden-
tify ?more important? posts for argument detec-
tion by applying the four categories of sentiment
scores to only those posts connected by each of
our 18 rhetorical relations. This is done for both
posts with an inner rhetorical connection (iden-
tified by the sub-tree for that post), as well as
for posts connected by a rhetorical relation in
the relation graph. This results in a total of (4
sentiment categories)?(2 (different + same post
connections))?(18 rhetorical relations) = 144 fea-
tures. This set is referred to as ?RhetSent?.
1173
4.4 Fragment Quotation Graphs
As previously noted in (Murakami and Raymond,
2010; Gomez et al., 2008), the structure of dis-
cussion threads can aid in disagreement detec-
tion. In online, threaded conversations, the stan-
dard approach to extracting conversational struc-
ture is through reply-to relations usually present
in online forums. However, if users strongly dis-
agree on a topic (or sub-topic), they may choose
to quote a specific paragraph (defined as a frag-
ment) of a previous post in their reply. Being able
to determine which specific fragments are linked
by relations may then be useful for more targeted
content-based features, helping to reduce noise. In
order to address this, we use the Fragment Quo-
tation Graph (FQG), an approach previously de-
veloped by Carenini et al. (2007) for dialogue
act modelling and topic segmentation (Joty et al.,
2011; Joty et al., 2013).
For our analysis, the FQG is found over the
entire Slashdot article. We then select the sub-
graph corresponding to those fragments in the tar-
get topic. From the fragment quotation sub-graph,
we are then able to extract features for disagree-
ment detection such as the number of connections,
total number of fragments, and the average path
length between nodes which we hypothesize to
be useful. We additionally extract the h-index
(Gomez et al., 2008) and average branching ratio
per fragment of the topic from the simpler reply-to
conversational structure. In total, there are 8 FQG
features.
4.5 N-gram models
As noted previously (Somasundaran and Wiebe,
2010; Thomas et al., 2006), it is often difficult to
outperform a unigram/bigram model in the task of
disagreement and argument detection. In this anal-
ysis, because of the very small number of sam-
ples, we do not consider dependency or part-of-
speech features, but do make a comparison with
a filtered unigram/bigram model. In the filtering,
we remove stop words and any words that occur in
fewer than three topics. This helps to prevent topic
specific words from being selected, and limits the
number of possible matches slightly. Additionally,
we use a lexicon of bias-words (Recasens et al.,
2013) to extract a bias-word frequency score over
all posts in the topic as a separate feature.
4.6 Structural Features
Length features have been well documented in
the literature to provide useful information about
whether or not arguments exist, especially in on-
line conversations that may be more informative
than subjective (Biyani et al., 2014; Yin et al.,
2012). In this work, length features include the
length of the post in sentences, the average num-
ber of words per sentence, the average number of
sentences per post, the number of contributing au-
thors, the rate of posting, and the total amount of
time of the conversation. This results in a total of
9 features.
4.7 Punctuation
Like many other features already described, fre-
quency counts of ???,?!?,???,??, and ?.? are found for
each fifth of the post (the first fifth, second fifth,
etc.). These counts are then aggregated across all
posts for a total of 5?5 = 25 features.
4.8 Referrals
Referrals have been found to help with the detec-
tion of disagreement (Mishne and Glance, 2006),
especially with respect to other authors. Since
there are no direct referrals to previous authors
in this corpus, references to variations of ?you?,
?they?, ?us?, ?I?, and ?he/she? in each fifth of the
post are included instead, for a total of 5?5 = 25
features.
4.9 Meta-Post Features
Slashdot allows users to rank others? posts with the
equivalent of a ?like? button, changing the ?score?
of the post (to a maximum of 5). They are also
encouraged to tag posts as either ?Interesting?,
?Informative?, ?Insightful?, ?Flamebait?, ?Troll?,
?Off-topic? or ?Funny?. Frequency counts of
these tags as a percentage of the total number of
comments are included as features, as well as the
overall fraction of posts which were tagged with
any category. The average score across the topic,
as well as the number of comments with a score
of 4 or 5, are also added. These are expected to
be informative features, since controversial topics
may encourage more up and down-voting on spe-
cific posts, and generally more user involvement.
This results in 9 meta-post features.
1174
Feature Set Random Forest SVM
P R F1 A ROC-AUC P R F1 A ROC-AUC
N-grams 0.71 0.57 0.63 0.67 0.69 0.52 0.60 0.56 0.53 0.53
Basic 0.69 0.67 0.68 0.69 0.73 0.62 0.62 0.62 0.63 0.67
Basic+N-grams 0.73 0.66 0.69 0.70 0.73 0.57 0.65 0.60 0.59 0.61
Basic+FQG 0.69 0.66 0.67 0.69 0.71 0.64 0.63 0.63 0.65 0.70
Basic+Sentiment 0.68 0.65 0.66 0.68 0.73 0.61 0.59 0.60 0.62 0.67
Basic+RhetStruct 0.71 0.70 0.70 0.71 0.73 0.73 0.70 0.71 0.73 0.78
Basic+RhetStruct+FQG 0.69 0.69 0.69 0.70 0.73 0.74 0.74 0.74 0.75 0.80
Basic+RhetAll 0.72 0.73 0.73 0.73 0.75 0.76 0.76 0.76 0.77 0.79
RhetStructOnly 0.69 0.72 0.71 0.71 0.72 0.76 0.72 0.74 0.75 0.79
RhetAllOnly 0.69 0.74 0.71 0.71 0.73 0.75 0.72 0.73 0.75 0.78
All 0.71 0.72 0.71 0.72 0.74 0.74 0.77 0.75 0.76 0.77
Table 2: Basic: Meta-post, all structural, bias words, discourse markers, referrals, punctuation RhetAll: Structural and sen-
timent based rhetorical features All: Basic, all rhetorical, sentiment and FQG features. The N-gram models include unigrams
and bi-grams. All feature sets in the bottom part of the table include rhetorical reatures.
5 Experiments
Experiments were all performed using the Weka
machine learning toolkit. Two different types of
experiments were conducted - one using all an-
notated topics in a binary classification of con-
taining disagreement or not, and one using only
those topics with confidence scores greater than
0.75 (corresponding to the more certain cases). All
results were obtained by performing 10 fold cross-
validation on a balanced test set. Additionally, in-
fold cross-validation was performed to determine
the optimal number of features to use for each fea-
ture set. Since this is done in-fold, a paired t-test
is still a valid comparison of different feature sets
to determine significant differences in F-score and
accuracy.
5.1 Classifiers
Two classifiers were used for this task: Random
Forest and SVM. Random Forest was selected be-
cause of its ability to avoid over-fitting data despite
large numbers of features for relatively few sam-
ples. For all runs, 100 trees were generated in the
Random Forest, with the number of features to use
being determined by in-fold optimization on the F-
score. For the SVM classifier, we use the normal-
ized poly-vector kernel with a maximum exponent
of 2 (the lowest possible), and a C parameter of
1.0 (Weka?s default value). This was chosen to
avoid over-fitting our data. We additionally use
a supervised in-fold feature selection algorithm,
Chi-Squared, to limit over-fitting in the SVM. The
number of features to be used is also optimized us-
ing in-fold cross-validation on the F-score. Both
the SVM classifier and the Random Forest classi-
fier were tested on the same training/testing fold
pairs, with a total of 10 iterations.
6 Results
The results of the experiments are shown in Ta-
bles 2 and 3. In order to compare to previous anal-
yses, unigram and bigram features are shown, as
well as a combination of the basic features with
the n-grams. When performing the experiments,
we noticed that the n-gram features were hurting
the performance of the classifiers when included
with most of our other feature sets (or not chang-
ing results significantly), and therefore those re-
sults are not shown here. As seen in the table,
Feature Set Random Forest SVM
P R F1 A ROC-AUC P R F1 A ROC-AUC
N-grams 0.70 0.70 0.70 0.71 0.77 0.63 0.70 0.66 0.63 0.66
Basic 0.74 0.69 0.72 0.72 0.77 0.73 0.70 0.72 0.71 0.78
Basic+FQG 0.72 0.67 0.69 0.69 0.76 0.73 0.63 0.68 0.69 0.76
Basic+Sentiment 0.71 0.65 0.68 0.68 0.76 0.73 0.67 0.70 0.70 0.76
Basic+RhetStruct 0.79 0.75 0.77 0.77 0.78 0.79 0.67 0.72 0.74 0.79
Basic+RhetStruct+FQG 0.76 0.71 0.73 0.73 0.77 0.74 0.64 0.69 0.70 0.78
Basic+RhetAll 0.77 0.75 0.76 0.76 0.78 0.72 0.69 0.71 0.70 0.76
RhetStructOnly 0.75 0.71 0.73 0.73 0.75 0.76 0.63 0.69 0.70 0.76
RhetAllOnly 0.73 0.76 0.74 0.73 0.76 0.67 0.62 0.65 0.65 0.67
All 0.73 0.69 0.71 0.70 0.76 0.71 0.70 0.70 0.69 0.74
Table 3: Precision, recall, F1, accuracy and ROC-AUC scores for the simpler task of identifying the cases deemed ?high
confidence? in the crowd-sourcing task.
1175
the best performing feature sets are those that in-
clude rhetorical features under the SVM+?
2
clas-
sifier. In fact, these feature sets perform signifi-
cantly better than a unigram/bigram baseline ac-
cording to a paired t-test between the best clas-
sifiers for each set (p < 0.0001). The inclusion
of rhetorical structure also significantly outper-
forms the ?basic? and ?basic+N-grams? feature
baselines (which includes discourse markers, re-
ferrals, punctuation, bias word counts and struc-
tural features) with respect to both the F-score
and accuracy (p < 0.02 for all feature sets with
rhetorical features). Overall, the feature sets ?Ba-
sic+RhetAll? and ?All? under the SVM+?
2
clas-
sifier perform best. This performance is also bet-
ter than previously reported results for the ARGUE
Corpus (Abbott et al., 2011), even though the ba-
sic and unigram/bigram features perform similarly
to that reported in previous analyses.
As an additional check, we also conduct exper-
iments on the ?high confidence? data (those topics
with a confidence score greater than 0.75). These
results are shown in Table 3. Clearly the basic
features perform better on this subset of the sam-
ples, although the addition of rhetorical structure
still provides significant improvement (p< 0.001).
Overall, this suggests that the rhetorical, sentiment
and FQG features help more when the disagree-
ment is more subtle.
7 Analysis and Discussion
In order to examine the quality of our features,
we report on the rhetorical features selected, and
show that these are reasonable and in many cases,
theoretically motivated. Furthermore, we check
whether the commonly selected features in each
of our feature categories are similar to those found
to be useful over the ARGUE corpus, as well as
within other argument detection tasks in online fo-
rums.
The rhetorical features that are consistently se-
lected are very well motivated in the context of ar-
gument detection. From the rhetorical structural
features, we find COMPARISON relation features
to be most commonly selected across all rhetorical
feature sets. Other highly ranked features include
the proportion of JOINT relations linking different
authors, EXPLANATION relations between differ-
ent authors, and the average depth of ELABORA-
TION relations.
The COMPARISON relations are expected to in-
Structural Length of topic in sentences, to-
tal number of authors, quotes in
first sentence, quotes in second
sentence, questions in first sen-
tence, questions in second sen-
tence, referrals to you and they
in first half of post
Meta Number of comments with la-
bels, number of comments la-
belled ?Flamebait?, number of
comments with scores of 4 or 5
FQG Number of connections, Num-
ber of fragments, Maximum
number of links from one node
RhetStruct COMPARISON (same to diff, diff
per post, diff percentage, avg
ratio, same per post, same per-
centage), EXPLANATION (avg
ratio, diff per post), JOINT
(diff percentage), ELABORA-
TION (average depth)
Discourse Markers Aggregated first sentence, Ag-
gregated middle, ?and?, ?oh?,
?but? frequency counts
N-grams ?don?t ??, ?plus?, ?private?,
?anti?, ?hey?, ?present?, ?mak-
ing?, ?developers?
RhetSent ELABORATION (variance in
same post), ATTRIBUTION
(variance in same post),
CONTRAST (range in same
post)
Sentiment Range of sentiment scores in
first sentence of all posts, range
of sentiment scores over all
posts
Table 4: Features found to be commonly selected over dif-
ferent iterations of the classifiers
dicate disagreement as motivated by theoretical
considerations (Horn, 1989). The importance of
other rhetorical relation features can also be ex-
plained by examining conversations in which they
appear. In particular, EXPLANATION relations of-
ten link authors who share viewpoints in a debate,
especially when one author is trying to support the
claim of another. The JOINT relations are also very
well motivated. In the extreme case, conversations
with a very high number of JOINT relations be-
tween different users are usually news based. The
high proportion of these relations indicates that
many users have added information to the conver-
sation about a specific item, such as adding new
suggested videogame features to an ongoing list.
Fewer JOINT relations seem to indicate disagree-
ment, especially when found in conjunction with
COMPARISON relations between different users.
This appears to generally indicate that users are
taking sides in a debate, and commenting specifi-
cally on evidence which supports their viewpoint.
1176
The average depth of ELABORATION relations
reveals how deep the perceived connections are
between users in a conversation over time. Deeper
ELABORATION connections seem to indicate that
the conversation is more cohesive. Alone, this
does not signify disagreement/agreement but does
seem to signify argument-style over news-style di-
alogues. This is particularly helpful for differ-
entiating between articles with many COMPARI-
SON relations, as COMPARISON may be present
in both news-style dialogues (e.g. comparing ci-
tation styles) as well as argument style dialogues
(e.g. arguing over which of two operating systems
is superior).
For the combined sentiment and rhetorical rela-
tions, range and variance in ELABORATION, CON-
TRAST and ATTRIBUTION within the same post
are found to be the most informative features. Ad-
ditionally, neither ATTRIBUTION nor CONTRAST
are useful features when only their structural in-
formation is considered. In the case of ATTRI-
BUTION, we hypothesize that the added sentiment
score within the post differentiates between a neu-
tral attribution (which would not signify disagree-
ment) and a negative or positive attribution (which
may signify disagreement). For CONTRAST, the
added sentiment helps to distinguish between re-
sponses such as ?We will be trying the Microsoft
software. We won?t, however, be able to test the
Apple equivalent.? and ?We will be trying the Mi-
crosoft software. We won?t, however, be trying the
inferior Apple equivalent.? where the second ex-
ample more likely signals, or even provokes, dis-
agreement.
Outside of the rhetorical features, the discourse
markers which are found to be the most useful in
our experiments agree with those found in the AR-
GUE corpus (Abbott et al., 2011). Namely, ?oh?,
?but?, ?because? and ?and? are discovered to be the
most informative features. We also find the aggre-
gated discourse marker frequency count in the first
part of each post to be useful.
Previous analysis on Slashdot as a social net-
work (Gomez et al., 2008) suggests that the h-
index of the conversation is relevant for detecting
controversy in a posted article. We include the
h-index as part of the Fragment Quotation Graph
feature set, but surprisingly do not find this to be
a useful feature. This may be due to our corpus
involving relatively shallow conversational trees -
the maximum h-index across all topics is three.
Comparing to Mishne and Glance?s work, we
also find quotations, questions and sentiment
range near the beginning of a post to be very in-
formative features. These are often selected across
all feature sets which include the ?basic? set.
The topics most often misclassified across all
feature sets are those with relatively few sen-
tences. In these cases, the rhetorical structure
is not very well defined, and there is much less
content available for detecting quotes, punctuation
and referrals. Additionally, the feature sets which
only use rhetorical and sentiment features consis-
tently misclassify the same set of conversations
(those that have lower quality discourse trees with
few connections). When combined with the ?ba-
sic? feature set, these errors are mitigated, and the
topics which the ?basic? features miss are picked
up by the rhetorical features. This leads to the best
overall accuracy and F-score.
7.1 Discourse Parser
A major source of error in detecting disagreement
arises because of inaccuracies in our discourse
parser. In particular, document-level discourse
parsing is a challenging task, with relatively few
parsers available at the time of this analysis (Joty
et al., 2013; Hernault et al., 2010). We chose to
use the discourse parser developed by Joty et al.
which both identifies elementary discourse units in
a text, and then builds a document-level discourse
tree using Conditional Random Fields. Because
their approach uses an optimal parsing algorithm
as opposed to a greedy parsing algorithm, they are
able to achieve much higher accuracies in rela-
tion and structure identification than other avail-
able parsers. Here, results from their parser on the
standard RST-DT dataset are presented since there
is no currently available dialogic corpora to com-
pare to.
RST-DT Instructional
Metrics Joty HILDA Human Joty
Span 83.84 74.68 88.70 81.88
Nuclearity 68.90 58.99 77.72 63.13
Relation 55.87 44.32 65.75 43.60
Table 5: Joty et al. document-level parser accuracy of the
parser used in this paper. The parser was originally tested
on two corpora: RST-DT and Instructional. HILDA was the
state-of-the-art parser at that time. Span and Nuclearity met-
rics assess the quality of the structure of the resulting tree,
while the Relation metric assesses the quality of the relation
labels.
Examining the relation labels confusion matrix
1177
Figure 2: Confusion matrix for relation labels on RST-DT.
The X-axis represents predicted relations, while the Y-axis
corresponds to true values. The relations are Topic-Change
(T-C), Topic-Comment (T-CM), Textual Organization (T-
O), Manner-Means (M-M), Comparison (CMP), Evaluation
(EV), Summary (SU), Condition (CND), Enablement (EN),
Cause (CA), Temporal (TE), Explanation (EX), Background
(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-
tion (AT) and Elaboration (EL).
for the discourse parser in Figure 2, some of the
chosen rhetorical features make even more sense.
In particular, the confusion of ELABORATION and
EXPLANATION may account for the perceived im-
portance of ELABORATION relations in the analy-
sis. Likewise, CAUSE (which may be present when
users attribute positive or negative qualities to an
entity, signalling disagreement) is often confused
with JOINT and ELABORATION which were often
picked as important features by our classifiers.
8 Conclusions and Future Work
In this paper, we have described a new set of fea-
tures for detecting disagreement in online blog fo-
rums. By treating a written conversation as a series
of linked monologues, we can apply a document
level discourse parser to extract a discourse tree
for the conversation. We then aggregate this infor-
mation in a relation graph, which allows us to cap-
ture post-level rhetorical relations between users.
Combining this approach with sentiment features
shows significantly improved performance in both
accuracy and F-score over a baseline consisting
of structural and lexical features as well as re-
ferral counts, punctuation, and discourse markers.
In building our new crowd-sourced corpus from
Slashdot, we have also shown the challenges of
detecting subtle disagreement in a dataset that con-
tains a significant number of news-style discus-
sions.
In future work, we will improve sentiment fea-
tures by considering methods to detect opinion-
topic pairs in conversation, similar to Somasun-
daran and Wiebe (2009). Additionally, we will
incorporate generalized dependency and POS fea-
tures (Abbott et al., 2011), which were not used
in this analysis due to the very small number of
training samples in our dataset. The fragment quo-
tation graph features did not perform as well as we
expected, and in future work we would like to in-
vestigate this further. Furthermore, we will also
explore how to create a discourse tree from the
thread structure of a conversation (instead of from
its temporal structure), and verify whether this im-
proves the accuracy of the relation graphs, espe-
cially when the temporal structure is not represen-
tative of the reply-to relationships.
Finally, we plan to apply our novel feature set to
other corpora (e.g., ARGUE) in order to study the
utility of these features across genres and with re-
spect to the accuracy of the discourse parser. This
may provide insights into where discourse parsers
can be most effectively used, as well as how to
modify parsers to better capture rhetorical rela-
tions between participants in conversation.
References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean
E. Fox Tree, Robeson Bowmani and Joseph King.
2011. How can you say such things?!?: Recogniz-
ing Disagreement in Informal Political Argument. In
Proceedings of LSM, pages 2-11.
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of ACL, pages
399-409.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In Pro-
ceedings of WWW, pages 529-535.
Rawia Awadallah, Maya Ramanath, Gerhard Weikum.
2012. Harmony and Dissonance: Organizing the
People?s Voices on Political Controversies. In Pro-
ceedings of WSDM, pages 523-532.
Prakhar Biyani, Sumit Bhatia, Cornelia Caragea,
Prasenjit Mitra. 2014. Using non-lexical features for
identifying factual and opinionative threads in online
forums. In Knowledge-Based Systems, in press.
Penelope Brown and Stephen Levinson. 1987. Polite-
ness: Some universals in language usage. Cam-
bridge University Press.
1178
Giuseppe Carenini and Jackie Cheung. 2008. Extrac-
tive vs. NLG-based Abstractive Summarization of
Evaluative Text: The Effect of Corpus Controver-
siality. In Proceedings of INLG, pages 33-41.
Giuseppe Carenini, Raymond Ng, Xiaodong Zhou.
2007. Summarizing Email Conversations with Clue
Words. In Proceedings of WWW, pages 91-100.
Yoonjung Choi, Yuchul Jung, and Sung-Hyon Myaeng.
2010. Identifying controversial issues and their sub-
topics in news articles. In Proceedings of PAISI,
pages 140-153.
Alexander Conrad, Janyce Wiebe and Rebecca
Hwa. 2012. Recognizing Arguing Subjectivity and
Argument Tags. In ACL Workshop on Extra-
Propositional Aspects of Meaning, pages 80-88.
Jean E. Fox Tree. 2010. Discourse markers across
speakers and settings. Language and Linguistics
Compass, 3(1):1-113.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependen-
cies. In Proceedings of ACL, pages 669-es.
Sebastian Germesin and Theresa Wilson. 2009. Agree-
ment Detection in Multiparty Conversation. In Pro-
ceedings of International Conference on Multimodal
Interfaces pages 7-14.
Vicenc Gomez, Andreas Kaltenbrunner and Vicente
Lopez. 2008. Statistical Analysis of the Social Net-
work and Discussion Threads in Slashdot. In Pro-
ceedings of WWW, pages 645-654.
Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What?s with the attitude?: identifying
sentences with attitude in online discussions. In Pro-
ceedings of EMNLP, pages 1245-1255.
Hugo Hernault, Helmut Prendinger, David A. duVerle
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser Using Support Vector Machine Classification.
Dialogue and Discourse, 1(3):1-33.
Laurence R. Horn. 1989. A natural history of negation.
Chicago University Press.
Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in asyn-
chronous conversations. In Proceedings of IJCAI,
pages 1807-1813.
Shafiq Joty, Giuseppe Carenini, Raymond Ng and
Yashar Mehdad. 2013. Combining Intra- and Multi-
sentential Rhetorical Parsing for Document-level
Discourse Analysis. In Proceedings of ACL.
Shafiq Joty, Giuseppe Carenini and Raymond Ng.
2013. Topic Segmentation and Labeling in Asyn-
chronous Conversations. Journal of AI Research,
47:521-573.
Ching-Sheng Lin, Samira Shaikh, Jennifer Stromer-
Galley, Jennifer Crowley, Tomek Strzalkowski,
Veena Ravishankar. 2013. Topical Positioning: A
New Method for Predicting Opinion Changes in
Conversation. In Proceedings of LASM, pages 41-
48.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243-281.
Gilad Mishne and Natalie Glance. 2006. Leave a reply:
An analysis of weblog comments. In Proceedings of
WWW.
Amita Misra and Marilyn Walker. 2013. Topic Inde-
pendent Identification of Agreement and Disagree-
ment in Social Media Dialogue. In Proceedings of
SIGDIAL, pages 41-50.
Arjun Mukherjee and Bing Liu. 2012. Modeling review
comments. In Proceedings of ACL, pages 320-329.
Akiko Murakami and Rudy Raymond. 2010. Support
or Oppose? Classifying Positions in Online De-
bates from Reply Activities and Opinion Expres-
sions. In Proceedings of the International Confer-
ence on Computational Linguistics, pages 869-875.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah Cai, Jennifer Midberry, Yuanxin Wang.
2014. Modeling topic control to detect influence
in conversations using nonparametric topic models.
Machine Learning 95:381-421.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing Product Features and Opinions from Reviews.
In Proceedings of HLT/EMNLP, pages 339-346.
Ana-Maria Popescu and Marco Pennacchiotti. 2010.
Detecting controversial events from twitter. In Pro-
ceedings of CIKM, pages 1873-1876.
Stephan Raaijmakers, Khiet Truong, Theresa Wilson.
2008. Multimodal subjectivity analysis of multiparty
conversation. In Proceedings of EMNLP, pages 466-
474.
Marta Recasens, Cristian Danescu-Niculescu-Mizil,
and Dan Jurafsky. 2013. Linguistic Models for Ana-
lyzing and Detecting Biased Language. In Proceed-
ings of ACL, pages 16501659.
Swapna Somasundaran, Josef Ruppenhofer, Janyce
Wiebe. 2007. Detecting Arguing and Sentiment in
Meetings. In Proceedings of SIGDIAL Workshop on
Discourse and Dialogue.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of ACL, pages 226-234.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of NAACL, Workshop on Computa-
tional Approaches to Analysis and Generation of
Emotion in Text, pages 116124.
1179
Amanda Stent and James Allen. 2000. Annotating Ar-
gumentation Acts in Spoken Dialog. Technical Re-
port.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, Manfred Stede. 2011. Lexicon-based
methods for sentiment analysis. Journal of Compu-
tational Linguistics, 37(2):267-307.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327-335.
Wen Wang, Sibel Yaman, Kristen Precoda, Colleen
Richey, and Geoffrey Raymond. 2011. Detection of
agreement and disagreement in broadcast conversa-
tions. In Proceedings of ACL, pages 374-378.
Bonnie Webber and Rashmi Prasad. 2008. Sentence-
initial discourse connectives, discourse structure and
semantics. In Proceedings of the Workshop on For-
mal and Experimental Approaches to Discourse Par-
ticles and Modal Adverbs.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying local and global agreement and dis-
agreement classification in online debates. In Pro-
ceedings of Computational Approaches to Subjectiv-
ity and Sentiment Analysis, pages 61-69.
1180
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1602?1613,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Abstractive Summarization of Product Reviews Using Discourse Structure
Shima Gerani
?? ?
Yashar Mehdad
? ?
Giuseppe Carenini
?
Raymond T. Ng
?
Bita Nejat
?
?
University of Lugano
?
University of British Columbia
Switzerland Vancouver, BC, Canada
{gerani,mehdad,carenini,rng,nejatb}@cs.ubc.ca
Abstract
We propose a novel abstractive summa-
rization system for product reviews by tak-
ing advantage of their discourse structure.
First, we apply a discourse parser to each
review and obtain a discourse tree repre-
sentation for every review. We then mod-
ify the discourse trees such that every leaf
node only contains the aspect words. Sec-
ond, we aggregate the aspect discourse
trees and generate a graph. We then select
a subgraph representing the most impor-
tant aspects and the rhetorical relations be-
tween them using a PageRank algorithm,
and transform the selected subgraph into
an aspect tree. Finally, we generate a
natural language summary by applying a
template-based NLG framework. Quan-
titative and qualitative analysis of the re-
sults, based on two user studies, show that
our approach significantly outperforms ex-
tractive and abstractive baselines.
1 Introduction
Most existing works on sentiment summarization
focus on predicting the overall rating on an en-
tity (Pang et al., 2002; Pang and Lee, 2004) or
estimating ratings for product features (Lu et al.,
2009; Lerman et al., 2009; Snyder and Barzilay,
2007; Titov and McDonald, 2008)). However, the
opinion summaries in such systems are extractive,
meaning that they generate a summary by concate-
nating extracts that are representative of opinion
on the entity or its aspects.
Comparing extractive and abstractive sum-
maries for evaluative texts has shown that an ab-
stractive approach is more appropriate for sum-
marizing evaluative text (Carenini et al., 2013;
?
The contribution of the first two authors to this paper
was equal.
Di Fabbrizio et al., 2014). This finding is also
supported by a previous study in the context of
summarizing news articles (Barzilay et al., 1999).
To the best of our knowledge, there are only three
previous works on abstractive opinion summariza-
tion (Ganesan et al., 2010; Carenini et al., 2013;
Di Fabbrizio et al., 2014). The first work (Gane-
san et al., 2010) proposes a graph-based method
for generating ultra concise opinion summaries
that are more suitable for viewing on devices with
small screens. This method does not provide a
well-formed grammatical abstract and the gener-
ated summary only contains words that occur in
the original texts. Therefore, this approach is more
extractive than abstractive. Another limitation is
that the generated summaries do not contain any
information about the distribution of opinions.
In the second work, (Carenini et al., 2013) ad-
dresses some of the aforementioned problems and
generates well-formed grammatical abstracts that
describe the distribution of opinion over the en-
tity and its features. However, for each product,
this approach requires a feature taxonomy hand-
crafted by humans as an input, which is not scal-
able. To partially address this problem (Mukherjee
and Joshi, 2013) has proposed a method for the au-
tomatic generation of a product attribute hierarchy
that leverages ConceptNet (Liu and Singh, 2004).
However, the resulting ontology tree has been used
only for sentiment classification and not for clas-
sification.
In the third and most recent study, (Di Fabbrizio
et al., 2014) proposed Starlet-H as a hybrid ab-
stractive/extractive sentiment summarizer. Starlet-
H uses extractive summarization techniques to se-
lect salient quotes from the input reviews and em-
beds them into the abstractive summary to exem-
plify, justify or provide evidence for the aggregate
positive or negative opinions. However, Starlet-H
assumes a limited number of aspects as input and
needs a large amount of training data to learn the
1602
ordering of aspects for summary generation.
Highlighting the reasons behind opinions in re-
views was also previously proposed in (Kim et al.,
2013). However, their approach is extractive and
similar to (Ganesan et al., 2010) does not cover the
distribution of opinions. Furthermore, it aims to
explain the opinion on only one aspect, rather than
explaining the overall opinion on the product, its
aspects and how they affect each other.
To address some of the above mentioned limita-
tions , in this paper we propose a novel abstrac-
tive summarization framework that generates an
aspect-based abstract from multiple reviews of a
product. In our framework, anything that is eval-
uated in the review is considered an aspect, in-
cluding the product itself. We propose a natural
language generation (NLG) framework that takes
aspects and their structured relation as input and
generates an abstractive summary. However, un-
like (Carenini et al., 2013), our method assumes no
domain knowledge about the entity in terms of a
user-defined feature taxonomy. On the other hand,
in contrast with Starlet-H, we do not limit the in-
put reviews to a small number of aspects and our
aspect ordering method takes advantage of rhetor-
ical information and does not require any training
data. Our method relies on the discourse struc-
ture and discourse relations of reviews to infer the
importance of aspects as well as the association
between them (e.g., which aspects relate to each
other).
Researchers have recently started using the dis-
course structure of text in sentiment analysis and
have shown its advantage in improving sentiment
classification accuracy (e.g., (Lazaridou et al.,
2013; Trivedi and Eisenstein, 2013; Somasun-
daran et al., 2009; Asher et al., 2008)). However,
to the best of our knowledge, none of the existing
works have looked into exploiting discourse struc-
ture in abstractive review summarization.
In our work, importance of aspects, derived
from the reviews? discourse structure and rela-
tions, is used to rank and select aspects to be in-
cluded in the summary. More specifically, we start
with the most important (highest ranked) aspects
to generate a summary and add more aspects to
the system until a summary of desired length is
obtained. Aspect association is considered to bet-
ter explain how the opinions on aspects affect each
other (e.g., opinion over specific aspects affect the
opinion over the more general ones). Consider
the following sentence as an example summary
generated by our system for the entity Camera
Canon G3: ?All reviewers who commented on the
camera, thought that it was really good mainly be-
cause of the photo quality.? This summary encap-
sulates all the following key pieces of information:
1) camera and photo quality are the most impor-
tant aspects, 2) People have positive opinion on
camera in general and on photo quality as one of
its features, and finally 3) photo quality is the main
reason behind users satisfaction on camera. Such
summary helps users understand the reason behind
a rating of a product or its aspects without going
through all reviews or reading scattered opinions
on different aspects in multiple sentences of an ex-
tractive summary.
This paper makes the following contributions:
1. We propose a novel content selection and struc-
turing strategy for review summarization, that as-
sumes no prior domain knowledge, by taking ad-
vantage of the discourse structure of reviews.
2. We propose a novel product-independent
template-based NLG framework to generate an ab-
stract based on the selected content, without re-
lying on deep syntactic knowledge or sophisti-
cated NLG methods. Our framework, similarly to
(Carenini et al., 2013), can effectively convey the
distribution of opinions.
3. We present the first study that investigates the
use of discourse structure information in both con-
tent selection and abstract generation for multi-
document summarization.
Quantitative and qualitative analysis over eval-
uation results of two user studies on a set of user
reviews on twelve different products show that our
system is an effective abstractive system for re-
view summarization.
2 Summarization Framework
At a high-level, our summarization framework in-
volves generating a summary from multiple in-
put reviews based on an Aspect Hierarchy Tree
(AHT) that reflects the importance of aspects as
well as the relationships between them. In our
framework, an AHT is generated automatically
from the set of input reviews, where each sen-
tence of every review is marked by the aspects pre-
sented in that sentence and the polarity of opin-
ions over them. There are various methods for
extracting the aspects and predicting the polar-
ity of opinion (Hu and Liu, 2004b; Hu and Liu,
1603
2006; Kim et al., 2011). In this paper we do not
focus on aspect extraction and sentiment predic-
tion but rather consider the aspect and their po-
larity/strength (P/S) information given as input to
the system. P/S scores are integer values in the
range [-3, +3], where +3 is the most positive and
-3 is the most negative polarity value. We also
do not attempt to automatically resolve corefer-
ences between aspects. For example, the aspect
?g3?, ?canon g3? and ?canon? were manually
collapsed as into ?camera?. This preprocessing
step helps to reduce the noise generated by inac-
curate aspect labeling in our reviews. Figure 1
shows two sample input reviews where the aspects
and their P/S scores are identified. For example, in
R1, aspects camera, photo quality and auto mode
are mentioned. The P/S values for the three as-
pects are [+2], [+3] and [+2] respectively which
indicate positive opinion on all aspects.
The first component of our system applies a dis-
course parser to each review and obtains a dis-
course tree representation for every review (e.g.
Figure 1 (a) and (b)). The discourse trees are then
modified such that every leaf node only contains
the aspect words. The output of the first compo-
nent is an aspect-based discourse tree (ADT) for
every review (e.g. Figure 1 (c) and (d)). In the
second component, we aggregate the ADTs and
generate a graph called Aggregated Rhetorical Re-
lation Graph (ARRG) (e.g. Figure 1 (f)). The
third component of our framework, is responsi-
ble for content selection and structuring. It takes
ARRG as input, runs Weighted PageRank, and se-
lects a subgraph (e.g. Figure 1 (g)) representing
the most important aspects. Finally it transforms
the selected subgraph into a tree and provides an
AHT as output (e.g. Figure 1 (h)). The gener-
ated AHT is the input of the last component which
generates a natural language summary by apply-
ing micro planning and sentence realization. We
now describe each component of our framework
in more detail.
3 Discourse Parsing
Any coherent text is structured so that we can
derive and interpret the information. This struc-
ture shows how discourse units (text spans such
as sentences or clauses) are connected and relate
to each other. Discourse analysis aims to reveal
this structure. Several theories have been pro-
posed in the past to describe the discourse struc-
ture, among which the Rhetorical Structure The-
ory (RST) (Mann and Thompson, 1988) is one of
the most popular. RST divides a text into min-
imal atomic units, called Elementary Discourse
Units (EDUs). It then forms a tree representa-
tion of a discourse called a Discourse Tree (DT)
using rhetorical relations (e.g., Elaboration, Ex-
planation, etc) as edges, and EDUs as leaves.
EDUs linked by a rhetorical relation are also dis-
tinguished based on their relative importance in
conveying the author?s message: nucleus is the
central part, whereas satellite is the peripheral
part.
We use a publicly available state-of-the-art dis-
course parser (Joty et al., 2013)
1
to generate a
DT for each product review. Figure 1 (a) and (b)
show DTs for two sample reviews where dotted
edges identify the satellite spans. DT1 in Figure 1
(a) shows that review R1 consists of three EDUs
with two relations Elaboration and Background
between them. It also shows that the first EDU
(i.e. I love camera) is the nucleus (shown by solid
line) of the relation Elaboration and so the rest of
the document (EDUs 2 and 3) is less important and
aims at elaborating on what the author meant in
the first EDU. Similarly, the structure shows that
the third EDU is mentioned as background infor-
mation for EDU2 and so is less important for real-
izing the core meaning of the document.
After obtaining the DTs, we remove all words
from the text spans of each EDU, except the aspect
words. Thus, for each review, we have a DT where
a leaf node represents the aspects occurring in the
corresponding EDU. Note that there may be EDUs
containing no aspects in a review. In such cases,
we keep the corresponding node and mark it with
no aspect. We call the resulting tree an Aspect-
based Discourse Tree (ADT) which will be used
in the next components. Figure 1 (c) and (d) show
ADTs generated from DTs.
4 Aspect Rhetorical Relation Graph
(ARRG)
In the second component, we aim at generat-
ing an ARRG for a product, based on the ADTs
which are the output from the previous compo-
nent. There are two motivations behind aggregat-
ing the ADTs and building the ARRG: i) while
each ADT can be rather noisy because of the infor-
mal language of the reviews and inaccuracies from
1
http://alt.qcri.org/discourse/Discourse Parser Dist.tar.gz
1604
Background
Elaboration
I love this 
camera
I  am amazed at 
the quality of 
photos
that I have took 
simply by using the 
auto mode
great camera !
It gives tons of 
control for 
photo buffs
but still has an 
auto mode
for the novice 
to use
INPUT:
R1: camera[+2], photo quality[+3], auto mode[+2]##I love this camera, I am amazed at the quality of photos that I have took simply using the auto mode
R2: camera[+2], control[+2], auto mode [+1]#great camera! It gives tons of control for photo buffs but still has an auto mode for the novice to use
camera
 photo 
quality
auto mode
Background
Elaboration
camera
control
auto mode
camera
-
(camera, Elaboration, photo quality, 0.5)
(camera, Elaboration, auto mode, 0.33)
(photo quality, Background, auto mode, 0.75)
(camera, Elaboration, auto mode, 0.375)
(camera, Elaboration, control, 0.5)
(control, Contrast, auto mode, 0.66)
Elaboration
Contrast
Elaboration
Elaboration
Contrast
Elaboration
camera
 photo quality
auto mode
control
E
l
a
b
o
r
a
t
i
o
n
,
0
.
5
Background,0.75
E
l
a
b
o
r
a
t
i
o
n
,
0
.
7
0
5
Elaboration,0.5
C
o
n
t
r
a
s
t
,
0
.
6
6
8 4
9
5
camera
 photo quality
control
E
l
a
b
o
r
a
t
i
o
n
,
0
.
5
Elaboration,0.5
camera
 photo quality
control
E
l
a
b
o
r
a
t
i
o
n
E
l
a
b
o
r
a
t
i
o
n
(d) ADT2
(c) ADT1(a) DT1
(b) DT2 (f) ARRG
(h) AHT
(g) ARRG-subgraph
(e) aspect relation tuples extracted from ADTs
OUTPUT:
All reviewers who commented on the camera, thought that it was really good mainly because of the photo quality. Accordingly, about half of the reviewers commented 
about the control and they thought it was fine.
(
i
)
 
M
i
c
r
o
p
l
a
n
n
i
n
g
S
e
n
t
e
n
c
e
 
r
e
a
l
i
z
a
t
i
o
n
Wednesday, May 28, 14
Figure 1: A simple example illustrating different components of our summarization framework.
automatic discourse parsing, aggregating all the
ADTs can reveal more reliable information; and
ii) the aggregated information highlights the most
important aspects overall as well as the strongest
connection between the aspects. This information
can effectively drive the content selection and ab-
stract generation phases.
ARRG is a directed graph in which we allow
multiple edges between two vertices. In ARRG,
vertices represent aspects. We associate to each
aspect/node an importance measure that aggre-
gates all the P/S values that the aspect receives
in all the reviews. By following (Carenini et al.,
2013), let PS(a) be the set of P/S values that an
aspect a receives. The direct measure of impor-
tance of the aspect is defined as:
dir-moi(a) =
?
ps?PS(a)
ps
2
(1)
In ARRG, edges indicate existence of a
rhetorical relation between text spans of a re-
view in which the aspects occurred. Edges are
labeled with the type of the relation as well
as a weight indicating our confidence in the
presence of the relation between the two aspects.
In ARRG, an edge with label r, w from node
u to node v, u
r, w
???? v, indicates the existance
of a relation r with confidence w between two
aspects u and v. Also, the direction of the arrow
indicates that u and v occurred in the satellite
and nucleus spans respectively. For example,
photo quality
elaboration, 0.8
???????????? camera indicates
that there is a high confidence (0.8) that aspect
photo quality was used in a text span to elaborate
aspect camera. Moreover, camera is a more
important aspect compared to photo quality.
To build ARRG, we use all the ADTs that are
output of the previous component (one for each
review). From each ADT
j
, we extract all tuples
of the form (u, r, v, w) in which u is an aspect oc-
curring in a satellite span, v is an aspect occurring
in a nucleus span, r is a relation type and w is the
weight of the tuple computed as follows:
w = 1?0.5
|EDUs between u and v|
|total EDUs in ADT
j
|
?0.5
d
r
d
(2)
where, |.| indicates cardinality of a set. d indi-
cates the depth of the ADT
j
and d
r
indicates the
depth of the sub-tree of ADT
j
rooted at relation
r. Equation 2 weighs a tuple based on two factors:
(i) the relative distance of the EDUs in which the
two aspects u and v participating in relation r oc-
cur. The intuition is that aspects occurring in close
proximity to each other are more related; and (ii)
the depth of the sub-tree at the point of the rela-
tion relative to the depth of the whole ADT
j
. This
is because as we move from leaves to the root of
a DT, the accuracy of the rhetorical structure has
been shown to decrease. Also, at higher levels
of an ADT (intra-sentential relations), it is more
1605
likely that aspects are related through non adjacent
EDUs and so are less strongly related. Figure 1 (e)
shows tuples extracted from sample ADTs.
Notice that every two aspects u and v may be
related by the same relation more than once in an
ADT for a review. Thus, we might have i tuples
with the same u,r, and v but confidence weights
which are not necessarily the same. From every
ADT
j
, we extract all (u, r, v, w
ij
) and select the
one with maximum confidence. We then aggre-
gate the selected tuples extracted from different
reviews. Putting these two steps together, for ev-
ery two aspects u and v related by relation r, we
obtain a single tuple (u, r, v, w?) where
w? =
?
j
max
i
w
ij
(3)
Figure 1 (f) shows an example ARRG built for the
sample reviews.
5 Content Selection and Structuring
The content of the summary is selected by extract-
ing from ARRG a subgraph containing the most
important aspects. Such content is then structured
by transforming the subgraph into an aspect hier-
archy.
5.1 Subgraph Extraction
In ARRG aspects/nodes are weighted by how fre-
quently and strongly they are evaluated in the re-
views (i.e, dir-moi) and edges are weighted by
how frequently and strongly the corresponding as-
pects are rhetorically related in the discourse trees
(Equation 3). In content selection, we want to
extract aspects that not only have high weight,
but that are also linked with heavy edges to other
heavy aspects. This problem can be effectively
addressed by Weighted Page Rank (WPR) (Xing
and Ghorbani, 2004). WPR takes the importance
of both the in-links and out-links of the aspects
into account and distributes rank scores based on
the weights of relations between aspects. In this
way, the heavier aspect nodes, that are either in
the nuclei of many relations or in the satellites of
relations with other heavy aspects, are promoted.
We then update the weight of nodes (aspects) with
the new score from WPR. Finally, we rank nodes
based on their updated score moi and select the
top N aspects.
moi(a) = ?dir-moi(a) + (1??)WPR(a) (4)
Here ? is a coefficient that can be tuned on a de-
velopment set or can be set to 0.5 without tuning.
Figure 1 (g) shows an example subgraph selected
from the sample ARRG.
5.2 Aspects Subgraph to Aspects Hierarchy
Transformation
In this step, we generate a hierarchical tree struc-
ture for aspects. Such a tree structure helps to
navigate over aspects and can be easily traversed
to find certain aspects and their relation to their
parent or children. The hierarchy of aspects also
matches the intuition that the root node is the most
frequent and general aspect (often the product) and
as the depth increases, nodes represent more spe-
cific aspects of the product with less frequency and
weight.
To obtain a hierarchical tree structure from the
extracted subgraph, we first build an undirected
graph as follows: we merge the edges connecting
two nodes and consider the sum of their weights
as the weight of the merged graph. We also ignore
the relation direction for the purpose of generat-
ing the tree. We then find the Maximum Span-
ning Tree of the undirected subgraph and set the
highest weighted aspect as the root of the tree.
This process results in a useful knowledge struc-
ture of aspects with their associated weight and
sentiment polarity connected with the rhetorical
relations called Aspect Hierarchical Tree (AHT).
Figure 1 (h) shows the generated AHT from the
sub-graph.
6 Abstract Generation
The automatic generation of a natural language
summary in our system involves the following
tasks (Reiter and Dale, 2000): (i) microplanning,
which covers lexical selection; and (ii) sentence
realization, which produces english text from the
output of the microplanner.
6.1 Microplanning
Once the content is selected and structured, it is
passed to the microplanning module which per-
forms lexical choice. Lexical choice is an impor-
tant component of microplanning. Lexical choice
is formulated in our system based on a ?formal?
style, language ?variability? and ?fluent? connec-
tivity among other lexical units. Table 1 demon-
strates our lexical choice strategy.
1606
Quantifiers:
if (relative-number == 1) : [?All users (x people) who commented about the aspect?, ?All costumers (x people) that reviewed the aspect?, ...]
if (relative-number >= 0.8) : [?Almost all users commented about the aspect and they?, ?Almost all costumers mentioned the aspect and they?, ...]
if (relative-number >= 0.6) : [?Most users commented about the aspect and they mainly?, ?Most shoppers mentioned aspect and they?, ...]
if (relative-number >= 0.45) : [?Almost half of the users commented about the aspect and they?, ?Almost 50% of the shoppers mentioned the aspect and they?, ...]
if (relative-number >= 0.2) : [?About y% of the reviewers commented about the aspect and they?, ?Around y% of the shoppers mentioned the aspect and they?, ...]
if (relative-number >= 0.0) : [?z reviewers commented about the aspect and in overall they?, ?z shoppers mentioned about the aspect and they?, ...]
Polarity verbs:
if (controversial(aspect)) : [?had controversial opinions about it?, ?expressed controversial opinions about this feature?, ...]
else: if (average <= ?2) : [?hated it?, ?felt that it was very poor?, ?thought that it was very poor?, ...]
if (average <= ?1) : [?disliked it?, ?felt that it was poor?, ?thought that it was poor?, ...]
if (average < 0) : [?did not like it?, ?felt that it was weak?, ?thought that it was weak?, ...]
if (average == 0) : [?did not express any strong positive or negative opinion about it?, ...]
if (average <= +1) : [?liked it?, ?felt that it was fine?, ?thought that it was satisfactory?, ...]
if (average <= +2) : [?absolutely liked it?, ?really liked this feature?, ?felt that it was a really good feature?, ?thought that it was really good?, ...]
if (average <= +3) : [?loved it?, ?felt that it was great?, ?thought that it was great?, ...]
Connectives
[?Also, related to the aspect?, ?Accordingly, ?, ?Moreover, regarding the aspect, ? ,?In relation to the aspect, ?, ?Talking about the aspect, ?, ...]
Table 1: Microplanning strategy for lexical choice. The selected lexical items will fill the template in the
realization step.
Sentence realization templates:
First sentence templates:
if (polarity-agreement(root,highest-weighted-child) & connecting-relation == [elaboration, explain, cause, summary, same-unit, background, evidence, justify]):
?quantifier + polarity-verb + ?mainly because of the? + highest-weighted-child?
else: ?quantifier + polarity-verb?
First level children (aspects) sentences templates:
?connective + ?, ? + quantifier + ? ? + polarity-verb?
Supporting sentences templates:
if (#children(aspect)==1): ?connective + quantifier + verb ?
elseif (#children(aspect)>1 & polarity-agreement(children)): ?connective + quantifier + verb + [and, similarly, while, ...] + quantifier + verb?
elseif (#children(aspect)>1 & !polarity-agreement(children)): ?connective + quantifier + verb + [but, in contrast, on contrary, ...] + quantifier + verb?
Table 2: Sentence realization templates.
Quantifiers: for each aspect, a quantifier is se-
lected based on both the absolute and relative
number of users whose opinions contributed to the
evaluation of the aspect.
Polarity verbs: for each aspect, a polarity verb is
selected based on the average sentiment polarity
strength for that aspect. Although the average, in
most cases, can be a good metric to evaluate the
polarity of an aspect, it fails when the distribution
of evaluations is centered on zero, for instance, if
there are equal numbers of positive and negative
evaluations (i.e., controversial). To partially solve
this problem, we first check whether the aspect
evaluation is controversial by applying the formula
proposed by (Carenini and Cheung, 2008). In the
case of controversiality, our microplanner selects
a lexical item to express the controversiality of the
aspect. In other cases, we use the average and se-
lect the polarity verb based on that.
Connectives: in order to form more fluent and
readable sentences and to increase the language
variability, we randomly select our connectives
from the list shown in Table 1. Moreover, when
a parent aspect (excluding the root in AHT) has
two children, they are connected by one of the co-
ordinating conjunction ?[and, similarly]? if they
agree on polarity, and they will be connected by
a choice of ?[on the contrary, in contrast]? other-
wise (see Supporting sentences templates in Table
2). As an alternative we could have selected con-
nectives based on the discourse relations specified
in the aspects tree. However, this is left as future
work.
6.2 Sentence Realization
The realization of our abstract generation is per-
formed by applying a rather simple and compre-
hensive template-based strategy. Depending on
the specific lexical choice in microplanning step,
an appropriate template and corresponding fillers
are selected as shown in Table 2. We develop three
different templates: i) generates the first abstract
sentence; ii) generates the abstract sentence for the
aspects with no children; and iii) generates sup-
porting sentences for aspects with children.
For illustration, assuming that we apply this
strategy to a 5-node variation of the AHT in Figure
1 (h), where the aspect ?control? has two children
?auto mode? and ?setting?, we obtain ?All review-
ers (45 people) who commented on the camera,
thought that it was really good mainly because of
the photo quality. Accordingly, about 24% of the
reviewers commented about the control and they
1607
thought it was fine. Also, related to the control, 7
users expressed their opinion about the auto mode
and they liked it, similarly, 6 shoppers commented
about the setting and they thought that it was sat-
isfactory.?
7 Experimental Setup
7.1 Dataset and Baselines
We conduct our experiments using the customer
reviews of twelve products obtained from (Hu and
Liu, 2004a): 4 digital cameras, 1 DVD player, 1
MP3 player, 2 routers, 2 phones, 1 diaper and 1
antivirus. The reviews were collected from Ama-
zon.com and Cnet.com. We use manually anno-
tated aspects and their associated sentiment from
the same dataset.
We compare the summaries generated by our
system with two state-of-the-art extractive base-
lines and a simpler version of our abstractive sys-
tem, as follows:
1) MEAD-LexRank (LR): we use the LexRank
(Erkan and Radev, 2004) implementation inside
the MEAD summarization framework (Radev et
al., 2004), which outperforms other algorithms
implemented in the MEAD framework.
2) MEADStar (MEAD*): a state-of-the-art ex-
tractive opinion summarization system (Carenini
et al., 2013), which is adapted from the
open source summarization framework MEAD.
MEAD* orders aspects by the number of sen-
tences evaluating that aspect, and selects a sen-
tence from each aspect until it reaches the word
limit. The sentence that is selected for each aspect
is the one with the highest sum of polarity/strength
evaluations for any aspect.
3) Simple Abstractive (SA): we sort the aspects
of each product based on dir-moi (Equation 1).
Then, for each aspect, we generate a sentence
based on a simple template ?quantifier + polarity-
verb? until the summary reaches the word limit.
We limit the length of our summaries to 150
words. In our experiment we use the default pa-
rameter in Equation 4 without tuning (i.e. ? =
0.5). Our system starts the content selection pro-
cess with 10 aspects and generates a summary
based on a AHT with 10 aspects. We add one as-
pect, reproduce the AHT and regenerate the sum-
mary. We repeat this process until the word limit
is reached.
7.2 Evaluation Framework
On one hand, the lack of product reviews datasets
with human written summaries, and on the other
hand, the difficulty of generating human-written
summaries for reviews, makes review summary
evaluation a very challenging task.
We evaluate the summaries generated by our
system by performing two user studies based on
pairwise preferences using a popular crowdsourc-
ing service.
2
The user preference evaluation is an
effective method for opinion summarization (e.g.,
(Lerman et al., 2009)). The main motivations be-
hind pairwise preferences evaluation is two-fold:
i) raters can make a preference decision more ef-
ficiently than a scoring judgment; and ii) rater
agreement is higher in preference decisions than
in scoring judgments (Ariely et al., 2003).
In both user studies, for each product, we run
six pairwise comparisons for four summaries. In
each rating assignment, two summaries of the
same product were placed in random order. Raters
were shown the name of each product along with
the relevant summaries and were asked to express
their preference for one summary over the other
using a simple set of criteria. For two summaries
S
1
and S
2
raters should choose one of the follow-
ing three options: 1) Prefer S
1
, 2) Prefer S
2
, 3) No
preference.
Raters were specifically instructed that their rat-
ing should express ?overall satisfaction with the
information provided by the summary?. Raters
were also asked to provide a brief comment jus-
tifying their choice. Over 48 raters participated in
each study, and each comparison was evaluated by
at least five raters generating more than 360 judg-
ments for each user study. We pre-select the high
skilled raters to ensure a higher quality results.
The main difference between the two user stud-
ies is that in ?user study 1?, we show two sum-
maries to the raters and ask them to choose the one
they prefer without showing them the original re-
views. In contrast, in ?user study 2?, we show two
summaries with links to the full text of the reviews
for the raters to explore. In order to make sure that
the raters read the reviews, we ask them to write
a short summary of the reviews before rating the
automatic summaries. We ran two different user
studies because: i) for each product there might be
many reviews to be included; ii) there is no guar-
anty that raters, in various evaluation settings, read
2
www.crowdflower.com
1608
System I vs System II Agreement No preference Preferred Sys I Preferred Sys II
User Studies
1 2 1 2 1 2 1 2
LR vs MEAD*
0.33 0.75 7% 6% 35% 20% 58% 74%
LR vs SA
0.42 0.83 0% 0% 38% 21% 62% 79%
LR vs Our System
0.50 1.00 0% 3% 26% 13% 74% 84%
MEAD* vs SA
0.58 0.83 0% 0% 38% 20% 62% 80%
MEAD* vs Our System
0.67 0.50 0% 3% 25% 30% 75% 67%
SA vs Our System
0.42 0.50 12% 11% 23% 32% 65% 57%
Table 3: Results of pairwise preference user studies. Statistically significant improvements (p < 0.01)
over the baselines are demonstrated by bold fonts. Italic fonts indicate statistical significance (p < 0.01)
of abstractive methods (SA and Our System) over extractive approaches (LR and MEAD*).
Systems LR MEAD* SA Our System
User Studies
1 2 1 2 1 2 1 2
Preference
33% 18% 41% 41% 49% 63% 71% 69%
Table 4: System preference results. Statistically significant improvements (p < 0.01) over the baselines
are demonstrated by bold fonts.
the reviews (partially or completely); and iii) there
is no evidence regarding the depth that each rater
would look into the reviews. Therefore, choosing
between user study 1 and 2 is not a straightforward
decision. In other words, designing the two user
studies in this way helps us to answer the ques-
tion: ?Does the fact that raters can read all the
reviews affect their ratings??.
8 Results
This section provides a quantitative and qualitative
analysis of the evaluation results
3
.
8.1 Quantitative Analysis
Quantitative results for both user studies are
shown in Table 3. The second column indicates
the percentage of judgments for which the raters
were in agreement. Agreement here is a weak
agreement, where four (out of five) raters are de-
fined to be in agreement if they all gave the same
rating. The next three columns indicate the per-
centage of judgments for each preference cate-
gory, grouped into two user studies. In addi-
tion, we measure the preference for each system
in both user studies (Table 4). For each system,
the preference is the number of times raters prefer
the system, divided by the total number of judg-
ments for that system (e.g., if A is preferred over
3
The evaluation results and summaries obtained
from CrowdFlower are publicly available and can
be downloaded from: https://www.cs.ubc.
ca/cs-research/lci/research-groups/
natural-language-processing/reviews/
user_study_results.zip
B 10 out of 30 times, and A is preferred over C
15 out of 20 times, the overall preference of A is
(10+15)/(30+20)=50%)
Abstractive vs. Extractive: the results of our sys-
tem and SA in Table 3 show statistically signifi-
cant improvements in pairwise preference over ex-
tractive baselines (LR and MEAD*) in both user
studies.
4
Moreover, the results of overall prefer-
ence in Table 4 demonstrates that two abstractive
systems are preferred over the extractive ones in
both studies. This further supports the findings in
the previous studies (e.g., (Carenini et al., 2013))
that users prefer abstractive summarization. We
can observe that, in both user studies, raters prefer
our system over other abstractive and extractive
baselines. Also, the highest pairwise preference
percentages occur comparing an extractive and an
abstractive system (e.g., LR vs Our System).
Abstractive Systems: the raters prefer our system
over SA in both user studies (65% and 57%), and
our system ranks first in our pairwise preference
user studies. Knowing that both systems are ab-
stractive and the differences between them comes
from using the rhetorical structure in the content
selection and abstract generation phases, proves
the effectiveness of using rhetorical structure and
relations in abstractive summarization of reviews.
Extractive Systems: the result in Table 3 and 4
demonstrate that raters prefer MEAD* over LR.
Although both systems are extractive, the MEAD*
system has been proposed for extractive opinion
4
The statistical significance tests was calculated by ap-
proximate randomization, as described in (Yeh, 2000).
1609
Preference Sys 1 to Sys 2 Reasons Examples of preference justification taken from the raters comments
Our System to LR and MEAD* Readability, coverage of aspects,
aggregation of opinions
better wording, more objective, more depth, I like the stats, more detail about
people opinion, less personal experience, detail comparison from different
reviews, a summary in a summary, mentions more features, ...
LR and MEAD* to Our System Descriptiveness, personal point of
views, product capabilities
explain how the product is positive, good characteristics about the product,
has lot more to tell, more descriptive about features, personal perspective,
not only characteristics but also ability, more true to the product itself, ...
Our System to SA The relations between the aspects,
more language variability
provides a bit more information, is very complete, not repetitive, more ele-
gant, coherent, .....
SA to Our System Simpler structure, more aspects written better, has touched variety of features, ...
Table 5: System preference results. The reasons are classified based on raters justifications preferring
the underlined systems.
summarization. In contrast, LR is a generic ex-
tractive summarization system which is not opti-
mized for opinion summarization. This also fur-
ther demonstrates the need for opinion and reviews
summarization systems.
User Study 1 vs. User Study 2: the first in-
teresting observation is that, although the over-
all ranking of systems in both user studies does
not change, there are some changes in the re-
sults. This indicates that reading the reviews ef-
fects preference decisions. We can observe that
in all cases except one (MEAD* vs Our System)
the agreement between the raters increases sig-
nificantly when they are given the reviews. This
can be interpreted as reading the reviews helps
the rater to choose a better summary easier and
more effectively. Moreover, we calculate the over-
all agreement for both user studies.
5
Case study 2
reports a higher overall agreement (70%) in com-
parison with the user study 1 (65%). This further
proves our finding that showing the reviews can
help the raters with their preference judgment.
In Table 3, the preference of sys 2 (last col-
umn) significantly rises for all cases when com-
pared with the LR system. This proves that raters
strongly prefer the summaries that cover opinion-
ated sentences, specifically when they are exposed
to the reviews. The same result is reflected in Ta-
ble 4, where the overall preference of LR drops
when the raters are given the reviews. We also ob-
serve a significant rise in preference of sys 2 when
MEAD* is compared with SA (Table 3) and in the
overall preference of SA (Table 4) in user study
2. This proves that raters become more confident
in preferring an abstractive summary over an ex-
tractive one when the reviews are given to them.
In contrast, we notice that the preference of sys
2 drops comparing ?MEAD* vs Our System? and
?SA vs Our System?. Knowing that the drop is
5
The agreement is calculated based on 100 randomly sam-
pled units selected from our crowdsourcing job.
not significant and the the overall ranking of sys-
tems remains unchanged, this case is less straight
forward to interpret.
8.2 Qualitative Analysis
We collect and group the rater justifications in the
results we obtain by crowdsourcing our evaluation
framework, when preferring a summary over an-
other, in Table 5. To make the comparison more
clear, Example 1 shows the summaries generated
by MEAD* and our system.
Comparing our system with the extractive base-
lines, raters? justifications are classified in three
main categories. Although the language of the ex-
tractive summaries is less formal, raters often pre-
fer our system in terms of presentation and lan-
guage. They justify their selections by expressing
phrases such as ?better grammar? or ?fewer er-
rors?. They also comment about the coverage of
aspects in the summaries generated by our system
and they realize that our system was capable of
aggregating the opinions for each aspect. In con-
trast, when they prefer the extractive summaries,
they like the descriptive language of the summary
and the technical details of the products that were
missing in our system summaries.
We also notice that raters realize the usage of
structure (AHT) in our system (both of content se-
lection and summary generation) and they appre-
ciate it by expressing phrases such as ?very com-
plete?, ?more elegant? or ?related features?. In
contrast, they sometimes appreciate a simpler lan-
guage in summaries generated by SA. Moreover,
few raters prefer the higher coverage in SA sum-
maries. This is mainly because not using connec-
tives and structure in SA leaves more space to in-
clude more aspects.
1610
Product: Nikon Coolpix 4300
MEAD*: it is very compact but the controls are so well designed that
they ?re still easy to use . It ?s easy for beginners to use , but has features
that more serious photographers will love , so it ?s an excellent camera
to grow into . But overall this is a good camera with a ? really good ?
picture clarity ; an exceptional close-up shooting capability .The battery
life is very good , i got about 90 minutes with the lcd turned on all
the time , the first time around , and i have been using it with the lcd
off every now and then , and have yet needed to recharge it . Yes ,
the picture quality and features which are too numerous to mention are
unmatched for any camera in this price range.
Our System: All reviewers (34 people), who commented on the cam-
era, felt that it was really good mainly because of the picture. Around
26% of the reviewers expressed their opinion about the picture quality
and they really liked it. Around 24% of the reviewers noted the use
and they thought that it was satisfactory. Talking about the use, around
24% of the reviewers expressed their opinion about the size and they
felt that it was fine. Only 6 reviewers commented about the scene mode
and in overall they thought that it was satisfactory. Moreover, regarding
the scene mode, 4 shoppers mentioned about the manual mode and they
thought that it was satisfactory, and similarly only 4 reviewers com-
mented about the auto mode and in overall they did not express any
strong positive or negative opinion about it. Only 4 costumers men-
tioned the software and they felt that it was really good.
Example 1. Summaries generated by our system
and MEAD* baseline for the Nikon Coolpix 4300
camera. For brevity we exclude other baselines.
9 Conclusions
We have presented a framework for abstractive
summarization of product reviews based on dis-
course structure. For content selection, we pro-
pose a graph model based on the importance
and association relations between aspects, that as-
sumes no prior domain knowledge, by taking ad-
vantage of the discourse structure of reviews. For
abstract generation, we propose a product inde-
pendent template-based natural language genera-
tion (NLG) framework that takes aspects and their
structured relation as input and generates an ab-
stractive summary. Quantitative evaluation results,
based on two pairwise preference user studies,
show substantial improvement over extractive and
abstractive baselines, including MEAD*, which
is considered a state-of-the-art opinion extractive
summarization system, and a simpler version of
our abstractive system. In future work, we plan
to extend the microplanning phase by taking ad-
vantage of the highly weighted rhetorical relations
between the aspects and select connective phrases
based on the discourse relations specified in the
aspects tree. In addition, we plan to develop and
evaluate an end-to-end system, in which the aspect
extraction and polarity estimation of aspects are
automated. In this way, we can achieve an end-to-
end automatic summarizaion system for product
reviews.
Acknowledgments
This work was supported in part by Swiss Na-
tional Science Foundation (PBTIP2-145659) and
NSERC Business Intelligence Network. We
would like to thank the anonymous reviewers for
their valuable comments. We also acknowledge
Shafiq Rayhan Joty for his help regarding rhetori-
cal parser.
References
Dan Ariely, George Loewenstein, and Drazen Prelec.
2003. ?Coherent Arbitrariness?: Stable Demand
Curves Without Stable Preferences. Quarterly Jour-
nal of Economics, 118:73?105.
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse: A
preliminary study. In Coling 2008: Companion vol-
ume: Posters and Demonstrations, pages 5?8.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings
of the 37th annual meeting of the Association for
Computational Linguistics on Computational Lin-
guistics, ACL ?99, pages 550?557, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Giuseppe Carenini and Jackie Chi Kit Cheung. 2008.
Extractive vs. nlg-based abstractive summarization
of evaluative text: the effect of corpus controversial-
ity. In INLG ?08: Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 33?41, Morristown, NJ, USA. Association for
Computational Linguistics.
Giuseppe Carenini, Jackie Chi Kit Cheung, and Adam
Pauls. 2013. Multi-document summarization
of evaluative text. Computational Intelligence,
29(4):545?576.
Giuseppe Di Fabbrizio, Amanda Stent, and Robert
Gaizauskas. 2014. A hybrid approach to multi-
document summarization of opinions in reviews. In
Proceedings of the 8th International Natural Lan-
guage Generation conference, INLG 2014.
G?unes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22(1):457?479,
December.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
340?348, Stroudsburg, PA, USA. Association for
Computational Linguistics.
1611
Minqing Hu and Bing Liu. 2004a. Mining and sum-
marizing customer reviews. In Proceedings of ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining 2004 (KDD 2004), pages 168?177,
Seattle, Washington.
Minqing Hu and Bing Liu. 2004b. Mining opinion
features in customer reviews. In Proceedings of the
Nineteenth National Conference on Artificial Intelli-
gence (AAAI-2004).
Minqing Hu and Bing Liu. 2006. Opinion feature
extraction using class sequential rules. In Pro-
ceedings of AAAI 2006 Spring Sympoia on Compu-
tational Approaches to Analyzing Weblogs (AAAI-
CAAW 2006).
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 486?496,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Hyun Duk Kim, Kavita Ganesan, Parikshit Sondhi, and
ChengXiang Zhai. 2011. Comprehensive review of
opinion summarization.
Hyun Duk Kim, Malu Castellanos, Meichun Hsu,
ChengXiang Zhai, Umeshwar Dayal, and Riddhi-
man Ghosh. 2013. Compact explanatory opinion
summarization. In Proceedings of the 22Nd ACM
International Conference on Conference on Infor-
mation &#38; Knowledge Management, CIKM ?13,
pages 1697?1702, New York, NY, USA. ACM.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1630?1639, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: Evaluat-
ing and learning user preferences. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?09, pages 514?522, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Hugo Liu and Push Singh. 2004. Conceptnet: A prac-
tical commonsense reasoning toolkit. BT Technol-
ogy Journal, 22(4):211?226.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In Proceedings of the 18th International
Conference on World Wide Web, WWW ?09, pages
131?140, New York, NY, USA. ACM.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Subhabrata Mukherjee and Sachindra Joshi. 2013.
Sentiment aggregation using conceptnet ontology.
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing, IJCNLP
2013.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42Nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natu-
ral Language Processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD ? A plat-
form for multidocument multilingual text summa-
rization. In Conference on Language Resources and
Evaluation (LREC), Lisbon, Portugal.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press, New York, NY, USA.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In
HLT-NAACL, pages 300?307.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, pages 170?179, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ivan Titov and Ryan T. McDonald. 2008. A joint
model of text and aspect ratings for sentiment sum-
marization. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics, June 15-20, 2008, Columbus, Ohio, USA,
ACL 2008, pages 308?316. Association for Compu-
tational Linguistics.
Rakshit S. Trivedi and Jacob Eisenstein. 2013. Dis-
course connectors for latent subjectivity in sentiment
analysis. In HLT-NAACL, pages 808?813. The As-
sociation for Computational Linguistics.
1612
Wenpu Xing and Ali Ghorbani. 2004. Weighted pager-
ank algorithm. In Proceedings of the Second Annual
Conference on Communication Networks and Ser-
vices Research, CNSR ?04, pages 305?314. IEEE
Computer Society.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics - Volume 2, COLING ?00, pages 947?
953, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1613
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 894?902,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interpretation and Transformation for Abstracting Conversations
Gabriel Murray
gabrielm@cs.ubc.ca
Giuseppe Carenini
carenini@cs.ubc.ca
Department of Computer Science, University of British Columbia
Vancouver, Canada
Raymond Ng
rng@cs.ubc.ca
Abstract
We address the challenge of automatically ab-
stracting conversations such as face-to-face
meetings and emails. We focus here on
the stages of interpretation, where sentences
are mapped to a conversation ontology, and
transformation, where the summary content
is selected. Our approach is fully developed
and tested on meeting speech, and we subse-
quently explore its application to email con-
versations.
1 Introduction
The dominant approach to the challenge of auto-
matic summarization has been extraction, where in-
formative sentences in a document are identified and
concatenated to form a condensed version of the
original document. Extractive summarization has
been popular at least in part because it is a binary
classification task that lends itself well to machine
learning techniques, and does not require a natural
language generation (NLG) component. There is ev-
idence that human abstractors at times use sentences
from the source documents nearly verbatim in their
own summaries, justifying this approach to some ex-
tent (Kupiec et al, 1995). Extrinsic evaluations have
also shown that, while extractive summaries may be
less coherent than human abstracts, users still find
them to be valuable tools for browsing documents
(He et al, 1999; McKeown et al, 2005; Murray et
al., 2008).
However, these same evaluations also indicate
that concise abstracts are generally preferred by
users and lead to higher objective task scores. The
limitation of a cut-and-paste summary is that the
end-user does not know why the selected sentences
are important; this can often only be discerned by
exploring the context in which each sentence origi-
nally appeared. One possible improvement is to cre-
ate structured extracts that represent an increased
level of abstraction, where selected sentences are
grouped according to phenomena such as decisions,
action items and problems, thereby giving the user
more information on why the sentences are being
highlighted. For example, the sentence Let?s go with
a simple chip represents a decision. An even higher
level of abstraction can be provided by generating
new text that synthesizes or extrapolates on the in-
formation contained in the structured summary. For
example, the sentence Sandra and Sue expressed
negative opinions about the remote control design
can be coupled with extracted sentences containing
these negative opinions, forming a hybrid summary.
Our summarization system ultimately performs both
types of abstraction, grouping sentences according
to various sentence-level phenomena, and generat-
ing novel text that describes this content at a higher
level.
In this work we describe the first two components
of our abstractive summarization system. In the in-
terpretation stage, sentences are mapped to nodes
in a conversation ontology by utilizing classifiers
relating to a variety of sentence-level phenomena
such as decisions, action items and subjective sen-
tences. These classifiers achieve high accuracy by
using a very large feature set integrating conversa-
tion structure, lexical patterns, part-of-speech (POS)
tags and character n-grams. In the transformation
stage, we select the most informative sentences by
maximizing a function based on the derived ontol-
ogy mapping and the coverage of weighted enti-
ties mentioned in the conversation. This transforma-
tion component utilizes integer linear programming
(ILP) and we compare its performance with several
greedy selection algorithms.
We do not discuss the generation compo-
nent of our summarization system in this pa-
per. The transformation component is still ex-
894
tractive in nature, but the sentences that are se-
lected in the transformation stage correspond to
objects in the ontology and the properties link-
ing them. Specifically, these are triples of the
form < participant, relation, entity > where a
participant is a person in the conversation, an
entity is an item under discussion, and a relation
such as positive opinion or action item links the two.
This intermediate output enables us to create struc-
tured extracts as described above, with the triples
also acting as input to the downstream NLG com-
ponent.
We have tested our approach in summarization
experiments on both meeting and email conversa-
tions, where the quality of a sentence is measured
by how effectively it conveys information in a model
abstract summary according to human annotators.
On meetings the ILP approach consistently outper-
forms several greedy summarization methods. A
key finding is that emails exhibit markedly varying
conversation structures, and the email threads yield-
ing the best summarization results are those that are
structured similarly to meetings. Other email con-
versation structures are less amenable to the current
treatment and require further investigation and pos-
sibly domain adaptation.
2 Related Research
The view that summarization consists of stages of
interpretation, transformation and generation was
laid out by Sparck-Jones (1999). Popular ap-
proaches to text extraction essentially collapse inter-
pretation and transformation into one step, with gen-
eration either being ignored or consisting of post-
processing techniques such as sentence compres-
sion (Knight and Marcu, 2000; Clarke and Lapata,
2006) or sentence merging (Barzilay and McKeown,
2005). In contrast, in this work we clearly separate
interpretation from transformation.
The most relevant research to ours is by Klein-
bauer et al (2007), similarly focused on meet-
ing abstraction. They create an ontology for the
AMI scenario meeting corpus (Carletta et al, 2005),
described in Section 5.1. The system uses topic
segments and topic labels, and for each topic seg-
ment in the meeting a sentence is generated that de-
scribes the most frequently mentioned content items
in that topic. Our systems differ in two major re-
spects: their summarization process uses human
gold-standard annotations of topic segments, topic
labels and content items from the ontology, while
our summarizer is fully automatic; and the ontology
used by Kleinbauer et al is specific not just to meet-
ings but to the AMI scenario meetings, while our
ontology applies to conversations in general.
While the work by Kleinbauer et al is among
the earliest research on abstracting multi-party dia-
logues, much attention in recent years has been paid
to extractive summarization of such conversations,
including meetings (Galley, 2006), emails (Rambow
et al, 2004; Carenini et al, 2007), telephone con-
versations (Zhu and Penn, 2006) and internet relay
chats (Zhou and Hovy, 2005).
Recent research has addressed the challenges of
detecting decisions (Hsueh et al, 2007), action items
(Purver et al, 2007; Murray and Renals, 2008) and
subjective sentences (Raaijmakers et al, 2008). In
our work we perform all of these tasks but rely on
general conversational features without recourse to
meeting-specific or email-specific features.
Our approach to transformation is an adaptation
of an ILP sentence selection algorithm described by
Xie et al (2009). We describe both ILP approaches
in Section 4.
3 Interpretation - Ontology Mapping
Source document interpretation in our system re-
lies on a simple conversation ontology. The ontol-
ogy is written in OWL/RDF and contains two core
upper-level classes: Participant and Entity. When
additional information is available about participant
roles in a given domain, Participant subclasses such
as ProjectManager can be utilized. The ontology
also contains six properties that express relations be-
tween the participants and the entities. For example,
the following snippet of the ontology indicates that
hasActionItem is a relationship between a meeting
participant (the property domain) and a discussed
entity (the property range).
<owl:ObjectProperty rdf:ID="hasActionItem">
<rdfs:domain rdf:resource="#Participant"/>
<rdfs:range rdf:resource="#Entity"/>
</owl:ObjectProperty>
Similar properties exist for decisions, actions,
problems, positive-subjective sentences, negative-
895
subjective sentences and general extractive sen-
tences (important sentences that may not match the
other categories), all connecting conversation par-
ticipants and entities. The goal is to populate the
ontology with participant and entity instances from
a given conversation and determine their relation-
ships. This involves identifying the important en-
tities and classifying the sentences in which they
occur as being decision sentences, action item sen-
tences, etc.
Our current definition of entity is simple. The en-
tities in a conversation are noun phrases with mid-
range document frequency. This is similar to the
definition of concept as defined by Xie et al (Xie
et al, 2009), where n-grams are weighted by tf.idf
scores, except that we use noun phrases rather than
any n-grams because we want to refer to the enti-
ties in the generated text. We use mid-range doc-
ument frequency instead of idf (Church and Gale,
1995), where the entities occur in between 10% and
90% of the documents in the collection. In Section 4
we describe how we use the entity?s term frequency
to detect the most informative entities. We do not
currently attempt coreference resolution for entities;
recent work has investigated coreference resolution
for multi-party dialogues (Muller, 2007; Gupta et
al., 2007), but the challenge of resolution on such
noisy data is highlighted by low accuracy (e.g. F-
measure of 21.21) compared with using well-formed
text (e.g. monologues).
We map sentences to our ontology?s object prop-
erties by building numerous supervised classifiers
trained on labeled decision sentences, action sen-
tences, etc. A general extractive classifier is also
trained on sentences simply labeled as important.
After predicting these sentence-level properties, we
consider a participant to be linked to an entity if
the participant mentioned the entity in a sentence in
which one of these properties is predicted. We give a
specific example of the ontology mapping using this
excerpt from the AMI corpus:
1. A: And you two are going to work together on
a prototype using modelling clay.
2. A: You?ll get specific instructions from your
personal coach.
3. C: Cool.
4. A: Um did we decide on a chip?
5. A: Let?s go with a simple chip.
Example entities are italicized. Sentences 1 and
2 are classified as action items. Sentence 3 is clas-
sified as positive-subjective, but because it contains
no entities, no < participant, relation, entity >
triple can be added to the ontology. Sentence
4 is classified as a decision sentence, and Sen-
tence 5 is both a decision sentence and a positive-
subjective sentence (because the participant is advo-
cating a particular position). The ontology is pop-
ulated by adding all of the sentence entities as in-
stances of the Entity class, all of the participants
as instances of the Participant class, and adding
< participant, relation, entity > triples for Sen-
tences 1, 2, 4 and 5. For example, Sentence 5 results
in the following two triples being added to the on-
tology:
<ProjectManager rdf:ID="participant-A">
<hasDecision rdf:resource="#simple-chip"/>
</ProjectManager>
<ProjectManager rdf:ID="participant-A">
<hasPos rdf:resource="#simple-chip"/>
</ProjectManager>
Elements in the ontology are associated with lin-
guistic annotations used by the generation compo-
nent of our system; since we do not discuss the gen-
eration task here, we presently skip the details of this
aspect of the ontology. In the following section we
describe the features used for the ontology mapping.
3.1 Feature Set
The interpretation component uses general features
that are applicable to any conversation domain. The
first set of features we use for ontology mapping are
features relating to conversational structure. These
are listed and briefly described in Table 1. The
Sprob and Tprob features measure how terms clus-
ter between conversation participants and conver-
sation turns. There are simple features measur-
ing sentence length (SLEN, SLEN2) and position
(TLOC, CLOC). Pause-style features indicate how
much time transpires between the previous turn, the
current turn and the subsequent turn (PPAU, SPAU).
For email conversations, pause features are based on
the timestamps between consecutive emails. Lexical
features capture cohesion (CWS) and cosine sim-
ilarity between the sentence and the conversation
(CENT1, CENT2). All structural features are nor-
malized by document length.
896
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cos. of conv. splits, w/ Sprob
COS2 cos. of conv. splits, w/ Tprob
PENT entro. of conv. up to sentence
SENT entro. of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob
Table 1: Features Key
While these features have been found to work
well for generic extractive summarization, we use
additional features for capturing the more specific
sentence-level phenomena of this research.
? Character trigrams We derive all of the char-
acter trigrams in the collected corpora and in-
clude features indicating the presence or ab-
sence of each trigram in a given sentence.
? Word bigrams We similarly derive all of the
word bigrams in the collected corpora.
? POS bigrams We similarly derive all of the
POS-tag bigrams in the collected corpora.
? Word pairs We consider w1, w2 to be a word
pair if they occur in the same sentence and w1
precedes w2. We derive all of the word pairs
in the collected corpora and includes features
indicating the presence or absence of each word
pair in the given sentence. This is essentially a
skip bigram where any amount of intervening
material is allowed as long as the words occur
in the same sentence.
? POS pairs We calculate POS pairs in the same
manner as word pairs, above. These are essen-
tially skip bigrams for POS tags.
? Varying instantiation ngrams We derive a
simplified set of VIN features for these exper-
iments. For each word bigram w1, w2, we fur-
ther represent the bigram as p1, w2 and w1, p2
so that each pattern consists of a word and a
POS tag. We include a feature indicating the
presence or absence of each of these varying
instantiation bigrams.
After removing features that occur fewer than five
times, we end up with 218,957 total features.
4 Transformation - ILP Content Selection
In the previous section we described how we
identify sentences that link participants and enti-
ties through a variety of sentence-level phenom-
ena. Having populated our ontology with these
triples to form a source representation, we now turn
to the task of transforming the source representa-
tion to a summary representation, identifying the <
participant, relation, entity > triples for which
we want to generate text. We adapt a method pro-
posed by Xie et al (2009) for extractive sentence
selection. They propose an ILP approach that cre-
ates a summary by maximizing a global objective
function:
maximize (1 ? ?) ?
?
i
wici + ? ?
?
j
ujsj (1)
subject to
?
j
ljsj < L (2)
where wi is the tf.idf score for concept i, uj is the
weight for sentence j using the cosine similarity to
the entire document, ci is a binary variable indicat-
ing whether concept i is selected (with the concept
represented by a unique weighted n-gram), sj is a
binary variable indicating whether sentence j is se-
lected, lj is the length of sentence j and L is the
desired summary length. The ? term is used to bal-
ance concept and sentence weights. This method se-
lects sentences that are weighted strongly and which
cover as many important concepts as possible. As
described by Gillick et al (2009), concepts and
sentences are tied together by two additional con-
straints:
?
j
sjoij ? ci ?i (3)
sjoij ? ci ?i,j (4)
897
where oij is the occurence of concept i in sentence
j. These constraints state that a concept can only be
selected if it occurs in a sentence that is selected,
and that a sentence can only be selected if all of its
concepts have been selected.
We adapt their method in several ways. As men-
tioned in the previous section, we use weighted noun
phrases as our entities instead of n-grams. In our
version of Equation 1, wi is the tf score of en-
tity i (the idf was already used to identify entities
as described previously). More importantly, our
sentence weight uj is the sum of all the posterior
probabilities for sentence j derived from the various
sentence-level classifiers. In other words, sentences
are weighted highly if they correspond to multiple
object properties in the ontology. To continue the
example from Section 3, the sentence Let?s go with
the simple chip may be selected because it represents
both a decision and a positive-subjective opinion, as
well as containing the entity simple chip which is
mentioned frequently in the conversation.
We include constraint 3 but not 4; it is possi-
ble for a sentence to be extracted even if not all
of its entities are. We know that all the sentences
under consideration will contain at least one en-
tity because sentences with no entities would not
have been mapped to the ontology in the form of
< participant, relation, entity > triples in the
first place. To begin with, we set the ? term at 0.75
as we are mostly concerned with identifying impor-
tant sentences containing multiple links to the on-
tology. In our case L is 20% of the total document
word count.
5 Experimental Setup
In this section we describe our conversation cor-
pora, the statistical classifiers used, and the evalu-
ation metrics employed.
5.1 Corpora
These experiments are conducted on both meeting
and email conversations, which we describe in turn.
5.1.1 The AMI Meetings Corpus
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Carletta
et al, 2005), where groups of four participants take
part in a series of four meetings and play roles within
a fictitious company. There are 140 of these meet-
ings in total, including a 20 meeting test set contain-
ing multiple human summary annotations per meet-
ing (the others are annotated by a single individual).
We report results on both manual and ASR tran-
scripts. The word error rate for the ASR transcripts
is 38.9%.
For the summary annotation, annotators wrote ab-
stract summaries of each meeting and extracted sen-
tences that best conveyed or supported the informa-
tion in the abstracts. The human-authored abstracts
each contain a general abstract summary and three
subsections for ?decisions,? ?actions? and ?prob-
lems? from the meeting. A many-to-many mapping
between transcript sentences and sentences from the
human abstract was obtained for each annotator. Ap-
proximately 13% of the total transcript sentences are
ultimately labeled as extracted sentences. A sen-
tence is considered a decision item if it is linked to
the decision portion of the abstract, and action and
problem sentences are derived similarly.
For the subjectivity annotation, we use annota-
tions of positive-subjective and negative-subjective
utterances on a subset of 20 AMI meetings (Wil-
son, 2008). Such subjective utterances involve
the expression of a private state, such as a pos-
itive/negative opinion, positive/negative argument,
and agreement/disagreement. Of the roughly 20,000
total sentences in the 20 AMI meetings, nearly 4000
are labeled as positive-subjective and nearly 1300 as
negative-subjective.
5.1.2 The BC3 Email Corpus
While our main experiments focus on the AMI
meeting corpus, we follow these up with an inves-
tigation into applying our abstractive techniques to
email data. The BC3 corpus (Ulrich et al, 2008)
contains email threads from the World Wide Web
Consortium (W3C) mailing list. The threads fea-
ture a variety of topics such as web accessibility and
planning face-to-face meetings. The annotated por-
tion of the mailing list consists of 40 threads. The
threads are annotated in the same manner as the AMI
corpus, with three human annotators per thread first
authoring abstracts and then linking email thread
sentences to the abstract sentences. The corpus also
contains speech act annotations. Unlike the AMI
corpus, however, there are no annotations for deci-
898
sions, actions and problems, an issue addressed later.
5.2 Classifiers
For these experiments we use a maximum entropy
classifier using the liblinear toolkit1 (Fan et al,
2008). For each of the AMI and BC3 corpora, we
perform 10-fold cross-validation on the data. In all
experiments we apply a 20% compression rate in
terms of the total document word count.
5.3 Evaluation
We evaluate the various classifiers described in Sec-
tion 3 using the ROC curve and the area under the
curve (AUROC), where a baseline AUROC is 0.5
and an ideal classifier approaches 1.
To evaluate the content selection in the transfor-
mation stage, we use weighted recall.This evaluation
metric is based on the links between extracted sen-
tences and the human gold-standard abstracts, with
the underlying motivation being that sentences with
more links to the human abstract are generally more
informative, as they provide the content on which an
effective abstract summary should be built. If M is
the number of sentences selected in the transforma-
tion step, O is the total number of sentences in the
document, and N is the number of annotators, then
Weighted Recall is given by
recall =
?M
i=1
?N
j=1 L(si, aj)
?O
i=1
?N
j=1 L(si, aj)
where L(si, aj) is the number of links for a sen-
tence si according to annotator aj . We can com-
pare machine performance with human performance
in the following way. For each annotator, we rank
their sentences from most-linked to least-linked and
select the best sentences until we reach the same
word count as our selections. We then calculate their
weighted recall score by using the other N-1 annota-
tions, and then average over all N annotators to get
an average human performance. We report all trans-
formation scores normalized by human performance
for that dataset.
6 Results
In this section we present results for our interpreta-
tion and transformation components.
1http://www.csie.ntu.edu.tw/ cjlin/liblinear/
6.1 Interpretation: Meetings
Figure 1 shows the ROC curves for the sentence-
level classifiers applied to manual transcripts. On
both manual and ASR transcripts, the classifiers
with the largest AUROCs are the action item and
general extractive classifiers. Action item sentences
can be detected very well with this feature set, with
the classifier having an AUROC of 0.92 on man-
ual transcripts and 0.93 on ASR, a result compa-
rable to previous findings of 0.91 and 0.93 (Mur-
ray and Renals, 2008) obtained using a speech-
specific feature set. General extractive classification
is also similar to other state-of-the-art extraction ap-
proaches on spoken data using speech features (Zhu
and Penn, 2006)2 with an AUROC of 0.87 on man-
ual and 0.85 on ASR. Decision sentences can also
be detected quite well, with AUROCs of 0.81 and
0.77. Positive-subjective, negative-subjective and
problem sentences are the most difficult to detect,
but the classifiers still give credible performance
with AUROCs of approximately 0.76 for manual
and 0.70-0.72 for ASR.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
actions
decisions
problems
positive-subjective
negative-subjective
extractive
random
Figure 1: ROC Curves for Ontology Mapping Classifiers
(Manual Transcripts)
6.2 Transformation: Meetings
In this section we present the weighted recall scores
for the sentences selected using the ILP method de-
scribed in Section 4. Remember, weighted recall
measures how useful these sentences would be in
generating sentences for an abstract summary. We
also assess the performance of three baseline sum-
marizers operating at the same compression level.
2Based on visual inspection of their reported best ROC curve
899
The simplest baseline (GREEDY) selects sentences
by ranking the posterior probabilites output by the
general extractive classifier. The second baseline
(CLASS COMBO) averages the posterior proba-
bilites output by all the classifiers and ranks sen-
tences from best to worst. The third baseline (RE-
TRAIN) uses the posterior probability outputs of all
the classifiers (except for the extractive classifier) as
new feature inputs for the general extractive classi-
fier.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Greedy
Class Combo
Retrain
ILP
W
ei
gh
te
d 
Re
ca
ll, 
No
rm
al
ize
d
manual
ASR
Figure 2: Weighted Recall Scores for AMI Meetings
Figure 2 shows the weighted recall scores, nor-
malized by human performance, for all approaches
on both manual and ASR transcripts. On man-
ual transcripts, the ILP approach (0.76) is better
than GREEDY (0.71) with a marginally significant
difference (p=0.07) and is significantly better than
CLASS COMBO and RETRAIN (both 0.68) ac-
cording to t-test (p < 0.05) . For ASR transcripts,
the ILP approach is significantly better than all other
approaches (p < 0.05). Xie et al (2009) reported
ROUGE-1 F-measures on a different meeting cor-
pus, and our ROUGE-1 scores are in the same range
of 0.64-0.69 (they used 18% compression ratio).
6.3 Interpretation: Emails
We applied the same summarization method to the
40 BC3 email threads, with contrasting results. Be-
cause the BC3 corpus does not currently contain an-
notations for decisions, actions and problems, we
simply ran the AMI-trained models over the data
for those three phenomena. We can assess the
performance of the extractive, positive-subjective
and negative-subjective classifiers by examining the
ROC curves displayed in Figure 3. Both the general
extractive and negative-subjective classifiers have
AUROCs of around 0.75. The positive-subjective
classifier initially has the worst performance with
an AUROC of 0.66, but we found that positive-
subjective performance increased dramatically to an
AUROC of 0.77 when we used only conversational
features and not word bigrams, character trigrams or
POS tags.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
TP
FP
positive-subjective
negative-subjective
extractive
random
Figure 3: ROC Curves for Ontology Mapping Classifiers
(BC3 Corpus)
6.4 Transformation: Emails
If we examine the weighted recall scores in Fig-
ure 4 we see that the ILP approach is worse than
the greedy summarizers on the BC3 dataset. How-
ever, the differences are not significant between ILP
and COMBO CLASS (p=0.15) and only marginally
significant compared with RETRAIN and GREEDY
(both p=0.08). The performance of the ILP approach
varies greatly across email threads. The top 15
threads (out of 40) yield ILP weighted recall scores
that are on par with human performance, while the
worst 15 are half that.
6.4.1 Email Corpus Analysis
Due to the large discrepancy in performance on
BC3 emails, we conducted additional experiments
for error analysis. We first explored whether we
could build a classifier that could discriminate the
best 15 emails from the worst 15 emails in terms of
weighted recall scores with the ILP approach, to de-
termine whether there are certain features that cor-
relate with good performance. Using the same fea-
900
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Greedy
Class Combo
Retrain
ILP
W
ei
gh
te
d 
Re
ca
ll, 
No
rm
al
ize
d
Figure 4: Weighted Recall Scores for BC3 Threads
tures described in Section 3.1, we built a logistic re-
gression classifier on the two classes and found that
they can be discriminated quite well (80% accuracy
on an approximately balanced dataset) and that the
conversation structure features are the most useful
for discerning them. Table 2 shows the weighted
recall scores and several conversation features that
were weighted most highly by the logistic regres-
sion model. In particular, we found that the email
threads that yielded good performance tended to fea-
ture more active participants (# Participants), were
not dominated by a single individual (BEGAUTH),
and featured a higher number of turns (# Turns)
that followed each other in quick succession without
long pauses (PPAU, pause as percentage of conver-
sation length). In other words, these emails were
structured more similarly to meetings. Note that
since we normalize weighted recall by human per-
formance, it is possible to have a weighted recall
score higher than 1. On the 15 best threads, our sys-
tem achieves human-level performance. Because we
used AMI-trained models for detecting decisions,
actions and problems in the BC3 data, it is not sur-
prising that performance was better on those emails
structured similarly to meetings. All of this indicates
that there are many different types of emails and that
we will have to focus on improving performance on
emails that differ markedly in structure.
7 Conclusion
We have presented two components of an abstractive
conversation summarization system. The interpreta-
tion component is used to populate a simple conver-
Metric Worst 15 Best 15
Weighted Recall 0.49 1.05
# Turns 6.27 6.73
# Participants 4.67 5.4
PPAU 0.18 0.12
BEGAUTH 0.31 0.18
Table 2: Selected Email Features, Averaged
sation ontology where conversation participants and
entities are linked by object properties such as deci-
sions, actions and subjective opinions. For this step
we show that highly accurate classifiers can be built
using a large set of features not specific to any con-
versation modality.
In the transformation step, a summary is cre-
ated by maximizing a function relating sentence
weights and entity weights, with the sentence
weights determined by the sentence-ontology map-
ping. Our evaluation shows that the sentences we
select are highly informative to generate abstract
summaries, and that our content selection method
outperforms several greedy selection approaches.
The system described thus far may appear extrac-
tive in nature, as the transformation step is iden-
tifying informative sentences in the conversation.
However, these selected sentences correspond to
< participant, relation, entity > triples in the
ontology, for which we can subsequently gener-
ate novel text by creating linguistic annotations of
the conversation ontology (Galanis and Androut-
sopolous, 2007). Even without the generation step,
the approach described above allows us to create
structured extracts by grouping sentences according
to specific phenomena such as action items and de-
cisions. The knowledge represented by the ontology
enables us to significantly improve sentence selec-
tion according to intrinsic measures and to generate
structured output that we hypothesize will be more
useful to an end user compared with a generic un-
structured extract.
Future work will focus on the generation compo-
nent and on applying the summarization system to
conversations in other modalities such as blogs and
instant messages. Based on the email error analysis,
we plan to pursue domain adaptation techniques to
improve performance on different types of emails.
901
References
R. Barzilay and K. McKeown. 2005. Sentence fusion for
multidocument news summarization. Computational
Linguistics, 31(3):297?328.
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proc. of ACM
WWW 07, Banff, Canada.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Well-
ner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
K. Church and W. Gale. 1995. Inverse document fre-
quency IDF: A measure of deviation from poisson. In
Proc. of the Third Workshop on Very Large Corpora,
pages 121?130.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proc. of COLING/ACL 2006, pages 144?
151.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large linear
classification. Journal of Machine Learning Research,
9:1871?1874.
D. Galanis and I. Androutsopolous. 2007. Generating
multilingual descriptions from linguistically annotated
owl ontologies: the naturalowl system. In Proc. of
ENLG 2007, Schloss Dagstuhl, Germany.
M. Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP 2006, Sydney, Australia, pages 364?
372.
D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-
Tu?r. 2009. A global optimization framework for meet-
ing summarization. In Proc. of ICASSP 2009, Taipei,
Taiwan.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky. 2007.
Resolving ?You? in multi-party dialog. In Proc. of
SIGdial 2007, Antwerp, Belgium.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-
summarization of audio-video presentations. In Proc.
of ACM MULTIMEDIA ?99, Orlando, FL, USA, pages
489?498.
P-Y. Hsueh, J. Kilgour, J. Carletta, J. Moore, and S. Re-
nals. 2007. Automatic decision detection in meeting
speech. In Proc. of MLMI 2007, Brno, Czech Repub-
lic.
K. Spa?rck Jones. 1999. Automatic summarizing: Factors
and directions. In I. Mani and M. Maybury, editors,
Advances in Automatic Text Summarization, pages 1?
12. MITP.
T. Kleinbauer, S. Becker, and T. Becker. 2007. Com-
bining multiple information layers for the automatic
generation of indicative meeting abstracts. In Proc. of
ENLG 2007, Dagstuhl, Germany.
K. Knight and D. Marcu. 2000. Statistics-based summa-
rization - step one: Sentence compression. In Proc. of
AAAI 2000, Austin, Texas, USA, pages 703?710.
J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable
document summarizer. In Proc. of the 18th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval. Seattle, Wash-
ington, USA, pages 68?73.
K. McKeown, J. Hirschberg, M. Galley, and S. Maskey.
2005. From text to speech summarization. In Proc. of
ICASSP 2005, Philadelphia, USA, pages 997?1000.
C. Muller. 2007. Resolving It, This and That in un-
restricted multi-party dialog. In Proc. of ACL 2007,
Prague, Czech Republic.
G. Murray and S. Renals. 2008. Detecting action items
in meetings. In Proc. of MLMI 2008, Utrecht, the
Netherlands.
G. Murray, T. Kleinbauer, P. Poller, S. Renals, T. Becker,
and J. Kilgour. 2008. Extrinsic summarization evalu-
ation: A decision audit task. In Proc. of MLMI 2008,
Utrecht, the Netherlands.
M. Purver, J. Dowding, J. Niekrasz, P. Ehlen, and
S. Noorbaloochi. 2007. Detecting and summariz-
ing action items in multi-party dialogue. In Proc. of
the 9th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Multi-
modal subjectivity analysis of multiparty conversation.
In Proc. of EMNLP 2008, Honolulu, HI, USA.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proc. of HLT-
NAACL 2004, Boston, USA.
J. Ulrich, G. Murray, and G. Carenini. 2008. A publicly
available annotated corpus for supervised email sum-
marization. In Proc. of AAAI EMAIL-2008 Workshop,
Chicago, USA.
T. Wilson. 2008. Annotating subjective content in meet-
ings. In Proc. of LREC 2008, Marrakech, Morocco.
S. Xie, B. Favre, D. Hakkani-Tu?r, and Y. Liu. 2009.
Leveraging sentence weights in a concept-based op-
timization framework for extractive meeting summa-
rization. In Proc. of Interspeech 2009, Brighton, Eng-
land.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet relay
chats. In Proc. of ACL 2005, Ann Arbor, MI, USA.
X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proc. of Interspeech 2006,
Pittsburgh, USA, pages 1531?1534.
902
Proceedings of NAACL-HLT 2013, pages 179?189,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Towards Topic Labeling with Phrase Entailment and Aggregation
Yashar Mehdad Giuseppe Carenini Raymond T. Ng Shafiq Joty
Department of Computer Science, University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
{mehdad, carenini, rng, rjoty}@cs.ubc.ca
Abstract
We propose a novel framework for topic la-
beling that assigns the most representative
phrases for a given set of sentences cover-
ing the same topic. We build an entailment
graph over phrases that are extracted from the
sentences, and use the entailment relations to
identify and select the most relevant phrases.
We then aggregate those selected phrases by
means of phrase generalization and merging.
We motivate our approach by applying over
conversational data, and show that our frame-
work improves performance significantly over
baseline algorithms.
1 Introduction
Given text segments about the same topic written in
different ways (i.e., language variability), topic la-
beling deals with the problem of automatically gen-
erating semantically meaningful labels for those text
segments. The potential of integrating topic label-
ing as a prerequisite for higher-level analysis has
been reported in several areas, such as summariza-
tion (Harabagiu and Lacatusu, 2010; Kleinbauer et
al., 2007; Dias et al, 2007), information extraction
(Allan, 2002) and conversation visualization (Liu
et al, 2012). Moreover, the huge amount of tex-
tual data generated everyday specifically in conver-
sations (e.g., emails and blogs) calls for automated
methods to analyze and re-organize them into mean-
ingful coherent clusters.
Table 1 shows an example of two human written
topic labels for a topic cluster collected from a blog1,
1http://slashdot.org
Text: a: Where do you think the term ?Horse laugh? comes
from?
b: And that rats also giggled when tickled.
c: My hypothesis- if an animal can play, it can ?laugh? or at
least it is familiar with the concept of ?laughing?.
Many animals play. There are various sorts of humour though.
Some involve you laughing because your brain suddenly made
a lots of unexpected connections.
Possible extracted phrases: animals play, rats have, laugh,
Horse laugh, rats also giggle, rats
Human-authored topic labels: animals which laugh, animal
laughter
Table 1: Topic labeling example.
and possible phrases that can be extracted from the
topic cluster using different approaches. This ex-
ample demonstrates that although most approaches
(Mei et al, 2007; Lau et al, 2011; Branavan et al,
2007) advocate extracting phrase-level topic labels
from the text segments, topically related text seg-
ments do not always contain one keyword or key
phrase that can capture the meaning of the topic. As
shown in this example, such labels do not exist in the
original text and cannot be extracted using the exist-
ing probabilistic models (e.g., (Mei et al, 2007)).
The same problem can be observed with many other
examples. This suggests the idea of aggregating and
generating topic labels, instead of simply extracting
them, as a challenging scenario for this field of re-
search.
Moreover, to generate a label for a topic we have
to be able to capture the overall meaning of a topic.
However, most current methods disregard semantic
relations, in favor of statistical models of word dis-
tributions and frequencies. This calls for the integra-
179
tion of semantic models for topic labeling.
Towards the solution of the mentioned problems,
in this paper we focus on two novel contributions:
1. Phrase aggregation. We propose to generate
topic labels using the extracted information by pro-
ducing the most representative phrases for each text
segment. We perform this task in two steps. First,
we generalize some lexically diverse concepts in
the extracted phrases. Second, we aggregate and
generate new phrases that can semantically imply
more than one original extracted phrase. For ex-
ample, the phrase ?rats also giggle? and ?horse
laugh? should be merged into a new phrase ?animals
laugh?. Although our method is still relying on ex-
tracting phrases, we move beyond current extractive
approaches, by generating new phrases through gen-
eralization and aggregation of the extracted ones.
2. Building a multidirectional entailment graph
over the extracted phrases to identify and select the
relevant information. We set such problem as an
application-oriented variant of the Textual Entail-
ment (TE) recognition task (Dagan and Glickman,
2004), to identify the information that are seman-
tically equivalent, novel, or more informative with
respect to the content of the others. In this way, we
prune the redundant and less informative text por-
tions (e.g., phrases), and produce semantically in-
formed phrases for the generation phase. In the case
of the example in Table 1, we eliminate phrases such
as ?rats have?, ?rats? and ?laugh? while keeping
?animal play?, ?Horse laugh? and ?rats also gig-
gle?.
The experimental results over conversational data
sets show that, in all cases, our approach outper-
forms other models significantly. Although conver-
sational data are known to be challenging (Carenini
et al, 2011), we choose to test our method on con-
versations because this is a genre in which topic
modeling is critically needed, as conversations lack
the structure and organization of, for instance, edited
monologues. The results indicate that our frame-
work is sufficiently robust to deal with topic labeling
in less structured, informal genres (when compared
with edited monologues). As an additional result of
our experiments, we show that the identification and
selection phase using semantic relations (entailment
graph) is a necessary step to perform the final step
(i.e., the phrase aggregation).
2 Topic Labeling Framework
Each topic cluster contains the sentences that can
semantically represent a topic. The task of cluster-
ing the sentences into a set of coherent topic clus-
ters is called topic segmentation (Joty et al, 2011),
which is out of the scope of this paper. Our goal is to
generate an understandable label (i.e., a sequence of
words) that could capture the semantic of the topic,
and distinguish a topic from other topics (based on
definition of a good topic label by (Mei et al, 2007)),
given a set of topic clusters. Among possible choices
of word sequences as topic labels, in order to balance
the granularity, we set phrases as valid topic labels.
Extract all
Filter/select
Entailment
Identify
Phrase extraction Entailment Graph
Generalize
Merge
Phraseaggregation? ? ?- -
1Figure 1: Topic labeling framework.
As shown in Figure 1, our framework consists of
three main components that we describe in more de-
tails in the following sections.
2.1 Phrase extraction
We tokenize and preprocess each cluster in the col-
lection of topic clusters with lemmas, stems, part-of-
speech tags, sense tags and chunks. We also extract
n-grams up to length 5 which do not start or end with
a stop word. In this phase, we do not include any
frequency count feature in our candidate extraction
pipeline. Once we have built the candidates pool,
the next step is to identify a subset containing the
most significant of those candidates. Since most top
systems in key phrase extraction use supervised ap-
proaches, we follow the same method (Kim et al,
2010b; Medelyan et al, 2008; Frank et al, 1999).
Initially, we consider a set of features used in the
other systems to determine whether a phrase is likely
to be a key phrase. However, since our dataset is
conversational (more details in Section 3), and the
text segments are not long, we aim for a classifier
with high recall. Thus, we only use TFxIDF (Salton
and McGill, 1986), position of the first occurrence
(Frank et al, 1999) and phrase length as our fea-
tures. We merge the training and test data released
180
for SemEval-2010 Task #5 (Kim et al, 2010b),
which consists of 244 scientific articles and 3705
key phrases, to train a Naive Bayes classifier in or-
der to learn a supervised model. We then apply our
model to extract the candidate phrases from the col-
lected candidates pool.
As a further step, to increase the coverage (re-
call) of our extracted phrases and to reduce the num-
ber of very short phrases (frequent keywords), we
choose the chunks containing any of the extracted
keywords. We add those chunks to our extracted
phrases and eliminate the associated keywords.
2.2 Entailment graph
So far, we have extracted a pool of key phrases from
each topic cluster. Many such phrases include re-
dundant information which are semantically equiv-
alent but vary in lexical choices. By identifying the
semantic relations between the phrases we can dis-
cover the information in one phrase that is seman-
tically equivalent, novel, or more/less informative
with respect to the content of the other phrase.
We set this problem as a variant of the Textual
Entailment (TE) recognition task (Mehdad et al,
2010b; Adler et al, 2012; Berant et al, 2011). We
build an entailment graph for each topic cluster,
where nodes are the extracted phrases and edges are
the entailment relations between nodes. Given two
phrases (ph1 and ph2), we aim at identifying and
handling the following cases:
i) ph1 and ph2 express the same meaning (bidirec-
tional entailment). In such cases one of the phrases
should be eliminated;
ii) ph1 is more informative than ph2 (unidirectional
entailment). In such cases, the entailing phrase
should replace or complement the entailed one;
iii) ph1 contains facts that are not present in ph2,
and vice-versa (the ?unknown? cases in TE par-
lance). In such cases, both phrases should remain.
Figure 2 shows how entailment relations can help
in selecting the phrases by removing the redun-
dant and less informative ones. For example, the
phrase ?animals laugh? entails ?rats giggle?, ?Horse
laugh? and ?Mice chuckle?,2 but not ?Animals play?.
2Assuming that ?animals laugh? is interpreted as ?all ani-
mals laugh?.
rats
giggle
Horse
laugh
laugh
rats
Mice
chuckle
animals
laugh
Animals
playx
x
1
Figure 2: Building an entailment graph over phrases. Ar-
rows and ?x? represent the entailment direction and un-
known cases respectively.
So we can keep ?animals laugh? and ?Animals play?
and eliminate others. In this way, TE-based phrase
identification method can be designed to distinguish
meaning-preserving variations from true divergence,
regardless of lexical choices and structures.
Similar to previous approaches in TE (e.g., (Be-
rant et al, 2011; Mehdad et al, 2010b; Mehdad et
al., 2010a)), we use supervised method. To train and
build the entailment graph, we perform the follow-
ing three steps.
2.2.1 Training set collection
In the last few years, TE corpora have been cre-
ated and distributed in the framework of several
evaluation campaigns, including the Recognizing
Textual Entailment (RTE) Challenge3 and Cross-
lingual textual entailment for content synchroniza-
tion4 (Negri et al, 2012). However, such datasets
cannot directly support our application. Specifi-
cally, our entailment graph is built over the extracted
phrases (with max. length of 5 tokens per phrase),
while the RTE datasets are composed of longer sen-
tences and paragraphs (Bentivogli et al, 2009; Negri
et al, 2011).
In order to collect a dataset which is more similar
to the goal of our entailment framework, we decide
to select a subset of the sixth and seventh RTE chal-
lenge main task (i.e., RTE within a Corpus). Our
3http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
4http://www.cs.york.ac.uk/semeval-2013/task8/
181
dataset choice is based on the following reasons: i)
the length of sentence pairs in RTE6 and RTE7 is
shorter than the others, and ii) RTE6 and RTE7 main
task datasets are originally created for summariza-
tion purpose which is closer to our work. We sort
the RTE6 and RTE7 dataset pairs based on the sen-
tence length and choose the first 2000 samples with
a equal number of positive and negative examples.
The average length of words in our training data is
6.7 words. There are certainly some differences be-
tween our training set and our phrases. However, the
collected training samples was the closest available
dataset to our purpose.
2.2.2 Feature representation and training
Working at the phrase level imposes another con-
straint. Phrases are short and in terms of syntactic
structure, they are not as rich as sentences. This lim-
its our features to the lexical level. Lexical mod-
els, on the other hand, are less computationally ex-
pensive and easier to implement and often deliver a
strong performance for RTE (Sammons et al, 2011).
Our entailment decision criterion is based on
similarity scores calculated with a phrase-to-phrase
matching process. Each example pair of phrases
(ph1 and ph2) is represented by a feature vector,
where each feature is a specific similarity score esti-
mating whether ph1 entails ph2.
We compute 18 similarity scores for each pair of
phrases. In order to adapt the similarity scores to the
entailment score, we normalize the similarity scores
by the length of ph2 (in terms of lexical items), when
checking the entailment direction from ph1 to ph2.
In this way, we can check the portion of informa-
tion/facts in ph2 which is covered by ph1.
The first 5 scores are computed based on the exact
lexical overlap between the phrases: word overlap,
edit distance, ngram-overlap, longest common sub-
sequence and Lesk (Lesk, 1986). The other scores
were computed using lexical resources: Word-
Net (Fellbaum, 1998), VerbOcean (Chklovski and
Pantel, 2004), paraphrases (Denkowski and Lavie,
2010) and phrase matching (Mehdad et al, 2011).
We used WordNet to compute the word similarity
as the least common subsumer between two words
considering the synonymy-antonymy, hypernymy-
hyponymy, and meronymy relations. Then, we cal-
culated the sentence similarity as the sum of the sim-
ilarity scores of the word pairs in Text and Hypothe-
sis, normalized by the number of words in Hypothe-
sis. We also use phrase matching features described
in (Mehdad et al, 2011) which consists of phrasal
matching at the level on ngrams (1 to 5 grams). The
rationale behind using different entailment features
is that combining various scores will yield a better
model (Berant et al, 2011).
To combine the entailment scores and optimize
their relative weights, we train a Support Vector Ma-
chine binary classifier, SVMlight (Joachims, 1999),
over an equal number of positive and negative exam-
ples. This results in an entailment model with 95%
accuracy over 2-fold and 5-fold cross-validation,
which further proves the effectiveness of our fea-
ture set for this lexical entailment model. The reason
that we gained a very high accuracy is because our
selected sentences are a subset of RTE6 and RTE7
with a shorter length (less number of words) which
makes the entailment recognition task much easier
than recognizing entailment between paragraphs or
complex long sentences.
2.2.3 Graph edge labeling
We set the edge labeling problem as a two-way
classification task. Two-way classification casts
multidirectional entailment as a unidirectional prob-
lem, where each pair is analyzed checking for en-
tailment in both directions (Mehdad et al, 2012). In
this condition, each original test example is correctly
classified if both pairs originated from it are cor-
rectly judged (?YES-YES? for bidirectional,?YES-
NO? and ?NO-YES? for unidirectional entailment
and ?NO-NO? for unknown cases). Two-way clas-
sification represents an intuitive solution to capture
multidimensional entailment relations. Moreover,
since our training examples are labeled with binary
judgments, we are not able to train a three-way clas-
sifier.
2.2.4 Identification and selection
Assigning all entailment relations between the ex-
tracted phrase pairs, we are aiming at identifying
relevant phrases and eliminating the redundant (in
terms of meaning) and less informative ones. In or-
der to perform this task we follow a set of rules based
on the graph edge labels. Note that since entailment
182
# Merging patterns
1 merge ( cw11(CPOS=[N|V |J]) ..w1n , cw21(CPOS=[N|V |J]) ..w2n ) = w11..w1n and w22..w2n
E.g. merge ( challenging situation , challenging problem ) = challenging situation and problem
2 merge ( w11..cw1n(CPOS=[N|V |J]) , w21..cw2n(CPOS=[N|V |J]) ) = w11..w1n?1 and w21..w2n
E.g. merge ( wet Mars , warm Mars ) = wet and warm Mars
3 merge ( w11..cw1n(CPOS=[N|V |J]) , cw21(CPOS=[N|V |J]) ..w2n ) = w11..w1n w22..w2n
E.g. merge ( interesting story , story continues ) = interesting story continues
4 merge ( cw11(CPOS=[N|V |J]) ..w1n , w21..cw2n(CPOS=[N|V |J]) ) = w21..w2n w12..w1n
E.g. merge ( LHC shutting down , details about LHC ) = details about LHC shutting down
5 merge ( w11Cpos , cw12(CPOS=[N|V |J]) , w13Cpos , w21Cpos , cw22(CPOS=[N|V |J]) , w23Cpos ) = w11 and w21 w22 w23
and w13
E.g. merge ( technology grow fast , media grow exponentially ) = technology and media grow exponentially and fast
Table 2: Phrase merging patterns.
is a transitive relation, our entailment graph is transi-
tive i.e., if entail(ph1,ph2) and entail(ph2,ph3) then
entail(ph1,ph3) (Berant et al, 2011).
Rule 1) If there is a chain of entailing nodes, we
keep the one which is in the root of the chain and
eliminate others (e.g. ?animals laugh? in Figure 2);
Rule 2) Among the nodes that are connected
with bidirectional entailment (semantically equiva-
lent nodes) we keep only the one with more outgoing
bidirectional and unidirectional entailment relations,
respectively;
Rule 3) Among the nodes that are connected with
unknown entailment (novel information with respect
to others) we keep the ones with no incoming entail-
ment relation (e.g., ?Animals play? in Figure 2).
Although deleting might be harsh, in our current
framework, we only rely on the performance of an
entailment model which gives us a yes/no entailment
decision. In future, we are planning to improve our
entailment graph by weighting the edges. In this
way, we can take advantage of the weights to make
a more conservative decision in pruning the entail-
ment chains.
2.3 Phrase aggregation
Once we have identified and selected the informa-
tive phrases, the generation of topic labels can be
done in two steps. First, we generalize the phrases
containing the concepts that are lexically connected.
Second, we merge the phrases with a set of hand
written linguistically motivated patterns.
2.3.1 Phrase generalization
In this step, we generalize phrases that contain
concepts which are lexically connected. For this
purpose, we search in phrases for different words
with the same part-of-speech and sense tag. Then,
we find the link between those words in WordNet. If
they are connected and the shortest path connecting
them is less than 3 (estimated over the development
set), we replace both by their common parent in the
WordNet. In the case that they belong to the same
synset, we can replace one by another. Note that we
limit our search to nouns and verbs. For example,
?rat? and ?horse? can be replaced by ?animal?, or
?giggle? and ?chuckle? can be replaced by ?laugh?.
The motivation behind the generalization step is to
enrich the common terms between the phrases in fa-
vor of increasing the chance that they could merge
to a single phrase. This also helps to move beyond
the limitation of original lexical choices.
2.3.2 Phrase merging
The goal is to merge the phrases that are con-
nected, and to generate a human readable phrase that
contains more information than a single extracted
phrase. Several approaches have been proposed to
aggregate and merge sentences in Natural Language
Generation (NLG) (e.g. (Barzilay and Lapata, 2006;
Cheng and Mellish, 2000)), however most of them
use syntactic structure of the sentences. To merge
phrases at the lexical level, we set few common lin-
guistically motivated aggregation patterns such as:
simple conjunction, and conjunction via shared par-
ticipants (Reiter and Dale, 2000).
Table 2 demonstrates the merging patterns, where
wij is the jth word (or segment) in phrase i, cw
is the common word (or segment) in both phrases
and CPOS is the common part-of-speech tag of
the corresponding word. To illustrate, pattern 1
183
looks for the first segment of each phrase (wi1).
If they are same (cwi1) and share the same POS
tag (CPOS), then we aggregate the first phrase
(w11..w1n) and the second phrase removing the first
element (w22..w2n) by using the connective ?and?.
For instance, the aggregation of ?animals laugh? and
?animals play? results in ?animals laugh and play?.
The rest of the patterns follow the same logic and for
the sake of brevity we avoid illustrating each pattern.
These patterns are among the most common domain
and application independent methods by which two
phrases/sentences can be aggregated, as described in
the NLG literature (Reiter and Dale, 2000).
In our aggregation pipeline, we group the phrases
based on their lexical overlap (number of common
words). The merging process is conducted over each
group in descending order (larger number of words
in common), in order to increase the chance of merg-
ing rules application. Then, we perform the merg-
ing over the resulting generated phrases from each
group. If our phrases cannot be merged (i.e., do not
match merging patterns), we select them as labels
for the topic cluster.
3 Datasets and Evaluation Metrics
3.1 Datasets
To verify the effectiveness of our approach, we ex-
periment with two different conversational datasets.
Our interest in dealing with conversational texts de-
rives from two reasons. First, the huge amount of
textual data generated everyday in these conversa-
tions validates the need of text analysis frameworks
to process such conversational texts effectively. Sec-
ond, conversational texts pose challenges to the tra-
ditional techniques, including redundancies, disflu-
encies, higher language variabilities and ill-formed
sentence structure (Liu et al, 2011).
Our conversational datasets are from two differ-
ent asynchronous media: email and blog. For email,
we use the dataset presented in (Joty et al, 2010),
where three individuals annotated the publicly avail-
able BC3 email corpus (Ulrich et al, 2008) with top-
ics. The corpus contains 40 email threads (or conver-
sations) at an average of 5 emails per thread. On av-
erage it has 26.3 sentences and 2.5 topics per thread.
A topic has an average length of 12.6 sentences. In
total, the three annotators found 269 topics in a cor-
pus of 1,024 sentences.
There are no publicly available blog corpora an-
notated with topics. For this study, we build our
own blog corpus containing 20 blog conversations of
various lengths from Slashdot, each annotated with
topics by three human annotators.5 The number of
comments per conversation varies from 30 to 101
with an average of 60.3 and the number of sentences
per conversation varies from 105 to 430 with an av-
erage of 220.6. The annotators first read a conversa-
tion and list the topics discussed in the conversation
by a short description (e.g., Game contents or size,
Bugs or faults) which provides a high-level overview
of the topic. Then, they assign the most appropriate
topic to each sentence in the conversation. The short
high-level descriptions of the topics serve as refer-
ence (or gold) topic labels in our experiments. The
target number of topics was not given in advance and
the annotators were instructed to find as many topics
as needed to convey the overall content structure of
the conversation. The annotators found 5 to 23 top-
ics per conversation with an average of 10.77. The
number of sentences per topic varies from 11.7 to
61.2 with an average of 27.16. In total, the three
annotators found 512 topics in our blog corpus con-
taining 4,411 sentences overall.
Note that our annotators performed topic segmen-
tation and labeling independently. In the email cor-
pus, the three annotators found 100, 77 and 92 top-
ics respectively (269 in total), and in the blog corpus,
they found 251, 119 and 192 topics respectively (562
in total). For the evaluation, there is a single gold
standard per topic written by each annotator. Table
1 shows a case in which two annotators selected the
same topical cluster and so we have two labels for
the same cluster.
3.2 Evaluation metrics
Traditionally, key phrase extraction is evaluated us-
ing precision, recall and f-measure based on exact
matches on all the extracted key phrases with gold
standards for a given text. However, as claimed
by (Kim et al, 2010a), this approach is not flexible
enough as it ignores the near-misses. Moreover, in
the case of topic labeling, most of the human written
5The new blog corpus annotated with topics will be made
publicly available for research purposes.
184
topic labels cannot be found in the text. Recently,
(Kim et al, 2010a) evaluated the utility of differ-
ent n-gram-based metrics for key phrase extraction
and showed that the metric R-precision correlates
most with human judgments. R-precision normal-
izes the approximate matching score by the maxi-
mum number of words in the reference and candi-
date phrases. Since this penalize our aggregation
phase, where the phrases tend to be longer than orig-
inal extracted phrase, we decide to use R-f1 as our
evaluation metric which considers length of both ref-
erence and candidate phrases.
R?precision = 1k
k?
i=1
overlap(candi, ref)
#words(candi)
R?recall = 1k
k?
i=1
overlap(candi, ref)
#words(ref)
R?f1 = 2 ?R?precision ?R?recall(R?precision + R?recall)
The metric described above only considers word
overlap and ignores other semantic relations (e.g.,
synonymy, hypernymy) between words. However,
annotators write labels of their own and may use
words that are not directly from the conversation but
are semantically related. Therefore, we propose to
also use another variant of R-f1 that incorporates se-
mantic relation between words. To calculate the Se-
mantic R-f1, we count the number of overlaps not
only when they have the same form, but also when
they are connected in WordNet with a synonymy,
hypernymy, hyponymy and entailment relation.
Its worth noting that the generalizations phase and
the evaluation method are completely independent.
In the generalization step, we try to generalize the
phrases which are automatically extracted from the
text segments. While, in the evaluation, we compare
the human written gold standards with the system
output. Therefore, using WordNet in the generaliza-
tion step does not bias the results in the evaluation.
4 Experiments and Results
4.1 Experimental settings
We conduct our experiments over the blog and email
datasets described in Section 3.1, after eliminating
the development set from the test datasets. In our ex-
periments, the development set was used for the pat-
tern extraction and the shortest path threshold con-
necting the words in Wordnet in the generalization
phase. Our test dataset consists of 461 topics (i.e.,
clusters and their associated topic labels) from 20
blog conversations and 242 topics from 40 email
conversations.
For preprocessing our dataset we use OpenNLP6
for tokenization, part-of-speech tagging and chunck-
ing. For sense disambiguation, we use the extended
gloss overlap measure with the window size of 5,
developed by (Pedersen et al, 2005). We also apply
Snowball algorithm (Porter, 2001) for stemming.
We compare our approach with two strong base-
lines. The first baseline Freq-BL ranks the words
according to their frequencies and select the top 5
candidates applying Maximum Marginal Relevance
algorithm (Carbonell and Goldstein, 1998) using
the same pre- and post-processing as the work by
(Mihalcea and Tarau, 2004). The second baseline
Lead-BL, ranks the words based on their relevance
to the leading sentences.7 The ranking criteria is
log(tfw,Lt + 1)? log(tfw,t + 1), where tfw,Lt and
tfw,t are the number of times word w appears in a
set of leading sentences Lt and topic cluster t, re-
spectively (Allan, 2002). The log expressions, as the
ranking criterion, assign more weights to the words
in the topic segment, that also appear in the leading
sentences. This is because topics tend to be intro-
duced in the first few sentences of a topical cluster.
We also measure the performance of our framework
at each step in order to compare the effectiveness of
each phase independently or in combination.
4.2 Results
We evaluate the performance of different models us-
ing the metrics R-f1 and Semantic R-f1 (Sem-R-f1),
described in Section 3.2. Table 4 shows the results
in percentage for different models. The results show
that our framework outperforms the baselines signif-
6http://opennlp.sourceforge.net/
7The key intuitions for this baseline is the leading sentences
of a topic cluster carry the most informative clues for the topic
labels. Based on our development set, when we consider the
first three sentences, the coverage of content words that appear
in human labeled topics are 39% and 49% for blog and email,
respectively.
185
Blog Email
Human-authored system generated Human-authored system generated
Shutting down the LHC story about the LHC shutting down (#3) How it affects coding it screws my coding
typical shutdown and upgrade times typical and scheduled shutdown (#2) Opinions and preferences of tools opinion about what tools
MARS was warm and wet 3B years ago Mars was warm and wet early history (#3) white on black for disabled users white text on black background (#3)
Moon Treaty and outer space treaty Moon and Outer Space Treaty (#2) Contact with Steven email to Steven Pemberton (#3)
Table 3: Successful examples of human-authored and system generated labels for blog and email datasets. The number
near some examples refers to the aggregation patterns in Table 2.
Models
R-f1 Sem-R-f1
blog email blog email
Lead-BL 13.5 14.0 34.5 30.1
Freq-BL 15.3 13.1 34.7 29.1
Extraction-BL 13.9 16.0 31.6 33.2
Entailment 12.2 15.6 30.8 33.3
Extraction+Aggregation 15.1 18.5 35.5 37.6
Extraction+Entailment+
Aggregation 17.9 20.4 38.7 41.6
Table 4: Results for candidate topic labels on blog and
email corpora.
icantly8 in both datasets.
On the blog corpus, our key phrase extraction
method (Extraction-BL) fails to beat the other base-
lines (Lead-BL and Freq-BL) in majority of cases
(except R-f1 for Lead-BL). However, in the email
dataset, it improves the performance over both base-
lines in both evaluation metrics. This might be due
to the shorter topic clusters (in terms of number of
sentences) in email corpus which causes a smaller
number of phrases to be extracted.
We also observe the effectiveness of the aggre-
gation phase. In all cases, there is a significant
improvement (p < 0.05) after applying the ag-
gregation phase over the extracted phrases (Extrac-
tion+Aggregation).
Note that there is no improvement over the ex-
traction phase after the entailment (Entailment row).
This is mainly due to the fact that the entailment
phase filters the equivalent phrases. This affects the
results negatively when such filtered phrases share
many common words with our human-authored
phrases. However, the results improve more sig-
nificantly (p < 0.01) when the aggregation is con-
ducted after the entailment. This demonstrates that,
the combination of these two steps are beneficial for
topic labeling over conversational datasets.
In addition, the differences between the results us-
8The statistical significance tests was calculated by approx-
imate randomization described in (Yeh, 2000).
ing R-f1 and Sem-R-f1 metrics suggests the need for
more flexible automatic evaluation methods for this
task. Moreover, although the same trend of improve-
ment is observed in blog and email corpora, the dif-
ferences between their performance suggest the in-
vestigation of specialized methods for various con-
versational modalities.
0 100 200 300 400 500 600
0
0.2
0.4
0.6
0.8
1
Blog
Se
m-
R-
f1
Ext
Ext+Ent
Ext+Agg
Ext+Ent+Agg
1
Figure 3: Sem-R-f1 results distribution after each phase
of our pipeline for blog corpus. The x-axis represents the
examples sorted based on their Sem-R-f1 score.
To further analyze the performance, in Figure 3,
we show the Sem-R-f1 results distribution for our
blog dataset.9 We can observe that the aggrega-
tion after the entailment phase (bold curve) clearly
increase the number of correct labels, while such
improvement can be only achieved when the en-
tailment relations is used to identify the relevant
phrases. This further highlights the need of seman-
tics in this task. Comparing both datasets, this ef-
fect is more dominant in blogs. We believe that this
is due to the length of topic clusters. Presumably,
building an entailment graph over a greater pool of
9For brevity?s sake we do not show the email dataset graph.
186
original phrases is more effective to filter the redun-
dant information and identify the relevant phrases.
5 Discussion
After analyzing the results and through manual veri-
fication of some cases, we observe that our approach
led to some interestingly successful examples. Table
3 shows few generated labels and the human written
topics for such cases.
In general, given that the results are expressed in
percentage, it appears that the performance is still
far from satisfactory level. This leaves an interesting
challenge for the research community to tackle.
However, this is not always due to the weakness
of our proposed model. We have identified three
different system independent sources of error:10
Type 1: Abstractive human-authored labels: the
nature of our method is based on extraction (with
the exception of our simple generalization phase)
and in many cases the human-written labels cannot
be extracted from the text and require more complex
generalizations. In fact, only 9.81% of the labels
in blog and 12.74% of the labels in email appear
verbatim in their respective conversations. For
example:
Human-authored label: meeting schedule and location
Generated phrases: meeting, Boston area, mid October
Type 2: Evaluation methods: in this work, we
proposed a semantic method to evaluate our system.
However, the current evaluation methods fail to
capture the meaning. For example:
Human-authored label: Food choices
Generated phrase: I would ask what people want to eat
Type 3: Subjective topic labels: often is not easy
for human to agree on one label for a topic cluster.11
For example:
Human-authored label 1: Member introduction
Human-authored label 2: Bio of Len
Generated phrases: own intro, Len Kasday, chair
In light of this analysis, we conclude that a more
comprehensive evaluation method (e.g., human eval-
uation) could better reveal the potential of our sys-
10There are many examples of such cases, however for
brevity we just mention one example for each type.
11The mean R-precision agreements computed based on one-
to-one mappings of the topic clusters are 20.22 and 36.84 on
blog and email data sets, respectively.
tem in dealing with topic labeling, specially on con-
versational data.
6 Conclusion
In this paper, we study the problem of automatic
topic labeling, and propose a novel framework to la-
bel topic clusters with meaningful readable phrases.
Within such framework, this paper makes two main
contributions. First, in contrast with most current
methods based on fully extractive models, we pro-
pose to aggregate topic labels by means of gener-
alizing and merging techniques. Second, beyond
current approaches which disregard semantic infor-
mation, we integrate semantics by means of build-
ing textual entailment graphs over the topic clusters.
To achieve our objectives, we successfully applied
our framework over two challenging conversational
datasets. Coherent results on both datasets demon-
strate the potential of our approach in dealing with
topic labeling task.
Future work will address both the improvement of
our aggregation phase and ranking the output candi-
date phrases for each topic cluster. On one hand,
we plan to accommodate more sophisticated NLG
techniques for the aggregation and generation phase.
Incorporating a better source of prior knowledge in
the generalization phase (e.g., YAGO or DBpedia) is
also an interesting research direction towards a bet-
ter phrase aggregation step. On the other hand, we
plan to apply a ranking strategy to select the top can-
didate phrases generated by our framework.
Acknowledgments
We would like to thank the anonymous reviewers
and Frank Tompa for their valuable comments and
suggestions to improve the paper, and the NSERC
Business Intelligence Network for financial support.
Yashar Mehdad also would like to acknowledge the
early discussions on the related topics with Matteo
Negri.
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application to
the health-care domain. In Proceedings of the ACL
2012 System Demonstrations, ACL ?12, pages 79?84,
187
Stroudsburg, PA, USA. Association for Computational
Linguistics.
James Allan. 2002. Topic detection and tracking: event-
based information organization.
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, HLT-NAACL ?06, pages 359?366, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In In
Proc Text Analysis Conference (TAC09.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
SRK Branavan, Pawan Deshpande, and Regina Barzilay.
2007. Generating a table-of-contents. In ACL, vol-
ume 45, page 544.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?98, pages 335?336, New York, NY, USA.
ACM.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for mining and summarizing text con-
versations.
Hua Cheng and Chris Mellish. 2000. Capturing the in-
teraction between aggregation and text planning in two
generation systems. In In Proceedings of the 1st In-
ternational Natural Language Generation Conference,
186193, Mitzpe.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 33?40, Barcelona, Spain,
July. Association for Computational Linguistics.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: improved evaluation
support for five target languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 339?342,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Gae?l Dias, Elsa Alves, and Jose? Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text summa-
rization and passage retrieval: an exhaustive evalua-
tion. In Proceedings of the 22nd national conference
on Artificial intelligence - Volume 2, AAAI?07, pages
1334?1339. AAAI Press.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceedings
of the Sixteenth International Joint Conference on Ar-
tificial Intelligence, IJCAI ?99, pages 668?673, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Sanda Harabagiu and Finley Lacatusu. 2010. Us-
ing topic themes for multi-document summarization.
ACM Trans. Inf. Syst., 28(3):13:1?13:47, July.
T. Joachims. 1999. Making large-scale svm learning
practical. LS8-Report 24, Universita?t Dortmund, LS
VIII-Report.
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and
Raymond T. Ng. 2010. Exploiting conversation struc-
ture in unsupervised topic segmentation for emails.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Shafiq Joty, Gabriel Murray, and Raymond T. Ng. 2011.
Supervised topic segmentation of email conversations.
In In ICWSM11. AAAI.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2010a. Evaluating n-gram based evaluation metrics
for automatic keyphrase extraction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 572?580, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010b. Semeval-2010 task 5: Automatic
keyphrase extraction from scientific articles. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, SemEval ?10, pages 21?26, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Thomas Kleinbauer, Stephanie Becker, and Tilman
Becker. 2007. Combining multiple information lay-
ers for the automatic generation of indicative meeting
abstracts. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, ENLG
?07, pages 151?154, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jey Han Lau, Karl Grieser, David Newman, and Timothy
Baldwin. 2011. Automatic labelling of topic models.
In ACL, pages 1536?1545.
188
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, pages 24?26, New York, NY,
USA. ACM.
F. Liu, Y. Liu, C. Busso, S. Harabagiu, and V. Ng. 2011.
Identifying the Gist of Conversational Text: Automatic
Keyword Extraction and Summarization. Ph.D. thesis,
THE UNIVERSITY OF TEXAS AT DALLAS.
Shixia Liu, Michelle X. Zhou, Shimei Pan, Yangqiu
Song, Weihong Qian, Weijia Cai, and Xiaoxiao Lian.
2012. Tiara: Interactive, topic-based visual text sum-
marization and analysis. ACM Trans. Intell. Syst.
Technol., 3(2):25:1?25:28, February.
O. Medelyan, I.H. Witten, and D. Milne. 2008. Topic
indexing with wikipedia. In Proceedings of the AAAI
WikiAI workshop.
Y. Mehdad, A. Moschitti, and F.M. Zanzotto. 2010a.
Syntactic semantic structures for textual entailment
recognition. Association for Computational Linguis-
tics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010b. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 321?324, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
ACL ?12, pages 120?124, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, KDD ?07, pages 490?499, New York, NY,
USA. ACM.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of EMNLP-04and
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, July.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: cross-lingual textual entailment
for content synchronization. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics - Volume 1: Proceedings of the main con-
ference and the shared task, and Volume 2: Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation, SemEval ?12, pages 399?407, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
T. Pedersen, S. Banerjee, and S. Patwardhan. 2005.
Maximizing Semantic Relatedness to Perform Word
Sense Disambiguation. Research Report UMSI
2005/25, University of Minnesota Supercomputing In-
stitute, March.
Martin F. Porter. 2001. Snowball: A language for stem-
ming algorithms.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems.
Gerard Salton and Michael J. McGill. 1986. Introduction
to modern information retrieval.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan Roth.
2011. Recognizing Textual Entailment. In Daniel M.
Bikel and Imed Zitouni, editors, Multilingual Natu-
ral Language Applications: From Theory to Practice.
Prentice Hall, Jun.
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for su-
pervised email summarization.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics -
Volume 2, COLING ?00, pages 947?953, Stroudsburg,
PA, USA. Association for Computational Linguistics.
189
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 486?496,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Combining Intra- and Multi-sentential Rhetorical Parsing for
Document-level Discourse Analysis
Shafiq Joty?
sjoty@qf.org.qa
Qatar Computing Research Institute
Qatar Foundation
Doha, Qatar
Giuseppe Carenini, Raymond Ng, Yashar Mehdad
{carenini, rng, mehdad}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, Canada
Abstract
We propose a novel approach for develop-
ing a two-stage document-level discourse
parser. Our parser builds a discourse tree
by applying an optimal parsing algorithm
to probabilities inferred from two Con-
ditional Random Fields: one for intra-
sentential parsing and the other for multi-
sentential parsing. We present two ap-
proaches to combine these two stages of
discourse parsing effectively. A set of
empirical evaluations over two different
datasets demonstrates that our discourse
parser significantly outperforms the state-
of-the-art, often by a wide margin.
1 Introduction
Discourse of any kind is not formed by inde-
pendent and isolated textual units, but by related
and structured units. Discourse analysis seeks
to uncover such structures underneath the surface
of the text, and has been shown to be benefi-
cial for text summarization (Louis et al, 2010;
Marcu, 2000b), sentence compression (Sporleder
and Lapata, 2005), text generation (Prasad et al,
2005), sentiment analysis (Somasundaran, 2010)
and question answering (Verberne et al, 2007).
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988), one of the most influential the-
ories of discourse, represents texts by labeled hier-
archical structures, called Discourse Trees (DTs),
as exemplified by a sample DT in Figure 1. The
leaves of a DT correspond to contiguous Elemen-
tary Discourse Units (EDUs) (six in the exam-
ple). Adjacent EDUs are connected by rhetori-
cal relations (e.g., Elaboration, Contrast), form-
ing larger discourse units (represented by internal
?This work was conducted at the University of British
Columbia, Vancouver, Canada.
nodes), which in turn are also subject to this re-
lation linking. Discourse units linked by a rhetori-
cal relation are further distinguished based on their
relative importance in the text: nucleus being the
central part, whereas satellite being the peripheral
one. Discourse analysis in RST involves two sub-
tasks: discourse segmentation is the task of identi-
fying the EDUs, and discourse parsing is the task
of linking the discourse units into a labeled tree.
While recent advances in automatic discourse
segmentation and sentence-level discourse parsing
have attained accuracies close to human perfor-
mance (Fisher and Roark, 2007; Joty et al, 2012),
discourse parsing at the document-level still poses
significant challenges (Feng and Hirst, 2012) and
the performance of the existing document-level
parsers (Hernault et al, 2010; Subba and Di-
Eugenio, 2009) is still considerably inferior com-
pared to human gold-standard. This paper aims
to reduce this performance gap and take discourse
parsing one step further. To this end, we address
three key limitations of existing parsers as follows.
First, existing discourse parsers typically model
the structure and the labels of a DT separately
in a pipeline fashion, and also do not consider
the sequential dependencies between the DT con-
stituents, which has been recently shown to be crit-
ical (Feng and Hirst, 2012). To address this limi-
tation, as the first contribution, we propose a novel
document-level discourse parser based on proba-
bilistic discriminative parsing models, represented
as Conditional Random Fields (CRFs) (Sutton et
al., 2007), to infer the probability of all possible
DT constituents. The CRF models effectively rep-
resent the structure and the label of a DT con-
stituent jointly, and whenever possible, capture the
sequential dependencies between the constituents.
Second, existing parsers apply greedy and sub-
optimal parsing algorithms to build the DT for a
document. To cope with this limitation, our CRF
models support a probabilistic bottom-up parsing
486
But he added:
"Some people use the purchasers?
 index as a leading indicator, some use it as a coincident indicator. But the thing it?s supposed to measure -- manufacturing strength --
it missed altogether last month." <P>Elaboration
Same-UnitContrast
Contrast
Attribution
(1)
(2) (3)
(4) (5)
(6)
Figure 1: Discourse tree for two sentences in RST-DT. Each of the sentences contains three EDUs. The
second sentence has a well-formed discourse tree, but the first sentence does not have one.
algorithm which is non-greedy and optimal.
Third, existing discourse parsers do not dis-
criminate between intra-sentential (i.e., building
the DTs for the individual sentences) and multi-
sentential parsing (i.e., building the DT for the
document). However, we argue that distinguish-
ing between these two conditions can result in
more effective parsing. Two separate parsing
models could exploit the fact that rhetorical re-
lations are distributed differently intra-sententially
vs. multi-sententially. Also, they could indepen-
dently choose their own informative features. As
another key contribution of our work, we devise
two different parsing components: one for intra-
sentential parsing, the other for multi-sentential
parsing. This provides for scalable, modular and
flexible solutions, that can exploit the strong cor-
relation observed between the text structure (sen-
tence boundaries) and the structure of the DT.
In order to develop a complete and robust dis-
course parser, we combine our intra-sentential
and multi-sentential parsers in two different ways.
Since most sentences have a well-formed dis-
course sub-tree in the full document-level DT (for
example, the second sentence in Figure 1), our first
approach constructs a DT for every sentence us-
ing our intra-sentential parser, and then runs the
multi-sentential parser on the resulting sentence-
level DTs. However, this approach would disre-
gard those cases where rhetorical structures vio-
late sentence boundaries. For example, consider
the first sentence in Figure 1. It does not have a
well-formed sub-tree because the unit containing
EDUs 2 and 3 merges with the next sentence and
only then is the resulting unit merged with EDU
1. Our second approach, in an attempt of dealing
with these cases, builds sentence-level sub-trees
by applying the intra-sentential parser on a sliding
window covering two adjacent sentences and by
then consolidating the results produced by over-
lapping windows. After that, the multi-sentential
parser takes all these sentence-level sub-trees and
builds a full rhetorical parse for the document.
While previous approaches have been tested on
only one corpus, we evaluate our approach on
texts from two very different genres: news articles
and instructional how-to-do manuals. The results
demonstrate that our contributions provide con-
sistent and statistically significant improvements
over previous approaches. Our final result com-
pares very favorably to the result of state-of-the-art
models in document-level discourse parsing.
In the rest of the paper, after discussing related
work in Section 2, we present our discourse pars-
ing framework in Section 3. In Section 4, we de-
scribe the intra- and multi-sentential parsing com-
ponents. Section 5 presents the two approaches
to combine the two stages of parsing. The exper-
iments and error analysis, followed by future di-
rections are discussed in Section 6. Finally, we
summarize our contributions in Section 7.
2 Related work
The idea of staging document-level discourse
parsing on top of sentence-level discourse parsing
was investigated in (Marcu, 2000a; LeThanh et al,
2004). These approaches mainly rely on discourse
markers (or cues), and use hand-coded rules to
build DTs for sentences first, then for paragraphs,
and so on. However, often rhetorical relations
are not explicitly signaled by discourse markers
(Marcu and Echihabi, 2002), and discourse struc-
tures do not always correspond to paragraph struc-
tures (Sporleder and Lascarides, 2004). Therefore,
rather than relying on hand-coded rules based on
discourse markers, recent approaches employ su-
pervised machine learning techniques with a large
set of informative features.
Hernault et al, (2010) presents the publicly
available HILDA parser. Given the EDUs in a doc-
487
Elaboration Joint AttributionSame-Unit Contrast Explanation
0
5
10
15
20
25
30 Multi-sentential
Intra-sentential
Figure 2: Distributions of six most frequent relations in
intra-sentential and multi-sentential parsing scenarios.
ument, HILDA iteratively employs two Support
Vector Machine (SVM) classifiers in pipeline to
build the DT. In each iteration, a binary classifier
first decides which of the adjacent units to merge,
then a multi-class classifier connects the selected
units with an appropriate relation label. They eval-
uate their approach on the RST-DT corpus (Carl-
son et al, 2002) of news articles. On a different
genre of instructional texts, Subba and Di-Eugenio
(2009) propose a shift-reduce parser that relies on
a classifier for relation labeling. Their classifier
uses Inductive Logic Programming (ILP) to learn
first-order logic rules from a set of features includ-
ing compositional semantics. In this work, we ad-
dress the limitations of these models (described in
Section 1) introducing our novel discourse parser.
3 Our Discourse Parsing Framework
Given a document with sentences already seg-
mented into EDUs, the discourse parsing prob-
lem is determining which discourse units (EDUs
or larger units) to relate (i.e., the structure), and
how to relate them (i.e., the labels or the discourse
relations) in the resulting DT. Since we already
have an accurate sentence-level discourse parser
(Joty et al, 2012), a straightforward approach to
document-level parsing could be to simply apply
this parser to the whole document. However this
strategy would be problematic because of scalabil-
ity and modeling issues. Note that the number of
valid trees grows exponentially with the number
of EDUs in a document.1 Therefore, an exhaus-
tive search over the valid trees is often unfeasible,
even for relatively small documents.
For modeling, the problem is two-fold. On the
one hand, it appears that rhetorical relations are
distributed differently intra-sententially vs. multi-
sententially. For example, Figure 2 shows a com-
parison between the two distributions of six most
1For n + 1 EDUs, the number of valid discourse trees is
actually the Catalan number Cn.
model
AlgorithmSentences segmented into EDUs
Document-level
 discourse tree
Intra-sententialparser Multi-sententialparser
model
Algorithm
Figure 3: Discourse parsing framework.
frequent relations on a development set containing
20 randomly selected documents from RST-DT.
Notice that relations Attribution and Same-Unit
are more frequent than Joint in intra-sentential
case, whereas Joint is more frequent than the other
two in multi-sentential case. On the other hand,
different kinds of features are applicable and in-
formative for intra-sentential vs. multi-sentential
parsing. For example, syntactic features like dom-
inance sets (Soricut and Marcu, 2003) are ex-
tremely useful for sentence-level parsing, but are
not even applicable in multi-sentential case. Like-
wise, lexical chain features (Sporleder and Las-
carides, 2004), that are useful for multi-sentential
parsing, are not applicable at the sentence level.
Based on these observations, our discourse
parsing framework comprises two separate mod-
ules: an intra-sentential parser and a multi-
sentential parser (Figure 3). First, the intra-
sentential parser produces one or more discourse
sub-trees for each sentence. Then, the multi-
sentential parser generates a full DT for the doc-
ument from these sub-trees. Both of our parsers
have the same two components: a parsing model
assigns a probability to every possible DT, and
a parsing algorithm identifies the most probable
DT among the candidate DTs in that scenario.
While the two models are rather different, the
same parsing algorithm is shared by the two mod-
ules. Staging multi-sentential parsing on top of
intra-sentential parsing in this way allows us to ex-
ploit the strong correlation between the text struc-
ture and the DT structure as explained in detail in
Section 5. Before describing our parsing models
and the parsing algorithm, we introduce some ter-
minology that we will use throughout the paper.
Following (Joty et al, 2012), a DT can be for-
mally represented as a set of constituents of the
form R[i,m, j], referring to a rhetorical relation
R between the discourse unit containing EDUs i
through m and the unit containing EDUs m+1
through j. For example, the DT for the sec-
ond sentence in Figure 1 can be represented as
488
{Elaboration-NS[4,4,5], Same-Unit-NN[4,5,6]}.
Notice that a relation R also specifies the nuclear-
ity statuses of the discourse units involved, which
can be one of Nucleus-Satellite (NS), Satellite-
Nucleus (SN) and Nucleus-Nucleus (NN).
4 Parsing Models and Parsing Algorithm
The job of our intra-sentential and multi-sentential
parsing models is to assign a probability to each
of the constituents of all possible DTs at the sen-
tence level and at the document level, respectively.
Formally, given the model parameters ?, for each
possible constituent R[i,m, j] in a candidate DT
at the sentence or document level, the parsing
model estimates P (R[i,m, j]|?), which specifies
a joint distribution over the label R and the struc-
ture [i,m, j] of the constituent.
4.1 Intra-Sentential Parsing Model
Recently, we proposed a novel parsing model
for sentence-level discourse parsing (Joty et
al., 2012), that outperforms previous approaches
by effectively modeling sequential dependencies
along with structure and labels jointly. Below we
briefly describe the parsing model, and show how
it is applied to obtain the probabilities of all possi-
ble DT constituents at the sentence level.
Figure 4 shows the intra-sentential parsing
model expressed as a Dynamic Conditional Ran-
dom Field (DCRF) (Sutton et al, 2007). The ob-
served nodes Uj in a sequence represent the dis-
course units (EDUs or larger units). The first layer
of hidden nodes are the structure nodes, where
Sj?{0, 1} denotes whether two adjacent discourse
units Uj?1 and Uj should be connected or not.
The second layer of hidden nodes are the relation
nodes, with Rj?{1 . . .M} denoting the relation
between two adjacent unitsUj?1 andUj , whereM
is the total number of relations in the relation set.
The connections between adjacent nodes in a hid-
den layer encode sequential dependencies between
the respective hidden nodes, and can enforce con-
straints such as the fact that a Sj= 1 must not fol-
low a Sj?1= 1. The connections between the two
hidden layers model the structure and the relation
of a DT (sentence-level) constituent jointly.
To obtain the probability of the constituents
of all candidate DTs for a sentence, we apply
the parsing model recursively at different levels
of the DT and compute the posterior marginals
over the relation-structure pairs. To illustrate the
U UU U U
2
2
2
3 j t-1 t
SS S S S
R R R R R
3
3 j
j t-1
t-1 t
Unit sequenceat level i
Structure sequence
Relationsequence
U1
t
Figure 4: A chain-structured DCRF as our intra-
sentential parsing model.
process, let us assume that the sentence contains
four EDUs. At the first (bottom) level, when all
the units are the EDUs, there is only one possible
unit sequence to which we apply our DCRF
model (Figure 5(a)). We compute the posterior
marginals P (R2, S2=1|e1, e2, e3, e4,?), P (R3,
S3=1|e1, e2, e3, e4,?) and P (R4, S4=1|e1, e2, e3,
e4,?) to obtain the probability of the con-
stituents R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4],
respectively. At the second level, there are
three possible unit sequences (e1:2, e3, e4),
(e1,e2:3, e4) and (e1,e2,e3:4). Figure 5(b) shows
their corresponding DCRFs. The posterior
marginals P (R3, S3=1|e1:2,e3,e4,?), P (R2:3
S2:3=1|e1,e2:3,e4,?), P (R4, S4=1|e1,e2:3,e4,?)
and P (R3:4, S3:4=1|e1,e2,e3:4,?) computed from
the three sequences correspond to the probability
of the constituents R[1, 2, 3], R[1, 1, 3], R[2, 3, 4]
and R[2, 2, 4], respectively. Similarly, we attain
the probability of the constituents R[1, 1, 4],
R[1, 2, 4] and R[1, 3, 4] by computing their
respective posterior marginals from the three
possible sequences at the third (top) level.
e 1 e e2
2
2
3
S S3
R R3
(a)
e 1e
S
R
1:2 3
3
3
e e
S
R
2:3
2:3
(b)
2:3e4
S4
R4
e4
S4
R4
e4
S4
R4
1e e
S
R
2
2
2 e3:4
S3:4
R3:4
1 e
S
R
1:3 4
4
4
e e
S
R
2:4
2:4
(c)
2:4 ee
S
R
1:2e
3:4
3:4
3:4
(i) (ii)
(iii)(i) (ii) (iii)
Figure 5: Our parsing model applied to the sequences at
different levels of a sentence-level DT. (a) Only possible se-
quence at the first level, (b) Three possible sequences at the
second level, (c) Three possible sequences at the third level.
At this point what is left to be explained is
how we generate all possible sequences for a
given number of EDUs in a sentence. Algorithm
1 demonstrates how we do that. More specifi-
cally, to compute the probabilities of each DT con-
489
stituent R[i, k, j], we need to generate sequences
like (e1, ? ? ? , ei?1, ei:k, ek+1:j , ej+1, ? ? ? , en) for
1 ? i ? k < j ? n. In doing so, we may
generate some duplicate sequences. Clearly, the
sequence (e1, ? ? ? , ei?1, ei:i, ei+1:j , ej+1, ? ? ? , en)
for 1 ? i ? k < j < n is already considered
for computing the probability of R[i+ 1, j, j+ 1].
Therefore, it is a duplicate sequence that we ex-
clude from our list of all possible sequences.
Input: Sequence of EDUs: (e1, e2, ? ? ? , en)
Output: List of sequences: L
for i = 1? n? 1 do
for j = i+ 1? n do
if j == n then
for k = i? j ? 1 do
L.append
((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))
end
else
for k = i+ 1? j ? 1 do
L.append
((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))
end
end
end
end
Algorithm 1: Generating all possible sequences
for a sentence with n EDUs.
Once we obtain the probability of all possible
DT constituents, the discourse sub-trees for the
sentences are built by applying an optimal prob-
abilistic parsing algorithm (Section 4.4) using one
of the methods described in Section 5.
4.2 Multi-Sentential Parsing Model
Given the discourse units (sub-trees) for all the
sentences of a document, a simple approach to
build the rhetorical tree of the document would be
to apply a new DCRF model, similar to the one
in Figure 4 (with different parameters), to all the
possible sequences generated from these units to
infer the probability of all possible higher-order
constituents. However, the number of possible se-
quences and their length increase with the number
of sentences in a document. For example, assum-
ing that each sentence has a well-formed DT, for
a document with n sentences, Algorithm 1 gener-
ates O(n3) sequences, where the sequence at the
bottom level has n units, each of the sequences at
the second level has n-1 units, and so on. Since
the model in Figure 4 has a ?fat? chain structure,
U Ut-1 t
S
R t
Adjacent Unitsat level i
Structure
Relation
t
Figure 6: A CRF as a multi-sentential parsing model.
we could use forwards-backwards algorithm for
exact inference in this model (Sutton and McCal-
lum, 2012). However, forwards-backwards on a
sequence containing T units costs O(TM2) time,
where M is the number of relations in our rela-
tion set. This makes the chain-structured DCRF
model impractical for multi-sentential parsing of
long documents, since learning requires to run in-
ference on every training sequence with an overall
time complexity of O(TM2n3) per document.
Our model for multi-sentential parsing is shown
in Figure 6. The two observed nodes Ut?1 and
Ut are two adjacent discourse units. The (hidden)
structure node S?{0, 1} denotes whether the two
units should be connected or not. The hidden node
R?{1 . . .M} represents the relation between the
two units. Notice that like the previous model, this
is also an undirected graphical model. It becomes
a CRF if we directly model the hidden (output)
variables by conditioning its clique potential (or
factor) ? on the observed (input) variables:
P (Rt, St|x,?) = 1Z(x,?)?(Rt, St|x,?) (1)
where x represents input features extracted from
the observed variables Ut?1 and Ut, and Z(x,?)
is the partition function. We use a log-linear rep-
resentation of the factor:
?(Rt, St|x,?) = exp(?T f(Rt, St, x)) (2)
where f(Rt, St, x) is a feature vector derived from
the input features x and the labels Rt and St, and
? is the corresponding weight vector. Although,
this model is similar in spirit to the model in Fig-
ure 4, we now break the chain structure, which
makes the inference much faster (i.e., complex-
ity of O(M2)). Breaking the chain structure also
allows us to balance the data for training (equal
number instances with S=1 and S=0), which dra-
matically reduces the learning time of the model.
We apply our model to all possible adjacent
units at all levels for the multi-sentential case, and
490
compute the posterior marginals of the relation-
structure pairs P (Rt, St=1|Ut?1, Ut,?) to obtain
the probability of all possible DT constituents.
4.3 Features Used in our Parsing Models
Table 1 summarizes the features used in our pars-
ing models, which are extracted from two adjacent
unitsUt?1 andUt. Since most of these features are
adopted from previous studies (Joty et al, 2012;
Hernault et al, 2010), we briefly describe them.
Organizational features include the length of
the units as the number of EDUs and tokens.
It also includes the distances of the units from
the beginning and end of the sentence (or text in
the multi-sentential case). Text structural fea-
tures indirectly capture the correlation between
text structure and rhetorical structure by counting
the number of sentence and paragraph boundaries
in the units. Discourse markers (e.g., because, al-
though) carry informative clues for rhetorical re-
lations (Marcu, 2000a). Rather than using a fixed
list of discourse markers, we use an empirically
learned lexical N-gram dictionary following (Joty
et al, 2012). This approach has been shown to
be more robust and flexible across domains (Bi-
ran and Rambow, 2011; Hernault et al, 2010). We
also include part-of-speech (POS) tags for the be-
ginning and end N tokens in a unit.
8 Organizational features Intra & Multi-Sentential
Number of EDUs in unit 1 (or unit 2).
Number of tokens in unit 1 (or unit 2).
Distance of unit 1 in EDUs to the beginning (or to the end).
Distance of unit 2 in EDUs to the beginning (or to the end).
4 Text structural features Multi-Sentential
Number of sentences in unit 1 (or unit 2).
Number of paragraphs in unit 1 (or unit 2).
8 N-gram features N?{1, 2, 3} Intra & Multi-Sentential
Beginning (or end) lexical N-grams in unit 1.
Beginning (or end) lexical N-grams in unit 2.
Beginning (or end) POS N-grams in unit 1.
Beginning (or end) POS N-grams in unit 2.
5 Dominance set features Intra-Sentential
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two units.
8 Lexical chain features Multi-Sentential
Number of chains start in unit 1 and end in unit 2.
Number of chains start (or end) in unit 1 (or in unit 2).
Number of chains skipping both unit 1 and unit 2.
Number of chains skipping unit 1 (or unit 2).
2 Contextual features Intra & Multi-Sentential
Previous and next feature vectors.
2 Substructure features Intra & Multi-Sentential
Root nodes of the left and right rhetorical sub-trees.
Table 1: Features used in our parsing models.
Lexico-syntactic features dominance sets
(Soricut and Marcu, 2003) are very effective for
intra-sentential parsing. We include syntactic
labels and lexical heads of head and attachment
nodes along with their dominance relationship
as features. Lexical chains (Morris and Hirst,
1991) are sequences of semantically related words
that can indicate topic shifts. Features extracted
from lexical chains have been shown to be useful
for finding paragraph-level discourse structure
(Sporleder and Lascarides, 2004). We compute
lexical chains for a document following the ap-
proach proposed in (Galley and McKeown, 2003),
that extracts lexical chains after performing word
sense disambiguation. Following (Joty et al,
2012), we also encode contextual and rhetorical
sub-structure features in our models. The rhetori-
cal sub-structure features incorporate hierarchical
dependencies between DT constituents.
4.4 Parsing Algorithm
Given the probability of all possible DT con-
stituents in the intra-sentential and multi-sentential
scenarios, the job of the parsing algorithm is to
find the most probable DT for that scenario. Fol-
lowing (Joty et al, 2012), we implement a prob-
abilistic CKY-like bottom-up algorithm for com-
puting the most likely parse using dynamic pro-
gramming. Specifically, with n discourse units,
we use the upper-triangular portion of the n?n
dynamic programming table D. Given Ux(0) and
Ux(1) are the start and end EDU Ids of unit Ux:
D[i, j] = P (R[Ui(0), Uk(1), Uj(1)]) (3)
where, k = argmax
i?p?j
P (R[Ui(0), Up(1), Uj(1)]).
Note that, in contrast to previous studies on
document-level parsing (Hernault et al, 2010;
Subba and Di-Eugenio, 2009; Marcu, 2000b),
which use a greedy algorithm, our approach finds
a discourse tree that is globally optimal.
5 Document-level Parsing Approaches
Now that we have presented our intra-sentential
and our multi-sentential parsers, we are ready to
describe how they can be effectively combined to
perform document-level discourse analysis. Re-
call that a key motivation for a two-stage parsing is
that it allows us to capture the correlation between
text structure and discourse structure in a scalable,
modular and flexible way. Below we describe two
different approaches to model this correlation.
491
5.1 1S-1S (1 Sentence-1 Sub-tree)
A key finding from several previous studies on
sentence-level discourse analysis is that most sen-
tences have a well-formed discourse sub-tree in
the full document-level DT (Joty et al, 2012;
Fisher and Roark, 2007). For example, Figure 7(a)
shows 10 EDUs in 3 sentences (see boxes), where
the DTs for the sentences obey their respective
sentence boundaries. The 1S-1S approach aims to
maximally exploit this finding. It first constructs
a DT for every sentence using our intra-sentential
parser, and then it provides our multi-sentential
parser with the sentence-level DTs to build the
rhetorical parse for the whole document.
1     2  3S 1
8  9   10S 34   5      6   7S 2
1    2   3S 1
8   9    10S 34   5    6    7S 2(a) (b)
???
Figure 7: Two possible DTs for three sentences.
5.2 Sliding Window
While the assumption made by 1S-1S clearly sim-
plifies the parsing process, it totally ignores the
cases where discourse structures violate sentence
boundaries. For example, in the DT shown in Fig-
ure 7(b), sentence S2 does not have a well-formed
sub-tree because some of its units attach to the
left (4-5, 6) and some to the right (7). Vliet and
Redeker (2011) call these cases as ?leaky? bound-
aries. Even though less than 5% of the sentences
have leaky boundaries in RST-DT, in other corpora
this can be true for a larger portion of the sen-
tences. For example, we observe over 12% sen-
tences with leaky boundaries in the Instructional
corpus of (Subba and Di-Eugenio, 2009). How-
ever, we notice that in most cases where discourse
structures violate sentence boundaries, its units are
merged with the units of its adjacent sentences, as
in Figure 7(b). For example, this is true for 75%
cases in our development set containing 20 news
articles from RST-DT and for 79% cases in our
development set containing 20 how-to-do manuals
from the Instructional corpus. Based on this obser-
vation, we propose a sliding window approach.
In this approach, our intra-sentential parser
works with a window of two consecutive sen-
tences, and builds a DT for the two sentences. For
example, given the three sentences in Figure 7, our
intra-sentential parser constructs a DT for S1-S2
and a DT for S2-S3. In this process, each sentence
in a document except the first and the last will be
associated with two DTs: one with the previous
sentence (say DTp) and one with the next (say
DTn). In other words, for each non-boundary sen-
tence, we will have two decisions: one from DTp
and one from DTn. Our parser consolidates the
two decisions and generates one or more sub-trees
for each sentence by checking the following three
mutually exclusive conditions one after another:
? Same in both: If the sentence has the same (in
terms of both structure and labels) well-formed
sub-tree in both DTp and DTn, we take this sub-
tree for the sentence. For example, in Figure 8(a),
S2 has the same sub-tree in the two DTs, i.e. a DT
for S1-S2 and a DT for S2-S3. The two decisions
agree on the DT for the sentence.
? Different but no cross: If the sentence has a
well-formed sub-tree in both DTp and DTn, but
the two sub-trees vary either in structure or in la-
bels, we pick the most probable one. For example,
consider the DT for S1-S2 in Figure 8(a) and the
DT for S2-S3 in Figure 8(b). In both cases S2 has
a well-formed sub-tree, but they differ in structure.
We pick the sub-tree which has the higher proba-
bility in the two dynamic programming tables.
1     2  3S1 8  9   10S34   5      6   7S2
1    2   3S1 8   9    10S3
4   5    6    7S2
(a)
(c)
8  9   10S 34   5    6     7S2 (b)
4   5    6    7S2(i) (ii)
4   5      6   7S2
Figure 8: Extracting sub-trees for S2.
? Cross: If either or both of DTp and DTn seg-
ment the sentence into multiple sub-trees, we pick
the one with more sub-trees. For example, con-
sider the two DTs in Figure 8(c). In the DT for
S1-S2, S2 has three sub-trees (4-5,6,7), whereas
in the DT for S2-S3, it has two (4-6,7). So, we ex-
tract the three sub-trees for S2 from the first DT. If
the sentence has the same number of sub-trees in
both DTp and DTn, we pick the one with higher
probability in the dynamic programming tables.
At the end, the multi-sentential parser takes all
these sentence-level sub-trees for a document, and
builds a full rhetorical parse for the document.
492
6 Experiments
6.1 Corpora
While previous studies on document-level parsing
only report their results on a particular corpus, to
show the generality of our method, we experiment
with texts from two very different genres. Our
first corpus is the standard RST-DT (Carlson et
al., 2002), which consists of 385 Wall Street Jour-
nal articles, and is partitioned into a training set
of 347 documents and a test set of 38 documents.
53 documents, selected from both sets were anno-
tated by two annotators, based on which we mea-
sure human agreement. In RST-DT, the original 25
rhetorical relations defined by (Mann and Thomp-
son, 1988) are further divided into a set of 18
coarser relation classes with 78 finer-grained rela-
tions. Our second corpus is the Instructional cor-
pus prepared by (Subba and Di-Eugenio, 2009),
which contains 176 how-to-do manuals on home-
repair. The corpus was annotated with 26 informa-
tional relations (e.g., Preparation-Act, Act-Goal).
6.2 Experimental Setup
We experiment with our discourse parser on the
two datasets using our two different parsing ap-
proaches, namely 1S-1S and the sliding window.
We compare our approach with HILDA (Hernault
et al, 2010) on RST-DT, and with the ILP-based
approach of (Subba and Di-Eugenio, 2009) on the
Instructional corpus, since they are the state-of-
the-art on the respective genres. On RST-DT, the
standard split was used for training and testing
purposes. The results for HILDA were obtained
by running the system with default settings on the
same inputs we provided to our system. Since we
could not run the ILP-based system of (Subba and
Di-Eugenio, 2009) (not publicly available) on the
Instructional corpus, we report the performances
presented in their paper. They used 151 documents
for training and 25 documents for testing. Since
we did not have access to their particular split,
we took 5 random samples of 151 documents for
training and 25 documents for testing, and report
the average performance over the 5 test sets.
To evaluate the parsing performance, we use
the standard unlabeled (i.e., hierarchical spans)
and labeled (i.e., nuclearity and relation) preci-
sion, recall and F-score as described in (Marcu,
2000b). To compare with previous studies, our
experiments on RST-DT use the 18 coarser rela-
tions. After attaching the nuclearity statuses (NS,
SN, NN) to these relations, we get 41 distinct re-
lations. Following (Subba and Di-Eugenio, 2009)
on the Instructional corpus, we use 26 relations,
and treat the reversals of non-commutative rela-
tions as separate relations. That is, Goal-Act and
Act-Goal are considered as two different relations.
Attaching the nuclearity statuses to these relations
gives 76 distinct relations. Analogous to previous
studies, we map the n-ary relations (e.g., Joint)
into nested right-branching binary relations.
6.3 Results and Error Analysis
Table 2 presents F-score parsing results for our
parsers and the existing systems on the two cor-
pora.2 On both corpora, our parser, namely, 1S-1S
(TSP 1-1) and sliding window (TSP SW), outper-
form existing systems by a wide margin (p<7.1e-
05).3 On RST-DT, our parsers achieve absolute
F-score improvements of 8%, 9.4% and 11.4%
in span, nuclearity and relation, respectively, over
HILDA. This represents relative error reductions
of 32%, 23% and 21% in span, nuclearity and rela-
tion, respectively. Our results are also close to the
upper bound, i.e. human agreement on this corpus.
On the Instructional genre, our parsers deliver
absolute F-score improvements of 10.5%, 13.6%
and 8.14% in span, nuclearity and relations, re-
spectively, over the ILP-based approach. Our
parsers, therefore, reduce errors by 36%, 27% and
13% in span, nuclearity and relations, respectively.
If we compare the performance of our parsers
on the two corpora, we observe higher results
on RST-DT. This can be explained in at least
two ways. First, the Instructional corpus has a
smaller amount of data with a larger set of rela-
tions (76 when nuclearity attached). Second, some
frequent relations are (semantically) very similar
(e.g., Preparation-Act, Step1-Step2), which makes
it difficult even for the human annotators to distin-
guish them (Subba and Di-Eugenio, 2009).
Comparison between our two models reveals
that TSP SW significantly outperforms TSP 1-1
only in finding the right structure on both corpora
(p<0.01). Not surprisingly, the improvement is
higher on the Instructional corpus. A likely ex-
planation is that the Instructional corpus contains
more leaky boundaries (12%), allowing the sliding
2Precision, Recall and F-score are the same when manual
segmentation is used (see Marcu, (2000b), page 143).
3Since we did not have access to the output or to the sys-
tem of (Subba and Di-Eugenio, 2009), we were not able to
perform a significance test on the Instructional corpus.
493
RST-DT Instructional
Metrics HILDA TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SW
Span 74.68 82.47* 82.74*? 88.70 70.35 79.67 80.88?
Nuclearity 58.99 68.43* 68.40* 77.72 49.47 63.03 63.10
Relation 44.32 55.73* 55.71* 65.75 35.44 43.52 43.58
Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA
(with p<7.1e-05) are denoted by *. Significant differences between TSP 1-1 and TSP SW (with p<0.01) are denoted by ?.
T-CT-OT-CMM-MCMPEVSUCNDENCATEEXBACOJOS-UATEL
T-C  T-O  T-CM      M-M     CMP  EV  SU     CND  EN     CA  TE    EX     BA   CO      JO   S-U   AT     EL10732111227114121291309359
00010203133350012722
0000100000001018532
 0020121007936757108
0000000302112332000
0001300102901921001
0001320004112121008
000000000070431001
000010001305111006
00000000242210000014
000000800000001000
0000100221010122010
0001010000011110000
000040000000020000
000000000000000000
001000000000000000
020000000000000001
000000000000000000
Figure 9: Confusion matrix for relation labels on the
RST-DT test set. Y-axis represents true and X-axis repre-
sents predicted relations. The relations are Topic-Change
(T-C), Topic-Comment (T-CM), Textual Organization (T-
O), Manner-Means (M-M), Comparison (CMP), Evaluation
(EV), Summary (SU), Condition (CND), Enablement (EN),
Cause (CA), Temporal (TE), Explanation (EX), Background
(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-
tion (AT) and Elaboration (EL).
window approach to be more effective in finding
those, without inducing much noise for the labels.
This clearly demonstrates the potential of TSP SW
for datasets with even more leaky boundaries e.g.,
the Dutch (Vliet and Redeker, 2011) and the Ger-
man Potsdam (Stede, 2004) corpora.
Error analysis reveals that although TSP SW
finds more correct structures, a corresponding im-
provement in labeling relations is not present be-
cause in a few cases, it tends to induce noise from
the neighboring sentences for the labels. For ex-
ample, when parsing was performed on the first
sentence in Figure 1 in isolation using 1S-1S, our
parser rightly identifies the Contrast relation be-
tween EDUs 2 and 3. But, when it is considered
with its neighboring sentences by the sliding win-
dow, the parser labels it as Elaboration. A promis-
ing strategy to deal with this and similar problems
that we plan to explore in future, is to apply both
approaches to each sentence and combine them by
consolidating three probabilistic decisions, i.e. the
one from 1S-1S and the two from sliding window.
To further analyze the errors made by our parser
on the hardest task of relation labeling, Figure 9
presents the confusion matrix for TSP 1-1 on the
RST-DT test set. The relation labels are ordered
according to their frequency in the RST-DT train-
ing set. In general, the errors are produced by two
different causes acting together: (i) imbalanced
distribution of the relations, and (ii) semantic sim-
ilarity between the relations. The most frequent
relation Elaboration tends to mislead others es-
pecially, the ones which are semantically similar
(e.g., Explanation, Background) and less frequent
(e.g., Summary, Evaluation). The relations which
are semantically similar mislead each other (e.g.,
Temporal:Background, Cause:Explanation).
These observations suggest two ways to im-
prove our parser. We would like to employ a more
robust method (e.g., ensemble methods with bag-
ging) to deal with the imbalanced distribution of
relations, along with taking advantage of a richer
semantic knowledge (e.g., compositional seman-
tics) to cope with the errors caused by semantic
similarity between the rhetorical relations.
7 Conclusion
In this paper, we have presented a novel discourse
parser that applies an optimal parsing algorithm
to probabilities inferred from two CRF models:
one for intra-sentential parsing and the other for
multi-sentential parsing. The two models exploit
their own informative feature sets and the distribu-
tional variations of the relations in the two parsing
conditions. We have also presented two novel ap-
proaches to combine them effectively. Empirical
evaluations on two different genres demonstrate
that our approach yields substantial improvement
over existing methods in discourse parsing.
Acknowledgments
We are grateful to Frank Tompa and the anony-
mous reviewers for their comments, and the
NSERC BIN and CGS-D for financial support.
494
References
O. Biran and O. Rambow. 2011. Identifying Justi-
fications in Written Dialogs by Classifying Text as
Argumentative. International Journal of Semantic
Computing, 5(4):363?381.
L. Carlson, D. Marcu, and M. Okurowski. 2002. RST
Discourse Treebank (RST-DT) LDC2002T07. Lin-
guistic Data Consortium, Philadelphia.
V. Feng and G. Hirst. 2012. Text-level Discourse Pars-
ing with Rich Linguistic Features. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, ACL ?12, pages 60?68,
Jeju Island, Korea. Association for Computational
Linguistics.
S. Fisher and B. Roark. 2007. The Utility of Parse-
derived Features for Automatic Discourse Segmen-
tation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, ACL
?07, pages 488?495, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In Pro-
ceedings of the 18th International Joint Conference
on Artificial Intelligence, IJCAI ?07, pages 1486?
1488, Acapulco, Mexico.
H. Hernault, H. Prendinger, D. duVerle, and
M. Ishizuka. 2010. HILDA: A Discourse Parser
Using Support Vector Machine Classification. Dia-
logue and Discourse, 1(3):1?33.
S. Joty, G. Carenini, and R. T. Ng. 2012. A Novel
Discriminative Framework for Sentence-Level Dis-
course Analysis. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ?12, pages 904?
915, Jeju Island, Korea. Association for Computa-
tional Linguistics.
H. LeThanh, G. Abeysinghe, and C. Huyck. 2004.
Generating Discourse Structures for Written Texts.
In Proceedings of the 20th international confer-
ence on Computational Linguistics, COLING ?04,
Geneva, Switzerland. Association for Computa-
tional Linguistics.
A. Louis, A. Joshi, and A. Nenkova. 2010. Discourse
Indicators for Content Selection in Summarization.
In Proceedings of the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
SIGDIAL ?10, pages 147?156, Tokyo, Japan. Asso-
ciation for Computational Linguistics.
W. Mann and S. Thompson. 1988. Rhetorical Struc-
ture Theory: Toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?02, pages
368?375. Association for Computational Linguis-
tics.
D. Marcu. 2000a. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-based Approach. Compu-
tational Linguistics, 26:395?448.
D. Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press,
Cambridge, MA, USA.
J. Morris and G. Hirst. 1991. Lexical Cohesion
Computed by Thesaural Relations as an Indicator
of Structure of Text. Computational Linguistics,
17(1):21?48.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki,
and B. Webber. 2005. The Penn Discourse Tree-
Bank as a Resource for Natural Language Gener-
ation. In Proceedings of the Corpus Linguistics
Workshop on Using Corpora for Natural Language
Generation, pages 25?32, Birmingham, U.K.
S. Somasundaran, 2010. Discourse-Level Relations for
Opinion Analysis. PhD thesis, University of Pitts-
burgh.
R. Soricut and D. Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. In Proceedings of the 2003 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology, NAACL-HLT ?03, pages 149?
156, Edmonton, Canada. Association for Computa-
tional Linguistics.
C. Sporleder and M. Lapata. 2005. Discourse Chunk-
ing and its Application to Sentence Compression.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, pages 257?264, Van-
couver, British Columbia, Canada. Association for
Computational Linguistics.
C. Sporleder and A. Lascarides. 2004. Combining Hi-
erarchical Clustering and Machine Learning to Pre-
dict High-Level Discourse Structure. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Geneva, Switzer-
land. Association for Computational Linguistics.
M. Stede. 2004. The Potsdam Commentary Corpus.
In Proceedings of the ACL-04 Workshop on Dis-
course Annotation, Barcelona. Association for Com-
putational Linguistics.
R. Subba and B. Di-Eugenio. 2009. An Effective Dis-
course Parser that Uses Rich Linguistic Information.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT-NAACL ?09, pages 566?574, Boul-
der, Colorado. Association for Computational Lin-
guistics.
495
C. Sutton and A. McCallum. 2012. An Introduction
to Conditional Random Fields. Foundations and
Trends in Machine Learning, 4(4):267?373.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting
Sequence Data. Journal of Machine Learning Re-
search (JMLR), 8:693?723.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating Discourse-based Answer Extrac-
tion for Why-question Answering. In Proceedings
of the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 735?736, Amsterdam, The Nether-
lands. ACM.
N. Vliet and G. Redeker. 2011. Complex Sentences as
Leaky Units in Discourse Parsing. In Proceedings
of Constraints in Discourse, Agay-Saint Raphael,
September.
496
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1220?1230,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Abstractive Summarization of Spoken and Written Conversations
Based on Phrasal Queries
Yashar Mehdad Giuseppe Carenini Raymond T. Ng
Department of Computer Science, University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
{mehdad, carenini, rng}@cs.ubc.ca
Abstract
We propose a novel abstractive query-
based summarization system for conversa-
tions, where queries are defined as phrases
reflecting a user information needs. We
rank and extract the utterances in a con-
versation based on the overall content and
the phrasal query information. We clus-
ter the selected sentences based on their
lexical similarity and aggregate the sen-
tences in each cluster by means of a word
graph model. We propose a ranking strat-
egy to select the best path in the con-
structed graph as a query-based abstract
sentence for each cluster. A resulting sum-
mary consists of abstractive sentences rep-
resenting the phrasal query information
and the overall content of the conversa-
tion. Automatic and manual evaluation
results over meeting, chat and email con-
versations show that our approach signifi-
cantly outperforms baselines and previous
extractive models.
1 Introduction
Our lives are increasingly reliant on multimodal
conversations with others. We email for business
and personal purposes, attend meetings in per-
son, chat online, and participate in blog or forum
discussions. While this growing amount of per-
sonal and public conversations represent a valu-
able source of information, going through such
overwhelming amount of data, to satisfy a partic-
ular information need, often leads to an informa-
tion overload problem (Jones et al, 2004). Au-
tomatic summarization has been proposed in the
past as a way to address this problem (e.g., (Sakai
and Sparck-Jones, 2001)). However, often a good
summary cannot be generic and should be a brief
and well-organized paragraph that answer a user?s
information need.
The Document Understanding Conference
(DUC)
1
has launched query-focused multidocu-
ment summarization as its main task since 2004,
by focusing on complex queries with very specific
answers. For example, ?How were the bombings
of the US embassies in Kenya and Tanzania
conducted? How and where were the attacks
planned??. Such complex queries are appropriate
for a user who has specific information needs and
can formulate the questions precisely. However,
especially when dealing with conversational data
that tend to be less structured and less topically
focused, a user is often initially only exploring the
source documents, with less specific information
needs. Moreover, following the common practice
in search engines, users are trained to form
simpler and shorter queries (Meng and Yu, 2010).
For example, when a user is interested in certain
characteristics of an entity in online reviews (e.g.,
?location? or ?screen?) or a specific entity in a
blog discussion (e.g., ?new model of iphone?), she
would not initially compose a complex query.
To address these issues, in this work, we tackle
the task of conversation summarization based on
phrasal queries. We define a phrasal query as a
concatenation of two or more keywords, which is
a more realistic representation of a user?s informa-
tion needs. For conversational data, this definition
is more similar to the concept of search queries in
information retrieval systems as well as to the con-
cept of topic labels in the task of topic modeling.
Example 1 shows two queries and their associated
human written summaries based on a single chat
log. We can observe that the two summaries, al-
though generated from the same chat log, are to-
tally distinct. This further demonstrates the impor-
tance of phrasal query-based summarization sys-
tems for long conversations.
To date, most systems in the area of summa-
1
http://www-nlpir.nist.gov/projects/duc/index.html
1220
Query-1: Test/Sample database for GNUe
Abstract-1: James Thompson asked Reinhard: I was going to work
on the sample tonight. You mentioned wanting a fishhook and all data
types. Any other things you want to see in there? Reinhard said that
master/detail would be good, as there have been bugs only appearing in
3-level case. James said he already included that and I know I need to
add a boolean. Did you want date as well as date-time? Reinhard said
yes - we also have time values (time without date). They are especially
interesting. James had not ever had use for something like that so I?m
not sure where I would graft that in.
Query-2: Passing parameters to Forms
Abstract-2: James Thompson (jamest) asked how did parameter sup-
port in forms change recently? He reported the trigger namespace func-
tion referencesGFForm.parameters - which no longer exists. Reinhard
said every GFForm should have a parameters. James said he was using
parameters in on-startup. Reinhard said that?s probably the only place
where they don?t work. James said that I?m thinking about moving that
to on-activation instead of on-startup anyway as it should still work for
a main form - but i still wonder if the on-startup parameter issue should
be considered a bug - as it shouldn?t choke. Reinhard was sure it should
be considered a bug but I have no idea how to fix it. We haven?t found a
way to deal with parameters that works for every case. I don?t know if
there is any chance to pass the parameters to the form before it is acti-
vated. James asked how are parameters handled now? Reinhard replied
that they are passed to activateForm so they are available from activa-
tion for the ?main? form, the command line parameters are passed and
for dialogs, the parameters are passed that were given in runDialog.
Example 1: Sample queries and associated
human-written query-based summaries for a chat
log.
rization focus on news or other well-written docu-
ments, while research on summarizing multiparty
written conversations (e.g., chats, emails) has been
limited. This is because traditional NLP ap-
proaches developed for formal texts often are not
satisfactory when dealing with multiparty written
conversations, which are typically in a casual style
and do not display a clear syntactic structure with
proper grammar and spelling. Even though some
works try to address the problem of summarizing
multiparty written conversions (e.g., (Mehdad et
al., 2013b; Wang and Cardie, 2013; Murray et
al., 2010; Zhou and Hovy, 2005; Gillick et al,
2009)), they do so in a generic way (not query-
based) and focus on only one conversational do-
main (e.g., meetings). Moreover, most of the pro-
posed systems for conversation summarization are
extractive.
To address such limitations, we propose a fully
automatic unsupervised abstract generation frame-
work based on phrasal queries for multimodal con-
versation summarization. Our key contributions in
this work are as follows:
1) To the best of our knowledge, our framework
is the first abstractive system that generates sum-
maries based on users phrasal queries, instead of
well-formed questions. As a by-product of our
approach, we also propose an extractive summa-
rization model based on phrasal queries to select
the summary-worthy sentences in the conversation
based on query terms and signature terms (Lin and
Hovy, 2000).
2) We propose a novel ranking strategy to select
the best path in the constructed word graph by tak-
ing the query content, overall information content
and grammaticality (i.e., fluency) of the sentence
into consideration.
3) Although most of the current summarization
approaches use supervised algorithms as a part
of their system (e.g., (Wang et al, 2013)), our
method can be totally unsupervised and does not
depend on human annotation.
4) Although different conversational modali-
ties (e.g., email vs. chat vs. meeting) underline
domain-specific characteristics, in this work, we
take advantage of their underlying similarities to
generalize away from specific modalities and de-
termine effective method for query-based summa-
rization of multimodal conversations.
We evaluate our system over GNUe Traffic
archive
2
Internet Relay Chat (IRC) logs, AMI
meetings corpus (Carletta et al, 2005) and BC3
emails dataset (Ulrich et al, 2008). Automatic
evaluation on the chat dataset and manual eval-
uation over the meetings and emails show that
our system uniformly and statistically significantly
outperforms baseline systems, as well as a state-
of-the-art query-based extractive summarization
system.
2 Phrasal Query Abstraction
Framework
Our phrasal query abstraction framework gener-
ates a grammatical abstract from a conversation
following three steps, as shown in Figure 1.
2.1 Utterance Extraction
Abstractive summary sentences can be created by
aggregating and merging multiple sentences into
an abstract sentence. In order to generate such
a sentence, we need to identify which sentences
from the original document should be extracted
and combined to generate abstract sentences. In
other words, we want to identify the summary-
worthy sentences in the text that can be combined
into an abstract sentence. This task can be con-
sidered as content selection. Moreover, this step,
stand alone, corresponds to an extractive summa-
rization system.
2
http://kt.earth.li/GNUe/index.html
1221
Original 
conversation  
Query 
Extracted 
utterances 
Filtered 
utterances 
Ex
tra
ctio
n 
Re
dun
dan
cy 
Re
mo
val
 
Ge
ner
atio
n 
Clusters Word graphs 
Top ranked 
sentences Query-based 
abstract 
Clustering Word Graph Ranking Construction 
Figure 1: Phrasal query abstraction framework. The steps (arrows) influenced by the query are high-
lighted.
Signature terms: navigator, functionality, reports, UI, schema, gnu
Chat log:
- but watching them build a UI in the flash demo?s is pretty damn im-
pressive... and have started moving my sales app to all UI being built
via ...
- i?ll be expanding the technotes in navigator for a while ...
- ... in terms of functionality of the underlying databases ...
- you mean if I start GNU again I have to read bug reports too?
- no, just in case you want to enter bug report
- ...I expand the schema before populating with test data ...
- i?m willing to scrap it if there is a better schema hidden in gnue some-
where :)
Example 2: Sample signature terms for a part of a
chat log.
In order to select and extract the informative
summary-worthy utterances, based on the phrasal
query and the original text, we consider two cri-
teria: i) utterances should carry the essence of the
original text; and ii) utterances should be relevant
to the query. To fulfill such requirements we define
the concepts of signature terms and query terms.
2.1.1 Signature Terms
Signature terms are generally indicative of the
content of a document or collection of docu-
ments. To identify such terms, we can use fre-
quency, word probability, standard statistic tests,
information-theoretic measures or log-likelihood
ratio. In this work, we use log-likelihood ratio to
extract the signature terms from chat logs, since
log-likelihood ratio leads to better results (Gupta
et al, 2007). We use a method described in (Lin
and Hovy, 2000) in order to identify such terms
and their associated weight. Example 2 demon-
strates a chat log and associated signature terms.
2.1.2 Query Terms
Query terms are indicative of the content in a
phrasal query. In order to identify such terms,
we first extract all content terms from the query.
Then, following previous studies (e.g., (Gonzalo
et al, 1998)), we use the synsets relations in Word-
Net for query expansion. We extract all concepts
that are synonyms to the query terms and add
them to the original set of query terms. Note that
we limit our synsets to the nouns since verb syn-
onyms do not prove to be effective in query ex-
pansion (Hunemark, 2010). While signature terms
are weighted, we assume that all query terms are
equally important and they all have wight equal to
1.
2.1.3 Utterance Scoring
To estimate the utterance score, we view both
the query terms and the signature terms as the
terms that should appear in a human query-based
summary. To achieve this, the most relevant
(summary-worthy) utterances that we select are
the ones that maximize the coverage of such terms.
Given the query terms and signature terms, we can
estimate the utterance score as follows:
Score
Q
=
1
n
n
?
i=1
t(q)
i
(1)
Score
S
=
1
n
n
?
i=1
t(s)
i
? w(s)
i
(2)
Score = ? ? Score
Q
+ ? ? Score
S
(3)
where n is number of content words in the ut-
terance, t(q)
i
= 1 if the term t
i
is a query term
and 0 otherwise, and t(s)
i
= 1 if t
i
is a signature
term and 0 otherwise, and w(s)
i
is the normalized
associated weight for signature terms. The param-
eters ? and ? are tuned on a development set and
sum up to 1.
After all the utterances are scored, the top
scored utterances are selected to be sent to the next
step. We estimate the percentage of the retrieved
utterances based on the development set.
1222
2.2 Redundancy Removal
Utterances selected in previous step often in-
clude redundant information, which is semanti-
cally equivalent but may vary in lexical choices.
By identifying the semantic relations between the
sentences, we can discover what information in
one sentence is semantically equivalent, novel, or
more/less informative with respect to the content
of the other sentences. Similar to earlier work
(Berant et al, 2011; Adler et al, 2012), we set
this problem as a variant of the Textual Entail-
ment (TE) recognition task (Dagan and Glickman,
2004). Using entailment in this phase is moti-
vated by taking advantage of semantic relations
instead of pure statistical methods (e.g., Maximal
Marginal Relevance) and shown to be more effec-
tive (Mehdad et al, 2013a). We follow the same
practice as (Mehdad et al, 2013a) to build an en-
tailment graph for all selected sentences to identify
relevant sentences and eliminate the redundant (in
terms of meaning) and less informative ones.
2.3 Abstract Generation
In this phase, our goal is to generate understand-
able informative abstract sentences that capture
the content of the source sentences and represents
the information needs defined by queries. There
are several ways of generating abstract sentences
(e.g. (Barzilay and McKeown, 2005; Liu and Liu,
2009; Ganesan et al, 2010; Murray et al, 2010));
however, most of them rely heavily on the sen-
tence structure. We believe that such approaches
are suboptimal, especially in dealing with conver-
sational data, because multiparty written conversa-
tions are often poorly structured. Instead, we ap-
ply an approach that does not rely on syntax, nor
on a standard NLG architecture. Moreover, since
dealing with user queries efficiency is an impor-
tant aspect, we aim for an approach that is also
motivated by the speed with which the abstracts
are obtained. We perform the task of abstract gen-
eration in three steps, as follows:
2.3.1 Clustering
In order to generate an abstract summary, we need
to identify which sentences from the previous step
(i.e., redundancy removal) can be clustered and
combined in generated abstract sentences. This
task can be viewed as sentence clustering, where
each sentence cluster can provide the content for
an abstract sentence.
We use the K-mean clustering algorithm by co-
sine similarity as a distance function between sen-
tence vectors composed of tf.idf scores. Also no-
tice that the lexical similarity between sentences in
one cluster facilitates both the construction of the
word graph and finding the best path in the word
graph, as described next.
2.3.2 Word Graph
In order to construct a word graph, we adopt
the method recently proposed by (Mehdad et al,
2013a; Filippova, 2010) with some optimizations.
Below, we show how the word graph is applied to
generate the abstract sentences.
Let G = (W,L) be a directed graph with the
set of nodes W representing words and a set of
directed edges L representing the links between
words. Given a cluster of related sentences S =
{s
1
, s
2
, ..., s
n
}, a word graph is constructed by it-
eratively adding sentences to it. In the first step,
the graph represents one sentence plus the start
and end symbols. A node is added to the graph for
each word in the sentence, and words adjacent are
linked with directed edges. When adding a new
sentence, a word from the sentence is merged in
an existing node in the graph providing that they
have the same POS tag and they satisfy one of the
following conditions:
i) They have the same word form;
ii) They are connected in WordNet by the syn-
onymy relation. In this case the lexical choice for
the node is selected based on the tf.idf score of
each node;
iii) They are from a hypernym/hyponym pair or
share a common direct hypernym. In this case,
both words are replaced by the hypernym;
iv) They are in an entailment relation. In this
case, the entailing word is replaced by the entailed
one.
The motivation behind merging non-identical
words is to enrich the common terms between
the phrases to increase the chance that they could
merge into a single phrase. This also helps to
move beyond the limitation of original lexical
choices. In case the merging is not possible a
new node is created in the graph. When a node
can be merged with multiple nodes (i.e., merging
is ambiguous), either the preceding and following
words in the sentence and the neighboring nodes
in the graph or the frequency is used to select the
candidate node.
We connect adjacent words with directed edges.
1223
For the new nodes or unconnected nodes, we draw
an edge with a weight of 1. In contrast, when two
already connected nodes are added (merged), the
weight of their connection is increased by 1.
2.3.3 Path Ranking
A word graph, as described above, may contain
many sequences connecting start and end. How-
ever, it is likely that most of the paths are not read-
able. We are aiming at generating an informative
abstractive sentence for each cluster based on a
user query. Moreover, the abstract sentence should
be grammatically correct.
In order to satisfy both requirements, we have
devised the following ranking strategy. First, we
prune the paths in which a verb does not exist,
to filter ungrammatical sentences. Then we rank
other paths as follows:
Query focus: to identify the summary sentence
with the highest coverage of query content, we
propose a score that counts the number of query
terms that appear in the path. In order to reward
the ranking score to cover more salient terms in
the query content, we also consider the tf.idf score
of query terms in the coverage formulation.
Q(P ) =
?
q
i
?P
tfidf (q
i
)
?
q
i
?G
tfidf (q
i
)
where the q
i
are the query terms.
Fluency: in order to improve the grammaticality
of the generated sentence, we coach our ranking
model to select more fluent (i.e., grammatically
correct) paths in the graph. We estimate the gram-
maticality of generated paths (Pr(P )) using a lan-
guage model.
Path weight: The purpose of this function is two-
fold: i) to generate a grammatical sentence by fa-
voring the links between nodes (words) which ap-
pear often; and ii) to generate an informative sen-
tence by increasing the weight of edges connecting
salient nodes. For a path P with m nodes, we de-
fine the edge weightw(n
i
, n
j
) and the path weight
W (P ) as below:
w(n
i
, n
j
) =
freq(n
i
) + freq(n
j
)
?
P
?
?G
n
i
,n
j
?P
?
diff (P
?
, n
i
, n
j
)
?1
W (P ) =
?
m?1
i=1
w(n
i
, n
i+1
)
m? 1
where the function diff(P
?
, n
i
, n
j
) refers to the
distance between the offset positions pos(P
?
, n
i
)
of nodes n
i
and n
j
in path P
?
(any path in G con-
taining n
i
and n
j
) and is defined as |pos(P
?
, n
j
)?
pos(P
?
, n
i
)|.
Overal ranking score: In order to generate a
query-based abstract sentence that combines the
scores above, we employ a ranking model. The
purpose of such a model is three-fold: i) to cover
the content of query information optimally; ii) to
generate a more readable and grammatical sen-
tence; and iii) to favor strong connections between
the concepts. Therefore, the final ranking score of
path P is calculated over the normalized scores as:
Score(P ) = ? ?Q(P ) + ? ? Pr(P )? ? ?W (P )
Where ?, ? and ? are the coefficient factors to
tune the ranking score and they sum up to 1. In or-
der to rank the graph paths, we select all the paths
that contain at least one verb and rerank them us-
ing our proposed ranking function to find the best
path as the summary of the original sentences in
each cluster.
3 Experimental Setup
In this section, we show the evaluation results of
our proposed framework and its comparison to the
baselines and a state-of-the-art query-focused ex-
tractive summarization system.
3.1 Datasets
One of the challenges of this work is to find suit-
able conversational datasets that can be used for
evaluating our query-based summarization sys-
tem. Most available conversational corpora do not
contain any human written summaries, or the gold
standard human written summaries are generic
(Carletta et al, 2005; Joty et al, 2013). In this
work, we use available corpora for emails and
chats for written conversations, while for spoken
conversation, we employ an available corpus in
multiparty meeting conversations.
Chat: to the best of our knowledge, the only pub-
licly available chat logs with human written sum-
maries can be downloaded from the GNUe Traffic
archive (Zhou and Hovy, 2005; Uthus and Aha,
2011; Uthus and Aha, 2013). Each chat log has
a human created summary in the form of a digest.
Each digest summarizes IRC logs for a period and
consists of few summaries over each chat log with
a unique title for the associated human written
summary. In this way, the title of each summary
1224
can be counted as a phrasal query and the cor-
responding summary is considered as the query-
based abstract of the associated chat log includ-
ing only the information most relevant to the title.
Therefore, we can use the human-written query-
based abstract as gold standards and evaluate our
system automatically. Our chat dataset consists of
66 query-based (title-based) human written sum-
maries with their associated queries (titles) and
chat logs, created from 40 original chat logs. The
average number of tokens are 1840, 325 and 6 for
chat logs, query-based summaries and queries, re-
spectively.
Meeting: we use the AMI meeting corpus (Car-
letta et al, 2005) that consists of 140 multiparty
meetings with a wide range of annotations, includ-
ing generic abstractive summaries for each meet-
ing. In order to create queries, we extract three
key-phrases from generic abstractive summaries
using TextRank algorithm (Mihalcea and Tarau,
2004). We use the extracted key-phrases as queries
to generate query-based abstracts. Since there is
no human-written query-based summary for AMI
corpus, we randomly select 10 meetings and eval-
uate our system manually.
Email: we use BC3 (Ulrich et al, 2008), which
contains 40 threads from the W3C corpus. BC3
corpus is annotated with generic human-written
abstractive summaries, and it has been used in sev-
eral previous works (e.g., (Joty et al, 2011)). In
order to adapt this corpus to our framework, we
followed the same query generation process as for
the meeting dataset. Finally, we randomly select
10 emails threads and evaluate the results manu-
ally.
3.2 Baselines
We compare our approach with the following
baselines:
1) Cosine-1st: we rank the utterances in the chat
log based on the cosine similarity between the ut-
terance and query. Then, we select the first ut-
trance as the summary;
2) Cosine-all: we rank the utterances in the chat
log based on the cosine similarity between the ut-
terance and query and then select the utterances
with a cosine similarity greater than 0;
3) TextRank: a widely used graph-based rank-
ing model for single-document sentence extraction
that works by building a graph of all sentences in a
document and use similarity as edges to compute
the salience of sentences in the graph (Mihalcea
and Tarau, 2004);
4) LexRank: another popular graph-based con-
tent selection algorithm for multi-document sum-
marization (Erkan and Radev, 2004);
5) Biased LexRank: is a state-of-the-art query-
focused summarization that uses LexRank algo-
rithm in order to recursively retrieve additional
passages that are similar to the query, as well as
to the other nodes in the graph (Otterbacher et al,
2009).
Moreover, we compare our abstractive system
with the first part of our framework (utterance ex-
traction in Figure 1), which can be presented as an
extractive query-based summarization system (our
extractive system). We also show the results of the
version we use in our pipeline (our pipeline ex-
tractive system). The only difference between the
two versions is the length of the generated sum-
maries. In our pipeline we aim at higher recall,
since we later filter sentences and aggregate them
to generate new abstract sentences. In contrast,
in the stand alone version (extractive system) we
limit the number of retrieved sentences to the de-
sired length of the summary. We also compare the
results of our full system (i.e., with tuning) with
a non-optimized version when the ranking coef-
ficients are distributed equally (? = ? = ? =
0.33). For parameters estimation, we tune all pa-
rameters (utterance selection and path ranking) ex-
haustively with 0.1 intervals using our develop-
ment set.
For manual evaluation of query-based abstracts
(meeting and email datasets), we perform a sim-
ple user study assessing the following aspects: i)
Overall quality given a query (5-point scale)?; and
ii) Responsiveness: how responsive is the gener-
ated summary to the query (5-point scale)? Each
query-based abstract was rated by two annotators
(native English speaker). Evaluators are presented
with the original conversation, query and gener-
ated summary. For the manual evaluation, we
only compare our full system with LexRank (LR)
and Biased LexRank (Biased LR). We also ask
the evaluators to select the best summary for each
query and conversation, given our system gener-
ated summary and the two baselines.
To evaluate the grammaticality of our generated
summaries, following common practice (Barzilay
and McKeown, 2005), we randomly selected 50
sentences from original conversations and system
1225
Models ROUGE-1 (%) ROUGE-2 (%)
Prc Rec F-1 Prc Rec F-1
Cosine-1st 71 5 8 30 3 5
Cosine-all 30 68 38 18 40 22
TextRank 25 76 34 15 44 20
LexRank 36 50 37 14 20 15
Biased LexRank 36 51 38 15 21 16
Utterance extraction (our extractive system) 34 66
?
40
??
20
??
40
?
24
??
Utterance extraction (our pipeline extractive system) 30 73
?
38 19
??
44
?
24
??
Our abstractive system (without tuning) 38
?
59
?
41
??
18
?
27
?
19
?
Our abstractive system (with tuning) 40
??
56
?
42
??
20
??
25
?
22
??
Table 1: Performance of different summarization algorithms on chat logs for query-based chat sum-
marization. Statistically significant improvements (p < 0.01) over the biased LexRank system are
marked with *. ? indicates statistical significance (p < 0.01) over extractive approaches (TextRank
and LexRank). Systems in italics use the query in generating the summary.
generated abstracts, for each dataset. Then, we
asked annotators to give one of three possible rat-
ings for each sentence based on grammaticality:
perfect (2 pts), only one mistake (1 pt) and not ac-
ceptable (0 pts), ignoring capitalization or punc-
tuation. Each sentence was rated by two annota-
tors. Note that each sentence was evaluated indi-
vidually, so the human judges were not affected
by intra-sentential problems posed by coreference
and topic shifts.
3.3 Experimental Settings
For preprocessing our dataset we use OpenNLP
3
for tokenization, stemming and part-of-speech
tagging. We use six randomly selected query-
logs from our chat dataset (about 10% of the
dataset) for tuning the coefficient parameters. We
set the k parameter in our clustering phase to
10 based on the average number of sentences
in the human written summaries. For our lan-
guage model, we use a tri-gram smoothed lan-
guage model trained using the newswire text pro-
vided in the English Gigaword corpus (Graff and
Cieri, 2003). For the automatic evaluation we use
the official ROUGE software with standard op-
tions and report ROUGE-1 and ROUGE-2 preci-
sion, recall and F-1 scores.
3.4 Results
3.4.1 Automatic Evaluation (Chat dataset)
Abstractive vs. Extractive: our full query-
based abstractive summariztion system show sta-
tistically significant improvements over baselines
3
http://opennlp.apache.org/
and other pure extractive summarization systems
for ROUGE-1
4
. This means our systems can ef-
fectively aggregate the extracted sentences and
generate abstract sentences based on the query
content. We can also observe that our full system
produces the highest ROUGE-1 precision score
among all models, which further confirms the suc-
cess of this model in meeting the user informa-
tion needs imposed by queries. The absolute im-
provement of 10% in precision for ROUGE-1 in
our abstractive model over our extractive model
(our pipeline) further confirms the effectiveness of
our ranking method in generating the abstract sen-
tences considering the query related information.
Our extractive query-based method beats all
other extractive systems with a higher ROUGE-
1 and ROUGE-2 which shows the effectiveness of
our utterance extraction model in comparison with
other extractive models. In other words, using
our extractive model described in section 2.1, as
a stand alone system, is an effective query-based
extractive summarization model. We also observe
that our extractive model outperforms our abstrac-
tive model for ROUGE-2 score. This can be due
to word merging and word replacement choices
in the word graph construction, which sometimes
change or remove a word in a bigram and conse-
quently may decrease the bigram overlap score.
Query Relevance: another interesting observa-
tion is that relying only on the cosine similarity
(i.e., cosine-all) to measure the query relevance
presents a quite strong baseline. This proves the
importance of query content in our dataset and fur-
ther supports the main claim of our work that a
4
The statistical significance tests was calculated by ap-
proximate randomization, as described in (Yeh, 2000).
1226
Dataset Overal Quality Responsiveness Preference
Our Sys Biased LR LR Our Sys Biased LR LR Our Sys Biased LR LR
Meeting 2.9 2.5 2.1 3.8 3.2 1.8 70% 30% 0%
Email 2.7 1.8 1.7 3.7 3.0 1.5 60% 30% 10%
Table 2: Manual evaluation scores for our phrasal query abstraction system in comparison with Biased
LexRank and LexRank (LR).
Dataset Grammar G=2 G=1 G=0
Orig Sys Orig Sys Orig Sys Orig Sys
Chat 1.8 1.6 84% 73% 16% 24% 0% 3%
Meeting 1.5 1.3 50% 40% 50% 55% 0% 5%
Email 1.9 1.6 85% 60% 15% 35% 0% 5%
Table 3: Average rating and distribution over grammaticality scores for phrasal query abstraction system
in comparison with original sentences.
good summary should express a brief and well-
organized abstract that answers the user?s query.
Moreover, a precision of 71% for ROUGE-1 from
the simple cosine-1st baseline confirms that some
utterances contain more query relevant informa-
tion in conversational discussions.
Query-based vs. Generic: the high recall
and low precision in TextRank baseline, both for
the ROUGE-1 and ROUGE-2 scores, shows the
strength of the model in extracting the generic in-
formation from chat conversations while missing
the query-relevant content. The LexRank baseline
improves the results of the TextRank system by
increasing the precision and balancing the preci-
sion and recall scores for ROUGE-1 score. We
believe that this is due to the robustness of the
LexRank method in dealing with noisy texts (chat
conversations) (Erkan and Radev, 2004). In addi-
tion, the Biased LexRank model slightly improves
the generic LexRank system. Considering this
marginal improvement and relatively high results
of pure extractive systems, we can infer that the
Biased LexRank extracted summaries do not carry
much query relevant content. In contrast, the sig-
nificant improvement of our model over the ex-
tractive methods demonstrates the success of our
approach in presenting the query related content
in generated abstracts.
An example of a short chat log, its related query
and corresponding manual and automatic sum-
maries are shown in Example 3.
3.4.2 Manual Evaluation
Content and User Preference: Table 2 demon-
strates overall quality, responsiveness (query re-
latedness) and user preference scores for the ab-
stracts generated by our system and two base-
lines. Results indicate that our system signif-
icantly outperforms baselines in overall quality
and responsiveness, for both meeting and email
datasets. This confirms the validity of the re-
sults we obtained by conducting automatic evalu-
ation over the chat dataset. We also can observe
that the absolute improvements in overall qual-
ity and responsiveness for emails (0.9 and 0.7) is
greater than for meetings (0.4 and 0.6). This is
expected since dealing with spoken conversations
is more challenging than written ones. Note that
the responsiveness scores are greater than over-
all scores. This further proves the effectiveness of
our approach in dealing with phrasal queries. We
also evaluate the users? summary preferences. For
both datasets (meeting and email), in majority of
cases (70% and 60% respectively), the users prefer
the query-based abstractive summary generated by
our system.
Grammaticality: Table 3 shows grammaticality
scores and distributions over the three possible
scores for all datasets. The chat dataset results
demonstrate the highest scores: 73% of the sen-
tences generated by our phrasal query abstrac-
tion model are grammatically correct and 24% of
the generated sentences are almost correct with
only one grammatical error, while only 3% of
the abstract sentences are grammatically incor-
rect. However, the results varies moving to other
datasets. For meeting dataset, the percentage of
completely grammatical sentences drops dramati-
cally. This is due to the nature of spoken conver-
sations which is more error prone and ungrammat-
ical. The grammaticality score of the original sen-
tences also proves that the sentences from meet-
1227
Query: Trigger namespace and the self property
Chat log:
A: good morning
B: good morning
C: good morning everyone
D: good morning
D: good night all
F: New GNUe Traffic online
F: loadsa deep metaphyisical stuff this week
F: D & E discuss the meaning of ?self? ;-)
E: yes, and he took the more metaphysical route, where I took the more
scientific route
E: I say self?s meaning is derived from one?s ancestry
E: self?s meaning is derived from how others use you
E: okay, analogy extended too far, I guess :)
F: is this a friends vs family debate?
E: also noted that the cool part about that is if you have code that needs
to happen both on a pre-insert and a pre-update - but only a few lines of
the code is different
E: you could have one block of trigger code that used self.action to find
out why it had been called and branch accordingly.
E: there was a big jump from the previous paragraph to that
E: that took that out of context
E: iirc, I was saying an alternative was that ?self? could refer to neither
the trigger?s owner nor to the trigger?s caller
E: but to the event itself
E: so self.action could be what action is being performed
E: self.parent could be the trigger?s owner
E: and self.context could be the caller
E: and self.name could be the name of the trigger
F: umm, I?m afraid apparant non-sequiturs are always a hazard of doing
summaries ;-)
E: iow, I was presenting a 3rd alternative where self referred to some-
thing specific to the trigger
F: I normally try to write around them, but not 100% successful
E: I?m just convulsing my thoughts to the irc log
E: for an errata next week :)
E: I think convulsing is a good description
F: heh
TexRank: There was a big jump from the previous paragraph to that.
that took that out of context iirc, I was saying an alternative was that
?self? could refer to neither the trigger?s owner nor to the trigger?s caller.
but to the event itself. so self.action could be what action is being per-
formed, self.parent could be the trigger?s owner, and self.context could
be the caller and self.name could be the name of the trigger. umm, I?m
afraid apparant non-sequiturs are always a hazard of doing summaries
;-)
LexRank: good morning everyone. heh. I?m just convulsing my
thoughts to the irc log good morning. Jason also noted that the cool
part about that is if you have code that needs to happen both on a pre-
insert and a pre-update - but only a few lines of the code is different -
you could have one block of trigger code that used self.action to find out
why it had been called and branch accordingly. for an errata next week
:) self?s meaning is derived from how others use you. I think convulsing
is a good description reinhard & jcater discuss the meaning of ?self? ;-)
Biased-LexRank: good morning everyone. heh. I?m just convulsing
my thoughts to the irc log. Jason also noted that the cool part about
that is if you have code that needs to happen both on a pre-insert and
a pre-update - but only a few lines of the code is different - you could
have one block of trigger code that used self.action to find out why it
had been called and branch accordingly. yes, and he took the more
metaphysical route, where I took the more scientific route there was
a big jump from the previous paragraph to that but to the event itself.
iow, I was presenting a 3rd alternative where self referred to something
specific to the trigger.
Our system: self could refer to neither the triggers owner nor caller.
I was saying an alternative where self referred to something specific to
the trigger. and self.name could be the name.
so self.action could be what action is being performed, self.parent the
triggers owner and self.context caller.
Gold: Further to, E clarified that he had suggested that ?self? could
refer to neither the trigger?s owner nor to the trigger?s caller - but to
the event itself. So self.action could be what action is being performed,
self.parent could be the trigger?s owner, and self.context could be the
caller. In other words, I was presenting a 3rd alternative where self
referred to something specific to the trigger.
Example 3. Summaries generated by our system
and other baselines in comparison with the human-
written summary for a short chat log. Speaker in-
formation have been anonymized.
ing transcripts, although generated by humans, are
not fully grammatical. In comparison with the
original sentences, for all datasets, our model re-
ports slightly lower results for the grammaticality
score. Considering the fact that the abstract sen-
tences are automatically generated and the orig-
inal sentences are human-written, the grammat-
icality score and the percentage of fully gram-
matical sentences generated by our system, with
higher ROUGE or quality scores in comparison
with other methods, demonstrates that our system
is an effective phrasal query abstraction frame-
work for both spoken and written conversations.
4 Conclusion
We have presented an unsupervised framework for
abstractive summarization of spoken and written
conversations based on phrasal queries. For con-
tent selection, we propose a sentence extraction
model that incorporates query relevance and con-
tent importance into the extraction process. For
the generation phase, we propose a ranking strat-
egy which selects the best path in the constructed
word graph based on fluency, query relevance
and content. Both automatic and manual evalua-
tion of our model show substantial improvement
over extraction-based methods, including Biased
LexRank, which is considered a state-of-the-art
system. Moreover, our system also yields good
grammaticality score for human evaluation and
achieves comparable scores with the original sen-
tences. Our future work is four-fold. First, we
are trying to improve our model by incorporating
conversational features (e.g., speech acts). Sec-
ond, we aim at implementing a strategy to or-
der the clusters for generating more coherent ab-
stracts. Third, we try to improve our generated
summary by resolving coreferences and incorpo-
rating speaker information (e.g., names) in the
clustering and sentence generation phases. Fi-
nally, we plan to take advantage of topic shifts to
better segment the relevant parts of conversations
in relation to phrasal queries.
Acknowledgments
We would like to thank the anonymous review-
ers for their valuable comments and suggestions
to improve the paper, and the NSERC Business In-
telligence Network for financial support. We also
would like to acknowledge the early discussions
on the related topics with Frank Tompa.
1228
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application
to the health-care domain. In Proceedings of
the ACL 2012 System Demonstrations, ACL ?12,
pages 79?84, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence Fusion for Multidocument News Sum-
marization. Comput. Linguist., 31(3):297?328,
September.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global Learning of Typed Entailment Rules.
In Proceedings of ACL, Portland, OR.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005.
The AMI meeting corpus: A pre-announcement. In
Proc. MLMI, pages 28?39.
I. Dagan and O. Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
G?unes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22(1):457?479,
December.
Katja Filippova. 2010. Multi-sentence compression:
finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 322?
330, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
340?348, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-tr. 2009. A global optimization
framework for meeting summarization. In Proc.
IEEE ICASSP, pages 4769?4772.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and
Juan M. Cigarrn. 1998. Indexing with wordnet
synsets can improve text retrieval. CoRR.
David Graff and Christopher Cieri. 2003. English Gi-
gaword Corpus. Technical report, Linguistic Data
Consortium, Philadelphia.
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 2007.
Measuring importance and query relevance in topic-
focused multi-document summarization. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 193?196, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Lisa Hunemark. 2010. Query expansion using search
logs and WordNet. Technical report, Uppsala Uni-
versity, mar. Masters thesis in Computational Lin-
guistics.
Quentin Jones, Gilad Ravid, and Sheizaf Rafaeli. 2004.
Information overload and the message dynamics
of online interaction spaces: A theoretical model
and empirical exploration. Info. Sys. Research,
15(2):194?210, June.
Shafiq Joty, Gabriel Murray, and Raymond T. Ng.
2011. Supervised topic segmentation of email con-
versations. In ICWSM11. AAAI.
Shafiq R. Joty, Giuseppe Carenini, and Raymond T.
Ng. 2013. Topic segmentation and labeling in asyn-
chronous conversations. J. Artif. Intell. Res. (JAIR),
47:521?573.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proc. Of the COLING Conference, pages
495?501.
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: can it be done by sen-
tence compression? In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 261?264, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Yashar Mehdad, Giuseppe Carenini, and Raymond
NG T. 2013a. Towards Topic Labeling with Phrase
Entailment and Aggregation. In Proceedings of
NAACL 2013, pages 179?189, Atlanta, USA, June.
Association for Computational Linguistics.
Yashar Mehdad, Giuseppe Carenini, Frank Tompa, and
Raymond T. NG. 2013b. Abstractive meeting sum-
marization with entailment and fusion. In Proceed-
ings of the 14th EuropeanWorkshop on Natural Lan-
guage Generation, pages 136?146, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Weiyi Meng and Clement T. Yu. 2010. Advanced
Metasearch Engine Technology. Synthesis Lectures
on Data Management. Morgan and Claypool Pub-
lishers.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, July.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010. Generating and validating abstracts of meet-
ing conversations: a user study. In Proceedings of
1229
the 6th International Natural Language Generation
Conference, INLG ?10, pages 105?113, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jahna Otterbacher, Gnes Erkan, and Dragomir R.
Radev. 2009. Biased lexrank: Passage retrieval us-
ing random walks with question-based priors. Inf.
Process. Manage., 45(1):42?54.
Tetsuya Sakai and Karen Sparck-Jones. 2001. Generic
summaries for indexing in information retrieval. In
Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?01, pages 190?198,
New York, NY, USA. ACM.
J. Ulrich, G. Murray, and G. Carenini. 2008. A
publicly available annotated corpus for supervised
email summarization. In AAAI08 EMAIL Workshop,
Chicago, USA. AAAI.
David C. Uthus and David W. Aha. 2011. Plans toward
automated chat summarization. In Proceedings of
the Workshop on Automatic Summarization for Dif-
ferent Genres, Media, and Languages, WASDGML
?11, pages 1?7, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David C. Uthus and David W. Aha. 2013. The ubuntu
chat corpus for multiparticipant chat analysis. In
AAAI Spring Symposium: Analyzing Microtext.
Lu Wang and Claire Cardie. 2013. Domain-
independent abstract generation for focused meet-
ing summarization. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1395?
1405, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1384?1394, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th Conference on Computational
Linguistics - Volume 2, COLING ?00, pages 947?
953. Association for Computational Linguistics.
Liang Zhou and Eduard Hovy. 2005. Digesting vir-
tual ?geek? culture: The summarization of technical
internet relay chats. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 298?305, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
1230
Extractive vs. NLG-based Abstractive Summarization of Evaluative 
Text: The Effect of Corpus Controversiality
Giuseppe Carenini and Jackie Chi Kit Cheung1
Department of Computer Science
University of British Columbia
Vancouver, B.C.  V6T 1Z4, Canada 
{carenini,cckitpw}@cs.ubc.ca
Abstract
Extractive  summarization  is  the  strategy  of 
concatenating  extracts  taken  from  a  corpus 
into a summary, while abstractive summariza-
tion  involves  paraphrasing  the  corpus  using 
novel sentences.  We define a novel  measure 
of  corpus  controversiality  of  opinions  con-
tained in evaluative text, and report the results 
of  a  user  study  comparing  extractive  and 
NLG-based abstractive summarization at dif-
ferent levels of controversiality. While the ab-
stractive summarizer performs better overall, 
the results suggest  that the margin by which 
abstraction  outperforms  extraction  is  greater 
when  controversiality  is  high,  providing  a 
context  in  which  the  need  for  generation-
based methods is especially great.
1 Introduction
There are two main approaches to the task of sum-
marization?extraction and abstraction (Hahn and 
Mani, 2000). Extraction involves concatenating ex-
tracts  taken  from  the  corpus  into  a  summary, 
whereas abstraction involves generating novel sen-
tences from information extracted from the corpus. 
It has been observed that in the context of multi-
document summarization of news articles, extrac-
tion may be inappropriate because it may produce 
summaries which are overly verbose or biased to-
wards some sources (Barzilay et al, 1999). How-
ever, there has been little work identifying specific 
factors which might affect the performance of each 
strategy  in  summarizing  evaluative  documents 
containing opinions and preferences, such as cus-
tomer reviews or blogs. This work aims to address 
this gap by exploring one dimension along which 
the effectiveness of the two paradigms could vary; 
namely,  the controversiality of  the opinions  con-
tained in the corpus.
In this paper, we make the following contribu-
tions.  Firstly,  we define a measure  of  controver-
siality of opinions in the corpus based on informa-
tion entropy. Secondly, we run a user study to test 
the  hypothesis  that  a  controversial  corpus  has 
greater  need  of  abstractive  methods  and  conse-
quently of NLG techniques. Intuitively, extracting 
sentences from multiple users whose opinions are 
diverse and wide-ranging may not reflect the over-
all  opinion,  whereas it  may be adequate content-
wise if opinions are roughly the same across users. 
As a secondary contribution, we propose a method 
for structuring text when summarizing controver-
sial corpora. This method is used in our study for 
generating abstractive summaries.
The results  of  the user  study support  our  hy-
pothesis  that  a  NLG summarizer  outperforms  an 
extractive summarizer by more  when the contro-
versiality is high.
2 Related Work
There  has  been  little  work  comparing  extractive 
and abstractive multi-document summarization. A 
previous  study  on  summarizing  evaluative  text 
(Carenini et. al, 2006) showed that extraction and 
abstraction performed about equally well,  though 
for different reasons. The study, however, did not 
1Authors are listed in alphabetical order.
33
look at the effect of the controversiality of the cor-
pus on the relative performance of the two strate-
gies.
To the best of our knowledge, the task of mea-
suring the controversiality of opinions in a corpus 
has  not  been  studied  before.  Some  well  known 
measures  are  related to  this  task,  including vari-
ance, information entropy,  and measures of inter-
rater reliability.  (e.g. Fleiss' Kappa (Fleiss, 1971), 
Krippendorff's Alpha (Krippendorff, 1980)). How-
ever, these existing measures do not satisfy certain 
properties that a sound measure of controversiality 
should possess, prompting us to develop our own 
based on information entropy.
Summary evaluation is a challenging open re-
search  area.  Existing  methods  include  soliciting 
human judgements, task-based approaches, and au-
tomatic approaches.
Task-based evaluation measures  the  effective-
ness of a summarizer for its intended purpose. (e.g. 
(McKeown et al, 2005)) This approach, however, 
is less applicable in this work because we are inter-
ested in evaluating specific properties of the sum-
mary such as the grammaticality and the content, 
which may be difficult to evaluate with an overall 
task-based  approach.  Furthermore,  the  design  of 
the task may intrinsically favour abstractive or ex-
tractive  summarization.  As  an  extreme  example, 
asking for a list of specific comments from users 
would clearly favour extractive summarization.
Another method for summary evaluation is the 
Pyramid method (Nenkova and Passonneau, 2004), 
which takes into account the fact that human sum-
maries with different content can be equally infor-
mative. Multiple human summaries are taken to be 
models, and chunks of meaning known as Summa-
ry Content  Units  (SCU)  are  manually  identified. 
Peer summaries are evaluated based on how many 
SCUs they share with the model  summaries,  and 
the  number  of  model  summaries  in  which  these 
SCUs are found.  Although this  method has been 
tested in DUC 2006 and DUC 2005 (Passonneau et 
al., 2006), (Passonneau et al, 2005) in the domain 
of news articles, it has not been tested for evalua-
tive text. A pilot study that we conducted on a set 
of customer reviews on a product using the Pyra-
mid method revealed several problems specific to 
the  evaluative  domain.  For  example,  summaries 
which  misrepresented  the  polarity of  the  evalua-
tions for a certain feature were not penalized, and 
human summaries sometimes produced contradic-
tory statements about the distribution of the opin-
ions. In one case, one model summary claimed that 
a feature is positively rated, while another claimed 
the opposite, whereas the machine summary indi-
cated that this feature drew mixed reviews. Clear-
ly, only one of these positions should be regarded 
as correct. Further work is needed to resolve these 
problems.
There are also automatic methods for summary 
evaluation,  such  as  ROUGE  (Lin,  2004),  which 
gives  a  score  based  on  the  similarity  in  the  se-
quences of words between a human-written model 
summary  and  the  machine  summary.  While 
ROUGE scores have been shown to often correlate 
quite well with human judgements (Nenkova et al, 
2007), they do not provide insights into the specif-
ic strengths and weaknesses of the summary.
The method of summarization evaluation used 
in this work is to ask users to complete a question-
naire about summaries that they are presented with. 
The questionnaire consists of questions asking for 
Likert  ratings  and  is  adapted  from the  question-
naire in (Carenini et al, 2006).
3 Representative Systems
In our user study, we compare an abstractive and 
an extractive multi-document summarizer that are 
both developed specifically for the evaluative do-
main. These summarizers have been found to pro-
duce quantitatively similar results, and both signif-
icantly outperform a baseline summarizer, which is 
the MEAD summarization framework with all op-
tions set to the default (Radev et al, 2000).
Both summarizers  rely on information  extrac-
tion from the corpus. First, sentences with opinions 
need to be identified, along with the features of the 
entity that are evaluated, the strength, and polarity 
(positive  or  negative)  of  the  evaluation.  For  in-
stance, in a corpus of customer reviews, the sen-
tence ?Excellent picture quality - on par with my 
Pioneer, Panasonic, and JVC players.? contains an 
opinion on the feature  picture quality of  a DVD 
player, and is a very positive evaluation (+3 on a 
scale from -3 to +3). We rely on methods from pre-
vious  work  for  these  tasks  (Hu  and  Liu,  2004). 
Once these features, called Crude Features (CFs), 
are extracted, they are mapped onto a taxonomy of 
User Defined Features (UDFs), so named because 
they can be defined by the user. This mapping pro-
vides a better conceptual organization of the CFs 
34
by  grouping  together  semantically  similar  CFs, 
such as jpeg picture and jpeg slide show under the 
UDF JPEG. For the purposes of our study, feature 
extraction,  polarity/strength identification and the 
mapping from CFs to UDFs are not done automati-
cally as in (Hu and Liu, 2004) and (Carenini et al 
2005). Instead, ?gold standard? annotations by hu-
mans are used in order to focus on the effect of the 
summarization strategy.
3.1 Abstractive Summarizer: SEA
The abstractive summarizer is the Summarizer of 
Evaluative Arguments (SEA), adapted from GEA, 
a system for generating evaluative text tailored to 
the user's preferences (Carenini and Moore, 2006).
In  SEA,  units  of  content  are  organized  by 
UDFs.  The importance of each UDF is based on 
the  number  and  strength  of  evaluations  of  CFs 
mapped to this UDF, as well as the importance of 
its children UDFs. Content selection consists of re-
peating the following two steps  until  the desired 
number of UDFs have been selected: (i)  greedily 
selecting the most important UDF (ii) recalculating 
the measure of importance scores for the remaining 
UDFs.
The content structuring, microplanning, and re-
alization  stages  of  SEA are  adapted  from GEA. 
Each selected UDF is realized in the final summary 
by one clause, generated from a template pattern 
based  on  the  number  and  distribution  of 
polarity/strength evaluations of the UDF. For ex-
ample, the UDF video output with an average po-
larity/strength of near -3 might be realized as ?sev-
eral customers found the video output to be terri-
ble.?
While experimenting with the SEA summariz-
er,  we  noticed  that  the  document  structuring  of 
SEA summaries, which is adapted from GEA and 
is based on guidelines from argumentation theory 
(Carenini  and Moore,  2000),  sometimes  sounded 
unnatural.  We  found  that  controversially  rated 
UDF features (roughly balanced positive and nega-
tive evaluations) were treated as contrasts to those 
which were uncontroversially rated (either mostly 
positive, or mostly negative evaluations). In SEA, 
contrast relations between features are realized by 
cue phrases signalling contrast such as ?however? 
and ?although?. These cue phrases appear to signal 
a contrast that is too strong for the relation between 
controversial and uncontroversial features. An ex-
ample of a SEA summary suffering from this prob-
lem can be found in Figure 1.
To solve this problem, we devised an alterna-
tive content structure for controversial corpora, in 
which  all  controversial  features  appear  first,  fol-
lowed by all  positively and negatively evaluated 
features.
3.2 Extractive Summarizer: MEAD*
The  extractive  approach  is  represented  by 
MEAD*, which is adapted from the open source 
summarization  framework  MEAD (Radev  et  al., 
2000).
After  information  extraction,  MEAD*  orders 
CFs  by the  number  of  sentences  evaluating  that 
CF, and selects a sentence from each CF until the 
word limit has been reached. The sentence that is 
selected for each CF is  the one with the highest 
sum of  polarity/strength evaluations  for  any fea-
ture, so sentences that mention more CFs tend to 
be  selected.  The  selected  sentences  are  then  or-
dered according to the UDF hierarchy by a depth-
first traversal through the UDF tree so that more 
abstract  features  tend  to  precede  more  specific 
ones.
MEAD* does not have a special mechanism to 
deal with controversial features. It is not clear how 
overall controversiality of a feature can be effec-
tively expressed with extraction, as each sentence 
conveys a specific and unique opinion. One could 
include two sentences of opposite polarity for each 
controversial  feature.  However,  in  several  cases 
that we considered, this produced extremely inco-
herent text that did not seem to convey the gist of 
the overall controversiality of the feature.
Customers had mixed opinions about the Apex AD2600. 
Although several customers found the video output to be 
poor and some customers disliked the user interface, cus-
tomers had mixed opinions about the range of compatible 
disc formats. However, users did agree on some things. 
Some users found the extra features to be very good even 
though customers had mixed opinions about the supplied 
universal remote control.
Figure 1: SEA summary of a controversial corpus with 
a document structuring problem. Controversial and un-
controversial features are interwoven. See Figure 3 for 
an example of a summary structured with our alterna-
tive strategy.
35
3.3 Links to the Corpus
In common with the previous study on which this 
is  based,  both  the  SEA and MEAD* summaries 
contain ?clickable footnotes? which are links back 
into an original user review, with a relevant sen-
tence highlighted. These footnotes serve to provide 
details  for  the  abstractive  SEA summarizer,  and 
context for the sentences chosen by the extractive 
MEAD*  summarizer.  They  also  aid  the  partici-
pants of the user study in checking the contents of 
the summary.  The sample sentences for SEA are 
selected by a method similar to the MEAD* sen-
tence selection algorithm. One of the questions in 
the questionnaire provided to users targets the ef-
fectiveness of the footnotes as an aid to the sum-
mary.
4 Measuring Controversiality
The opinion sentences in the corpus are annotated 
with  the  CF  that  they  evaluate  as  well  as  the 
strength, from 1 to 3, and polarity, positive or neg-
ative, of the evaluation. It is natural then, to base a 
measure  of  controversiality on these  annotations. 
To  measure  the  controversiality  of  a  corpus,  we 
first  measure  the  controversiality  of  each  of  the 
features in the corpus. We list two properties that a 
measure of feature controversiality should satisfy.
Strength-sensitivity:  The  measure  should  be 
sensitive to the strength of the evaluations. e.g. Po-
larity/strength  (P/S)  evaluations  of  -2  and  +2 
should be less controversial than -3 and +3
Polarity-sensitivity: The measure should be sen-
sitive the polarity of the evaluations. e.g. P/S eval-
uations of -1 and +1 should be more controversial 
than +1 and +3.
The rationale for this property is that positive 
and negative evaluations are fundamentally differ-
ent, and this distinction is more important than the 
difference in intensity.  Thus,  though a numerical 
scale would suggest that -1 and +1 are as distant as 
+1  and  +3,  a  suitable  controversiality  measure 
should not treat them so.
In addition, the overall measure of corpus con-
troversiality should also satisfy the following two 
features.
CF-weighting: CFs should be weighted by the 
number of evaluations they contain when calculat-
ing the overall value of controversiality for the cor-
pus.
CF-independence: The controversiality of indi-
vidual CFs should not affect each other.
An alternative is to calculate controversiality by 
UDFs  instead  of  CFs.  However,  not  all  CFs 
mapped to the same UDF represent the same con-
cept. For example, the CFs picture clarity and col-
or signal are both mapped to the UDF video out-
put.
4.1 Existing Measures of Variability
Since the problem of measuring the variability of a 
distribution has been well studied, we first exam-
ined existing metrics including variance, entropy, 
kappa, weighted kappa, Krippendorff?s alpha, and 
information  entropy.  Each  of  these,  however,  is 
problematic in their canonical form, leading us to 
devise a new metric based on information entropy 
which satisfies the above properties. Existing met-
rics will now be examined in turn.
Variance:  Variance  does  not  satisfy  polarity-
sensitivity, as the statistic only takes into account 
the difference of each data point to the mean, and 
the sign of the data point plays no role.
Information Entropy: The canonical form of in-
formation entropy does not satisfy strength or po-
larity  sensitivity,  because  the  measure  considers 
the discrete values of the distribution to be an un-
ordered set.
Measures of Inter-rater Reliability: Many mea-
sures exist  to assess inter-rater agreement or dis-
agreement,  which  is  the  task  of  measuring  how 
similarly two or more judges rate one or more sub-
jects beyond chance (dis)agreement.  Various ver-
sions  of  Kappa  and  Krippendorff's  Alpha  (Krip-
pendorff, 1980), which have shown to be equiva-
lent in their most generalized forms (Passonneau, 
1997), can be modified to satisfy all the properties 
listed above. However, there are important differ-
ences between the tasks of  measuring controver-
siality and measuring inter-rater reliability. Kappa 
and Krippendorff's Alpha correct for chance agree-
ment  between raters,  which is  appropriate  in  the 
context  of  inter-rater  reliability  calculations,  be-
cause judges are asked to give their  opinions on 
items that are given to them. In contrast,  expres-
sions  of  opinion  are  volunteered  by  users,  and 
users  self-select  the  features  they  comment  on. 
Thus,  it  is  reasonable  to  assume  that  they never 
randomly  select  an  evaluation  for  a  feature,  and 
chance agreement does not exist.
36
4.2 Entropy-based Controversiality
We define here  our novel  measure  of  controver-
siality, which is based on information entropy be-
cause  it  can  be  more  easily  adapted  to  measure 
controversiality. As has been stated, entropy in its 
original form over the evaluations of a CF is not 
sensitive to strength or polarity. To correct this, we 
first  aggregate  the  positive  and  negative  evalua-
tions for each CF separately, and then calculate the 
entropy based on the resultant Bernoulli distribu-
tion.
Let ps(cfj) be the set of polarity/strength evalua-
tions  for  cfj.  Let  the  importance  of  a  feature, 
imp(cfj), be the sum of the absolute values of the 
polarity/strength evaluations for cfj.
imp?cf j ?= ?
ps k? ps? cf j?
?psk?
Define:
imp_ pos ?cf j?= ?
psk ? ps? cf j?? psk?0
?psk?
imp_ neg ?cf j ?= ?
psk? ps ?cf j??psk?0
?psk?
Now, calculate the entropy of the Bernoulli dis-
tribution  corresponding  to  the  importance  of  the 
two polarities  to  satisfy polarity-sensitivity.  That 
is, Bernoulli with parameter 
? j=imp_ pos ?cf j? /imp?cf j ?
H ?? j?=?? j log2? j??1?? j ? log2 ?1?? j?
Next, we scale this score by the importance of 
the evaluations divided by the maximum possible 
importance for this number of evaluations to satis-
fy strength-sensitivity. Since our scale is from -3 to 
+3, the maximum possible importance for a feature 
is three times the number of evaluations.
max_imp?cf j ?=3??ps ?cf j ??
Then the controversiality of a feature is:
contro ?cf j ?= imp?cf j??H ?? j?max_imp?cf j ?
The case corresponding to the highest possible 
feature  controversiality,  then,  would  be  the  bi-
modal case with equal numbers of evaluations on 
the extreme positive and negative bins (Figure 2). 
Note, however, that controversiality is not simply 
bimodality. A unimodal normal-like distribution of 
evaluations  centred on zero,  for  example,  should 
intuitively  be  somewhat  controversial,  because 
there are equal numbers  of  positive and negative 
evaluations. Our entropy-based feature controver-
siality measure is able to take this into account.
To calculate the controversiality of the corpus, 
a weighted average is taken over the CF controver-
siality scores, with the weight being equal to one 
less  than the  number  of  evaluations  for  that  CF. 
We subtract one to eliminate any CF where only 
one evaluation is made, as that CF has an entropy 
score of one by default  before scaling by impor-
tance.  This  procedure  satisfies  properties  CF-
weighting and CF-independence.
w ?cf j?=?ps ?cf j ???1
contro ?corpus?=? w?cf j ??contro ?cf j?? w?cf j?
Although the annotations in this  corpus range 
from -3 to +3, it would be easy to rescale opinion 
annotations of different corpora to apply this met-
ric. Note that empirically,  this measure correlates 
highly with Kappa and Krippendorff's Alpha.
5 User Study
Our main hypothesis that extractive summarization 
is outperformed even more in the case of contro-
versial corpora was tested by a user study, which 
compared the results of MEAD* and the modified 
SEA. First, ten subsets of 30 user reviews were se-
lected from the corpus of 101 reviews of the Apex 
AD2600  DVD  player  from  amazon.com  by 
stochastic  local  search.  Five of  these  subsets  are 
controversial, with controversiality scores between 
0.83 and 0.88, and five of these are uncontrover-
sial, with controversiality scores of 0. A set of thir-
Figure 2: Sample feature controversiality scores for 
three different distributions of polarity/strength evalua-
tions.
37
ty user reviews per subcorpus was needed to create 
a summary of sufficient length, which in our case 
was about 80 words in length.
Twenty university students were recruited and 
presented with two summaries of the same subcor-
pus,  one  generated  from  SEA  and  one  from 
MEAD*. We generated ten subcorpora in total, so 
each subcorpus was assigned to two participants. 
One of these participants was shown the SEA sum-
mary first, and the other was shown the MEAD* 
summary first, to eliminate the order of presenta-
tion as a source of variation.
The participants were asked to take on the role 
of an employee of Apex, and told that they would 
have to write a summary for the quality assurance 
department  of  the  company about  the  product  in 
question. The purpose of this was to prime them to 
look for information that should be included in a 
summary  of  this  corpus.  They were  given  thirty 
minutes to read the reviews, and take notes.
They were then presented with a questionnaire 
on the summaries,  consisting of ten Likert rating 
questions. Five of these questions targeted the lin-
guistic quality of the summary, based on linguistic 
well-formedness questions used at DUC 2005, one 
targeted the ?clickable footnotes? linking to sample 
sentences  in  the  summary  (see  section  3.3),  and 
three evaluated the contents of the summary.  The 
three questions targeted Recall,  Precision, and the 
general Accuracy of the summary contents respec-
tively. The tenth question asked for a general over-
all quality judgement of the summary.
After  familiarizing  themselves  with  the  ques-
tionnaire, the participants were presented with the 
two summaries in sequence, and asked to fill out 
the questionnaire while reading the summary. They 
were  allowed to  return to  the  original  set  of  re-
views during this time. Lastly, they were given an 
additional questionnaire which asked them to com-
pare  the  two  summaries  that  they  were  shown. 
Questions  in  the  questionnaire  not  found  in 
(Carenini et al, 2006) are attached in Appendix A.
6 Results
6.1 Quantitative Results
We convert the Likert responses from a scale from 
Strongly Disagree to Strong Agree to a scale from 
1 to 5, with 1 corresponding to Strongly Disagree, 
and 5 to Strongly Agree. We group the ten ques-
tions into four categories: linguistic (questions 1 to 
5),  content  (questions 6 to 8),  footnote (question 
9),  and  overall  (question  10).  See  Table  1 for  a 
breakdown of the  responses for  each question at 
each controversiality level.
For our analysis, we adopt a two-step approach 
that has been applied in Computational Linguistics 
(Di Eugenio et al, 2002) as well as in HCI (Hinck-
ley et al, 1997).
First, we perform a two-way Analysis of Vari-
ance (ANOVA) test using the average response of 
the questions in each category. The two factors are 
controversiality of the corpus (high or low) as in-
dependent  samples,  and the summarizer  (SEA or 
MEAD*)  as  repeated  measures.  We  repeat  this 
procedure  for  the  average  of  the  ten  questions, 
termed  Macro below. The p-values of these tests 
are summarized in Table 2.
The results  of  the ANOVA tests  indicate that 
SEA significantly outperforms MEAD* in terms of 
linguistic and overall quality, as well as for all the 
questions combined. It does not significantly out-
perform MEAD* by content, or in the amount that 
the  included  sample  sentences  linked  to  by  the 
footnotes aid the summary.  No significant differ-
ences are found in the performance of the summa-
SEA
Customers had mixed opinions about the Apex AD2600 1,2 
possibly because users were divided on the range of compatible 
disc formats 3,4 and there was disagreement among the users 
about the video output 5,6. However, users did agree on some 
things. Some purchasers found the extra features 7 to be very 
good and some customers really liked the surround sound sup-
port 8 and thought the user interface 9 was poor.
MEAD*
When we tried to hook up the first one , it was broken - the 
motor would not eject discs or close the door . 1 The build 
quality feels solid , it does n't shake or whine while playing 
discs , and the picture and sound is top notch ( both dts and 
dd5.1 sound good ) . 2 The progressive scan option can be 
turned off easily by a button on the remote control which is 
one of the simplest and easiest remote controls i have ever 
seen or used . 3 It plays original dvds and cds and plays 
mp3s and jpegs . 4 
Figure 3: Sample SEA and MEAD* summaries for a controversial corpus. The numbers within the summaries are 
footnotes linking the summary to an original user review from the corpus.
38
rizers  over  the  two levels  of  controversiality  for 
any of the question sets .
While  the  average  differences  in  scores  be-
tween  the  SEA  and  MEAD*  summarizers  are 
greater in the controversial case for the linguistic, 
content, and macro averages as well as the ques-
tion on the overall quality, the p-values for interac-
tion between the two factors in the two-way ANO-
VA test are not significant.
For the second step of the analysis,  we use a 
one-tailed  sign  test  (Siegel  and  Castellan,  1988) 
over the difference in performance of the summa-
rizers at the two levels of controversiality for the 
questions in the questionnaire. We encode + in the 
case  where  the  difference  between  SEA  and 
MEAD* is greater for a question in the controver-
sial setting, ? if the difference is smaller, and we 
discard a question if the difference is the same (e.g. 
the Footnote question). Since the Overall question 
is likely correlated with the responses of the other 
questions, we did not include it in the test. After 
discarding the Footnote question, the p-value over 
the  remaining  eight  questions  is  0.0352,  which 
lends support to our hypothesis that the abstraction 
is better by more when the corpus is controversial.
We  also  analyze  the  users'  summary  prefer-
ences at the two levels of controversiality. A strong 
preference  for  SEA  is  encoded  as  a  5,  while  a 
strong preference for MEAD* is encoded as a 1, 
with 3 being neutral. Using a two-tailed unpaired 
two-sample t-test, we do not find a significant dif-
ference  in  the  participants'  summary  preferences 
(p=0.6237). However, participants sometimes pre-
ferred summaries for reasons other than linguistic 
or  content  quality,  or  may  base  their  judgement 
only on one aspect of the summary. For instance, 
one  participant  rated  SEA  at  least  as  well  as 
MEAD* in all questions except Footnote, yet pre-
ferred MEAD* to SEA overall  precisely because 
MEAD* was felt  to have made better use of the 
footnotes than SEA.
6.2 Qualitative Results
The  qualitative  comments  that  participants  were 
asked to provide along with the Likert scores con-
firmed the observations that led us to formulate the 
initial hypothesis.
In  the  controversial  subcorpora,  participants 
generally  agreed  that  the  abstractive  nature  of 
SEA's generated text was an advantage. For exam-
ple, one participant lauded SEA for attempting to 
?synthesize the reviews? and said that it  ?did re-
flect the mixed nature of the reviews, and covered 
some common complaints.? The participant, how-
ever, said that SEA ?was somewhat misleading in 
that it understated the extent to which reviews were 
negative. In particular, agreement was reported on 
Question 
Set
Controver-
siality
Summarizer Controversiality 
x Summarizer
Linguistic 0.7226 <0.0001 0.2639
Content 0.9215 0.1906 0.2277
Footnote 0.2457 0.7805 1
Overall 0.6301 0.0115 0.2000
Macro 0.7127 0.0003 0.1655
Table 2: Two-way ANOVA p-values.
Table 1: Breakdown of average Likert question responses for each summary at the two levels of controversiali-
ty as well as the difference between SEA and MEAD*.
Controversial Uncontroversial
SEA MEAD* (SEA ? MEAD*)SEA MEAD* (SEA ? MEAD*)
Question Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev.
Grammaticality 4.5 0.53 3.4 1.26 1.1 0.99 4.2 0.92 2.78 1.3 1.56 1.51
Non-redundancy 4.2 0.92 4 1.07 0.25 1.58 3.7 0.95 3.8 1.14 -0.1 1.45
Referential clarity 4.5 0.53 3.44 1.33 1 1.22 4.2 1.03 3.5 1.18 0.7 1.34
Focus 4.11 1.27 2.1 0.88 2.22 0.83 3.9 1.1 2.6 1.35 1.3 1.57
Structure and Coherence 4.1 0.99 1.9 0.99 2.2 1.14 3.8 1.4 2.3 1.06 1.5 1.9
Linguistic 4.29 0.87 2.91 1.35 1.39 1.34 3.96 1.07 3 1.29 0.98 1.63
Recall 2.8 1.32 1.8 1.23 1 1.33 2.5 1.27 2.5 1.43 0 1.89
Precision 3.9 1.1 2.7 1.64 1.2 1.23 3.5 1.27 3.3 0.95 0.2 1.93
Accuracy 3.4 0.97 3.3 1.57 0.1 1.2 3.1 1.52 3.2 1.03 -0.1 2.28
Content 3.37 1.19 2.6 1.57 0.77 1.3 3.03 1.38 3 1.17 0.03 1.97
Footnote 4 1.05 3.9 0.88 0.1 1.66 3.6 1.07 3.5 1.35 0.1 1.6
Overall 3.8 0.79 2.4 1.17 1.4 1.07 3.2 1.23 2.7 0.82 0.5 1.84
Macro ? Footnote 3.92 1.06 2.75 1.41 1.17 1.32 3.57 1.26 2.97 1.2 0.61 1.81
Macro 3.93 1.05 2.87 1.4 1.06 1.39 3.57 1.24 3.02 1.22 0.56 1.79
39
some  features  where  none existed,  and problems 
with reliability were not mentioned.?
Participants disagreed on the information cover-
age of the MEAD* summary. One participant said 
that MEAD* includes ?almost all the information 
about the Apex 2600 DVD player?, while another 
said that it ?does not reflect all information from 
the customer reviews.?
In the  uncontroversial  subcorpora,  more  users 
criticized SEA for its inaccuracy in content selec-
tion. One participant felt that SEA ?made general-
izations that were not precise or accurate.? Partici-
pants  had  specific  comments  about  the  features 
that SEA mentioned that they did not consider im-
portant.  For  example,  one  comment  was  that 
?Compatibility with CDs was not a general prob-
lem,  nor were issues with the remote  control,  or 
video output (when it worked).? MEAD* was criti-
cized for being ?overly specific?, but users praised 
MEAD* for being ?not at all redundant?, and said 
that it ?included information I felt was important.?
7 Conclusion and Future Work
We have explored the controversiality of opinions 
in a corpus of evaluative text as an aspect which 
may determine how well abstractive and extractive 
summarization  strategies  perform.  We  have  pre-
sented a novel measure of controversiality, and re-
ported on the results of a user study which suggest 
that abstraction by NLG outperforms extraction by 
a larger amount in more controversial corpora. We 
have also presented a document structuring strate-
gy for summarization of controversial corpora.
Our  work  has  implications  in  practical  deci-
sions on summarization strategy choice; an extrac-
tive approach, which may be easier to implement 
because of its lack of requirement for natural lan-
guage generation, may suffice if the controversiali-
ty of opinions in a corpus is sufficiently low.
A future approach to summarization of evalua-
tive text might combine extraction and abstraction 
in  order  to  combine  the  different  strengths  that 
each bring to the summary. The controversiality of 
the corpus might be one factor determining the mix 
of abstraction and extraction in the summary. The 
footnotes linking to sample sentences in the corpus 
in SEA are already one form of this combined ap-
proach.  Further  work  is  needed  to  integrate  this 
text into the summary itself, possibly in a modified 
form.
As a final note to our user study, further studies 
should be done with different corpora and summa-
rization systems to increase the external validity of 
our results.
Acknowledgements
We would like to thank Raymond T. Ng, Gabriel 
Murray  and  Lucas  Rizoli  for  their  helpful  com-
ments. This work was partially funded by the Nat-
ural Sciences and Engineering Research Council of 
Canada.
References
Regina Barzilay,  Kathleen R. McKeown, and Michael 
Elhadad. 1999. Information fusion in the context of 
multi-document summarization. In  Proc. 37th ACL, 
pages 550?557.
Giuseppe Carenini and Johanna D. Moore. 2006. Gener-
ating and evaluating evaluative arguments.  Artificial  
Intelligence, 170(11):925-952.
Giuseppe  Carenini,  Raymond  Ng  and  Adam  Pauls. 
2006.  Multi-document  summarization  of  evaluative 
text. In Proc. 11th EACL 2006, pages 305-312.
Giuseppe  Carenini,  Raymond  T.  Ng  and  Ed  Zwart. 
2005.  Extracting Knowledge from Evaluative Text. 
In Proc. 3rd International Conference on Knowledge 
Capture, pages 11-18.
Giuseppe  Carenini  and  Johanna  D.  Moore.  2000.  A 
strategy for generating evaluative arguments. In First  
INLG, pages 47-54, Mitzpe Ramon, Israel.
Barbara Di Eugenio, Michael Glass, and Michael J. Tro-
lio. 2002. The DIAG experiments: Natural language 
generation  for  intelligent  tutoring  systems.  In  INL-
G02, The 2nd INLG, pages 120-127.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment  among  many  raters.  Psychological  Bulletin. 
76:378-382.
U. Hahn and I. Mani. 2000. The challenges of automatic 
summarization. IEEE Computer, 33(11):29-36.
Ken  Hinckley,  Randy  Pausch,  Dennis  Proffitt,  James 
Patten, and Neal Kassell. 1997. Cooperative bimanu-
al action. In Proc. CHI Conference.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In  Proc. 10th ACM SIGKDD 
conference, pages 168-177.
Klaus Krippendorff. 1980.  Content Analysis: An Intro-
duction to Its Methodology. Sage Publications, Bev-
erly Hills, CA.
Chin-Yew Lin. 2004. ROUGE: A Package for Automat-
ic Evaluation of Summaries.  In  Proc. of  Workshop 
on Text Summarization Branches Out, Post-Confer-
ence Workshop of ACL 2004, Barcelona, Spain.
40
2005. Linguistic quality questions from the 2005 docu-
ment  understanding  conference. 
http://duc.nist.gov/duc2005/quality-questions.txt
Kathleen McKeown, Rebecca Passonneau, David Elson, 
Ani Nenkova, and Julia Hirschberg. 2005. Do sum-
maries help? A task-based evaluation of multi-docu-
ment summarization. In Proc. SIGIR 2005.
Ani  Nenkova,  Rebecca  J.  Passonneau,  and K.  McKe-
own. 2007. The pyramid method: incorporating hu-
man  content  selection  variation  in  summarization 
evaluation.  ACM Transactions on Speech and Lan-
guage Processing, 4(2).
Ani Nenkova and Rebecca J. Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proc. NAACL/HLT.
Rebecca  J.  Passonneau,  Kathleen  McKeown,  Sergey 
Sigleman, and Adam Goodkind. 2006. Applying the 
pyramid method in the 2006 Document Understand-
ing Conference. In Proc. DUC'06.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigleman. 2005. Applying the pyra-
mid method in DUC 2005. In Proc. DUC'05.
Rebecca J. Passonneau. 1997. Applying Reliability Met-
rics  to  Co-Reference  Annotation.  Department  of 
Computer  Science,  Columbia  University,  TR 
CUCS-017-97.
Dragomir  Radev,  Hongyan  Jing,  and  Malgorzata 
Budzikowska.  2000.  Centroid-based  summarization 
of  multiple  documents:  sentence  extraction,  utility-
based  evaluation,  and  user  studies.  In  Proc.  
ANLP/NAACL Workshop on Automatic Summariza-
tion.
S. Siegel and N. J. Castellan, Jr. 1988. Nonparametric 
statistics for the behaviorial sciences. McGraw Hill.
Appendix A. Additional Questions
Footnotes: a) Did you use the footnotes when re-
viewing the summary?
b) Answer this question only if you answered 
?Yes? to the previous question. The clickable foot-
notes were a helpful addition to the summary.
Summary Comparison Questions:
1) List any Pros and Cons you can think of for 
each of the summaries. Point form is okay.
2) Overall, which summary did you prefer?
3) Why did you  prefer  this  summary?  (If  the 
reason overlaps with some points from question 1, 
put a star next to those points in the chart.)
4) Do you have any other comments about the 
reviews or summaries, the tasks, or the experiment 
in general? If so, please write them below.
41
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 16?22,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Domain Adaptation to Summarize Human Conversations 
  Oana Sandu, Giuseppe Carenini, Gabriel Murray, and Raymond Ng University of British Columbia Vancouver, Canada {oanas,carenini,gabrielm,rng}@cs.ubc.ca     Abstract  We are interested in improving the sum-marization of conversations by using domain adaptation. Since very few email corpora have been annotated for summa-rization purposes, we attempt to leverage the labeled data available in the multi-party meetings domain for the summari-zation of email threads. In this paper, we compare several approaches to super-vised domain adaptation using out-of-domain labeled data, and also try to use unlabeled data in the target domain through semi-supervised domain adapta-tion. From the results of our experiments, we conclude that with some in-domain labeled data, training in-domain with no adaptation is most effective, but that when there is no labeled in-domain data, domain adaptation algorithms such as structural correspondence learning can improve summarization.  1 Introduction On a given day, many people engage in conver-sations via several modalities, including face-to-face speech, telephone, email, SMS, chat, and blogs. Being able to produce automatic summa-ries of multi-party conversations occurring in one or several of these modalities would enable the parties involved to keep track of and make sense of this diverse data. However, summarizing spo-ken dialogue is more challenging than summariz-ing written monologues such as books and arti-cles, as speech tends to be more fragmented and disfluent. We are interested in using both fully and semi-supervised techniques to produce extractive summaries for conversations, where each sen-
tence of a text is labeled with its informativeness, and a subset of sentences are concatenated into an extractive summary of the text. In previous work (Murray and Carenini, 2008), it has been shown that conversations in different modalities can be effectively characterized by a set of ?con-versational? features that are useful in detecting informativeness for the task of extractive sum-marization. However, because of privacy con-cerns, annotated corpora are rarely publicly available for conversational data, including for the email domain. One promising solution to this problem is domain adaptation, which aims to use labeled data in a well-studied source domain and a limited amount of labeled data from a different target domain to train a model that performs well in that target domain. In this work, we investi-gate using domain adaptation that leverages la-beled data in the domain of meetings along with labeled and unlabeled email data for summariz-ing email threads. We evaluate several domain adaptation algorithms, using both a small set of conversational features and a large set of simple lexical features to determine what settings will yield the best results for summarizing email con-versations. In our experiments, we do not get a significant improvement from using out-of-domain data in addition to in-domain data in su-pervised domain adaptation, though in the setting where only unlabeled in-domain data is avail-able, we gain from using it through structural correspondence learning. We also observe that conversational features are more useful in super-vised methods, whereas lexical features are bet-ter leveraged in semi-supervised adaptation.  The next section surveys past research in do-main adaptation and in summarizing conversa-tional data. In section 3 we present the corpora and feature sets we used, and we describe our experimental setting in section 4. We then com-pare the performance of different methods in sec-tion 5 and draw conclusions in section 6.  
16
2 Related Work We give an overview first of work on supervised and semi-supervised domain adaptation, then of research on summarization of conversations. 2.1 Supervised Domain Adaptation Many domain adaptation methods have been proposed for the supervised case, where a small amount of labeled data in the target domain is used along with a larger amount of labeled source data. Two baseline approaches are to train only on the source data or only on target training data. One way of using information from both domains is merging the source and target labeled data sets and training a model on the combina-tion. A method inspired by boosting is to take a linear combination of the predictions of two clas-sifiers, one trained on the source and one trained on the target training data. Another simple me-thod is to train a predictor on the source data, run it on the target data, and then use its predictions on each instance as additional features for a target-trained model. This was first introduced by Florian et al (2004), who applied it to multilingual named entity recognition. The prior method of domain adaptation by Chelba and Acero (2006) involves using the source data to find optimal parameter values of a maximum entropy model on that data, and then setting these as a prior on the values of a model trained on the target data. They find improve-ment in a capitalizer that adapts using out-of-domain and a small amount of in-domain data versus only training on out-of-domain WSJ data.  Similar to the prior method, Daume?s MEGA model also trains a MEMM. It achieves domain adaptation through hyperparameters that indicate whether an instance is generated by a source, target, or general distribution, and finds the op-timal values of the parameters through condi-tional EM (Daume and Marcu, 2006). A simpler method of domain adaptation, that achieves a performance similar to prior and MEGA, was proposed by Daume (2007) and successfully ap-plied to a variety of NPL sequence labeling prob-lems, such as named entity recognition, shallow parsing, and part-of-speech (POS) tagging. Fur-thermore, this approach is straightforward to ap-ply by copying feature values so there is a source version, a target version, and a general version of the feature, and was found to be faster to train than MEGA and prior. For all these reasons, we use Daume?s method and not the other two in our experiments. 
2.2 Semi-supervised Domain Adaptation Because unlabeled data is usually much easier to collect than labeled data in a new domain, semi-supervised domain adaptation methods that ex-ploit unlabeled data are potentially very useful.  In self-training, a training set is used that is originally composed of labeled data, and repeat-edly augmented with the highest confidence pre-dictions on unlabeled data. McClosky et al (2006) apply this in a domain adaptation setting for parsing: with only unlabeled data in the target Brown domain, and labeled and unlabeled datasets in the news domain (WSJ and NANC respectively), a self-trained reranking parser per-forms almost as well as a parser trained only on Brown labeled data. However, McClosky con-cludes that self-training alone is not beneficial, and most of the improvements they get over pre-vious work on domain adaptation for parsing are due to using the reranker to select the candidate instances produced in each iteration of self-training. Thus, one of the issues addressed in this paper is to asses whether self-training is useful for domain adaptation. A more sophisticated semi-supervised domain adaptation method is structural correspondence learning (SCL). SCL uses unlabeled data to de-termine correspondences between features in the two domains by correlating them with so-called pivot features, which are features exhibiting similar behaviors in the source and target do-mains. Blitzer applied this algorithm successfully to POS tagging (Blitzer et al, 2006) and senti-ment classification (Blitzer et al, 2007). SCL seems promising for other tasks as well, for ex-ample parse disambiguation (Plank, 2009). 2.3 Summarization We would like to use domain adaptation to aid in summarizing multi-party conversations hailing from different modalities. This contrasts with much of previous work on summarization of conversations, which has focused on domain-specific features (e.g., Rambow et al 2004). We will treat summarization as a supervised binary classification problem where the sentences of a conversation are rated by their informativeness and a subset is selected to form an extractive summary. Research in meeting summarization relevant to our task has investigated the utility of employing a large feature set including prosodic information, speaker status, lexical and structural discourse features (Murray et al, 2006; Galley, 2006). For email summarization, we view an 
17
email thread as a conversation. For summarizing email threads, Rambow (2004) used lexical fea-tures such as tf.idf, features that considered the thread to be a sequence of turns, and email-specific features such as number of recipients and the subject line. Asynchronous multi-party conversations were successfully represented for summarization through a small number of con-versational features by Murray and Carenini (2008). This paved the way to cross-domain conversation summarization by representing both email threads and meetings with a set of common conversational features. The work we present here investigates using data from both emails and meetings in summarizing emails, and compares using conversational versus lexical features. 3 Summarization setting Because the meetings domain has a large corpus, AMI, annotated for summarization, we will use it as the source domain for adaptation and the email domain as the target, with data from the Enron corpus as unlabeled email data, and the BC3 corpus as test data. 3.1 Datasets The AMI meeting corpus: We use the scenario portion of the AMI corpus (Carletta et al, 2005), for which groups of four participants take part in a series of four meetings and play roles within a fictitious company. While the scenario given to them is artificial, the speech and the actions are completely spontaneous and natural. The dataset contains approximately 115000 dialogue act (DA) segments. For the annotation, annotators wrote abstract summaries of each meeting and extracted transcript DA segments that best con-veyed or supported the information in the ab-stracts. A many-to-many mapping between tran-script DAs and sentences from the human ab-stract was obtained for each annotator, with three annotators assigned to each meeting. We con-sider a dialogue act to be a positive example if it is linked to a given human summary, and a nega-tive example otherwise. Approximately 13% of the total DAs are ultimately labeled as positive. The BC3 email corpus 1 : composed of 40 email threads from the World Wide Web Con-sortium (W3C) mailing list which feature a vari-ety of topics such as web accessibility and plan-ning face-to-face meetings. Each thread is anno-tated similarly to the AMI corpus, with three an-                                                1 http://www.cs.ubc.ca/labs/lci/bc3.html 
notators authoring abstracts and linking email thread sentences to the abstract sentences. The Enron email corpus 2 : a collection of emails released as part of the investigation into the Enron corporation, it has become a popular corpus for NLP research due to being realistic, naturally-occurring data from a corporate envi-ronment. We use 39 threads from this corpus to supplement the BC3 email data. 3.2 Features Used We consider two sets of features for each sen-tence: a small set of conversational structure fea-tures, and a large set of lexical features.  Conversational features: We extract 24 con-versational features from both the email and meetings domain, and which consider both emails and meetings to be conversations com-prised of turns between multiple participants. For an email thread, a turn consists of a single email fragment in the exchange. Similarly, for meet-ings, a turn is a sequence of dialogue acts by the same speaker. The conversational features, which are described in detail in (Murray and Carenini, 2008), include sentence length, sen-tence position in the conversation and in the cur-rent turn, pause-style features, lexical cohesion, centroid scores, and features that measure how terms cluster between conversation participants and conversation turns. Lexical features: We derive an extensive set of lexical features, originally proposed in (Murray et al, 2010) from the AMI and BC3 datasets, and then compute their occurrence in the Enron cor-pus. After throwing out features that occur less than five times, we end up with approximately 200,000 features. The features derived are: char-acter trigrams, word bigrams, POS tag bigrams, word pairs, POS pairs, and varying instantiation ngram (VIN) features. For word pairs, we extract the ordered pairs of words that occur in the same sentence, and similarly for POS pairs. To derive VIN features, we take each word bigram w1,w2 and further represent it as two patterns p1,w2 and w1,p2 each consisting of a word and a POS tag.  3.3 Classifier In all of our experiments, we train logistic re-gression classifiers using the liblinear toolkit3. This choice was partly motivated by our earlier summarization research, where logistic regres-sion classifiers were compared alongside support                                                 2 http://www.cs.cmu.edu/?enron/  3 http://www.csie.ntu.edu.tw/?cjlin/liblinear/ 
18
vector machines. The two types of classifier yielded very similar results, with logistic regres-sion classifiers being much faster to train.  3.4 Evaluation Metric Given the predicted labels on a test set and the existing gold-standard labels of the test set data, in each of our experiments we compute the area under the receiver operator curve as a measure of  performance. The area under the ROC (auROC) is a common summary statistic used to measure the quality of binary classification, where a per-fect classifier would achieve an auROC of 1.0, and a random classifier, near 0.5. 4 Experiments 4.1 Experimental Design The available labeled BC3 data totals about 3000 sentences, and the available labeled AMI data totals over 100,000 sentences, so for both effi-ciency and to not overwhelm the in-domain data, in each of our runs we subsample 10,000 sen-tences from the AMI data to use for training. Af-ter some initial experiments, where increasing the amount of target data beyond this did not im-prove accuracy, we decided not to incur the run-time cost of training on larger amounts of source data. Similarly, given that we extracted about 200,000 lexical features from our corpora, from our initial experiments trading off auROC and runtime, we decided to select a subset of 10,000 lexical features chosen by having the top mutual information with respect to the summarization labels. We did 5-fold cross-validation to split the target set into training and testing portions, and ran all the domain adaptation methods using the same split. We report the auROC performance of each method averaged over three runs of the 5-fold cross-validation. To test for significant dif-ferences between the performances of the various methods, we compute pairwise t-tests between the auROC values obtained on the same run. To account for an increased chance of false positives in reporting results of several pairwise t-tests, we report significance for p-values < 0.005 rather than at the customary 0.05 level.  4.2 Methods Implemented We compare supervised domain adaptation me-thods to the baseline INDOMAIN, in which only the training folds of the target data are used for training. In the MERGE method, we simply combine the labeled source and target sets and train on their combination. For ENSEMBLE, we 
train a classifier on the source training data, a classifier on the target training data, run each of them on the target test data, and for each test in-stance compute the average of the two probabili-ties predicted by the classifiers and use it to make a label prediction. We could vary the trade-off between the contribution of the source and target classifier in ENSEMBLE and determine the optimal parameter by cross-validation, though for simplicity we used 0.5 which pro-duced satisfying results. For the PRED approach, we use the source data to train a classifier, use it to make a prediction for the label of each point in the target data, and add the predicted probability as an additional feature to an in-domain trained classifier. The final supervised method FEAT-COPY (Daume, 2007) takes the existing features and extends the feature space by making a gen-eral, a source-specific, and a target-specific ver-sion of each feature. Hence, a sentence with fea-tures (x) gets represented as (x, x, 0) if it comes from the source domain, and as (x, 0, x) if it comes from the target domain.  For semi-supervised domain adaptation meth-ods, our baseline does not exploit any unlabeled target data. We train a classifier on the source data only, and call this TRANSFER. In contrast our two semi-supervised methods try to leverage unlabeled target data to help a classifier trained with labeled source data be more suited to the target domain.  For the SCL approach, we implemented Blit-zer?s structural correspondence learning (SCL) algorithm. An important part of the algorithm is training a classifier for each of a set of m selected pivot features to determine the correlations of the other features with respect to the pivot. The m models? weights are combined in a matrix, and its SVD with truncation factor of k is then ap-plied to the data to yield k new features for the data, that are added to the existing features. For the larger set of lexical features, we ran SCL with Blitzer?s original choice of m=1000 and k=50, but since the computation was extremely time consuming we scale down m to 100. For the tests with conversational features, since the number of features is 24, we picked m=24 and k=24. We also test SCLSMALL, which uses the same algorithm as SCL to find augmented fea-tures, except it then uses only these k features to train, not adding them to the original features.  This possibility was suggested in (Blitzer 2008).  As a second semi-supervised method, we im-plemented SELFTRAIN. The standard self-training algorithm we implemented, inspired by 
19
Blum and Mitchell (1998), is to start with a la-beled training set T, create a subset of a fixed size of the unlabeled data U, and then iterate training a classifier on T, making a prediction on the data in U, and take the highest-confidence positive p predictions and highest-confidence negative n predictions from U with their pre-
dicted labels to add to T before replenishing U from the rest of the unlabeled data. We picked the size of the subset U as 200, and to select the top p=3 and bottom n=17 predictions at each step in order to achieve a ratio of summary to total sentences of 15%, which is near to the known ratio of the labels for AMI.  
method indomain merge ensem-ble featcopy pred transfer selftrain scl sclsmall using conversational features auROC 0.838 0.747 0.751 0.839 0.838 0.677 0.678 0.663 0.646 time(s) 0.79 2.42 2.64 8.44 5.38 2.08 100.2 52.85 66.74 using lexical features auROC 0.623 0.638 0.667 0.615 0.625 0.636 0.636 0.651 0.742 time(s) 4.87 13.64 13.77 78.63 30.99 9.73 448.8 813.7 828.3 Table 1. Performance and time of domain adaptation methods with the two feature sets 
5 Results In our first experiment, we ran all the domain adaptation methods on the data with conversa-tional features; in our second experiment, we did the same on the data with lexical features. We computed the average of the auROCs and run-ning times obtained for each method in each ex-periment. Table 1 lists the results of the super-vised methods MERGE, ENSEMBLE, and FEATCOPY with baseline INDOMAIN, and the semi-supervised methods SELFTRAIN, SCL, and SCLSMALL with baseline TRANSFER.  The best results for supervised methods (and overall) are achieved by FEATCOPY, PRED, and INDOMAIN with the conversational fea-tures, with a similar performance that is signifi-cantly better than for MERGE and ENSEMBLE. However, for lexical features MERGE and EN-SEMBLE beat their performance, with the sig-nificant differences from the baseline INDO-MAIN being those of ENSEMBLE and FEAT-COPY, the latter now being the worst performer.  For the set of lexical features, all semi-supervised methods improve on TRANSFER. In this setting, all of the differences are significant, with SCLSMALL generating a considerable gain of 10%. For the set of conversational features, SELFTRAIN yields an auROC similar to TRANSFER, and the small difference between the two is not significant. Unlike when using lexical features, SCL and SCLSMALL perform 
significantly worse than TRANSFER, though this is not unexpected. Because it relies on de-termining correlation between features, we be-lieve that structural correspondence learning is more appropriate in a high rather than low-dimensional feature space. Figure 1 shows, for each of the methods, a dark grey bar representing the auROC obtained with the set of conversational features next to a lighter grey one for the lexical features. For the super-vised methods on the left (INDOMAIN to PRED), the conversational features yield better performance, and this by an absolute ROC dif-ference of more than 5%. However, notice that no method outperform the baseline INDOMAIN. For the semi-supervised methods on the right, the difference in performance between the two fea-ture sets is less marked, although the auROC of SCLSMALL with lexical features is exception-ally larger. As shown in Table 1, every one of the domain adaptation methods has a higher average time with lexical features than with conversational features. The semi-supervised methods take longer than the fully supervised methods, and this is due to their algorithms involving more steps. Both SCL and SELFTRAIN take minutes instead of seconds to make a prediction, though their running times are more reasonable than with the initial parameter settings we used in pre-liminary experiments.  
20
 Figure 1. Comparison of auROCs of all domain adaptation methods and baselines 6 Conclusions and Future Work This paper is a comparative study of the per-formance of several domain adaptation methods on the task of summarizing conversational data when a large amount of annotated data is avail-able in the domain of meetings and a smaller (or no) amount of annotation exists in the target do-main of email threads.  One surprising finding of our experiments is that of the methods we implemented, the best performance is achieved by training on in-domain data using conversational features. Hence, it seems that when sufficient labeled in-domain data is available, supervised domain ad-aptation is not useful for summarization of emails with the features and amounts of labeled data we used. However, semi-supervised methods using unla-beled data and labeled out-of-domain data are useful in the absence of these labels, with the SCLSMALL method greatly outperforming the baseline. This is a promising result for using an-notated corpora in well-studied domains or con-versational modalities to summarize data in new domains.  In our experiments, we have explored the effec-tiveness of conversational and lexical features separately. The two sets of features differ in their impact on domain adaptation: with conversa-tional features, no method improves significantly over the baseline, whereas with lexical features, the semi-supervised methods given no labeled target data perform better than the supervised baseline of training in-domain. One hypothesis to explain this is that lexical features behave simi-larly in the two domains, so training on the larger amount of labeled target data is beneficial, while conversational features are more domain spe-
cific, likely because emails and meetings are structured differently. As the next step in our work, we intend to combine the two sets of fea-tures. In doing this, we will have to ensure that the conversational features are not washed out by a very large number of lexical features.   A scenario of practical interest in domain adap-tation for new domains is when the target domain has a considerable amount of unlabeled data and a subset of this data can easily be annotated by hand, for example five threads in the email do-main. We are currently exploring injecting a small amount of labeled target data into the semi-supervised methods we have implemented to ac-count for differences that cannot be observed in the unlabeled data. Blitzer (2008) did such an adjustment to SCL using a small amount of la-beled target data to correct misaligned features and thus improve accuracy. Finally, it may be worth investigating how to combine several of the methods, for example by adding the feature of PRED based on training a classifier on the source, alongside augmented features using more unlabeled data through SCL, and adding the highest-confidence labels from SELFTRAIN to the training set.  References  Blitzer, J. (2008). Domain Adaptation of Natural Language Processing Systems. PhD Thesis. Blitzer, J., Dredze, M., & Pereira, F. (2007). Biogra-phies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proc. of ACL 2007. Blitzer, J., McDonald, R., & Pereira, F. (2006). Do-main adaptation with structural correspondence learning. In Proc. of EMNLP 2006. Blum, A., & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proc. CLT. Carletta, J., Ashby, S., Bourban, S., Flynn, M., Guillemot, M. et al (2005). The AMI meeting cor-pus: A pre-announcement. In Proc. of MLMI 2005. Chelba, C., & Acero, A. (2006). Adaptation of maxi-mum entropy capitalizer: Little data can help a lot. Computer Speech & Language, 20(4), 382-399. Daume III, H. (2007). Frustratingly easy domain ad-aptation. In Proc. of ACL 2007. Daume III, H., & Marcu, D. (2006). Domain Adapta-tion for Statistical Classifiers. Journal of Artificial Intelligence Research, 26, 101?126. Florian, R., Hassan, H., Ittycheriah, A., Jing, H., Kambhatla, N., Luo, X., et al (2004). A statistical 
21
model for multilingual entity detection and track-ing. In Proc. HLT-NAACL 2004. Galley, M. (2006). A skip-chain conditional random field for ranking meeting utterances by importance. In Proc. of EMNLP 2006. McClosky, D., Charniak, E., & Johnson, M. (2006). Effective self-training for parsing. In Proc. of HLT-NAACL 2006 . Murray, G., & Carenini, G. (2008). Summarizing spoken and written conversations. In Proc. of EMNLP 2008. Murray, G., Carenini, G., & Ng, R. (2010). Interpreta-tion and transformation for abstracting conversa-tions. In Proc. of HLT-NAACL 2010. Murray, G., Renals, S., Moore, J., & Carletta, J. (2006). Incorporating speaker and discourse fea-tures into speech summarization. In Proc. of HLT-NAACL 2006. Plank, B. (2009). Structural correspondence learning for parse disambiguation. In Proc. of EACL 2009: Student Research Workshop. Rambow, O., Shrestha, L., & Chen, J. (2004). Sum-marizing email threads. In Proc. of HLT-NAACL 2004. 
22
Generating and Validating Abstracts of Meeting Conversations: a User
Study
Gabriel Murray
gabrielm@cs.ubc.ca
Giuseppe Carenini
carenini@cs.ubc.ca
Department of Computer Science, University of British Columbia
Vancouver, Canada
Raymond Ng
rng@cs.ubc.ca
Abstract
In this paper we present a complete sys-
tem for automatically generating natural
language abstracts of meeting conversa-
tions. This system is comprised of com-
ponents relating to interpretation of the
meeting documents according to a meet-
ing ontology, transformation or content
selection from that source representation
to a summary representation, and gener-
ation of new summary text. In a forma-
tive user study, we compare this approach
to gold-standard human abstracts and ex-
tracts to gauge the usefulness of the dif-
ferent summary types for browsing meet-
ing conversations. We find that our auto-
matically generated summaries are ranked
significantly higher than human-selected
extracts on coherence and usability crite-
ria. More generally, users demonstrate a
strong preference for abstract-style sum-
maries over extracts.
1 Introduction
The most common solution to the task of summa-
rizing spoken and written data is sentence (or ut-
terance) extraction, where binary sentence classi-
fication yields a cut-and-paste summary compris-
ing informative sentences from the document con-
catenated in a new, condensed document. Such
extractive approaches have dominated the field of
automatic summarization for decades, in large part
because extractive systems do not require a natu-
ral language generation (NLG) component since
the summary sentences are simply lifted from the
source document.
Extrinsic evaluations have shown that, while ex-
tractive summaries may be less coherent than hu-
man abstracts, users still find them to be valuable
tools for browsing documents (He et al, 1999;
McKeown et al, 2005; Murray et al, 2009). How-
ever, these previous evaluations also illustrate that
concise abstracts are generally preferred by users
and lead to higher objective task scores. A weak-
ness of typical extractive summaries is that the end
user does not know why the extracted sentences
are important; exploring the original sentence con-
text may be the only way to resolve this uncer-
tainty. And if the input source document consists
of noisy, unstructured text such as ungrammatical,
disfluent multi-party speech, then the resultant ex-
tract is likely to be noisy and unstructured as well.
Herein we describe a complete and fully auto-
matic system for generating abstract summaries
of meeting conversations. Our abstractor maps
input sentences to a meeting ontology, generates
messages that abstract over multiple sentences,
selects the most informative messages, and ulti-
mately generates new text to describe these rele-
vant messages at a high level. We conduct a user
study where participants must browse a meeting
conversation within a very constrained timeframe,
having a summary at their disposal. We compare
our automatic abstracts with human abstracts and
extracts and find that our abstract summaries sig-
nificantly outperform extracts in terms of coher-
ence and usability according to human ratings. In
general, users rate abstract-style summaries much
more highly than extracts for these conversations.
2 Related Research
Automatic summarizaton has been described as
consisting of interpretation, transformation and
generation (Jones, 1999). Popular approaches to
text extraction essentially collapse interpretation
and transformation into one step, with genera-
tion either being ignored or consisting of post-
processing techniques such as sentence compres-
sion (Knight and Marcu, 2000; Clarke and Lapata,
2006) or sentence merging (Barzilay and McKe-
own, 2005). In contrast, in this work we clearly
separate interpretation from transformation and in-
corporate an NLG component to generate new text
to describe meeting conversations.
While extraction remains the most common ap-
proach to text summarization, one application in
which abstractive summarization is widely used is
data-to-text generation. Summarization is critical
for data-to-text generation because the amount of
collected data may be massive. Examples of such
applications include the summarization of inten-
sive care unit data in the medical domain (Portet
et al, 2009) and data from gas turbine sensors (Yu
et al, 2007). Our approach is similar except that
our input is text data in the form of conversations.
We otherwise utilize a very similar architecture of
pattern recognition, pattern abstraction, pattern
selection and summary generation.
Kleinbauer et al (2007) carry out topic-based
meeting abstraction. Our systems differ in two
major respects: their summarization process uses
human gold-standard annotations of topic seg-
ments, topic labels and content items from the on-
tology, while our summarizer is fully automatic;
secondly, the ontology they used is specific not
just to meetings but to the AMI scenario meetings
(Carletta et al, 2005), while our ontology applies
to conversations in general, allowing our approach
to be extended to emails, blogs, etc.
In this work we conduct a user study where par-
ticipants use summaries to browse meeting tran-
scripts. Some previous work has compared ex-
tracts and abstracts for the task of a decision au-
dit (Murray et al, 2009) , finding that human ab-
stracts are a challenging gold-standard in terms
of enabling participants to work quickly and cor-
rectly identify the relevant information. For that
task, automatic extracts and the semi-automatic
abstracts of Kleinbauer et al (2007) were found
to be competitive with one another in terms of
user satisfaction and resultant task scores. Other
research on comparing extracts and abstracts has
found that an automatic abstractor outperforms a
generic extractor in the domains of technical ar-
ticles (Saggion and Lapalme, 2002) and evalua-
tive reviews (Carenini and Cheung, 2008), and that
human-written abstracts were rated best overall.
3 Interpretation - Ontology Mapping
Source document interpretation in our system re-
lies on a general conversation ontology. The on-
tology is written in OWL/RDF and contains upper-
level classes such as Participant, Entity, Utterance,
and DialogueAct. When additional information is
available about participant roles in a given domain,
Participant subclasses such as ProjectManager can
be utilized. Object properties connect instances of
ontology classes; for example, the following entry
in the ontology states that the object property has-
Speaker has an instance of Utterance as its domain
and an instance of Participant as its range.
<owl:ObjectProperty rdf:about="#hasSpeaker">
<rdfs:range rdf:resource="#Participant"/>
<rdfs:domain rdf:resource="#Utterance"/>
</owl:ObjectProperty>
The DialogueAct class has subclasses cor-
responding to a variety of sentence-level phe-
nomena: decisions, actions, problems, positive-
subjective sentences, negative-subjective sen-
tences and general extractive sentences (important
sentences that may not match the other categories).
Utterance instances are connected to DialogueAct
subclasses through an object property hasDAType.
A single utterance may correspond to more than
one DialogueAct; for example, it may represent
both a positive-subjective sentence and a decision.
Our current definition of Entity instances is
simple. The entities in a conversation are noun
phrases with mid-range document frequency. This
is similar to the definition of concept proposed by
Xie et al (2009), where n-grams are weighted
by tf.idf scores, except that we use noun phrases
rather than any n-grams because we want to refer
to the entities in the generated text. We use mid-
range document frequency instead of idf (Church
and Gale, 1995), where the entities occur in be-
tween 10% and 90% of the documents in the col-
lection. We do not currently attempt coreference
resolution for entities; recent work has investi-
gated coreference resolution for multi-party dia-
logues (Muller, 2007; Gupta et al, 2007), but the
challenge of resolution on such noisy data is high-
lighted by low accuracy (e.g. F-measure of 21.21)
compared with using well-formed text.
We map sentences to our ontology classes by
building numerous supervised classifiers trained
on labeled decision sentences, action sentences,
etc. A general extractive classifier is also trained
on sentences simply labeled as important. We give
a specific example of the ontology mapping using
the following excerpt from the AMI corpus, with
entities italicized and resulting sentence classifica-
tions shown in bold:
? A: And you two are going to work together
on a prototype using modelling clay. [action]
? A: You?ll get specific instructions from your
personal coach. [action]
? C: Cool. [positive-subjective]
? A: Um did we decide on a chip? [decision]
? A: Let?s go with a simple chip. [decision,
positive-subjective]
The ontology is populated by adding all of
the sentence entities as instances of the Entity
class, all of the participants as instances of the
Participant class (or its subclasses such as Pro-
jectManager when these are represented), and all
of the utterances as instances of Utterance with
their associated hasDAType properties indicating
the utterance-level phenomena of interest. Here
we show a sample Utterance instance:
<Utterance rdf:about="#ES2014a.B.dact.37">
<hasSpeaker rdf:resource="#IndustrialDesigner"/>
<hasDAType rdf:resource="#PositiveSubjective"/>
<begTime>456.58</begTime>
<endTime>458.832</endTime>
</Utterance>
3.1 Feature Set
The interpretation component as just described re-
lies on supervised classifiers for the detection of
items such as decisions, actions, and problems.
This component uses general features that are ap-
plicable to any conversation domain. The first set
of features we use for this ontology mapping are
features relating to conversational structure. They
include sentence length, sentence position in the
conversation and in the current turn, pause-style
features, lexical cohesion, centroid scores, and
features that measure how terms cluster between
conversation participants and conversation turns.
While these features have been found to work
well for generic extractive summarization (Murray
and Carenini, 2008), we use additional features
for capturing the more specific sentence-level phe-
nomena of this research. These include character
trigrams, word bigrams, part-of-speech bigrams,
word pairs, part-of-speech pairs, and varying in-
stantiation n-grams, described in more detail in
(Murray et al, 2010). After removing features
that occur fewer than five times, we end up with
218,957 total features.
3.2 Message Generation
Rather than merely classifying individual sen-
tences as decisions, action items, and so on, we
also aim to detect larger patterns ? or messages
? within the meeting. For example, a given par-
ticipant may repeatedly make positive comments
about an entity throughout the meeting, or may
give contrasting opinions of an entity. In or-
der to determine which messages are essential for
summarizing meetings, three human judges con-
ducted a detailed analysis of four development
set meetings. They first independently examined
previously-written human abstracts for the meet-
ings to identify which messages were present in
the summaries. In the second step, the judges met
together to decide on a final message set. This
resulted in a set of messages common to all the
meetings and agreed upon by all the judges. The
messages that our summarizer will automatically
generate are defined as follows:
? OpeningMessage and ClosingMessage: Briefly de-
scribes opening/closing of the meeting
? RepeatedPositiveMessage and RepeatedNegativeMes-
sage: Describes a participant making positive/negative
statements about a giv en entity
? ActionItemsMessage: Indicates that a participant has
action items relating to some entity
? DecisionMessage: Indicates that a participant was in-
volved in a decision-making process regarding some
entity
? ProblemMessage: Indicates that a participant repeat-
edly discussed problems or issues about some entity
? GeneralDiscussionMessage: Indicates that a partici-
pant repeatedly discussed a given entity
Message generation takes as input the ontology
mapping described in the previous section, and
outputs a set of messages for a particular meeting.
This is done by identifying pairs of Participants
and Entities that repeatedly co-occur with the var-
ious sentence-level predictions. For example, if
the project manager repeatedly discusses the inter-
face using utterances that are classified as positive-
subjective, a RepeatedPositiveMessage is gener-
ated for that Participant-Entity pair. Messages are
generated in a similar fashion for all other mes-
sage types except for the opening and closing mes-
sages. These latter two messages are created sim-
ply by identifying which participants were most
active in the introductory and concluding portions
of the meeting and generating messages that de-
scribe that participant opening or closing the meet-
ing.
Messages types are defined within the OWL on-
tology, and the ontology is populated with mes-
sage instances for each meeting. The following
message describes the Marketing Expert making
a decision concerning the television, and lists the
relevant sentences contained by that decision mes-
sage.
<DecisionMessage rdf:about="#dec9">
<messageSource rdf:resource="#MarketingExpert"/>
<messageTarget rdf:resource="#television"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.55"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.63"/>
</DecisionMessage>
4 Transformation - ILP Content
Selection for Messages
Having detected all the messages for a given meet-
ing conversation, we now turn to the task of
transforming the source representation to a sum-
mary representation, which involves identifying
the most informative messages for which we will
generate text. We choose an integer linear pro-
gramming (ILP) approach to message selection.
ILP has previously been used for sentence selec-
tion in an extractive framework. Xie et al (2009)
used ILP to create a summary by maximizing a
global objective function combining sentence and
entity weights. Our method is similar except that
we are selecting messages based on optimizing
an objective function combining message and sen-
tence weights:
maximize (1??)?
?
i
wisi +??
?
j
ujmj (1)
subject to
?
i
lisi < L (2)
where wi is the score for sentence i, uj is the
score for message j, si is a binary variable in-
dicating whether sentence i is selected, mj is a
binary variable indicating whether message j is
selected, li is the length of sentence i and L is
the desired summary length. The ? term is used
to balance sentence and message weights. Our
sentence weight wi is the sum of all the poste-
rior probabilities for sentence i derived from the
various sentence-level classifiers. In other words,
sentences are weighted highly if they correspond
to multiple object properties in the ontology. To
continue the example from Section 3, the sen-
tence Let?s go with the simple chip will be highly
weighted because it represents both a decision and
a positive-subjective opinion. The message score
uj is the number of sentences contained by the
message j. For instance, the DecisionMessage
at the end of Section 3.2 contains two sentences.
We can create a higher level of abstraction in our
summaries if we select messages which contain
numerous utterances. Similar to how sentences
and concepts are combined in the previous ILP ex-
traction approach (Xie et al, 2009; Gillick et al,
2009), messages and sentences are tied together by
two additional constraints:
?
j
mjoij ? si ?i (3)
mjoij ? si ?ij (4)
where oij is the occurence of sentence i in mes-
sage j. These constraints state that a sentence can
only be selected if it occurs in a message that is
selected, and that a message can only be selected
if all of its sentences have also been selected.
For these initial experiments, ? is set to 0.5. The
summary length L is set to 15% of the conver-
sation word count. Note that this is a constraint
on the length of the selected utterances; we ad-
ditionally place a length constraint on the gener-
ated summary described in the following section.
The reason for both types of length constraint is to
avoid creating an abstract that is linked to a great
many conversation utterances but is very brief and
likely to be vague and uninformative.
5 Summary Generation
The generation component of our system fol-
lows the standard pipeline architecture (Reiter and
Dale, 2000), comprised of a text planner, a micro-
planner and a realizer. We describe each of these
in turn.
5.1 Text Planning
The input to the document planner is an ontol-
ogy which contains the selected messages from
the content selection stage. We take a top-
down, schema-based approach to document plan-
ning (Reiter and Dale, 2000). This method is ef-
fective for summaries with a canonical structure,
as is the case with meetings. There are three high-
level schemas invoked in order: opening mes-
sages, body messages, and closing messages. For
the body of the summary, messages are retrieved
from the ontology using SPARQL, an SQL-style
query language for ontologies, and are clustered
according to entities. Entities are temporally or-
dered according to their average timestamp in the
meeting. In the overall document plan tree struc-
ture, the body plan is comprised of document sub-
plans for each entity, and the document sub-plan
for each entity is comprised of document sub-
plans for each message type. The output of the
document planner is a tree structure with messages
as its leaves and document plans for its internal
nodes. Our text planner is implemented within the
Jena semantic web programming framework1.
5.2 Microplanning
The microplanner takes the document plan as in-
put and performs two operations: aggregation and
generation of referring expressions.
5.2.1 Aggregation
There are several possibilities for aggregation in
this domain, such as aggregating over participants,
entities and message types. The analysis of our
four development set meetings revealed that ag-
gregation over meeting participants is quite com-
mon in human abstracts, so our system supports
such aggregation. This involves combining mes-
sages that differ in participants but share a com-
mon entity and message type; for example, if there
are two RepeatedPositiveMessage instances about
the user interface, one with the project manager
as the source and one with the industrial designer
as the source, a single RepeatedPositiveMessage
instance is created that contains two sources. We
do not aggregate over entities for the sole reason
that the text planner already clustered messages
according to entity. The entity clustering is in-
tended to give the summary a more coherent struc-
ture but has the effect of prohibiting aggregation
over entities.
5.2.2 Referring Expressions
To reduce redundancy in our generated abstracts,
we generate alternative referring expressions when
a participant or an entity is mentioned multiple
times in sequence. For participants, this means
the generation of a personal pronoun. For entities,
rather than referring repeatedly to, e.g., the remote
control, we generate expressions such as that issue
or this matter.
5.3 Realization
The text realizer takes the output of the microplan-
ner and generates a textual summary of a meet-
ing. This is accomplished by first associating ele-
ments of the ontology with linguistic annotations.
For example, participants are associated with a
noun phrase denoting their role, such as the project
manager. Since entities were defined simply as
noun phrases with mid-frequency IDF scores, an
entity instance is associated with that noun phrase.
Messages themselves are associated with verbs,
1to be made publicly available upon publicaton
subject templates and object templates. For exam-
ple, instances of DecisionMessage are associated
with the verb make, have a subject template set to
the noun phrase of the message source, and have
an object template [NP a decision PP [concern-
ing ]] where the object of the prepositional
phrase is the noun phrase associated with the mes-
sage target.
To give a concrete example, consider the fol-
lowing decision message:
<DecisionMessage rdf:about="#dec9">
<rdf:type rdf:resource="&owl;Thing"/>
<hasVerb>make</hasVerb>
<hasCompl>a decision</hasCompl>
<messageSource rdf:resource="#MarketingExpert"/>
<messageSource rdf:resource="#ProjectManager"/>
<messageTarget rdf:resource="#television"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.55"/>
<containsUtterance rdf:resource="#ES2014a.D.dact.63"/>
</DecisionMessage>
There are two message sources,
ProjectManager and MarketingExpert,
and one message target, television. The
subjects of the message are set to be the noun
phrases associated with the marketing expert and
the project manager, while the object template is
filled with the noun phrase the television. This
message is realized as The project manager and
the marketing expert made a decision about the
television.
For our realizer we use simpleNLG2. We tra-
verse the document plan output by the microplan-
ner and generate a sentence for each message leaf.
A new paragraph is created when both the message
type and target of the current message are different
than the message type and target for the previous
message.
6 Task-Based User Study
We carried out a formative user study in order to
inform this early work on automatic conversation
abstraction. This task required participants to re-
view meeting conversations within a short time-
frame, having a summary at their disposal. We
compared human abstracts and extracts with our
automatically generated abstracts. The interpre-
tation component and a preliminary version of
the transformation component have already been
tested in previous work (Murray et al, 2010). The
sentence-level classifiers were found to perform
well according to the area under the receiver op-
erator characteristic (AUROC) metric, which eva-
lutes the true-positive/false-positive ratio as the
2http://www.csd.abdn.ac.uk/?ereiter/simplenlg/
posterior threshold is varied, with scores ranging
from 0.76 for subjective sentences to 0.92 for ac-
tion item sentences. In the following, we focus
on the formative evaluation of the complete sys-
tem. We first describe the corpus we used, then
the materials, participants and procedure. Finally
we discuss the study results.
6.1 AMI Meeting Corpus
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Car-
letta et al, 2005), where groups of four partici-
pants take part in a series of four meetings and
play roles within a fictitious company. There are
140 of these meetings in total. For the sum-
mary annotation, annotators wrote abstract sum-
maries of each meeting and extracted sentences
that best conveyed or supported the information
in the abstracts. The human-authored abstracts
each contain a general abstract summary and three
subsections for ?decisions,? ?actions? and ?prob-
lems? from the meeting. A many-to-many map-
ping between transcript sentences and sentences
from the human abstract was obtained for each an-
notator. Approximately 13% of the total transcript
sentences are ultimately labeled as extracted sen-
tences. A sentence is considered a decision item
if it is linked to the decision portion of the ab-
stract, and action and problem sentences are de-
rived similarly. We additionally use subjectivity
and polarity annotations for the AMI corpus (Wil-
son, 2008).
6.2 Materials, Participants and Procedures
We selected five AMI meetings for this user study,
with each stage of the four-stage AMI scenario
represented. The meetings average approximately
500 sentences each. We included the follow-
ing three types of summaries for each meeting:
(EH) gold-standard human extracts, (AH) gold-
standard human abstracts described in Section
6.1, and (AA) the automatic abstracts output by
our abstractor. All three conditions feature man-
ual transcriptions of the conversation. Each sum-
mary contains links to the sentences in the meet-
ing transcript. For extracts, this is a one-to-one
mapping. For the two abstract conditions, this can
be a many-to-many mapping between abstract sen-
tences and transcript sentences.
Participants were given instructions to browse
each meeting in order to understand the gist of
the meeting, taking no longer than 15 minutes per
meeting. They were asked to consider the sce-
nario in which they were a company employee
who wanted to quickly review a previous meet-
ing by using a browsing interface designed for this
task. Figure 1 shows the browsing interface for
meeting IS1001d with an automatically generated
abstract on the left-hand side and the transcript on
the right. In the screenshot, the user has clicked
the abstract sentence The industrial designer made
a decision on the cost and has been linked to a
transcript utterance, highlighted in yellow, which
reads Also for the cost, we should only put one bat-
tery in it. Notice that this output is not entirely cor-
rect, as the decision pertained to the battery, which
impacted the cost. This sentence was generated
because the entity cost appeared in several deci-
sion sentences.
The time constraint meant that it was not fea-
sible to simply read the entire transcript straight
through. Participants were free to adopt whatever
browsing strategy suited them, including skim-
ming the transcript and using the summary as they
saw fit. Upon finishing their review of each meet-
ing, participants were asked to rate their level of
agreement or disagreement on several Likert-style
statements relating to the difficulty of the task and
the usefulness of the summary. There were six
statements to be evaluated on a 1-5 scale, with
1 indicating strong disagreement and 5 indicating
strong agreement:
? Q1: I understood the overall content of the discussion.
? Q2: It required a lot of effort to review the meeting in
the allotted time.
? Q3: The summary was coherent and readable.
? Q4: The information in the summary was relevant.
? Q5: The summary was useful for navigating the dis-
cussion.
? Q6: The summary was missing relevant information.
Participants were also asked if there was any-
thing they would have liked to have seen in the
summary, and whether they had any general com-
ments on the summary.
We recruited 19 participants in total, with each
receiving financial reimbursement for their partic-
ipation. Each participant saw one summary per
meeting and rated every summary condition dur-
ing the experiment. We varied the order of the
meetings and summary conditions. With 19 sub-
jects, three summary conditions and six Likert
statements, we collected a total of 342 user judg-
ments. To ensure fair comparison between the
three summary types, we limit summary length to
Figure 1: Summary Interface
be equal to the length of the human abstract for
each meeting. This ranges from approximately
190 to 350 words per meeting summary.
6.2.1 Results and Discussion
Participants took approximately 12 minutes on av-
erage to review each meeting, slightly shorter than
the maximum allotted fifteen minutes.
Figure 2 shows the average ratings for each
summary condition on each Likert statement. For
Q1, which concerns general comprehension of
the meeting discussion, condition AH (human
abstracts) is rated significantly higher than EH
(human extracts) and AA (automatic abstracts)
(p=0.0016 and p=0.0119 according to t-test, re-
spectively). However, for the other statement that
addresses the overall task, Q2, AA is rated best
overall. Note that for Q2 a lower score is better.
While there are no significantly differences on this
criterion, it is a compelling finding that automatic
abstracts can greatly reduce the effort required for
reviewing the meeting, at a level comparable to
human abstracts.
Q3 concerns coherence and readability. Condi-
tion AH is significantly better than both EH and
AA (p<0.0001 and p=0.0321). Our condition AA
is also significantly better than the extractive con-
dition EH (p=0.0196). In the introduction we men-
tioned that a potential weakness of extractive sum-
maries is that coherence and readability decrease
when sentences are removed from their original
contexts, and that extracts of noisy, unstructured
source documents will tend to be noisy and un-
structured as well. These ratings confirm that ex-
tracts are not rated well on coherence and readabil-
ity.
Q4 concerns the perceived relevance of the
summary. Condition AH is again significantly bet-
ter than EH and AH (both p<0.0001). AA is rated
substantially higher than EH on summary rele-
vance, but not at a significant level.
Q5 is a key question because it directly ad-
dresses the issue of summary usability for such a
task. Condition AH is significantly better than EH
and AA (both p<0.0001), but we also find that AA
is significantly better than EH (p=0.0476). Ex-
tracts have an average score of only 2.37 out of
5, compared with 3.21 and 4.63 for automatic and
human abstracts, respectively. For quickly review-
ing a meeting conversation, abstracts are much
more useful than extracts.
Q6 indicates whether the summaries were miss-
ing any relevant information. As with Q2, a lower
score is better. Condition AH is significantly bet-
ter than EH and AA (p<0.0001 and p=0.0179),
while AA is better than EH with marginal signif-
icance (p=0.0778). This indicates that our auto-
matic abstracts were better at containing all the
relevant information than were human-selected
extracts.
All participants gave written answers to the
open-ended questions, yielding insights into the
strengths and weaknesses of the different sum-
mary types. Regarding the automatic abstracts
(AA), the most common criticisms were that the
 0
 1
 2
 3
 4
 5
Q1 - Understood Meeting
Q2 - Required Effort**
Q3 - Summary Coherent
Q4 - Summary Relevant
Q5 - Summary Useful
Q6 - Summary Missing Info**
Av
era
ge 
Us
er 
Ra
ting
s
Human AbstractsAuto AbstractsHuman Extracts
Figure 2: User Ratings (** indicates lower score
is better)
summaries are too vague (e.g. ?more concrete
would help?) and that the phrasing can be repet-
itive. There is a potential many-to-many map-
ping between abstract sentences and transcript
sentences, and some participants felt that it was
unnecessarily redundant to be linked to the same
transcript sentence more than once (e.g. ?quite a
few repetitive citations?). Several participants felt
that the sentences regarding positive-subjective
and negative-subjective opinions were overstated
and that the actual opinions were either more sub-
tle or neutral. One participant wrote that these sen-
tences constituted ?a lot of bias in the summary.?
On the positive side, several participants consid-
ered the links between abstract sentences and tran-
script sentences to be very helpful, e.g. ?it re-
ally linked to the transcript well? and ?I like how
the summary has links connected to the transcript.
Easier to follow-up on the meeting w/ the aid of
the summary.? One participant particularly liked
the subjectivity-oriented sentences: ?Lifting some
of the positive/negative from the discussion into
the summary can mean the discussion does not
even need to be included to get understanding.?
The written comments on the extractive condi-
tion (EH) were almost wholly negative. Many par-
ticipants felt that the extracts did not even con-
stitute a summary or that a cut-and-paste from
the transcript does not make a sufficient summary
(e.g. ?The summary was not helpful @ all be-
cause it?s just cut from the transcript?, ?All copy
and paste not a summary?, ?Not very clear sum-
mary - looked like the transcript?, and ?No ef-
fort was made in the summary to put things into
context?). Interestingly, several participants criti-
cized the extracts for not containing the most im-
portant sentences from the transcript despite these
being human-selected extracts, demonstrating that
a good summary is a subjective matter.
The comments on human abstracts (AH) were
generally very positive, e.g. ?easy to follow?, ?it
was good, clear?, and ?I could?ve just read the
summary and still understood the bulk of the meet-
ing?s content.? The most frequent negative criti-
cisms were that the abstract sentences sometimes
contained too many links to the transcript (?mas-
sive amount of links look daunting?), and that the
summaries were sometimes too vague (?perhaps
some points from the discussion can be included,
instead of just having topic outlines?, ?[want] spe-
cific details?). It is interesting to observe that this
latter criticism is shared between human abstracts
and our automatic abstracts. When generalizing
over the source document, details are sometimes
sacrificed.
7 Conclusion
We have presented a system for automatically gen-
erating abstracts of meeting conversations. This
summarizer relies on first mapping sentences to
a conversation ontology representing phenomena
such as decisions, action items and sentiment, then
identifying message patterns that abstract over
multiple sentences. We select the most informa-
tive messages through an ILP optimization ap-
proach, aggregate messages, and finally generate
text describing all of the selected messages. A
formative user study shows that, overall, our auto-
matic abstractive summaries rate very well in com-
parison with human extracts, particularly regard-
ing readability, coherence and usefulness. The
automatic abstracts are also significantly better in
terms of containing all of the relevant information
(Q6), and it is impressive that an automatic ab-
stractor substantially outperforms human-selected
content on such a metric. In future work we aim
to bridge the performance gap between automatic
and human abstracts by identifying more specific
messages and reducing redundancy in the sentence
mapping. We plan to improve the NLG output by
introducing more linguistic variety and better text
structuring. We are also investigating the impact
of ASR transcripts on abstracts and extracts, with
encouraging early results.
Acknowledgments Thanks to Nicholas Fitzgerald for
work on implementing the top-down planner.
References
R. Barzilay and K. McKeown. 2005. Sentence fusion
for multidocument news summarization. Computa-
tional Linguistics, 31(3):297?328.
G. Carenini and JCK Cheung. 2008. Extractive vs.
nlg-based abstractive summarization of evaluative
text: The effect of corpus controveriality. In Proc.
of the 5th International Natural Generation Confer-
ence.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28?39.
K. Church and W. Gale. 1995. Inverse document fre-
quency IDF: A measure of deviation from poisson.
In Proc. of the Third Workshop on Very Large Cor-
pora, pages 121?130.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proc. of COLING/ACL 2006, pages 144?
151.
D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-
Tu?r. 2009. A global optimization framework for
meeting summarization. In Proc. of ICASSP 2009,
Taipei, Taiwan.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.
2007. Resolving ?You? in multi-party dialog. In
Proc. of SIGdial 2007, Antwerp, Belgium.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999.
Auto-summarization of audio-video presentations.
In Proc. of ACM MULTIMEDIA ?99, Orlando, FL,
USA, pages 489?498.
K. Spa?rck Jones. 1999. Automatic summarizing: Fac-
tors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization,
pages 1?12. MITP.
T. Kleinbauer, S. Becker, and T. Becker. 2007. Com-
bining multiple information layers for the automatic
generation of indicative meeting abstracts. In Proc.
of ENLG 2007, Dagstuhl, Germany.
K. Knight and D. Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. In
Proc. of AAAI 2000, Austin, Texas, USA, pages 703?
710.
K. McKeown, J. Hirschberg, M. Galley, and S. Maskey.
2005. From text to speech summarization. In Proc.
of ICASSP 2005, Philadelphia, USA, pages 997?
1000.
C. Muller. 2007. Resolving It, This and That in un-
restricted multi-party dialog. In Proc. of ACL 2007,
Prague, Czech Republic.
G. Murray and G. Carenini. 2008. Summarizing spo-
ken and written conversations. In Proc. of EMNLP
2008, Honolulu, HI, USA.
G. Murray, T. Kleinbauer, P. Poller, S. Renals,
T. Becker, and J. Kilgour. 2009. Extrinsic sum-
marization evaluation: A decision audit task. ACM
Transactions on SLP, 6(2).
G. Murray, G. Carenini, and R. Ng. 2010. Interpre-
tation and transformation for abstracting conversa-
tions. In Proc. of NAACL 2010, Los Angeles, USA.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence, 173:789?816.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press, Cambridge, GB.
H. Saggion and G. Lapalme. 2002. Generat-
ing indicative-informative summaries with sumum.
Computational Linguistics, 28(4):497?526.
T. Wilson. 2008. Annotating subjective content in
meetings. In Proc. of LREC 2008, Marrakech, Mo-
rocco.
S. Xie, B. Favre, D. Hakkani-Tu?r, and Y. Liu. 2009.
Leveraging sentence weights in a concept-based op-
timization framework for extractive meeting sum-
marization. In Proc. of Interspeech 2009, Brighton,
England.
J. Yu, E. Reiter, J. Hunter, and C. Mellish. 2007.
Choosing the content of textual summaries of large
time-series data sets. Journal of Natural Language
Engineering, 13:25?49.
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 10?18,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Using the Omega Index for Evaluating Abstractive Community Detection
Gabriel Murray
Computer Information Systems
University of the Fraser Valley
gabriel.murray@ufv.ca
Giuseppe Carenini
Computer Science
University of British Columbia
carenini@cs.ubc.ca
Raymond Ng
Computer Science
University of British Columbia
rng@cs.ubc.ca
Abstract
Numerous NLP tasks rely on clustering or
community detection algorithms. For many
of these tasks, the solutions are disjoint, and
the relevant evaluation metrics assume non-
overlapping clusters. In contrast, the relatively
recent task of abstractive community detection
(ACD) results in overlapping clusters of sen-
tences. ACD is a sub-task of an abstractive
summarization system and represents a two-
step process. In the first step, we classify sen-
tence pairs according to whether the sentences
should be realized by a common abstractive
sentence. This results in an undirected graph
with sentences as nodes and predicted abstrac-
tive links as edges. The second step is to
identify communities within the graph, where
each community corresponds to an abstrac-
tive sentence to be generated. In this paper,
we describe how the Omega Index, a met-
ric for comparing non-disjoint clustering so-
lutions, can be used as a summarization eval-
uation metric for this task. We use the Omega
Index to compare and contrast several commu-
nity detection algorithms.
1 Introduction
Automatic summarization has long been proposed
as a helpful tool for managing the massive amounts
of language data in our modern lives (Luhn, 1958;
Edmundson, 1969; Teufel and Moens, 1997; Car-
bonell and Goldstein, 1998; Radev et al, 2001).
Most summarization systems are extractive, mean-
ing that a subset of sentences from an input docu-
ment forms a summary of the whole. Particular sig-
nificance may be attached to the chosen sentences,
e.g. that they are relevant to a provided query, gen-
erally important for understanding the overall doc-
ument, or represent a particular phenomenon such
as action items from a meeting. In any case, ex-
traction consists of binary classification of candidate
sentences, plus post-processing steps such as sen-
tence ranking and compression. In contrast, recent
work attempts to replicate the abstractive nature of
human-authored summaries, wherein new sentences
are generated that describe the input document from
a higher-level perspective. While some abstractive
summary sentences are very similar to individual
sentences from the document, others are created
by synthesizing multiple document sentences into
a novel abstract sentence. In this paper, we ad-
dress a component of this latter task, namely iden-
tifying which sentences from the source documents
should be combined in generated abstract sentences.
We call this task abstractive community detection
(ACD), and apply the task to a publicly available
meeting dataset.
Herein we focus on describing how the Omega
Index (Collins and Dent, 1988), a metric for com-
paring non-disjoint clustering solutions, can be used
as a summarization evaluation metric for the ACD
task. Metrics such as the Rand Index (Rand, 1971)
are insufficient since they are intended only for dis-
joint clusters.
ACD itself is carried out in two steps. First, we
classify sentence pairs according to whether they
should be realized by a common abstractive sen-
tence. For this step, we use supervised machine
learning that exploits human-annotated links be-
tween abstracts and extracts for a given document.
This results in an undirected graph with nodes repre-
senting sentences and edges representing predicted
abstractive links. Second, we identify communi-
ties within the graph, where each community cor-
responds to an abstractive sentence to be generated.
We experiment with several divisive community de-
10
tection algorithms, and highlight the importance of
selecting an algorithm that allows overlapping com-
munities, owing to the fact that a document sentence
can be expressed by, and linked to, more than one
abstract summary sentence in the gold-standard.
The structure of the paper is as follow. In Sec-
tion 2, we compare and contrast ACD with other
relevant tasks such as extractive summarization and
topic clustering. In Sections 3-4, we describe the
two ACD steps before we can fully discuss evalua-
tion methods. Section 5 describes the experimental
setup and corpora used, including a description of
the abstractive and extractive summary annotations
and the links between them. In Section 6, we give a
detailed description of the Omega Index and explain
how it differs from the more common Rand Index.
In Sections 7-8 we present results and draw conclu-
sions.
2 Related Work
The ACD task differs from more common extrac-
tive summarization (Mani, 2001a; Jurafsky and Mar-
tin, 2008). Whereas extraction involves simply clas-
sifying sentences as important or not, ACD is a
sub-task of abstractive summarization wherein doc-
ument sentences are grouped according to whether
they can be jointly realized by a common abstrac-
tive sentence. The first step of ACD, where we pre-
dict links between sentence pairs, can be seen to en-
compass extraction since the link is via an as-yet-
ungenerated abstract sentence, i.e. each linked sen-
tence is considered summary-worthy. However, the
second step moves away from extraction by cluster-
ing the linked sentences from the document in order
to generate abstract summary sentences.
ACD also differs from topic clustering (Malioutov
and Barzilay, 2006; Joty et al, 2010), though there
are superficial similarities. A first observation is that
topic links and abstract links are genuinely differ-
ent phenomena, though sometimes related. A sin-
gle abstract sentence can reference more than one
topic, e.g. They talked about the interface design
and the budget report, and a single topic can be
referenced in numerous abstract sentences. From a
practical standpoint, in our work on ACD we can-
not use many of the methods and evaluation metrics
designed for topic clustering, due to the fact that a
document sentence can belong to more than one ab-
stract sentence. This leads to overlapping commu-
nities, whereas most work on topic clustering has
focused primarily on disjoint communities where a
sentence belongs to a single topic. In Section 4, we
discuss community detection algorithms and evalu-
ation metrics that allow overlapping communities.
Work on detecting adjacency pairs (Shriberg et
al., 2004; Galley et al, 2004) also involves classify-
ing sentence pairs as being somehow related. For ex-
ample, if sentence B directly follows sentence A, we
might determine that they have a relationship such
as question-answer or request-accept. In contrast,
with ACD there is no requirement that sentence pairs
be adjacent or even in proximity to one another, nor
must they be in a rhetorical relation.
Work on sentence fusion (Barzilay and McKe-
own, 2005) identifies sentences containing similar
or repeated information and combines them into
new sentences. In contrast, in our task sentences
need not contain repeated information in order to be
linked. For example, two sentences could be linked
to a common abstract sentence due to a more com-
plex rhetorical relationship such as proposal-reject
or question-answer.
ACD is a more general problem that may incor-
porate elements of topic clustering, adjacency pair
detection and other sentence clustering or pairing
tasks. Here we try to directly learn the abstrac-
tive sentence links using lower-level features such as
shared n-grams and cosine similarity, as described in
Section 3, but in future work we will model higher-
level features of topics and rhetorical structure.
3 Step 1: Building a Sentence Pair Graph
In order to describe the use of the Omega Index for
the ACD task, we must first introduce the ACD task
in some detail. The first step in ACD is to determine
which sentence pairs are linked. If two sentences are
linked, it means they can be at least partly realized
in the abstract summary by a common sentence. A
document sentence may ?belong? to more than one
abstract sentence. We take a supervised classifica-
tion approach to this problem, training on a dataset
containing explicit links between extract sentences
and abstract sentences. The corpus and relevant an-
notation are described in detail in Section 5. For
11
Figure 1: Linked Sentences
our gold-standard data, a sentence pair is considered
linked if both sentences are linked to a common ab-
stract sentence and not-linked otherwise.
Figure 1 shows an example snippet of linked sen-
tences from our corpus. The first and second sen-
tences are linked via one abstract sentence while the
first and third sentences are linked via a different ab-
stract sentence. While it is not shown in this exam-
ple, note that two sentences can also be linked via
more than one abstract sentence.
We take a supervised machine learning approach
toward predicting whether a sentence pair is linked.
For each pair, we extract features that can be classed
as follows:
? Structural: The intervening number of sen-
tences, the document position as indicated by
the midpoint of the two sentences, the com-
bined length and the difference in length be-
tween the two sentences, and whether the two
sentences share the same speaker.
? Linguistic: The number of shared bigrams,
shared part-of-speech tags, the sum and aver-
age of tf.idf weights, and the cosine similarity
of the sentence vectors.
We run the trained classifier over sentence pairs,
predicting abstractive links between sentences in the
document. This results in an unweighted, undirected
graph where nodes represent sentences and edges
Figure 2: Graph with Sentence Nodes
represent an abstractive link. Continuing with the
conversation snippet from Figure 1, we would end
up with a graph like Figure 2. This very simple
example of a graph shows that there are abstractive
links predicted between sentences s1 and s2 and be-
tween sentences s1 and s3. There is no direct link
predicted between sentences s2 and s3. However,
it is possible for two sentences with no predicted
link between them to wind up in the same abstractive
community after running a community detection al-
gorithm on the graph. We discuss this community
detection step in the following section.
4 Step 2: Discovering Abstractive
Sentence Communities
In the first step of ACD, we predicted whether pairs
of sentences can be at least partly realized by a com-
mon abstractive sentence. We then want to identify
communities or clusters within the graph. Each of
these communities will correspond to an abstractive
12
Figure 3: Overlapping Communities in Graph
sentence that we will generate. Continuing with our
simple example, Figure 3 shows two communities
that have been identified in the graph. Note that
the communities are overlapping, as each contains
sentence s1; we would generate one abstractive sen-
tence describing sentences s1 and s2 and another de-
scribing sentences s1 and s3. We will return to this
critical issue of overlapping communities shortly.
The task of identifying communities in networks
or graphs has received considerable attention (Porter
et al, 2009). The Girvan-Newman algorithm (Gir-
van and Newman, 2002) is a popular community de-
tection method based on a measure of betweenness.
The betweenness score for an edge is the number of
shortest paths between pairs of nodes in the graph
that run along that edge. An edge with a high be-
tweenness score is likely to be between two commu-
nities and is therefore a good candidate for removal,
as the goal is to break the initial graph into distinct
communities. The Girvan-Newman algorithm pro-
ceeds as follows:
1. Calculate the betweenness of each edge in the
graph.
2. Remove the edge with the highest betweenness.
3. For any edge affected by Step 2, recalculate be-
tweenness.
4. Repeat steps 2 and 3 until no edges remain
In this way we proceed from the full graph with all
edges intact to the point where no edges remain and
each node is in its own community. The intermediate
steps can be visualized by the resulting dendrogram,
such as seen in Figure 4 1.
The top row, the ?leaves? of the dendrogram, rep-
resents the individual nodes in the graph. The rest
1Image Source: Wikimedia Commons (Mhbrugman)
Figure 4: Community Dendrogram
of the dendrogram shows how these nodes are sit-
uated in nested communities, e.g. b and c form a
community bc that combines with def to form bcdef.
In our case, where nodes are sentences, the dendro-
gram shows us how sentences combine into nested
communities. This can be useful for generating ab-
stracts of different granularities, e.g. we could de-
scribe bcdef in one sentence or generate two sen-
tences to separately describe bc and def.
The drawback of Girvan-Newman for our pur-
poses is that it does not allow overlapping commu-
nities, and we know that our human-annotated data
contain overlaps. Note from Figure 4 that all com-
munities decompose into disjoint nested communi-
ties, such as bcdef being comprised of bc and def,
not bc and bdef or some other overlapping case.
We therefore hypothesize that Girvan-Newman in its
traditional form is not sufficient for our current re-
search. For the same reason, recent graph-based ap-
proaches to topic clustering (Malioutov and Barzi-
lay, 2006; Joty et al, 2010) are not directly applica-
ble here.
It is only in recent years that much attention has
been paid to the problem of overlapping (or non-
disjoint) communities. Here we consider two recent
modifications to the Girvan-Newman algorithm that
allow for overlaps. The CONGA algorithm (Gre-
gory, 2007) extends Girvan-Newman so that instead
of removing an edge on each iteration, we either
remove an edge or copy a node. When a node is
copied, an overlap is created. Nodes are associated
with a betweenness score (called the split between-
ness) derived from the edge betweenness scores, and
at each step we either remove the edge with the high-
est betweenness score or copy the node with the
13
Figure 5: CONGA algorithm
highest split betweenness, if it is greater. The edge
and node betweenness scores are then recalculated.
In such a manner we can detect overlapping com-
munities. Figure 5 shows the CONGA copying and
splitting operations applied to our simple example,
so that sentence s1 now exists in two communities.
The CONGO algorithm (Gregory, 2008) is an ap-
proximation of CONGA that is more efficient for
large graphs. Girvan-Newman (and hence CONGA)
are not feasible algorithms for very large graphs, due
to the number of repeated betweenness calculations.
CONGO addresses this problem by using local be-
tweenness scores. Instead of calculating between-
ness using the shortest paths of every pair of nodes
in the graph, only nodes within a given horizon h of
an edge are considered. When h =? then CONGO
and CONGA are identical. Gregory (Gregory, 2008)
found good results using h = 2 or h = 3 on a va-
riety of datasets including blog networks; here we
experiment h = 2.
For the community detection step of our system,
we run both CONGA and CONGO on our graphs
and compare our results with the Girvan-Newman
algorithm. For all community detection methods,
as well as human annotations, any sentences that
are not linked to at least one other sentence in Step
1 are assigned to their own singleton communities.
Also, the algorithms we are evaluating are hierarchi-
cal (see Figure 4), and we evaluate at n = 18 clus-
ters, since that is the average number of sentences
per abstractive meeting summary in the training set.
5 Experimental Setup
In this section we describe the dataset used, includ-
ing relevant annotations, as well as the statistical
classifiers used for Step 1.
5.1 AMI Corpus
For these experiments we use the AMI meeting cor-
pus (Carletta, 2006), specifically, the subset of sce-
nario meetings where participants play roles within
a fictional company. For each meeting, an annotator
first authors an abstractive summary. Multiple an-
notators then create extractive summaries by linking
sentences from the meeting transcript to sentences
within the abstract. This generates a many-to-many
mapping between transcript sentences and abstract
sentences, so that a given transcript sentence can
relate to more than one abstract sentence and vice-
verse. A sample of this extractive-abstractive linking
was shown in Figure 1.
It is known that inter-annotator agreement can be
quite low for the summarization task (Mani et al,
1999; Mani, 2001b), and this is the case with the
AMI extractive summarization codings. The aver-
age ? score is 0.45.
In these experiments, we use only human-
authored transcripts and plan to use speech recog-
nition transcripts in the future. Note that our overall
approach is not specific to conversations or to speech
data. Step 2 is completely general, while Step 1 uses
a single same-speaker feature that is specific to con-
versations. That feature can be dropped to make our
approach completely general (or, equivalently, that
binary feature can be thought of as always 1 when
applied to monologic text).
5.2 Classifiers
For Step 1, predicting abstractive links between sen-
tences, we train a logistic regression classifier us-
ing the liblinear toolkit2. The training set consists
of 98 meetings and there are nearly one million sen-
tence pair instances since we consider every pairing
of sentences within a meeting. The test set consists
of 20 meetings on which we perform our evaluation.
6 Evaluation Metrics
In this section, we present our evaluation metrics for
the two steps of the task.
6.1 Step 1 Evaluation: PRF and AUROC
For evaluating Step 1, predicting abstractive sen-
tence links, we present both precision/recall/f-score
2http://www.csie.ntu.edu.tw/ cjlin/liblinear/
14
as well as the area under the receiver operator char-
acteristic curve (AUROC). While the former scores
evaluate the classifier at a particular posterior proba-
bility threshold, the AUROC evaluates the classifier
more generally by comparing the true-positive and
false-positive rates at varying probability thresholds.
6.2 Step 2 Evaluation: The Omega Index
For evaluating Step 2, ACD, we employ a metric
called the Omega Index which is designed for com-
paring disjoint clustering solutions. To describe and
motivate our use of this metric, it is necessary to de-
scribe previous metrics upon which the Omega In-
dex improves. The Rand Index (Rand, 1971) is a
way of comparing disjoint clustering solutions that
is based on pairs of the objects being clustered. Two
solutions are said to agree on a pair of objects if they
each put both objects into the same cluster or each
into different clusters. The Rand Index can then be
formalized as
(a+ d)/N
where N is the number of pairs of objects, a is
the number of times the solutions agree on putting
a pair in the same cluster and d is the number of
times the solutions agree on putting a pair in differ-
ent clusters. That is, the Rand Index is the number of
pairs that are agreed on by the two solutions divided
by the total number of pairs. The Rand Index is in-
sufficient for overlapping solutions because pairs of
objects can exist together in more than one commu-
nity. In those cases, two solutions might agree on
the occurrence of a pair of objects in one commu-
nity but disagree on the occurrence of that pair in
another community. The Rand Index cannot capture
that distinction.
An improvement to the Rand Index is the Ad-
justed Rand Index (Hubert and Arabie, 1985) which
adjusts the level of agreement according to the ex-
pected amount of agreement based on chance. How-
ever, the Adjusted Rand Index also cannot account
for disjoint solutions.
The Omega Index (Collins and Dent, 1988) builds
on both the Rand Index and Adjusted Rand Index
by accounting for disjoint solutions and correcting
for chance agreement. The Omega Index considers
the number of clusters in which a pair of objects is
together. The observed agreement between solutions
is calculated by
Obs(s1, s2) =
min(J,K)?
j=0
Aj/N
where J and K represent the maximum number of
clusters in which any pair of objects appears together
in solutions 1 and 2, respectively, Aj is the number
of the pairs agreed by both solutions to be assigned
to number of clusters j, and N is again the number
of pairs of objects. That is, the observed agreement
is the proportion of pairs classified the same way by
the two solutions. The expected agreement is given
by:
Exp(s1, s2) =
min(J,K)?
j=0
Nj1Nj2/N
2
where Nj1 is the total number of pairs assigned
to number of clusters j in solution 1, and Nj2 is the
total number of pairs assigned to number of clusters
j in solution 2. The Omega Index is then calculated
as
Omega(s1, s2) =
Obs(s1, s2)? Exp(s1, s2)
1? Exp(s1, s2)
The numerator is the observed agreement adjusted
by expected agreement, while the denominator is
maximum possible agreement adjusted by expected
agreement. The highest possible score of 1 indicates
that two solutions perfectly agree on how each pair
of objects is clustered. With the Omega Index, we
can now evaluate the overlapping solutions discov-
ered by our community detection algorithms.3
7 Results
In this section we present the results for both steps
of ACD. Because the Omega Index is not used for
evaluating Step 1, we keep that discussion brief.
7.1 Step 1 Results: Predicting Abstractive
Sentence Links
For the task of predicting abstractive links within
sentence pairs, the resulting graphs have an aver-
age of 133 nodes and 1730 edges, though this varies
3Software for calculating the Omega Index will be released
upon publication of this paper.
15
System Prec. Rec. F-Score AUROC
Lower-Bound 0.18 1 0.30 0.50
Message Links 0.30 0.03 0.05 -
Abstractive Links 0.62 0.54 0.54 0.89
Table 1: P/R/F and AUROCs for Link Prediction
widely depending on meeting length (from 37 nodes
and 61 edges for one short meeting to 224 edges and
5946 edges for a very long meeting). In compar-
ison, the gold-standard graphs have an average of
113 nodes and 1360 edges. The gold-standards sim-
ilarly show huge variation in graph size depending
on meeting length.
Table 1 reports both the precision/recall/f-scores
as well as the AUROC metrics. We compare our
supervised classifier (labeled ?Abstractive Links?)
with a lower-bound where all instances are predicted
as positive, leading to perfect recall and low preci-
sion. Our system scores moderately well on both
precision and recall, with an average f-score of 0.54.
The AUROC for the abstractive link classifier is
0.89.
It is difficult to compare with previous work since,
to our knowledge, nobody has previously modeled
these extractive-abstractive mappings between doc-
ument sentences and associated abstracts. We can
compare with the results of Murray et al (2010),
however, who linked sentences by aggregating them
into messages. In that work, each message is com-
prised of sentences that share a dialogue act type
(e.g. an action item) and mention at least one com-
mon entity (e.g. remote control). Similar to our
work, sentences can belong to more than one mes-
sage. We assess how well their message-based ap-
proach captures these abstractive links, reporting
their precision/recall/f-scores for this task in Table 1,
with their system labeled ?Message Links?. While
their precision is above the lower-bound, the recall
and f-score are extremely low. This demonstrates
that their notion of message links does not capture
the phenomenon of abstractive sentence linking.
7.2 Step 2 Results: Discovering Abstractive
Communities
For the task of discovering abstractive communi-
ties in our sentence graphs, Table 2 reports the
Omega Index for the CONGA, CONGO and Girvan-
Newman algorithms. We also report the average
Omega Index for the human annotators themselves,
derived by comparing each pair of annotator solu-
tions for each meeting.
It is not surprising that the Omega Index is low for
the inter-annotator comparison; we reported previ-
ously that the ? score for the extractive summaries of
this corpus is 0.45. That ? score indicates that there
is high disagreement about which sentences are most
important in a meeting. We should not be surprised
then that there is further disagreement about how the
sentences are linked to one another. What is surpris-
ing is that the automatic community detection al-
gorithms achieve higher Omega Index scores than
do the annotators. Note that the higher scores of
the community detection algorithms relative to hu-
man agreement is not simply an artefact of identify-
ing clustering solutions that have more overlap than
human solutions, since even the disjoint Girvan-
Newman solutions are higher than inter-annotator
levels. One possible explanation is that the annota-
tors are engaged in a fairly local task when they cre-
ate extractive summaries; for each abstractive sen-
tence, they are looking for a set of sentences from
the document that relate to that abstract sentence,
and because of high redundancy in the document the
different annotators choose subsets of sentences that
have little overlap but are still similar (Supporting
this, we have found that we can train on annotator
A?s extractive codings and test on annotator B?s and
get good classification results even if A and B have a
low ? score.). In contrast, the community detection
algorithms are taking a more comprehensive, global
approach by considering all predicted links between
sentences (Step 1) and identifying the overlapping
communities among them (Step 2).
When looking for differences between automatic
and human community detection, we observed that
the algorithms assigned more overlap to sentences
16
System Omega
Girvan-Newman 0.254
CONGA 0.263
CONGO 0.241
Human 0.209
Table 2: Omega Index for Community Detection
than did the human annotators. For example, the
CONGA algorithm assigned each sentence to an av-
erage of 1.1 communities while the human annota-
tors assigned each to an average of 1.04 communi-
ties. Note that every sentence belongs to at least one
community since unlinked sentences belong to their
own singleton communities, and most sentences are
unlinked, explaining why both scores are close to 1.
Comparing the algorithms themselves, we find
that CONGA is better than both Girvan-Newman
(marginally significant, p = 0.07) and CONGO
(p = 0.015) according to paired t-test. We be-
lieve that the superiority of CONGA over Girvan-
Newman points to the importance of allowing over-
lapping communities. And while CONGO is an ef-
ficient approximation of CONGA that can be useful
for very large graphs where CONGA and Girvan-
Newman cannot be applied, in these experiments the
local betweenness used by CONGO leads to lower
overall scores. Furthermore, our networks are small
enough that both CONGA and Girvan-Newman are
able to finish quickly and there is therefore no need
to rely on CONGO.
Our Step 2 results are dependent on the qual-
ity of the Step 1 results. We therefore test how
good our community detection results would be if
we had gold-standard graphs rather than the imper-
fect output from Step 1. We report two sets of re-
sults. In the first case, we take an annotator?s gold-
standard sentence graph showing links between sen-
tences and proceed to run our algorithms over that
graph, comparing our community detection results
with the communities detected by all annotators. In
the second case, we again take an annotator?s gold-
standard graph and apply our algorithms, but then
only compare our community detection results with
the communities detected by the annotator who sup-
plied the gold-standard graph. Table 3 shows both
sets of results. We can see that the latter set contains
System Omega Omega
All Annots. 1 Annot.
Girvan-Newman 0.445 0.878
CONGA 0.454 0.896
CONGO 0.453 0.894
Table 3: Omega Index, Gold-Standard Graphs
much higher scores, again reflecting that annotators
disagree with each other on this task.
Given gold-standard sentence graphs, CONGA
and CONGO perform very similarly; the differences
are negligible. Both are substantially better than the
Girvan-Newman algorithm (all p < 0.01). This tells
us that it is necessary to employ community detec-
tion algorithms that allow overlapping communities.
These results also tell us that the CONGO algorithm
is more sensitive to errors in the Step 1 output since
it performed well using the gold-standard but worse
than Girvan-Newman when using the automatically
derived graphs.
8 Conclusion
After giving an overview of the ACD task and our
approach to it, we described how the Omega Index
can be used as a summarization evaluation metric for
this task, and explained why other community de-
tection metrics are insufficient. The Omega Index is
suitable because it can account for overlapping clus-
tering solutions, and corrects for chance agreement.
The main surprising result was that all of the com-
munity detection algorithms have higher Omega In-
dex scores than the human-human Omega scores
representing annotator agreement. We have offered
one possibe explanation; namely, that while the hu-
man annotators have numerous similar candidate
sentences from the document that each could be
linked to a given abstract sentence, they may be sat-
isfied to only link (and thus extract) a small repre-
sentative handful, whereas the community detection
algorithms work to find all extractive-abstractive
links. We plan to further research this issue, and po-
tentially derive other evaluation metrics that better
account for this phenomenon.
17
References
R. Barzilay and K. McKeown. 2005. Sentence fusion for
multidocument news summarization. Computational
Linguistics, 31(3):297?328.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval 1998, Melbourne, Australia, pages 335?
336.
J. Carletta. 2006. Unleashing the killer corpus: expe-
riences in creating the multi-everything ami meeting
corpus. In Proc. of LREC 2006, Genoa, Italy, pages
181?190.
L. Collins and C. Dent. 1988. Omega: A general formu-
lation of the rand index of cluster recovery suitable for
non-disjoint solutions. Multivariate Behavioral Re-
search, 23:231?242.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. J. ACM, 16(2):264?285.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use of bayesian networks to model
pragmatic dependencies. In Proc. of ACL 2004.
M. Girvan and M.E.J. Newman. 2002. Community
structure in social and biological networks. Proc. of
the National Academy of Sciences, 99:7821?7826.
S. Gregory. 2007. An algorithm to find overlap-
ping community structure in networks. In Proc. of
ECML/PKDD 2007, Warsaw, Poland.
S. Gregory. 2008. A fast algorithm to find overlapping
communities in networks. In Proc. of ECML/PKDD
2008, Antwerp, Belgium.
L. Hubert and P. Arabie. 1985. Comparing partitions.
Journal of Classification, 2:193?218.
S. Joty, G. Carenini, G. Murray, and R. Ng. 2010. Ex-
ploiting conversation structure in unsupervised topic
segmentation for emails. In Proc. of EMNLP 2010,
Cambridge, MA, USA.
D. Jurafsky and J. H. Martin, 2008. Speech and Lan-
guage Processing. Prentice Hall.
H. P. Luhn. 1958. The automatic creation of litera-
ture abstracts. IBM Journal of Research Development,
2(2):159?165.
I. Malioutov and R. Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. In Proc. of ACL
2006, Sydney, Australia.
I. Mani, D. House, G. Klein, L. Hirschman, T. Firmin,
and B. Sundheim. 1999. The TIPSTER SUMMAC
text summarization evaluation. In Proc. of EACL
1999, Bergen, Norway, pages 77?85.
I. Mani. 2001a. Automatic Summarization. John Ben-
jamin, Amsterdam, NL.
I. Mani. 2001b. Summarization evaluation: An
overview. In Proc. of the NTCIR Workshop 2 Meeting
on Evaluation of Chinese and Japanese Text Retrieval
and Text Summarization, Tokyo, Japan, pages 77?85.
G. Murray, G. Carenini, and R. Ng. 2010. Generating
and validating abstracts of meeting conversations: a
user study. In Proc. of INLG 2010, Dublin, Ireland.
M. Porter, J-P. Onnela, and P. Mucha. 2009. Communi-
ties in networks. Notices of the American Mathemati-
cal Society, 56:1082?1097.
D. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Ex-
periments in single and multi-document summariza-
tion using MEAD. In Proc. of DUC 2001, New Or-
leans, LA, USA.
W.M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66:846?850.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, , and H. Car-
vey. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proceedings of SIGdial Workshop
on Discourse and Dialogue, Cambridge, MA, USA,
pages 97?100.
S. Teufel and M. Moens. 1997. Sentence extraction as a
classification task. In Proc. of ACL 1997, Workshop on
Intelligent and Scalable Text Summarization, Madrid,
Spain, pages 58?65.
18
Proceedings of the 14th European Workshop on Natural Language Generation, pages 136?146,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Abstractive Meeting Summarization with Entailment and Fusion
Yashar Mehdad
?
Giuseppe Carenini
?
Frank W. Tompa
??
Raymond T. NG
?
Department of Computer Science
?University of British Columbia ??University of Waterloo
{mehdad, carenini, rng}@cs.ubc.ca fwtompa@cs.uwaterloo.ca
Abstract
We propose a novel end-to-end frame-
work for abstractive meeting summariza-
tion. We cluster sentences in the in-
put into communities and build an entail-
ment graph over the sentence communi-
ties to identify and select the most relevant
sentences. We then aggregate those se-
lected sentences by means of a word graph
model. We exploit a ranking strategy to
select the best path in the word graph as
an abstract sentence. Despite not relying
on the syntactic structure, our approach
significantly outperforms previous models
for meeting summarization in terms of in-
formativeness. Moreover, the longer sen-
tences generated by our method are com-
petitive with shorter sentences generated
by the previous word graph model in terms
of grammaticality.
1 Introduction
The huge amount of data generated every day in
meetings calls for developing automated methods
to efficiently process these data to meet users?
needs. Automatic summarization is a popular task
that can help users to browse a large amount of
recorder speech in text format. This paper tackles
the task of recorded meeting summarization, ad-
dressing the key limitations of existing approaches
by proposing the following contributions:
1) Various approaches that were recently devel-
oped for meeting summarization (such as (Gillick
et al, 2009; Garg et al, 2009)) focus on extract-
ing important sentences (or dialogue acts) from
speech transcripts, either manual transcripts or au-
tomatic speech recognition (ASR) output. How-
ever, it has been observed in the context of meet-
ing summarization users generally prefer concise
abstracts over extracts, and abstracts lead to higher
objective task scores; likewise automatic abstrac-
tive summaries are preferred in comparison with
human extracts (Murray et al, 2010). Moreover,
most of the abstractive summarization approaches
focus on one component of the system, such as
generation (e.g., (Genest and Lapalme, 2010)) or
content selection (e.g., (Murray et al, 2012)), in-
stead of developing the full framework for abstrac-
tive summarization. To address these limitations,
as the main contribution of this paper, we pro-
pose a full pipeline to generate an abstractive sum-
mary for each meeting transcript. Our system is
similar to that of Murray et al (2010) in terms
of generating abstractive summaries for meeting
transcripts. However, we take a lighter supervi-
sion for the content selection phase and a different
approach towards the language generation phase,
which does not rely on the conventional Natural
Language Generation (NLG) architecture (Reiter
and Dale, 2000).
2) We propose a word graph based approach
to aggregate and generate the abstractive sentence
summary. Our work extends the word graph
method proposed by Filippova (2010) with the fol-
lowing novel contributions: i) We take advantage
of lexical knowledge to merge similar nodes by
finding their relations in WordNet; ii) We gener-
ate new sentences through generalization and ag-
gregation of the original ones, which means that
our generated sentences are not necessarily com-
posed of the original words; and iii) We adopt a
new ranking strategy to select the best path in the
graph by taking the information content and the
grammaticality (i.e. fluency) of the sentence into
consideration.
3) In order to generate an abstract summary for
a meeting, we have to be able to capture the over-
all content of the conversation. This can be done
by identifying the essential content from the most
informative sentences using the semantic relations
among all sentences in a meeting transcript. How-
136
ever, most current methods disregard such rela-
tions in favor of statistical models of word distri-
butions and frequencies. Moreover, the data from
meeting transcripts often contains many highly re-
dundant sentences. As one of the key contribu-
tions of this paper, we propose to build a multi-
directional entailment graph over the sentences to
identify and select relevant information from the
most informative sentences.
4) The textual data from meeting conversa-
tion transcripts are typically in a casual style and
do not exhibit a clear syntactic structure with
proper grammar and spelling. Therefore, abstrac-
tive summarization approaches developed for for-
mal texts, such as scientific or news articles, of-
ten are not satisfactory when dealing with infor-
mal texts. Our proposed method for abstractive
meeting summarization requires minimal syntac-
tic and structural information and is robust in deal-
ing with text that suffers from transcription errors,
ill-formed sentences and unknown lexical choices
such as typically formed in meeting transcripts.
We evaluate our system over meeting tran-
scripts. Our result compares favorably to the result
of previous extractive and abstractive approaches
in terms of information content. Moreover, we
show that our method can generate longer sen-
tences with competitive grammaticality scores, in
comparison with previous abstractive approaches.
Furthermore, we evaluate the impact of each com-
ponent of our system through an ablation test.
As an additional result of our experiments, we
also show that using semantic relations (entail-
ment graph) is important in efficiently performing
the final step of our summarization pipeline (i.e.,
the sentence fusion).
2 Abstractive Summarization
Framework
Similar to Murray et al (2010), our goal is to
generate a meeting summary, i.e. a set of sen-
tences, that could capture the semantics of the
meeting. While (Murray et al, 2010) requires
extensive annotations to train several classifiers
to detect important sentences, opinions and dia-
log acts, we only use a subset of that annotation,
which includes a human abstract and links from
each sentence in the abstract to the source meet-
ing sentences. In addition, instead of generat-
ing an abstractive sentence via the conventional
NLG pipeline (Reiter and Dale, 2000), we exploit
a word graph approach.
Linking
Detection
Entailment
Identify
Community
Detection Entailment Graph
Word Graph
Ranking
Sentence Fusion? ? ?
- -
1Figure 1: Meeting summarization framework.
As shown in Figure 1, our framework consists
of three main components, which we describe in
more detail in the following sections.
2.1 Community Detection
While some abstractive summary sentences are
very similar to original sentences from the meeting
transcript, others can be created by aggregating
and merging multiple sentences into an abstract
sentence. In order to generate such a sentence, we
need to identify which sentences from the original
meeting transcript should be combined in gener-
ated abstract sentences. This task can be consid-
ered as the first step of abstractive meeting sum-
marization and is called ?abstractive community
detection (ACD)? (Murray et al, 2012). To per-
form ACD, we follow the same method proposed
by Murray et al (2012), in two steps:
First, we classify sentence pairs according to
whether or not they should be realized by a com-
mon abstractive sentence. For each pair, we ex-
tract its structural and linguistic features, and we
train a logistic regression classifier over all our
training data (described in Section 3.1) exploiting
such features. We run the trained classier over sen-
tence pairs, predicting abstractive links between
sentences in the document. The result can be rep-
resented as an undirected graph where nodes are
the sentences, and edges represent whether two
nodes are linked.
Second, we have to identify which nodes (i.e.,
sentences from the meeting transcript) can be clus-
tered as a community to generate an abstract sen-
tence. For this purpose, we apply the CONGA al-
gorithm (Gregory, 2007) for community detection
that uses betweenness to detect communities in a
graph. The betweenness score for an edge is the
number of shortest paths between pairs of nodes
in the graph that run along that edge.
If a sentence is not connected to at least one
other sentence in the first step, it?s assigned to its
own singleton community.
137
CD
E
F
GA Bx
x
1
Figure 2: Building an entailment graph over sentences. Ar-
rows and ?x? represent the entailment direction and unknown
cases respectively.
2.2 Entailment Graph
Sentences in a community often include redundant
information which are semantically equivalent but
vary in lexical choices. By identifying the seman-
tic relations between the sentences in each com-
munity, we can discover the information in one
sentence that is semantically equivalent, novel, or
more/less informative with respect to the content
of the other sentences.
Similar to earlier work (Lloret et al, 2008;
Mehdad et al, 2010; Berant et al, 2011; Adler et
al., 2012; Mehdad et al, 2013), we set this prob-
lem as a variant of the Textual Entailment (TE)
recognition task (Dagan and Glickman, 2004). We
build an entailment graph for each community of
sentences, where nodes are the linked sentences
and edges are the entailment relations between
nodes. Given two sentences (s1 and s2), we aim
at identifying the following cases:
i) s1 and s2 express the same meaning (bidirec-
tional entailment). In such cases one of the sen-
tences should be eliminated;
ii) s1 is more informative than s2 (unidirectional
entailment). In such cases, the entailing sentence
should replace or complement the entailed one;
iii) s1 contains facts that are not present in s2, and
vice-versa (the ?unknown? cases in TE parlance).
In such cases, both sentences should remain.
Figure 2 shows how entailment relations can
help in selecting the sentences by removing the re-
dundant and less informative ones. As we show in
the figure, the sentence ?A? entails ?E?, ?F? and
?G?, but not ?B?. So we can keep ?A? and ?B?
and eliminate others. For example, the sentence
?we should discuss about the remote control and
its color? entails ?about the remote?, ?let?s talk
about the remote? and ?um remote?s color?, but
not ?remote?s size is also important?. So we can
keep ?we should discuss about the remote con-
trol and its color? and ?remote?s size is also im-
portant? and eliminate the others. In this way,
TE-based sentence identification can be designed
to distinguish meaning-preserving variations from
true divergence, regardless of lexical choices and
structures.
Similar to previous approaches in TE (e.g., (Be-
rant et al, 2011)), we use a supervised method. To
train and build the entailment graph, we perform
three steps described in the following subsections.
2.2.1 Training set collection
In the last few years, TE corpora have been cre-
ated and distributed in the framework of several
evaluation campaigns, including the Recognizing
Textual Entailment (RTE) Challenge1 and Cross-
lingual textual entailment for content synchroniza-
tion2 (Negri et al, 2012). However, such datasets
cannot directly support our application, since the
RTE datasets are often composed of longer well-
formed sentences and paragraphs (Bentivogli et
al., 2009; Negri et al, 2011).
In order to collect a dataset that is more sim-
ilar to the goal of our entailment framework, we
select a subset of the sixth and seventh RTE chal-
lenge main task (i.e., RTE within a Corpus). Our
dataset choice is based on the following reasons:
i) the length of sentence pairs in RTE6 and RTE7
is shorter than the others, and ii) RTE6 and RTE7
main task datasets are originally created for sum-
marization purpose, which is closer to our work.
We sort the RTE6 and RTE7 dataset pairs based
on the sentence length and choose the first 2000
samples with an equal number of positive and neg-
ative examples. The average length of words in
our training data is 7 words. There are certainly
some differences between our training set and our
sentences from the meeting corpus. However, the
collected training samples was the closest avail-
able dataset to our needs.
2.2.2 Feature representation and training
Working with meeting transcripts imposes some
constraints on feature selection. Meeting tran-
scripts are not often well-formed in terms of sen-
1http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
2http://www.cs.york.ac.uk/semeval-2013/task8/
138
tence structure and contain errors. This limits
our features to the lexical level. Fortunately, lexi-
cal models are less computationally expensive and
easier to implement and often deliver a strong per-
formance for RTE (Sammons et al, 2011).
Our entailment decision criterion is based on
similarity scores calculated with a sentence-to-
sentence matching process. Each example pair of
sentences (s1 and s2) is represented by a feature
vector, where each feature is a specific similarity
score estimating whether s1 entails s2.
We compute 18 similarity scores for each pair
of sentences. Before aggregating the similarity
scores to form an entailment score, we normalize
the similarity scores by the length of s2 (in terms
of lexical items), when checking the entailment di-
rection from s1 to s2. In this way, we can estimate
the portion of information/facts in s2 which is cov-
ered by s1.
The first five scores are computed based on the
exact lexical overlap between the phrases: word
overlap, edit distance, ngram-overlap, longest
common subsequence and Lesk (Lesk, 1986).
The other scores were computed using lexical
resources: WordNet (Fellbaum, 1998), VerbO-
cean (Chklovski and Pantel, 2004), paraphrases
(Denkowski and Lavie, 2010) and phrase match-
ing (Mehdad et al, 2011). We used WordNet
to compute the word similarity as the least com-
mon subsumer between two words considering the
synonymy-antonymy, hypernymy-hyponymy, and
meronymy relations. Then, we calculated the sen-
tence similarity as the sum of the similarity scores
of the word pairs in Text and Hypothesis, nor-
malized by the number of words in Hypothesis.
We also use phrase matching features described in
(Mehdad et al, 2011) which consists of phrasal
matching at the level on ngrams (1 to 5 tokens).
The rationale behind using different entailment
features is that combining various scores will yield
a better model (Berant et al, 2011).
To combine the entailment scores and optimize
their relative weights, we train a Support Vector
Machine binary classifier, SVMlight (Joachims,
1999), over an equal number of positive and nega-
tive examples. This results in an entailment model
with 95% accuracy over 2-fold and 5-fold cross-
validation, which further proves the effectiveness
of our feature set for this lexical entailment model.
The reason that we gained a very high accuracy
is because our selected sentences are a subset
of RTE6 and RTE7 with a shorter length (fewer
words) which makes the entailment recognition
task much easier than recognizing entailment be-
tween paragraphs or long sentences.
2.2.3 Entailment graph edge labeling
Since our training examples are labeled with bi-
nary judgments, we are not able to train a three-
way classifier. Therefore, we set the edge label-
ing problem as a two-way classification task that
casts multidirectional entailment as a unidirec-
tional problem, where each pair is analyzed check-
ing for entailment in both directions (Mehdad et
al., 2012). In this condition, each original test
example is correctly classified if both pairs origi-
nated from it are correctly judged (?YES-YES? for
bidirectional,?YES-NO? and ?NO-YES? for unidi-
rectional entailment and ?NO-NO? for unknown
cases). Two-way classification represents an intu-
itive solution to capture multidimensional entail-
ment relations.
2.2.4 Identification and selection
By assigning all entailment relations between the
extracted sentence pairs, we identify relevant sen-
tences to eliminate the redundant (in terms of
meaning) and less informative ones. In order to
perform this task we follow a set of rules based
on the graph edge labels. Note that since entail-
ment is a transitive relation, our entailment graph
is transitive i.e., if entail(s1,s2) and entail(s2,s3)
then entail(s1,s3) (Berant et al, 2011).
Rule 1) Among the nodes that are connected with
bidirectional entailment (semantically equivalent
nodes) we keep only the one with more outgo-
ing bidirectional and unidirectional entailment re-
lations;
Rule 2) If there is a chain of entailing nodes, we
keep the one that is the root of the chain and elim-
inate others.
2.3 Multi-sentence Fusion
Sentence fusion is a well-known challenge for
summarization systems. In this phase, our goal
is to generate an understandable informative sen-
tence that maximally captures the content of the
sentences in a sentence community.
There are several ways of generating an abstract
sentence (e.g. (Barzilay and McKeown, 2005; Liu
and Liu, 2009; Ganesan et al, 2010; Murray et
al., 2010)); however, most of them rely heavily
on the syntactic structure. We believe that such
139
we
um
should
/must
choose/
deter-
mine
The remote control is
beca-
use
the
cost/
price
important
/crucial
use of uh
Start End
1
Figure 3: Word graph constructed from sentences (1-4) and possible fusion paths. Double line nodes represent merged words
in the graph.
approaches are suboptimal, especially in dealing
with written conversational data (e.g., email) or
the data from speech transcripts, whether manual
transcription or automatic speech recognition out-
put. Instead, we apply an approach that does not
rely on syntax, nor on a standard NLG architec-
ture. More specifically, we build a word graph
from all the words of the sentences in a commu-
nity and aggregate them in order to generate a new
abstractive sentence.
We perform the task of multi-sentence fusion in
two steps. First, we construct a word graph over
sentences in each community that were selected
from the entailment graph. Second, we rank the
valid paths in the word graph and select the top
path as the abstract sentence summary.
2.3.1 Constructing a Word Graph
In order to construct a word graph, our model ex-
tends the word graph method proposed by Filip-
pova (2010) with the following novel contribu-
tions:
1- The basic word graph method disregards se-
mantic and lexical relations between the words
in constructing the word graph, in favor of re-
dundancy and word frequencies. To move be-
yond such limitation, we take advantage of lexi-
cal knowledge to map the similar nodes by finding
their relations in WordNet. In this way, for exam-
ple, two synonym words can be mapped into the
same node.
2- Filippova?s approach is essentially extractive
in nature, which means the generated sentence is
composed by the same words from the original
sentences. We move beyond this by generating
new sentences through generalization and aggre-
gation of the original ones. This means that our
generated sentences are not necessarily composed
of the original words. In this way, we are one step
closer to abstractive summarization.
3- Our proposed method aggregates and gen-
erates new readable sentences, regardless of their
lengths, that can semantically imply several orig-
inal sentences, by taking the information content
and the readability (i.e. fluency) of the sentence
into consideration.
Following Filippova?s method, given a set of re-
lated sentences, we build a word graph by itera-
tively adding sentences to it. Figure 1 illustrates
a small graph composed of 4 sentences, including
the start and end nodes.
1- we must determine the use of uh remote.
2- The remote control is important because the
cost.
3- um we should choose the control.
4- The remote price is crucial.
As one of the main steps of word graph con-
struction, we merge the words that have the same
POS tags under the following conditions:
1) The words are identical (e.g. ?remote?).
2) The words are synonyms. The replacement
choice is based on the word?s commonality, i.e.
tfidf (e.g. ?important? and ?crucial?).
3) The words form a hypernym/hyponym pair or
share a common hypernym. Both words are re-
placed by the hypernym (e.g. ?price? and ?cost?).
4) The words are in an entailment relation. Both
words are replaced by the entailed one (e.g. ?pay?
and ?buy?).
Note that, similar to Filippova?s approach,
where merging is ambiguous we check the context
(a word before and after in the sentence and the
neighboring nodes in the graph) and select the can-
didate that has larger overlap in the context, or the
one with a greater frequency. Similar to the origi-
nal word graph model, we connect adjacent words
with directed edges. For the new nodes or uncon-
nected nodes, we draw an edge with a weight of
1. In contrast, weights between already connected
nodes are increased by 1.
140
2.3.2 Path Selection and Ranking
The word graph, as described above, will generate
many sequences connecting start and end. How-
ever, it is likely that most of the paths are not read-
able. Since we are aiming at generating a good
abstractive sentence, some constraints need to be
considered.
A good abstractive sentence should cover most
of the concepts that exist in the original sentences.
Moreover, it should be grammatically correct.
In order to satisfy these constraints we adopt a
ranking strategy that combines the characteristics
of a good summary sentence. To filter ungram-
matical sentences, we prune the paths in which a
verb does not exist. Our ranking formulation is
summarized as below:
Fluency: Our word graph process generates
many possible paths as abstractive summaries.
We need now to decide which of these paths
are more readable and fluent. As in other areas
of NLP (e.g. machine translation and speech
recognition), the answer can be estimated by a
language model. We assign a probability Pr(P )
to each path P based on a n-gram language model.Pr(P ) = mY
i=1
Pr(pi|pi 11 ) ? mY
i=1
Pr(pi|pi 1i n+1)
?
mX
i=1
 logPr(pi|pi 1i n+1)
Coverage: To identify the summary with the high-
est coverage, we propose a second score that esti-
mates the number of nouns that appear in the path.
In order to reward the ranking score to cover more
salient nouns, we also consider the tfidf score of
nouns in the coverage formulation.
C overage(P ) = Ppi2P tfidf (pi)P
pi2G tfidf (pi)
where the pi are nouns and G is the graph.
Edge weight: As a third score, we adopt the Filip-
pova?s edge weighting formulation w(pi, pj) and
define the edge weight of the path W (P ) as be-
low: w(pi, pj) = freq(pi) + freq(pj)P
P2G
pi,pj2P
di? (P, pi, pj) 1W (P ) = Pm 1i=1 w(pi, pi+1)m  1
where the function diff(P, pi, pj) refers to the
distance between the offset positions pos(P, pi)
of words pi and pj in path P and is defined as
|pos(P, pj)   pos(P, pi)| and m is the number of
words in path P .
Ranking score: In order to generate a summary
sentence that combines the scores above, we
employ a ranking model. The purpose of such
a model is three-fold: i) to generate a more
readable and grammatical sentence; ii) to cover
the content of original sentences optimally; and
iii) to favor strong connections between the
concepts. Therefore, the final ranking score of
path P is calculated over the normalized scores as:Score(P ) = Pr(P )? Coverage(P )W (P )
We select all the paths that contain at least one
verb and rerank them using our proposed ranking
function to find the best path as the summary of
the original sentences.
3 Experiments and Results
We now describe a preliminary, formative evalua-
tion of our framework, whose main goal is to as-
sess strengths and weaknesses of the various com-
ponents and generate ideas for further develop-
ments.
3.1 Dataset
To verify the effectiveness of our approach, we ex-
periment with the AMI meeting corpus (Carletta
et al, 2005) that consists of 140 multi-party meet-
ings with a wide range of annotations, including
abstactive summaries for each meeting and links
from each sentence in the summary to the set of
sentences in the original transcripts that sentence
is summarizing. We use this information as our
gold standard since it provides many examples in
which a set of sentences in the meeting (a commu-
nity) is linked to a human written sentence sum-
marizing that community.
141
In our experiments, we use human authored
transcripts. Note that our approach is not specific
to conversations, however it is designed to deal
with ill-formed sentences and structural errors.
Moreover, the first two components of our sys-
tem are supervised, while the word graph model
is completely unsupervised and domain indepen-
dent.
In order to train our logistic regression classifier
for the first phase of our pipeline, we randomly
select a training set that consists of 98 meetings.
Note that there are about one million sentence pair
instances in the training set since we consider ev-
ery pairing of sentences within a meeting. The rest
is selected as a test set for the evaluation phase.
3.2 Experimental Settings
For preprocessing our dataset we use OpenNLP3
for tokenization and part-of-speech tagging. When
the number of sentences in each community is
more than 10 (which happens in around 10% of
the cases), the community is first clustered using
the MajorClust (Stein and Niggemann, 1999) al-
gorithm when sentences are represented as nor-
malized tfidf vectors and the similarity between
the sentences is measured using cosine similarity.
Then, each cluster is treated as a separate com-
munity. The clustering guarantees a higher over-
lap between the sentences as well as a word graph
with fewer paths. Next, we construct a word graph
over each cluster and rank the valid paths. We
choose the first ranked path as the abstractive sum-
mary of the cluster. For our language model, we
use a tri-gram smoothed language model trained
using the newswire text provided in the English
Gigaword corpus (Graff and Cieri, ).
3.3 Evaluation Metrics
To evaluate performance, we use the ROUGE-1
and ROUGE-2 (unigram and bigram overlap) F1
score, which correlate well with human rankings
of summary quality (Lin and Hovy, 2003). We
also ignore stopwords to reduce the impact of high
overlap when matching them.
Furthermore, to evaluate the grammaticality of
our generated summaries in comparison with the
original word graph method, following common
practice (Barzilay and McKeown, 2005), we ran-
domly selected 10 meeting summaries (total 150
sentences). Then, we asked annotators to give one
3http://opennlp.apache.org/
Models ROUGE-1 ROUGE-2
MMR-centroid 18 3
MMR-cosine 21 -
ILP 24 -
TextRank 25.0 4.4
ClusterRank 27.5 5.1
Orig. word graph 26.9 3.8
Our model (-ent) 32.3 4.8
Our model (GC) 32.1 4.0
Our model (full) 28.7 4.2
Table 1: Performance of different summarization algorithms
on human transcripts for meeting conversations. 5
of three possible ratings for each sentence in a
summary based on grammaticality: perfect (2 pts),
only one mistake (1 pt) and not acceptable (0 pts),
ignoring the capitalization or punctuation. Each
meeting was rated by two annotators (Computer
Science graduate students).
3.4 Baselines
We compare our approach with various extrac-
tive baselines: 1) MMR-centroid system (Car-
bonell and Goldstein, 1998); 2) MMR-cosine sys-
tem (Gillick et al, 2009); 3) ILP-based system
(Gillick et al, 2009); 4) TextRank system (Mihal-
cea and Tarau, 2004); and 5) ClusterRank system
(Garg et al, 2009) and with one abstractive base-
line: 6) Original word graph model (Orig. word
graph) (Filippova, 2010).
In order to measure the effectiveness of dif-
ferent components, we also evaluated our sys-
tem using human-annotated sentence communities
(GC) in comparison with our community detection
model (full). Moreover, we measure the perfor-
mance of our system (GC) ablating the entailment
module (-ent).
3.5 Results
Table 1 shows the results for our proposed ap-
proach in comparison with these strong baselines
for meeting summarization. The results show that
our model outperforms the baselines significantly4
for ROUGE-1 over human transcripts for meet-
ing conversations, which proves the effectiveness
of our approach in dealing with summarization of
5The MMR-cosine and ILP systems did not report the
ROUGE-2 score.
4The statistical significance tests was calculated by ap-
proximate randomization described in (Yeh, 2000).
142
Models Read. R=2 R=1 R=0 Avg Len.
Orig. word graph 1.41 55% 32% 13% 8
Our model 1.34 47% 39% 14% 14
Table 2: Average rating and distribution over rating scores for abstractive word graph models.
meeting conversations. However, the ClusterRank
and TextRank systems outperform our model for
ROUGE-2 score. This can be due to word merging
and word replacement choices in the word graph
construction (see Section 2.3.1), which sometimes
changes a word in a bigram and consequently de-
creases the bigram overlap score. A more detailed
analysis of this problem is left as future work.
Note that there is a drop in ROUGE score when
we use entailment in our system in comparison
with ablating the entailment phase (-ent). This is
mainly due to the fact that the entailment phase
filters equivalent sentences. This affects the re-
sults negatively when such filtered sentences share
many common words with our human-authored
abstracts. We believe that this drop is partly as-
sociated with our evaluation metric rather than
meaning. In other words, we expect no difference
in performance when a human evaluation is ap-
plied. However, the entailment phase helps in im-
proving the efficiency of our pipeline significantly.
If each graph has e edges, n nodes, and p paths,
then finding all the paths results in time complex-
ity O((np + 1)(e + n)), using depth-first search.
Decreasing the number of sentences will reduce
the number of nodes and edges, which leads to
the smaller number of paths. This is even more
significant when there are many sentences in a
community in comparison with the gold standard.
Note that it?s impossible to finish the graph build-
ing phase after 12 hours on a 2.3 GHz quad-core
machine without performing the entailment phase,
when we use our community detection model.
This would be especially problematic in a real-
time setting.
Comparing the gold standard sentence commu-
nities (GC) and our fully automatic system, we can
notice that inaccuracies in the community detec-
tion phase affects the overall performance. How-
ever, using our community detection model, we
still outperform the previous models significantly.
Table 2 shows grammaticality scores, distribu-
tions over the three scores and average sentence
lengths in tokens. The results demonstrate that
47% of the sentences generated by our method are
grammatically correct and 39% of the generated
sentences are almost correct. In comparison with
the original word graph method, our model reports
slightly lower results for the grammaticality score
and the percentage of correct sentences. How-
ever, considering the correlation between sentence
length and grammatical complexity, our model is
capable of generating longer sentences with more
information content (according to ROUGE) and
competitive grammaticality scores.
4 Discussion
After analyzing the results and through manual
verification of some cases, we observe that our ap-
proach produces some interestingly successful ex-
amples. Nevertheless, it appears that the perfor-
mance is still far from satisfactory. This leaves an
interesting challenge for the research community
to tackle. We have identified five different sources
of error:
Type 1: Abstractive human-authored summaries:
the nature of our method is based on extracting
the relevant sentences and generating an abstract
sentence by aggregating such sentences. Also due
to this, our generated abstracts are often infor-
mal and closer to the transcripts? style. However,
in many cases, the human-written summaries are
composed by understanding the original sentences
and produce a formal style abstract sentence, of-
ten using a different vocabulary and structure. For
example:
Human-authored: The industrial designer and user in-
terface designer presented the prototype they created,
which was designed to look like a banana.
System: Working on the principle of a fruit it?s basically
designed around a banana.
Type 2: Evaluation method: The current evalu-
ation methods fail to capture the meaning and re-
lies only on matching the words at uni- or bigram
level. Therefore, we believe that a manual eval-
uation can reveal more potential of our system in
generating abstractive summaries that are closer to
human-written summaries.
143
Human-authored: the project manager recapped the de-
cisions made in the previous meeting.
System: I told you guys about the three new require-
ments ... so that was the last meeting.
Type 3: Subjective abstractive summaries: of-
ten it is not easy for humans to agree on one sum-
mary for a meeting. It is well known that inter-
annotator agreement is quite low for the summa-
rization task (Mani, 2001). For example:
Human-authored 1: They do tool training with a white-
board and each person introduces themselves and draws
their favorite animal on the board.
Human-authored 2: The group introduced themselves to
each other and acquainted themselves with the meeting-
room materials by drawing on the whiteboard.
System: We are gonna know each other and then draw
your little animal.
Type 4: Speaker information: since the nature of
our method is based on extracting the relevant sen-
tences or speaker utterances, we do not take the
speaker information into consideration. However,
the human-written summaries for meetings take
the speaker into account. We plan to extend our
framework to include this feature. For example:
Human-authored: The project manager opened the
meeting and stated the agenda to the team members.
System: I hope you?re ready for this functional design
meeting know at the end projects requirement.
Type 5: Transcription errors: as mentioned be-
fore, the meeting transcripts often contain struc-
ture, grammar, vocabulary choice and dictation er-
rors. This always raises more challenges for algo-
rithms dealing with such texts. For example:
Transcript: if it i if it isn?t more expensive for us to k
make because as far as I understand it.
In light of this analysis, we conclude that a
more comprehensive evaluation method (e.g., hu-
man evaluation), including speaker information in
the pipeline and using text normalization tech-
niques to reduce the effects of noisy transcripts can
better reveal the potential of our system in dealing
with meeting summarization.
5 Conclusion and Future Work
In this paper, we study the problem of abstrac-
tive meeting summarization, and propose a novel
framework to generate summaries composed of
grammatical sentences. Within this framework,
this paper makes three main contributions. First,
in contrast with most current methods based on
fully extractive models, we propose to take advan-
tage of a word graph model for sentence fusion
to generate abstractive summary sentences. Sec-
ond, beyond most of the current approaches which
disregard semantic information, we integrate se-
mantics by means of building textual entailment
graphs over sentence communities. Third, our
framework uses minimal syntactic information in
comparison with previous methods and does not
require a domain specific, engineered conven-
tional NLP component.
We successfully applied our framework over
a challenging meeting dataset, the AMI corpus.
Some significant improvements over our dataset,
in comparison with previous methods, demon-
strates the potential of our approach in dealing
with meeting summarization. Moreover, we prove
that our model can generate longer sentences with
only a minimal loss in grammaticality.
In light of the results of our preliminary forma-
tive evaluation, future work will address the im-
provement of the community detection and sen-
tence fusion phases. On the one hand, we plan to
improve our community detection graph by adding
more relevant features into our current supervised
model. On the other hand, we plan to incorporate
a better source of lexical knowledge in the word
graph construction (e.g., YAGO or DBpedia). We
are also interested in improving our ranking model
by assigning tuned weights to each component. In
addition, we are exploring the replacement of pro-
nouns by their referents (e.g., replacing ?I? by the
name or role of the speaker) to improve both the
entailment and word graph models. Once we will
have explored all these improvements, we plan to
run more comprehensive human evaluations.
Acknowledgments
We would like to thank the anonymous review-
ers for their valuable comments and suggestions
to improve the paper, our annotators for their valu-
able work, and the NSERC Business Intelligence
Network for financial support.
144
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application
to the health-care domain. In Proceedings of
the ACL 2012 System Demonstrations, ACL ?12,
pages 79?84, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence Fusion for Multidocument News Sum-
marization. Comput. Linguist., 31(3):297?328,
September.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
Fifth PASCAL Recognizing Textual Entailment
Challenge. In Proc Text Analysis Conference
(TAC09.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global Learning of Typed Entailment Rules.
In Proceedings of ACL, Portland, OR.
Jaime Carbonell and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In Proceed-
ings of the 21st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?98, pages 335?336, New
York, NY, USA. ACM.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005.
The AMI meeting corpus: A pre-announcement. In
Proc. MLMI, pages 28?39.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of EMNLP 2004, pages 33?40,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
I. Dagan and O. Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
Michael Denkowski and Alon Lavie. 2010.
METEOR-NEXT and the METEOR paraphrase ta-
bles: improved evaluation support for five target
language. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, WMT ?10, pages 339?342, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Katja Filippova. 2010. Multi-sentence compression:
finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 322?
330, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
340?348, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Nikhil Garg, Benoit Favre, Korbinian Reidhammer,
and Dilek Hakkani Tu?r. 2009. ClusterRank: A
Graph Based Method for Meeting Summarization.
Idiap-RR Idiap-RR-09-2009, Idiap, P.O. Box 592,
CH-1920 Martigny, Switzerland, 6.
Pierre-Etienne Genest and Guy Lapalme. 2010. Text
Generation for Abstractive Summarization. In Pro-
ceedings of the Third Text Analysis Conference,
Gaithersburg, Maryland, USA. National Institute
of Standards and Technology, National Institute of
Standards and Technology.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-tr. 2009. A global optimization
framework for meeting summarization. In Proc.
IEEE ICASSP, pages 4769?4772.
David Graff and Christopher Cieri. English Gigaword
Corpus?, year = 2003, institution = Linguistic Data
Consortium, address = Philadelphia,. Technical re-
port.
Steve Gregory. 2007. An Algorithm to Find Over-
lapping Community Structure in Networks. In
Proceedings of the 11th European conference on
Principles and Practice of Knowledge Discovery in
Databases, PKDD 2007, pages 91?102, Berlin, Hei-
delberg. Springer-Verlag.
T. Joachims. 1999. Making large-Scale SVMLearning
Practical. LS8-Report 24, Universita?t Dortmund, LS
VIII-Report.
Michael Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to tell
a pine cone from an ice cream cone. In Proceed-
ings of the 5th annual international conference on
Systems documentation, SIGDOC ?86, pages 24?26,
New York, NY, USA. ACM.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using N-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1, NAACL ?03,
pages 71?78, Stroudsburg, PA, USA. Association
for Computational Linguistics.
145
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: can it be done by sen-
tence compression? In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 261?264, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Elena Lloret, O?scar Ferra?ndez, Rafael Mun?oz, and
Manuel Palomar. 2008. A Text Summarization Ap-
proach under the Influence of Textual Entailment. In
NLPCS, pages 22?31.
I. Mani. 2001. Automatic summarization. Natural
Language Processing, 3. J. Benjamins Publishing
Company.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards cross-lingual textual entailment.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 321?324, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2011. Using bilingual parallel corpora for
cross-lingual textual entailment. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1336?1345,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Short Papers
- Volume 2, ACL ?12, pages 120?124, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Yashar Mehdad, Giuseppe Carenini, and Raymond
NG T. 2013. Towards Topic Labeling with Phrase
Entailment and Aggregation. In Proceedings of
NAACL 2013, pages 179?189, Atlanta, USA, June.
Association for Computational Linguistics.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, July.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010. Generating and validating abstracts of meet-
ing conversations: a user study. In Proceedings of
the 6th International Natural Language Generation
Conference, INLG ?10, pages 105?113, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2012. Using the omega index for evaluating ab-
stractive community detection. In Proceedings of
Workshop on Evaluation Metrics and System Com-
parison for Automatic Summarization, pages 10?18,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad,
Danilo Giampiccolo, and Alessandro Marchetti.
2011. Divide and conquer: crowdsourcing the cre-
ation of cross-lingual textual entailment corpora. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 670?679, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: cross-lingual textual entail-
ment for content synchronization. In Proceedings of
the First Joint Conference on Lexical and Compu-
tational Semantics - Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, SemEval ?12, pages 399?
407, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge Univer-
sity Press, New York, NY, USA.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan
Roth. 2011. Recognizing textual entailment. In
Multilingual Natural Language Applications: From
Theory to Practice. Prentice Hall, Jun.
Benno Stein and Oliver Niggemann. 1999. On the Na-
ture of Structure and Its Identification. In Proceed-
ings of the 25th International Workshop on Graph-
Theoretic Concepts in Computer Science, WG ?99,
pages 122?134, London, UK, UK. Springer-Verlag.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th Conference on Computational
Linguistics - Volume 2, COLING ?00, pages 947?
953. Association for Computational Linguistics.
146
Proceedings of the SIGDIAL 2013 Conference, pages 117?121,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Dialogue Act Recognition in
Synchronous and Asynchronous Conversations
Maryam Tavafi?, Yashar Mehdad?, Shafiq Joty?, Giuseppe Carenini?, Raymond Ng?
?Department of Computer Science, University of British Columbia, Vancouver, Canada
?Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
?{tavafi, mehdad, carenini, rng}@cs.ubc.ca ?sjoty@qf.org.qa
Abstract
In this work, we study the effectiveness of
state-of-the-art, sophisticated supervised
learning algorithms for dialogue act mod-
eling across a comprehensive set of differ-
ent spoken and written conversations in-
cluding: emails, forums, meetings, and
phone conversations. To this aim, we com-
pare the results of SVM-multiclass and
two structured predictors namely SVM-
hmm and CRF algorithms. Extensive em-
pirical results, across different conversa-
tional modalities, demonstrate the effec-
tiveness of our SVM-hmm model for di-
alogue act recognition in conversations.
1 Introduction
Revealing the underlying conversational struc-
ture in dialogues is important for detecting the
human social intentions in spoken conversations
and in many applications including summariza-
tion (Murray, 2010), dialogue systems and di-
alogue games (Carlson, 1983) and flirt detec-
tion (Ranganath, 2009). As an additional example,
Ravi and Kim (2007) show that dialogue acts can
be used for analyzing the interaction of students in
educational forums.
Recently, there have been increasing interests
for dialogue act (DA) recognition in spoken and
written conversations, which include meetings,
phone conversations, emails and blogs. However,
most of the previous works are specific to one of
these domains. There are potentially useful fea-
tures and algorithms for each of these domains,
but due to the underlying similarities between
these types of conversations, we aim to identify a
domain-independent DA modeling approach that
can achieve good results across all types of con-
versations. Such a domain-independent dialogue
act recognizer makes it possible to automatically
recognize dialogue acts in a wide variety of con-
versational data, as well as in conversations span-
ning multiple domains/modalities; for instance a
conversation that starts in a meeting and then con-
tinues via email.
While previous work in DA modeling has fo-
cused on studying only one (Carvalho, 2005;
Shrestha, 2004; Ravi, 2007; Ferschke, 2012; Kim,
2010a; Sun, 2012) or, in a few cases, a couple of
conversational domains (Jeong, 2009; Joty, 2011),
in this paper, we analyze the performance of su-
pervised DA modeling on a comprehensive set
of different spoken and written conversations that
includes: emails, forums, meetings, and phone
conversations. More specifically, we compare
the performance of three state-of-the-art, sophis-
ticated machine learning algorithms, which in-
clude SVM-multiclass and two structured predic-
tors SVM-hmm and Conditional Random Fields
(CRF) for DA modeling. We present an exten-
sive set of experiments studying the effectiveness
of DA modeling on different types of conversa-
tions such as emails, forums, meeting, and phone
discussions. The experimental results show that
the SVM-hmm algorithm outperforms other su-
pervised algorithms across all datasets.
2 Related Work
There have been several studies on supervised
dialogue act (DA) modeling. To the best of
our knowledge, none of them compare the per-
formance of DA recognition on different syn-
chronous (e.g., meeting and phone) and asyn-
chronous (e.g., email and forum) conversations.
Most of the works analyze DA modeling in a spe-
cific domain. Carvalho and Cohen (2005) propose
classifying emails into their dialogue acts accord-
ing to two ontologies for nouns and verbs. The
ontologies are used for determining the speech
acts of each single email with verb-noun pairs.
Shrestha and McKeown (2004) also study the
117
problem of DA modeling in email conversations
considering the two dialogue acts of question and
answer. Likewise, Ravi and Kin (2007) present
a DA recognition method for detecting questions
and answers in educational discussions. Ferschke
et al (2012) apply DA modeling to Wikipedia dis-
cussions to analyze the collaborative process of
editing Wikipedia pages. Kim et al (2010a) study
the task of supervised classification of dialogue
acts in one-to-one online chats in the shopping do-
main.
All these previous studies focus on DA recog-
nition in one or two domains, and do not sys-
tematically analyze the performance of different
dialog act modeling approaches on a compre-
hensive set of conversation domains. As far as
we know, the present work is the first that pro-
poses domain-independent supervised DA model-
ing techniques, and analyzes their effectiveness on
different modalities of conversations.
3 Dialogue Act Recognition
3.1 Conversational structure
Adjacent utterances in a conversation have a
strong correlation in terms of their dialogue acts.
As an example, if speaker 1 asks a question to
speaker 2, it is a high probability that the next ut-
terance of the conversation would be an answer
from speaker 2. Therefore, the conversational
structure is a paramount factor that should be taken
into account for automatic DA modeling. The con-
versational structure differs in spoken and written
discussions. In spoken conversations, the discus-
sion between the speakers is synchronized. The
speakers hear each other?s ideas and then state
their opinions. So the temporal order of the ut-
terances can be considered as the conversational
structure in these types of conversations. How-
ever, in written conversations such as email and
forum, authors contribute to the discussion in dif-
ferent order, and sometimes they do not pay atten-
tion to the content of previous posts. Therefore,
the temporal order of the conversation cannot be
used as the conversational structure in these do-
mains, and appropriate techniques should be used
to extract the underlying structure in these conver-
sations.
To this aim, when reply links are available in
the dataset, we use them to capture the conversa-
tion structure. To obtain a conversational structure
that is often even more refined than the reply links,
we build the Fragment Quotation Graph. To this
end, we follow the procedure proposed by Joty et
al. (2011) to extract the graph structure of a thread.
3.2 Features
In defining the feature set, we have two primary
criteria, being domain independent and effective-
ness in previous works. Lexical features such as
unigrams and bigrams have been shown to be use-
ful for the task of DA modeling in previous stud-
ies (Sun, 2012; Ferschke, 2012; Kim, 2010a; Ravi,
2007; Carvalho, 2005). In addition, unigrams have
been shown to be the most effective among the
two. So, as the lexical feature, we include the fre-
quency of unigrams in our feature set.
Moreover, length of the utterance is another
beneficial feature for DA recognition (Ferschke,
2012; Shrestha, 2004; Joty, 2011), which we add
to our feature set. The speaker of an utterance
has shown its utility for recognizing speech acts
(Sun, 2012; Kim, 2010a; Joty, 2011). Sun and
Morency (2012) specifically employ a speaker-
adaptation technique to demonstrate the effective-
ness of this feature for DA modeling. We also
include the relative position of a sentence in a
post for DA modeling since most of previous stud-
ies (Ferschke, 2012; Kim, 2010a; Joty, 2011)
prove the efficiency of this feature.
3.3 Algorithms
Since most top performing DA models use su-
pervised approaches (Carvalho, 2005; Shrestha,
2004; Ravi, 2007; Ferschke, 2012; Kim, 2010a),
to analyze the performance of DA modeling on a
comprehensive set of different spoken and written
conversations, we compare the state-of-the-art su-
pervised algorithms.
We employ three state-of-the-art, sophisticated
supervised learning algorithms:
SVM-hmm predicts labels for the examples
in a sequence (Tsochantaridis, 2004). This
approach uses the Viterbi algorithm to find the
highest scoring tag sequence for a given obser-
vation sequence. Being a Hidden Markov Model
(HMM), the model makes the Markov assump-
tion, which means that the label of a particular
example is assigned only by considering the
label of the previous example. This approach is
considered an SVM because the parameters of the
model are trained discriminatively to separate the
label of sequences by a large margin.
118
CRF is a probabilistic framework to label and
segment sequence data (Lafferty, 2001). The
main advantage of CRF over HMM is that it re-
laxes the assumption of conditional independence
of observed data. HMM is a generative model
that assigns a joint distribution over label and
observation sequences. Whereas, CRF defines the
conditional probability distribution over label se-
quences given a particular observation sequence.
SVM-multiclass is a generalization of binary
SVM to a multiclass predictor (Crammer, 2001).
The SVM-multiclass does not consider the
sequential dependency between the examples.
4 Corpora
Gathering conversational corpora for DA model-
ing is an expensive and time-consuming task. Due
to the privacy issues, there are few available con-
versational datasets.
For asynchronous conversations, we use avail-
able corpora for email and forum discussions. For
synchronous domains we employ available cor-
pora in multi-party meeting and phone conversa-
tions.
BC3 (Email): As the labeled dataset for email
conversations, we use BC3 (Ulrich, 2008), which
contains 40 threads from W3C corpus. The
BC3 corpus is annotated with twelve domain-
independent dialogue acts, which are mainly
adopted from the MRDA tagset, and it has been
used in several previous works (e.g., (Joty, 2011)).
CNET (Forum): As the labeled forum dataset,
we use the available CNET corpus, which is an-
notated with eleven domain-independent dialogue
acts in a post-level (Kim et al 2010b). This corpus
consists of 320 threads and a total of 1332 posts,
which are mostly from technical forums.
MRDA (Meeting): ICSI-MRDA dataset is
used as labeled data for meeting conversation,
which contains 75 meetings with 53 unique speak-
ers (Shriberg, 2004). The ICSI-MRDA dataset re-
quires one general tag per sentence followed by
variable number of specific tags. There are 11
general tags and 39 specific tags in the annotation
scheme. We reduce their tagset to the eleven gen-
eral tags to be consistent with the other datasets.
SWBD (Phone): In addition to multi-party
meeting conversations, we also report our experi-
mental results on Switchboard-DAMSL (SWBD),
which is a large-scale corpus containing telephone
speech (Jurafsky, 1997). This corpus is annotated
with the SWBD-DAMSL tagset, which consists of
220 tags. We use the mapping table presented by
Jeong (2009) to reduce the tagset to 16 domain-
independent dialogue acts.
All the available corpora are annotated with di-
alogue acts at the sentence-level. The only excep-
tion is the CNET forum dataset, on which we ap-
ply DA classification at the post-level.
5 Experiments and Results
5.1 Experimental settings
In our experiments, we use the SVM-hmm1 and
SVM-multiclass2 packages developed with the
SVM-light software. We use the Mallet package3
for the CRF algorithm. The results of supervised
classifications are compared to the baseline, which
is the majority class of each dataset. We apply
5-fold cross-validation for the supervised learn-
ing methods to each dataset, and compare the re-
sults of different methods using micro-averaged
and macro-averaged accuracies.
5.2 Results
Table 1 shows the results of supervised classifi-
cation on different conversation modalities. We
observe that SVM-hmm and CRF classifiers out-
perform SVM-multiclass classifier in all conversa-
tional domains. Both SVM-hmm and CRF classi-
fiers consider the sequential structure of conversa-
tions, while this is ignored in the SVM-multiclass
classifier. This shows that the sequential structure
of the conversation is beneficial independently of
the conversational modality. We can also observe
that the SVM-hmm algorithm results in the highest
performance in all datasets. As shown in (Altun,
2003), generalization performace of SVM-hmm
is superior to CRF. This superiority also applies
to the DA modeling task across all the conversa-
tional modalities. However, as it was investigated
by Keerthi and Sundararajan (2007), the discrep-
ancy in the performance of these methods may
arise from different feature functions that these
two methods use, and they might perform simi-
larly when they use the same feature functions.
Comparing the results across different datasets,
we can also note that the largest improvement
of SVM-hmm and CRF is on the SWBD, the
1http://www.cs.cornell.edu/people/tj/svm_light/svm_hmm.html
2http://svmlight.joachims.org/svm_multiclass.html
3http://mallet.cs.umass.edu
119
Corpus Baseline SVM-multiclass SVM-hmm CRFMicro Macro Micro Macro Micro Macro Micro Macro
BC3 69.56 8.34 73.57 (4.01) 8.34 (0) 77.75 (8.19) 18.20 (9.86) 72.18 (2.62) 14.9 (6.56)
CNET 36.75 9.09 34.8 (-1.95) 9.3 (0.21) 58.7 (21.95) 17.1 (8.01) 40.3 (3.55) 11.5 (2.41)
MRDA 66.47 9.09 66.47 (0) 9.09 (0) 80.5 (14.03) 32.4 (23.31) 77.8 (11.33) 22.9 (13.81)
SWBD 46.44 6.25 46.5 (0.06) 6.25 (0) 74.32 (27.88) 30.13 (23.88) 73.04 (26.6) 24.05 (17.8)
Table 1: Results of supervised DA modeling; columns are micro-averaged and macro-averaged accura-
cies with difference with baseline in parentheses.
phone conversation dataset. Moreover, super-
vised DA recognition on synchronous conversa-
tions achieves a better performance than on asyn-
chronous conversations. We can argue that this is
due to the less complex sequential structure of syn-
chronous conversations. A lower macro-averaged
accuracy in asynchronous conversations (i.e., fo-
rums and emails) can be justified in the same way.
By looking at the results in asynchronous con-
versations, we observe a larger improvement of
micro-averaged accuracy over the CNET corpus.
This might be due to two reasons: i) the DA tagsets
in both corpora are different (i.e., no overlap in
tagsets); and ii) the conversational structure in fo-
rums and emails is different.
5.3 Discussion
We analyze the strengths and weakness of super-
vised DA modeling with SVM-hmm in different
conversations individually.
BC3: SVM-hmm succeeds in classifying most
of the statement and yes-no question speech acts in
the BC3 corpus. However, it does not show a high
accuracy for classifying polite mechanisms such
as ?thanks? and ?regards?. Through the error anal-
ysis, we observed that in most of these cases the
error arose from the voting algorithm. Moreover,
the improvement of supervised DA modeling on
the BC3 corpus is smaller than the other datasets.
This may suggest that email conversation is a chal-
lenging domain for DA recognition.
CNET: The inventory of dialogue acts in the
CNET dataset can be considered as two groups of
question and answer dialogue acts, and we would
need more sophisticated features in order to clas-
sify the posts into the fine-grained dialogue acts.
The SVM-hmm succeeds in predicting the labels
of question-question and answer-answer dialogue
acts, but it performs poorly for the other labels.
The improvement of DA modeling over the base-
line is significant for this dataset. To further im-
prove the performance, a hierarchical DA classifi-
cation can be applied. In this way, the posts would
be classified into question and non-question dia-
logue acts in the first level.
MRDA: SVM-hmm performs well for pre-
dicting the classes of statement, floor holder,
backchannel, and wh-question. Floor holders and
backchannels are mostly the short utterances such
as ?ok?, ?um?, and ?so?, and we believe the length
and unigrams features are very effective for pre-
dicting these dialogue acts. On the other hand,
SVM-hmm fails in predicting the other types of
questions such as rhetorical questions and open-
ended questions by classifying them as statements.
Arguably by adding more sophisticated features
such as POS tags, SVM-hmm would perform bet-
ter for classifying these speech acts.
SWBD: The improvement of supervised DA
recognition on the SWBD is higher than the other
domains. Supervised DA classification correctly
predicts most of the classes of statement, reject re-
sponse, wh-question, and backchannel. However,
SVM-hmm cannot predict some specific dialogue
acts of phone conversations such as self-talk and
signal-non-understanding. There are a few utter-
ances in the corpus with these dialogue acts, and
most of them are classified as statements.
6 Conclusion and Future Work
We have studied the effectiveness of sophisticated
supervised learning algorithms for DA modeling
across a comprehensive set of different spoken and
written conversations. Through an extensive ex-
periment, we have shown that our proposed SVM-
hmm algorithm with the domain-independent fea-
ture set can achieve high results on different syn-
chronous and asynchronous conversations.
In future, we will incorporate other lexical and
syntactic features in our supervised framework.
We also plan to augment our feature set with
domain-specific features like prosodic features for
spoken conversations. We will also investigate the
performance of our domain-independent approach
in a semi-supervised framework.
120
References
Congkai Sun and Louise-Philippe Morency. 2012. Di-
alogue Act Recognition using Reweighted Speaker
Adaptation. 13th Annual SIGdial Meeting on Dis-
course and Dialogue.
Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL labeling project
coder?s manual, draft 13. Technical report, Univ. of
Colorado Institute of Cognitive Science.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI Meet-
ing Recorder Dialog Act (MRDA) Corpus. HLT-
NAACL SIGDIAL Workshop.
Gabriel Murray, Giuseppe Carenini, and Raymond T.
Ng. 2010. Generating and validating abstracts of
meeting conversations: a user study. INLG?10.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and struc-
tured output spaces. Proceedings of the 21st Inter-
national Conference on Machine Learning (ICML).
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for
supervised email summarization. EMAIL?08 Work-
shop. AAAI.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. Intl. Conf. on Machine Learning.
Koby Crammer and Yoram Singer. 2001. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search.
Lari Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. D. Reidel.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. Proceedings of the 20th Biennial Int. Conf. on
Computational Linguistics.
Minwoo Jeong, Chin-Yew Lin, and Gary G. Lee.
2009. The Semi-supervised speech act recognition
in emails and forums. Proceedings of the 2009 Conf.
Empirical Methods in Natural Language Processing.
Oliver Ferschke, Iryna Gurevych, and Yevgen Cheb-
otar. 2012. Behind the Article: Recognizing Dia-
log Acts in Wikipedia Talk Pages. Proceedings of
the 13th Conference of the European Chapter of the
ACL.
Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.
2009. Its not you, its me: Detecting flirting and its
misperception in speed-dates. EMNLP-09.
S. S. Keerthi and S. Sundararajan. 2007. CRF versus
SVM-Struct for sequence labeling. Technical report,
Yahoo Research.
Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in
asynchronous conversations. IJCAI.
Su N. Kim, Lawrence Cavedon, and Timothy Baldwin.
2010a. Classifying dialogue acts in one-on-one live
chats. EMNLP?10.
Su N. Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, CoNLL ?10.
Sujith Ravi and Jihie Kim. 2007. Profiling student
interactions in threaded discussions with speech act
classifiers. AIED?07, LA, USA.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email "speech acts".
Proceedings of the 31st Annual Int. ACM SIGIR
Conf. on Research and Development in Information
Retrieval.
Yasemin Altun and Ioannis Tsochantaridis and Thomas
Hofmann. 2003. Hidden Markov Support Vector
Machines. Proceedings of the 20th International
Conference on Machine Learning.
7 Appendix A. Frequency of Dialogue
Acts in the Corpora
Tag Dialogue Acts Email(BC3) Forum(CNET) Meeting(MRDA) Phone(SWBD)A Accept response 2.07% ? ? 6.96%AA Acknowledge and appreciate 1.24% ? ? 2.12%AC Action motivator 6.09% ? ? 0.38%P Polite mechanism 6.97% ? ? 0.12%QH Rhetorical question 0.75% ? 0.34% 0.25%QO Open-ended question 1.32% ? 0.17% 0.3%QR Or/or-clause question 1.10% ? ? 0.2%QW Wh-question 2.29% ? 1.63% 0.95%QY Yes-no question 6.75% ? 4.75% 2.62%R Reject response 1.06% ? ? 1.03%S Statement 69.56% ? 66.47% 46.44%U Uncertain response 0.79% ? ? 0.15%Z Hedge ? ? ? 11.55%B Backchannel ? ? 14.44% 26.62%D Self-talk ? ? ? 0.1%C Signal-non-understanding ? ? ? 0.14%FH Floor holder ? ? 7.96% ?FG Floor grabber ? ? 2.96% ?H Hold ? ? 0.76% ?QRR Or clause after yes-no question ? ? 0.38% ?QR Or question ? ? 0.2% ?QQ Question-question ? 27.92% ? ?QA Question-add ? 11.67% ? ?QCN Question-confirmation ? 3.89% ? ?QCC Question-correction ? 0.36% ? ?AA Answer-answer ? 36.75% ? ?AD Answer-add ? 8.84% ? ?AC Answer-confirmation ? 0.36% ? ?RP Reproduction ? 0.71% ? ?AO Answer-objection ? 1.07% ? ?RS Resolution ? 7.78% ? ?O Other ? 0.71% ? ?
Table 2: Dialogue act categories and their relative
frequency.
Table 2 indicates the dialogue acts of each cor-
pus and their relative frequencies in that dataset.
The table shows that the distribution of dialogue
acts in the datasets are not balanced. Most of the
utterances in the datasets are labeled as statements.
Consequently, during the classification step, most
of the utterances are labeled as the statement dia-
logue act. This always affects the performance of
a classifier in dealing with low frequency classes.
A possible approach to tackle this problem is to
cluster the correlative dialogue acts into the same
group and apply a DA modeling approach in a hi-
erarchical manner.
121
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 45?52,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Interactive Exploration of Asynchronous Conversations: Applying a
User-centered Approach to Design a Visual Text Analytic System
Enamul Hoque, Giuseppe Carenini
{enamul,carenini}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, Canada
Shafiq Joty
sjoty@qf.org.qa
Qatar Computing Research Institute
Qatar Foundation
Doha, Qatar
Abstract
Exploring an online conversation can be
very difficult for a user, especially when
it becomes a long complex thread. We fol-
low a human-centered design approach to
tightly integrate text mining methods with
interactive visualization techniques to sup-
port the users in fulfilling their informa-
tion needs. The resulting visual text ana-
lytic system provides multifaceted explo-
ration of asynchronous conversations. We
discuss a number of open challenges and
possible directions for further improve-
ment including the integration of interac-
tive human feedback in the text mining
loop, applying more advanced text analy-
sis methods with visualization techniques,
and evaluating the system with real users.
1 Introduction
With the rapid adoption of Web-based social me-
dia, asynchronous online conversations are be-
coming extremely common for supporting com-
munication and collaboration. An asynchronous
conversation such as a blog may start with a news
article or an editorial opinion, and later generate a
long and complex thread as comments are added
by the participants (Carenini et al., 2011). Con-
sider a scenario, where a reader opens a blog con-
versation about Obama?s healthcare policy. The
reader wants to know why people are supporting
or opposing ObamaCare. However, since some
related discussion topics like student loan and job
recession are introduced, the reader finds it hard
to keep track of the comments about ObamaCare,
which end up being buried in the long discussion.
This may lead to an information overload problem,
where the reader gets overwhelmed, starts to skip
comments, and eventually leaves the conversation
without satisfying her information needs (Jones et
al., 2004).
How can we support the user in performing this
and similar information seeking tasks? Arguably,
supporting this task requires tight integration be-
tween Natural Language Processing (NLP) and in-
formation visualization (InfoVis) techniques, but
what specific text analysis methods should be ap-
plied? What metadata of the conversation could be
useful to the user? How this data should be visual-
ized to the user? And even more importantly, how
NLP and InfoVis techniques should be effectively
integrated? Our hypothesis is that to answer these
questions effectively, we need to apply human-
centered design methodologies originally devised
for generic InfoVis (e.g., (Munzner, 2009; Sedl-
mair et al., 2012)). Starting from an analysis of
user behaviours and needs in the target conversa-
tional domain, such methods help uncover useful
task and data abstractions that can guide system
design. On the one hand, task and data abstrac-
tions can characterize the type of information that
needs to be extracted from the conversation; on the
other hand, they can inform the design of the vi-
sual encodings and interaction techniques. More
tellingly, as both the NLP and the InfoVis compo-
nents of the resulting system refer to a common set
of task and data abstractions, they are more likely
to be consistent and synergistic.
We have explored this hypothesis in developing
ConVis, a visual analytic system to support the in-
teractive analysis of blog conversations. In the first
part of the paper, we describe the development of
ConVis, from characterizing the domain of blogs,
its users, tasks and data, to designing and imple-
menting specific NLP and InfoVis techniques in-
formed by our user-centered design. In the second
part of the paper, starting from an informal evalu-
ation of Convis and a comprehensive literature re-
view, we discuss several ideas on howConVis (and
similar systems) could be further improved and
tested. These include the integration of interac-
tive human feedback in the text mining techniques
45
(which are based on Machine Learning), the cou-
pling of even more advanced NLP methods with
the InfoVis techniques, and the challenges in run-
ning evaluations of ConVis and similar interfaces.
2 Related Work
While in the last decade, NLP and InfoVis meth-
ods have been investigated to support the user in
making sense of conversational data, most of this
work has been limited in several ways.
For example, earlier works on visualizing
asynchronous conversations primarily investigated
how to reveal the thread structure of a conversation
using tree visualization techniques, such as using
a mixed-model visualization to show both chrono-
logical sequence and reply relationships (Venolia
and Neustaedter, 2003), thumbnail metaphor using
a sequence of rectangles (Wattenberg and Millen,
2003; Kerr, 2003), and radial tree layout (Pascual-
Cid and Kaltenbrunner, 2009). However, such vi-
sualizations did not focus on analysing the actual
content (i.e., the text) of the conversations, which
is something that according to our user-centred de-
sign users are very interested in.
On the other hand, text mining approaches
that perform content analysis of the conversations,
such as finding primary themes (or topics) within
conversations (Sack, 2000; Dave et al., 2004), or
visualizing the content evolution over time (Wei et
al., 2010; Vi?egas et al., 2006), often did not derive
their visual encodings and interactive techniques
from task and data abstractions based on a detailed
analysis of specific user needs and requirements in
the target domains.
Furthermore, more on the technical side, the
text analysis methods employed by these ap-
proaches are not designed to exploit the spe-
cific characteristics of asynchronous conversations
(e.g., use of quotation). Recently, (Joty et al.,
2013b) has shown that topic segmentation and la-
beling models are more accurate when these spe-
cific characteristics are taken into account. The
methods presented in (Joty et al., 2013b) are
adopted in ConVis.
In general, to the best of our knowledge, no
previous work has applied user-centred design to
tightly integrate text mining methods with interac-
tive visualization in the domain of asynchronous
conversations.
3 Domains and User Activities
Conversational domains: The phenomenal adop-
tion of novel Web-based social media has lead to
the rise of textual conversations in many different
modalities. While email remains a fundamental
way of communicating for most people, other con-
versational modalities such as blogs, microblogs
(e.g., Twitter) and discussion fora have quickly be-
come widely popular. Since the nature of data and
tasks may vary significantly from one domain to
the other, rather than trying to build an one-size-
fit-all interface, we follow a design methodology
that is driven by modeling the tasks and usage
characteristics in a specific domain.
In this work, we focus on blogs, where people
can express their thoughts and engage in online
discussions. Due to the large number of comments
with complex thread structure (Joty et al., 2013b),
mining and visualizing blog conversations can be-
come a challenging problem. However, the visual-
ization can be effective for other threaded discus-
sions (e.g., news stories, Youtube comments).
Users: As shown in Table 1, blog users can be
categorized into two groups based on their activ-
ities: (a) participants who already contributed to
the conversations, and (b) non-participants who
wish to join the conversations or analyze the con-
versations. Depending on different user groups the
tasks might vary as well, something that needs to
be taken into account in the design process.
For example, imagine a participant who has ex-
pressed her opinion about a major political issue.
After some time, she may become interested to
know what comments were made supporting or
opposing her opinion, and whether those com-
ments require a reply right away. On the contrary,
a non-participant, who is interested in joining the
ongoing conversation on that particular political
issue, may want to decide whether and how she
should contribute by quickly skimming through a
long thread of blog comments. Another group of
users may include the analysts, a policy maker for
instance, who does not wish to join the conversa-
tion, but may want to make an informed decision
based on a summary of arguments used to support
or oppose the political issue.
Once the conversation becomes inactive (i.e.,
no further comments are added), still a distinction
may remain between the activities of participants
and non-participants on tasks (see Table 1). In our
work, we have initially concentrated on supporting
46
User
types
Ongoing conver-
sation
Inactive/past conver-
sation
Participant Already joined the
conversation (wants
to get updated and
possibly make new
comments)
Wants to delve into
the past conversations
and re-examine what
was discussed, what
she commented on,
what other people
replied, etc.
Non-
participant
Potential partici-
pant (wants to join
the conversation)
Analyst (wants to
analyze the ongo-
ing conversation,
but does not intend
to join)
Wants to analyze and
gain insight about the
past conversation.
Table 1: User categorization for asynchronous
conversation.
the non-participant?s activity on an inactive con-
versation (as opposed to an ongoing conversation).
4 Designing ConVis: From Tasks to NLP
and InfoVis Techniques
We now briefly describe our design approach for
integrating text mining techniques with interactive
visualization in ConVis. We first characterize the
domain of blogs and perform the data and tasks
abstraction according to the nested model of de-
sign study (Munzner, 2009). We then mine the
data as appeared to be essential from that data and
task analysis, followed by iteratively refining the
design of ConVis that aims to effectively support
the identified blog reading tasks (A more detailed
analysis of the task abstractions and visual design
is provided in (Hoque and Carenini, 2014)).
4.1 Tasks
To understand the blog reading tasks, we re-
viewed the literature focusing on why and how
people read blogs. From the analysis, we
found that the primary goals of reading blogs in-
clude information seeking, fact checking, guid-
ance/opinion seeking, and political surveillance
(Kaye, 2005). People may also read blogs to con-
nect to their communities of interest (Dave et al.,
2004; Mishne, 2006), or just for fun/ enjoyment
(Baumer et al., 2008; Kaye, 2005).
Some studies have also revealed interesting be-
havioural patterns of blog readers. For example,
people often look for variety of opinions and have
tendencies to switch from one topic to another
quickly (Singh et al., 2010; Munson and Resnick,
2010). In addition, they often exhibit exploratory
behaviour, i.e., they quickly skim through a few
posts about a topic before delving deeper into its
details (Zinman, 2011). Therefore, the interface
should facilitate open-ended exploration, by pro-
viding navigational cues that help the user to seek
interesting comments.
From the analyses of primary goals of blog
reading, we compile a list of tasks and the asso-
ciated data variables that one would wish to visu-
alize for these tasks. These tasks can be framed
as a set of questions, for instance, ?what do peo-
ple say about topic X??, ?how other people?s view-
points differ from my current viewpoint on topic
X??, ?what are some interesting/funny comments
to read?? We then identify the primary data vari-
ables involved in these tasks and their abstract
types. For instance, most of these questions in-
volve topics discussed and sentiments expressed
in the conversation. Note that some questions may
additionally require to know people-centric infor-
mation and relate such information to the visual-
ization design. We also identify a set of meta-
data to be useful cues for navigating a conversa-
tion (the position of the comments, thread struc-
ture, and comment length) (Narayan and Cheshire,
2010; Baumer et al., 2008). We choose to encode
the position of the comments (ordinal) as opposed
to their timestamps (quantitative); since the exact
timestamp of a comment is less important to users
than its chronological position with respect to the
other comments (Baumer et al., 2008).
4.2 Text Analysis
Since most of the blog reading tasks we identi-
fied involved topics and sentiments expressed in
the conversation, we applied both topic modeling
and sentiment analysis on a given conversation.
In topic modeling, we group the sentences of a
blog conversation into a number of topical clusters
and label each cluster by assigning a short infor-
mative topic descriptor (i.e., a keyphrase). To find
the topical clusters and their associated labels, we
apply the topic segmentation and labeling models
recently proposed by (Joty et al., 2013b) for asyn-
chronous conversations, and successfully evalu-
ated on email and blog datasets. More specifically,
for topic segmentation, we use their best unsu-
pervised topic segmentation model LCSeg+FQG,
which extends the generic lexical cohesion based
topic segmenter (LCSeg) (Galley et al., 2003)
47
Figure 1: A snapshot of ConVis showing a blog conversation from Slashdot, where the user has hovered
the mouse over a topic element (?major army security?) that highlights the connecting visual links, brush-
ing the related authors(right), and providing visual prominence to the related comments in the Thread
Overview (middle).
to consider a fine-grain conversational structure
of the conversation, i.e., the Fragment Quotation
Graph (FQG) (Carenini et al., 2007). The FQG
captures the reply relations between text frag-
ments, which are extracted by analyzing the actual
body of the comments, thus provides a finer rep-
resentation of the conversation than the reply-to
structure. Similarly, the topic labels are found by
using their best unsupervised graph-based rank-
ing model (i.e., BiasedCorank) that extracts rep-
resentative keyphrases for each topical segment
by combining informative clues from initial sen-
tences of the segment and the fine-grain conversa-
tional structure, i.e., the FQG.
For sentiment analysis, we apply the Seman-
tic Orientation CALculator (SO-CAL) (Taboada
et al., 2011), which is a lexicon-based approach
(i.e., unsupervised) for determining sentiment of
a text. Its performance is consistent across vari-
ous domains and on completely unseen data, thus
making a suitable tool for our purpose. We define
five different polarity intervals (-2 to +2), and for
each comment we count how many sentences fall
in any of these polarity intervals to compute the
polarity distribution for that comment.
While designing and implementing ConVis, we
have been mainly working with blog conversations
from two different sources: Slashdot
1
?a technol-
ogy related blog site, and Daily Kos
2
? a political
analysis blog site.
1
http://slashdot.org
2
http://www.dailykos.com
4.3 Designing Interactive Visualization
Upon identifying the tasks and data variables, we
design the visual encoding and user interactions.
Figure 1 shows an initial prototype of ConVis.
3
It is designed as an overview + details interface,
since it has been found to be more effective for
text comprehension tasks than other approaches
such as zooming and focus+context (Cockburn et
al., 2008). The overview consists of what was dis-
cussed by whom (i.e., topics and authors) and a
visual summary of the whole conversation (i.e.,
the Thread Overview), while the detailed view
represents the actual conversation. The Thread
Overview visually represents each comment of
the discussion as a horizontal stacked bar, where
each stacked bar encodes three different meta-
data (comment length, position of the comment
in the thread, and depth of the comment within
the thread). To express the sentiment distribution
within a comment, the number of sentences that
belong to a particular sentiment orientation is in-
dicated by the width of each cell within a stacked
bar. A set of five diverging colors was used to vi-
sualize this distribution in a perceptually meaning-
ful order, ranging from purple (highly negative) to
orange (highly positive). Thus, the distribution of
colors in the Thread Overview can help the user to
perceive the kind of conversation they are going to
deal with. For example, if the Thread Overview is
3
https://www.cs.ubc.ca/cs-research/lci/research-
groups/natural-language-processing/ConVis.html
48
mostly in strong purple color, then the conversa-
tion has many negative comments.
The primary facets of the conversations, namely
topics and authors are presented in a circular
layout around the Thread Overview. Both top-
ics and authors are positioned according to their
chronological order in the conversation starting
from the top, allowing the user to understand how
the conversation evolves as the discussion pro-
gresses. The font size of facet items helps the
user to quickly identify what are the mostly dis-
cussed themes and who are the most dominant
participants within a conversation. Finally, the
facet elements are connected to their correspond-
ing comments in the Thread Overview via subtle
curved links indicating topic-comment-author re-
lationships. While a common way to relate various
elements in multiple views is synchronized visual
highlighting, we choose visual links to connect
related entities. This was motivated by the find-
ings that users can locate visually linked elements
in complex visualizations more quickly and with
greater subjective satisfaction than plain highlight-
ing (Steinberger et al., 2011). Finally, the Conver-
sation View displays the actual text of the com-
ments in the discussion as a scrollable list. At
the left side of each comment, the following meta-
data are presented: title, author name, photo, and a
stacked bar representing the sentiment distribution
(mirrored from Thread Overview).
Exploring Conversations: ConVis sup-
ports multi-faceted exploration of conversations
through a set of lightweight interactions (Lam,
2008) that can be easily triggered without causing
drastic modifications to the visual encoding. The
user can explore interesting topics/ authors by
hovering the mouse on them, which highlights
the connecting curved links and related comments
in the Thread Overview (see Figure 1). As such,
one can quickly understand how multiple facet
elements are related, which is useful for the tasks
that require the user to interpret the relationships
between facets. If the reader becomes further
interested in specific topic/ author, she can
subsequently click on it, resulting in drawing a
thick vertical outline next to the corresponding
comments in the Thread Overview. Such outlines
are also mirrored in the Conversation View.
Moreover, the user can select multiple facet items
(for instance a topic and an author) to quickly
understand who said about what topics.
Besides exploring by the topics/ authors, the
reader can browse individual comments by hover-
ing and clicking on them in the Thread Overview,
that causes to highlight its topic and scrolling to
the relevant comment in the Conversation View.
Thus, the user can easily locate the comments that
belong to a particular topic and/or author. More-
over, the keyphrases of the relevant topic and sen-
timents are highlighted in the Conversation View
upon selection, providing more details on demand
about what makes a particular comment positive/
negative or how it is related to a particular topic.
5 Further Challenges and Directions
After implementing the prototype, we ran an infor-
mal evaluation (Lam et al., 2012) with five target
users (age range 18 to 24, 2 female) to evaluate
the higher levels of the nested model (Munzner,
2009), where the aim was to collect anecdotal ev-
idence that the system met its design goals. The
participants? feedback from our evaluation sug-
gests that ConVis can help the user to identify
the topics and opinions expressed in the conver-
sation; supporting the user in exploring comments
of interest, even if they are buried near the end of
the thread. We also identified further challenges
from the observations and participants feedback.
Based on our experience and literature review, we
provide potential directions to address these chal-
lenges as we describe below.
5.1 Human in the Loop: Interactive Topic
Revision
Although the topic modeling method we applied
enhances the accuracy over traditional methods
for non-conversational text, the informal evalua-
tion reveals that still the extracted topics may not
always match user?s information need. In some
cases, the results of topic modeling can mismatch
with the reference set of topics/ concepts described
by human (Chuang et al., 2013). Even the in-
terpretations of topics can vary among people ac-
cording to expertise and the current task in hand.
In fact, during topic annotations by human experts,
there was considerable disagreement on the num-
ber of topics and on the assignment of sentences
to topic clusters (Joty et al., 2013b). Depending
on user?s mental model and current tasks, the topic
modeling results may require to be more specific
in some cases, and more generic in other cases. As
such, the topic model needs to be revised based
49
on user feedback to better support her analysis
tasks. Thus, our goal is to support a human-in-
the-loop topic modeling for asynchronous conver-
sations via interactive visualization.
There have been some recent works for incorpo-
rating user supervision in probabilistic topic mod-
els (e.g., Latent Dirichlet Allocation (LDA)) by
adding constraints in the form of must-link and
cannot-link (Andrzejewski et al., 2009; Hu et al.,
2011), or in the form of a one-to-one mapping be-
tween LDA?s latent topics and user tags (Ramage
et al., 2009). The feedback from users has been
also integrated through visualizations, that steers a
semi-supervised topic model (Choo et al., 2013).
In contrast to the above-mentioned methods that
are designed for generic documents, we are fo-
cusing on how our topic modeling approach that
is specific to asynchronous conversations, can be
steered by the end-users. We are planning to com-
bine a visual interface for expressing the user?s in-
tention via a set of actions, and a semi-supervised
version of the topic model that can be iteratively
refined from such user actions.
A set of possible topic revision operations are
shown in Figure 2. Splitting a topic into further
sub-topics can be useful when the user wants to
explore the conversation at a finer-topic granular-
ity (Figure 2(a)). A merging operation serves the
opposite purpose, i.e., when the user wants to ana-
lyze the conversation at a coarser topic granularity
(Figure 2(b)). Together, these two operations are
intended to help the user in dynamically changing
the granularity levels of different topics.
Since each topic is currently represented by a
set of keyphrases, they can also be effectively
used to revise the topic model. Consider an ex-
ample, where the sentences related to two dif-
ferent keyphrases, namely ?Obama health policy?
and ?job recession? are grouped together under the
same topic. The user may realize that the sen-
tences related to ?job recession? should have been
separated from its original topic into a new one
(Figure 2(c)). Finally, topic assignment modifi-
cation can be performed, when the domain ex-
pert believes that a group of sentences are wrongly
grouped/clustered (Figure 2(d)) by the system.
In order to design the interactive visualization
and algorithms for incorporating user feedback, a
number of open questions need to be answered.
Some of these questions are related to the user re-
quirement analysis of the problem domain, e.g.,
(a) Split (b) Merge
(c) Create topic by a
keyphrase
(d) Topic assignment
modification
Figure 2: Four different possible user actions for
topic revision
what are the tasks for exploring asynchronous con-
versation that require the introduction of user feed-
back to refine the topic model? What data should
be shown to the user to help her decide what topic
refinement actions are appropriate?
In terms of designing the set of interaction tech-
niques, the aim is to define a minimum set of
model refinement operations, and allowing the
user to express these operations from the visual
interface in a way that enhances the ability to pro-
vide feedback. A domain expert could possibly
express these operations through the direct manip-
ulation method (e.g., dragging a topic node over
another). A related open question is: how can we
minimize the cognitive load associated with inter-
preting the modeling results and deciding the next
round of topic revision operations?
From the algorithmic perspective, the most cru-
cial challenge seems to be devising an efficient
semi-supervised method in the current graph-
based topic segmentation and labeling framework
(Joty et al., 2013b). It needs to be fast enough to
respond to the user refinement actions and update
results in an acceptable period of time. In addition,
determining the number of topics is a challenging
problem when running the initial model and when
splitting a topic further.
5.2 Coupling Advanced NLP Methods with
Interactive Visualizations
In light of the informal evaluation, we also investi-
gate how current NLP methods are supporting the
tasks we identified and what additional methods
could be incorporated? For example, one of the
crucial data variable in most of the tasks is opin-
ion. However, during the evaluation two users did
50
not find the current sentiment analysis sufficient
enough in revealing whether a comment is sup-
porting/ opposing a preceding one. It seems that
opinion seeking tasks (e.g., ?why people were sup-
porting or opposing an opinion??) would require
the reader to know the argumentation flow within
the conversation, namely the rhetorical structure
of each comment (Joty et al., 2013a) and how
these structures are linked to each other.
An early work (Yee and Hearst, 2005) at-
tempted to organize the comments using a tree-
map like layout, where the parent comment is
placed on top as a text block and the space below
the parent node is divided between supporting and
opposing statements. We plan to follow this idea
in ConVis, but incorporating a higher level dis-
course relation analysis of the conversations and
detecting controversial topics.
Incorporating additional complex text analysis
results into the visualization may require us to re-
visit some of the higher levels of the nested model,
i.e., data abstraction and visual encoding. It may
impose further tradeoffs for visual encoding; for
instance how can we visually represent the argu-
mentation structure within a conversation? How
can we represent such structure, while preserv-
ing the data already found to be useful such as
topic and thread structure? How can we represent
that a topic is controversial? Besides text analysis
results, some additional facets can become more
useful to the participants (e.g., moderation scores,
named entities), while an existing facet being less
useful. In such cases, allowing the user to dynam-
ically change the facets of interest can be useful.
5.3 Evaluation in the Wild
While controlled experiments allow us to mea-
sure the user performance on specific tasks for the
given interface, they may not accurately capture
real world uses scenario (Lam et al., 2012). In this
context, an ecologically valid evaluation of Con-
Vis would be to allow the users to use the system
to read their own conversations of interest over an
extended period of time. Such longitudinal study
would provide valuable insights regarding the util-
ity of the interface.
Evaluating the topic refinement approach for
asynchronous conversation can be even more chal-
lenging. An initial approach could be to formu-
late some quantitative evaluation metrics, that help
us understand whether the iterative feedback from
the user would improve the resultant topic model
in terms of agreement with the reference set of
topics described by human annotators. However,
such approach would not capture the subjective
differences of the users in interpreting the topic
model. It would be more interesting to see, how
much users would actually care about providing
the feedback to refine the model in a real world
scenario? What refinement operations would be
performed more often? Would these operations
eventually support the user to perform some anal-
ysis tasks more effectively?
6 Conclusions
Understanding the user behaviours, needs, and re-
quirements in the target domain is critical in ef-
fectively combining NLP and InfoVis techniques.
In this paper, we apply a visualization design
method (Munzner, 2009) to identify what infor-
mation should be mined from the conversation as
well as how the visual encoding and interaction
techniques should be designed. We claim that the
NLP and the InfoVis components of the resulting
system, ConVis, are more consistent and better in-
tegrated, because they refer to a common set of
task and data abstractions. In future work, we aim
to explore a set of open challenges that were moti-
vated by an initial informal evaluation of ConVis.
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via dirichlet forest priors. In Proc. Conf.
on Machine Learning, pages 25?32.
Eric Baumer, Mark Sueyoshi, and Bill Tomlinson.
2008. Exploring the role of the reader in the activity
of blogging. In Proc. of CHI, pages 1111?1120.
G. Carenini, R. T. Ng, and X. Zhou. 2007. Summariz-
ing Email Conversations with Clue Words. In Proc.
conf. on World Wide Web, pages 91?100.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text
Conversations. Morgan Claypool.
Jaegul Choo, Changhyun Lee, Chandan K Reddy, and
Haesun Park. 2013. Utopian: User-driven topic
modeling based on interactive nonnegative matrix
factorization. IEEE Trans. Visualization & Comp.
Graphics, 19(12):1992?2001.
Jason Chuang, Sonal Gupta, Christopher Manning, and
Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In
Proc. Conf. on Machine Learning, pages 612?620.
51
Andy Cockburn, Amy Karlson, and Benjamin B Bed-
erson. 2008. A review of overview+ detail, zoom-
ing, and focus+ context interfaces. ACM Computing
Surveys (CSUR), 41(1):2.
Kushal Dave, Martin Wattenberg, and Michael Muller.
2004. Flash forums and forumreader: navigating a
new kind of large-scale online discussion. In Proc.
ACM Conf. on CSCW, pages 232?241.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proc. of
ACL, pages 562?569.
Enamul Hoque and Giuseppe Carenini. 2014. ConVis:
A visual text analytic system for exploring blog con-
versations. (Computer Graphic Forum (to appear)).
Yuening Hu, Jordan Boyd-Graber, and Brianna Sati-
noff. 2011. Interactive topic modeling. In Proc. of
ACL.
Quentin Jones, Gilad Ravid, and Sheizaf Rafaeli. 2004.
Information overload and the message dynamics
of online interaction spaces: A theoretical model
and empirical exploration. Information Systems Re-
search, 15(2):194?210.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013a. Combining intra-and
multi-sentential rhetorical parsing for document-
level discourse analysis. In Proc. of ACL.
Shafiq Joty, Giuseppe Carenini, and Raymond T Ng.
2013b. Topic segmentation and labeling in asyn-
chronous conversations. Journal of Artificial Intelli-
gence Research, 47:521?573.
B. K. Kaye. 2005. Web side story: An exploratory
study of why weblog users say they use weblogs.
AEJMC Annual Conf.
Bernard Kerr. 2003. Thread arcs: An email thread
visualization. In IEEE Symposium on Information
Visualization, pages 211?218.
H. Lam, E. Bertini, P. Isenberg, C. Plaisant, and
S. Carpendale. 2012. Empirical studies in infor-
mation visualization: Seven scenarios. IEEE Trans.
Visualization & Comp. Graphics, 18(9):1520?1536.
Heidi Lam. 2008. A framework of interaction costs
in information visualization. IEEE Trans. Visualiza-
tion & Comp. Graphics, 14(6):1149?1156.
Gilad Mishne. 2006. Information access challenges in
the blogspace. In Workshop on Intelligent Informa-
tion Access (IIIA).
Sean A Munson and Paul Resnick. 2010. Presenting
diverse political opinions: how and how much. In
Proc. of CHI, pages 1457?1466.
Tamara Munzner. 2009. A nested model for visualiza-
tion design and validation. IEEE Trans. Visualiza-
tion & Comp. Graphics, 15(6):921?928.
S. Narayan and C. Cheshire. 2010. Not too long to
read: The tldr interface for exploring and navigating
large-scale discussion spaces. In Hawaii Conf. on
System Sciences (HICSS), pages 1?10.
V?ctor Pascual-Cid and Andreas Kaltenbrunner. 2009.
Exploring asynchronous online discussions through
hierarchical visualisation. In IEEE Conf. on Infor-
mation Visualization, pages 191?196.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D Manning. 2009. Labeled LDA: A su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proc. of EMNLP, pages 248?
256.
Warren Sack. 2000. Conversation map: an interface
for very-large-scale conversations. Journal of Man-
agement Information Systems, 17(3):73?92.
Michael Sedlmair, Miriah Meyer, and Tamara Mun-
zner. 2012. Design study methodology: reflections
from the trenches and the stacks. IEEE Trans. Visu-
alization & Comp. Graphics, 18(12):2431?2440.
Param Vir Singh, Nachiketa Sahoo, and Tridas
Mukhopadhyay. 2010. Seeking variety: A dynamic
model of employee blog reading behavior. Available
at SSRN 1617405.
Markus Steinberger, Manuela Waldner, Marc Streit,
Alexander Lex, and Dieter Schmalstieg. 2011.
Context-preserving visual links. IEEE Trans. Visu-
alization & Comp. Graphics, 17(12):2249?2258.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Gina Danielle Venolia and Carman Neustaedter. 2003.
Understanding sequence and reply relationships
within email conversations: a mixed-model visual-
ization. In Proc. of CHI, pages 361?368.
Fernanda B Vi?egas, Scott Golder, and Judith Donath.
2006. Visualizing email content: portraying rela-
tionships from conversational histories. In Proc. of
CHI, pages 979?988.
Martin Wattenberg and David Millen. 2003. Conver-
sation thumbnails for large-scale discussions. In ex-
tended abstracts on CHI, pages 742?743.
Furu Wei, Shixia Liu, Yangqiu Song, Shimei Pan,
Michelle X Zhou, Weihong Qian, Lei Shi, Li Tan,
and Qiang Zhang. 2010. Tiara: a visual exploratory
text analytic system. In Proc. ACM Conf. on Knowl-
edge Discovery and Data Mining, pages 153?162.
Ka-Ping Yee and Marti Hearst. 2005. Content-
centered discussion mapping. Online Deliberation
2005/DIAC-2005.
Aaron Robert Zinman. 2011. Me, myself, and my hy-
perego: understanding people through the aggrega-
tion of their digital footprints. Ph.D. thesis, MIT.
52
Proceedings of the SIGDIAL 2014 Conference, pages 133?140,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
Extractive Summarization and Dialogue Act Modeling on Email 
Threads: An Integrated Probabilistic Approach 
 
 
 
Tatsuro Oya and Giuseppe Carenini 
Department of Computer Science 
University of British Columbia 
Vancouver, B.C. Canada 
{toya, carenini}@cs.ubc.ca 
 
 
Abstract 
In this paper, we present a novel supervised 
approach to the problem of summarizing 
email conversations and modeling dialogue 
acts. We assume that there is a relationship 
between dialogue acts and important sen-
tences. Based on this assumption, we intro-
duce a sequential graphical model approach 
which simultaneously summarizes email 
conversation and models dialogue acts. We 
compare our model with sequential and 
non-sequential models, which independent-
ly conduct the tasks of extractive summari-
zation and dialogue act modeling. An 
empirical evaluation shows that our ap-
proach significantly outperforms all base-
lines in classifying correct summary 
sentences without losing performance on 
dialogue act modeling task.  
1 Introduction 
Nowadays, an overwhelming amount of text in-
formation can be found on the web. Most of this 
information is redundant and thus the task of 
document summarization has attracted much at-
tention. Since emails in particular are used for a 
wide variety of purposes, the process of automat-
ically summarizing emails might be of great 
benefit in dealing with this excessive amount of 
information. Much work has already been con-
ducted on email summarization. The first re-
search on this topic was conducted by Rambow 
et al. (2004), who took a supervised learning ap-
proach to extracting important sentences. A 
study on the supervised summarization of email 
threads was also performed by Ulrich et al. 
(2009). This study used the regression-based 
method for classification. There have been stud-
ies on unsupervised summarization of email 
threads as well. Zhou et al. (2007, 2008) pro-
posed a graph-based unsupervised approach to 
email conversation summarization using clue 
words, i.e., recurring words contained in replies. 
In addition, the task of labeling sentences 
with dialogue acts has become important and has 
been employed in many conversation analysis 
systems. For example, applications such as meet-
ing summarization and collaborative task learn-
ing agents use dialogue acts as their underlying 
structure (Allen et al., 2007; Murray et al., 
2010). In a previous work, Cohen et al. (2004) 
defined a set of ?email acts? and employed text 
classification methods to detect these acts in 
emails. Later, Carvalho et al. (2006) employed a 
combination of n-gram sequences as features and 
then used a supervised machine learning method 
to improve the accuracy of this email act classifi-
cation. In addition, Shafiq et al. (2011) presented 
unsupervised dialogue act labeling methods. In 
their work, they introduced a graph-based meth-
od and two probabilistic sequence-labeling 
methods for modeling dialogue acts. 
However, little work has been done on dis-
covering the relationship between dialogue acts 
and extractive summaries. If there is a relation-
ship between them, combining these approaches 
so as to model both simultaneously will yield 
better results. In this paper, we investigate this 
hypothesis by introducing a new sequential 
graphical model approach that performs dialogue 
act modeling and extractive summarization joint-
ly on email threads.   
2 Related Work 
While email summarization and dialogue act 
modeling have been effectively studied, in most 
previous work, these tasks were studied inde-
pendently. This section provides related work for 
each task separately. 
133
2.1 Extractive Summarization 
Rambow et al. (2004) introduced sentence ex-
traction techniques that work for email threads. 
In their work, they introduced email-specific fea-
tures and used a machine learning method to 
classify whether or not a sentence should be in-
corporated into a summary. Their experiments 
demonstrated that their features were highly ef-
fective for email summarization. 
Ulrich et al. (2009) proposed a regression-
based machine learning approaches to email 
thread summarization. They compared regres-
sion-based classifiers to binary classifiers and 
showed that their approach significantly im-
proves the summarization accuracy. They em-
ployed the feature set introduced by Rambow et 
al. (2004) as their baseline and introduced new 
features that are also effective for email summa-
rization. Some of their features refer to dialogue 
acts but the assumption is that they are computed 
before the summarization task is performed. Our 
work is aimed at a much closer integration of the 
two tasks by modeling them simultaneously. 
Carenini et al. (2007) developed a fragment 
quotation graph that can capture a fine-grain 
conversation structure in email threads, which 
we will describe in detail in Section 3. They then 
introduced a ClueWordSummarizer (CWS), a 
graph-based unsupervised summarization ap-
proach based on the concept of clue words, 
which are recurring words found in email replies. 
Their experiment showed that the CWS performs 
better than the email summarization approach in 
Rambow et al. (2004).  
Extractive summarization using a sequential 
labeling technique has also been studied. While 
this is not an email summarization, Shen et al. 
(2007) proposed a linear-chain Conditional Ran-
dom Field (CRF) based approach for extractive 
document summarization. In their work, they 
treated the summarization task as a sequence la-
beling problem to take advantage of interaction 
relationships between sentences; their approach 
showed significant improvement when compared 
with non-sequential classifiers. 
2.2 Dialogue Act Modeling 
The first studies on the dialogue act modeling in 
emails were performed by Cohen et al. (2004). 
They defined ?email speech acts? (e.g., Request, 
Deliver, Propose, and Commit) and used ma-
chine learning methods to classify emails accord-
ing to the intent of the sender.  
Carvalho et al. (2006) further developed this 
initial proposal by using contextual information 
such as combinations of n-gram sequences in 
emails as their features for a supervised learning 
approach. The experiment showed that their ap-
proach reduced classification error rates by 
26.4%. Shafiq et al. (2011) proposed unsuper-
vised dialogue act modeling in email threads and 
on forums.  They introduced a graph-based and 
two probabilistic unsupervised approaches for 
modeling dialogue acts. By comparing those ap-
proaches, they demonstrated that the probabilis-
tic approaches were quite effective and 
performed better than the graph-based one. 
While the following work is not done on the 
email domain, Kim et al. (2010) introduced a 
dialogue act classification on one-on-one online 
chat forums. To be able to capture sequential 
dialogue act dependency on chats, they applied a 
CRF model. They demonstrated that, compared 
with other classifiers, their CRF model per-
formed the best. In their later work (Kim et al., 
2012), they extended the domain to multi-party 
live chats and proposed new features for that 
domain. 
3 Capturing Conversation Structure in 
Email Threads  
In this section, we describe how to build a frag-
ment quotation graph which captures the conver-
sation structure of any email thread at finer 
granularity. This graph was developed and 
shown to be effective by Carenini et al. (2011). 
A key assumption of this approach is that in or-
der to effectively perform summarization and 
dialogue act modeling, a fine graph representa-
tion of the underlying conversation structure is 
needed. 
Here, we start with the sample email conver-
sation shown in Figure 1 (a).  For convenience, 
the content of the emails is represented as a se-
quence of fragments.  
First, we identify all new and quoted frag-
ments. For example, email E1 is composed of 
one new fragment, ?b?, and one quoted fragment, 
?a?.  As for email E3, since we do not yet know 
whether or not ?d? and ?e? are different frag-
ments, we consider E3 as being composed of one 
new fragment, ?de? and one quoted fragment, ?b?.  
Second, we identify distinct fragments. To do 
this, we first identify overlaps by comparing 
fragments with each other. If necessary, we split 
the fragments and remove any duplicates from 
them.  For example, a fragment, ?de?, in E3 is 
134
split into ?d? and ?e? after being compared with 
fragments in E4 and the duplicates are removed. 
By applying this process to all of the emails, 
seven distinct fragments, a, b ..., and, g remain in 
this example. 
In the third step, edges which represent the 
replying relationships among the fragments are 
created. These edges are determined based on the 
assumption that any fragment is a reply to neigh-
boring quotations (the quoted fragments immedi-
ately preceding or following the current one). For 
example, the neighboring nodes of ?f? in E4 are 
?d? and ?e?. Thus, we create two edges from node 
?f? in E4 to node ?d? and ?e? in E3.  In the same 
way, we see that the neighboring node of ?g? in 
E4 is ?e?. Hence, there is one edge from node ?g? 
to ?e?.  If no quotation is contained in a reply 
email, we connect the fragments in the email to 
fragments in emails to which it reply.   
In email threads, there are cases in which the 
original email with its quotations is missing from 
the user?s folder, as in the case of ?a? in Figure 1 
(a). These types of emails are called hidden 
emails. Carenini et al. (2005) studied in detail 
how these email types might be treated and their 
influence on email summarization. 
Figure 1 (b) shows the completed fragment 
quotation graph of the email thread shown in 
Figure 1 (a). In the fragment quotation graph 
structure, all paths (e.g., a-b-c, a-b-d-f, a-b-e-f, 
and a-b-e-g in Figure 1 (b)) capture the adjacent 
relationships between email fragments. Hence, 
we use every path that can be derived from the 
graph as our dataset. However, in this case, when 
we run the labeling task on these paths, we ob-
tain multiple labels for some of the sentences 
because the sentences in fragments such as ?a?, 
?b?, and ?f? in Figure 1 (b) are shared among 
multiple paths. Therefore, to assign a label to one 
of these sentences, we take the label more fre-
quently assigned to that sentence when all its 
paths are considered (i.e., the majority vote). 
4 Features 
For both dialogue act modeling and extractive 
summarization, many effective sentence features 
have been discovered so far. Interestingly, some 
common features are shown to be effective in 
both tasks. This section explains the features 
used in our model. We begin with the features 
for extractive summarization and then describe 
how we derive the features for dialogue act mod-
eling. All the features explained in this section, 
whether they belong to extractive summarization 
or dialogue act modeling, are included in our 
model. 
 
(a) A possible configuration of an email conversation 
(E2 and E3 reply to E1, and E4 replies to E3) 
 
(b) An example of a fragment quotation graph 
Figure 1: A fragment quotation graph derived from a 
possible configuration of an email conversation 
4.1 Extractive Summarization Features  
The features we use for extractive summarization 
are mostly from Carenini et al. (2008) and Ram-
bow et al. (2004) and have proven to be effective 
on conversational data. Details of these features 
are described below. Note that all sentences in an 
email thread are ordered based on paths derived 
from a fragment quotation graph. 
 
Length Feature: The number of words in 
each sentence. 
Relative Position Feature: The number of 
sentences preceding the current divided by 
the total number of sentences in one path. 
Thread Name Overlaps Feature: The num-
ber of overlaps of the content words between 
the email thread title and a sentence. 
Subject Name Overlaps Feature: The num-
ber of overlaps of the content words between 
the subject of the email and a sentence. 
Question Feature: A binary feature that in-
dicates whether or not a sentence has a ques-
tion mark. 
CC Feature: A binary feature that indicates 
whether or not an email contains CC. 
135
Participation Dominance Feature: The 
number of utterances each person makes in 
one path. 
 
Finally, we also include a simplified version of 
the ClueWordScore (CWS) developed by 
Carenini et al. (2007), which is listed below.   
 
Simplified CWS Feature: The number of 
overlaps of the content words that occur in 
both the current and adjacent sentences in the 
path, ignoring stopwords. 
4.2 Dialogue Act Features  
The relative positions and length features have 
proven to be beneficial to both tasks (Jeong et al., 
2009; Carenini et al., 2008). Hence, these are 
categorized as both dialogue acts and extractive 
summarization features. In addition, we use word 
and POS n-grams as our features for dialogue act 
modeling. These features are extracted by the 
following process explained in Carvalho et al. 
(2006).  However, we extend the original ap-
proach in order to further abstract n-gram fea-
tures to avoid making them too sparse to be 
effective. In this section, we describe the deriva-
tion process in detail. 
A multi-step approach is used to generate 
word n-gram features. First, all words are tagged 
with the named entity using the Stanford Named 
Entity Recognizer (Finkel et al., 2005), and are 
then replaced with these tags. Second, a se-
quence of word-replacement tasks is applied to 
all email messages. Initially, some types of punc-
tuation marks (e.g., <>()[];:. and ,) and extra 
spaces are removed. Then, shortened phrases 
such as ?I?m? and ?We?ll? are substituted for 
more formal versions such as ?I am? and ?We 
will?. Next, other replacement tasks are per-
formed. Some of them are described in Table1. 
In the third step, unigrams and bigrams are ex-
tracted. In this paper, unigrams and bigrams refer 
to all possible sequences of length one and two 
terms. After extracting all unigrams and bigrams 
for each dialogue act, we then compute Infor-
mation Gain Score (Forman, 2003) and select the 
n-grams whose scores are in the top five greatest 
on the training set. In this way, we can automati-
cally detect features that represent the character-
istics of each dialogue act. In addition to word n-
grams, we also include POS n-grams in our fea-
tures. In a similar way, we first tag each word in 
sentences with POS using the Stanford POS tag-
ger (Toutanova et al., 2003). Then, for each dia-
logue act, we extract bigrams and trigrams, all of 
which are scored by the Information Gain. Based 
on their scores, we select the POS bigram and 
trigram features whose scores are within the top 
five greatest. One example of word n-gram fea-
tures for a Question dialogue act selected by this 
derivation method is shown in Table 2. 
 
Pattern Replacement 
?why?,  ?where?,  ?who?,  ?what? ?when? [WWHH] 
nominative pronouns [I] 
objective pronouns [ME] 
'it',  'those',  'these',  'this',  'that' [IT] 
'will',  ?would',  'shall',  'should', 'must' [MODAL_STRONG] 
?can',  'could',  'may',  'might' [MODAL_WEAK] 
'do',  'does',  'did',  ?done' [DO] 
'is',  'was',  'were',  'are',  'been' 'be',  'am' [BE] 
 'after' , 'before',  'during' [AAAFTER] 
?Jack?, ?Wendy? [Personal_PRONOUN] 
?New York? [LOCATION] 
?Acme Corp.? [ORGANIZATION] 
Table 1: Some Preprocessing Replacement Pattern 
Word Unigram Word Bigram 
? [MODAL_STRONG] [I] 
anyone [IT] ? 
WWHH [DO] anyone 
deny [WWHH] [BE] 
[Personal _PRONOUN] [BE] [IT] 
Table 2: Sample word n-grams selected as the fea-
tures for Question dialogue act 
5 The Sequential Labeling Task 
We use a Dynamic Conditional Random Field 
(DCRF) (Sutton et al., 2004) for labeling tasks. 
A DCRF is a generalization of a linear-chain 
CRF which allows us to represent complex inter-
action between labels. To be more precise, it is a 
conditionally-trained undirected graphical model 
whose structure and parameters are repeated over 
a sequence. Hence, it is the most appropriate 
method for performing multiple labeling tasks on 
the same sequence. 
136
Our DCRF uses the graph structure shown in 
Figure 2 with one chain (the top X nodes) model-
ing extractive summary and the other (the middle 
Y nodes) modeling dialogue acts.  Each node in 
the observation sequence (the bottom Z nodes) 
corresponds to each sentence in a path of the 
fragment quotation graph of the email thread. As 
shown in Figure 2, the graph structure captures 
the relationship between extractive summaries 
and dialogue acts by connecting their nodes.  
We use Mallet1 (McCallum, 2002) to implement 
our DCRF model.  It uses l2-based regularization 
to avoid overfitting, and a limited BFGS fitting 
algorithm to learn the DCRF model parameters. 
Also, it uses tree-based reparameterization 
(Wainwright et al., 2002) to compute the poste-
rior marginal, or inference. 
 
Figure 2: The DCRF model used to create extractive 
summaries and model dialogue acts 
6 Empirical Evaluations 
6.1 Dataset Setup  
In our experiment, the publically available BC3 
corpus2 (Ulrich et al., 2008) is used for training 
and evaluation purposes. The corpus contains 
email threads from the World Wide Web Con-
sortium (W3C) mailing list.  It consists of 40 
threads with an average of five emails per thread. 
The corpus provides extractive summaries of 
each email thread, all of which were annotated 
by three annotators. Hence, we use sentences that 
are selected by more than one annotator as the 
gold standard summary for each conversation. 
In addition, all sentences in the 39 out of 40 
threads are annotated for dialogue act tags. The 
tagset consists of five general and 12 specific 
tags. All of these tags are based on Jeong et al. 
(2009). For our experiment, considering that our 
data is relatively small, we decide to use the 
coarser five tag set. The details are shown in Ta-
ble 3. 
                                                          
1 http://mallet.cs.umass.edu 
2 http://www.cs.ubc.ca/nest/lci/bc3.html 
Tag Description Relative Frequency (%) 
S Statement 73.8 
Q Question 7.92 
R Reply 5.23 
Su Suggestion 5.62 
M Miscellaneous 7.46 
Table 3: Dialogue act tag categories and their relative 
frequency in the BC3 corpus 
After removing quoted sentences and redundant 
information such as senders and addresses, 1300 
distinct sentences remain in the 39 email threads. 
The detailed content of the corpus is summarized 
in Table 4. 
 
 Total 
Dataset 
No. of Threads 39 
No. of Sentences 1300 
No. of Extractive Summary Sentences 521 
No. of S Sentences 959 
No. of Q Sentences 103 
No. of R Sentences 68 
No. of Su Sentences 73 
No. of M Sentences  97 
Table 4: Detailed content of the BC3 corpus 
6.2 Evaluation Metrics  
Here, we introduce evaluation metrics for our 
joint model of extractive summarization and dia-
logue act recognition.  
The CRF model has been shown to be the ef-
fective one in both dialogue act modeling and 
extractive summarization (Shen et al., 2007; Kim 
et al., 2010; Kim et al., 2012). Hence, for com-
parison, we implement two different CRFs, one 
for extractive summarization and the other for 
dialogue act modeling. When classifying extrac-
tive summaries using the CRF, we only use its 
extractive summarization features. Similarly, 
when modeling dialogue acts, we only use its 
dialogue act features. In addition, we also com-
137
pare our system with a non-sequential classifier, 
a support vector machine (SVM), with the same 
settings as those described above. For these im-
plementations, we use Mallet and SVM-light 
package3 (Joachims, 1999).  
In our experiment, we first measure separate-
ly the performance of extractive summarization 
and dialogue act modeling. The performance of 
extractive summarization is measured by its av-
eraged precision, recall, and F-measure. For dia-
logue acts, we report the averaged-micro and 
macro accuracies as well as the averaged accura-
cies of each dialogue act. 
Second, we evaluate the combined perfor-
mance of extractive summarization and dialogue 
act modeling tasks. In general, we are interested 
in the dialogue acts in summary sentences be-
cause they can be later used as input for other 
natural language processing applications such as 
automatic abstractive summarization (Murray et 
al., 2010). Therefore, we measure the perfor-
mance of our model with the following modified 
precision (Pre?), recall (Rec?), and F-measure 
(F?): 
 
     
{                                         }
{                                              }
 (1) 
 
     
{                                        }
{                            }
                       (2)  
 
   
           
         
                                                               (3) 
 
where a correctly classified sentence refers to a 
true summary sentence that is classified as such 
and whose dialogue acts are also correctly classi-
fied. 
6.3 Experiment Procedure  
For all cases, we run five sets of 10-fold cross 
validation to train and test the classifiers on a 
shuffled dataset and calculate the average of the 
results. For each cross validation run, we extract 
all features following the process described in 
Section 4 on the training set. When comparing 
these two baselines with our model, we report p-
values obtained from a student paired t-test on 
the results to determine their significance.   
 
 
 
 
                                                          
3 http://www.cs.cornell.edu/people/tj/svm_light 
6.4 Results 
The performances of extractive summarization 
and dialogue act modeling using the three meth-
ods are summarized in Table 5 and 6, respective-
ly. 
 
 DCRF CRF SVM 
F-measure 0.485 0.428 0.397 
t-test?s  p-value   0.00046 2.5E-07 
Precision 0.562 0.591 0.675 
Recall 0.457 0.370 0.308 
Table 5: A comparison of the extractive summariza-
tion performance of our DCRF model and the two 
baselines based on precision, recall, and F-measure 
 DCRF CRF SVM 
Micro Accuracy 0.785 0.779 0.775 
t-test?s p-value   0.116 0.036 
Macro Accuracy 0.516 0.516 0.304 
t-test?s p-value   0.950 5.2E-32 
S Accuracy 0.901 0.892 0.999 
Q Accuracy 0.832 0.809 0.465 
R Accuracy 0.580 0.575 0.05 
Su Accuracy 0.139 0.108 0.00 
M Accuracy 0.126 0.198 0.00 
Table 6: A comparison of the dialogue act modeling 
performance of our DCRF model and the two base-
lines based on averaged accuracies 
From Table 5, we observe that, in terms of 
extractive summarization results, our DCRF 
model significantly outperforms the two base-
lines. Noticeable improvements can be seen for 
the recall and F-measure. In terms of F-measure, 
compared with the CRF and SVM, our model 
improves by 5.7% and 8.8% respectively. The p-
values obtained from the t-test indicate that our 
results are statistically significantly different (p < 
0.05) from those of the two baselines.  
Regarding dialogue act modeling, the results 
are summarized in Table 6. While no improve-
ment is shown for the micro-averaged accuracy, 
our model and the CRF significantly outperform 
the SVM in terms of the macro-averaged accura-
138
cy. Both our model and the CRF consider the 
sequential structure of the conversation, which is 
not captured in the SVM model. Clearly, this 
indicates that the sequential models are effective 
in modeling dialogue acts due to their ability to 
capture the inter-utterance relations of conversa-
tions.  
Compared with the CRF, our DCRF model 
outperforms it in most cases except in classifying 
the ?M? dialogue act. However these improve-
ments are not significant as t-test of both macro 
and micro-averaged accuracies indicate that the 
differences are not statistically significant (p > 
0.05).   
Another item to be mentioned here is that the 
accuracies of classifying ?R?, ?Su? and ?M? dia-
logue acts are relatively low. This issue applies 
to all classifiers and is plausibly due to the small 
dataset. There are only 68, 73 and 97 sentences, 
respectively, out of 1300 that are labeled as ?R?, 
?Su? and ?M? in the BC3 corpus. Since our dia-
logue act classifiers rely heavily on n-gram fea-
tures, were the data small, these features would 
be too sparse to effectively represent the charac-
teristics of the dialogue acts. However, compared 
with the SVM results, our joint model and the 
CRF perform significantly better in classifying 
these dialogue acts. This also explains why the 
sequential model is preferable in dialogue act 
modeling. 
Note that despite the small dataset, all the 
classifiers are relatively accurate in classifying 
?Q?. This is because n-gram features selected for 
?Q? such as ??? and ?WWHH? are very specific to 
this dialogue act, which makes the task of ?Q? 
classification easier compared to those of others.   
Next, we discuss the result of the com-
bined performance. The performances of our 
model and the two baselines are summarized in 
Table 7. 
 
 DCRF CRF SVM 
F-measure? 0.352 0.324 0.292 
t-test?s  p-value  0.015 3.3E-05 
Precision? 0.407 0.450 0.501 
Recall? 0.335 0.280 0.227 
Table 7: A comparison of the overall performance of 
our DCRF model and the two baselines based on 
modified precision, recall and F-measure 
 
We see that our DCRF model significantly 
outperforms the two baselines. While our model 
yields the lowest Pre? of all, its Rec? is much 
greater than the other two baselines and this 
leads to its achieving the highest F?. Compared 
with the CRF and SVM, the F? obtained from 
our system improves by 2.8% and 6% respec-
tively. In addition, the p-values show that the 
results of our model are statistically significant 
(p < 0.05) compared with those of the two base-
lines. 
Overall, these experiments clearly indicate 
that our model is effective in classifying both 
dialogue acts and summary sentences. 
7 Conclusions and Future Work 
In this work, we have explored a new automated 
approach for extractive summarization and dia-
logue act modeling on email threads. In particu-
lar, we have presented a statistical approach for 
jointly modeling dialogue acts and extractive 
summarization in a single DCRF. The empirical 
results demonstrate that our approach outper-
forms the two baselines on the summarization 
task without loss of performance on the dialogue 
act modeling one. In the future, we would like to 
extend our approach by exploiting more effective 
features. We also plan to apply our approach to 
different domains possessing large dataset. 
Acknowledgements 
We are grateful to Yashar Mehdad, Raimond Ng, 
Maryam Tavafi and Shafiq Joty for their com-
ments and UBC LCI group and ICICS for finan-
cial support. 
References  
J. Allen, N. Chambers, G. Ferguson, L. Galescu, H. 
Jung, and W. Taysom. Plow: A collaborative task 
learning agent. In AAAI-07, pages 22?26, 2007. 
Giuseppe Carenini, Gabriel Murray, and Raymond 
Ng. 2011. Methods for Mining and Summarizing 
Text Conversations. Morgan Claypool. 
Giuseppe Carenini, Raymond Ng, and Xiaodong 
Zhou. 2005. Scalable discovery of hidden emails 
from large folders. In ACM SIGKDD?05, pages 
544?549. 
Giuseppe Carenini, Raymond Ng, and Xiaodong 
Zhou. 2008. Summarizing Emails with Conversa-
tional Cohesion and Subjectivity In proceeding 
46th Annual Meetint Assoc.for Computational Lin-
guistics, page 353-361. 
139
Giuseppe Carenini, Raymond Ng, and Xiaodong 
Zhou. 2007. Summarizing email conversations 
with clue words. 16th International World Wide 
Web Conference (ACM WWW?07). 
Vitor R. Carvalho and William W. Cohen. 2006. Im-
proving ?email speech acts? analysis via n-gram 
selection. In Proceedings of the HLT-NAACL 2006 
Workshop on Analyzing Conversations in Text and 
Speech, ACTS ?09, pages 35?41, Stroudsburg, PA, 
USA. Association for Computational Linguistics. 
William W. Cohen, Vitor R. Carvalho, and Tom M. 
Mitchell. 2004. Learning to classify email into 
?speech acts?. In Proceedings of Empirical Meth-
ods in Natural Language Processing, pages 309?
316, Barcelona, Spain, July. 
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Infor-
mation into Information Extraction Systems by 
Gibbs Sampling. In Proceedings of the 43nd An-
nual Meeting of the Association for Computational 
Linguistics (ACL 2005), pp. 363-370. 
George Forman. 2003. An extensive empirical study 
of feature selection metrics for text classification. 
The Journal of Machine Learning Research, 
3:1289?1305. 
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae 
Lee. 2009. Semi-supervised speech act recognition 
in emails and forums. In Proceedings of the 2009 
Conference on Empirical Methods in Natural Lan-
guage Processing. 
Thorsten Joachims. 1999 Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.), MIT-Press, 1999.  
Shafiq Joty, Giuseppe Carenini, and Lin, Chin-Yew 
Lin. 2011. Unsupervised Modeling of Dialog Acts 
in Asynchronous Conversations. In Proceedings of 
the twenty second International Joint Conference 
on Artificial Intelligence (IJCAI) 2011. Barcelona, 
Spain. 
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and 
Raymond Ng. 2009 Finding Topics in Emails: Is 
LDA enough? NIPS-2009 workshop on applica-
tions for topic models: text and beyond. Whistler, 
Canada. 
McCallum, A. Kachites, 2002. MALLET: A Machine 
Learning for Language Toolkit. 
http://mallet.cs.umass.edu. 
Su Nam Kim, Lawrence Cavedon, and Timothy 
Baldwin. 2010a. Classifying dialogue acts in 1-to-1 
live chats. In Proceedings of the 2010 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2010), pages 862?871, Boston, 
USA. 
Su Nam Kim, Lawrence Cavedon and Timothy Bald-
win (2012) Classifying Dialogue Acts in Multi-
party Live Chats, In Proceedings of the 26th Pacif-
ic Asia Conference on Language, Information and 
Computation (PACLIC 26), Bali, Indonesia, pp. 
463?472. 
Gabriel Murray and Giuseppe Carenini. 2008. Sum-
marizing Spoken and Written Conversations. Em-
pirical Methods in NLP (EMNLP 2008), Waikiki, 
Hawaii, 2008. 
Gabriel Murray and Giuseppe Carenini. 2010. Sum-
marizing Spoken and Written Conversations. Gen-
erating and Validating Abstracts of Meeting 
Conversations: a User study (INLG 2010), Dublin, 
Ireland, 2010. 
Gabriel Murray, Renals Steve, and Carletta Jean. 
2005a. Extrative summarization of meeting record-
ings. In Proceeding of Interspeech 2005, Lisbon, 
Portugal, pages 593-596. 
Owen Rambow, Lokesh Shrestha, John Chen, and 
Chirsty Lauridsen. 2004. Summarizing email 
threads. In Proceedings of HLTNAACL 2004. 
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and 
Zheng Chen. 2007. Document summarization us-
ing conditional random fields. In Proc. of IJCAI, 
volume 7, 2862?2867. 
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004.  Dynamic conditional 
random fields: Factorized probabilistic models for 
labeling and segmenting sequence data. In Proc. 
ICML. 
Maryam Tavafi, Yashar Mehdad, Shafiq Joty, 
Giuseppe Carenini and Raymond Ng. 2013. Dia-
logue Act Recognition in Synchronous and Asyn-
chronous Conversations. In Proceedings of the 
SIGDIAL 2013 Conference, pages 117?121, Metz, 
France. Association for Computational Linguistics. 
Kristina Toutanova, Dan Klein, Christopher Manning, 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of HLT-NAACL 2003, pp. 
252-259.  
Jan Ulrich, Giuseppe Carenini, Gabriel Murray, 
and Raymond T. Ng: Regression-Based Summari-
zation of Email Conversations. ICWSM 2009 
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini. 
2008. A publicly available annotated corpus for 
supervised email summarization. AAAI-2008 
EMAIL Workshop. 
Martin J. Wainwright, Tommi Jaakkola, and Alan S. 
Willsky. 2002. Treebased Reparameterization for 
Approximate Inference on Loopy Graphs. In Ad-
vances in Neural Information Processing Systems 
14, pages 1001 1008. MIT Press. 
140
Proceedings of the 8th International Natural Language Generation Conference, pages 45?53,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Template-based Abstractive Meeting Summarization: Leveraging 
Summary and Source Text Relationships 
 
Tatsuro Oya, Yashar Mehdad, Giuseppe Carenini, Raymond Ng 
Department of Computer Science 
University of British Columbia, Vancouver, Canada 
{toya, mehdad, carenini, rng}@cs.ubc.ca 
 
 
 
Abstract 
In this paper, we present an automatic 
abstractive summarization system of 
meeting conversations. Our system ex-
tends a novel multi-sentence fusion algo-
rithm in order to generate abstract tem-
plates. It also leverages the relationship 
between summaries and their source 
meeting transcripts to select the best 
templates for generating abstractive 
summaries of meetings. Our manual and 
automatic evaluation results demonstrate 
the success of our system in achieving 
higher scores both in readability and in-
formativeness. 
1. Introduction 
People spend a vast amount of time in meetings 
and these meetings play a prominent role in their 
lives. Consequently, study of automatic meeting 
summarization has been attracting peoples? atten-
tion as it can save a great deal of their time and 
increase their productivity. 
The most common approaches to automatic 
meeting summarization have been extractive. 
Since extractive approaches do not require natu-
ral language generation techniques, they are ar-
guably simpler to apply and have been extensive-
ly investigated. However, a user study conducted 
by Murray et al. (2010) indicates that users pre-
fer abstractive summaries to extractive ones. 
Thereafter, more attention has been paid to ab-
stractive meeting summarization systems (Me-
hdad et al.  2013; Murray et al. 2010; Wang and 
Cardie 2013). However, the approaches intro-
duced in previous studies create summaries by 
either heavily relying on annotated data or by 
fusing human utterances which may contain 
grammatical mistakes. In this paper, we address 
these issues by introducing a novel summariza-
tion approach that can create readable summaries 
with less need for annotated data. Our system 
first acquires templates from human-authored 
summaries using a clustering and multi-sentence 
fusion algorithm. It then takes a meeting tran-
script to be summarized, segments the transcript 
based on topics, and extracts important phrases 
from it. Finally, our system selects templates by 
referring to the relationship between human-
authored summaries and their sources and fills 
the templates with the phrases to create summar-
ies. 
The main contributions of this paper are: 1) 
The successful adaptation of a word graph algo-
rithm to generate templates from human-
authored summaries; 2) The implementation of a 
novel template selection algorithm that effective-
ly leverages the relationship between human-
authored summary sentences and their source 
transcripts; and 3) A comprehensive testing of 
our approach, comprising both automatic and 
manual evaluations. 
 We instantiate our framework on the AMI 
corpus (Carletta et al., 2005) and compare our 
summaries with those created from a state-of-
the-art systems. The evaluation results demon-
strate that our system successfully creates in-
formative and readable summaries. 
2. Related Work 
Several studies have been conducted on creating 
automatic abstractive meeting summarization 
systems. One of them includes the system pro-
posed by Mehdad et al., (2013). Their approach 
first clusters human utterances into communities 
(Murray et al., 2012) and then builds an entail-
ment graph over each of the latter in order to se-
lect the salient utterances. It then applies a se-
mantic word graph algorithm to them and creates 
abstractive summaries. Their results show some 
improvement in creating informative summaries. 
45
However, since they create these summaries by 
merging human utterances, their summaries are 
still partially extractive.  
Recently, there have been some studies on 
creating abstract summaries of specific aspects of 
meetings such as decisions, actions and problems 
(Murray et al. 2010; Wang and Cardie, 2013). 
These summaries are called the Focused Meeting 
Summaries (Carenini et al., 2011). 
The system introduced by Murray et al. first 
classifies human utterances into specific aspects 
of meetings, e.g. decisions, problem, and action, 
and then maps them onto ontologies. It then se-
lects the most informative subsets from these on-
tologies and finally generates abstractive sum-
maries of them, utilizing a natural language gen-
eration tool, simpleNLG (Gatt and Reiter, 2009). 
Although their approach is essentially focused 
meeting summarization, after creating summaries 
of specific aspects, they aggregate them into one 
single summary covering the whole meeting. 
Wang and Cardie introduced a template-based 
focused abstractive meeting summarization sys-
tem. Their system first clusters human-authored 
summary sentences and applies a Multiple-
Sequence Alignment algorithm to them to gener-
ate templates. Then, given a meeting transcript to 
be summarized, it identifies a human utterance 
cluster describing a specific aspect and extracts 
all summary-worthy relation instances, i.e. indi-
cator-argument pairs, from it. Finally, the tem-
plates are filled with these relation instances and 
ranked accordingly, to generate summaries of a 
specific aspect of the meeting.  
Although the two approaches above are both 
successful in creating readable summaries, they 
rely on much annotated information, such as dia-
log act and sentiment types, and also require the 
accurate classification of human utterances that 
contain much noise and much ill-structured 
grammar. 
Our approach is inspired by the works intro-
duced here but improves on their shortcomings. 
Unlike those of Murray et al. (2010) and Wang 
and Cardie (2013), our system relies less on an-
notated training data and does not require a clas-
sifier. In addition, our evaluation indicates that 
our system can create summaries of the entire 
conversations that are more informative and 
readable than those of Mehdad et al.(2013). 
3. Framework 
In order for summaries to be readable and in-
formative, they should be grammatically correct 
and contain important information in meetings. 
To this end, we have created our framework con-
sisting of the following two components: 1) An 
off-line template generation module, which gen-
eralizes collected human-authored summaries 
and creates templates from them; and 2) An on-
line summary generation module, which seg-
ments meeting transcripts based on the topics 
discussed, extracts the important phrases from 
these segments, and generate abstractive sum-
maries of them by filling the phrases into the ap-
propriate templates. Figure 1 depicts our frame-
work. In the following sections, we describe each 
of the two components in detail. 
 
Figure 1: Our meeting summarization framework. Top: off-line Template generation module. Bottom: on-line 
Summary Generation module. 
46
3.1 Template Generation Module 
Our template generation module attempts to sat-
isfy two possibly conflicting objectives. First, 
templates should be quite specific such that they 
accept only the relevant fillers. Second, our 
module should generate generalized templates 
that can be used in many situations. We assume 
that the former is achieved by labeling phrases 
with their hypernyms that are not too general and 
the latter by merging related templates. Based on 
these assumptions, we divide our module into the 
three tasks: 1) Hypernym labeling; 2) Clustering; 
and 3) Template fusion. 
3.1.1 Hypernym Labeling 
Templates are derived from human-authored 
meeting summaries in the training data. We first 
collect sentences whose subjects are meeting par-
ticipant(s) and that contain active root verbs, 
from the summaries. This is achieved by utilizing 
meeting participant information provided in the 
corpus and parsing sentences with the Stanford 
Parser (Marneffe et al., 2006). The motivation 
behind this process is to collect sentences that are 
syntactically similar. We then identify all noun 
phrases in these sentences using the Illinois 
Chunker (Punyakanok and Roth, 2001). This 
chunker extracts all noun phrases as well as part 
of speech (POS) for all words. To add further in-
formation on each noun phrase, we label the right 
most nouns (the head nouns) in each phrase with 
their hypernyms using WordNet (Fellbaum, 
1998). In WordNet, hypernyms are organized in-
to hierarchies ranging from the most abstract to 
the most specific. For our work, we utilize the 
fourth most abstract hypernyms in light of the 
first goal discussed at the beginning of Section 
3.1, i.e. not too general. For disambiguating the 
sense of the nouns, we simply select the sense 
that has the highest frequency in WordNet.  
At this stage, all noun phrases in sentences 
are tagged with their hypernyms defined in 
WordNet, such as ?artifact.n.01?, and ?act.n.02?, 
where n?s stands for nouns and the two digit 
numbers represent their sense numbers. We treat 
these hypernym-labeled sentences as templates 
and the phrases as blanks. 
In addition, we also create two additional 
rules for tagging noun phrases: 1) Since the sub-
jects of all collected sentences are meeting par-
ticipant(s), we label all subject noun phrases as 
?speaker?; and 2) If the noun phrases consist of 
meeting specific terms such as ?the meeting? or 
?the group?, we do not convert them into blanks. 
These two rules guarantee the creation of tem-
plates suitable for meetings. 
 
Figure 2: Some examples of hypernym labeling task 
3.1.2 Clustering 
Next, we cluster the templates into similar 
groups. We utilize root verb information for this 
process assuming that these verbs such as ?dis-
cuss? and ?suggest? that appear in summaries are 
the most informative factors in describing meet-
ings. Therefore, after extracting root verbs in 
summary sentences, we create fully connected 
graphs where each node represents the root verbs 
and each edge represents a score denoting how 
similar the two word senses are. To measure the 
similarity of two verbs, we first identify the verb 
senses based on their frequency in WordNet and 
compute the similarity score based on the short-
est path that connects the senses in the hypernym 
taxonomy. We then convert the graph into a 
similarity matrix and apply a Normalized Cuts 
method (Shi and Malik, 2000) to cluster the root 
verbs. Finally, all templates are organized into 
the groups created by their root verbs. 
 
Figure 3: A word graph generated from related templates and the highest scored path (shown in bold) 
47
3.1.3 Template Fusion 
We further generalize the clustered templates by 
applying a word graph algorithm. The algorithm 
was originally proven to be effective in summa-
rizing a cluster of related sentences (Boudin and 
Morin, 2013; Filippova, 2010; Mehdad et al., 
2013). We extend it so that it can be applied to 
templates. 
Word Graph Construction  
In our system, a word graph is a directed graph 
with words or blanks serving as nodes and edges 
representing adjacency relations.  
Given a set of related templates in a group, 
the graph is constructed by first creating a start 
and end node, and then iteratively adding tem-
plates to it. When adding a new template, the al-
gorithm first checks each word in the template to 
see if it can be mapped onto existing nodes in the 
graph. The word is mapped onto a node if the 
node consists of the same word and the same 
POS tag, and no word from this template has 
been mapped onto this node yet. Then, it checks 
each blank in the template and maps it onto a 
node if the node consists of the same hypernym-
labeled blank and no blank from this template 
has been mapped onto this node yet.  
When more than one node refer to the same 
word or blank in the template, or when more than 
one word or blank in the template can be mapped 
to the same node in the graph, the algorithm 
checks the neighboring nodes in the current 
graph as well as the preceding and the subse-
quent words or blanks in the template. Then, 
those word-node or blank-node pairs with higher 
overlap in the context are selected for mapping. 
Otherwise, a new node is created and added to 
the graph. As a simplified illustration, we show a 
word graph in Figure 3 obtained from the follow-
ing four templates. 
  
? After introducing [situation.n.01], [speaker] then dis-
cussed [content.n.05] . 
? Before beginning [act.n.02] of [artifact.n.01], [speaker] 
discussed [act.n.02] and [content.n.05] for [arti-
fact.n.01] . 
? [speaker] discussed [content.n.05] of [artifact.n.01] and 
[material.n.01] . 
? [speaker] discussed [act.n.02] and [asset.n.01] in attract-
ing [living_thing.n.01] . 
Path Selection  
The word graph generates many paths connect-
ing its start and end nodes, not all of which are 
readable and cannot be used as templates. Our 
aim is to create concise and generalized tem-
plates. Therefore, we create the following rank-
ing strategy to be able to select the ideal paths. 
First, to filter ungrammatical or complex tem-
plates, the algorithm prunes away the paths hav-
ing more than three blanks; having subordinate 
clauses; containing no verb; having two consecu-
tive blanks; containing blanks which are not la-
beled by any hypernym; or whose length are 
shorter than three words. Note that these rules, 
which were defined based on close observation 
of the results obtained from our development set, 
greatly reduce the chance of selecting ill-
structured templates. Second, the remaining 
paths are reranked by 1) A normalized path 
weight and 2) A language model learned from 
hypernym-labeled human-authored summaries in 
our training data, each of which is described be-
low. 
1) Normalized Path Weight 
We adapt Filippova (2010)?s approach to com-
pute the edge weight. The formula is shown as: 
         
                  
?                 
                
    
where ei,j  is an edge that connects the nodes i 
and j in a graph, freq(i) is the number of words 
and blanks in the templates that are mapped to 
node i and diff(p,i,j) is the distance between the 
offset positions of nodes i and j in path p. This 
weight is defined so that the paths that are in-
formative and that contain salient (frequent) 
words are selected. To calculate a path score, 
W(p), all the edge weights on the path are 
summed and normalized by its length. 
2) Language Model 
Although the goal is to create concise templates, 
these templates must be grammatically correct. 
Hence, we train an n-gram language model using 
all templates generated from the training data in 
the hypernym labeling stage. Then for each path, 
we compute a sum of negative log probabilities 
of n-gram occurrences and normalize the score 
by its length, which is represented as H(p).  
The final score of each path is calculated as 
follows: 
                                   
where ? and ? are the coefficient factors which 
are tuned using our development set. For each 
48
group of clusters, the top ten best scored paths 
are selected as templates and added to its group. 
As an illustration, the path shown in bold in 
Figure 3 is the highest scored path obtained from 
this path ranking strategy.  
3.2 Summary Generation Module 
This section explains our summary generation 
module consisting of four tasks: 1) Topic seg-
mentation; 2) Phrase and speaker extraction; 3) 
Template selection and filling; and 4) Sentence 
ranking.  
3.2.1 Topic Segmentation 
It is important for a summary to cover all topics 
discussed in the meeting. Therefore, given a 
meeting transcript to be summarized, after re-
moving speech disfluencies such as ?uh?, and 
?ah?, we employ a topic segmenter, LCSeg (Gal-
ley et al., 2003) which create topic segments by 
observing word repetitions.  
One shortcoming of LCSeg is that it ignores 
speaker information when segmenting transcripts. 
Important topics are often discussed by one or 
two speakers. Therefore, in order to take ad-
vantage of the speaker information, we extend 
LCSeg by adding the following post-process 
step: If a topic segment contains more than 25 ut-
terances, we subdivide the segment based on the 
speakers. These subsegments are then compared 
with one another using cosine similarity, and if 
the similarity score is greater than that of the 
threshold (0.05), they are merged. The two num-
bers, i.e. 25 and 0.05, were selected based on the 
development set so that, when segmenting a tran-
script, the system can effectively take into ac-
count speaker information without creating too 
many segments. 
3.2.2 Phrase And Speaker Extraction 
All salient phrases are then extracted from each 
topic segment in the same manner as performed 
in the template generation module in Section 3.1, 
by: 1) Extracting all noun phrases; and 2) Label-
ing each phrase with the hypernym of its head 
noun. Furthermore, to be able to select salient 
phrases, these phrases are subsequently scored 
and ranked based on the sum of the frequency of 
each word in the segment. Finally, to handle re-
dundancy, we remove phrases that are subsets of 
others. 
In addition, for each utterance in the meeting, 
the transcript contains its speaker?s name. There-
fore, we extract the most dominant speakers? 
name(s) for each topic segment and label them as 
?speaker?. These phrases and this speaker infor-
mation will later be used in the template filling 
process. Table 1 below shows an example of 
dominant speakers and high scored phrases ex-
tracted from a topic segment. 
Dominant speakers 
Project Manager (speaker) 
Industrial Designer (speaker) 
High scored phrases (hypernyms) 
the whole look (appearance.n.01) 
the company logo (symbol.n.01) 
the product (artifact.n.01) 
the outside (region.n.01) 
electronics (content.n.05) 
the fashion (manner.n.01) 
Table 1: Dominant speakers and high scored 
phrases extracted from a topic segment 
3.2.3 Template Selection and Filling 
In terms of our training data, all human-authored 
abstractive summary sentences have links to the 
subsets of their source transcripts which support 
and convey the information in the abstractive 
sentences as illustrated in Figure 4. These subsets 
are called communities. Since each community is 
used to create one summary sentence, we hy-
pothesize that each community covers one spe-
cific topic.  
Thus, to find the best templates for each topic 
segment, we refer to our training data. In particu-
lar, we first find communities in the training set 
that are similar to the topic segment and identify 
the templates derived from the summary sen-
tences linked to these communities.   
 
Figure 4: A link from an abstractive summary sentence to a subset of a meeting transcript that conveys or sup-
ports the information in the abstractive sentence 
49
This process is done in two steps, by: 1) As-
sociating the communities in the training data 
with the groups containing templates that were 
created in our template generation module; and 
2) Finding templates for each topic segment by 
comparing the similarities between the segments 
and all sets of communities associated with the 
template groups. Below, we describe the two 
steps in detail. 
1) Recall that in the template generation 
module in Section 3.1, we label human-authored 
summary sentences in training data with hyper-
nyms and cluster them into similar groups. Thus, 
as shown in Figure 5, we first associate all sets of 
communities in the training data into these 
groups by determining to which groups the 
summary sentences linked by these communities 
belong. 
 
Figure 5: An example demonstrating how each com-
munity in training data is associated with a group con-
taining templates  
2) Next, for each topic segment, we compute 
average cosine similarity between the segment 
and all communities in all of the groups.  
 
Figure 6: Computing the average cosine similarities 
between a topic segment and all sets of com munities 
in each group 
At this stage, each community is already as-
sociated with a group that contains ranked tem-
plates. In addition, each segment has a list of av-
erage-scores that measures how similar the seg-
ment is to the communities in each group. Hence, 
the templates used for each segment are decided 
by selecting the ones from the groups with higher 
scores.  
Our system now contains for each segment a 
set of phrases and ideal templates, both of which 
are scored, as well as the most dominant speakers? 
name(s). Thus, candidate sentences are generated 
for each segment by: first, selecting speakers? 
name(s), then selecting phrases and templates 
based on their scores; and finally filling the tem-
plates with matching labels. Here, we limit the 
maximum number of sentences created for each 
topic segment to 30. This number is defined so 
that the system can avoid generating sentences 
consisting of low scored phrases and templates. 
Finally, these candidate sentences are passed to 
our sentence ranking module. 
3.2.4 Sentence Ranking 
Our system will create many candidate sentenc-
es, and most of them will be redundant. Hence, 
to be able to select the most fluent, informative 
and appropriate sentences, we create a sentence 
ranking model considering 1) Fluency, 2) Cover-
age, and 3) The characteristics of the meeting, 
each of which are summarized below: 
1) Fluency 
We estimate the fluency of the generated sen-
tences in the same manner as in Section 3.1.3. 
That is, we train a language model on human-
authored abstract summaries from the training 
portions of meeting data and then compute a 
normalized sum of negative log probabilities of 
n-gram occurrences in the sentence. The fluency 
score is represented as H(s) in the equation be-
low. 
2) Coverage 
To select sentences that cover important topics, 
we give special rewards to the sentences that 
contain the top five ranked phrases.   
3) The Characteristics of the Meeting 
We also add three additional scoring rules that 
are specific to the meeting summaries. In particu-
lar, these three rules are created based on phrases 
often used in the opening and closing of meet-
ings in a development set: 1) If sentences derived 
50
from the first segment contain the words ?open? 
or ?meeting?, they will be rewarded; 2) If sen-
tences derived from the last segment contain the 
words ?close? or ?meeting?, the sentences will 
again be rewarded; and 3)  If sentences not de-
rived  from the first or last segment contains the 
words ?open? or ?close?,  they will be penalized. 
The final ranking score of the candidate sen-
tences is computed using the follow formula: 
      s  ?  s  ? ?
i
 
i 1  i s  ?  i
 
i 1  i s     
where, Ri (s) is a binary that indicates whether 
the top i ranked phrase exists in sentence s; Mi (s) 
is also a binary that indicates whether the i th 
meeting specific rule can be met for sentence s; 
and ?, ? i and ? i are the coefficient factors to tune 
the ranking score, all of which are tuned using 
our development set. 
Finally, the sentence ranked the highest in 
each segment is selected as the summary sen-
tence, and the entire meeting summary is created 
by collecting these sentences and sorting them by 
the chronological order of the topic segments. 
4. Evaluation 
In this section, we describe an evaluation of our 
system. First, we describe the corpus data. Next, 
the results of the automatic and manual evalua-
tions of our system against various baseline ap-
proaches are discussed. 
4.1 Data 
For our meeting summarization experiments, we 
use manually transcripted meeting records and 
their human-authored summaries in the AMI 
corpus. The corpus contains 139 meeting records 
in which groups of four people play different 
roles in a fictitious team. We reserved 20 meet-
ings for development and implemented a three-
fold cross-validation using the remaining data.   
4.2 Automatic Evaluation 
We report the F1-measure of ROUGE-1, 
ROUGE-2 and ROUGE-SU4 (Lin and Hovy, 
2003) to assess the performance of our system. 
The scores of automatically generated summaries 
are calculated by comparing them with human-
authored ones.  
For our baselines, we use the system intro-
duced by Mehdad et al. (2013) (FUSION), which 
creates abstractive summaries from extracted 
sentences and was proven to be effective in cre-
ating abstractive meeting summaries; and Tex-
tRank (Mihalcea and Tarau, 2004), a graph based 
sentence ranker that is suitable for creating ex-
tractive summaries. Our system can create sum-
maries of any length by adjusting the number of 
segments to be created by LCSeg. Thus, we cre-
ate summaries of three different lengths (10, 15, 
and 20 topic segments) with the average number 
of words being 100, 137, and 173, respectively. 
These numbers generally corresponds to human-
authored summary length in the corpus which 
varies from 82 to 200 words.  
Table 2 shows the results of our system in 
comparison with those of the two baselines. The 
results show that our model significantly outper-
forms the two baselines. Compared with FU-
SION, our system with 20 segments achieves 
about 3 % of improvement in all ROUGE scores. 
This indicates that our system creates summaries 
that are more lexically similar to human-authored 
ones. Surprisingly, there was not a significant 
change in our ROUGE scores over the three dif-
ferent summary lengths. This indicates that our 
system can create summaries of any length with-
out losing its content. 
Models Rouge-1 Rouge-2 Rouge-SU4 
TextRank 21.7 2.5 6.5 
FUSION 27.9 4.0 8.1 
Our System 10 Seg. 28.4 6.7 10.1 
Our System 15 Seg. 30.6 6.8 10.9 
Our System 20 Seg. 31.5 6.7 11.4 
Table 2: An evaluation of summarization performance 
using the F1 measure of ROUGE-1 2, and SU4 
4.3 Manual Evaluation 
We also conduct manual evaluations utilizing a 
crowdsourcing tool1. In this experiment, our sys-
tem with 15 segments is compared with FUSION, 
human-authored summaries (ABS) and, human-
annotated extractive summaries (EXT).  
After randomly selecting 10 meetings, 10 par-
ticipants were selected for each meeting and giv-
en instructions to browse the transcription of the 
meeting so as to understand its gist. They were 
then asked to read all different types of summar-
ies described above and rate each of them on a 1-
5 scale for the following three items: 1) The 
summary?s overall quality, with ?5? being the 
best and ?1? being the worst possible quality; 2) 
The summary?s fluency, ignoring the capitaliza-
tion or punctuation, with ?5? indicating no 
grammatical mistakes and ?1? indicating too 
many; and 3) The summary?s informativeness, 
with ?5? indicating that the summary covers all 
meeting content and ?1? indicating that the 
                                                 
1 http://www.crowdflower.com/ 
51
summary does not cover the content at all.  
The results are described in Table 3. Overall, 
58 people worldwide, who are among the most 
reliable contributors accounting for 7 % of over-
all members and who maintain the highest levels 
of accuracy on test questions provided in pervi-
ous crowd sourcing jobs, participated in this rat-
ing task.  As to statistical significance, we use the 
2-tail pairwise t-test to compare our system with 
the other three approaches. The results are sum-
marized in Table 4. 
Models Quality Fluency Informativeness 
Our System  3.52 3.69 3.54 
ABS 3.96 4.03 3.87 
EXT 3.02 3.16 3.30 
FUSION 3.16 3.14 3.05 
Table 3: Average rating scores. 
Models  
Compared 
Quality 
(P-value) 
Fluency 
(P-value) 
Informativeness 
(P-value) 
Our System 
vs. ABS 
0.000162 0.000437 0.00211 
Our System 
vs. FUSION 
0.00142 0.0000135 0.000151 
Our System 
vs. EXT. 
0.000124 0.0000509 0.0621 
Table 4: T-test results of manual evaluation 
As expected, for all of the three items, ABS 
received the highest of all ratings, while our sys-
tem received the second highest. The t-test re-
sults indicate that the difference in the rating data 
is statistically significant for all cases except that 
of informativeness between ours and the extrac-
tive summaries. This can be understood because 
the extractive summaries were manually created 
by an annotator and contain all of the important 
information in the meetings. 
From this observation, we can conclude that 
users prefer our template-based summaries over 
human-annotated extractive summaries and ab-
stractive summaries created from extracted sali-
ent sentences. Furthermore, it demonstrates that 
our summaries are as informative as human-
annotated extractive ones. 
Finally, we show in Figure 7 one of the sum-
maries created by our system in line-with a hu-
man-authored one.  
5. Conclusion and Future Work 
In this paper, we have demonstrated a robust ab-
stractive meeting summarization system. Our ap-
proach makes three main contributions. First, we 
have proposed a novel approach for generating 
templates leveraging a multi-sentence fusion al-
gorithm and lexico-semantic information. Sec-
ond, we have introduced an effective template 
selection method, which utilize the relationship 
between human-authored summaries and their 
source transcripts. Finally, comprehensive evalu-
ation demonstrated that summaries created by 
our system are preferred over human-annotated 
extractive ones as well as those created from a 
state-of-the-art meeting summarization system.  
The current version of our system uses only 
hypernym information in WordNet to label 
phrases. Considering limited coverage in Word-
Net, future work includes extending our frame-
work by applying a more sophisticated labeling 
task utilizing a richer knowledge base (e.g., YA-
GO). Also, we plan to apply our framework to 
different multi-party conversational domains 
such as chat logs and forum discussions.  
Human-Authored Summary 
The project manager opened the meeting and had the 
team members introduce themselves and describe their 
roles in the upcoming project. The project manager then 
described the upcoming project. The team then discussed 
their experiences with remote controls. They also 
discussed the project budget and which features they 
would like to see in the remote control they are to create. 
The team discussed universal usage, how to find remotes 
when misplaced, shapes and colors, ball shaped remotes, 
marketing strategies, keyboards on remotes, and remote 
sizes. team then discussed various features to consider in 
making the remote. 
 
Summary Created by Our System with 15 Segment 
project manager summarized their role of the meeting .  
user interface expert and project manager talks about a 
universal remote . the group recommended using the 
International Remote Control Association rather than a 
remote control . project manager offered the ball 
idea .user interface expert suggested few buttons . user 
interface expert and industrial designer then asked a 
member about a nice idea for The idea . project manager 
went over a weak point . the group announced the one-
handed design . project manager and industrial designer 
went over their remote control idea . project manager 
instructed a member to research the ball function .  
industrial designer went over stability point .industrial 
designer went over definite points . 
Figure 7: A comparison between a human-authored 
summary and a summary created by our system 
Acknowledgements 
We would like to thank all members in UBC 
NLP group for their comments and UBC LCI 
group and ICICS for financial support. 
 References 
Florian Boudin and Emmanuel Morin. 2013. 
Keyphrase Extraction for N-best Reranking in Mul-
ti-Sentence Compression. In  Proceedings of the 
52
2013 Conference of the North American Chapter of 
the Association for Computational Linguistics: 
Human Language Technologies (NAACL-HLT 
2013), 2013. 
Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 
2011. Methods for Mining and Summarizing Text 
Conversations. Morgan Claypool. 
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guil-
lemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, 
M. Kronenthal, G. Lathoud, M. Lincoln, A. 
Lisowska, I. McCowan, W. Post, D. Reidsma, and 
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proceeding of MLMI 2005, Ed-
inburgh, UK, pages 28?39. 
Christiane Fellbaum 1998. WordNet, An Electronic 
Lexical Database. The MIT Press. Cambridge, MA. 
Katja Filippova. 2010. Multi-sentence compression: 
finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 322?
330, Stroudsburg, PA, USA. Association for Com-
putational Linguistic 
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics (ACL 2003). 562?
569. Sapporo, Japan. 
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a 
Realisation Engine for Practical Applications. In 
ENLG?09: Proceedings of the 12th European 
Workshop on Natural Language Generation, pages 
90?93, Morristown, NJ, USA. Association for 
Computational Linguistics. 
Ravi Kondadadi, Blake Howald and Frank Schilder. 
2013. A Statistical NLG Framework for Aggregat-
ed Planning and Realization. In Proceeding of the 
Annual Conferene for the Association of Computa-
tional Linguistic (ACL 2013).  
Marie-Catherine de Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation 
(LREC'06). 
Yashar Mehdad, Giuseppe Carenini, Frank Tompa. 
2013. Abstractive Meeting Summarization with En-
tailment and Fusion. In Proceedings of the 14th Eu-
ropean Natural Language Generation (ENLG - 
SIGGEN 2013), Sofia, Bulgaria. 
Rata Mihalcea and Paul Tarau 2004. TextRank: 
Bringing order into texts. In Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing, July. 
Gabriel Murray, Giuseppe Carenini, and Raymond T. 
Ng. 2010. Generating and validating abstracts of 
meeting conversations: a user study. In INLG 2010. 
Gabriel Murray, Giuseppe Carenini and Raymond Ng. 
2012. Using the Omega Index for Evaluating Ab-
stractive Community Detection, NAACL 2012, 
Workshop on Evaluation Metrics and System Com-
parison for Automatic Summarization, Montreal, 
Canada. 
Vasin Punyakanok and Dan Roth. 2001. The Use of 
Classifiers in Sequential Inference. NIPS (2001) pp. 
995-1001. 
Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts 
& Image Segmentation. IEEE Trans. of PAMI, Aug 
2000. 
David C. Uthus and David W. Aha. 2011. Plans to-
ward automated chat summarization. In Proceed-
ings of the Workshop on Automatic Summarization 
for Different Genres, Media, and Languages, 
WASDGML?11, pages 1-7, Stroudsburg, PA, USA. 
Association for Computational Linguistics. 
Lu Wang and Claire Cardie. 2013. Domain-
Independent Abstract Generation for Focused 
Meeting Summarization.  In ACL 2013. 
Liang Zhou and Eduard Hovy. 2005. Digesting virtual 
?geek? culture: The summarization of technical in-
ternet relay chats. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational 
Linguistics (ACL?05), pages 298-305, Ann Arbor, 
Michigan, June. Association for Computational 
Linguistics. 
53
