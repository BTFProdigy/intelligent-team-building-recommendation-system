A Trigger Language Model-based IR System 
ZHANG Jun-lin   SUN Le   QU Wei-min   SUN Yu-fang  
 
Open System & Chinese Information Processing Center 
Institute of Software, The Chinese Academy of Sciences 
P.O.BOX 8718,Beijing 100080 
junlin01@iscas.cn
 
Abstract 
Language model based IR system proposed in 
recent 5 years has introduced the language 
model approach in the speech recognition 
area into the IR community and improves the 
performance of the IR system effectively. 
However, the assumption that all the indexed 
words are irrelative behind the method is not 
the truth. Though statistical MT approach 
alleviates the situation by taking the 
synonymy factor into account, it never helps 
to judge the different meanings of the same 
word in varied context. In this paper we 
propose the trigger language model based IR 
system to resolve the problem. Firstly we 
compute the mutual information of the words 
from training corpus and then design the 
algorithm to get the triggered words of the 
query in order to fix down the topic of query 
more clearly. We introduce the relative 
parameters into the document language model 
to form the trigger language model based IR 
system. Experiments show that the 
performance of trigger language model based 
IR system has been improved greatly. The 
precision of trigger language model increased 
12% and recall increased nearly 10.8% 
compared with Ponte language model 
method. 
 
1 Introduction 
 
Using language models for information 
retrieval has been studied extensively 
recently(Jin et al2002 Lafferty and Zhai 2001 
Srikanth and Srihari 2002  Lavrenko and Croft 
2001 Liu and Croft 2002). The basic idea is to 
compute the conditional probability P(Q|D), i.e. 
the probability of generating a query Q given the 
observation of a document D. Several different 
methods have been applied to compute this 
conditional probability. In most approaches, the 
computation is conceptually decomposed into 
two distinct steps: (1) Estimating a document 
language model; (2) Computing the query 
likelihood using the estimated document model 
based on some query model. For example, Ponte 
and Croft emphasized the first step, and used 
several heuristics to smooth the Maximum 
Likelihood of the document language model, and 
assumed that the query is generated under a 
multivariate Bernoulli model (Ponte and Croft 
1998). The BBN method (Miller et al1999) 
emphasized the second step and used a two-state 
hidden Markov model as the basis for generating 
queries, which, in effect, is to smooth the MLE 
with linear interpolation, a strategy also adopted 
in Hiemstra and Kraaij (Hiemstra and  Kraaij 
1999). In Zhai and Lafferty (Zhai and  Lafferty 
2001), it has been found that the retrieval 
performance is affected by both the estimation 
accuracy of document language models and the 
appropriate modeling of the query, and a two 
stage smoothing method was suggested to 
explicitly address these two distinct steps. 
It?s not hard to see that the unigram 
language model IR method contains the 
following assumption: Each word appearing in 
the document set and query has nothing to do 
with any other word. Obviously this assumption 
is not true in reality. Though statistical MT 
approach (Berger and  Lafferty 1999 ) alleviates 
the situation by taking the synonymy factor into 
account, it never helps to judge the different 
meanings of the same word in varied context. In 
this paper we propose the trigger language model 
based IR system to resolve the problem. Though 
the basic idea of using the triggered words to 
improve the performance of language model was 
proposed by Raymond almost 10 years ago 
(Raymond et al1993), Our method adopts a 
different approach for other objectivity in the IR 
field. Firstly we compute the mutual information 
of the words from training corpus and then 
design the algorithm to get the triggered words of 
the query in order to fix down the topic of query 
more clearly. We introduce the relative 
parameters into the document language model to 
form the trigger language model based IR system. 
Experiments show that the performance of trigger 
language model based IR system has been 
improved greatly.   
In what follows, Section 2 describes trigger 
language model based IR system in detail. 
Section 3 is our evaluation about the model. 
Finally, Section 4 summarizes the work in this 
paper. 
2 Trigger Language Model based IR 
System 
 
2.1 Inter-relationship of Indexing Words 
  
In order to find out the inter-relationship of 
words in some specific context, we consider the 
co-occurring times of different words within 
fixed sized text window of the document. When 
the co-occurring time is large enough, we think 
that relationship is meaningful. Mutual 
Information is a common tool to be applied under 
this situation. So we compute the mutual 
information as following: 
 
)()()1(
),,(
)()(
)1(
),,(
),(
baw
wwba
w
b
w
a
ww
wba
ba
wNwNL
NLwwN
N
wN
N
wN
LN
LwwN
ww
???
?=
???
?
???
?????
?
???
?
??=?
             
(1) 
  
where  denotes the size of the vocabulary, 
 is the co-occurring times of 
word  and  within   sized window 
in training set.  is the count of the word 
 appearing in the training set and  is 
the count of word  appearing in the training 
set. 
wN
, wb L ),( a wwN
aw
aw
bw
(N
wL
)aw
bw
)( bwN
We use the corpus provided by IR task of 
NTCIR2 (NTCIR 2002) as the training set to 
compute the mutual information of words. This 
corpus contains nearly 100 thousands news 
articles encoding in BIG5 charset. We think the 
mutual information which is larger than 25 is 
meaningful. Considering the stop words in 
document or query are useless to represent the 
content, we remove 200 highest frequent words 
from the document before computation. Table 1 
shows some examples with higher mutual 
information. 
 
? ? 
(test) 
? ? ? (alphabet):1895 
??(rail):1353  
? ? (delimitation):758 
? ? ? ?
(windtunnel):473   
???(meter):421   
?? (test paper):403  
? ?
(missile) 
? ? ? ?
(antiaircraft):1063  
??(develop):708 
? ?? (long-range):472 
???(anti-tank):354 
?? 
(bribe) 
? ? (tax dodging):3462 
????(jobbery):2603 
????(FBI):1041 
???(voter):730 
??(zhanjiang):478 
???(Utah):427  
? ? ?
(truculency)
????(scrutator):710 
? ? ? ? (long-range 
missile):497  
????(terrorism):457 
? ? (biochemistry):390 
??(equipoise):327 
??(plague):334   
???(Bagdad):325   
 
      Table 1. Examples of Mutual Information 
2.2 Algorithm of Triggered Words by 
Query  
Generally speaking, a word always 
represents many different meanings and its exact 
meaning adopted in specific topic can be 
determined by the co-occurring words in its 
context. Different meaning of a word often lead 
to the different vocabulary set of related word. 
In order to find out the exact meaning of the 
words contained by the query in IR system, we 
design the algorithm to compute the triggered 
vocabularies of query. It is just these triggered 
words that show the exact meaning of the words 
in query in some specific context and help fix 
down the topic of query more clearly. The basic 
idea behind the algorithm is as following: By 
computing the mutual information, we can derive 
the relative words of a query word. All these 
words mean the semantically related vocabularies 
of the query word under different contexts. We 
propose that if the intersection of the derived 
related words of different words in query is not 
null, the words in the intersection is useful to 
judge the exact meaning of the words in query. 
At the same time, the more times an intersection 
word appears in related vocabulary set of 
different query word, the higher the weight of 
this word to fix down the topic of the query is. So 
we design the following algorithm to compute 
the triggered vocabulary set of query:     
 
Algorithm 1:Triggered vocabularies by query 
Input: Vocabulary set I of query word and its 
co-occurring words after removing the stop 
words in the query. 
},......,,......,,,{ 2211 ><><><><= nnii SqSqSqSqI
Output: Triggered vocabulary set T. 
 
Setp 1. Initialize the set ?=T . 
Setp 2. for(i =2;i<=n;i++) 
{ 
for(j=1;j<= ;j++) inC
{ 
2.1get the different 
combination },......,,,{ ,,2,2,1,1, ><><><= ijijjjjjj SqSqSqL
i
 
which contains  elements from set I ; 
2.2 if any vocabulary set )1(, ikS kj <=<  
in  contains no element, then we turn to 2.4 , 
otherwise we turn to 2.3; 
jL
2.3 Compute the intersection  of all 
vocabulary set  in . 
Here
jiT ,
< mw
)1(, ikS kj <=<
,......,, 221 ><> w
jL
>i },,{ 1, <=ji wT ??? ,
where
2
log i
w =? , ( iw <==<1 ). w?  is the 
word  weight decided by the length of ; jL
jiTT ,?=
w?
q
,,{ 11 ,,......, 22 <>>< mww ???
+? )|( jij dqp
><><=
=
other
ww
dq
q
ji
)
>m
)( i
cs
q
<wm,,,,{ 2211 ??
},.... )(Qli q
?
2.4 T , adopting the higher word 
weight during the merging process; 
} 
}  
Step 3. Output the triggered vocabulary set T ;    
      
2.3 Similarity Computation of Query and 
Document 
We use the similar strategy with Ponte 
language model method (Ponte and Croft 1998) 
to compute the similarity between the query and 
the document. That is, we firstly construct the 
simple language model according to the 
statistical information of vocabulary and then 
compute the generative probability of the query. 
The difference is that the trigger language model 
method takes the context information of a word 
into account. So we compute the triggered words 
set of query  according to algorithm 1.This 
way we get the triggered vocabulary set  
}><= mq wT . 
This set contains the words triggered by query 
and it is these triggered words that determine the 
exact meaning of the vocabularies in query 
among the several optional choices. This helps 
fix down the topic of query more clearly.   
Introducing the triggered words factor into the 
document language model, we can form the 
trigger language model based information 
retrieval system. 
The similarity of query and document can 
be computed as following: 
? ?
= =
=
)(
1
)(
1
()|(
Ql
i
dl
j
d
tf
CMQP   (2) 
??
??
?
???= Tddqdqp jjijji
0
}),......()(
1
)|( ?  (3) 
 
(1) ,......,{ 21 qqqQ =  denotes query 
and is the length of the query;  )(Ql
(2) denotes the trigger language model of 
document ;  
dM
d
(3) denotes a 
document in document set and is the length 
of the document; 
},....,....,{ )(21 dlj ddddd =
(l )d
(4) 
)(
)(
dl
df
C jj =
jd
 is the weight parameter of 
words  in a document. Here  means 
the account of the words  appearing in the 
document. 
)( jdf
jd
(5)  denotes the probability of  
being triggered by the document word .When 
2 words are same, the probability equals 1. If 
they are different and the word  belongs to 
the triggered vocabulary set of query, the 
probability equals the according parameter in the 
,otherwise the probability is 0? 
)|( ji dqp iq
jd
jd
qT
(6)
cs
qtf i )(
)iq
iq
is used for data smoothing; here 
 denotes times of query word  
appearing in document set and   denotes the 
total length of documents which contains the 
word . 
(tf iq
cs
 
3 Experiment Results  
3.1 Corpus 
 
The corpus we used to evaluate the 
performance of our proposed trigger language 
model IR system is the document set offered by 
the traditional Chinese Document set of NTCIR3 
for the IR task. The corpus consists of 381681 
news articles from Hong Kong and Taiwan with 
varied topics. After the word segmentation, the 
document set contains 150700953 words. Among 
them,127519 different words are the entries of 
the vocabulary. The average length of each 
document is 394. 
The 50 queries offered by NTCIR3 IR task 
are contained in a XML file and each query 
consists of following elements: Topic 
Number(NUM),Topic Title(TITLE),Topic 
question(DESC),Topic Narrative(NARR) and 
Topic Concepts(CONC). In order to make it 
easer to compare the performance of the different 
IR methods, we adopt the Topic Question field as 
the query and regard the top 1000 retrieval 
documents as the standard result of the 
experiment.    
 
3.2 Analysis of Experiment Results 
We design 3 relative experiments to 
evaluate the trigger language model IR method: 
vector space model, Ponte language model based 
method and the trigger language model approach. 
Precision and recall are two main evaluation 
parameters. As for the trigger model IR method, 
the optimal size of the text window is 20 content 
words and the mutual information over 25 is 
regarded as the meaningful information. 
Experiment results can be seen in table 2. 
The data of column %  in table 2 shows 
the performance improvement of Ponte language 
model compared with vector space model. The 
data tells us that the precision of language model 
based method increased 10% and recall increased 
nearly 13.7%. The data of column %?  in table 
2 shows the performance improvement of trigger 
language model compared with Ponte language 
model method. From the data we can see that the 
precision of trigger language model increased 
12% and recall increased nearly 10.8%. We can 
draw the conclusion that the trigger language 
model has improved the performance greatly. 
The performance comparison can be showed 
more clearly in figure 1. 
1?
2
    
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.2 0.4 0.6 0.8 1
Recall
Pr
ec
is
io
n
tfidf
Ponte
Language
Model
Trigger
Language
Model
 
 Figure 1. Precision-Recall of 3 methods 
 
  Tfidf Lm(ponte) Trigger lm % 1?  %  2?
Relevant: 3284 3284 3284 ----
- 
----
- 
Rel.ret: 1843 2096 2322 13.7 10.8 
Precision:      
0. 00 
0. 10 
0. 20 
0. 30 
0. 40 
0. 50 
0. 60 
0. 70 
0. 80 
0. 90 
1. 00 
Avg: 
0. 6016 
0. 4607 
0. 3812 
0. 3336 
0. 2738 
0. 2495 
0. 2179 
0. 1566 
0. 0978 
0. 0389 
0. 0019 
0.2377 
0.6109 
0.4844 
0.4123 
0.3757 
0.3255 
0.2854 
0.2313 
0.1716 
0.1041 
0.0474 
0.0025 
0.2610 
0. 7537 
0. 5314 
0. 4541 
0. 4094 
0. 3648 
0. 3237 
0. 2538 
0. 2011 
0. 1153 
0. 0435 
0. 0055 
0. 2933 
+2 
+5 
+8 
+12 
+18 
+14 
+6 
+9 
+6 
+21 
+31 
+10 
+23 
+10 
+10 
+9 
+12 
+13 
+9 
+17 
+10 
-8 
+120 
+12 
 
Table 2. Experiment results
 
4  Conclusion  
Language model based IR system proposed 
in recent 5 years has introduced the language 
model approach in the speech recognition area 
into the IR community and improves the 
performance of the IR system effectively. 
However, the assumption that all the indexed 
words are irrelative behind the method is not the 
truth. Though statistical MT approach alleviates 
the situation by taking the synonymy factor into 
account, it never helps to judge the different 
meanings of the same word in varied context. In 
this paper we propose the trigger language model 
based IR system to resolve the problem. . Firstly 
we compute the mutual information of the words 
from training corpus and then design the 
algorithm to get the triggered words of the query 
in order to fix down the topic of query more 
clearly. We introduce the relative parameters into 
the document language model to form the trigger 
language model based IR system. Experiments 
show that the performance of trigger language 
model based IR system has been improved 
greatly. 
Acknowledgement 
This work is supported by Beijing New Star Plan 
of Technology & Science(NO.H020820790130) 
and the National Science Fund of China under 
contact 60203007. 
References  
Berger A. and  Lafferty J. (1999). Information 
retrieval as statistical translation. In Proceedings of 
SIGIR ?99. pp. 222-229. 
 
Jin R., Hauptmann A.G.  and Zhai C.(2002) Title 
Language Model for Information Retrieval. In 
Proceedings of the 2002 ACM SIGIR Conference 
on Research and Development in Information 
Retrieval. 
 
Hiemstra D. and  Kraaij W. (1999), Twenty-One at 
TREC-7: ad-hoc and cross-language track, In 
Proceedings of the seventh Text Retrieval 
Conference TREC-7, NIST Special Publication 
500-242, pages 227-238, 1999. 
 
Lafferty J. and Zhai. C. (2001) Document language 
models,query models and risk minimization for 
information retrieval. In Proceedings of the 24th 
ACM SIGIR Conference,pp.111-119. 
 
Lavrenko,V., and Croft,W.B.(2001) .Relevance based 
language models. In Proceedings of the 24th ACM 
SIGIR Conference.pp.120-127. 
 
Liu,X. and Croft,W.B.(2002).Passage Retrieval 
Based on Language Models. In Proceedings of the 
11th  International Conference on Information and 
Knowledge Management. Pp.375-382 
 
Miller D.,  Leek T.  and Schwartz  R. M. (1999). 
A hidden Markov model information retrieval 
system. Proceedings of SIGIR?1999, pp. 214-222. . 
 
NTCIR Workshop 
(research.nii.ac.jp/ntcir/index-en.html) 
  
Ponte J. and Croft W. B. (1998). A language 
modeling approach to information retrieval. In 
Proceedings of SIGIR? 1998, pp. 275-281. 
 
Raymond Lau, Roni Rosenfeld and Salim 
Roukos(1993) Trigger-based Language Models: A 
Maximum Entropy Apporach. Proceedings ICASSP 
'93, Minneapolis, MN, pp. II-45 - II-48. 
 
Srikanth M. and  Srihari. R(2002). Biterm 
Language Models for Document Retrieval. In 
Proceedings of the 2002 ACM SIGIR Conference 
on Research and Development in Information 
Retrieval.  
 
Zhai C. and Lafferty J.(2001). A study of smoothing 
methods for language models applied to ad hoc 
information retrieval. In Proceeding of SIGIR?01, 
2001, pp. 334-342. 
 
 
 
Query Translation in Chinese-English Cross-Language 
Information Retrieval 
Zhang Yibo, Sun Le, Du Lin, Sun Yufang 
Chinese Information Processing Center, 
Institute of Software, Chinese Academy of Sciences, 
P.O.Box 8718, Beijing, 100080, P.R. China 
e-mail: zyb, lesun, !du, yfsun@sonata.iscas.ac.cn 
Abstract 
This paper proposed a new query translation 
method based on the mutual information 
matrices of terms in the Chinese and 
English corpora. Instead of looking up a 
? bilingual phrase dictionary, the 
compositional phrase (the translation of 
phrase can be derived from the translation 
of its components) in the query can be 
indirectly translated via a general-purpose 
Chinese-English dictionary look-up 
procedure. A novel selection method for 
translations of query terms is also presented 
in detail. Our query translation method 
ultimately constructs an English query in 
which each query term has a weight. The 
evaluation results show that the retrieval 
performance achieved by our query 
translation method is about 73% of 
monolingual information retrieval and is 
about 28% higher than that of simple word- 
by-word translation way. 
Introduction 
With the rapid growth of electronic documents 
and the great development of network in China, 
there are more and more people touching the 
Intemet, on which, however, English is the most 
popular language being used. It is difficult for 
most people in China to use English fluently, so 
they would like to use Chinese to express their 
queries to retrieval the relevant English 
documents. This situation motivates research in 
Cross Language Information Retrieval (CLIR). 
There are two approaches to CLIR, one is 
query translation; the other is translating 
original language documents to destination 
This research was supported by the National 
Science Fund of China for Distinguished Young 
Scholars under contact 69983009. 
language quivalents. Obviously, the latter is a 
very expensive task since there are so many 
documents in a collection and there is not yet a 
reliable machine translation system that can be 
used to process automatically. Most researchers 
are inclined to choose the query translation 
approach \[Oard. (1996)\]. Methods for query 
translation have focused on three areas: the 
employment of machine translation techniques, 
dictionary based translation \[Hull & 
Grefenstette (1996); Ballesteros & Croft (1996)\], 
parallel or comparable corpora for generating 
a translation model \[Davis & Dunning (1995); 
Sheridan & Ballerini (1996); Nie, Jian-Yun et 
a1.(1999)\]. Machine translation (MT) method 
has many obstacles to prevent its employment 
into CLIR such as deep syntactic and semantic 
analysis, user queries consisting of only one or 
two words, and an arduous task to build a MT 
system. Dictionary based query translation is the 
most popular method because of its easiness to 
perform. The main reasons leading to the great 
drops in CLIP,. effectiveness by this method are 
ambiguities caused by more than one translation 
of a query term and failures to translate phrases 
during query translation. Previous studies \[Hull 
& Grefenstette (1996); Ballesteros & Croft 
(1996)\] have shown that automatic word-by- 
word (WBW) query translation via machine 
readable dictionary (MKD) results in a 40-60% 
loss in effectiveness below that of monolingual 
retrieval. With regard to the use of parallel 
corpora translation method, the critiques one 
often raises concern the availability of reliable 
parallel text corpora. An alternative way is that 
making use of the comparable corpora because 
they are easier to be obtained and there are more 
and more bilingual even multilingual documents 
on the Internet. From analyzing a document 
collection, an associated word list can be 
yielded and it is often used to expansion the 
query in monolingual information retrieval \[Qiu 
& Frei(1993); Jing & Croft(1994)\]. 
104 
In this paper, a new query translation is 
presented by combination dictionary based 
method with the comparable corpora analyzing. 
Ambiguity problem and phrase information lost 
are attacked in dictionary based Chinese- 
English Cross-Language information Retrieval 
(CECLIR). The remainder of this paper is 
organized as follows: section 1 gives a method 
to calculate the mutual information matrices of 
Chinese-English comparable corpora. Section 2 
develops a scheme to select the translations of 
the Chinese query terms and introduces how the 
compositional phrase can be kept in our method. 
Section 3 presents a set of preliminary 
experiment on comparable corpora to evaluate 
our query translation method and gives some 
explanations. 
1 .Mutual information matrices 
calculation 
We hypothesize that the words in a sentence 
after being removed the stop words be 
associated with each other and work together to 
express a query requirement. The association 
relationship between two words can be 
indicated by their mutual information, which 
can be further used to discover phrases \[Church 
:& Hanks (1990)\]. If two words are independent 
with each other, their mutual information would 
be close to zero. On the other hand, if they are 
strongly related, the mutual information would 
be much greater than zero and they would be 
much like to be a phrase; if they occur 
complementarily, the mutual information would 
be negative. In conclusion? the bigger the 
mutual information of word pair, the more 
probable the word phrase would be a phrase. 
According to \[Fano (1961)\], we can define the 
mutual information M1 (tl,t z) of term t I and 
t z as formula (1). 
MI(q,t2) =log  z P(t~'t2) (1) 
P(t~)P(t2) 
Where 
P(tl, t z) is the co-occurrence probability of 
t~ and t~ in a Chinese sentence. The reason we 
select a Chinese sentence to be a window other 
than a fixed length window is that a full Chinese 
sentence can keep more linguistic information 
and consequently, it is more reasonable that we 
can regard t~ and t 2 to be a phrase when they 
co-occur in a sentence. P(t l) and P(t 2) are 
the occurrence probabilities of term t I and t 2 
in a sentence. These probabilities can be 
calculated by the occurrence of term t~ and t 2 
in the collection as equation (2), (3) and (4). 
P(tl) = n,__~_ (2) 
N 
P(t2) = n,2 (3) 
N 
P(tl, t2 ) = n,,,,~ (4) 
N 
Where 
nt~ , nt2 is the individual term frequency of 
term t I and t 2 respectively if either of them 
occur in a sentence of the collection, ntt,t ~ is 
the co-occurrence frequency of term t I and t 2 
i f  they are all in a sentence of the collection. N 
is the number of sentences of the collection. 
Replacing (1) with equation (2), (3) and (4), the 
mutual information of term t I and t 2 can be 
expressed by following formula. 
n,,. N 
MI(q,t 2) = log 2 '- (5) 
H h nt 2 
Table 2 and table 3 show the occurrence 
frequency values and mutual information values 
calculated by formula (5) for three Chinese 
compositional phrases and their corresponding 
English phrases respectively found in our 
comparable corpora. 
t 1 It 2 n,, n,2 n,,,,: MI 
~,\[-~l~f\]~ 106 84 45 9.28 
j~p l t~ 45 97 21 9.21 
~\ ]~\ [g~ 73 22 19 10.51 
Table 2: Mutual information of three Chinese 
phrases (N  = 123,000) 
tl I t2 nt~ ntz nt.t~ M1 
File I system 158 126 52 8.91 
User I management 59 112 18 8.97 
Graphic \[ interface 92 41 34 10.70 
Table 3: Mutual information of three English 
phrases (N  = 184,000) 
Anal)zing the Chinese-English comparable 
corpora in this way, we can get two mutual 
information value matrices to indicate which 
two terms (as to the Chinese collection, they are 
105 
almost Chinese words after segmentation) 
would be most possible to be a phrase. A word 
list associated to each Chinese query term can 
be obtained by looking up the mutual 
information value matrix of the Chinese corpus 
with a cutoff of M1 =1.50. As discussed 
above, the bigger the mutual information value 
between two terms, the more possible the two 
words would be a phrase. We can infer that the 
associated word list of the query term contains 
the terms that are the most possible components 
of a compositional phrase. In other words, the 
phrase information can be kept by this way. The 
Chinese query is translated into English via 
looking up the English senses of Chinese query 
term and words in its associated word list in a 
Chinese-English dictionary. The procedures 
how to select appropriate tranlations and to 
construct he English query are discussed in 
section 2. 
2 Translations selection and phrase 
keeping 
It is a naive method to translate a Chinese query 
only by looking up each Chinese term to get its 
English senses in a Chinese-English dictionary. 
This method, however, results in too many 
ambiguities during the query translation and 
offers no path to select appropriate ones among 
the translations. In addition, phrases in the query 
can not be translated effectively. Previous tudy 
has showed that failure to translate phrases 
greatly reduces the performance by up to 25% 
over automatic word-by-word (WBW) query 
translation \[Ballesteros & Croft (1996)\]. 
In our method, those English translations 
most likely co-occur with each other can be 
obtained via looking up the mutual information 
value matrix of the English corpus with a cutoff 
M1 = 1.50. In this way, the English senses of 
terms in the associated word list can provide a 
good context for the translation of the Chinese 
query term and give a significant clue for its 
translations selection. In addition, the 
information of two terms (either Chinese or 
English) to be a phrase can also be stored in the 
associated word list. In the following, we firstly 
describe our method to select translations in 
detail, and then we give an example to 
demonstrate how to keep the phrase information 
in our method. 
Supposing the Chinese query is expressed by 
(e 1 ,e~ ,.--, e, ). el, e2,... , e, are the segmented 
Chinese words of the query after removing the 
stop words. The translations of 
e m (m = 1,...,r)by looking up the Chinese- 
English bilingual dictionary can be ordered in 
descending by following formula. 
W(fm t ) = lOglO(Ot'i_Ml(f ~)+ fl "o_Ml(fm t )) (6) 
l~'llgmkl" t " " 
z z 
i _M i ( fm l ) = k=l j= l  / 
I~llrmkl (7) 
k=l  
r \]~1 l 
Z ZMI(f,~,J~ k) 
o_ MI(fm l ) = i=l,i?m k=l (8) 
r  lYd 
? i=l,i~m 
Where 
f~ is one sense of the English translation set 
F m of the word e,~ (l = 1,...,IF.b g. is the 
association word set of e m . The size of E m is 
le.I and its element is e~ (k = 1 .... ,le.I) 
F~ is the English translation set of emk, its 
element is f,,~. ct is the coefficient to 
emphasize the inner mutual information 
between the English sense f t of the single 
Chinese query term e m and the English sense 
f ,~ of the e m's association word emk. The 
first part of the formula (6) i _M I ( f~)  
reflects the probability of English translation 
f,~ and f ,~ to be a phrase. /3 is the 
coefficient o emphasize the outside mutual 
information between f,~ and the English sense 
~* of the other Chinese terms included in the 
query. The second part of the formula (6) 
o_  Ml ( f~)  reflects the relevant value between 
the English sense f ,~of  e m and the whole 
query concept. 
Our method of translations selection can be 
described as follows: if the weight of any 
translation of the Chinese query term is greater 
than 1.00, the sense is selected to construct the 
English query. If there is no weight of any 
translation of the Chinese query term greater 
than 1.00, the sense with biggest one is selected 
to construct the English query. In this way, we 
can make an English query by the following 
Boolean expression. 
106 
r / I F~I .  t "~ Query =21\[ XI ~glra'W(gra))) (9) 
Where o I is set element after the English Ora  
translation sense set F m which is detruncated 
by our translation selection method. 
In order to demonstrate he procedure of our 
method, we give an example and explain how 
the English translations are selected and how the 
phrase information is kept. Given a simple 
Chinese query " ~ fi' , '~ ~ , ~ ~ (user, 
management, command)" after segmentation 
and removing stop words, the associated word 
list of term " ~ ~ (user)" is " '~" 2~ 
(management) , 4-~ ,~ (information) , --f- ~" 
(manual)" and the associated word list of term 
"'~ ~E(management)" is "~ ~ (user), *J~(harrd 
disk), ~,Aq-(file)". We process the associated 
word "'~2~2(management)" of the query term 
"~q ~ (user)" in a special way by adding an 
appropriate value to their mutual information 
value to let theirs be the biggest in the 
associated word list, because the associated 
word " '~(management)"  also occurs in the 
original query. Similar way is done with the 
associated word "~ ~ (user)" of the query term 
"~ ~ (management)". In this way, , the 
compositional phrase " h~ ~ '~ JX (user 
management)" can be kept in both associated 
word list of term " I t / "  (user)" and term "~X 
(management)". 
When term ")~ ~"  is translated into English 
by looking up the general-purpose Chinese- 
English bilingual dictionary, we get its English 
sense set "user, consumer" ordered by the 
formula (6). When term "~'JE" is translated 
into English, we get its English sense set 
"management, administration, supervision, run" 
ordered by the formula (6). We can fred the first 
positions of the English translation set of the 
query term "~ P"  and term '"~'JX" are "user" 
and "management" respectively. From the point 
of view of translation, the phrase "user 
management" can be regarded as the English 
phrase translation of "~ # '~ ~" .  According to 
our translation selection and formula (9), we can 
construct the English Boolean query as follows, 
in which each query term has a weight. 
Query = (user, 1.86)and ((management, 1.83) 
or (administration, 1.63)) and (command, 1.92). 
3 Evaluation and discussion 
To evaluate our query translation method, we 
did a set of experiment to compare it to the 
word-by-word (WBW) translation method and 
manual translation method. In the word-by- 
word translation method, the Chinese queries 
are automatically segmented and the Chinese 
terms included in them are translated into 
English only by looking up the general-purpose 
Chinese-English bilingual dictionary. In the 
manual translation method, the Chinese queries 
are translated into English by a Ph.D. student. 
The segmentation we used is based on a small 
general-purpose Chinese-English bilingual 
dictionary that only contains 46,570 pairs in 
which each Chinese word has several English 
translations. The forward and backward 
maximum matching algorithm is used to 
segment he texts and find the combinatorial 
ambiguities. Of all the combinatorial 
ambiguities, 91.2% are removed with the word 
uni-gram prior probabilities. A stop word list of 
1210 elements is set up, which contains 
frequently used functional words as well as 
symbols \[Du & Sun (2000)\]. Our Chinese query 
translation process contains following steps: 
(1) Segment the Chinese query according to the 
method introduced above. 
(2) Get the associated word list of each Chinese 
term included in the query from the Chinese 
mutual information matrix. 
(3) Look up the English sense set of each 
Chinese term and its associated word in the 
general-purpose Chinese-English bilingual 
dictionary. 
(4) Select the English translation sense by the 
method introduced in section 2 (in formula 
(6) the coefficents tx and fl are selected 
by 1.0 and 0.5 respectively in our 
experiment) and construct the English query 
on the basis of the formula (9). 
The document collection used in our 
experiments consists of several Chinese and 
corresponding English computer manuals, 
which include Linux-HOWTO, PostgreSQL 
handbook, Mysql handbook, Linux kernel* and 
Linux Gazette 17 volumes (from July, 1998 to 
Dec., 1999)". In order get a large number 
document Chinese and English collections, we 
decomposed these manuals and let every 
document no more than 15 sentences. As a 
* http://www.linux forum.nct/books/index.html 
* *http://www.linuxgazette.com.cn 
107 
result, Chinese-English bilingual comparable 
corpora are obtained in which contain about 
8,200 Chinese documents and 12,500 English 
documents. We design 13 Chinese queries, the 
average length is about 7 single Chinese 
character (about hree Chinese words). All work 
in this study was performed on the Search2000 
information retrieval system \[Du & Zhang 
(2000)\], which can process both Chinese and 
English Boolean queries. 
Table 4 shows the precision and recall table for 
the three methods. The first column in table 4 
contains precision values averaged 13 queries and 
interpolated to eleven recall points from 0.0 to 1.0 
in steps of 0.1. The third column contains 
precision values achieved by our translation 
method (QT). 
Precision Precision Precision 
Recall (WBW) (Manual) (QT) 
at 0.00 
at 0.10 
at 0.20 
at 0.30 
at 0.40 
at 0.50 
at 0.60 
at 0.70 
at 0.80 
at 0.90 
at 1.00 
Avg. 
0.5831 0.8975 0.6642 
0.5132 0.7884 0.5825 
0.4036 0.6573 0.5174 
0.3771 0.6206 0.4728 
0.3128 0.5840 0.4163 
0.2816 0.5118 0.3838 
0.2143 0.4876 0.3104 
0.1641 0.3833 0.2645 
0.1110 0.2114 0.1702 
0.0741 0.1667 0.1020 
0.0212 0.0428 0.0342 
0.2778 0.4865 0.3562 
Table 4: The results of the three methods 
The results in table 4 suggest that in this case, 
the WBW query translation leads to a great drop 
in effectiveness of 42.90% below that for 
monolingual retrieval (manual translation 
method). The result of our query translation 
method greatly improves effectiveness by 
28.22% over the WBW method, and its 
effectiveness is about 73.21% of that for 
monolingual retrieval. Although phrase 
translation is not executed directly in our 
method, the phrase information is kept 
effectively in the associated word list. Therefore, 
the phrase can be well ~anslated. The associated 
word list also provides a good context for 
translation of the Chinese query terms 
(corresponding to the first part of formula (6) 
i _Ml ( f~t) )  and a good English translation is 
given a relatively high weight. The results in 
table 4 show that our query translation method 
can construct a good English query and indeed 
improve the effectiveness. 
Conclusion 
Automatic word-by-word query translation is an 
attractive method because it is easy to perform, 
resources are readily available, and performance 
is similar to that of other CLIP,. methods. 
However, there are a lot of ambiguities in 
translation of the query terms and failures to 
translate phrases correctly, which are mainly 
responsible for the large drops in effectiveness 
below monolingual retrieval performance. 
Aiming to tackle with these problems, we 
develop a new scheme for how to select 
translations in this paper. In addition, rather than 
using a bilingual phrase dictionary, we also put 
forward a new method to translate phrases 
indirectly by using the mutual information 
between two words in a full sentence and keep 
the phrase information in the associated word 
list effectively. As a result of our query 
translation method, an English query is 
constructed in which each query term has a 
weight. 
In this study, our method leads to improve 
the effectiveness by 28.22% over the word by 
word query translation method, but is still about 
27% below the monolingual retrieval 
performance. If query expansion is employed in 
our method, we expect he performance should 
be further improved. A shortcoming of our 
method is that the cost of calculation of the 
mutual information matrices is very large. We 
are currently exploring an algorithm to generate 
the matrices more efficiently and the selection 
of coefficients in formula (6) also needs further 
research. 
Acknowledgements 
The authors wish to express their appreciation to
those interpreters of computer manuals. Without 
theft selfless contribution, our experiment 
would be impossible. Thanks to the anonymous 
reviewers for their helpful comments. 
References 
Ballesteros, L. and Croft, W. B.(1996). Dictionary- 
based methods for cross-lingual information 
retrieval. In Proceedings of the 7 '~ International 
DEXA Conference on Database and Expert 
Systems Applications,pp.791-801 . . . .  
108 
Church, K. W. and Hanks, P. (1990). Word 
association norms, mutual information and 
lexicography. Computational Linguistics, 16(1), pp. 
22-29. 
Davis, M. and Dunning, T. (1995). Query translation 
using evolutionary programming for multi-lingual 
information retrieval. In Proceedings of the 4 'h 
Annual Conference on Evolutionary Programming, 
pp. 175-185. 
Du, Lin and Sun, Yufang. (2000). A new indexing 
method based on word proximity for Chinese text 
retrieval. Journal of Computer Science and 
Technology,15(3),pp.280-286. 
Du, Lin; Zhang, Yibo and Sun, Yufang. (2000). The 
Design and Implementation of WEB-Based 
Chinese Text Retrieval System Search2000, (in 
Chinese). In Proceedings of 2000 International 
Conference on Multilingual Information 
Processing,pp.44-50. 
Fano,. R. (1961). Transmission of Information: A 
statistical theory of Communications. MIT Press, 
Cambridge, MA. 
Hull, D. A. and Grefenstette, G. (1996). Querying 
across languages: A dictionary-based approach to 
multilingual informaiton retrieval. In Proceedings 
of the 19 th International Conference on Research 
and Development in Information Retrieval,pp.49- 
57. 
Jing, Yufeng and Croft, W. Bruce. (1994). An 
association thesaurus for information retrieval. In 
Proceedings of RIA 0 94,pp. 146-160. 
Nie, Jian-Yun; Brisebois M. and Ren, Xiaobo. 
(1996). On Chinese text retrieval. In Proceedings 
of the 19 'h Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval,pp.225-233. 
Oard, D. W. (1996). A survey of multilingual text 
retrieval. Technical Report UMIACS-TR-96- 
19,http://www.ee.umd.edu/medlab/filter/papers/sig 
ir96.ps. 
Qiu, Yonggang, and Frei , H. P. (1993). Concept 
based query expansion. In Proceedings of the 16 'h 
Annual International ACM SIGIR Conference on 
Research and Development in Information 
Retrieval,pp. 160-169. 
Sheridan, P. and Ballenni, J. P. (1996). Experiments 
in multilingual information retrieval using the 
spider system. In Proceedings of the 19 ~h 
International Conference on Research and 
Development i  Information Retrieval,pp.58-65. 
109 
Word Alignment of English-Chinese Bilingual Corpus 
Based on Chunks 
Sun Le, Jin Youbing, Du Lin, Sun Yufang 
Chinese Information Processing Center 
Institute of Software 
Chinese Academy of Sciences 
Beijing 100080 
P. R. China 
lesun, ybjin, yfsun, ldu@sonata.iscas.ac.cn 
Abstract 
In this paper, a method for the word alignment 
of English-Chinese corpus based on chunks is 
proposed. The chunks of English sentences are 
identified firstly. Then the chunk boundaries of 
Chinese sentences are predicted by the 
translations of English chunks and heuristic 
information. The ambiguities of Chinese chunk 
boundaries are resolved by the coterminous 
words in English chunks. With the chunk 
aligned bilingual corpus, a translation relation 
probability is proposed to align words. Finally, 
we evaluate our system by real corpus and 
present the experiment results. 
Key Words: Word Alignment, Chunk Alignment, 
Bilingual Corpus, Lexicon Extraction 
1 Introduction 
With the easier access to bilingual corpora, there 
is a tendency in NLP community to process and 
refine the bilingual corpora, which can serve as 
the knowledge base in support of many NLP 
applications, such as automatic or human-aid 
translation, multilingual terminology and 
lexicography, multilingual information retrieval 
system, etc. 
Different NLP applications need different 
bilingual corpora, which are aligned at different 
level. They can be divided by the nature of the 
segment to section level, paragraph level, 
sentence level, phrase level, word level, byte 
level, etc. 
As for our applications, we choose the chunk 
level to do alignment based on following 
considerations. Firstly, our applications, which 
include an example-based machine translation 
system, a computer aid translation system and a 
multilingual information retrieval system, need 
the alignment below the sentence level, on 
which we can acquire bilingual word and phrase 
dictionaries and. other useful translation 
information. Secondly, the word level alignment 
between English and Chinese language is 
difficult to deal with. There are no cognate 
words. The change in Chinese word order and 
word POS always produce many null and 
mistake correspondences. Next, we observe the 
phenomenon that when we translate the English 
sentence to Chinese sentence, all the words in 
one English chunk tend to be translated as one 
block of Chinese words which are coterminous. 
The word orders within these blocks tend to 
keep with the English chunk, also. So there are 
stronger boundaries between chunks than 
between words when we translate texts. Finally, 
as we all known, chunk has been assigned 
syntactic structure (Steven Abney, 1991), which 
comprises a connected sub-graph of the 
sentence's parse tree. So it's possible to align 
sentence structure and obtain translation 
grammars based on chunks by parsing. 
Many researchers have studied the text 
alignment problem and a number of quite 
encouraging results have been reported to 
different level alignments. With 
sentence-aligned corpus ready in hand, we focus 
our attention on the intra-sentence alignment 
between the sentence pairs. In this paper, a 
method for the word alignment of 
English-Chinese corpus based on chunks is 
proposed. The chunks of English sentences are 
identified firstly. Then the chunk boundaries of 
Chinese sentences are predicted by the bilingual 
lexicon and synonymy Chinese dictionary and 
heuristic information. The ambiguities of 
Chinese chunk boundaries are resolved by the 
coterminous words in English chunks. With the 
110 
chunk aligned bilingual corpus, a translation 
relation probability is proposed to align words. 
Although this paper is related to 
English-Chinese word alignment, he idea can 
be used to any other language bilingual corpora. 
In the following sections, we first present a brief 
review of related work in word alignment. Then 
discuss our alignment algorithm based on 
chunks in detail. Following this is an analysis of 
our experimental results. Finally, we close our 
paper with a discussion of future work. 
2 Related Work 
There are basically two kinds of approaches on 
word alignment: he statistical-based approaches 
(Brown et. al., 1990; Gale & Church, 1991; 
Dagan et. al. 1993; Chang, 1994), and the 
lexicon-based approaches (Ker & Chang, 1997; 
Wang et. al., 1999). 
Several translation models based on word 
alignment are built by Brown et al (1990) in 
order to implement the English-French 
statistical machine translation. The probabilities, 
such as translation probability, fertility 
probability, distortion probability, are estimated 
by EM algorithm. The Z 2 measure is used by 
Gale & Church (1991) to align partial words. 
Dagan (1993) uses an improved Brown model to 
align the words for texts including OCR noise. 
They first align word partially by character 
string matching. Then use the translation model 
to align words. Chang (1994) uses the POS 
probability rather than translation probability in 
Brown model to align the English-Chinese POS 
tagged corpus. Ker & Chang (1997) propose an 
approach to align Chinese English corpus based 
on semantic class. There are two semantic 
classes are used in their model. One is the 
semantic class of Longman lexicon of 
contemporary English, the other is synonymy 
Chinese dictionary. The semantic lass rules of 
translation between Chinese and English are 
extracted from large-scale training corpus. Then 
Chinese and English words are aligned by these 
rules. Wang (1999) also uses the lexicons to 
align the Chinese English bilingual corpus. His 
model is based on bilingual lexicon, sense 
similarity and location distortion probability. 
The statistical-based approaches need complex 
training and are sensitive to training data. It's a 
pity that almost no linguistic knowledge is used 
in these approaches. The lexicon-based 
approaches seem simplify the word alignment 
problem and can't obtain much translation 
information above word level. To combine these 
two approaches in a better way is the direction 
in near future. In this paper we proposed a 
method to align the bilingual corpus base on 
chunks. The linguistic knowledge such as POS 
tag and Chunk tag are used in a simply 
statistical model. 
3 Alignment Algorithm 
3.1 Outline of Algorithm 
For our procedure in this paper, the bilingual 
corpus has been aligned at the sentence l vel, 
and the English language texts have been tagged 
with POS tag, and the Chinese language texts 
have been segmented and tagged with POS tag. 
We have available a bilingual lexicon which 
lists typical translation for many of the words in 
the corpus. We have available a synonymy 
Chinese dictionary, also. We identify the chunks 
of English sentences and then predict he chunk 
boundaries of Chinese sentences from the 
translation of every English chunks and 
heuristic information by use of the bilingual 
lexicon. The ambiguities of Chinese chunk 
boundaries are resolved by the coterminous 
words in English chunks. After produce the 
word candidate sets by statistical method, we 
calculate the translation relation probability 
between every word pair and select the best 
alignment forms. The detail algorithm for word 
alignment is given in table 1. 
Step 1: According to the definition of Chunk in 
English, separate the English sentence into 
a few chunks and labeled with order 
number from left to fight. 
Step 2: Try to find the Chinese translation of 
every English chunk created in step 1 by 
bilingual dictionary and synonymy Chinese 
dictionary. If the Chinese translation is fred, 
then label the Chinese words with the same 
number used for the English chunk in step 
1. 
Step 3: Disambiguate the multi-label Chinese 
words by the translation location of 
coterminous words within the same English 
chunk. 
Step 4: Separate the Chinese sentence into a few 
chunks by heuristic information. 
Step 5: Save all the alignment at chunk level in 
111 
whole corpus as a base for word alignment. 
Step 6: Produce the word candidate sets by 
statistical method. 
Step 7: Calculate the translation relation 
probability between every word and it's 
candidate translation words. 
Step 8: Select he best translation by comparing 
the total TRP value in different alignment 
forms. 
Table 1. Outline of Alignment Algorithm 
3.2 Chunk Identifying of English 
Sentence 
Following Steven Abney (1991), there are two 
separate stages in chunking parser, which is the 
chunker and the attacher. The chunker converts 
a stream of words into a stream of chunks, and 
the attacher converts the stream of chunks into a 
stream of sentences. So only the chunker is 
needed in this paper. It's a non-deterministic 
version of a LR parser. For detail about chunker 
and the used grammars, please see Abney 
(1991). Then the chunks in one sentence are 
labeled with order number from left to right. 
3.3 Chunk Boundary Prediction of 
Chinese Sentence 
We observe the phenomenon that when we 
translate the English sentence to Chinese 
sentence, all the words in one English chunk 
tend to be translated as one block of Chinese 
words that are coterminous. The word orders 
within these blocks tend to keep with the 
English chunk, also. There are three examples in 
figure 1. The first sentence pair is chosen from 
an example sentence of Abney (1991). The 
second sentence pair is from a computer 
handbook. In these sentence pair all English 
chunks can find the exactly Chinese Chunk. In 
the third sentence pair only one English chunk 
can't find the exactly Chinese chunk for this 
sentence is chosen from a story and the 
translation is not literally. 
In order to find the Chinese translation of every 
English chunk, we use the bilingual dictionary 
and synonymy Chinese dictionary to implement 
the matching. If the Chinese translation of any 
words within the English chunk is found, then 
label the Chinese word with the same number 
used for labeling the English chunk. 
If there are Chinese words, which are labeled 
simultaneously by two or more number of 
English chunks, we use the number of nearby 
Chinese words to disambiguate. For example, in 
figure 2, the first Chinese word /~ j  may be 
correspondent to the English chunk 5 or 7. We 
have known that the words in one English chunk 
tend to be translated as one block of Chinese 
words that are coterminous, So it's easy to 
decide the first Chinese word )x~ ffJ is 
correspondent to the English chunk 7, the second 
Chinese word )x~ ~ is correspondent to the 
English chunk 5. By the same way, we can find 
the correct ranslations of Chinese word ~ 
and ~ is English chunk 6 and chunk 8 
respectively. In Step 4 of figure 2, the Chinese 
words with the same label number are bracketed 
with in one chunk. Finally, we separate the 
Chinese sentence into a few chunks by heuristic 
information based on POS tag (especially the 
preposition, conjunction, and auxiliary words) 
and the grammatical knowledge-base of 
contemporary Chinese (Yu shi wen, 1998). 
\[The b~ald man\] \[was itting\] [on his suitcase\]. 
\[To a c c e ~ _ ~ _ ~ _ ~ c l i c k \ ]  Ion "Su2.p..9.~'l. 
\[I gathered\] \[from what hey said\] ,\[that an elder sister\] [of his\] \[ was coming \] \[to stay with them\],\[ and 
that s h e ~ \ ]  \[ that e v ~  
\ [~ ' f l ' \ ]~qb\ ] \ [~\ ] \ [~\ ] \ [  - ~ \ ] \ [~\ ] \ [~ l ' \ ]~- -~\ ] ,  \ [~ .R~\ ] \ [~_h\ ] \ [~ l J \ ] .  
Figure 1. Three Examples of Chunk Afignment 
112 
Step 1 English chunks with order number 
\[This product 1\] \[is designed 2\] for \[low-cost 3\], \[turnkey solutions 4\] and \[mission-critical 
applications 5\] that \[require 6\] \[a central application host 7\] and \[ do not require 8\] \[networking 9\]. 
Step 2 Label the translation of English chunk with it's order number 
i~(1) ~( I )  ~ ~j ~(6 /8 )  --~(7) ~,~,(7) ~(5 /7 )  5\]~01,(7) ~ ~(8) ~(6 /8)  I~ 
~(9)  {k~(3) ~2~:(3). ~ ~\ ] '~ '~ ~(4)  ~~(4)  ~ ~.~'~(5)  ' f~-~(5 /7)  ~ ~gJ- 
(2)? 
Step 3 Disambiguate the multi-label Chinese words 
i.~(1) ~:~(1) ~ ~ ~,~(6)  - -~(7)  @,~,(7) ~(7)  5\]E;~R(7) ~ ~(8)  ~(8)  ~\ ]~(9)  
t~ (3) ~.  (3). ~ ~ 9  ~: (4 )  ~ (4) 7A Y~'Pi(5) i~-  ~ (5) ~ ~2@~9 (2), 
Step 4. Separate the Chinese sentence into a few chunks 
~(3)3, E~ ~'~9 ~:  ~(4) \ ]  ~ \[Y~'I~ ~ ~(5) \ ]  ~ \[~,i-~9(2)\]0 
Figure 2. An Example for Chunk Alignment Algorithm from Step 1 to 4 
3.4 Calculation of Translation Relation 
Probability for Words 
With the alignments at chunk level of whole 
corpus, we propose a Translation Relation 
Probability (TRP) to implement the word 
alignment. The translation Relation probability 
of words are given by following equation: 
P~ - L:~ (1) 
L 'L  
Where f? is the frequency of English word in 
whole corpus; fc is the frequency of Chinese 
Word in whole corpus; f~ is calculated by 
follow equation: 
N /ln( 2Lay ) + ln(Lav) 
I L~i + Lci 
(2) 
Where Lmv is the average words number of all 
English chunks and all Chinese chunks which 
are related to the English word in whole Corpus; 
L~i is the word number of the English chunk in 
which the English candidate words co-occur 
with the Chinese words; ~ is the word number 
of the Chinese chunk in which the English 
candidate words co-occur with the Chinese 
words; N is the total number of chunks in which 
the English word co-occur with the Chinese 
word; 13?e is the penalty value to indicate the 
POS change between the Engfish word and the 
Chinese word. 
By this equation we connect he chunk length 
and POS change with the co-occurrence 
frequency. The less the chunk length, the higher 
the translation relation probability. For example, 
the chunk pak, which is composed by one 
English word and two Chinese words, is more 
reliable than the chunk pair, which is composed 
by four English words and four Chinese words. 
An example is given in figure 3. There are 5 
possible alignment forms in our consideration 
for this chunk, which includes three Engfish 
words and three Chinese words. Then calculate 
the total TRP value for every possible alignment 
word pairs in each alignment form by equation 
(1). After we get the total TRP value for each 
alignment form, we choose the biggest one. 
floppy disk drive 
III 
A 
floppy disk drive floppy disk drive 
B C 
floppy disk drive floppy disk drive 
X1 
D E 
Figure 3. The Possible Word Alignment Forms in One Chunk 
113 
4 Experimental Results 
4.1 System Architecture 
English 
corpus 
Chunk L Identifying 
~ lSentence  Aligned 
ingual Corp~ 
! 
Word Dictionary ~ "~ 
Segmented an  
tagged Chinese 
Corpus 
~"~Grammar Rule fo r~ 
-.. Chunk Constructing ~ 
Tagged 
Sentence 
~cUristic information f r~ 
hunk Constructing J 1 E y!  
~ ual Database ~-~ Translation Dictionary ) 
I I 
Source Text 
I 
Example Based Machine 
Translation System 
User's Languages Inquiry ~ Multilingual Information t Retrieval Results 
Retrieval System ~> 
Chunk 
Identifying 
Chunk 1 Tagged 
Sentence 
Target Text~ Computer Aid
? Translation System 
1 
Figure 4. System Architecture 
114 
4.2 Experiment Results 
We tested our system with an English-Chinese 
bilingual corpus, which is part of a computer 
handbook (Sco Unix handbook). There are 
about 2246 English sentence and 2169 Chinese 
sentence in this computer handbook after filter 
noisy figures and tables. Finally we extracted 
14,214 chunk pairs from the corpus. The 
accuracy for automatic chunk alignment is 
85.7%. The accuracy for word alignment based 
on correctly aligned chunk pairs is 93.6%. The 
errors mainly due to the following reasons: 
Chinese segmentation error, stop words noise, 
POS tag error. The parameter 13ec we used in 
equation (2) should be chosen from the training 
corpus. In table 2, the total TRP values of 
example in figure 3 are showed. The alignment 
form D is the best. 
(floppy I -~)  
(disk I ~)  
(drive I ~) 
(floppy \[ ~ ~)  
(disk drive I :~:) 
(floppy\[ ~) 
(disk drive I ~ ~) 
(floppy disk\[ .~7~) 
(drive I -Sg~ :~) 
(floppy disk \[ ~3~ ~J)  
(drivel ~)  
0.9444 
0.0212 
0.1722 
0.2857 
0.1765 
0.9444 
0.3529 
0.8333 
0.8947 
X 1/3 
X 1/3 
X 1/3 
X 1/2 
X 1/2 
X 1/2 
X 1/2 
X 1/2 
X 1/2 
0.3429 X 1/2 
0.1722 X 1/2 
Total TRP of A =0.3792 
Total TRP of B =0.3194 
Total TRP of C =0.6485 
Total TRP of D =0.8640 
Total TRP of E =0.2576 
Table 2. Total TRP Value for Example in Figure 3 
5 Conclusions and Future Work 
With the more and more bilingual corpora, there 
is a tendency in NLP community to process and 
refine the bilingual corpora, which can serve as 
the knowledge base in support of many NLP 
applications. In this paper, a method for the 
word alignment of English-Chinese corpus 
based on chunks is presented. After identified 
the chunks of English sentences, we predict he 
chunk boundaries of Chinese sentences by the 
bilingual exicon, synonymy Chinese dictionary 
and heuristic information. The ambiguities of 
Chinese chunk boundaries are resolved by the 
coterminous words in English chunks. After 
produce the word candidate sets by statistical 
method, we calculate the translation relation 
probability between every word pair and select 
the best alignment forms. We evaluate our 
system by real corpus and present the results. 
Although the results we got are quite promising 
to bilingual English Chinese text, there are still 
much to do in near future. The corpus we use in 
our experinaent is a relative small corpus about 
computer handbook, in which the terms are 
translated with high consistency. We should 
extend our method to the large corpus of other 
domains without lost much accuracy. To 
increase the correct rate of Chinese word 
segmentation is important for our word 
alignment. To extract he corresponding syntax 
information of English Chinese bilingual corpus 
by shallow parsing is a direction for future work, 
also. 
Acknowledgements 
This research was. funded by Natural Science 
Foundation of China (Grant No. 69983009). 
The authors would like to thank the anonymous 
reviewers for their helpful comments. 
References 
Abney, Steven, 1991. Parsing by Chunks. In: Robert 
Berwick, Steven Abney and Carol Tenny (eds.), 
Pringciple-Based Parsing, Kluwer Academic 
Publishers 
Brown, P. F., Della Pietra, S. A., Della Pietra, V., J., 
and Mercer, R. L., 1993. The Mathematics of 
Statistical Machine Translation: Parameter 
Estimation. In.Computational Linguistics, 19(2), 
pp.263-311. .... 
Chang, J. S., and Chert, M. H. C. 1994 Using Partial 
115 
Aligned Parallel Text and Part-of-speech 
Information in Word Alignment. In Proceedings of 
the First Conference of the Association for 
Machine Translation in the Americas(AMTA94), 
pp 16-23 
Dagan, I. and Church, K. W. 1994 Termight: 
Identofying and Translating. Technical 
terminology. InProceedings ofEACL 
Fung, P., and Church, K. W., 1994. K-vec: A New 
Approach for Aligning Parallel Texts. In 
Proceedings of the 15th International Conference 
on Computational Linguistics (COLING94), 
Japan, pp. 1096-1102, 
Gale, W. A., and Church, K. W., 1991. A Program for 
Aligning Sentences in Bilingual Corpora. In 
Proceedings of the 29th Annual Meeting of the 
Association for Computational Linguistics 
(ACLgl), pp. 177-184 
Kay, M., and Roscheisen M., 1993. Text-Translation 
Alignment. Computational Linguistics, 
19/l,pp.121 
Ker, M. and Chang, J. S. 1997 A Class-Based 
Approach to Word Aligmnent. Computational 
Linguistics,23(2),pp 313-343 
Langlais, Ph., Simard, M., Veronis, J., Armstong, S., 
Bonhomme, P., Debili, F., Isabelle, P., Souissi, E., 
and Theron, P., 1998. Arcade: A cooperative 
research project on parallel text alignment 
evaluation. In First International Conference on 
Language Resources and Evaluation, Granada, 
Spain. 
Melamed, I. D. 1996. Automatic Detection of 
Omissions in Translations. In Proceedings of the 
16th International Conference on Computational 
Linguistics, Copenhagen, Denmark 
Sun, Le, Du, Lin, Sun, Yufang, Jin, Youbin 1999 
Sentence Alignment of English-Chinese Complex 
Bilingual Corpora. Proceeding of the workshop 
MAL99, 135-139 
Wang, Bin, Liu, Qun, and Zhang, Xiang, 1999 An 
Automatic Chinese-English Word Alignment 
System. Proceedings of ICMI99, ppl00-104, 
Hong Kong 
Wu, Daikai.and Xia, Xuanyin. 1995. Large-Scale 
Automatic Extraction of an English-Chinese 
translation Lexicon. Machine Translation, 
9:3--4,285-313 
Yu, Shiwen, Zhu, Xuefeng, Wang, Hui, Zhang 
Yunyun, 1998 The Grammatical Knowledge-base 
of Contemporary Chinese: A complete 
Specification. Tsinghua University Publishers 
116 
Constructing of a Large-Scale Chinese-English  
Parallel Corpus 
 
Le Sun, Song Xue, Weimin Qu, Xiaofeng Wang,Yufang Sun 
Chinese Information Processing Center 
Institute of Software, Chinese Academy of Sciences 
Beijing 100080, P. R. China 
lesun, bradxue, qwm, wxf, yfsun@sonata.iscas.ac.cn 
 
Abstract  
This paper describes the constructing of a 
large-scale (above 500,000 pair sentences) 
Chinese-English parallel corpus. The current 
status of Chinese corpora is overviewed with 
the emphasis on parallel corpus. The XML 
coding principles for Chinese?English 
parallel corpus are discussed. The sentence 
alignment algorithm used in this project is 
described with a computer-aided checking 
processing. Finally, we show the design of 
the concordance of the parallel corpus and 
the prospect to further development. 
Introduction 
With the development of the corpus linguistics, 
more and more language resources have been 
established and used in language engineering 
research and applications. As we all know, there 
are different kinds of corpora for different kinds 
applications. For example, the Chinese 
Part-Of-Speech annotation corpus used to train 
program for Chinese word segmentation and 
POS tag, the Chinese tree bank used to Chinese 
syntax study, and so on.  
 
In this paper the constructing of a large-scale 
Chinese-English parallel corpus, which is totally 
above 500,000 pair sentences and the first year 
task is 100,000 pair sentences, is described. The 
applications of the large-scale Chinese-English 
parallel corpus put emphasis on the sentence 
template extracting for EBMT (Example-Based 
Machine Translation) and translation model 
training for SBMT (Statistical-Based Machine 
Translation). The latent applications may include 
the bilingual lexicon extraction, special term or 
phase extraction, bilingual teaching, 
Chinese-English contrastive study, etc.  
 
Numerous corpus data gathering efforts exit all 
of the world. The rapid multiplication of such 
efforts has made it critical to create a set of 
standards for encoding corpora. CES (Corpus 
Encoding Standard), which is conformant to the 
TEI Guideline for Electronic Text Encoding and 
Interchange of the Text Encoding Initiative (TEI 
2002), has been adopted by many corpus-based 
work. The XML Corpus Encoding Standard 
(XCES) is a part of the Guideline developed by 
the Expert Advisory Group on Language 
Engineering Standards (Ide, N., Bonhomme, P., 
Romary, L. 2000). The coding of our 
Chinese-English Parallel Corpus is in broad 
agreement with the TEI Guideline for electronic 
texts. 
 
In the following section, we first present a brief 
review of the current status of Chinese corpora 
with the emphasis on parallel corpus. Then the 
XML coding principles for Chinese?English 
parallel corpus are discussed in detail. Following 
this is the sentence alignment algorithm used in 
this project with a computer-aided checking 
processing. Finally, we show the design of the 
concordance of the parallel corpus and the 
prospect to further development. 
1 Chinese Corpus Project Overview 
The Chinese Corpus constructing work started in 
1920?s, See Zhiwei Feng (2001). The 
machine-readable corpora established in 1980?s 
are listed as following: 
 Chinese Modern Literature Corpus 
(1979), 5.27 Million Chinese 
Characters, WuHan University; 
 Modern Chinese Corpus (1983), 20 
Million Chinese Characters, Beijing 
University of Aeronautics and 
Astronautics; 
 Middle School Chinese Book 
Corpus (1983), 1.06 Million Chinese 
Characters, Beijing Normal University; 
 Modern Chinese Word Frequency 
Corpus (1983), 1.82 million Chinese 
characters, Beijing Language & 
Culture University. 
 
The first national large-scale Chinese corpus 
project is proposed in 1991 by State Language 
Commission in China. The Chinese texts used in 
this corpus are selected carefully under the 
condition of times, genre, and field. Now the 
corpus is about 20 million Chinese characters. 
 
From 1992, there are several large-scale Chinese 
corpus constructed by different institutes. The 
most noticeable in them is the Chinese POS 
annotation corpus accomplished by Institute of 
Computational Linguistics, Peking University, 
with the cooperation with Fujitsu Company. The 
content of this corpus is people?s daily, one of 
the most popular newspapers in China. The 
Chinese texts are segmented and added POS tag 
with high precision. The total Chinese 
Characters are about 27 million.  
 
There are several Chinese corpora in Tsinghua 
University also. The corpus, which is used for 
Chinese segmentation study, includes 100 
million Chinese characters. The Hua Yu corpus 
(2 million Chinese characters) is a POS tagged 
field-balance corpus. And the 10 percent of this 
corpus has been used for constructing Chinese 
tree bank. 
 
These are also other valuable Chinese corpora 
established in ShanXi University, Harbin 
technical University, ShangHai Normal 
University, City University of Hong Kong, 
Taiwan Academia Sinica, University of 
Pennsylvania and so on. Please refer to Zhiwei 
Feng (2001) for detail.  
 
In October 2001, a national corpus project, that 
is, national 863 project about Chinese 
Information Processing Platform, is launched. 
It?s a cooperation project between five institutes 
in China, including Institute of Software, 
Chinese Academy of Sciences, Institute of 
Computational Linguistics, Peking University, 
Tsinghua University, Nanjing University and 
Institute of Language, State Language 
Commission. The content of corpora and 
intended scale in this project are showed in table 
1 in detail. The large-scale Chinese-English 
parallel corpus described in this paper is one of 
the scheming corpora in this project. 
 
The multilingual corpus is important for 
computational linguistics research and 
contrastive linguistics study. So there are many 
multilingual corpus have been established or 
being developed in many institutes in China 
mainland. The table 2 shows the 
Chinese-English parallel corpus had been 
constructed in Mainland China. There are also 
some bilingual corpora about other language pair, 
such as Chinese-Japanese, Chinese-German, etc.
 
 
Sub-Project Name Responsible Institute First-Year  Scale 
Scheming
 Scale 
Chinese Balance Corpus State Language Commission 70 MCC 150 MCC
Chinese-English Parallel Corpus  IOS, Chinese Academy of Science 100 TS 500 TS 
Chinese POS Annotation Corpus ICL, Peking University 7 MCC 30 MCC 
Chinese Tree Bank Tsinghua University 15 TS 60 TS 
Chinese Concept Dictionary ICL, Peking University 20 TC 60 TC 
Chinese Semantic Knowledge Base Tsinghua University 8 TW 24TW 
Table 1 The 863 Chinese corpus project 
 
MCC: Million Chinese Character       TS: Thousand Sentence 
TC: Thousand Concept                 TW: Thousand Word 
Institute  Corpus Describing  
 Scale 
ICL, Peking University Sentence & Phrase Alignment 5 TS 
Harbin Institute of Technology  Sentence, Phrase, Word Alignment Above 5 TS 
State Language Commission Computer Science and Plato Unknown 
Beijing Foreign Studies University Literary, Science and Civilization in 
China 
Unknown 
Northeastern University Sentence & Phrase Alignment Unknown 
IOS, Chinese Academy of Science Sentence Alignment  8 TS 
Table 2 The Chinese-English parallel corpus in Mainland 
 
It has been noticed by many scholars that we 
should build a principle for sharing language 
resource in research work and to avoid the waste 
in time and effort in repeated construction. 
 
2 Resource Collection 
Unlike single linguistic resource, the parallel 
resource for special language pair is limited no 
matter what language pair is. Although the 
Chinese and English both are most popular 
language in the world, we still encounter much 
difficult in obtaining parallel corpus resource 
from Internet for following reasons: 
There are seldom web pages in China 
provide the same content in English 
pages and in Chinese pages; 
The English news in web are translated 
freely other than literally with many 
content omission; 
Some bilingual texts are restricted and 
used only to member.  
 
After two years efforts, there are totally about 
16,000KB untagged Chinese-English parallel 
texts in hand. The genres of the resource we 
collected are showed in table 3. 
 
 
Chinese Genre About Percent
News 10% 
Literature 30% 
Government 
Report 25% 
Sciences & 
Technology 35% 
Table 3 The genre in parallel texts 
3 Coding 
3.1 General Principles 
The coding of the parallel corpus is in broad 
agreement with the TEI Guideline for electronic 
texts. The eXtendible Make-up Language (XML) 
is used for the text coding. Textual features are 
marked by tags enclosed within angle brackets. 
For example, a title is marked by start tag <title> 
and an end tag </title >. Every element has some 
attributes to identifier of the element.  
 
The document type definition (DTD) for the 
texts in the corpus may differs in some respects 
from the TEI model. The general principle for 
coding are based on following consideration: 
Comply with TEI guide lines on the 
whole; 
Define the tag with clear meaning used 
by most people in china; 
Only used the attributes which can be 
easily and automatically get from source 
texts, except the alignment link, which is 
the key attribute in this corpus and 
several steps are used to keep high 
precise (See section 4 for detail); 
Try to keep all the interim resource in 
hand in case information loses, such as, 
the title tag in HTML files. 
 
The overall structure of a Chinese-English 
Parallel corpus is shown by this example: 
<article id=?UH001?> 
<Header type =?Unix Handbook?> 
</Header> 
<text> 
</text> 
</article> 
There are two main parts in a text: a header and 
the main text. Every text has an unique identifier 
that is, article id, in this case UH001 (indicating 
text 001 of the Unix Handbook) 
3.2 The header 
Each text is described by a header, which has 
four parts in accordance with the TEI guidelines: 
a file description, an encoding description, a 
profile description, and a revision description. 
The file description gives bibliographical 
information on the source text. The elements 
include title, author, www address (If the text is 
obtain from Internet), etc. The encoding 
description in our corpus is very brief, only the 
project name and the DTD file name are listed.  
 
The country or region use the language is 
indicated in the profile description. The 
description under <language> used in our corpus 
is in terms of labels like: Mainland Chinese 
(MaC), Hong Kong Chinese (HKC), Taiwan 
Chinese (TwC), Singapore Chinese (SiC), 
American English (AmE), British English (BrE), 
Canadian English (CaE), etc. 
 
Another tag used in the profile description is 
<textclass>. According to the parallel resource 
in hand, the texts are grouped into 4 genres (as 
show in table 3), such as, News , Literature, 
Science & Technical?Government Report. 
A series of changes are listed in the revise 
description and specified the change, the date of 
the change, the person responsible for the 
change, and the nature of the change. 
3.3 Text Units and Alignment Unit 
The corpus texts are segmented according to the 
natural units, such as: chapter, paragraph, 
sentence (S-unit), and word. The English words 
are simply marked by spacing as in ordinary 
written text. The Chinese words are not 
indicated by space in order to avoid the segment 
error. 
 
An ID is given to every paragraph to indicate the 
relative position in whole chapter. The sentence 
is called S-unit, the same as Johansson, Ebeling 
and Oksefjell (1999) to underline that they are 
not necessarily sentences in a grammatical sense. 
 
The sentence alignment type between Chinese 
S-unit and English S-unit maybe 1:1, 2:1, 3:1, 
1:2, 1:3,2:2, 3:2, 2:3. Links between parallel 
texts are showed by attributes of S-Alignment. 
One of the Chinese alignment unit (it may 
beyond one S-unit) are linked with the 
correspondence English alignment unit.   
3.4 Sample Text 
A sample text of our Chinese-English parallel 
corpus is showed in figure 1.  
 
 
Figure1 Sample Text  
4 Sentence Alignment 
4.1 Algorithm Overview 
The key attribute in this corpus is alignment link, 
which connect the one or more Chinese sentence 
with one or more correspond English sentence. 
In order to keep high precise in sentence 
alignment, several steps are used with the human 
and computer cooperation. 
 
The first step to extract structural information for 
parallel corpus is paragraph alignment and 
sentence alignment, that is noting which 
paragraph and sentence in one language 
correspond to which paragraph and sentence in 
another language.  
 
This problem has been studied by many 
researchers and a number of quite encouraging 
results have been reported. However, almost all 
bilingual corpora used in research are clear 
(nearly without sentence omission or insertion) 
and literal translation bilingual texts. The 
performance tends to deteriorate significantly 
when these approaches are applied to noisy 
complex corpora (with sentence omission or 
insertion, less literal translation). 
 
There are basically three kinds of approaches on 
sentence alignment: the length-based approach 
(Gale & Church 1991 and Brown et al 1991), 
the lexical approach (key & Roscheisen 1993), 
and the combination of them (Chen 1993, Wu 
1994 and Langlais 1998, etc.). 
 
The first published algorithms for aligning 
sentences in parallel texts are length-based 
approach proposed by Gale & Church (1991) 
and Brow et al(1991). Based on the observation 
that short sentences tend to be translated as short 
sentences and long sentences as long sentences, 
they calculate the most likely sentence 
correspondences as a function of the relative 
length of the candidates. The basic approach of 
Brow et al is similar to Gale and Church, but 
works by comparing sentence length in words 
rather than characters. While the idea is simple, 
the models can still be quite effective when used 
to clear and literal translated corpora. Once the 
algorithm had accidentally mis-aligned a pair 
sentence, it tends to be unable to correct itself 
and get back on track before the end of the 
paragraph. Use alone, length-based alignment 
algorithms are therefore neither very robust nor 
reliable. 
 
Kay & Roscheisen (1993) use a partial 
alignment of lexical items induce a maximum 
likelihood at sentence level. The method is 
reliable but time consuming.  
 
Chen (1993) combines the length-based 
approach and lexicon-based approach together. 
A translation model is used to estimate the cost 
of a certain alignment, and the best alignment is 
found by using dynamic programming as the 
length-based method. The method is robust, fast 
enough to be practical and more accurate than 
previous methods. 
 
The first sentence alignment model used to align 
English-Chinese bilingual texts is proposed by 
Wu (1994). For lack of cognates in 
English-Chinese, he used lexical cues to add the 
robust of his model. 
 
All of these works are test on nearly clear and 
literal translation bilingual corpora. 
 
There are seldom papers related to paragraph 
alignment. It's believed by most of the 
researchers that the paragraph alignment is an 
easier task than sentence alignment. Gale & 
Church (1991) suggest that the same 
length-based algorithm can be used to align 
paragraph also. 
 
4.2 The Alignment Steps 
Sentence alignment algorithm of our system can 
be outlined as follows: 
 
Step 1: Align sentence by the improved 
length-based algorithm.(Desicibed in Sun 
etc. 1999) 
Step 2: A lexicon checking process is added to 
judge all the alignment results in step 1. 
A score is given to every alignment pair 
(A Chinese word segmentation system is 
used in this process to find Chinese 
word). 
Step 3: The alignments whose score above a 
threshold C1 are judged as correct 
alignment. Remove these correct 
alignments from bilingual texts temporally. 
Step 4: The rest parts are aligned again by length 
based approach. 
Step 5: Repeat step 2, the score of every 
lignment is showed as a reference to human 
checking. 
 
4.3 Computer-Aided Checking 
It's obviously difficult to increase greatly the 
accuracy and robust of sentence alignment only by 
length based approach. So a lexicon checking 
process is added to our system. The alignment 
results obtained by length based approach are 
checked by an English-Chinese lexicon. A score SA 
is given to every alignment sentence pair. The 
score SA is calculated by following idea, that is, 
the twice number of correctly matched English 
words and Chinese words to the sum of number 
of English and Chinese words. In figure 2, the 
interface for human checking is showed in order 
to processes the noise Chinese-English parallel 
resource. 
 
4.4 Experiment Results 
 
We tested our alignment algorithm with part of a 
computer handbook (Sco Unix handbook). There 
are about 4681 English sentences and 4430 
Chinese sentences in this computer handbook 
after filter noisy figures and tables. The detail 
experiment result of automatic sentence 
alignment is show in table 4. The total precision 
is about 95%.  
 
 
Figure 2 Interface for Human Checking 
 
Class of 
Alignment 
No. of 
Aligned Sentence 
Pair 
No. of 
Correct 
Sentence Pair 
No. of 
Error 
Sentence Pair 
 
Precision 
1:1 2992 2957 35 98.83% 
1:2 238 211 27 88.66% 
2:1 414 352 62 85.02% 
2:2 113 97 16 85.84% 
1:3 35 24 11 68.57% 
3:1 75 49 26 65.33% 
2:3 13 6 7 46.15% 
3:2 22 16 6 72.72% 
3:3 6 3 3 50.00% 
0:1 3 2 1 66.67% 
1:0 7 4 3 75.00% 
Total 3918 3721 197 94.97% 
Table 4 The detail experiment result of automatic sentence alignment 
5 Bilingual Concordance Design 
We also designed a bilingual concordance tool 
used for discovering facts during the translation 
between Chinese and English. Besides a listing 
of the keywords with the contexts in which they 
appear, the correspondence translation sentence 
also be presented in this tool. The options may 
include bilingual concordances, sorting in a 
variety of orders, and producing basic text 
statistics. The intended interface is showed in 
figure 3.  
  
 
Figure 3 The Interface for bilingual Concordance 
 
6 Conclusion & Further Prospects 
In this paper, we introduce the developing project, 
that is, the constructing of a large-scale (above 
500,000 pair sentences) Chinese-English parallel 
corpus. The current status of Chinese corpora is 
overviewed with the emphasis on parallel corpus. 
The XML coding principles for Chinese?English 
parallel corpus are discussed. The sentence 
alignment algorithm used in this project is 
described with a computer-aided checking 
processing in order to processes the noise 
Chinese-English parallel resource.. We show the 
design of the bilingual concordance for the 
parallel corpus, also.  
 
As a beginning project, there is still much room 
for further development. The parallel resource is 
relative rare, so the new ways, such as, data 
exchange with other researcher institute and 
translation company, should be launched to 
obtain more parallel resource which can be used 
to research society. The coding principle should 
be adjusted in real work. A coding rule in more 
detail should form in near future. We also intend 
to add the option for recommendation the 
correspondence translation word for input 
keywords in concordance tool.   
 
Acknowledgements 
This work is supported by China 863 project 
(Grant No. 2001AA114040) and the National 
Science Fund of China under contact 69983009. 
Our thanks go to all the project members from 
five institutes for discussion and the anonymous 
reviewers for kind suggestions. . 
 
References  
Catherine N. Bal (1997), Tutorial: 
Concordances and Corpora, 
http://www.georgetown.edu/cball 
/corpora/tutorial.html  
D. Wu, (1994) Aligning a Parallel 
English-Chinese Corpus Statistically with 
Lexical Criteria, In Proceedings of the 32nd 
Annual Meeting of the Association for 
Computational Linguistics (ACL'94), 
pp.80-87 
I. D. Melamed. (1996)  Automatic Detection of 
Omissions in Transaltions, In Proceedings of  
the 16th International Conference on 
Computational Linguistics, Copenhagen, 
Denmark 
ISLE, International Standards for Language 
Engineering 
http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE
_Home_Page.htm 
J.S. Chang and M. H. Chen (1997)  An 
alignment method for noisy parallel corpora 
based on image processing techniques, In 
Proceedings of the 35th Meeting of the 
Association for Computational Linguistics, 
Madrid, pp. 297-304 
Kay M., and Roscheisen M. (1993). 
Text-Translation Alignment, Computational 
Linguistics, 19/1,pp.121-142 
Le Sun , Lin Du, Yufang Sun, Jin Youbin (1999) 
Sentence Alignment of English-Chinese 
Complex Bilingual Corpora. Proceeding of 
the workshop MAL'99, 135-139 
N. Ide, L. Romary (2001). A Common 
Framework for Syntactic Annotation 
Proceedings of ACL'2001, Toulouse, 298-305 
N. Ide, L. Romary, (2000) XML Support for 
Annotated Language Resources. Proceedings 
of the Workshop on Web-based Language 
Documentation and Description, 
Philadelphia, 148-153. 
N. Ide, P. Bonhomme,, L. Romary (2000). 
XCES: An XML-based Standard for 
Linguistic Corpora.. Proceedings of the 
Second Language Resources and Evaluation 
Conference (LREC), Athens, Greece, 825-30. 
P. F. Brown, J. C. Lai, and R. L. Mercer (1991) 
Aligning Sentences in Parallel Corpora, In 
Proceedings of the 29th Annual Meeting of 
the Association for Computational 
Linguistics (ACL'91), pp.169-176. 
P. Fung, and K. W. Church (1994)  K-vec: A 
New Approach for Aligning Parallel Texts, In 
Proceedings of the 15th International 
Conference on Computational Linguistics 
(COLING'94), Tokyo, Japan, pp. 1096-1102, 
Ph. Langlais, M. Simard, J. Veronis, 
S.Armstong, P. Bonhomme, F. Debili, P. 
Isabelle, E. Souissi, and P. Theron. (1998)  
Arcade: A cooperative research project on 
parallel text alignment evaluation. In First 
International Conference on Language 
Resources and Evaluation, Granada, Spain. 
S. F. Chen, (1993) Aligning Sentences in 
Bilingual Corpora Using Lexical Information. 
In Proceedings of the 31th Annual Meeting 
of the Association for Computational 
Linguistics, pp. 9-16 
Shiwen Yu, Xuefeng Zhu, Hui Wang, Yunyun 
Zhang (1998), The Grammatical 
Knowledge-base of Contemporary Chinese: 
A complete Specification. Tsinghua 
University Publishers 
Stig Johansson, Jarle Ebeling, Signe Oksefjell 
(1999), English-Norwegian Parallel 
Corpus:Manual, 
http://www.hf.uio.no/iba/prosjekt/ 
TEI (2002) The XML Version of the TEI Guidelines   http://www.hcu.ox.ac.uk/TEI/Guidelines/ 
W. A. Gale, and K. W. Church (1991) A Program 
for Aligning Sentences in Bilingual Corpora, 
In Proceedings of the 29th Annual Meeting of 
the Association for Computational Linguistics 
(ACL'91), pp. 177-184 
Zhiwei Feng (2001), The History and Current 
status of Chinese Corpus Research, 
International Conference on Chinese 
Computing ICCC2001, pp. 1-15 (In Chinese)  
 
