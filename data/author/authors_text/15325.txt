Proceedings of BioNLP Shared Task 2011 Workshop, pages 102?111,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP 2011 Task Bacteria Biotope ? The Alvis system 
Zorana Ratkovic1,2   Wiktoria Golik1    Pierre Warnier1   Philippe Veber1   Claire N?dellec1 
1 MIG INRA UR1077, Domaine de Vilvert F-850 Jouy-en-Josas, France forename.name@jouy.inra.fr   
2 LaTTiCe UMR 8094 CNRS Univ. Paris 3 1 rue Maurice Arnoux F-92120 MONTROUGE   Abstract 
This paper describes the system of the INRA Bibliome research group applied to the Bacteria Biotope (BB) task of the Bi-oNLP 2011 shared tasks. Bacteria, geo-graphical locations and host entities were processed by a pattern-based approach and domain lexical resources. For the extraction of environment locations, we propose a framework based on semantic analysis sup-ported by an ontology of the biotope do-main. Domain-specific rules were devel-oped for dealing with Bacteria anaphora. Official results show that our Alvis system achieves the best performance of participat-ing systems. 
1 Introduction Given a set of Web pages, the information extrac-tion goal of the Bacteria Biotope (BB) task is to precisely identify bacteria and their locations and to relate them. The type of the predicted locations has to be selected among eight types. Among them the host and host-part locations have to be related by the part-of relation. Three teams participated in the challenge.  BB task example Ureaplasma parvum is a mycoplasma and a pathogenic 
ureolytic mollicute which colonises 
 the urogenital tracts of humans.  One of the specificities of the BB task is that the bacteria location vocabulary is very large and vari-ous as opposed to protein subcellular locations in 
biology challenges (Kim et al, 2010) and geo-graphical locations (Zhou et al, 2005). Locations include natural environments and hosts as well as food and medical locations. In order to deal with this heterogeneity, we propose a framework based on a term analysis of the test corpus and a shallow mapping of these terms to a bacteria biotope (BB) termino-ontology. This mapping derives the type of location terms and filters out non-location terms. Large external dictionaries of host names (i.e. NCBI taxonomy) and geographical names (i.e. Agrovoc thesaurus) complete the lexical resources. The high frequency of bacteria anaphora and ambiguous antecedent candidates in the corpus was also a difficulty. Our Alvis system implements an anaphora resolution algorithm that takes into con-sideration the anaphoric distance and the position of the antecedent in the sentence. Alvis predicts the bacteria names and their relation to the locations with the help of hand-made patterns based on lin-guistic analysis and lexical resources.  The methods for predicting and typing locations (section 2) and bacteria (section 3) are first de-scribed. Section 4 details the method for relating them. Section 5 comments the experimental results. 2 Location  Our system handles separately the recognition of host and geographical names by dictionary map-pings, while the recognition of locations of the en-vironment and host part types is based on linguistic analysis and ontology inference.  Host names and geographical names appeared to be easier to predict by using a named-entity recog-nition strategy than the other types of location. They are less subject to variation than environ-mental locations, which can include any physical feature. For host name extraction, we used the NCBI taxonomy as the major source. Only the eu-karyote subtree was considered for host detection. 
Localization 
Part-of 
102
Our system filters out the ambiguous names such as Indicator (honeyguides) or Dialysis (xylophage insect) by comparing them to a list of common words in English. The host name list was enriched with additional common names including non-taxonomic host groups (e.g. herbivores), progeny names (e.g. calf) and human categories (e.g. pa-tient). The resulting host name list contains more than 1,800,000 scientific names and 60,000 com-mon names. The geographical name recognition component uses a small dictionary of all geo-graphic terms from the Agrovoc thesaurus sub-vocabularies. At first, we considered using the very rich resource GeoNames. However, it contains too many ambiguous names to be directly usable by short-term development. 2.1 Location of Environment type The identification of environment locations is done in two steps. First, the automatic extraction of all candidate terms from the test corpus, then the as-signment of a location type to these terms with the help of the Bacteria Biotope (BB) termino-ontology. The type assigned to a given term is the type of the closest concept label in the ontology. Since the BB termino-ontology was originally not structured according to the eight types, in order to be usable it first had to be enriched by the new concepts and then mapped to this topology.  Corpus term extraction. The corpus terms were automatically extracted by the AlvisNLP/ML pipe-line (Nedellec et al, 2008) with BioYatea (Nedel-lec et al, 2010). BioYatea is the version of Yatea (Hamon & Aubin, 2006) adapted to the biology domain. We modified BioYatea setting according to the training dataset study. We observed that most of the location terms in the training dataset are noun phrases with adjective modifiers (e.g. ro-dent nests) while prepositional phrases are rather rare (e.g. breaks in the skin). We set the term boundaries of BioYatea to include all prepositions except the of preposition. Considering other prepo-sitions such as with may yield syntactic attachment errors, thus we prefer the risk of incomplete terms to incorrect prepositional attachments. Bacteria Biotope ontology. We used the Bacte-ria Biotope (BB) termino-ontology for typing the extracted terms. It is under development for the study of bacteria phenotypes and habitats. The high level of the habitat part is structured in a manner similar to that proposed by the one level classifica-
tion by Floyd (Floyd et al, 2005). It has a fine-grained structure with the same goal as the general-ist EnvO habitat ontology (Field et al, 2008), but it focuses on bacteria phenotype and biotope model-ing. It includes a terminological level that records lexical forms of the concepts including terms, synonyms and variations. For the purpose of the challenge, the initial on-tology was manually completed using location concepts. The training corpus, as well as the habitat and isolation site fields of the GOLD database on sequenced prokaryotes (Liolios et al, 2009) are the main sources of location terms and synonyms. The analysis of the training corpus mainly led to the addition of adjectival forms of host parts (e.g. lym-phatic, intracellular) and human references (e.g. patient, infant, progeny).  The GOLD database isolation site field is a very rich source of bacteria location terms. It is filled by natural language descriptions of matters, natural habitats, hosts and geographical locations. For in-stance, the isolation site of Anoxybacillus flavi-thermus bacterium is waste water drain at the Wairakei geothermal power station in New Zea-land. The term analysis of GOLD isolation site en-tries yielded 3,415 location terms including 1,050 geographical names. Hundreds of these terms were manually added to the BB termino-ontology. The lack of time as well as the full sentence structure of the GOLD resource prevented us from correctly handling them in a fully automatic way. We are currently developing a method for the automatic alignment of the terms extracted from GOLD to the BB termino-ontology. Additionally, the GOLD habitat field provided around a hundred different terms that have been directly integrated into the BB termino-ontology. The current version of the habitat subpart of the BB termino-ontology contains 1,247 concepts and 266 synonyms.  Location types in Bacteria Biotope ontology. The BB termino-ontology has been developed pre-vious to the BB task and the structure of its habitat subpart does not reflect the eight location types of the task. In order to reuse the ontology for the BB task, we assigned types to each location concept. We manually associated the high level nodes of the location hierarchies to the eight BB task types. The types of the lower level concepts were then auto-matically inferred. For instance, the concept aquatic environment is tagged Water in the ontol-
103
ogy and all of its descendants lake, sea, ocean are of type Water as well. Local type exceptions were manually tagged. For instance, the waste tree in-cludes water-carried wastes of type Water and solid industrial residues of type Environment. This way all concepts in the resulting typed ontology were assigned a unique type. The concept types are then propagated to their associated term classes at the terminological level. For instance, underground water and its synonym subterranean water are both typed as Water. The resulting typed BB termino-ontology is then usable for deriving the types of the terms extracted from the test corpus. Derivation of location type. The BB termino-ontology scope is too limited for the correct predic-tion of all candidate term types by Boolean and exact comparison. From the 2,290 candidate terms of the test corpus, only 152 belong as such to the BB termino-ontology. We propose a method based on the head comparison of the candidate and BB terms for the derivation of the candidate term type.  The quality of the ontology-based annotation depends to a large extent on an accurate match be-tween the resource and the terms extracted from the corpus. Our method targets the syntactic structure of terms (candidate and BB terms) in order to gath-er the most of semantically similar terms. This approach differs from the ontology alignment and population methods that also use the information from the ontology structure in order to infer seman-tic relationships (e.g. hyponyms, meronyms) (Eu-zenat, 2007). It also differs from semantic annota-tion supported by context analysis such as distribu-tional semantics (Grefenstette, 1994) or Hearst pat-terns (Hearst, 1992). It belongs to the class of methods that focus on the morphology of the cor-pus terms, which use string-based (Levensthein, 1966, Jaro, 1989) or linguistic-based methods (Jac-quemin & Tzoukermann, 1999).  Even though the context-based approach should produce very good results, we chose a less time-consuming method that is easier and faster to set up, which is based on morphosyntactic analysis.  In our case, string similarity measures turn out to be irrelevant (laboratory rat does not mean rat labo-ratory). We observed that in candidate and BB terms, the head is very often the most informative element. Thus, the linguistic-based analysis of terms, in particular the head-similarity analysis (Hamon & Nazarenko, 2001), represents a promis-ing alternative. Our method is inspired by 
MetaMap (Aronson, 2001). MetaMap tags bio-medical corpora with the UMLS Metathesaurus by syntactic analysis that takes into account lexical heads of terms. The similarity scores computed by linguistically-based metrics are higher for terms whose heads have previously been analyzed.  The MetaMap method includes a variant compu-tation that maps acronyms, abbreviations, syno-nyms as well as derivational, inflectional and spell-ing variants. Our term typing method is less sophis-ticated and uses a few lexical variants due to the lack of a complete resource. Some ontology en-richment applications also use head-supported term matching, as in Desmontils (Desmontils et al, 2003). In Desmontils, new concepts belonging to WordNet (Fellbaum, 1998) are automatically added to the ontology in order to improve the indexing process. However, the analysis of the results shows that a great number of concepts found in the texts are not considered because they do not exist in WordNet. Our typing task uses a similar head-based method, but only for type derivation.  Our system derives the location type of candi-date terms in several steps. First, if there is a term in the BB termino-ontology that is strictly equal to the candidate term, it is assigned the same type. Then, the other candidate terms are assigned types according to the comparison of their heads to the BB term heads. We assume that in most of the cases the term head conveys the information about the type and is non-ambiguous. A given head H is non-ambiguous if all BB terms with head H are of the same type. The location term head set is the set of all habitat term heads found in the BB termino-ontology. The current version contains 693 differ-ent heads. Let Te denote the extracted term to be typed. If the head of Te does not belong to the BB term head set, then the type of Te is simply not Lo-cation (e.g. high metabolic diversity). If Te head does belong to the BB term head set and the head is non-ambiguous, then Te is assigned the associated type. For instance, the head of the extracted term stratified lake is lake. The type of all the BB terms with lake head is Water (e.g. meromictic lake). Stratified lake is therefore typed as Water.  Specific processing is applied to terms with am-biguous heads. The associative set of BB term heads and types exhibits some cases of ambiguous heads with multiple types that we analyzed in de-tail. There are two kinds of ambiguities that were 
104
processed in different ways. In the first, multiple types reflect different roles of the same object. In the second, the head is non-informative with re-spect to the type.  In the latter case the type is con-veyed by the subterm (term after head removal). We qualify non-informative BB term heads as neu-tral. They mainly denote habitats (habitat, envi-ronment, medium, zone) and extracts (sample, sur-face, isolate, material, content). In this case, the type is derived from the subterm. For instance, the head isolate of the extracted term marine isolate is neutral. After head removal, it is assigned the type Water since marine is of type Water. Freshwater has the same type as freshwater medium or fresh-water environment since medium and environment are neutral heads.  Some heads have more than one type although they denote specific locations. Their multiple types reflect different uses or states. For instance, the head bottle has two types: Food and Medical. The type Food is derived from the BB concept water bottle and the type Medical is derived from bedside water bottles in a hospital environment. The correct type for the extracted terms is then selected by a set of patterns based on the context of the term in the document. For instance, many vegetables and meats could be either of type Host or Food. The type is Host by default. One pattern states that if a term includes or is preceded by a food processing-related word (e.g. cooked, grilled, fermented), then the term is reassigned the type Food. Another pat-tern states that if a host is preceded by a death-related adjective (dead, decaying), then its type should be revised as Environment.  Our system currently includes nine disambigua-tion/retyping patterns. The first version of the type derivation method was automatically applied to the 1,263 GOLD terms after head analysis. Manual examination of the results yielded an extension of the two lists of neutral heads and heads with am-biguous types. There are 20 neutral heads and 21 ambiguous heads in the current version of the BB termino-ontology. The head-matching algorithm appears to be quite productive for the biotope terms. The procedure applied to the test corpus yielded the following figures: BioYatea extracted 2,290 terms. 416 terms matching the post-processing filters were discarded. This includes terms which are too general (i.e. approach, diver-sity), terms containing irrelevant or non desirable adjectives (i.e. numerous deficiencies, known spe-
cies) and terms containing forbidden words accord-ing to the annotation location rules (i.e. bacteria, pathogen, contaminated, parasite). Finally, 1,873 candidate terms were kept. Among these figures:  - 152  terms belong to the BB termino-ontology  - 90  terms were typed using the ontology heads - 6 terms with several types were handled by disambiguation patterns. We plan to extend the list of neutral heads and dis-criminate adjectives for type disambiguation by machine learning classification applied to the BB termino-ontology modifiers. Location entity boundary. The analysis of term extraction result from the training corpus shows that the predicted boundaries of locations were not fully consistent with the task annotation guidelines. Post-processing adjusts incorrect boundaries by filtering irrelevant words, packing and merging terms. Irrelevant words (e.g. contaminated, in-fected, host species, disease, inflammation) were removed from the location candidate terms inde-pendently of their types (e.g. contaminated Bach-man Road site vs. Bachman Road ; host plant vs. plant). Note that BioYatea extracts not only the maximum terms (e.g. contaminated Bachman Road site), but also their constituents (Bachman Road site, Bachman Road and site). Boundary adjust-ment often consists in selecting the relevant alter-native among the subterms.  Other boundary issues are handled by several patterns, which are applied after the typing stage. These patterns are type-dependent: each pattern only applies to one type or a subset of location types. When necessary, they shift the boundaries in order to include relevant modifiers.  They also split location terms or join adjacent location terms. BioYatea may have missed relevant modifiers be-cause of POS-tagging errors. For instance, if a na-tionality name precedes a location, then it is in-cluded (e.g. German oil field). Also, it frequently happens that hosts are modifiers of host parts (e.g. insect gut). BioYatea extracts the whole term and its constituents. The term is correctly typed as Host-part and the host modifier as Host. In order to avoid embedded locations, a specific pattern is de-voted to the splitting of these terms. In this way insect gut (Host-part) becomes insect (Host) and gut (Host-part). Most of these patterns involve several specific lexicons, including cardinal directions, relevant and 
105
irrelevant modifiers for each type of location, as well as types, which can be merged and split. The current resources were manually built by examin-ing the location terms of the training set and GOLD isolation fields. The acquisition of relevant and irrelevant modifiers could be automated by ma-chine learning. Some linguistic phenomena could be better handled by the customization of BioYa-tea. For instance BioYatea considers the preposi-tion with as a term boundary so it cannot extract terms containing with, like areas with high sulfur and salt concentrations.  3 Extraction of Bacteria names  We observed in the training corpus that not only were bacteria names tagged, but also higher level taxa (families) and lower level taxa (strains). We used the NCBI taxonomy as the main bacteria tax-on resource since it includes all organism levels and is kept up-to-date. This bacteria dictionary was enriched by taxa from the training corpus, in par-ticular by non standard abbreviations (e.g. Chl. = Chlorobium, ssp. = subsp) and plurals, (Vibrios as the plural for Vibrio) that were hopefully rather rare. Determining the boundaries of the bacteria names was one of the main issues because corpus strain names do not always follow conventional nomenclature rules.  Also, the recognition of bacte-ria name is evaluated using a strict exact match. Patterns were developed to account for such cases. They handle inversion (LB400 of Burkholderia xe-novorans instead of Burkholderia xenovorans LB400) and parenthesis (Tropheryma whipplei (the Twist strain) instead of Tropheryma whipplei strain Twist).  The corpus also mentions names of bacte-ria that contain modifiers not found in the NCBI dictionary, such as antimicrobial-resistant C. coli or L. pneumophila serogroup 1. Such cases, as well as abbreviations (e.g. GSB for green sulfur bacte-ria) and partial strain names (e.g. strain DSMZ 245 T for Chlorobium limicola strain DSMZ 245 T) were also specifically handled. The main source of error in bacteria name pre-diction is due to the mixture of family names and strain name abbreviations in the same text. It fre-quently happens that the strain name is abbreviated into the first word of the name. For instance Bar-tonella henselae is abbreviated as Bartonella. Un-fortunately, Bartonella is a genus mentioned in the 
same text, thus yielding ambiguities between the anaphora and the family name, which are identical. 3.1 Bacteria anaphora resolution Anaphors are frequent in the text, especially for bacteria reference and to a smaller extent for host reference. Our effort focused on bacteria anaphora resolution ignoring host anaphora. The extraction method of location relations (section 4) assumes that the relation arguments, location and bacterium (or anaphora of the bacterium) occur in the same sentence. From a total of 2,296 sentences in the training corpus, only 363 sentences contain both the location and the explicit bacterium, while 574 mention only the location. Two thirds of the loca-tions do not co-occur with bacteria. This demon-strates the importance of recovering the bacteria for these cases, which is potentially referred to by an explicit anaphora.  The manual examination of the training corpus showed that the most frequent anaphora of bacteria are not pronouns but higher level taxa, often pre-ceded by a demonstrative determinant, (i.e. This bacteria, This Clostridium) and sortal anaphora (i.e. genus, organism, species and strain), both of which are commonly found in biological texts (Torri & Vijay-Shanker, 2007). The style of some of the documents is rather relaxed and the antece-dent may be ambiguous even for a human reader. We observed three types of anaphora in the corpus. First, the standard anaphora which includes both pronouns and sortal anaphora, which requires a unique bacterial antecedent. Second, bi-anaphora or an anaphora that requires two bacteria antece-dents. This happens when the properties of two strains are compared in the document. Finally, the case of a higher taxon being used to refer to a lower taxon, which we named name taxon anaph-ora.  Anaphora with a unique antecedent C. coli is pathogenic in animals and humans. Peo-ple usually get infected by eating poultry that con-tained the bacteria, eating raw food, drinking raw milk, and drinking bottle water [?].  Anaphora with two antecedents C. coli is usually found hand in hand with its bac-teria relative, C. jejuni. These two organisms are recognized as the two most leading causes of acute inflammation of intestine in the United States and other nations. 
106
 Name taxon anaphora Ticks become infected with Borrelia duttonii while feeding on an infected rodent. Borrelia then multi-plies rapidly, causing a generalized infection throughout the tick.  For anaphora detection and resolution a pattern-based approach was preferred to machine learning because the constraints for relating anaphora to antecedent candidates of the same taxonomy level were mainly semantic and domain-dependent and the annotation of anaphora was not provided in the training corpus.  Anaphora detection consists of identifying po-tential anaphora in the corpus, given a list of pro-nouns, sortal anaphora and taxa and then filtering out irrelevant cases (Segura-Bedmar et al, 2010, Lin & Lian, 2004) before anaphora resolution. Not all the pronouns, sortal anaphora terms and higher taxon bacteria are anaphoric. For example, if a higher taxon is preceded or followed by the word genus, this signals that it is not anaphoric but that the text is actually about the higher taxon.   Non-anaphoric higher taxon Burkholderia cenocepacia HI2424[?] The genus Burkholderia consists of some 35 bacte-rial species, most of which are soil saprophytes and phytopathogens that occupy a wide range of environmental niches.  The anaphora resolution algorithm takes into ac-count two features: the distance to the antecedent candidate and its position in the sentence. The an-tecedent is usually found in proximity to the ana-phora, in order to maintain the coherence of the text. Therefore, our method ranks the antecedent candidates according to the anaphoric distance counted in sentences.  If more than one bacterium is found in a given sentence, their position is discriminate. Centering theory states that in a sentence the most prominent entities and therefore the most probable antecedent candidates are in the order: subject > object > other position (Grosz et al, 1995). In English, due to the SVO order of the language the subject is most of-ten found at the beginning of the sentence, fol-lowed by the object and the others. Therefore, the method retains the leftmost bacterium in the sen-tence when searching for the best antecedent can-didate. 
More precisely, the method selects the first ante-cedent that it finds according to the following pre-cedence list: - First bacterium in the current sentence (s) - First bacterium in the previous sentence    (s-1) - First bacterium in sentence s-2 - First bacterium in sentence s-3 - First bacterium in the current paragraph - Last bacterium in the previous paragraph - First bacterium in the first sentence of the document - The first bacterium ever mentioned. -  The method only relates anaphora to antecedents that are found before. It does not handle cataphors since they are rarely found in the corpus. For ana-phors that require two antecedents we use the same criteria but search for two bacteria in each sentence or paragraph, instead of one. For taxon anaphora we look for the presence of a lower taxon in the document found before the anaphora that is com-patible according to the species taxonomy. The counts of anaphora detected by the patterns are given in Table 1.   Corpus Single ante Bi ante Taxon ante Train 933 4 129 Dev 204 3 22 Test 240 0 18 Total 1,377 7 169  Table 1. The count of the types of anaphora per corpus.  The anaphora resolution algorithm allowed us to retrieve more sentences that contain both a bacte-rium and a location.  Out of the 574 sentences that contain only a location, 436 were found to contain an anaphora related to at least one bacterium. The remaining 138 sentences are cases where there is no bacterial anaphora or the bacterium name is im-plicit. It frequently happens that the bacterium is referred to through its action. For example in the sentence below, the bacterium name could be de-rived from the name of the disease that it causes.   In the 1600s anthrax was known as the "Black bane" and killed over 60,000 cows.  One of the questions we had about the resolution of anaphora is whether anaphora that are found in the same sentence together with a bacterium (there-fore potentially its antecedent) should be consid-
107
ered or not.  We tested this on the development set. We found that removing such anaphora from con-sideration improved the overall score. It yielded an F-score of 53.22% (precision: 46.17%, recall: 62.81%), compared to the original F-score of 50.15% (precision: 41.06%, recall: 64.44%). This improvement in F-score is solely due to an increase in precision, which shows that while resolving anaphora is important and required, the incorrect recognition of terms as anaphora and incorrect anaphora resolution can introduce noise. 4 Relation extraction In this work we concentrated most of our effort on the prediction of entities. For the prediction of events we used a strategy based on the co-occurrence of  arguments and trigger words within a sentence: - If a bacteria name, a location and a trigger word are present in a sentence, then the system pre-dicts a Localization event between the bacte-rium and the location. - If a bacteria anaphora, a location and a trigger word are present in a sentence, then the system predicts a Localization event between each ana-phora antecedent and the location. - If a host, a host part, a bacterium and at least one trigger word are present in a sentence, then the system predicts a PartOf event between the host and the host part.  The list of trigger words contains 20 verbs (e.g. inhabit, colonize, but also discover, isolate), 16 disease markers (e.g. chronic, pathogen) and 19 other relevant words (e.g. ingest, environment, niche). This list was designed by ranking words in the sentences of the training corpus containing both a bacteria name and a location. The ranking crite-rion used was the information gain with respect to whether the sentence contained an event or not. The ranked list was adjusted by removing spurious words and adding domain knowledge words. By removing the constraint of the occurrence of a trigger word in the sentence, we can determine that the maximum recall the method can achieve with this strategy is 47% (precision: 41%, F-score: 44%). The selected trigger word list yielded a re-call close to the maximum, thus it seems that the trigger words do not affect the recall and are suit-able for the task.  
5 Results Table 2 summarizes the official scores that the Bib-liome Alvis system achieved for the Bacteria Biotope Task. It ranked first among three partici-pants. The first column gives the recall of entity prediction. The prediction of hosts and bacteria named-entities achieved a good recall of 84 and 82, respectively.    Entity recall Event recall Event Precis. F-score Bacteria 84 - - - Host 82 61 48 53 Host part 72 53 42 47 Env. 53 29 24 26 Geo. 29 13 38 19 Food - - 29 41 Medical 100 50 33 40 Water 83 60 55 57 Soil 86 69 59 63 Total  45 45 45  Table 2. Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011.  However, geographical locations based on a similar strategy were poorly predicted (29%). Our system predicted only 15 countries. A more appropriate resource of geographical names than the Agrovoc thesaurus would certainly increase the recall of geographical locations.  The host parts, medical, water and soil locations predicted with the same ontology-based method were surprisingly good with a recall of 72, 100, 83 and 86, respectively. The small size of the ontology and the small number of different term heads (i.e. 51 different heads) initially appeared as a limitation factor for reuse on new corpora. The good recall shows that the location vocabulary of the test set has similarities with the training set compared to potential space of location names.  The potential space is reflected by the richness of the GOLD iso-lation site field. This demonstrates the robustness of the type derivation approach based on term heads. The correctness of the derivation type can-not be calculated without a corpus where all the locations and not only bacteria ones are annotated. The recall of the environment location prediction is a little bit lower, 53%. The environment type in-
108
cludes many different types that cannot all be an-ticipated. Therefore the coverage of the BB ter-mino-ontology environment part is limited except for water and soil, which are more focused topics.  The localization event recall (column 2) is on average 20% lower for all types than the location entity recall. The regularity of the difference may suggest that once the argument is identified, the localization relation is equally harder to find by our method independently of the type. The localization event precision (column 3) is more difficult to ana-lyze because many sources of error may be in-volved, such as an incorrect arguments, incorrect anaphora resolution, relation to the wrong bacte-rium among several or the absence of a relation.  The prediction precision of localization events involving soil, water and host is better than envi-ronment and food. The manual analysis of the test corpus shows that in some cases environmental locations were mentioned as potential sources of industrial applications without actually being bac-teria isolation places. For instance, in Other fields of application for thermostable enzymes are starch-processing, organic synthesis, diagnostics, waste treatment, pulp and paper manufacture, and ani-mal feed and human food, the Alvis system errone-ously predicted waste treatment, paper manufac-ture, animal feed and human food.  This is due to the fact that the system does not handle modalities. Such hypotheses are specific to the BB task text genre, i.e. Bacteria sequencing projects. Such pro-jects contain details for potential industrial applica-tions, which are absent from academic literature. Ambiguous types are also a source of error. De-spite the host dictionary cleaning, some ambigui-ties remained. For example, the head canal in tooth root canal is erroneously typed as water and should be disambiguated with its tooth host-part modifier.  After test publication we measured the gain of anaphora resolution by using the on-line service. The anaphora resolution algorithm was found to have a strong impact on the final result.  Running the test set using all of the modules except for the anaphora resolution algorithm yielded a decrease in the F-score by almost 13% (F-score: 32.5%, preci-sion: 48.5%, 24.4%).  This shows that the addition of an anaphora resolution algorithm significantly increases the precision and that a resolution algo-rithm adapted to the Bacteria domain is necessary for the Biotope corpus. 
The part-of event prediction relies on the strict co-occurrence of a bacterium, trigger word, host and host part within a sentence. An additional run with the more relaxed constraint where the bacte-rium can be denoted by an anaphora as well yielded a gain of 6 recall points, a loss of 5 preci-sion points with a net benefit of 1 F-measure point. 6 Discussion The use of trigger words for the selection of sen-tences for relation extraction does not take into ac-count the structure or syntax of the sentence for the prediction of relation arguments. The system pre-dicts all combinations of bacteria and locations as localization events and all combination of host and host parts as part-of event. This has a negative ef-fect on the precision measure since some pairs are irrelevant as in the sentence below.  Baumannia cicadellinicola. This newly discovered or-ganism is an obligate endosymbiont of the leafhopper insect Homalodisca coagulata (Say), also known as the Glassy-Winged Sharpshooter, which feeds on the xylem of plants.  It has been shown that the use of syntactic de-pendencies to extract biological events (such as protein-protein interactions) improves the results of such systems (Erkan et al, 2007, Manine et al, 2008, Airola et al 2008). The use of syntactic de-pendencies could offer a more in depth examina-tion of the syntax and the semantics and therefore allow for a more refined extraction of bacteria-localization and host-host part relations.   Term extraction appears to be a good method for predicting locations including unseen terms, but it is limited by the typing strategy that filters out all terms with unknown heads (with respect to the BB termino-ontology). In the future, we will study the effect of linguistic markers such as enumeration and exemplification structures for recovering addi-tional location terms. For instance, in heated or-ganic materials such as compost heaps, rotting hay, manure piles or mushroom growth medium, our system has correctly typed heated organic ma-terials as environment but not the other examples because of their unknown heads. The promising performance of the Alvis system on the BB task shows that a combination of semantic analysis and domain-adapted resources is a good strategy for information extraction in the biology domain. 
109
References  Agrovoc: http://aims.fao.org/website/AGROVOC-Thesaurus Antti Airola, Sampo Pyysalo, Jari Bj?rne, Tapio Pah-nikkala, Filip Ginter, and Tapio Salakoski. 2008. A Graph Kernel for Protein-Protein Interaction Extrac-tion. BioNLP2008: Current Trends in Biomedical Natural Language Processing, pages 1-9. Alan R. Aronson. 2001. Effective mapping of biomedi-cal  text to the UMLS Metathesaurus: The MetaMap program. Proceedings of AMIA Symposium 2001, pages 17-21. Emmanuel Desmontils, Christine Jacquin and  Laurent Simon. 2003. Ontology enrichment and indexing process. Research report RR-IRIN-03.05, Institut de Recherche en Informatique de Nantes, Nantes, Fran-ce. J?r?me Euzenat and Pavel Shvaiko. 2007. Ontology matching, Springer Verlag, Heidelberg (DE),page 333. Dawn Field et al 2008. Towards a richer description of our complete collection of genomes and metage-nomes: the Minimum Information about a Genome Sequence (MIGS) specification. Nature Biotechnol-ogy 26, pages 541-547. GeoNames: http://www.geonames.org/  Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery.  Natural Language Processing and Machine Translation. London: Kluwer Academic Publishers. Barbara J. Grosz, Araving K. Joshi and Scott Weinstein. 1995. Centering: A Framework for Modelling the Lo-cal Coherence of Discourse.  University of Pennsyl-vania Institute for Research in Cognitive Science Technical Reports Series. G?ne? Erkan, Arzucan ?zg?r and Dragomir R. Radev. 2007. Semi-Supervised Classification for Extracting Protein Interaction Sentences using Dependency Parsing. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pag-es 228-237. Thierry Hamon and Sophie Aubin. 2006. Improving term extraction with terminological resources. In Salakoski, T. et al, editors, Advances in Natural Lan-guage Processing 5th International Conference on NLP (Fin- TAL?06), pages 380?387. Springer. Thierry Hamon and Adeline Nazarenko. 2001. Detection of synonymy links between terms: experiment and re-
sults, Recent Advances in Computational Terminol-ogy. Pages 185-208. John Benjamins. Marti A. Hearst. 1992. Automatic acquisition of hypo-nyms from large text corpora. In Zampolli, A.(ed.), Proceedings of the 14 th COLING, pages 539?545, Nantes, France. Christian Jacquemin and Evelyne Tzoukermann. 1999. NLP for term variant extraction: A synergy of mor-phology, lexicon, and syntax. In Strzalkowski, T. (ed.), Natural language information retrieval, volume 7 of Text, speech and language technology, chapter 2, pages  25?74. Dordrecht & Boston: Kluwer Aca-demic Publishers. Matthew A. Jaro. 1989. Advances in record linkage me-thodology as applied to matching the 1985 census of Tampa, Florida. Journal of the American Statistical Association 84(406), pages 414-20. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano and Jun?ichi Tsujii. (to appear). Extract-ing bio-molecular events from literature - the Bi-oNLP?09 shared task. Special issue of the Interna-tional Journal of Computational Intelligence. Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Dok-lady akademii nauk SSSR, 163(4):845-848, 1965. In Russian. English translation in Soviet Physics Dok-lady, 10(8), pages 707-710. Yu-Hsiang Lin and Tyne Liang. 2004. Pronomial and Sortal Anaphora Resolution for Biomedical Litera-ture. In Proceedings of ROCLING XVI: Conference on Computational Linguistics and Speech Processing.  Konstantinos Liolios, I-Min A. Chen., Konstantinos Mavromatis, Nektarios Tavernarakis, Philip Hugen-holtz, Victor M. Markowitz and Nikos C. Kyrpides. 2009. The Genomes On Line Database (GOLD) in 2009: status of genomic and metagenomic projects and their associated metadata. NAR Epub. Alain-Pierre Manine, Erick Alphonse and Philippe Bes-si?res. 2008. Information extraction as an ontology population task and its application to genic interac-tions, 20th IEEE Intl. Conf. Tools with Artificial In-telligence, ICTAI'08., vol. II, pp. 74-81. NCBI taxonomy: http://www.ncbi.nlm.nih.gov/Taxonomy/  Claire N?dellec, Wiktoria Golik, Sophie Aubin and Robert Bossy. 2010. Building Large Lexicalized On-tologies from Text: a Use Case in Indexing Biotech-nology Patents, International Conference on Knowl-edge Engineering and Knowledge Management (EKAW 2010), Lisbon, Portugal. 
110
Isabel Segura-Bedmar, Mario Crespo, C?sar de De Pa-blo-S?nchez and Paloma Mart?nez. 2010. Resolving anaphoras for the extraction of drug-drug interactions in pharmacological documents. BMC Bioinformatics 11(Supl 2):S1. Manabu Torii and K. Vijay-Shanker. 2007. Sortal Anaphora Resolution in Medline Abstracts. Computa-tional Intelligence 23, pages 15-27. Zhou GuoDong, Su Jian, Zhang Jie and Zhang Min. 2005. Exploring Various Knowledge in Relation Ex-traction. In Proceedings of the 43rd Annual Meeting of the ACL, pages 427-434, Ann Arbor. Association for Computational Linguistics.  
111
Proceedings of BioNLP Shared Task 2011 Workshop, pages 121?129,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Sentence Filtering for BioNLP: Searching for Renaming Acts
Pierre Warnier1,2 Claire Ne?dellec1
1MIG INRA UR 1077, F78352 Jouy-en-Josas, France
2LIG Universite? de Grenoble, France
forename.lastname@jouy.inra.fr
Abstract
The Bacteria Gene Renaming (RENAME)
task is a supporting task in the BioNLP Shared
Task 2011 (BioNLP-ST?11). The task con-
sists in extracting gene renaming acts and gene
synonymy reminders in scientific texts about
bacteria. In this paper, we present in details
our method in three main steps: 1) the doc-
ument segmentation into sentences, 2) the re-
moval of the sentences exempt of renaming act
(false positives) using both a gene nomencla-
ture and supervised machine learning (feature
selection and SVM), 3) the linking of gene
names by the target renaming relation in each
sentence. Our system ranked third at the of-
ficial test with 64.4% of F-measure. We also
present here an effective post-competition im-
provement: the representation as SVM fea-
tures of regular expressions that detect com-
binations of trigger words. This increases the
F-measure to 73.1%.
1 Introduction
The Bacteria Gene Renaming (Rename) supporting
task consists in extracting gene renaming acts and
gene synonymy reminders in scientific texts about
bacteria. The history of bacterial gene naming has
led to drastic amounts of homonyms and synonyms
that are often missing in gene databases or even
worse, erroneous (Nelson et al, 2000). The auto-
matic extraction of gene renaming proposals from
scientific papers is an efficient way to maintain gene
databases up-to-date and accurate. The present work
focuses on the recognition of renaming acts in the
literature between gene synonyms that are recorded
in the Bacillus subtilis gene databases. We assume
that renaming acts do not involve unknown gene
names. Instead, our system verifies the accuracy of
synonymy relations as reported in gene databases by
insuring that the literature attests these synonymy re-
lations.
1.1 Example
This positive example of the training corpus is rep-
resentative of the IE task:
?Thus, a separate spoVJ gene as defined by the 517
mutation does not exist and is instead identical with
spoVK.?
There are 2 genes in this sentence:
ID Start End Name
T1 17 22 spoVJ
T2 104 109 spoVK
Table 1: Example of provided data.
There is also a renaming act: R1 Renaming For-
mer:T1 New:T2
Given all gene positions and identifications (Tn),
the Rename task consists in predicting all renaming
acts (Rn) between Bacillus subtilis genes in multi-
sentence documents. The gene names involved are
all acronyms or short names. Gene and protein
names often have both a short and a long form. Link-
ing short to long names is a relatively well-known
task but linking short names together remains lit-
tle explored (Yu et al, 2002). Moreover, specifying
some of these synonymy relations as renaming ap-
pears quite rare (Weissenbacher, 2004). This task
relates to the more general search of relations of
121
synonymous nicknames, aliases or pseudonyms of
proper nouns from definitory contexts in encyclope-
dia or dictionaries. For instance, in Alexander III
of Macedonia commonly known as Alexander the
Great the synonymy relation is supported by com-
monly known as between the proper noun Alexan-
der III of Macedonia and the nickname Alexander
the Great. Renaming act extraction differs from the
search of coreferences or acronyms by the linguistic
markers involved.
1.2 Datasets
The renaming corpus is a set of 1,648 PubMed refer-
ences of bacterial genetics and genome studies. The
references include the title and the abstract. The
annotations provided are: the position and name of
genes (see Table 1) for all sets and the renaming acts
in the training and the development sets only.
Train Dev. Test
Documents 1146 246 252
Genes 14372 3331 3375
Unique Genes 3415 1017 1126
New genes 0 480 73
Relations 308 65 88
Words / Doc 209 212 213
Genes / Doc 12.5 12.7 13.4
Unique Genes / Doc 3.0 4.1 4.5
Relations / Doc 0.27 0.26 0.35
Table 2: Datasets of the Rename task corpus.
2 Methods
An early finding is that renaming acts very seldom
span several sentences (i.e. former and new are in
the same sentence). For the training set, 95.4% of
the relations verify this claim and in the develop-
ment set, 96.1%. Thus, it is decided to first segment
the documents into sentences and then to look for re-
naming acts inside independent sentences. Thus the
maximum expected recall is then 96.1% on the de-
velopment set. This is done by automatically filter-
ing all the sentences out that do not contain evidence
of a renaming act and then to relate the gene names
occurring in the renaming sentences. The AlvisNLP
pipeline (Ne?dellec et al, 2009) is used throughout
this process (see Fig. 1).
List based filtering
Machine learning based filtering
Attribute selection on lemmas
(AnnotationClassifierAttributeSelection)
Classification: grid search
(AnnotationClassifierTrain)
Selection of best parameters
Bacteria 
Nomenclature
Tagging
(AnnotationClassifierTag)
Lemmatization
(TreeTagger)
Gene search
.a2 files
Fix forms
(SimpleContentProjector)
Word segmentation
(WoSMIG)
Sentence segmentation
(SeSMIG)
Genes
Species
Molecules
Acronyms (imp)
Abreviations (imp)
Bacteria (imp)
Stop words
Bacteria
Regular 
expressions (imp)
Figure 1: Flowchart: Notes represent the resources used
and (imp) represent later improvements not used for the
official submission.
2.1 Word and sentence segmentation
Word and sentence segmentation is achieved by the
Alvis NLP pipeline. Named entity recognition sup-
plements general segmentation rules.
2.1.1 Derivation of boundaries from named
entities
Named entities often contains periods that should
not be confused with sentence ends. Species abbre-
viations with periods are specially frequent in the
task corpus. First, dictionaries of relevant named
entities from the molecular biology domain (e.g.
122
genes, species and molecules) are projected onto
the documents before sentence segmentation, so
that periods that are part of named entities are dis-
ambiguated and not interpreted as sentence ends.
Moreover, named enties are frequently multi-word.
Named entity recognition prior to segmentation pre-
vents irrelevant word segmentation. For example,
the projection of named entity dictionaries on the ex-
cerpt below reveals the framed multi-word entities:
?Antraformin, a new inhibitor of Bacillus subtilis
transformation. [...] During this screening program,
Streptomyces sp. 7725-CC1 was found to produce
a specific inhibitor of B. subtilis transformation.?
2.1.2 Word segmenter
The word segmenter (WosMIG in Fig. 1) has the
following properties: 1) primary separator: space,
2) punctuation isolation: customized list, 3) custom
rules for balanced punctuation, 4) fixed words: not
splittable segments The following list of terms is
obtained from the example:
[?Antraformin? , ?,?, ?a?, ?new?, ?inhibitor?, ?of?,
? Bacillus subtilis ?, ?transformation?, ?.?, [...],
?During?, ?this?, ?screening?, ?program?, ?,?,
? Streptomyces sp. ?, ? 7725-CC1 ?, ?was?, ?found?,
?to?, ?produce?, ?a?, ?specific?, ?inhibitor?, ?of?,
? B. subtilis ?, ?transformation?, ?.?]
2.1.3 Sentence segmenter
The sentence segmenter (SeSMIG in Fig. 1) has
the following properties: 1) strong punctuation:
customized list; 2) tokens forcing the end of a
sentence (e.g. etc...); 3) an upper case letter must
follow the end of a sentence. The system works
very well but could be improved with supervised
machine learning to improve the detection of
multi-word named entities. Finally, the list of words
is split into sentences:
[[?Antraformin? , ?,?, ?a?, ?new?, ?inhibitor?, ?of?,
? Bacillus subtilis ?, ?transformation?, ?.?],
[...],
[?During?, ?this?, ?screening?, ?program?, ?,?,
? Streptomyces sp. ?, ? 7725-CC1 ?, ?was?, ?found?,
?to?, ?produce?, ?a?, ?specific?, ?inhibitor?, ?of?,
? B. subtilis ?, ?transformation?, ?.?]]
2.2 Sentence filtering
Once the corpus is segmented into sentences, the
system filters out the numerous sentences that most
likely do not contain any renaming act. This way,
the further relation identification step focuses on rel-
evant sentences and increases the precision of the
results (Nedellec et al, 2001). Before the filtering,
the recall is maximum (not 100% due to few renam-
ing acts spanning two sentences), but the precision
is very low. The sentence filters aim at keeping the
recall as high as possible while gradually increasing
the precision. It is composed of two filters. The first
filter makes use of an a priori knowledge in the form
of a nomenclature of known synonyms while the
second filter uses machine learning to filter the re-
maining sentences. In the following, the term Bacil-
lus subtilis gene nomenclature is used in the sense of
an exhaustive inventory of names for Bacillus sub-
tilis genes.
2.2.1 Filtering with a gene nomenclature
We developed a tool for automatically building
a nomenclature of Bacillus subtilis gene and pro-
tein names. It aggregates the data from various
gene databases with the aim of producing the most
exhaustive nomenclature. The result is then used
to search for pairs of synonyms in the documents.
Among various information on biological sequences
or functions, the entries of gene databases record
the identifiers of the genes and proteins as asserted
by the biologist community of the species. Bacil-
lus subtilis community as opposed to other species
has no nomenclature committee. Each database cu-
rator records unilateral naming decisions that may
not reported elsewhere. The design of an exhaus-
tive nomenclature require the aggregation of multi-
ple sources.
Databases Our sources for the Bacillus subtilis
nomenclature are six publicly available databases
plus an in-house database. The public databases
are generalist (1 to 3) or devoted to Bacillus subtilis
genome (4 to 6) (see Table 3):
GenBank The genetic sequence database managed
by the National Center for Biotechnology In-
formation (NCBI) (Benson et al, 2008). It con-
tains the three official versions of the annotated
123
genome of B. subtilis with all gene canonical
names;
UniProt the protein sequence database managed by
the Swiss Institute of Bioinformatics (SIB),
the European Bioinformatics Institute (EBI)
and the Protein Information Resource (PIR)
(Bairoch et al, 2005). It contains man-
ual annotated protein sequences (Swiss-Prot)
and automatically annotated protein sequences
(TrEMBL (Bairoch and Apweiler, 1996)). Its
policy is to conserve a history of all informa-
tion relative to these sequences and in particu-
lar all names of the genes that code for these
sequences.
Genome Reviews The genome database managed
by the European Bioinformatics Institute (EBI)
(Sterk et al, 2006). It contains the re-annotated
versions of the two first official versions of the
annotated genome of B. subtilis;
BSORF The Japanese Bacillus subtilis genome
database (Ogiwara et al, 1996);
Genetic map the original genetic map of Bacillus
subtilis;
GenoList A multi-genome database managed by
the Institut Pasteur (Lechat et al, 2008). It con-
tains an updated version of the last official ver-
sion of the annotated genome of B. subtilis;
SubtiWiki A wiki managed by the Institute for Mi-
crobiology and Genetics in Go?ttingen (Flo?rez
et al, 2009) for Bacillus subtilis reannotation.
It is a free collaborative resource for the Bacil-
lus community;
EA List a local lexicon manually designed from
papers curation by Anne Goelzer and E?lodie
Marchadier (MIG/INRA) for Systems Biology
modeling (Goelzer et al, 2008).
Nomenclature merging We developed a tool for
periodically dumping the content of the seven source
databases through Web access. With respect to gene
naming the entries of all the databases contain the
same type of data per gene:
? a unique identifier (required);
? a canonical name, which is the currently rec-
ommended name (required);
? a list of synonyms considered as deprecated
names (optional).
The seven databases are handled one after the
other. The merging process follows the rules:
? the dump of the first database (SubtiWiki, see
Table 3 for order) in the list is considered the
most up-to-date and is used as the reference
for the integration of the dumps of the other
databases;
? for all next dumps, if the unique gene identifier
is new, the whole entry is considered as new
and the naming data of the entry is added to the
current merge;
? else, if the unique identifier is already present
into the merge, the associated gene names are
compared to the names of the merge. If the
name does not exist in the merge, it is added to
the merge as a new name for this identifier and
synonym of the current names. The synonym
class is not ordered.
Order Databases AE AN
1 SubtiWiki 4 261 5920
2 GenoList 0 264
3 EA List 33 378
4 BSORF 0 42
5 UniProt 0 74
6 Genome 0 0
Reviews
7 GenBank 0 7
8 Genetic Map 0 978
Total 4 294 7 663
Table 3: Database figures. AE: number of added entries,
AN: number of added names.
Synonym pair dictionary: The aggregated
nomenclature is used to produce a dictionary of all
combinations of pairs in the synonym classes.
124
Sentence filtering by gene cooccurrence: For
each sentence in the corpus, if a pair of gene syn-
onyms according to the lexicon is found inside then
the sentence is kept for the next stage. Other-
wise, it is definitively discarded. The comparison
is a case-insensitive exact match preserving non al-
phanumeric symbols. The recall at this step is re-
spectively 90.9% and 90.2% on the train and devel-
opment sets. The recall loss is due to typographic
errors in gene names in the nomenclature. The pre-
cision at this stage is respectively 38.9% and 38.1%
on the train and development sets. There are still
many false positives due to gene homologies or re-
naming acts concerning other species than Bacillus
subtilis for instance.
2.2.2 Sentence filtering by SVM
Feature selection The second filtering step aims
at improving the precision by machine learning clas-
sification of the remaining sentences after the first
filtering step. Feature selection is applied to enhance
the performances of the SVM as it is shown to suffer
from high dimensionality (Weston et al, 2001). Fea-
ture selection is applied to a bag-of-word representa-
tion using the Information Gain metrics of the Weka
library (Hall et al, 2009). Words are lemmatized by
TreeTagger (Schmid, 1994). A manual inspection
of the resulting sorting highly ranks words such as
formerly or rename and parentheses while ranking
other words such as cold or encode surprisingly cer-
tainly due to over-fitting. Although the feature se-
lection is indeed not particularly efficient compared
to the manual selection of relevant features but does
help filtering out unhelpful words and then drasti-
cally reducing the space dimension from 1919 to
141 for the best run.
Sentence classification and grid search: A SVM
algorithm (LibSVM) with a RBF kernel is applied
to the sentences encoded as bag of words. The two
classes are: ?contains a renaming act? (True) or not
(False). There are 4 parameters to tune: 1) the num-
ber of features to use (N ? 1, 5, 10, ..., 150) mean-
ing the N first words according to the feature selec-
tion, 2) the weight of the classes: True is fixed to 1
and False is tuned (W ? 0.2, 0.4, ..., 5.0), 3) the er-
rors weight (C ? 2?5,?7,...,9), 4) the variance of the
Gaussian kernel (G ? 2?11,?9,...,1). Thus, to find
the best combination of parameters for this problem,
#N ?#W ?#C ?#G = 31 ? 25 ? 8 ? 7 = 43, 400
models are trained using 10-fold cross-validation on
the training and development sets together (given
the relatively small size of the training set) and
ranked by F-measure. This step is mandatory be-
cause the tuning of C and G alone yield variations
of F-measure from 0 to the maximum. The grid
search is run on a cluster of 165 processors and takes
around 30 minutes. The best model is the model
with the highest F-measure found by the grid search.
Test sentence filtering: Finally the test set is sub-
mitted to word and sentence segmentation, feature
filtering and tagged by the best SVM model (Anno-
tationClassifierTag in Fig. 1). The sentences that are
assumed to contain a renaming act are kept and the
others are discarded (see Fig. 2).
2.3 Gene position searching
At this step, all remaining sentences are assumed to
be true positives. They all contain at least one pair
of genes that are synonymous according to our gene
nomenclature. The other gene names are not con-
sidered. The method for relating gene candidates by
a renaming relation, relies on the assumption that
all gene names are involved in at least one relation.
Most of the time, sentences contain only two genes.
We assume in this case that they are related by a re-
naming act. When there are more than two genes
in a sentence, the following algorithm is applied: 1)
compute all combinations of couples of genes; 2)
look-up the lexicon for those couples and discard
those that are not present; 3) if a given gene in a
couple has multiple occurrences, take the nearest in-
stance from the other gene involved in the renaming
act.
3 Discussion
The system ranks 3rd/3 among three participants
in the Rename task official evaluation with a F-
measure of 64.4% (see Fig. 4), five points behind the
second. The general approach we used for this task
is pragmatic: 1) simplify the problem by focusing on
sentences instead of whole documents for a minimal
loss, 2) then use a series of filters to improve the pre-
cision of the sentence classification while keeping
the recall to its maximum, 3) and finally relate gene
125
names known to be synonymous inside sentences for
a minimal loss (around 2% of measure). As opposed
to what is observed in Gene Normalization tasks
(Hirschman et al, 2005), the Rename task is char-
acterised by the lack of morphological resemblance
of gene synonyms. The gene synonyms are not ty-
pographic variants and the recognition of renaming
act requires gene context analysis. The clear bottle-
neck of our system is the sentence filtering part and
in particular the feature selection that brings a lot
of noise by ranking statistically spurious terms. On
the plus side, the whole system is fully automated
to the exception of the resources used for the word
segmentation that were designed manually for other
tasks. Moreover, our strategy does not assume that
the gene pairs from the nomenclature may be men-
tioned for other reasons than renaming, it then tends
to overgeneralize. However, many occurrences of
the predicted gene pairs are not involved in renaming
acts because the reasons for mentioning synonyms
may be different than renaming. In particular, equiv-
alent genes of other species (orthologues) with high
sequence similarities may have the same name as in
Bacillus subtilis. An obvious improvement of our
method would consists in first relating the genes to
their actual species before relating the only Bacillus
subtilis gene synonyms by the renaming relation.
Team Pre. Rec. F-M.
U. of Turku 95.9 79.6 87.0
Concordia U. 74.4 65.9 69.9
INRA 57.0 73.9 64.4
Table 4: Official scores in percentage on the test set.
3.1 Method improvement by IE patterns
After the official submission and given the result of
our system compared to competitors, a simple mod-
ification of the feature selection was tested with sig-
nificant benefits: the addition of regular expressions
as additional features. Intuitively there are words or
patterns that strongly appeal to the reader as impor-
tant markers of renaming acts. For example, vari-
ations of rename or adverbs such originally or for-
merly would certainly be reasonable candidates. Fif-
teen such shallow patterns were designed (see Table
5) supplemented by six more complex ones, orig-
inally designed to single out gene names. In ap-
pendix A, one of them is presented, the precision
of which is 95.3% and recall 27.5%. That is, more
than a quarter of renaming acts in the training and
development sets together. Interestingly, in table
5 the word formerly (3rd in feature selection rank-
ing) alone recalls 10.7% of the renaming acts with
a precision of 96.9%. In contrast, the words origi-
nally and reannotated although having 100% preci-
sion are respectively ranked 33rd and 777th. In total,
21 patterns are represented as boolean features of
the classification step in addition to the ones selected
by feature selection. Unsurprisingly, the best classi-
fiers, according to the cross-validation F-measure af-
ter the grid search, only used the regular expressions
as features neglecting the terms chosen by feature
selection. A significant improvement is achieved:
+8.7% of F-measure on the test set (see Fig. 2).
Pattern Pre. Rec. F-M.
(reannotated) 100.0 0.4 0.7
(also called) 100.0 0.4 0.7
(formerly) 96.9 10.7 19.2
(originally) 100.0 1.4 2.8
((also)? known as) 100.0 1.8 3.4
(were termed) 100.0 0.4 0.7
(identity of) 100.0 0.7 1.4
(be referred (to|as)?) 100.0 0.4 0.7
(new designation) 100.0 0.4 0.7
( allel\w+) 80.0 2.8 5.4
(split into) 100.0 0.4 0.7
( rename ) 83.4 1.8 3.4
( renamed ) 88.5 8.0 14.6
( renaming ) 100.0 0.4 0.7
(E(\.|scherichia) coli) 11.3 4.5 6.4
Table 5: Handwritten patterns. Scores are in percentage
on the training and development sets together after the
gene nomenclature filtering step. A very low precision
means the pattern could be used to filter out rather than
in.
3.2 Error analysis
The false positive errors of the sentence filter-
ing step, using hand-written patterns can be clas-
sified as follows: 1) omission: Characteriza-
tion of abn2 (yxiA), encoding a Bacillus subtilis
GH43 arabinanase, Abn2, and its role in arabino-
126
before filtering after nomenclature after SVM final evaluation0
10
20
30
40
50
60
70
80
90
100
F-m
eas
ure
 (%
)
4.1
53.5
71.7 69.2
4.1
53.5
80.8 78.3
64.4
73.1
DevDev with patternsTestTest with patterns
Figure 2: Evolution of F-measure at different measure
points for the Rename task. Dev: training on train set
and testing on dev set. Test: training on train + dev sets
and testing on test set (no intermediary measure). 64.4%
is the official submitted score. 73.1% is the best score
achieved by the system on the test set.
polysaccharide degradation. (PMID 18408032). In
this case the sentence has been filtered out by the
SVM and then the couple abn2/yxia was not an-
notated as a renaming act, 2) incorrect informa-
tion in the nomenclature: These results substanti-
ate the view that sigE is the distal member of a
2-gene operon and demonstrate that the upstream
gene (spoIIGA) is necessary for sigma E forma-
tion. (PMID 2448286). Here, the integration of
the Genetic Map to the nomenclature has introduced
a wrong synonymy relation between spoIIGA and
sigE, 3) homology with another species: We report
the cloning of the wild-type allele of divIVB1 and
show that the mutation lies within a stretch of DNA
containing two open reading frames whose pre-
dicted products are in part homologous to the prod-
ucts of the Escherichia coli minicell genes minC and
minD. (PMID 1400224). The name pair actually
exists in the nomenclature but here, divIVB1 is a
gene of B. subtilis and minC is a gene of E. Coli,
4) another problem linked to the lexicon is the fact
the synonym classes are not disjoint. Some depre-
cated names of given genes are reused as canoni-
cal names of other genes. For example, purF and
purB referred to two different genes of B. subtilis
but purB was also formerly known as purF: The
following gene order has been established: pbuG-
purB-purF-purM-purH-purD-tre (PMID 3125411).
Hence, purF and purB are uncorrectly recognized
as synonyms while they refer to two different genes
in this context. Possible solutions for improving the
system could be: 1) the inclusion of species names
as SVM features, 2) the removal of some couples
from the nomenclature (PurF/purB for instance),
3) evaluate the benefits of each resource part of the
nomenclature.
4 Conclusion
Our system detects renaming acts of Bacillus sub-
tilis genes with a final F-measure of 64.4%. Af-
ter sentence segmentation, the emphasis is on sen-
tence filtering using an exhaustive nomenclature and
a SVM. An amelioration of this method using pat-
terns as features of the machine learning algorithm
was shown to improve significantly (+8.7%) the fi-
nal performance. It was also shown that the bag of
words representation is sub-optimal for text classi-
fication experiments (Fagan, 1987; Caropreso and
Matwin, 2006) With the use of such patterns, the fil-
tering step is now very efficient. The examination
of the remaining errors showed the limits of the cur-
rent shallow system. A deeper linguistic approach
using syntactic parsing seems indicated to improve
the filtering step further.
Acknowledgments
The authors would like to thank Julien Jourde for
granting them the permission to use the Bacteria
subtilis synonym nomenclature that he is currently
building and Philippe Veber for his insightful ad-
vices on text classification. This research is partly
founded by the French Oseo QUAERO project.
References
A. Bairoch and R. Apweiler. 1996. The SWISS-PROT
protein sequence data bank and its new supplement
TREMBL. Nucleic Acids Research, 24(1):21.
A. Bairoch, R. Apweiler, C.H. Wu, W.C. Barker,
B. Boeckmann, S. Ferro, E. Gasteiger, H. Huang,
R. Lopez, M. Magrane, and Others. 2005. The uni-
versal protein resource (UniProt). Nucleic Acids Re-
search, 33(suppl 1):D154.
127
D.A. Benson, I. Karsch-Mizrachi, D.J. Lipman, J. Ostell,
and D.L. Wheeler. 2008. GenBank. Nucleic acids
research, 36(suppl 1):D25.
M. Caropreso and S. Matwin. 2006. Beyond the Bag of
Words: A Text Representation for Sentence Selection.
Advances in Artificial Intelligence, pages 324?335.
J.L. Fagan. 1987. Experiments in automatic phrase in-
dexing for document retrieval: a comparison of syn-
tactic and non-syntactic methods.
LA Flo?rez, SF Roppel, A.G. Schmeisky, C.R. Lammers,
and J. Stu?lke. 2009. A community-curated consensual
annotation that is continuously updated: the Bacillus
subtilis centred wiki SubtiWiki. Database: The Jour-
nal of Biological Databases and Curation, 2009.
A Goelzer, B Brikci, I Martin-Verstraete, P Noirot,
P Bessie`res, S Aymerich, and V Fromion. 2008. Re-
construction and analysis of the genetic and metabolic
regulatory networks of the central metabolism of
Bacillus subtilis. BMC systems biology, 2(1):20.
M Hall, E Frank, G Holmes, B Pfahringer, P Reute-
mann, and I H Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Lynette Hirschman, Alexander Yeh, Christian Blaschke,
and Alfonso Valencia. 2005. Overview of BioCre-
AtIvE: critical assessment of information extraction
for biology. BMC bioinformatics, 6 Suppl 1:S1, Jan-
uary.
P. Lechat, L. Hummel, S. Rousseau, and I. Moszer. 2008.
GenoList: an integrated environment for comparative
analysis of microbial genomes. Nucleic Acids Re-
search, 36(suppl 1):D469.
C. Nedellec, M. Abdel Vetah, and Philippe Bessie`res.
2001. Sentence filtering for information extraction in
genomics, a classification problem. Principles of Data
Mining and Knowledge Discovery, pages 326?337.
C Ne?dellec, A Nazarenko, and R Bossy. 2009. Infor-
mation Extraction. Handbook on Ontologies, pages
663?685.
K E Nelson, I T Paulsen, J F Heidelberg, and C M
Fraser. 2000. Status of genome projects for non-
pathogenic bacteria and archaea. Nature biotechnol-
ogy, 18(10):1049?54, October.
A. Ogiwara, N. Ogasawara, M. Watanabe, and T. Tak-
agi. 1996. Construction of the Bacillus subtilis ORF
database (BSORF DB). Genome Informatics, pages
228?229.
H Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees.
P. Sterk, P.J. Kersey, and R. Apweiler. 2006. Genome
reviews: standardizing content and representation of
information about complete genomes. Omics: a jour-
nal of integrative biology, 10(2):114?118.
Davy Weissenbacher. 2004. La relation de synonymie en
Ge?nomique. RECITAL.
J. Weston, S. Mukherjee, O Chapelle, M. Pontil, T. Pog-
gio, and V. Vapnik. 2001. Feature selection for SVMs.
Advances in neural information processing systems,
pages 668?674.
Hong Yu, Vasileios Hatzivassiloglou, Carol Friedman,
Andrey Rzhetsky, and W.J. Wilbur. 2002. Automatic
extraction of gene and protein synonyms from MED-
LINE and journal articles. In Proceedings of the AMIA
Symposium, page 919. American Medical Informatics
Association.
128
A Gene or operon couple matching pattern
Pattern that uses bacteria gene naming rules (3 lower
case + 1 upper case letters), short genes (3 lower
case letters), long gene names, factorized operons
(3 lower case + several upper case letters), gene
names including special and/or numerical characters
in presence or not of signal words such as named,
renamed, formerly, formally, here, herein, here-
after, now, previously, as, designated, termed and/or
called, only if the pattern does not begin with and
or orf. Although this pattern could be used to di-
rectly filter in sentences containing a renaming act,
its recall is too low thus it is used as a feature of the
classifier instead.
and|orf\
GENE|OPERON-fact\
[|((now|as|previously|formerly|formally|here(in|after))\
((re)named|called|designated|termed) (now|as|previously|formerly|formally|here(in|after))\
GENE|OPERON-fact)|]
Table 6: Long pattern used for gene pair matching.
Terms matched Pattern PMID
short-GENE (short-GENE) cotA (formerly pig) 8759849
long-GENE (long-GENE) cotSA (ytxN) 10234840
fact-OPERON (fact-OPERON) ntdABC (formally yhjLKJ) 14612444
spe-GENE (spe-GENE) lpa-8 (sfp) 10471562
GENE (GENE) cwlB [lytC] 8759849
GENE (now designated GENE) yfiA (now designated glvR) 11489864
GENE (previously GENE) nhaC (previously yheL) 11274110
GENE (formerly called GENE) bkdR (formerly called yqiR) 10094682
GENE (now termed GENE) yqgR (now termed glcK) 9620975
GENE (GENE) other forms fosB(yndN) 11244082
GENE (hereafter renamed GENE) yhdQ (hereafter renamed cueR) 14663075
GENE (herein renamed GENE) yqhN (herein renamed mntR) 10760146
GENE (formally GENE) ntdR (formally yhjM) 14612444
GENE (formerly GENE) mtnK (formerly ykrT) 11545674
GENE (renamed GENE) yfjS (renamed pdaA) 12374835
GENE (named GENE) yvcE (named cwlO) 16233686
GENE (GENE) pdaA (yfjS) 14679227
Table 7: Examples matched with the long pattern.
129
